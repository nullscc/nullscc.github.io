
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.AS_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/eess.AS_2023_07_18/" class="article-date">
  <time datetime="2023-07-17T16:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/eess.AS_2023_07_18/">eess.AS - 2023-07-18 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Low-bit-rate-binaural-link-for-improved-ultra-low-latency-low-complexity-multichannel-speech-enhancement-in-Hearing-Aids"><a href="#Low-bit-rate-binaural-link-for-improved-ultra-low-latency-low-complexity-multichannel-speech-enhancement-in-Hearing-Aids" class="headerlink" title="Low bit rate binaural link for improved ultra low-latency low-complexity multichannel speech enhancement in Hearing Aids"></a>Low bit rate binaural link for improved ultra low-latency low-complexity multichannel speech enhancement in Hearing Aids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08858">http://arxiv.org/abs/2307.08858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nils L. Westhausen, Bernd T. Meyer</li>
<li>for: 提高听力器的听音质量</li>
<li>methods: 使用深度学习模型和 Filter-and-Sum 处理，并采用量化承载和集群通信来减少模型大小和计算负担</li>
<li>results: 可以匹配 oracle 双耳 LCMV 扬声器在非低延迟配置下的表现，且只需2毫秒的延迟Here’s the Chinese text in the format you requested:</li>
<li>for: 提高听力器的听音质量</li>
<li>methods: 使用深度学习模型和 Filter-and-Sum 处理，并采用量化承载和集群通信来减少模型大小和计算负担</li>
<li>results: 可以匹配 oracle 双耳 LCMV 扬声器在非低延迟配置下的表现，且只需2毫秒的延迟<details>
<summary>Abstract</summary>
Speech enhancement in hearing aids is a challenging task since the hardware limits the number of possible operations and the latency needs to be in the range of only a few milliseconds. We propose a deep-learning model compatible with these limitations, which we refer to as Group-Communication Filter-and-Sum Network (GCFSnet). GCFSnet is a causal multiple-input single output enhancement model using filter-and-sum processing in the time-frequency domain and a multi-frame deep post filter. All filters are complex-valued and are estimated by a deep-learning model using weight-sharing through Group Communication and quantization-aware training for reducing model size and computational footprint. For a further increase in performance, a low bit rate binaural link for delayed binaural features is proposed to use binaural information while retaining a latency of 2ms. The performance of an oracle binaural LCMV beamformer in non-low-latency configuration can be matched even by a unilateral configuration of the GCFSnet in terms of objective metrics.
</details>
<details>
<summary>摘要</summary>
听见助手中的语音提升是一项具有挑战性的任务，因为硬件的限制只能进行一定数量的操作，并且响应时间需要在几毫秒内。我们提议一种深度学习模型，称之为群组通信滤波和总网络（GCFSnet）。GCFSnet是一种 causal 多输入单出提升模型，使用时域频域的滤波和总处理，并使用多帧深度后 filters。所有滤波器都是复数值的，并由深度学习模型使用 weight-sharing 和量化感知训练来 estimates。为了进一步提高性能，我们提议使用低比特率双耳链接，以使用双耳信息而不超过2毫秒的响应时间。GCFSnet 的单边配置可以与非低延迟配置下的 oracle 双耳 LCMV 扫描器匹配，以对象指标来衡量。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-multi-channel-speaker-diarization-with-cross-channel-attention"><a href="#Semi-supervised-multi-channel-speaker-diarization-with-cross-channel-attention" class="headerlink" title="Semi-supervised multi-channel speaker diarization with cross-channel attention"></a>Semi-supervised multi-channel speaker diarization with cross-channel attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08688">http://arxiv.org/abs/2307.08688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilong Wu, Jun Du, Maokui He, Shutong Niu, Hang Chen, Haitao Tang, Chin-Hui Lee</li>
<li>for: 本研究提出了一种半监督式的Speaker diarization系统，以利用大规模多通道训练数据，并生成pseudo标签来labels无标签数据。</li>
<li>methods: 本研究引入了 Cross-channel attention机制，以更好地学习speaker embedding的通道上下文信息。</li>
<li>results: 在CHiME-7 Mixer6数据集上，我们的系统比基于分 clustering 模型的开发集上的相对减少率为57.01%。在CHiME-6数据集上，当使用80%和50%标签的训练数据时，我们的系统与使用100%标签的训练数据相对性能相似。<details>
<summary>Abstract</summary>
Most neural speaker diarization systems rely on sufficient manual training data labels, which are hard to collect under real-world scenarios. This paper proposes a semi-supervised speaker diarization system to utilize large-scale multi-channel training data by generating pseudo-labels for unlabeled data. Furthermore, we introduce cross-channel attention into the Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding (NSD-MA-MSE) to learn channel contextual information of speaker embeddings better. Experimental results on the CHiME-7 Mixer6 dataset which only contains partial speakers' labels of the training set, show that our system achieved 57.01% relative DER reduction compared to the clustering-based model on the development set. We further conducted experiments on the CHiME-6 dataset to simulate the scenario of missing partial training set labels. When using 80% and 50% labeled training data, our system performs comparably to the results obtained using 100% labeled data for training.
</details>
<details>
<summary>摘要</summary>
大多数神经网络发音分类系统都需要充足的手动训练数据标签，这些标签在实际场景下很难收集。这篇论文提议一种半监督的发音分类系统，利用大规模多通道训练数据生成 pseudo-标签，以便于无标签数据的学习。此外，我们在 Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding (NSD-MA-MSE) 中引入了ChannelContextual Information的权重学习，以更好地学习发音者特征的通道信息。实验结果表明，在 CHiME-7 Mixer6 数据集上，我们的系统与 clustering-based 模型在开发集上实现了57.01%的相对减少性能。我们进一步在 CHiME-6 数据集上进行了 simulate 失去部分训练集标签的场景，当使用 80% 和 50% 标签过滤的训练数据时，我们的系统与使用 100% 标签训练的结果相当。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/eess.AS_2023_07_18/" data-id="cllta0lio006bny886shl61tg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/eess.IV_2023_07_18/" class="article-date">
  <time datetime="2023-07-17T16:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/eess.IV_2023_07_18/">eess.IV - 2023-07-18 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Regression-free-Blind-Image-Quality-Assessment"><a href="#Regression-free-Blind-Image-Quality-Assessment" class="headerlink" title="Regression-free Blind Image Quality Assessment"></a>Regression-free Blind Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09279">http://arxiv.org/abs/2307.09279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XiaoqiWang/regression-free-iqa">https://github.com/XiaoqiWang/regression-free-iqa</a></li>
<li>paper_authors: Xiaoqi Wang, Jian Xiong, Hao Gao, Weisi Lin</li>
<li>for: 提高图像质量评估模型的准确性，避免因训练样本偏袋而导致的模型参数估计偏离 reality。</li>
<li>methods: 基于检索相似图像的快速准确评估方法，包括semantic-based classification（SC）模块和distortion-based classification（DC）模块。</li>
<li>results: 对四个标准数据库进行实验，研究发现该方法可以remarkably outperform当前最佳的 regression-based 模型。<details>
<summary>Abstract</summary>
Regression-based blind image quality assessment (IQA) models are susceptible to biased training samples, leading to a biased estimation of model parameters. To mitigate this issue, we propose a regression-free framework for image quality evaluation, which is founded upon retrieving similar instances by incorporating semantic and distortion features. The motivation behind this approach is rooted in the observation that the human visual system (HVS) has analogous visual responses to semantically similar image contents degraded by the same distortion. The proposed framework comprises two classification-based modules: semantic-based classification (SC) module and distortion-based classification (DC) module. Given a test image and an IQA database, the SC module retrieves multiple pristine images based on semantic similarity. The DC module then retrieves instances based on distortion similarity from the distorted images that correspond to each retrieved pristine image. Finally, the predicted quality score is derived by aggregating the subjective quality scores of multiple retrieved instances. Experimental results on four benchmark databases validate that the proposed model can remarkably outperform the state-of-the-art regression-based models.
</details>
<details>
<summary>摘要</summary>
“受训数据受损”问题导致抽象� returns blind图像质量评估（IQA）模型受损。为了解决这个问题，我们提出了一种不含回归的图像质量评估框架，基于检索相似实例。我们发现，人视系统（HVS）在Semantic� 相似的图像内容下具有相似的视觉响应，这成为我们的 Motivation。该框架包括两个分类模块：Semantic-based Classification（SC）模块和Distortion-based Classification（DC）模块。给定一个测试图像和IQA数据库，SC模块首先检索相似的整图，然后DC模块从相应的扭曲图像中检索具有相同扭曲的实例。最后，预测的质量分数由多个检索到的实例的主观质量分数进行汇总得来。实验结果表明，我们提出的模型可以很好地超越当前的回归型模型。
</details></li>
</ul>
<hr>
<h2 id="Soft-IntroVAE-for-Continuous-Latent-space-Image-Super-Resolution"><a href="#Soft-IntroVAE-for-Continuous-Latent-space-Image-Super-Resolution" class="headerlink" title="Soft-IntroVAE for Continuous Latent space Image Super-Resolution"></a>Soft-IntroVAE for Continuous Latent space Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09008">http://arxiv.org/abs/2307.09008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Song Liu, Zijia Wang, Zhen Jia</li>
<li>for: 这个研究是为了提出一个基于Variational AutoEncoder的连续图像超解析方法，以提供实用和灵活的图像扩展 для不同的显示器。</li>
<li>methods: 本研究使用了Local implicit image representation来将坐标和2D特征映射到隐藏空间中，并通过一种新的潜在空间对抗训练来实现照相实际的图像重建。</li>
<li>results: 研究人员透过量化和质感比较，证明了提案的Soft-introVAE-SR方法的效果，并且显示了其在对照噪声和实际图像超解析中的一般化能力。<details>
<summary>Abstract</summary>
Continuous image super-resolution (SR) recently receives a lot of attention from researchers, for its practical and flexible image scaling for various displays. Local implicit image representation is one of the methods that can map the coordinates and 2D features for latent space interpolation. Inspired by Variational AutoEncoder, we propose a Soft-introVAE for continuous latent space image super-resolution (SVAE-SR). A novel latent space adversarial training is achieved for photo-realistic image restoration. To further improve the quality, a positional encoding scheme is used to extend the original pixel coordinates by aggregating frequency information over the pixel areas. We show the effectiveness of the proposed SVAE-SR through quantitative and qualitative comparisons, and further, illustrate its generalization in denoising and real-image super-resolution.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>latest continuous image super-resolution (SR) technology has gained significant attention from researchers due to its practical and flexible image scaling capabilities for various displays. local implicit image representation is a method that can map coordinates and 2D features to latent space for interpolation. inspired by Variational AutoEncoder, we propose a Soft-introVAE for continuous latent space image super-resolution (SVAE-SR). a novel latent space adversarial training is achieved for photo-realistic image restoration. to further improve quality, a positional encoding scheme is used to extend the original pixel coordinates by aggregating frequency information over pixel areas. we demonstrate the effectiveness of the proposed SVAE-SR through quantitative and qualitative comparisons, and further illustrate its generalization in denoising and real-image super-resolution.Here's the translation in Traditional Chinese:<<SYS>>将文本翻译成简化中文。</SYS>>最新的连续图像超解析（SR）技术在研究人员中获得了很大的关注，因为它具有实用和 flexible 的图像扩展功能 для多种显示器。本地隐式图像表示是一种可以将坐标和2D特征映射到 latent space 中的方法，以便进行插值。受 Variational AutoEncoder 的启发，我们提议了 Soft-introVAE  для连续 latent space 图像超解析（SVAE-SR）。我们还实现了一种新的 latent space 反击训练，以达到真实图像 Restoration。为了进一步提高质量，我们使用了一个位置编码方案，将原始像素坐标与像素区域的频率信息聚合。我们显示了 SVAE-SR 的效果，通过量itative和质感比较，并进一步显示其扩展到干扰和真实图像超解析。
</details></li>
</ul>
<hr>
<h2 id="Frequency-mixed-Single-source-Domain-Generalization-for-Medical-Image-Segmentation"><a href="#Frequency-mixed-Single-source-Domain-Generalization-for-Medical-Image-Segmentation" class="headerlink" title="Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation"></a>Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09005">http://arxiv.org/abs/2307.09005</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamheng/non-iid_medical_image_segmentation">https://github.com/liamheng/non-iid_medical_image_segmentation</a></li>
<li>paper_authors: Heng Li, Haojin Li, Wei Zhao, Huazhu Fu, Xiuyun Su, Yan Hu, Jiang Liu</li>
<li>for: 提高医疗影像分类模型的普遍性，特别是当标注数据短缺时。</li>
<li>methods: 提出了一个叫做“频率混合单源领域普遍化法”（FreeSDG），利用不同频率的混合 Spectrum 来增强单源领域，同时运用自我监督来学习具有上下文感知的表示。</li>
<li>results: 实验结果显示，FreeSDG 比前一代方法更有效率，可以优化医疗影像分类模型的普遍性，特别是当标注数据短缺时。<details>
<summary>Abstract</summary>
The annotation scarcity of medical image segmentation poses challenges in collecting sufficient training data for deep learning models. Specifically, models trained on limited data may not generalize well to other unseen data domains, resulting in a domain shift issue. Consequently, domain generalization (DG) is developed to boost the performance of segmentation models on unseen domains. However, the DG setup requires multiple source domains, which impedes the efficient deployment of segmentation algorithms in clinical scenarios. To address this challenge and improve the segmentation model's generalizability, we propose a novel approach called the Frequency-mixed Single-source Domain Generalization method (FreeSDG). By analyzing the frequency's effect on domain discrepancy, FreeSDG leverages a mixed frequency spectrum to augment the single-source domain. Additionally, self-supervision is constructed in the domain augmentation to learn robust context-aware representations for the segmentation task. Experimental results on five datasets of three modalities demonstrate the effectiveness of the proposed algorithm. FreeSDG outperforms state-of-the-art methods and significantly improves the segmentation model's generalizability. Therefore, FreeSDG provides a promising solution for enhancing the generalization of medical image segmentation models, especially when annotated data is scarce. The code is available at https://github.com/liamheng/Non-IID_Medical_Image_Segmentation.
</details>
<details>
<summary>摘要</summary>
医学影像分割的标注缺乏问题使得深度学习模型的训练数据不够，这会导致模型在未见的数据域上不好地泛化。为了解决这个问题，域泛化（DG）技术被开发出来，以提高分割模型在未见的数据域上的性能。然而，DG设置需要多个源域，这阻碍了临床应用中的深度学习模型的有效部署。为了解决这个挑战并提高分割模型的泛化性，我们提出了一种新的方法：频率混合单源域泛化方法（FreeSDG）。通过分析频率对域差异的效果，FreeSDG利用混合频率谱来扩展单源域。此外，我们还构建了基于频率域的自我超vision来学习Context-aware表示。实验结果表明，FreeSDG方法可以高效地提高分割模型的泛化性。我们对五个数据集进行了五种modalities的实验，并证明FreeSDG方法可以与当前状态的方法相比，显著提高分割模型的泛化性。因此，FreeSDG方法提供了一种有效的解决医学影像分割模型的标注缺乏问题的方法，特别是当 annotated data scarce 时。代码可以在 <https://github.com/liamheng/Non-IID_Medical_Image_Segmentation> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learned-Scalable-Video-Coding-For-Humans-and-Machines"><a href="#Learned-Scalable-Video-Coding-For-Humans-and-Machines" class="headerlink" title="Learned Scalable Video Coding For Humans and Machines"></a>Learned Scalable Video Coding For Humans and Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08978">http://arxiv.org/abs/2307.08978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Hadizadeh, Ivan V. Bajić</li>
<li>for: 这个论文主要是为了支持自动视频分析，而不是人类视觉。</li>
<li>methods: 该论文使用了深度神经网络（DNN）来实现视频编码，并使用了 conditional coding 来提高压缩效果。</li>
<li>results: 实验结果表明，该系统在基层和优化层中都可以实现更好的压缩效果，并且可以在机器视觉任务和人类视觉任务之间进行可替换。<details>
<summary>Abstract</summary>
Video coding has traditionally been developed to support services such as video streaming, videoconferencing, digital TV, and so on. The main intent was to enable human viewing of the encoded content. However, with the advances in deep neural networks (DNNs), encoded video is increasingly being used for automatic video analytics performed by machines. In applications such as automatic traffic monitoring, analytics such as vehicle detection, tracking and counting, would run continuously, while human viewing could be required occasionally to review potential incidents. To support such applications, a new paradigm for video coding is needed that will facilitate efficient representation and compression of video for both machine and human use in a scalable manner. In this manuscript, we introduce the first end-to-end learnable video codec that supports a machine vision task in its base layer, while its enhancement layer supports input reconstruction for human viewing. The proposed system is constructed based on the concept of conditional coding to achieve better compression gains. Comprehensive experimental evaluations conducted on four standard video datasets demonstrate that our framework outperforms both state-of-the-art learned and conventional video codecs in its base layer, while maintaining comparable performance on the human vision task in its enhancement layer. We will provide the implementation of the proposed system at www.github.com upon completion of the review process.
</details>
<details>
<summary>摘要</summary>
视频编码传统上是为服务如视频流式、视频会议、数字电视等服务开发的。主要目的是为人类观看编码内容。然而，随着深度神经网络（DNN）的发展，编码的视频已经在机器自动分析中得到了广泛的应用。例如，在自动交通监测应用中，机器可以通过视频分析来探测、跟踪和计数交通车辆。在这些应用中，人类只需 occasionally 查看可能的意外事件。为支持这些应用，我们需要一种新的视频编码 paradigma，可以帮助高效地表示和压缩视频，以便同时支持机器和人类的使用。在这篇论文中，我们介绍了首个可学习的视频编码器，其基层支持机器视觉任务，而增强层支持人类视觉输入重建。我们的系统基于 conditional coding 的概念，以实现更好的压缩收益。我们在四个标准视频数据集上进行了广泛的实验评估，并证明了我们的框架在基层上比现状 learned 和 conventional 视频编码器更高效，而且在人类视觉任务的增强层中保持了相似的性能。我们将在 GitHub 上提供实现的 propose 系统。
</details></li>
</ul>
<hr>
<h2 id="Deep-Physics-Guided-Unrolling-Generalization-for-Compressed-Sensing"><a href="#Deep-Physics-Guided-Unrolling-Generalization-for-Compressed-Sensing" class="headerlink" title="Deep Physics-Guided Unrolling Generalization for Compressed Sensing"></a>Deep Physics-Guided Unrolling Generalization for Compressed Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08950">http://arxiv.org/abs/2307.08950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guaishou74851/prl">https://github.com/guaishou74851/prl</a></li>
<li>paper_authors: Bin Chen, Jiechong Song, Jingfen Xie, Jian Zhang</li>
<li>for: 这篇论文主要是为了提出一种高精度且可解释的图像重建方法，兼顾了模型驱动和数据驱动方法的优点，以解决 inverse imaging  зада题中的问题。</li>
<li>methods: 这篇论文提出了一种基于高维特征空间的Physics-guided unrolled recovery learning（PRL）框架，通过普通迭代法实现高精度的图像重建。此外，作者还提出了两种实现方式：PRL-PGD和PRL-RND。</li>
<li>results: 实验表明，PRL 网络比其他状态 искусственный方法具有显著的性能和效率优势，并且还有很大的应用前景，可以应用于其他 inverse imaging 问题或优化模型。<details>
<summary>Abstract</summary>
By absorbing the merits of both the model- and data-driven methods, deep physics-engaged learning scheme achieves high-accuracy and interpretable image reconstruction. It has attracted growing attention and become the mainstream for inverse imaging tasks. Focusing on the image compressed sensing (CS) problem, we find the intrinsic defect of this emerging paradigm, widely implemented by deep algorithm-unrolled networks, in which more plain iterations involving real physics will bring enormous computation cost and long inference time, hindering their practical application. A novel deep $\textbf{P}$hysics-guided un$\textbf{R}$olled recovery $\textbf{L}$earning ($\textbf{PRL}$) framework is proposed by generalizing the traditional iterative recovery model from image domain (ID) to the high-dimensional feature domain (FD). A compact multiscale unrolling architecture is then developed to enhance the network capacity and keep real-time inference speeds. Taking two different perspectives of optimization and range-nullspace decomposition, instead of building an algorithm-specific unrolled network, we provide two implementations: $\textbf{PRL-PGD}$ and $\textbf{PRL-RND}$. Experiments exhibit the significant performance and efficiency leading of PRL networks over other state-of-the-art methods with a large potential for further improvement and real application to other inverse imaging problems or optimization models.
</details>
<details>
<summary>摘要</summary>
通过吸收模型和数据驱动方法的优点，深度物理参与学习方案实现高精度和可解释的图像重建。它在反射图像任务中吸引了越来越多的关注，成为主流。但是，对于图像压缩感知（CS）问题，我们发现了深度算法拆箱网络广泛实施的内在缺陷：更多的简单迭代 iterations 会带来巨大的计算成本和长时间推理时间，限制其实际应用。为解决这个问题，我们提出了一种深度物理指导的解析学习（PRL）框架，通过将传统的迭代回归模型从图像域（ID）扩展到高维特征域（FD），提高网络容量和保持实时推理速度。此外，我们还开发了一种嵌入式多尺度拆箱架构，以增强网络的扩展性和灵活性。为了实现PRL网络的实现，我们提出了两种实现方法：PRL-PGD和PRL-RND。首先，我们使用权值迭代（PGD）方法来实现PRL网络的迭代过程，其中每个迭代都是一个简单的PGD过程。其次，我们使用几何范围零空间分解（RND）方法来实现PRL网络的解析过程，这种方法可以快速地解决图像的缺失信息。实验结果表明，PRL网络在反射图像任务中表现出色，与其他当前最佳方法相比，具有显著的性能和效率优势，同时还有很大的潜在提升空间和实际应用前景。
</details></li>
</ul>
<hr>
<h2 id="Image-Processing-Methods-Applied-to-Motion-Tracking-of-Nanomechanical-Buckling-on-SEM-Recordings"><a href="#Image-Processing-Methods-Applied-to-Motion-Tracking-of-Nanomechanical-Buckling-on-SEM-Recordings" class="headerlink" title="Image Processing Methods Applied to Motion Tracking of Nanomechanical Buckling on SEM Recordings"></a>Image Processing Methods Applied to Motion Tracking of Nanomechanical Buckling on SEM Recordings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08786">http://arxiv.org/abs/2307.08786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ege Erdem, Berke Demiralp, Hadi S Pisheh, Peyman Firoozy, Ahmet Hakan Karakurt, M. Selim Hanay</li>
<li>for: 这个论文是为了解决扫描电子显微镜（SEM）记录的动态纳米电romechanical系统（NEMS）的问题，因为噪声引起的低帧率、不足的分辨率和由应用的电 potential所引起的模糊。</li>
<li>methods: 这个论文使用了一种基于物理系统的图像处理算法，用于跟踪NEMS结构在高噪声水平下的动态运动。该算法包括一个图像滤波器、两个数据滤波器和一个非线性回归模型，利用物理解决方案的预期形式。</li>
<li>results: 该算法可以跟踪NEMS的动态运动和捕捉了压缩力对矩形杆的弯曲强度的依赖关系。通过该算法，可以清晰地分解NEMS在SEM记录中的转换从间隙弯曲到内隙弯曲的过程。<details>
<summary>Abstract</summary>
The scanning electron microscope (SEM) recordings of dynamic nano-electromechanical systems (NEMS) are difficult to analyze due to the noise caused by low frame rate, insufficient resolution and blurriness induced by applied electric potentials. Here, we develop an image processing algorithm enhanced by the physics of the underlying system to track the motion of buckling NEMS structures in the presence of high noise levels. The algorithm is composed of an image filter, two data filters, and a nonlinear regression model, which utilizes the expected form of the physical solution. The method was applied to the recordings of a NEMS beam about 150 nm wide, undergoing intra-and inter-well post-buckling states with a transition rate of approximately 0.5 Hz. The algorithm can track the dynamical motion of the NEMS and capture the dependency of deflection amplitude on the compressive force on the beam. With the help of the proposed algorithm, the transition from inter-well to intra-well motion is clearly resolved for buckling NEMS imaged under SEM.
</details>
<details>
<summary>摘要</summary>
电子透镜记录动态纳米电romechanical系统（NEMS）具有较低的帧率、不够的分辨率和应用电压所引起的噪声，使得分析变得困难。在这种情况下，我们开发了一种基于系统物理学的图像处理算法，用于跟踪NEMS结构在高噪声水平下的动态运动。该算法包括一个图像滤波器、两个数据滤波器和一个非线性回归模型，其利用了系统物理学的预期解。该方法应用于一个宽约150nm的NEMS梁，在0.5Hz的过渡率下进行了内部和外部受压变换。该算法可以跟踪NEMS的动态运动和捕捉压缩力的影响于梁的折弯幅度。通过提posed algorithm，对buckling NEMS的图像进行了清晰的分辨和解决。
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-a-perception-system-for-autonomous-vehicles-using-a-detection-segmentation-network-in-SoC-FPGA"><a href="#Implementation-of-a-perception-system-for-autonomous-vehicles-using-a-detection-segmentation-network-in-SoC-FPGA" class="headerlink" title="Implementation of a perception system for autonomous vehicles using a detection-segmentation network in SoC FPGA"></a>Implementation of a perception system for autonomous vehicles using a detection-segmentation network in SoC FPGA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08682">http://arxiv.org/abs/2307.08682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vision-agh/mt_kria">https://github.com/vision-agh/mt_kria</a></li>
<li>paper_authors: Maciej Baczmanski, Mateusz Wasala, Tomasz Kryjak</li>
<li>for: 本研究旨在开发一种高效、实时、能效的自动驾驶感知控制系统，以满足不同道路条件下的障碍物识别和环境元素识别等功能要求。</li>
<li>methods: 本文使用MultiTaskV3检测分割网络作为感知系统的基础，并对其进行了适当的训练、量化和实现于AMD Xilinx Kria KV260 Vision AI嵌入式平台。通过这种设备，可以并行加速计算，同时减少能耗。</li>
<li>results: 实验结果显示，该系统在对象检测和图像分割方面具有高度准确性（mAP大于97%和mIoU大于90%），并且在实时性和能效性方面也具有优异表现。<details>
<summary>Abstract</summary>
Perception and control systems for autonomous vehicles are an active area of scientific and industrial research. These solutions should be characterised by high efficiency in recognising obstacles and other environmental elements in different road conditions, real-time capability, and energy efficiency. Achieving such functionality requires an appropriate algorithm and a suitable computing platform. In this paper, we have used the MultiTaskV3 detection-segmentation network as the basis for a perception system that can perform both functionalities within a single architecture. It was appropriately trained, quantised, and implemented on the AMD Xilinx Kria KV260 Vision AI embedded platform. By using this device, it was possible to parallelise and accelerate the computations. Furthermore, the whole system consumes relatively little power compared to a CPU-based implementation (an average of 5 watts, compared to the minimum of 55 watts for weaker CPUs, and the small size (119mm x 140mm x 36mm) of the platform allows it to be used in devices where the amount of space available is limited. It also achieves an accuracy higher than 97% of the mAP (mean average precision) for object detection and above 90% of the mIoU (mean intersection over union) for image segmentation. The article also details the design of the Mecanum wheel vehicle, which was used to test the proposed solution in a mock-up city.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆的感知和控制系统是科学技术和工业领域的活跃领域。这些解决方案应具备高效地认知障碍物和其他环境元素，实时性和能效性。实现这种功能需要适当的算法和适当的计算平台。在这篇论文中，我们使用了MultiTaskV3检测-分割网络作为感知系统的基础，可以同时完成这两个功能 within 一个架构。它被正确地训练、量化和在AMD Xilinx Kria KV260 Vision AI嵌入式平台上实现。通过使用这个设备，可以并行化和加速计算。此外，整个系统的功耗相对较少，只有5瓦特，比较弱的CPU实现的最低功耗55瓦特，而且平台的尺寸（119mm x 140mm x 36mm）也很小，可以在空间有限的设备中使用。它还实现了对 объек detection的准确率高于97%的mAP，以及对图像分割的准确率高于90%的mIoU。文章还详细介绍了使用的Mecanum轮胎车，该车在模拟城市中测试了提议的解决方案。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/eess.IV_2023_07_18/" data-id="cllta0lj5007xny88co7x673v" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/cs.LG_2023_07_17/" class="article-date">
  <time datetime="2023-07-16T16:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/cs.LG_2023_07_17/">cs.LG - 2023-07-17 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Study-on-the-Performance-of-Generative-Pre-trained-Transformer-GPT-in-Simulating-Depressed-Individuals-on-the-Standardized-Depressive-Symptom-Scale"><a href="#A-Study-on-the-Performance-of-Generative-Pre-trained-Transformer-GPT-in-Simulating-Depressed-Individuals-on-the-Standardized-Depressive-Symptom-Scale" class="headerlink" title="A Study on the Performance of Generative Pre-trained Transformer (GPT) in Simulating Depressed Individuals on the Standardized Depressive Symptom Scale"></a>A Study on the Performance of Generative Pre-trained Transformer (GPT) in Simulating Depressed Individuals on the Standardized Depressive Symptom Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08576">http://arxiv.org/abs/2307.08576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijin Cai, Nanfeng Zhang, Jiaying Zhu, Yanjie Liu, Yongjin Zhou<br>for:* 这个论文的目的是评估GPT技术在诊断抑郁症中的潜力。methods:* 这个论文使用了三种抑郁症评估工具（HAMD-17、SDS、GDS-15），并在两个实验中使用GPT模拟抑郁症和正常个体的响应。results:* GPT在抑郁症评估中表现出了准确的性能，能够与正常个体和抑郁症个体的反应保持一致。* GPT在不同的抑郁程度下的表现存在一些差异，其表现更好的是在高敏感度的评估工具上。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Background: Depression is a common mental disorder with societal and economic burden. Current diagnosis relies on self-reports and assessment scales, which have reliability issues. Objective approaches are needed for diagnosing depression. Objective: Evaluate the potential of GPT technology in diagnosing depression. Assess its ability to simulate individuals with depression and investigate the influence of depression scales. Methods: Three depression-related assessment tools (HAMD-17, SDS, GDS-15) were used. Two experiments simulated GPT responses to normal individuals and individuals with depression. Compare GPT's responses with expected results, assess its understanding of depressive symptoms, and performance differences under different conditions. Results: GPT's performance in depression assessment was evaluated. It aligned with scoring criteria for both individuals with depression and normal individuals. Some performance differences were observed based on depression severity. GPT performed better on scales with higher sensitivity. Conclusion: GPT accurately simulates individuals with depression and normal individuals during depression-related assessments. Deviations occur when simulating different degrees of depression, limiting understanding of mild and moderate cases. GPT performs better on scales with higher sensitivity, indicating potential for developing more effective depression scales. GPT has important potential in depression assessment, supporting clinicians and patients.
</details>
<details>
<summary>摘要</summary>
背景：抑郁是一种常见的心理疾病，带来社会和经济的负担。现在的诊断仍然基于自我报告和评估工具，这些工具的可靠性存在问题。需要更Objective的方法来诊断抑郁。目标：评估GPT技术在诊断抑郁方面的潜力。判断它能够模拟抑郁症状和调查不同程度的抑郁症状对响应。方法：使用三种抑郁相关的评估工具（HAMD-17、SDS、GDS-15）。进行两个实验，通过GPT对正常人和抑郁人群的响应进行模拟，与预期结果进行比较，评估它对抑郁症状的理解程度和不同情况下的表现差异。结果：GPT在抑郁诊断方面的表现被评估。它与正常人和抑郁人群的评分标准相符。有些情况下，随着抑郁的严重程度不同，GPT的表现会有所不同。GPT在感知度较高的评价工具上表现更好，表明GPT可能有助于开发更有效的抑郁评价工具。结论：GPT可以准确模拟正常人和抑郁人群在抑郁相关评估中的表现，但是在不同程度的抑郁中会出现一些差异。GPT在感知度较高的评价工具上表现更好，表明它可能有助于开发更有效的抑郁评价工具，支持临床医生和患者。
</details></li>
</ul>
<hr>
<h2 id="FedCME-Client-Matching-and-Classifier-Exchanging-to-Handle-Data-Heterogeneity-in-Federated-Learning"><a href="#FedCME-Client-Matching-and-Classifier-Exchanging-to-Handle-Data-Heterogeneity-in-Federated-Learning" class="headerlink" title="FedCME: Client Matching and Classifier Exchanging to Handle Data Heterogeneity in Federated Learning"></a>FedCME: Client Matching and Classifier Exchanging to Handle Data Heterogeneity in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08574">http://arxiv.org/abs/2307.08574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Nie, Danyang Xiao, Lei Yang, Weigang Wu</li>
<li>for: 本研究的目的是解决 federated learning 中的数据不同性问题，以提高模型的全球性性和性能。</li>
<li>methods: 本研究提出了一种新的 federated learning 框架，即 FedCME，通过客户端匹配和分类器交换来解决数据不同性问题。在本方法中，客户端的匹配和分类器交换可以更好地调整本地模型的训练方向，从而缓解本地更新偏移。此外，本研究还提出了一种特征对齐方法来增强特征提取器的训练。</li>
<li>results: 实验结果表明，FedCME 比 FedAvg、FedProx、MOON 和 FedRS 在 FMNIST 和 CIFAR10 等常用 federated learning 测试 benchmark 上表现更好，特别在数据不同性的情况下。<details>
<summary>Abstract</summary>
Data heterogeneity across clients is one of the key challenges in Federated Learning (FL), which may slow down the global model convergence and even weaken global model performance. Most existing approaches tackle the heterogeneity by constraining local model updates through reference to global information provided by the server. This can alleviate the performance degradation on the aggregated global model. Different from existing methods, we focus the information exchange between clients, which could also enhance the effectiveness of local training and lead to generate a high-performance global model. Concretely, we propose a novel FL framework named FedCME by client matching and classifier exchanging. In FedCME, clients with large differences in data distribution will be matched in pairs, and then the corresponding pair of clients will exchange their classifiers at the stage of local training in an intermediate moment. Since the local data determines the local model training direction, our method can correct update direction of classifiers and effectively alleviate local update divergence. Besides, we propose feature alignment to enhance the training of the feature extractor. Experimental results demonstrate that FedCME performs better than FedAvg, FedProx, MOON and FedRS on popular federated learning benchmarks including FMNIST and CIFAR10, in the case where data are heterogeneous.
</details>
<details>
<summary>摘要</summary>
“数据不同性是联邦学习（FL）中的一个关键挑战，可能会 slow down 全球模型的融合和甚至弱化全球模型的性能。现有的方法通常通过对本地模型更新进行约束，通过服务器提供的全球信息进行参考。这可以减轻全球模型的性能下降。与现有方法不同，我们将注重客户端之间的信息交换，可以提高本地训练的效iveness，并通过生成高性能的全球模型。具体来说，我们提出了一种新的联邦学习框架，名为FedCME。在FedCME中，客户端之间的数据分布差异较大的将被匹配，并在本地训练阶段进行交换类型的分类器。由于本地数据决定本地模型训练方向，我们的方法可以正确更新类ifiers，有效地缓解本地更新偏移。此外，我们还提出了特征对齐来强化特征提取器的训练。实验结果表明，FedCME在FMNIST和CIFAR10等流行的联邦学习 benchmark 上表现比 FedAvg、FedProx、MOON 和 FedRS 更好，即使数据具有不同性。”
</details></li>
</ul>
<hr>
<h2 id="Revisiting-the-Robustness-of-the-Minimum-Error-Entropy-Criterion-A-Transfer-Learning-Case-Study"><a href="#Revisiting-the-Robustness-of-the-Minimum-Error-Entropy-Criterion-A-Transfer-Learning-Case-Study" class="headerlink" title="Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study"></a>Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08572">http://arxiv.org/abs/2307.08572</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lpsilvestrin/mee-finetune">https://github.com/lpsilvestrin/mee-finetune</a></li>
<li>paper_authors: Luis Pedro Silvestrin, Shujian Yu, Mark Hoogendoorn</li>
<li>for: 本研究旨在探讨在实际任务中常见的分布偏移问题下，使用传输学习方法达到良好性能。</li>
<li>methods: 本研究使用了最小错误Entropy（MEE） criterion，一种广泛用于统计信号处理中对非高斯噪声的优化目标，并investigated its feasibility和usefulness在实际传输学习回归任务中。</li>
<li>results: 研究发现，通过简单地将基本传输学习算法中的MSE损失换为MEE，可以达到与现状技术传输学习算法相当的性能。<details>
<summary>Abstract</summary>
Coping with distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-of-the-art transfer learning algorithms. We justify our arguments on both synthetic data and 5 real-world time-series data.
</details>
<details>
<summary>摘要</summary>
handle distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-of-the-art transfer learning algorithms. We justify our arguments on both synthetic data and 5 real-world time-series data.Here's the translation in Traditional Chinese:handle distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-of-the-art transfer learning algorithms. We justify our arguments on both synthetic data and 5 real-world time-series data.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-with-Passive-Optical-Nonlinear-Mapping"><a href="#Deep-Learning-with-Passive-Optical-Nonlinear-Mapping" class="headerlink" title="Deep Learning with Passive Optical Nonlinear Mapping"></a>Deep Learning with Passive Optical Nonlinear Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08558">http://arxiv.org/abs/2307.08558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Xia, Kyungduk Kim, Yaniv Eliezer, Liam Shaughnessy, Sylvain Gigan, Hui Cao</li>
<li>for: 这项研究旨在开发一种基于光学加速器的 Deep Learning 系统，以提高人工智能的性能和能效性。</li>
<li>methods: 该研究使用了多散射在反射室中的技术，实现了无需加速器的光学非线性随机映射，以提高计算性能。</li>
<li>results: 研究发现，通过光学数据压缩和数字解码器，可以实现高性能、高压缩比的实时人体检测和其他计算任务。<details>
<summary>Abstract</summary>
Deep learning has fundamentally transformed artificial intelligence, but the ever-increasing complexity in deep learning models calls for specialized hardware accelerators. Optical accelerators can potentially offer enhanced performance, scalability, and energy efficiency. However, achieving nonlinear mapping, a critical component of neural networks, remains challenging optically. Here, we introduce a design that leverages multiple scattering in a reverberating cavity to passively induce optical nonlinear random mapping, without the need for additional laser power. A key advantage emerging from our work is that we show we can perform optical data compression, facilitated by multiple scattering in the cavity, to efficiently compress and retain vital information while also decreasing data dimensionality. This allows rapid optical information processing and generation of low dimensional mixtures of highly nonlinear features. These are particularly useful for applications demanding high-speed analysis and responses such as in edge computing devices. Utilizing rapid optical information processing capabilities, our optical platforms could potentially offer more efficient and real-time processing solutions for a broad range of applications. We demonstrate the efficacy of our design in improving computational performance across tasks, including classification, image reconstruction, key-point detection, and object detection, all achieved through optical data compression combined with a digital decoder. Notably, we observed high performance, at an extreme compression ratio, for real-time pedestrian detection. Our findings pave the way for novel algorithms and architectural designs for optical computing.
</details>
<details>
<summary>摘要</summary>
A key advantage emerging from our work is that we show we can perform optical data compression, facilitated by multiple scattering in the cavity, to efficiently compress and retain vital information while also decreasing data dimensionality. This allows rapid optical information processing and generation of low-dimensional mixtures of highly nonlinear features. These are particularly useful for applications demanding high-speed analysis and responses such as in edge computing devices.Utilizing rapid optical information processing capabilities, our optical platforms could potentially offer more efficient and real-time processing solutions for a broad range of applications. We demonstrate the efficacy of our design in improving computational performance across tasks, including classification, image reconstruction, key-point detection, and object detection, all achieved through optical data compression combined with a digital decoder. Notably, we observed high performance, at an extreme compression ratio, for real-time pedestrian detection.Our findings pave the way for novel algorithms and architectural designs for optical computing.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-based-Colorectal-Tissue-Classification-via-Acoustic-Resolution-Photoacoustic-Microscopy"><a href="#Machine-Learning-based-Colorectal-Tissue-Classification-via-Acoustic-Resolution-Photoacoustic-Microscopy" class="headerlink" title="Machine-Learning-based Colorectal Tissue Classification via Acoustic Resolution Photoacoustic Microscopy"></a>Machine-Learning-based Colorectal Tissue Classification via Acoustic Resolution Photoacoustic Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08556">http://arxiv.org/abs/2307.08556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shangqing Tong, Peng Ge, Yanan Jiao, Zhaofu Ma, Ziye Li, Longhai Liu, Feng Gao, Xiaohui Du, Fei Gao</li>
<li>for: 检测肠癌的有效方法</li>
<li>methods: 使用机器学习基于ARPAM技术进行肠部细胞分类</li>
<li>results: 通过多种机器学习方法对肠部细胞进行分类，并对结果进行量化和质量分析以评估方法效果<details>
<summary>Abstract</summary>
Colorectal cancer is a deadly disease that has become increasingly prevalent in recent years. Early detection is crucial for saving lives, but traditional diagnostic methods such as colonoscopy and biopsy have limitations. Colonoscopy cannot provide detailed information within the tissues affected by cancer, while biopsy involves tissue removal, which can be painful and invasive. In order to improve diagnostic efficiency and reduce patient suffering, we studied machine-learningbased approach for colorectal tissue classification that uses acoustic resolution photoacoustic microscopy (ARPAM). With this tool, we were able to classify benign and malignant tissue using multiple machine learning methods. Our results were analyzed both quantitatively and qualitatively to evaluate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
COLERECTAL CANCER 是一种致命的疾病，在最近几年内逐渐增加。早期检测是保存生命的关键，但传统的诊断方法，如colonoscopy和biopsy，有限制。colonoscopy无法提供癌变组织中的详细信息，而biopsy则需要组织切除，可能很痛苦和侵入性。为了改善诊断效率和减少患者的痛苦，我们研究了机器学习基于的抑制癌变组织分类方法，使用高分辨率光子振荡显微镜（ARPAM）。我们使用多种机器学习方法来分类健康和癌变组织。我们的结果经过了量化和质数分析，以评估我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Multi-class-point-cloud-completion-networks-for-3D-cardiac-anatomy-reconstruction-from-cine-magnetic-resonance-images"><a href="#Multi-class-point-cloud-completion-networks-for-3D-cardiac-anatomy-reconstruction-from-cine-magnetic-resonance-images" class="headerlink" title="Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images"></a>Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08535">http://arxiv.org/abs/2307.08535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Julius Ossenberg-Engels, Vicente Grau</li>
<li>for: 这篇论文是为了提出一种全自动的三维心脏形态重建pipeline，用于从硬件磁共振成像（cine MRI）数据中提取多类三维心脏形态模型。</li>
<li>methods: 该ipeline使用了一种新型的多类点云完成网络（PCCN）来解决三维重建任务中的稀疏性和不对称性问题，并且在大量的Synthetic数据集上进行了评估。</li>
<li>results: 研究发现，使用PCCN可以在不同程度的扫描角度下实现下面或相似于原始图像分辨率的 Chamfer距离，并且相比于参考的3D U-Net模型，减少了32%和24%的重建误差。此外，该ipeline在UK Biobank研究中的1000个主题中实现了准确且 topological plausible的双心脏形态模型，并且与之前的文献中的临床指标相似。 finally, 研究发现该方法在多种常见异常条件下的稳定性。<details>
<summary>Abstract</summary>
Cine magnetic resonance imaging (MRI) is the current gold standard for the assessment of cardiac anatomy and function. However, it typically only acquires a set of two-dimensional (2D) slices of the underlying three-dimensional (3D) anatomy of the heart, thus limiting the understanding and analysis of both healthy and pathological cardiac morphology and physiology. In this paper, we propose a novel fully automatic surface reconstruction pipeline capable of reconstructing multi-class 3D cardiac anatomy meshes from raw cine MRI acquisitions. Its key component is a multi-class point cloud completion network (PCCN) capable of correcting both the sparsity and misalignment issues of the 3D reconstruction task in a unified model. We first evaluate the PCCN on a large synthetic dataset of biventricular anatomies and observe Chamfer distances between reconstructed and gold standard anatomies below or similar to the underlying image resolution for multiple levels of slice misalignment. Furthermore, we find a reduction in reconstruction error compared to a benchmark 3D U-Net by 32% and 24% in terms of Hausdorff distance and mean surface distance, respectively. We then apply the PCCN as part of our automated reconstruction pipeline to 1000 subjects from the UK Biobank study in a cross-domain transfer setting and demonstrate its ability to reconstruct accurate and topologically plausible biventricular heart meshes with clinical metrics comparable to the previous literature. Finally, we investigate the robustness of our proposed approach and observe its capacity to successfully handle multiple common outlier conditions.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 是当今心脏形态和功能评估的标准金属。然而，它通常只取得心脏三维形态的二维图像，因此限制了对健康和疾病心脏形态和physiology的理解和分析。在这篇论文中，我们提议一种全自动表面重建管道，可以从raw cine MRI获得多类三维心脏形态矩阵。其关键组件是一种多类点云完成网络（PCCN），可以在三维重建任务中解决缺失和不对称问题。我们首先在大量的人工数据集上评估PCCN，并观察到下同图像分辨率下的Chamfer距离。此外，我们发现与参考3D U-Net模型相比，PCCN可以降低重建错误的 Hausdorff距离和平均表面距离，分别降低32%和24%。然后，我们将PCCN作为自动重建管道的一部分应用于UK Biobank研究中的1000名参与者，并证明它能够重建准确和可靠的两个心脏宫室表面矩阵，与前一代文献的临床指标相符。最后，我们调查了我们提议的方法的稳定性，并发现它可以成功处理多种常见异常情况。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Processing-with-Linear-Optics"><a href="#Nonlinear-Processing-with-Linear-Optics" class="headerlink" title="Nonlinear Processing with Linear Optics"></a>Nonlinear Processing with Linear Optics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08533">http://arxiv.org/abs/2307.08533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa Yildirim, Niyazi Ulas Dinc, Ilker Oguz, Demetri Psaltis, Christophe Moser</li>
<li>for: 这个论文旨在实现多层光网络，并且解决在不使用电子组件的情况下实现多层光网络的挑战。</li>
<li>methods: 这篇论文提出了一种新的框架，使用多散射来实现可编程的线性和非线性变换，并且可以在低光功率 kontinuierender CW 光中实现非线性光计算。</li>
<li>results: 理论和实验研究表明，通过多散射重复数据可以实现低功率连续波光的非线性计算。<details>
<summary>Abstract</summary>
Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scattering enables non-linear optical computing at low power continuous wave light.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：深度神经网络在数据处理多层级处理中提取隐藏表示的成就很大，但是需要大量电子计算能力。为了提高能效性和速度， оптиче实现神经网络寻求利用光波宽频带和光通信的优点。在缺乏低功率光非线性下，实现多层光网络的挑战在于不使用电子组件实现多个光层。本研究提出了一种新的框架，利用多散射实现可编程的线性和非线性变换，并在低光力连续波光下实现非线性光计算。理论和实验研究表明，通过多散射复制数据可实现低功率连续波光非线性计算。</SYS>Here is the translation of the text into Simplified Chinese:深度神经网络在数据处理多层级处理中提取隐藏表示的成就很大，但是需要大量电子计算能力。为了提高能效性和速度， оптиче实现神经网络寻求利用光波宽频带和光通信的优点。在缺乏低功率光非线性下，实现多层光网络的挑战在于不使用电子组件实现多个光层。本研究提出了一种新的框架，利用多散射实现可编程的线性和非线性变换，并在低光力连续波光下实现非线性光计算。理论和实验研究表明，通过多散射复制数据可实现低功率连续波光非线性计算。
</details></li>
</ul>
<hr>
<h2 id="LuckyMera-a-Modular-AI-Framework-for-Building-Hybrid-NetHack-Agents"><a href="#LuckyMera-a-Modular-AI-Framework-for-Building-Hybrid-NetHack-Agents" class="headerlink" title="LuckyMera: a Modular AI Framework for Building Hybrid NetHack Agents"></a>LuckyMera: a Modular AI Framework for Building Hybrid NetHack Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08532">http://arxiv.org/abs/2307.08532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pervasive-ai-lab/luckymera">https://github.com/pervasive-ai-lab/luckymera</a></li>
<li>paper_authors: Luigi Quarantiello, Simone Marzeddu, Antonio Guzzi, Vincenzo Lomonaco</li>
<li>for: 这个论文的目的是提出一个可 configurable、可扩展的 AI 框架，用于在 NetHack 游戏中测试和训练 AI 代理人。</li>
<li>methods: 这个框架使用了 симвоlic 和神经网络学习方法，并提供了一些实用功能来保存经验和用于训练神经网络。</li>
<li>results: 经验证明，这个框架可以实现 state-of-the-art 的表现在完整的 NetHack 游戏中，并且提供了一个强大的基线代理人。<details>
<summary>Abstract</summary>
In the last few decades we have witnessed a significant development in Artificial Intelligence (AI) thanks to the availability of a variety of testbeds, mostly based on simulated environments and video games. Among those, roguelike games offer a very good trade-off in terms of complexity of the environment and computational costs, which makes them perfectly suited to test AI agents generalization capabilities. In this work, we present LuckyMera, a flexible, modular, extensible and configurable AI framework built around NetHack, a popular terminal-based, single-player roguelike video game. This library is aimed at simplifying and speeding up the development of AI agents capable of successfully playing the game and offering a high-level interface for designing game strategies. LuckyMera comes with a set of off-the-shelf symbolic and neural modules (called "skills"): these modules can be either hard-coded behaviors, or neural Reinforcement Learning approaches, with the possibility of creating compositional hybrid solutions. Additionally, LuckyMera comes with a set of utility features to save its experiences in the form of trajectories for further analysis and to use them as datasets to train neural modules, with a direct interface to the NetHack Learning Environment and MiniHack. Through an empirical evaluation we validate our skills implementation and propose a strong baseline agent that can reach state-of-the-art performances in the complete NetHack game. LuckyMera is open-source and available at https://github.com/Pervasive-AI-Lab/LuckyMera.
</details>
<details>
<summary>摘要</summary>
在过去几十年中，人工智能（AI）领域已经经历了 significative 的发展，很大一部分归功于各种测试环境和游戏的可用性。 Among them, roguelike 游戏提供了一个非常好的复杂性环境和计算成本的trade-off，使其成为测试 AI 代理的 идеal 选择。在这项工作中，我们介绍了 LuckyMera，一个灵活、可模块化、可扩展和配置化的 AI 框架，基于 NetHack terminal 基于单player 游戏。这个库的目标是为 AI 代理的开发提供简单化和加速的方式，并提供一个高级接口来设计游戏策略。LuckyMera 包含一些固定的符号学模块（called "skills"）和神经网络学习方法，以及可创建compositional 混合解决方案。此外，LuckyMera 还提供了一些实用功能，以保存其经验为 trajectories 进行后续分析，并直接与 NetHack Learning Environment 和 MiniHack 进行交互。通过实验 validate 我们的技能实现，并提出了一个强大的基线代理，可以在完整的 NetHack 游戏中达到状态艺术表现。LuckyMera 是开源的，可以在 https://github.com/Pervasive-AI-Lab/LuckyMera 上获取。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Lagrangian-Turbulence-by-Generative-Diffusion-Models"><a href="#Synthetic-Lagrangian-Turbulence-by-Generative-Diffusion-Models" class="headerlink" title="Synthetic Lagrangian Turbulence by Generative Diffusion Models"></a>Synthetic Lagrangian Turbulence by Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08529">http://arxiv.org/abs/2307.08529</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smartturb/diffusion-lagr">https://github.com/smartturb/diffusion-lagr</a></li>
<li>paper_authors: Tianyi Li, Luca Biferale, Fabio Bonaccorso, Martino Andrea Scarpolini, Michele Buzzicotti</li>
<li>For: The paper aims to generate single-particle trajectories in three-dimensional turbulence at high Reynolds numbers using a machine learning approach.* Methods: The paper proposes a state-of-the-art Diffusion Model to generate the trajectories, which bypasses the need for direct numerical simulations or experiments to obtain reliable Lagrangian data.* Results: The model demonstrates the ability to quantitatively reproduce all relevant statistical benchmarks over the entire range of time scales, including the presence of fat tails distribution for the velocity increments, anomalous power law, and enhancement of intermittency around the dissipative scale. The model also exhibits good generalizability for extreme events, achieving unprecedented intensity and rarity.<details>
<summary>Abstract</summary>
Lagrangian turbulence lies at the core of numerous applied and fundamental problems related to the physics of dispersion and mixing in engineering, bio-fluids, atmosphere, oceans, and astrophysics. Despite exceptional theoretical, numerical, and experimental efforts conducted over the past thirty years, no existing models are capable of faithfully reproducing statistical and topological properties exhibited by particle trajectories in turbulence. We propose a machine learning approach, based on a state-of-the-art Diffusion Model, to generate single-particle trajectories in three-dimensional turbulence at high Reynolds numbers, thereby bypassing the need for direct numerical simulations or experiments to obtain reliable Lagrangian data. Our model demonstrates the ability to quantitatively reproduce all relevant statistical benchmarks over the entire range of time scales, including the presence of fat tails distribution for the velocity increments, anomalous power law, and enhancement of intermittency around the dissipative scale. The model exhibits good generalizability for extreme events, achieving unprecedented intensity and rarity. This paves the way for producing synthetic high-quality datasets for pre-training various downstream applications of Lagrangian turbulence.
</details>
<details>
<summary>摘要</summary>
拉格朗日流动在许多应用和基础问题中扮演重要角色，包括物理杂化和混合在工程、生物流体、大气、海洋和astrophysics中。尽管过去三十年来有过 Exceptional theoretical, numerical, and experimental efforts，但现有的模型无法准确地复制流动中粒子轨迹的统计和 тополоڤ���IC Properties。我们提出一种基于当前最佳Diffusion Model的机器学习方法，可以在高 Reynolds 数下生成三维流动中的单粒子轨迹，并且不需要直接进行数值 simulate or experiment to obtain reliable Lagrangian data。我们的模型能够准确地复制所有有关时间尺度的统计标准，包括粒子增量的轮廓分布、罕见的功率律和在混合度下的增强。这种模型具有良好的通用性，能够生成extreme events，达到了历史上最高的Intensity和罕见性。这些Synthetic高质量数据可以用于PRE-TRAINING various downstream应用程序。
</details></li>
</ul>
<hr>
<h2 id="Multi-Domain-Learning-with-Modulation-Adapters"><a href="#Multi-Domain-Learning-with-Modulation-Adapters" class="headerlink" title="Multi-Domain Learning with Modulation Adapters"></a>Multi-Domain Learning with Modulation Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08528">http://arxiv.org/abs/2307.08528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekaterina Iakovleva, Karteek Alahari, Jakob Verbeek</li>
<li>For: The paper is written for computer vision tasks, specifically for image classification across multiple domains.* Methods: The paper introduces Modulation Adapters, which update the convolutional filter weights of the model in a multiplicative manner for each task. The adaptation weights are parameterized in a factored manner, allowing for flexible scaling of the number of per-task parameters and different parameter-accuracy trade-offs.* Results: The approach yields excellent results on the Visual Decathlon challenge and the ImageNet-to-Sketch benchmark, with accuracies that are comparable to or better than those of existing state-of-the-art approaches.<details>
<summary>Abstract</summary>
Deep convolutional networks are ubiquitous in computer vision, due to their excellent performance across different tasks for various domains. Models are, however, often trained in isolation for each task, failing to exploit relatedness between tasks and domains to learn more compact models that generalise better in low-data regimes. Multi-domain learning aims to handle related tasks, such as image classification across multiple domains, simultaneously. Previous work on this problem explored the use of a pre-trained and fixed domain-agnostic base network, in combination with smaller learnable domain-specific adaptation modules. In this paper, we introduce Modulation Adapters, which update the convolutional filter weights of the model in a multiplicative manner for each task. Parameterising these adaptation weights in a factored manner allows us to scale the number of per-task parameters in a flexible manner, and to strike different parameter-accuracy trade-offs. We evaluate our approach on the Visual Decathlon challenge, composed of ten image classification tasks across different domains, and on the ImageNet-to-Sketch benchmark, which consists of six image classification tasks. Our approach yields excellent results, with accuracies that are comparable to or better than those of existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Image-Captions-are-Natural-Prompts-for-Text-to-Image-Models"><a href="#Image-Captions-are-Natural-Prompts-for-Text-to-Image-Models" class="headerlink" title="Image Captions are Natural Prompts for Text-to-Image Models"></a>Image Captions are Natural Prompts for Text-to-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08526">http://arxiv.org/abs/2307.08526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiye Lei, Hao Chen, Sen Zhang, Bo Zhao, Dacheng Tao</li>
<li>for: 提高文本到图像生成模型的训练数据 Informative 性和多样性</li>
<li>methods: 使用高级captioning模型生成图像描述，并将描述与分类名称 concatenate 用作生成模型的训练数据</li>
<li>results: 对ImageNette、ImageNet-100和ImageNet-1K进行了广泛的实验，并 verify 了我们的方法可以significantly improve 模型在 sintetic 训练数据上的表现，即平均提高10%的分类精度。<details>
<summary>Abstract</summary>
With the rapid development of Artificial Intelligence Generated Content (AIGC), it has become common practice in many learning tasks to train or fine-tune large models on synthetic data due to the data-scarcity and privacy leakage problems. Albeit promising with unlimited data generation, owing to massive and diverse information conveyed in real images, it is challenging for text-to-image generative models to synthesize informative training data with hand-crafted prompts, which usually leads to inferior generalization performance when training downstream models. In this paper, we theoretically analyze the relationship between the training effect of synthetic data and the synthetic data distribution induced by prompts. Then we correspondingly propose a simple yet effective method that prompts text-to-image generative models to synthesize more informative and diverse training data. Specifically, we caption each real image with the advanced captioning model to obtain informative and faithful prompts that extract class-relevant information and clarify the polysemy of class names. The image captions and class names are concatenated to prompt generative models for training image synthesis. Extensive experiments on ImageNette, ImageNet-100, and ImageNet-1K verify that our method significantly improves the performance of models trained on synthetic training data, i.e., 10% classification accuracy improvements on average.
</details>
<details>
<summary>摘要</summary>
随着人工智能生成内容（AIGC）的快速发展，在许多学习任务中通常是通过人工生成的数据进行训练或细化大型模型，因为实际数据的缺乏和隐私泄露问题。虽然人工生成的数据具有无限数据的优势，但是由于实际图像中含有庞大和多样化的信息， Text-to-image生成模型很难以通过手工提示生成有用的训练数据，这通常会导致下游模型训练时的泛化性能差。在这篇论文中，我们 theoretically 分析了人工生成数据训练效果和提示数据分布之间的关系，然后对应提出了一种简单 yet effective的方法。具体来说，我们使用高级描述模型将每个实际图像描述为 faithful 和有用的提示，以提取类别相关的信息并减少类名的多义性。图像描述和类别名称 concatenated 作为提示生成模型进行训练图像生成。我们在 ImageNette、ImageNet-100 和 ImageNet-1K 进行了广泛的实验，结果显示，我们的方法可以在使用人工生成数据进行训练时提高模型的性能，即平均提高10%的分类精度。
</details></li>
</ul>
<hr>
<h2 id="Results-on-Counterfactual-Invariance"><a href="#Results-on-Counterfactual-Invariance" class="headerlink" title="Results on Counterfactual Invariance"></a>Results on Counterfactual Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08519">http://arxiv.org/abs/2307.08519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jake Fawkes, Robin J. Evans</li>
<li>for: 本文提供了对Counterfactual invariants的理论分析。</li>
<li>methods: 文章presented a variety of existing definitions, studied their relationships and graphical implications.</li>
<li>results: 文章showed that counterfactual invariance implies conditional independence, but conditional independence does not provide any information about the likelihood of satisfying counterfactual invariance. Additionally, for discrete causal models, counterfactually invariant functions are often restricted to being functions of specific variables or constant.Here’s the same information in Traditional Chinese:</li>
<li>for: 本文的目的是提供Counterfactual invariants的理论分析。</li>
<li>methods: 文章使用了多种现有的定义，研究它们之间的关系和图形 implications。</li>
<li>results: 文章显示了Counterfactual invariance implies conditional independence, but conditional independence does not provide any information about the likelihood of satisfying counterfactual invariance. In addition, for discrete causal models, counterfactually invariant functions are often restricted to being functions of specific variables or constant.<details>
<summary>Abstract</summary>
In this paper we provide a theoretical analysis of counterfactual invariance. We present a variety of existing definitions, study how they relate to each other and what their graphical implications are. We then turn to the current major question surrounding counterfactual invariance, how does it relate to conditional independence? We show that whilst counterfactual invariance implies conditional independence, conditional independence does not give any implications about the degree or likelihood of satisfying counterfactual invariance. Furthermore, we show that for discrete causal models counterfactually invariant functions are often constrained to be functions of particular variables, or even constant.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提供了对Counterfactual invariants的理论分析。我们提供了多种现有的定义，研究它们之间的关系以及它们在图形上的含义。然后，我们转向现在主要关注的问题：Counterfactual invariants与Conditional independence之间的关系。我们表明，Counterfactual invariants imply Conditional independence,但Conditional independence不能提供关于满足Counterfactual invariants的度或概率的任何信息。此外，我们表明，对于排序 causal模型，Counterfactually invariants的函数frequently是特定变量或常数。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Based-Testing-for-Single-Cell-Differential-Analysis"><a href="#Kernel-Based-Testing-for-Single-Cell-Differential-Analysis" class="headerlink" title="Kernel-Based Testing for Single-Cell Differential Analysis"></a>Kernel-Based Testing for Single-Cell Differential Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08509">http://arxiv.org/abs/2307.08509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anthoozier/kernel_testsda">https://github.com/anthoozier/kernel_testsda</a></li>
<li>paper_authors: Anthony Ozier-Lafontaine, Camille Fourneaux, Ghislain Durif, Céline Vallot, Olivier Gandrillon, Sandrine Giraud, Bertrand Michel, Franck Picard</li>
<li>for: 这种方法用于比较单个细胞中分子特征的分布, 例如基因表达和epigenomic修饰。</li>
<li>methods: 该方法基于kernel embedding的非线性比较框架, 可以进行单元细胞特征之间的 feature-wise 分析以及全面的 transcriptome 或 epigenome 比较, 考虑到它们的复杂依赖关系。</li>
<li>results: 该方法可以检测单元细胞中的不同类型 células, 并且可以成功地识别在分化过程中的细胞在转化过程中的阶段。 此外，通过分析单元细胞 ChIP-Seq 数据, 该方法可以找到不受治疗的乳腺癌细胞中的 persistenter 细胞 под类型。<details>
<summary>Abstract</summary>
Single-cell technologies have provided valuable insights into the distribution of molecular features, such as gene expression and epigenomic modifications. However, comparing these complex distributions in a controlled and powerful manner poses methodological challenges. Here we propose to benefit from the kernel-testing framework to compare the complex cell-wise distributions of molecular features in a non-linear manner based on their kernel embedding. Our framework not only allows for feature-wise analyses but also enables global comparisons of transcriptomes or epigenomes, considering their intricate dependencies. By using a classifier to discriminate cells based on the variability of their embedding, our method uncovers heterogeneities in cell populations that would otherwise go undetected. We show that kernel testing overcomes the limitations of differential analysis methods dedicated to single-cell. Kernel testing is applied to investigate the reversion process of differentiating cells, successfully identifying cells in transition between reversion and differentiation stages. Additionally, we analyze single-cell ChIP-Seq data and identify a subpopulation of untreated breast cancer cells that exhibit an epigenomic profile similar to persister cells.
</details>
<details>
<summary>摘要</summary>
单元技术提供了价值的内在分布的分析，如基因表达和聚合酶修饰。然而，对这些复杂的分布进行控制和有力的比较具有挑战性。我们提议利用kernel-测试框架来比较单元细胞水平的分布，以非线性方式基于其内存映射。我们的框架不仅允许特征值分析，而且允许全球比较单元胞营养或者聚合酶修饰，考虑其复杂的依赖关系。通过使用一个分类器来根据单元细胞的变化程度来识别单元细胞，我们的方法揭示了单元细胞群体中的异质性，这些异质性否则可能会被忽略。我们在研究单元细胞的还原过程中成功地使用kernel测试方法，并成功地识别在还原和分化过程中的单元细胞。此外，我们分析单元细胞ChIP-Seq数据，并发现一个未经治疗的乳腺癌细胞subpopulation，其聚合酶修饰profile与持续细胞类似。
</details></li>
</ul>
<hr>
<h2 id="Efficient-and-Accurate-Optimal-Transport-with-Mirror-Descent-and-Conjugate-Gradients"><a href="#Efficient-and-Accurate-Optimal-Transport-with-Mirror-Descent-and-Conjugate-Gradients" class="headerlink" title="Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients"></a>Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08507">http://arxiv.org/abs/2307.08507</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adaptive-agents-lab/mdot-pncg">https://github.com/adaptive-agents-lab/mdot-pncg</a></li>
<li>paper_authors: Mete Kemertas, Allan D. Jepson, Amir-massoud Farahmand</li>
<li>for: 本文提出了一种新的优化运输算法，基于优化运输、投影下降和 conjugate gradients 等方法。</li>
<li>methods: 该算法可以计算优化运输成本，无论精度如何，而不会遇到数值稳定性问题。它利用 GPU 进行高效实现，并在许多情况下比传统算法 such as Sinkhorn’s Algorithm 更快 converges。</li>
<li>results: 我们对 marginal 分布 entropy 进行了特别关注，并证明高 entropy marginals 会导致更难的优化运输问题，而我们的算法适合这类问题。我们还进行了精心的减少分析，并对算法和问题参数进行了精心的调整。我们的结果表明，我们的算法可以为优化运输问题提供一个有用的工具。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/adaptive-agents-lab/MDOT-PNCG">https://github.com/adaptive-agents-lab/MDOT-PNCG</a> 上获取。<details>
<summary>Abstract</summary>
We design a novel algorithm for optimal transport by drawing from the entropic optimal transport, mirror descent and conjugate gradients literatures. Our algorithm is able to compute optimal transport costs with arbitrary accuracy without running into numerical stability issues. The algorithm is implemented efficiently on GPUs and is shown empirically to converge more quickly than traditional algorithms such as Sinkhorn's Algorithm both in terms of number of iterations and wall-clock time in many cases. We pay particular attention to the entropy of marginal distributions and show that high entropy marginals make for harder optimal transport problems, for which our algorithm is a good fit. We provide a careful ablation analysis with respect to algorithm and problem parameters, and present benchmarking over the MNIST dataset. The results suggest that our algorithm can be a useful addition to the practitioner's optimal transport toolkit. Our code is open-sourced at https://github.com/adaptive-agents-lab/MDOT-PNCG .
</details>
<details>
<summary>摘要</summary>
我们设计了一种新的优化交通算法，基于优化交通、镜像下降和 conjugate gradients 文献。我们的算法可以计算优化交通成本，无论精度如何，而不会遇到数值稳定性问题。我们的算法可以高效地运行在 GPU 上，并在许多情况下被证明可以更快 converge than 传统的算法，如 sinkhorn 算法，以数 iteration 和墙 clock 时间来说。我们特别关注到 marginal 分布的 entropy，并证明高 entropy marginal 会导致更难的优化交通问题，而我们的算法适合这种情况。我们进行了仔细的减少分析，并对算法和问题参数进行了精心的调整。我们在 MNIST 数据集上进行了 benchmarking，结果表明，我们的算法可以成为优化交通工具箱中的有用工具。我们的代码可以在 https://github.com/adaptive-agents-lab/MDOT-PNCG 上获取。
</details></li>
</ul>
<hr>
<h2 id="Does-Visual-Pretraining-Help-End-to-End-Reasoning"><a href="#Does-Visual-Pretraining-Help-End-to-End-Reasoning" class="headerlink" title="Does Visual Pretraining Help End-to-End Reasoning?"></a>Does Visual Pretraining Help End-to-End Reasoning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08506">http://arxiv.org/abs/2307.08506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Sun, Calvin Luo, Xingyi Zhou, Anurag Arnab, Cordelia Schmid</li>
<li>for:  investigate whether end-to-end learning of visual reasoning can be achieved with general-purpose neural networks, and confirm the feasibility of a neural network “generalist” to solve visual recognition and reasoning tasks.</li>
<li>methods: use a simple and general self-supervised framework which “compresses” each video frame into a small set of tokens with a transformer network, and reconstructs the remaining frames based on the compressed temporal context.</li>
<li>results: observe that pretraining is essential to achieve compositional generalization for end-to-end visual reasoning, and our proposed framework outperforms traditional supervised pretraining, including image classification and explicit object detection, by large margins.<details>
<summary>Abstract</summary>
We aim to investigate whether end-to-end learning of visual reasoning can be achieved with general-purpose neural networks, with the help of visual pretraining. A positive result would refute the common belief that explicit visual abstraction (e.g. object detection) is essential for compositional generalization on visual reasoning, and confirm the feasibility of a neural network "generalist" to solve visual recognition and reasoning tasks. We propose a simple and general self-supervised framework which "compresses" each video frame into a small set of tokens with a transformer network, and reconstructs the remaining frames based on the compressed temporal context. To minimize the reconstruction loss, the network must learn a compact representation for each image, as well as capture temporal dynamics and object permanence from temporal context. We perform evaluation on two visual reasoning benchmarks, CATER and ACRE. We observe that pretraining is essential to achieve compositional generalization for end-to-end visual reasoning. Our proposed framework outperforms traditional supervised pretraining, including image classification and explicit object detection, by large margins.
</details>
<details>
<summary>摘要</summary>
我们的目标是 investigate  Whether end-to-end 视觉逻辑学习可以通过通用神经网络实现，帮助了由 visual pretraining 。一个正面的结果会证明 Explicit 视觉抽象（例如对象检测）不是必要的 для compositional generalization 的视觉逻辑任务，并证明神经网络 "通用" 可以解决视识别和逻辑任务。我们提出了一个简单和通用的自我超vised framework，将每帧视频图片"压缩" 成一个小集合 of tokens 使用 transformer 网络，并使用压缩的时间上下文来重建剩下的帧。为了降低重建损失，网络必须学习每幅图片的紧凑表示，以及从时间上下文中捕捉时间动态和对象的持续性。我们在 CATER 和 ACRE 两个视觉逻辑标准benchmark上进行评估，发现预训练是必要的，以实现 compositional generalization 的 end-to-end 视觉逻辑学习。我们提出的方法在图像分类和显式对象检测的传统预训练下，都有大幅度的优势。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-Evaluation-of-Topic-Models-and-Dimensionality-Reduction-Methods-for-2D-Text-Spatialization"><a href="#Large-Scale-Evaluation-of-Topic-Models-and-Dimensionality-Reduction-Methods-for-2D-Text-Spatialization" class="headerlink" title="Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization"></a>Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11770">http://arxiv.org/abs/2307.11770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cgshpi/topic-models-and-dimensionality-reduction-benchmark">https://github.com/cgshpi/topic-models-and-dimensionality-reduction-benchmark</a></li>
<li>paper_authors: Daniel Atzberger, Tim Cech, Willy Scheibel, Matthias Trapp, Rico Richter, Jürgen Döllner, Tobias Schreck</li>
<li>for: 这个论文的目的是 investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots.</li>
<li>methods: 该论文使用了多种主题模型和维度减少算法，并对它们的组合进行了大规模的计算评估。</li>
<li>results: 根据计算结果， interpretable topic models 能够很好地捕捉文本 Corpora 的结构，而 t-SNE 作为维度减少算法也有良好的效果。<details>
<summary>Abstract</summary>
Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that are combinations of topic models and dimensionality reductions, and (3) quality metrics for quantifying the resulting layout. The corpora are given as document-term matrices, and each document is assigned to a thematic class. The chosen metrics quantify the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots. By evaluating the benchmark on a computing cluster, we derived a multivariate dataset with over 45 000 individual layouts and corresponding quality metrics. Based on the results, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions. As a main result, we show that interpretable topic models are beneficial for capturing the structure of text corpora. We furthermore recommend the use of t-SNE as a subsequent dimensionality reduction.
</details>
<details>
<summary>摘要</summary>
To investigate the effectiveness of topic models and dimensionality reduction methods for spatializing text corpora as two-dimensional scatter plots, we conducted a large-scale, benchmark-based computational evaluation. Our evaluation consisted of three parts:1. A set of corpora, given as document-term matrices, with each document assigned to a thematic class.2. A set of layout algorithms that combined topic models and dimensionality reductions.3. Quality metrics to quantify the resulting layout, including the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots.We evaluated the benchmark on a computing cluster and derived a multivariate dataset with over 45,000 individual layouts and corresponding quality metrics. Our results show that interpretable topic models are beneficial for capturing the structure of text corpora, and we recommend the use of t-SNE as a subsequent dimensionality reduction. Based on our findings, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions.
</details></li>
</ul>
<hr>
<h2 id="Can-We-Trust-Race-Prediction"><a href="#Can-We-Trust-Race-Prediction" class="headerlink" title="Can We Trust Race Prediction?"></a>Can We Trust Race Prediction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08496">http://arxiv.org/abs/2307.08496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cangyuanli/pyethnicity">https://github.com/cangyuanli/pyethnicity</a></li>
<li>paper_authors: Cangyuan Li</li>
<li>for: 这个论文是为了提高美国选民登记数据中的人口统计和地理编码的准确性而写的。</li>
<li>methods: 作者使用了irectional Long Short-Term Memory (BiLSTM) 模型和一个ensemble模型，并使用了一个新的选民登记数据集来提高模型的性能。</li>
<li>results: 作者通过创建了一个全面的美国人名和姓氏分布数据库，并提供了一个高品质的比较数据集，以提高 bayesian improved surname geocoding (BISG) 和 bayesian improved firstname surname geocoding (BIFSG) 的准确性。<details>
<summary>Abstract</summary>
In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies. In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models in the literature. Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to improve the coverage and accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.
</details>
<details>
<summary>摘要</summary>
在没有敏感的种族和族谱数据的情况下，研究人员、监管机构和公司都会寻找代理。在这篇论文中，我将一个 bidirectional Long Short-Term Memory（BiLSTM）模型训练在所有50个美国州的选民登记数据上，创建了一个ensemble，其在验证样本外的F1分数高达36.8%，较文献中最高performing机器学习模型高。此外，我还建立了美国最完整的姓名和名字分布数据库，以提高Bayesian Improved Surname Geocoding（BISG）和Bayesian Improved Firstname Surname Geocoding（BIFSG）的覆盖和精度。最后，我提供了第一个高品质的比较基准集，以便比较现有模型和未来的模型开发者。
</details></li>
</ul>
<hr>
<h2 id="Fairness-in-KI-Systemen"><a href="#Fairness-in-KI-Systemen" class="headerlink" title="Fairness in KI-Systemen"></a>Fairness in KI-Systemen</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08486">http://arxiv.org/abs/2307.08486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Janine Strotherm, Alissa Müller, Barbara Hammer, Benjamin Paaßen</li>
<li>for: 本文提供了关于机器学习中的公平研究的导入，包括主要的公平定义和实现公平的策略。</li>
<li>methods: 本文使用了可见的示例和图像来解释公平定义和策略，适用于多学科读者。</li>
<li>results: 本文提供了一个欧洲Context中的公平研究的导入，包括主要的公平定义和实现公平的策略。<details>
<summary>Abstract</summary>
The more AI-assisted decisions affect people's lives, the more important the fairness of such decisions becomes. In this chapter, we provide an introduction to research on fairness in machine learning. We explain the main fairness definitions and strategies for achieving fairness using concrete examples and place fairness research in the European context. Our contribution is aimed at an interdisciplinary audience and therefore avoids mathematical formulation but emphasizes visualizations and examples.   --   Je mehr KI-gest\"utzte Entscheidungen das Leben von Menschen betreffen, desto wichtiger ist die Fairness solcher Entscheidungen. In diesem Kapitel geben wir eine Einf\"uhrung in die Forschung zu Fairness im maschinellen Lernen. Wir erkl\"aren die wesentlichen Fairness-Definitionen und Strategien zur Erreichung von Fairness anhand konkreter Beispiele und ordnen die Fairness-Forschung in den europ\"aischen Kontext ein. Unser Beitrag richtet sich dabei an ein interdisziplin\"ares Publikum und verzichtet daher auf die mathematische Formulierung sondern betont Visualisierungen und Beispiele.
</details>
<details>
<summary>摘要</summary>
更多的AI助け的决策对人们的生活产生了影响，因此决策的公正性变得越来越重要。在这章中，我们提供了对决策公正性的研究Introduction，并解释了主要的公正性定义和在实际示例中实现公正性的策略。我们的贡献是向多学科读者群体进行了定向，因此弃用了数学表述，而是强调视觉化和示例。In this chapter, we provide an introduction to research on fairness in machine learning. We explain the main fairness definitions and strategies for achieving fairness using concrete examples and place fairness research in the European context. Our contribution is aimed at an interdisciplinary audience and therefore avoids mathematical formulation but emphasizes visualizations and examples. As AI-assisted decisions increasingly affect people's lives, the fairness of such decisions becomes more important.
</details></li>
</ul>
<hr>
<h2 id="Cross-Feature-Selection-to-Eliminate-Spurious-Interactions-and-Single-Feature-Dominance-Explainable-Boosting-Machines"><a href="#Cross-Feature-Selection-to-Eliminate-Spurious-Interactions-and-Single-Feature-Dominance-Explainable-Boosting-Machines" class="headerlink" title="Cross Feature Selection to Eliminate Spurious Interactions and Single Feature Dominance Explainable Boosting Machines"></a>Cross Feature Selection to Eliminate Spurious Interactions and Single Feature Dominance Explainable Boosting Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08485">http://arxiv.org/abs/2307.08485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shree Charran R, Sandipan Das Mahapatra</li>
<li>for: 本研究旨在提高EBM模型的解释性和可靠性，并应用于各种预测任务中。</li>
<li>methods: 本研究使用了 alternate 横向选择、集成特征和模型配置变更等技术来解决EBM模型中的干扰和单个特征主导问题。</li>
<li>results: 对三个 benchmark 数据集进行了评估，结果表明 alternate 技术可以超越原始 EBM 方法，提供更好的解释性和特征选择稳定性，并提高模型的预测性能。<details>
<summary>Abstract</summary>
Interpretability is a crucial aspect of machine learning models that enables humans to understand and trust the decision-making process of these models. In many real-world applications, the interpretability of models is essential for legal, ethical, and practical reasons. For instance, in the banking domain, interpretability is critical for lenders and borrowers to understand the reasoning behind the acceptance or rejection of loan applications as per fair lending laws. However, achieving interpretability in machine learning models is challenging, especially for complex high-performance models. Hence Explainable Boosting Machines (EBMs) have been gaining popularity due to their interpretable and high-performance nature in various prediction tasks. However, these models can suffer from issues such as spurious interactions with redundant features and single-feature dominance across all interactions, which can affect the interpretability and reliability of the model's predictions. In this paper, we explore novel approaches to address these issues by utilizing alternate Cross-feature selection, ensemble features and model configuration alteration techniques. Our approach involves a multi-step feature selection procedure that selects a set of candidate features, ensemble features and then benchmark the same using the EBM model. We evaluate our method on three benchmark datasets and show that the alternate techniques outperform vanilla EBM methods, while providing better interpretability and feature selection stability, and improving the model's predictive performance. Moreover, we show that our approach can identify meaningful interactions and reduce the dominance of single features in the model's predictions, leading to more reliable and interpretable models.   Index Terms- Interpretability, EBM's, ensemble, feature selection.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>机器学习模型的可解释性是一个关键的特点，它使得人们可以更好地理解和信任机器学习模型的决策过程。在实际应用中，机器学习模型的可解释性是非常重要的，特别是在银行领域。在这个领域中，可解释性是对借款申请的 Accept 或 Reject 决策进行法律、伦理和实用上的要求。然而，实现机器学习模型的可解释性是一个挑战，特别是在复杂高性能模型中。因此，可解释性增强的机器学习模型（EBM）在各种预测任务中得到了广泛的应用。然而，这些模型可能会面临一些问题，如 redundancy 特征之间的干扰和单个特征在所有交互中的占主导地位，这些问题可能会影响模型预测的可靠性和解释性。在这篇论文中，我们探讨了一些新的方法来解决这些问题，包括使用另一种 Cross-feature 选择、ensemble 特征和模型配置变化技术。我们的方法包括一个多步特征选择过程，选择一组候选特征、ensemble特征，然后使用 EBM 模型来评估。我们在三个 benchmark 数据集上评估了我们的方法，并显示了它们在比vanilla EBM方法更高的可解释性和特征选择稳定性，以及提高模型预测性能。此外，我们还发现了我们的方法可以找到有意义的交互和减少单个特征在模型预测中的主导地位，从而提高模型的可靠性和解释性。Index Terms- 可解释性, EBM, ensemble, 特征选择.
</details></li>
</ul>
<hr>
<h2 id="A-Fast-Task-Offloading-Optimization-Framework-for-IRS-Assisted-Multi-Access-Edge-Computing-System"><a href="#A-Fast-Task-Offloading-Optimization-Framework-for-IRS-Assisted-Multi-Access-Edge-Computing-System" class="headerlink" title="A Fast Task Offloading Optimization Framework for IRS-Assisted Multi-Access Edge Computing System"></a>A Fast Task Offloading Optimization Framework for IRS-Assisted Multi-Access Edge Computing System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08474">http://arxiv.org/abs/2307.08474</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uic-jq/iopo">https://github.com/uic-jq/iopo</a></li>
<li>paper_authors: Jianqiu Wu, Zhongyi Yu, Jianxiong Guo, Zhiqing Tang, Tian Wang, Weijia Jia</li>
<li>for: 这个论文旨在提高无线网络，尤其是基于飞行器多访问边缘计算系统。</li>
<li>methods: 该论文提出了一种基于深度学习的优化框架，称为迭代保持顺序决策（IOPO），用于生成高质量的任务卸载分配。</li>
<li>results: 实验结果表明，该提议的框架可以在很短的时间内生成高效的任务卸载决策，超过其他标准方法。<details>
<summary>Abstract</summary>
Terahertz communication networks and intelligent reflecting surfaces exhibit significant potential in advancing wireless networks, particularly within the domain of aerial-based multi-access edge computing systems. These technologies enable efficient offloading of computational tasks from user electronic devices to Unmanned Aerial Vehicles or local execution. For the generation of high-quality task-offloading allocations, conventional numerical optimization methods often struggle to solve challenging combinatorial optimization problems within the limited channel coherence time, thereby failing to respond quickly to dynamic changes in system conditions. To address this challenge, we propose a deep learning-based optimization framework called Iterative Order-Preserving policy Optimization (IOPO), which enables the generation of energy-efficient task-offloading decisions within milliseconds. Unlike exhaustive search methods, IOPO provides continuous updates to the offloading decisions without resorting to exhaustive search, resulting in accelerated convergence and reduced computational complexity, particularly when dealing with complex problems characterized by extensive solution spaces. Experimental results demonstrate that the proposed framework can generate energy-efficient task-offloading decisions within a very short time period, outperforming other benchmark methods.
</details>
<details>
<summary>摘要</summary>
“tera兆通信网络和智能反射表示技术在提高无线网络方面具有 significannot potential，特别是在基于飞行器多访问边缘计算系统中。这些技术可以有效地减轻用户电子设备中的计算任务到无人飞行机或本地执行。为生成高质量的任务卸载分配，常见的数字优化方法经常陷入在限制性减震时间内的复杂 combinatorial 优化问题中，从而无法快速应对系统条件的动态变化。为解决这个挑战，我们提出了一种基于深度学习的优化框架，即迭代保持顺序分配优化（IOPO）。与极限搜索方法不同，IOPO可以在毫秒级时间内生成能效的任务卸载决策，而无需进行极限搜索，从而降低计算复杂性，特别是在面临复杂问题时。实验结果表明，提议的框架可以在很短的时间内生成能效的任务卸载决策，超过了其他 Referenced 方法。”
</details></li>
</ul>
<hr>
<h2 id="Hidden-Markov-Models-with-Random-Restarts-vs-Boosting-for-Malware-Detection"><a href="#Hidden-Markov-Models-with-Random-Restarts-vs-Boosting-for-Malware-Detection" class="headerlink" title="Hidden Markov Models with Random Restarts vs Boosting for Malware Detection"></a>Hidden Markov Models with Random Restarts vs Boosting for Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10256">http://arxiv.org/abs/2307.10256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Raghavan, Fabio Di Troia, Mark Stamp</li>
<li>for: 这个研究是为了提高适当的黑客病毒检测方法。</li>
<li>methods: 这个研究使用了隐藏Markovchain模型（HMM）和AdaBoost数据集训练方法。</li>
<li>results: 研究发现，Random Restarts方法在训练数据匮乏时表现出来 surprisingly well，而boosting则仅在最困难的“冰结”案例（训练数据匮乏）中能够提供足够的改善，以正fi运算阶段的computational cost。<details>
<summary>Abstract</summary>
Effective and efficient malware detection is at the forefront of research into building secure digital systems. As with many other fields, malware detection research has seen a dramatic increase in the application of machine learning algorithms. One machine learning technique that has been used widely in the field of pattern matching in general-and malware detection in particular-is hidden Markov models (HMMs). HMM training is based on a hill climb, and hence we can often improve a model by training multiple times with different initial values. In this research, we compare boosted HMMs (using AdaBoost) to HMMs trained with multiple random restarts, in the context of malware detection. These techniques are applied to a variety of challenging malware datasets. We find that random restarts perform surprisingly well in comparison to boosting. Only in the most difficult "cold start" cases (where training data is severely limited) does boosting appear to offer sufficient improvement to justify its higher computational cost in the scoring phase.
</details>
<details>
<summary>摘要</summary>
“有效和高效的恶意软件检测是当前安全数字系统研究的核心。与其他领域一样，恶意软件检测研究在使用机器学习算法方面表现了快速增长。一种广泛应用于模式匹配领域（包括恶意软件检测）的机器学习技术是隐藏Markov模型（HMM）。HMM训练基于山峰搜索，因此可以通过不同的初始值进行多次训练以改进模型。在这项研究中，我们比较了使用AdaBoost加强HMM和多个随机重启来进行恶意软件检测。这些技术在许多复杂的恶意软件数据集中进行应用。我们发现，随机重启 surprisingly well 在对抗“冰结”（训练数据 severely limited）情况下表现出色，而boosting 只在这些情况下能够提供足够的改进，以 justify its higher computational cost in the scoring phase。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Classification-of-UHF-Partial-Discharge-Signals-in-Gas-Insulated-HVDC-Systems-Using-Neural-Networks"><a href="#Generalizable-Classification-of-UHF-Partial-Discharge-Signals-in-Gas-Insulated-HVDC-Systems-Using-Neural-Networks" class="headerlink" title="Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks"></a>Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08466">http://arxiv.org/abs/2307.08466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steffen Seitz, Thomas Götz, Christopher Lindenberg, Ronald Tetzlaff, Stephan Schlegel</li>
<li>for: 本研究旨在为高压直流绝缘系统（HVDC GIS）中检测未探测到的部分磁振（PD）提供一种基于神经网络的分类方法，无需基于振荡序列分析特征。</li>
<li>methods: 本研究使用神经网络分类方法，并对时域和频域输入信号进行比较，以及不同 нормализа schemes的影响。</li>
<li>results: 研究发现，使用神经网络分类方法可以区分由金属凸起和导电粒子引起的PD信号，并且可以在不同的运行电压多пли中进行泛化。<details>
<summary>Abstract</summary>
Undetected partial discharges (PDs) are a safety critical issue in high voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC voltage is well-established, the analysis of PDs under DC voltage remains an active research field. A key focus of these investigations is the classification of different PD sources to enable subsequent sophisticated analysis.   In this paper, we propose and analyze a neural network-based approach for classifying PD signals caused by metallic protrusions and conductive particles on the insulator of HVDC GIS, without relying on pulse sequence analysis features. In contrast to previous approaches, our proposed model can discriminate the studied PD signals obtained at negative and positive potentials, while also generalizing to unseen operating voltage multiples. Additionally, we compare the performance of time- and frequency-domain input signals and explore the impact of different normalization schemes to mitigate the influence of free-space path loss between the sensor and defect location.
</details>
<details>
<summary>摘要</summary>
未探测的偏置负荷（PD）是高压直流瓦尔瑙系统（GIS）中的安全关键问题。 Although the diagnosis of PDs under AC voltage is well established, the analysis of PDs under DC voltage remains an active research field. A key focus of these investigations is the classification of different PD sources to enable subsequent sophisticated analysis.在本文中，我们提出了一种基于神经网络的方法，用于分类HVDC GIS中的卷积物和导电粒子引起的PD信号，不需要基于激射序列分析特征。 与前一些方法不同，我们的提议的模型可以在负和正潜能下分辨出 studied PD signals，同时还能泛化到未经见过的操作电压倍数。 此外，我们还比较了时域和频域输入信号的性能，并探讨了不同的normalization schemes来减少卷积物和导电粒子之间的自由空间通路损失的影响。
</details></li>
</ul>
<hr>
<h2 id="A-benchmark-of-categorical-encoders-for-binary-classification"><a href="#A-benchmark-of-categorical-encoders-for-binary-classification" class="headerlink" title="A benchmark of categorical encoders for binary classification"></a>A benchmark of categorical encoders for binary classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09191">http://arxiv.org/abs/2307.09191</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drcohomology/encoderbenchmarking">https://github.com/drcohomology/encoderbenchmarking</a></li>
<li>paper_authors: Federico Matteucci, Vadim Arzamasov, Klemens Boehm</li>
<li>for: 本研究是机器学习领域中 categorical 编码器的最全面的比较研究，涵盖了多种编码器家族的32种配置，以及36种实验因素的组合，在50个数据集上进行了广泛的评估。</li>
<li>methods: 本研究使用了多种编码器家族的32种配置，以及36种实验因素的组合，在50个数据集上进行了广泛的评估。</li>
<li>results: 研究发现，选择数据集、实验因素和综合策略会对比较结论产生深远的影响，这些因素在先前的编码器比较中未得到考虑。<details>
<summary>Abstract</summary>
Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models. Existing encoder benchmark studies lack generalizability because of their limited choice of (1) encoders, (2) experimental factors, and (3) datasets. Additionally, inconsistencies arise from the adoption of varying aggregation strategies. This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 36 combinations of experimental factors, and on 50 datasets. The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions -- aspects disregarded in previous encoder benchmarks.
</details>
<details>
<summary>摘要</summary>
categorical 编码器将 categorical 特征转换为数字表示形式，这些表示形式是机器学习模型的不可或缺的一部分。现有的编码器比较研究受到限制因为它们选择的（1）编码器、（2）实验因素和（3）数据集的选择有限。此外，由于不同的汇集策略的采用，导致了不一致性。这篇论文是目前最全面的 categorical 编码器比较研究，包括了32种编码器家族中的广泛评估，以及36种实验因素的组合，和50个数据集的评估。研究显示数据集选择、实验因素和汇集策略对比较的结论产生了深远的影响，这些因素在前一次编码器比较中被忽略了。
</details></li>
</ul>
<hr>
<h2 id="SBMLtoODEjax-efficient-simulation-and-optimization-of-ODE-SBML-models-in-JAX"><a href="#SBMLtoODEjax-efficient-simulation-and-optimization-of-ODE-SBML-models-in-JAX" class="headerlink" title="SBMLtoODEjax: efficient simulation and optimization of ODE SBML models in JAX"></a>SBMLtoODEjax: efficient simulation and optimization of ODE SBML models in JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08452">http://arxiv.org/abs/2307.08452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flowersteam/sbmltoodejax">https://github.com/flowersteam/sbmltoodejax</a></li>
<li>paper_authors: Mayalen Etcheverry, Michael Levin, Clément Moulin-Frier, Pierre-Yves Oudeyer</li>
<li>for: 这篇论文是为了提供一个可以自动将系统生物学标记语言（SBML）模型转换成Python代码的轻量级库。</li>
<li>methods: 该库使用JAX高性能数学计算库自动推导 capabilities来实现高效的数字化模拟和优化。</li>
<li>results: 该库可以帮助研究人员快速将SBML模型integrated into ихPython项目和机器学习管道，只需几行代码即可实现高性能的数字化模拟和优化。<details>
<summary>Abstract</summary>
Developing methods to explore, predict and control the dynamic behavior of biological systems, from protein pathways to complex cellular processes, is an essential frontier of research for bioengineering and biomedicine. Thus, significant effort has gone in computational inference and mathematical modeling of biological systems. This effort has resulted in the development of large collections of publicly-available models, typically stored and exchanged on online platforms (such as the BioModels Database) using the Systems Biology Markup Language (SBML), a standard format for representing mathematical models of biological systems. SBMLtoODEjax is a lightweight library that allows to automatically parse and convert SBML models into python models written end-to-end in JAX, a high-performance numerical computing library with automatic differentiation capabilities. SBMLtoODEjax is targeted at researchers that aim to incorporate SBML-specified ordinary differential equation (ODE) models into their python projects and machine learning pipelines, in order to perform efficient numerical simulation and optimization with only a few lines of code. SBMLtoODEjax is available at https://github.com/flowersteam/sbmltoodejax.
</details>
<details>
<summary>摘要</summary>
开发方法来探索、预测和控制生物系统的动态行为，从蛋白道路到复杂的细胞过程，是生物工程和生物医学研究的关键前沿。因此，在计算推理和数学模型化方面的努力很大，以致已经形成了大量的公共可用模型，通常通过在线平台（如生物模型数据库）存储和交换，使用系统生物学标记语言（SBML），这是表示生物系统数学模型的标准格式。SBMLtoODEjax 是一个轻量级库，它可以自动解析和将 SBML 模型转换为 Python 模型，并将其写入终端到终端在 JAX 中，JAX 是一个高性能的数值计算库，具有自动导数能力。SBMLtoODEjax 是为研究者们提供，他们想将 SBML 规定的常微分方程（ODE）模型 integrate 到他们的 Python 项目和机器学习管道中，以实现高效的数值优化和优化，只需几行代码。SBMLtoODEjax 可以在 GitHub 上找到：https://github.com/flowersteam/sbmltoodejax。
</details></li>
</ul>
<hr>
<h2 id="From-random-walks-to-graph-sprints-a-low-latency-node-embedding-framework-on-continuous-time-dynamic-graphs"><a href="#From-random-walks-to-graph-sprints-a-low-latency-node-embedding-framework-on-continuous-time-dynamic-graphs" class="headerlink" title="From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs"></a>From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08433">http://arxiv.org/abs/2307.08433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Naser Eddin, Jacopo Bono, David Aparício, Hugo Ferreira, João Ascensão, Pedro Ribeiro, Pedro Bizarro</li>
<li>for: 这篇研究是为了提出一个能够实现低延迟、高效的动态图像学习框架，并且能够处理真实世界的动态图像资料。</li>
<li>methods: 这篇研究使用了流动测量的方法，即时间感知的点 cloud 来捕捉多阶资讯，并且使用了单一阶资讯来计算时间感知的点 cloud。</li>
<li>results: 研究结果显示，使用 graph-sprints 的方法可以实现与现有的高延迟模型相同或更好的性能，并且可以实现低延迟的推断运算。<details>
<summary>Abstract</summary>
Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operations on the incoming edges. We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive performance (outperforming all baselines for the node classification tasks in five datasets). Simultaneously, graph-sprints significantly reduce inference latencies, achieving close to an order of magnitude speed-up in our experimental setting.
</details>
<details>
<summary>摘要</summary>
muchos datos del mundo real tienen una estructura de grafo subyacente dinámica, donde las entidades y sus interacciones evolucionan con el tiempo. Los modelos de aprendizaje automático deben considerar estas dinámicas para aprovechar su potencial completo en tareas downstream. Los enfoques anteriores para aprender representaciones de grafos han centrado en la muestra de neighbborhoods k-hop, similar a búsqueda en profundidad, o caminatas aleatorias, similar a búsqueda en anchura. Sin embargo, estos métodos son costosos en términos de computación y no son adecuados para inferencia en tiempo real y baja latencia en grafos dinámicos. Para superar estos límites, propusimos graph-sprints, un marco general de extracción de características para grafos continuos en tiempo dinámico (CTDGs) que tiene baja latencia y es competitivo con modelos de estado del arte de mayor latencia. Para lograr esto, se propone una aproximación de bajo latencia y streaming a las características basadas en caminatas aleatorias. En nuestro marco, las embeddings de nodos conscientes del tiempo resumen la información de múltiples hop utilizando solo operaciones de un hop en las aristas entrantes. Evaluamos nuestro enfoque propuesto en tres conjuntos de datos abiertos y dos conjuntos de datos internos, y lo comparamos con tres algoritmos estado del arte (TGN-attn, TGN-ID, Jodie). Demostramos que nuestras características de graph-sprints, combinadas con una clase de aprendizaje automático, logran rendimientos competitivos (superando a todos los baselines para las tareas de clasificación de nodos en cinco conjuntos de datos). Al mismo tiempo, graph-sprints reduce significativamente las latencias de inferencia, logrando una reducción de cerca de un orden de magnitud en nuestra configuración experimental.
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-Science-in-Quantum-Atomistic-and-Continuum-Systems"><a href="#Artificial-Intelligence-for-Science-in-Quantum-Atomistic-and-Continuum-Systems" class="headerlink" title="Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems"></a>Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08423">http://arxiv.org/abs/2307.08423</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/divelab/AIRS">https://github.com/divelab/AIRS</a></li>
<li>paper_authors: Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, YuQing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stärk, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Alán Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Liò, Rose Yu, Stephan Günnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, Shuiwang Ji</li>
<li>for: 这篇论文主要针对的是利用人工智能（AI）进行自然科学研究（AI4Science）的新 paradigm。</li>
<li>methods: 论文使用的方法包括深度学习方法，以捕捉自然系统中的物理原理，特别是对称变换的equivariance。</li>
<li>results: 论文提供了一种Foundational and unified treatment of AI for quantum, atomistic, and continuum systems，并提出了一些解释性、过度分布采样和不确定性评估等技术挑战。<details>
<summary>Abstract</summary>
Advances in artificial intelligence (AI) are fueling a new paradigm of discoveries in natural sciences. Today, AI has started to advance natural sciences by improving, accelerating, and enabling our understanding of natural phenomena at a wide range of spatial and temporal scales, giving rise to a new area of research known as AI for science (AI4Science). Being an emerging research paradigm, AI4Science is unique in that it is an enormous and highly interdisciplinary area. Thus, a unified and technical treatment of this field is needed yet challenging. This paper aims to provide a technically thorough account of a subarea of AI4Science; namely, AI for quantum, atomistic, and continuum systems. These areas aim at understanding the physical world from the subatomic (wavefunctions and electron density), atomic (molecules, proteins, materials, and interactions), to macro (fluids, climate, and subsurface) scales and form an important subarea of AI4Science. A unique advantage of focusing on these areas is that they largely share a common set of challenges, thereby allowing a unified and foundational treatment. A key common challenge is how to capture physics first principles, especially symmetries, in natural systems by deep learning methods. We provide an in-depth yet intuitive account of techniques to achieve equivariance to symmetry transformations. We also discuss other common technical challenges, including explainability, out-of-distribution generalization, knowledge transfer with foundation and large language models, and uncertainty quantification. To facilitate learning and education, we provide categorized lists of resources that we found to be useful. We strive to be thorough and unified and hope this initial effort may trigger more community interests and efforts to further advance AI4Science.
</details>
<details>
<summary>摘要</summary>
人工智能技术的发展（AI）正在推动一种新的发现 paradigm in 自然科学。今天，AI已经开始为自然科学的研究提供了改进、加速和实现自然现象的理解，从而创造了一个新的研究领域——AI for science（AI4Science）。作为一个emerging research paradigm，AI4Science具有巨大的多学科性和挑战性，因此需要一种统一和技术性的处理。本文的目的是提供AI4Science中的一个子领域的技术深入报告，即用AI研究量子、原子istic和连续体系。这些领域旨在理解自然世界从子原子（振荡函数和电子密度）、原子（分子、蛋白质、材料和交互）到宏观（液体、气候和地壳）级别的物理世界，并形成AI4Science中一个重要的子领域。这些领域之间共享许多挑战，因此可以实现一种统一和基础的处理。一个关键的共同挑战是如何通过深度学习方法捕捉自然系统中的物理基本原理，特别是对称性。我们提供了深入 yet 易于理解的对于实现对称变换的方法的详细讲解。我们还讨论了其他一些常见的技术挑战，包括可解释性、out-of-distribution扩展、基础和大语言模型知识传递、和不确定性评估。为便于学习和教育，我们提供了分类列表，我们认为是有用的资源。我们努力保持统一和完整，希望这个初步努力可以触发更多的社区兴趣和努力，以进一步推动AI4Science的发展。
</details></li>
</ul>
<hr>
<h2 id="Neurosymbolic-AI-for-Reasoning-on-Biomedical-Knowledge-Graphs"><a href="#Neurosymbolic-AI-for-Reasoning-on-Biomedical-Knowledge-Graphs" class="headerlink" title="Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs"></a>Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08411">http://arxiv.org/abs/2307.08411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Nicole DeLong, Ramon Fernández Mir, Zonglin Ji, Fiona Niamh Coulter Smith, Jacques D. Fleuriot</li>
<li>for: 这篇论文旨在介绍基于神经符号智能的 hybrid 方法，以及其在生物医学领域中的应用和优势。</li>
<li>methods: 这篇论文主要介绍了基于 embedding 和符号 logic 的 hybrid 方法，以及它们在生物医学领域中的应用。</li>
<li>results: 论文总结了 hybrid 方法的优势和可能性，并指出了它们在生物医学领域中的应用可能性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Biomedical datasets are often modeled as knowledge graphs (KGs) because they capture the multi-relational, heterogeneous, and dynamic natures of biomedical systems. KG completion (KGC), can, therefore, help researchers make predictions to inform tasks like drug repositioning. While previous approaches for KGC were either rule-based or embedding-based, hybrid approaches based on neurosymbolic artificial intelligence are becoming more popular. Many of these methods possess unique characteristics which make them even better suited toward biomedical challenges. Here, we survey such approaches with an emphasis on their utilities and prospective benefits for biomedicine.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vocoder-drift-compensation-by-x-vector-alignment-in-speaker-anonymisation"><a href="#Vocoder-drift-compensation-by-x-vector-alignment-in-speaker-anonymisation" class="headerlink" title="Vocoder drift compensation by x-vector alignment in speaker anonymisation"></a>Vocoder drift compensation by x-vector alignment in speaker anonymisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08403">http://arxiv.org/abs/2307.08403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Panariello, Massimiliano Todisco, Nicholas Evans</li>
<li>for: 本研究旨在探讨xvector基于的听者隐私保护方法中， vocoding而不是核心隐私函数对听者隐私的保护产生了主要的影响。</li>
<li>methods: 本研究使用了一种新的方法来衡量 vocoder drift，并提出了一种新的隐私函数来减少 vocoder drift。</li>
<li>results: 研究发现，使用新的隐私函数可以有效地减少 vocoder drift，并提供了更好的控制 sobre xvector 空间。<details>
<summary>Abstract</summary>
For the most popular x-vector-based approaches to speaker anonymisation, the bulk of the anonymisation can stem from vocoding rather than from the core anonymisation function which is used to substitute an original speaker x-vector with that of a fictitious pseudo-speaker. This phenomenon can impede the design of better anonymisation systems since there is a lack of fine-grained control over the x-vector space. The work reported in this paper explores the origin of so-called vocoder drift and shows that it is due to the mismatch between the substituted x-vector and the original representations of the linguistic content, intonation and prosody. Also reported is an original approach to vocoder drift compensation. While anonymisation performance degrades as expected, compensation reduces vocoder drift substantially, offers improved control over the x-vector space and lays a foundation for the design of better anonymisation functions in the future.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "x-vector" is translated as "语音特征向量" (yùn zhòng yǐn xiàng wù)* "vocoding" is translated as "语音编码" (yùn zhòng biān mǎ)* "anonymization" is translated as "匿名化" (mìng mìng huà)* "core anonymization function" is translated as "核心匿名函数" (zhū xīn mìng mìng fù xiàng)* "fictitious pseudo-speaker" is translated as "虚拟的假发音者" (xū yì de jiǎ fā yīn zhě)* "linguistic content, intonation, and prosody" are translated as "语言内容、听调和语调" (yǔ yán nèi xìng, tīng diào hé yǔ diào)* "vocoder drift compensation" is translated as "语音编码落差补做" (yùn zhòng biān mǎ lù chē bǔ zuò)
</details></li>
</ul>
<hr>
<h2 id="On-the-application-of-Large-Language-Models-for-language-teaching-and-assessment-technology"><a href="#On-the-application-of-Large-Language-Models-for-language-teaching-and-assessment-technology" class="headerlink" title="On the application of Large Language Models for language teaching and assessment technology"></a>On the application of Large Language Models for language teaching and assessment technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08393">http://arxiv.org/abs/2307.08393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Caines, Luca Benedetto, Shiva Taslimipoor, Christopher Davis, Yuan Gao, Oeistein Andersen, Zheng Yuan, Mark Elliott, Russell Moore, Christopher Bryant, Marek Rei, Helen Yannakoudakis, Andrew Mullooly, Diane Nicholls, Paula Buttery</li>
<li>for: 这篇论文探讨了用大型自然语言处理模型（PaLM和GPT-4）在语言教学和评估系统中的潜在应用。</li>
<li>methods: 论文考虑了几个研究领域，并讨论了在教育技术中使用生成AI的风险和伦理问题。</li>
<li>results: 研究发现大型语言模型在文本生成方面有所改进，但是在自动评分和语法错误检查方面，大型语言模型独立使用不能超越现有的状态艺术metric。<details>
<summary>Abstract</summary>
The recent release of very large language models such as PaLM and GPT-4 has made an unprecedented impact in the popular media and public consciousness, giving rise to a mixture of excitement and fear as to their capabilities and potential uses, and shining a light on natural language processing research which had not previously received so much attention. The developments offer great promise for education technology, and in this paper we look specifically at the potential for incorporating large language models in AI-driven language teaching and assessment systems. We consider several research areas and also discuss the risks and ethical considerations surrounding generative AI in education technology for language learners. Overall we find that larger language models offer improvements over previous models in text generation, opening up routes toward content generation which had not previously been plausible. For text generation they must be prompted carefully and their outputs may need to be reshaped before they are ready for use. For automated grading and grammatical error correction, tasks whose progress is checked on well-known benchmarks, early investigations indicate that large language models on their own do not improve on state-of-the-art results according to standard evaluation metrics. For grading it appears that linguistic features established in the literature should still be used for best performance, and for error correction it may be that the models can offer alternative feedback styles which are not measured sensitively with existing methods. In all cases, there is work to be done to experiment with the inclusion of large language models in education technology for language learners, in order to properly understand and report on their capacities and limitations, and to ensure that foreseeable risks such as misinformation and harmful bias are mitigated.
</details>
<details>
<summary>摘要</summary>
最近发布的非常大的自然语言处理模型，如PaLM和GPT-4，在流行媒体和公众意识中产生了无前例的影响，引发了诸多人的兴奋和担忧，对其能力和应用领域的潜在影响。这些发展在教育技术方面具有巨大潜力，在这篇论文中，我们专门关注在AI驱动的语言教学和评估系统中可能的应用。我们考虑了多个研究领域，并讨论了生成AI在教育技术中的风险和伦理考虑。总之，更大的语言模型在文本生成方面提供了改进，打开了新的内容生成途径，但是需要谨慎地提供提示，并且输出可能需要重新处理。在自动评分和 grammatical error correction 方面，初步调查表明，大语言模型独立使用不会超越现有的标准评价指标。在评分方面，使用现有的语言特征仍然是最佳选择，而在 error correction 方面，模型可能提供不同的反馈样式，不同于现有方法的敏感度评价。总之，需要进行实验来探索将大语言模型包含在教育技术中的可能性和局限性，以确保预期的风险，如误导和不良偏见，得到控制。
</details></li>
</ul>
<hr>
<h2 id="Correlation-aware-Spatial-Temporal-Graph-Learning-for-Multivariate-Time-series-Anomaly-Detection"><a href="#Correlation-aware-Spatial-Temporal-Graph-Learning-for-Multivariate-Time-series-Anomaly-Detection" class="headerlink" title="Correlation-aware Spatial-Temporal Graph Learning for Multivariate Time-series Anomaly Detection"></a>Correlation-aware Spatial-Temporal Graph Learning for Multivariate Time-series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08390">http://arxiv.org/abs/2307.08390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/astha-chem/mvts-ano-eval">https://github.com/astha-chem/mvts-ano-eval</a></li>
<li>paper_authors: Yu Zheng, Huan Yee Koh, Ming Jin, Lianhua Chi, Khoa T. Phan, Shirui Pan, Yi-Ping Phoebe Chen, Wei Xiang</li>
<li>for: 本研究旨在提出一种新的多变量时间序列异常检测方法，以解决现有方法中的非线性关系捕捉和时间序列异常检测问题。</li>
<li>methods: 本方法基于多变量时间序列相关学习模块，并采用空间时间图学习网络（STGNN）来编码复杂的变量间相关性。另外，通过借鉴一元和多元邻居信息，我们的STGNN组件可以吸收复杂的空间信息。同时，我们还提出了一种新的异常分数组件，可以在无监督情况下估计异常程度。</li>
<li>results: 实验结果表明，CST-GL方法可以在一般情况下有效地检测异常，并且可以在不同的时间延迟下进行早期检测。<details>
<summary>Abstract</summary>
Multivariate time-series anomaly detection is critically important in many applications, including retail, transportation, power grid, and water treatment plants. Existing approaches for this problem mostly employ either statistical models which cannot capture the non-linear relations well or conventional deep learning models (e.g., CNN and LSTM) that do not explicitly learn the pairwise correlations among variables. To overcome these limitations, we propose a novel method, correlation-aware spatial-temporal graph learning (termed CST-GL), for time series anomaly detection. CST-GL explicitly captures the pairwise correlations via a multivariate time series correlation learning module based on which a spatial-temporal graph neural network (STGNN) can be developed. Then, by employing a graph convolution network that exploits one- and multi-hop neighbor information, our STGNN component can encode rich spatial information from complex pairwise dependencies between variables. With a temporal module that consists of dilated convolutional functions, the STGNN can further capture long-range dependence over time. A novel anomaly scoring component is further integrated into CST-GL to estimate the degree of an anomaly in a purely unsupervised manner. Experimental results demonstrate that CST-GL can detect anomalies effectively in general settings as well as enable early detection across different time delays.
</details>
<details>
<summary>摘要</summary>
多变量时间序列异常检测在许多应用程序中非常重要，包括零售、交通、电力网络和水处理厂。现有的方法通常使用统计模型，这些模型不能很好地捕捉非线性关系，或者使用传统的深度学习模型（如CNN和LSTM），这些模型不直接学习时间序列变量之间的对比关系。为了解决这些限制，我们提出了一种新的方法，即相关意识空间时间图学习（CST-GL），用于时间序列异常检测。CST-GL使用多变量时间序列相关学习模块，该模块可以识别时间序列变量之间的对比关系。然后，通过基于这些相关关系的空间时间图 neural network（STGNN）的开发，我们可以融合复杂的对比关系信息，以获得rich的空间信息。此外，我们还使用一个包含扩展延迟 convolutional functions的时间模块，以捕捉长距离时间关系。最后，我们还添加了一个异常分数组件，以无监督的方式估算异常的程度。实验结果表明，CST-GL可以有效地检测异常情况，并且可以在不同的时间延迟下进行早期检测。
</details></li>
</ul>
<hr>
<h2 id="Tabular-Machine-Learning-Methods-for-Predicting-Gas-Turbine-Emissions"><a href="#Tabular-Machine-Learning-Methods-for-Predicting-Gas-Turbine-Emissions" class="headerlink" title="Tabular Machine Learning Methods for Predicting Gas Turbine Emissions"></a>Tabular Machine Learning Methods for Predicting Gas Turbine Emissions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08386">http://arxiv.org/abs/2307.08386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca Potts, Rick Hackney, Georgios Leontidis</li>
<li>for: 这个研究旨在评估机器学习模型在预测液压机发动机排放气体中的性能。</li>
<li>methods: 我们比较了一个现有的预测排放模型（化学动力学模型）和我们基于SAINT和XGBoost的两种机器学习模型，以示机器学习技术可以提供更好的预测性能。</li>
<li>results: 我们发现，使用机器学习技术可以提高氮氧化物（NOx）和碳 моно氧化物（CO）的预测性能。<details>
<summary>Abstract</summary>
Predicting emissions for gas turbines is critical for monitoring harmful pollutants being released into the atmosphere. In this study, we evaluate the performance of machine learning models for predicting emissions for gas turbines. We compare an existing predictive emissions model, a first principles-based Chemical Kinetics model, against two machine learning models we developed based on SAINT and XGBoost, to demonstrate improved predictive performance of nitrogen oxides (NOx) and carbon monoxide (CO) using machine learning techniques. Our analysis utilises a Siemens Energy gas turbine test bed tabular dataset to train and validate the machine learning models. Additionally, we explore the trade-off between incorporating more features to enhance the model complexity, and the resulting presence of increased missing values in the dataset.
</details>
<details>
<summary>摘要</summary>
预测液压机排放是监测污染物排入大气中的关键。本研究对机器学习模型的表现进行评估，以证明使用机器学习技术可以改善液压机排放氮氧化物和碳 моно氧化物的预测性能。我们比较了现有的预测排放模型、基于化学动力学的首要原理模型，与我们基于SAINT和XGBoost开发的两种机器学习模型，以示出机器学习技术的改善性。我们的分析使用了Siemens Energy液压机测试床数据集来训练和验证机器学习模型。此外，我们还探讨了增加特征以增强模型复杂性所带来的数据缺失问题。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Battery-Lifetime-Under-Varying-Usage-Conditions-from-Early-Aging-Data"><a href="#Predicting-Battery-Lifetime-Under-Varying-Usage-Conditions-from-Early-Aging-Data" class="headerlink" title="Predicting Battery Lifetime Under Varying Usage Conditions from Early Aging Data"></a>Predicting Battery Lifetime Under Varying Usage Conditions from Early Aging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08382">http://arxiv.org/abs/2307.08382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingkai Li, Zihao Zhou, Adam Thelen, David Howey, Chao Hu</li>
<li>For: 预测 Lithium-ion 电池寿命，以便预防维护、赔偿和改进电池设计和生产。* Methods: 利用 early-life 数据（例如容量-电压数据） derivate 新的特征，以预测 cells 在不同充放速度、放电速度和充放深度下的寿命。* Results: 使用新生成的数据集（来自 225 个 nickel-manganese-cobalt&#x2F;graphite Li-ion 电池），实现了准确预测 in-distribution cells 的寿命（15.1% 的 mean absolute percentage error），并且使用 hierarchical Bayesian regression model 可以更好地预测 extrapolation 情况（21.8% 的 mean absolute percentage error）。<details>
<summary>Abstract</summary>
Accurate battery lifetime prediction is important for preventative maintenance, warranties, and improved cell design and manufacturing. However, manufacturing variability and usage-dependent degradation make life prediction challenging. Here, we investigate new features derived from capacity-voltage data in early life to predict the lifetime of cells cycled under widely varying charge rates, discharge rates, and depths of discharge. Features were extracted from regularly scheduled reference performance tests (i.e., low rate full cycles) during cycling. The early-life features capture a cell's state of health and the rate of change of component-level degradation modes, some of which correlate strongly with cell lifetime. Using a newly generated dataset from 225 nickel-manganese-cobalt/graphite Li-ion cells aged under a wide range of conditions, we demonstrate a lifetime prediction of in-distribution cells with 15.1% mean absolute percentage error using no more than the first 15% of data, for most cells. Further testing using a hierarchical Bayesian regression model shows improved performance on extrapolation, achieving 21.8% mean absolute percentage error for out-of-distribution cells. Our approach highlights the importance of using domain knowledge of lithium-ion battery degradation modes to inform feature engineering. Further, we provide the community with a new publicly available battery aging dataset with cells cycled beyond 80% of their rated capacity.
</details>
<details>
<summary>摘要</summary>
importante battery lifetime prediction è importante per la manutenzione preventiva, le garanzie e l'improvviso design e produzione di celle. However, la variabilità di produzione e la degradazione dipendenti dall'utilizzo rendono la predizione della vita difficile. Ecco, investigiamo nuove feature derivate dai dati di capacità e tensione in primis dell' vita per predir la durata delle celle ciclate sotto caricate e discariche widely varying. Le feature sono estratte dai test di riferimento regolarmente programmati (ad esempio, cicli full low rate) durante il ciclo. Le feature early-life capture lo stato di salute della cella e il tasso di cambiamento dei modi di degradazione dei componenti, alcuni dei quali correlano strettamente con la durata della cella. Utilizzando un nuovo dataset generato da 225 celle nickel-manganese-cobalt/graphite Li-ion aged under a wide range of conditions, dimostriamo una predizione di vita di cellule in-distribution con un errore assoluto del 15,1% using no more than the first 15% of data, per la maggior parte delle celle. Further testing using a hierarchical Bayesian regression model shows improved performance on extrapolation, achieving 21,8% errore assoluto percentage per le cellule out-of-distribution. Our approach highlights the importance of using domain knowledge of lithium-ion battery degradation modes to inform feature engineering. Further, we provide the community with a new publicly available battery aging dataset with cells cycled beyond 80% of their rated capacity.
</details></li>
</ul>
<hr>
<h2 id="Q-D-O-ES-Population-based-Quality-Diversity-Optimisation-for-Post-Hoc-Ensemble-Selection-in-AutoML"><a href="#Q-D-O-ES-Population-based-Quality-Diversity-Optimisation-for-Post-Hoc-Ensemble-Selection-in-AutoML" class="headerlink" title="Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble Selection in AutoML"></a>Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble Selection in AutoML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08364">http://arxiv.org/abs/2307.08364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LennartPurucker/PopulationBasedQDO-PostHocEnsembleSelectionAutoML">https://github.com/LennartPurucker/PopulationBasedQDO-PostHocEnsembleSelectionAutoML</a></li>
<li>paper_authors: Lennart Purucker, Lennart Schneider, Marie Anastacio, Joeran Beel, Bernd Bischl, Holger Hoos</li>
<li>for: 提高预测性能（Post hoc ensemble learning）</li>
<li>methods: 引入两种新的人口基于 ensemble selection方法（QO-ES和QDO-ES），比较GES</li>
<li>results: 在71个分类数据集上测试，QO-ES和QDO-ES常常超过GES，但只有在验证数据上 statistically significant，并且发现多样性可以优化后期ensemble，但也增加预测风险。<details>
<summary>Abstract</summary>
Automated machine learning (AutoML) systems commonly ensemble models post hoc to improve predictive performance, typically via greedy ensemble selection (GES). However, we believe that GES may not always be optimal, as it performs a simple deterministic greedy search. In this work, we introduce two novel population-based ensemble selection methods, QO-ES and QDO-ES, and compare them to GES. While QO-ES optimises solely for predictive performance, QDO-ES also considers the diversity of ensembles within the population, maintaining a diverse set of well-performing ensembles during optimisation based on ideas of quality diversity optimisation. The methods are evaluated using 71 classification datasets from the AutoML benchmark, demonstrating that QO-ES and QDO-ES often outrank GES, albeit only statistically significant on validation data. Our results further suggest that diversity can be beneficial for post hoc ensembling but also increases the risk of overfitting.
</details>
<details>
<summary>摘要</summary>
自动机器学习（AutoML）系统通常会 ensemble 模型后增进预测性能，通常透过单调式排序（GES）。但我们认为 GES 可能不是最佳，因为它只是一个简单决策的排序。在这个工作中，我们引入了两种新的人口基于的ensemble选择方法，QO-ES 和 QDO-ES，并与 GES 进行比较。而 QO-ES 则优化仅对预测性能，而 QDO-ES 则考虑ensemble population 中的多样性，保持一个多样的集合 ensemble 的多样性在依据质量多样化优化。这些方法在 AutoML benchmark 中的 71 个分类 datasets 上进行评估，结果显示 QO-ES 和 QDO-ES 通常比 GES 高，但仅在验证数据上 statistically significant。我们的结果还表明了多样性可以帮助后续的ensemble，但也增加了过滤的风险。
</details></li>
</ul>
<hr>
<h2 id="Universal-Online-Learning-with-Gradual-Variations-A-Multi-layer-Online-Ensemble-Approach"><a href="#Universal-Online-Learning-with-Gradual-Variations-A-Multi-layer-Online-Ensemble-Approach" class="headerlink" title="Universal Online Learning with Gradual Variations: A Multi-layer Online Ensemble Approach"></a>Universal Online Learning with Gradual Variations: A Multi-layer Online Ensemble Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08360">http://arxiv.org/abs/2307.08360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou</li>
<li>for: 这个论文是为了提出一种在线 convex optimization 方法，该方法可以在不同级别上适应不同类型和凹度的损失函数。</li>
<li>methods: 该方法基于一种多层在线 ensemble，包括新的优势函数和层次误差 correction。</li>
<li>results: 该方法可以获得 $\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$ 和 $\hat{\mathcal{O}}(\sqrt{V_T})$ 的 regret bounds，其中 $d$ 是维度、$V_T$ 是问题依赖于的梯度变化。此外，该方法还有广泛的应用和意义，包括保证最坏情况下的 guarantees，直接从分析中提取小损 bounds，以及与对抗&#x2F;随机 convex optimization 和游戏理论的深刻连接。<details>
<summary>Abstract</summary>
In this paper, we propose an online convex optimization method with two different levels of adaptivity. On a higher level, our method is agnostic to the specific type and curvature of the loss functions, while at a lower level, it can exploit the niceness of the environments and attain problem-dependent guarantees. To be specific, we obtain $\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$ and $\hat{\mathcal{O}}(\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and $\hat{\mathcal{O}}(\cdot)$-notation omits logarithmic factors on $V_T$. Our result finds broad implications and applications. It not only safeguards the worst-case guarantees, but also implies the small-loss bounds in analysis directly. Besides, it draws deep connections with adversarial/stochastic convex optimization and game theory, further validating its practical potential. Our method is based on a multi-layer online ensemble incorporating novel ingredients, including carefully-designed optimism for unifying diverse function types and cascaded corrections for algorithmic stability. Remarkably, despite its multi-layer structure, our algorithm necessitates only one gradient query per round, making it favorable when the gradient evaluation is time-consuming. This is facilitated by a novel regret decomposition equipped with customized surrogate losses.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种在线凸优化方法，具有两个不同的水平的适应性。在更高的水平上，我们的方法是对具体类型和曲率的损失函数不偏袋，而在更低的水平上，它可以利用环境的温柔性并实现问题依赖的保证。具体来说，我们获得了$\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$和$\hat{\mathcal{O}}(\sqrt{V_T})$的 regret bound，其中$d$是维度，$V_T$表示问题依赖的梯度变化，$\hat{\mathcal{O}}(\cdot)$-notation忽略了$V_T$的对数因子。我们的结果具有广泛的应用和意义。它不仅保证了最坏情况的保证，而且直接从分析中获得了小损失的 bound。此外，它还与反对抗/随机凸优化和游戏理论之间存在深刻的连接，进一步证明其实用性。我们的方法基于一种多层在线ensemble，包括新的优势因子，例如精心设计的乐观性以及随机逻辑的级联 corrections。备注意的是，即使具有多层结构，我们的算法只需要每个回合一次获取梯度，因此在梯度评估是时间耗费的情况下，它是有利的。这是由一种新的 regret decomposition和自定义损失函数帮助实现的。
</details></li>
</ul>
<hr>
<h2 id="Zero-th-Order-Algorithm-for-Softmax-Attention-Optimization"><a href="#Zero-th-Order-Algorithm-for-Softmax-Attention-Optimization" class="headerlink" title="Zero-th Order Algorithm for Softmax Attention Optimization"></a>Zero-th Order Algorithm for Softmax Attention Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08352">http://arxiv.org/abs/2307.08352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Zhihang Li, Sridhar Mahadevan, Zhao Song</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）的优化技术，特别是在计算梯度时的效率。</li>
<li>methods: 本研究提出了一种针对软max单元的零次顺序算法，通过只进行前向传输来approximately计算梯度。</li>
<li>results: 我们的算法能够高效地计算大规模LLM的梯度，并且可以在不同的语言模型中实现效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have brought about significant transformations in human society. Among the crucial computations in LLMs, the softmax unit holds great importance. Its helps the model generating a probability distribution on potential subsequent words or phrases, considering a series of input words. By utilizing this distribution, the model selects the most probable next word or phrase, based on the assigned probabilities. The softmax unit assumes a vital function in LLM training as it facilitates learning from data through the adjustment of neural network weights and biases.   With the development of the size of LLMs, computing the gradient becomes expensive. However, Zero-th Order method can approximately compute the gradient with only forward passes. In this paper, we present a Zero-th Order algorithm specifically tailored for Softmax optimization. We demonstrate the convergence of our algorithm, highlighting its effectiveness in efficiently computing gradients for large-scale LLMs. By leveraging the Zeroth-Order method, our work contributes to the advancement of optimization techniques in the context of complex language models.
</details>
<details>
<summary>摘要</summary>
With the development of the size of LLMs, computing the gradient becomes expensive. However, the Zero-th Order method can approximately compute the gradient with only forward passes. In this paper, we present a Zero-th Order algorithm specifically tailored for Softmax optimization. We demonstrate the convergence of our algorithm, highlighting its effectiveness in efficiently computing gradients for large-scale LLMs. By leveraging the Zeroth-Order method, our work contributes to the advancement of optimization techniques in the context of complex language models.(Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation may differ slightly from the traditional Chinese writing system used in Hong Kong and Taiwan.)
</details></li>
</ul>
<hr>
<h2 id="M-FLAG-Medical-Vision-Language-Pre-training-with-Frozen-Language-Models-and-Latent-Space-Geometry-Optimization"><a href="#M-FLAG-Medical-Vision-Language-Pre-training-with-Frozen-Language-Models-and-Latent-Space-Geometry-Optimization" class="headerlink" title="M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization"></a>M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08347">http://arxiv.org/abs/2307.08347</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cheliu-computation/m-flag-miccai2023">https://github.com/cheliu-computation/m-flag-miccai2023</a></li>
<li>paper_authors: Che Liu, Sibo Cheng, Chen Chen, Mengyun Qiao, Weitong Zhang, Anand Shah, Wenjia Bai, Rossella Arcucci</li>
<li>for: 这篇研究旨在提出一种新的医疗视力语言模型预训练方法，以提高医疗视力语言模型的训练稳定性和效率。</li>
<li>methods: 提案方法名为M-FLAG，它利用固定的语言模型进行预训练，并引入一个新的正交对映损失来调和视力语言模型的内存空间几何。</li>
<li>results: 实验结果显示，M-FLAG方法可以对医疗视力语言模型进行有效的预训练，并在三个下游任务中表现出色：医疗图像分类、分割和物体检测。尤其是在分割任务中，M-FLAG方法只使用了RSNA数据集的1%，却可以超越已经精心适应的ImageNet预训练模型。<details>
<summary>Abstract</summary>
Medical vision-language models enable co-learning and integrating features from medical imaging and clinical text. However, these models are not easy to train and the latent representation space can be complex. Here we propose a novel way for pre-training and regularising medical vision-language models. The proposed method, named Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), leverages a frozen language model for training stability and efficiency and introduces a novel orthogonality loss to harmonize the latent space geometry. We demonstrate the potential of the pre-trained model on three downstream tasks: medical image classification, segmentation, and object detection. Extensive experiments across five public datasets demonstrate that M-FLAG significantly outperforms existing medical vision-language pre-training approaches and reduces the number of parameters by 78\%. Notably, M-FLAG achieves outstanding performance on the segmentation task while using only 1\% of the RSNA dataset, even outperforming ImageNet pre-trained models that have been fine-tuned using 100\% of the data.
</details>
<details>
<summary>摘要</summary>
医疗视语模型可以同时学习医疗影像和临床文本特征。然而，这些模型不易于训练，其潜在表示空间可能很复杂。在这里，我们提出了一种新的医疗视语预训练方法，名为医疗视语预训练with Frozen language models和Latent spAce Geometry optimization（M-FLAG）。我们利用一个冻结的语言模型来保持训练稳定和高效，并引入了一种新的正交准则来融和潜在空间准则。我们在三个下游任务中展示了预训练模型的潜力：医疗影像分类、 segmentation 和对象检测。我们在五个公共数据集进行了广泛的实验，并证明了M-FLAG在现有的医疗视语预训练方法中显著超越，并将参数数量减少了78%。特别是，M-FLAG在分割任务上表现出色，只使用了RSNA数据集的1%，甚至超过了ImageNet预训练模型，这些模型在100%的数据上进行了精细调节。
</details></li>
</ul>
<hr>
<h2 id="Efficient-selective-attention-LSTM-for-well-log-curve-synthesis"><a href="#Efficient-selective-attention-LSTM-for-well-log-curve-synthesis" class="headerlink" title="Efficient selective attention LSTM for well log curve synthesis"></a>Efficient selective attention LSTM for well log curve synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10253">http://arxiv.org/abs/2307.10253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankai Zhou, Huanyu Li, Hu liu</li>
<li>for: 这 paper 是为了提出一种机器学习方法，用于预测缺失的井 logging 曲线。</li>
<li>methods: 该方法基于传统的 Long Short-Term Memory (LSTM) 神经网络，并添加了一个自注意机制来分析数据的空间相关性。</li>
<li>results: 实验结果表明，该方法可以高效地预测缺失的井 logging 曲线，并且比传统的 Fully Connected Neural Networks (FCNN) 和 LSTM 方法更高精度。<details>
<summary>Abstract</summary>
Non-core drilling has gradually become the primary exploration method in geological engineering, and well logging curves have increasingly gained importance as the main carriers of geological information. However, factors such as geological environment, logging equipment, borehole quality, and unexpected events can all impact the quality of well logging curves. Previous methods of re-logging or manual corrections have been associated with high costs and low efficiency. This paper proposes a machine learning method that utilizes existing data to predict missing well logging curves, and its effectiveness and feasibility have been validated through experiments. The proposed method builds upon the traditional Long Short-Term Memory (LSTM) neural network by incorporating a self-attention mechanism to analyze the spatial dependencies of the data. It selectively includes the dominant computational results in the LSTM, reducing the computational complexity from O(n^2) to O(nlogn) and improving model efficiency. Experimental results demonstrate that the proposed method achieves higher accuracy compared to traditional curve synthesis methods based on Fully Connected Neural Networks (FCNN) and LSTM. This accurate, efficient, and cost-effective prediction method holds practical value in engineering applications.
</details>
<details>
<summary>摘要</summary>
非核心钻探逐渐成为地质工程的主要探测方法，而井 logging 曲线也逐渐成为主要的地质信息传递者。然而，地质环境、钻探设备、井井质量和意外事件等因素都会影响井 logging 曲线的质量。过去的重新采样或手动修正方法均具有高成本和低效率。这篇论文提出了一种使用现有数据预测缺失井 logging 曲线的机器学习方法，并通过实验证明其效果和可行性。该方法基于传统的 Long Short-Term Memory (LSTM) 神经网络，并在该网络中添加了自注意机制来分析数据的空间相关性。它选择性地包含 LSTM 中的主导计算结果，从而将计算复杂性从 O(n^2) 降低到 O(nlogn)，提高模型效率。实验结果表明，提议的方法与基于 Fully Connected Neural Networks (FCNN) 和 LSTM 的传统曲线合成方法相比，具有更高的准确率。这种准确、有效、Cost-effective 的预测方法在工程应用中具有实际价值。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-processes-for-Bayesian-inverse-problems-associated-with-linear-partial-differential-equations"><a href="#Gaussian-processes-for-Bayesian-inverse-problems-associated-with-linear-partial-differential-equations" class="headerlink" title="Gaussian processes for Bayesian inverse problems associated with linear partial differential equations"></a>Gaussian processes for Bayesian inverse problems associated with linear partial differential equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08343">http://arxiv.org/abs/2307.08343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianming Bai, Aretha L. Teckentrup, Konstantinos C. Zygalakis</li>
<li>for: 该论文关注使用 Gaussian 替身模型解决 bayesian 反问题，特别是只有小量训练数据的情况下。</li>
<li>methods:  authors extend Raissi et al. (2017) 的框架，使用 PDE-informed Gaussian 假设来构建不同的approximate posteriors。</li>
<li>results:  numerical experiments 表明，使用 PDE-informed Gaussian 假设可以提高模型的性能，比传统假设更好。Here’s the same information in English:</li>
<li>for: The paper focuses on using Gaussian surrogate models for Bayesian inverse problems associated with linear partial differential equations, particularly in the regime where only a small amount of training data is available.</li>
<li>methods: The authors extend the framework of Raissi et al. (2017) to construct PDE-informed Gaussian priors, which are used to construct different approximate posteriors.</li>
<li>results: Numerical experiments demonstrate the superiority of the PDE-informed Gaussian priors over more traditional priors.<details>
<summary>Abstract</summary>
This work is concerned with the use of Gaussian surrogate models for Bayesian inverse problems associated with linear partial differential equations. A particular focus is on the regime where only a small amount of training data is available. In this regime the type of Gaussian prior used is of critical importance with respect to how well the surrogate model will perform in terms of Bayesian inversion. We extend the framework of Raissi et. al. (2017) to construct PDE-informed Gaussian priors that we then use to construct different approximate posteriors. A number of different numerical experiments illustrate the superiority of the PDE-informed Gaussian priors over more traditional priors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RAYEN-Imposition-of-Hard-Convex-Constraints-on-Neural-Networks"><a href="#RAYEN-Imposition-of-Hard-Convex-Constraints-on-Neural-Networks" class="headerlink" title="RAYEN: Imposition of Hard Convex Constraints on Neural Networks"></a>RAYEN: Imposition of Hard Convex Constraints on Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08336">http://arxiv.org/abs/2307.08336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leggedrobotics/rayen">https://github.com/leggedrobotics/rayen</a></li>
<li>paper_authors: Jesus Tordesillas, Jonathan P. How, Marco Hutter</li>
<li>for: 这个论文是用来实现神经网络中的硬 convex 约束的框架，保证任何输入或者神经网络的参数，约束都会被满足。</li>
<li>methods: 这个框架使用了一些新的技术，例如不需要计算量占用的正交投影步骤、不需要软约束（不能保证约束在测试时都会被满足）、不需要保守的近似约束集和不需要慢速的内部梯度下降来保证约束。</li>
<li>results: 使用这个框架，可以很快地（比如1Kquadratic约束在1000维变量上的 overhead低于8ms，300x300稠密矩阵LMI约束在10000维变量上的 overhead低于12ms）进行约束优化问题的解决，而且可以保证约束的满足，计算时间比状态艺术算法快，计算结果几乎与最优解一致。<details>
<summary>Abstract</summary>
This paper presents RAYEN, a framework to impose hard convex constraints on the output or latent variable of a neural network. RAYEN guarantees that, for any input or any weights of the network, the constraints are satisfied at all times. Compared to other approaches, RAYEN does not perform a computationally-expensive orthogonal projection step onto the feasible set, does not rely on soft constraints (which do not guarantee the satisfaction of the constraints at test time), does not use conservative approximations of the feasible set, and does not perform a potentially slow inner gradient descent correction to enforce the constraints. RAYEN supports any combination of linear, convex quadratic, second-order cone (SOC), and linear matrix inequality (LMI) constraints, achieving a very small computational overhead compared to unconstrained networks. For example, it is able to impose 1K quadratic constraints on a 1K-dimensional variable with an overhead of less than 8 ms, and an LMI constraint with 300x300 dense matrices on a 10K-dimensional variable in less than 12 ms. When used in neural networks that approximate the solution of constrained optimization problems, RAYEN achieves computation times between 20 and 7468 times faster than state-of-the-art algorithms, while guaranteeing the satisfaction of the constraints at all times and obtaining a cost very close to the optimal one.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Machine-Learning-based-Empirical-Evaluation-of-Cyber-Threat-Actors-High-Level-Attack-Patterns-over-Low-level-Attack-Patterns-in-Attributing-Attacks"><a href="#A-Machine-Learning-based-Empirical-Evaluation-of-Cyber-Threat-Actors-High-Level-Attack-Patterns-over-Low-level-Attack-Patterns-in-Attributing-Attacks" class="headerlink" title="A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks"></a>A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10252">http://arxiv.org/abs/2307.10252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Umara Noor, Sawera Shahid, Rimsha Kanwal, Zahid Rashid<br>for:这个论文旨在探讨了Cyber threat attribution的问题，即在网络空间中识别攻击者的过程。methods:这篇论文使用了手动分析攻击 patrerns，包括骗财机制、侵入检测系统、防火墙和trace-back过程，以及高级指标（IOC）和低级指标（IOC）的比较。results:实验结果显示，高级指标（IOC）训练模型可以有95%的准确率地归类攻击，而低级指标（IOC）训练模型的准确率只有40%。<details>
<summary>Abstract</summary>
Cyber threat attribution is the process of identifying the actor of an attack incident in cyberspace. An accurate and timely threat attribution plays an important role in deterring future attacks by applying appropriate and timely defense mechanisms. Manual analysis of attack patterns gathered by honeypot deployments, intrusion detection systems, firewalls, and via trace-back procedures is still the preferred method of security analysts for cyber threat attribution. Such attack patterns are low-level Indicators of Compromise (IOC). They represent Tactics, Techniques, Procedures (TTP), and software tools used by the adversaries in their campaigns. The adversaries rarely re-use them. They can also be manipulated, resulting in false and unfair attribution. To empirically evaluate and compare the effectiveness of both kinds of IOC, there are two problems that need to be addressed. The first problem is that in recent research works, the ineffectiveness of low-level IOC for cyber threat attribution has been discussed intuitively. An empirical evaluation for the measure of the effectiveness of low-level IOC based on a real-world dataset is missing. The second problem is that the available dataset for high-level IOC has a single instance for each predictive class label that cannot be used directly for training machine learning models. To address these problems in this research work, we empirically evaluate the effectiveness of low-level IOC based on a real-world dataset that is specifically built for comparative analysis with high-level IOC. The experimental results show that the high-level IOC trained models effectively attribute cyberattacks with an accuracy of 95% as compared to the low-level IOC trained models where accuracy is 40%.
</details>
<details>
<summary>摘要</summary>
“网络威胁识别是指在网络空间中识别攻击事件的具体执行者。正确和及时的威胁识别对于防止未来攻击提供了重要的防御机制。现今，安全分析师仍然采用手动分析攻击模式，包括骗子部署、入侵检测系统、防火墙等，以及跟踪返回过程，进行威胁识别。这些攻击模式被称为低级别征识（Indicators of Compromise，IOC）。它们表示敌对者在其攻击活动中使用的策略、技术、程序（Tactics, Techniques, Procedures，TTP）和软件工具。敌对者很少重复使用这些攻击模式，它们也可以被修改，导致假的和不公正的威胁识别。为了empirically评估和比较低级别征识和高级别征识的效果，这些问题需要被解决。第一个问题是，在当前的研究中，低级别征识的效果不够有效性已经被直观提出。empirical评估基于实际数据集的低级别征识效果缺失。第二个问题是，可用的高级别征识数据集中每个预测类别的单个实例不能直接用于机器学习模型训练。为解决这些问题，我们在这项研究中Empirically评估了低级别征识的效果，并使用特定 для比较分析的实际数据集。实验结果显示，高级别征识训练模型可以准确地归类攻击事件，准确率达95%，而低级别征识训练模型的准确率只有40%。”
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Impact-of-Adversarial-Examples-on-Explainable-Machine-Learning"><a href="#Analyzing-the-Impact-of-Adversarial-Examples-on-Explainable-Machine-Learning" class="headerlink" title="Analyzing the Impact of Adversarial Examples on Explainable Machine Learning"></a>Analyzing the Impact of Adversarial Examples on Explainable Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08327">http://arxiv.org/abs/2307.08327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prathyusha Devabhakthini, Sasmita Parida, Raj Mani Shukla, Suvendu Chandan Nayak</li>
<li>for: 这个论文探讨了深度学习模型对针对性攻击的抵触性。</li>
<li>methods: 作者采用了一种ML基于的文本分类模型，然后引入了针对性攻击来影响模型的分类性能。</li>
<li>results: 研究发现，针对性攻击可以轻松地使模型错误地预测文本。此外，作者还分析了模型的解释性前后针对性攻击，以了解模型在攻击后的性能。<details>
<summary>Abstract</summary>
Adversarial attacks are a type of attack on machine learning models where an attacker deliberately modifies the inputs to cause the model to make incorrect predictions. Adversarial attacks can have serious consequences, particularly in applications such as autonomous vehicles, medical diagnosis, and security systems. Work on the vulnerability of deep learning models to adversarial attacks has shown that it is very easy to make samples that make a model predict things that it doesn't want to. In this work, we analyze the impact of model interpretability due to adversarial attacks on text classification problems. We develop an ML-based classification model for text data. Then, we introduce the adversarial perturbations on the text data to understand the classification performance after the attack. Subsequently, we analyze and interpret the model's explainability before and after the attack
</details>
<details>
<summary>摘要</summary>
adversarial 攻击是一种针对机器学习模型的攻击，攻击者故意修改输入，以让模型作出错误预测。 adversarial 攻击可能会有严重的后果，特别是在自动驾驶、医疗诊断和安全系统等应用中。我们在深度学习模型对 adversarial 攻击的抵触性上进行了研究。我们开发了一个基于 ML 的文本数据分类模型，然后引入了对文本数据的 adversarial 偏移，以了解攻击后分类性能。接着，我们分析和解释模型在攻击后的解释性。
</details></li>
</ul>
<hr>
<h2 id="A-Secure-Aggregation-for-Federated-Learning-on-Long-Tailed-Data"><a href="#A-Secure-Aggregation-for-Federated-Learning-on-Long-Tailed-Data" class="headerlink" title="A Secure Aggregation for Federated Learning on Long-Tailed Data"></a>A Secure Aggregation for Federated Learning on Long-Tailed Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08324">http://arxiv.org/abs/2307.08324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanna Jiang, Baihe Ma, Xu Wang, Guangsheng Yu, Caijun Sun, Wei Ni, Ren Ping Liu</li>
<li>for: 本研究针对 Federated Learning (FL) 面临的两大挑战：数据分布不均和模型攻击。</li>
<li>methods: 提出了一种新的两层聚合方法，可以拒绝恶意模型和选择值得模型，并具有较好的抗辐射性。</li>
<li>results: 实验表明，思想团（think tank）可以有效地选择模型进行全局聚合。<details>
<summary>Abstract</summary>
As a distributed learning, Federated Learning (FL) faces two challenges: the unbalanced distribution of training data among participants, and the model attack by Byzantine nodes. In this paper, we consider the long-tailed distribution with the presence of Byzantine nodes in the FL scenario. A novel two-layer aggregation method is proposed for the rejection of malicious models and the advisable selection of valuable models containing tail class data information. We introduce the concept of think tank to leverage the wisdom of all participants. Preliminary experiments validate that the think tank can make effective model selections for global aggregation.
</details>
<details>
<summary>摘要</summary>
作为分布式学习的一种形式， federated learning (FL) 面临两大挑战：训练数据在参与者中的不均匀分布，以及由拜占庭节点引起的模型攻击。在这篇论文中，我们考虑了在 FL 场景中存在长板分布和拜占庭节点的情况下的长板分布。我们提出了一种新的两层聚合方法，用于拒绝恶意模型和选择值得采用的模型，并具有拥有尾类数据信息。我们引入了“思想库”的概念，以利用所有参与者的智慧。初步实验表明，思想库可以做到有效地选择模型进行全球聚合。
</details></li>
</ul>
<hr>
<h2 id="Airway-Label-Prediction-in-Video-Bronchoscopy-Capturing-Temporal-Dependencies-Utilizing-Anatomical-Knowledge"><a href="#Airway-Label-Prediction-in-Video-Bronchoscopy-Capturing-Temporal-Dependencies-Utilizing-Anatomical-Knowledge" class="headerlink" title="Airway Label Prediction in Video Bronchoscopy: Capturing Temporal Dependencies Utilizing Anatomical Knowledge"></a>Airway Label Prediction in Video Bronchoscopy: Capturing Temporal Dependencies Utilizing Anatomical Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08318">http://arxiv.org/abs/2307.08318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ron Keuth, Mattias Heinrich, Martin Eichenlaub, Marian Himstedt</li>
<li>For: This paper provides a novel approach for navigation guidance during bronchoscopy interventions without the need for electromagnetic tracking or patient-specific CT scans.* Methods: The proposed approach uses topological bronchoscope localization and incorporates sequences of CNN-based airway likelihoods into a Hidden Markov Model, leveraging anatomical constraints and temporal context for improved accuracy.* Results: The approach is evaluated in a lung phantom model and achieves an accuracy of up to 0.98 compared to 0.81 for a classification based on individual frames, demonstrating the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
Purpose: Navigation guidance is a key requirement for a multitude of lung interventions using video bronchoscopy. State-of-the-art solutions focus on lung biopsies using electromagnetic tracking and intraoperative image registration w.r.t. preoperative CT scans for guidance. The requirement of patient-specific CT scans hampers the utilisation of navigation guidance for other applications such as intensive care units.   Methods: This paper addresses navigation guidance solely incorporating bronchosopy video data. In contrast to state-of-the-art approaches we entirely omit the use of electromagnetic tracking and patient-specific CT scans. Guidance is enabled by means of topological bronchoscope localization w.r.t. an interpatient airway model. Particularly, we take maximally advantage of anatomical constraints of airway trees being sequentially traversed. This is realized by incorporating sequences of CNN-based airway likelihoods into a Hidden Markov Model.   Results: Our approach is evaluated based on multiple experiments inside a lung phantom model. With the consideration of temporal context and use of anatomical knowledge for regularization, we are able to improve the accuracy up to to 0.98 compared to 0.81 (weighted F1: 0.98 compared to 0.81) for a classification based on individual frames.   Conclusion: We combine CNN-based single image classification of airway segments with anatomical constraints and temporal HMM-based inference for the first time. Our approach renders vision-only guidance for bronchoscopy interventions in the absence of electromagnetic tracking and patient-specific CT scans possible.
</details>
<details>
<summary>摘要</summary>
目的：用视频镜头导航是肺间化学疗法中不可或缺的一种重要需求。现代解决方案主要关注于基于电磁场追踪和在手术过程中对先前的CT扫描图进行图像匹配的医学器械导航。但是，需要患者特定的CT扫描图的使用限制了导航导航的使用范围只能用于血液急救室等其他应用。方法：本文提出一种具有视频镜头导航功能的新方法，与现有方法不同之处在于完全没有使用电磁场追踪和患者特定的CT扫描图。导航是基于气管内部空间模型和视频镜头数据进行的，具有较高的准确率和可靠性。结果：我们在肺脏模型中进行了多次实验，结果表明，通过利用空间和时间上的约束和图像分类的权重补做，我们可以提高准确率至0.98（weighted F1 score: 0.98），比对ividual帧的分类结果（0.81）高出了17.6%。结论：我们在空间和时间上具有约束的HMM模型中结合了CNN基于单帧图像分类和空间约束，实现了没有电磁场追踪和患者特定CT扫描图的视频镜头导航。这种方法可以在肺间化学疗法中提供更加可靠和高效的导航导航。
</details></li>
</ul>
<hr>
<h2 id="Soft-Prompt-Tuning-for-Augmenting-Dense-Retrieval-with-Large-Language-Models"><a href="#Soft-Prompt-Tuning-for-Augmenting-Dense-Retrieval-with-Large-Language-Models" class="headerlink" title="Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models"></a>Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08303">http://arxiv.org/abs/2307.08303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhiyuanpeng/sptar">https://github.com/zhiyuanpeng/sptar</a></li>
<li>paper_authors: Zhiyuan Peng, Xuyang Wu, Yi Fang</li>
<li>for: 提高 dense retrieval 模型的性能，尤其是在lacking domain-specific training data的情况下。</li>
<li>methods: 使用 soft prompt tuning 方法，通过优化任务特定的软提示来提高 LLMs 生成的弱查询语句质量，然后使用这些弱查询语句来训练任务特定的 dense retriever。</li>
<li>results: SPTAR 方法在不supervised baselines BM25 和 LLMs-based augmentation method 的基础上具有更高的性能，可以提高 dense retrieval 模型的搜索效果。<details>
<summary>Abstract</summary>
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.
</details>
<details>
<summary>摘要</summary>
dense retrieval (DR) 将查询和文档转换为紧凑表示并在 vector 空间中度量查询和文档之间的相似性。DR 的一个挑战是缺乏域pecific 训练数据。虽然 DR 模型可以通过转移学习从大规模公共数据集如 MS MARCO 学习，但证据表明不 все DR 模型和领域可以受益于转移学习相同。 reciently，一些研究人员已经使用大语言模型 (LLMs) 来提高零ocket 和几ocket DR 模型。然而，使用的 hard prompts 或人工写的 prompts 无法保证生成的弱 queries 的好质量。为了解决这个问题，我们提出了软提示调整 для增强 DR (SPTAR)：对每个任务，我们利用软提示调整来优化任务特定的软提示，然后使用 LLMs 将标注无标注文档，生成足够的弱文档-查询对以训练任务特定的紧凑检索器。我们设计了一个筛选器来选择高质量的示例文档-查询对，以进一步提高弱标注查询的质量。到目前为止，没有什么先进的工作利用软提示调整来增强 DR 模型。实验表明，SPTAR 超过了无监督基准和最近提出的基于 LLMs 的增强方法。
</details></li>
</ul>
<hr>
<h2 id="GBT-Two-stage-transformer-framework-for-non-stationary-time-series-forecasting"><a href="#GBT-Two-stage-transformer-framework-for-non-stationary-time-series-forecasting" class="headerlink" title="GBT: Two-stage transformer framework for non-stationary time series forecasting"></a>GBT: Two-stage transformer framework for non-stationary time series forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08302">http://arxiv.org/abs/2307.08302</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/origamisl/gbt">https://github.com/origamisl/gbt</a></li>
<li>paper_authors: Li Shen, Yuning Wei, Yangzhu Wang</li>
<li>for: 本研究旨在解决时间序列预测变换器（TSFT）的严重过拟合问题，尤其是在处理非站ARY时间序列时。</li>
<li>methods: 我们提出了一种新的两阶段变换器框架，称为Good Beginning Transformer（GBT），它将TSFT的预测过程分解成两个阶段：自动回归阶段和自我回归阶段。在自动回归阶段，预测结果作为一个更好的初始化方法，并在自我回归阶段进行进一步的预测。</li>
<li>results: 我们在七个基准数据集上进行了广泛的实验，结果显示GBT在预测能力方面超过了现有的TSFT和其他预测模型（SCINet、N-HiTS等），并且具有较低的时间和空间复杂度。GBT还可以与这些模型结合使用，以增强其预测能力。<details>
<summary>Abstract</summary>
This paper shows that time series forecasting Transformer (TSFT) suffers from severe over-fitting problem caused by improper initialization method of unknown decoder inputs, esp. when handling non-stationary time series. Based on this observation, we propose GBT, a novel two-stage Transformer framework with Good Beginning. It decouples the prediction process of TSFT into two stages, including Auto-Regression stage and Self-Regression stage to tackle the problem of different statistical properties between input and prediction sequences.Prediction results of Auto-Regression stage serve as a Good Beginning, i.e., a better initialization for inputs of Self-Regression stage. We also propose Error Score Modification module to further enhance the forecasting capability of the Self-Regression stage in GBT. Extensive experiments on seven benchmark datasets demonstrate that GBT outperforms SOTA TSFTs (FEDformer, Pyraformer, ETSformer, etc.) and many other forecasting models (SCINet, N-HiTS, etc.) with only canonical attention and convolution while owning less time and space complexity. It is also general enough to couple with these models to strengthen their forecasting capability. The source code is available at: https://github.com/OrigamiSL/GBT
</details>
<details>
<summary>摘要</summary>
To further enhance the forecasting capability of GBT, we propose an Error Score Modification module. This module adjusts the error scores of the Self-Regression stage to better handle the difference in statistical properties between the input and prediction sequences.Our extensive experiments on seven benchmark datasets show that GBT outperforms state-of-the-art TSFTs (FEDformer, Pyraformer, ETSformer, etc.) and other forecasting models (SCINet, N-HiTS, etc.) with only canonical attention and convolution, while requiring less time and space complexity. Additionally, GBT is general enough to be combined with these models to strengthen their forecasting capability. The source code is available at: <https://github.com/OrigamiSL/GBT>.
</details></li>
</ul>
<hr>
<h2 id="Systematic-Testing-of-the-Data-Poisoning-Robustness-of-KNN"><a href="#Systematic-Testing-of-the-Data-Poisoning-Robustness-of-KNN" class="headerlink" title="Systematic Testing of the Data-Poisoning Robustness of KNN"></a>Systematic Testing of the Data-Poisoning Robustness of KNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08288">http://arxiv.org/abs/2307.08288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannan Li, Jingbo Wang, Chao Wang</li>
<li>for: 这篇论文目的是提高机器学习基于训练集的软件组件的数据欺走抵触性。</li>
<li>methods: 该论文提出了一种系统测试基于方法，可以证明和证伪数据欺走 robustness。</li>
<li>results: 该方法比基eline枚举方法快速和准确，可以快速缩小搜索空间，并通过系统测试在具体空间找到实际的违反。测试结果表明，该方法可以有效地判断k- nearest neighbors（KNN）预测结果的数据欺走Robustness。<details>
<summary>Abstract</summary>
Data poisoning aims to compromise a machine learning based software component by contaminating its training set to change its prediction results for test inputs. Existing methods for deciding data-poisoning robustness have either poor accuracy or long running time and, more importantly, they can only certify some of the truly-robust cases, but remain inconclusive when certification fails. In other words, they cannot falsify the truly-non-robust cases. To overcome this limitation, we propose a systematic testing based method, which can falsify as well as certify data-poisoning robustness for a widely used supervised-learning technique named k-nearest neighbors (KNN). Our method is faster and more accurate than the baseline enumeration method, due to a novel over-approximate analysis in the abstract domain, to quickly narrow down the search space, and systematic testing in the concrete domain, to find the actual violations. We have evaluated our method on a set of supervised-learning datasets. Our results show that the method significantly outperforms state-of-the-art techniques, and can decide data-poisoning robustness of KNN prediction results for most of the test inputs.
</details>
<details>
<summary>摘要</summary>
“数据毒化”是一种攻击机器学习基础的软件元件，通过污染它的训练集，让它的预测结果对测试输入进行变化。现有的方法可以评估数据毒化Robustness，但是它们的精度受限，或者执行时间很长，而且它们只能认证一些真正可靠的情况，但是无法确定不可靠的情况。为了解决这个限制，我们提出了一个系统性的测试方法，可以确定以及否定数据毒化Robustness，这个方法在一个广泛使用的超过近边法（KNN）上进行了评估。我们的方法比基准枚举方法更快和更精度，是因为我们使用了一种新的抽象领域中的误差分析，快速地缩小搜索空间，并且在实际领域中进行系统性的测试，实际找到了违背的情况。我们在一些超过近边法的数据上进行了评估，结果显示，我们的方法可以在大多数的测试输入上实现数据毒化Robustness的决定。”
</details></li>
</ul>
<hr>
<h2 id="Going-Beyond-Linear-Mode-Connectivity-The-Layerwise-Linear-Feature-Connectivity"><a href="#Going-Beyond-Linear-Mode-Connectivity-The-Layerwise-Linear-Feature-Connectivity" class="headerlink" title="Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity"></a>Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08286">http://arxiv.org/abs/2307.08286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanpeng Zhou, Yongyi Yang, Xiaojiang Yang, Junchi Yan, Wei Hu</li>
<li>for: 本研究探讨了神经网络训练过程中的一些有趣实验现象，包括Linear Mode Connectivity（LMC）等。</li>
<li>methods: 本研究使用了多种方法来探讨LMC和Layerwise Linear Feature Connectivity（LLFC）的现象，包括随机排序和生成新的网络等。</li>
<li>results: 研究发现，当两个训练过的网络满足LMC时，它们通常也满足LLFC在大多数层次。此外，研究还探讨了LLFC的下面因素，提供了新的思路和技术来理解LMC和LLFC。<details>
<summary>Abstract</summary>
Recent work has revealed many intriguing empirical phenomena in neural network training, despite the poorly understood and highly complex loss landscapes and training dynamics. One of these phenomena, Linear Mode Connectivity (LMC), has gained considerable attention due to the intriguing observation that different solutions can be connected by a linear path in the parameter space while maintaining near-constant training and test losses. In this work, we introduce a stronger notion of linear connectivity, Layerwise Linear Feature Connectivity (LLFC), which says that the feature maps of every layer in different trained networks are also linearly connected. We provide comprehensive empirical evidence for LLFC across a wide range of settings, demonstrating that whenever two trained networks satisfy LMC (via either spawning or permutation methods), they also satisfy LLFC in nearly all the layers. Furthermore, we delve deeper into the underlying factors contributing to LLFC, which reveal new insights into the spawning and permutation approaches. The study of LLFC transcends and advances our understanding of LMC by adopting a feature-learning perspective.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Complexity-Matters-Rethinking-the-Latent-Space-for-Generative-Modeling"><a href="#Complexity-Matters-Rethinking-the-Latent-Space-for-Generative-Modeling" class="headerlink" title="Complexity Matters: Rethinking the Latent Space for Generative Modeling"></a>Complexity Matters: Rethinking the Latent Space for Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08283">http://arxiv.org/abs/2307.08283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyang Hu, Fei Chen, Haonan Wang, Jiawei Li, Wenjia Wang, Jiacheng Sun, Zhenguo Li</li>
<li>for: 本研究旨在探讨generative模型中latent space的选择，尤其是如何选择最佳的latent space，以提高generative性能。</li>
<li>methods: 我们提出了一种基于模型复杂度的latent space选择方法，并提出了一种两阶段训练策略called Decoupled Autoencoder (DAE)，可以改善latent distribution并提高生成性能。</li>
<li>results: 我们的理论分析和实验结果表明，DAE可以提高sample质量，同时降低模型的复杂度。<details>
<summary>Abstract</summary>
In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel "distance" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity.
</details>
<details>
<summary>摘要</summary>
在生成模型中，许多成功的方法利用低维度的隐藏空间，例如稳定扩散模型，通过一个匹配的解码器生成图像。although the selection of the latent space is crucial, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel "distance" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity.Here's the translation in Traditional Chinese:在生成模型中，许多成功的方法利用低维度的隐藏空间，例如稳定扩散模型，通过一个匹配的解码器生成图像。although the selection of the latent space is crucial, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel "distance" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity.
</details></li>
</ul>
<hr>
<h2 id="Certifying-the-Fairness-of-KNN-in-the-Presence-of-Dataset-Bias"><a href="#Certifying-the-Fairness-of-KNN-in-the-Presence-of-Dataset-Bias" class="headerlink" title="Certifying the Fairness of KNN in the Presence of Dataset Bias"></a>Certifying the Fairness of KNN in the Presence of Dataset Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08722">http://arxiv.org/abs/2307.08722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannan Li, Jingbo Wang, Chao Wang</li>
<li>for: The paper is written for certifying the fairness of the classification result of the k-nearest neighbors (KNN) algorithm under the assumption of historical bias in the training data.</li>
<li>methods: The paper proposes a method for certifying fairness based on three variants of fairness definitions: individual fairness, $\epsilon$-fairness, and label-flipping fairness. The method uses sound approximations of the complex arithmetic computations used in the state-of-the-art KNN algorithm to reduce computational cost.</li>
<li>results: The paper shows the effectiveness of the proposed method through experimental evaluation on six widely used datasets in the fairness research literature. The method is able to obtain fairness certifications for a large number of test inputs despite the presence of historical bias in the datasets.<details>
<summary>Abstract</summary>
We propose a method for certifying the fairness of the classification result of a widely used supervised learning algorithm, the k-nearest neighbors (KNN), under the assumption that the training data may have historical bias caused by systematic mislabeling of samples from a protected minority group. To the best of our knowledge, this is the first certification method for KNN based on three variants of the fairness definition: individual fairness, $\epsilon$-fairness, and label-flipping fairness. We first define the fairness certification problem for KNN and then propose sound approximations of the complex arithmetic computations used in the state-of-the-art KNN algorithm. This is meant to lift the computation results from the concrete domain to an abstract domain, to reduce the computational cost. We show effectiveness of this abstract interpretation based technique through experimental evaluation on six datasets widely used in the fairness research literature. We also show that the method is accurate enough to obtain fairness certifications for a large number of test inputs, despite the presence of historical bias in the datasets.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于证明某种广泛使用的直接学习算法（k-最近邻）的分类结果是否公平，假设训练数据可能受到历史偏见的影响，特别是保护少数群体样本的系统化错误标注。根据我们所知，这是第一种基于三种公平定义（个体公平、ε-公平和标签抓取公平）的公平证明方法。我们首先定义了公平证明问题，然后提出了使用现代KNN算法中的复杂数学计算的准确估计方法，以减少计算成本。我们通过实验评估六个广泛用于公平研究文献中的数据集，并证明了这种抽象计算方法的有效性。我们还证明了方法可以快速获得大量测试输入的公平证明，即使训练数据中存在历史偏见。
</details></li>
</ul>
<hr>
<h2 id="Automated-Action-Model-Acquisition-from-Narrative-Texts"><a href="#Automated-Action-Model-Acquisition-from-Narrative-Texts" class="headerlink" title="Automated Action Model Acquisition from Narrative Texts"></a>Automated Action Model Acquisition from Narrative Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10247">http://arxiv.org/abs/2307.10247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqi Li, Leyang Cui, Songtuan Lin, Patrik Haslum</li>
<li>for: 本研究旨在提高AI代理人的规划技术应用，通过自动从叙述文本中提取струк成事件和生成 планинг语言风格的动作模型。</li>
<li>methods: 本研究使用了自动提取叙述文本中的结构事件，并通过预测通用常识事件关系、文本矛盾和相似性来生成 planning-language-style 动作模型。</li>
<li>results: 实验结果表明，NaRuto可以在经典叙述规划领域生成高质量的动作模型，与现有的完全自动方法相当，甚至与半自动方法相当。<details>
<summary>Abstract</summary>
Action models, which take the form of precondition/effect axioms, facilitate causal and motivational connections between actions for AI agents. Action model acquisition has been identified as a bottleneck in the application of planning technology, especially within narrative planning. Acquiring action models from narrative texts in an automated way is essential, but challenging because of the inherent complexities of such texts. We present NaRuto, a system that extracts structured events from narrative text and subsequently generates planning-language-style action models based on predictions of commonsense event relations, as well as textual contradictions and similarities, in an unsupervised manner. Experimental results in classical narrative planning domains show that NaRuto can generate action models of significantly better quality than existing fully automated methods, and even on par with those of semi-automated methods.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文。<</SYS>>行动模型，即前提/效果axioms，为AI代理人提供了 causal 和 motivational 连接。行动模型获取被识别为规划技术应用的瓶颈，尤其在叙述规划领域。自动从叙述文本中获取行动模型是重要，但具有内在复杂性。我们提出了NaRuto系统，该系统通过预测常识事件关系以及文本矛盾和相似性来自动生成 планинг语言风格的行动模型，并在经验领域中达到了现有完全自动方法的水平。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-on-Traffic-Sign-Recognition-A-Survey"><a href="#Adversarial-Attacks-on-Traffic-Sign-Recognition-A-Survey" class="headerlink" title="Adversarial Attacks on Traffic Sign Recognition: A Survey"></a>Adversarial Attacks on Traffic Sign Recognition: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08278">http://arxiv.org/abs/2307.08278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Svetlana Pavlitska, Nico Lambing, J. Marius Zöllner</li>
<li>for: 本研究旨在探讨攻击 autonomous driving 系统的可能性，尤其是针对交通标识模型的攻击。</li>
<li>methods: 本研究准确地描述了现有的攻击方法，包括数字和实际攻击。</li>
<li>results: 研究发现，现有的攻击方法可以轻松地破坏交通标识模型的正常工作，需要进一步的研究以减少这些攻击的风险。<details>
<summary>Abstract</summary>
Traffic sign recognition is an essential component of perception in autonomous vehicles, which is currently performed almost exclusively with deep neural networks (DNNs). However, DNNs are known to be vulnerable to adversarial attacks. Several previous works have demonstrated the feasibility of adversarial attacks on traffic sign recognition models. Traffic signs are particularly promising for adversarial attack research due to the ease of performing real-world attacks using printed signs or stickers. In this work, we survey existing works performing either digital or real-world attacks on traffic sign detection and classification models. We provide an overview of the latest advancements and highlight the existing research areas that require further investigation.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆的辨识功能中，交通标志识别是一个关键组件，目前大多使用深度神经网络（DNN）来实现。但是，DNN受到恶意攻击的可能性很高。先前的研究已经证明了对交通标志识别模型的攻击的可能性。由于交通标志的易攻击性，使得实际攻击更加容易。在这种情况下，我们对现有的数字和实际攻击研究进行了抽象和概述，并高亮了需要进一步研究的领域。
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Networks-and-Brain-Alignment-Brain-Encoding-and-Decoding-Survey"><a href="#Deep-Neural-Networks-and-Brain-Alignment-Brain-Encoding-and-Decoding-Survey" class="headerlink" title="Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)"></a>Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10246">http://arxiv.org/abs/2307.10246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard, Frederic Alexandre, Xavier Hinaut</li>
<li>for: 研究大脑如何表示不同的信息模式，以及设计一个系统可以自动理解用户的思维？</li>
<li>methods: 使用 функциональ磁共振成像（fMRI）记录大脑活动，并提出了多种基于深度学习的编码和解码模型。</li>
<li>results: 这些模型可以用于评估和诊断神经科学问题，以及设计大脑机器或计算机界面。<details>
<summary>Abstract</summary>
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for designing brain-machine or brain-computer interfaces. Inspired by the effectiveness of deep learning models for natural language processing, computer vision, and speech, recently several neural encoding and decoding models have been proposed. In this survey, we will first discuss popular representations of language, vision and speech stimuli, and present a summary of neuroscience datasets. Further, we will review popular deep learning based encoding and decoding architectures and note their benefits and limitations. Finally, we will conclude with a brief summary and discussion about future trends. Given the large amount of recently published work in the `computational cognitive neuroscience' community, we believe that this survey nicely organizes the plethora of work and presents it as a coherent story.
</details>
<details>
<summary>摘要</summary>
如何让脑子表示不同的信息？我们可以通过研究脑电图像（fMRI）来回答这些问题。脑科学社区已经提供了许多大量的认知神经科学数据集，这些数据集关于静止阅读/听取/观看概念词、故事、图片和电影。使用这些数据集，以前已经提出了编码和解码模型。这些模型可以用于基础研究认知科学和神经科学。编码模型可以自动生成脑电图像，它们有许多实际应用，如诊断和治疗神经系统疾病。解码模型可以 reconstruction 脑电图像，它们有用于设计脑机或脑计算机界面。鼓励于深度学习模型在自然语言处理、计算机视觉和语音处理等领域的效果，最近几年有很多 neural encoding 和 decoding 模型被提出。在这篇评论中，我们将首先讲讲语言、视觉和听说 stimuli 的受欢迎表示，并提供脑科学数据集的摘要。然后，我们将回顾深度学习基于编码和解码架构的一些模型，并注意它们的优点和局限性。最后，我们将结束于简要的总结和讨论，并讨论未来的趋势。由于最近出版的大量工作在 'computational cognitive neuroscience' 社区，我们认为这篇评论 nicely 组织了这些工作，并将它们表现为一个coherent 的故事。
</details></li>
</ul>
<hr>
<h2 id="Transferable-Graph-Neural-Fingerprint-Models-for-Quick-Response-to-Future-Bio-Threats"><a href="#Transferable-Graph-Neural-Fingerprint-Models-for-Quick-Response-to-Future-Bio-Threats" class="headerlink" title="Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats"></a>Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01921">http://arxiv.org/abs/2308.01921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Chen, Yihui Ren, Ai Kagawa, Matthew R. Carbone, Samuel Yen-Chi Chen, Xiaohui Qu, Shinjae Yoo, Austin Clyde, Arvind Ramanathan, Rick L. Stevens, Hubertus J. J. van Dam, Deyu Liu</li>
<li>for: 这个论文的目的是为了快速屏测药物分子，以便在药物发现过程中快速搜索出有效的药物候选者。</li>
<li>methods: 这个论文使用的方法是基于蛋白质绑定亲和力的图 neural fingerprint方法，这种方法可以在高速和高准确性之间进行药物 docking 模拟。</li>
<li>results: 这个论文的结果表明，使用图 neural fingerprint方法可以对 COVID-19 药物 docking 问题进行高效的虚拟屏测，并且其预测精度比传统的圆形指纹方法更高， сред平方误差小于 $0.21$ kcal&#x2F;mol。此外， authors 还提出了一种可以适用于未知目标的转移性图 neural fingerprint方法，该方法可以在多个目标上进行训练，并且与特定目标的图 neural fingerprint模型具有相似的准确性。<details>
<summary>Abstract</summary>
Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transferable model exhibits superb training and data efficiency. We highlight that the impact of this study extends beyond COVID-19 dataset, as our approach for fast virtual ligand screening can be easily adapted and integrated into a general machine learning-accelerated pipeline to battle future bio-threats.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:快速药物探测基于药物结合亲和力是药物发现管道中一个重要步骤。图像神经指纹是一种有前途的方法，可以快速、高精度地实现药物对抗体结合。在这项研究中，我们建立了一个COVID-19药物探测集合，包含约300,000个药物候选者，对23种新型冠状病毒蛋白目标进行了探测。使用这些数据集，我们训练了图像神经指纹对高通量虚拟COVID-19药物探测进行了训练。图像神经指纹模型在多数探测目标上显示了高精度预测吸附分数，与传统径向指纹方法相比，显示了明显的改善。为使 neural fingerprint 可以应用于未知目标，我们还提出了多目标图像神经指纹方法。与特定目标图像神经指纹模型相比，多目标模型在训练和数据效率方面表现出色。我们强调，这项研究的影响不仅限于COVID-19数据集，我们的方法可以轻松地适应和整合到一个通用的机器学习加速管道中，以应对未来的生物威胁。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-and-Enhancing-Robustness-of-Deep-Recommendation-Systems-Against-Hardware-Errors"><a href="#Evaluating-and-Enhancing-Robustness-of-Deep-Recommendation-Systems-Against-Hardware-Errors" class="headerlink" title="Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors"></a>Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10244">http://arxiv.org/abs/2307.10244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vu-detail/pytei">https://github.com/vu-detail/pytei</a></li>
<li>paper_authors: Dongning Ma, Xun Jiao, Fred Lin, Mengshi Zhang, Alban Desmaison, Thomas Sellinger, Daniel Moore, Sriram Sankar</li>
<li>for: 这 paper 是关于深度推荐系统（DRS）的可靠性研究，以寻找在大规模队列系统中发现的硬件错误对 DRS 的影响。</li>
<li>methods: 这 paper 使用了 PyTorch 构建了一个简单、高效、可扩展的错误插入框架（Terrorch），以测试 DRS 的可靠性。</li>
<li>results: 研究发现，DRS 对硬件错误的抵抗力受到多种因素的影响，包括模型参数和输入特征。研究还发现，使用活动clipping可以提高 AUC-ROC 分数，达到30%的恢复率。<details>
<summary>Abstract</summary>
Deep recommendation systems (DRS) heavily depend on specialized HPC hardware and accelerators to optimize energy, efficiency, and recommendation quality. Despite the growing number of hardware errors observed in large-scale fleet systems where DRS are deployed, the robustness of DRS has been largely overlooked. This paper presents the first systematic study of DRS robustness against hardware errors. We develop Terrorch, a user-friendly, efficient and flexible error injection framework on top of the widely-used PyTorch. We evaluate a wide range of models and datasets and observe that the DRS robustness against hardware errors is influenced by various factors from model parameters to input characteristics. We also explore 3 error mitigation methods including algorithm based fault tolerance (ABFT), activation clipping and selective bit protection (SBP). We find that applying activation clipping can recover up to 30% of the degraded AUC-ROC score, making it a promising mitigation method.
</details>
<details>
<summary>摘要</summary>
深度推荐系统（DRS）强依赖特殊的高性能计算硬件和加速器来优化能效和推荐质量。尽管大规模队列系统中DRS的可靠性受到许多硬件错误的影响，但DRS的可靠性问题还尚未得到足够的关注。本文提出了DRS可靠性对硬件错误的首次系统性研究。我们开发了一个简单、高效和灵活的错误插入框架——Terrorch，并在PyTorch上实现。我们对各种模型和数据集进行了广泛的测试，发现DRS对硬件错误的可靠性受到多种因素的影响，从模型参数到输入特征。我们还探讨了3种错误缓解方法，包括算法基于缺陷tolerance（ABFT）、活动截断和选择性位保护（SBP）。我们发现通过实施活动截断可以恢复30%的降低的AUC-ROC分数，这表明这是一种有前途的缓解方法。
</details></li>
</ul>
<hr>
<h2 id="Convex-Bi-Level-Optimization-Problems-with-Non-smooth-Outer-Objective-Function"><a href="#Convex-Bi-Level-Optimization-Problems-with-Non-smooth-Outer-Objective-Function" class="headerlink" title="Convex Bi-Level Optimization Problems with Non-smooth Outer Objective Function"></a>Convex Bi-Level Optimization Problems with Non-smooth Outer Objective Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08245">http://arxiv.org/abs/2307.08245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roey Merchav, Shoham Sabach</li>
<li>for: 解决 convex bi-level 优化问题</li>
<li>methods: 提出 Bi-Sub-Gradient (Bi-SG) 方法，基于 classical sub-gradient 方法的一种泛化</li>
<li>results:  Bi-SG 方法可以在 convex bi-level 优化问题中实现 sub-线性速率，并且如果外部目标函数具有强度 convexity，可以提高外部速率至线性速率。此外，我们证明 Bi-SG 方法生成的序列与 bi-level 优化问题的优化解的距离 converges to zero.<details>
<summary>Abstract</summary>
In this paper, we propose the Bi-Sub-Gradient (Bi-SG) method, which is a generalization of the classical sub-gradient method to the setting of convex bi-level optimization problems. This is a first-order method that is very easy to implement in the sense that it requires only a computation of the associated proximal mapping or a sub-gradient of the outer non-smooth objective function, in addition to a proximal gradient step on the inner optimization problem. We show, under very mild assumptions, that Bi-SG tackles bi-level optimization problems and achieves sub-linear rates both in terms of the inner and outer objective functions. Moreover, if the outer objective function is additionally strongly convex (still could be non-smooth), the outer rate can be improved to a linear rate. Last, we prove that the distance of the generated sequence to the set of optimal solutions of the bi-level problem converges to zero.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了Bi-Sub-Gradient（Bi-SG）方法，这是对凸二级优化问题的一种普适化。这是一种一阶方法，只需计算相关的贸易映射或外层非凸目标函数的子gradient，以及内部优化问题的质量步骤。我们证明，在非常轻松的假设下，Bi-SG可以解决二级优化问题，并在内部和外部目标函数上实现下行速率。此外，如果外层目标函数另外是强Converter (仍然可能是非凸)，我们可以提高外层速率到线性速率。最后，我们证明生成的序列与二级优化问题的最佳解集的距离 converge to zero。
</details></li>
</ul>
<hr>
<h2 id="A-Look-into-Causal-Effects-under-Entangled-Treatment-in-Graphs-Investigating-the-Impact-of-Contact-on-MRSA-Infection"><a href="#A-Look-into-Causal-Effects-under-Entangled-Treatment-in-Graphs-Investigating-the-Impact-of-Contact-on-MRSA-Infection" class="headerlink" title="A Look into Causal Effects under Entangled Treatment in Graphs: Investigating the Impact of Contact on MRSA Infection"></a>A Look into Causal Effects under Entangled Treatment in Graphs: Investigating the Impact of Contact on MRSA Infection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08237">http://arxiv.org/abs/2307.08237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Ma, Chen Chen, Anil Vullikanti, Ritwick Mishra, Gregory Madden, Daniel Borrajo, Jundong Li</li>
<li>for: The paper is written to study the problem of causal effect estimation with treatment entangled in a graph, and to propose a novel method (NEAT) to tackle this challenge.</li>
<li>methods: The proposed method NEAT explicitly leverages the graph structure to model the treatment assignment mechanism, and mitigates confounding biases based on the treatment assignment modeling.</li>
<li>results: The proposed method is validated through experiments on both synthetic datasets and a real-world MRSA dataset, and provides effective results in estimating causal effects with entangled treatments.<details>
<summary>Abstract</summary>
Methicillin-resistant Staphylococcus aureus (MRSA) is a type of bacteria resistant to certain antibiotics, making it difficult to prevent MRSA infections. Among decades of efforts to conquer infectious diseases caused by MRSA, many studies have been proposed to estimate the causal effects of close contact (treatment) on MRSA infection (outcome) from observational data. In this problem, the treatment assignment mechanism plays a key role as it determines the patterns of missing counterfactuals -- the fundamental challenge of causal effect estimation. Most existing observational studies for causal effect learning assume that the treatment is assigned individually for each unit. However, on many occasions, the treatments are pairwisely assigned for units that are connected in graphs, i.e., the treatments of different units are entangled. Neglecting the entangled treatments can impede the causal effect estimation. In this paper, we study the problem of causal effect estimation with treatment entangled in a graph. Despite a few explorations for entangled treatments, this problem still remains challenging due to the following challenges: (1) the entanglement brings difficulties in modeling and leveraging the unknown treatment assignment mechanism; (2) there may exist hidden confounders which lead to confounding biases in causal effect estimation; (3) the observational data is often time-varying. To tackle these challenges, we propose a novel method NEAT, which explicitly leverages the graph structure to model the treatment assignment mechanism, and mitigates confounding biases based on the treatment assignment modeling. We also extend our method into a dynamic setting to handle time-varying observational data. Experiments on both synthetic datasets and a real-world MRSA dataset validate the effectiveness of the proposed method, and provide insights for future applications.
</details>
<details>
<summary>摘要</summary>
MRSA（多剂肠炎杆菌）是一种抗药菌，它的感染难以预防。在抗生素耗用多年的尝试下，许多研究被提出来估计MRSA感染的 causal effect，从观察数据中获得。在这个问题中，治疗分配机制扮演着关键的角色，它确定了潜在的缺失对照数据的模式——基本挑战 causal effect 估计。大多数现有的观察数据研究假设每个单元都 individually 接受了治疗。然而，在许多情况下，治疗是在图表中连接的单元之间分配的，即不同单元的治疗是 entangled 的。忽略这些杂合的治疗可能会妨碍 causal effect 估计。在这篇文章中，我们研究了图表中的 causal effect 估计问题。虽然有一些对 entangled 治疗的探索，但这个问题仍然具有挑战，因为：（1）杂合带来了对 treatment assignment mechanism 的模型和利用的困难;（2）可能存在隐藏的假设因素，导致 causal effect 估计受到抵消的影响;（3）观察数据通常是时间变化的。为了解决这些挑战，我们提出了一种新方法 NEAT，它明确利用图表结构来模型治疗分配机制，并根据治疗分配模型来减少假设因素的影响。我们还将方法推广到动态设定，以处理时间变化的观察数据。在 synthetic 数据和一个实际MRSA数据上进行了实验，并证明了我们的方法的有效性，并提供了未来应用的参考。
</details></li>
</ul>
<hr>
<h2 id="HeroLT-Benchmarking-Heterogeneous-Long-Tailed-Learning"><a href="#HeroLT-Benchmarking-Heterogeneous-Long-Tailed-Learning" class="headerlink" title="HeroLT: Benchmarking Heterogeneous Long-Tailed Learning"></a>HeroLT: Benchmarking Heterogeneous Long-Tailed Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08235">http://arxiv.org/abs/2307.08235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssskj/herolt">https://github.com/ssskj/herolt</a></li>
<li>paper_authors: Haohui Wang, Weijie Guan, Jianpeng Chen, Zi Wang, Dawei Zhou</li>
<li>for: 本研究旨在提供一个系统性的长尾学习视角，涵盖数据长尾性、域困难度和新任务多样性等三个纬度。</li>
<li>methods: 本研究开发了包括13种现状之最先进算法和6种评价指标的最全面的长尾学习 benchmark 名为 HeroLT，并在14个真实 benchmark 数据集上进行了264项实验。</li>
<li>results: 研究人员通过对 HeroLT  benchmark 进行了全面的实验和分析，并提出了一些有 Promise 的未来方向。<details>
<summary>Abstract</summary>
Long-tailed data distributions are prevalent in a variety of domains, including finance, e-commerce, biomedical science, and cyber security. In such scenarios, the performance of machine learning models is often dominated by the head categories, while the learning of tail categories is significantly inadequate. Given abundant studies conducted to alleviate the issue, this work aims to provide a systematic view of long-tailed learning with regard to three pivotal angles: (A1) the characterization of data long-tailedness, (A2) the data complexity of various domains, and (A3) the heterogeneity of emerging tasks. To achieve this, we develop the most comprehensive (to the best of our knowledge) long-tailed learning benchmark named HeroLT, which integrates 13 state-of-the-art algorithms and 6 evaluation metrics on 14 real-world benchmark datasets across 4 tasks from 3 domains. HeroLT with novel angles and extensive experiments (264 in total) enables researchers and practitioners to effectively and fairly evaluate newly proposed methods compared with existing baselines on varying types of datasets. Finally, we conclude by highlighting the significant applications of long-tailed learning and identifying several promising future directions. For accessibility and reproducibility, we open-source our benchmark HeroLT and corresponding results at https://github.com/SSSKJ/HeroLT.
</details>
<details>
<summary>摘要</summary>
长尾数据分布广泛存在多个领域，如金融、电商、生物医学和网络安全。在这些场景下，机器学习模型的性能frequently受到主要类别的影响，而tail categories的学习则是不足的。鉴于丰富的相关研究，本工作想要提供长尾学习的系统视图，涉及以下三个重要角度：（A1）数据长尾性的特征，（A2）各领域的数据复杂性，以及（A3）emerging task的多样性。为实现这一目标，我们开发了最 complet（到我们所知）的长尾学习 benchmarck named HeroLT，该benchmark integrate 13种state-of-the-art算法和6种评价指标在14个真实世界 benchmark数据集上。 HeroLT通过新的角度和广泛的实验（共264个），帮助研究者和实践者对新提出的方法进行有效和公平的评估，并与现有基准值进行比较。最后，我们 conclude by highlighting long-tailed learning的重要应用和未来发展的一些可能性。为便捷性和可重复性，我们在 GitHub 上公开了我们的 benchmark HeroLT 和相应的结果。
</details></li>
</ul>
<hr>
<h2 id="Learning-for-Counterfactual-Fairness-from-Observational-Data"><a href="#Learning-for-Counterfactual-Fairness-from-Observational-Data" class="headerlink" title="Learning for Counterfactual Fairness from Observational Data"></a>Learning for Counterfactual Fairness from Observational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08232">http://arxiv.org/abs/2307.08232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Ma, Ruocheng Guo, Aidong Zhang, Jundong Li</li>
<li>for: 避免机器学习模型具有对某些子群体的偏见（如种族、性别、年龄等），实现对所有 subgroup 的公正预测。</li>
<li>methods: Counterfactual fairness 是一种从 causal 角度定义的公正性观，通过比较每个个体在原始世界和在对敏感特征值进行修改后的世界中的预测，来衡量模型的公正性。但在实际应用中，通常无法获得准确的 causal 模型，因此直接使用这些模型可能会带来偏见。本文提出了一种新的框架 CLAIRE，通过对数据进行 counterfactual 数据扩展和一种对称约束来减轻敏感特征的偏见。</li>
<li>results:  experiments 表明，CLAIRE 在对实际数据进行预测时比其他方法更好，同时也能够保证对所有 subgroup 的公正预测。<details>
<summary>Abstract</summary>
Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is risky to directly trust the causal models obtained from information sources with unknown reliability and even causal discovery methods, as incorrect causal models can consequently bring biases to the predictor and lead to unfair predictions. In this work, we address the problem of counterfactually fair prediction from observational data without given causal models by proposing a novel framework CLAIRE. Specifically, under certain general assumptions, CLAIRE effectively mitigates the biases from the sensitive attribute with a representation learning framework based on counterfactual data augmentation and an invariant penalty. Experiments conducted on both synthetic and real-world datasets validate the superiority of CLAIRE in both counterfactual fairness and prediction performance.
</details>
<details>
<summary>摘要</summary>
“对待公平机器学习在多个领域中引起了广泛关注，例如在网络广告、个人化推荐和社交媒体分析中的网络应用程序。对待公平机器学习的目标是删除机器学习模型对某些子群体（敏感特征）的偏袋，例如性别、年龄和种族。许多现有的公平定义中，Counterfactual fairness是一种受欢迎的定义，它从 causal 的角度定义了公平的定义。Counterfactual fairness 的定义是根据每个个体在原始世界中的预测和在替代世界中的预测来衡量模型的公平。现有的方法以前需要人类对敏感特征的 causal 模型有充分的知识。但在实际情况下，背景 causal 模型通常是未知的，获取这种人类知识可能是很困难的。在这些情况下，直接对这些信息来源不确定的 causal 模型进行信任可能是很危险的。在这个工作中，我们解决了从观察数据中进行 counterfactually 公平预测的问题，不需要人类对敏感特征的 causal 模型的知识。我们提出了一个名为 CLAIRE 的新框架，它在满足一些一般假设下，可以对敏感特征进行优化，并且使用 counterfactual 数据增强和不变 penalty 来减少偏袋。实验结果显示，CLAIRE 在 counterfactual 公平和预测性能方面具有优越性。”
</details></li>
</ul>
<hr>
<h2 id="Can-Euclidean-Symmetry-be-Leveraged-in-Reinforcement-Learning-and-Planning"><a href="#Can-Euclidean-Symmetry-be-Leveraged-in-Reinforcement-Learning-and-Planning" class="headerlink" title="Can Euclidean Symmetry be Leveraged in Reinforcement Learning and Planning?"></a>Can Euclidean Symmetry be Leveraged in Reinforcement Learning and Planning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08226">http://arxiv.org/abs/2307.08226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linfeng Zhao, Owen Howell, Jung Yeon Park, Xupeng Zhu, Robin Walters, Lawson L. S. Wong</li>
<li>for: 这个论文的目的是设计改进的学习算法，用于控制和规划任务，具有欧几何群同质性。</li>
<li>methods: 论文使用了一种统一优化算法，可以应用于离散和连续的 symmetry 问题，包括优化算法和样本生成算法。</li>
<li>results: 实验证明，通过具有欧几何群同质性的算法，可以更好地解决自然的控制问题。<details>
<summary>Abstract</summary>
In robotic tasks, changes in reference frames typically do not influence the underlying physical properties of the system, which has been known as invariance of physical laws.These changes, which preserve distance, encompass isometric transformations such as translations, rotations, and reflections, collectively known as the Euclidean group. In this work, we delve into the design of improved learning algorithms for reinforcement learning and planning tasks that possess Euclidean group symmetry. We put forth a theory on that unify prior work on discrete and continuous symmetry in reinforcement learning, planning, and optimal control. Algorithm side, we further extend the 2D path planning with value-based planning to continuous MDPs and propose a pipeline for constructing equivariant sampling-based planning algorithms. Our work is substantiated with empirical evidence and illustrated through examples that explain the benefits of equivariance to Euclidean symmetry in tackling natural control problems.
</details>
<details>
<summary>摘要</summary>
在机器人任务中，参照系统的变化通常不会影响系统的物理性质，这被称为不变性法律。这些变化包括同构射影、旋转和反射，合称为欧几何群。在这个工作中，我们深入探讨改进学习算法的设计，以便在奖励学习和规划任务中具有欧几何群的对称性。我们提出了对往年的绝对同构和连续同构在奖励学习、规划和最优控制中的统一理论。算法方面，我们进一步扩展了二维路径规划，并提出了一个管道的构建同构抽样计划算法。我们的工作得到了实验证明，并通过例子解释了在自然控制问题中如何通过对维持欧几何群的同构性来获得利益。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Framework-for-High-Quality-Code-Generation"><a href="#A-Lightweight-Framework-for-High-Quality-Code-Generation" class="headerlink" title="A Lightweight Framework for High-Quality Code Generation"></a>A Lightweight Framework for High-Quality Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08220">http://arxiv.org/abs/2307.08220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Latif Siddiq, Beatrice Casey, Joanna C. S. Santos</li>
<li>for: This paper aims to improve the quality and security of automatically generated source codes using transformer-based code generation models.</li>
<li>methods: The proposed framework, FRANC, includes a static filter and a quality-aware ranker to sort code snippets based on compilability and quality scores. Prompt engineering is also used to fix persistent quality issues.</li>
<li>results: FRANC improves the compilability of Java and Python code suggestions by 9% to 46% and 10% to 43%, respectively. The average improvement in NDCG@10 score is 0.0763, and the repairing techniques repair the highest 80% of prompts. The framework takes approximately 1.98 seconds for Java and 0.08 seconds for Python.<details>
<summary>Abstract</summary>
In recent years, the use of automated source code generation utilizing transformer-based generative models has expanded, and these models can generate functional code according to the requirements of the developers. However, recent research revealed that these automatically generated source codes can contain vulnerabilities and other quality issues. Despite researchers' and practitioners' attempts to enhance code generation models, retraining and fine-tuning large language models is time-consuming and resource-intensive. Thus, we describe FRANC, a lightweight framework for recommending more secure and high-quality source code derived from transformer-based code generation models. FRANC includes a static filter to make the generated code compilable with heuristics and a quality-aware ranker to sort the code snippets based on a quality score. Moreover, the framework uses prompt engineering to fix persistent quality issues. We evaluated the framework with five Python and Java code generation models and six prompt datasets, including a newly created one in this work (SOEval). The static filter improves 9% to 46% Java suggestions and 10% to 43% Python suggestions regarding compilability. The average improvement over the NDCG@10 score for the ranking system is 0.0763, and the repairing techniques repair the highest 80% of prompts. FRANC takes, on average, 1.98 seconds for Java; for Python, it takes 0.08 seconds.
</details>
<details>
<summary>摘要</summary>
近年来，使用自动生成源代码的使用者模型（transformer-based generative models）的使用已扩展。这些模型可以根据开发者的需求生成功能代码。然而，最新的研究发现，这些自动生成的代码可能含有漏洞和质量问题。尽管研究人员和实践者尝试了增强代码生成模型，但是重新训练和精度调整大型自然语言模型是时间consuming和资源占用。因此，我们描述了FRANC框架，它是一个轻量级的框架，可以为基于 transformer 的代码生成模型提供更安全和更高质量的源代码。FRANC 包括一个静态筛选器，使得生成的代码可以遵循规范和质量评分器，以根据代码片段的质量进行排序。此外，框架还使用 prompt 工程来修复持续存在的质量问题。我们对五种 Python 和 Java 代码生成模型，以及六个提示集进行评估。静态筛选器可以提高 Java 建议的可 compiling 率由 9% 到 46%，Python 建议的可 compiling 率由 10% 到 43%。具有 NDCG@10 指标的平均提升为 0.0763，并且修复技术可以修复最高 80% 的提示。FRANC 平均需要 1.98 秒钟 для Java，占用 0.08 秒钟 для Python。
</details></li>
</ul>
<hr>
<h2 id="Forward-Laplacian-A-New-Computational-Framework-for-Neural-Network-based-Variational-Monte-Carlo"><a href="#Forward-Laplacian-A-New-Computational-Framework-for-Neural-Network-based-Variational-Monte-Carlo" class="headerlink" title="Forward Laplacian: A New Computational Framework for Neural Network-based Variational Monte Carlo"></a>Forward Laplacian: A New Computational Framework for Neural Network-based Variational Monte Carlo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08214">http://arxiv.org/abs/2307.08214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruichen Li, Haotian Ye, Du Jiang, Xuelan Wen, Chuwei Wang, Zhe Li, Xiang Li, Di He, Ji Chen, Weiluo Ren, Liwei Wang</li>
<li>for: 能够扩展NN-VMC的应用范围到更大的系统，包括更多的原子、分子和化学反应。</li>
<li>methods: 使用了一种新的计算框架 named Forward Laplacian，通过高效的前进传播过程计算了神经网络中的 Laplacian，从而大幅提高了NN-VMC的计算效率。</li>
<li>results: 对于一系列的原子、分子和化学反应，NN-VMC通过Empirical数据示出了可以解决通用量子力学问题的潜力。<details>
<summary>Abstract</summary>
Neural network-based variational Monte Carlo (NN-VMC) has emerged as a promising cutting-edge technique of ab initio quantum chemistry. However, the high computational cost of existing approaches hinders their applications in realistic chemistry problems. Here, we report the development of a new NN-VMC method that achieves a remarkable speed-up by more than one order of magnitude, thereby greatly extending the applicability of NN-VMC to larger systems. Our key design is a novel computational framework named Forward Laplacian, which computes the Laplacian associated with neural networks, the bottleneck of NN-VMC, through an efficient forward propagation process. We then demonstrate that Forward Laplacian is not only versatile but also facilitates more developments of acceleration methods across various aspects, including optimization for sparse derivative matrix and efficient neural network design. Empirically, our approach enables NN-VMC to investigate a broader range of atoms, molecules and chemical reactions for the first time, providing valuable references to other ab initio methods. The results demonstrate a great potential in applying deep learning methods to solve general quantum mechanical problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Stealthy-Backdoor-Attacks-against-Speech-Recognition-via-Elements-of-Sound"><a href="#Towards-Stealthy-Backdoor-Attacks-against-Speech-Recognition-via-Elements-of-Sound" class="headerlink" title="Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound"></a>Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08208">http://arxiv.org/abs/2307.08208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hanbocai/badspeech_soe">https://github.com/hanbocai/badspeech_soe</a></li>
<li>paper_authors: Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, Yiming Li</li>
<li>for: 这个论文的目的是研究潜在攻击者可以通过恶意投入到语音识别模型的训练过程中，使模型具有恶意预测行为的问题。</li>
<li>methods: 这篇论文使用了一些新的攻击方法，包括使用高频谱的尖声作为触发器，并将其与其他音频 clip 混合以实现更隐蔽的攻击。它们还使用了timbre特征来实现隐蔽的攻击。</li>
<li>results: 实验结果表明，这些攻击方法可以在不同的设定下（例如，all-to-one、all-to-all、干净标签、物理和多个攻击点设定）下实现高效的攻击。这些攻击方法也比较隐蔽，可以逃脱检测。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have been widely and successfully adopted and deployed in various applications of speech recognition. Recently, a few works revealed that these models are vulnerable to backdoor attacks, where the adversaries can implant malicious prediction behaviors into victim models by poisoning their training process. In this paper, we revisit poison-only backdoor attacks against speech recognition. We reveal that existing methods are not stealthy since their trigger patterns are perceptible to humans or machine detection. This limitation is mostly because their trigger patterns are simple noises or separable and distinctive clips. Motivated by these findings, we propose to exploit elements of sound ($e.g.$, pitch and timbre) to design more stealthy yet effective poison-only backdoor attacks. Specifically, we insert a short-duration high-pitched signal as the trigger and increase the pitch of remaining audio clips to `mask' it for designing stealthy pitch-based triggers. We manipulate timbre features of victim audios to design the stealthy timbre-based attack and design a voiceprint selection module to facilitate the multi-backdoor attack. Our attacks can generate more `natural' poisoned samples and therefore are more stealthy. Extensive experiments are conducted on benchmark datasets, which verify the effectiveness of our attacks under different settings ($e.g.$, all-to-one, all-to-all, clean-label, physical, and multi-backdoor settings) and their stealthiness. The code for reproducing main experiments are available at \url{https://github.com/HanboCai/BadSpeech_SoE}.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在语音识别应用中广泛采用和部署。近期一些研究表明，这些模型容易受到后门攻击，敌人可以通过恶意污染训练过程中植入Malicious prediction behaviors。在这篇文章中，我们再次研究了对语音识别的poison-only后门攻击。我们发现现有方法不够隐蔽，因为启发模式是人类或机器检测的。这是因为启发模式通常是简单的噪声或分离的和特征的音频clip。我们被这些发现 motivated，我们提议利用音频元素（如抑声和 timbre）设计更隐蔽又有效的poison-only后门攻击。我们插入短暂的高频声讯作为启发，并增加剩下的音频clip的抑声来mask它。我们操纵受害者音频的timbre特征来设计隐蔽的timbre-based攻击，并设计一个voiceprint选择模块来促进多个后门攻击。我们的攻击可以生成更自然的杂 poisoned samples，因此更隐蔽。我们在标准数据集上进行了广泛的实验，以验证我们的攻击在不同的设置（例如all-to-one、all-to-all、spot、physical和多个后门设置）下的效果和隐蔽性。代码可以在\url{https://github.com/HanboCai/BadSpeech_SoE}中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Quantum-Convolutional-Neural-Network-Approach-for-Object-Detection-and-Classification"><a href="#A-Quantum-Convolutional-Neural-Network-Approach-for-Object-Detection-and-Classification" class="headerlink" title="A Quantum Convolutional Neural Network Approach for Object Detection and Classification"></a>A Quantum Convolutional Neural Network Approach for Object Detection and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08204">http://arxiv.org/abs/2307.08204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gowri Namratha Meedinti, Kandukuri Sai Srirekha, Radhakrishnan Delhibabu</li>
<li>for: 这篇论文主要评估量子卷积神经网络（QCNN）的潜在能力，与经典卷积神经网络（CNN）和人工神经网络（ANN）模型进行比较。</li>
<li>methods: 本论文使用了量子计算方法，将数据存储在量子环境中，并应用了CNN结构来处理这些数据。</li>
<li>results: 分析结果表明，QCNNs在某些应用场景下可以超越经典CNN和ANN模型，both in terms of accuracy and efficiency。此外，QCNNs还可以处理更大的复杂性水平。<details>
<summary>Abstract</summary>
This paper presents a comprehensive evaluation of the potential of Quantum Convolutional Neural Networks (QCNNs) in comparison to classical Convolutional Neural Networks (CNNs) and Artificial / Classical Neural Network (ANN) models. With the increasing amount of data, utilizing computing methods like CNN in real-time has become challenging. QCNNs overcome this challenge by utilizing qubits to represent data in a quantum environment and applying CNN structures to quantum computers. The time and accuracy of QCNNs are compared with classical CNNs and ANN models under different conditions such as batch size and input size. The maximum complexity level that QCNNs can handle in terms of these parameters is also investigated. The analysis shows that QCNNs have the potential to outperform both classical CNNs and ANN models in terms of accuracy and efficiency for certain applications, demonstrating their promise as a powerful tool in the field of machine learning.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Here's the translation in Simplified Chinese:这篇论文对量子卷积神经网络（QCNN）与经典卷积神经网络（CNN）以及人工神经网络（ANN）模型进行了全面的评估。随着数据量不断增加，使用计算方法如CNN在实时中变得越来越困难。QCNNs利用量子粒子来表示数据，并在量子计算机上应用卷积结构，从而超越了经典模型的限制。在不同的批处理大小和输入大小条件下，QCNNs的时间和准确率与经典CNNs和ANN模型进行了比较。此外，QCNNs的最大复杂度水平也进行了调查。分析结果表明，QCNNs在某些应用中可以在准确率和效率方面超越经典模型，表明它们在机器学习领域是一种有力的工具。
</details></li>
</ul>
<hr>
<h2 id="Noise-removal-methods-on-ambulatory-EEG-A-Survey"><a href="#Noise-removal-methods-on-ambulatory-EEG-A-Survey" class="headerlink" title="Noise removal methods on ambulatory EEG: A Survey"></a>Noise removal methods on ambulatory EEG: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02437">http://arxiv.org/abs/2308.02437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Johari, Gowri Namratha Meedinti, Radhakrishnan Delhibabu, Deepak Joshi</li>
<li>for: 本研究旨在实时处理患者短访EEG数据，以提高医疗干预的精确性和效率。</li>
<li>methods: 本研究使用了许多检测和移除噪声的技术，包括模式识别、机器学习、和信号处理等。</li>
<li>results: 本研究发现，不同条件下的EEG数据可以使用不同的检测和移除噪声技术，以提高医疗干预的精确性和效率。<details>
<summary>Abstract</summary>
Over many decades, research is being attempted for the removal of noise in the ambulatory EEG. In this respect, an enormous number of research papers is published for identification of noise removal, It is difficult to present a detailed review of all these literature. Therefore, in this paper, an attempt has been made to review the detection and removal of an noise. More than 100 research papers have been discussed to discern the techniques for detecting and removal the ambulatory EEG. Further, the literature survey shows that the pattern recognition required to detect ambulatory method, eye open and close, varies with different conditions of EEG datasets. This is mainly due to the fact that EEG detected under different conditions has different characteristics. This is, in turn, necessitates the identification of pattern recognition technique to effectively distinguish EEG noise data from a various condition of EEG data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HOPE-High-order-Polynomial-Expansion-of-Black-box-Neural-Networks"><a href="#HOPE-High-order-Polynomial-Expansion-of-Black-box-Neural-Networks" class="headerlink" title="HOPE: High-order Polynomial Expansion of Black-box Neural Networks"></a>HOPE: High-order Polynomial Expansion of Black-box Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08192">http://arxiv.org/abs/2307.08192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harrypotterxtx/hope">https://github.com/harrypotterxtx/hope</a></li>
<li>paper_authors: Tingxiong Xiao, Weihang Zhang, Yuxiao Cheng, Jinli Suo</li>
<li>for: 这篇论文旨在提供一种方法，使深度神经网络变得更加可解，以便在需要作出有理的决策的领域中应用。</li>
<li>methods: 这篇论文使用了高阶多项式扩展（High-order Polynomial Expansion，HOPE）方法，将神经网络拓展成高阶多项式的参考输入。特别是，authors derive了高阶DERIVATIVE规则 для复杂函数，并将其扩展到神经网络，以快速和准确地计算神经网络的高阶DERIVATIVE。</li>
<li>results: 数值分析表明，提案的方法具有高精度、低计算复杂度和良好的收敛性。此外，authors还用HOPE方法实现了深度学习中的功能发现、快速推理和特征选择等广泛应用。<details>
<summary>Abstract</summary>
Despite their remarkable performance, deep neural networks remain mostly ``black boxes'', suggesting inexplicability and hindering their wide applications in fields requiring making rational decisions. Here we introduce HOPE (High-order Polynomial Expansion), a method for expanding a network into a high-order Taylor polynomial on a reference input. Specifically, we derive the high-order derivative rule for composite functions and extend the rule to neural networks to obtain their high-order derivatives quickly and accurately. From these derivatives, we can then derive the Taylor polynomial of the neural network, which provides an explicit expression of the network's local interpretations. Numerical analysis confirms the high accuracy, low computational complexity, and good convergence of the proposed method. Moreover, we demonstrate HOPE's wide applications built on deep learning, including function discovery, fast inference, and feature selection. The code is available at https://github.com/HarryPotterXTX/HOPE.git.
</details>
<details>
<summary>摘要</summary>
尽管它们的表现很出色，深度神经网络仍然具有大量的“黑盒子”特性，这限制了它们在需要做合理决策的领域应用。我们在这里介绍HOPE（高阶多项式扩展）方法，它可以将神经网络扩展成参考输入的高阶多项式。我们 derivated高阶DERIVATIVE规则 для复杂函数，并将这个规则扩展到神经网络，从而快速和高精度地计算神经网络的高阶DERIVATIVE。基于这些DERIVATIVE，我们可以计算神经网络的泰勒多项式，从而获得神经网络的本地解释。数值分析表明HOPE的精度高、计算复杂度低，并且 converge 很好。此外，我们还证明HOPE在深度学习建立的各种应用中具有广泛的应用前景，包括函数发现、快速推理和特征选择。代码可以在https://github.com/HarryPotterXTX/HOPE.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Mini-Giants-“Small”-Language-Models-and-Open-Source-Win-Win"><a href="#Mini-Giants-“Small”-Language-Models-and-Open-Source-Win-Win" class="headerlink" title="Mini-Giants: “Small” Language Models and Open Source Win-Win"></a>Mini-Giants: “Small” Language Models and Open Source Win-Win</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08189">http://arxiv.org/abs/2307.08189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengping Zhou, Lezhi Li, Xinxi Chen, Andy Li</li>
<li>for: 这篇论文主要是为了讨论小语言模型的发展和应用。</li>
<li>methods: 论文使用了开源社区和小语言模型来实现技术、伦理和社会上的赢利。</li>
<li>results: 论文提出了小语言模型在实际应用场景中的需求和潜力，并进行了对小语言模型的比较研究和评估方法。<details>
<summary>Abstract</summary>
ChatGPT is phenomenal. However, it is prohibitively expensive to train and refine such giant models. Fortunately, small language models are flourishing and becoming more and more competent. We call them "mini-giants". We argue that open source community like Kaggle and mini-giants will win-win in many ways, technically, ethically and socially. In this article, we present a brief yet rich background, discuss how to attain small language models, present a comparative study of small language models and a brief discussion of evaluation methods, discuss the application scenarios where small language models are most needed in the real world, and conclude with discussion and outlook.
</details>
<details>
<summary>摘要</summary>
chatgpt是非常出色的，但是它的训练和精细化过程却非常昂贵。幸好，小语言模型在繁殖和成熔的过程中逐渐强大起来。我们称之为“小巨人”。我们认为开源社区如Kaggle和小巨人在技术、道德和社会各个方面都将取得胜利。在这篇文章中，我们会提供简短 yet rich的背景介绍，讲述如何获得小语言模型，进行小语言模型的比较研究， brief discussion of evaluation methods，介绍实际世界中小语言模型的应用场景，并结束 WITH discussion and outlook。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Investigation-of-Pre-trained-Model-Selection-for-Out-of-Distribution-Generalization-and-Calibration"><a href="#An-Empirical-Investigation-of-Pre-trained-Model-Selection-for-Out-of-Distribution-Generalization-and-Calibration" class="headerlink" title="An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration"></a>An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08187">http://arxiv.org/abs/2307.08187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiroki Naganuma, Ryuichiro Hataya</li>
<li>for: 提高out-of-distribution泛化性能和推理不确定性</li>
<li>methods:  investigate pre-trained model selection的影响，并比较不同的数据集和模型参数对性能指标的影响</li>
<li>results: 发现预训练模型选择对out-of-distribution泛化性能有显著影响，大型模型表现较好，但需要进一步研究memorization和真正的泛化之间的平衡。<details>
<summary>Abstract</summary>
In the realm of out-of-distribution generalization tasks, finetuning has risen as a key strategy. While the most focus has been on optimizing learning algorithms, our research highlights the influence of pre-trained model selection in finetuning on out-of-distribution performance and inference uncertainty. Balancing model size constraints of a single GPU, we examined the impact of varying pre-trained datasets and model parameters on performance metrics like accuracy and expected calibration error. Our findings underscore the significant influence of pre-trained model selection, showing marked performance improvements over algorithm choice. Larger models outperformed others, though the balance between memorization and true generalization merits further investigation. Ultimately, our research emphasizes the importance of pre-trained model selection for enhancing out-of-distribution generalization.
</details>
<details>
<summary>摘要</summary>
在异常分布泛化任务中， fine-tuning 已成为一项关键策略。而我们的研究表明，预训练模型选择在 fine-tuning 中对异常分布性能和推理不确定性产生了重要影响。我们在单个 GPU 的模型大小限制下对不同的预训练数据集和模型参数进行了研究，发现预训练模型选择对性能指标如准确率和预期抽象误差产生了显著的影响。大型模型表现更好，但是要找到Memorization 和真正的泛化之间的平衡仍然需要进一步的调查。最终，我们的研究强调了预训练模型选择对异常分布泛化的重要性。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Faithfulness-in-Chain-of-Thought-Reasoning"><a href="#Measuring-Faithfulness-in-Chain-of-Thought-Reasoning" class="headerlink" title="Measuring Faithfulness in Chain-of-Thought Reasoning"></a>Measuring Faithfulness in Chain-of-Thought Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13702">http://arxiv.org/abs/2307.13702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, Ethan Perez</li>
<li>for:  investigate how Chain-of-Thought (CoT) reasoning may be unfaithful in large language models (LLMs)</li>
<li>methods: examine how model predictions change when intervening on the CoT (e.g., adding mistakes or paraphrasing)</li>
<li>results: CoT’s performance boost is not due to added test-time compute or information encoded in the CoT’s phrasing, and models produce less faithful reasoning as they become larger and more capable.Here’s the full abstract in Simplified Chinese:for:  investigate how Chain-of-Thought (CoT) reasoning may be unfaithful in large language models (LLMs)methods: examine how model predictions change when intervening on the CoT (e.g., adding mistakes or paraphrasing)results: CoT’s performance boost is not due to added test-time compute or information encoded in the CoT’s phrasing, and models produce less faithful reasoning as they become larger and more capable.<details>
<summary>Abstract</summary>
Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大型语言模型（LLM）在回答问题时，可以通过“链接思维”（CoT）的步骤式 reasoning 表现更好，但是未知模型的实际运算是否忠于 CoT 的解释。我们 investigate 假设 CoT 的不实际运算，通过对 CoT 进行干扰（例如添加错误或重新诠释）来探索。我们发现模型在不同任务上对 CoT 的conditioning 程度有很大的变化，有时将重点放在 CoT 上，有时则几乎忽略它。CoT 的性能提升不似乎来自 CoT 的额外测试计算或由特定表述 CoT 中的信息。当模型变得更大和更强大时，它们在大多数任务上显示出不忠的运算。总之，我们的结果表明，CoT 可以忠实，只要选择适当的模型大小和任务。
</details></li>
</ul>
<hr>
<h2 id="Question-Decomposition-Improves-the-Faithfulness-of-Model-Generated-Reasoning"><a href="#Question-Decomposition-Improves-the-Faithfulness-of-Model-Generated-Reasoning" class="headerlink" title="Question Decomposition Improves the Faithfulness of Model-Generated Reasoning"></a>Question Decomposition Improves the Faithfulness of Model-Generated Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11768">http://arxiv.org/abs/2307.11768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anthropics/decompositionfaithfulnesspaper">https://github.com/anthropics/decompositionfaithfulnesspaper</a></li>
<li>paper_authors: Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Sam McCandlish, Sheer El Showk, Tamera Lanham, Tim Maxwell, Venkatesa Chandrasekaran, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, Ethan Perez</li>
<li>for: 帮助验证大型自然语言模型（LLM）的正确性和安全性。</li>
<li>methods: 使用 decomposition-based methods，即将问题拆分成多个子问题，让模型在不同的上下文中答swers simpler subquestions，以提高模型生成的 reasoning 的准确性。</li>
<li>results: 研究表明，通过使用 decomposition-based methods，可以提高模型生成的 reasoning 的准确性，而不会 sacrifice  too much的性能。这些方法可以帮助我们更好地验证 LLM 的正确性和安全性。<details>
<summary>Abstract</summary>
As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.
</details>
<details>
<summary>摘要</summary>
To improve the faithfulness of CoT reasoning, we have developed methods that decompose questions into subquestions. This approach achieves strong performance on question-answering tasks and sometimes approaches the performance of CoT while improving the faithfulness of the model's stated reasoning on several recently proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we significantly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT.Our results show that it is possible to improve the faithfulness of model-generated reasoning. Continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Prediction-of-Peptide-Self-assembly-through-Sequential-and-Graphical-Encoding"><a href="#Efficient-Prediction-of-Peptide-Self-assembly-through-Sequential-and-Graphical-Encoding" class="headerlink" title="Efficient Prediction of Peptide Self-assembly through Sequential and Graphical Encoding"></a>Efficient Prediction of Peptide Self-assembly through Sequential and Graphical Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09169">http://arxiv.org/abs/2307.09169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Liu, Jiaqi Wang, Yun Luo, Shuang Zhao, Wenbin Li, Stan Z. Li</li>
<li>for: 这篇论文旨在探讨深度学习如何应用于蛋白质自组装预测，以提高预测精度。</li>
<li>methods: 本研究使用了现代深度学习模型，包括RNN、LSTM、Transformer和GCN、GAT、GraphSAGE等，进行了系统性的检查，以探讨蛋白质编码的影响。</li>
<li>results: 研究发现，Transformer模型是最有力的序列编码基于深度学习模型，可以预测蛋白质自组装的精度。此外，研究还发现了不同的蛋白质编码方法对预测精度的影响。<details>
<summary>Abstract</summary>
In recent years, there has been an explosion of research on the application of deep learning to the prediction of various peptide properties, due to the significant development and market potential of peptides. Molecular dynamics has enabled the efficient collection of large peptide datasets, providing reliable training data for deep learning. However, the lack of systematic analysis of the peptide encoding, which is essential for AI-assisted peptide-related tasks, makes it an urgent problem to be solved for the improvement of prediction accuracy. To address this issue, we first collect a high-quality, colossal simulation dataset of peptide self-assembly containing over 62,000 samples generated by coarse-grained molecular dynamics (CGMD). Then, we systematically investigate the effect of peptide encoding of amino acids into sequences and molecular graphs using state-of-the-art sequential (i.e., RNN, LSTM, and Transformer) and structural deep learning models (i.e., GCN, GAT, and GraphSAGE), on the accuracy of peptide self-assembly prediction, an essential physiochemical process prior to any peptide-related applications. Extensive benchmarking studies have proven Transformer to be the most powerful sequence-encoding-based deep learning model, pushing the limit of peptide self-assembly prediction to decapeptides. In summary, this work provides a comprehensive benchmark analysis of peptide encoding with advanced deep learning models, serving as a guide for a wide range of peptide-related predictions such as isoelectric points, hydration free energy, etc.
</details>
<details>
<summary>摘要</summary>
在最近几年，因为蛋白质的发展和市场潜力而增加了对深度学习应用于蛋白质性质预测的研究，特别是蛋白质自组合的预测。分子动力学技术为蛋白质自组合预测提供了可靠的训练数据，但是蛋白质编码问题的缺乏系统性分析，使得预测精度的提高成为了紧迫的问题。为解决这问题，我们首先收集了高质量的大型蛋白质自组合 simulated annealing 数据集，包含62,000个样本，并使用现状最佳的序列（如 RNN、LSTM 和 Transformer）和结构深度学习模型（如 GCN、GAT 和 GraphSAGE）进行系统性分析，以 investigate the effect of peptide encoding of amino acids into sequences and molecular graphs on the accuracy of peptide self-assembly prediction. 经过广泛的比较研究，我们发现Transformer是最强的序列编码基于深度学习模型，可以为蛋白质自组合预测提供最高的精度，并且可以推动蛋白质自组合预测至十个氨基酸。总之，这项研究提供了深度学习模型的全面性分析，可以作为蛋白质相关预测，如离子点、� hydration free energy 等的指南。
</details></li>
</ul>
<hr>
<h2 id="Multi-Objective-Optimization-of-Performance-and-Interpretability-of-Tabular-Supervised-Machine-Learning-Models"><a href="#Multi-Objective-Optimization-of-Performance-and-Interpretability-of-Tabular-Supervised-Machine-Learning-Models" class="headerlink" title="Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models"></a>Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08175">http://arxiv.org/abs/2307.08175</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/paper_2023_eagga">https://github.com/slds-lmu/paper_2023_eagga</a></li>
<li>paper_authors: Lennart Schneider, Bernd Bischl, Janek Thomas</li>
<li>for: 提高超参数优化和解释性之间的协同优化，以提高表格数据上的预测性能和解释性。</li>
<li>methods: 利用多目标优化问题的方法，将超参数优化和解释性之间的质量考虑为一个单一的优化问题，并通过增加特征选择、交互和 monotonicity 约束来扩展学习算法的搜索空间。</li>
<li>results: 在 benchmark 实验中，提出了一种新的进化算法，可以高效地在扩展的搜索空间上进行优化，并在表格数据上提高了性能和解释性的模型。<details>
<summary>Abstract</summary>
We present a model-agnostic framework for jointly optimizing the predictive performance and interpretability of supervised machine learning models for tabular data. Interpretability is quantified via three measures: feature sparsity, interaction sparsity of features, and sparsity of non-monotone feature effects. By treating hyperparameter optimization of a machine learning algorithm as a multi-objective optimization problem, our framework allows for generating diverse models that trade off high performance and ease of interpretability in a single optimization run. Efficient optimization is achieved via augmentation of the search space of the learning algorithm by incorporating feature selection, interaction and monotonicity constraints into the hyperparameter search space. We demonstrate that the optimization problem effectively translates to finding the Pareto optimal set of groups of selected features that are allowed to interact in a model, along with finding their optimal monotonicity constraints and optimal hyperparameters of the learning algorithm itself. We then introduce a novel evolutionary algorithm that can operate efficiently on this augmented search space. In benchmark experiments, we show that our framework is capable of finding diverse models that are highly competitive or outperform state-of-the-art XGBoost or Explainable Boosting Machine models, both with respect to performance and interpretability.
</details>
<details>
<summary>摘要</summary>
我们提出了一个模型不偏向的框架，用于同时优化supervised机器学习模型的预测性能和可解性。可解性是通过三个度量来衡量：特征稀缺、特征之间的互动稀缺和非升序特征效应的稀缺。我们将hyperparameter优化问题定义为多目标优化问题，以便在单一优化运行中生成兼顾高性能和易于理解的模型。我们通过将特征选择、互动和升序约束添加到学习算法的搜索空间中来实现高效的优化。我们示出，优化问题实际上是找到允许互动的分组选择的Pareto优化集，以及这些分组中每个特征的最佳升序约束和学习算法的优化参数。然后，我们引入了一种新的进化算法，可以高效地在这个扩展的搜索空间上运行。在测试中，我们发现我们的框架能够找到高竞争力或超越当前XGBoost或Explainable Boosting Machine模型， both with respect to performance and interpretability。
</details></li>
</ul>
<hr>
<h2 id="Discovering-User-Types-Mapping-User-Traits-by-Task-Specific-Behaviors-in-Reinforcement-Learning"><a href="#Discovering-User-Types-Mapping-User-Traits-by-Task-Specific-Behaviors-in-Reinforcement-Learning" class="headerlink" title="Discovering User Types: Mapping User Traits by Task-Specific Behaviors in Reinforcement Learning"></a>Discovering User Types: Mapping User Traits by Task-Specific Behaviors in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08169">http://arxiv.org/abs/2307.08169</a></li>
<li>repo_url: None</li>
<li>paper_authors: L. L. Ankile, B. S. Ham, K. Mao, E. Shin, S. Swaroop, F. Doshi-Velez, W. Pan</li>
<li>for: 用于描述用户在帮助人类学习（RL）中的行为，并研究用户特征来减少 intervención设计中的时间。</li>
<li>methods: 使用RL代理表示用户，研究用户行为与特征之间的关系，并提出一种用于研究用户类型崩溃的直观工具。</li>
<li>results: 确认了不同实际环境中的用户类型崩溃是一样的，并将这一观察形式化为环境之间的等式关系。通过在同一等式类中转移 intervención设计，可以快速个性化 intervención。<details>
<summary>Abstract</summary>
When assisting human users in reinforcement learning (RL), we can represent users as RL agents and study key parameters, called \emph{user traits}, to inform intervention design. We study the relationship between user behaviors (policy classes) and user traits. Given an environment, we introduce an intuitive tool for studying the breakdown of "user types": broad sets of traits that result in the same behavior. We show that seemingly different real-world environments admit the same set of user types and formalize this observation as an equivalence relation defined on environments. By transferring intervention design between environments within the same equivalence class, we can help rapidly personalize interventions.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在帮助人类用户进行学习增强（RL）时，我们可以将用户表示为RL agent，并研究关键参数，即“用户特征”，以便设计干预。我们研究用户行为（策略类型）和用户特征之间的关系。给定环境，我们引入一种直观的工具来研究“用户类型”的分解：广泛的特征集合，导致同样的行为。我们发现不同的实际环境都可以拥有同一组用户类型，并将这一观察形式化为环境之间的等式关系。通过在同一等式类中传递干预设计，我们可以帮助快速个化干预。
</details></li>
</ul>
<hr>
<h2 id="Integer-Factorisation-Fermat-Machine-Learning-on-a-Classical-Computer"><a href="#Integer-Factorisation-Fermat-Machine-Learning-on-a-Classical-Computer" class="headerlink" title="Integer Factorisation, Fermat &amp; Machine Learning on a Classical Computer"></a>Integer Factorisation, Fermat &amp; Machine Learning on a Classical Computer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12290">http://arxiv.org/abs/2308.12290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Blake</li>
<li>for: 该论文提出了一种基于深度学习的整数分解算法。</li>
<li>methods: 该算法使用欧拉扩展的法拉第分解算法将整数分解问题转化为二分类问题，并使用大量的synthetic数据进行训练。</li>
<li>results: 该论文介绍了算法的实现和一些实验结果，并分析了实验的缺陷。它还呼吁其他研究人员复现、验证和改进这种方法，以确定其可scalability和实用性。<details>
<summary>Abstract</summary>
In this paper we describe a deep learning--based probabilistic algorithm for integer factorisation. We use Lawrence's extension of Fermat's factorisation algorithm to reduce the integer factorisation problem to a binary classification problem. To address the classification problem, based on the ease of generating large pseudo--random primes, a corpus of training data, as large as needed, is synthetically generated. We will introduce the algorithm, summarise some experiments, analyse where these experiments fall short, and finally put out a call to others to reproduce, verify and see if this approach can be improved to a point where it becomes a practical, scalable factorisation algorithm.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了一种深度学习基于概率算法的整数分解方法。我们使用劳伦斯扩展的费马分解算法将整数分解问题转化为二分类问题。为解决这个分类问题，我们使用大量生成的假随机 prime 数据集进行训练。我们将算法介绍、summarize一些实验结果、分析实验的缺陷，最后呼吁其他人重现、验证以及提高这种方法，以使其成为实用、可扩展的分解算法。
</details></li>
</ul>
<hr>
<h2 id="Feedback-is-All-You-Need-Real-World-Reinforcement-Learning-with-Approximate-Physics-Based-Models"><a href="#Feedback-is-All-You-Need-Real-World-Reinforcement-Learning-with-Approximate-Physics-Based-Models" class="headerlink" title="Feedback is All You Need: Real-World Reinforcement Learning with Approximate Physics-Based Models"></a>Feedback is All You Need: Real-World Reinforcement Learning with Approximate Physics-Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08168">http://arxiv.org/abs/2307.08168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Westenbroek, Jacob Levy, David Fridovich-Keil</li>
<li>for: 本研究旨在开发高效可靠的政策优化策略，用于机器学习实际数据上的 робоット学习。</li>
<li>methods: 本研究使用政策梯度方法，并系统地利用一个可能很简单的第一原理模型，以生成有限量的实际数据上的精确控制策略。</li>
<li>results: 本研究通过理论分析和硬件实验，证明了这种方法可以在几分钟的实际数据上学习精确控制策略，并且可以重新使用。<details>
<summary>Abstract</summary>
We focus on developing efficient and reliable policy optimization strategies for robot learning with real-world data. In recent years, policy gradient methods have emerged as a promising paradigm for training control policies in simulation. However, these approaches often remain too data inefficient or unreliable to train on real robotic hardware. In this paper we introduce a novel policy gradient-based policy optimization framework which systematically leverages a (possibly highly simplified) first-principles model and enables learning precise control policies with limited amounts of real-world data. Our approach $1)$ uses the derivatives of the model to produce sample-efficient estimates of the policy gradient and $2)$ uses the model to design a low-level tracking controller, which is embedded in the policy class. Theoretical analysis provides insight into how the presence of this feedback controller addresses overcomes key limitations of stand-alone policy gradient methods, while hardware experiments with a small car and quadruped demonstrate that our approach can learn precise control strategies reliably and with only minutes of real-world data.
</details>
<details>
<summary>摘要</summary>
我们主要专注于开发高效可靠的政策优化策略，以对现实世界数据进行机器学习。在最近几年中，政策势方法emerged as a promising paradigm for training control policies in simulation. However, these approaches often remain too data inefficient or unreliable to train on real robotic hardware. In this paper, we introduce a novel policy gradient-based policy optimization framework which systematically leverages a (possibly highly simplified) first-principles model and enables learning precise control policies with limited amounts of real-world data.Our approach $1)$ uses the derivatives of the model to produce sample-efficient estimates of the policy gradient and $2)$ uses the model to design a low-level tracking controller, which is embedded in the policy class. Theoretical analysis provides insight into how the presence of this feedback controller addresses overcomes key limitations of stand-alone policy gradient methods, while hardware experiments with a small car and quadruped demonstrate that our approach can learn precise control strategies reliably and with only minutes of real-world data.Here's the translation in Traditional Chinese:我们主要专注于开发高效可靠的政策优化策略，以对现实世界数据进行机器学习。在最近几年中，政策势方法emerged as a promising paradigm for training control policies in simulation. However, these approaches often remain too data inefficient or unreliable to train on real robotic hardware. In this paper, we introduce a novel policy gradient-based policy optimization framework which systematically leverages a (possibly highly simplified) first-principles model and enables learning precise control policies with limited amounts of real-world data.Our approach $1)$ uses the derivatives of the model to produce sample-efficient estimates of the policy gradient and $2)$ uses the model to design a low-level tracking controller, which is embedded in the policy class. Theoretical analysis provides insight into how the presence of this feedback controller addresses overcomes key limitations of stand-alone policy gradient methods, while hardware experiments with a small car and quadruped demonstrate that our approach can learn precise control strategies reliably and with only minutes of real-world data.
</details></li>
</ul>
<hr>
<h2 id="Computing-the-gradients-with-respect-to-all-parameters-of-a-quantum-neural-network-using-a-single-circuit"><a href="#Computing-the-gradients-with-respect-to-all-parameters-of-a-quantum-neural-network-using-a-single-circuit" class="headerlink" title="Computing the gradients with respect to all parameters of a quantum neural network using a single circuit"></a>Computing the gradients with respect to all parameters of a quantum neural network using a single circuit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08167">http://arxiv.org/abs/2307.08167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gphehub/grad2210">https://github.com/gphehub/grad2210</a></li>
<li>paper_authors: Guang Ping He</li>
<li>for: 计算量子神经网络的梯度时，需要计算两次Cost函数，一次是为了计算单个可调参数的梯度。当总参数数量很高时，量子电路需要调整和运行多次。</li>
<li>methods: 我们提出了一种方法，可以使用单个Circuit来计算所有梯度，减少Circuit的深度和классическийRegister的数量。</li>
<li>results: 我们实验表明，我们的方法可以在真实的量子硬件和模拟器上实现速度减少， compiling circuit takes significantly less time than conventional approach, resulting in a total runtime speedup.<details>
<summary>Abstract</summary>
When computing the gradients of a quantum neural network using the parameter-shift rule, the cost function needs to be calculated twice for the gradient with respect to a single adjustable parameter of the network. When the total number of parameters is high, the quantum circuit for the computation has to be adjusted and run for many times. Here we propose an approach to compute all the gradients using a single circuit only, with a much reduced circuit depth and less classical registers. We also demonstrate experimentally, on both real quantum hardware and simulator, that our approach has the advantages that the circuit takes a significantly shorter time to compile than the conventional approach, resulting in a speedup on the total runtime.
</details>
<details>
<summary>摘要</summary>
当计算量子神经网络中参数的梯度使用参数变化规则时，需要计算两次函数值以计算单个可变参数的梯度。当总参数数量较高时，量子电路需要调整并运行多次。我们提出了一种方法，可以通过单个电路计算所有梯度，减少电路深度和классический寄存器数量。我们还在实际中进行了实验，在真实的量子硬件和模拟器上证明了我们的方法可以减少电路编译时间，从而提高总时间的速度。Note: The word "quantum" is translated as "量子" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Neural-Stream-Functions"><a href="#Neural-Stream-Functions" class="headerlink" title="Neural Stream Functions"></a>Neural Stream Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08142">http://arxiv.org/abs/2307.08142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skywolf829/neuralstreamfunction">https://github.com/skywolf829/neuralstreamfunction</a></li>
<li>paper_authors: Skylar Wolfgang Wurster, Hanqi Guo, Tom Peterka, Han-Wei Shen</li>
<li>for: 这个论文是为了计算流函数的 neural network 方法，流函数是一种可以描述流体动态的scalar函数，其梯度与给定的vector field正交。</li>
<li>methods: 该论文使用了一种 implicit neural network 方法，将输入vector field作为输入，并使用内积loss函数来学习一个流函数。该网络可以将输入坐标映射到流函数值上，并且可以通过梯度内积来保证梯度的正交性。</li>
<li>results: 该论文的结果表明，使用这种方法可以生成高质量的流函数解，并且可以根据不同的regularizing loss函数来生成流函数解的不同版本。另外，论文还提供了一些关于如何正确visualize和提取artefact-free的流函数解的建议。<details>
<summary>Abstract</summary>
We present a neural network approach to compute stream functions, which are scalar functions with gradients orthogonal to a given vector field. As a result, isosurfaces of the stream function extract stream surfaces, which can be visualized to analyze flow features. Our approach takes a vector field as input and trains an implicit neural representation to learn a stream function for that vector field. The network learns to map input coordinates to a stream function value by minimizing the inner product of the gradient of the neural network's output and the vector field. Since stream function solutions may not be unique, we give optional constraints for the network to learn particular stream functions of interest. Specifically, we introduce regularizing loss functions that can optionally be used to generate stream function solutions whose stream surfaces follow the flow field's curvature, or that can learn a stream function that includes a stream surface passing through a seeding rake. We also discuss considerations for properly visualizing the trained implicit network and extracting artifact-free surfaces. We compare our results with other implicit solutions and present qualitative and quantitative results for several synthetic and simulated vector fields.
</details>
<details>
<summary>摘要</summary>
我们提出了一种神经网络方法来计算流函数，这些函数的梯度与给定的vector场垂直。因此，iso面流函数提取流面，可以用于分析流体特征。我们的方法通过输入vector场来训练一个隐式神经表示，以学习一个流函数。神经网络将输入坐标映射到流函数值上，通过神经网络输出的梯度和vector场的内积来进行折叠。由于流函数解可能不唯一，我们可以选择ally加入regularizing loss函数，以学习特定的流函数解。例如，我们可以添加一个束制约损失函数，使流函数解的流面与流体场的弯曲性相符，或者学习一个流函数解，其中流面通过种子托铁 passing through。我们还讨论了对训练完成后的隐式神经表示进行正确的visual化和提取 artifact-free的流面。我们与其他隐式解相比较，并对一些synthetic和simulated vector fields进行了质量和量化的结果。
</details></li>
</ul>
<hr>
<h2 id="DynamicFL-Balancing-Communication-Dynamics-and-Client-Manipulation-for-Federated-Learning"><a href="#DynamicFL-Balancing-Communication-Dynamics-and-Client-Manipulation-for-Federated-Learning" class="headerlink" title="DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning"></a>DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06267">http://arxiv.org/abs/2308.06267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bocheng Chen, Nikolay Ivanov, Guangjing Wang, Qiben Yan</li>
<li>for: 这篇论文的目的是提出一个新的联合学习（Federated Learning，FL）框架，以解决联合学习中的高系统多样性问题。</li>
<li>methods: 这篇论文使用了一个特殊的客户端操作策略，将客户端选择基于其网络预测和训练数据质量。此外，它还使用了一个长期追攻策略来解决网络动态环境中的性能下降问题。</li>
<li>results: 相比于现有的客户端选择方案，这篇论文的方法可以实现更好的模型准确性，并且只需要18.9%-84.0%的墙时钟时间。此外，论文还进行了分成和敏感性研究，以证明其在实际应用中的稳定性和可靠性。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning (ML) paradigm, aiming to train a global model by exploiting the decentralized data across millions of edge devices. Compared with centralized learning, FL preserves the clients' privacy by refraining from explicitly downloading their data. However, given the geo-distributed edge devices (e.g., mobile, car, train, or subway) with highly dynamic networks in the wild, aggregating all the model updates from those participating devices will result in inevitable long-tail delays in FL. This will significantly degrade the efficiency of the training process. To resolve the high system heterogeneity in time-sensitive FL scenarios, we propose a novel FL framework, DynamicFL, by considering the communication dynamics and data quality across massive edge devices with a specially designed client manipulation strategy. \ours actively selects clients for model updating based on the network prediction from its dynamic network conditions and the quality of its training data. Additionally, our long-term greedy strategy in client selection tackles the problem of system performance degradation caused by short-term scheduling in a dynamic network. Lastly, to balance the trade-off between client performance evaluation and client manipulation granularity, we dynamically adjust the length of the observation window in the training process to optimize the long-term system efficiency. Compared with the state-of-the-art client selection scheme in FL, \ours can achieve a better model accuracy while consuming only 18.9\% -- 84.0\% of the wall-clock time. Our component-wise and sensitivity studies further demonstrate the robustness of \ours under various real-life scenarios.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）是一种分布式机器学习（ML）模式，旨在透过分散在数百万副本设备（例如移动设备、车辆、火车、或地铁）上的分散数据，训练全球模型。相比中央学习，FL 保持客户端隐私，不直接下载客户端数据。然而，在野外的分散式边缘设备上，因为高度动态的网络环境，聚合所有模型更新从参与设备会带来不可预测的长尾延迟。这将严重损害训练过程的效率。为解决高度系统多样性的时间敏感FL情况下，我们提出了一个新的FL框架，即动态FL，通过考虑网络预测和训练数据质量，对于大量边缘设备进行特殊设计的客户端操作策略。我们在选择客户端进行模型更新时，会根据其网络条件预测和训练数据质量进行选择。此外，我们还使用了长期追击策略，以解决因为短期调度而导致的系统性能下降。最后，为寻求训练过程中的平衡，我们在训练过程中动态调整观察窗口的长度，以便最佳化系统效率。相比之前的客户端选择方案，我们的方案可以获得更好的模型精度，并且只需消耗18.9%-84.0%的壁网时间。我们的组件实验和敏感性研究显示了我们的方案在实际情况下的可持续性。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-graphs-model-spatial-relationships-between-biological-entities-for-breast-cancer-diagnosis"><a href="#Heterogeneous-graphs-model-spatial-relationships-between-biological-entities-for-breast-cancer-diagnosis" class="headerlink" title="Heterogeneous graphs model spatial relationships between biological entities for breast cancer diagnosis"></a>Heterogeneous graphs model spatial relationships between biological entities for breast cancer diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08132">http://arxiv.org/abs/2307.08132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akhila Krishna K, Ravi Kant Gupta, Nikhil Cherian Kurian, Pranav Jeevan, Amit Sethi</li>
<li>for: 这篇论文旨在提高乳癌早期检测、诊断和治疗选择的准确性，挑战乳癌病例的多样性。</li>
<li>methods: 这篇论文使用graph neural network（GNN）来捕捉乳癌病例中细胞和组织之间的空间关系，并将细胞和组织转换为网络结构，以提高对乳癌病例的检测和诊断。</li>
<li>results: 这篇论文的模型在三个公开可用的乳癌数据集上（BRIGHT、BreakHis和BACH）上显示出超过比较器-基于的现有方法的高准确性，并且显示出较低的参数数量和训练时间。<details>
<summary>Abstract</summary>
The heterogeneity of breast cancer presents considerable challenges for its early detection, prognosis, and treatment selection. Convolutional neural networks often neglect the spatial relationships within histopathological images, which can limit their accuracy. Graph neural networks (GNNs) offer a promising solution by coding the spatial relationships within images. Prior studies have investigated the modeling of histopathological images as cell and tissue graphs, but they have not fully tapped into the potential of extracting interrelationships between these biological entities. In this paper, we present a novel approach using a heterogeneous GNN that captures the spatial and hierarchical relations between cell and tissue graphs to enhance the extraction of useful information from histopathological images. We also compare the performance of a cross-attention-based network and a transformer architecture for modeling the intricate relationships within tissue and cell graphs. Our model demonstrates superior efficiency in terms of parameter count and achieves higher accuracy compared to the transformer-based state-of-the-art approach on three publicly available breast cancer datasets -- BRIGHT, BreakHis, and BACH.
</details>
<details>
<summary>摘要</summary>
breast cancer 的多样性呈现出较大的检测早期、诊断和治疗选择的挑战。 convolutional neural networks 经常忽略图像中的空间关系，这可能会限制其精度。 graph neural networks （GNNs）提供了一个有前途的解决方案，通过编码图像中的空间关系。先前的研究已经研究了模型 histopathological 图像为细胞和组织图像，但它们没有充分利用了提取这些生物体系间的关系的潜在。在本文中，我们提出了一种新的方法，使用多样性 GNN 捕捉图像中的空间和层次关系，以提高对 histopathological 图像的EXTRACT 有用信息。我们还对 cross-attention 网络和 transformer 架构进行比较，以模型图像中的复杂关系。我们的模型在三个公共可用的 breast cancer 数据集（BRIGHT、BreakHis 和 BACH）上达到了更高的准确率，并且在参数计数方面表现出了更高的效率，比较 transformer 基于 state-of-the-art 方法。
</details></li>
</ul>
<hr>
<h2 id="INFLECT-DGNN-Influencer-Prediction-with-Dynamic-Graph-Neural-Networks"><a href="#INFLECT-DGNN-Influencer-Prediction-with-Dynamic-Graph-Neural-Networks" class="headerlink" title="INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks"></a>INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08131">http://arxiv.org/abs/2307.08131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/banking-analytics-lab/inflect">https://github.com/banking-analytics-lab/inflect</a></li>
<li>paper_authors: Elena Tiukhova, Emiliano Penaloza, María Óskarsdóttir, Bart Baesens, Monique Snoeck, Cristián Bravo</li>
<li>for: 本研究旨在透过 integrate 动态图 neural network 和 recurrent neural network 等技术，提高 influencer 预测的准确性。</li>
<li>methods: 本研究提出了一种新的 INFLECT-DGNN 框架， combining 图 neural network 和 recurrent neural network ，并使用 weighted loss functions、Synthetic Minority Oversampling TEchnique (SMOTE) 和 rolling-window strategy。</li>
<li>results: 研究结果表明，使用 RNN 编码时间特征并与 GNN 结合使用，能够显著提高 influencer 预测的准确性。 compare various models， demonstrate  capture 图表示、时间依赖和使用财务驱动方法评价的重要性。<details>
<summary>Abstract</summary>
Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive performance. We compare the results of various models to demonstrate the importance of capturing graph representation, temporal dependencies, and using a profit-driven methodology for evaluation.
</details>
<details>
<summary>摘要</summary>
利用网络信息进行预测模型已经在多个领域广泛应用。在推荐和目标营销领域中，Influencer detection  stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships。为了开发这个想法，我们介绍了 INFLECT-DGNN，一个新的框架 для INFLuencer prEdiCTion with Dynamic Graph Neural Networks，该框架结合图 neural network (GNN) 和回归神经网络 (RNN)，并使用负权重函数、Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data，以及一种精心制定的滚动窗口策略。为了评估预测性能，我们使用了一个独特的企业数据集，并 derivated a profit-driven evaluation methodology for influencer prediction。我们的结果表明，使用 RNN 来编码时间特征 alongside GNNs 可以显著提高预测性能。我们对各种模型进行比较，以示出捕捉图表示、时间依赖和使用财务驱动的评估方法的重要性。
</details></li>
</ul>
<hr>
<h2 id="Tangent-Transformers-for-Composition-Privacy-and-Removal"><a href="#Tangent-Transformers-for-Composition-Privacy-and-Removal" class="headerlink" title="Tangent Transformers for Composition, Privacy and Removal"></a>Tangent Transformers for Composition, Privacy and Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08122">http://arxiv.org/abs/2307.08122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Yu Liu, Aditya Golatkar, Stefano Soatto</li>
<li>for: 这篇论文是为了提出一种名为 Tangent Attention Fine-Tuning (TAFT) 的方法，用于精度调整 linearized transformers。</li>
<li>methods: 这种方法使用计算 First-order Taylor Expansion 的方式来计算 Jacobian-Vector Product，从而在单个前进 pass 中计算训练和推理成本，与原始非线性网络相同的数目参数。</li>
<li>results: 当应用于不同的视觉分类任务时，使用 TAFT 精度调整 Tangent Transformer 可以与原始非线性网络精度调整相比肩，而且在同样的参数数量下。此外，TAFT 具有许多优点，例如模型组合、并行训练、机器忘却和差分隐私等。<details>
<summary>Abstract</summary>
We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy.
</details>
<details>
<summary>摘要</summary>
我们介绍 Tangent Attention Fine-Tuning（TAFT），一种精简 Linearized Transformers 的方法，通过计算首项泰利扩展来初始化预训练。我们证明了 Jacobian-Vector Product 的计算可以在单一前进中进行高效地进行，因此训练和测试成本与原始非线性网络相同的阶层，同时使用相同的参数数量。此外，我们显示了在不同的下游视觉分类任务中，使用 TAFT 精简 Tangent Transformer 的 fine-tuning 可以与原始非线性网络的 fine-tuning 相比。因为 Tangent Transformers 是对新的参数集线性的，并且 fine-tuning 的损失函数是凸函数，我们显示了 TAFT 在模型结构、平行训练、机器学习推广和数据隐私方面具有多个优点。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalisation-with-Bidirectional-Encoder-Representations-from-Vision-Transformers"><a href="#Domain-Generalisation-with-Bidirectional-Encoder-Representations-from-Vision-Transformers" class="headerlink" title="Domain Generalisation with Bidirectional Encoder Representations from Vision Transformers"></a>Domain Generalisation with Bidirectional Encoder Representations from Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08117">http://arxiv.org/abs/2307.08117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sw-packages/d23c4b6afa05094a23071333bd230aceceec08117355003f5c0ea958e60c9c98">https://github.com/sw-packages/d23c4b6afa05094a23071333bd230aceceec08117355003f5c0ea958e60c9c98</a></li>
<li>paper_authors: Hamza Riaz, Alan F. Smeaton</li>
<li>for: 这篇论文主要针对domain generalization问题，即将知识从源领域传递到未见领域，以实现深度学习模型的通用化。</li>
<li>methods: 本论文使用了vision transformer（ViT）、LeViT、DeiT和BEIT四种架构进行领域通用化，并在out-of-distribution（OOD）数据上进行初步评估。最终选择了BEIT架构进行进一步的实验。</li>
<li>results: 本论文的结果显示，使用BEIT架构进行领域通用化可以获得显著的提升，具体来说是在PACS、Home-Office和DomainNet三个benchmark上有着优秀的验证和测试准确率表现。此外，本论文的实现也能够填补在 Within-distribution和OOD数据之间的差距。<details>
<summary>Abstract</summary>
Domain generalisation involves pooling knowledge from source domain(s) into a single model that can generalise to unseen target domain(s). Recent research in domain generalisation has faced challenges when using deep learning models as they interact with data distributions which differ from those they are trained on. Here we perform domain generalisation on out-of-distribution (OOD) vision benchmarks using vision transformers. Initially we examine four vision transformer architectures namely ViT, LeViT, DeiT, and BEIT on out-of-distribution data. As the bidirectional encoder representation from image transformers (BEIT) architecture performs best, we use it in further experiments on three benchmarks PACS, Home-Office and DomainNet. Our results show significant improvements in validation and test accuracy and our implementation significantly overcomes gaps between within-distribution and OOD data.
</details>
<details>
<summary>摘要</summary>
域名总结是将多个源域的知识汇集到一个可以总结到未看到的目标域的模型中。近期在域名总结中使用深度学习模型时，面临了与训练数据分布不同的数据分布相互作用的挑战。我们在out-of-distribution（OOD）视觉审核中进行域名总结，初步分析了四种视觉变换器架构，即ViT、LeViT、DeiT和BEIT。其中， bidirectional encoder representation from image transformers（BEIT）架构表现最佳，因此我们在三个审核标准 benchmark（PACS、Home-Office和DomainNet）上进行了进一步的实验。我们的结果表明，使用BEIT架构可以在验证和测试精度上实现显著改进，并且我们的实现可以弥补在 dentro-distribution和OOD数据之间的差距。
</details></li>
</ul>
<hr>
<h2 id="Tangent-Model-Composition-for-Ensembling-and-Continual-Fine-tuning"><a href="#Tangent-Model-Composition-for-Ensembling-and-Continual-Fine-tuning" class="headerlink" title="Tangent Model Composition for Ensembling and Continual Fine-tuning"></a>Tangent Model Composition for Ensembling and Continual Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08114">http://arxiv.org/abs/2307.08114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Yu Liu, Stefano Soatto</li>
<li>for: 这种方法用于组合独立地练习过的模型，以实现逐步学习、集成、或忘记学习。</li>
<li>methods: 这种方法使用 Tangent Model Composition (TMC) 方法，该方法可以在推理时将组件模型相加、缩放或减去，以支持逐步学习、集成、或忘记学习。</li>
<li>results: TMC 方法可以提高精度，比采用非线性练习模型的集成方法高出4.2%，并且在推理成本的2.5倍至10倍的减少下，推理成本与单个模型相同。此外，TMC 方法可以免除额外成本，并且不会留下任何残留效应。<details>
<summary>Abstract</summary>
Tangent Model Composition (TMC) is a method to combine component models independently fine-tuned around a pre-trained point. Component models are tangent vectors to the pre-trained model that can be added, scaled, or subtracted to support incremental learning, ensembling, or unlearning. Component models are composed at inference time via scalar combination, reducing the cost of ensembling to that of a single model. TMC improves accuracy by 4.2% compared to ensembling non-linearly fine-tuned models at a 2.5x to 10x reduction of inference cost, growing linearly with the number of component models. Each component model can be forgotten at zero cost, with no residual effect on the resulting inference. When used for continual fine-tuning, TMC is not constrained by sequential bias and can be executed in parallel on federated data. TMC outperforms recently published continual fine-tuning methods almost uniformly on each setting -- task-incremental, class-incremental, and data-incremental -- on a total of 13 experiments across 3 benchmark datasets, despite not using any replay buffer. TMC is designed for composing models that are local to a pre-trained embedding, but could be extended to more general settings.
</details>
<details>
<summary>摘要</summary>
tangent模型组合（TMC）是一种方法，可以独立地微调component模型，然后在预训练点上组合。component模型是预训练模型的 tangent вектор，可以加、乘、减以支持逐步学习、集成或忘记学习。在推理时，component模型通过scalar组合来实现，因此推理成本只是一个模型的成本。TMC提高了精度4.2%，相比 Ensemble非线性微调模型，并且在推理成本的2.5倍至10倍之间减少了1.5倍。每个component模型可以忘记于零成本，无残留效果。在用于 continual fine-tuning 时，TMC不受顺序偏见的限制，可以在 federated data 上并行执行。TMC在任务逐步、类逐步和数据逐步的13个实验中，对 reciprocal fine-tuning 方法 almost uniformly 的性能优于其他方法，即使不使用 replay buffer。TMC是针对本地预训练 embedding 的模型组合方法，可以扩展到更广泛的设置。
</details></li>
</ul>
<hr>
<h2 id="Discovering-a-reaction-diffusion-model-for-Alzheimer’s-disease-by-combining-PINNs-with-symbolic-regression"><a href="#Discovering-a-reaction-diffusion-model-for-Alzheimer’s-disease-by-combining-PINNs-with-symbolic-regression" class="headerlink" title="Discovering a reaction-diffusion model for Alzheimer’s disease by combining PINNs with symbolic regression"></a>Discovering a reaction-diffusion model for Alzheimer’s disease by combining PINNs with symbolic regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08107">http://arxiv.org/abs/2307.08107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Zhang, Zongren Zou, Ellen Kuhl, George Em Karniadakis</li>
<li>for: 这些研究旨在描述阿尔ツ海默病的发展和病理过程中，蛋白质tau的折叠错误的角色。</li>
<li>methods: 这些研究使用深度学习和人工智能技术，以发现阿尔ツ海默病的数学模型。具体来说，他们使用物理学 Informed Neural Networks (PINNs) 和符号回归来发现tau蛋白质折叠错误的征化方程。</li>
<li>results: 这些研究发现，在46名可能发展阿尔ツ海默病的个体和30名健康控制群体的tau蛋白质扫描数据上，使用PINNs和符号回归可以发现不同的折叠模型，而且阿尔ツ海默病群体的折叠模型比健康控制群体快。这些结果表明，PINNs 和符号回归可以用于发现阿尔ツ海默病中tau蛋白质折叠错误的数学模型。<details>
<summary>Abstract</summary>
Misfolded tau proteins play a critical role in the progression and pathology of Alzheimer's disease. Recent studies suggest that the spatio-temporal pattern of misfolded tau follows a reaction-diffusion type equation. However, the precise mathematical model and parameters that characterize the progression of misfolded protein across the brain remain incompletely understood. Here, we use deep learning and artificial intelligence to discover a mathematical model for the progression of Alzheimer's disease using longitudinal tau positron emission tomography from the Alzheimer's Disease Neuroimaging Initiative database. Specifically, we integrate physics informed neural networks (PINNs) and symbolic regression to discover a reaction-diffusion type partial differential equation for tau protein misfolding and spreading. First, we demonstrate the potential of our model and parameter discovery on synthetic data. Then, we apply our method to discover the best model and parameters to explain tau imaging data from 46 individuals who are likely to develop Alzheimer's disease and 30 healthy controls. Our symbolic regression discovers different misfolding models $f(c)$ for two groups, with a faster misfolding for the Alzheimer's group, $f(c) = 0.23c^3 - 1.34c^2 + 1.11c$, than for the healthy control group, $f(c) = -c^3 +0.62c^2 + 0.39c$. Our results suggest that PINNs, supplemented by symbolic regression, can discover a reaction-diffusion type model to explain misfolded tau protein concentrations in Alzheimer's disease. We expect our study to be the starting point for a more holistic analysis to provide image-based technologies for early diagnosis, and ideally early treatment of neurodegeneration in Alzheimer's disease and possibly other misfolding-protein based neurodegenerative disorders.
</details>
<details>
<summary>摘要</summary>
互助蛋白质在阿尔茨海默病的发展和病理中扮演了关键角色。最新的研究表明，蛋白质的折叠发生 follows a reaction-diffusion type equation的特征。然而，正确的数学模型和参数，用于描述蛋白质的发展过程，仍然未得到完全理解。在这里，我们使用深度学习和人工智能，以发现阿尔茨海默病的数学模型。特别是，我们将物理学习神经网络（PINNs）和符号回归相结合，以找到蛋白质折叠的液态方程。首先，我们在 sintetic data 上验证了我们的模型和参数的潜力。然后，我们将我们的方法应用于46名可能发展阿尔茨海默病的个体和30名健康控制组的tau imaging数据中，以发现最佳的模型和参数，以解释蛋白质的折叠。我们的符号回归发现了两个组的不同的折叠模型，即 $f(c) = 0.23c^3 - 1.34c^2 + 1.11c$ 和 $f(c) = -c^3 + 0.62c^2 + 0.39c$。我们的结果表明，PINNs，补充符号回归，可以发现阿尔茨海默病中蛋白质折叠的液态方程。我们预计我们的研究将成为蛋白质折叠技术的开端，以提供早期诊断和治疗阿尔茨海默病和其他折叠蛋白质基因性神经退化疾病的技术。
</details></li>
</ul>
<hr>
<h2 id="Using-Decision-Trees-for-Interpretable-Supervised-Clustering"><a href="#Using-Decision-Trees-for-Interpretable-Supervised-Clustering" class="headerlink" title="Using Decision Trees for Interpretable Supervised Clustering"></a>Using Decision Trees for Interpretable Supervised Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08104">http://arxiv.org/abs/2307.08104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Natallia Kokash, Leonid Makhnist</li>
<li>for: 本研究探讨了对标注数据集中的分类数据进行可解释的分群问题，即interpretable supervised clustering。</li>
<li>methods: 本文提出了一种迭代方法，使用基于决策树的分类器作为最直观的学习方法，并讨论了节点选择方法以提高分群质量。</li>
<li>results: 本文获得了高密度分群，并通过描述分群的规则集来描述分群。<details>
<summary>Abstract</summary>
In this paper, we address an issue of finding explainable clusters of class-uniform data in labelled datasets. The issue falls into the domain of interpretable supervised clustering. Unlike traditional clustering, supervised clustering aims at forming clusters of labelled data with high probability densities. We are particularly interested in finding clusters of data of a given class and describing the clusters with the set of comprehensive rules. We propose an iterative method to extract high-density clusters with the help of decisiontree-based classifiers as the most intuitive learning method, and discuss the method of node selection to maximize quality of identified groups.
</details>
<details>
<summary>摘要</summary>
在本文中，我们讨论了一个标签数据集中找到可解释的封闭集的问题。这个问题属于可解释supervised clustering的领域。不同于传统封闭，supervised clustering寻求高概率密度的封闭，以便更好地描述数据。我们特别关注找到某个类型的数据的封闭，并使用设计树基于分类器来描述封闭。我们提出了一种迭代方法，使用决策树基于分类器来提取高密度封闭，并讨论了选择节点以提高寻索到的集的质量。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-max-affine-spline-approximation-of-neural-networks-using-the-Legendre-transform-of-a-convex-concave-representation"><a href="#A-max-affine-spline-approximation-of-neural-networks-using-the-Legendre-transform-of-a-convex-concave-representation" class="headerlink" title="A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation"></a>A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09602">http://arxiv.org/abs/2307.09602</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adamgoodtime/legendre_net">https://github.com/adamgoodtime/legendre_net</a></li>
<li>paper_authors: Adam Perrett, Danny Wood, Gavin Brown</li>
<li>for: 该 paper 的目的是提出一种将神经网络转换为spline表示的算法。</li>
<li>methods: 该算法不需要 convex 和 piecewise-affine 网络操作符，而是关注函数的 bounded-ness 和 second derivative 的定义性。</li>
<li>results: 该算法可以在整个网络中进行，而不仅仅是在每层独立进行。这种方法可以 bridge 神经网络和近似理论之间，同时允许抽象网络特征图。实验证明了该算法的正确性和效果。<details>
<summary>Abstract</summary>
This work presents a novel algorithm for transforming a neural network into a spline representation. Unlike previous work that required convex and piecewise-affine network operators to create a max-affine spline alternate form, this work relaxes this constraint. The only constraint is that the function be bounded and possess a well-define second derivative, although this was shown experimentally to not be strictly necessary. It can also be performed over the whole network rather than on each layer independently. As in previous work, this bridges the gap between neural networks and approximation theory but also enables the visualisation of network feature maps. Mathematical proof and experimental investigation of the technique is performed with approximation error and feature maps being extracted from a range of architectures, including convolutional neural networks.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种新的算法，用于将神经网络转换成spline表示形式。与前一些研究不同，这个算法不需要几何和分割的网络运算符来创建一个最大 afine spline alternate form。它只需要函数是有界的，且具有定义的二阶导数，尽管实验表明这并不是必要的。此外，这个算法还可以在整个网络上进行，而不仅是每层独立进行。与以前的研究相似，这种技术将神经网络与近似理论相连接，同时允许网络特征地图的可视化。这个研究包括数学证明和实验调查，并从多种架构，包括卷积神经网络中提取了近似误差和特征地图。
</details></li>
</ul>
<hr>
<h2 id="EasyTPP-Towards-Open-Benchmarking-the-Temporal-Point-Processes"><a href="#EasyTPP-Towards-Open-Benchmarking-the-Temporal-Point-Processes" class="headerlink" title="EasyTPP: Towards Open Benchmarking the Temporal Point Processes"></a>EasyTPP: Towards Open Benchmarking the Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08097">http://arxiv.org/abs/2307.08097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ant-research/easytemporalpointprocess">https://github.com/ant-research/easytemporalpointprocess</a></li>
<li>paper_authors: Siqiao Xue, Xiaoming Shi, Zhixuan Chu, Yan Wang, Fan Zhou, Hongyan Hao, Caigao Jiang, Chen Pan, Yi Xu, James Y. Zhang, Qingsong Wen, Jun Zhou, Hongyuan Mei</li>
<li>for: This paper is written to establish a central benchmark for evaluating temporal point processes (TPPs) in order to promote reproducible research and accelerate progress in the field.</li>
<li>methods: The paper uses eight highly cited neural TPPs and integrates commonly used evaluation metrics and datasets into a standardized benchmarking pipeline. The benchmark is implemented in a universal framework that supports multiple machine learning libraries and custom implementations.</li>
<li>results: The paper presents a comprehensive implementation of TPPs and a standardized benchmarking pipeline for comparing different methods on different datasets, which can help promote reproducible research and accelerate progress in the field. The benchmark is open-sourced and available at a Github repository.<details>
<summary>Abstract</summary>
Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most advanced generative models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there is still no comprehensive benchmark to evaluate them. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field. In this paper, we present EasyTPP, which aims to establish a central benchmark for evaluating TPPs. Compared to previous work that also contributed datasets, our EasyTPP has three unique contributions to the community: (i) a comprehensive implementation of eight highly cited neural TPPs with the integration of commonly used evaluation metrics and datasets; (ii) a standardized benchmarking pipeline for a transparent and thorough comparison of different methods on different datasets; (iii) a universal framework supporting multiple ML libraries (e.g., PyTorch and TensorFlow) as well as custom implementations. Our benchmark is open-sourced: all the data and implementation can be found at this \href{https://github.com/ant-research/EasyTemporalPointProcess}{\textcolor{blue}{Github repository}}\footnote{\url{https://github.com/ant-research/EasyTemporalPointProcess}.}. We will actively maintain this benchmark and welcome contributions from other researchers and practitioners. Our benchmark will help promote reproducible research in this field, thus accelerating research progress as well as making more significant real-world impacts.
</details>
<details>
<summary>摘要</summary>
continuous-time event sequences在真实世界中的应用领域，如医疗、金融、在线购物、社交网络等，扮演着重要的角色。为模型这种数据，时间点过程（TPP）已经成为最先进的生成模型，在学术和应用社区中产生了深见的影响。尽管最近几年出现了许多强大的模型，但是还没有一个通用的标准准则来评估它们。这种标准化的缺失使得研究人员和实践者无法比较方法和重现结果，可能会抑制这个领域的进步。在这篇论文中，我们提出了EasyTPP，它的目标是建立TPP的中心评估标准。与之前的工作相比，EasyTPP有三个独特的贡献：（i）对八种最具影响力的神经网络TPP进行了完整的实现，并集成了通用的评估指标和数据集；（ii）提供了一个标准化的评估管道，使得不同的方法在不同的数据集上进行了公平的比较；（iii）支持多种Machine Learning库（如PyTorch和TensorFlow）以及自定义实现。我们的标准是开源的：所有的数据和实现可以在这个 \href{https://github.com/ant-research/EasyTemporalPointProcess}{\textcolor{blue}{Github repository}} 中找到。我们将积极维护这个标准，并欢迎其他研究人员和实践者的贡献。我们的标准将助推可重复性的研究进步，从而加速这个领域的研究进步，并在真实世界中产生更 significative的影响。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-of-Forgetting-in-Deep-Learning-Beyond-Continual-Learning"><a href="#A-Comprehensive-Survey-of-Forgetting-in-Deep-Learning-Beyond-Continual-Learning" class="headerlink" title="A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning"></a>A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09218">http://arxiv.org/abs/2307.09218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ennengyang/awesome-forgetting-in-deep-learning">https://github.com/ennengyang/awesome-forgetting-in-deep-learning</a></li>
<li>paper_authors: Zhenyi Wang, Enneng Yang, Li Shen, Heng Huang</li>
<li>for: This paper is written to provide a comprehensive survey of forgetting in deep learning, beyond its conventional boundaries, and to explore the potential advantages of forgetting in certain cases, such as privacy-preserving scenarios.</li>
<li>methods: The paper uses a broad range of methods to examine forgetting in various research domains within deep learning, including generative models and federated learning. It also draws upon ideas and approaches from other fields that have dealt with forgetting.</li>
<li>results: The paper presents a nuanced understanding of forgetting as a double-edged sword, highlighting its potential advantages in certain cases, and provides a comprehensive list of papers about forgetting in various research fields. It encourages the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文是为了提供深度学习中忘记的全面检讨，超出传统的 bound，并 explore忘记在特定情况下的优点，如隐私保护场景。</li>
<li>methods: 论文使用了多种方法来检查深度学习中忘记的不同领域，包括生成器模型和联合学习。它还借鉴了其他领域对忘记的经验和方法。</li>
<li>results: 论文提供了忘记作为双刃剑的综合理解，并 highlight了忘记在特定情况下的优点。它还提供了关于忘记的广泛列表，并鼓励开发新的方法来 mitigate、利用或甚至欢迎忘记在实际应用中。<details>
<summary>Abstract</summary>
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context, we aim to present a more nuanced understanding of this phenomenon and highlight its potential advantages. Through this comprehensive survey, we aspire to uncover potential solutions by drawing upon ideas and approaches from various fields that have dealt with forgetting. By examining forgetting beyond its conventional boundaries, in future work, we hope to encourage the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications. A comprehensive list of papers about forgetting in various research fields is available at \url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning}.
</details>
<details>
<summary>摘要</summary>
忘卷（Forgetting）指的是在学习过程中失去或衰退已经获得的信息或知识。 existing surveys on forgetting 主要集中在持续学习领域，但是忘卷在深度学习中的研究领域中也是非常普遍的现象。忘卷在生成模型中的生成器变化和 federated learning 中的客户端数据分布不同而导致的现象。 Addressing 忘卷涉及到保持过去任务知识的 equilibrio 和快速学习新任务的挑战，以及处理任务干扰和矛盾目标的挑战。此外，现有的持续学习survey  implicit  assumes that forgetting is always harmful。相反，我们的survey  argue that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios。通过探讨忘卷在更广泛的上下文中，我们希望呈现一种更加细腻的理解，并高亮其潜在的优点。通过这种全面的survey，我们希望探讨可以从不同领域中的想法和方法中练习解决忘卷。在未来的工作中，我们希望通过探讨忘卷的不同方面，激发开发 novel strategies for mitigating, harnessing, or even embracing forgetting in real applications。一个完整的关于忘卷的paper的列表可以在 \url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning} 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/cs.LG_2023_07_17/" data-id="cllta0lha001rny88c0ywhayh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/cs.SD_2023_07_17/" class="article-date">
  <time datetime="2023-07-16T16:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/cs.SD_2023_07_17/">cs.SD - 2023-07-17 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TST-Time-Sparse-Transducer-for-Automatic-Speech-Recognition"><a href="#TST-Time-Sparse-Transducer-for-Automatic-Speech-Recognition" class="headerlink" title="TST: Time-Sparse Transducer for Automatic Speech Recognition"></a>TST: Time-Sparse Transducer for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08323">http://arxiv.org/abs/2307.08323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Zhang, Mangui Liang, Zhengkun Tian, Jiangyan Yi, Jianhua Tao</li>
<li>for: 这篇论文主要是为了解决端到端模型，特别是循环神经网络推导器（RNN-T）在语音识别中的长序处理问题。</li>
<li>methods: 作者提出了一个名为时间叠 sparse 的模型，内置了时间叠 Mechanism。这个 Mechanism 通过将时间解析为更低的时间解析获得了中间表示，然后使用权重平均算法融合这些表示，以生成简单的隐藏状态。</li>
<li>results: 实验结果显示，与 RNN-T 相比，时间叠 transducer 的字元错误率几乎相同，并且实时因子为原始的 50%。通过调整时间解析，时间叠 transducer 可以降低实时因子至原始的 16.54%，但是这需要付出一些精度的损失（4.94%）。<details>
<summary>Abstract</summary>
End-to-end model, especially Recurrent Neural Network Transducer (RNN-T), has achieved great success in speech recognition. However, transducer requires a great memory footprint and computing time when processing a long decoding sequence. To solve this problem, we propose a model named time-sparse transducer, which introduces a time-sparse mechanism into transducer. In this mechanism, we obtain the intermediate representations by reducing the time resolution of the hidden states. Then the weighted average algorithm is used to combine these representations into sparse hidden states followed by the decoder. All the experiments are conducted on a Mandarin dataset AISHELL-1. Compared with RNN-T, the character error rate of the time-sparse transducer is close to RNN-T and the real-time factor is 50.00% of the original. By adjusting the time resolution, the time-sparse transducer can also reduce the real-time factor to 16.54% of the original at the expense of a 4.94% loss of precision.
</details>
<details>
<summary>摘要</summary>
endpoint模型，特别是Recurrent Neural Network Transducer (RNN-T)，在语音识别中取得了很大成功。然而，抽取器需要较大的内存占用量和计算时间，特别是处理长度较长的解码序列。为解决这个问题，我们提出了一种模型，即时间稀缺抽取器（Time-Sparse Transducer）。在这种机制中，我们通过减少隐藏状态的时间分辨率来获得中间表示。然后，我们使用权重平均算法将这些表示与解码器结合。在使用AISHELL-1 dataset进行所有实验后，我们发现，相比RNN-T，时间稀缺抽取器的字符错误率几乎相同，实时因子为原始的50.00%。通过调整时间分辨率，时间稀缺抽取器还可以降低实时因子至原始的16.54%，同时付出了4.94%的精度损失。
</details></li>
</ul>
<hr>
<h2 id="ivrit-ai-A-Comprehensive-Dataset-of-Hebrew-Speech-for-AI-Research-and-Development"><a href="#ivrit-ai-A-Comprehensive-Dataset-of-Hebrew-Speech-for-AI-Research-and-Development" class="headerlink" title="ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development"></a>ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08720">http://arxiv.org/abs/2307.08720</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yairl/ivrit.ai">https://github.com/yairl/ivrit.ai</a></li>
<li>paper_authors: Yanir Marmor, Kinneret Misgav, Yair Lifshitz</li>
<li>for: 提高希伯来语自动语音识别技术的研究和开发</li>
<li>methods: 使用了3,300小时的希伯来语音数据，包括1,000多个不同的说话人，并提供了不同的研究需求的三种数据形式：原始未处理的音频数据、后Voice Activity Detection的数据，以及部分转写的数据</li>
<li>results: 提供了一个大量的希伯来语音数据资源，可以免费使用，对研究人员、开发者和商业机构都是一个重要的资源，可以推进希伯来语言在人工智能技术中的发展<details>
<summary>Abstract</summary>
We introduce "ivrit.ai", a comprehensive Hebrew speech dataset, addressing the distinct lack of extensive, high-quality resources for advancing Automated Speech Recognition (ASR) technology in Hebrew. With over 3,300 speech hours and a over a thousand diverse speakers, ivrit.ai offers a substantial compilation of Hebrew speech across various contexts. It is delivered in three forms to cater to varying research needs: raw unprocessed audio; data post-Voice Activity Detection, and partially transcribed data. The dataset stands out for its legal accessibility, permitting use at no cost, thereby serving as a crucial resource for researchers, developers, and commercial entities. ivrit.ai opens up numerous applications, offering vast potential to enhance AI capabilities in Hebrew. Future efforts aim to expand ivrit.ai further, thereby advancing Hebrew's standing in AI research and technology.
</details>
<details>
<summary>摘要</summary>
我们介绍“ivrit.ai”，一个全面的希伯来语 speech dataset，填补了希伯来语自动语音识别（ASR）技术的缺乏丰富资源。该dataset包含了超过3,300小时的希伯来语 speech，来自超过1,000名多样化的说话者，覆盖了不同的场景。它提供了三种形式来满足不同的研究需求：原始的未处理音频数据，经过语音活动检测后的数据，以及部分转写的数据。该dataset的legal accessible，即可免费使用，因此成为了研究人员、开发者和商业机构的重要资源。“ivrit.ai”开启了许多应用程序，具有很大的潜力提高希伯来语AI技术。未来努力将ivrit.ai继续扩展，以提高希伯来语在AI研究和技术中的地位。
</details></li>
</ul>
<hr>
<h2 id="BASS-Block-wise-Adaptation-for-Speech-Summarization"><a href="#BASS-Block-wise-Adaptation-for-Speech-Summarization" class="headerlink" title="BASS: Block-wise Adaptation for Speech Summarization"></a>BASS: Block-wise Adaptation for Speech Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08217">http://arxiv.org/abs/2307.08217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, Bhiksha Raj</li>
<li>for: 本研究旨在提高端到端speech summarization的性能，但现有模型受限于计算能力，因此通常只能使用宽度有限的输入序列进行训练。</li>
<li>methods: 本研究提出了一种逐块训练摘要模型的方法，通过分割输入序列进行批处理，以便在不同块之间传递语义上下文。</li>
<li>results: 实验结果表明，采用逐块训练方法可以提高ROUGE-L指标的表现，相比于 truncated input 基准值，提高了3个绝对点。<details>
<summary>Abstract</summary>
End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
</details>
<details>
<summary>摘要</summary>
听力摘要可以提高模型性能，但是训练在极长输入（多个分钟或小时）上受到计算限制，因此通常使用剪辑模型输入。剪辑会导致模型较差，我们的解决方案是使用块式模型，即在输入块中进行处理。在这篇论文中，我们开发了一种可以在极长序列上逐步训练摘要模型的方法。我们将听力摘要视为流动的过程，每个块基于新的听音信息更新假设摘要。我们还提出了将语义上下文传递到块中的策略。在How2 dataset上进行了实验，并证明了我们的块式训练方法可以与 truncated input 基准相比提高约3个绝对 ROUGE-L 分数。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Binary-Classification-Loss-For-Speaker-Verification"><a href="#Exploring-Binary-Classification-Loss-For-Speaker-Verification" class="headerlink" title="Exploring Binary Classification Loss For Speaker Verification"></a>Exploring Binary Classification Loss For Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08205">http://arxiv.org/abs/2307.08205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hunterhuan/sphereface2_speaker_verification">https://github.com/hunterhuan/sphereface2_speaker_verification</a></li>
<li>paper_authors: Bing Han, Zhengyang Chen, Yanmin Qian</li>
<li>for: 这个论文旨在提高speaker verification任务中的表现，减少因为训练和评估频道的差异所导致的性能下降。</li>
<li>methods: 这个论文使用了多个二元分类器来训练speaker模型，而不是传统的多类别分类法，以提高表现的稳定性和精度。</li>
<li>results: 实验结果显示，SphereFace2方法可以优化speaker模型的表现，特别是在困难的实验中，并且可以与大margin fine-tuning策略相结合以获得更好的结果。此外，SphereFace2方法还表现出对于分类数据的耐读性，可以在半supervised训练 scenrio中实现更好的表现。<details>
<summary>Abstract</summary>
The mismatch between close-set training and open-set testing usually leads to significant performance degradation for speaker verification task. For existing loss functions, metric learning-based objectives depend strongly on searching effective pairs which might hinder further improvements. And popular multi-classification methods are usually observed with degradation when evaluated on unseen speakers. In this work, we introduce SphereFace2 framework which uses several binary classifiers to train the speaker model in a pair-wise manner instead of performing multi-classification. Benefiting from this learning paradigm, it can efficiently alleviate the gap between training and evaluation. Experiments conducted on Voxceleb show that the SphereFace2 outperforms other existing loss functions, especially on hard trials. Besides, large margin fine-tuning strategy is proven to be compatible with it for further improvements. Finally, SphereFace2 also shows its strong robustness to class-wise noisy labels which has the potential to be applied in the semi-supervised training scenario with inaccurate estimated pseudo labels. Codes are available in https://github.com/Hunterhuan/sphereface2_speaker_verification
</details>
<details>
<summary>摘要</summary>
通常情况下，靠近集训练和开集测试之间的差异会导致语音识别任务的性能下降。现有的损失函数和多类划分方法都会受到有效对的搜索的限制，而且在未seen speaker上测试时通常会出现下降。在这种情况下，我们引入了SphereFace2框架，它使用多个二分类器来训练说话者模型，而不是进行多类划分。这种学习模式可以有效地减少训练和评估之间的差距。在Voxceleb上进行的实验表明，SphereFace2可以比其他损失函数更高效地处理hard trial。此外，大margin精细调整策略与之相容，可以进一步提高性能。最后，SphereFace2还表明其强大的鲁棒性，可以在类别噪声标签的情况下进行 semi-supervised 训练。代码可以在https://github.com/Hunterhuan/sphereface2_speaker_verification 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/cs.SD_2023_07_17/" data-id="cllta0lhy003vny882n8agbxg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/eess.AS_2023_07_17/" class="article-date">
  <time datetime="2023-07-16T16:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/eess.AS_2023_07_17/">eess.AS - 2023-07-17 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dynamic-Kernel-Convolution-Network-with-Scene-dedicate-Training-for-Sound-Event-Localization-and-Detection"><a href="#Dynamic-Kernel-Convolution-Network-with-Scene-dedicate-Training-for-Sound-Event-Localization-and-Detection" class="headerlink" title="Dynamic Kernel Convolution Network with Scene-dedicate Training for Sound Event Localization and Detection"></a>Dynamic Kernel Convolution Network with Scene-dedicate Training for Sound Event Localization and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08239">http://arxiv.org/abs/2307.08239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Huang, Jianfeng Chen, Jisheng Bai, Yafei Jia, Dongzhe Zhang</li>
<li>for: 这篇论文的目的是提出一种高效的声事件地理位置检测和检测系统，用于真实的空间声场。</li>
<li>methods: 该系统使用动态核心 convolution 模块来适应不同的感知范围，以及 SELDnet 和 EINv2 框架。此外，在训练阶段，还引入了两种场景专门的策略以提高系统在真实空间声场中的通用性。</li>
<li>results: 实验结果表明，提出的系统在 Sony-TAu 真实空间声场 dataset 上的表现出色，并超过了 fixes-kernel convolution SELD 系统。此外，该系统在 DCASE SELD 任务中获得了0.348的 SELD 分数，超过了 State-of-the-Art 方法。<details>
<summary>Abstract</summary>
DNN-based methods have shown high performance in sound event localization and detection(SELD). While in real spatial sound scenes, reverberation and the imbalanced presence of various sound events increase the complexity of the SELD task. In this paper, we propose an effective SELD system in real spatial scenes.In our approach, a dynamic kernel convolution module is introduced after the convolution blocks to adaptively model the channel-wise features with different receptive fields. Secondly, we incorporate the SELDnet and EINv2 framework into the proposed SELD system with multi-track ACCDOA. Moreover, two scene-dedicated strategies are introduced into the training stage to improve the generalization of the system in realistic spatial sound scenes. Finally, we apply data augmentation methods to extend the dataset using channel rotation, spatial data synthesis. Four joint metrics are used to evaluate the performance of the SELD system on the Sony-TAu Realistic Spatial Soundscapes 2022 dataset.Experimental results show that the proposed systems outperform the fixed-kernel convolution SELD systems. In addition, the proposed system achieved an SELD score of 0.348 in the DCASE SELD task and surpassed the SOTA methods.
</details>
<details>
<summary>摘要</summary>
使用 Deep Neural Network (DNN) 方法的 зву频事件localization和检测 (SELD) 表现非常高。然而，在实际空间声场中，吸收和各种声事件的不均衡存在，使得 SELD 任务变得更加复杂。在这篇论文中，我们提出了一个有效的 SELD 系统，用于实际空间声场中。我们的方法包括：1. 在卷积块后引入动态核心 convolution 模块，以自适应地处理不同感受场的通道特征。2. 将 SELDnet 和 EINv2 框架integrated 到我们的 SELD 系统中，并使用多轨迹 ACCDOA。3. 在训练阶段，引入了两种适应性战略，以提高系统在真实空间声场中的普适性。4. 使用数据扩展方法，将数据集扩展到更多的频率和空间数据。我们使用了四个联合评价指标来评价 SELD 系统在 Sony-TAu Realistic Spatial Soundscapes 2022 数据集上的性能。实验结果表明，我们的系统在固定核心 convolution SELD 系统上表现出色，并且在 DCASE SELD 任务中达到了最佳效果，超过了 State-of-the-Art 方法。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Large-Language-Model-with-Speech-for-Fully-Formatted-End-to-End-Speech-Recognition"><a href="#Adapting-Large-Language-Model-with-Speech-for-Fully-Formatted-End-to-End-Speech-Recognition" class="headerlink" title="Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition"></a>Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08234">http://arxiv.org/abs/2307.08234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></li>
<li>paper_authors: Shaoshi Ling, Yuxuan Hu, Shuangbei Qian, Guoli Ye, Yao Qian, Yifan Gong, Ed Lin, Michael Zeng</li>
<li>for: 这个论文的目的是提高端到端语音识别（E2E ASR）模型的性能。</li>
<li>methods: 这个论文使用了预训练的大型语言模型（LLMs）来改进E2E ASR模型的性能。</li>
<li>results: 该方法可以有效地利用预训练的LLMs来生成更易读的ASR转录。对于具有不同领域的完整E2E ASR转录任务，我们的模型可以超越强大的ASR模型，如Whisper，在识别错误率方面。<details>
<summary>Abstract</summary>
Most end-to-end (E2E) speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. Pretrained large language models (LLMs) have the potential to improve the performance of E2E ASR. However, integrating a pretrained language model into an E2E speech recognition model has shown limited benefits due to the mismatches between text-based LLMs and those used in E2E ASR. In this paper, we explore an alternative approach by adapting a pretrained LLMs to speech. Our experiments on fully-formatted E2E ASR transcription tasks across various domains demonstrate that our approach can effectively leverage the strengths of pretrained LLMs to produce more readable ASR transcriptions. Our model, which is based on the pretrained large language models with either an encoder-decoder or decoder-only structure, surpasses strong ASR models such as Whisper, in terms of recognition error rate, considering formats like punctuation and capitalization as well.
</details>
<details>
<summary>摘要</summary>
大多数端到端（E2E）语音识别模型由编码和解码块组成，这些块执行音频和语言模型功能。预训练大型语言模型（LLMs）有可能提高E2E ASR的性能。然而，将预训练语言模型与E2E语音识别模型结合使用显示有限的好处，这主要是因为文本基于的LLMs与E2E ASR中使用的模型之间存在差异。在这篇论文中，我们探讨一种替代方法，即适应预训练LLMs到语音。我们的实验表明，我们的方法可以有效地利用预训练LLMs的优势，生成更易读的ASR讯号。我们的模型基于预训练大型语言模型，可以是编码-解码结构或解码 только结构，在各个领域的完全格式E2E ASR转写任务中表现出色，胜过如喊叫等强大ASR模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/eess.AS_2023_07_17/" data-id="cllta0lim0065ny88hp2k93yh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/eess.IV_2023_07_17/" class="article-date">
  <time datetime="2023-07-16T16:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/eess.IV_2023_07_17/">eess.IV - 2023-07-17 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Reconstructed-Convolution-Module-Based-Look-Up-Tables-for-Efficient-Image-Super-Resolution"><a href="#Reconstructed-Convolution-Module-Based-Look-Up-Tables-for-Efficient-Image-Super-Resolution" class="headerlink" title="Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution"></a>Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08544">http://arxiv.org/abs/2307.08544</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuguandu/rc-lut">https://github.com/liuguandu/rc-lut</a></li>
<li>paper_authors: Guandu Liu, Yukang Ding, Mading Li, Ming Sun, Xing Wen, Bin Wang</li>
<li>for: 提高单个图像超分解（SR）任务的效果</li>
<li>methods: 使用新型的重构卷积（RC）模块，它将通道和空间计算解耦，从而降低LUT大小并保持$n\times n$的辐射场</li>
<li>results: 与state-of-the-art LUT基elineSR方法相比，提出的RCLUT方法可以提高辐射场大小9倍，并在5个流行的benchmark数据集上达到优秀表现，同时可以作为LUT基elineSR方法的替换部件进行改进。<details>
<summary>Abstract</summary>
Look-up table(LUT)-based methods have shown the great efficacy in single image super-resolution (SR) task. However, previous methods ignore the essential reason of restricted receptive field (RF) size in LUT, which is caused by the interaction of space and channel features in vanilla convolution. They can only increase the RF at the cost of linearly increasing LUT size. To enlarge RF with contained LUT sizes, we propose a novel Reconstructed Convolution(RC) module, which decouples channel-wise and spatial calculation. It can be formulated as $n^2$ 1D LUTs to maintain $n\times n$ receptive field, which is obviously smaller than $n\times n$D LUT formulated before. The LUT generated by our RC module reaches less than 1/10000 storage compared with SR-LUT baseline. The proposed Reconstructed Convolution module based LUT method, termed as RCLUT, can enlarge the RF size by 9 times than the state-of-the-art LUT-based SR method and achieve superior performance on five popular benchmark dataset. Moreover, the efficient and robust RC module can be used as a plugin to improve other LUT-based SR methods. The code is available at https://github.com/liuguandu/RC-LUT.
</details>
<details>
<summary>摘要</summary>
look-up 表(LUT)-based 方法在单个图像超解像(SR) 任务中表现出色。然而，先前的方法忽视了 Look-up 表中 restricted 收发Field(RF) 的关键原因，这是因为混合空间和通道特征在 vanilla 核函数中所引起的。他们只能通过线性增加 LUT 大小来增加 RF。为了使 RF 增加而不是 LUT 大小线性增加，我们提议一种新的 Reconstructed Convolution(RC) 模块。这可以表示为 $n^2$ 1D LUT，以维护 $n\times n$ 收发Field。与之前的 $n\times n$D LUT 不同，这明显更小。我们的 RC 模块生成的 LUT 存储量低于 1/10000 比 SR-LUT 基eline。我们提议的 Reconstructed Convolution 模块基于 LUT 方法，称为 RCLUT，可以将 RF 尺寸提高至先前的 9 倍，并在五个流行的 benchmark 数据集上实现出色的性能。此外，我们的有效和可靠 RC 模块可以作为 LUT-based SR 方法的插件来改进其性能。代码可以在 https://github.com/liuguandu/RC-LUT 上获取。
</details></li>
</ul>
<hr>
<h2 id="Study-of-Vision-Transformers-for-Covid-19-Detection-from-Chest-X-rays"><a href="#Study-of-Vision-Transformers-for-Covid-19-Detection-from-Chest-X-rays" class="headerlink" title="Study of Vision Transformers for Covid-19 Detection from Chest X-rays"></a>Study of Vision Transformers for Covid-19 Detection from Chest X-rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09402">http://arxiv.org/abs/2307.09402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandeep Angara, Sharath Thirunagaru</li>
<li>for: 这个研究旨在检测 COVID-19 病毒，使用视觉转换器进行检测，以提高检测效率和准确率。</li>
<li>methods: 本研究使用了许多现代的视觉转换器模型，包括 Vision Transformer (ViT)、Swin-transformer、Max vision transformer (MViT) 和 Pyramid Vision transformer (PVT)，通过转移学习IMAGENET 权重来实现高度的检测精度。</li>
<li>results: 实验结果显示，视觉转换器模型在 COVID-19 检测中达到了状态对的性能，即 98.75% 到 99.5% 的准确率，超过了传统方法和卷积神经网络（CNNs）的性能， highlighting the potential of Vision Transformers as a powerful tool for COVID-19 detection.<details>
<summary>Abstract</summary>
The COVID-19 pandemic has led to a global health crisis, highlighting the need for rapid and accurate virus detection. This research paper examines transfer learning with vision transformers for COVID-19 detection, known for its excellent performance in image recognition tasks. We leverage the capability of Vision Transformers to capture global context and learn complex patterns from chest X-ray images. In this work, we explored the recent state-of-art transformer models to detect Covid-19 using CXR images such as vision transformer (ViT), Swin-transformer, Max vision transformer (MViT), and Pyramid Vision transformer (PVT). Through the utilization of transfer learning with IMAGENET weights, the models achieved an impressive accuracy range of 98.75% to 99.5%. Our experiments demonstrate that Vision Transformers achieve state-of-the-art performance in COVID-19 detection, outperforming traditional methods and even Convolutional Neural Networks (CNNs). The results highlight the potential of Vision Transformers as a powerful tool for COVID-19 detection, with implications for improving the efficiency and accuracy of screening and diagnosis in clinical settings.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行导致全球卫生危机，高亮了快速和准确病毒检测的需求。这篇研究论文研究了通过视力变换器进行 COVID-19 检测，这种技术在图像识别任务中表现出色。我们利用视力变换器捕捉全局上下文和学习复杂的图像模式。在这项工作中，我们探索了最新的转换器模型，包括视力变换器（ViT）、Swin-transformer、Max视力变换器（MViT）和Pyramid视力变换器（PVT），以进行 COVID-19 检测使用 CXR 图像。通过使用转换学习IMAGENET 重量，模型实现了各种准确率范围为 98.75% 到 99.5%。我们的实验表明，视力变换器在 COVID-19 检测中实现了状态艺术表现，超越传统方法和卷积神经网络（CNNs）。结果表明，视力变换器是一种有力的工具，可以提高检测和诊断的效率和准确率。
</details></li>
</ul>
<hr>
<h2 id="EGE-UNet-an-Efficient-Group-Enhanced-UNet-for-skin-lesion-segmentation"><a href="#EGE-UNet-an-Efficient-Group-Enhanced-UNet-for-skin-lesion-segmentation" class="headerlink" title="EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation"></a>EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08473">http://arxiv.org/abs/2307.08473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jcruan519/ege-unet">https://github.com/jcruan519/ege-unet</a></li>
<li>paper_authors: Jiacheng Ruan, Mingye Xie, Jingsheng Gao, Ting Liu, Yuzhuo Fu</li>
<li>for: 这篇论文的目的是提出一个更有效率的医疗图像分类方法，以便应用于移动健康应用程序中。</li>
<li>methods: 这篇论文使用了一种名为Efficient Group Enhanced UNet（EGE-UNet）的方法，它将一种名为Group multi-axis Hadamard Product Attention（GHPA）和一种名为Group Aggregation Bridge（GAB）模组组合在一起，以提高分类精度和减少计算负载。</li>
<li>results: 根据实验结果，EGE-UNet比较 existed 的方法有着更好的分类性能，并且降低了参数和计算负载的比例，具体是494倍和160倍。此外，这是第一个参数数量只有50KB的模型。<details>
<summary>Abstract</summary>
Transformer and its variants have been widely used for medical image segmentation. However, the large number of parameter and computational load of these models make them unsuitable for mobile health applications. To address this issue, we propose a more efficient approach, the Efficient Group Enhanced UNet (EGE-UNet). We incorporate a Group multi-axis Hadamard Product Attention module (GHPA) and a Group Aggregation Bridge module (GAB) in a lightweight manner. The GHPA groups input features and performs Hadamard Product Attention mechanism (HPA) on different axes to extract pathological information from diverse perspectives. The GAB effectively fuses multi-scale information by grouping low-level features, high-level features, and a mask generated by the decoder at each stage. Comprehensive experiments on the ISIC2017 and ISIC2018 datasets demonstrate that EGE-UNet outperforms existing state-of-the-art methods. In short, compared to the TransFuse, our model achieves superior segmentation performance while reducing parameter and computation costs by 494x and 160x, respectively. Moreover, to our best knowledge, this is the first model with a parameter count limited to just 50KB. Our code is available at https://github.com/JCruan519/EGE-UNet.
</details>
<details>
<summary>摘要</summary>
“transformer和其 variants 在医疗影像 segmentation 方面广泛应用。然而，这些模型的参数数量和计算负担使得它们不适合移动医疗应用。为解决这个问题，我们提出了一种更高效的方法，efficient Group Enhanced UNet (EGE-UNet)。我们在轻量级的情况下嵌入了Group multi-axis Hadamard Product Attention module (GHPA)和Group Aggregation Bridge module (GAB)。GHPA 将输入特征分组并在不同轴上执行 Hadamard Product Attention 机制 (HPA)，以提取多个视角下的疾病信息。GAB 有效地将多尺度信息 fusion，通过分组低级特征、高级特征和解码器在每个阶段生成的掩码。经过了 ISIC2017 和 ISIC2018 数据集的广泛实验，我们的 EGE-UNet 超越了现有的状态态-of-the-art 方法。总之，相比于 TransFuse，我们的模型实现了更高效的 segmentation 性能，同时减少参数数量和计算成本，减少了 494 倍和 160 倍。此外，我们知道的是，这是第一个参数数量限制在 50KB 的模型。我们的代码可以在 <https://github.com/JCruan519/EGE-UNet> 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-using-Silver-Standard-Masks-for-Lateral-Ventricle-Segmentation-in-FLAIR-MRI"><a href="#Domain-Adaptation-using-Silver-Standard-Masks-for-Lateral-Ventricle-Segmentation-in-FLAIR-MRI" class="headerlink" title="Domain Adaptation using Silver Standard Masks for Lateral Ventricle Segmentation in FLAIR MRI"></a>Domain Adaptation using Silver Standard Masks for Lateral Ventricle Segmentation in FLAIR MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08456">http://arxiv.org/abs/2307.08456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen Crystal, Pejman J. Maralani, Sandra Black, Alan R. Moody, April Khademi</li>
<li>for: 这个研究旨在提出一种基于转移学习的左 Lateral ventricular volume (LVV) 分割方法，用于 Fluid-attenuated inversion recovery (FLAIR) MRI 图像。</li>
<li>methods: 这种方法使用了域 adaptation 技术，以便在目标领域中优化性能，并使用了一种新的传统图像处理 algorithm 生成了Silver standard (SS) mask。</li>
<li>results: 测试结果表明，使用 SS+GS 模型（在目标 SS Mask 和源 GS Mask 上进行练习和微调）在三个目标领域上得到了最好的和最稳定的性能（mean DSC &#x3D; 0.89，CoV &#x3D; 0.05），并与源 GS 模型在三个目标领域上显著 differently (p &lt; 0.05)。这些结果表明，在目标领域中生成的噪声标签可以帮助模型适应到 dataset-specific 特征，并提供了一个 robust 的参数初始化。<details>
<summary>Abstract</summary>
Lateral ventricular volume (LVV) is an important biomarker for clinical investigation. We present the first transfer learning-based LVV segmentation method for fluid-attenuated inversion recovery (FLAIR) MRI. To mitigate covariate shifts between source and target domains, this work proposes an domain adaptation method that optimizes performance on three target datasets. Silver standard (SS) masks were generated from the target domain using a novel conventional image processing ventricular segmentation algorithm and used to supplement the gold standard (GS) data from the source domain, Canadian Atherosclerosis Imaging Network (CAIN). Four models were tested on held-out test sets from four datasets: 1) SS+GS: trained on target SS masks and fine-tuned on source GS masks, 2) GS+SS: trained on source GS masks and fine-tuned on target SS masks, 3) trained on source GS (GS CAIN Only) and 4) trained on target SS masks (SS Only). The SS+GS model had the best and most consistent performance (mean DSC = 0.89, CoV = 0.05) and showed significantly (p < 0.05) higher DSC compared to the GS-only model on three target domains. Results suggest pre-training with noisy labels from the target domain allows the model to adapt to the dataset-specific characteristics and provides robust parameter initialization while fine-tuning with GS masks allows the model to learn detailed features. This method has wide application to other medical imaging problems where labeled data is scarce, and can be used as a per-dataset calibration method to accelerate wide-scale adoption.
</details>
<details>
<summary>摘要</summary>
lateral ventricular volume (LVV) 是一个重要的临床探索指标。本研究提出了首个将 Transfer Learning 应用于 fluid-attenuated inversion recovery (FLAIR) MRI 的 LVV 分割方法。为了减少对于源和目标领域之间的差异，本研究提出了一种领域适应方法，将目标领域中的 silver standard (SS) 标签转换为 gold standard (GS) 标签，并将其用于补充来自源领域的 GS 标签。本研究测试了四种模型，包括：1) SS+GS：使用目标领域中的 SS 标签进行 fine-tuning，并使用源领域中的 GS 标签进行训练；2) GS+SS：使用源领域中的 GS 标签进行 fine-tuning，并使用目标领域中的 SS 标签进行训练；3) 使用源领域中的 GS 标签进行训练（GS CAIN Only）；4) 使用目标领域中的 SS 标签进行训练（SS Only）。结果显示 SS+GS 模型的表现最佳和最稳定（平均 DSC = 0.89，CoV = 0.05），并与三个目标领域中的 GS-only 模型相比有 statistically significant 的差异（p < 0.05）。结果显示在目标领域中使用不精确的标签进行预训练可以让模型适应到dataset-specific的特性，并提供了Robust的初始化，而在 fine-tuning 过程中使用 GS 标签可以让模型学习到详细的特征。这种方法可以延伸到其他医学影像问题， где 标签是稀缺的，并且可以作为一种可靠的准备方法，帮助快速普遍推广。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Snake-Convolution-based-on-Topological-Geometric-Constraints-for-Tubular-Structure-Segmentation"><a href="#Dynamic-Snake-Convolution-based-on-Topological-Geometric-Constraints-for-Tubular-Structure-Segmentation" class="headerlink" title="Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation"></a>Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08388">http://arxiv.org/abs/2307.08388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaoleiqi/dscnet">https://github.com/yaoleiqi/dscnet</a></li>
<li>paper_authors: Yaolei Qi, Yuting He, Xiaoming Qi, Yuan Zhang, Guanyu Yang</li>
<li>for: 提高 tubular 结构分割的准确性和效率，在各种领域 Ensure accuracy and efficiency in various fields.</li>
<li>methods: 使用动态蛇 convolution 精确捕捉 tubular 结构特征，并在多视图Feature fusion中补充注意力。 Propose a multi-view feature fusion strategy to complement attention to features from multiple perspectives during feature fusion.</li>
<li>results: 对 2D 和 3D 数据集进行实验，与其他方法比较，DSCNet 在 tubular 结构分割任务中提供更高的准确性和连续性。 Our experiments on 2D and 3D datasets show that our DSCNet provides better accuracy and continuity on the tubular structure segmentation task compared with several methods.<details>
<summary>Abstract</summary>
Accurate segmentation of topological tubular structures, such as blood vessels and roads, is crucial in various fields, ensuring accuracy and efficiency in downstream tasks. However, many factors complicate the task, including thin local structures and variable global morphologies. In this work, we note the specificity of tubular structures and use this knowledge to guide our DSCNet to simultaneously enhance perception in three stages: feature extraction, feature fusion, and loss constraint. First, we propose a dynamic snake convolution to accurately capture the features of tubular structures by adaptively focusing on slender and tortuous local structures. Subsequently, we propose a multi-view feature fusion strategy to complement the attention to features from multiple perspectives during feature fusion, ensuring the retention of important information from different global morphologies. Finally, a continuity constraint loss function, based on persistent homology, is proposed to constrain the topological continuity of the segmentation better. Experiments on 2D and 3D datasets show that our DSCNet provides better accuracy and continuity on the tubular structure segmentation task compared with several methods. Our codes will be publicly available.
</details>
<details>
<summary>摘要</summary>
精准分割 topological tubular 结构，如血管和道路，在各个领域是关键，以确保准确性和效率。然而，许多因素使得这个任务变得复杂，包括细小的地方结构和变化的全球形态。在这种情况下，我们注意到 tubular 结构的特殊性，并使用这些知识来引导我们的 DSCNet 在三个阶段中同时提高听见：特征提取、特征融合和损失约束。首先，我们提出了动态蛇 convolution，以准确地捕捉 tubular 结构的特征，并在细小和折衔的地方结构中进行适应性地调整。然后，我们提出了多视图特征融合策略，在特征融合时，从多个角度来补充对特征的注意力，以确保保留不同全球形态中的重要信息。最后，我们提出了基于 persistente homology 的连续性约束损失函数，以更好地限制分割结果的topological连续性。在 2D 和 3D 数据集上进行了实验，发现我们的 DSCNet 在 tubular 结构分割任务上提供了更高的准确性和连续性，比较多种方法。我们的代码将在公共可用。
</details></li>
</ul>
<hr>
<h2 id="Component-wise-Power-Estimation-of-Electrical-Devices-Using-Thermal-Imaging"><a href="#Component-wise-Power-Estimation-of-Electrical-Devices-Using-Thermal-Imaging" class="headerlink" title="Component-wise Power Estimation of Electrical Devices Using Thermal Imaging"></a>Component-wise Power Estimation of Electrical Devices Using Thermal Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08354">http://arxiv.org/abs/2307.08354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, Simon Grosche, Akarsh Bharadwaj, André Kaup</li>
<li>for: 这 paper 用于估计电子板子上不同活动组件的功率消耗。</li>
<li>methods: 该方法使用热成像技术，不需要特殊的高反射层。可以通过手动标注、物体检测方法或利用布局信息获得热图分割。</li>
<li>results: 评估结果显示，使用低分辨率消耗功率大于300mW的consumer infrared镜头，可以达到mean estimation error为10%。<details>
<summary>Abstract</summary>
This paper presents a novel method to estimate the power consumption of distinct active components on an electronic carrier board by using thermal imaging. The components and the board can be made of heterogeneous material such as plastic, coated microchips, and metal bonds or wires, where a special coating for high emissivity is not required. The thermal images are recorded when the components on the board are dissipating power. In order to enable reliable estimates, a segmentation of the thermal image must be available that can be obtained by manual labeling, object detection methods, or exploiting layout information. Evaluations show that with low-resolution consumer infrared cameras and dissipated powers larger than 300mW, mean estimation errors of 10% can be achieved.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的方法，用温存像来估算电子承载板上不同活动部件的能量消耗。这些部件和板可以由不同材料组成，如塑料、覆监微型逻辑器和金属带或电缆，而不需要特殊的高温透明层。温存像记录在部件上散热时，并进行了可靠的分割，可以通过手动标注、物体检测方法或利用布局信息来获得。评估结果显示，使用低分辨率消耗电频相机和大于300mW的散热功率，可以实现平均估算误差10%。
</details></li>
</ul>
<hr>
<h2 id="Neural-Modulation-Fields-for-Conditional-Cone-Beam-Neural-Tomography"><a href="#Neural-Modulation-Fields-for-Conditional-Cone-Beam-Neural-Tomography" class="headerlink" title="Neural Modulation Fields for Conditional Cone Beam Neural Tomography"></a>Neural Modulation Fields for Conditional Cone Beam Neural Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08351">http://arxiv.org/abs/2307.08351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samuelepapa/cond-cbnt">https://github.com/samuelepapa/cond-cbnt</a></li>
<li>paper_authors: Samuele Papa, David M. Knigge, Riccardo Valperga, Nikita Moriakov, Miltos Kofinas, Jan-Jakob Sonke, Efstratios Gavves</li>
<li>for: 提高深度学习方法在 cone beam geometry computed tomography (CBCT) 重建中的精度和效率，使其能够在更复杂的CBCT重建中提供更好的结果。</li>
<li>methods: 基于神经场 (NF) 的方法，通过在输入空间中学习一个连续的神经网络来近似重建的密度。新提议的方法是通过使用每个扫描中的本地修饰来Conditioning Cone Beam Neural Tomography (CondCBNT)，使其能够更好地适应不同扫描数据中的变化。</li>
<li>results: CondCBNT 在不同数量的可用投射下对噪声数据和清晰数据都显示了改进的性能，比如使用单个CondCBNT模型可以在低投射数下达到高精度水平。<details>
<summary>Abstract</summary>
Conventional Computed Tomography (CT) methods require large numbers of noise-free projections for accurate density reconstructions, limiting their applicability to the more complex class of Cone Beam Geometry CT (CBCT) reconstruction. Recently, deep learning methods have been proposed to overcome these limitations, with methods based on neural fields (NF) showing strong performance, by approximating the reconstructed density through a continuous-in-space coordinate based neural network. Our focus is on improving such methods, however, unlike previous work, which requires training an NF from scratch for each new set of projections, we instead propose to leverage anatomical consistencies over different scans by training a single conditional NF on a dataset of projections. We propose a novel conditioning method where local modulations are modeled per patient as a field over the input domain through a Neural Modulation Field (NMF). The resulting Conditional Cone Beam Neural Tomography (CondCBNT) shows improved performance for both high and low numbers of available projections on noise-free and noisy data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-coding-of-360°-videos-exploiting-inactive-regions-in-projection-formats"><a href="#Efficient-coding-of-360°-videos-exploiting-inactive-regions-in-projection-formats" class="headerlink" title="Efficient coding of 360° videos exploiting inactive regions in projection formats"></a>Efficient coding of 360° videos exploiting inactive regions in projection formats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08344">http://arxiv.org/abs/2307.08344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, Mohammadreza Jamali, Stéphane Coulombe, Carlos Vazquez, Ahmad Vakili</li>
<li>for: 提高360度视频编码效率，即使在无法观看的区域忽略 pixels 值。</li>
<li>methods: 利用无效区域忽略 pixels 值，在重建Equirectangular格式或视口中减少损失。</li>
<li>results: 可以达到10%的比特率减少。<details>
<summary>Abstract</summary>
This paper presents an efficient method for encoding common projection formats in 360$^\circ$ video coding, in which we exploit inactive regions. These regions are ignored in the reconstruction of the equirectangular format or the viewport in virtual reality applications. As the content of these pixels is irrelevant, we neglect the corresponding pixel values in ratedistortion optimization, residual transformation, as well as inloop filtering and achieve bitrate savings of up to 10%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Power-Modeling-for-Virtual-Reality-Video-Playback-Applications"><a href="#Power-Modeling-for-Virtual-Reality-Video-Playback-Applications" class="headerlink" title="Power Modeling for Virtual Reality Video Playback Applications"></a>Power Modeling for Virtual Reality Video Playback Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08338">http://arxiv.org/abs/2307.08338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, Stéphane Coulombe, Ahmad Vakili, André Kaup</li>
<li>for: 评估和模型现代虚拟现实播放和流式应用程序在智能手机上的能 consumption。</li>
<li>methods: 通过进行功率测量，进一步构建一个用于估算真实能 consumption的模型，并且可以在关键电池水平下保存能源。</li>
<li>results: 结果显示，降低输入视频分辨率可以减少能 consumption。<details>
<summary>Abstract</summary>
This paper proposes a method to evaluate and model the power consumption of modern virtual reality playback and streaming applications on smartphones. Due to the high computational complexity of the virtual reality processing toolchain, the corresponding power consumption is very high, which reduces operating times of battery-powered devices. To tackle this problem, we analyze the power consumption in detail by performing power measurements. Furthermore, we construct a model to estimate the true power consumption with a mean error of less than 3.5%. The model can be used to save power at critical battery levels by changing the streaming video parameters. Particularly, the results show that the power consumption is significantly reduced by decreasing the input video resolution.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种方法来评估和模拟现代虚拟现实播放和流媒体应用程序在智能手机上的电力消耗。由于虚拟现实处理排序链的计算复杂性很高，相应的电力消耗很大，导致耗电器上的运行时间受限。为解决这个问题，我们进行了电力测量，并构建了一个估算真实电力消耗的模型，模型的误差低于3.5%。这个模型可以在关键的电池水平下保存能量，通过修改流媒体参数来降低输入视频分辨率。结果表明，降低输入视频分辨率可以减少电力消耗。
</details></li>
</ul>
<hr>
<h2 id="Power-Efficient-Video-Streaming-on-Mobile-Devices-Using-Optimal-Spatial-Scaling"><a href="#Power-Efficient-Video-Streaming-on-Mobile-Devices-Using-Optimal-Spatial-Scaling" class="headerlink" title="Power-Efficient Video Streaming on Mobile Devices Using Optimal Spatial Scaling"></a>Power-Efficient Video Streaming on Mobile Devices Using Optimal Spatial Scaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08337">http://arxiv.org/abs/2307.08337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, André Kaup, Stéphane Coulombe, Ahmad Vakili</li>
<li>for: 这个论文是为了实现功能强大的无线视频流媒体，以提高移动设备上的视频播放效率和能效性。</li>
<li>methods: 该论文使用了一种基于文献中的电源模型和主观质量评估指标，来 derive最佳的空间缩放和比特率控制参数。</li>
<li>results: 研究发现，可以通过调整输入视频的分辨率，以优化质量-能效性的交易。对于高清序列，可以保持10%的电源储备，而无损质量损失，或者保持15%的电源储备，而tolerable distortion。测试结果表明，该方法在Wi-Fi和移动网络中具有普遍适用性。<details>
<summary>Abstract</summary>
This paper derives optimal spatial scaling and rate control parameters for power-efficient wireless video streaming on portable devices. A video streaming application is studied, which receives a high-resolution and high-quality video stream from a remote server and displays the content to the end-user.We show that the resolution of the input video can be adjusted such that the quality-power trade-off is optimized. Making use of a power model from the literature and subjective quality evaluation using a perceptual metric, we derive optimal combinations of the scaling factor and the rate-control parameter for encoding. For HD sequences, up to 10% of power can be saved at negligible quality losses and up to 15% of power can be saved at tolerable distortions. To show general validity, the method was tested for Wi-Fi and a mobile network as well as for two different smartphones.
</details>
<details>
<summary>摘要</summary>
这篇论文研究了对移动设备进行功能强化的无线视频流式传输中的空间缩放和速率控制参数优化。一个视频流应用程序被研究，它从远程服务器接收高分辨率和高质量视频流，并将内容显示给终端用户。我们表明，可以根据输入视频的分辨率进行调整，以优化质量-功耗交易。使用文献中提供的电力模型和主观质量评价使用一种感知指标，我们得出了最佳的缩放因子和编码参数的组合。对高清序列，可以在不影响质量的情况下将电力减少10%，或者在可接受的损害下减少15%。为证明普适性，方法在Wi-Fi和移动网络以及两种不同的智能手机上进行了测试。
</details></li>
</ul>
<hr>
<h2 id="Combiner-and-HyperCombiner-Networks-Rules-to-Combine-Multimodality-MR-Images-for-Prostate-Cancer-Localisation"><a href="#Combiner-and-HyperCombiner-Networks-Rules-to-Combine-Multimodality-MR-Images-for-Prostate-Cancer-Localisation" class="headerlink" title="Combiner and HyperCombiner Networks: Rules to Combine Multimodality MR Images for Prostate Cancer Localisation"></a>Combiner and HyperCombiner Networks: Rules to Combine Multimodality MR Images for Prostate Cancer Localisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08279">http://arxiv.org/abs/2307.08279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Yan, Bernard Chiu, Ziyi Shen, Qianye Yang, Tom Syer, Zhe Min, Shonit Punwani, Mark Emberton, David Atkinson, Dean C. Barratt, Yipeng Hu</li>
<li>For: This paper aims to demonstrate the feasibility of using low-dimensional parametric models to model decision rules for radiologists’ reading of multiparametric prostate MR scans, and to improve the efficiency of automated radiologist labeling.* Methods: The proposed Combiner networks use a linear mixture model or a nonlinear stacking model to model PI-RADS decision rules, and train a single image segmentation network that can be conditioned on these hyperparameters during inference.* Results: Experimental results based on data from 850 patients show that the proposed combiner networks outperform other commonly-adopted end-to-end networks, and provide added advantages in obtaining and interpreting the modality combining rules. The paper also presents three clinical applications for prostate cancer segmentation, including modality availability assessment, importance quantification, and rule discovery.<details>
<summary>Abstract</summary>
One of the distinct characteristics in radiologists' reading of multiparametric prostate MR scans, using reporting systems such as PI-RADS v2.1, is to score individual types of MR modalities, T2-weighted, diffusion-weighted, and dynamic contrast-enhanced, and then combine these image-modality-specific scores using standardised decision rules to predict the likelihood of clinically significant cancer. This work aims to demonstrate that it is feasible for low-dimensional parametric models to model such decision rules in the proposed Combiner networks, without compromising the accuracy of predicting radiologic labels: First, it is shown that either a linear mixture model or a nonlinear stacking model is sufficient to model PI-RADS decision rules for localising prostate cancer. Second, parameters of these (generalised) linear models are proposed as hyperparameters, to weigh multiple networks that independently represent individual image modalities in the Combiner network training, as opposed to end-to-end modality ensemble. A HyperCombiner network is developed to train a single image segmentation network that can be conditioned on these hyperparameters during inference, for much improved efficiency. Experimental results based on data from 850 patients, for the application of automating radiologist labelling multi-parametric MR, compare the proposed combiner networks with other commonly-adopted end-to-end networks. Using the added advantages of obtaining and interpreting the modality combining rules, in terms of the linear weights or odds-ratios on individual image modalities, three clinical applications are presented for prostate cancer segmentation, including modality availability assessment, importance quantification and rule discovery.
</details>
<details>
<summary>摘要</summary>
一个 radiologists 在多 Parametric prostate MR 图像读取中的特征是将不同类型的 MR 模式分别评分，使用 PI-RADS v2.1 报告系统，并将这些图像模式特定的分数相互结合使用标准化的决策规则预测肉眼标签的可能性。本研究旨在证明可以使用低维度parametric模型来模型这些决策规则，无需妥协精度预测肉眼标签。首先，研究表明，线性混合模型或非线性堆叠模型都可以模型PI-RADS决策规则，用于本地化肉眼悬液肿瘤。其次，通过将这些（通用）线性模型的参数作为 гипер参数，可以将多个独立表示不同图像模式的网络在Combiner网络训练中进行权重调整，而不是END-TO-END模式ensemble。在这个HyperCombiner网络中，可以在推理时通过conditioning来控制这些参数，以提高效率。实验结果基于850名患者的数据，对于自动化肉眼标注多参量MR的应用，与其他常见的END-TO-END网络进行比较。通过获得和解释这些组合规则，即图像模式特定的线性权重或抽象比率，可以对肉眼标注进行多种优化和应用。例如，可以根据图像模式的可用性进行评估，或者根据图像模式的重要性进行量化，还可以通过发现新的规则来进行肉眼标注。
</details></li>
</ul>
<hr>
<h2 id="Liver-Tumor-Screening-and-Diagnosis-in-CT-with-Pixel-Lesion-Patient-Network"><a href="#Liver-Tumor-Screening-and-Diagnosis-in-CT-with-Pixel-Lesion-Patient-Network" class="headerlink" title="Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network"></a>Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08268">http://arxiv.org/abs/2307.08268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Yan, Xiaoli Yin, Yingda Xia, Fakai Wang, Shu Wang, Yuan Gao, Jiawen Yao, Chunli Li, Xiaoyu Bai, Jingren Zhou, Ling Zhang, Le Lu, Yu Shi</li>
<li>for: liver tumor segmentation and classification</li>
<li>methods: 使用mask transformer进行同时分割和类别 each lesion,以及image-wise classifier来Integrate global信息</li>
<li>results: 在非对照CT预处理任务中，PLAN achieved 95%和96%的患者级敏感性和特异性; 在对照CT任务中，我们的肿体分割精度、回卷率和类别精度分别达92%, 89%和86%,超过了广泛使用的CNN和transformers для肿体分割; 我们还对250个例进行了读者研究, PLAN的结果与一名高级人类放射学家一样，表明我们的结果具有临床意义。<details>
<summary>Abstract</summary>
Liver tumor segmentation and classification are important tasks in computer aided diagnosis. We aim to address three problems: liver tumor screening and preliminary diagnosis in non-contrast computed tomography (CT), and differential diagnosis in dynamic contrast-enhanced CT. A novel framework named Pixel-Lesion-pAtient Network (PLAN) is proposed. It uses a mask transformer to jointly segment and classify each lesion with improved anchor queries and a foreground-enhanced sampling loss. It also has an image-wise classifier to effectively aggregate global information and predict patient-level diagnosis. A large-scale multi-phase dataset is collected containing 939 tumor patients and 810 normal subjects. 4010 tumor instances of eight types are extensively annotated. On the non-contrast tumor screening task, PLAN achieves 95% and 96% in patient-level sensitivity and specificity. On contrast-enhanced CT, our lesion-level detection precision, recall, and classification accuracy are 92%, 89%, and 86%, outperforming widely used CNN and transformers for lesion segmentation. We also conduct a reader study on a holdout set of 250 cases. PLAN is on par with a senior human radiologist, showing the clinical significance of our results.
</details>
<details>
<summary>摘要</summary>
liver tumor分割和分类是计算机辅助诊断中的重要任务。我们想要解决三个问题：liver tumor在非对照计算机 Tomography（CT）中的检测和初步诊断，以及在动态对照CT中的分化诊断。我们提出了一个名为Pixel-Lesion-pAtient Network（PLAN）的框架。它使用一个面Mask transformer来同时分割和分类每个肿瘤，并使用改进的锚点查询和前景增强抽象损失来提高分割精度。它还有一个图像级别分类器，以有效地汇集全像信息并预测patient级诊断。我们收集了一个大规模多阶段数据集，包括939个肿瘤病人和810个正常Subject。4010个肿瘤实例中有八种类型进行了广泛的注释。在非对照肿瘤检测任务上，PLAN达到了95%和96%的patient级敏感性和特异性。在对照CT任务上，我们的肿瘤分割精度、检测精度和分类精度分别为92%, 89%和86%，超过了广泛使用的CNN和transformers для肿瘤分割。我们还进行了一个读者研究，其中PLAN与一名 senior human radiologist相当，表明了我们的结果的临床意义。
</details></li>
</ul>
<hr>
<h2 id="Extreme-Image-Compression-using-Fine-tuned-VQGAN-Models"><a href="#Extreme-Image-Compression-using-Fine-tuned-VQGAN-Models" class="headerlink" title="Extreme Image Compression using Fine-tuned VQGAN Models"></a>Extreme Image Compression using Fine-tuned VQGAN Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08265">http://arxiv.org/abs/2307.08265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Mao, Tinghan Yang, Yinuo Zhang, Shuyin Pan, Meng Wang, Shiqi Wang, Siwei Ma</li>
<li>for: 提高压缩数据的感知质量，特别是在低比特率下。</li>
<li>methods: 引入生成器模型（VQGAN），使用vector quantization（VQ）将图像表示为矢量编码。</li>
<li>results: 在EXTREMELY低比特率下（&lt;0.1 bpp），提高图像压缩后的感知质量，并且超过了现有的代码库。<details>
<summary>Abstract</summary>
Recent advances in generative compression methods have demonstrated remarkable progress in enhancing the perceptual quality of compressed data, especially in scenarios with low bitrates. Nevertheless, their efficacy and applicability in achieving extreme compression ratios ($<0.1$ bpp) still remain constrained. In this work, we propose a simple yet effective coding framework by introducing vector quantization (VQ)-based generative models into the image compression domain. The main insight is that the codebook learned by the VQGAN model yields strong expressive capacity, facilitating efficient compression of continuous information in the latent space while maintaining reconstruction quality. Specifically, an image can be represented as VQ-indices by finding the nearest codeword, which can be encoded using lossless compression methods into bitstreams. We then propose clustering a pre-trained large-scale codebook into smaller codebooks using the K-means algorithm. This enables images to be represented as diverse ranges of VQ-indices maps, resulting in variable bitrates and different levels of reconstruction quality. Extensive qualitative and quantitative experiments on various datasets demonstrate that the proposed framework outperforms the state-of-the-art codecs in terms of perceptual quality-oriented metrics and human perception under extremely low bitrates.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "generative compression methods" is translated as "生成压缩方法" (shēngchǎn zhùsuā fāngyì)* "perceptual quality" is translated as "感知质量" (gǎnzhì zhìliàng)* "codebook" is translated as "代码本" (dàimódian)* "VQGAN model" is translated as "VQGAN模型" (VQGAN módeli)* "K-means algorithm" is translated as "K-means算法" (K-means suānfǎ)* "variable bitrates" is translated as "变量比特率" (biànlvèng bǐtiéshù)* "different levels of reconstruction quality" is translated as "不同的重建质量" (bùdōng de zhòngjiàn zhìliàng)
</details></li>
</ul>
<hr>
<h2 id="Adaptively-Placed-Multi-Grid-Scene-Representation-Networks-for-Large-Scale-Data-Visualization"><a href="#Adaptively-Placed-Multi-Grid-Scene-Representation-Networks-for-Large-Scale-Data-Visualization" class="headerlink" title="Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization"></a>Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02494">http://arxiv.org/abs/2308.02494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skywolf829/apmgsrn">https://github.com/skywolf829/apmgsrn</a></li>
<li>paper_authors: Skylar Wolfgang Wurster, Tianyu Xiong, Han-Wei Shen, Hanqi Guo, Tom Peterka</li>
<li>for: 这 paper 是为了提高 scientific data 的压缩和可视化而提出的Scene Representation Networks (SRNs)。</li>
<li>methods: 该 paper 使用了适应放置的多重网格 SRN (APMGSRN) 和域 decomposit 训练和推理技术来加速多个 GPU 系统上的训练。</li>
<li>results: 该 paper 表明，使用 APMGSRN 可以提高 SRNs 的重建精度，而无需耗费贵重的 octree 细化、截割和搜索。 它还提供了一个开源的 neural volume rendering 应用程序，可以轻松地在任何 PyTorch-based SRN 上进行渲染。<details>
<summary>Abstract</summary>
Scene representation networks (SRNs) have been recently proposed for compression and visualization of scientific data. However, state-of-the-art SRNs do not adapt the allocation of available network parameters to the complex features found in scientific data, leading to a loss in reconstruction quality. We address this shortcoming with an adaptively placed multi-grid SRN (APMGSRN) and propose a domain decomposition training and inference technique for accelerated parallel training on multi-GPU systems. We also release an open-source neural volume rendering application that allows plug-and-play rendering with any PyTorch-based SRN. Our proposed APMGSRN architecture uses multiple spatially adaptive feature grids that learn where to be placed within the domain to dynamically allocate more neural network resources where error is high in the volume, improving state-of-the-art reconstruction accuracy of SRNs for scientific data without requiring expensive octree refining, pruning, and traversal like previous adaptive models. In our domain decomposition approach for representing large-scale data, we train an set of APMGSRNs in parallel on separate bricks of the volume to reduce training time while avoiding overhead necessary for an out-of-core solution for volumes too large to fit in GPU memory. After training, the lightweight SRNs are used for realtime neural volume rendering in our open-source renderer, where arbitrary view angles and transfer functions can be explored. A copy of this paper, all code, all models used in our experiments, and all supplemental materials and videos are available at https://github.com/skywolf829/APMGSRN.
</details>
<details>
<summary>摘要</summary>
Scene representation networks (SRNs) 有最近提出用于数据压缩和可视化的新方法。然而，当前的SRNs不会根据科学数据中复杂的特征进行分配可用的网络参数，导致重建质量下降。我们解决这个缺陷，通过适应地在多个网格上分布多个特性网络（APMGSRN），并提出基于多个GPU系统的域分解训练和执行技术。我们还发布了基于PyTorch的开源神经量化渲染应用，可以方便地在任意的PyTorch-based SRN上进行渲染。我们的APMGSRN架构使用多个空间自适应特征网格，以学习在域中的位置，以动态分配更多的神经网络资源，以提高SRNs的重建精度。在我们的域分解方法中，我们在不同的GPU系统上并行训练多个APMGSRN，以降低训练时间，而不需要昂贵的octree优化、剪辑和搜索。之后，我们使用轻量级的SRN进行实时神经量化渲染。一个包含这篇论文、所有代码、所有在我们实验中用到的模型、以及所有补充材料和视频的报告可以在https://github.com/skywolf829/APMGSRN中找到。
</details></li>
</ul>
<hr>
<h2 id="GastroVision-A-Multi-class-Endoscopy-Image-Dataset-for-Computer-Aided-Gastrointestinal-Disease-Detection"><a href="#GastroVision-A-Multi-class-Endoscopy-Image-Dataset-for-Computer-Aided-Gastrointestinal-Disease-Detection" class="headerlink" title="GastroVision: A Multi-class Endoscopy Image Dataset for Computer Aided Gastrointestinal Disease Detection"></a>GastroVision: A Multi-class Endoscopy Image Dataset for Computer Aided Gastrointestinal Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08140">http://arxiv.org/abs/2307.08140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/debeshjha/gastrovision">https://github.com/debeshjha/gastrovision</a></li>
<li>paper_authors: Debesh Jha, Vanshali Sharma, Neethi Dasu, Nikhil Kumar Tomar, Steven Hicks, M. K. Bhuyan, Pradip K. Das, Michael A. Riegler, Pål Halvorsen, Ulas Bagci, Thomas de Lange<br>for:这个研究是为了解决融合实时人工智能（AI）系统在临床实践中的挑战，包括扩展和acceptability。methods:这个研究使用了多中心开放存取的胃肠综合镜像数据集（GastroVision），包括不同的解剖特征、病理异常、肿瘤移除 caso和正常找到（总共27个类别）的胃肠道。数据集包括来自挪威巴鲁姆医院和瑞典卡罗琳斯卡大学医院的8,000幅照片，并由经验轻肠综合医生进行标注和验证。results:我们 validate了我们的数据集的重要性，使用了广泛的benchmarking，基于受欢迎的深度学习基础模型。我们相信我们的数据集可以促进AI基于算法的胃肠疾病检测和分类的发展。我们的数据集可以在 \url{<a target="_blank" rel="noopener" href="https://osf.io/84e7f/%7D">https://osf.io/84e7f/}</a> 上获取。<details>
<summary>Abstract</summary>
Integrating real-time artificial intelligence (AI) systems in clinical practices faces challenges such as scalability and acceptance. These challenges include data availability, biased outcomes, data quality, lack of transparency, and underperformance on unseen datasets from different distributions. The scarcity of large-scale, precisely labeled, and diverse datasets are the major challenge for clinical integration. This scarcity is also due to the legal restrictions and extensive manual efforts required for accurate annotations from clinicians. To address these challenges, we present \textit{GastroVision}, a multi-center open-access gastrointestinal (GI) endoscopy dataset that includes different anatomical landmarks, pathological abnormalities, polyp removal cases and normal findings (a total of 27 classes) from the GI tract. The dataset comprises 8,000 images acquired from B{\ae}rum Hospital in Norway and Karolinska University Hospital in Sweden and was annotated and verified by experienced GI endoscopists. Furthermore, we validate the significance of our dataset with extensive benchmarking based on the popular deep learning based baseline models. We believe our dataset can facilitate the development of AI-based algorithms for GI disease detection and classification. Our dataset is available at \url{https://osf.io/84e7f/}.
</details>
<details>
<summary>摘要</summary>
临床应用人工智能（AI）系统整合面临挑战，包括可扩展性和接受性。这些挑战包括数据可用性、结果偏见、数据质量、透明度不足和不同分布下的性能下降。医疗数据的罕见性是临床整合的主要挑战之一，这也是由于法律限制和精度的手动准备所致。为解决这些挑战，我们介绍了《胃视》，一个多中心开放访问胃肠细胞图像数据集，包括胃肠脏器的不同解剖特征、疾病畸形、肿瘤除除例和正常发现（总共27个类）。该数据集包括8,000张从挪威布莱姆医院和瑞典卡罗琳斯卡大学医院所获取的图像，由经验丰富的胃肠镜头医生进行了标注和验证。此外，我们还验证了我们的数据集的重要性，通过基于深度学习的标准模型的比较。我们认为，我们的数据集可以促进基于AI的胃肠疾病检测和分类算法的发展。我们的数据集可以在 <https://osf.io/84e7f/> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Neural-Orientation-Distribution-Fields-for-Estimation-and-Uncertainty-Quantification-in-Diffusion-MRI"><a href="#Neural-Orientation-Distribution-Fields-for-Estimation-and-Uncertainty-Quantification-in-Diffusion-MRI" class="headerlink" title="Neural Orientation Distribution Fields for Estimation and Uncertainty Quantification in Diffusion MRI"></a>Neural Orientation Distribution Fields for Estimation and Uncertainty Quantification in Diffusion MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08138">http://arxiv.org/abs/2307.08138</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Consagra, Lipeng Ning, Yogesh Rathi</li>
<li>for: 这篇论文主要是用于描述一种新的深度学习方法，用于精确地估算 diffusion MRI（dMRI）信号中的方向分布函数（ODF）。</li>
<li>methods: 该方法使用神经网络（NF）来 parameterize一种随机列表表示的秘密 ODF 场，并通过显式地模型数据中的空间相关性结构，以提高精度和效率。</li>
<li>results: 对于 synthetic 和实际的 in-vivo diffusion数据，该方法与现有方法相比，具有更高的精度和更低的不确定性。<details>
<summary>Abstract</summary>
Inferring brain connectivity and structure \textit{in-vivo} requires accurate estimation of the orientation distribution function (ODF), which encodes key local tissue properties. However, estimating the ODF from diffusion MRI (dMRI) signals is a challenging inverse problem due to obstacles such as significant noise, high-dimensional parameter spaces, and sparse angular measurements. In this paper, we address these challenges by proposing a novel deep-learning based methodology for continuous estimation and uncertainty quantification of the spatially varying ODF field. We use a neural field (NF) to parameterize a random series representation of the latent ODFs, implicitly modeling the often ignored but valuable spatial correlation structures in the data, and thereby improving efficiency in sparse and noisy regimes. An analytic approximation to the posterior predictive distribution is derived which can be used to quantify the uncertainty in the ODF estimate at any spatial location, avoiding the need for expensive resampling-based approaches that are typically employed for this purpose. We present empirical evaluations on both synthetic and real in-vivo diffusion data, demonstrating the advantages of our method over existing approaches.
</details>
<details>
<summary>摘要</summary>
推断脑内连接和结构需要准确地估计Diffusion MRI（dMRI）信号中的方向分布函数（ODF），该函数包含脑组织重要的地方性特性。然而，从dMRI信号中估计ODF是一个困难的反向问题，因为存在干扰、高维度参数空间和缺乏方向测量的问题。在本文中，我们解决这些挑战，提出了一种基于深度学习的方法，用于连续地估计和评估空间变化的ODF场。我们使用神经场（NF）来参数化 latent ODFs 的随机列表表示，间接地模拟了通常被忽略的但有价值的空间相关结构，从而在稀缺和干扰的情况下提高效率。我们 Derive 一个analytic approximation to the posterior predictive distribution，可以用来评估 ODF 估计中任何空间位置的不确定性，避免使用常见的重新采样基本方法。我们在 synthetic 和实际的 in vivo  diffusion 数据上进行了实验，并证明了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Untrained-neural-network-embedded-Fourier-phase-retrieval-from-few-measurements"><a href="#Untrained-neural-network-embedded-Fourier-phase-retrieval-from-few-measurements" class="headerlink" title="Untrained neural network embedded Fourier phase retrieval from few measurements"></a>Untrained neural network embedded Fourier phase retrieval from few measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08717">http://arxiv.org/abs/2307.08717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyuan-2000/trad">https://github.com/liyuan-2000/trad</a></li>
<li>paper_authors: Liyuan Ma, Hongxia Wang, Ningyi Leng, Ziyang Yuan</li>
<li>for: 这篇论文旨在解决快速执行 Fourier 频分析 (FPR) 问题，以减少时间和硬件成本。</li>
<li>methods: 该论文提出了一种基于 alternating direction method of multipliers (ADMM) 框架的无经验神经网络 (NN) 嵌入算法，用于解决 FPR 问题。</li>
<li>results: 实验结果表明，该算法在计算资源少的情况下表现更好，甚至可以与经过训练的神经网络 (NN) 算法竞争。<details>
<summary>Abstract</summary>
Fourier phase retrieval (FPR) is a challenging task widely used in various applications. It involves recovering an unknown signal from its Fourier phaseless measurements. FPR with few measurements is important for reducing time and hardware costs, but it suffers from serious ill-posedness. Recently, untrained neural networks have offered new approaches by introducing learned priors to alleviate the ill-posedness without requiring any external data. However, they may not be ideal for reconstructing fine details in images and can be computationally expensive. This paper proposes an untrained neural network (NN) embedded algorithm based on the alternating direction method of multipliers (ADMM) framework to solve FPR with few measurements. Specifically, we use a generative network to represent the image to be recovered, which confines the image to the space defined by the network structure. To improve the ability to represent high-frequency information, total variation (TV) regularization is imposed to facilitate the recovery of local structures in the image. Furthermore, to reduce the computational cost mainly caused by the parameter updates of the untrained NN, we develop an accelerated algorithm that adaptively trades off between explicit and implicit regularization. Experimental results indicate that the proposed algorithm outperforms existing untrained NN-based algorithms with fewer computational resources and even performs competitively against trained NN-based algorithms.
</details>
<details>
<summary>摘要</summary>
法ouveau频段恢复（FPR）是一项广泛应用的复杂任务，涉及于从傅里叶频域无法量测数据中恢复未知信号。FPR WITH few measurements是一项重要的应用，可以降低时间和硬件成本，但它受到严重的不定性困难。最近，无经过训练的神经网络（NN）已经提供了新的方法，通过引入学习的约束来缓解不定性，不需要任何外部数据。然而，它们可能无法完美地复制图像中的细节，并且可能具有高计算成本。这篇论文提出了一种无经过训练NN嵌入算法，基于 alternating direction method of multipliers（ADMM）框架来解决FPR WITH few measurements。特别是，我们使用一个生成网络来表示要恢复的图像，这使得图像受到生成网络的结构所限制。为了提高图像中高频信息的恢复，我们添加了总变量（TV）正则化，以便促进图像中的本地结构的恢复。此外，为了降低主要由无经过训练NN的参数更新所导致的计算成本，我们开发了一种可适应的加速算法，可以自适应地让拥有更多计算资源的计算机进行更多的计算。实验结果表明，我们的算法比现有的无经过训练NN基于算法更高效，甚至可以与经过训练NN基于算法竞争。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/eess.IV_2023_07_17/" data-id="cllta0lj6007zny88cx2b4isy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/16/cs.SD_2023_07_16/" class="article-date">
  <time datetime="2023-07-15T16:00:00.000Z" itemprop="datePublished">2023-07-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/16/cs.SD_2023_07_16/">cs.SD - 2023-07-16 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NoiseBandNet-Controllable-Time-Varying-Neural-Synthesis-of-Sound-Effects-Using-Filterbanks"><a href="#NoiseBandNet-Controllable-Time-Varying-Neural-Synthesis-of-Sound-Effects-Using-Filterbanks" class="headerlink" title="NoiseBandNet: Controllable Time-Varying Neural Synthesis of Sound Effects Using Filterbanks"></a>NoiseBandNet: Controllable Time-Varying Neural Synthesis of Sound Effects Using Filterbanks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08007">http://arxiv.org/abs/2307.08007</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adrianbarahona/noisebandnet">https://github.com/adrianbarahona/noisebandnet</a></li>
<li>paper_authors: Adrián Barahona-Ríos, Tom Collins</li>
<li>for: 本研究旨在提出一种可控制的神经音频合成方法，以便生成具有时间和频率分辨率的各种听起来不同的声音效果。</li>
<li>methods: 该方法使用滤波器链来过滤白噪，从而实现声音效果的生成和控制。</li>
<li>results: 对于十种声音效果的测试，NoiseBandNet得分高于四种变体的DDSP滤波器synthesizer，在九个评价类别中得分更高，表明NoiseBandNet可以生成具有时间和频率分辨率的各种听起来不同的声音效果。<details>
<summary>Abstract</summary>
Controllable neural audio synthesis of sound effects is a challenging task due to the potential scarcity and spectro-temporal variance of the data. Differentiable digital signal processing (DDSP) synthesisers have been successfully employed to model and control musical and harmonic signals using relatively limited data and computational resources. Here we propose NoiseBandNet, an architecture capable of synthesising and controlling sound effects by filtering white noise through a filterbank, thus going further than previous systems that make assumptions about the harmonic nature of sounds. We evaluate our approach via a series of experiments, modelling footsteps, thunderstorm, pottery, knocking, and metal sound effects. Comparing NoiseBandNet audio reconstruction capabilities to four variants of the DDSP-filtered noise synthesiser, NoiseBandNet scores higher in nine out of ten evaluation categories, establishing a flexible DDSP method for generating time-varying, inharmonic sound effects of arbitrary length with both good time and frequency resolution. Finally, we introduce some potential creative uses of NoiseBandNet, by generating variations, performing loudness transfer, and by training it on user-defined control curves.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified Chinese� controllable neural audio synthesis of sound effects is a challenging task due to the potential scarcity and spectro-temporal variance of the data. Differentiable digital signal processing (DDSP) synthesisers have been successfully employed to model and control musical and harmonic signals using relatively limited data and computational resources. Here we propose NoiseBandNet, an architecture capable of synthesising and controlling sound effects by filtering white noise through a filterbank, thus going further than previous systems that make assumptions about the harmonic nature of sounds. We evaluate our approach via a series of experiments, modelling footsteps, thunderstorm, pottery, knocking, and metal sound effects. Comparing NoiseBandNet audio reconstruction capabilities to four variants of the DDSP-filtered noise synthesiser, NoiseBandNet scores higher in nine out of ten evaluation categories, establishing a flexible DDSP method for generating time-varying, inharmonic sound effects of arbitrary length with both good time and frequency resolution. Finally, we introduce some potential creative uses of NoiseBandNet, by generating variations, performing loudness transfer, and by training it on user-defined control curves.Translation:控制可能的神经音频合成声效是一个挑战性的任务，因为声效数据的可能性和spectro-temporal variance很大。 diferenciable digital signal processing（DDSP）Synthesisers have been successfully employed to model and control musical and harmonic signals using relatively limited data and computational resources. 我们提议NoiseBandNet，一种可以通过filterbank filtering white noise来实现和控制声效的架构。这超过了之前的系统，它们假设声效的和谐性。我们通过一系列实验，模拟了踏步、雨天、陶艺、打击和金属声效。 Comparing NoiseBandNet的声音重建能力与四种DDSP滤波器处理的噪声合成器，NoiseBandNet在十个评价类别中得分高于其他四个， Establishing a flexible DDSP method for generating time-varying, inharmonic sound effects of arbitrary length with both good time and frequency resolution。最后，我们介绍了一些可能的创造性使用NoiseBandNet，如生成变化、卷积传递和用户定义的控制曲线。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/16/cs.SD_2023_07_16/" data-id="cllta0lhz0041ny88bac7gyjm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/16/eess.AS_2023_07_16/" class="article-date">
  <time datetime="2023-07-15T16:00:00.000Z" itemprop="datePublished">2023-07-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/16/eess.AS_2023_07_16/">eess.AS - 2023-07-16 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Model-Adaptation-for-ASR-in-low-resource-Indian-Languages"><a href="#Model-Adaptation-for-ASR-in-low-resource-Indian-Languages" class="headerlink" title="Model Adaptation for ASR in low-resource Indian Languages"></a>Model Adaptation for ASR in low-resource Indian Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07948">http://arxiv.org/abs/2307.07948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhayjeet Singh, Arjun Singh Mehta, Ashish Khuraishi K S, Deekshitha G, Gauri Date, Jai Nanavati, Jesuraja Bandekar, Karnalius Basumatary, Karthika P, Sandhya Badiger, Sathvik Udupa, Saurabh Kumar, Savitha, Prasanta Kumar Ghosh, Prashanthi V, Priyanka Pai, Raoul Nanavati, Rohan Saxena, Sai Praneeth Reddy Mora, Srinivasa Raghavan</li>
<li>for: The paper aims to improve automatic speech recognition (ASR) performance for low-resource languages, specifically Indian languages like Bengali and Bhojpuri.</li>
<li>methods: The paper uses self-supervised learning (SSL) based acoustic models like wav2vec2 and large-scale multi-lingual training like Whisper, and explores the use of adaptation and fine-tuning techniques to overcome the low-resource nature of the data.</li>
<li>results: The paper aims to understand the importance of each modality (acoustics and text) in building a reliable ASR system for low-resource languages, and to explore the applicability of these approaches to various languages spoken around the world.<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) performance has improved drastically in recent years, mainly enabled by self-supervised learning (SSL) based acoustic models such as wav2vec2 and large-scale multi-lingual training like Whisper. A huge challenge still exists for low-resource languages where the availability of both audio and text is limited. This is further complicated by the presence of multiple dialects like in Indian languages. However, many Indian languages can be grouped into the same families and share the same script and grammatical structure. This is where a lot of adaptation and fine-tuning techniques can be applied to overcome the low-resource nature of the data by utilising well-resourced similar languages.   In such scenarios, it is important to understand the extent to which each modality, like acoustics and text, is important in building a reliable ASR. It could be the case that an abundance of acoustic data in a language reduces the need for large text-only corpora. Or, due to the availability of various pretrained acoustic models, the vice-versa could also be true. In this proposed special session, we encourage the community to explore these ideas with the data in two low-resource Indian languages of Bengali and Bhojpuri. These approaches are not limited to Indian languages, the solutions are potentially applicable to various languages spoken around the world.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）性能在最近几年内有了惊人的提升，主要归功于基于自我超级学习（SSL）的声音模型，如wave2vec2以及大规模多语言训练如Whisper。然而，低资源语言仍然存在巨大的挑战，主要是因为语音和文本数据的可用性受限。这更加复杂，因为印度语言有多种方言。然而，许多印度语言可以分组，并且共享同一个字母和 grammatical structure。这使得可以应用大量的适应和精度调整技术来缓解低资源数据的问题，使用已有的资源更加有利。在这个特别会议中，我们邀请社区探讨以下想法：使用声音和文本Modalities 之间的关系来构建可靠的 ASR。可能是，一个语言有充足的声音数据，可以减少文本 corpora 的需求。或者，由于各种预训练声音模型的可用性，可以相反的情况。我们鼓励社区在孟买利语和帕雷语两种低资源印度语言中进行研究。这些方法不仅适用于印度语言，而且可能适用于世界各地的语言。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/16/eess.AS_2023_07_16/" data-id="cllta0lil0061ny88230j8yhx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/16/eess.IV_2023_07_16/" class="article-date">
  <time datetime="2023-07-15T16:00:00.000Z" itemprop="datePublished">2023-07-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/16/eess.IV_2023_07_16/">eess.IV - 2023-07-16 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TransNuSeg-A-Lightweight-Multi-Task-Transformer-for-Nuclei-Segmentation"><a href="#TransNuSeg-A-Lightweight-Multi-Task-Transformer-for-Nuclei-Segmentation" class="headerlink" title="TransNuSeg: A Lightweight Multi-Task Transformer for Nuclei Segmentation"></a>TransNuSeg: A Lightweight Multi-Task Transformer for Nuclei Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08051">http://arxiv.org/abs/2307.08051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenqi-he/transnuseg">https://github.com/zhenqi-he/transnuseg</a></li>
<li>paper_authors: Zhenqi He, Mathias Unberath, Jing Ke, Yiqing Shen</li>
<li>for: 这篇论文是为了提出一种基于Transformer的 nuclei segmentation方法，以解决现有的自动 nuclei segmentation方法具有较高的参数数量和训练时间。</li>
<li>methods: 这篇论文使用了一种叫做TransNuSeg的pure Transformer框架，其中包括了一个tri-decoder结构，用于同时进行nuclei实例、nuclei边缘和分布式边缘分割。此外， authors还提出了一种新的自适应loss函数，以确保不同分支的预测结果之间的一致性。</li>
<li>results: 实验表明，TransNuSeg方法可以在两个不同的数据集上，与state-of-the-art counterparts such as CA2.5-Net比较，提高了2-3%的Dice指标，同时减少了30%的参数数量。这表明，Transformer在nuclei segmentation领域中具有强大的能力，可以作为实际临床应用中的有效解决方案。<details>
<summary>Abstract</summary>
Nuclei appear small in size, yet, in real clinical practice, the global spatial information and correlation of the color or brightness contrast between nuclei and background, have been considered a crucial component for accurate nuclei segmentation. However, the field of automatic nuclei segmentation is dominated by Convolutional Neural Networks (CNNs), meanwhile, the potential of the recently prevalent Transformers has not been fully explored, which is powerful in capturing local-global correlations. To this end, we make the first attempt at a pure Transformer framework for nuclei segmentation, called TransNuSeg. Different from prior work, we decouple the challenging nuclei segmentation task into an intrinsic multi-task learning task, where a tri-decoder structure is employed for nuclei instance, nuclei edge, and clustered edge segmentation respectively. To eliminate the divergent predictions from different branches in previous work, a novel self distillation loss is introduced to explicitly impose consistency regulation between branches. Moreover, to formulate the high correlation between branches and also reduce the number of parameters, an efficient attention sharing scheme is proposed by partially sharing the self-attention heads amongst the tri-decoders. Finally, a token MLP bottleneck replaces the over-parameterized Transformer bottleneck for a further reduction in model complexity. Experiments on two datasets of different modalities, including MoNuSeg have shown that our methods can outperform state-of-the-art counterparts such as CA2.5-Net by 2-3% Dice with 30% fewer parameters. In conclusion, TransNuSeg confirms the strength of Transformer in the context of nuclei segmentation, which thus can serve as an efficient solution for real clinical practice. Code is available at https://github.com/zhenqi-he/transnuseg.
</details>
<details>
<summary>摘要</summary>
nuclei 看起来很小，但在实际临床实践中，全球空间信息和背景和核或亮度对比的色彩或亮度对比，被视为精度核 segmentation 的关键组成部分。然而，核心 automatic segmentation 领域被 Convolutional Neural Networks (CNNs) 所主导，而 transformer 的潜力尚未得到充分探索，这是强大地捕捉当地-全球对应关系的。为此，我们提出了首个纯 transformer 框架，称为 TransNuSeg。与先前的工作不同，我们将挑战性的核 segmentation 任务分解成内在多任务学习任务，其中使用 tri-decoder 结构进行核实例、核边和集群边 segmentation 等。为了消除先前工作中分支的不一致预测，我们引入了一种新的自我抽象损失函数，以显式地强制分支之间的一致性规则。此外，我们还提出了一种高效的注意力共享方案，通过在 tri-decoders 中共享自注意力头来降低模型参数数量。最后，我们将 токен MLP 瓶颈取代了过参数化的 transformer 瓶颈，以进一步降低模型复杂性。在两个不同的模式数据上进行了实验，包括 MoNuSeg，我们的方法可以与 state-of-the-art 对手 CA2.5-Net 相比，提高 Dice 指标2-3%，并且减少参数数量30%。结论：TransNuSeg 证明了 transformer 在核 segmentation 上的力量，这些可以作为实际临床实践中的高效解决方案。代码可以在 <https://github.com/zhenqi-he/transnuseg> 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-SLCA-UNet-Architecture-for-Automatic-MRI-Brain-Tumor-Segmentation"><a href="#A-Novel-SLCA-UNet-Architecture-for-Automatic-MRI-Brain-Tumor-Segmentation" class="headerlink" title="A Novel SLCA-UNet Architecture for Automatic MRI Brain Tumor Segmentation"></a>A Novel SLCA-UNet Architecture for Automatic MRI Brain Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08048">http://arxiv.org/abs/2307.08048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tejashwini P S, Thriveni J, Venugopal K R</li>
<li>for: 预测和检测脑肿瘤，以降低因脑肿瘤而导致的死亡率。</li>
<li>methods: 使用深度学习方法，特别是UNet架构，自动化生物医学影像探索工具。</li>
<li>results: 提出了一种修改后的UNet架构，可以有效地捕捉脑肿瘤影像中的粗细特征信息，并在Brain Tumor Segmentation（BraTS）数据集上达到了良好的性能，具体表现为0.845、0.845、0.999和8.1等指标。<details>
<summary>Abstract</summary>
Brain tumor is deliberated as one of the severe health complications which lead to decrease in life expectancy of the individuals and is also considered as a prominent cause of mortality worldwide. Therefore, timely detection and prediction of brain tumors can be helpful to prevent death rates due to brain tumors. Biomedical image analysis is a widely known solution to diagnose brain tumor. Although MRI is the current standard method for imaging tumors, its clinical usefulness is constrained by the requirement of manual segmentation which is time-consuming. Deep learning-based approaches have emerged as a promising solution to develop automated biomedical image exploration tools and the UNet architecture is commonly used for segmentation. However, the traditional UNet has limitations in terms of complexity, training, accuracy, and contextual information processing. As a result, the modified UNet architecture, which incorporates residual dense blocks, layered attention, and channel attention modules, in addition to stacked convolution, can effectively capture both coarse and fine feature information. The proposed SLCA UNet approach achieves good performance on the freely accessible Brain Tumor Segmentation (BraTS) dataset, with an average performance of 0.845, 0.845, 0.999, and 8.1 in terms of Dice, Sensitivity, Specificity, and Hausdorff95 for BraTS 2020 dataset, respectively.
</details>
<details>
<summary>摘要</summary>
脑肿是一种严重的健康问题，可能导致个体寿命下降，并被认为是全球致死率的一大原因。因此，在时间上掌握和预测脑肿的诊断是非常重要的。生物医学图像分析是一种广泛应用的解决方案，但现有的MRI技术受到手动 segmentation 的限制，这是耗时consuming。深度学习基本单元（Deep Learning-based Approaches）已经出现为开发自动生物医学图像探索工具的有力的解决方案之一。然而，传统的 UNet  Architecture 受到复杂性、训练、准确率和上下文信息处理等限制。为此，我们提出了修改后的 UNet 架构，包括循环堆叠、层次注意力和渠道注意力模块，可以有效地捕捉粗细特征信息。我们的 SLCA UNet 方法在公共可用的 Brain Tumor Segmentation（BraTS）数据集上达到了良好的性能，其中 BraTS 2020 数据集的平均性能为 0.845、0.845、0.999 和 8.1 分别在 Dice、敏感性、特异性和 Hausdorff95 方面。
</details></li>
</ul>
<hr>
<h2 id="SHAMSUL-Simultaneous-Heatmap-Analysis-to-investigate-Medical-Significance-Utilizing-Local-interpretability-methods"><a href="#SHAMSUL-Simultaneous-Heatmap-Analysis-to-investigate-Medical-Significance-Utilizing-Local-interpretability-methods" class="headerlink" title="SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods"></a>SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08003">http://arxiv.org/abs/2307.08003</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anondo1969/shamsul">https://github.com/anondo1969/shamsul</a></li>
<li>paper_authors: Mahbub Ul Alam, Jaakko Hollmén, Jón Rúnar Baldvinsson, Rahim Rahmani</li>
<li>For: This paper aims to improve the interpretability of deep neural networks in the medical and healthcare domain by applying and comparing four well-established interpretability methods (LIME, SHAP, Grad-CAM, and LRP) to a chest radiography dataset.* Methods: The paper uses transfer learning and a multi-label-multi-class chest radiography dataset to interpret predictions pertaining to specific pathology classes. The authors evaluate the performance of the four interpretability methods through quantitative and qualitative investigations, and compare the results against human expert annotation.* Results: The paper finds that Grad-CAM demonstrates the most favorable performance in quantitative evaluation, while the LIME heatmap segmentation visualization exhibits the highest level of medical significance. The research highlights the strengths and limitations of the four interpretability methods and suggests that a multimodal-based approach could offer additional insights for enhancing interpretability in the medical domain.Here is the same information in Simplified Chinese text:* For: 本研究旨在提高深度神经网络在医疗领域的解释性，通过应用和比较四种已知的解释方法（LIME、SHAP、Grad-CAM、LRP）来解释特定疾病类型的预测结果。* Methods: 本研究使用了传输学习和多类多标签的胸部X射线数据集来解释特定疾病类型的预测结果。作者们通过量化和质量调查来评估四种解释方法的性能，并与人工专家标注进行比较。* Results: 研究发现，Grad-CAM在量化评估中表现最佳，而 LIME 热图分割视觉化显示最高的医学意义。研究揭示了四种解释方法的优缺点，并建议在医疗领域使用多Modal 基于的方法可以提供更多的解释。<details>
<summary>Abstract</summary>
The interpretability of deep neural networks has become a subject of great interest within the medical and healthcare domain. This attention stems from concerns regarding transparency, legal and ethical considerations, and the medical significance of predictions generated by these deep neural networks in clinical decision support systems. To address this matter, our study delves into the application of four well-established interpretability methods: Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations (SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise Relevance Propagation (LRP). Leveraging the approach of transfer learning with a multi-label-multi-class chest radiography dataset, we aim to interpret predictions pertaining to specific pathology classes. Our analysis encompasses both single-label and multi-label predictions, providing a comprehensive and unbiased assessment through quantitative and qualitative investigations, which are compared against human expert annotation. Notably, Grad-CAM demonstrates the most favorable performance in quantitative evaluation, while the LIME heatmap segmentation visualization exhibits the highest level of medical significance. Our research highlights the strengths and limitations of these interpretability methods and suggests that a multimodal-based approach, incorporating diverse sources of information beyond chest radiography images, could offer additional insights for enhancing interpretability in the medical domain.
</details>
<details>
<summary>摘要</summary>
《深度神经网络可读性的研究在医疗领域引发了广泛的关注，主要是由于透明度、法律和伦理考虑以及在临床决策支持系统中神经网络预测的医学意义。为解决这个问题，我们的研究探讨了四种已有的可读性方法：本地可读性模型自定义解释（LIME）、Shapley添加itive exPlanations（SHAP）、梯度权重分类活动映射（Grad-CAM）和层次 relevance propagation（LRP）。通过将这些方法应用于一个多标签多类胸部X射像数据集，我们想要解释具体疾病类型的预测结果。我们的分析包括单标签和多标签预测，并通过量化和质量调查对比人工专家标注进行了全面和无偏评估。结果显示，Grad-CAM在量化评估中表现最佳，而LIME热图分 segmentation 可读性方法显示最高水平的医学意义。我们的研究描述了这些可读性方法的优缺点，并表明在医疗领域可能需要结合多种信息源以获得更多的解释。》
</details></li>
</ul>
<hr>
<h2 id="MoTIF-Learning-Motion-Trajectories-with-Local-Implicit-Neural-Functions-for-Continuous-Space-Time-Video-Super-Resolution"><a href="#MoTIF-Learning-Motion-Trajectories-with-Local-Implicit-Neural-Functions-for-Continuous-Space-Time-Video-Super-Resolution" class="headerlink" title="MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution"></a>MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07988">http://arxiv.org/abs/2307.07988</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sichun233746/motif">https://github.com/sichun233746/motif</a></li>
<li>paper_authors: Yi-Hsin Chen, Si-Cun Chen, Yi-Hsin Chen, Yen-Yu Lin, Wen-Hsiao Peng</li>
<li>for: 这篇论文的目的是提出一种能够在任意扩大比例下提高视频的空间时间超分辨率（C-STVSR）技术。</li>
<li>methods: 该技术使用了一种空间时间本地隐藏函数，可以学习输入视频帧之间的前进动态信息。该函数有着学习前进动态信息的特点，而不是学习一个混合动态信息的混合函数。为了使得动态信息 interpolate 更加容易，该技术使用了从输入视频中提取的稀疏样本前进动态信息作为上下文输入。</li>
<li>results: 该技术在C-STVSR领域实现了状态机器的性能记录，并提供了一个可用的源代码MoTIF。<details>
<summary>Abstract</summary>
This work addresses continuous space-time video super-resolution (C-STVSR) that aims to up-scale an input video both spatially and temporally by any scaling factors. One key challenge of C-STVSR is to propagate information temporally among the input video frames. To this end, we introduce a space-time local implicit neural function. It has the striking feature of learning forward motion for a continuum of pixels. We motivate the use of forward motion from the perspective of learning individual motion trajectories, as opposed to learning a mixture of motion trajectories with backward motion. To ease motion interpolation, we encode sparsely sampled forward motion extracted from the input video as the contextual input. Along with a reliability-aware splatting and decoding scheme, our framework, termed MoTIF, achieves the state-of-the-art performance on C-STVSR. The source code of MoTIF is available at https://github.com/sichun233746/MoTIF.
</details>
<details>
<summary>摘要</summary>
这个工作Addresses continuous space-time video super-resolution (C-STVSR)，它的目标是通过任何缩放因子将输入视频 both spatially and temporally up-scale。一个关键挑战是在输入视频帧之间传递信息。为此，我们引入了一个空间时本地隐藏神经函数。它有突出的特点是学习输入视频帧中的前进运动。我们从输入视频的动作轨迹学习的角度出发，而不是学习混合动作轨迹中的后向运动。为了简化运动插值，我们将输入视频中稀疏样本的前进运动编码为上下文输入。与一种可靠性感知扩散和解码方案相结合，我们的框架，称之为MoTIF，实现了C-STVSR领域的状态级性能。MoTIF的源代码可以在https://github.com/sichun233746/MoTIF上获取。
</details></li>
</ul>
<hr>
<h2 id="Panoramic-Voltage-Sensitive-Optical-Mapping-of-Contracting-Hearts-using-Cooperative-Multi-View-Motion-Tracking-with-12-to-24-Cameras"><a href="#Panoramic-Voltage-Sensitive-Optical-Mapping-of-Contracting-Hearts-using-Cooperative-Multi-View-Motion-Tracking-with-12-to-24-Cameras" class="headerlink" title="Panoramic Voltage-Sensitive Optical Mapping of Contracting Hearts using Cooperative Multi-View Motion Tracking with 12 to 24 Cameras"></a>Panoramic Voltage-Sensitive Optical Mapping of Contracting Hearts using Cooperative Multi-View Motion Tracking with 12 to 24 Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07943">http://arxiv.org/abs/2307.07943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shrey Chowdhary, Jan Lebert, Shai Dickman, Jan Christoph</li>
<li>for: 这研究用于图像心脏的活动电位波，以高空间和时间分辨率观察心脏表面的变形。</li>
<li>methods: 这种多摄像头光学映射技术使用24个高速低成本摄像头，可以在整个扭形的心脏表面上图像活动电位波。</li>
<li>results: 研究发现可以使用12个摄像头获得0.5-1.0兆Pixel的合并分辨率，并使用计算机视觉技术进行三维协同多视图动态重建和高分辨率电子敏感测量。通过这种设置，研究者在兔心中测量到了不同心律rhythm中的活动电位波，包括正常律 rhythm、脉冲心律和心肺综合症。这种设置定义了现有技术的新状态，可以用于研究心脏的电机动力学 dynamics during health和疾病。<details>
<summary>Abstract</summary>
Action potential waves triggering the heart's contractions can be imaged at high spatial and temporal resolutions across the heart surface using voltage-sensitive optical mapping. However, for over three decades, optical mapping has been performed with contraction-inhibited hearts. While it was recently demonstrated that action potential waves can be imaged on parts of the three-dimensional deforming ventricular surface using multi-camera optical mapping, panoramic measurements of action potential waves across the entire beating heart surface remained elusive. Here, we introduce a high-resolution multi-camera optical mapping system consisting of up to 24 high-speed, low-cost cameras with which it is possible to image action potential waves at high resolutions on the entire, strongly deforming ventricular surface of the heart. We imaged isolated hearts inside a custom-designed soccerball-shaped imaging chamber, which facilitates imaging and even illumination with excitation light from all sides in a panoramic fashion. We found that it is possible to image the entire ventricular surface using 12 cameras with 0.5-1.0 megapixels combined resolution. The 12 calibrated cameras generate 1.5 gigabytes of video data per second at imaging speeds of 500 fps, which we process using various computer vision techniques, including three-dimensional cooperative multi-view motion tracking, to generate three-dimensional dynamic reconstructions of the deforming heart surface with corresponding high-resolution voltage-sensitive optical measurements. With our setup, we measured action potential waves at unprecedented resolutions on the contracting three-dimensional surface of rabbit hearts during sinus rhythm, paced rhythm as well as ventricular fibrillation. Our imaging setup defines a new state-of-the-art in the field and can be used to study the heart's electromechanical dynamics during health and disease.
</details>
<details>
<summary>摘要</summary>
心脏的刺激波可以通过电容性光Mapping在心脏表面上获得高空间和时间分辨率的刺激波图像。然而，在过去三十年内，光Mapping都是使用干扰心脏的方法进行的。而现在，我们已经成功地在三维凹陷心脏表面上获得刺激波图像，但这些图像仅限于部分心脏表面。在本文中，我们介绍了一种高分辨率多camera光Mapping系统，该系统由24个高速、低成本摄像头组成，可以在整个弯曲的心脏表面上获得刺激波图像。我们使用自定义的足球形封装室进行心脏封装，以便从所有方向进行扫描和照明。我们发现，使用12个0.5-1.0 megapixels摄像头可以获得整个心脏表面的图像。这12个可 kalibrated摄像头每秒钟生成1.5 gigabytes的视频数据，我们使用了多种计算机视觉技术，包括三维合作多视图运动跟踪，来生成三维动态重建三维凹陷心脏表面的高分辨率电容性光测量。通过我们的设置，我们在各种心脏rhythm中测量了刺激波的历史最高分辨率图像。我们的捕捉设置定义了心脏研究领域的新状态符。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/16/eess.IV_2023_07_16/" data-id="cllta0lj4007tny884dhrfhbl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/16/cs.LG_2023_07_16/" class="article-date">
  <time datetime="2023-07-15T16:00:00.000Z" itemprop="datePublished">2023-07-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/16/cs.LG_2023_07_16/">cs.LG - 2023-07-16 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dataset-Distillation-Meets-Provable-Subset-Selection"><a href="#Dataset-Distillation-Meets-Provable-Subset-Selection" class="headerlink" title="Dataset Distillation Meets Provable Subset Selection"></a>Dataset Distillation Meets Provable Subset Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08086">http://arxiv.org/abs/2307.08086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murad Tukan, Alaa Maalouf, Margarita Osadchy</li>
<li>for: 提高 dataset distillation 的效果，减少数据量和计算成本。</li>
<li>methods: 使用 sampling-based 方法初始化 distilled set，并在训练过程中使用 importance 定义来选择数据集。</li>
<li>results: 实验结果表明，我们的方法可以启用 exiting dataset distillation 技术，并提高其性能。<details>
<summary>Abstract</summary>
Deep learning has grown tremendously over recent years, yielding state-of-the-art results in various fields. However, training such models requires huge amounts of data, increasing the computational time and cost. To address this, dataset distillation was proposed to compress a large training dataset into a smaller synthetic one that retains its performance -- this is usually done by (1) uniformly initializing a synthetic set and (2) iteratively updating/learning this set according to a predefined loss by uniformly sampling instances from the full data. In this paper, we improve both phases of dataset distillation: (1) we present a provable, sampling-based approach for initializing the distilled set by identifying important and removing redundant points in the data, and (2) we further merge the idea of data subset selection with dataset distillation, by training the distilled set on ``important'' sampled points during the training procedure instead of randomly sampling the next batch. To do so, we define the notion of importance based on the relative contribution of instances with respect to two different loss functions, i.e., one for the initialization phase (a kernel fitting function for kernel ridge regression and $K$-means based loss function for any other distillation method), and the relative cross-entropy loss (or any other predefined loss) function for the training phase. Finally, we provide experimental results showing how our method can latch on to existing dataset distillation techniques and improve their performance.
</details>
<details>
<summary>摘要</summary>
深度学习在最近几年内发展 extremely rapidly，在不同领域取得了状态的艺术 Results。然而，训练这些模型需要巨量数据和计算资源，这导致了训练时间和成本的增加。为解决这个问题，人们提出了数据集缩写，将大量的训练数据缩写成一个更小的合成数据集，保持其性能。在这篇论文中，我们改进了两个数据集缩写阶段：1. 我们提出了一种可证明的抽样方法，通过重要性分析和减少数据中的重复项来初始化缩写集。2. 我们进一步将数据subset选择纳入数据集缩写，在训练过程中使用“重要”的抽样点训练缩写集而不是随机抽样下一个批。我们定义了“重要”的概念，根据数据点对两个不同的损失函数（一个是 kernel ridge regression 和 $K$-means 基于损失函数，另一个是距离损失函数）的相对贡献来确定。最后，我们提供了实验结果，证明我们的方法可以与现有的数据集缩写技术相结合，提高其性能。
</details></li>
</ul>
<hr>
<h2 id="POMDP-inference-and-robust-solution-via-deep-reinforcement-learning-An-application-to-railway-optimal-maintenance"><a href="#POMDP-inference-and-robust-solution-via-deep-reinforcement-learning-An-application-to-railway-optimal-maintenance" class="headerlink" title="POMDP inference and robust solution via deep reinforcement learning: An application to railway optimal maintenance"></a>POMDP inference and robust solution via deep reinforcement learning: An application to railway optimal maintenance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08082">http://arxiv.org/abs/2307.08082</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giarcieri/robust-optimal-maintenance-planning-through-reinforcement-learning-and-rllib">https://github.com/giarcieri/robust-optimal-maintenance-planning-through-reinforcement-learning-and-rllib</a></li>
<li>paper_authors: Giacomo Arcieri, Cyprien Hoelzl, Oliver Schwery, Daniel Straub, Konstantinos G. Papakonstantinou, Eleni Chatzi</li>
<li>for: 这项研究的目的是提出一种 combining framework for inference and robust solution of POMDPs via deep RL, 用于解决复杂的顺序决策问题在不确定环境中。</li>
<li>methods: 该framework包括使用Markov Chain Monte Carlo sampling来 JOINTLYINFER transition和observation模型参数，然后使用深度学习技术解决POMDP问题，并通过域随机化将参数 Distributions incorporated into the solution。</li>
<li>results: 该研究 comparing the use of transformers和long short-term memory networks，以及model-based&#x2F;model-free hybrid approach，并应用于实际世界的轨道资产维护规划问题。<details>
<summary>Abstract</summary>
Partially Observable Markov Decision Processes (POMDPs) can model complex sequential decision-making problems under stochastic and uncertain environments. A main reason hindering their broad adoption in real-world applications is the lack of availability of a suitable POMDP model or a simulator thereof. Available solution algorithms, such as Reinforcement Learning (RL), require the knowledge of the transition dynamics and the observation generating process, which are often unknown and non-trivial to infer. In this work, we propose a combined framework for inference and robust solution of POMDPs via deep RL. First, all transition and observation model parameters are jointly inferred via Markov Chain Monte Carlo sampling of a hidden Markov model, which is conditioned on actions, in order to recover full posterior distributions from the available data. The POMDP with uncertain parameters is then solved via deep RL techniques with the parameter distributions incorporated into the solution via domain randomization, in order to develop solutions that are robust to model uncertainty. As a further contribution, we compare the use of transformers and long short-term memory networks, which constitute model-free RL solutions, with a model-based/model-free hybrid approach. We apply these methods to the real-world problem of optimal maintenance planning for railway assets.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a combined framework for inferring and solving POMDPs via deep RL. First, all transition and observation model parameters are jointly inferred via Markov Chain Monte Carlo (MCMC) sampling of a hidden Markov model (HMM), conditioned on actions, to recover full posterior distributions from the available data. The POMDP with uncertain parameters is then solved via deep RL techniques, with the parameter distributions incorporated into the solution via domain randomization, to develop solutions that are robust to model uncertainty.As a further contribution, we compare the use of transformers and long short-term memory (LSTM) networks, which constitute model-free RL solutions, with a model-based/model-free hybrid approach. We apply these methods to the real-world problem of optimal maintenance planning for railway assets.
</details></li>
</ul>
<hr>
<h2 id="Flexible-and-efficient-spatial-extremes-emulation-via-variational-autoencoders"><a href="#Flexible-and-efficient-spatial-extremes-emulation-via-variational-autoencoders" class="headerlink" title="Flexible and efficient spatial extremes emulation via variational autoencoders"></a>Flexible and efficient spatial extremes emulation via variational autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08079">http://arxiv.org/abs/2307.08079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Likun Zhang, Xiaoyu Ma, Christopher K. Wikle, Raphaël Huser</li>
<li>for: 用于模elling Complex 的 spatial extremes 性质</li>
<li>methods: 使用 encoding-decoding 结构的 variational autoencoder (extVAE)</li>
<li>results: 可以更快和更高精度地 simulate  spatial extremes 模型输出，并且可以更好地 capture  tail 部分的性质<details>
<summary>Abstract</summary>
Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models such as Gaussian scale mixtures and single-station conditioning models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from. In this paper, we develop a new spatial extremes model that has flexible and non-stationary dependence properties, and we integrate it in the encoding-decoding structure of a variational autoencoder (extVAE). The extVAE can be used as a spatio-temporal emulator that characterizes the distribution of potential mechanistic model output states and produces outputs that have the same properties as the inputs, especially in the tail. Through extensive simulation studies, we show that our extVAE is vastly more time-efficient than traditional Bayesian inference while also outperforming many spatial extremes models with a stationary dependence structure. To further demonstrate the computational power of the extVAE, we analyze a high-resolution satellite-derived dataset of sea surface temperature in the Red Sea, which includes daily measurements at 16703 grid cells.
</details>
<details>
<summary>摘要</summary>
很多现实世界的过程都具有复杂的尾部依赖结构，不能使用传统的 Gaussian 过程来描述。更灵活的 spatial extremes 模型，如 Gaussian scale mixtures 和 single-station conditioning models，具有吸引人的极端依赖性质，但是常常非常困难 fitted 和 simulate from。在这篇论文中，我们开发了一种新的 spatial extremes 模型，具有灵活和非站ary 的依赖性质。我们将其集成到 encoding-decoding 结构中，并将其用作一种 spatio-temporal emulator，可以Characterizes the distribution of potential mechanistic model output states and produces outputs that have the same properties as the inputs, especially in the tail.通过广泛的 simulations 研究，我们表明我们的 extVAE 在时间效率方面远胜传统的 Bayesian inference，同时也能够超越许多 stationary 的 spatial extremes 模型。为了进一步展示 extVAE 的计算能力，我们分析了一个高分辨率的卫星Derived 数据集，包括每天 measurement 的 16703 个网格单元的 sea surface temperature 在红海。
</details></li>
</ul>
<hr>
<h2 id="MaGNAS-A-Mapping-Aware-Graph-Neural-Architecture-Search-Framework-for-Heterogeneous-MPSoC-Deployment"><a href="#MaGNAS-A-Mapping-Aware-Graph-Neural-Architecture-Search-Framework-for-Heterogeneous-MPSoC-Deployment" class="headerlink" title="MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment"></a>MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08065">http://arxiv.org/abs/2307.08065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohanad Odema, Halima Bouzidi, Hamza Ouarnoughi, Smail Niar, Mohammad Abdullah Al Faruque</li>
<li>for: 这 paper 是为了提高图像推理 tasks 的性能和能效性而设计的。</li>
<li>methods: 该 paper 使用了一种叫做 MaGNAS 的图 neural architecture search 框架，该框架可以在多处理器系统上（SoC）上进行图 neural network 的设计和映射。</li>
<li>results:  experiments 表明，使用 MaGNAS 可以在 NVIDIA Xavier AGX 平台上提高图像推理 tasks 的性能和能效性，比基eline 的 GPU-only 部署提高 1.57 倍的延迟速度和 3.38 倍的能效率，同时保持平均的准确率下降 0.11%。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are becoming increasingly popular for vision-based applications due to their intrinsic capacity in modeling structural and contextual relations between various parts of an image frame. On another front, the rising popularity of deep vision-based applications at the edge has been facilitated by the recent advancements in heterogeneous multi-processor Systems on Chips (MPSoCs) that enable inference under real-time, stringent execution requirements. By extension, GNNs employed for vision-based applications must adhere to the same execution requirements. Yet contrary to typical deep neural networks, the irregular flow of graph learning operations poses a challenge to running GNNs on such heterogeneous MPSoC platforms. In this paper, we propose a novel unified design-mapping approach for efficient processing of vision GNN workloads on heterogeneous MPSoC platforms. Particularly, we develop MaGNAS, a mapping-aware Graph Neural Architecture Search framework. MaGNAS proposes a GNN architectural design space coupled with prospective mapping options on a heterogeneous SoC to identify model architectures that maximize on-device resource efficiency. To achieve this, MaGNAS employs a two-tier evolutionary search to identify optimal GNNs and mapping pairings that yield the best performance trade-offs. Through designing a supernet derived from the recent Vision GNN (ViG) architecture, we conducted experiments on four (04) state-of-the-art vision datasets using both (i) a real hardware SoC platform (NVIDIA Xavier AGX) and (ii) a performance/cost model simulator for DNN accelerators. Our experimental results demonstrate that MaGNAS is able to provide 1.57x latency speedup and is 3.38x more energy-efficient for several vision datasets executed on the Xavier MPSoC vs. the GPU-only deployment while sustaining an average 0.11% accuracy reduction from the baseline.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在视觉应用中日益受欢迎，因为它们可以自然地模型图像帧中不同部分之间的结构和上下文关系。另一方面，由于近期的深度视觉应用在边缘得到了广泛的应用，因此GNN在这些应用中必须遵循同样的执行要求。然而，与普通的深度神经网络不同，图学习操作的不规则流动对于运行GNN在多核心系统中带来了挑战。在这篇论文中，我们提出了一种统一的设计映射方法，用于有效地处理视觉GNN工作负荷在多核心系统上。特别是，我们开发了MaGNAS，一个具有Mapping-Aware Graph Neural Architecture Search（MaGNAS）框架。MaGNAS将GNN建立的建筑设计空间与多核心SoC上的可能的映射选项相结合，以便标识最佳的设备资源利用率。为了实现这一点，MaGNAS采用了两层演化搜索，以确定最佳的GNN和映射对的性能交互。通过基于最近的Vision GNN（ViG）架构设计了一个超网，我们在四个state-of-the-art视觉数据集上进行了实验。我们的实验结果表明，MaGNAS能够提供1.57倍的延迟速度提升和3.38倍的能量效率提升，而在NVIDIA Xavier AGX多核心系统上执行视觉数据集时与GPU-only部署相比，保持了0.11%的准确率下降。
</details></li>
</ul>
<hr>
<h2 id="Fast-Quantum-Algorithm-for-Attention-Computation"><a href="#Fast-Quantum-Algorithm-for-Attention-Computation" class="headerlink" title="Fast Quantum Algorithm for Attention Computation"></a>Fast Quantum Algorithm for Attention Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08045">http://arxiv.org/abs/2307.08045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeqi Gao, Zhao Song, Xin Yang, Ruizhe Zhang</li>
<li>for: 大型自然语言处理（NLP）模型（LLMs）的性能表现出色，它们通过高级深度学习技术得到了广泛应用。</li>
<li>methods: 本研究使用Grover搜寻算法来高效 Compute sparse attention computation matrix。</li>
<li>results: 我们的量子算法可以取得 polynomial 的速度提升，并且 attention matrix 具有Extra low-rank 结构，可以帮助获得更快的 LLM 训练算法。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks. These models, powered by advanced deep learning techniques, have revolutionized the field of natural language processing (NLP) and have achieved remarkable results in various language-related tasks.   LLMs have excelled in tasks such as machine translation, sentiment analysis, question answering, text generation, text classification, language modeling, and more. They have proven to be highly effective in capturing complex linguistic patterns, understanding context, and generating coherent and contextually relevant text. The attention scheme plays a crucial role in the architecture of large language models (LLMs). It is a fundamental component that enables the model to capture and utilize contextual information during language processing tasks effectively. Making the attention scheme computation faster is one of the central questions to speed up the LLMs computation. It is well-known that quantum machine has certain computational advantages compared to the classical machine. However, it is currently unknown whether quantum computing can aid in LLM.   In this work, we focus on utilizing Grover's Search algorithm to compute a sparse attention computation matrix efficiently. We achieve a polynomial quantum speed-up over the classical method. Moreover, the attention matrix outputted by our quantum algorithm exhibits an extra low-rank structure that will be useful in obtaining a faster training algorithm for LLMs. Additionally, we present a detailed analysis of the algorithm's error analysis and time complexity within the context of computing the attention matrix.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在各种语言处理任务中表现出色，应用先进的深度学习技术。这些模型在机器翻译、情感分析、问答、文本生成、文本分类、语言模型等任务中均有卓越表现。它们能够吸收复杂的语言模式，理解上下文，并生成具有上下文相关性的文本。模型中的注意力结构是重要的基本 ком成分，它在语言处理任务中实现了效果。目前，尚未知道Quantum computing是否可以应用于LLM。在这个工作中，我们专注于使用Grover搜寻算法计算稀疏注意力computation матриrice，以取得高效的量子速度优化。我们获得了对级数方法的多项式优化，并且注意力矩阵的出力显示了额外的低维结构，这将会帮助获得更快的LLM训练算法。此外，我们还提供了计算注意力矩阵的错误分析和时间复杂度分析。
</details></li>
</ul>
<hr>
<h2 id="Towards-Flexible-Time-to-event-Modeling-Optimizing-Neural-Networks-via-Rank-Regression"><a href="#Towards-Flexible-Time-to-event-Modeling-Optimizing-Neural-Networks-via-Rank-Regression" class="headerlink" title="Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression"></a>Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08044">http://arxiv.org/abs/2307.08044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teboozas/dart_ecai23">https://github.com/teboozas/dart_ecai23</a></li>
<li>paper_authors: Hyunjun Lee, Junhyun Lee, Taehwa Choi, Jaewoo Kang, Sangbum Choi</li>
<li>for: 预测时间事件发生的时间点 (predicting the time of occurrence of an event)</li>
<li>methods: 使用深度学习模型，基于Gehan的排名统计 (using a deep learning model based on Gehan’s rank statistic)</li>
<li>results: 在多个 benchmark 数据集上实现了显著的提升，无需额外参数或复杂的模型架构 (achieved significant improvements on multiple benchmark datasets without additional hyperparameters or complex model architectures)<details>
<summary>Abstract</summary>
Time-to-event analysis, also known as survival analysis, aims to predict the time of occurrence of an event, given a set of features. One of the major challenges in this area is dealing with censored data, which can make learning algorithms more complex. Traditional methods such as Cox's proportional hazards model and the accelerated failure time (AFT) model have been popular in this field, but they often require assumptions such as proportional hazards and linearity. In particular, the AFT models often require pre-specified parametric distributional assumptions. To improve predictive performance and alleviate strict assumptions, there have been many deep learning approaches for hazard-based models in recent years. However, representation learning for AFT has not been widely explored in the neural network literature, despite its simplicity and interpretability in comparison to hazard-focused methods. In this work, we introduce the Deep AFT Rank-regression model for Time-to-event prediction (DART). This model uses an objective function based on Gehan's rank statistic, which is efficient and reliable for representation learning. On top of eliminating the requirement to establish a baseline event time distribution, DART retains the advantages of directly predicting event time in standard AFT models. The proposed method is a semiparametric approach to AFT modeling that does not impose any distributional assumptions on the survival time distribution. This also eliminates the need for additional hyperparameters or complex model architectures, unlike existing neural network-based AFT models. Through quantitative analysis on various benchmark datasets, we have shown that DART has significant potential for modeling high-throughput censored time-to-event data.
</details>
<details>
<summary>摘要</summary>
时间到事分析（也称为存存分析）的目标是预测事件发生的时间， givens 一组特征。 这个领域的一个主要挑战是处理 censored 数据，可以使学习算法更加复杂。传统方法 such as Cox 的对数加速破碎模型和加速失败时间（AFT）模型在这个领域非常受欢迎，但它们经常需要假设，例如对比例的危险和线性。特别是 AF 模型经常需要预先指定的参数分布假设。为了提高预测性能和缓解严格假设，在过去的几年中有很多深度学习方法在预测 hazard-based 模型中得到应用。然而， representation learning 在神经网络文献中对 AFT 模型的应用还很少，尽管它在比较简单和可读性方面比 hazard-focused 方法更有优势。在这项工作中，我们介绍了 Deep AFT Rank-regression 模型（DART），这个模型使用基于 Gehan 排名统计的目标函数，这是一种高效的 representation learning 方法。在 eliminating the requirement to establish a baseline event time distribution 的同时，DART 保留了标准 AFT 模型中的优点，直接预测事件时间。我们的提案的方法是一种 semi-parametric AFT 模型，不需要任何参数分布假设，这也消除了需要额外的 гипер Parameters 或复杂的模型架构，与现有的神经网络基于 AFT 模型不同。通过对各种 benchmark 数据进行量化分析，我们表明了 DART 在高通过率 censored time-to-event 数据模型中具有显著的潜力。
</details></li>
</ul>
<hr>
<h2 id="Bivariate-DeepKriging-for-Large-scale-Spatial-Interpolation-of-Wind-Fields"><a href="#Bivariate-DeepKriging-for-Large-scale-Spatial-Interpolation-of-Wind-Fields" class="headerlink" title="Bivariate DeepKriging for Large-scale Spatial Interpolation of Wind Fields"></a>Bivariate DeepKriging for Large-scale Spatial Interpolation of Wind Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08038">http://arxiv.org/abs/2307.08038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pratik Nag, Ying Sun, Brian J Reich</li>
<li>for: 这篇论文旨在提供一种高精度风场数据的大规模插值或降阶方法，用于气象、海洋学和气候研究等领域。</li>
<li>methods: 本文提出了一种称为“深度拟合”的方法，它是一个具有空间对预的深度神经网络，具有空间对预的嵌入层，用于预测二维空间资料。</li>
<li>results: 比较traditional cokriging预测器，深度拟合方法的预测性能更高，且可以快速 Compute，实现高效的数据预测。在中东地区506,771个位置上应用了深度拟合方法，结果表明其预测性能佳，且大大减少了计算时间。<details>
<summary>Abstract</summary>
High spatial resolution wind data are essential for a wide range of applications in climate, oceanographic and meteorological studies. Large-scale spatial interpolation or downscaling of bivariate wind fields having velocity in two dimensions is a challenging task because wind data tend to be non-Gaussian with high spatial variability and heterogeneity. In spatial statistics, cokriging is commonly used for predicting bivariate spatial fields. However, the cokriging predictor is not optimal except for Gaussian processes. Additionally, cokriging is computationally prohibitive for large datasets. In this paper, we propose a method, called bivariate DeepKriging, which is a spatially dependent deep neural network (DNN) with an embedding layer constructed by spatial radial basis functions for bivariate spatial data prediction. We then develop a distribution-free uncertainty quantification method based on bootstrap and ensemble DNN. Our proposed approach outperforms the traditional cokriging predictor with commonly used covariance functions, such as the linear model of co-regionalization and flexible bivariate Mat\'ern covariance. We demonstrate the computational efficiency and scalability of the proposed DNN model, with computations that are, on average, 20 times faster than those of conventional techniques. We apply the bivariate DeepKriging method to the wind data over the Middle East region at 506,771 locations. The prediction performance of the proposed method is superior over the cokriging predictors and dramatically reduces computation time.
</details>
<details>
<summary>摘要</summary>
高空间分辨率风数据是气候、海洋学和气象研究中的重要工具。大规模的风场 interpolación或下采样是一项复杂的任务，因为风数据往往非 Gaussian 分布，具有高空间变化和不均匀性。在空间统计中，cokriging 是广泛使用的方法，但predictor 不是优化的，除非使用 Gaussian 过程。此外，cokriging 对大量数据来说是计算昂贵的。在这篇论文中，我们提出了一种方法，即双向 DeepKriging，它是一种具有空间依赖性的双向深度神经网络（DNN），其中 embedding 层由空间径向基函数构建。我们然后开发了一种不含分布的不确定性评估方法，基于 bootstrap 和集成 DNN。我们的提出的方法在传统的 cokriging 预测器中超越了常用的 covariance 函数，如线性模型协调和灵活的双向 Matér  covariance。我们 demonstate 了我们的 DNN 模型的计算效率和可扩展性，计算时间比传统技术平均快20倍。我们应用了双向 DeepKriging 方法于中东地区的风数据，包括506,771个位置。我们的预测性能较传统的 cokriging 预测器更高，计算时间减少了90%。
</details></li>
</ul>
<hr>
<h2 id="Magnetic-Field-Based-Reward-Shaping-for-Goal-Conditioned-Reinforcement-Learning"><a href="#Magnetic-Field-Based-Reward-Shaping-for-Goal-Conditioned-Reinforcement-Learning" class="headerlink" title="Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning"></a>Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08033">http://arxiv.org/abs/2307.08033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Ding, Yuanze Tang, Qing Wu, Bo Wang, Chunlin Chen, Zhi Wang</li>
<li>for: 提高目标填充RL任务中样本效率，即使面临动态环境和奖励稀缺。</li>
<li>methods: 基于磁场的奖励定制（MFRS）方法，通过将目标和障碍物视为常见磁铁，根据磁场强度的非线性和不规则分布来设置奖励函数。</li>
<li>results: 在 simulate 和实际 робо控任务中，MFRS 比相关现有方法更高效，可以有效提高RL算法在目标填充任务中的样本效率，并且可以适应不同的目标和障碍物动态。<details>
<summary>Abstract</summary>
Goal-conditioned reinforcement learning (RL) is an interesting extension of the traditional RL framework, where the dynamic environment and reward sparsity can cause conventional learning algorithms to fail. Reward shaping is a practical approach to improving sample efficiency by embedding human domain knowledge into the learning process. Existing reward shaping methods for goal-conditioned RL are typically built on distance metrics with a linear and isotropic distribution, which may fail to provide sufficient information about the ever-changing environment with high complexity. This paper proposes a novel magnetic field-based reward shaping (MFRS) method for goal-conditioned RL tasks with dynamic target and obstacles. Inspired by the physical properties of magnets, we consider the target and obstacles as permanent magnets and establish the reward function according to the intensity values of the magnetic field generated by these magnets. The nonlinear and anisotropic distribution of the magnetic field intensity can provide more accessible and conducive information about the optimization landscape, thus introducing a more sophisticated magnetic reward compared to the distance-based setting. Further, we transform our magnetic reward to the form of potential-based reward shaping by learning a secondary potential function concurrently to ensure the optimal policy invariance of our method. Experiments results in both simulated and real-world robotic manipulation tasks demonstrate that MFRS outperforms relevant existing methods and effectively improves the sample efficiency of RL algorithms in goal-conditioned tasks with various dynamics of the target and obstacles.
</details>
<details>
<summary>摘要</summary>
traditional reinforcement learning（RL）框架中的目标条件RL是一种有趣的扩展，因为动态环境和奖励稀缺可能使得传统的学习算法失效。奖励形成是一种实用的方法来提高样本效率，其中将人类领域知识 embed 到学习过程中。现有的奖励形成方法 для goal-conditioned RL 通常基于距离度量，这可能无法提供动态环境中的高复杂性所需的充分信息。这篇论文提出了一种基于磁场的奖励形成（MFRS）方法，用于goal-conditioned RL 任务中的动态目标和障碍物。我们根据物体的物理性质，将目标和障碍物视为永久磁铁，并根据这些磁铁生成的磁场强度设置奖励函数。非线性和不均匀的磁场强度分布可以提供更多的可访问和渠道化信息关于优化地图，因此引入一种更加复杂的磁奖。此外，我们将我们的磁奖转换为 potential-based 奖励形成，通过同时学习次要潜在函数来确保我们的方法的优化策略不变性。实验结果表明，MFRS在模拟和实际 робоック抓取任务中表现出色，超越了相关的现有方法，并有效地提高了RL算法在目标条件下的样本效率。
</details></li>
</ul>
<hr>
<h2 id="Noise-aware-Speech-Enhancement-using-Diffusion-Probabilistic-Model"><a href="#Noise-aware-Speech-Enhancement-using-Diffusion-Probabilistic-Model" class="headerlink" title="Noise-aware Speech Enhancement using Diffusion Probabilistic Model"></a>Noise-aware Speech Enhancement using Diffusion Probabilistic Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08029">http://arxiv.org/abs/2307.08029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuchen005/nase">https://github.com/yuchen005/nase</a></li>
<li>paper_authors: Yuchen Hu, Chen Chen, Ruizhe Li, Qiushi Zhu, Eng Siong Chng</li>
<li>for: 提高扩散模型下的生成散音增强（SE）性能，尤其是对于未经测试的噪声。</li>
<li>methods: 提出一种基于噪声特征的散音增强（NASE）方法，通过EXTRACTING噪声特征来导引反推进程。同时，提出一种多任务学习方案，以 JOINTLY 优化 SE 和 NC 任务，以提高噪声特征的提取精度。</li>
<li>results: 实验证明，NASE 可以与多种主流扩散 SE 模型结合使用，并在 VoiceBank-DEMAND 数据集上显示出显著提高，特别是对于未经测试的噪声。<details>
<summary>Abstract</summary>
With recent advances of diffusion model, generative speech enhancement (SE) has attracted a surge of research interest due to its great potential for unseen testing noises. However, existing efforts mainly focus on inherent properties of clean speech for inference, underexploiting the varying noise information in real-world conditions. In this paper, we propose a noise-aware speech enhancement (NASE) approach that extracts noise-specific information to guide the reverse process in diffusion model. Specifically, we design a noise classification (NC) model to produce acoustic embedding as a noise conditioner for guiding the reverse denoising process. Meanwhile, a multi-task learning scheme is devised to jointly optimize SE and NC tasks, in order to enhance the noise specificity of extracted noise conditioner. Our proposed NASE is shown to be a plug-and-play module that can be generalized to any diffusion SE models. Experiment evidence on VoiceBank-DEMAND dataset shows that NASE achieves significant improvement over multiple mainstream diffusion SE models, especially on unseen testing noises.
</details>
<details>
<summary>摘要</summary>
Recent advances in diffusion models have led to a surge of research interest in generative speech enhancement (SE) due to its great potential for unseen testing noises. However, existing efforts primarily focus on the inherent properties of clean speech for inference, neglecting the varying noise information in real-world conditions. In this paper, we propose a noise-aware speech enhancement (NASE) approach that extracts noise-specific information to guide the reverse process in the diffusion model. Specifically, we design a noise classification (NC) model to produce an acoustic embedding as a noise conditioner for guiding the reverse denoising process. Moreover, we devise a multi-task learning scheme to jointly optimize the SE and NC tasks, enhancing the noise specificity of the extracted noise conditioner. Our proposed NASE is shown to be a plug-and-play module that can be generalized to any diffusion SE models. Experimental evidence on the VoiceBank-DEMAND dataset demonstrates that NASE achieves significant improvement over multiple mainstream diffusion SE models, especially on unseen testing noises.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Implicit-Models-Sparsity-Trade-offs-Capability-in-Weight-tied-Model-for-Vision-Tasks"><a href="#Revisiting-Implicit-Models-Sparsity-Trade-offs-Capability-in-Weight-tied-Model-for-Vision-Tasks" class="headerlink" title="Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks"></a>Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08013">http://arxiv.org/abs/2307.08013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haobo Song, Soumajit Majumder, Tao Lin</li>
<li>for: 该文章旨在探讨隐式模型（DEQs）在视觉任务上的表现，以及其相对于其他方法的比较。</li>
<li>methods: 该文章使用Weight-tied模型，并对其进行了深入研究，以及提出了使用特异 sparse masks 来提高模型容量。</li>
<li>results: 研究发现，Weight-tied模型在视觉任务上比DEQ variants更有效稳定，并且可以更好地 generalized to other learning paradigms。<details>
<summary>Abstract</summary>
Implicit models such as Deep Equilibrium Models (DEQs) have garnered significant attention in the community for their ability to train infinite layer models with elegant solution-finding procedures and constant memory footprint. However, despite several attempts, these methods are heavily constrained by model inefficiency and optimization instability. Furthermore, fair benchmarking across relevant methods for vision tasks is missing. In this work, we revisit the line of implicit models and trace them back to the original weight-tied models. Surprisingly, we observe that weight-tied models are more effective, stable, as well as efficient on vision tasks, compared to the DEQ variants. Through the lens of these simple-yet-clean weight-tied models, we further study the fundamental limits in the model capacity of such models and propose the use of distinct sparse masks to improve the model capacity. Finally, for practitioners, we offer design guidelines regarding the depth, width, and sparsity selection for weight-tied models, and demonstrate the generalizability of our insights to other learning paradigms.
</details>
<details>
<summary>摘要</summary>
匿名模型（DEQs）在社区中受到了广泛关注，因为它们可以训练无穷层模型，并且拥有简洁的解决方案和常量内存占用。然而，虽然有几次尝试，但这些方法受到了模型不充分利用和优化不稳定的限制。此外，对于视觉任务的比较是缺失的。在这项工作中，我们回顾了匿名模型的线索，并发现Weight-tied模型在视觉任务上更有效率、稳定、并且更高效。通过这些简单而干净的Weight-tied模型，我们进一步研究了这些模型的基本限制，并提出了使用特定的稀疏mask来提高模型容量。最后，我们对于实践者提供了depth、宽和稀疏选择的设计指南，并证明了我们的理解在其他学习方法上也是可行。
</details></li>
</ul>
<hr>
<h2 id="For-One-Shot-Decoding-Self-supervised-Deep-Learning-Based-Polar-Decoder"><a href="#For-One-Shot-Decoding-Self-supervised-Deep-Learning-Based-Polar-Decoder" class="headerlink" title="For One-Shot Decoding: Self-supervised Deep Learning-Based Polar Decoder"></a>For One-Shot Decoding: Self-supervised Deep Learning-Based Polar Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08004">http://arxiv.org/abs/2307.08004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiying Song, Yihao Luo, Yuma Fukuzawa</li>
<li>for: 实现一种基于深度学习的排序码解码方案，具有一次解码功能。</li>
<li>methods: 透过自动化学习并利用生成矩阵来训练神经网络，将神经网络训练为bounded distance解码器。</li>
<li>results: Computer simulations show that the proposed scheme can achieve similar performance to the maximum a posteriori (MAP) decoder for very short packets, and the proposed neural network decoder (NND) has better generalization ability than the conventional one.<details>
<summary>Abstract</summary>
We propose a self-supervised deep learning-based decoding scheme that enables one-shot decoding of polar codes. In the proposed scheme, rather than using the information bit vectors as labels for training the neural network (NN) through supervised learning as the conventional scheme did, the NN is trained to function as a bounded distance decoder by leveraging the generator matrix of polar codes through self-supervised learning. This approach eliminates the reliance on predefined labels, empowering the potential to train directly on the actual data within communication systems and thereby enhancing the applicability. Furthermore, computer simulations demonstrate that (i) the bit error rate (BER) and block error rate (BLER) performances of the proposed scheme can approach those of the maximum a posteriori (MAP) decoder for very short packets and (ii) the proposed NN decoder (NND) exhibits much superior generalization ability compared to the conventional one.
</details>
<details>
<summary>摘要</summary>
我们提出一种自动超vised深度学习的解码方案，可以实现一击解码楔形码。在我们的方案中，而不是通过指定标签来用深度学习训练神经网络（NN），就是通过楔形码生成矩阵自我超vised学习训练NN。这种方法消除了靠定标签的依赖，使得可以直接在通信系统中训练NN，从而提高可用性。此外，计算机实验表明，（i）提案的方案可以在很短的包长度下达到MAP解码器的比Error rate和块Error rate性能，（ii）提案的NN解码器（NND）在对比传统方法的情况下显示出了很好的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Joint-Microseismic-Event-Detection-and-Location-with-a-Detection-Transformer"><a href="#Joint-Microseismic-Event-Detection-and-Location-with-a-Detection-Transformer" class="headerlink" title="Joint Microseismic Event Detection and Location with a Detection Transformer"></a>Joint Microseismic Event Detection and Location with a Detection Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09207">http://arxiv.org/abs/2307.09207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Yang, Claire Birnie, Tariq Alkhalifah</li>
<li>for: 这个研究旨在提出一个能够同时探测和定位微地震事件的方法，以实现实时微地震监控。</li>
<li>methods: 本研究使用了卷积神经网和Encoder-Decoder Transformer，并应用了一个基于集合的匈牙利损失函数，以直接处理录取到的波形资料。</li>
<li>results: 实验结果显示，提案的方法能够正确地探测和定位微地震事件，并且在实际应用中能够获得高效和可靠的结果。<details>
<summary>Abstract</summary>
Microseismic event detection and location are two primary components in microseismic monitoring, which offers us invaluable insights into the subsurface during reservoir stimulation and evolution. Conventional approaches for event detection and location often suffer from manual intervention and/or heavy computation, while current machine learning-assisted approaches typically address detection and location separately; such limitations hinder the potential for real-time microseismic monitoring. We propose an approach to unify event detection and source location into a single framework by adapting a Convolutional Neural Network backbone and an encoder-decoder Transformer with a set-based Hungarian loss, which is applied directly to recorded waveforms. The proposed network is trained on synthetic data simulating multiple microseismic events corresponding to random source locations in the area of suspected microseismic activities. A synthetic test on a 2D profile of the SEAM Time Lapse model illustrates the capability of the proposed method in detecting the events properly and locating them in the subsurface accurately; while, a field test using the Arkoma Basin data further proves its practicability, efficiency, and its potential in paving the way for real-time monitoring of microseismic events.
</details>
<details>
<summary>摘要</summary>
微型地震事件检测和定位是微型地震监测的两个关键组成部分，它为我们提供了无价的地层下预测和演化的准确信息。传统的方法通常受到人工干预和/或重量计算的限制，而当前的机器学习帮助的方法通常分别处理检测和定位，这些限制了实时微型地震监测的可能性。我们提出了一种将事件检测和源位置嵌入到同一个框架中的方法，通过采用卷积神经网络背景和encoder-decoder转换器，并使用集合基于hungarian损失函数进行训练。该网络在记录波形上直接应用。我们对多个微型地震事件的同时发生进行了synthetic数据生成，并在2DProfile上进行了一个synthetic测试，这些测试结果表明了我们的方法可以正确地检测事件并准确地定位其位置在地层下。此外，我们还对Arkoma Basin数据进行了一个实际测试，这些测试结果表明了我们的方法的实用性、效率和实时监测微型地震事件的潜在性。
</details></li>
</ul>
<hr>
<h2 id="LUCYD-A-Feature-Driven-Richardson-Lucy-Deconvolution-Network"><a href="#LUCYD-A-Feature-Driven-Richardson-Lucy-Deconvolution-Network" class="headerlink" title="LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network"></a>LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07998">http://arxiv.org/abs/2307.07998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ctom2/lucyd-deconvolution">https://github.com/ctom2/lucyd-deconvolution</a></li>
<li>paper_authors: Tomáš Chobola, Gesine Müller, Veit Dausmann, Anton Theileis, Jan Taucher, Jan Huisken, Tingying Peng</li>
<li>for: 提高微scopic图像质量和可读性</li>
<li>methods: 结合Richardson-Lucy方程和深度学习特征，提出了一种基于特征驱动的图像恢复模型</li>
<li>results: 对于synthetic和实际微scopic图像，LUCYD方法表现出色，提高图像质量和一致性，并且可以处理不同的微scopic模式和捕捉条件。<details>
<summary>Abstract</summary>
The process of acquiring microscopic images in life sciences often results in image degradation and corruption, characterised by the presence of noise and blur, which poses significant challenges in accurately analysing and interpreting the obtained data. This paper proposes LUCYD, a novel method for the restoration of volumetric microscopy images that combines the Richardson-Lucy deconvolution formula and the fusion of deep features obtained by a fully convolutional network. By integrating the image formation process into a feature-driven restoration model, the proposed approach aims to enhance the quality of the restored images whilst reducing computational costs and maintaining a high degree of interpretability. Our results demonstrate that LUCYD outperforms the state-of-the-art methods in both synthetic and real microscopy images, achieving superior performance in terms of image quality and generalisability. We show that the model can handle various microscopy modalities and different imaging conditions by evaluating it on two different microscopy datasets, including volumetric widefield and light-sheet microscopy. Our experiments indicate that LUCYD can significantly improve resolution, contrast, and overall quality of microscopy images. Therefore, it can be a valuable tool for microscopy image restoration and can facilitate further research in various microscopy applications. We made the source code for the model accessible under https://github.com/ctom2/lucyd-deconvolution.
</details>
<details>
<summary>摘要</summary>
生物科学中获取微型图像的过程经常会导致图像异常和损害，表现为图像噪声和模糊，这会对数据分析和解释提出 significiant 挑战。这篇论文提出了一种名为LUCYD的新方法，用于修复Volume Microscopy 图像。该方法结合了Richardson-Lucy 减 convolution 方程和基于深度学习的卷积网络来提高图像的品质。通过将图像形成过程包含在一个特征驱动的修复模型中，该方法希望提高修复后图像的质量，同时降低计算成本并保持高度可读性。我们的结果表明，LUCYD 在 synthetic 和实际 Microscopy 图像中都超过了现有方法的性能，在图像质量和泛化性方面表现出色。我们通过评估其在不同 Microscopy 模式和拍摄条件下的表现，发现LUCYD 可以处理不同的 Microscopy 模式和拍摄条件。我们的实验表明，LUCYD 可以显著提高 Microscopy 图像的分辨率、对比度和整体质量。因此，它可以成为 Microscopy 图像修复的有价值工具，并促进了不同 Microscopy 应用的进一步研究。我们将模型的源代码公开在 <https://github.com/ctom2/lucyd-deconvolution> 上。
</details></li>
</ul>
<hr>
<h2 id="MargCTGAN-A-“Marginally’’-Better-CTGAN-for-the-Low-Sample-Regime"><a href="#MargCTGAN-A-“Marginally’’-Better-CTGAN-for-the-Low-Sample-Regime" class="headerlink" title="MargCTGAN: A “Marginally’’ Better CTGAN for the Low Sample Regime"></a>MargCTGAN: A “Marginally’’ Better CTGAN for the Low Sample Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07997">http://arxiv.org/abs/2307.07997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tejuafonja/margctgan">https://github.com/tejuafonja/margctgan</a></li>
<li>paper_authors: Tejumade Afonja, Dingfan Chen, Mario Fritz</li>
<li>for: This paper is written for evaluating the effectiveness of synthetic tabular data generation methods, specifically in low sample scenarios, and addressing the oversight of neglecting statistical properties in current evaluation methods.</li>
<li>methods: The paper uses three state-of-the-art synthetic tabular data generators, including CTGAN, and evaluates their performance based on marginal distribution, column-pair correlation, joint distribution, and downstream task utility. The proposed MargCTGAN model adds feature matching of de-correlated marginals to improve the statistical properties and downstream utility of the synthetic data.</li>
<li>results: The paper shows that CTGAN underperforms in low sample settings in terms of utility, but the proposed MargCTGAN model consistently improves downstream utility as well as statistical properties of the synthetic data.<details>
<summary>Abstract</summary>
The potential of realistic and useful synthetic data is significant. However, current evaluation methods for synthetic tabular data generation predominantly focus on downstream task usefulness, often neglecting the importance of statistical properties. This oversight becomes particularly prominent in low sample scenarios, accompanied by a swift deterioration of these statistical measures. In this paper, we address this issue by conducting an evaluation of three state-of-the-art synthetic tabular data generators based on their marginal distribution, column-pair correlation, joint distribution and downstream task utility performance across high to low sample regimes. The popular CTGAN model shows strong utility, but underperforms in low sample settings in terms of utility. To overcome this limitation, we propose MargCTGAN that adds feature matching of de-correlated marginals, which results in a consistent improvement in downstream utility as well as statistical properties of the synthetic data.
</details>
<details>
<summary>摘要</summary>
现实生成的数据的潜在价值很大。然而，目前的生成synthetic tabular数据的评估方法主要关注下游任务的有用性，经常忽略了统计性质的重要性。这种欠缺特别在低样本情况下显著，同时这些统计度量快速下降。在这篇论文中，我们解决这个问题，通过评估三种现状最佳的synthetic tabular数据生成器的边缘分布、列对协同分布、共同分布和下游任务有用性的表现，从高样本到低样本的场景中进行评估。CTGAN模型在下游任务上表现良好，但在低样本情况下表现不佳，其 Utility 下降。为了解决这个限制，我们提议MargCTGAN，它通过匹配特征的分布，使得下游任务的有用性和统计性质都得到了改进。
</details></li>
</ul>
<hr>
<h2 id="CoNAN-Conditional-Neural-Aggregation-Network-For-Unconstrained-Face-Feature-Fusion"><a href="#CoNAN-Conditional-Neural-Aggregation-Network-For-Unconstrained-Face-Feature-Fusion" class="headerlink" title="CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion"></a>CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10237">http://arxiv.org/abs/2307.10237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhavin Jawade, Deen Dayal Mohan, Dennis Fedorishin, Srirangaraj Setlur, Venu Govindaraju</li>
<li>for: 长距离、低分辨率、不同视角、照明、姿势和 atmospheric conditions 下的面部识别，提高face recognition的精度和可靠性。</li>
<li>methods: 基于面部特征的分布信息来Conditioning，通过学习一个context vector来对特征进行权重调整，以提高face recognition的精度和可靠性。</li>
<li>results: 在BTS和DroneSURF等长距离未控制的面部识别数据集上达到了state-of-the-art的结果，证明了我们的方法的优势。<details>
<summary>Abstract</summary>
Face recognition from image sets acquired under unregulated and uncontrolled settings, such as at large distances, low resolutions, varying viewpoints, illumination, pose, and atmospheric conditions, is challenging. Face feature aggregation, which involves aggregating a set of N feature representations present in a template into a single global representation, plays a pivotal role in such recognition systems. Existing works in traditional face feature aggregation either utilize metadata or high-dimensional intermediate feature representations to estimate feature quality for aggregation. However, generating high-quality metadata or style information is not feasible for extremely low-resolution faces captured in long-range and high altitude settings. To overcome these limitations, we propose a feature distribution conditioning approach called CoNAN for template aggregation. Specifically, our method aims to learn a context vector conditioned over the distribution information of the incoming feature set, which is utilized to weigh the features based on their estimated informativeness. The proposed method produces state-of-the-art results on long-range unconstrained face recognition datasets such as BTS, and DroneSURF, validating the advantages of such an aggregation strategy.
</details>
<details>
<summary>摘要</summary>
面部识别从图像集中获取在无法控制的设置下进行，如在远距离、低分辨率、不同视点、照明、姿势和大气条件下，是有挑战的。在这些识别系统中，面部特征聚合起important role，即将一个模板中的多个特征表示聚合到一个全局表示中。现有的传统面部特征聚合方法可以通过metadata或高维度中间特征表示来估计特征质量进行聚合。然而，在EXTREMELY LOW-RESOLUTION的脸部图像中，生成高质量的metadata或样式信息是不可能的。为了突破这些限制，我们提出了一种特征分布条件的方法called CoNAN для模板聚合。具体来说，我们的方法是学习一个conditioned over the distribution information of the incoming feature set，用于对特征进行权重调整基于估计的有用性。我们的方法在long-range和高高度设置下的无结构化脸部识别数据集，如BTS和DroneSURF， producessate-of-the-art结果，证明了such an aggregation strategy的优势。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Techniques-for-Optimizing-Transformer-Inference"><a href="#A-Survey-of-Techniques-for-Optimizing-Transformer-Inference" class="headerlink" title="A Survey of Techniques for Optimizing Transformer Inference"></a>A Survey of Techniques for Optimizing Transformer Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07982">http://arxiv.org/abs/2307.07982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishna Teja Chitty-Venkata, Sparsh Mittal, Murali Emani, Venkatram Vishwanath, Arun K. Somani</li>
<li>for: 本文旨在把transformer网络的推理阶段优化技术综述一下，以便为研究人员和学生提供一个全面的参考。</li>
<li>methods: 本文总结了多种优化技术，包括知识传承、剪枝、量化、神经网络搜索和特征硬件设计等，以提高transformer网络的推理效率。</li>
<li>results: 本文对多种模型和技术进行了评估，并展示了它们的数据量和精度之间的质量评估。同时，本文还预测了未来这一领域的发展趋势。<details>
<summary>Abstract</summary>
Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review hardware-level optimization techniques and the design of novel hardware accelerators for transformers. We summarize the quantitative results on the number of parameters/FLOPs and accuracy of several models/techniques to showcase the tradeoff exercised by them. We also outline future directions in this rapidly evolving field of research. We believe that this survey will educate both novice and seasoned researchers and also spark a plethora of research efforts in this field.
</details>
<details>
<summary>摘要</summary>
近年来，变换神经网络家族，包括矢量代码表（BERT）、生成预训练变换网络（GPT）和视觉转换网络（ViT），在自然语言处理（NLP）和计算机视觉（CV）领域中表现出色。变换网络如ChatGPT对公众生活产生了深远的影响。然而，为了 достичь高预测性能，变换网络的内存和计算核心覆盖面积呈极值增长趋势。研究人员提出了优化变换推理阶段的多种技术。本文对优化变换网络推理阶段的技术进行了全面的检视，包括知识储存、剪辑、量化、神经网络搜索和轻量级网络设计等算法级别的技术。此外，我们还评估硬件级别的优化技术和专门为变换网络设计的新硬件加速器。我们对多种模型和技术的参数/计算量和准确率进行了评量分析，并对这些技术的许多特点进行了详细的描述。我们还预测未来这一领域的发展趋势。我们相信，这篇评论将为初学者和季读者都提供深刻的了解，并且将激发这一领域的大量研究。
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Robust-Distributed-Online-Learning-Taming-Adversarial-Participants-in-An-Adversarial-Environment"><a href="#Byzantine-Robust-Distributed-Online-Learning-Taming-Adversarial-Participants-in-An-Adversarial-Environment" class="headerlink" title="Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment"></a>Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07980">http://arxiv.org/abs/2307.07980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanger521/ogd">https://github.com/wanger521/ogd</a></li>
<li>paper_authors: Xingrong Dong, Zhaoxian Wu, Qing Ling, Zhi Tian</li>
<li>for: 这个论文研究了分布式在线学习下的拜尼袭击问题。</li>
<li>methods: 该论文使用了一种robust aggregation rule，并提出了一种Byzantine-robust分布式在线摘要算法来实现sublinear的权衡误差 bound。</li>
<li>results: 该论文证明了，即使使用State-of-the-art robust aggregation rule，分布式在线学习仍然只能实现线性的拜尼袭击 regret bound，这是不可避免的。但是，当环境不完全是敌对的，即honest participant的损失是独立同分布的时，我们可以实现sublinear的随机误差 bound。<details>
<summary>Abstract</summary>
This paper studies distributed online learning under Byzantine attacks. The performance of an online learning algorithm is often characterized by (adversarial) regret, which evaluates the quality of one-step-ahead decision-making when an environment provides adversarial losses, and a sublinear bound is preferred. But we prove that, even with a class of state-of-the-art robust aggregation rules, in an adversarial environment and in the presence of Byzantine participants, distributed online gradient descent can only achieve a linear adversarial regret bound, which is tight. This is the inevitable consequence of Byzantine attacks, even though we can control the constant of the linear adversarial regret to a reasonable level. Interestingly, when the environment is not fully adversarial so that the losses of the honest participants are i.i.d. (independent and identically distributed), we show that sublinear stochastic regret, in contrast to the aforementioned adversarial regret, is possible. We develop a Byzantine-robust distributed online momentum algorithm to attain such a sublinear stochastic regret bound. Extensive numerical experiments corroborate our theoretical analysis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Finite-element-inspired-networks-Learning-physically-plausible-deformable-object-dynamics-from-partial-observations"><a href="#Finite-element-inspired-networks-Learning-physically-plausible-deformable-object-dynamics-from-partial-observations" class="headerlink" title="Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations"></a>Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07975">http://arxiv.org/abs/2307.07975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shamil Mamedov, A. René Geist, Jan Swevers, Sebastian Trimpe</li>
<li>For: The paper aims to develop a human-interpretable and data-efficient model for simulating the dynamics of deformable linear objects (DLOs).* Methods: The authors draw inspiration from the rigid finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. They train the dynamics network jointly with a physics-informed encoder to ensure that the state representation is physically meaningful.* Results: The authors demonstrate the effectiveness of their approach in a robot experiment, showing that the “Finite element inspired network” (FEN) forms a capable DLO dynamics model that yields physically interpretable predictions from partial observations.Here’s the information in Simplified Chinese text:</li>
<li>for: 该文章目标是开发一种可以快速预测并具有人类可读性的材料线性对象动力学模型。</li>
<li>methods: 作者们引入了刚体Finite element方法（R-FEM），将材料线性对象模型为一串连接的刚体体 whose internal state通过时间的推进。他们将动力学网络与物理学习编码器同时训练，以确保状态表示具有物理意义。</li>
<li>results: 作者们在Robot实验中展示了他们的方法的效果，显示了”Finite element inspired network”（FEN）可以快速预测材料线性对象动力学行为，并且从部分观察数据中获得物理意义的预测结果。<details>
<summary>Abstract</summary>
The accurate simulation of deformable linear object (DLO) dynamics is challenging if the task at hand requires a human-interpretable and data-efficient model that also yields fast predictions. To arrive at such model, we draw inspiration from the rigid finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. As this state is not observed directly, the dynamics network is trained jointly with a physics-informed encoder mapping observed motion variables to the body chain's state. To encourage that the state acquires a physically meaningful representation, we leverage the forward kinematics (FK) of the underlying R-FEM model as a decoder. We demonstrate in a robot experiment that this architecture - being termed "Finite element inspired network" - forms an easy to handle, yet capable DLO dynamics model yielding physically interpretable predictions from partial observations.   The project code is available at: \url{https://tinyurl.com/fei-networks}
</details>
<details>
<summary>摘要</summary>
对于不可归纳的线性物体（DLO）动力学的准确模拟是一项挑战，特别是当需要一个人类可读的和数据效率的模型，同时还需要快速预测。为了实现这种模型，我们从可变finite element方法（R-FEM）中继承灵感，将DLO表示为一串连续的刚体体 whose internal state是通过时间的卷积来实现的。由于这个状态不直接可见，因此我们将动力网络与物理学整体编码器一起训练，以将观察到的动作变量映射到body链的状态上。为了使状态具有物理意义，我们利用了下行骨干（FK）的前向几何学模型作为解码器。我们在Robot实验中展示了这种架构，称之为"finite element inspired network"，可以提供容易处理、具有物理解释能力的DLO动力学模型，从部分观察数据中提取物理意义的预测结果。Code project可以在以下链接中找到：https://tinyurl.com/fei-networks
</details></li>
</ul>
<hr>
<h2 id="Heteroscedastic-Causal-Structure-Learning"><a href="#Heteroscedastic-Causal-Structure-Learning" class="headerlink" title="Heteroscedastic Causal Structure Learning"></a>Heteroscedastic Causal Structure Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07973">http://arxiv.org/abs/2307.07973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baosws/host">https://github.com/baosws/host</a></li>
<li>paper_authors: Bao Duong, Thin Nguyen</li>
<li>for: 学习导向的不同树（DAGs），即从观察数据中找到 causal 关系的编码问题。</li>
<li>methods: 我们提出了一种基于 Gaussian 噪声的不同树学习算法，即 HOST（Heteroscedastic causal STructure learning）算法，它可以在 polynomial 时间复杂度下解决 causal 结构学习问题。</li>
<li>results: 我们通过广泛的实验评估表明，HOST 算法在 causal 顺序学习和结构学习问题中与状态之前的方法竞争。<details>
<summary>Abstract</summary>
Heretofore, learning the directed acyclic graphs (DAGs) that encode the cause-effect relationships embedded in observational data is a computationally challenging problem. A recent trend of studies has shown that it is possible to recover the DAGs with polynomial time complexity under the equal variances assumption. However, this prohibits the heteroscedasticity of the noise, which allows for more flexible modeling capabilities, but at the same time is substantially more challenging to handle. In this study, we tackle the heteroscedastic causal structure learning problem under Gaussian noises. By exploiting the normality of the causal mechanisms, we can recover a valid causal ordering, which can uniquely identify the causal DAG using a series of conditional independence tests. The result is HOST (Heteroscedastic causal STructure learning), a simple yet effective causal structure learning algorithm that scales polynomially in both sample size and dimensionality. In addition, via extensive empirical evaluations on a wide range of both controlled and real datasets, we show that the proposed HOST method is competitive with state-of-the-art approaches in both the causal order learning and structure learning problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Energy-Efficiency-and-Reliability-in-Autonomous-Systems-Estimation-using-Neuromorphic-Approach"><a href="#Enhancing-Energy-Efficiency-and-Reliability-in-Autonomous-Systems-Estimation-using-Neuromorphic-Approach" class="headerlink" title="Enhancing Energy Efficiency and Reliability in Autonomous Systems Estimation using Neuromorphic Approach"></a>Enhancing Energy Efficiency and Reliability in Autonomous Systems Estimation using Neuromorphic Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07963">http://arxiv.org/abs/2307.07963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Ahmadvand, Sarah Safura Sharif, Yaser Mike Banad</li>
<li>for: 这篇论文的目的是提出一个基于脉冲编程理论和脉冲神经网络（SNN）的估计框架，以便实现低Size、Weight、Power（SWaP）电脑的能效性和可靠性。</li>
<li>methods: 本研究使用了SNN-基于的卡尔曼约瑟（KF）和修改后的滑块创新续推（MSIF），并且设计了系统模型对应的网络重量矩阵，以消除学习需求。</li>
<li>results: 比较了提案的策略和其算法对应的KF和MSIF，使用了 Monte Carlo 实验，并评估了SNN-MSIF的稳定性，包括在模型不确定性和神经元损失的情况下。结果显示了提案的方法的可行性和SNN-MSIF的精度和稳定性的超越性。此外，脉冲图从网络中观察到的脉冲图亮点，表明了提案的方法实现了约97%的脉冲减少。<details>
<summary>Abstract</summary>
Energy efficiency and reliability have long been crucial factors for ensuring cost-effective and safe missions in autonomous systems computers. With the rapid evolution of industries such as space robotics and advanced air mobility, the demand for these low size, weight, and power (SWaP) computers has grown significantly. This study focuses on introducing an estimation framework based on spike coding theories and spiking neural networks (SNN), leveraging the efficiency and scalability of neuromorphic computers. Therefore, we propose an SNN-based Kalman filter (KF), a fundamental and widely adopted optimal strategy for well-defined linear systems. Furthermore, based on the modified sliding innovation filter (MSIF) we present a robust strategy called SNN-MSIF. Notably, the weight matrices of the networks are designed according to the system model, eliminating the need for learning. To evaluate the effectiveness of the proposed strategies, we compare them to their algorithmic counterparts, namely the KF and the MSIF, using Monte Carlo simulations. Additionally, we assess the robustness of SNN-MSIF by comparing it to SNN-KF in the presence of modeling uncertainties and neuron loss. Our results demonstrate the applicability of the proposed methods and highlight the superior performance of SNN-MSIF in terms of accuracy and robustness. Furthermore, the spiking pattern observed from the networks serves as evidence of the energy efficiency achieved by the proposed methods, as they exhibited an impressive reduction of approximately 97 percent in emitted spikes compared to possible spikes.
</details>
<details>
<summary>摘要</summary>
“能效率和可靠性在自动系统计算机中已经是长期关键因素，以确保成本效果和安全的任务。随着空间 робо技术和高级空中交通等行业的快速发展，小型、轻量、低功耗（SWaP）计算机的需求增长了 significatively。本研究提出了基于射频编码理论和射频神经网络（SNN）的估算框架，利用神经计算机的效率和可扩展性。因此，我们提出了基于SNN的卡尔曼畸（KF），是一种广泛采用的优质策略，适用于定义良好的线性系统。此外，基于修改的滑动创新畸（MSIF），我们提出了一种可靠的策略，称为SNN-MSIF。各种网络权重矩阵按照系统模型设计，无需学习。通过对提出的策略与算法策略进行比较，我们评估了提案的有效性。此外，我们还对SNN-MSIF的可靠性进行了评估，并在模型不确定性和神经丢失的情况下与SNN-KF进行比较。结果表明提案的方法的可行性和SNN-MSIF的精度和可靠性的提高。此外，由网络产生的射频模式表明了提案的能效率实现，它们在可能发生的射频中减少了约97%。”
</details></li>
</ul>
<hr>
<h2 id="Automated-Polynomial-Filter-Learning-for-Graph-Neural-Networks"><a href="#Automated-Polynomial-Filter-Learning-for-Graph-Neural-Networks" class="headerlink" title="Automated Polynomial Filter Learning for Graph Neural Networks"></a>Automated Polynomial Filter Learning for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07956">http://arxiv.org/abs/2307.07956</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Wendi Yu, Zhichao Hou, Xiaorui Liu</li>
<li>For: This paper is written for researchers and practitioners in the field of graph neural networks (GNNs) and related areas, as it explores the potential and limitations of polynomial graph filter learning approaches and proposes a novel automated polynomial graph filter learning framework called Auto-Polynomial.* Methods: The paper uses polynomial graph filters as guiding principles in the design of GNNs, and proposes a novel automated learning framework called Auto-Polynomial that efficiently learns better filters capable of adapting to various complex graph signals.* Results: The paper demonstrates significant and consistent performance improvements on both homophilic and heterophilic graphs across multiple learning settings considering various labeling ratios, which unleashes the potential of polynomial filter learning.<details>
<summary>Abstract</summary>
Polynomial graph filters have been widely used as guiding principles in the design of Graph Neural Networks (GNNs). Recently, the adaptive learning of the polynomial graph filters has demonstrated promising performance for modeling graph signals on both homophilic and heterophilic graphs, owning to their flexibility and expressiveness. In this work, we conduct a novel preliminary study to explore the potential and limitations of polynomial graph filter learning approaches, revealing a severe overfitting issue. To improve the effectiveness of polynomial graph filters, we propose Auto-Polynomial, a novel and general automated polynomial graph filter learning framework that efficiently learns better filters capable of adapting to various complex graph signals. Comprehensive experiments and ablation studies demonstrate significant and consistent performance improvements on both homophilic and heterophilic graphs across multiple learning settings considering various labeling ratios, which unleashes the potential of polynomial filter learning.
</details>
<details>
<summary>摘要</summary>
偏微分 графические фильтры已经广泛应用于图神经网络（GNNs）的设计中。最近，可变学习偏微分 графические фильтры的应用已经展示了在模型图信号上的出色表现，感谢它们的灵活性和表达能力。在这项工作中，我们进行了一项新的初步研究，探索偏微分 графические филь特的潜在和局限性，发现了严重的欠拟合问题。为了改善偏微分 графические filters的效果，我们提出了 Auto-Polynomial，一种新的、通用的自动偏微分 графические filters 学习框架，可以高效地学习更好的 filters，能够适应不同的复杂图信号。经过完善的实验和割除研究，我们发现了在不同的学习设定下，包括不同的标签比例，在Homophilic和Heterophilic图上，Auto-Polynomial 可以在多种情况下提供显著和稳定的性能改进。
</details></li>
</ul>
<hr>
<h2 id="SentimentGPT-Exploiting-GPT-for-Advanced-Sentiment-Analysis-and-its-Departure-from-Current-Machine-Learning"><a href="#SentimentGPT-Exploiting-GPT-for-Advanced-Sentiment-Analysis-and-its-Departure-from-Current-Machine-Learning" class="headerlink" title="SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning"></a>SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10234">http://arxiv.org/abs/2307.10234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiana Kheiri, Hamid Karimi</li>
<li>for: 这个研究探讨了基于Transformer生成器（GPT）的多种方法在 sentiment analysis 中的表现，具体是使用 SemEval 2017 数据集进行 Task 4 的分析。</li>
<li>methods: 这个研究使用了三种主要策略：1）使用高级 GPT-3.5 Turbo 进行提示工程，2）精度调整 GPT 模型，3）一种创新的嵌入分类方法。</li>
<li>results: 研究结果表明，GPT 方法在 predictive performance 方面表现出了明显的优势，相比之前的 state-of-the-art 模型，GPT 模型的 F1 分数提高了 более 22%。此外，研究还探讨了 sentiment analysis 任务中的常见挑战，如理解上下文和检测蔑词。研究表明，GPT 模型在处理这些复杂性方面具有强大的能力。<details>
<summary>Abstract</summary>
This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other current, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22\% in F1-score compared to the state-of-the-art. Further, the paper sheds light on common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores the enhanced capabilities of the GPT models to effectively handle these complexities. Taken together, these findings highlight the promising potential of GPT models in sentiment analysis, setting the stage for future research in this field. The code can be found at https://github.com/DSAatUSU/SentimentGPT
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accelerating-Distributed-ML-Training-via-Selective-Synchronization"><a href="#Accelerating-Distributed-ML-Training-via-Selective-Synchronization" class="headerlink" title="Accelerating Distributed ML Training via Selective Synchronization"></a>Accelerating Distributed ML Training via Selective Synchronization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07950">http://arxiv.org/abs/2307.07950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahil Tyagi, Martin Swany</li>
<li>for: 这篇论文主要旨在提出一种实用、低开销的深度神经网络（DNN）训练方法，以提高分布式训练中的效率。</li>
<li>methods: 本文使用了一种名为\texttt{SelSync}的实际方法，它在每步决定是否进行交互，以提高训练效率。此外，\texttt{SelSync}还提出了一些优化策略来提高在半同步训练中的整合。</li>
<li>results: 根据实验结果，\texttt{SelSync}可以与传统的分布式训练方法（如BSP）具有相同或更好的准确性，同时减少训练时间，最多减少14倍。<details>
<summary>Abstract</summary>
In distributed training, deep neural networks (DNNs) are launched over multiple workers concurrently and aggregate their local updates on each step in bulk-synchronous parallel (BSP) training. However, BSP does not linearly scale-out due to high communication cost of aggregation. To mitigate this overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous Parallel (SSP) either reduce synchronization frequency or eliminate it altogether, usually at the cost of lower final accuracy. In this paper, we present \texttt{SelSync}, a practical, low-overhead method for DNN training that dynamically chooses to incur or avoid communication at each step either by calling the aggregation op or applying local updates based on their significance. We propose various optimizations as part of \texttt{SelSync} to improve convergence in the context of \textit{semi-synchronous} training. Our system converges to the same or better accuracy than BSP while reducing training time by up to 14$\times$.
</details>
<details>
<summary>摘要</summary>
在分布式训练中，深度神经网络（DNNs）会在多个工作者同时进行并将本地更新集中发送到每个步骤上进行大规模同步并发训练（BSP）。然而，BSP不会线性扩展，因为协调成本过高。为了减少这些开销， alternativas como Federated Averaging（FedAvg）和Stale-Synchronous Parallel（SSP）可以减少同步频率或完全消除同步，通常是在牺牲最终准确性的代价。在这篇论文中，我们提出了\texttt{SelSync}，一种实用、低开销的DNN训练方法，可以在每个步骤之间动态选择是否进行通信。我们还提出了多种优化，以提高在半同步训练的 конверGENCE。我们的系统可以与BSP相比，提高训练时间的效率，最多可以减少训练时间的14倍。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Domain-Adaptive-3D-Object-Detection-by-Reliable-Diverse-and-Class-balanced-Pseudo-Labeling"><a href="#Revisiting-Domain-Adaptive-3D-Object-Detection-by-Reliable-Diverse-and-Class-balanced-Pseudo-Labeling" class="headerlink" title="Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling"></a>Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07944">http://arxiv.org/abs/2307.07944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhuoxiao-chen/redb-da-3ddet">https://github.com/zhuoxiao-chen/redb-da-3ddet</a></li>
<li>paper_authors: Zhuoxiao Chen, Yadan Luo, Zheng Wang, Mahsa Baktashmotlagh, Zi Huang</li>
<li>for: 本文targets at domain-adaptive 3D object detection, specifically addressing the challenge of low-quality pseudo labels and class imbalance in multi-class training settings.</li>
<li>methods: 本文提出了一种基于pseudo labeling的域 adapted 3D object detection方法，包括ReDB框架、cross-domain examination（CDE）和overlapped boxes counting（OBC）等技术。</li>
<li>results: 实验结果表明， compared to existing 3D domain adaptation methods, our proposed ReDB approach significantly improves 3D object detection performance, with a 23.15% mAP improvement on the nuScenes $\rightarrow$ KITTI task.<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g., scales and point densities), we design an overlapped boxes counting (OBC) metric that allows to uniformly downsample pseudo-labeled objects across different geometric characteristics. To confront the issue of inter-class imbalance, we progressively augment the target point clouds with a class-balanced set of pseudo-labeled target instances and source objects, which boosts recognition accuracies on both frequently appearing and rare classes. Experimental results on three benchmark datasets using both voxel-based (i.e., SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our proposed ReDB approach outperforms existing 3D domain adaptation methods by a large margin, improving 23.15% mAP on the nuScenes $\rightarrow$ KITTI task. The code is available at https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet.
</details>
<details>
<summary>摘要</summary>
“对于多类别训练设定下的隐式领域适应（DA）， Pseudo 标签技术已经成为一种重要的方法。然而，现有的 DA 方法在多类别训练设定下会受到较大的性能下降，这是因为 Pseudo 标签质量低下和分布不均匀问题。在这篇文章中，我们提出了一个名为 ReDB 的框架，用于同时探索所有类别。我们的方法可以生成可靠、多样和分布均匀的 Pseudo 3D 标签，并在不同的目标领域中将其适应自我训练。为了解决环境差异所导致的干扰（例如：批量数量），我们提出了跨领域评估（CDE），用于评估 Pseudo 标签的正确性，并在目标环境中复制目标实体。为了减少计算负载和补偿物体移动（例如：数量和点密度），我们设计了重 overlap 的盒子（OBC）度量，允许对不同的几何特征进行均匀抽样。为了解决类别不均匀问题，我们逐渐将目标点云补充了一些具有分布均匀的 Pseudo 标签目标实体和原始点云，这样可以提高识别率，包括常见的类别和罕见的类别。实验结果显示，我们的 ReDB 方法在三个 benchmark 数据集上比较早的 3D 领域适应方法提高了 23.15% mAP，对于 nuScenes ← KITTI 任务来说，这表明我们的方法可以更好地适应不同的领域。代码可以在 https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet 上找到。”
</details></li>
</ul>
<hr>
<h2 id="KECOR-Kernel-Coding-Rate-Maximization-for-Active-3D-Object-Detection"><a href="#KECOR-Kernel-Coding-Rate-Maximization-for-Active-3D-Object-Detection" class="headerlink" title="KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection"></a>KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07942">http://arxiv.org/abs/2307.07942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Luoyadan/KECOR-active-3Ddet">https://github.com/Luoyadan/KECOR-active-3Ddet</a></li>
<li>paper_authors: Yadan Luo, Zhuoxiao Chen, Zhen Fang, Zheng Zhang, Zi Huang, Mahsa Baktashmotlagh</li>
<li>for: 提高自动驾驶中LiDAR检测器的可靠性，使用活动学习方法来减少标注量。</li>
<li>methods: 使用信息论来选择最有用的点云获取标注，并通过搜索算法选择最有用的点云。</li>
<li>results: 比起现有方法，提高了监督学习性能的同时减少了约44%的标注成本和26%的计算时间。<details>
<summary>Abstract</summary>
Achieving a reliable LiDAR-based object detector in autonomous driving is paramount, but its success hinges on obtaining large amounts of precise 3D annotations. Active learning (AL) seeks to mitigate the annotation burden through algorithms that use fewer labels and can attain performance comparable to fully supervised learning. Although AL has shown promise, current approaches prioritize the selection of unlabeled point clouds with high uncertainty and/or diversity, leading to the selection of more instances for labeling and reduced computational efficiency. In this paper, we resort to a novel kernel coding rate maximization (KECOR) strategy which aims to identify the most informative point clouds to acquire labels through the lens of information theory. Greedy search is applied to seek desired point clouds that can maximize the minimal number of bits required to encode the latent features. To determine the uniqueness and informativeness of the selected samples from the model perspective, we construct a proxy network of the 3D detector head and compute the outer product of Jacobians from all proxy layers to form the empirical neural tangent kernel (NTK) matrix. To accommodate both one-stage (i.e., SECOND) and two-stage detectors (i.e., PVRCNN), we further incorporate the classification entropy maximization and well trade-off between detection performance and the total number of bounding boxes selected for annotation. Extensive experiments conducted on two 3D benchmarks and a 2D detection dataset evidence the superiority and versatility of the proposed approach. Our results show that approximately 44% box-level annotation costs and 26% computational time are reduced compared to the state-of-the-art AL method, without compromising detection performance.
</details>
<details>
<summary>摘要</summary>
需要一个可靠的LiDAR基于对象检测器在自动驾驶中，但其成功取决于获得大量精度3D注释。活动学习（AL）希望减轻注释压力通过使用 fewer labels 和可以达到完全监督学习的性能。 although AL has shown promise, current approaches prioritize the selection of unlabeled point clouds with high uncertainty and/or diversity, leading to the selection of more instances for labeling and reduced computational efficiency.在这篇论文中，我们采用了一种新的幂 coding rate maximization（KECOR）策略，该策略目的是通过信息理论来标识最有用的点云来获得标注。 我们使用批处理来寻找想要标注的点云，以便最大化最小数量的比特位数据编码 latent 特征。为了确定选择的样本唯一性和有用性，我们构建了一个3D检测头的卫星网络，并计算所有卫星层的外积Jacobian的 outer product，以形成empirical neural tangent kernel（NTK）矩阵。为了适应一stage（i.e., SECOND）和two-stage检测器（i.e., PVRCNN），我们进一步包括分类 entropy maximization 和检测性能和总绘制盒数之间的平衡。我们在两个3D benchmark和一个2D检测数据集上进行了广泛的实验，结果显示了我们的方法的优越性和多样性。我们发现，相比于状态 искусственный智能方法，我们的方法可以降低盒级注释成本约44%，并且不会影响检测性能。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Compression-of-Unit-Norm-Vectors-in-the-High-Distortion-Regime"><a href="#Optimal-Compression-of-Unit-Norm-Vectors-in-the-High-Distortion-Regime" class="headerlink" title="Optimal Compression of Unit Norm Vectors in the High Distortion Regime"></a>Optimal Compression of Unit Norm Vectors in the High Distortion Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07941">http://arxiv.org/abs/2307.07941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Zhu, Avishek Ghosh, Arya Mazumdar</li>
<li>for: 这篇论文探讨了将单位 нор 向量压缩成最少位元数，以确保可以在接受到的水平上实现重建的可接受程度。</li>
<li>methods: 本研究将Rate-Distortion&#x2F;covering code文献中的问题探讨，但专注于”高 Distortion”  regime。研究在最坏情况下进行，无任何对vector的假设，但允许使用随机压缩 Maps。</li>
<li>results: 研究发现，简单的压缩方案几乎是最佳的。结果包括部分新的结果和已知结果，但是在这篇论文中汇集了所有结果 для 完整性。<details>
<summary>Abstract</summary>
Motivated by the need for communication-efficient distributed learning, we investigate the method for compressing a unit norm vector into the minimum number of bits, while still allowing for some acceptable level of distortion in recovery. This problem has been explored in the rate-distortion/covering code literature, but our focus is exclusively on the "high-distortion" regime. We approach this problem in a worst-case scenario, without any prior information on the vector, but allowing for the use of randomized compression maps. Our study considers both biased and unbiased compression methods and determines the optimal compression rates. It turns out that simple compression schemes are nearly optimal in this scenario. While the results are a mix of new and known, they are compiled in this paper for completeness.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Motivated by the need for communication-efficient distributed learning, we investigate the method for compressing a unit norm vector into the minimum number of bits, while still allowing for some acceptable level of distortion in recovery. This problem has been explored in the rate-distortion/covering code literature, but our focus is exclusively on the 'high-distortion' regime. We approach this problem in a worst-case scenario, without any prior information on the vector, but allowing for the use of randomized compression maps. Our study considers both biased and unbiased compression methods and determines the optimal compression rates. It turns out that simple compression schemes are nearly optimal in this scenario. While the results are a mix of new and known, they are compiled in this paper for completeness." into Simplified Chinese.翻译文本：<<SYS>>驱动通信效率的分布式学习需求，我们调查了最小化单元 нор 向量 bit 数量的压缩方法，同时仍允许接受一定的损害率。这个问题在 rate-distortion/covering code 文献中已经被研究过，但我们专注于 "高损害"  régime。我们在最坏情况下进行研究，不具备向量的任何先前信息，但允许使用随机压缩映射。我们的研究包括偏向和无偏向压缩方法，并确定了最优压缩率。结果显示，简单的压缩方案几乎是最优的。虽然结果包含一些新知识和已知的成果，但它们在这篇文章中被编辑成完整。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Truncated-Norm-Regularization-Method-for-Multi-channel-Color-Image-Denoising"><a href="#A-Novel-Truncated-Norm-Regularization-Method-for-Multi-channel-Color-Image-Denoising" class="headerlink" title="A Novel Truncated Norm Regularization Method for Multi-channel Color Image Denoising"></a>A Novel Truncated Norm Regularization Method for Multi-channel Color Image Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07932">http://arxiv.org/abs/2307.07932</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangzhi82/DtNFM">https://github.com/wangzhi82/DtNFM</a></li>
<li>paper_authors: Yiwen Shan, Dong Hu, Haoming Ding, Chunming Yang, Zhi Wang</li>
<li>for: 这篇研究旨在提出一种基于双重权重核心幂方法的彩色图像干扰除法，以解决现实世界中彩色图像干扰除中存在跨通道差异和空间变化的问题。</li>
<li>methods: 该方法使用了双重权重核心幂方法（DtNFM），通过利用干扰图像中的非本地自相似性，将相似的结构聚集起来，并将每个组建立一个DtNFM模型来估算其干扰版本。最终的干扰除图像由多个估算结果 concatenate 得到。</li>
<li>results: 该方法在 synthetic 和实际噪声数据集上进行了广泛的实验，并证明了它在现实世界中彩色图像干扰除中表现出优于许多状态之前的方法。<details>
<summary>Abstract</summary>
Due to the high flexibility and remarkable performance, low-rank approximation methods has been widely studied for color image denoising. However, those methods mostly ignore either the cross-channel difference or the spatial variation of noise, which limits their capacity in real world color image denoising. To overcome those drawbacks, this paper is proposed to denoise color images with a double-weighted truncated nuclear norm minus truncated Frobenius norm minimization (DtNFM) method. Through exploiting the nonlocal self-similarity of the noisy image, the similar structures are gathered and a series of similar patch matrices are constructed. For each group, the DtNFM model is conducted for estimating its denoised version. The denoised image would be obtained by concatenating all the denoised patch matrices. The proposed DtNFM model has two merits. First, it models and utilizes both the cross-channel difference and the spatial variation of noise. This provides sufficient flexibility for handling the complex distribution of noise in real world images. Second, the proposed DtNFM model provides a close approximation to the underlying clean matrix since it can treat different rank components flexibly. To solve the problem resulted from DtNFM model, an accurate and effective algorithm is proposed by exploiting the framework of the alternating direction method of multipliers (ADMM). The generated subproblems are discussed in detail. And their global optima can be easily obtained in closed-form. Rigorous mathematical derivation proves that the solution sequences generated by the algorithm converge to a single critical point. Extensive experiments on synthetic and real noise datasets demonstrate that the proposed method outperforms many state-of-the-art color image denoising methods.
</details>
<details>
<summary>摘要</summary>
due to the high flexibility and remarkable performance, low-rank approximation methods have been widely studied for color image denoising. however, those methods mostly ignore either the cross-channel difference or the spatial variation of noise, which limits their capacity in real-world color image denoising. to overcome these drawbacks, this paper proposes a double-weighted truncated nuclear norm minus truncated frobenius norm minimization (DtNFM) method for denoising color images. by exploiting the nonlocal self-similarity of the noisy image, similar structures are gathered and a series of similar patch matrices are constructed. for each group, the DtNFM model is conducted to estimate its denoised version. the denoised image is obtained by concatenating all the denoised patch matrices. the proposed DtNFM model has two merits. first, it models and utilizes both the cross-channel difference and the spatial variation of noise, providing sufficient flexibility for handling the complex distribution of noise in real-world images. second, the proposed DtNFM model provides a close approximation to the underlying clean matrix, treating different rank components flexibly. to solve the problem resulted from the DtNFM model, an accurate and effective algorithm is proposed by exploiting the framework of the alternating direction method of multipliers (ADMM). the generated subproblems are discussed in detail, and their global optima can be easily obtained in closed form. rigorous mathematical derivation proves that the solution sequences generated by the algorithm converge to a single critical point. extensive experiments on synthetic and real noise datasets demonstrate that the proposed method outperforms many state-of-the-art color image denoising methods.
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Split-Learning-against-Adversarial-Attacks"><a href="#On-the-Robustness-of-Split-Learning-against-Adversarial-Attacks" class="headerlink" title="On the Robustness of Split Learning against Adversarial Attacks"></a>On the Robustness of Split Learning against Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07916">http://arxiv.org/abs/2307.07916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmy266/SplitADV">https://github.com/fmy266/SplitADV</a></li>
<li>paper_authors: Mingyuan Fan, Cen Chen, Chengyu Wang, Wenmeng Zhou, Jun Huang</li>
<li>for: 评估分布式学习中对 adversarial 攻击的Robustness,特别是在无法访问全模型的情况下。</li>
<li>methods: 开发了一种特定于分布式学习的攻击方法，称为 SPADV，它包括两个阶段：1）阴影模型训练，解决模型缺失部分的问题，2）本地 adversarial 攻击，生成 adversarial 例子来评估。</li>
<li>results: SPADV 需要only a few unlabeled non-IID data，并且可以在第二阶段通过对 naturalsamples中的Intermediate output进行拟合来生成 adversarial examples，这表明了分布式学习对 adversarial 攻击的Robustness surprisingly vulnerable。<details>
<summary>Abstract</summary>
Split learning enables collaborative deep learning model training while preserving data privacy and model security by avoiding direct sharing of raw data and model details (i.e., sever and clients only hold partial sub-networks and exchange intermediate computations). However, existing research has mainly focused on examining its reliability for privacy protection, with little investigation into model security. Specifically, by exploring full models, attackers can launch adversarial attacks, and split learning can mitigate this severe threat by only disclosing part of models to untrusted servers.This paper aims to evaluate the robustness of split learning against adversarial attacks, particularly in the most challenging setting where untrusted servers only have access to the intermediate layers of the model.Existing adversarial attacks mostly focus on the centralized setting instead of the collaborative setting, thus, to better evaluate the robustness of split learning, we develop a tailored attack called SPADV, which comprises two stages: 1) shadow model training that addresses the issue of lacking part of the model and 2) local adversarial attack that produces adversarial examples to evaluate.The first stage only requires a few unlabeled non-IID data, and, in the second stage, SPADV perturbs the intermediate output of natural samples to craft the adversarial ones. The overall cost of the proposed attack process is relatively low, yet the empirical attack effectiveness is significantly high, demonstrating the surprising vulnerability of split learning to adversarial attacks.
</details>
<details>
<summary>摘要</summary>
分学促进了分布式深度学习模型的共同训练，同时保护数据隐私和模型安全，因为服务器和客户端只持有部分子网和交换中间计算。然而，现有研究主要集中在隐私保护的可靠性上，很少探讨模型安全性。具体来说，通过探索全模型，攻击者可以发起对抗性攻击，而分学促可以通过仅披露部分模型来减轻这种严重的威胁。这篇论文的目的是评估分学促的可抗性 against 对抗性攻击，特别是在最具挑战性的设定下，即不可信服务器仅有访问模型中间层。现有的对抗性攻击主要集中在中央设定下，而不是合作设定，因此，为更好地评估分学促的可抗性，我们开发了一种特定的攻击方法，称为 SPADV。 SPADV 包括两个阶段：1）遮盾模型培训，解决因缺少部分模型而产生的问题，2）本地对抗性攻击，生成对抗性示例来评估。第一阶段只需要一些非相关的非独特数据，而第二阶段，SPADV 对于自然示例的间接输出进行了扰动，以生成对抗性示例。整个攻击过程的总成本较低，但实际攻击效果却非常高，表明了分学促对于对抗性攻击的意外脆弱性。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-FPGA-Capabilities-for-Accelerated-Biomedical-Computing"><a href="#Exploiting-FPGA-Capabilities-for-Accelerated-Biomedical-Computing" class="headerlink" title="Exploiting FPGA Capabilities for Accelerated Biomedical Computing"></a>Exploiting FPGA Capabilities for Accelerated Biomedical Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07914">http://arxiv.org/abs/2307.07914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kayode Inadagbo, Baran Arig, Nisanur Alici, Murat Isik</li>
<li>for: 这个研究旨在通过使用 Field Programmable Gate Arrays (FPGAs) 提高心电信号分析，并提出了多种高级神经网络架构，包括卷积神经网络 (CNN)、循环神经网络 (RNN)、长期短TERM Memory网络 (LSTMs) 和深度信念网络 (DBNs)。</li>
<li>methods: 我们使用 MIT-BIH 心电性股库进行训练和验证，并在模型中引入 Gaussian 噪声以提高算法的Robustness。我们还使用 EarlyStopping 回调和 Dropout 层来避免过拟合。此外，我们还开发了一个自定义的 Tensor Compute Unit (TCU) 加速器，用于 PYNQ Z1 板。</li>
<li>results: 我们计算了各种性能指标，如延迟和通过put，以获得实际的应用级别高性能的FPGA在生物医学计算中的潜在性。这种研究最终提供了优化神经网络性能在 FPGAs 上的指南，为不同应用场景提供参考。<details>
<summary>Abstract</summary>
This study presents advanced neural network architectures including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTMs), and Deep Belief Networks (DBNs) for enhanced ECG signal analysis using Field Programmable Gate Arrays (FPGAs). We utilize the MIT-BIH Arrhythmia Database for training and validation, introducing Gaussian noise to improve algorithm robustness. The implemented models feature various layers for distinct processing and classification tasks and techniques like EarlyStopping callback and Dropout layer are used to mitigate overfitting. Our work also explores the development of a custom Tensor Compute Unit (TCU) accelerator for the PYNQ Z1 board, offering comprehensive steps for FPGA-based machine learning, including setting up the Tensil toolchain in Docker, selecting architecture, configuring PS-PL, and compiling and executing models. Performance metrics such as latency and throughput are calculated for practical insights, demonstrating the potential of FPGAs in high-performance biomedical computing. The study ultimately offers a guide for optimizing neural network performance on FPGAs for various applications.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这个研究提出了使用场程可编程阵列（FPGAs）进行高性能生物医学计算的先进神经网络架构，包括卷积神经网络（CNN）、循环神经网络（RNN）、长期短Memory神经网络（LSTM）和深度信仰神经网络（DBNs）。我们使用MIT-BIH心跳频数据库进行训练和验证，并在神经网络中引入 Gaussian 噪声以提高算法的稳定性。实现的模型包括不同层次的处理和分类任务，并使用 EarlyStopping 回调函数和 Dropout 层来避免过拟合。我们的工作还探讨了基于 PYNQ Z1 板的自定义 Tensor Compute Unit (TCU) 加速器的开发，并提供了完整的 FPGA-based 机器学习实现方法，包括在 Docker 中设置 Tensil 工具链、选择架构、配置 PS-PL 和编译并执行模型。实验中计算的性能指标包括延迟和吞吐量，以提供实用的指导，展示 FPGAs 在高性能生物医学计算中的潜力。研究最终提供了优化神经网络性能在 FPGAs 上的指南，用于多种应用。
</details></li>
</ul>
<hr>
<h2 id="Predicting-mechanical-properties-of-Carbon-Nanotube-CNT-images-Using-Multi-Layer-Synthetic-Finite-Element-Model-Simulations"><a href="#Predicting-mechanical-properties-of-Carbon-Nanotube-CNT-images-Using-Multi-Layer-Synthetic-Finite-Element-Model-Simulations" class="headerlink" title="Predicting mechanical properties of Carbon Nanotube (CNT) images Using Multi-Layer Synthetic Finite Element Model Simulations"></a>Predicting mechanical properties of Carbon Nanotube (CNT) images Using Multi-Layer Synthetic Finite Element Model Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07912">http://arxiv.org/abs/2307.07912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaveh Safavigerdini, Koundinya Nouduri, Ramakrishna Surya, Andrew Reinhard, Zach Quinlan, Filiz Bunyak, Matthew R. Maschmann, Kannappan Palaniappan</li>
<li>for: 预测碳纳米管（CNT）林集成体的机械性能</li>
<li>methods: 使用深度学习模型和人工智能（AI）基 materials发现</li>
<li>results: 提出一种使用多层合成图像（MLS）或 quasi-2.5D 图像进行数据增强的管道，可以更好地预测 CNT 林集成体的机械性能。<details>
<summary>Abstract</summary>
We present a pipeline for predicting mechanical properties of vertically-oriented carbon nanotube (CNT) forest images using a deep learning model for artificial intelligence (AI)-based materials discovery. Our approach incorporates an innovative data augmentation technique that involves the use of multi-layer synthetic (MLS) or quasi-2.5D images which are generated by blending 2D synthetic images. The MLS images more closely resemble 3D synthetic and real scanning electron microscopy (SEM) images of CNTs but without the computational cost of performing expensive 3D simulations or experiments. Mechanical properties such as stiffness and buckling load for the MLS images are estimated using a physics-based model. The proposed deep learning architecture, CNTNeXt, builds upon our previous CNTNet neural network, using a ResNeXt feature representation followed by random forest regression estimator. Our machine learning approach for predicting CNT physical properties by utilizing a blended set of synthetic images is expected to outperform single synthetic image-based learning when it comes to predicting mechanical properties of real scanning electron microscopy images. This has the potential to accelerate understanding and control of CNT forest self-assembly for diverse applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一个气候预测碳纳米管（CNT）林图像的机械性能预测管道，使用深度学习模型来实现人工智能（AI）基于材料发现。我们的方法包括一种创新的数据增强技术，使用多层合成（MLS）或 quasi-2.5D 图像，这些图像由拼接2D 合成图像而成。MLS 图像更接近3D 合成和实验室扫描电子显微镜（SEM）图像，但没有 computationally Expensive 3D 模拟或实验的成本。机械性能如刚性和填充荷 для MLS 图像通过物理基础模型进行估算。我们的提议的深度学习架构，CNTNeXt，基于我们之前的 CNTNet 神经网络，使用 ResNeXt 特征表示 followed by random forest 回归估计器。我们的机器学习方法，通过使用拼接 synthetic 图像来预测 CNT 物理性能，预计能够超越单独使用 synthetic 图像来预测实际 SEM 图像中的机械性能，从而加速了 CNT 林自组装的理解和控制，并且具有广泛的应用前景。
</details></li>
</ul>
<hr>
<h2 id="MESOB-Balancing-Equilibria-Social-Optimality"><a href="#MESOB-Balancing-Equilibria-Social-Optimality" class="headerlink" title="MESOB: Balancing Equilibria &amp; Social Optimality"></a>MESOB: Balancing Equilibria &amp; Social Optimality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07911">http://arxiv.org/abs/2307.07911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Guo, Lihong Li, Sareh Nabi, Rabih Salhab, Junzi Zhang</li>
<li>for: This paper aims to provide a novel optimization method for multi-level and multi-agent games with anonymous agents and complex interplay between competition and cooperation.</li>
<li>methods: The proposed method is called MESOB-OMO, which combines a mean-field approximation with an occupation measure optimization method to solve a bi-objective optimization problem.</li>
<li>results: The proposed method is effective in balancing the interests of different parties and handling the competitive nature of bidders, and outperforms baseline methods that only consider either the competitive or cooperative aspects.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是提供一种新的优化方法，用于处理多层多代理人的游戏，具有大量匿名代理人和复杂的竞争与合作关系。</li>
<li>methods: 提议的方法是MESOB-OMO，它将mean-field approximation与占用度优化方法相结合，解决bi-objective优化问题。</li>
<li>results: MESOB-OMO方法能够均衡不同党之利益，抑制竞争性代理人的行为，并与基准方法相比显示出优异性。<details>
<summary>Abstract</summary>
Motivated by bid recommendation in online ad auctions, this paper considers a general class of multi-level and multi-agent games, with two major characteristics: one is a large number of anonymous agents, and the other is the intricate interplay between competition and cooperation. To model such complex systems, we propose a novel and tractable bi-objective optimization formulation with mean-field approximation, called MESOB (Mean-field Equilibria & Social Optimality Balancing), as well as an associated occupation measure optimization (OMO) method called MESOB-OMO to solve it. MESOB-OMO enables obtaining approximately Pareto efficient solutions in terms of the dual objectives of competition and cooperation in MESOB, and in particular allows for Nash equilibrium selection and social equalization in an asymptotic manner. We apply MESOB-OMO to bid recommendation in a simulated pay-per-click ad auction. Experiments demonstrate its efficacy in balancing the interests of different parties and in handling the competitive nature of bidders, as well as its advantages over baselines that only consider either the competitive or the cooperative aspects.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用在在线广告拍卖中的拍卖推荐为动机，这篇论文考虑了一个总体来说是多层次和多代理人的游戏，具有两个主要特征：一是一大量的匿名代理人，二是竞争和合作之间的复杂互动。为了模型这些复杂系统，我们提出了一种新的和可行的双目标优化方法，称为MESOB（Mean-field Equilibria & Social Optimality Balancing），以及与之相关的占用度优化方法MESOB-OMO（MESOB-Occupation Measure Optimization）来解决它。MESOB-OMO可以在MESOB中获得约等价的竞争和合作两个目标的解，并且可以在极限情况下实现纳什均衡选择和社会平等。我们在模拟的一个基于拍卖的点播广告拍卖中应用MESOB-OMO。实验表明，它可以均衡不同党的利益，同时处理竞争性的拍卖者，以及与基准值（只考虑竞争或合作方面）相比，具有优势。
</details></li>
</ul>
<hr>
<h2 id="Seeing-is-not-Believing-Robust-Reinforcement-Learning-against-Spurious-Correlation"><a href="#Seeing-is-not-Believing-Robust-Reinforcement-Learning-against-Spurious-Correlation" class="headerlink" title="Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation"></a>Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07907">http://arxiv.org/abs/2307.07907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Ding, Laixi Shi, Yuejie Chi, Ding Zhao</li>
<li>for:  Handle spurious correlation in reinforcement learning to improve the robustness of real-world tasks.</li>
<li>methods:  Propose a new framework called Robust State-Confounded Markov Decision Processes (RSC-MDPs) and design an empirical algorithm to learn the robust optimal policy.</li>
<li>results:  Outperform all baselines in eight realistic self-driving and manipulation tasks.<details>
<summary>Abstract</summary>
Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have causality but have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and sequential structure of RL, is difficult to characterize and identify. Existing robust algorithms that assume simple and unstructured uncertainty sets are therefore inadequate to address this challenge. To solve this issue, we propose Robust State-Confounded Markov Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in breaking spurious correlations compared with other robust RL counterparts. We also design an empirical algorithm to learn the robust optimal policy for RSC-MDPs, which outperforms all baselines in eight realistic self-driving and manipulation tasks.
</details>
<details>
<summary>摘要</summary>
robustness 在强化学习（RL）中已经广泛研究，以处理不同形式的不确定性，如随机干扰、罕见事件和恶意攻击。在这项工作中，我们考虑了一种关键的一种强度对假设相关性，即不同的状态部分没有 causality，但由不见的干扰因素引起的相关性。这种假设相关性在实际任务中很普遍，例如一个自驾车通常在白天会观察到压力很大的交通，而在夜晚则是非常少的交通，这是由于不可见的人类活动引起的。如果模型学习这种无用或甚至有害的相关性，那么在测试案例中，当干扰因素与训练案例不同时，模型可能会catastrophically fail。虽然有动机，但使模型具有假设相关性的鲁棒性具有 significante challenges，因为不确定集，由不见的干扰因素和RL的顺序结构塑造，很难characterize和识别。现有的鲁棒算法假设简单的和无结构的不确定集，因此无法解决这一问题。为解决这个问题，我们提出了Robust State-Confounded Markov Decision Processes（RSC-MDPs），并 theoretically demonstrab其在破坏假设相关性方面的优越性。我们还设计了一种实际算法，用于学习RSC-MDPs中的鲁棒优胜策略，并在八个实际自驾和操作任务中超过所有基elines。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Detection-in-Automated-Fibre-Placement-Learning-with-Data-Limitations"><a href="#Anomaly-Detection-in-Automated-Fibre-Placement-Learning-with-Data-Limitations" class="headerlink" title="Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations"></a>Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07893">http://arxiv.org/abs/2307.07893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Assef Ghamisi, Todd Charter, Li Ji, Maxime Rivard, Gil Lund, Homayoun Najjaran</li>
<li>for: Automated Fibre Placement (AFP) 自动纤维放置系统中的缺陷检测</li>
<li>methods: 无监督深度学习和经典计算机视觉算法</li>
<li>results: 可以减少训练图像数量，同时检测到各种表面问题，并且可以准确地标识缺陷位置<details>
<summary>Abstract</summary>
Conventional defect detection systems in Automated Fibre Placement (AFP) typically rely on end-to-end supervised learning, necessitating a substantial number of labelled defective samples for effective training. However, the scarcity of such labelled data poses a challenge. To overcome this limitation, we present a comprehensive framework for defect detection and localization in Automated Fibre Placement. Our approach combines unsupervised deep learning and classical computer vision algorithms, eliminating the need for labelled data or manufacturing defect samples. It efficiently detects various surface issues while requiring fewer images of composite parts for training. Our framework employs an innovative sample extraction method leveraging AFP's inherent symmetry to expand the dataset. By inputting a depth map of the fibre layup surface, we extract local samples aligned with each composite strip (tow). These samples are processed through an autoencoder, trained on normal samples for precise reconstructions, highlighting anomalies through reconstruction errors. Aggregated values form an anomaly map for insightful visualization. The framework employs blob detection on this map to locate manufacturing defects. The experimental findings reveal that despite training the autoencoder with a limited number of images, our proposed method exhibits satisfactory detection accuracy and accurately identifies defect locations. Our framework demonstrates comparable performance to existing methods, while also offering the advantage of detecting all types of anomalies without relying on an extensive labelled dataset of defects.
</details>
<details>
<summary>摘要</summary>
Our framework employs an innovative sample extraction method that leverages AFP's inherent symmetry to expand the dataset. By inputting a depth map of the fibre layup surface, we extract local samples aligned with each composite strip (tow). These samples are processed through an autoencoder, trained on normal samples for precise reconstructions, highlighting anomalies through reconstruction errors. Aggregated values form an anomaly map for insightful visualization. The framework employs blob detection on this map to locate manufacturing defects.Experimental findings show that our proposed method exhibits satisfactory detection accuracy and accurately identifies defect locations, despite training the autoencoder with a limited number of images. Our framework demonstrates comparable performance to existing methods, while also offering the advantage of detecting all types of anomalies without relying on an extensive labelled dataset of defects.
</details></li>
</ul>
<hr>
<h2 id="Multitemporal-SAR-images-change-detection-and-visualization-using-RABASAR-and-simplified-GLR"><a href="#Multitemporal-SAR-images-change-detection-and-visualization-using-RABASAR-and-simplified-GLR" class="headerlink" title="Multitemporal SAR images change detection and visualization using RABASAR and simplified GLR"></a>Multitemporal SAR images change detection and visualization using RABASAR and simplified GLR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07892">http://arxiv.org/abs/2307.07892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiying Zhao, Charles-Alban Deledalle, Loïc Denis, Henri Maître, Jean-Marie Nicolas, Florence Tupin</li>
<li>for: 这个论文主要是为了检测土地表面的变化，尤其是不同类型的变化，如农田、建筑、港口和洪涝等。</li>
<li>methods: 本论文提出了一种简化的总体可能比率（SGLR）方法，假设同时间像素具有相同的等效数量看（ENL）。此外，本论文还提出了一种改进的光谱划分法和一种改进的变化类别法。</li>
<li>results: 本论文通过处理模拟和Synthetic Aperture Radar（SAR）图像，证明了提出的方法的效果，特别是在检测农田、建筑、港口和洪涝等区域变化方面。<details>
<summary>Abstract</summary>
Understanding the state of changed areas requires that precise information be given about the changes. Thus, detecting different kinds of changes is important for land surface monitoring. SAR sensors are ideal to fulfil this task, because of their all-time and all-weather capabilities, with good accuracy of the acquisition geometry and without effects of atmospheric constituents for amplitude data. In this study, we propose a simplified generalized likelihood ratio ($S_{GLR}$) method assuming that corresponding temporal pixels have the same equivalent number of looks (ENL). Thanks to the denoised data provided by a ratio-based multitemporal SAR image denoising method (RABASAR), we successfully applied this similarity test approach to compute the change areas. A new change magnitude index method and an improved spectral clustering-based change classification method are also developed. In addition, we apply the simplified generalized likelihood ratio to detect the maximum change magnitude time, and the change starting and ending times. Then, we propose to use an adaptation of the REACTIV method to visualize the detection results vividly. The effectiveness of the proposed methods is demonstrated through the processing of simulated and SAR images, and the comparison with classical techniques. In particular, numerical experiments proved that the developed method has good performances in detecting farmland area changes, building area changes, harbour area changes and flooding area changes.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>理解改变区域的状况需要提供精确的改变信息。因此，检测不同类型的改变是重要的 для土地表面监测。SAR感知器非常适合完成这项任务，因为它们在任何时间和任何天气情况下都有良好的探测geometry的准确性，而无需 atmospheric constituents的影响。在这项研究中，我们提出了一种简化的通用概率比例（SGLR）方法，假设相应的时间像素有同等的等效数量 Looks（ENL）。由于降噪数据提供了由比例基于多 temporal SAR图像降噪方法（RABASAR），我们成功地应用了这种相似性测试方法来计算改变区域。此外，我们还开发了一种改进的spectral clustering-based改变类型分类方法和一种最大改变幅度时间检测方法。然后，我们使用简化的SGLR方法检测改变的开始和结束时间。最后，我们提出了使用adapted REACTIV方法来Visual化检测结果的方法。实验证明了我们提出的方法的效果，包括对农地改变、建筑改变、港口改变和洪涝改变等方面的检测。
</details></li>
</ul>
<hr>
<h2 id="Investigation-of-compressor-cascade-flow-based-on-physics-informed-neural-networks"><a href="#Investigation-of-compressor-cascade-flow-based-on-physics-informed-neural-networks" class="headerlink" title="Investigation of compressor cascade flow based on physics-informed neural networks"></a>Investigation of compressor cascade flow based on physics-informed neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04501">http://arxiv.org/abs/2308.04501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihui Li, Francesco Montomoli, Sanjiv Sharma</li>
<li>for: 这项研究使用新兴的物理告知神经网络（PINNs）方法，为首次预测压缩机风场。</li>
<li>methods: 这种方法基于两维问题，包括液体力学方程在前向和反向问题中。</li>
<li>results: PINNs能够高精度地预测压缩机风场，并且在无部分边界条件的情况下，PINNs显示出了与传统CFD方法相比的明显优势。<details>
<summary>Abstract</summary>
In this study, we utilize the emerging Physics Informed Neural Networks (PINNs) approach for the first time to predict the flow field of a compressor cascade. The approach is demonstrated on a two-dimensional problem, incorporating Navier-Stokes equations in both the forward and inverse problems. In the forward problem, PINNs effectively predict the flow field of the compressor. The key advantage over Deep Neural Networks (DNNs) is that the PINNs model incorporates a physical relationship between the relevant quantities, resulting in more precise predictions. PINNs show obvious advantages over the traditional CFD approaches when dealing with inverse problems in the absence of partial boundary conditions. PINNs successfully reconstruct the flow field of the compressor cascade solely based on partial velocity vectors and wall pressure information. This research provides compelling evidence that PINNs offer turbomachinery designers a promising alternative to the current dominant CFD methods, delivering higher accuracy compared to DNNs.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们首次利用emerging Physics Informed Neural Networks（PINNs）方法预测压缩机螺旋叶流场。该方法在二维问题上进行了示例，并包括了 Navier-Stokes 方程在前向和反向问题中。在前向问题中，PINNs有效地预测了压缩机的流场。与深度神经网络（DNNs）相比，PINNs 模型具有物理关系的相互关系，从而实现了更加精确的预测。在 inverse 问题中，PINNs 显示出了与传统 CFD 方法相比的明显优势，可以在缺少部分边界条件时成功重建压缩机螺旋叶流场。这项研究提供了证明 PINNs 对液压机设计师提供了一个可靠的代替方法，比 DNNs 更高精度。
</details></li>
</ul>
<hr>
<h2 id="Handwritten-and-Printed-Text-Segmentation-A-Signature-Case-Study"><a href="#Handwritten-and-Printed-Text-Segmentation-A-Signature-Case-Study" class="headerlink" title="Handwritten and Printed Text Segmentation: A Signature Case Study"></a>Handwritten and Printed Text Segmentation: A Signature Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07887">http://arxiv.org/abs/2307.07887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Gholamian, Ali Vahdat</li>
<li>for: 提高手写和印刷文本分类精度</li>
<li>methods: 引入新的数据集SignaTR6K和模型架构</li>
<li>results: 比对先前工作提高17.9%和7.3%的IOU分数<details>
<summary>Abstract</summary>
While analyzing scanned documents, handwritten text can overlap with printed text. This overlap causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses solely on the binary classification of handwritten text or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This approach results in the assignment of overlapping handwritten and printed pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches to address the challenges of handwritten and printed text segmentation. Our objective is to recover text from different classes in their entirety, especially enhancing the segmentation performance on overlapping sections. To support this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as a new model architecture for the handwritten and printed text segmentation task. Our best configuration outperforms prior work on two different datasets by 17.9% and 7.3% on IoU scores. The SignaTR6K dataset is accessible for download via the following link: https://forms.office.com/r/2a5RDg7cAY.
</details>
<details>
<summary>摘要</summary>
While analyzing scanned documents, handwritten text can overlap with printed text. This overlap causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses solely on the binary classification of handwritten text or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This approach results in the assignment of overlapping handwritten and printed pixels to only one of the classes, and thus, they are not accounted for in the other class. Therefore, in this research, we develop novel approaches to address the challenges of handwritten and printed text segmentation. Our objective is to recover text from different classes in their entirety, especially enhancing the segmentation performance on overlapping sections. To support this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as a new model architecture for the handwritten and printed text segmentation task. Our best configuration outperforms prior work on two different datasets by 17.9% and 7.3% on IoU scores. The SignaTR6K dataset is accessible for download via the following link: https://forms.office.com/r/2a5RDg7cAY.
</details></li>
</ul>
<hr>
<h2 id="Intuitionistic-Fuzzy-Broad-Learning-System-Enhancing-Robustness-Against-Noise-and-Outliers"><a href="#Intuitionistic-Fuzzy-Broad-Learning-System-Enhancing-Robustness-Against-Noise-and-Outliers" class="headerlink" title="Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers"></a>Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08713">http://arxiv.org/abs/2307.08713</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Sajid, A. K. Malik, M. Tanveer<br>for: 提高 Broad Learning System (BLS) 的稳定性和有效性，应对实际数据集中噪声和异常值的问题。methods: 提出了两种改进 BLS 模型：含隐式含度的 F-BLS 模型和基于直觉含数理论的 IF-BLS 模型。两种模型都使用距离函数来评估训练点的成员度，但是 F-BLS 模型只考虑训练点与类中心的距离，而 IF-BLS 模型则考虑训练点的含度和非含度。results: 对于 44 个 UCI 数据集和 ADNI 数据集，提出的 F-BLS 和 IF-BLS 模型都显示出优于基eline 模型的总体化能力和鲁棒性。具有噪声的 UCI 数据集上，提出的方法也能够保持比较高的总体化能力和鲁棒性。<details>
<summary>Abstract</summary>
In the realm of data classification, broad learning system (BLS) has proven to be a potent tool that utilizes a layer-by-layer feed-forward neural network. It consists of feature learning and enhancement segments, working together to extract intricate features from input data. The traditional BLS treats all samples as equally significant, which makes it less robust and less effective for real-world datasets with noises and outliers. To address this issue, we propose the fuzzy BLS (F-BLS) model, which assigns a fuzzy membership value to each training point to reduce the influence of noises and outliers. In assigning the membership value, the F-BLS model solely considers the distance from samples to the class center in the original feature space without incorporating the extent of non-belongingness to a class. We further propose a novel BLS based on intuitionistic fuzzy theory (IF-BLS). The proposed IF-BLS utilizes intuitionistic fuzzy numbers based on fuzzy membership and non-membership values to assign scores to training points in the high-dimensional feature space by using a kernel function. We evaluate the performance of proposed F-BLS and IF-BLS models on 44 UCI benchmark datasets across diverse domains. Furthermore, Gaussian noise is added to some UCI datasets to assess the robustness of the proposed F-BLS and IF-BLS models. Experimental results demonstrate superior generalization performance of the proposed F-BLS and IF-BLS models compared to baseline models, both with and without Gaussian noise. Additionally, we implement the proposed F-BLS and IF-BLS models on the Alzheimers Disease Neuroimaging Initiative (ADNI) dataset, and promising results showcase the models effectiveness in real-world applications. The proposed methods offer a promising solution to enhance the BLS frameworks ability to handle noise and outliers.
</details>
<details>
<summary>摘要</summary>
在数据分类领域，广泛学习系统（BLS）已经证明是一种强大的工具，使用层次 feed-forward 神经网络。它包括特征学习和提升段，共同提取输入数据中细腻的特征。传统的 BLS 对所有样本都视为一样重要，这使得它在真实世界的数据集中 less robust 和 less effective。为解决这个问题，我们提出了模糊 BLS（F-BLS）模型，它将对each training point 分配模糊会员价值，以降低噪声和异常值的影响。在分配会员价值时，F-BLS 模型只考虑样本与类中心的距离在原始特征空间中，不考虑类外的非属性程度。我们还提出了基于INTUITIONISTIC FUZZY理论（IF-BLS）的新模型。该模型使用INTUITIONISTIC FUZZY数字，基于模糊会员价值和非会员价值来对训练点进行分配分数。我们对44个 UCI benchmark 数据集进行了评估，并在一些 UCI 数据集上添加了高斯噪声，以评估我们提出的 F-BLS 和 IF-BLS 模型的Robustness。实验结果表明我们的提出的 F-BLS 和 IF-BLS 模型在对比基eline模型时表现出优化的总体化能力，同时在噪声和异常值存在的情况下也有优异的表现。此外，我们在ADNI 数据集上实现了我们的 F-BLS 和 IF-BLS 模型，并得到了有 promise 的结果，这表明我们的方法在真实世界应用中具有潜在的价值。我们的方法可以增强 BLS 框架对噪声和异常值的处理能力。
</details></li>
</ul>
<hr>
<h2 id="Gradient-free-training-of-neural-ODEs-for-system-identification-and-control-using-ensemble-Kalman-inversion"><a href="#Gradient-free-training-of-neural-ODEs-for-system-identification-and-control-using-ensemble-Kalman-inversion" class="headerlink" title="Gradient-free training of neural ODEs for system identification and control using ensemble Kalman inversion"></a>Gradient-free training of neural ODEs for system identification and control using ensemble Kalman inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07882">http://arxiv.org/abs/2307.07882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/computationalscience/eki-neural-ode">https://gitlab.com/computationalscience/eki-neural-ode</a></li>
<li>paper_authors: Lucas Böttcher</li>
<li>for:  solves inverse problems within a Bayesian framework for system identification and optimal control tasks</li>
<li>methods:  Ensemble Kalman inversion (EKI), a sequential Monte Carlo method that is gradient-free and only requires forward passes to evaluate artificial neural networks</li>
<li>results:  EKI is an efficient method for training neural ODEs, with competitive runtime and solution quality compared to commonly used gradient-based optimizers.<details>
<summary>Abstract</summary>
Ensemble Kalman inversion (EKI) is a sequential Monte Carlo method used to solve inverse problems within a Bayesian framework. Unlike backpropagation, EKI is a gradient-free optimization method that only necessitates the evaluation of artificial neural networks in forward passes. In this study, we examine the effectiveness of EKI in training neural ordinary differential equations (neural ODEs) for system identification and control tasks. To apply EKI to optimal control problems, we formulate inverse problems that incorporate a Tikhonov-type regularization term. Our numerical results demonstrate that EKI is an efficient method for training neural ODEs in system identification and optimal control problems, with runtime and quality of solutions that are competitive with commonly used gradient-based optimizers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Graph-Embedded-Intuitionistic-Fuzzy-RVFL-for-Class-Imbalance-Learning"><a href="#Graph-Embedded-Intuitionistic-Fuzzy-RVFL-for-Class-Imbalance-Learning" class="headerlink" title="Graph Embedded Intuitionistic Fuzzy RVFL for Class Imbalance Learning"></a>Graph Embedded Intuitionistic Fuzzy RVFL for Class Imbalance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07881">http://arxiv.org/abs/2307.07881</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. A. Ganaie, M. Sajid, A. K. Malik, M. Tanveer</li>
<li>for: 该论文目的是解决机器学习领域中的类异常学习问题，即处理含有少量样本的训练集时，模型往往受到类别偏好的影响，导致少量类型的样本被排除。</li>
<li>methods: 该论文提出了一种基于图像函数链（RVFL）网络的新型分类模型，称为图像函数链 intuitionistic 瑞利（GE-IFRVFL）模型。该模型利用图像函数来提取数据集中的含义丰富信息，并使用 Intuitionistic 瑞利集来处理数据中的不确定性和不准确性。</li>
<li>results: 该论文在多个 benchmark 不均衡数据集上进行了实验，结果表明，与传统 RVFL 网络相比，GE-IFRVFL 模型在处理不均衡数据集时表现出了明显的优势。此外，该论文还应用了该模型在 ADNI 数据集上，并取得了良好的结果，证明该模型在实际应用中也具有出色的表现。<details>
<summary>Abstract</summary>
The domain of machine learning is confronted with a crucial research area known as class imbalance learning, which presents considerable hurdles in the precise classification of minority classes. This issue can result in biased models where the majority class takes precedence in the training process, leading to the underrepresentation of the minority class. The random vector functional link (RVFL) network is a widely-used and effective learning model for classification due to its speed and efficiency. However, it suffers from low accuracy when dealing with imbalanced datasets. To overcome this limitation, we propose a novel graph embedded intuitionistic fuzzy RVFL for class imbalance learning (GE-IFRVFL-CIL) model incorporating a weighting mechanism to handle imbalanced datasets. The proposed GE-IFRVFL-CIL model has a plethora of benefits, such as $(i)$ it leverages graph embedding to extract semantically rich information from the dataset, $(ii)$ it uses intuitionistic fuzzy sets to handle uncertainty and imprecision in the data, $(iii)$ and the most important, it tackles class imbalance learning. The amalgamation of a weighting scheme, graph embedding, and intuitionistic fuzzy sets leads to the superior performance of the proposed model on various benchmark imbalanced datasets, including UCI and KEEL. Furthermore, we implement the proposed GE-IFRVFL-CIL on the ADNI dataset and achieved promising results, demonstrating the model's effectiveness in real-world applications. The proposed method provides a promising solution for handling class imbalance in machine learning and has the potential to be applied to other classification problems.
</details>
<details>
<summary>摘要</summary>
machine learning 领域面临一个重要的研究领域，即类别不均衡学习（Class Imbalance Learning，简称 CIL），这种情况可能导致模型偏向主要类别，从而导致少数类别的下 represencing。Random vector functional link（RVFL）网络是一种广泛使用和高效的学习模型，但它在不均衡数据集上表现不佳。为了解决这个限制，我们提出了一种基于图embeded intuitionistic fuzzy RVFL（GE-IFRVFL-CIL）模型，该模型具有以下优点：$(i)$ 利用图embeded提取数据集中具有含义的信息；$(ii)$ 使用intuitionistic fuzzy sets处理数据中的不确定和不准确性；$(iii)$ 特别是，解决类别不均衡学习问题。通过将权重机制、图embeded和intuitionistic fuzzy sets结合在一起，我们的模型在多个benchmark不均衡数据集上表现出色，包括UCI和KEEL。此外，我们在ADNI数据集上实现了该模型，并获得了可观的结果，证明了模型在实际应用中的效果。本方法为处理机器学习中的类别不均衡提供了一个有 Promise的解决方案，并可以应用于其他分类问题。
</details></li>
</ul>
<hr>
<h2 id="Why-Does-Little-Robustness-Help-Understanding-Adversarial-Transferability-From-Surrogate-Training"><a href="#Why-Does-Little-Robustness-Help-Understanding-Adversarial-Transferability-From-Surrogate-Training" class="headerlink" title="Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training"></a>Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07873">http://arxiv.org/abs/2307.07873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi, Minghui Li, Xiaogeng Liu, Wei Wan, Hai Jin</li>
<li>for: 本研究旨在更深入地理解对 Deep Neural Networks (DNNs) 的抗击例软件的可迁移性，尤其是在代理模型方面。</li>
<li>methods: 本研究使用了一系列的理论和实验分析，探讨了两个主要因素——模型平滑性和梯度相似性——如何影响对 DNNs 的抗击例软件的可迁移性。</li>
<li>results: 研究发现，在对 DNNs 进行 adversarial 训练时，模型平滑性和梯度相似性之间存在负相关关系，而这两个因素又与对 DNNs 的抗击例软件的可迁移性有着普遍的影响。通过调整数据增强和梯度正则化，可以同时提高模型平滑性和梯度相似性，从而提高对 DNNs 的抗击例软件的可迁移性。<details>
<summary>Abstract</summary>
Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.
</details>
<details>
<summary>摘要</summary>
深度学习模型（DNN）的敌对示例（AE）已经被证明可以传播：AE 可以在不同的模型结构下骗别的黑盒模型。虽然一些实验研究提供了生成高度传播的AE的指导，但是大多数这些发现缺乏解释，甚至导致不一致的建议。在这篇文章中，我们带领读者一步进一步地了解对抗传播性，特别是在代理方面。我们从小的Robustness现象出发，其中模型在弱相对抗样本上进行了适应性训练后，可以作为更好的代理模型。我们归因这一现象于两个主要因素的贡献：模型的平滑性和梯度相似性。我们的分析将注重这两个因素之间的共同效应，而不是它们与传播性之间的相互关系。通过一系列理论和实验分析，我们提出了数据分布shift在对抗训练中的影响，以及如何通过数据增强和梯度规则来调节传播性。最后，我们提出了一种通用的制定机制，可以同时优化模型的平滑性和梯度相似性，并通过广泛的实验证明其效果。因此，我们呼吁关注这两个因素的共同影响，而不是仅仅优化一个而忽略另一个，并强调在制定代理模型时的重要性。
</details></li>
</ul>
<hr>
<h2 id="Does-Double-Descent-Occur-in-Self-Supervised-Learning"><a href="#Does-Double-Descent-Occur-in-Self-Supervised-Learning" class="headerlink" title="Does Double Descent Occur in Self-Supervised Learning?"></a>Does Double Descent Occur in Self-Supervised Learning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07872">http://arxiv.org/abs/2307.07872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonatangideoni/double_descent_tiny_paper">https://github.com/yonatangideoni/double_descent_tiny_paper</a></li>
<li>paper_authors: Alisia Lupidi, Yonatan Gideoni, Dulhan Jayalath</li>
<li>for:  investigate the existence of double descent in self-supervised models</li>
<li>methods: use standard and linear autoencoders, two previously unstudied settings</li>
<li>results: the test loss either has a classical U-shape or monotonically decreases, without exhibiting a double-descent curve.<details>
<summary>Abstract</summary>
Most investigations into double descent have focused on supervised models while the few works studying self-supervised settings find a surprising lack of the phenomenon. These results imply that double descent may not exist in self-supervised models. We show this empirically using a standard and linear autoencoder, two previously unstudied settings. The test loss is found to have either a classical U-shape or to monotonically decrease instead of exhibiting a double-descent curve. We hope that further work on this will help elucidate the theoretical underpinnings of this phenomenon.
</details>
<details>
<summary>摘要</summary>
大多数调查双峰现象都集中在指导学习模型上，而自适应学习设置中的研究却有很少。这些结果表明双峰现象可能不存在于自适应模型中。我们通过标准和线性自动编码器两种未研究过的设置来证实这一点。测试损失的曲线可以分为两种：一种是经典的U型曲线，另一种是 monotonically decrease 而不是展现双峰曲线。我们希望未来的研究能够深入探讨这一现象的理论基础。
</details></li>
</ul>
<hr>
<h2 id="The-SocialAI-School-Insights-from-Developmental-Psychology-Towards-Artificial-Socio-Cultural-Agents"><a href="#The-SocialAI-School-Insights-from-Developmental-Psychology-Towards-Artificial-Socio-Cultural-Agents" class="headerlink" title="The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents"></a>The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07871">http://arxiv.org/abs/2307.07871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grgur Kovač, Rémy Portelas, Peter Ford Dominey, Pierre-Yves Oudeyer</li>
<li>for: 这个论文主要是为了探讨人工智能在社会交互 Setting中的发展，以及如何通过发展心理学来帮助AI研究社会智能。</li>
<li>methods: 这篇论文使用了Michael Tomasello和Jerome Bruner等发展心理学家的理论，并提出了一个可 parameterized 的社交AI学校，用于帮助研究者进行相关的实验和研究。</li>
<li>results: 这篇论文的结果表明，通过使用社交AI学校，可以让RL代理和大语言模型在社交交互 Setting中展现出更高的社会智能能力。同时，这篇论文也提供了一个简单的入门点，以帮助AI研究者更好地理解和应用发展心理学。<details>
<summary>Abstract</summary>
Developmental psychologists have long-established the importance of socio-cognitive abilities in human intelligence. These abilities enable us to enter, participate and benefit from human culture. AI research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). We argue that AI research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture too. We discuss the theories of Michael Tomasello and Jerome Bruner to introduce some of their concepts to AI and outline key concepts and socio-cognitive abilities. We present The SocialAI school - a tool including a customizable parameterized uite of procedurally generated environments, which simplifies conducting experiments regarding those concepts. We show examples of such experiments with RL agents and Large Language Models. The main motivation of this work is to engage the AI community around the problem of social intelligence informed by developmental psychology, and to provide a tool to simplify first steps in this direction. Refer to the project website for code and additional information: https://sites.google.com/view/socialai-school.
</details>
<details>
<summary>摘要</summary>
We draw on the theories of Michael Tomasello and Jerome Bruner to introduce some of their concepts in AI and outline key socio-cognitive abilities. We present the SocialAI school, a tool that includes a customizable parameterized suite of procedurally generated environments, which simplifies conducting experiments regarding these concepts. We demonstrate examples of such experiments with reinforcement learning (RL) agents and large language models.Our main motivation is to engage the AI community in the problem of social intelligence informed by developmental psychology and provide a tool to simplify initial steps in this direction. For more information and access to the project's code, please refer to the project website at <https://sites.google.com/view/socialai-school>.
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-as-Superpositions-of-Cultural-Perspectives"><a href="#Large-Language-Models-as-Superpositions-of-Cultural-Perspectives" class="headerlink" title="Large Language Models as Superpositions of Cultural Perspectives"></a>Large Language Models as Superpositions of Cultural Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07870">http://arxiv.org/abs/2307.07870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grgur Kovač, Masataka Sawayama, Rémy Portelas, Cédric Colas, Peter Ford Dominey, Pierre-Yves Oudeyer</li>
<li>for: 这 paper 主要研究了大型自然语言模型 (LLMs) 是如何被识别为具有人性或价值观的问题。</li>
<li>methods: 作者使用了问卷调查 (PVQ, VSM, IPIP) 来研究 LLMS 在不同情境下表达的价值和人性特质是如何变化的。他们还通过质量实验来示例 LLMS 在不同情境下表达的价值是如何改变的。</li>
<li>results: 研究发现 LLMS 在不同情境下表达的价值和人性特质是 Context-dependent 的，并且可以通过不同的方法来控制这些表达。同时，作者还发现了不同模型的 drivability 和可控性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrating their context-dependent nature). We then conduct quantitative experiments to study the controllability of different models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the effectiveness of various methods for inducing perspectives, and the smoothness of the models' drivability. We conclude by examining the broader implications of our work and outline a variety of associated scientific questions. The project website is available at https://sites.google.com/view/llm-superpositions .
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）经常被误认为具有人格或一组价值观。我们认为，LLM可以看作是一个积加的视角，具有不同的价值观和人格特质。LLM在不同的上下文中展现出不同的价值观和人格特质，而人类在不同上下文中的价值观和人格特质往往更加一致。我们提出了“视角可控性”概念，即模型可以采取不同的视角，以便采取不同的价值观和人格特质。我们通过心理测试（PVQ、VSM、IPIP）研究了LLM在不同上下文中表达的价值观和人格特质是如何变化的。我们还通过质量实验表明，LLM在不同的提示下表达不同的价值观，而且这些价值观不一定是明确地表达出来的（ demonstrate 其上下文相依性）。我们然后进行了量化实验，研究不同模型（GPT-4、GPT-3.5、OpenAssistant、StableVicuna、StableLM）的可控性，不同方法的影响和模型的顺略性。我们最后结论，我们的工作有很多相关科学问题，并提出了一些新的科学问题。相关研究网站地址为 <https://sites.google.com/view/llm-superpositions>。
</details></li>
</ul>
<hr>
<h2 id="Custom-DNN-using-Reward-Modulated-Inverted-STDP-Learning-for-Temporal-Pattern-Recognition"><a href="#Custom-DNN-using-Reward-Modulated-Inverted-STDP-Learning-for-Temporal-Pattern-Recognition" class="headerlink" title="Custom DNN using Reward Modulated Inverted STDP Learning for Temporal Pattern Recognition"></a>Custom DNN using Reward Modulated Inverted STDP Learning for Temporal Pattern Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07869">http://arxiv.org/abs/2307.07869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vijay Shankaran Vivekanand, Rajkumar Kubendran</li>
<li>for: 本研究旨在提出一种高效的时间峰检测算法，用于各种领域，如异常检测、关键词检测和神经科学。</li>
<li>methods: 该算法结合奖金补偿行为、HEBBbian和反HEBBbian基于学习方法，可以在动态数据集上高效地识别时间峰模式。</li>
<li>results: 对于一个复杂的语音数据集，该算法的表现比 estado-of-the-art 更高， indicating that the algorithm can effectively recognize temporal spike patterns in real-world data.<details>
<summary>Abstract</summary>
Temporal spike recognition plays a crucial role in various domains, including anomaly detection, keyword spotting and neuroscience. This paper presents a novel algorithm for efficient temporal spike pattern recognition on sparse event series data. The algorithm leverages a combination of reward-modulatory behavior, Hebbian and anti-Hebbian based learning methods to identify patterns in dynamic datasets with short intervals of training. The algorithm begins with a preprocessing step, where the input data is rationalized and translated to a feature-rich yet sparse spike time series data. Next, a linear feed forward spiking neural network processes this data to identify a trained pattern. Finally, the next layer performs a weighted check to ensure the correct pattern has been detected.To evaluate the performance of the proposed algorithm, it was trained on a complex dataset containing spoken digits with spike information and its output compared to state-of-the-art.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Temporal spike recognition plays a crucial role in various domains, including anomaly detection, keyword spotting, and neuroscience. This paper presents a novel algorithm for efficient temporal spike pattern recognition on sparse event series data. The algorithm leverages a combination of reward-modulatory behavior, Hebbian, and anti-Hebbian based learning methods to identify patterns in dynamic datasets with short intervals of training. The algorithm begins with a preprocessing step, where the input data is rationalized and translated to a feature-rich yet sparse spike time series data. Next, a linear feed forward spiking neural network processes this data to identify a trained pattern. Finally, the next layer performs a weighted check to ensure the correct pattern has been detected. To evaluate the performance of the proposed algorithm, it was trained on a complex dataset containing spoken digits with spike information and its output compared to state-of-the-art.中文简体版： Temporal spike recognition在各种领域都扮演着关键角色，包括异常检测、关键词检测和神经科学。本文提出了一种高效的时间脉冲模式识别算法，用于处理缺省事件序列数据。该算法结合奖励调节行为、希伯纳和反希伯纳基本学习方法，在短期培训下 indentify动态数据中的模式。该算法的前期处理步骤将输入数据理解化和转换为具有丰富特征 yet sparse spike时间序列数据。接着，一个线性径向冲击神经网络处理这些数据，以标识已经训练的模式。最后，下一层 performs一个权重检查，以确保正确的模式已经被检测到。为评估提出的算法性能，它被训练在一个包含 spoken digits 的复杂数据集上，并与当前领先的输出进行比较。
</details></li>
</ul>
<hr>
<h2 id="Contrasting-the-efficiency-of-stock-price-prediction-models-using-various-types-of-LSTM-models-aided-with-sentiment-analysis"><a href="#Contrasting-the-efficiency-of-stock-price-prediction-models-using-various-types-of-LSTM-models-aided-with-sentiment-analysis" class="headerlink" title="Contrasting the efficiency of stock price prediction models using various types of LSTM models aided with sentiment analysis"></a>Contrasting the efficiency of stock price prediction models using various types of LSTM models aided with sentiment analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07868">http://arxiv.org/abs/2307.07868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varun Sangwan, Vishesh Kumar Singh, Bibin Christopher V</li>
<li>for: 该研究目标是找到使用公司预测和行业表现来正确预测股票价格，包括短期和长期目标。</li>
<li>methods: 该研究使用公司预测和行业表现来建立模型，以便预测股票价格。</li>
<li>results: 该研究得到的结果可以帮助投资者更好地理解公司的股票价格，并且可以用于长期和短期投资决策。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Our research aims to find the best model that uses companies projections and sector performances and how the given company fares accordingly to correctly predict equity share prices for both short and long term goals.
</details>
<details>
<summary>摘要</summary>
我们的研究目标是找到最佳的模型，该使用公司预测和行业表现来正确预测股票价格，包括短期和长期目标。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-the-Effectiveness-of-Classification-Algorithms-and-SVM-Kernels-for-Dry-Beans"><a href="#Benchmarking-the-Effectiveness-of-Classification-Algorithms-and-SVM-Kernels-for-Dry-Beans" class="headerlink" title="Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans"></a>Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07863">http://arxiv.org/abs/2307.07863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anant Mehta, Prajit Sengupta, Divisha Garg, Harpreet Singh, Yosi Shacham Diamand</li>
<li>for: 增强作物产量，植物育种者和农业研究人员可以通过识别感兴趣特征、疾病抵抗力和营养含量来提高作物产量。</li>
<li>methods: 本研究使用了不同的支持向量机(SVM)分类算法，包括直线、多项式和径向基函数(RBF)，以及其他流行的分类算法进行比较和分析。为了降低维度，使用了主成分分析(PCA)进行预处理。</li>
<li>results: 研究发现，使用径向基函数(RBF) SVM 算法可以达到最高的准确率（93.34%）、精度（92.61%）、回归率（92.35%）和 F1 分数（91.40%）。此外，研究还提供了有用的视觉化和实验分析，为识别不同 SVM 算法在复杂和非线性结构的数据集中的重要性提供了有价值的指导。<details>
<summary>Abstract</summary>
Plant breeders and agricultural researchers can increase crop productivity by identifying desirable features, disease resistance, and nutritional content by analysing the Dry Bean dataset. This study analyses and compares different Support Vector Machine (SVM) classification algorithms, namely linear, polynomial, and radial basis function (RBF), along with other popular classification algorithms. The analysis is performed on the Dry Bean Dataset, with PCA (Principal Component Analysis) conducted as a preprocessing step for dimensionality reduction. The primary evaluation metric used is accuracy, and the RBF SVM kernel algorithm achieves the highest Accuracy of 93.34%, Precision of 92.61%, Recall of 92.35% and F1 Score as 91.40%. Along with adept visualization and empirical analysis, this study offers valuable guidance by emphasizing the importance of considering different SVM algorithms for complex and non-linear structured datasets.
</details>
<details>
<summary>摘要</summary>
植物育种者和农业研究人员可以通过识别有利特征、疾病抵抗力和营养含量来提高作物产量。这个研究分析和比较不同的支持向量机器学习（SVM）分类算法，包括直线、多项式和径向基函数（RBF），以及其他流行的分类算法。研究使用了干豇数据集，先进行了主成分分析（PCA）作为维度减少步骤。主要评价指标是准确率，RBF SVM 算法实现了最高的准确率为 93.34%、精度为 92.61%、回归率为 92.35% 和 F1 分数为 91.40%。此外，研究还提供了丰富的视觉化和实证分析，为复杂和非线性结构数据中的SVM算法选择提供了有价值的指导。
</details></li>
</ul>
<hr>
<h2 id="Automated-Knowledge-Modeling-for-Cancer-Clinical-Practice-Guidelines"><a href="#Automated-Knowledge-Modeling-for-Cancer-Clinical-Practice-Guidelines" class="headerlink" title="Automated Knowledge Modeling for Cancer Clinical Practice Guidelines"></a>Automated Knowledge Modeling for Cancer Clinical Practice Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10231">http://arxiv.org/abs/2307.10231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pralaypati Ta, Bhumika Gupta, Arihant Jain, Sneha Sree C, Arunima Sarkar, Keerthi Ram, Mohanasankar Sivaprakasam<br>for:This paper aims to develop an automated method for extracting knowledge from National Comprehensive Cancer Network (NCCN) Clinical Practice Guidelines (CPGs) in Oncology and generating a structured model containing the retrieved knowledge.methods:The proposed method uses natural language processing (NLP) techniques to extract knowledge from NCCN CPGs, and employs a knowledge model to represent the extracted knowledge in a structured format. The method also includes three enrichment strategies to enhance the model: Cancer staging information, UMLS Metathesaurus &amp; NCIt concepts, and Node classification.results:The proposed method was tested using two versions of NCCN Non-Small Cell Lung Cancer (NSCLC) CPG, and achieved a high accuracy of 0.81 in Node classification using a Support Vector Machine (SVM) model with 10-fold cross-validation.<details>
<summary>Abstract</summary>
Clinical Practice Guidelines (CPGs) for cancer diseases evolve rapidly due to new evidence generated by active research. Currently, CPGs are primarily published in a document format that is ill-suited for managing this developing knowledge. A knowledge model of the guidelines document suitable for programmatic interaction is required. This work proposes an automated method for extraction of knowledge from National Comprehensive Cancer Network (NCCN) CPGs in Oncology and generating a structured model containing the retrieved knowledge. The proposed method was tested using two versions of NCCN Non-Small Cell Lung Cancer (NSCLC) CPG to demonstrate the effectiveness in faithful extraction and modeling of knowledge. Three enrichment strategies using Cancer staging information, Unified Medical Language System (UMLS) Metathesaurus & National Cancer Institute thesaurus (NCIt) concepts, and Node classification are also presented to enhance the model towards enabling programmatic traversal and querying of cancer care guidelines. The Node classification was performed using a Support Vector Machine (SVM) model, achieving a classification accuracy of 0.81 with 10-fold cross-validation.
</details>
<details>
<summary>摘要</summary>
临床实践指南 (CPGs) for cancer diseases 在新证据的激发下不断发展。目前，CPGs 主要以文档格式发布，这种格式不适合管理这些发展中的知识。这项工作提出了一种自动提取 CPGS 文档中的知识并生成一个结构化模型的方法。该方法在使用两个版本的 National Comprehensive Cancer Network (NCCN) Non-Small Cell Lung Cancer (NSCLC) CPG 进行测试，并证明了 faithful 提取和模型知识的效果。此外，文章还提出了三种润色策略，使得模型具有可programmatic traversal和querying cancer care guidelines的能力。这三种润色策略分别是使用 Cancer 分期信息、Unified Medical Language System (UMLS) Metathesaurus & National Cancer Institute thesaurus (NCIt) 概念以及节点分类。Node 分类使用 Support Vector Machine (SVM) 模型，在10-fold cross-validation中达到了0.81的分类精度。
</details></li>
</ul>
<hr>
<h2 id="Variational-Inference-with-Gaussian-Score-Matching"><a href="#Variational-Inference-with-Gaussian-Score-Matching" class="headerlink" title="Variational Inference with Gaussian Score Matching"></a>Variational Inference with Gaussian Score Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07849">http://arxiv.org/abs/2307.07849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/modichirag/gsm-vi">https://github.com/modichirag/gsm-vi</a></li>
<li>paper_authors: Chirag Modi, Charles Margossian, Yuling Yao, Robert Gower, David Blei, Lawrence Saul</li>
<li>For: The paper is written for researchers and practitioners interested in Bayesian inference and variational inference.* Methods: The paper proposes a new approach to variational inference called score matching variational inference (GSM-VI), which is based on the principle of score matching and can be applied to a wide class of models. The algorithm iteratively adjusts the variational estimate to match the scores at a newly sampled value of the latent variables.* Results: The paper compares GSM-VI to black box variational inference (BBVI) and studies how GSM-VI behaves as a function of the problem dimensionality, the condition number of the target covariance matrix, and the degree of mismatch between the approximating and exact posterior distribution. The results show that GSM-VI is faster than BBVI and requires fewer gradient evaluations to obtain a comparable quality of approximation.<details>
<summary>Abstract</summary>
Variational inference (VI) is a method to approximate the computationally intractable posterior distributions that arise in Bayesian statistics. Typically, VI fits a simple parametric distribution to the target posterior by minimizing an appropriate objective such as the evidence lower bound (ELBO). In this work, we present a new approach to VI based on the principle of score matching, that if two distributions are equal then their score functions (i.e., gradients of the log density) are equal at every point on their support. With this, we develop score matching VI, an iterative algorithm that seeks to match the scores between the variational approximation and the exact posterior. At each iteration, score matching VI solves an inner optimization, one that minimally adjusts the current variational estimate to match the scores at a newly sampled value of the latent variables. We show that when the variational family is a Gaussian, this inner optimization enjoys a closed form solution, which we call Gaussian score matching VI (GSM-VI). GSM-VI is also a ``black box'' variational algorithm in that it only requires a differentiable joint distribution, and as such it can be applied to a wide class of models. We compare GSM-VI to black box variational inference (BBVI), which has similar requirements but instead optimizes the ELBO. We study how GSM-VI behaves as a function of the problem dimensionality, the condition number of the target covariance matrix (when the target is Gaussian), and the degree of mismatch between the approximating and exact posterior distribution. We also study GSM-VI on a collection of real-world Bayesian inference problems from the posteriorDB database of datasets and models. In all of our studies we find that GSM-VI is faster than BBVI, but without sacrificing accuracy. It requires 10-100x fewer gradient evaluations to obtain a comparable quality of approximation.
</details>
<details>
<summary>摘要</summary>
“统计学中的统计推理（Variational Inference，VI）是一种方法估计computationally intractable的 posterior distribution。通常，VI使用一个简单的 parametric distribution 来替代目标 posterior，并且使用一个适当的目标函数，例如证据下界（Evidence Lower Bound，ELBO）来对其进行最佳化。在这个研究中，我们提出了一种基于得分匹配原理的新方法，即得分匹配VI（Score Matching VI，SM-VI）。这个方法的基本思想是，如果两个分布相同，则它们的得分函数（即分布的LOG值的导数）在它们的支持集上也是相同的。我们透过对SM-VI进行迭代运算，将得分匹配到目标 posterior 中的分布。在每个迭代中，SM-VI解决一个内部优化问题，将当前的渠道估计匹配到目标 posterior 中的分布。当渠道家族为 Gaussian 时，内部优化问题具有关注形式的解，我们称之为 Gaussian Score Matching VI（GSM-VI）。GSM-VI 也是一个“黑盒子”渠道推理方法，它只需要一个可微的共同分布，并且可以应用到广泛的模型中。我们与黑盒子推理（BBVI）进行比较，BBVI 的要求相同，但是它将 ELBO 优化而不是得分匹配。我们研究了 GSM-VI 的行为，包括问题的维度、目标均值矩阵的条件数（当目标为 Gaussian 时）和渠道估计和实际 posterior 的匹配程度。我们还对一些真实世界的 Bayesian 推理问题进行了研究，包括 posteriorDB 数据库中的数据和模型。在所有的研究中，我们发现 GSM-VI 比 BBVI 快速，并且无需牺牲精度。GSM-VI 需要 10-100 倍 fewer gradient evaluations 以取得相同质量的渠道估计。”
</details></li>
</ul>
<hr>
<h2 id="Neural-Video-Recovery-for-Cloud-Gaming"><a href="#Neural-Video-Recovery-for-Cloud-Gaming" class="headerlink" title="Neural Video Recovery for Cloud Gaming"></a>Neural Video Recovery for Cloud Gaming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07847">http://arxiv.org/abs/2307.07847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Zhaoyuan He, Yifan Yang, Shuozhe Li, Diyuan Dai, Lili Qiu</li>
<li>for: 提高云游戏的视频恢复率和质量，以提供更好的游戏体验。</li>
<li>methods: 使用游戏状态进行视频恢复，并使用部分解码的帧来恢复丢失的视频部分。开发了一个整体系统，包括提取游戏状态、修改H.264视频解码器生成恢复帧的掩码，以及设计一种新的神经网络来恢复完整或部分的视频帧。</li>
<li>results: 通过iPhone 12和笔记机实现，证明了游戏状态在视频恢复中的重要性，以及我们的总体设计的有效性。<details>
<summary>Abstract</summary>
Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural network to recover either complete or partial video frames. Our approach is extensively evaluated using iPhone 12 and laptop implementations, and we demonstrate the utility of game states in the game video recovery and the effectiveness of our overall design.
</details>
<details>
<summary>摘要</summary>
云游戏是一个多亿美元的industry。一个客户端在云游戏中将其运动发送到游戏服务器上的互联网上，服务器将其渲染并将结果视频回传。为了提供良好的游戏体验，云游戏需要的延迟时间在80ms左右。这意味着视频渲染、编码、传输、解码和显示都需要在这个时间段内完成，这是特别是由服务器过载、网络拥堵和 losses 而具有挑战性。在这篇论文中，我们提出了一种新的视频帧恢复方法，与传统的视频帧恢复方法不同的是，我们的方法使用游戏状态以显著提高恢复精度，并使用部分解码的帧来恢复丢失的部分。我们开发了一个整体系统，包括（i）高效地提取游戏状态，（ii）修改H.264视频解码器生成一个面积指示需要恢复的视频帧部分，以及（iii）设计一种新的神经网络来恢复完整或部分的视频帧。我们的方法在iPhone 12和笔记机实现中进行了广泛的测试，并证明了游戏状态在游戏视频恢复中的重要性和我们的整体设计的有效性。
</details></li>
</ul>
<hr>
<h2 id="Transformers-are-Universal-Predictors"><a href="#Transformers-are-Universal-Predictors" class="headerlink" title="Transformers are Universal Predictors"></a>Transformers are Universal Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07843">http://arxiv.org/abs/2307.07843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danderfer/Comp_Sci_Sem_2">https://github.com/danderfer/Comp_Sci_Sem_2</a></li>
<li>paper_authors: Sourya Basu, Moulik Choraria, Lav R. Varshney</li>
<li>for: 这篇论文是研究Transformer架构的语言模型限制和其在信息论中的通用预测性的。</li>
<li>methods: 论文使用了信息论的方法来分析Transformer架构的性能，并在非对称数据 regime中分析不同组件的作用，特别是在数据效率训练中。</li>
<li>results: 实验表明，Transformer架构在 sintetic 和实际数据上具有良好的性能，且可以在数据效率训练中获得优秀的结果。<details>
<summary>Abstract</summary>
We find limits to the Transformer architecture for language modeling and show it has a universal prediction property in an information-theoretic sense. We further analyze performance in non-asymptotic data regimes to understand the role of various components of the Transformer architecture, especially in the context of data-efficient training. We validate our theoretical analysis with experiments on both synthetic and real datasets.
</details>
<details>
<summary>摘要</summary>
我们发现 transformer 架构在语言模型预测中存在限制，并证明它有一种普遍预测性质在信息理论上。我们进一步分析不同组件的转换器架构在非对称数据 régime 中的表现，尤其是在数据效果训练中。我们 validate our 理论分析通过实验测试 synthetic 和实际数据集。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="RegExplainer-Generating-Explanations-for-Graph-Neural-Networks-in-Regression-Task"><a href="#RegExplainer-Generating-Explanations-for-Graph-Neural-Networks-in-Regression-Task" class="headerlink" title="RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task"></a>RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07840">http://arxiv.org/abs/2307.07840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxing Zhang, Zhuomin Chen, Hao Mei, Dongsheng Luo, Hua Wei</li>
<li>for: 这 paper 的目的是解释图像 regression 模型（XAIG-R）的含义，以便更好地理解图像学习任务中的推理过程。</li>
<li>methods: 这 paper 使用了信息瓶颈理论基于的一个新目标函数，以及一种新的混合框架，可以支持不同的 GNN 模型在一种模型无关的方式上。此外，它还提出了一种对比学习策略来解决 regression 任务中的连续顺序标签问题。</li>
<li>results: 这 paper 通过三个 benchmark 数据集和一个实际数据集进行了广泛的实验，证明了该方法的有效性在解释 GNN 模型在 regression 任务中。<details>
<summary>Abstract</summary>
Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation. Extensive experiments show the effectiveness of the proposed method in interpreting GNN models in regression tasks.
</details>
<details>
<summary>摘要</summary>
GRaph regression是一个基本任务，在各种图学习任务中受到了越来越多的关注。然而，推断过程经常不可解释。大多数现有的解释技术仅适用于理解 GNN 的类型任务。在这项工作中，我们寻求一种可解释的方法，用于解释图 regression 模型（XAIG-R）。我们发现现有方法忽略了分布Shift和连续顺序决策边界，这会阻碍它们在回归任务中应用。为解决这些挑战，我们提出了一个基于信息瓶颈理论的新目标函数，并提出了一种新的混合框架，可以支持多种 GNN 在一种模型无关的方式上。此外，我们还提出了一种对比学习策略，用于处理连续顺序标签在回归任务中。为证明提出的方法的有效性，我们引入了三个标准数据集和一个真实数据集进行评估。广泛的实验表明，我们的方法可以有效地解释 GNN 模型在回归任务中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/16/cs.LG_2023_07_16/" data-id="cllta0ljo009bny88aeg708xr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/10/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/12/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
