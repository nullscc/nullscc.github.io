
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.SP_2023_10_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/25/eess.SP_2023_10_25/" class="article-date">
  <time datetime="2023-10-25T08:00:00.000Z" itemprop="datePublished">2023-10-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/25/eess.SP_2023_10_25/">eess.SP - 2023-10-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mode-Selection-and-Target-Classification-in-Cognitive-Radar-Networks"><a href="#Mode-Selection-and-Target-Classification-in-Cognitive-Radar-Networks" class="headerlink" title="Mode Selection and Target Classification in Cognitive Radar Networks"></a>Mode Selection and Target Classification in Cognitive Radar Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17006">http://arxiv.org/abs/2310.17006</a></li>
<li>repo_url: None</li>
<li>paper_authors: William W. Howard, Samuel R. Shebert, Benjamin H. Kirk, R. Michael Buehrer</li>
<li>For: The paper proposes a cognitive radar network that leverages the adaptability of cognitive radar networks to trade between active radar observation and passive signal parameter estimation, and learns the optimal action choices for each type of target.* Methods: The paper uses a multi-armed bandit model with current class information as prior information to select between available actions, and estimates the physical behavior of targets through radar emissions when the active radar action is selected, and estimates the radio behavior of targets through passive sensing when the passive action is selected.* Results: The network collects observed behavior of targets and forms clusters of similarly-behaved targets, and meta-learns the target class distributions while learning the optimal mode selections for each target class.Here are the three points in Simplified Chinese text:* For: 该 paper 提出了一种基于认知雷达网络的方法，以便在不同的目标类型下选择最佳的行动。* Methods: 该 paper 使用了一种多重武器模型，并使用当前类信息作为先验信息来选择可用的行动。当选择活动雷达时，节点通过雷达发射来估计目标的物理行为；当选择被动时，节点通过感知来估计目标的电磁行为。* Results: 网络通过收集目标的行为观察并组织类似目标的集群，从而meta-学习目标类型分布，同时学习每个目标类型的优化模式选择。<details>
<summary>Abstract</summary>
Cognitive Radar Networks were proposed by Simon Haykin in 2006 to address problems with large legacy radar implementations - primarily, single-point vulnerabilities and lack of adaptability. This work proposes to leverage the adaptability of cognitive radar networks to trade between active radar observation, which uses high power and risks interception, and passive signal parameter estimation, which uses target emissions to gain side information and lower the power necessary to accurately track multiple targets. The goal of the network is to learn over many target tracks both the characteristics of the targets as well as the optimal action choices for each type of target. In order to select between the available actions, we utilize a multi-armed bandit model, using current class information as prior information. When the active radar action is selected, the node estimates the physical behavior of targets through the radar emissions. When the passive action is selected, the node estimates the radio behavior of targets through passive sensing. Over many target tracks, the network collects the observed behavior of targets and forms clusters of similarly-behaved targets. In this way, the network meta-learns the target class distributions while learning the optimal mode selections for each target class.
</details>
<details>
<summary>摘要</summary>
cognitive radar networks 由谢韦金（Simon Haykin）在2006年提出，以解决传统雷达实施中的单点漏洞和不可靠性问题。这项工作提议利用智能雷达网络的适应性，在高功率和风险 intercept 的 aktive雷达观测和低功率的 passive信号参数估算之间进行交换。网络的目标是通过多个目标轨迹学习target的特征和最佳行为选择。为选择可用的行动，我们使用多重武器模型，使用当前类信息作为先验信息。当选择 active雷达动作时，节点估算目标的物理行为通过雷达发射。当选择 passive动作时，节点估算目标的电磁行为通过被动探测。在许多目标轨迹中，网络收集了目标的观测行为，并将其分为类似目标类型的集群。这样，网络可以meta-学习目标类型的分布，同时学习每个目标类型的优化模式选择。
</details></li>
</ul>
<hr>
<h2 id="Neural-Distributed-Compressor-Discovers-Binning"><a href="#Neural-Distributed-Compressor-Discovers-Binning" class="headerlink" title="Neural Distributed Compressor Discovers Binning"></a>Neural Distributed Compressor Discovers Binning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16961">http://arxiv.org/abs/2310.16961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ezgi Ozyilkan, Johannes Ballé, Elza Erkip</li>
<li>for: 这种研究是为了解决分布式源编码中的吞吐率问题，具体来说是威纳-赫茨问题。</li>
<li>methods: 这种研究使用机器学习的方法，特别是变量量量化，来实现数据驱动的压缩方案。</li>
<li>results: 研究发现，使用这种数据驱动的压缩方案可以Recover一些理想的理论解的特点，如源空间中的分割和使用侧 информацию进行最优的组合。这些行为 emerge although 没有直接使用源分布的知识。<details>
<summary>Abstract</summary>
We consider lossy compression of an information source when the decoder has lossless access to a correlated one. This setup, also known as the Wyner-Ziv problem, is a special case of distributed source coding. To this day, practical approaches for the Wyner-Ziv problem have neither been fully developed nor heavily investigated. We propose a data-driven method based on machine learning that leverages the universal function approximation capability of artificial neural networks. We find that our neural network-based compression scheme, based on variational vector quantization, recovers some principles of the optimum theoretical solution of the Wyner-Ziv setup, such as binning in the source space as well as optimal combination of the quantization index and side information, for exemplary sources. These behaviors emerge although no structure exploiting knowledge of the source distributions was imposed. Binning is a widely used tool in information theoretic proofs and methods, and to our knowledge, this is the first time it has been explicitly observed to emerge from data-driven learning.
</details>
<details>
<summary>摘要</summary>
我们考虑在损失压缩的资料来源中进行损失less压缩，当decoder有无损失的存取相关的一来。这个设置称为吴纽-兹维问题，是分布式源码编码的特殊情况。至今为止，实用的方法 для吴纽-兹维问题仍未被完全开发或严重调查。我们提议一个基于机器学习的数据驱动方法，利用人工神经网络的通用函数近似能力。我们发现，我们的神经网络压缩方案，基于可变量化，可以重新现出一些吴纽-兹维问题的理想解答，例如在源空间中的binning以及对于副信息的优化 комбина�tion。这些行为 emerge，即使没有采用知情源分布的结构化知识。binning是信息论中广泛使用的工具，而且，根据我们所知，这是第一次由数据驱动学习中明示地观察到这种行为。
</details></li>
</ul>
<hr>
<h2 id="How-to-Extend-3D-GBSM-to-Integrated-Sensing-and-Communication-Channel-with-Sharing-Feature"><a href="#How-to-Extend-3D-GBSM-to-Integrated-Sensing-and-Communication-Channel-with-Sharing-Feature" class="headerlink" title="How to Extend 3D GBSM to Integrated Sensing and Communication Channel with Sharing Feature?"></a>How to Extend 3D GBSM to Integrated Sensing and Communication Channel with Sharing Feature?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16765">http://arxiv.org/abs/2310.16765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yameng Liu, Jianhua Zhang, Yuxiang Zhang, Huiwen Gong, Tao Jiang, Guangyi Liu</li>
<li>for: This paper is written to support the development of Integrated Sensing and Communication (ISAC) technology in 6G systems, specifically by extending the existing 3D Geometry-Based Stochastic Model (GBSM) to include sensing channels.</li>
<li>methods: The paper proposes a new ISAC channel model that captures the sharing feature of both communication and sensing channels, including shared scatterers, clusters, paths, and similar propagation parameters. The model is based on a cascade of TX-target, radar cross section, and target-RX, with a novel parameter S for shared target extraction.</li>
<li>results: The proposed ISAC channel implementation framework offers flexible configuration of sharing feature and the joint generation of communication and sensing channels, and is compatible with the 3GPP standards, offering promising support for ISAC technology evaluation.<details>
<summary>Abstract</summary>
Integrated Sensing and Communication (ISAC) is a promising technology in 6G systems. The existing 3D Geometry-Based Stochastic Model (GBSM), as standardized for 5G systems, addresses solely communication channels and lacks consideration of the integration with sensing channel. Therefore, this letter extends 3D GBSM to support ISAC research, with a particular focus on capturing the sharing feature of both channels, including shared scatterers, clusters, paths, and similar propagation param-eters, which have been experimentally verified in the literature. The proposed approach can be summarized as follows: Firstly, an ISAC channel model is proposed, where shared and non-shared components are superimposed for both communication and sensing. Secondly, sensing channel is characterized as a cascade of TX-target, radar cross section, and target-RX, with the introduction of a novel parameter S for shared target extraction. Finally, an ISAC channel implementation framework is proposed, allowing flexible configuration of sharing feature and the joint generation of communication and sensing channels. The proposed ISAC channel model can be compatible with the 3GPP standards and offers promising support for ISAC technology evaluation.
</details>
<details>
<summary>摘要</summary>
《集成感知通信（ISAC）技术在6G系统中具有极大潜力。现有的3DGeometry-Based Stochastic Model（GBSM），为5G系统制定的标准，仅考虑了通信频道，不考虑感知频道的integration。因此，本文extend GBSM，以支持ISAC研究，特别是捕捉两个频道之间的共享特征，包括共享雷达目标、群集、路径和相似的传播参数，这些参数在文献中经过实验验证。 proposeapproach可以概括为以下三个步骤：1. 建立ISAC通信频道模型，其中共享和非共享组成部分相互重叠。2. 描述感知频道为TX-目标、雷达cross section和目标-RX的链式，并引入一个新参数S，用于捕捉共享目标。3. 提出ISAC通信频道实现框架，允许共享特征的灵活配置和共同生成通信和感知频道。提议的ISAC通信频道模型可以与3GPP标准兼容，并且对ISAC技术评估具有极大潜力。
</details></li>
</ul>
<hr>
<h2 id="Spherical-Wavefront-Near-Field-DoA-Estimation-in-THz-Automotive-Radar"><a href="#Spherical-Wavefront-Near-Field-DoA-Estimation-in-THz-Automotive-Radar" class="headerlink" title="Spherical Wavefront Near-Field DoA Estimation in THz Automotive Radar"></a>Spherical Wavefront Near-Field DoA Estimation in THz Automotive Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16724">http://arxiv.org/abs/2310.16724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmet M. Elbir, Kumar Vijay Mishra, Symeon Chatzinotas</li>
<li>for: 这项研究旨在探讨 THz 频率带汽车雷达的方向OF arrival（DoA）估计方法。</li>
<li>methods: 该研究提出了一种使用多信号分类（MUSIC）算法来估计目标DoA和距离，同时也考虑了near-field中的板缩影响。</li>
<li>results: numerical experiments表明该方法可以准确地估计目标参数。<details>
<summary>Abstract</summary>
Automotive radar at terahertz (THz) band has the potential to provide compact design. The availability of wide bandwidth at THz-band leads to high range resolution. Further, very narrow beamwidth arising from large arrays yields high angular resolution up to milli-degree level direction-of-arrival (DoA) estimation. At THz frequencies and extremely large arrays, the signal wavefront is spherical in the near-field that renders traditional far-field DoA estimation techniques unusable. In this work, we examine near-field DoA estimation for THz automotive radar. We propose an algorithm using multiple signal classification (MUSIC) to estimate target DoAs and ranges while also taking beam-squint in near-field into account. Using an array transformation approach, we compensate for near-field beam-squint in noise subspace computations to construct the beam-squint-free MUSIC spectra. Numerical experiments show the effectiveness of the proposed method to accurately estimate the target parameters.
</details>
<details>
<summary>摘要</summary>
自动驱动radar在tera哈勒tz（THz）频带有可能提供更加紧凑的设计。 THz频带的宽频率导致高分辨率范围。此外，非常窄的扫描幅由大型阵列产生，使得高度分辨率的方向来源估计（DoA）。在THz频率和极其大的阵列下，信号波front在近场是球形的，使得传统的远场DoA估计技术无法使用。在这种情况下，我们研究了THz自动驱动radar的近场DoA估计。我们提出了使用多个信号分类（MUSIC）算法来估计目标DoAs和距离，同时也考虑近场扫描幅的影响。使用阵列变换方法，我们在噪声空间计算中补做近场扫描幅的影响，构建了扫描幅自由的MUSIC谱。 numerically experiments show the effectiveness of the proposed method to accurately estimate the target parameters.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Power-Optimization-in-Satellite-Communication-Using-Multi-Intelligent-Reflecting-Surfaces"><a href="#Power-Optimization-in-Satellite-Communication-Using-Multi-Intelligent-Reflecting-Surfaces" class="headerlink" title="Power Optimization in Satellite Communication Using Multi-Intelligent Reflecting Surfaces"></a>Power Optimization in Satellite Communication Using Multi-Intelligent Reflecting Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16625">http://arxiv.org/abs/2310.16625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ihsan Khalil</li>
<li>for: 这个研究旨在提高卫星到地面通信系统的能源效率，通过集成多个反射智能表面（RIS）的两个创新方法。</li>
<li>methods: 这两个方法包括对问题的分解，首先将RIS元素的相位调整为最大化电力接收，然后使用选择多标点对RIS元素进行最大化电力评估。第二个任务是使用二进制线性规划问题来实现最低功耗，并使用二进制粒子群推导（BPSO）技术来解决。</li>
<li>results: 研究发现这两个方法可以实现卫星到地面通信系统的能源效率提高，并且在实际运行中可以实现可 COUNTING 的能源储存。<details>
<summary>Abstract</summary>
This study introduces two innovative methodologies aimed at augmenting energy efficiency in satellite-to-ground communication systems through the integration of multiple Reflective Intelligent Surfaces (RISs). The primary objective of these methodologies is to optimize overall energy efficiency under two distinct scenarios. In the first scenario, denoted as Ideal Environment (IE), we enhance energy efficiency by decomposing the problem into two sub-optimal tasks. The initial task concentrates on maximizing power reception by precisely adjusting the phase shift of each RIS element, followed by the implementation of Selective Diversity to identify the RIS element delivering maximal power. The second task entails minimizing power consumption, formulated as a binary linear programming problem, and addressed using the Binary Particle Swarm Optimization (BPSO) technique. The IE scenario presupposes an environment where signals propagate without any path loss, serving as a foundational benchmark for theoretical evaluations that elucidate the systems optimal capabilities. Conversely, the second scenario, termed Non-Ideal Environment (NIE), is designed for situations where signal transmission is subject to path loss. Within this framework, the Adam algorithm is utilized to optimize energy efficiency. This non ideal setting provides a pragmatic assessment of the systems capabilities under conventional operational conditions. Both scenarios emphasize the potential energy savings achievable by the satellite RIS system. Empirical simulations further corroborate the robustness and effectiveness of our approach, highlighting its potential to enhance energy efficiency in satellite-to-ground communication systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Pilot-Based-Uplink-Power-Control-in-Single-UE-Massive-MIMO-Systems-With-1-Bit-ADCs"><a href="#Pilot-Based-Uplink-Power-Control-in-Single-UE-Massive-MIMO-Systems-With-1-Bit-ADCs" class="headerlink" title="Pilot-Based Uplink Power Control in Single-UE Massive MIMO Systems With 1-Bit ADCs"></a>Pilot-Based Uplink Power Control in Single-UE Massive MIMO Systems With 1-Bit ADCs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16601">http://arxiv.org/abs/2310.16601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amila Ravinath, Bikshapathi Gouda, Italo Atzeni, Antti Tölli</li>
<li>for: 提高大量多输入多输出系统中用户设备（UE）的传输功率控制精度。</li>
<li>methods: 使用1比特分数器和多普逻耳sequence设计了一种closed-loop上行功率控制方法，包括单shot和算法控制方法两种。</li>
<li>results: 比较conventional closed-loop上行功率控制方法，提出的方法具有更高的精度和更好的性能。<details>
<summary>Abstract</summary>
We propose uplink power control (PC) methods for massive multiple-input multiple-output systems with 1-bit analog-to-digital converters, which are specifically tailored to address the non-monotonic data detection performance with respect to the transmit power of the user equipment (UE). Considering a single UE, we design a multi-amplitude pilot sequence to capture the aforementioned non-monotonicity, which is utilized at the base station to derive UE transmit power adjustments via single-shot or differential power control (DPC) techniques. Both methods enable closed-loop uplink PC using different feedback approaches. The single-shot method employs one-time multi-bit feedback, while the DPC method relies on continuous adjustments with 1-bit feedback. Numerical results demonstrate the superiority of the proposed schemes over conventional closed-loop uplink PC techniques.
</details>
<details>
<summary>摘要</summary>
我们提出了大量多输入多 outputs系统中的上传功率控制（PC）方法，特别是为了解决用户设备（UE）的传输功率与数据探测性的非单对数关系。对于单一UE，我们设计了多极性导航序列来捕捉上述非单对数关系，这些序列在基站端被用来 derivUE传输功率调整 via 单一射击或差分功率控制（DPC）技术。这两种方法均允许关闭loop上传PC使用不同的反馈方法。单一射击方法使用一次多位反馈，而DPC方法则靠 Continuous adjustments with 1-bit feedback。数字结果显示我们的提案方案比于传统关闭loop上传PC技术更有优势。
</details></li>
</ul>
<hr>
<h2 id="Terahertz-Enpowered-Communications-and-Sensing-in-6G-Systems-Opportunities-and-Challenges"><a href="#Terahertz-Enpowered-Communications-and-Sensing-in-6G-Systems-Opportunities-and-Challenges" class="headerlink" title="Terahertz-Enpowered Communications and Sensing in 6G Systems: Opportunities and Challenges"></a>Terahertz-Enpowered Communications and Sensing in 6G Systems: Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16548">http://arxiv.org/abs/2310.16548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Hans D. Schotten</li>
<li>for: 这篇论文旨在探讨6G系统中使用THz频段的可能性和挑战。</li>
<li>methods: 论文提出了一些可能的THz频段应用和挑战，包括通信和感知、定位和成像等方面。</li>
<li>results: 论文未提出具体的结果，主要是为了探讨6G系统中THz频段的可能性和挑战。<details>
<summary>Abstract</summary>
The current focus of academia and the telecommunications industry has been shifted to the development of the six-generation (6G) cellular technology, also formally referred to as IMT-2030. Unprecedented applications that 6G aims to accommodate demand extreme communications performance and, in addition, disruptive capabilities such as network sensing. Recently, there has been a surge of interest in terahertz (THz) frequencies as it offers not only massive spectral resources for communication but also distinct advantages in sensing, positioning, and imaging. The aim of this paper is to provide a brief outlook on opportunities opened by this under-exploited band and challenges that must be addressed to materialize the potential of THz-based communications and sensing in 6G systems.
</details>
<details>
<summary>摘要</summary>
现在学术界和电信产业的焦点已经转移到第六代（6G）无线技术的开发，即IMT-2030。6G旨在满足极高通信性能的应用需求，同时还具有破坏性能，如网络感知。最近，人们对tera兆赫兹（THz）频率的利用表现出了很大的兴趣，因为它不仅提供了巨大的频率资源 для通信，而且在感知、定位和成像方面具有明显的优势。本文的目的是提供6G系统中THz频率的可能性和挑战的简要预测。
</details></li>
</ul>
<hr>
<h2 id="Transmitting-Data-Through-Reconfigurable-Intelligent-Surface-A-Spatial-Sigma-Delta-Modulation-Approach"><a href="#Transmitting-Data-Through-Reconfigurable-Intelligent-Surface-A-Spatial-Sigma-Delta-Modulation-Approach" class="headerlink" title="Transmitting Data Through Reconfigurable Intelligent Surface: A Spatial Sigma-Delta Modulation Approach"></a>Transmitting Data Through Reconfigurable Intelligent Surface: A Spatial Sigma-Delta Modulation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16347">http://arxiv.org/abs/2310.16347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wai-Yiu Keung, Hei Victor Cheng, Wing-Kin Ma</li>
<li>for: 这个论文旨在解决未来能效通信系统中数据传输中使用可重新配置智能表面（RIS）的问题。</li>
<li>methods: 该论文使用了一种名为Sigma-Delta（$\Sigma\Delta$)干扰模ulasi的方法，该方法在空间域中应用首级Sigma-Delta干扰来处理相位量化。</li>
<li>results: 该论文的实验结果显示，使用Sigma-Delta干扰模ulasi可以实现与不量化ZF方案相同的比特错误率性能。<details>
<summary>Abstract</summary>
Transmitting data using the phases on reconfigurable intelligent surfaces (RIS) is a promising solution for future energy-efficient communication systems. Recent work showed that a virtual phased massive multiuser multiple-input-multiple-out (MIMO) transmitter can be formed using only one active antenna and a large passive RIS. In this paper, we are interested in using such a system to perform MIMO downlink precoding. In this context, we may not be able to apply conventional MIMO precoding schemes, such as the simple zero-forcing (ZF) scheme, and we typically need to design the phase signals by solving optimization problems with constant modulus constraints or with discrete phase constraints, which pose challenges with high computational complexities. In this work, we propose an alternative approach based on Sigma-Delta ($\Sigma\Delta$) modulation, which is classically famous for its noise-shaping ability. Specifically, first-order $\Sigma\Delta$ modulation is applied in the spatial domain to handle phase quantization in generating constant envelope signals. Under some mild assumptions, the proposed phased $\Sigma\Delta$ modulator allows us to use the ZF scheme to synthesize the RIS reflection phases with negligible complexity. The proposed approach is empirically shown to achieve comparable bit error rate performance to the unquantized ZF scheme.
</details>
<details>
<summary>摘要</summary>
通过可重新配置智能表面（RIS）传输数据是未来能效通信系统的承诺之一。最近的研究表明，只需一个活动天线和一大串pascal RIS可以形成虚拟 phase massive MIMO发射器。在这篇论文中，我们关心使用这种系统来执行MIMO下降干扰。在这种情况下，我们通常无法采用传统的MIMO预处理方案，如简单的零偏置（ZF）方案，而是需要设计相位信号通过优化问题的解决，这会带来高计算复杂度的挑战。在这种情况下，我们提议使用Sigma-Delta（$\Sigma\Delta$)模ulation，这是经典知名的噪声定向技术。 Specifically, we apply first-order $\Sigma\Delta$ modulation in the spatial domain to handle phase quantization in generating constant envelope signals. Under some mild assumptions, the proposed phased $\Sigma\Delta$ modulator allows us to use the ZF scheme to synthesize the RIS reflection phases with negligible complexity. The proposed approach is empirically shown to achieve comparable bit error rate performance to the unquantized ZF scheme.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/25/eess.SP_2023_10_25/" data-id="cloq1wlgx01cb7o88f86y9ub6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/24/cs.SD_2023_10_24/" class="article-date">
  <time datetime="2023-10-24T15:00:00.000Z" itemprop="datePublished">2023-10-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/24/cs.SD_2023_10_24/">cs.SD - 2023-10-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="IA-Para-el-Mantenimiento-Predictivo-en-Canteras-Modelado"><a href="#IA-Para-el-Mantenimiento-Predictivo-en-Canteras-Modelado" class="headerlink" title="IA Para el Mantenimiento Predictivo en Canteras: Modelado"></a>IA Para el Mantenimiento Predictivo en Canteras: Modelado</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16140">http://arxiv.org/abs/2310.16140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Marcos, Rodrigo Tamaki, Mateo Cámara, Virginia Yagüe, José Luis Blanco</li>
<li>for: 该论文目的是优化采矿业中的操作。</li>
<li>methods: 该论文使用无监督学习方法，训练一个变量自动编码器模型，使其能够从处理线操作时录制的听录中提取有用信息。</li>
<li>results: 研究结果表明，该模型能够将录制的听录重建并表示在隐藏空间中，并能够捕捉操作条件之间和设备之间的差异。未来，这可能会促进听录的分类和机器衰老的探测。<details>
<summary>Abstract</summary>
Dependence on raw materials, especially in the mining sector, is a key part of today's economy. Aggregates are vital, being the second most used raw material after water. Digitally transforming this sector is key to optimizing operations. However, supervision and maintenance (predictive and corrective) are challenges little explored in this sector, due to the particularities of the sector, machinery and environmental conditions. All this, despite the successes achieved in other scenarios in monitoring with acoustic and contact sensors. We present an unsupervised learning scheme that trains a variational autoencoder model on a set of sound records. This is the first such dataset collected during processing plant operations, containing information from different points of the processing line. Our results demonstrate the model's ability to reconstruct and represent in latent space the recorded sounds, the differences in operating conditions and between different equipment. In the future, this should facilitate the classification of sounds, as well as the detection of anomalies and degradation patterns in the operation of the machinery.
</details>
<details>
<summary>摘要</summary>
现代经济中Raw materials的依赖性，特别是采矿业，是非常重要的。各种粒子材料是第二重要的原材料，占用率很高。通过数字化转型，可以优化操作。但是，监督和维护（预测和修复）在这个领域尚未得到充分的探索，这主要归结于采矿业的特殊性、机器和环境条件。尽管在其他场景中监测器和接触传感器已经取得了成功，但这些成果在采矿业中尚未得到充分利用。我们提出了一种不监督学习方案，通过对一组声音记录进行训练，并将其模型化为一种变分自动机器学习模型。这是首次在处理厂操作过程中收集的声音数据集，包含不同点检测的声音信息。我们的结果表明模型能够重建和表示声音记录中的差异和不同设备的操作条件。未来，这将使得声音的分类和机器设备的磨损和腐食特征的检测变得更加容易。
</details></li>
</ul>
<hr>
<h2 id="CDSD-Chinese-Dysarthria-Speech-Database"><a href="#CDSD-Chinese-Dysarthria-Speech-Database" class="headerlink" title="CDSD: Chinese Dysarthria Speech Database"></a>CDSD: Chinese Dysarthria Speech Database</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15930">http://arxiv.org/abs/2310.15930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyi Sun, Ming Gao, Xinchen Kang, Shiru Wang, Jun Du, Dengfeng Yao, Su-Jing Wang</li>
<li>For: The paper is written for researchers and professionals working in the field of dysarthria, specifically those interested in speech recognition and dysarthric speech.* Methods: The paper describes the data collection and annotation processes for the Chinese Dysarthria Speech Database (CDSD), as well as an approach for establishing a baseline for dysarthric speech recognition. The authors also conducted a speaker-dependent dysarthric speech recognition experiment using additional data from one participant.* Results: The paper reports that extensive data-driven model training and fine-tuning limited quantities of specific individual data can yield commendable results in speaker-dependent dysarthric speech recognition. However, the authors observe significant variations in recognition results among different dysarthric speakers.Here is the information in Simplified Chinese text:* For: 这篇论文是为了探讨异常语音障碍（dysarthria）研究而写的，特别是关注语音识别和异常语音识别。* Methods: 论文描述了中国异常语音语音库（CDSD）的数据收集和标注过程，以及一种建立异常语音识别基线的方法。作者还进行了一个参与者具体语音识别实验。* Results: 论文发现，通过大量数据驱动模型训练和特定个体数据精细调整，可以在参与者具体语音识别中获得了可夸dp的成绩。然而，作者发现异常语音 speaker之间的识别结果存在显著的差异。这些发现可以作为异常语音识别的参考点。<details>
<summary>Abstract</summary>
We present the Chinese Dysarthria Speech Database (CDSD) as a valuable resource for dysarthria research. This database comprises speech data from 24 participants with dysarthria. Among these participants, one recorded an additional 10 hours of speech data, while each recorded one hour, resulting in 34 hours of speech material. To accommodate participants with varying cognitive levels, our text pool primarily consists of content from the AISHELL-1 dataset and speeches by primary and secondary school students. When participants read these texts, they must use a mobile device or the ZOOM F8n multi-track field recorder to record their speeches. In this paper, we elucidate the data collection and annotation processes and present an approach for establishing a baseline for dysarthric speech recognition. Furthermore, we conducted a speaker-dependent dysarthric speech recognition experiment using an additional 10 hours of speech data from one of our participants. Our research findings indicate that, through extensive data-driven model training, fine-tuning limited quantities of specific individual data yields commendable results in speaker-dependent dysarthric speech recognition. However, we observe significant variations in recognition results among different dysarthric speakers. These insights provide valuable reference points for speaker-dependent dysarthric speech recognition.
</details>
<details>
<summary>摘要</summary>
我们介绍中国带有肥瘤症（dysarthria）演说数据库（CDSD）作为肥瘤症研究的有价值资源。这个数据库包括24名参与者的肥瘤症演说数据。这些参与者中有一些录制了额外的10小时演说数据，而每个人录制了1小时演说，共计34小时的演说材料。为了适应参与者的不同认知水平，我们的文本池主要来自AISHELL-1数据集和primary和secondary学校学生的演说。当参与者阅读这些文本时，他们需要使用移动设备或ZOOM F8n多轨采集器来记录他们的演说。在这篇论文中，我们详细介绍了数据收集和注释过程，并提出了基准建立肥瘤症演说识别的方法。此外，我们通过使用一名参与者的额外10小时演说数据进行了一个参与者依存的肥瘤症演说识别实验。我们的研究发现，通过大量数据驱动模型训练和精细调整限量的具体个人数据，可以在参与者依存的肥瘤症演说识别中获得优秀的结果。但我们发现，不同的肥瘤症演说者之间存在显著的识别结果差异。这些发现提供了价值的参考点 для参与者依存的肥瘤症演说识别。
</details></li>
</ul>
<hr>
<h2 id="FOLEY-VAE-Generacion-de-efectos-de-audio-para-cine-con-inteligencia-artificial"><a href="#FOLEY-VAE-Generacion-de-efectos-de-audio-para-cine-con-inteligencia-artificial" class="headerlink" title="FOLEY-VAE: Generación de efectos de audio para cine con inteligencia artificial"></a>FOLEY-VAE: Generación de efectos de audio para cine con inteligencia artificial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15663">http://arxiv.org/abs/2310.15663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateo Cámara, José Luis Blanco</li>
<li>for: 这个研究旨在开发一种基于变量自动编码器的界面，用于创新创造FOLEY效果。</li>
<li>methods: 该模型通过各种自然声音训练，可以在实时传输新的声音特征到预录的音频或 Microphone 捕集的语音。此外，它还允许用户在实时进行交互式修改潜在变量，以实现精细化和个性化的艺术调整。</li>
<li>results: 研究基于上一年度这同学会上的研究，分析了现有的 RAVE 模型（一种特性化于音频效果生成的变量自动编码器）。该模型在 Audio 效果生成方面取得了成功，包括电磁、科幻、水声等效果。这种创新的方法已经为西班牙第一部使用人工智能生成的短片电影做出了贡献，这个突破口显示了人工智能在电影制作中的潜在价值和创新潜力。<details>
<summary>Abstract</summary>
In this research, we present an interface based on Variational Autoencoders trained with a wide range of natural sounds for the innovative creation of Foley effects. The model can transfer new sound features to prerecorded audio or microphone-captured speech in real time. In addition, it allows interactive modification of latent variables, facilitating precise and customized artistic adjustments. Taking as a starting point our previous study on Variational Autoencoders presented at this same congress last year, we analyzed an existing implementation: RAVE [1]. This model has been specifically trained for audio effects production. Various audio effects have been successfully generated, ranging from electromagnetic, science fiction, and water sounds, among others published with this work. This innovative approach has been the basis for the artistic creation of the first Spanish short film with sound effects assisted by artificial intelligence. This milestone illustrates palpably the transformative potential of this technology in the film industry, opening the door to new possibilities for sound creation and the improvement of artistic quality in film productions.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了基于变量自动编码器的界面，用于创新 FOLEY 效果。该模型可以在实时传输新的声音特征，并允许互动修改潜在变量，以达到精确和定制化的艺术调整。作为上一年在这同会议上发表的前一项研究的起点，我们分析了现有的实现：RAVE [1]。这个模型已经专门用于音频效果生成。它在不同的音频效果上取得了成功，包括电磁、科幻和水声等，与此研究一起发表。这一创新方法已经成为了艺术创作中使用人工智能帮助生成音效的首个 milestone，这个突破口将开启新的可能性，以提高电影制作中的艺术质量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/24/cs.SD_2023_10_24/" data-id="cloq1wlb800yh7o884d964sld" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/24/eess.AS_2023_10_24/" class="article-date">
  <time datetime="2023-10-24T14:00:00.000Z" itemprop="datePublished">2023-10-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/24/eess.AS_2023_10_24/">eess.AS - 2023-10-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Pre-training-Music-Classification-Models-via-Music-Source-Separation"><a href="#Pre-training-Music-Classification-Models-via-Music-Source-Separation" class="headerlink" title="Pre-training Music Classification Models via Music Source Separation"></a>Pre-training Music Classification Models via Music Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15845">http://arxiv.org/abs/2310.15845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cgaroufis/msspt">https://github.com/cgaroufis/msspt</a></li>
<li>paper_authors: Christos Garoufis, Athanasia Zlatintsi, Petros Maragos</li>
<li>for: 这个论文研究了 Whether music source separation can be used as a pre-training strategy for music representation learning, targeted at music classification tasks.</li>
<li>methods: 作者首先采用了 U-Net 网络，在不同的音乐源分离目标下进行了预训练，例如从音乐作品中隔离声乐或乐器源; 然后，他们附加了一个 convolutional tail network 到预训练后的 U-Net 上，并将整个网络进行了共同训练。 skip connections 也使得 separation 网络中学习的特征传递给了 tail network。</li>
<li>results: 实验结果表明，在两个公共可用的数据集上，采用预训练 U-Net 与 music source separation 目标可以提高 music classification 性能，特别是在使用 vocal separation 时的 music auto-tagging 任务中，以及在 multi-source separation 情况下的 music genre classification 任务中。<details>
<summary>Abstract</summary>
In this paper, we study whether music source separation can be used as a pre-training strategy for music representation learning, targeted at music classification tasks. To this end, we first pre-train U-Net networks under various music source separation objectives, such as the isolation of vocal or instrumental sources from a musical piece; afterwards, we attach a convolutional tail network to the pre-trained U-Net and jointly finetune the whole network. The features learned by the separation network are also propagated to the tail network through skip connections. Experimental results in two widely used and publicly available datasets indicate that pre-training the U-Nets with a music source separation objective can improve performance compared to both training the whole network from scratch and using the tail network as a standalone in two music classification tasks: music auto-tagging, when vocal separation is used, and music genre classification for the case of multi-source separation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了music源分离是否可以作为music表示学习的预训练策略，targeted at music分类任务。为此，我们首先在不同的music源分离目标下预训练U-Net网络，例如从音乐作品中隔离声乐或乐器源;然后，我们将预训练后的U-Net网络与一个 convolutional 尾网络结合，并同时练习整个网络。learned by separation network的特征也通过skip connections传递给尾网络。实验结果表明，预训练U-Nets with music source separation objective可以提高music classification tasks中的表现，比如音乐自动标签任务中使用声乐分离，以及music genre classification任务中的多源分离情况。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/24/eess.AS_2023_10_24/" data-id="cloq1wlcw01287o88fh2bbi5e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/24/cs.CV_2023_10_24/" class="article-date">
  <time datetime="2023-10-24T13:00:00.000Z" itemprop="datePublished">2023-10-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/24/cs.CV_2023_10_24/">cs.CV - 2023-10-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="On-the-Foundations-of-Shortcut-Learning"><a href="#On-the-Foundations-of-Shortcut-Learning" class="headerlink" title="On the Foundations of Shortcut Learning"></a>On the Foundations of Shortcut Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16228">http://arxiv.org/abs/2310.16228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katherine L. Hermann, Hossein Mobahi, Thomas Fel, Michael C. Mozer</li>
<li>for: 这个论文的目的是研究深度学习模型如何选择特征，以及这些特征是如何影响模型的预测性和可用性。</li>
<li>methods: 这个论文使用了一种微型的生成框架，用于 sintesizing 分类数据集，并测试了不同输入属性的可用性和预测性如何影响模型的特征选择。</li>
<li>results: 研究发现，当 introducing a single hidden layer with ReLU or Tanh units 时，模型会带有一定的短cut bias，而 linear models 则相对较不受这种偏见。此外，研究还发现在实际应用中，模型在不同数据集中的可用性 manipulate 会增加模型的短cut bias。<details>
<summary>Abstract</summary>
Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on predictivity-how reliably a feature indicates train-set labels-but also on availability-how easily the feature can be extracted, or leveraged, from inputs. The literature on shortcut learning has noted examples in which models privilege one feature over another, for example texture over shape and image backgrounds over foreground objects. Here, we test hypotheses about which input properties are more available to a model, and systematically study how predictivity and availability interact to shape models' feature use. We construct a minimal, explicit generative framework for synthesizing classification datasets with two latent features that vary in predictivity and in factors we hypothesize to relate to availability, and quantify a model's shortcut bias-its over-reliance on the shortcut (more available, less predictive) feature at the expense of the core (less available, more predictive) feature. We find that linear models are relatively unbiased, but introducing a single hidden layer with ReLU or Tanh units yields a bias. Our empirical findings are consistent with a theoretical account based on Neural Tangent Kernels. Finally, we study how models used in practice trade off predictivity and availability in naturalistic datasets, discovering availability manipulations which increase models' degree of shortcut bias. Taken together, these findings suggest that the propensity to learn shortcut features is a fundamental characteristic of deep nonlinear architectures warranting systematic study given its role in shaping how models solve tasks.
</details>
<details>
<summary>摘要</summary>
深度学习模型可以从数据中提取丰富的特征。这些特征的选择不仅取决于预测性能——数据集标签的可预测性——还取决于可用性——从输入中提取特征的可能性。文献中的快捷学习例子表明模型会偏爱某些特征，例如文本而不是形状，图像背景而不是前景对象。在这里，我们测试假设关于输入属性的可用性，并系统地研究预测性和可用性之间如何相互影响模型的特征使用。我们构建了一个最小、显式生成框架，用于生成分类数据集，其中两个隐藏特征变化于预测性和我们假设与可用性相关的因素。我们量化模型的快捷偏好——它过度依赖于快捷特征，而忽略核心特征。我们发现线性模型相对偏倚，但是在添加一个单一的隐藏层并使用ReLU或Tanh单元时，模型就会偏爱快捷特征。我们的实验结果与基于Neural Tangent Kernels的理论质量相符。最后，我们研究了实际使用的模型在自然化数据集中是如何交易预测性和可用性的。我们发现可以通过可用性操作来增加模型的快捷偏好。总之，这些发现表明深度非线性架构中的快捷特征学习是一种基本特征，需要系统地研究，因为它会影响模型如何解决任务。
</details></li>
</ul>
<hr>
<h2 id="ShadowSense-Unsupervised-Domain-Adaptation-and-Feature-Fusion-for-Shadow-Agnostic-Tree-Crown-Detection-from-RGB-Thermal-Drone-Imagery"><a href="#ShadowSense-Unsupervised-Domain-Adaptation-and-Feature-Fusion-for-Shadow-Agnostic-Tree-Crown-Detection-from-RGB-Thermal-Drone-Imagery" class="headerlink" title="ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for Shadow-Agnostic Tree Crown Detection from RGB-Thermal Drone Imagery"></a>ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for Shadow-Agnostic Tree Crown Detection from RGB-Thermal Drone Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16212">http://arxiv.org/abs/2310.16212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rudrakshkapil/shadowsense">https://github.com/rudrakshkapil/shadowsense</a></li>
<li>paper_authors: Rudraksh Kapil, Seyed Mojtaba Marvasti-Zadeh, Nadir Erbilgin, Nilanjan Ray</li>
<li>for: This paper is written for detecting individual tree crowns from remote sensing data, specifically in the context of dense forests with diverse environmental variations.</li>
<li>methods: The proposed method, called ShadowSense, is entirely self-supervised and leverages domain adversarial training and feature pyramid networks to adapt domain-invariant representations and improve the accuracy of tree crown detection.</li>
<li>results: The proposed method outperforms both the baseline RGB-trained detector and state-of-the-art techniques that rely on unsupervised domain adaptation or early image fusion, as demonstrated through extensive experiments.Here is the text in Simplified Chinese:</li>
<li>for: 本研究旨在通过远程感知数据检测个体树冠，特别是在杂化的森林环境下。</li>
<li>methods: 提议的方法是自适应域难变换和特征PYRAMID网络，以适应域不同的表示，并提高树冠检测的准确性。</li>
<li>results: 提议的方法比基础RGB训练的检测器和使用无监督领域适应或早期图像融合的状态前方法更高效，经过广泛的实验证明。<details>
<summary>Abstract</summary>
Accurate detection of individual tree crowns from remote sensing data poses a significant challenge due to the dense nature of forest canopy and the presence of diverse environmental variations, e.g., overlapping canopies, occlusions, and varying lighting conditions. Additionally, the lack of data for training robust models adds another limitation in effectively studying complex forest conditions. This paper presents a novel method for detecting shadowed tree crowns and provides a challenging dataset comprising roughly 50k paired RGB-thermal images to facilitate future research for illumination-invariant detection. The proposed method (ShadowSense) is entirely self-supervised, leveraging domain adversarial training without source domain annotations for feature extraction and foreground feature alignment for feature pyramid networks to adapt domain-invariant representations by focusing on visible foreground regions, respectively. It then fuses complementary information of both modalities to effectively improve upon the predictions of an RGB-trained detector and boost the overall accuracy. Extensive experiments demonstrate the superiority of the proposed method over both the baseline RGB-trained detector and state-of-the-art techniques that rely on unsupervised domain adaptation or early image fusion. Our code and data are available: https://github.com/rudrakshkapil/ShadowSense
</details>
<details>
<summary>摘要</summary>
原文：检测个体树冠FROM remote sensing数据中具有挑战性，因为森林穹顶叶物 dense 和多样化的环境变化（如重叠叶物、遮挡和不同的照明条件），而且缺乏数据 для训练可靠的模型，增加了研究复杂的森林条件的限制。本文提出了一种新的方法（ShadowSense），可以检测遮挡的树冠，并提供了一个包含约50k个RGB-热图像的挑战性数据集，以便未来的研究人员可以利用这些数据进行光照不敏感的检测。本方法是完全自动化的，不需要源频道注释，通过领域对抗训练来提取特征和对前景特征进行对齐，以适应频道不敏感的表示。然后，它将RGB和热图像的补充信息 fusion，以提高RGB检测器的预测结果。广泛的实验表明，提议的方法在RGB检测器和状态足的技术上都具有显著的优势。我们的代码和数据可以在 GitHub 上获得：https://github.com/rudrakshkapil/ShadowSense。Translation:原文：检测个体树冠FROM remote sensing数据中具有挑战性，因为森林穹顶叶物 dense 和多样化的环境变化（如重叠叶物、遮挡和不同的照明条件），而且缺乏数据 для训练可靠的模型，增加了研究复杂的森林条件的限制。本文提出了一种新的方法（ShadowSense），可以检测遮挡的树冠，并提供了一个包含约50k个RGB-热图像的挑战性数据集，以便未来的研究人员可以利用这些数据进行光照不敏感的检测。本方法是完全自动化的，不需要源频道注释，通过领域对抗训练来提取特征和对前景特征进行对齐，以适应频道不敏感的表示。然后，它将RGB和热图像的补充信息 fusion，以提高RGB检测器的预测结果。广泛的实验表明，提议的方法在RGB检测器和状态足的技术上都具有显著的优势。我们的代码和数据可以在 GitHub 上获得：https://github.com/rudrakshkapil/ShadowSense。
</details></li>
</ul>
<hr>
<h2 id="Sea-Land-Cloud-Segmentation-in-Satellite-Hyperspectral-Imagery-by-Deep-Learning"><a href="#Sea-Land-Cloud-Segmentation-in-Satellite-Hyperspectral-Imagery-by-Deep-Learning" class="headerlink" title="Sea-Land-Cloud Segmentation in Satellite Hyperspectral Imagery by Deep Learning"></a>Sea-Land-Cloud Segmentation in Satellite Hyperspectral Imagery by Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16210">http://arxiv.org/abs/2310.16210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jonalvjusto/s_l_c_segm_hyp_img">https://github.com/jonalvjusto/s_l_c_segm_hyp_img</a></li>
<li>paper_authors: Jon Alvarez Justo, Joseph Landon Garrett, Mariana-Iuliana Georgescu, Jesus Gonzalez-Llorente, Radu Tudor Ionescu, Tor Arne Johansen</li>
<li>For: The paper is written for on-board Artificial Intelligence (AI) techniques for enhancing the autonomy of satellite platforms through edge inference, specifically focusing on multi-class segmentation of High-Resolution Satellite (HS) imagery for sea, land, and cloud formations.* Methods: The paper employs 16 different deep learning (DL) models for segmenting HS imagery, including both shallow and deep models, and proposes four new DL models. The models are trained and evaluated for in-orbit deployment, considering performance, parameter count, and inference time.* Results: The paper shows that the proposed 1D-Justo-LiuNet model consistently outperforms state-of-the-art models for sea-land-cloud segmentation in terms of performance (0.93 accuracy) and parameter count (4,563), but presents longer inference time (15s) in the tested processing architecture. Additionally, the paper demonstrates that reducing spectral channels down to 3 can lower models’ parameters and inference time, but at the cost of weaker segmentation performance.<details>
<summary>Abstract</summary>
Satellites are increasingly adopting on-board Artificial Intelligence (AI) techniques to enhance platforms' autonomy through edge inference. In this context, the utilization of deep learning (DL) techniques for segmentation in HS satellite imagery offers advantages for remote sensing applications, and therefore, we train 16 different models, whose codes are made available through our study, which we consider to be relevant for on-board multi-class segmentation of HS imagery, focusing on classifying oceanic (sea), terrestrial (land), and cloud formations. We employ the HYPSO-1 mission as an illustrative case for sea-land-cloud segmentation, and to demonstrate the utility of the segments, we introduce a novel sea-land-cloud ranking application scenario. Our system prioritizes HS image downlink based on sea, land, and cloud coverage levels from the segmented images. We comparatively evaluate the models for in-orbit deployment, considering performance, parameter count, and inference time. The models include both shallow and deep models, and after we propose four new DL models, we demonstrate that segmenting single spectral signatures (1D) outperforms 3D data processing comprising both spectral (1D) and spatial (2D) contexts. We conclude that our lightweight DL model, called 1D-Justo-LiuNet, consistently surpasses state-of-the-art models for sea-land-cloud segmentation, such as U-Net and its variations, in terms of performance (0.93 accuracy) and parameter count (4,563). However, the 1D models present longer inference time (15s) in the tested processing architecture, which is clearly suboptimal. Finally, after demonstrating that in-orbit image segmentation should occur post L1b radiance calibration rather than on raw data, we additionally show that reducing spectral channels down to 3 lowers models' parameters and inference time, at the cost of weaker segmentation performance.
</details>
<details>
<summary>摘要</summary>
卫星在board上采用人工智能(AI)技术以提高平台的自主性，在这种情况下，使用深度学习(DL)技术进行高分辨率卫星影像（HS）中的分割具有优势，因此我们在本研究中训练了16个模型，代码在我们的研究中公布。我们认为这些模型适用于HS影像的board上多类分割，主要是为了将海洋（海）、陆地（地）和云形态分别分类。我们使用HYPSO-1任务作为示例，以示 segmentation 的实用性，并在HS影像中提供了一个新的海地陆云排名应用场景。我们的系统根据HS影像中的海、地和云覆盖水平来决定下载链接。我们对在空间中部署的模型进行比较评估，考虑性能、参数计数和计算时间。模型包括浅层和深度模型，而我们还提出了四种新的深度学习模型。我们发现，通过单 spectral signature（1D）进行分割，可以超过三个维度（2D）的数据处理。我们认为我们的轻量级深度学习模型，称为1D-Justo-LiuNet，在海地陆云分割方面 consistently  exceeds  state-of-the-art 模型（如UNet和其变种），以性能（0.93准确率）和参数计数（4563）为标准。然而，1D模型在测试的处理架构中具有较长的计算时间（15s），这显然不是最佳。最后，我们还证明了在空间中进行卫星影像分割应该在L1b辐射均衡化后进行，而不是在原始数据上进行。此外，我们发现，将 spectral channel 降低到3个可以降低模型的参数和计算时间，但是这将导致分割性能弱化。
</details></li>
</ul>
<hr>
<h2 id="Learning-Low-Rank-Latent-Spaces-with-Simple-Deterministic-Autoencoder-Theoretical-and-Empirical-Insights"><a href="#Learning-Low-Rank-Latent-Spaces-with-Simple-Deterministic-Autoencoder-Theoretical-and-Empirical-Insights" class="headerlink" title="Learning Low-Rank Latent Spaces with Simple Deterministic Autoencoder: Theoretical and Empirical Insights"></a>Learning Low-Rank Latent Spaces with Simple Deterministic Autoencoder: Theoretical and Empirical Insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16194">http://arxiv.org/abs/2310.16194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alokendu Mazumder, Tirthajit Baruah, Bhartendu Kumar, Rishab Sharma, Vishwajeet Pattanaik, Punit Rathore</li>
<li>for: 提高自动编码器的数据表示效果，使其更加紧凑地表示数据。</li>
<li>methods:  incorporated 一个低级别正则化项，使自动编码器适应ively reconstruction低维 latent space，保持基本的目标。</li>
<li>results:  empirical research shows that our model outperforms traditional autoencoders in various tasks such as image generation and downstream classification, and the theoretical analysis also proves the effectiveness of our model.<details>
<summary>Abstract</summary>
The autoencoder is an unsupervised learning paradigm that aims to create a compact latent representation of data by minimizing the reconstruction loss. However, it tends to overlook the fact that most data (images) are embedded in a lower-dimensional space, which is crucial for effective data representation. To address this limitation, we propose a novel approach called Low-Rank Autoencoder (LoRAE). In LoRAE, we incorporated a low-rank regularizer to adaptively reconstruct a low-dimensional latent space while preserving the basic objective of an autoencoder. This helps embed the data in a lower-dimensional space while preserving important information. It is a simple autoencoder extension that learns low-rank latent space. Theoretically, we establish a tighter error bound for our model. Empirically, our model's superiority shines through various tasks such as image generation and downstream classification. Both theoretical and practical outcomes highlight the importance of acquiring low-dimensional embeddings.
</details>
<details>
<summary>摘要</summary>
“自动Encoder是一种无监督学习架构，旨在创建一个简单的内在表示方法，以最小化重建损失。但是，它往往忽略了许多资料（图像）是嵌入在较低维度的空间中，这是有效的资料表示的关键。为了解决这个限制，我们提出了一种新的方法 called Low-Rank Autoencoder (LoRAE)。在LoRAE中，我们添加了一个低维度正规化项，以适应地重建一个较低维度的内在空间，保留了基本的自动Encoder目标。这将资料嵌入在较低维度的空间中，保留了重要的信息。它是一个简单地将自动Encoder扩展为学习低维度内在空间的方法。理论上，我们建立了一个更紧的错误范围，实际上，我们的模型在不同的任务上（如图像生成和下游分类）表现出了优越的成果。实际和理论的结果都显示了低维度内在空间的重要性。”
</details></li>
</ul>
<hr>
<h2 id="G-CASCADE-Efficient-Cascaded-Graph-Convolutional-Decoding-for-2D-Medical-Image-Segmentation"><a href="#G-CASCADE-Efficient-Cascaded-Graph-Convolutional-Decoding-for-2D-Medical-Image-Segmentation" class="headerlink" title="G-CASCADE: Efficient Cascaded Graph Convolutional Decoding for 2D Medical Image Segmentation"></a>G-CASCADE: Efficient Cascaded Graph Convolutional Decoding for 2D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16175">http://arxiv.org/abs/2310.16175</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SLDGroup/G-CASCADE">https://github.com/SLDGroup/G-CASCADE</a></li>
<li>paper_authors: Md Mostafijur Rahman, Radu Marculescu</li>
<li>for: 这篇论文是用于医疗影像分类的重要应用，并提出了一个新的图像分类模型，即弹性图像分类实现器（G-CASCADE）。</li>
<li>methods: 这个模型使用了弹性图像分类实现器，并与多个层次变换器结合，以进行多阶段特征地图。</li>
<li>results: 试验结果显示，这个模型在五个医疗影像分类任务（包括腹部器官、心脏器官、肿瘤涂炭、皮肤涂炭和视力器官）中都超过了其他现有的State-of-the-art（SOTA）方法。此外，该模型还可以轻松地与其他层次енкоーダ结合，用于通用的 semantic和医疗影像分类任务。<details>
<summary>Abstract</summary>
In recent years, medical image segmentation has become an important application in the field of computer-aided diagnosis. In this paper, we are the first to propose a new graph convolution-based decoder namely, Cascaded Graph Convolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation. G-CASCADE progressively refines multi-stage feature maps generated by hierarchical transformer encoders with an efficient graph convolution block. The encoder utilizes the self-attention mechanism to capture long-range dependencies, while the decoder refines the feature maps preserving long-range information due to the global receptive fields of the graph convolution block. Rigorous evaluations of our decoder with multiple transformer encoders on five medical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp lesions, Skin lesions, and Retinal vessels) show that our model outperforms other state-of-the-art (SOTA) methods. We also demonstrate that our decoder achieves better DICE scores than the SOTA CASCADE decoder with 80.8% fewer parameters and 82.3% fewer FLOPs. Our decoder can easily be used with other hierarchical encoders for general-purpose semantic and medical image segmentation tasks.
</details>
<details>
<summary>摘要</summary>
Recently, medical image segmentation has become an important application in the field of computer-aided diagnosis. In this paper, we propose a new graph convolution-based decoder, namely Cascaded Graph Convolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation. G-CASCADE progressively refines multi-stage feature maps generated by hierarchical transformer encoders with an efficient graph convolution block. The encoder uses the self-attention mechanism to capture long-range dependencies, while the decoder refines the feature maps while preserving long-range information due to the global receptive fields of the graph convolution block. We evaluate our decoder with multiple transformer encoders on five medical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp lesions, Skin lesions, and Retinal vessels) and show that our model outperforms other state-of-the-art (SOTA) methods. We also demonstrate that our decoder achieves better DICE scores than the SOTA CASCADE decoder with 80.8% fewer parameters and 82.3% fewer FLOPs. Our decoder can easily be used with other hierarchical encoders for general-purpose semantic and medical image segmentation tasks.
</details></li>
</ul>
<hr>
<h2 id="iNVS-Repurposing-Diffusion-Inpainters-for-Novel-View-Synthesis"><a href="#iNVS-Repurposing-Diffusion-Inpainters-for-Novel-View-Synthesis" class="headerlink" title="iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis"></a>iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16167">http://arxiv.org/abs/2310.16167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Kant, Aliaksandr Siarohin, Michael Vasilkovsky, Riza Alp Guler, Jian Ren, Sergey Tulyakov, Igor Gilitschenski</li>
<li>for: The paper is written for generating consistent novel views from a single source image, with a focus on maximizing the reuse of visible pixels from the source image.</li>
<li>methods: The paper uses a monocular depth estimator to transfer visible pixels from the source view to the target view, and trains the method on the large-scale Objaverse dataset to learn 3D object priors. The paper also introduces a novel masking mechanism based on epipolar lines to further improve the quality of the approach.</li>
<li>results: The paper demonstrates the zero-shot abilities of the framework on three challenging datasets: Google Scanned Objects, Ray Traced Multiview, and Common Objects in 3D, and shows that the approach can generate high-quality novel views without requiring any additional training data.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文是为了从单一的源图像中生成一致的新视图，并将可见像素从源视图传输到目标视图。</li>
<li>methods: 这篇论文使用单目深度估计器来传输可见像素从源视图到目标视图，并在Objaverse大规模数据集上训练方法来学习3D对象假设。文章还引入了一种新的蒙版机制基于轴线，以进一步提高方法的质量。</li>
<li>results: 这篇论文在Google扫描物体、光Trace多视图和通用物体3D等三个复杂的数据集上展示了零shot能力，并证明了方法可以生成高质量的新视图无需任何额外的训练数据。<details>
<summary>Abstract</summary>
We present a method for generating consistent novel views from a single source image. Our approach focuses on maximizing the reuse of visible pixels from the source image. To achieve this, we use a monocular depth estimator that transfers visible pixels from the source view to the target view. Starting from a pre-trained 2D inpainting diffusion model, we train our method on the large-scale Objaverse dataset to learn 3D object priors. While training we use a novel masking mechanism based on epipolar lines to further improve the quality of our approach. This allows our framework to perform zero-shot novel view synthesis on a variety of objects. We evaluate the zero-shot abilities of our framework on three challenging datasets: Google Scanned Objects, Ray Traced Multiview, and Common Objects in 3D. See our webpage for more details: https://yashkant.github.io/invs/
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，可以从单个源图像生成一致的新视图。我们的方法是利用可见像素的最大重复使用来实现这一点。我们使用一个独立深度估计器，将源视图中的可见像素传递到目标视图中。我们从一个预训练的2D填充扩散模型开始，然后在宽泛的Objaverse数据集上训练我们的方法，以学习3D物体假设。在训练过程中，我们使用一种新的masking机制，基于epipolar线，以进一步提高我们的方法的质量。这 позволяет我们的框架在多种物体上进行零aser的新视图合成。我们在Google Scanned Objects、Ray Traced Multiview和Common Objects in 3D等三个挑战性 dataset上评估了我们的框架的零aser能力。更多细节请参考我们的网站：https://yashkant.github.io/invs/
</details></li>
</ul>
<hr>
<h2 id="MyriadAL-Active-Few-Shot-Learning-for-Histopathology"><a href="#MyriadAL-Active-Few-Shot-Learning-for-Histopathology" class="headerlink" title="MyriadAL: Active Few Shot Learning for Histopathology"></a>MyriadAL: Active Few Shot Learning for Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16161">http://arxiv.org/abs/2310.16161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nico Schiavone, Jingyi Wang, Shuangzhi Li, Roger Zemp, Xingyu Li<br>for:This paper addresses the issue of limited annotation budget in active learning (AL) and few-shot learning (FSL) scenarios, particularly in the context of histopathology where labelling is expensive.methods:The proposed Myriad Active Learning (MAL) framework includes a contrastive-learning encoder, pseudo-label generation, and novel query sample selection in the loop. Unlabelled data is massaged in a self-supervised manner to obtain data representations and clustering knowledge, which are used to activate the AL loop. The pseudo-labels of unlabelled data are refined with feedback from an oracle in each AL cycle, and the updated pseudo-labels are used to improve active learning query selection.results:Extensive experiments on two public histopathology datasets show that MAL has superior test accuracy, macro F1-score, and label efficiency compared to prior works, and can achieve a comparable test accuracy to a fully supervised algorithm while labelling only 5% of the dataset.<details>
<summary>Abstract</summary>
Active Learning (AL) and Few Shot Learning (FSL) are two label-efficient methods which have achieved excellent results recently. However, most prior arts in both learning paradigms fail to explore the wealth of the vast unlabelled data. In this study, we address this issue in the scenario where the annotation budget is very limited, yet a large amount of unlabelled data for the target task is available. We frame this work in the context of histopathology where labelling is prohibitively expensive. To this end, we introduce an active few shot learning framework, Myriad Active Learning (MAL), including a contrastive-learning encoder, pseudo-label generation, and novel query sample selection in the loop. Specifically, we propose to massage unlabelled data in a self-supervised manner, where the obtained data representations and clustering knowledge form the basis to activate the AL loop. With feedback from the oracle in each AL cycle, the pseudo-labels of the unlabelled data are refined by optimizing a shallow task-specific net on top of the encoder. These updated pseudo-labels serve to inform and improve the active learning query selection process. Furthermore, we introduce a novel recipe to combine existing uncertainty measures and utilize the entire uncertainty list to reduce sample redundancy in AL. Extensive experiments on two public histopathology datasets show that MAL has superior test accuracy, macro F1-score, and label efficiency compared to prior works, and can achieve a comparable test accuracy to a fully supervised algorithm while labelling only 5% of the dataset.
</details>
<details>
<summary>摘要</summary>
aktive læring (AL) og few shot læring (FSL) er to metoder, der har opnået excellente resultater på seneste tid. Men de fleste forrige værker i begge læring paradigmer har ignoreret de rigdomme af utydelige data. I denne studie adresserer vi dette problem i scenarioen, hvor annotation-budgetten er meget begrænset, men der er en stor mængde utydelige data for måletasken. Vi placerer dette arbejde i konteksten af histopatologi, hvor etikettering er forbudtivis. For at gøre dette, introducerer vi et aktivt few shot læring-ramme, kaldet Myriad Active Learning (MAL), der inkluderer en kontrastiv-læring encoder, pseudo-etiketteringsgenerering og en ny query-sample-udvælgelse i loop. Specielt proposerer vi at masseere utydelige data i en selv-superviset måde, hvor de erhvervede datarepræsentationer og clustering-viden danner grunden til at aktivere AL-loopet. Med tilbakemelding fra oraklet i hver AL-cirkel, opdaterer vi pseudo-etiketterne af utydelige data ved at optimere en skalmål-specifik net på toppen af encoderen. Disse opdaterede pseudo-etiketter tjener til at informere og forbedre den aktive læring query-udvælgelse proces. Desudtan introducerer vi en ny opskrift til at kombinere eksisterende usikkerhedsmål og bruge hele usikkerhedslisten til at reducere sample-redundansen i AL. Vores omfattende eksperimenter på to offentlige histopatologi-datasæt viser, at MAL har overlegne prøveaccuracy, makro F1-score og etiket-effektivitet i forhold til tidligere værker, og kan opnå en kompatibel prøveaccuracy til en fuldt-superviset algoritme, mens kun 5% af datasets er etiket.
</details></li>
</ul>
<hr>
<h2 id="Pix2HDR-–-A-pixel-wise-acquisition-and-deep-learning-based-synthesis-approach-for-high-speed-HDR-videos"><a href="#Pix2HDR-–-A-pixel-wise-acquisition-and-deep-learning-based-synthesis-approach-for-high-speed-HDR-videos" class="headerlink" title="Pix2HDR – A pixel-wise acquisition and deep learning-based synthesis approach for high-speed HDR videos"></a>Pix2HDR – A pixel-wise acquisition and deep learning-based synthesis approach for high-speed HDR videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16139">http://arxiv.org/abs/2310.16139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caixin Wang, Jie Zhang, Matthew A. Wilson, Ralph Etienne-Cummings</li>
<li>for: 能够高速 capture 高动态范围视频，尤其是在低照度和暗背景下，是许多视觉应用中的关键。</li>
<li>methods: 我们使用像素级别可编程的图像感知器，通过在不同曝光和相位偏移中采样视频帧，同时捕捉快速运动和高动态范围。然后，我们使用深度神经网络对像素级别输出进行端到端学习，以实现高空间分辨率和减少运动模糊。</li>
<li>results: 我们实现了1000帧&#x2F;秒的高动态范围视频捕捉，并能够减少运动模糊。我们的方法可以在各种动态条件下提高视觉系统的适应性和性能。<details>
<summary>Abstract</summary>
Accurately capturing dynamic scenes with wide-ranging motion and light intensity is crucial for many vision applications. However, acquiring high-speed high dynamic range (HDR) video is challenging because the camera's frame rate restricts its dynamic range. Existing methods sacrifice speed to acquire multi-exposure frames. Yet, misaligned motion in these frames can still pose complications for HDR fusion algorithms, resulting in artifacts. Instead of frame-based exposures, we sample the videos using individual pixels at varying exposures and phase offsets. Implemented on a pixel-wise programmable image sensor, our sampling pattern simultaneously captures fast motion at a high dynamic range. We then transform pixel-wise outputs into an HDR video using end-to-end learned weights from deep neural networks, achieving high spatiotemporal resolution with minimized motion blurring. We demonstrate aliasing-free HDR video acquisition at 1000 FPS, resolving fast motion under low-light conditions and against bright backgrounds - both challenging conditions for conventional cameras. By combining the versatility of pixel-wise sampling patterns with the strength of deep neural networks at decoding complex scenes, our method greatly enhances the vision system's adaptability and performance in dynamic conditions.
</details>
<details>
<summary>摘要</summary>
必须精准捕捉动态场景中的广泛运动和光强变化是许多视觉应用中的关键。然而，获取高速高动态范围（HDR）视频具有摄像机框率限制动态范围。现有方法为了获得多曝光帧而 sacrifices 速度。然而，这些多曝光帧中的运动不同相对可能会对HDR融合算法产生难以控制的缺陷，从而导致 artifacts。相比 frame-based 曝光，我们使用个别像素的不同曝光和阶段偏移来采样视频。在一个像素可程序化图像感知器上实现的这种采样模式中，我们可以同时捕捉快速运动和高动态范围。然后，我们使用深度神经网络学习到的权重将像素级别输出转换为HDR视频，实现高空间时间分辨率，并最小化运动模糊。我们在1000 FPS下获得无扭曲HDR视频，解决快速运动在低光环境和灯光背景下的捕捉问题，这些问题对传统摄像机来说都是挑战。通过将像素级别采样模式与深度神经网络的解码能力结合，我们的方法可以大幅提高视觉系统的适应性和性能在动态条件下。
</details></li>
</ul>
<hr>
<h2 id="Subtle-Signals-Video-based-Detection-of-Infant-Non-nutritive-Sucking-as-a-Neurodevelopmental-Cue"><a href="#Subtle-Signals-Video-based-Detection-of-Infant-Non-nutritive-Sucking-as-a-Neurodevelopmental-Cue" class="headerlink" title="Subtle Signals: Video-based Detection of Infant Non-nutritive Sucking as a Neurodevelopmental Cue"></a>Subtle Signals: Video-based Detection of Infant Non-nutritive Sucking as a Neurodevelopmental Cue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16138">http://arxiv.org/abs/2310.16138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ostadabbas/nns-detection-and-segmentation">https://github.com/ostadabbas/nns-detection-and-segmentation</a></li>
<li>paper_authors: Shaotong Zhu, Michael Wan, Sai Kumar Reddy Manne, Emily Zimmerman, Sarah Ostadabbas<br>for:This paper aims to develop a vision-based algorithm for non-contact detection of non-nutritive sucking (NNS) activity in infants using baby monitor footage.methods:The proposed algorithm utilizes optical flow and temporal convolutional networks to detect and amplify subtle infant-sucking signals from baby monitor videos.results:The authors successfully classify short video clips of uniform length into NNS and non-NNS periods, and investigate manual and learning-based techniques to piece together local classification results for segmenting longer mixed-activity videos into NNS and non-NNS segments of varying duration. Two novel datasets of annotated infant videos are introduced, including one sourced from a clinical study featuring 19 infant subjects and 183 hours of overnight baby monitor footage.<details>
<summary>Abstract</summary>
Non-nutritive sucking (NNS), which refers to the act of sucking on a pacifier, finger, or similar object without nutrient intake, plays a crucial role in assessing healthy early development. In the case of preterm infants, NNS behavior is a key component in determining their readiness for feeding. In older infants, the characteristics of NNS behavior offer valuable insights into neural and motor development. Additionally, NNS activity has been proposed as a potential safeguard against sudden infant death syndrome (SIDS). However, the clinical application of NNS assessment is currently hindered by labor-intensive and subjective finger-in-mouth evaluations. Consequently, researchers often resort to expensive pressure transducers for objective NNS signal measurement. To enhance the accessibility and reliability of NNS signal monitoring for both clinicians and researchers, we introduce a vision-based algorithm designed for non-contact detection of NNS activity using baby monitor footage in natural settings. Our approach involves a comprehensive exploration of optical flow and temporal convolutional networks, enabling the detection and amplification of subtle infant-sucking signals. We successfully classify short video clips of uniform length into NNS and non-NNS periods. Furthermore, we investigate manual and learning-based techniques to piece together local classification results, facilitating the segmentation of longer mixed-activity videos into NNS and non-NNS segments of varying duration. Our research introduces two novel datasets of annotated infant videos, including one sourced from our clinical study featuring 19 infant subjects and 183 hours of overnight baby monitor footage.
</details>
<details>
<summary>摘要</summary>
非营养性吸引行为（NNS），即吸食瓶、手指或类似物的行为而无营养摄入，在健康早期发展中扮演着关键角色。在幼儿期，NNS行为的特点对神经和运动发展提供了价值的信息。此外，NNS活动还被提议为新生儿死亡综合征（SIDS）的防范手段。然而，NNS评估的临床应用受到劳动力 INTENSIVE 和主观的吸食嘴部评估的限制。因此，研究人员经常使用昂贵的压力传感器来实现对NNS信号的 объектив测量。为了提高NNS信号监测的可 accessibility 和可靠性，我们提出了一种视觉基于的算法，用于非接触地检测幼儿吸食活动，使用宝宝监控器的视频记录。我们的方法包括对光流和时间卷积神经网络进行全面探索，以检测和强制 infant-吸食信号。我们成功地将短视频段分类为NNS和非NNS期间。此外，我们还 investigate 手动和学习基于的技术，以便将本地分类结果组装成长期混合活动视频的NNS和非NNS分 segments。我们的研究 introduce 了一个新的幼儿视频标注 Dataset，包括来自我们临床研究的19名婴儿主体和183小时的夜间宝宝监控器视频记录。
</details></li>
</ul>
<hr>
<h2 id="Stereoscopic-Depth-Perception-Through-Foliage"><a href="#Stereoscopic-Depth-Perception-Through-Foliage" class="headerlink" title="Stereoscopic Depth Perception Through Foliage"></a>Stereoscopic Depth Perception Through Foliage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16120">http://arxiv.org/abs/2310.16120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Kerschner, Rakesh John Amala Arokia Nathan, Rafal Mantiuk, Oliver Bimber</li>
<li>for: 这篇论文是为了探讨人工和计算机方法在推断受遮盖物体深度方面的可能性和限制。</li>
<li>methods: 这篇论文使用了计算机 оптиче sintetic aperture sensing技术，以及人类视觉能力的融合，以实现对受遮盖物体深度的推断。</li>
<li>results: 研究发现，只有当使用了计算机方法和人类视觉能力的融合时，人类可以成功推断受遮盖物体深度。<details>
<summary>Abstract</summary>
Both humans and computational methods struggle to discriminate the depths of objects hidden beneath foliage. However, such discrimination becomes feasible when we combine computational optical synthetic aperture sensing with the human ability to fuse stereoscopic images. For object identification tasks, as required in search and rescue, wildlife observation, surveillance, and early wildfire detection, depth assists in differentiating true from false findings, such as people, animals, or vehicles vs. sun-heated patches at the ground level or in the tree crowns, or ground fires vs. tree trunks. We used video captured by a drone above dense woodland to test users' ability to discriminate depth. We found that this is impossible when viewing monoscopic video and relying on motion parallax. The same was true with stereoscopic video because of the occlusions caused by foliage. However, when synthetic aperture sensing was used to reduce occlusions and disparity-scaled stereoscopic video was presented, whereas computational (stereoscopic matching) methods were unsuccessful, human observers successfully discriminated depth. This shows the potential of systems which exploit the synergy between computational methods and human vision to perform tasks that neither can perform alone.
</details>
<details>
<summary>摘要</summary>
人类和计算方法都有困难在检测被树叶所隐藏的物体的深度。然而，当我们结合计算光学合成开口探测与人类的双目视觉融合时，这种检测变得可能。对于搜索救援、野生动物观察、监测和早期森林火灾检测等任务，深度可以帮助分辨真实的发现和假阳性发现，如人、动物或车辆与地面热腋或树叶之间的区分。我们使用了飞行器飞行在密集的森林区 capture的视频进行测试，发现在单目视频和运动相差探测时，人类无法分辨深度。同时，使用折射 Synthetic Aperture Sensing 减少遮挡和尺度缩放双目视频显示，而计算（双目视觉匹配）方法失败，人类观察员成功地分辨深度。这表明了将计算方法和人类视觉相结合的系统可以执行计算方法和人类无法执行的任务。
</details></li>
</ul>
<hr>
<h2 id="Wakening-Past-Concepts-without-Past-Data-Class-Incremental-Learning-from-Online-Placebos"><a href="#Wakening-Past-Concepts-without-Past-Data-Class-Incremental-Learning-from-Online-Placebos" class="headerlink" title="Wakening Past Concepts without Past Data: Class-Incremental Learning from Online Placebos"></a>Wakening Past Concepts without Past Data: Class-Incremental Learning from Online Placebos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16115">http://arxiv.org/abs/2310.16115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaoyao Liu, Yingying Li, Bernt Schiele, Qianru Sun</li>
<li>for: 这篇研究是针对分类增培学习（Class-Incremental Learning，CIL）中对旧类知识的保留问题进行了深入研究。</li>
<li>methods: 这篇研究使用了知识浓缩（Knowledge Distillation，KD）技术来解决旧类知识的保留问题。具体来说，这篇研究使用了新类数据来进行KD，并发现这会降低模型的适应度和效率。因此，这篇研究提出了使用流行的自由图像数据库，如Google Images，来选择合适的地方（placebos）进行KD。</li>
<li>results: 这篇研究的结果显示了以下几个点：1）使用流行的自由图像数据库选择的placebos可以实现高效的旧类知识保留，2）不需要额外的监督或记忆预算，3）在使用较低的记忆预算时，与一些顶尖的CIL方法进行比较，表现更好。<details>
<summary>Abstract</summary>
Not forgetting old class knowledge is a key challenge for class-incremental learning (CIL) when the model continuously adapts to new classes. A common technique to address this is knowledge distillation (KD), which penalizes prediction inconsistencies between old and new models. Such prediction is made with almost new class data, as old class data is extremely scarce due to the strict memory limitation in CIL. In this paper, we take a deep dive into KD losses and find that "using new class data for KD" not only hinders the model adaption (for learning new classes) but also results in low efficiency for preserving old class knowledge. We address this by "using the placebos of old classes for KD", where the placebos are chosen from a free image stream, such as Google Images, in an automatical and economical fashion. To this end, we train an online placebo selection policy to quickly evaluate the quality of streaming images (good or bad placebos) and use only good ones for one-time feed-forward computation of KD. We formulate the policy training process as an online Markov Decision Process (MDP), and introduce an online learning algorithm to solve this MDP problem without causing much computation costs. In experiments, we show that our method 1) is surprisingly effective even when there is no class overlap between placebos and original old class data, 2) does not require any additional supervision or memory budget, and 3) significantly outperforms a number of top-performing CIL methods, in particular when using lower memory budgets for old class exemplars, e.g., five exemplars per class.
</details>
<details>
<summary>摘要</summary>
不忘旧知识是隐藏学习（CIL）中的关键挑战，当模型不断适应新类时。一种常见的解决方法是知识阶段（KD），它评估新和旧模型的预测不一致。在这篇论文中，我们深入研究KD损失，发现“使用新类数据进行KD”不仅阻碍模型适应新类，而且效率不高于保留旧类知识。我们解决这问题，通过“使用旧类的地精选择”，其中地精选择自由图像流，如Google Images，以自动化和经济的方式。为此，我们训练了在线地精选策略，以快速评估流动图像质量（好或坏地精），并只使用好的地精进行一次 feed-forward 计算。我们将策略训练过程形式化为在线Markov决策过程（MDP），并引入在线学习算法来解决这个MDP问题，不需要多少计算成本。在实验中，我们发现我们的方法具有以下优点：1）效果很好，甚至在没有类 overlap 情况下也能够达到良好的效果，2）不需要额外的监督或存储预算，3）与一些顶尖的CIL方法相比，特别是使用较低的存储预算，例如每个类五个示例。
</details></li>
</ul>
<hr>
<h2 id="Towards-long-tailed-multi-label-disease-classification-from-chest-X-ray-Overview-of-the-CXR-LT-challenge"><a href="#Towards-long-tailed-multi-label-disease-classification-from-chest-X-ray-Overview-of-the-CXR-LT-challenge" class="headerlink" title="Towards long-tailed, multi-label disease classification from chest X-ray: Overview of the CXR-LT challenge"></a>Towards long-tailed, multi-label disease classification from chest X-ray: Overview of the CXR-LT challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16112">http://arxiv.org/abs/2310.16112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory Holste, Yiliang Zhou, Song Wang, Ajay Jaiswal, Mingquan Lin, Sherry Zhuge, Yuzhe Yang, Dongkyun Kim, Trong-Hieu Nguyen-Mau, Minh-Triet Tran, Jaehyup Jeong, Wongi Park, Jongbin Ryu, Feng Hong, Arsh Verma, Yosuke Yamagishi, Changhyun Kim, Hyeryeong Seo, Myungjoo Kang, Leo Anthony Celi, Zhiyong Lu, Ronald M. Summers, George Shih, Zhangyang Wang, Yifan Peng</li>
<li>for: 这个论文是为了研究长尾学习在医疗影像识别中的应用。</li>
<li>methods: 论文使用了多种方法，包括长尾学习、多标签分类和视觉语言基础模型。</li>
<li>results: 研究发现了许多高性能解决方案，并提供了实践的建议 для长尾多标签医疗影像分类。此外，研究还提出了基于视觉语言基础模型的几种新的方法。<details>
<summary>Abstract</summary>
Many real-world image recognition problems, such as diagnostic medical imaging exams, are "long-tailed" $\unicode{x2013}$ there are a few common findings followed by many more relatively rare conditions. In chest radiography, diagnosis is both a long-tailed and multi-label problem, as patients often present with multiple findings simultaneously. While researchers have begun to study the problem of long-tailed learning in medical image recognition, few have studied the interaction of label imbalance and label co-occurrence posed by long-tailed, multi-label disease classification. To engage with the research community on this emerging topic, we conducted an open challenge, CXR-LT, on long-tailed, multi-label thorax disease classification from chest X-rays (CXRs). We publicly release a large-scale benchmark dataset of over 350,000 CXRs, each labeled with at least one of 26 clinical findings following a long-tailed distribution. We synthesize common themes of top-performing solutions, providing practical recommendations for long-tailed, multi-label medical image classification. Finally, we use these insights to propose a path forward involving vision-language foundation models for few- and zero-shot disease classification.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The traditional Chinese form is used in Hong Kong, Macau, and Taiwan.)
</details></li>
</ul>
<hr>
<h2 id="Complex-Image-Generation-SwinTransformer-Network-for-Audio-Denoising"><a href="#Complex-Image-Generation-SwinTransformer-Network-for-Audio-Denoising" class="headerlink" title="Complex Image Generation SwinTransformer Network for Audio Denoising"></a>Complex Image Generation SwinTransformer Network for Audio Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16109">http://arxiv.org/abs/2310.16109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YoushanZhang/CoxImgSwinTransformer">https://github.com/YoushanZhang/CoxImgSwinTransformer</a></li>
<li>paper_authors: Youshan Zhang, Jialu Li</li>
<li>for: 这篇论文旨在提高实际应用中的音频干扰除除颗粒性。</li>
<li>methods: 该论文将音频干扰除问题转化为一个图像生成任务，并使用复杂的SwinTransformer网络捕捉更多的复杂傅立叶域信息。然后，通过结构相似和细节损失函数生成高质量图像，并使用SDR损失函数最小化清洁和干扰音频之间的差异。</li>
<li>results: 对两个标准数据集进行了广泛的实验，结果表明我们提出的模型比现有的方法更高效。<details>
<summary>Abstract</summary>
Achieving high-performance audio denoising is still a challenging task in real-world applications. Existing time-frequency methods often ignore the quality of generated frequency domain images. This paper converts the audio denoising problem into an image generation task. We first develop a complex image generation SwinTransformer network to capture more information from the complex Fourier domain. We then impose structure similarity and detailed loss functions to generate high-quality images and develop an SDR loss to minimize the difference between denoised and clean audios. Extensive experiments on two benchmark datasets demonstrate that our proposed model is better than state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
高性能音频减噪仍然是实际应用中的挑战。现有的时间频域方法通常忽略生成的频域图像质量。本文将音频减噪问题转换成图像生成任务。我们首先开发了复杂的图像生成SwinTransformer网络，以捕捉更多的复杂 Fourier 频域信息。然后，我们对生成的图像强制实施结构相似性和细节损失函数，以生成高质量的图像。同时，我们还开发了SDR损失函数，以最小化减噪后和清晰音频之间的差异。我们在两个标准测试集上进行了广泛的实验，结果表明，我们的提出的模型在比state-of-the-art方法更高的性能。
</details></li>
</ul>
<hr>
<h2 id="LaksNet-an-end-to-end-deep-learning-model-for-self-driving-cars-in-Udacity-simulator"><a href="#LaksNet-an-end-to-end-deep-learning-model-for-self-driving-cars-in-Udacity-simulator" class="headerlink" title="LaksNet: an end-to-end deep learning model for self-driving cars in Udacity simulator"></a>LaksNet: an end-to-end deep learning model for self-driving cars in Udacity simulator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16103">http://arxiv.org/abs/2310.16103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lakshmikar R. Polamreddy, Youshan Zhang</li>
<li>for: 降低机动车事故的风险，提高自动驾驶技术的效果</li>
<li>methods: 提出了一种新的快速深度学习模型，称为&#96;LaksNet’，包括四个卷积层和两个全连接层</li>
<li>results: 对于UDacity simulator中的训练数据，LaksNet模型与许多现有的预训练ImageNet和NVIDIA模型相比，在车辆无法离开轨道的时间方面表现出色。<details>
<summary>Abstract</summary>
The majority of road accidents occur because of human errors, including distraction, recklessness, and drunken driving. One of the effective ways to overcome this dangerous situation is by implementing self-driving technologies in vehicles. In this paper, we focus on building an efficient deep-learning model for self-driving cars. We propose a new and effective convolutional neural network model called `LaksNet' consisting of four convolutional layers and two fully connected layers. We conduct extensive experiments using our LaksNet model with the training data generated from the Udacity simulator. Our model outperforms many existing pre-trained ImageNet and NVIDIA models in terms of the duration of the car for which it drives without going off the track on the simulator.
</details>
<details>
<summary>摘要</summary>
大多数道路事故是因为人类错误所致，包括分心、无负责感和酒后驾车。为了解决这个危险情况，我们可以在车辆中实施自动驾驶技术。在这篇论文中，我们专注于建立高效的深度学习模型 для自动驾驶车辆。我们提出了一个新的和有效的核心神经网络模型，称为“LaksNet”，它包括四个核心神经层和两个全连接层。我们对我们的LaksNet模型进行了广泛的实验，使用来自UDacity simulator的训练数据。我们的模型在训练数据上表现出色，与许多现有的预训ImageNet和NVIDIA模型相比，在 simulator 上驾车时间更长。
</details></li>
</ul>
<hr>
<h2 id="Learned-Uncertainty-driven-Adaptive-Acquisition-for-Photon-Efficient-Multiphoton-Microscopy"><a href="#Learned-Uncertainty-driven-Adaptive-Acquisition-for-Photon-Efficient-Multiphoton-Microscopy" class="headerlink" title="Learned, Uncertainty-driven Adaptive Acquisition for Photon-Efficient Multiphoton Microscopy"></a>Learned, Uncertainty-driven Adaptive Acquisition for Photon-Efficient Multiphoton Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16102">http://arxiv.org/abs/2310.16102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cassandra Tong Ye, Jiashu Han, Kunzan Liu, Anastasios Angelopoulos, Linda Griffith, Kristina Monakhova, Sixian You</li>
<li>for: 这个论文是为了提高多光子镜像技术的信息准确性和速度而写的。</li>
<li>methods: 该论文使用了深度学习方法来降噪多光子镜像测量结果，同时也可以提供图像 pixel 层次的不确定性预测。</li>
<li>results: 该论文使用实验数据证明了其方法可以保持细节特征，并且在降噪和预测不确定性方面超过其他方法。此外，该方法还可以基于图像重新扫描优化采样技术，从而实现120倍的采样时间和总光谱强度减少。<details>
<summary>Abstract</summary>
Multiphoton microscopy (MPM) is a powerful imaging tool that has been a critical enabler for live tissue imaging. However, since most multiphoton microscopy platforms rely on point scanning, there is an inherent trade-off between acquisition time, field of view (FOV), phototoxicity, and image quality, often resulting in noisy measurements when fast, large FOV, and/or gentle imaging is needed. Deep learning could be used to denoise multiphoton microscopy measurements, but these algorithms can be prone to hallucination, which can be disastrous for medical and scientific applications. We propose a method to simultaneously denoise and predict pixel-wise uncertainty for multiphoton imaging measurements, improving algorithm trustworthiness and providing statistical guarantees for the deep learning predictions. Furthermore, we propose to leverage this learned, pixel-wise uncertainty to drive an adaptive acquisition technique that rescans only the most uncertain regions of a sample. We demonstrate our method on experimental noisy MPM measurements of human endometrium tissues, showing that we can maintain fine features and outperform other denoising methods while predicting uncertainty at each pixel. Finally, with our adaptive acquisition technique, we demonstrate a 120X reduction in acquisition time and total light dose while successfully recovering fine features in the sample. We are the first to demonstrate distribution-free uncertainty quantification for a denoising task with real experimental data and the first to propose adaptive acquisition based on reconstruction uncertainty
</details>
<details>
<summary>摘要</summary>
多 photon 微scopie (MPM) 是一种强大的成像工具，对生物组织成像具有关键作用。然而，由于大多数多 photon 微scopie 平台采用点扫描方式，因此在获取时间、预览 angle （FOV）、光学毒性和图像质量之间存在内在的交易，常导致快速、大 FOV 和/或温和成像时需要噪声损害。深度学习可以用于Multiphoton 微scopie 的噪声除除，但这些算法可能会出现幻觉，这可能是医学和科学应用中致命的问题。我们提出了一种同时减噪和预测像素级不确定性的方法，以提高深度学习预测的可信度，并为生成统计保证。此外，我们建议使用这些学习到的像素级不确定性来驱动适应式获取技术，重新扫描样本中的不确定区域。我们在实验室中使用噪声多 photon 微scopie 测量数据进行示例，并证明我们可以保持细节和其他减噪方法的表现相似，同时预测每个像素的不确定性。最后，我们使用我们的适应式获取技术，实现了120倍减少获取时间和总光子剂量，并成功地恢复样本中的细节特征。我们是首次在实验室数据中实现分布无关的不确定性量化，并首次提出适应式获取基于成像重建不确定性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Feature-Registration-for-Unsupervised-Domain-Adaptation"><a href="#Deep-Feature-Registration-for-Unsupervised-Domain-Adaptation" class="headerlink" title="Deep Feature Registration for Unsupervised Domain Adaptation"></a>Deep Feature Registration for Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16100">http://arxiv.org/abs/2310.16100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youshan Zhang, Brian D. Davison</li>
<li>for: 本研究旨在探讨如何更好地对准源频率和目标频率进行匹配，以提高预测性能。</li>
<li>methods: 我们提出了一种深度特征注册（DFR）模型，通过维护域不变特征并同时降低域不同特征和目标频率之间的差异，来实现域适应。此外，我们还提出了一种假标签纠正过程，包括概率软选择和中心基于硬选择，以提高目标域中假标签的质量。</li>
<li>results: 我们在多个UDA benchmark上进行了广泛的实验，结果表明我们的DFR模型能够提高预测性能，达到新的领先水平。<details>
<summary>Abstract</summary>
While unsupervised domain adaptation has been explored to leverage the knowledge from a labeled source domain to an unlabeled target domain, existing methods focus on the distribution alignment between two domains. However, how to better align source and target features is not well addressed. In this paper, we propose a deep feature registration (DFR) model to generate registered features that maintain domain invariant features and simultaneously minimize the domain-dissimilarity of registered features and target features via histogram matching. We further employ a pseudo label refinement process, which considers both probabilistic soft selection and center-based hard selection to improve the quality of pseudo labels in the target domain. Extensive experiments on multiple UDA benchmarks demonstrate the effectiveness of our DFR model, resulting in new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
而尚未得到监督的领域适应（Unsupervised Domain Adaptation，UDA）已经被探索，以利用源领域的标注数据来适应目标领域的无标注数据。然而，如何更好地对应源和目标特征进行对应还没有得到充分的解决。在这篇论文中，我们提出了深度特征注册（Deep Feature Registration，DFR）模型，以生成具有领域不变特征的注册特征，同时使得注册特征和目标特征之间的差异最小化via histogram matching。此外，我们还employs一种假标签重新定义过程，包括概率软选择和中心基于硬选择，以提高目标领域的假标签质量。我们在多个UDA benchmark上进行了广泛的实验，并demonstrate了我们DFR模型的效果，从而实现了新的领先性表现。
</details></li>
</ul>
<hr>
<h2 id="From-Posterior-Sampling-to-Meaningful-Diversity-in-Image-Restoration"><a href="#From-Posterior-Sampling-to-Meaningful-Diversity-in-Image-Restoration" class="headerlink" title="From Posterior Sampling to Meaningful Diversity in Image Restoration"></a>From Posterior Sampling to Meaningful Diversity in Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16047">http://arxiv.org/abs/2310.16047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noa Cohen, Hila Manor, Yuval Bahat, Tomer Michaeli</li>
<li>for: 这个论文主要目标是解决图像恢复中的多样性问题，即每个降低图像都可以有无数多个有效的恢复方案。</li>
<li>methods: 作者提出了一些后处理技术，可以与多样性图像恢复方法结合使用，以生成符号意义的多样性输出。此外，作者还提出了一种实用的方法，使得Diffusion based图像恢复方法可以生成多样性输出，而且只增加了negligible的计算开销。</li>
<li>results: 作者通过广泛的用户研究发现，减少输出之间的相似性是与后采样的策略相比significantly有利的。codes和示例可以在<a target="_blank" rel="noopener" href="https://noa-cohen.github.io/MeaningfulDiversityInIR%E6%89%BE%E5%88%B0%E3%80%82">https://noa-cohen.github.io/MeaningfulDiversityInIR找到。</a><details>
<summary>Abstract</summary>
Image restoration problems are typically ill-posed in the sense that each degraded image can be restored in infinitely many valid ways. To accommodate this, many works generate a diverse set of outputs by attempting to randomly sample from the posterior distribution of natural images given the degraded input. Here we argue that this strategy is commonly of limited practical value because of the heavy tail of the posterior distribution. Consider for example inpainting a missing region of the sky in an image. Since there is a high probability that the missing region contains no object but clouds, any set of samples from the posterior would be entirely dominated by (practically identical) completions of sky. However, arguably, presenting users with only one clear sky completion, along with several alternative solutions such as airships, birds, and balloons, would better outline the set of possibilities. In this paper, we initiate the study of meaningfully diverse image restoration. We explore several post-processing approaches that can be combined with any diverse image restoration method to yield semantically meaningful diversity. Moreover, we propose a practical approach for allowing diffusion based image restoration methods to generate meaningfully diverse outputs, while incurring only negligent computational overhead. We conduct extensive user studies to analyze the proposed techniques, and find the strategy of reducing similarity between outputs to be significantly favorable over posterior sampling. Code and examples are available in https://noa-cohen.github.io/MeaningfulDiversityInIR
</details>
<details>
<summary>摘要</summary>
Image restoration problems typically have infinite solutions, making it difficult to choose the correct one. Many methods generate diverse outputs by sampling from the posterior distribution of natural images. However, this approach is limited because the posterior distribution has a heavy tail, and the generated outputs are often similar. For example, inpainting a missing region of the sky in an image, the posterior distribution is likely to contain only sky, with little variation. Instead of presenting users with multiple identical sky completions, it would be more useful to show alternative solutions such as airships, birds, and balloons. In this paper, we study meaningfully diverse image restoration and propose several post-processing approaches to achieve semantically meaningful diversity. We also propose a practical approach to generate diverse outputs with negligible computational overhead. We conduct user studies to evaluate the proposed techniques and find that reducing similarity between outputs is significantly favorable over posterior sampling. Code and examples are available at https://noa-cohen.github.io/MeaningfulDiversityInIR.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Stanford-ORB-A-Real-World-3D-Object-Inverse-Rendering-Benchmark"><a href="#Stanford-ORB-A-Real-World-3D-Object-Inverse-Rendering-Benchmark" class="headerlink" title="Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark"></a>Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16044">http://arxiv.org/abs/2310.16044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengfei Kuang, Yunzhi Zhang, Hong-Xing Yu, Samir Agarwala, Shangzhe Wu, Jiajun Wu</li>
<li>for: 这个论文旨在提供一个真实世界的3D物体反向渲染测试准则，以评估和比较不同反向渲染方法的性能。</li>
<li>methods: 这篇论文使用了一个新的真实世界对象库，包含了各种不同的自然场景下的物体捕捉数据，以及相应的3D扫描数据、多视图图像和环境照明数据。</li>
<li>results: 该论文首次在真实世界场景下提供了一个完整的对象反向渲染评估准则，并对多种现有方法进行了比较。<details>
<summary>Abstract</summary>
We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering Benchmark. Recent advances in inverse rendering have enabled a wide range of real-world applications in 3D content generation, moving rapidly from research and commercial use cases to consumer devices. While the results continue to improve, there is no real-world benchmark that can quantitatively assess and compare the performance of various inverse rendering methods. Existing real-world datasets typically only consist of the shape and multi-view images of objects, which are not sufficient for evaluating the quality of material recovery and object relighting. Methods capable of recovering material and lighting often resort to synthetic data for quantitative evaluation, which on the other hand does not guarantee generalization to complex real-world environments. We introduce a new dataset of real-world objects captured under a variety of natural scenes with ground-truth 3D scans, multi-view images, and environment lighting. Using this dataset, we establish the first comprehensive real-world evaluation benchmark for object inverse rendering tasks from in-the-wild scenes, and compare the performance of various existing methods.
</details>
<details>
<summary>摘要</summary>
我们引入 Stanford-ORB，一个新的实际世界3D物体反向渲染评估标准。 latest advances in inverse rendering 已经实现了广泛的实际应用在3D内容生成中，从研究和商业用例到consumer devices。 although the results continue to improve, there is no real-world benchmark that can quantitatively assess and compare the performance of various inverse rendering methods. existing real-world datasets typically only consist of the shape and multi-view images of objects, which are not sufficient for evaluating the quality of material recovery and object relighting. methods capable of recovering material and lighting often resort to synthetic data for quantitative evaluation, which on the other hand does not guarantee generalization to complex real-world environments. we introduce a new dataset of real-world objects captured under a variety of natural scenes with ground-truth 3D scans, multi-view images, and environment lighting. using this dataset, we establish the first comprehensive real-world evaluation benchmark for object inverse rendering tasks from in-the-wild scenes, and compare the performance of various existing methods.
</details></li>
</ul>
<hr>
<h2 id="ConvBKI-Real-Time-Probabilistic-Semantic-Mapping-Network-with-Quantifiable-Uncertainty"><a href="#ConvBKI-Real-Time-Probabilistic-Semantic-Mapping-Network-with-Quantifiable-Uncertainty" class="headerlink" title="ConvBKI: Real-Time Probabilistic Semantic Mapping Network with Quantifiable Uncertainty"></a>ConvBKI: Real-Time Probabilistic Semantic Mapping Network with Quantifiable Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16020">http://arxiv.org/abs/2310.16020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joey Wilson, Yuewei Fu, Joshua Friesen, Parker Ewen, Andrew Capodieci, Paramsothy Jayakumar, Kira Barton, Maani Ghaffari</li>
<li>for: 这个论文旨在开发一种卷积神经网络，用于实时 semantic mapping 在不确定环境中。</li>
<li>methods: 这个方法使用 Explicitly updating per-voxel probabilistic distributions within a neural network layer，combines the reliability of classical probabilistic algorithms with the performance and efficiency of modern neural networks。</li>
<li>results: 研究人员通过比较 ConvBKI 与现有深度学习方法和概率算法，发现 ConvBKI 具有更高的可靠性和性能。在实际的实验中，ConvBKI 在具有困难的感知 task 中表现出色。<details>
<summary>Abstract</summary>
In this paper, we develop a modular neural network for real-time semantic mapping in uncertain environments, which explicitly updates per-voxel probabilistic distributions within a neural network layer. Our approach combines the reliability of classical probabilistic algorithms with the performance and efficiency of modern neural networks. Although robotic perception is often divided between modern differentiable methods and classical explicit methods, a union of both is necessary for real-time and trustworthy performance. We introduce a novel Convolutional Bayesian Kernel Inference (ConvBKI) layer which incorporates semantic segmentation predictions online into a 3D map through a depthwise convolution layer by leveraging conjugate priors. We compare ConvBKI against state-of-the-art deep learning approaches and probabilistic algorithms for mapping to evaluate reliability and performance. We also create a Robot Operating System (ROS) package of ConvBKI and test it on real-world perceptually challenging off-road driving data.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了一种模块化神经网络，用于实时semantic mapping在不确定环境中，这种神经网络层中的每个小块拥有精确的概率分布。我们的方法结合了经典的概率算法和现代神经网络的性能和效率。虽然机器人观察通常被分为现代差分方法和经典显式方法，但是两者的结合是实时和可靠性的关键。我们引入了一种新的卷积 bayesian kernel inference（ConvBKI）层，通过 conjugate priors 将semantic segmentation预测结果在3D地图中进行深度wise convolution，并在实时更新per-voxel的概率分布。我们将ConvBKI与当前的深度学习方法和概率算法进行比较，以评估可靠性和性能。我们还创建了一个 Robot Operating System（ROS）包，并在实际的困难的Off-road驾驶数据上测试了ConvBKI。
</details></li>
</ul>
<hr>
<h2 id="CVPR-2023-Text-Guided-Video-Editing-Competition"><a href="#CVPR-2023-Text-Guided-Video-Editing-Competition" class="headerlink" title="CVPR 2023 Text Guided Video Editing Competition"></a>CVPR 2023 Text Guided Video Editing Competition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16003">http://arxiv.org/abs/2310.16003</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/showlab/loveu-tgve-2023">https://github.com/showlab/loveu-tgve-2023</a></li>
<li>paper_authors: Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, Rui He, Feng Hu, Junhua Hu, Hai Huang, Hanyu Zhu, Xu Cheng, Jie Tang, Mike Zheng Shou, Kurt Keutzer, Forrest Iandola</li>
<li>for: 这 paper 是为了提出一个新的视频编辑数据集（TGVE），并且在 CVPR 会议上进行了一场竞赛，以评估模型在这个数据集上的性能。</li>
<li>methods: 这 paper 使用了文本到图像模型（如 Stable Diffusion 和 Imagen）进行生成 AI，并且提出了一个新的竞赛数据集来评估这些模型。</li>
<li>results: 这 paper 描述了竞赛中的赢家方法，并且提供了一个 retrapective 来评估竞赛中的模型性能。<details>
<summary>Abstract</summary>
Humans watch more than a billion hours of video per day. Most of this video was edited manually, which is a tedious process. However, AI-enabled video-generation and video-editing is on the rise. Building on text-to-image models like Stable Diffusion and Imagen, generative AI has improved dramatically on video tasks. But it's hard to evaluate progress in these video tasks because there is no standard benchmark. So, we propose a new dataset for text-guided video editing (TGVE), and we run a competition at CVPR to evaluate models on our TGVE dataset. In this paper we present a retrospective on the competition and describe the winning method. The competition dataset is available at https://sites.google.com/view/loveucvpr23/track4.
</details>
<details>
<summary>摘要</summary>
每天人们观看超过100亿小时视频，大多数这些视频都是人工编辑的，这是一项繁琐的过程。然而，基于文本到图像模型如稳定扩散和图像，生成AI在视频任务上有了很大的进步。然而，评估这些视频任务的进步很难，因为没有标准的评价标准。因此，我们提出了一个新的文本引导视频编辑（TGVE）数据集，并在CVPR上举办了一场比赛来评估模型在我们的TGVE数据集上的性能。在这篇论文中，我们提供了一个回顾这场比赛的报告，并描述了赢家的方法。TGVE数据集可以在以下链接下载：https://sites.google.com/view/loveucvpr23/track4。
</details></li>
</ul>
<hr>
<h2 id="Integrating-View-Conditions-for-Image-Synthesis"><a href="#Integrating-View-Conditions-for-Image-Synthesis" class="headerlink" title="Integrating View Conditions for Image Synthesis"></a>Integrating View Conditions for Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16002">http://arxiv.org/abs/2310.16002</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/viiika/viewcontrol">https://github.com/viiika/viewcontrol</a></li>
<li>paper_authors: Jinbin Bai, Zhen Dong, Aosong Feng, Xiao Zhang, Tian Ye, Kaicheng Zhou, Mike Zheng Shou</li>
<li>for: 图像处理领域中进行细致 semantic 修改现有图像的挑战。</li>
<li>methods: 该 paper 提出了一种创新的框架，通过视点信息来增强图像编辑任务的控制。</li>
<li>results: 通过对现有方法进行评估和与当前领先方法进行比较，我们提供了吸引人的证据，证明我们的框架在多个维度上表现出色。<details>
<summary>Abstract</summary>
In the field of image processing, applying intricate semantic modifications within existing images remains an enduring challenge. This paper introduces a pioneering framework that integrates viewpoint information to enhance the control of image editing tasks. By surveying existing object editing methodologies, we distill three essential criteria, consistency, controllability, and harmony, that should be met for an image editing method. In contrast to previous approaches, our method takes the lead in satisfying all three requirements for addressing the challenge of image synthesis. Through comprehensive experiments, encompassing both quantitative assessments and qualitative comparisons with contemporary state-of-the-art methods, we present compelling evidence of our framework's superior performance across multiple dimensions. This work establishes a promising avenue for advancing image synthesis techniques and empowering precise object modifications while preserving the visual coherence of the entire composition.
</details>
<details>
<summary>摘要</summary>
在图像处理领域，在现有图像中进行细腻 semantic 修改仍然是一项挑战。这篇论文介绍了一种先进的框架，将视点信息integrated到图像编辑任务中，以提高图像编辑的控制能力。通过对现有 объек editing 方法进行检查，我们提炼出三个 essence  criterion，即一致性、可控性和和谐性，这些 criterion 应被满足以解决图像合成的挑战。与先前的方法不同，我们的方法在这些 criterion 中脱颖而出，并在多个维度上提供了吸引人的证据。通过全面的实验，包括量化评估和当今领先的方法进行比较，我们展示了我们的框架在多个维度的优秀表现。这项工作为图像合成技术的发展开辟了新的 Avenues，并为精准对象修改而保持整体图像的视觉一致性提供了新的能力。
</details></li>
</ul>
<hr>
<h2 id="Transitivity-Recovering-Decompositions-Interpretable-and-Robust-Fine-Grained-Relationships"><a href="#Transitivity-Recovering-Decompositions-Interpretable-and-Robust-Fine-Grained-Relationships" class="headerlink" title="Transitivity Recovering Decompositions: Interpretable and Robust Fine-Grained Relationships"></a>Transitivity Recovering Decompositions: Interpretable and Robust Fine-Grained Relationships</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15999">http://arxiv.org/abs/2310.15999</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhrac/trd">https://github.com/abhrac/trd</a></li>
<li>paper_authors: Abhra Chaudhuri, Massimiliano Mancini, Zeynep Akata, Anjan Dutta</li>
<li>for: 本研究旨在批处细腻表示学习中的抽象关系学习，以实现状态环境下的最佳结果。</li>
<li>methods: 本研究使用的方法是基于图像视图的可读性图表示抽象关系，并通过图像视图之间的关系恢复抽象关系的含义。</li>
<li>results: 研究表明，使用可读性图表示抽象关系可以提高表示学习的可读性和稳定性，并且可以与现有的状态环境下的最佳结果匹配。<details>
<summary>Abstract</summary>
Recent advances in fine-grained representation learning leverage local-to-global (emergent) relationships for achieving state-of-the-art results. The relational representations relied upon by such methods, however, are abstract. We aim to deconstruct this abstraction by expressing them as interpretable graphs over image views. We begin by theoretically showing that abstract relational representations are nothing but a way of recovering transitive relationships among local views. Based on this, we design Transitivity Recovering Decompositions (TRD), a graph-space search algorithm that identifies interpretable equivalents of abstract emergent relationships at both instance and class levels, and with no post-hoc computations. We additionally show that TRD is provably robust to noisy views, with empirical evidence also supporting this finding. The latter allows TRD to perform at par or even better than the state-of-the-art, while being fully interpretable. Implementation is available at https://github.com/abhrac/trd.
</details>
<details>
<summary>摘要</summary>
最近的细腻表示学技术发展借鉴本地-全局（emergent）关系以实现当前最佳效果。这些方法使用的关系表示是抽象的。我们想要拆分这种抽象，表示它们为可解释的图像视图。我们开始通过理论来证明，抽象的关系表示实际上是地址本地视图之间的转移关系。基于这一点，我们设计了归纳恢复分解（TRD），一种图像空间搜索算法，可以在实例和类层次上寻找可解释的相似关系，无需后续计算。我们还证明了TRD在噪音视图下的Robustness，并且有实际证明支持这一点。这意味着TRD可以与当前最佳性能相当或更高，同时具有可解释性。实现可以在https://github.com/abhrac/trd上找到。
</details></li>
</ul>
<hr>
<h2 id="Vision-Language-Pseudo-Labels-for-Single-Positive-Multi-Label-Learning"><a href="#Vision-Language-Pseudo-Labels-for-Single-Positive-Multi-Label-Learning" class="headerlink" title="Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning"></a>Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15985">http://arxiv.org/abs/2310.15985</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mvrl/vlpl">https://github.com/mvrl/vlpl</a></li>
<li>paper_authors: Xin Xing, Zhexiao Xiong, Abby Stylianou, Srikumar Sastry, Liyu Gong, Nathan Jacobs</li>
<li>For: 这篇论文是为了解决单个图像可以同时属于多个类别的问题。* Methods: 该论文提出了一种新的视觉语言假标签方法（VLPL），使用视觉语言模型提供强有力的正确和负有力假标签，并与当前最佳方法相比，在 Pascal VOC、MS-COCO、NUS-WIDE 和 CUB-Birds 等四个dataset上减少了5.5%、18.4%、15.2% 和8.4%的错误率。* Results: 该论文的实验结果表明，使用VLPL方法可以在单个图像上预测多个类别时，与当前最佳方法相比，提高了5.5%、18.4%、15.2% 和8.4%的准确率。<details>
<summary>Abstract</summary>
This paper presents a novel approach to Single-Positive Multi-label Learning. In general multi-label learning, a model learns to predict multiple labels or categories for a single input image. This is in contrast with standard multi-class image classification, where the task is predicting a single label from many possible labels for an image. Single-Positive Multi-label Learning (SPML) specifically considers learning to predict multiple labels when there is only a single annotation per image in the training data. Multi-label learning is in many ways a more realistic task than single-label learning as real-world data often involves instances belonging to multiple categories simultaneously; however, most common computer vision datasets predominantly contain single labels due to the inherent complexity and cost of collecting multiple high quality annotations for each instance. We propose a novel approach called Vision-Language Pseudo-Labeling (VLPL), which uses a vision-language model to suggest strong positive and negative pseudo-labels, and outperforms the current SOTA methods by 5.5% on Pascal VOC, 18.4% on MS-COCO, 15.2% on NUS-WIDE, and 8.4% on CUB-Birds. Our code and data are available at https://github.com/mvrl/VLPL.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Geometry-Aware-Video-Quality-Assessment-for-Dynamic-Digital-Human"><a href="#Geometry-Aware-Video-Quality-Assessment-for-Dynamic-Digital-Human" class="headerlink" title="Geometry-Aware Video Quality Assessment for Dynamic Digital Human"></a>Geometry-Aware Video Quality Assessment for Dynamic Digital Human</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15984">http://arxiv.org/abs/2310.15984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Guangtao Zhai</li>
<li>for: 本研究旨在提出一种基于DDH的无参考视频质量评估方法，以解决DDH在生成和传输过程中受到噪声和压缩扭曲的问题。</li>
<li>methods: 本方法使用DDH的统计参数来描述几何特征，从渲染视频中提取空间和时间特征，并将所有特征集成并回归到质量值上。</li>
<li>results: 实验结果表明，提出的方法在DDH-QA数据库上达到了状态对应的性能。<details>
<summary>Abstract</summary>
Dynamic Digital Humans (DDHs) are 3D digital models that are animated using predefined motions and are inevitably bothered by noise/shift during the generation process and compression distortion during the transmission process, which needs to be perceptually evaluated. Usually, DDHs are displayed as 2D rendered animation videos and it is natural to adapt video quality assessment (VQA) methods to DDH quality assessment (DDH-QA) tasks. However, the VQA methods are highly dependent on viewpoints and less sensitive to geometry-based distortions. Therefore, in this paper, we propose a novel no-reference (NR) geometry-aware video quality assessment method for DDH-QA challenge. Geometry characteristics are described by the statistical parameters estimated from the DDHs' geometry attribute distributions. Spatial and temporal features are acquired from the rendered videos. Finally, all kinds of features are integrated and regressed into quality values. Experimental results show that the proposed method achieves state-of-the-art performance on the DDH-QA database.
</details>
<details>
<summary>摘要</summary>
“几何动态人体”（DDH）是一种三维数字模型，通过预先定义的动作来动态显示，但是在生成和传输过程中受到噪音/移动的干扰，需要进行感知评估。通常，DDH会被显示为2Drendered动画影片，因此可以将影片质量评估（VQA）方法应用到DDH质量评估（DDH-QA）任务中。但是，VQA方法对于视点有高度依赖，较少关注几何基于的扭曲。因此，在本文中，我们提出一种新的无参考（NR）几何意识的影片质量评估方法，用于DDH-QA挑战。几何特征被描述为DDH几何特征分布的统计参数。从rendered影片中获取的空间和时间特征。最后，所有类型的特征被统合并对质量值进行回推。实验结果显示，提议的方法在DDH-QA数据库中实现了国际级的表现。
</details></li>
</ul>
<hr>
<h2 id="Decoupled-DETR-Spatially-Disentangling-Localization-and-Classification-for-Improved-End-to-End-Object-Detection"><a href="#Decoupled-DETR-Spatially-Disentangling-Localization-and-Classification-for-Improved-End-to-End-Object-Detection" class="headerlink" title="Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection"></a>Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15955">http://arxiv.org/abs/2310.15955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manyuan Zhang, Guanglu Song, Yu Liu, Hongsheng Li<br>for: 这个研究旨在提高DETR的物体检测性能。methods: 这篇研究使用了DETR的核心架构，并将分类和位置检测转化为不同的任务，以解决DETR的训练问题。results: 这篇研究在MSCOCO数据集上进行了广泛的实验，并证明了该方法可以提高DETR的表现，比如提高Conditional DETR的AP值4.5。<details>
<summary>Abstract</summary>
The introduction of DETR represents a new paradigm for object detection. However, its decoder conducts classification and box localization using shared queries and cross-attention layers, leading to suboptimal results. We observe that different regions of interest in the visual feature map are suitable for performing query classification and box localization tasks, even for the same object. Salient regions provide vital information for classification, while the boundaries around them are more favorable for box regression. Unfortunately, such spatial misalignment between these two tasks greatly hinders DETR's training. Therefore, in this work, we focus on decoupling localization and classification tasks in DETR. To achieve this, we introduce a new design scheme called spatially decoupled DETR (SD-DETR), which includes a task-aware query generation module and a disentangled feature learning process. We elaborately design the task-aware query initialization process and divide the cross-attention block in the decoder to allow the task-aware queries to match different visual regions. Meanwhile, we also observe that the prediction misalignment problem for high classification confidence and precise localization exists, so we propose an alignment loss to further guide the spatially decoupled DETR training. Through extensive experiments, we demonstrate that our approach achieves a significant improvement in MSCOCO datasets compared to previous work. For instance, we improve the performance of Conditional DETR by 4.5 AP. By spatially disentangling the two tasks, our method overcomes the misalignment problem and greatly improves the performance of DETR for object detection.
</details>
<details>
<summary>摘要</summary>
DETR的引入标志着对象检测领域的新 paradigm。然而，DETR的解码器使用共享查询和交叉注意层进行类别和框定位任务，导致不佳的结果。我们发现不同的视觉特征图像区域适合进行查询类别和框定位任务，即使是同一个对象。焦点区域提供了关键的信息 для类别，而周围的边缘区域更适合框定位。然而，这种视觉空间不一致性使得DETR的训练受到很大的阻碍。因此，在这项工作中，我们关注DETR中的地方分解。为达到这一目标，我们提出了一种名为空间地解决DETR（SD-DETR）的新设计方案。我们仔细设计了任务意识查询初始化过程，并将权重分配块分解成多个视觉区域。同时，我们也发现了高分类信息和精准定位预测不一致的问题，因此我们提出了一种对齐损失来进一步引导空间地解DETR的训练。通过广泛的实验，我们证明了我们的方法在COCO dataset上比前一次的工作提高了4.5个AP。通过空间地解DETR，我们解决了视觉空间不一致性问题，并大幅提高了DETR对象检测的性能。
</details></li>
</ul>
<hr>
<h2 id="Improving-Robustness-and-Reliability-in-Medical-Image-Classification-with-Latent-Guided-Diffusion-and-Nested-Ensembles"><a href="#Improving-Robustness-and-Reliability-in-Medical-Image-Classification-with-Latent-Guided-Diffusion-and-Nested-Ensembles" class="headerlink" title="Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles"></a>Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15952">http://arxiv.org/abs/2310.15952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xingbpshen/nested-diffusion">https://github.com/xingbpshen/nested-diffusion</a></li>
<li>paper_authors: Xing Shen, Hengguan Huang, Brennan Nichyporuk, Tal Arbel</li>
<li>for: 提高深度学习模型在医疗图像分析任务中的Robustness，无需预先定义数据增强策略。</li>
<li>methods: 提出了一种基于转换器和condition diffusion模型的三stage方法，通过建立层次特征表示，采用反卷积过程和生成型预测方法，提高模型对医疗图像变化的Robustness。</li>
<li>results: 通过对医疗图像benchmark数据集进行广泛的实验，示出了方法在比较状态方法的Robustness和信任折衔calibration方面的改进，同时也提出了实例水平预测不确定性的评估策略。<details>
<summary>Abstract</summary>
While deep learning models have achieved remarkable success across a range of medical image analysis tasks, deployment of these models in real clinical contexts requires that they be robust to variability in the acquired images. While many methods apply predefined transformations to augment the training data to enhance test-time robustness, these transformations may not ensure the model's robustness to the diverse variability seen in patient images. In this paper, we introduce a novel three-stage approach based on transformers coupled with conditional diffusion models, with the goal of improving model robustness to the kinds of imaging variability commonly encountered in practice without the need for pre-determined data augmentation strategies. To this end, multiple image encoders first learn hierarchical feature representations to build discriminative latent spaces. Next, a reverse diffusion process, guided by the latent code, acts on an informative prior and proposes prediction candidates in a generative manner. Finally, several prediction candidates are aggregated in a bi-level aggregation protocol to produce the final output. Through extensive experiments on medical imaging benchmark datasets, we show that our method improves upon state-of-the-art methods in terms of robustness and confidence calibration. Additionally, we introduce a strategy to quantify the prediction uncertainty at the instance level, increasing their trustworthiness to clinicians using them in clinical practice.
</details>
<details>
<summary>摘要</summary>
深度学习模型在医疗影像分析任务中已经取得了惊人的成功，但是在真实的临床上应用时，这些模型需要能够抗抗各种影像获取过程中的变化。许多方法通过预先定义的变换来增强训练数据，以提高测试时的Robustness，但这些变换可能并不能 garantía模型对实际医疗影像中的多样性的Robustness。在这篇论文中，我们提出了一种新的三个阶段方法，基于 transformers 和 condition diffusion 模型，以提高模型在实际医疗影像中的Robustness，无需预先定义数据增强策略。在这个方法中，多个图像Encoder 先学习层次特征表示，以建立特征空间的掌握。接下来，根据嵌入码，一种逆 diffusion 过程，带有有用的先验，提出了生成性的预测候选者。最后，多个预测候选者通过二级聚合协议来组合，生成最终输出。通过对医疗影像标准 benchmark 数据集进行广泛的实验，我们证明了我们的方法可以比对当前的方法提高Robustness和信任度Calibration。此外，我们还介绍了一种实例水平的预测uncertainty量化策略，使其在临床实践中增加了信任性。
</details></li>
</ul>
<hr>
<h2 id="Language-driven-Scene-Synthesis-using-Multi-conditional-Diffusion-Model"><a href="#Language-driven-Scene-Synthesis-using-Multi-conditional-Diffusion-Model" class="headerlink" title="Language-driven Scene Synthesis using Multi-conditional Diffusion Model"></a>Language-driven Scene Synthesis using Multi-conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15948">http://arxiv.org/abs/2310.15948</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andvg3/LSDM">https://github.com/andvg3/LSDM</a></li>
<li>paper_authors: An Vuong, Minh Nhat Vu, Toan Tien Nguyen, Baoru Huang, Dzung Nguyen, Thieu Vo, Anh Nguyen</li>
<li>for: 本研究旨在提出一种语言驱动的场景生成任务，该任务将文本提示、人体动作和现有物品综合使用，以生成自然的场景。</li>
<li>methods: 我们提出了一种多 condional 干扰模型，该模型通过显式预测导向点来处理和编码多个条件，并且与其他干扰文献的隐式统一方法不同。</li>
<li>results: 我们的方法在实验中表现出色，超过了当前标准准则，并允许自然的场景编辑应用。<details>
<summary>Abstract</summary>
Scene synthesis is a challenging problem with several industrial applications. Recently, substantial efforts have been directed to synthesize the scene using human motions, room layouts, or spatial graphs as the input. However, few studies have addressed this problem from multiple modalities, especially combining text prompts. In this paper, we propose a language-driven scene synthesis task, which is a new task that integrates text prompts, human motion, and existing objects for scene synthesis. Unlike other single-condition synthesis tasks, our problem involves multiple conditions and requires a strategy for processing and encoding them into a unified space. To address the challenge, we present a multi-conditional diffusion model, which differs from the implicit unification approach of other diffusion literature by explicitly predicting the guiding points for the original data distribution. We demonstrate that our approach is theoretically supportive. The intensive experiment results illustrate that our method outperforms state-of-the-art benchmarks and enables natural scene editing applications. The source code and dataset can be accessed at https://lang-scene-synth.github.io/.
</details>
<details>
<summary>摘要</summary>
Scene synthesis is a challenging problem with many industrial applications. Recently, a lot of effort has been put into synthesizing scenes using human motions, room layouts, or spatial graphs as input. However, few studies have addressed this problem from multiple modalities, especially combining text prompts. In this paper, we propose a new task called language-driven scene synthesis, which integrates text prompts, human motion, and existing objects for scene synthesis. Unlike other single-condition synthesis tasks, our problem involves multiple conditions and requires a strategy for processing and encoding them into a unified space. To address the challenge, we present a multi-conditional diffusion model, which differs from the implicit unification approach of other diffusion literature by explicitly predicting the guiding points for the original data distribution. We demonstrate that our approach is theoretically supportive. The intensive experiment results show that our method outperforms state-of-the-art benchmarks and enables natural scene editing applications. The source code and dataset can be accessed at https://lang-scene-synth.github.io/.Here's the translation in Traditional Chinese:Scene synthesis is a challenging problem with many industrial applications. Recently, a lot of effort has been put into synthesizing scenes using human motions, room layouts, or spatial graphs as input. However, few studies have addressed this problem from multiple modalities, especially combining text prompts. In this paper, we propose a new task called language-driven scene synthesis, which integrates text prompts, human motion, and existing objects for scene synthesis. Unlike other single-condition synthesis tasks, our problem involves multiple conditions and requires a strategy for processing and encoding them into a unified space. To address the challenge, we present a multi-conditional diffusion model, which differs from the implicit unification approach of other diffusion literature by explicitly predicting the guiding points for the original data distribution. We demonstrate that our approach is theoretically supportive. The intensive experiment results show that our method outperforms state-of-the-art benchmarks and enables natural scene editing applications. The source code and dataset can be accessed at https://lang-scene-synth.github.io/.
</details></li>
</ul>
<hr>
<h2 id="ShARc-Shape-and-Appearance-Recognition-for-Person-Identification-In-the-wild"><a href="#ShARc-Shape-and-Appearance-Recognition-for-Person-Identification-In-the-wild" class="headerlink" title="ShARc: Shape and Appearance Recognition for Person Identification In-the-wild"></a>ShARc: Shape and Appearance Recognition for Person Identification In-the-wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15946">http://arxiv.org/abs/2310.15946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haidong Zhu, Wanrong Zheng, Zhaoheng Zheng, Ram Nevatia</li>
<li>for: 这篇论文旨在提出一种多模态方法，用于在无控制环境中进行人识别，以提高人识别精度。</li>
<li>methods: 该方法使用了两个编码器：一个姿势和形态编码器（PSE）和一个综合应用编码器（AAE）。PSE编码人体形状通过二维素投影、骨骼运动和三维体形等方式，而AAE提供了两种时间domain的特征聚合方法：注意力基于的特征聚合和平均聚合。</li>
<li>results:  comparative experiments on public datasets（CCVID、MEVID和BRIAR）表明，该方法可以在人识别任务中显著提高精度，比 EXISTS 的方法更高。<details>
<summary>Abstract</summary>
Identifying individuals in unconstrained video settings is a valuable yet challenging task in biometric analysis due to variations in appearances, environments, degradations, and occlusions. In this paper, we present ShARc, a multimodal approach for video-based person identification in uncontrolled environments that emphasizes 3-D body shape, pose, and appearance. We introduce two encoders: a Pose and Shape Encoder (PSE) and an Aggregated Appearance Encoder (AAE). PSE encodes the body shape via binarized silhouettes, skeleton motions, and 3-D body shape, while AAE provides two levels of temporal appearance feature aggregation: attention-based feature aggregation and averaging aggregation. For attention-based feature aggregation, we employ spatial and temporal attention to focus on key areas for person distinction. For averaging aggregation, we introduce a novel flattening layer after averaging to extract more distinguishable information and reduce overfitting of attention. We utilize centroid feature averaging for gallery registration. We demonstrate significant improvements over existing state-of-the-art methods on public datasets, including CCVID, MEVID, and BRIAR.
</details>
<details>
<summary>摘要</summary>
在未控制的视频设置下，识别人员是生物ometric分析中的一项有价值 yet challenging task，因为人体外观、环境、损害和遮挡会导致人脸识别的困难。在这篇论文中，我们提出了 ShARc，一种多模态方法，用于在未控制环境下进行视频基于人脸识别。我们提出了两个编码器：一个 pose和形体编码器（PSE），以及一个综合应用 aparearance编码器（AAE）。PSE编码器通过矩阵化的轮廓、骨骼运动和三维体形来编码人体形状，而 AAE 提供了两种时间上的特征聚合方法：注意力基于的特征聚合和平均聚合。为了注意力基于的特征聚合，我们使用空间和时间的注意力来强调关键区域的人脸分辨率。为了平均聚合，我们引入了一种新的扁平层来抽取更多的分辨率信息，并避免注意力的过拟合。我们使用中心点特征平均进行 галерее注册。我们在公共数据集上达到了与现有状态的显著提升。
</details></li>
</ul>
<hr>
<h2 id="RePoseDM-Recurrent-Pose-Alignment-and-Gradient-Guidance-for-Pose-Guided-Image-Synthesis"><a href="#RePoseDM-Recurrent-Pose-Alignment-and-Gradient-Guidance-for-Pose-Guided-Image-Synthesis" class="headerlink" title="RePoseDM: Recurrent Pose Alignment and Gradient Guidance for Pose Guided Image Synthesis"></a>RePoseDM: Recurrent Pose Alignment and Gradient Guidance for Pose Guided Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16074">http://arxiv.org/abs/2310.16074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anant Khandelwal</li>
<li>For:  pose-guided person image synthesis task* Methods:  recurrent pose alignment, gradient guidance from pose interaction fields* Results:  photorealistic appearance, flawless pose transfer, plausible pose transfer trajectories, efficient gradient guidanceHere’s the full text in Simplified Chinese:</li>
<li>for: 人像图像合成任务，需要重新渲染参考图像，以保持高真实度和精准的 pose 传输。由于人像图像具有高结构化特征，现有方法通常需要密集的连接来处理复杂的扭变和遮挡。但是， convolutional neural networks 生成的特征图不具有对称性，因此即使使用多级扭变和masking在latent space中，pose 对Alignment也不是完美的。我们引用了 diffusion 模型可以生成高真实度的图像，并提出了 recurrent pose alignment 来提供 pose-aligned texture 特征作为 conditional guidance。此外，我们还提出了基于 pose 互动场的梯度导航，可以输出target pose 的可能性场，帮助学习 plausible pose transfer 曲线，以保持图像的真实度和细节。</li>
<li>methods:  recurrent pose alignment, gradient guidance from pose interaction fields</li>
<li>results: 高真实度的出现，精准的 pose 传输，可靠的 pose transfer 曲线，高效的梯度导航I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Pose-guided person image synthesis task requires re-rendering a reference image, which should have a photorealistic appearance and flawless pose transfer. Since person images are highly structured, existing approaches require dense connections for complex deformations and occlusions because these are generally handled through multi-level warping and masking in latent space. But the feature maps generated by convolutional neural networks do not have equivariance, and hence even the multi-level warping does not have a perfect pose alignment. Inspired by the ability of the diffusion model to generate photorealistic images from the given conditional guidance, we propose recurrent pose alignment to provide pose-aligned texture features as conditional guidance. Moreover, we propose gradient guidance from pose interaction fields, which output the distance from the valid pose manifold given a target pose as input. This helps in learning plausible pose transfer trajectories that result in photorealism and undistorted texture details. Extensive results on two large-scale benchmarks and a user study demonstrate the ability of our proposed approach to generate photorealistic pose transfer under challenging scenarios. Additionally, we prove the efficiency of gradient guidance in pose-guided image generation on the HumanArt dataset with fine-tuned stable diffusion.
</details>
<details>
<summary>摘要</summary>
pose-guided人像图像合成任务需要重新渲染引用图像，该图像应该具有摄影真实的外观和完美的pose倾斜。由于人像图像具有高度结构，现有方法通常需要密集的连接来处理复杂的变形和遮挡。但是，由 convolutional neural networks 生成的特征图不具有对称性，因此，即使使用多级扭曲和masking在 latent space中也不会得到完美的pose对齐。为了解决这个问题，我们提出了 recurrent pose alignment，它可以提供pose-aligned的текстура特征作为条件引用。此外，我们还提出了来自pose交互场的梯度导航，它输出了目标pose manifold 中的距离。这有助于学习plausible的pose转移轨迹，以实现摄影真实和不受扭曲的текстура细节。我们在两个大规模的benchmark上和用户研究中展示了我们提出的方法能够在复杂的场景下生成摄影真实的pose转移。此外，我们还证明了梯度导航在pose-guided图像生成中的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Mitigate-Domain-Shift-by-Primary-Auxiliary-Objectives-Association-for-Generalizing-Person-ReID"><a href="#Mitigate-Domain-Shift-by-Primary-Auxiliary-Objectives-Association-for-Generalizing-Person-ReID" class="headerlink" title="Mitigate Domain Shift by Primary-Auxiliary Objectives Association for Generalizing Person ReID"></a>Mitigate Domain Shift by Primary-Auxiliary Objectives Association for Generalizing Person ReID</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15913">http://arxiv.org/abs/2310.15913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qilei Li, Shaogang Gong</li>
<li>for: 提高人脸识别模型在不同频谱环境下的泛化能力</li>
<li>methods: 通过同时学习主要人脸识别任务和副任务（pedestrian saliency检测）来减少模型域特性，并通过PAOA机制来协调两个任务的损失函数</li>
<li>results: 实验表明提出的PAOA模型在不同频谱环境下表现出色，比较主流的单任务模型和多任务模型更高效。<details>
<summary>Abstract</summary>
While deep learning has significantly improved ReID model accuracy under the independent and identical distribution (IID) assumption, it has also become clear that such models degrade notably when applied to an unseen novel domain due to unpredictable/unknown domain shift. Contemporary domain generalization (DG) ReID models struggle in learning domain-invariant representation solely through training on an instance classification objective. We consider that a deep learning model is heavily influenced and therefore biased towards domain-specific characteristics, e.g., background clutter, scale and viewpoint variations, limiting the generalizability of the learned model, and hypothesize that the pedestrians are domain invariant owning they share the same structural characteristics. To enable the ReID model to be less domain-specific from these pure pedestrians, we introduce a method that guides model learning of the primary ReID instance classification objective by a concurrent auxiliary learning objective on weakly labeled pedestrian saliency detection. To solve the problem of conflicting optimization criteria in the model parameter space between the two learning objectives, we introduce a Primary-Auxiliary Objectives Association (PAOA) mechanism to calibrate the loss gradients of the auxiliary task towards the primary learning task gradients. Benefiting from the harmonious multitask learning design, our model can be extended with the recent test-time diagram to form the PAOA+, which performs on-the-fly optimization against the auxiliary objective in order to maximize the model's generative capacity in the test target domain. Experiments demonstrate the superiority of the proposed PAOA model.
</details>
<details>
<summary>摘要</summary>
深度学习已经在独立和同样分布（IID）假设下明显提高了人识别模型的准确率，但同时也变得明显在未见的新领域中运行时失效，这是因为不可预测的领域转移。当前的领域总结（DG）人识别模型在学习领域不可变的表示方法时遇到了困难，我们认为深度学习模型受到领域特有的特征，如背景噪音、缩放和视角变化，这限制了学习的模型通用性，我们假设人识别对象在不同领域中具有相同的结构特征。为了让人识别模型免受这些纯净的人识别对象的影响，我们提出了一种方法，即通过同时学习主要人识别实例分类目标和 auxiliary 任务来导向模型学习。为了解决模型参数空间中主要和auxiliary任务之间的冲突问题，我们提出了主要-auxiliary任务关联（PAOA）机制，该机制可以在模型参数空间中均衡两个学习任务的损失导数。通过和谐多任务学习设计，我们的模型可以通过添加最近的测试时 diagram 来形成 PAOA+，该模型在测试目标领域中进行在线优化，以最大化模型的生成能力。实验结果表明我们的 PAOA 模型具有优势。
</details></li>
</ul>
<hr>
<h2 id="YOLO-Angio-An-Algorithm-for-Coronary-Anatomy-Segmentation"><a href="#YOLO-Angio-An-Algorithm-for-Coronary-Anatomy-Segmentation" class="headerlink" title="YOLO-Angio: An Algorithm for Coronary Anatomy Segmentation"></a>YOLO-Angio: An Algorithm for Coronary Anatomy Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15898">http://arxiv.org/abs/2310.15898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Liu, Hui Lin, Aggelos K. Katsaggelos, Adrienne Kline</li>
<li>for: 本研究旨在提供一种快速和准确的自动 coronary artery disease 诊断方法，用于替代人工评估。</li>
<li>methods: 本研究使用了一种三 stage 方法，包括预处理和特征选择，然后使用 YOLOv8  ensemble 模型生成可能的血管候选者，并最后使用一种逻辑基本方法重建 coronary tree。</li>
<li>results: 本研究在 MICCAI 2023 自动 coronary artery disease 诊断挑战中获得第三名，并在官方评估 metric 上得到了 F1 分数为 0.422 和 0.4289 的好成绩。<details>
<summary>Abstract</summary>
Coronary angiography remains the gold standard for diagnosis of coronary artery disease, the most common cause of death worldwide. While this procedure is performed more than 2 million times annually, there remain few methods for fast and accurate automated measurement of disease and localization of coronary anatomy. Here, we present our solution to the Automatic Region-based Coronary Artery Disease diagnostics using X-ray angiography images (ARCADE) challenge held at MICCAI 2023. For the artery segmentation task, our three-stage approach combines preprocessing and feature selection by classical computer vision to enhance vessel contrast, followed by an ensemble model based on YOLOv8 to propose possible vessel candidates by generating a vessel map. A final segmentation is based on a logic-based approach to reconstruct the coronary tree in a graph-based sorting method. Our entry to the ARCADE challenge placed 3rd overall. Using the official metric for evaluation, we achieved an F1 score of 0.422 and 0.4289 on the validation and hold-out sets respectively.
</details>
<details>
<summary>摘要</summary>
coronary angiography 仍然是 coronary artery disease 诊断的标准方法，全球最常见的死亡原因之一。尽管这个过程每年超过 2 百万次执行，但是尚未有快速和准确的自动测量疾病和 coronary anatomy 的方法。在这里，我们介绍了我们在 MICCAI 2023 上参加的 Automatic Region-based Coronary Artery Disease diagnostics using X-ray angiography images (ARCADE) 挑战的解决方案。 для artery segmentation 任务，我们采用了三个阶段的方法，首先使用类传统计算机视觉技术进行预处理和特征选择，以增强血管对比度，然后使用 YOLOv8  ensemble model 提出可能的血管候选者，生成血管地图。最后，我们使用逻辑基于方法来重建 coronary tree，并使用图表分类法来排序。我们在 ARCADE 挑战中的参赛作品位列全国第三。使用官方的评价指标，我们在验证集和保留集上的 F1 分数分别为 0.422 和 0.4289。
</details></li>
</ul>
<hr>
<h2 id="Correlation-Debiasing-for-Unbiased-Scene-Graph-Generation-in-Videos"><a href="#Correlation-Debiasing-for-Unbiased-Scene-Graph-Generation-in-Videos" class="headerlink" title="Correlation Debiasing for Unbiased Scene Graph Generation in Videos"></a>Correlation Debiasing for Unbiased Scene Graph Generation in Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16073">http://arxiv.org/abs/2310.16073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anant Khandelwal</li>
<li>for: 生成视频中的动态Scene Graph（SGG）需要不仅具备对场景中 объек 的全面理解，还需要模型这些对象之间的时间变化和互动。</li>
<li>methods: 我们提出了Flow-aware temporal consistency和Correlation Debiasing with uncertainty attenuation（FloCoDe），它使用流动特征扭曲来检测帧中的时间一致性，并使用相关偏置来学习不偏的关系表示。</li>
<li>results: 我们的方法可以减少生成的Scene Graph中的长untailed分布问题，并提高生成的Scene Graph的性能，相比普通方法可以提高到4.1%。<details>
<summary>Abstract</summary>
Dynamic scene graph generation (SGG) from videos requires not only comprehensive understanding of objects across the scenes that are prone to temporal fluctuations but also a model the temporal motions and interactions with different objects. Moreover, the long-tailed distribution of visual relationships is the crucial bottleneck of most dynamic SGG methods, since most of them focus on capturing spatio-temporal context using complex architectures, which leads to the generation of biased scene graphs. To address these challenges, we propose FloCoDe: Flow-aware temporal consistency and Correlation Debiasing with uncertainty attenuation for unbiased dynamic scene graphs. FloCoDe employs feature warping using flow to detect temporally consistent objects across the frames. In addition, it uses correlation debiasing to learn the unbiased relation representation for long-tailed classes. Moreover, to attenuate the predictive uncertainties, it uses a mixture of sigmoidal cross-entropy loss and contrastive loss to incorporate label correlations to identify the commonly co-occurring relations and help debias the long-tailed ones. Extensive experimental evaluation shows a performance gain as high as 4.1% showing the superiority of generating more unbiased scene graphs.
</details>
<details>
<summary>摘要</summary>
干净场景图生成（SGG）从视频中需要不仅具备场景中对象的全面理解，而且还需要模型这些对象的时间运动和不同对象之间的交互。此外，视觉关系的长尾分布是大多数动态SGG方法的关键瓶颈，因为大多数方法强调通过复杂的建筑来捕捉空间时间上下文，从而导致生成偏向的场景图。为解决这些挑战，我们提出了FloCoDe：流程承诺和对称偏移以降低不当的动态场景图生成。FloCoDe使用流程映射来检测帧中的时间一致对象。此外，它使用对称偏移来学习不偏向的关系表示，并使用权重混合的sigmoid混合 entropy损失和对比损失来吸收标签相关性，以帮助减少预测不确定性。广泛的实验评估表明，FloCoDe可以提高生成不偏向场景图的性能，最高提高4.1%。
</details></li>
</ul>
<hr>
<h2 id="On-Responsible-Machine-Learning-Datasets-with-Fairness-Privacy-and-Regulatory-Norms"><a href="#On-Responsible-Machine-Learning-Datasets-with-Fairness-Privacy-and-Regulatory-Norms" class="headerlink" title="On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms"></a>On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15848">http://arxiv.org/abs/2310.15848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surbhi Mittal, Kartik Thakral, Richa Singh, Mayank Vatsa, Tamar Glaser, Cristian Canton Ferrer, Tal Hassner</li>
<li>for: 本研究旨在提出一个责任机器学习数据集框架，以评估数据集的可信worthiness。</li>
<li>methods: 本研究使用了责任机器学习数据集框架，考虑了公平、隐私和法规遵循性等方面，并提出了改进数据集文件的建议。</li>
<li>results: 经过100多个数据集的调查，发现现有数据集中有严重的公平、隐私和法规遵循性问题。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. In recent years, there have been severe concerns over the trustworthiness of AI technologies. The scientific community has focused on the development of trustworthy AI algorithms. However, machine and deep learning algorithms, popular in the AI community today, depend heavily on the data used during their development. These learning algorithms identify patterns in the data, learning the behavioral objective. Any flaws in the data have the potential to translate directly into algorithms. In this study, we discuss the importance of Responsible Machine Learning Datasets and propose a framework to evaluate the datasets through a responsible rubric. While existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand its role in the algorithm. We discuss responsible datasets through the lens of fairness, privacy, and regulatory compliance and provide recommendations for constructing future datasets. After surveying over 100 datasets, we use 60 datasets for analysis and demonstrate that none of these datasets is immune to issues of fairness, privacy preservation, and regulatory compliance. We provide modifications to the ``datasheets for datasets" with important additions for improved dataset documentation. With governments around the world regularizing data protection laws, the method for the creation of datasets in the scientific community requires revision. We believe this study is timely and relevant in today's era of AI.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）已经渗透到了不同的科学领域，提供了惊人的改进，对各种任务的表现有了很大的提高。然而，在最近几年，关于AI技术的可靠性产生了严重的担忧。科学社区在开发可靠AI算法方面做出了巨大的努力。然而，机器学习和深度学习算法，今天科学社区中流行的算法，具有很强的数据依赖性。这些学习算法通过数据中的模式识别，学习行为目标。数据中的任何漏洞都可能直接影响算法。在这种情况下，我们认为负责任机器学习数据集的重要性是不可或缺的。我们提出了一种评估数据集的负责任框架，通过评估数据集的可责任性来理解它在算法中的角色。我们通过公平、隐私保护和法规遵守来评估数据集的负责任性。我们对100余个数据集进行了调查，并选择60个数据集进行分析，发现这些数据集中有很多问题，包括公平、隐私保护和法规遵守。我们对数据集的“数据Sheet”进行了重要的修改，以便更好地记录数据集的信息。随着政府在全球范围内规范数据保护法律，科学社区在创建数据集的方法需要进行修订。我们认为这种研究在当今AI时代非常时间和有 relevance。
</details></li>
</ul>
<hr>
<h2 id="CPSeg-Finer-grained-Image-Semantic-Segmentation-via-Chain-of-Thought-Language-Prompting"><a href="#CPSeg-Finer-grained-Image-Semantic-Segmentation-via-Chain-of-Thought-Language-Prompting" class="headerlink" title="CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought Language Prompting"></a>CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought Language Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16069">http://arxiv.org/abs/2310.16069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Li</li>
<li>for: 该研究旨在提高图像分割性能，通过 integrate 文本信息和图像进行语言导向的数据利用。</li>
<li>methods: 该framework 使用了一种新的 “Chain-of-Thought” 过程，通过将多个句子中的文本信息编码，形成一个 coherent 的链接。</li>
<li>results: 我们的质量和量测试表明，CPSeg 能够有效地提高图像分割性能。<details>
<summary>Abstract</summary>
Natural scene analysis and remote sensing imagery offer immense potential for advancements in large-scale language-guided context-aware data utilization. This potential is particularly significant for enhancing performance in downstream tasks such as object detection and segmentation with designed language prompting. In light of this, we introduce the CPSeg, Chain-of-Thought Language Prompting for Finer-grained Semantic Segmentation), an innovative framework designed to augment image segmentation performance by integrating a novel "Chain-of-Thought" process that harnesses textual information associated with images. This groundbreaking approach has been applied to a flood disaster scenario. CPSeg encodes prompt texts derived from various sentences to formulate a coherent chain-of-thought. We propose a new vision-language dataset, FloodPrompt, which includes images, semantic masks, and corresponding text information. This not only strengthens the semantic understanding of the scenario but also aids in the key task of semantic segmentation through an interplay of pixel and text matching maps. Our qualitative and quantitative analyses validate the effectiveness of CPSeg.
</details>
<details>
<summary>摘要</summary>
自然场景分析和远程感知影像具有巨大的潜力，可以提高大规模语言引导的上下文意识数据利用的性能。特别是在对象检测和分割任务中，采用设计语言提示可以提高性能。为此，我们介绍了CPSeg（语义粒度分割）框架，它通过 integrate 一种新的 "链条思维" 过程，将图像与文本信息相结合，以提高图像分割性能。我们在洪水灾害场景中应用了这种创新性的方法。CPSeg 将文本信息转化为谱文本，并将它们组织成一个 coherent 的链条思维。我们提出了一个新的视力语言数据集，FloodPrompt，该数据集包括图像、semantic 面积和相应的文本信息。这不仅强化了洪水场景的 semantic 理解，还帮助实现semantic segmentation 任务中的像素和文本匹配图。我们的质量和量统计分析证明了 CPSeg 的有效性。
</details></li>
</ul>
<hr>
<h2 id="Unpaired-MRI-Super-Resolution-with-Self-Supervised-Contrastive-Learning"><a href="#Unpaired-MRI-Super-Resolution-with-Self-Supervised-Contrastive-Learning" class="headerlink" title="Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning"></a>Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15767">http://arxiv.org/abs/2310.15767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Li, Quanwei Liu, Jianan Liu, Xiling Liu, Yanni Dong, Tao Huang, Zhihan Lv</li>
<li>for: 提高临床设备的诊断精度</li>
<li>methods: 使用自我指导的对比学习来提高SR性能，使用真实的高解度图像和生成的SR图像构建正例对</li>
<li>results: 使用限制性的训练数据可以获得显著提高的峰值信号噪声比和结构相似度指标<details>
<summary>Abstract</summary>
High-resolution (HR) magnetic resonance imaging (MRI) is crucial for enhancing diagnostic accuracy in clinical settings. Nonetheless, the inherent limitation of MRI resolution restricts its widespread applicability. Deep learning-based image super-resolution (SR) methods exhibit promise in improving MRI resolution without additional cost. However, these methods frequently require a substantial number of HR MRI images for training, which can be challenging to acquire. In this paper, we propose an unpaired MRI SR approach that employs self-supervised contrastive learning to enhance SR performance with limited training data. Our approach leverages both authentic HR images and synthetically generated SR images to construct positive and negative sample pairs, thus facilitating the learning of discriminative features. Empirical results presented in this study underscore significant enhancements in the peak signal-to-noise ratio and structural similarity index, even when a paucity of HR images is available. These findings accentuate the potential of our approach in addressing the challenge of limited training data, thereby contributing to the advancement of high-resolution MRI in clinical applications.
</details>
<details>
<summary>摘要</summary>
高分辨率（HR）磁共振成像（MRI）是诊断精度的关键因素，但MRI的自然限制使其广泛应用受到限制。深度学习基于图像超分辨（SR）方法表现出提高MRI分辨率的承诺，但这些方法frequently需要大量的HR MRI图像进行训练，这可能困难以获得。本文提出了一种没有对应HR图像的MRI SR方法，使用自我超级vised学习来提高SR性能，并利用高分辨率图像和生成的SR图像来构建正例对和负例对，从而促进学习抽象特征。实验结果表明，即使只有少量HR图像可用，SR性能也能得到显著提高，包括峰值信号噪声比和结构相似度指数。这些结果强调了我们的方法在有限训练数据情况下的潜在价值，并为高分辨率MRI在临床应用中的进一步发展提供了贡献。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Models-for-Classification-of-COVID-19-Cases-by-Medical-Images"><a href="#Deep-Learning-Models-for-Classification-of-COVID-19-Cases-by-Medical-Images" class="headerlink" title="Deep Learning Models for Classification of COVID-19 Cases by Medical Images"></a>Deep Learning Models for Classification of COVID-19 Cases by Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16851">http://arxiv.org/abs/2310.16851</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aditya-saxena-7/Detection-of-COVID-19-from-Chest-X-Ray-images-using-CNNs">https://github.com/aditya-saxena-7/Detection-of-COVID-19-from-Chest-X-Ray-images-using-CNNs</a></li>
<li>paper_authors: Amir Ali</li>
<li>for: 这个研究旨在提高新型冠状病毒诊断的准确性和速度，通过利用深度学习模型对患者的Computed Tomography（CT）图像进行精确分类。</li>
<li>methods: 我们的研究使用了深度转移学习模型，包括DenseNet201、GoogleNet和AlexNet，进行 Covid-19 分类。我们还对这些模型进行了精心的超参数调整，以提高其性能。</li>
<li>results: 我们的研究结果表明，使用深度学习模型可以提高 Covid-19 诊断的准确性和速度。我们的模型可以处理多种医疗图像类型，并能够准确地识别特征性的 Covid-19 特征。这些成果表明了这些模型在全球 Covid-19 斗争中的潜在作用。<details>
<summary>Abstract</summary>
In recent times, the use of chest Computed Tomography (CT) images for detecting coronavirus infections has gained significant attention, owing to their ability to reveal bilateral changes in affected individuals. However, classifying patients from medical images presents a formidable challenge, particularly in identifying such bilateral changes. To tackle this challenge, our study harnesses the power of deep learning models for the precise classification of infected patients. Our research involves a comparative analysis of deep transfer learning-based classification models, including DenseNet201, GoogleNet, and AlexNet, against carefully chosen supervised learning models. Additionally, our work encompasses Covid-19 classification, which involves the identification and differentiation of medical images, such as X-rays and electrocardiograms, that exhibit telltale signs of Covid-19 infection. This comprehensive approach ensures that our models can handle a wide range of medical image types and effectively identify characteristic patterns indicative of Covid-19. By conducting meticulous research and employing advanced deep learning techniques, we have made significant strides in enhancing the accuracy and speed of Covid-19 diagnosis. Our results demonstrate the effectiveness of these models and their potential to make substantial contributions to the global effort to combat COVID-19.
</details>
<details>
<summary>摘要</summary>
Recently, the use of chest Computed Tomography (CT) images for detecting coronavirus infections has gained significant attention due to their ability to reveal bilateral changes in affected individuals. However, classifying patients from medical images presents a formidable challenge, particularly in identifying such bilateral changes. To tackle this challenge, our study employs deep learning models for precise classification of infected patients. Our research involves a comparative analysis of deep transfer learning-based classification models, including DenseNet201, GoogleNet, and AlexNet, against carefully chosen supervised learning models. Additionally, our work encompasses Covid-19 classification, which involves the identification and differentiation of medical images, such as X-rays and electrocardiograms, that exhibit telltale signs of Covid-19 infection. This comprehensive approach ensures that our models can handle a wide range of medical image types and effectively identify characteristic patterns indicative of Covid-19. By conducting meticulous research and employing advanced deep learning techniques, we have made significant strides in enhancing the accuracy and speed of Covid-19 diagnosis. Our results demonstrate the effectiveness of these models and their potential to make substantial contributions to the global effort to combat COVID-19.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-Temporal-and-Causal-Reasoners-for-Video-Question-Answering"><a href="#Large-Language-Models-are-Temporal-and-Causal-Reasoners-for-Video-Question-Answering" class="headerlink" title="Large Language Models are Temporal and Causal Reasoners for Video Question Answering"></a>Large Language Models are Temporal and Causal Reasoners for Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15747">http://arxiv.org/abs/2310.15747</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlvlab/Flipped-VQA">https://github.com/mlvlab/Flipped-VQA</a></li>
<li>paper_authors: Dohwan Ko, Ji Soo Lee, Wooyoung Kang, Byungseok Roh, Hyunwoo J. Kim<br>for: 这paper是为了解决视频问答任务中的语言偏见问题，使用大语言模型(LLMs)提供了有效的假设，但是这些假设常导致模型过于依赖问题，而忽略视频内容。methods: 该paper提出了一种新的框架 called Flipped-VQA，它鼓励模型预测所有的 $\langle$V, Q, A$\rangle$  triplet，并通过翻转源对和目标标签来理解它们之间的复杂关系。results: 在五个挑战性的视频问答 benchmark 上，LLaMA-VQA模型，基于 Flipped-VQA 框架，超过了基于 LLMs 和非 LLMS 的模型的性能。此外，该框架可以应用于不同的 LLMs (OPT 和 GPT-J)，并在这些模型中提高了表现。empirical 表明，Flipped-VQA 不仅增强了语言偏见的利用，还减轻了语言偏见，导致错误答案依赖于问题。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting $\textit{linguistic shortcuts}$ for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, $\textit{i.e.}$, $\textit{linguistic bias}$, while ignoring visual content. This is also known as `ungrounded guesses' or `hallucinations'. To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在各种自然语言理解和生成任务上表现出色。我们发现LLMs在视频问答（VideoQA）任务上提供了有效的先验知识，但这些先验知识经常导致视频内容被忽略，从而导致 incorrect answers。这也被称为“不扎实的猜测”或“幻觉”。为解决这个问题并利用LLMs的先验知识，我们提出了一种新的框架——翻转VQA（Flipped-VQA），强制模型预测所有的 $\langle$V, Q, A$\rangle$ 三元组。在这篇论文中，我们将应用Flipped-VQA于 LLMA 模型，并在五个复杂的 VideoQA  benchmark 上取得了比 LLMS 和非 LLMS 模型更高的表现。此外，我们的 Flipped-VQA 是一个通用的框架，可以应用于多种 LLMS（OPT 和 GPT-J），并在不同的 LLMS 上提高其表现。我们经验表明，Flipped-VQA 不仅增强了语言短cut的利用，还减轻语言偏见，这种偏见导致 incorrect answers 依赖于问题。代码可以在 https://github.com/mlvlab/Flipped-VQA 上获取。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Medical-Image-Classification-using-Prototype-Learning-and-Privileged-Information"><a href="#Interpretable-Medical-Image-Classification-using-Prototype-Learning-and-Privileged-Information" class="headerlink" title="Interpretable Medical Image Classification using Prototype Learning and Privileged Information"></a>Interpretable Medical Image Classification using Prototype Learning and Privileged Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15741">http://arxiv.org/abs/2310.15741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xrad-ulm/proto-caps">https://github.com/xrad-ulm/proto-caps</a></li>
<li>paper_authors: Luisa Gallee, Meinrad Beer, Michael Goetz<br>for: 这个论文主要是为了提高医疗图像分类的可解释性和性能。methods: 这个论文使用了 capsule networks、prototype learning 和特权信息来提高模型的可解释性和性能。results: 对于 LIDC-IDRI 数据集，该方法可以同时提高预测精度和可解释性，比基eline模型提高了6.3%的精度。同时，模型可以提供 случа例理解和原型表示，让 radiologist 定义的特征得到视觉验证。<details>
<summary>Abstract</summary>
Interpretability is often an essential requirement in medical imaging. Advanced deep learning methods are required to address this need for explainability and high performance. In this work, we investigate whether additional information available during the training process can be used to create an understandable and powerful model. We propose an innovative solution called Proto-Caps that leverages the benefits of capsule networks, prototype learning and the use of privileged information. Evaluating the proposed solution on the LIDC-IDRI dataset shows that it combines increased interpretability with above state-of-the-art prediction performance. Compared to the explainable baseline model, our method achieves more than 6 % higher accuracy in predicting both malignancy (93.0 %) and mean characteristic features of lung nodules. Simultaneously, the model provides case-based reasoning with prototype representations that allow visual validation of radiologist-defined attributes.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT interpretability 是医学影像领域中的一项重要需求。高级深度学习方法可以满足这一需求，但同时也需要解释性和高性能的模型。在这项工作中，我们investigate了是否可以在训练过程中使用额外信息来创建一个可理解的和强大的模型。我们提出了一种创新解决方案，即Proto-Caps，该解决方案利用了圆拱网络、原型学习和特权信息的使用。对于LIDC-IDRI数据集进行评估，我们发现该方法可以同时实现高度解释性和上状态艺术性的预测性能。相比于解释性基准模型，我们的方法在预测肿瘤性（93.0%）和肿瘤特征特征的平均值方面达到了6.0%以上的提升。此外，模型还提供了基于案例的理解，使得可以通过板准表示来视觉验证 ради医定义的特征。Note: The text is translated using the Google Translate API, which may not be perfect and may not capture all the nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Query-adaptive-DETR-for-Crowded-Pedestrian-Detection"><a href="#Query-adaptive-DETR-for-Crowded-Pedestrian-Detection" class="headerlink" title="Query-adaptive DETR for Crowded Pedestrian Detection"></a>Query-adaptive DETR for Crowded Pedestrian Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15725">http://arxiv.org/abs/2310.15725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Gao, Jiaxu Leng, Ji Gan, Xinbo Gao</li>
<li>for: 本研究旨在提高DETR和其变体在拥挤人群检测中的性能，特别是在不同程度的拥挤场景下自动调整DETRs的查询数量。</li>
<li>methods: 本文分析了两种当前的查询生成方法，并提出了四个指导原则 для设计适应查询生成方法。然后，我们提出了rank-based adaptive query generation（RAQG）方法，包括设计一个预测rank的rank prediction head，以及基于预测rank的adaptive选择方法来生成查询。此外，我们还提出了Soft Gradient L1 Loss来更好地训练rank prediction head。</li>
<li>results: 我们的方法可以具体地应用于任何DETRs，并在Crowdhuman dataset和Citypersons dataset上实现了竞争性的 Result。尤其是在Crowdhuman dataset上，我们的方法可以 дости得状态的最佳39.4% MR。<details>
<summary>Abstract</summary>
DEtection TRansformer (DETR) and its variants (DETRs) have been successfully applied to crowded pedestrian detection, which achieved promising performance. However, we find that, in different degrees of crowded scenes, the number of DETRs' queries must be adjusted manually, otherwise, the performance would degrade to varying degrees. In this paper, we first analyze the two current query generation methods and summarize four guidelines for designing the adaptive query generation method. Then, we propose Rank-based Adaptive Query Generation (RAQG) to alleviate the problem. Specifically, we design a rank prediction head that can predict the rank of the lowest confidence positive training sample produced by the encoder. Based on the predicted rank, we design an adaptive selection method that can adaptively select coarse detection results produced by the encoder to generate queries. Moreover, to train the rank prediction head better, we propose Soft Gradient L1 Loss. The gradient of Soft Gradient L1 Loss is continuous, which can describe the relationship between the loss value and the updated value of model parameters granularly. Our method is simple and effective, which can be plugged into any DETRs to make it query-adaptive in theory. The experimental results on Crowdhuman dataset and Citypersons dataset show that our method can adaptively generate queries for DETRs and achieve competitive results. Especially, our method achieves state-of-the-art 39.4% MR on Crowdhuman dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转化文本到简化中文。<</SYS>>DEtection TRansformer（DETR）和其变体（DETRs）已经成功应用于人群检测，实现了出色的表现。然而，我们发现在不同的人群场景中，DETRs的查询数量需要手动调整，否则表现会随之下降。在这篇论文中，我们首先分析了两种当前的查询生成方法，并总结出四个指导方针 для设计适应查询生成方法。然后，我们提议 Rank-based Adaptive Query Generation（RAQG）来解决这个问题。具体来说，我们设计了一个排名预测头，可以预测Encoder输出最低信心正例的排名。根据预测的排名，我们设计了一种适应选择方法，可以适应地选择Encoder输出的粗略检测结果来生成查询。此外，为了训练排名预测头更好，我们提议Soft Gradient L1 Loss。Soft Gradient L1 Loss的梯度是连续的，可以描述模型参数更新值与梯度值之间的关系细致。我们的方法简单而有效，可以将其插入到任何DETRs中，让它成为可query-adaptive的。实验结果表明，我们的方法可以适应地生成查询 для DETRs，并实现了竞争性的结果。特别是，我们的方法在Crowdhuman dataset上达到了39.4%的MR。
</details></li>
</ul>
<hr>
<h2 id="GNeSF-Generalizable-Neural-Semantic-Fields"><a href="#GNeSF-Generalizable-Neural-Semantic-Fields" class="headerlink" title="GNeSF: Generalizable Neural Semantic Fields"></a>GNeSF: Generalizable Neural Semantic Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15712">http://arxiv.org/abs/2310.15712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanlin Chen, Chen Li, Mengqi Guo, Zhiwen Yan, Gim Hee Lee</li>
<li>For: 本研究提出了一种基于神经 implicit representation的3D场景分割方法，以便只需在2D层次上进行训练，并且可以在推理时通过多视图图像特征和 semantic map 来适应不同场景。* Methods: 我们的方法首先从多视图图像特征和 semantic map 中提取特征，然后使用一种新的软投票机制来融合不同视图的2D semantic信息。此外，我们还编码了视图差信息，以便在推理时根据视图的距离对3D点的投票分数进行调整。最后，我们还设计了一个可见模块来检测和过滤出 occluded 视图中的不良信息。* Results: 我们的方法可以在不同场景下进行3D semantic segmentation，并且可以与场景特定的方法匹配性。此外，我们的方法还可以在只有2D semantic annotation 的情况下进行 Synthetic semantic map 的生成和3D semantic segmentation。实验结果表明，我们的方法可以与场景特定的方法匹配性，并且可以在只有2D annotation 的情况下进行3D semantic segmentation。更重要的是，我们的方法可以在有限的2D annotation 的情况下，超越现有的强supervision-based方法。我们的源代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/HLinChen/GNeSF%E3%80%82">https://github.com/HLinChen/GNeSF。</a><details>
<summary>Abstract</summary>
3D scene segmentation based on neural implicit representation has emerged recently with the advantage of training only on 2D supervision. However, existing approaches still requires expensive per-scene optimization that prohibits generalization to novel scenes during inference. To circumvent this problem, we introduce a generalizable 3D segmentation framework based on implicit representation. Specifically, our framework takes in multi-view image features and semantic maps as the inputs instead of only spatial information to avoid overfitting to scene-specific geometric and semantic information. We propose a novel soft voting mechanism to aggregate the 2D semantic information from different views for each 3D point. In addition to the image features, view difference information is also encoded in our framework to predict the voting scores. Intuitively, this allows the semantic information from nearby views to contribute more compared to distant ones. Furthermore, a visibility module is also designed to detect and filter out detrimental information from occluded views. Due to the generalizability of our proposed method, we can synthesize semantic maps or conduct 3D semantic segmentation for novel scenes with solely 2D semantic supervision. Experimental results show that our approach achieves comparable performance with scene-specific approaches. More importantly, our approach can even outperform existing strong supervision-based approaches with only 2D annotations. Our source code is available at: https://github.com/HLinChen/GNeSF.
</details>
<details>
<summary>摘要</summary>
三维场景分割基于神经隐式表示最近几年发展起来，具有训练只需2D监督的优势。然而，现有方法仍然需要贵重的每个场景优化，这限制了扩展到新场景的推理。为了解决这个问题，我们介绍了一个普适的三维分割框架基于隐式表示。Specifically，我们的框架接受多视图图像特征和 semantic maps作为输入，而不是只是空间信息，以避免过拟合特定场景的 Géometric 和 Semantic 信息。我们提出了一种新的软投票机制，用于对每个3D点的2D semantic信息进行集成。此外，我们还编码了视图差信息，以预测投票分数。直观地说，这使得邻近视图的semantic信息能够更大地贡献。此外，我们还设计了一个隐藏信息检测和过滤模块，以避免由 occluded 视图引入的损害信息。由于我们的提出的方法具有普适性，我们可以在具有 solely 2D semantic supervision 的情况下生成 semantic maps or 进行3D semantic segmentation for novel scenes。实验结果表明，我们的方法可以与场景特定方法匹配性能，而且甚至可以超越基于强监督的现有方法。我们的源代码可以在 GitHub 上找到：https://github.com/HLinChen/GNeSF。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-with-Power-Enhanced-Residual-Network-for-Interpolation-and-Inverse-Problems"><a href="#Physics-Informed-with-Power-Enhanced-Residual-Network-for-Interpolation-and-Inverse-Problems" class="headerlink" title="Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems"></a>Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15690">http://arxiv.org/abs/2310.15690</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cmmai/resnet_for_pinn">https://github.com/cmmai/resnet_for_pinn</a></li>
<li>paper_authors: Amir Noorizadegan, D. L. Young, Y. C. Hon, C. S. Chen</li>
<li>for: 提高神经网络的 interpolating 能力，包括平滑和非平滑函数的 interpolating。</li>
<li>methods: 提出了一种新的神经网络结构 Power-Enhancing residual network，通过添加力量项来提高网络的表达力。研究考虑了网络深度、宽度以及优化方法，并证明了该architecture的适应性和性能优势。</li>
<li>results: 研究表明，提出的Power-Enhancing residual network具有非常高的准确率，特别是对非平滑函数的 interpolating。实际应用中也证明了该网络的superiority，包括准确率、速度和效率。此外，研究还探讨了更深的网络的影响。最后，该architecture被应用于解决反射 Burgers 方程的问题，并达到了优秀的性能。<details>
<summary>Abstract</summary>
This paper introduces a novel neural network structure called the Power-Enhancing residual network, designed to improve interpolation capabilities for both smooth and non-smooth functions in 2D and 3D settings. By adding power terms to residual elements, the architecture boosts the network's expressive power. The study explores network depth, width, and optimization methods, showing the architecture's adaptability and performance advantages. Consistently, the results emphasize the exceptional accuracy of the proposed Power-Enhancing residual network, particularly for non-smooth functions. Real-world examples also confirm its superiority over plain neural network in terms of accuracy, convergence, and efficiency. The study also looks at the impact of deeper network. Moreover, the proposed architecture is also applied to solving the inverse Burgers' equation, demonstrating superior performance. In conclusion, the Power-Enhancing residual network offers a versatile solution that significantly enhances neural network capabilities. The codes implemented are available at: \url{https://github.com/CMMAi/ResNet_for_PINN}.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的神经网络结构，称为能量增强径 residual network，用于提高 interpolation 能力，包括平滑和非平滑函数的情况。通过添加能量项到径元中，这种建筑增强了网络的表达能力。研究探讨了网络的深度、宽度和优化方法，并证明了该建筑的适应性和性能优势。结果表明提案的 Power-Enhancing residual network 具有 Exceptional accuracy，特别是对非平滑函数。实际例子也证明了它在精度、速度和稳定性方面的优越性。此外，该建筑还应用于解决 inverse Burgers' equation，并达到了优秀的性能。 conclude，Power-Enhancing residual network 提供了一种多样化的解决方案，可以显著提高神经网络的能力。实现的代码可以在：\url{https://github.com/CMMAi/ResNet_for_PINN} 中找到。
</details></li>
</ul>
<hr>
<h2 id="Nighttime-Thermal-Infrared-Image-Colorization-with-Feedback-based-Object-Appearance-Learning"><a href="#Nighttime-Thermal-Infrared-Image-Colorization-with-Feedback-based-Object-Appearance-Learning" class="headerlink" title="Nighttime Thermal Infrared Image Colorization with Feedback-based Object Appearance Learning"></a>Nighttime Thermal Infrared Image Colorization with Feedback-based Object Appearance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15688">http://arxiv.org/abs/2310.15688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fuyaluo/foalgan">https://github.com/fuyaluo/foalgan</a></li>
<li>paper_authors: Fu-Ya Luo, Shu-Lin Liu, Yi-Jun Cao, Kai-Fu Yang, Chang-Yong Xie, Yong Liu, Yong-Jie Li</li>
<li>for: 提高夜间热动像（TIR）图像的可读性和可用性，以便在夜间场景中进行视觉识别和任务执行。</li>
<li>methods: 提出了一种基于生成对抗网络的feedback型物体外观学习（FoalGAN）方法，通过增加 occlusion-aware mixup 模块和对应的外观一致损失来提高小对象翻译性能。</li>
<li>results: 经验表明，提出的 FoalGAN 方法不仅可以提高小对象的外观学习，还能够在nighttime street scene中提高 semantic preservation和edge consistency。<details>
<summary>Abstract</summary>
Stable imaging in adverse environments (e.g., total darkness) makes thermal infrared (TIR) cameras a prevalent option for night scene perception. However, the low contrast and lack of chromaticity of TIR images are detrimental to human interpretation and subsequent deployment of RGB-based vision algorithms. Therefore, it makes sense to colorize the nighttime TIR images by translating them into the corresponding daytime color images (NTIR2DC). Despite the impressive progress made in the NTIR2DC task, how to improve the translation performance of small object classes is under-explored. To address this problem, we propose a generative adversarial network incorporating feedback-based object appearance learning (FoalGAN). Specifically, an occlusion-aware mixup module and corresponding appearance consistency loss are proposed to reduce the context dependence of object translation. As a representative example of small objects in nighttime street scenes, we illustrate how to enhance the realism of traffic light by designing a traffic light appearance loss. To further improve the appearance learning of small objects, we devise a dual feedback learning strategy to selectively adjust the learning frequency of different samples. In addition, we provide pixel-level annotation for a subset of the Brno dataset, which can facilitate the research of NTIR image understanding under multiple weather conditions. Extensive experiments illustrate that the proposed FoalGAN is not only effective for appearance learning of small objects, but also outperforms other image translation methods in terms of semantic preservation and edge consistency for the NTIR2DC task.
</details>
<details>
<summary>摘要</summary>
这个文章讨论了在不良环境（例如总黑暗）下稳定图像摄取的问题。因为热色干扰（TIR）相机在夜间场景认知中是一种普遍的选择，但是TIR图像的低 контра斯特和无彩色性使得人类阅读和后续的RGB基于视觉算法的应用受到阻碍。因此，将夜间TIR图像转换为相应的日间彩色图像（NTIR2DC）是一个有必要的步骤。尽管在NTIR2DC任务上已经做出了卓越的进步，但是如何提高小物类的译像性仍然是尚未探讨的问题。为了解决这个问题，我们提出了一个基于对应学习的生成 adversarial network（FoalGAN）。specifically，我们提出了一个遮瑕节module和相应的出现整合损失，以减少物类译像的上下文依赖。作为夜间街道场景中小物件的示例，我们详细说明了如何增强交通信号灯的现实感。此外，我们还提出了一个双重反馈学习策略，以选择性地调整不同的样本学习频率。此外，我们还提供了Brno dataset中一 subset的像素级注释，以便帮助夜间多种天气下NTIR图像理解的研究。实验结果显示，我们提出的FoalGAN不仅有效地进行小物类的出现学习，而且也在NTIR2DC任务中比其他图像转换方法具有更高的semantic preservation和edge consistency。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Vision-Centric-Multi-Modal-Expertise-for-3D-Object-Detection"><a href="#Leveraging-Vision-Centric-Multi-Modal-Expertise-for-3D-Object-Detection" class="headerlink" title="Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection"></a>Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15670">http://arxiv.org/abs/2310.15670</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opendrivelab/birds-eye-view-perception">https://github.com/opendrivelab/birds-eye-view-perception</a></li>
<li>paper_authors: Linyan Huang, Zhiqi Li, Chonghao Sima, Wenhai Wang, Jingdong Wang, Yu Qiao, Hongyang Li</li>
<li>for: 提高camera-only 3D对象检测器（学生）的准确率，通过从LiDAR-或多Modal-基于的对手（师）获得知识传递。</li>
<li>methods: 使用Uni-modal Distillation，即主要使用摄像头特征，并采用师模型与学生模型的同构结构，以减少特征差异。同时，引入了一种基于运动误差的细化 trajectory-based distillation模块，以进一步改进学生模型的性能。</li>
<li>results: 在nuscenes上，VCD-A实现了63.1% NDS的新州OF-the-art成绩，超过了其他多Modal或LiDAR-based模型。<details>
<summary>Abstract</summary>
Current research is primarily dedicated to advancing the accuracy of camera-only 3D object detectors (apprentice) through the knowledge transferred from LiDAR- or multi-modal-based counterparts (expert). However, the presence of the domain gap between LiDAR and camera features, coupled with the inherent incompatibility in temporal fusion, significantly hinders the effectiveness of distillation-based enhancements for apprentices. Motivated by the success of uni-modal distillation, an apprentice-friendly expert model would predominantly rely on camera features, while still achieving comparable performance to multi-modal models. To this end, we introduce VCD, a framework to improve the camera-only apprentice model, including an apprentice-friendly multi-modal expert and temporal-fusion-friendly distillation supervision. The multi-modal expert VCD-E adopts an identical structure as that of the camera-only apprentice in order to alleviate the feature disparity, and leverages LiDAR input as a depth prior to reconstruct the 3D scene, achieving the performance on par with other heterogeneous multi-modal experts. Additionally, a fine-grained trajectory-based distillation module is introduced with the purpose of individually rectifying the motion misalignment for each object in the scene. With those improvements, our camera-only apprentice VCD-A sets new state-of-the-art on nuScenes with a score of 63.1% NDS.
</details>
<details>
<summary>摘要</summary>
当前研究主要目标是提高Camera-only 3D对象检测器（学生）的准确率，通过从LiDAR-或多模态基础的对手（专家）传递知识。然而，域之间差距和时间融合不兼容性使得采用热静融合方法的改进效果受到限制。为了解决这个问题，我们提出了VCD框架，包括学生友好的多模态专家和时间融合友好的热静融合监督。多模态专家VCD-E采用与Camera-only学生相同的结构，以减轻特征差异，并利用LiDAR输入为深度估计来重建3D场景，实现与其他多模态专家相同的性能。此外，我们还引入了细腻的轨迹基于热静融合模块，以减轻每个对象在场景中的运动误差。通过这些改进，我们的Camera-only学生VCD-A在nuScenes上设置新的状态标准，分数为63.1% NDS。
</details></li>
</ul>
<hr>
<h2 id="Region-controlled-Style-Transfer"><a href="#Region-controlled-Style-Transfer" class="headerlink" title="Region-controlled Style Transfer"></a>Region-controlled Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15658">http://arxiv.org/abs/2310.15658</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kakinglow/Selective-Style-Transfer">https://github.com/kakinglow/Selective-Style-Transfer</a></li>
<li>paper_authors: Junjie Kang, Jinsong Wu, Shiqi Jiang</li>
<li>for: 提高图像风格传递的控制精度，使得图像风格更加自然和有趣。</li>
<li>methods: 提出了一种基于损失函数的启用方法，可以在不同区域中控制风格强度，并且引入了一种新的特征融合方法，可以将内容特征变换到风格特征的空间中，保持 semantic关系。</li>
<li>results: 经过广泛的实验 validate 了我们提出的方法的有效性。<details>
<summary>Abstract</summary>
Image style transfer is a challenging task in computational vision. Existing algorithms transfer the color and texture of style images by controlling the neural network's feature layers. However, they fail to control the strength of textures in different regions of the content image. To address this issue, we propose a training method that uses a loss function to constrain the style intensity in different regions. This method guides the transfer strength of style features in different regions based on the gradient relationship between style and content images. Additionally, we introduce a novel feature fusion method that linearly transforms content features to resemble style features while preserving their semantic relationships. Extensive experiments have demonstrated the effectiveness of our proposed approach.
</details>
<details>
<summary>摘要</summary>
Computational vision 中的图像风格传递是一项复杂的任务。现有的算法可以通过控制神经网络的特征层来传递风格图像的颜色和文字感，但它们无法控制不同区域的文字强度。为解决这个问题，我们提出了一种使用损失函数来约束不同区域的风格强度的训练方法。这种方法基于风格和内容图像的梯度关系来导引风格特征的传递强度在不同区域。此外，我们还介绍了一种新的特征融合方法，该方法将内容特征线性变换为风格特征，以保持它们的 semantic 关系。我们进行了广泛的实验，并证明了我们的提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Breaking-of-brightness-consistency-in-optical-flow-with-a-lightweight-CNN-network"><a href="#Breaking-of-brightness-consistency-in-optical-flow-with-a-lightweight-CNN-network" class="headerlink" title="Breaking of brightness consistency in optical flow with a lightweight CNN network"></a>Breaking of brightness consistency in optical flow with a lightweight CNN network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15655">http://arxiv.org/abs/2310.15655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linyicheng1/LET-NET">https://github.com/linyicheng1/LET-NET</a></li>
<li>paper_authors: Yicheng Lin, Shuo Wang, Yunlong Jiang, Bin Han</li>
<li>for: 提高高Dynamic Range（HDR）环境中光流算法的性能，适用于各种计算机视觉任务。</li>
<li>methods: 使用轻量级网络提取强协VARIABILITY的抽象特征和强角度的方向特征，并修改传统的光流方法的亮度一致性要求。</li>
<li>results: 提高了93%的翻译错误，并在公共HDR数据集上验证了方法的可靠性和稳定性。<details>
<summary>Abstract</summary>
Sparse optical flow is widely used in various computer vision tasks, however assuming brightness consistency limits its performance in High Dynamic Range (HDR) environments. In this work, a lightweight network is used to extract illumination robust convolutional features and corners with strong invariance. Modifying the typical brightness consistency of the optical flow method to the convolutional feature consistency yields the light-robust hybrid optical flow method. The proposed network runs at 190 FPS on a commercial CPU because it uses only four convolutional layers to extract feature maps and score maps simultaneously. Since the shallow network is difficult to train directly, a deep network is designed to compute the reliability map that helps it. An end-to-end unsupervised training mode is used for both networks. To validate the proposed method, we compare corner repeatability and matching performance with origin optical flow under dynamic illumination. In addition, a more accurate visual inertial system is constructed by replacing the optical flow method in VINS-Mono. In a public HDR dataset, it reduces translation errors by 93\%. The code is publicly available at https://github.com/linyicheng1/LET-NET.
</details>
<details>
<summary>摘要</summary>
广泛应用在计算机视觉任务中的稀肥光流尚未在高动态范围（HDR）环境中表现出色，因为它假设照明一致性。在这种工作中，我们使用轻量级网络提取抗照明强健的 convolutional 特征和角度特征。对于传统的光流方法的照明一致性来改变光流方法，得到了光敏感混合光流方法。提案的网络在商业CPU上运行于190帧/秒，因为它只使用了四个 convolutional 层来提取特征图和分数图同时。由于这个浅网络困难直接训练，我们设计了深度网络来计算可靠度地图。我们使用无监督的整体训练模式来训练两个网络。为验证我们的方法，我们比较了角 repeatability 和匹配性与原始光流方法在动态照明下。此外，我们将光流方法在 VINS-Mono 中换为我们的方法，从而构建了更加准确的视ер普系统。在一个公共 HDR 数据集上，它降低了翻译错误率93%。代码可以在 <https://github.com/linyicheng1/LET-NET> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Mean-Teacher-DETR-with-Masked-Feature-Alignment-A-Robust-Domain-Adaptive-Detection-Transformer-Framework"><a href="#Mean-Teacher-DETR-with-Masked-Feature-Alignment-A-Robust-Domain-Adaptive-Detection-Transformer-Framework" class="headerlink" title="Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework"></a>Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15646">http://arxiv.org/abs/2310.15646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weixi Weng, Chun Yuan<br>for:这 paper 的目的是提出一种基于 Mean Teacher 和 DETR 的 two-stage 无监督领域适应对象检测方法 (MTM)，以解决现有方法存在的性能波动和训练停滞问题。methods:这 paper 使用了两种 masked feature alignment 方法，包括 masked domain query-based feature alignment (MDQFA) 和 masked token-wise feature alignment (MTWFA)，以减轻领域偏移问题，并提高模型的目标性能。results: experiments 表明，MTM 可以在三个复杂的情况下提供更高的性能，而且理论分析表明，MTM 可以更好地减轻领域偏移问题。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation object detection(UDAOD) research on Detection Transformer(DETR) mainly focuses on feature alignment and existing methods can be divided into two kinds, each of which has its unresolved issues. One-stage feature alignment methods can easily lead to performance fluctuation and training stagnation. Two-stage feature alignment method based on mean teacher comprises a pretraining stage followed by a self-training stage, each facing problems in obtaining reliable pretrained model and achieving consistent performance gains. Methods mentioned above have not yet explore how to utilize the third related domain such as target-like domain to assist adaptation. To address these issues, we propose a two-stage framework named MTM, i.e. Mean Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we utilize labeled target-like images produced by image style transfer to avoid performance fluctuation. In the self-training stage, we leverage unlabeled target images by pseudo labels based on mean teacher and propose a module called Object Queries Knowledge Transfer(OQKT) to ensure consistent performance gains of the student model. Most importantly, we propose masked feature alignment methods including Masked Domain Query-based Feature Alignment(MDQFA) and Masked Token-wise Feature Alignment(MTWFA) to alleviate domain shift in a more robust way, which not only prevent training stagnation and lead to a robust pretrained model in the pretraining stage, but also enhance the model's target performance in the self-training stage. Experiments on three challenging scenarios and a theoretical analysis verify the effectiveness of MTM.
</details>
<details>
<summary>摘要</summary>
<<SYS>>本文主要研究无监督领域适应对检测变换器（DETR）的问题，具体来说是在特征对齐方面。现有的方法可以分为两类，各自带有不解决的问题。一类是一stage特征对齐方法，容易导致性能波动和训练停滞。另一类是基于mean teacher的两stage特征对齐方法，但每个阶段都面临着获得可靠预训练模型和实现一致性提升的问题。以上方法尚未考虑利用第三个相关领域，如目标类似领域，来支持适应。为了解决这些问题，我们提出了一个名为MTM（Mean Teacher-DETR with Masked Feature Alignment）的两stage框架。在预训练阶段，我们利用了标注目标类似图像生成的image style transfer来避免性能波动。在自我训练阶段，我们利用了无标签目标图像和pseudo标签基于mean teacher，并提出了一个名为Object Queries Knowledge Transfer（OQKT）的模块，以确保学生模型的一致性提升。最重要的是，我们提出了一些masked feature alignment方法，包括Masked Domain Query-based Feature Alignment（MDQFA）和Masked Token-wise Feature Alignment（MTWFA），以减轻领域偏移，不仅在预训练阶段避免训练停滞，还在自我训练阶段提高模型的目标性能。实验结果和理论分析证明了MTM的有效性。
</details></li>
</ul>
<hr>
<h2 id="GUPNet-Geometry-Uncertainty-Propagation-Network-for-Monocular-3D-Object-Detection"><a href="#GUPNet-Geometry-Uncertainty-Propagation-Network-for-Monocular-3D-Object-Detection" class="headerlink" title="GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection"></a>GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15624">http://arxiv.org/abs/2310.15624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/supermhp/gupnet">https://github.com/supermhp/gupnet</a></li>
<li>paper_authors: Yan Lu, Xinzhu Ma, Lei Yang, Tianzhu Zhang, Yating Liu, Qi Chu, Tong He, Yonghui Li, Wanli Ouyang<br>for:* image-based monocular 3D object detectionmethods:* using perspective projection to estimate object depth* modeling geometry projection in a probabilistic mannerresults:* state-of-the-art (SOTA) performance in image-based monocular 3D detection* superiority in efficacy with a simplified framework<details>
<summary>Abstract</summary>
Geometry plays a significant role in monocular 3D object detection. It can be used to estimate object depth by using the perspective projection between object's physical size and 2D projection in the image plane, which can introduce mathematical priors into deep models. However, this projection process also introduces error amplification, where the error of the estimated height is amplified and reflected into the projected depth. It leads to unreliable depth inferences and also impairs training stability. To tackle this problem, we propose a novel Geometry Uncertainty Propagation Network (GUPNet++) by modeling geometry projection in a probabilistic manner. This ensures depth predictions are well-bounded and associated with a reasonable uncertainty. The significance of introducing such geometric uncertainty is two-fold: (1). It models the uncertainty propagation relationship of the geometry projection during training, improving the stability and efficiency of the end-to-end model learning. (2). It can be derived to a highly reliable confidence to indicate the quality of the 3D detection result, enabling more reliable detection inference. Experiments show that the proposed approach not only obtains (state-of-the-art) SOTA performance in image-based monocular 3D detection but also demonstrates superiority in efficacy with a simplified framework.
</details>
<details>
<summary>摘要</summary>
To address this problem, we propose a novel Geometry Uncertainty Propagation Network (GUPNet++) that models the geometry projection in a probabilistic manner. This ensures that depth predictions are well-bounded and associated with a reasonable uncertainty. The significance of introducing such geometric uncertainty is twofold:1. It models the uncertainty propagation relationship of the geometry projection during training, improving the stability and efficiency of the end-to-end model learning.2. It can be derived to a highly reliable confidence to indicate the quality of the 3D detection result, enabling more reliable detection inference.Experiments show that the proposed approach not only achieves state-of-the-art performance in image-based monocular 3D detection but also demonstrates superiority in efficiency with a simplified framework.
</details></li>
</ul>
<hr>
<h2 id="Grasp-Multiple-Objects-with-One-Hand"><a href="#Grasp-Multiple-Objects-with-One-Hand" class="headerlink" title="Grasp Multiple Objects with One Hand"></a>Grasp Multiple Objects with One Hand</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15599">http://arxiv.org/abs/2310.15599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyang Li, Bo Liu, Yiran Geng, Puhao Li, Yaodong Yang, Yixin Zhu, Tengyu Liu, Siyuan Huang</li>
<li>for: 这篇论文的目的是解决机器人多物 grasping 问题，即同时抓持和操作多个物体，如对象转移和手部操作。</li>
<li>methods: 这篇论文提出了一种两阶段方法，名为 MultiGrasp，用于在桌面上使用多指灵活手柔腕抓取多个物体。该方法包括（i）生成 pré-grasp 提议和（ii）执行抓取和升起操作。</li>
<li>results: 实验主要关注 dual-object grasping，成功率为 44.13%，表明方法能够适应未看到的物体配置和不精准的抓取。方法还能够抓取更多的物体，尽管推理速度减少。<details>
<summary>Abstract</summary>
The human hand's complex kinematics allow for simultaneous grasping and manipulation of multiple objects, essential for tasks like object transfer and in-hand manipulation. Despite its importance, robotic multi-object grasping remains underexplored and presents challenges in kinematics, dynamics, and object configurations. This paper introduces MultiGrasp, a two-stage method for multi-object grasping on a tabletop with a multi-finger dexterous hand. It involves (i) generating pre-grasp proposals and (ii) executing the grasp and lifting the objects. Experimental results primarily focus on dual-object grasping and report a 44.13% success rate, showcasing adaptability to unseen object configurations and imprecise grasps. The framework also demonstrates the capability to grasp more than two objects, albeit at a reduced inference speed.
</details>
<details>
<summary>摘要</summary>
人类手部的复杂动态学允许同时抓取和操作多个物体，这是对象传递和手部执行中的重要功能。尽管其重要性，机器人多物体抓取仍然受欠发展和研究，包括动力学、物体配置和 grasping 等方面的挑战。本文介绍 MultiGrasp，一种基于两个阶段的多物体抓取方法，用于表面上的多指dexterous 手。其包括（i）生成 pré-grasp 建议和（ii）执行抓取和升起 Objects。实验结果主要关注 dual-object grasping，成功率为 44.13%，展示了适应不同物体配置和不精确的抓取。框架还证明可以抓取更多的物体，尽管速度较慢。
</details></li>
</ul>
<hr>
<h2 id="Facial-Data-Minimization-Shallow-Model-as-Your-Privacy-Filter"><a href="#Facial-Data-Minimization-Shallow-Model-as-Your-Privacy-Filter" class="headerlink" title="Facial Data Minimization: Shallow Model as Your Privacy Filter"></a>Facial Data Minimization: Shallow Model as Your Privacy Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15590">http://arxiv.org/abs/2310.15590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwen Pu, Jiahao Chen, Jiayu Pan, Hao li, Diqun Yan, Xuhong Zhang, Shouling Ji</li>
<li>for: 保护用户面部数据隐私</li>
<li>methods: 提出了一种数据隐私最小化变换方法（PMT），可以基于授权服务器的浅型模型进行处理，以获得干扰数据。此外，还提出了一种增强干扰方法来提高PMT的Robustness。</li>
<li>results: 经过广泛的实验测试，PMT方法能够有效地防止面部数据泄露和滥用，同时保持面recognition精度。<details>
<summary>Abstract</summary>
Face recognition service has been used in many fields and brings much convenience to people. However, once the user's facial data is transmitted to a service provider, the user will lose control of his/her private data. In recent years, there exist various security and privacy issues due to the leakage of facial data. Although many privacy-preserving methods have been proposed, they usually fail when they are not accessible to adversaries' strategies or auxiliary data. Hence, in this paper, by fully considering two cases of uploading facial images and facial features, which are very typical in face recognition service systems, we proposed a data privacy minimization transformation (PMT) method. This method can process the original facial data based on the shallow model of authorized services to obtain the obfuscated data. The obfuscated data can not only maintain satisfactory performance on authorized models and restrict the performance on other unauthorized models but also prevent original privacy data from leaking by AI methods and human visual theft. Additionally, since a service provider may execute preprocessing operations on the received data, we also propose an enhanced perturbation method to improve the robustness of PMT. Besides, to authorize one facial image to multiple service models simultaneously, a multiple restriction mechanism is proposed to improve the scalability of PMT. Finally, we conduct extensive experiments and evaluate the effectiveness of the proposed PMT in defending against face reconstruction, data abuse, and face attribute estimation attacks. These experimental results demonstrate that PMT performs well in preventing facial data abuse and privacy leakage while maintaining face recognition accuracy.
</details>
<details>
<summary>摘要</summary>
《面部识别服务中的数据隐私保护》面部识别服务在各个领域中得到广泛应用，为人们带来了很大的便利。然而，一旦用户的面部数据被提供给服务提供者，用户就会失去面部数据的控制权。随着面部数据泄露的问题的出现，有许多隐私和安全问题被提出。虽然许多隐私保护方法已经被提出，但它们通常因为不能抵御敌对策略或辅助数据的攻击而失效。因此，在这篇论文中，我们根据面部识别服务系统中的两种上传方式（上传面部图像和上传面部特征），提出了一种数据隐私减少转换（PMT）方法。这种方法可以基于授权服务的浅型模型处理原始面部数据，以获得干扰后的数据。这些干扰后的数据可以保持授权服务器模型的表现和限制未授权服务器模型的表现，同时防止原始隐私数据泄露。此外，服务提供者可能会对接收到的数据进行预处理操作，因此我们还提出了一种加强干扰方法以提高PMT的Robustness。此外，为了让一张面部图像同时授权多个服务模型，我们还提出了一种多重限制机制以提高PMT的扩展性。最后，我们进行了广泛的实验和评估，并证明了PMT在防止面部数据滥用和隐私泄露的同时保持面部识别精度的效iveness。
</details></li>
</ul>
<hr>
<h2 id="VMAF-Re-implementation-on-PyTorch-Some-Experimental-Results"><a href="#VMAF-Re-implementation-on-PyTorch-Some-Experimental-Results" class="headerlink" title="VMAF Re-implementation on PyTorch: Some Experimental Results"></a>VMAF Re-implementation on PyTorch: Some Experimental Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15578">http://arxiv.org/abs/2310.15578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kirill Aistov, Maxim Koroteev</li>
<li>for: 本研究提出了一种基于PyTorch框架的VMAF实现，并与标准（libvmaf）实现进行比较，显示两者之间的差异在VMAF单位下小于10^-2。</li>
<li>methods: 本研究使用了VMAF作为目标函数，并 investigate了在计算梯度时的问题。结果表明，通过使用VMAF作为目标函数进行训练，不会导致梯度计算出现问题。</li>
<li>results: 本研究的实验结果表明，使用PyTorch框架实现的VMAF和标准（libvmaf）实现之间的差异在VMAF单位下小于10^-2。<details>
<summary>Abstract</summary>
Based on the standard VMAF implementation we propose an implementation of VMAF using PyTorch framework. For this implementation comparisons with the standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We investigate gradients computation when using VMAF as an objective function and demonstrate that training using this function does not result in ill-behaving gradients.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese) Based on 标准 VMAF 实现，我们提议使用 PyTorch 框架来实现 VMAF。与标准 (libvmaf) 进行比较，我们发现 VMAF 单位下的差异在 $10^{-2}$ 以下。我们调查了使用 VMAF 作为目标函数时的梯度计算，并证明了在训练这个函数时不会出现不正确的梯度问题。
</details></li>
</ul>
<hr>
<h2 id="I-2-MD-3D-Action-Representation-Learning-with-Inter-and-Intra-modal-Mutual-Distillation"><a href="#I-2-MD-3D-Action-Representation-Learning-with-Inter-and-Intra-modal-Mutual-Distillation" class="headerlink" title="I$^2$MD: 3D Action Representation Learning with Inter- and Intra-modal Mutual Distillation"></a>I$^2$MD: 3D Action Representation Learning with Inter- and Intra-modal Mutual Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15568">http://arxiv.org/abs/2310.15568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunyao Mao, Jiajun Deng, Wengang Zhou, Zhenbo Lu, Wanli Ouyang, Houqiang Li</li>
<li>for: 这篇论文主要用于提出一种基于对比学习的自动生成3D人体动作表示学习方法，以解决现有的对比学习框架中不充分利用不同骨骼模式之间的资源。</li>
<li>methods: 该方法基于一种泛化对比学习框架，称为Inter-和Intra-模式共同静止（I$^2$MD）框架。在I$^2$MD中，我们首先将交叉模式交互改为一种交叉模式共同静止（CMD） proces。此外，我们还采用了一种内部模式共同静止策略（IMD），以解决相似样本之间的干扰和利用其下面的上下文。</li>
<li>results: 对三个数据集进行了广泛的实验，并取得了一系列新纪录。<details>
<summary>Abstract</summary>
Recent progresses on self-supervised 3D human action representation learning are largely attributed to contrastive learning. However, in conventional contrastive frameworks, the rich complementarity between different skeleton modalities remains under-explored. Moreover, optimized with distinguishing self-augmented samples, models struggle with numerous similar positive instances in the case of limited action categories. In this work, we tackle the aforementioned problems by introducing a general Inter- and Intra-modal Mutual Distillation (I$^2$MD) framework. In I$^2$MD, we first re-formulate the cross-modal interaction as a Cross-modal Mutual Distillation (CMD) process. Different from existing distillation solutions that transfer the knowledge of a pre-trained and fixed teacher to the student, in CMD, the knowledge is continuously updated and bidirectionally distilled between modalities during pre-training. To alleviate the interference of similar samples and exploit their underlying contexts, we further design the Intra-modal Mutual Distillation (IMD) strategy, In IMD, the Dynamic Neighbors Aggregation (DNA) mechanism is first introduced, where an additional cluster-level discrimination branch is instantiated in each modality. It adaptively aggregates highly-correlated neighboring features, forming local cluster-level contrasting. Mutual distillation is then performed between the two branches for cross-level knowledge exchange. Extensive experiments on three datasets show that our approach sets a series of new records.
</details>
<details>
<summary>摘要</summary>
最近的进展在自监视3D人体动作表示学习中很大程度上归功于对比学习。然而，在传统的对比框架中，不同骨骼模式之间的丰富补充关系尚未得到充分利用。此外，使用自我增强样本进行优化的模型在有限的动作类型下难以处理大量相似正例的问题。在这项工作中，我们解决了上述问题通过介入一种通用的Inter-和Intra-modal Mutual Distillation（I$^2$MD）框架。在I$^2$MD中，我们首先将交叉模式交互重新表述为交叉模式共同馈敷（CMD）过程。与现有的馈敷解决方案不同，在CMD中，知识不断更新和bidirectionally馈敷 между模式 during pre-training。为了解决相似样本之间的干扰和利用他们的基础上下文，我们进一步设计了Intra-modal Mutual Distillation（IMD）策略。在IMD中，我们首次引入了Dynamic Neighbors Aggregation（DNA）机制，其中每个模式中增加了一个额外的群集级别分支。它适应地聚合高度相关的邻近特征，形成局部群集级别对比。然后，我们进行了相互馈敷 между两个分支，以实现交叉知识交换。我们在三个 dataset 上进行了广泛的实验，结果显示，我们的方法创造了一系列新的纪录。
</details></li>
</ul>
<hr>
<h2 id="PET-Synthesis-via-Self-supervised-Adaptive-Residual-Estimation-Generative-Adversarial-Network"><a href="#PET-Synthesis-via-Self-supervised-Adaptive-Residual-Estimation-Generative-Adversarial-Network" class="headerlink" title="PET Synthesis via Self-supervised Adaptive Residual Estimation Generative Adversarial Network"></a>PET Synthesis via Self-supervised Adaptive Residual Estimation Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15550">http://arxiv.org/abs/2310.15550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Xue, Lei Bi, Yige Peng, Michael Fulham, David Dagan Feng, Jinman Kim</li>
<li>for: 降低对 positron emission tomography (PET) 的辐射暴露，同时维持高品质的 molecular imaging 图像。</li>
<li>methods: 使用 convolutional neural networks (CNNs) 生成来源低剂量 PET 图像，并使用自适应 residual estimation 生成 adversarial network (SS-AEGAN) 来解决 texture 和结构之间的差异问题，以及处理资料分布的迁移问题。</li>
<li>results: SS-AEGAN 在一个公共 benchmark 资料集上的实验中，与 state-of-the-art 合成方法相比，具有较高的效果，并且可以降低辐射暴露。<details>
<summary>Abstract</summary>
Positron emission tomography (PET) is a widely used, highly sensitive molecular imaging in clinical diagnosis. There is interest in reducing the radiation exposure from PET but also maintaining adequate image quality. Recent methods using convolutional neural networks (CNNs) to generate synthesized high-quality PET images from low-dose counterparts have been reported to be state-of-the-art for low-to-high image recovery methods. However, these methods are prone to exhibiting discrepancies in texture and structure between synthesized and real images. Furthermore, the distribution shift between low-dose PET and standard PET has not been fully investigated. To address these issues, we developed a self-supervised adaptive residual estimation generative adversarial network (SS-AEGAN). We introduce (1) An adaptive residual estimation mapping mechanism, AE-Net, designed to dynamically rectify the preliminary synthesized PET images by taking the residual map between the low-dose PET and synthesized output as the input, and (2) A self-supervised pre-training strategy to enhance the feature representation of the coarse generator. Our experiments with a public benchmark dataset of total-body PET images show that SS-AEGAN consistently outperformed the state-of-the-art synthesis methods with various dose reduction factors.
</details>
<details>
<summary>摘要</summary>
Positron emission tomography (PET) 是一种广泛使用、高度敏感的分子成像技术，在临床诊断中有广泛应用。然而，PET扫描产生的辐射暴露可能会导致影像质量下降。为了解决这问题，我们开发了一种自适应差分估计生成 adversarial neural network（SS-AEGAN）。我们提出了以下两点：1. 适应差分估计映射机制（AE-Net），用于在低剂量PET图像的基础上，动态修正预先生成的PET图像，通过将低剂量PET图像和生成输出之间的差分Map作为输入，以 rectify 预先生成的PET图像。2. 一种自动预训练策略，用于增强生成器的特征表示。我们的实验表明，SS-AEGAN在一个公共的整体PET图像benchmark dataset上，一直表现出优于当前的生成方法，并且可以处理不同的辐射减少因子。
</details></li>
</ul>
<hr>
<h2 id="Segue-Side-information-Guided-Generative-Unlearnable-Examples-for-Facial-Privacy-Protection-in-Real-World"><a href="#Segue-Side-information-Guided-Generative-Unlearnable-Examples-for-Facial-Privacy-Protection-in-Real-World" class="headerlink" title="Segue: Side-information Guided Generative Unlearnable Examples for Facial Privacy Protection in Real World"></a>Segue: Side-information Guided Generative Unlearnable Examples for Facial Privacy Protection in Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16061">http://arxiv.org/abs/2310.16061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiling Zhang, Jie Zhang, Kui Zhang, Wenbo Zhou, Weiming Zhang, Nenghai Yu</li>
<li>for: 实现隐私保护，防止人脸识别系统从数据中学习歧视特征。</li>
<li>methods: 使用“不可学习例子”的概念，在模型训练阶段添加不可见的干扰，以防止模型学习歧视特征。</li>
<li>results: 提出了一个名为Segue的新方法，可以快速生成可转移的不可学习例子，并且具有对抗JPEG压缩、敌方训练和一些标准的数据增强等特性。<details>
<summary>Abstract</summary>
The widespread use of face recognition technology has given rise to privacy concerns, as many individuals are worried about the collection and utilization of their facial data. To address these concerns, researchers are actively exploring the concept of ``unlearnable examples", by adding imperceptible perturbation to data in the model training stage, which aims to prevent the model from learning discriminate features of the target face. However, current methods are inefficient and cannot guarantee transferability and robustness at the same time, causing impracticality in the real world. To remedy it, we propose a novel method called Segue: Side-information guided generative unlearnable examples. Specifically, we leverage a once-trained multiple-used model to generate the desired perturbation rather than the time-consuming gradient-based method. To improve transferability, we introduce side information such as true labels and pseudo labels, which are inherently consistent across different scenarios. For robustness enhancement, a distortion layer is integrated into the training pipeline. Extensive experiments demonstrate that the proposed Segue is much faster than previous methods (1000$\times$) and achieves transferable effectiveness across different datasets and model architectures. Furthermore, it can resist JPEG compression, adversarial training, and some standard data augmentations.
</details>
<details>
<summary>摘要</summary>
广泛使用人脸识别技术已引起隐私问题，许多人担心模型会收集和利用他们的面部数据。为解决这些问题，研究人员正在积极探索“不可学习示例”的概念，通过在模型训练阶段添加不可见的干扰，以防止模型学习权倾向的面部特征。然而，当前的方法存在效率和可靠性问题，导致实际应用中存在不可避免的问题。为此，我们提出了一种新的方法：Segue：侧情况引导生成不可学习示例。具体来说，我们利用一个已经训练过的多用模型来生成所需的干扰，而不是时间消耗的梯度基本方法。为了提高传输性，我们引入了侧情况，如真实标签和假标签，这些情况是不同场景中的自然一致性。为了增强Robustness，我们在训练管道中添加了扭曲层。广泛的实验表明，我们提出的Segue比前一代方法（1000倍）更快速，并在不同的数据集和模型结构上实现了传输性和可靠性。此外，它还能抵抗JPEG压缩、反击训练和一些标准的数据扩展。
</details></li>
</ul>
<hr>
<h2 id="Learning-with-Noisy-Labels-Using-Collaborative-Sample-Selection-and-Contrastive-Semi-Supervised-Learning"><a href="#Learning-with-Noisy-Labels-Using-Collaborative-Sample-Selection-and-Contrastive-Semi-Supervised-Learning" class="headerlink" title="Learning with Noisy Labels Using Collaborative Sample Selection and Contrastive Semi-Supervised Learning"></a>Learning with Noisy Labels Using Collaborative Sample Selection and Contrastive Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15533">http://arxiv.org/abs/2310.15533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing Miao, Xiaohe Wu, Chao Xu, Yanli Ji, Wangmeng Zuo, Yiwen Guo, Zhaopeng Meng</li>
<li>for: 提高 Learning with Noisy Labels (LNL) 中的泛化性能。</li>
<li>methods: 提出了一种名为协同样本选择 (CSS) 的方法，利用大规模预训练模型 CLIP，从混合噪声样本中减少混合噪声样本。通过将 CLIP 的概率与 DNN 分类器的预测结果组合，使用 2D-GMM 训练。同时，通过对 CLIP 的招待语言进行练习，提高了 DNN 分类器的特征表示，提高了分类性能。</li>
<li>results: 在多个 benchmark 数据集上实验结果表明，提出的方法在比 estado-of-the-art 方法更高的泛化性能。<details>
<summary>Abstract</summary>
Learning with noisy labels (LNL) has been extensively studied, with existing approaches typically following a framework that alternates between clean sample selection and semi-supervised learning (SSL). However, this approach has a limitation: the clean set selected by the Deep Neural Network (DNN) classifier, trained through self-training, inevitably contains noisy samples. This mixture of clean and noisy samples leads to misguidance in DNN training during SSL, resulting in impaired generalization performance due to confirmation bias caused by error accumulation in sample selection. To address this issue, we propose a method called Collaborative Sample Selection (CSS), which leverages the large-scale pre-trained model CLIP. CSS aims to remove the mixed noisy samples from the identified clean set. We achieve this by training a 2-Dimensional Gaussian Mixture Model (2D-GMM) that combines the probabilities from CLIP with the predictions from the DNN classifier. To further enhance the adaptation of CLIP to LNL, we introduce a co-training mechanism with a contrastive loss in semi-supervised learning. This allows us to jointly train the prompt of CLIP and the DNN classifier, resulting in improved feature representation, boosted classification performance of DNNs, and reciprocal benefits to our Collaborative Sample Selection. By incorporating auxiliary information from CLIP and utilizing prompt fine-tuning, we effectively eliminate noisy samples from the clean set and mitigate confirmation bias during training. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed method in comparison with the state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
学习噪声标签（LNL）已经得到了广泛的研究，现有的方法通常采用一种框架，即 alternate between clean sample selection和半supervised learning（SSL）。然而，这种方法有一个限制：深度神经网络（DNN）分类器，通过自我训练，选择的干净集（clean set）一定会包含噪声样本。这种混合干净和噪声样本的组合会导致DNN训练在SSL阶段的偏导，从而导致随机误差的积累，从而降低分类性能。为解决这个问题，我们提出了一种方法called Collaborative Sample Selection（CSS），它利用了大规模预训练的模型CLIP。CSS的目标是从已知干净集中除掉混合的噪声样本。我们通过训练一个2维Gaussian Mixture Model（2D-GMM），将CLIP的概率与DNN分类器的预测结合起来，实现这一目标。为了进一步提高CLIP在LNL中的适应性，我们引入了一种contrastive loss的co-training机制，使得我们可以同时训练CLIP的提示和DNN分类器，从而提高分类性能。通过利用CLIP的auxiliary信息和提示细化，我们可以有效地从干净集中除掉噪声样本，避免随机误差的积累，并提高分类性能。我们在多个benchmark数据集上进行了实验，与状态 искусственныйints的方法进行比较，结果表明我们的提posed方法的效果。
</details></li>
</ul>
<hr>
<h2 id="NetDistiller-Empowering-Tiny-Deep-Learning-via-In-Situ-Distillation"><a href="#NetDistiller-Empowering-Tiny-Deep-Learning-via-In-Situ-Distillation" class="headerlink" title="NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation"></a>NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19820">http://arxiv.org/abs/2310.19820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shunyao Zhang, Yonggan Fu, Shang Wu, Jyotikrishna Dass, Haoran You, Yingyan, Lin</li>
<li>for: 提高 tiny neural network (TNN) 的任务准确率，以便在具有限制的 Edge 设备上部署 TNN。</li>
<li>methods: 提出了一种名为 NetDistiller 的框架，该框架通过将 TNN 视为一个权重共享的老师模型的子网络，并通过（1）梯度手术和（2）uncertainty-aware distillation来处理梯度冲突和老师模型过度适应。</li>
<li>results: 对多种任务进行了广泛的实验，证明 NetDistiller 可以有效地提高 TNN 的可达准确率，并且超过了现有方法的性能。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/GATECH-EIC/NetDistiller">https://github.com/GATECH-EIC/NetDistiller</a> 上下载。<details>
<summary>Abstract</summary>
Boosting the task accuracy of tiny neural networks (TNNs) has become a fundamental challenge for enabling the deployments of TNNs on edge devices which are constrained by strict limitations in terms of memory, computation, bandwidth, and power supply. To this end, we propose a framework called NetDistiller to boost the achievable accuracy of TNNs by treating them as sub-networks of a weight-sharing teacher constructed by expanding the number of channels of the TNN. Specifically, the target TNN model is jointly trained with the weight-sharing teacher model via (1) gradient surgery to tackle the gradient conflicts between them and (2) uncertainty-aware distillation to mitigate the overfitting of the teacher model. Extensive experiments across diverse tasks validate NetDistiller's effectiveness in boosting TNNs' achievable accuracy over state-of-the-art methods. Our code is available at https://github.com/GATECH-EIC/NetDistiller.
</details>
<details>
<summary>摘要</summary>
增强小神经网络（TNNs）的任务准确率已成为实现TNNs在边缘设备上的部署的基本挑战，这些设备受限于内存、计算、带宽和电池供应等方面的严格限制。为此，我们提出了名为NetDistiller的框架，用于提高TNNs的可达准确率。具体来说，我们将Target TNN模型和扩展annels的weight-sharing教师模型一起培训，使得它们可以共享参数。我们通过（1）梯度手术和（2）uncertainty-aware distillation来解决这两个模型之间的梯度冲突和教师模型过拟合。我们的实验证明，NetDistiller可以覆盖多个任务中的TNNs，并且与现有方法相比具有更高的效果。我们的代码可以在https://github.com/GATECH-EIC/NetDistiller中找到。
</details></li>
</ul>
<hr>
<h2 id="Cross-view-Self-localization-from-Synthesized-Scene-graphs"><a href="#Cross-view-Self-localization-from-Synthesized-Scene-graphs" class="headerlink" title="Cross-view Self-localization from Synthesized Scene-graphs"></a>Cross-view Self-localization from Synthesized Scene-graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15504">http://arxiv.org/abs/2310.15504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryogo Yamamoto, Kanji Tanaka</li>
<li>for: 本研究旨在解决跨视角自地标记问题，该问题中提供的数据库图像来自稀疏的视角。</li>
<li>methods: 本研究使用NeRF技术生成数据库图像，并使用图像纹理场学习抽象出视点不变的外观特征和视点相关的空间 semantic特征。然后，这两种特征 fusion到Scene Graph中，通过图 neural network压缩学习和识别。</li>
<li>results: 研究人员通过一个新的混合场景模型，将视点不变的外观特征和视点相关的空间 semantic特征 fusion到Scene Graph中，并使用图 neural network压缩学习和识别。实验结果表明，提案的方法可以在跨视角自地标记问题中提供高性能。<details>
<summary>Abstract</summary>
Cross-view self-localization is a challenging scenario of visual place recognition in which database images are provided from sparse viewpoints. Recently, an approach for synthesizing database images from unseen viewpoints using NeRF (Neural Radiance Fields) technology has emerged with impressive performance. However, synthesized images provided by these techniques are often of lower quality than the original images, and furthermore they significantly increase the storage cost of the database. In this study, we explore a new hybrid scene model that combines the advantages of view-invariant appearance features computed from raw images and view-dependent spatial-semantic features computed from synthesized images. These two types of features are then fused into scene graphs, and compressively learned and recognized by a graph neural network. The effectiveness of the proposed method was verified using a novel cross-view self-localization dataset with many unseen views generated using a photorealistic Habitat simulator.
</details>
<details>
<summary>摘要</summary>
cross-view自本地化是视觉地标识中的一种具有挑战性的场景，其中数据库图像来自稀见的视角。近些年，使用NeRF（神经辐射场）技术生成数据库图像从未看过的视角的方法出现了，表现出色。然而，这些技术生成的图像与原始图像质量相比较低，同时也增加了数据库存储成本。在这项研究中，我们探索了一种新的混合场景模型，它结合了raw图像中的视变不变特征和synthesized图像中的视依存空间semantic特征。这两种特征然后被融合到场景图中，并通过图 neural网络压缩学习和识别。研究的效果得到了一个新的cross-view自本地化数据集，使用了真实的Habitat模拟器生成的多个未看过的视角。
</details></li>
</ul>
<hr>
<h2 id="Salient-Object-Detection-in-RGB-D-Videos"><a href="#Salient-Object-Detection-in-RGB-D-Videos" class="headerlink" title="Salient Object Detection in RGB-D Videos"></a>Salient Object Detection in RGB-D Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15482">http://arxiv.org/abs/2310.15482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kerenfu/rdvs">https://github.com/kerenfu/rdvs</a></li>
<li>paper_authors: Ao Mou, Yukang Lu, Jiahao He, Dingyao Min, Keren Fu, Qijun Zhao</li>
<li>for: 这篇论文主要用于研究RGB-D视频中的鲜明 объек检测（SOD）技术，以提高视频中对象的检测精度。</li>
<li>methods: 该论文提出了一个新的三流网络模型（DCTNet+），其中RGB模式作为主要输入模式，并使用了depth和运动流作为auxiliary modalities。具有多modal注意模块（MAM）和修复融合模块（RFM）等两个模块，以便强化特征提升、修复和融合，以实现高精度的最终预测。</li>
<li>results: 对于17个视频SOD模型和14个RGB-D SOD模型，DCTNet+在pseudo RGB-D视频 dataset上的实验表明其在鲜明 объек检测任务中表现出优于其他模型。此外，对于真实的RGB-D视频 dataset，DCTNet+也表现出了优于其他模型。<details>
<summary>Abstract</summary>
Given the widespread adoption of depth-sensing acquisition devices, RGB-D videos and related data/media have gained considerable traction in various aspects of daily life. Consequently, conducting salient object detection (SOD) in RGB-D videos presents a highly promising and evolving avenue. Despite the potential of this area, SOD in RGB-D videos remains somewhat under-explored, with RGB-D SOD and video SOD (VSOD) traditionally studied in isolation. To explore this emerging field, this paper makes two primary contributions: the dataset and the model. On one front, we construct the RDVS dataset, a new RGB-D VSOD dataset with realistic depth and characterized by its diversity of scenes and rigorous frame-by-frame annotations. We validate the dataset through comprehensive attribute and object-oriented analyses, and provide training and testing splits. Moreover, we introduce DCTNet+, a three-stream network tailored for RGB-D VSOD, with an emphasis on RGB modality and treats depth and optical flow as auxiliary modalities. In pursuit of effective feature enhancement, refinement, and fusion for precise final prediction, we propose two modules: the multi-modal attention module (MAM) and the refinement fusion module (RFM). To enhance interaction and fusion within RFM, we design a universal interaction module (UIM) and then integrate holistic multi-modal attentive paths (HMAPs) for refining multi-modal low-level features before reaching RFMs. Comprehensive experiments, conducted on pseudo RGB-D video datasets alongside our RDVS, highlight the superiority of DCTNet+ over 17 VSOD models and 14 RGB-D SOD models. Ablation experiments were performed on both pseudo and realistic RGB-D video datasets to demonstrate the advantages of individual modules as well as the necessity of introducing realistic depth. Our code together with RDVS dataset will be available at https://github.com/kerenfu/RDVS/.
</details>
<details>
<summary>摘要</summary>
由于深度感知设备的广泛应用，RGB-D视频和相关数据/媒体在不同方面得到了广泛的应用。因此，在RGB-D视频中进行突出物体检测（SOD）是一个非常有前途的和发展中的领域。尽管这个领域的潜力很大，但RGB-D SOD和视频 SOD（VSOD）通常是隔离的，这个领域的探索仍然很有前途。为了探索这个新兴领域，本文做出了两个主要贡献： datasets 和模型。一方面，我们构建了 RDVS dataset，一个新的 RGB-D VSOD dataset，其特点是多样化的场景和精心注解每帧数据。我们通过了全面的特征和物体分析，并提供了训练和测试分割。此外，我们引入 DCTNet+，一种三流网络，其中RGB模式具有主导性，并将深度和光流视为助手模式。为了提高特征优化、融合和混合，我们提出了两个模块：多模式注意力模块（MAM）和修复融合模块（RFM）。为了增强 RFM 之间的互动和融合，我们设计了一个通用互动模块（UIM），并将整体多模式注意力路径（HMAPs）integrated into RFMs。对 pseudo RGB-D 视频 dataset 和我们的 RDVS 进行了广泛的实验，显示 DCTNet+ 在 17 个 VSOD 模型和 14 个 RGB-D SOD 模型中表现出色。我们还进行了对 pseudo 和真实 RGB-D 视频 dataset 的ablation 实验，以示各个模块的优势和引入真实深度的必要性。我们的代码以及 RDVS dataset 将在 GitHub 上提供，链接为：https://github.com/kerenfu/RDVS/.
</details></li>
</ul>
<hr>
<h2 id="DeepIron-Predicting-Unwarped-Garment-Texture-from-a-Single-Image"><a href="#DeepIron-Predicting-Unwarped-Garment-Texture-from-a-Single-Image" class="headerlink" title="DeepIron: Predicting Unwarped Garment Texture from a Single Image"></a>DeepIron: Predicting Unwarped Garment Texture from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15447">http://arxiv.org/abs/2310.15447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyun-Song Kwon, Sung-Hee Lee</li>
<li>for: 创建虚拟人物和虚拟试穿</li>
<li>methods: 使用纹理抽象 Framework，包括Texture Unwarper，将输入图像中的纹理抽象到3D裤子模型中</li>
<li>results: 生成高质量的纹理图像，可以在新的姿势下显示真实扭曲的3D裤子模型<details>
<summary>Abstract</summary>
Realistic reconstruction of 3D clothing from an image has wide applications, such as avatar creation and virtual try-on. This paper presents a novel framework that reconstructs the texture map for 3D garments from a single image with pose. Assuming that 3D garments are modeled by stitching 2D garment sewing patterns, our specific goal is to generate a texture image for the sewing patterns. A key component of our framework, the Texture Unwarper, infers the original texture image from the input clothing image, which exhibits warping and occlusion of texture due to the user's body shape and pose. The Texture Unwarper effectively transforms between the input and output images by mapping the latent spaces of the two images. By inferring the unwarped original texture of the input garment, our method helps reconstruct 3D garment models that can show high-quality texture images realistically deformed for new poses. We validate the effectiveness of our approach through a comparison with other methods and ablation studies.
</details>
<details>
<summary>摘要</summary>
现实重建三维服装图像从图像中的应用广泛，如创建化身和虚拟试穿。本文提出了一种新的框架，可以从单个图像中恢复三维服装的纹理地图。我们的具体目标是生成适合三维服装模型的纹理图像。我们的框架中的纹理恢复器（Texture Unwarper）可以从输入服装图像中推理出原始纹理图像，这个图像受用户的身体形态和姿势的扭曲和遮盖。纹理恢复器可以将输入和输出图像的 latent space 进行可靠地映射，从而将输入服装图像恢复成高质量的三维服装图像。我们验证了我们的方法的有效性通过与其他方法进行比较和简化研究。
</details></li>
</ul>
<hr>
<h2 id="Fast-Propagation-is-Better-Accelerating-Single-Step-Adversarial-Training-via-Sampling-Subnetworks"><a href="#Fast-Propagation-is-Better-Accelerating-Single-Step-Adversarial-Training-via-Sampling-Subnetworks" class="headerlink" title="Fast Propagation is Better: Accelerating Single-Step Adversarial Training via Sampling Subnetworks"></a>Fast Propagation is Better: Accelerating Single-Step Adversarial Training via Sampling Subnetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15444">http://arxiv.org/abs/2310.15444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojun Jia, Jianshu Li, Jindong Gu, Yang Bai, Xiaochun Cao</li>
<li>For: 提高模型的对抗性和训练效率* Methods: 使用单步攻击基于模型内部块的动态抽象网络，并提供了理论分析以证明模型对抗性的提高* Results: 比对前方法更好地降低训练成本，同时达到更高的模型对抗性水平<details>
<summary>Abstract</summary>
Adversarial training has shown promise in building robust models against adversarial examples. A major drawback of adversarial training is the computational overhead introduced by the generation of adversarial examples. To overcome this limitation, adversarial training based on single-step attacks has been explored. Previous work improves the single-step adversarial training from different perspectives, e.g., sample initialization, loss regularization, and training strategy. Almost all of them treat the underlying model as a black box. In this work, we propose to exploit the interior building blocks of the model to improve efficiency. Specifically, we propose to dynamically sample lightweight subnetworks as a surrogate model during training. By doing this, both the forward and backward passes can be accelerated for efficient adversarial training. Besides, we provide theoretical analysis to show the model robustness can be improved by the single-step adversarial training with sampled subnetworks. Furthermore, we propose a novel sampling strategy where the sampling varies from layer to layer and from iteration to iteration. Compared with previous methods, our method not only reduces the training cost but also achieves better model robustness. Evaluations on a series of popular datasets demonstrate the effectiveness of the proposed FB-Better. Our code has been released at https://github.com/jiaxiaojunQAQ/FP-Better.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过对抗训练来建立对抗示例的模型 robustness 已经显示出了承诺。对抗训练的一个主要缺点是生成对抗示例所带来的计算开销。为了解决这个限制，基于单步攻击的对抗训练已经得到了探索。先前的工作从不同的角度改进了单步对抗训练，例如样本初始化、损失规则化和训练策略。大多数情况下，这些方法都将模型视为黑盒子。在这种情况下，我们提议使用模型的内部建筑部件来提高效率。具体来说，我们提议在训练过程中动态选择轻量级子网络作为代理模型。这样，在前向和反向传播中，都可以加速对抗训练。此外，我们提供了理论分析，证明单步对抗训练可以通过采样轻量级子网络来提高模型的鲁棒性。此外，我们还提出了一种新的采样策略，其中采样的层次和迭代次数都会变化。相比之前的方法，我们的方法不仅减少了训练成本，还可以达到更好的模型鲁棒性。我们的代码已经在https://github.com/jiaxiaojunQAQ/FP-Better上发布。Note: The text has been translated using Google Translate, and some parts may not be perfectly accurate or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="G2-MonoDepth-A-General-Framework-of-Generalized-Depth-Inference-from-Monocular-RGB-X-Data"><a href="#G2-MonoDepth-A-General-Framework-of-Generalized-Depth-Inference-from-Monocular-RGB-X-Data" class="headerlink" title="G2-MonoDepth: A General Framework of Generalized Depth Inference from Monocular RGB+X Data"></a>G2-MonoDepth: A General Framework of Generalized Depth Inference from Monocular RGB+X Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15422">http://arxiv.org/abs/2310.15422</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wang-xjtu/g2-monodepth">https://github.com/wang-xjtu/g2-monodepth</a></li>
<li>paper_authors: Haotian Wang, Meng Yang, Nanning Zheng</li>
<li>for: This paper aims to solve the problem of monocular depth inference for robots, which is a fundamental problem for scene perception.</li>
<li>methods: The paper proposes a unified task of monocular depth inference, which uses a unified data representation, a novel unified loss, an improved network, and a data augmentation pipeline to well propagate diverse scene scales from input to output.</li>
<li>results: The paper demonstrates the effectiveness of its approach on three sub-tasks, including depth estimation, depth completion with different sparsity, and depth enhancement in unseen scenes, and outperforms state-of-the-art baselines on both real-world data and synthetic data.Here’s the Chinese version:</li>
<li>for: 本研究旨在解决机器人场景识别中的单目深度推断问题，是机器人Scene perception的基础问题。</li>
<li>methods: 本paper提出了一种统一任务的单目深度推断方法，使用统一的数据表示，一种新的统一损失函数，改进的网络和数据增强管道，以很好地传递不同场景比例的输入数据到输出场景中。</li>
<li>results: 本paper在三个子任务中，包括深度估计、深度缺失不同粒度和场景中的深度增强，都以状态体系内的基线性能为代表，并在实际数据和 sintetic数据上都超过了状态体系内的基线性能。<details>
<summary>Abstract</summary>
Monocular depth inference is a fundamental problem for scene perception of robots. Specific robots may be equipped with a camera plus an optional depth sensor of any type and located in various scenes of different scales, whereas recent advances derived multiple individual sub-tasks. It leads to additional burdens to fine-tune models for specific robots and thereby high-cost customization in large-scale industrialization. This paper investigates a unified task of monocular depth inference, which infers high-quality depth maps from all kinds of input raw data from various robots in unseen scenes. A basic benchmark G2-MonoDepth is developed for this task, which comprises four components: (a) a unified data representation RGB+X to accommodate RGB plus raw depth with diverse scene scale/semantics, depth sparsity ([0%, 100%]) and errors (holes/noises/blurs), (b) a novel unified loss to adapt to diverse depth sparsity/errors of input raw data and diverse scales of output scenes, (c) an improved network to well propagate diverse scene scales from input to output, and (d) a data augmentation pipeline to simulate all types of real artifacts in raw depth maps for training. G2-MonoDepth is applied in three sub-tasks including depth estimation, depth completion with different sparsity, and depth enhancement in unseen scenes, and it always outperforms SOTA baselines on both real-world data and synthetic data.
</details>
<details>
<summary>摘要</summary>
<<SYS>> simulationMonocular depth inference 是场景理解机器人的基本问题。特定机器人可能配备了一个摄像头 plus 任何类型的深度传感器，并处在不同的尺度和不同的场景中。而最近的进步导致了多个个人子任务的分化，从而导致了大规模工业化的高成本定制。这篇论文研究了一个统一的幌 depth inference 任务，可以从所有类型的输入原始数据中推算出高质量的深度图。为了实现这一目标，我们开发了一个基本的标准准则 G2-MonoDepth，它包括以下四个Component：（a）一种统一的数据表示RGB+X，可以容纳RGB plus Raw depth 的多种场景尺度/semantics、深度缺失（[0%, 100%））和错误（孔隙/噪声/模糊）。（b）一种新的统一损失，可以适应输入原始数据中的多种深度缺失/错误和输出场景中的多种尺度。（c）一种改进的网络，可以好地传播输入场景中的多种尺度到输出场景中。（d）一个数据增强管道，可以模拟所有类型的真实遗传 artifacts 在原始深度图中，用于训练。G2-MonoDepth 在三个子任务中应用，包括深度估算、深度完成不同缺失和深度增强在未看到场景中，并一直高于 SOTA 基eline 在实际数据和 sintetic 数据上。>>>
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/24/cs.CV_2023_10_24/" data-id="cloq1wl5y00kh7o8813115864" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/24/cs.AI_2023_10_24/" class="article-date">
  <time datetime="2023-10-24T12:00:00.000Z" itemprop="datePublished">2023-10-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/24/cs.AI_2023_10_24/">cs.AI - 2023-10-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Speakerly-A-Voice-based-Writing-Assistant-for-Text-Composition"><a href="#Speakerly-A-Voice-based-Writing-Assistant-for-Text-Composition" class="headerlink" title="Speakerly: A Voice-based Writing Assistant for Text Composition"></a>Speakerly: A Voice-based Writing Assistant for Text Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16251">http://arxiv.org/abs/2310.16251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Kumar, Vipul Raheja, Alice Kaiser-Schatzlein, Robyn Perry, Apurva Joshi, Justin Hugues-Nuger, Samuel Lou, Navid Chowdhury</li>
<li>for: 这篇论文是为了描述一个新的实时语音基于写作帮助系统，帮助用户在电子邮件、快递短信和笔记等不同场景下进行文本compose。</li>
<li>methods: 该系统使用小型、任务特定的模型，以及预训练的语言模型，实现快速和有效的文本compose，同时支持多种输入方式以提高使用性。</li>
<li>results: 该系统可以生成排版好的和协调的文档，并且可以在大规模部署中实现。<details>
<summary>Abstract</summary>
We present Speakerly, a new real-time voice-based writing assistance system that helps users with text composition across various use cases such as emails, instant messages, and notes. The user can interact with the system through instructions or dictation, and the system generates a well-formatted and coherent document. We describe the system architecture and detail how we address the various challenges while building and deploying such a system at scale. More specifically, our system uses a combination of small, task-specific models as well as pre-trained language models for fast and effective text composition while supporting a variety of input modes for better usability.
</details>
<details>
<summary>摘要</summary>
我们现在推出Speakerly，一个新的实时语音基于文本协助系统，帮助用户在不同的用例中（如电子邮件、即时消息和笔记）进行文本组成。用户可以通过指令或讲解与系统交互，系统会生成排版完善、流畅的文档。我们详细介绍系统架构，并解决了在建立和扩展这种系统的许多挑战。更specifically，我们的系统使用小型、任务特定的模型，以及预训练的语言模型，以快速和高效地进行文本组成，同时支持多种输入模式，以提高用户体验。
</details></li>
</ul>
<hr>
<h2 id="A-clustering-tool-for-interrogating-finite-element-models-based-on-eigenvectors-of-graph-adjacency"><a href="#A-clustering-tool-for-interrogating-finite-element-models-based-on-eigenvectors-of-graph-adjacency" class="headerlink" title="A clustering tool for interrogating finite element models based on eigenvectors of graph adjacency"></a>A clustering tool for interrogating finite element models based on eigenvectors of graph adjacency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16249">http://arxiv.org/abs/2310.16249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramaseshan Kannan</li>
<li>for: 用于 debug Finite Element（FE）模型中的错误</li>
<li>methods: 使用无监督学习算法，对FE模型中的度量自由度进行卷积 clustering，基于数值的邻域稠密矩阵的数学性质</li>
<li>results: 已经成功应用于实际世界FE模型 debug，并提供了使用示例<details>
<summary>Abstract</summary>
This note introduces an unsupervised learning algorithm to debug errors in finite element (FE) simulation models and details how it was productionised. The algorithm clusters degrees of freedom in the FE model using numerical properties of the adjacency of its stiffness matrix. The algorithm has been deployed as a tool called `Model Stability Analysis' tool within the commercial structural FE suite Oasys GSA (www.oasys-software.com/gsa). It has been used successfully by end-users for debugging real world FE models and we present examples of the tool in action.
</details>
<details>
<summary>摘要</summary>
这份笔记介绍了一种无监督学习算法，用于 finite element（FE）模拟器中错误的调试，并详细介绍了其生产化过程。该算法使用 numerical properties of the adjacency of its stiffness matrix to cluster degrees of freedom in the FE model。该工具被命名为“Model Stability Analysis”工具，并在商业structural FE集成环境Oasys GSA（www.oasys-software.com/gsa）中部署。它已经由用户成功地用于真实世界FE模型的调试，我们现在提供了这种工具在行动的示例。
</details></li>
</ul>
<hr>
<h2 id="Pixel-Level-Clustering-Network-for-Unsupervised-Image-Segmentation"><a href="#Pixel-Level-Clustering-Network-for-Unsupervised-Image-Segmentation" class="headerlink" title="Pixel-Level Clustering Network for Unsupervised Image Segmentation"></a>Pixel-Level Clustering Network for Unsupervised Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16234">http://arxiv.org/abs/2310.16234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cuong Manh Hoang, Byeongkeun Kang</li>
<li>for: 提高图像分割精度和效率，无需ground truth标注。</li>
<li>methods: 提出了一种基于像素嵌入、注意力机制、图像统计和超像素分割的无监督图像分割框架。</li>
<li>results: 实验结果表明，提出的方法在三个公开 dataset（Berkeley segmentation dataset、PASCAL VOC 2012 dataset和COCO-Stuff dataset）上表现出色，与前一代方法相比有所提高。<details>
<summary>Abstract</summary>
While image segmentation is crucial in various computer vision applications, such as autonomous driving, grasping, and robot navigation, annotating all objects at the pixel-level for training is nearly impossible. Therefore, the study of unsupervised image segmentation methods is essential. In this paper, we present a pixel-level clustering framework for segmenting images into regions without using ground truth annotations. The proposed framework includes feature embedding modules with an attention mechanism, a feature statistics computing module, image reconstruction, and superpixel segmentation to achieve accurate unsupervised segmentation. Additionally, we propose a training strategy that utilizes intra-consistency within each superpixel, inter-similarity/dissimilarity between neighboring superpixels, and structural similarity between images. To avoid potential over-segmentation caused by superpixel-based losses, we also propose a post-processing method. Furthermore, we present an extension of the proposed method for unsupervised semantic segmentation. We conducted experiments on three publicly available datasets (Berkeley segmentation dataset, PASCAL VOC 2012 dataset, and COCO-Stuff dataset) to demonstrate the effectiveness of the proposed framework. The experimental results show that the proposed framework outperforms previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
《Image Segmentation without Annotations: A Pixel-level Clustering Framework》Introduction:Image segmentation is a crucial technology in various computer vision applications, such as autonomous driving, grasping, and robot navigation. However, annotating all objects at the pixel-level for training is nearly impossible. Therefore, the study of unsupervised image segmentation methods is essential. In this paper, we propose a pixel-level clustering framework for segmenting images into regions without using ground truth annotations.Methodology:The proposed framework includes the following modules:1. Feature Embedding Modules with Attention Mechanism: These modules extract features from the input images and embed them into a lower-dimensional space using an attention mechanism.2. Feature Statistics Computing Module: This module computes the statistics of the embedded features to capture the distribution of the data.3. Image Reconstruction: This module reconstructs the input image from the embedded features to measure the similarity between the embedded features and the original image.4. Superpixel Segmentation: This module segments the input image into superpixels using the reconstructed image.Training Strategy:We propose a training strategy that utilizes intra-consistency within each superpixel, inter-similarity/dissimilarity between neighboring superpixels, and structural similarity between images. To avoid potential over-segmentation caused by superpixel-based losses, we also propose a post-processing method.Extension to Unsupervised Semantic Segmentation:We extend the proposed framework to perform unsupervised semantic segmentation by incorporating a semantic label predictor. We conduct experiments on three publicly available datasets (Berkeley segmentation dataset, PASCAL VOC 2012 dataset, and COCO-Stuff dataset) to demonstrate the effectiveness of the proposed framework. The experimental results show that the proposed framework outperforms previous state-of-the-art methods.In summary, the proposed pixel-level clustering framework provides an effective solution for unsupervised image segmentation without annotated data. The framework has broad applications in computer vision and can be further extended to other tasks such as object detection and semantic segmentation.
</details></li>
</ul>
<hr>
<h2 id="CleanCoNLL-A-Nearly-Noise-Free-Named-Entity-Recognition-Dataset"><a href="#CleanCoNLL-A-Nearly-Noise-Free-Named-Entity-Recognition-Dataset" class="headerlink" title="CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset"></a>CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16225">http://arxiv.org/abs/2310.16225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flairnlp/cleanconll">https://github.com/flairnlp/cleanconll</a></li>
<li>paper_authors: Susanna Rücker, Alan Akbik</li>
<li>for: 本文旨在提高CoNLL-03数据集的标注质量，以便对名实Recognition（NER）模型进行比较和分析。</li>
<li>methods: 本研究使用自动一致性检查和实体链接注意力，对英文CoNLL-03数据集进行了全面的重新标注。</li>
<li>results: 我们的实验发现，使用我们的数据集，现有的状态aset的模型可以达到97.1%的F1分数，而且错误率下降了从47%到6%。这表明我们的资源适用于分析现有模型的错误，并且证明了现有的模型尚未达到理论的最高性能边界。<details>
<summary>Abstract</summary>
The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches and analyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a comprehensive relabeling effort assisted by automatic consistency checking that corrects 7.0% of all labels in the English CoNLL-03. Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality. Our experimental evaluation finds not only that state-of-the-art approaches reach significantly higher F1-scores (97.1%) on our data, but crucially that the share of correct predictions falsely counted as errors due to annotation noise drops from 47% to 6%. This indicates that our resource is well suited to analyze the remaining errors made by state-of-the-art models, and that the theoretical upper bound even on high resource, coarse-grained NER is not yet reached. To facilitate such analysis, we make CleanCoNLL publicly available to the research community.
</details>
<details>
<summary>摘要</summary>
CoNLL-03 资料集是可能最具知名度和使用度的命名实体识别（NER）的标准benchmark dataset。然而，前一些研究发现CoNLL-03中存在大量的注释错误、不完整性和不一致性问题。这会带来对NERapproaches的比较和分析其错误的困难，因为当前的State-of-the-art模型在CoNLL-03中达到了F1分数，与或者超过了估计的注释噪声水平。为解决这个问题，我们提供了一项全面的重新标注努力，利用自动一致性检查来更正CoNLL-03英文版的7.0%的标签。我们的努力还添加了实体关联注释，以提高NER标签的解释性和作为额外的注释质量保障。我们的实验评估发现，不仅State-of-the-art方法在我们的数据上达到了97.1%的F1分数，而且关键的是，由于注释噪声而被误 counted为错误的分数从47%降至6%。这表示我们的资源适用于分析State-of-the-art模型的剩下错误，并且证明了高资源、粗粒度NER的理论最高 bound还没有被实现。为便于这种分析，我们将CleanCoNLL公开提供给研究社区。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Randomized-Smoothing"><a href="#Hierarchical-Randomized-Smoothing" class="headerlink" title="Hierarchical Randomized Smoothing"></a>Hierarchical Randomized Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16221">http://arxiv.org/abs/2310.16221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ManojKumarPatnaik/Major-project-list">https://github.com/ManojKumarPatnaik/Major-project-list</a></li>
<li>paper_authors: Yan Scholten, Jan Schuchardt, Aleksandar Bojchevski, Stephan Günnemann</li>
<li>for: 提高模型对输入数据小变化的抗干扰性和准确性。</li>
<li>methods: 使用层次随机熵法，在不同级别上随机添加噪声，以提高模型的抗干扰性和准确性。</li>
<li>results: 在图像和节点分类任务中，通过层次随机熵法获得了更好的抗干扰性-准确性质量平衡，比传统方法更高。<details>
<summary>Abstract</summary>
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. We experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. Overall, hierarchical smoothing is an important contribution towards models that are both - certifiably robust to perturbations and accurate.
</details>
<details>
<summary>摘要</summary>
实际世界数据是复杂的，经常可以划分为多个实体（例如图像可以分解为像素）。随机简化是一个强大的框架，使模型对小改变输入的证明抗坏性 - 通过确保多数票的抗坏性在随机添加噪音之前。然而，在复杂数据上证明抗坏性通过随机简化是困难的，因为敌人不会随机改变整个对象（例如图像），而是只会改变一部分其实体（例如像素）。为解决这个问题，我们引入层次随机简化：我们在随机选择对象的一部分实体上添加随机噪音。由于我们在添加噪音的方式更加精准，我们可以获得更强的抗坏性保证，同时保持高准确率。我们使用不同的噪音分布初始化层次简化，得到了新的抗坏性证明 certificates  для精度和连续域。我们在图像和节点分类中实际证明了层次简化的重要性，它在抗坏性-准确度质量上提供了优于现有方法的质量。总之，层次简化是一个重要的贡献，它使得模型可以同时拥有证明抗坏性和高准确率的能力。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Editing-for-Large-Language-Models-A-Survey"><a href="#Knowledge-Editing-for-Large-Language-Models-A-Survey" class="headerlink" title="Knowledge Editing for Large Language Models: A Survey"></a>Knowledge Editing for Large Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16218">http://arxiv.org/abs/2310.16218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, Jundong Li<br>for:This paper aims to provide a comprehensive and in-depth overview of recent advances in the field of Knowledge-based Model Editing (KME) for pre-trained language models (LLMs).methods:The paper uses a general formulation of KME to encompass different KME strategies, and introduces an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs.results:The paper provides an in-depth analysis of existing KME strategies, including their key insights, advantages, and limitations. It also introduces representative metrics, datasets, and applications of KME.<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在学术和工业领域的应用中已经导致了巨大的改变，主要是因为它们具有巨大的知识和推理能力，能够理解、分析和生成文本。然而，LLM的执行成本仍然是一个主要的障碍，尤其是当需要时常更新它们时。因此，发展有效和高效的LLM更新技术是非常重要的。传统的方法是通过直接精度调整 LLM 来将新知识给嵌入。然而，规范地再训练 LLM 可能是 computationally 的消耗性和对已有的知识的损害。因此， Knowledge-based Model Editing（KME）在最近引起了越来越多的关注，它目的是将特定知识给 LLM 中，而不影响其他无关知识。在这篇评论中，我们希望提供一个全面和深入的 KME 发展的总结。我们首先提出了 KME 的一般表述，然后提出了一个创新的 KME 分类法，根据新知识如何在预训 LLM 中整合。接着，我们分析了现有的 KME 策略，并评估了每个类别的关键见解、优点和局限性。此外，我们还介绍了 KME 的代表性 метри克、数据集和应用。最后，我们对 KME 的实用性和未来挑战进行了深入分析，并建议了可能的进一步发展方向。
</details></li>
</ul>
<hr>
<h2 id="Length-is-a-Curse-and-a-Blessing-for-Document-level-Semantics"><a href="#Length-is-a-Curse-and-a-Blessing-for-Document-level-Semantics" class="headerlink" title="Length is a Curse and a Blessing for Document-level Semantics"></a>Length is a Curse and a Blessing for Document-level Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16193">http://arxiv.org/abs/2310.16193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gowitheflow-1998/la-ser-cubed">https://github.com/gowitheflow-1998/la-ser-cubed</a></li>
<li>paper_authors: Chenghao Xiao, Yizhi Li, G Thomas Hudson, Chenghua Lin, Noura Al Moubayed</li>
<li>for: This paper aims to investigate the length generalizability of contrastive learning (CL) models and develop a new framework for learning semantically robust sentence representations that is not vulnerable to length-induced semantic shift.</li>
<li>methods: The authors use unsupervised CL methods that rely solely on the semantic signal provided by document length to devise a new framework for learning sentence representations.</li>
<li>results: The proposed framework, LA(SER)$^{3}$, achieves state-of-the-art unsupervised performance on the standard information retrieval benchmark, demonstrating the effectiveness of the length-agnostic self-reference approach in learning semantically robust sentence representations.<details>
<summary>Abstract</summary>
In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CL-based models, i.e., their vulnerability towards length-induced semantic shift. We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length. We first derive the theoretical foundations underlying length attacks, showing that elongating a document would intensify the high intra-document similarity that is already brought by CL. Moreover, we found that isotropy promised by CL is highly dependent on the length range of text exposed in training. Inspired by these findings, we introduce a simple yet universal document representation learning framework, LA(SER)$^{3}$: length-agnostic self-reference for semantically robust sentence representation learning, achieving state-of-the-art unsupervised performance on the standard information retrieval benchmark.
</details>
<details>
<summary>摘要</summary>
在最近几年，对比学习（CL）已经广泛应用于恢复先验语言模型中的句子和文档级编码能力。在这项工作中，我们质疑CL-based模型对长度的敏感性，即因文档长度而导致的 semantics shift。我们不仅证明了长度敏感性是一个被忽略的研究漏洞，而且我们可以靠文档长度的semantic signal来静默CL方法。我们首先 derive了对length attack的理论基础，表明了在CL中延长文档会增强高于文档内的同样性。此外，我们发现CL中的均匀性强度与文档训练中文本的长度范围有着高度相关性。 inspirited by这些发现，我们提出了一个简单 yet universally applicable的文档表示学习框架，即LA(SER)$^{3}$：不受文档长度影响的自referential学习方法，可以在标准信息检索benchmark上实现领先的无监督性能。
</details></li>
</ul>
<hr>
<h2 id="Correction-with-Backtracking-Reduces-Hallucination-in-Summarization"><a href="#Correction-with-Backtracking-Reduces-Hallucination-in-Summarization" class="headerlink" title="Correction with Backtracking Reduces Hallucination in Summarization"></a>Correction with Backtracking Reduces Hallucination in Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16176">http://arxiv.org/abs/2310.16176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenzhen Liu, Chao Wan, Varsha Kishore, Jin Peng Zhou, Minmin Chen, Kilian Q. Weinberger<br>for: 本研究旨在提高神经网络抽象摘要模型的可靠性，减少幻想（也称为假想）现象，以生成简洁而准确的摘要。methods: 本研究提出了一种简单 yet efficient的技术——CoBa，通过两个步骤来减少幻想：检测幻想和抑制幻想。检测幻想可以通过测量 conditional word probabilities 和距离上下文字的距离来实现。而抑制幻想可以使用直观的 backtracking 技术。results: 对于三个文本摘要 benchmark 数据集，我们进行了广泛的评估。结果表明，CoBa 能够有效地减少幻想现象，同时具有很好的适应性和灵活性。<details>
<summary>Abstract</summary>
Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hallucination, and offers great adaptability and flexibility.
</details>
<details>
<summary>摘要</summary>
摘要抄写目标在于生成自然语言摘要，以减少源文档中重要元素的抽象。尽管最近很多进步，神经网络抄写模型仍然容易受到幻觉（或更正确地说，杂谈）的影响，即生成摘要中的细节不基于源文档。在这篇论文中，我们介绍了一种简单 yet efficient的技术，CoBa，以减少抄写幻觉。该方法包括两个步骤：幻觉检测和缓解。我们表明，前者可以通过测量条件词概率和与上下文词的距离来实现。此外，我们示示了直观的回溯 surprisingly 有效于缓解。我们在三个文本摘要 benchmark 上进行了严格的评估，结果显示，CoBa 有效地减少了幻觉，并具有很好的适应性和灵活性。
</details></li>
</ul>
<hr>
<h2 id="Context-aware-feature-attribution-through-argumentation"><a href="#Context-aware-feature-attribution-through-argumentation" class="headerlink" title="Context-aware feature attribution through argumentation"></a>Context-aware feature attribution through argumentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16157">http://arxiv.org/abs/2310.16157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinfeng Zhong, Elsa Negre</li>
<li>for: 本研究旨在提高现有的特征归因方法的精度和可解性，并考虑用户的上下文，以提高预测结果的准确性。</li>
<li>methods: 本研究使用了一种新的特征归因框架，即Context-Aware Feature Attribution Through Argumentation（CA-FATA），它基于了论证来处理每个特征，并将特征归因视为一种论证过程。</li>
<li>results: 对比 existed 方法，CA-FATA 能够更高度地考虑用户的上下文，并提高特征归因的精度和可解性。<details>
<summary>Abstract</summary>
Feature attribution is a fundamental task in both machine learning and data analysis, which involves determining the contribution of individual features or variables to a model's output. This process helps identify the most important features for predicting an outcome. The history of feature attribution methods can be traced back to General Additive Models (GAMs), which extend linear regression models by incorporating non-linear relationships between dependent and independent variables. In recent years, gradient-based methods and surrogate models have been applied to unravel complex Artificial Intelligence (AI) systems, but these methods have limitations. GAMs tend to achieve lower accuracy, gradient-based methods can be difficult to interpret, and surrogate models often suffer from stability and fidelity issues. Furthermore, most existing methods do not consider users' contexts, which can significantly influence their preferences. To address these limitations and advance the current state-of-the-art, we define a novel feature attribution framework called Context-Aware Feature Attribution Through Argumentation (CA-FATA). Our framework harnesses the power of argumentation by treating each feature as an argument that can either support, attack or neutralize a prediction. Additionally, CA-FATA formulates feature attribution as an argumentation procedure, and each computation has explicit semantics, which makes it inherently interpretable. CA-FATA also easily integrates side information, such as users' contexts, resulting in more accurate predictions.
</details>
<details>
<summary>摘要</summary>
feature 归属是机器学习和数据分析中的基本任务，它的目的是确定每个特征或变量对模型输出的贡献。这个过程可以帮助 Identify the most important features for predicting an outcome. 在过去，feature 归属方法的历史可以追溯到通用加itive模型（GAMs），它们将线性回归模型扩展到包括非线性висиendent和独立变量之间的关系。在过去几年，梯度基本方法和代理模型已经应用于解读复杂的人工智能（AI）系统，但这些方法有限制。GAMs通常具有较低的准确率，梯度基本方法可能难以理解，而代理模型经常受稳定性和准确性的问题。此外，大多数现有方法不考虑用户的上下文，这可能会对用户的偏好产生重要影响。为了缓解这些限制并提高当前状态艺术，我们定义了一种新的特征归属框架，即Context-Aware Feature Attribution Through Argumentation（CA-FATA）。我们的框架利用了论证的力量，将每个特征视为一个论证，这些论证可以支持、攻击或中和预测。CA-FATA还将特征归属定义为论证过程，每个计算有显式 semantics，这使其自然地可读性高。CA-FATA还容易集成用户的上下文信息，导致更加准确的预测。
</details></li>
</ul>
<hr>
<h2 id="Yin-Yang-Convolutional-Nets-Image-Manifold-Extraction-by-the-Analysis-of-Opposites"><a href="#Yin-Yang-Convolutional-Nets-Image-Manifold-Extraction-by-the-Analysis-of-Opposites" class="headerlink" title="Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis of Opposites"></a>Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis of Opposites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16148">http://arxiv.org/abs/2310.16148</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nosaveddata/yinyang_cnn">https://github.com/nosaveddata/yinyang_cnn</a></li>
<li>paper_authors: Augusto Seben da Rosa, Frederico Santos de Oliveira, Anderson da Silva Soares, Arnaldo Candido Junior</li>
<li>for: 这项研究的目的是提出一种基于生物学革新的计算机视觉模型，以便更好地模仿大脑的运作。</li>
<li>methods: 该模型采用了一种称为“阴阳 convolutional network”的新型架构，该架构包括分离颜色和形状的分析块，以模拟 occipital lobe 的运作。</li>
<li>results: 研究结果表明，该模型在 CIFAR-10 数据集上达到了 State-of-the-Art 级别的效率，其中第一个模型达到了 93.32% 的测试精度，比之前的 SOTA 高出 0.8%，同时具有 150k  fewer parameters (726k 总参数). 第二个模型使用了 52k 参数，产生的测试精度下降了 3.86%。此外，我们还对 ImageNet 进行了分析，达到了 66.49% 的验证精度，使用了 1.6M 参数。代码已经公开在 GitHub 上：<a target="_blank" rel="noopener" href="https://github.com/NoSavedDATA/YinYang_CNN%E3%80%82">https://github.com/NoSavedDATA/YinYang_CNN。</a><details>
<summary>Abstract</summary>
Computer vision in general presented several advances such as training optimizations, new architectures (pure attention, efficient block, vision language models, generative models, among others). This have improved performance in several tasks such as classification, and others. However, the majority of these models focus on modifications that are taking distance from realistic neuroscientific approaches related to the brain. In this work, we adopt a more bio-inspired approach and present the Yin Yang Convolutional Network, an architecture that extracts visual manifold, its blocks are intended to separate analysis of colors and forms at its initial layers, simulating occipital lobe's operations. Our results shows that our architecture provides State-of-the-Art efficiency among low parameter architectures in the dataset CIFAR-10. Our first model reached 93.32\% test accuracy, 0.8\% more than the older SOTA in this category, while having 150k less parameters (726k in total). Our second model uses 52k parameters, losing only 3.86\% test accuracy. We also performed an analysis on ImageNet, where we reached 66.49\% validation accuracy with 1.6M parameters. We make the code publicly available at: https://github.com/NoSavedDATA/YinYang_CNN.
</details>
<details>
<summary>摘要</summary>
通用计算机视觉在总体上表现出了许多进步，如训练优化、新的架构（纯注意力、高效块、视觉语言模型、生成模型等）。这些进步使得在多个任务中表现得更好，如分类等。然而，大多数这些模型都强调与真实神经科学方法相关的距离减少。在这种情况下，我们采用了更加生物启发的方法，并提出了灵感来自于脑的Yin Yang卷积网络架构，该架构可以提取视觉拟合，其各层分析颜色和形状，类似于 occipital lobe 的运作。我们的结果表明，我们的架构在 CIFAR-10 数据集中提供了 State-of-the-Art 的效率，其中首个模型达到了 93.32% 的测试精度，较之前的 SOTA 高于 0.8%，同时具有 150k  fewer 参数（总共 726k）。我们的第二个模型使用了 52k 参数，产生的损失只有 3.86%。此外，我们还对 ImageNet 进行了分析，其中我们达到了 66.49% 的验证精度，使用了 1.6M 参数。我们将代码公开发布在 GitHub 上：https://github.com/NoSavedDATA/YinYang_CNN。
</details></li>
</ul>
<hr>
<h2 id="PreWoMe-Exploiting-Presuppositions-as-Working-Memory-for-Long-Form-Question-Answering"><a href="#PreWoMe-Exploiting-Presuppositions-as-Working-Memory-for-Long-Form-Question-Answering" class="headerlink" title="PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering"></a>PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16147">http://arxiv.org/abs/2310.16147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wookje Han, Jinsol Park, Kyungjae Lee</li>
<li>for: 这个论文是为了解决信息搜寻问题中的假设和假设推理问题。</li>
<li>methods: 该论文使用了提取问题中的假设，并将其作为工作记忆来生成反馈和行动，以解决问题。</li>
<li>results: 实验表明，该方法不仅可以处理假设问题，还可以处理正常的问题， thereby demonstrating the effectiveness of leveraging presuppositions, feedback, and action for real-world QA settings。<details>
<summary>Abstract</summary>
Information-seeking questions in long-form question answering (LFQA) often prove misleading due to ambiguity or false presupposition in the question. While many existing approaches handle misleading questions, they are tailored to limited questions, which are insufficient in a real-world setting with unpredictable input characteristics. In this work, we propose PreWoMe, a unified approach capable of handling any type of information-seeking question. The key idea of PreWoMe involves extracting presuppositions in the question and exploiting them as working memory to generate feedback and action about the question. Our experiment shows that PreWoMe is effective not only in tackling misleading questions but also in handling normal ones, thereby demonstrating the effectiveness of leveraging presuppositions, feedback, and action for real-world QA settings.
</details>
<details>
<summary>摘要</summary>
常见的信息搜寻问题在长形问答中经常导致误导，因为问题中含有模糊或虚假假设。虽然现有的方法可以处理误导问题，但它们只适用于有限的问题类型，这些类型在实际应用中是不可预测的。在这项工作中，我们提出了PreWoMe方法，可以处理任何类型的信息搜寻问题。PreWoMe方法的关键思想是提取问题中的假设，并将其作为工作记忆来生成反馈和行动。我们的实验表明，PreWoMe方法不仅能够处理误导问题，还能够处理正常的问题，这说明了在实际QA场景中，可以通过借鉴假设、反馈和行动来提高问答效果。
</details></li>
</ul>
<hr>
<h2 id="From-Heuristic-to-Analytic-Cognitively-Motivated-Strategies-for-Coherent-Physical-Commonsense-Reasoning"><a href="#From-Heuristic-to-Analytic-Cognitively-Motivated-Strategies-for-Coherent-Physical-Commonsense-Reasoning" class="headerlink" title="From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning"></a>From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18364">http://arxiv.org/abs/2310.18364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sled-group/heuristic-analytic-reasoning">https://github.com/sled-group/heuristic-analytic-reasoning</a></li>
<li>paper_authors: Zheyuan Zhang, Shane Storks, Fengyuan Hu, Sungryull Sohn, Moontae Lee, Honglak Lee, Joyce Chai</li>
<li>for: 本研究旨在提高语言模型（PLM）的合理性和可靠性，通过 incorporating 认知心理学中的快速和直观思维和慢速和探索性思维两种不同的过程。</li>
<li>methods: 我们在 PLM 的 fine-tuning 和 contextual learning 中采用了这两种不同的思维过程，并应用到了两种语言理解任务，需要 Physical Common Sense 的合理化。</li>
<li>results: 我们的方法可以很大程度地提高 PLM 的合理化结果，达到了 TRIP 任务的州OF-THE-ART  результаados，并发现这种改进的合理化是受到更加 faithful 的语言上下文注意力的直接结果。<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have shown impressive performance in various language tasks. However, they are prone to spurious correlations, and often generate illusory information. In real-world applications, PLMs should justify decisions with formalized, coherent reasoning chains, but this challenge remains under-explored. Cognitive psychology theorizes that humans are capable of utilizing fast and intuitive heuristic thinking to make decisions based on past experience, then rationalizing the decisions through slower and deliberative analytic reasoning. We incorporate these interlinked dual processes in fine-tuning and in-context learning with PLMs, applying them to two language understanding tasks that require coherent physical commonsense reasoning. We show that our proposed Heuristic-Analytic Reasoning (HAR) strategies drastically improve the coherence of rationalizations for model decisions, yielding state-of-the-art results on Tiered Reasoning for Intuitive Physics (TRIP). We also find that this improved coherence is a direct result of more faithful attention to relevant language context in each step of reasoning. Our findings suggest that human-like reasoning strategies can effectively improve the coherence and reliability of PLM reasoning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Clinfo-ai-An-Open-Source-Retrieval-Augmented-Large-Language-Model-System-for-Answering-Medical-Questions-using-Scientific-Literature"><a href="#Clinfo-ai-An-Open-Source-Retrieval-Augmented-Large-Language-Model-System-for-Answering-Medical-Questions-using-Scientific-Literature" class="headerlink" title="Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature"></a>Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16146">http://arxiv.org/abs/2310.16146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro Lozano, Scott L Fleming, Chia-Chun Chiang, Nigam Shah</li>
<li>For: This paper aims to provide an open-source WebApp called Clinfo.ai that answers clinical questions based on dynamically retrieved scientific literature, and to evaluate the performance of such retrieval-augmented language models (LLMs) using a specified information retrieval and abstractive summarization task.* Methods: The authors use a dataset of 200 questions and corresponding answers derived from published systematic reviews, named PubMed Retrieval and Synthesis (PubMedRS-200), to evaluate the performance of Clinfo.ai and other publicly available OpenQA systems.* Results: The authors report benchmark results for Clinfo.ai and other OpenQA systems on PubMedRS-200, demonstrating the effectiveness of their approach in answering clinical questions and summarizing relevant scientific literature.<details>
<summary>Abstract</summary>
The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner. While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking. Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools. We address these issues with four contributions: we release Clinfo.ai, an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.ai and other publicly available OpenQA systems on PubMedRS-200.
</details>
<details>
<summary>摘要</summary>
随着医学文献的快速扩展，临床医生和研究人员很难在时间上保持和总结最新、相关的发现。虽然现在有一些基于大语言模型（LLM）的关闭源摘要工具存在，但是对这些工具的评估还缺乏严格和系统的评估。此外，有限的高质量数据集和适当的benchmark任务，用于评估这些工具的性能也缺乏。我们通过以下四个贡献来解决这些问题：我们发布了一个开源的WebApp，名为Clinfo.ai，可以根据动态 retrieve scientific literature来回答临床问题；我们定义了一个信息检索和抽象摘要任务，用于评估这些基于检索加强的LLM系统的性能；我们发布了200个问题和相应的答案，这些答案来自已发布的系统性文献审查，我们称之为PubMed Retrieval and Synthesis（PubMedRS-200）；我们还对Clinfo.ai和其他公共可用的OpenQA系统在PubMedRS-200上的性能进行了标准化测试。
</details></li>
</ul>
<hr>
<h2 id="A-Language-Model-with-Limited-Memory-Capacity-Captures-Interference-in-Human-Sentence-Processing"><a href="#A-Language-Model-with-Limited-Memory-Capacity-Captures-Interference-in-Human-Sentence-Processing" class="headerlink" title="A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing"></a>A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16142">http://arxiv.org/abs/2310.16142</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Timkey, Tal Linzen</li>
<li>for: 本研究旨在开发一种基于循环神经网络的自然语言处理模型，以更好地模拟人类句子处理中的内存系统。</li>
<li>methods: 本研究使用了一个带有单个自我注意头的循环神经网络模型，以更好地模拟人类句子处理中的内存系统。</li>
<li>results: 研究发现，该模型的单个注意头可以捕捉人类句子处理中的 semantic和 sintactic干扰效应。<details>
<summary>Abstract</summary>
Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in specialized attention heads of GPT-2 are consistent with similarity-based interference, a key prediction of cue-based retrieval models, their method requires identifying syntactically specialized attention heads, and makes the cognitively implausible assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model's single attention head captures semantic and syntactic interference effects observed in human experiments.
</details>
<details>
<summary>摘要</summary>
两个中心因素被认为影响人类句子处理困难是预期和从工作内存中 retrieval。一项最近尝试创建一个统一的认知模型，旨在 integrating 这两个因素，基于 transformer 语言模型的自注意机制和人工内存中缺失 theories 的缘故（Ryu 和 Lewis 2021）。而 Ryu 和 Lewis 表明，特殊注意头中的注意力强度与相似性基本干扰相关，但 их方法需要标识语法特殊注意头，并假设 hundreds 个内存检索操作发生在平行的。在当前工作中，我们开发了一个 recurrent neural language model ，具有单一的自注意头，更加匹配人类认知理论中的内存系统。我们显示，我们模型的单一注意头能够捕捉 semantic 和语法干扰效应，观察到人类实验中。
</details></li>
</ul>
<hr>
<h2 id="Context-aware-explainable-recommendations-over-knowledge-graphs"><a href="#Context-aware-explainable-recommendations-over-knowledge-graphs" class="headerlink" title="Context-aware explainable recommendations over knowledge graphs"></a>Context-aware explainable recommendations over knowledge graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16141">http://arxiv.org/abs/2310.16141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinfeng Zhong, Elsa Negre</li>
<li>for: 用于模型用户在不同上下文中的偏好，并可以根据知识图中对物品的semantic关系进行适应 recombination。</li>
<li>methods: 使用Context-Aware Knowledge Graph Convolutional Network（CA-KGCN）框架，可以模型用户在不同上下文中的偏好，并可以根据知识图中对物品的semantic关系进行适应 recombination。</li>
<li>results: 在三个实际数据集上进行实验，证明了CA-KGCN框架的效果：可以模型用户在不同上下文中的偏好，并可以对 recombination 提供适应于context的解释。<details>
<summary>Abstract</summary>
Knowledge graphs contain rich semantic relationships related to items and incorporating such semantic relationships into recommender systems helps to explore the latent connections of items, thus improving the accuracy of prediction and enhancing the explainability of recommendations. However, such explainability is not adapted to users' contexts, which can significantly influence their preferences. In this work, we propose CA-KGCN (Context-Aware Knowledge Graph Convolutional Network), an end-to-end framework that can model users' preferences adapted to their contexts and can incorporate rich semantic relationships in the knowledge graph related to items. This framework captures users' attention to different factors: contexts and features of items. More specifically, the framework can model users' preferences adapted to their contexts and provide explanations adapted to the given context. Experiments on three real-world datasets show the effectiveness of our framework: modeling users' preferences adapted to their contexts and explaining the recommendations generated.
</details>
<details>
<summary>摘要</summary>
知识图中的semantic关系rich related to items, 将这些semantic关系integrated into recommender systems可以探索item的隐藏连接, thereby improving the accuracy of predictions and enhancing the explainability of recommendations. However, such explainability is not adapted to users' contexts, which can significantly influence their preferences. In this work, we propose CA-KGCN (Context-Aware Knowledge Graph Convolutional Network), an end-to-end framework that can model users' preferences adapted to their contexts and can incorporate rich semantic relationships in the knowledge graph related to items. This framework captures users' attention to different factors: contexts and features of items. More specifically, the framework can model users' preferences adapted to their contexts and provide explanations adapted to the given context. Experiments on three real-world datasets show the effectiveness of our framework: modeling users' preferences adapted to their contexts and explaining the recommendations generated.Here's the breakdown of the translation:* 知识图 (knowledge graph) becomes 知识图中 (in the knowledge graph)* semantic关系 (semantic relationships) becomes semantic关系rich (rich semantic relationships)* integrated becomes integrated into* 隐藏连接 (hidden connections) becomes 隐藏连接 (latent connections)* predictions becomes 预测 (predictions)* explainability becomes 解释 (explainability)* users' contexts becomes 用户的上下文 (users' contexts)*  CA-KGCN (Context-Aware Knowledge Graph Convolutional Network) becomes  Context-Aware Knowledge Graph Convolutional Network (CA-KGCN)*  end-to-end framework becomes 终端框架 (end-to-end framework)*  can model becomes 可以模型 (can model)*  users' preferences becomes 用户的偏好 (users' preferences)*  adapted to their contexts becomes 适应到他们的上下文 (adapted to their contexts)*  and provide explanations becomes 并提供解释 (and provide explanations)*  real-world datasets becomes 实际数据集 (real-world datasets)I hope this helps! Let me know if you have any further questions or if there's anything else I can help you with.
</details></li>
</ul>
<hr>
<h2 id="Alquist-5-0-Dialogue-Trees-Meet-Generative-Models-A-Novel-Approach-for-Enhancing-SocialBot-Conversations"><a href="#Alquist-5-0-Dialogue-Trees-Meet-Generative-Models-A-Novel-Approach-for-Enhancing-SocialBot-Conversations" class="headerlink" title="Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations"></a>Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16119">http://arxiv.org/abs/2310.16119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondřej Kobza, Jan Čuhel, Tommaso Gargiani, David Herel, Petr Marek</li>
<li>for: 这篇论文是为了描述作者开发的 SocialBot Alquist~5.0，以及该系统如何 integrating NRG Barista 和 multimodal devices。</li>
<li>methods: 该论文使用了多种创新approaches，包括NRG Barista 和 multimodal devices的支持，以提高对话体验。</li>
<li>results: 该论文提供了关于 Alquist~5.0 的开发，以及其能够满足用户对话需求的细节信息。<details>
<summary>Abstract</summary>
We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize SocialBot Grand Challenge~5. Building upon previous versions of our system, we introduce the NRG Barista and outline several innovative approaches for integrating Barista into our SocialBot, improving the overall conversational experience. Additionally, we extend our SocialBot to support multimodal devices. This paper offers insights into the development of Alquist~5.0, which meets evolving user expectations while maintaining empathetic and knowledgeable conversational abilities across diverse topics.
</details>
<details>
<summary>摘要</summary>
我们现在推出我们的社交机器人——Alquist~5.0——，这是为Alexa Prize SocialBot Grand Challenge~5所开发的。基于之前的版本，我们引入了NRG Barista，并提出了一些创新的方法来集成Barista到我们的社交机器人中，从而改善总体的对话体验。此外，我们扩展了我们的社交机器人以支持多模态设备。这篇论文为Alquist~5.0的开发提供了深入的启示，这种系统能够适应用户对话习惯的不断变化，同时保持对多种话题的对话能力和同理心。
</details></li>
</ul>
<hr>
<h2 id="Anatomically-aware-Uncertainty-for-Semi-supervised-Image-Segmentation"><a href="#Anatomically-aware-Uncertainty-for-Semi-supervised-Image-Segmentation" class="headerlink" title="Anatomically-aware Uncertainty for Semi-supervised Image Segmentation"></a>Anatomically-aware Uncertainty for Semi-supervised Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16099">http://arxiv.org/abs/2310.16099</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adigasu/anatomically-aware_uncertainty_for_semi-supervised_segmentation">https://github.com/adigasu/anatomically-aware_uncertainty_for_semi-supervised_segmentation</a></li>
<li>paper_authors: Sukesh Adiga V, Jose Dolz, Herve Lombaert</li>
<li>for: 这个论文的目的是提出一种新的图像分割不精确性估计方法，以便更好地指导图像分割网络。</li>
<li>methods: 这个论文使用了一种基于 segmentation masks 的 anatomically-aware 表示方法，通过将 prediction 映射到 anatomically-plausible segmentation，来估计图像分割不精确性。</li>
<li>results: 这个论文在两个公共可用的图像分割数据集上进行了测试，并与当前最佳 semi-supervised 方法进行了比较，得到了更高的分割精度。<details>
<summary>Abstract</summary>
Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. A prominent way to exploit unlabeled data is to regularize model predictions. Since the predictions of unlabeled data can be unreliable, uncertainty-aware schemes are typically employed to gradually learn from meaningful and reliable predictions. Uncertainty estimation methods, however, rely on multiple inferences from the model predictions that must be computed for each training step, which is computationally expensive. Moreover, these uncertainty maps capture pixel-wise disparities and do not consider global information. This work proposes a novel method to estimate segmentation uncertainty by leveraging global information from the segmentation masks. More precisely, an anatomically-aware representation is first learnt to model the available segmentation masks. The learnt representation thereupon maps the prediction of a new segmentation into an anatomically-plausible segmentation. The deviation from the plausible segmentation aids in estimating the underlying pixel-level uncertainty in order to further guide the segmentation network. The proposed method consequently estimates the uncertainty using a single inference from our representation, thereby reducing the total computation. We evaluate our method on two publicly available segmentation datasets of left atria in cardiac MRIs and of multiple organs in abdominal CTs. Our anatomically-aware method improves the segmentation accuracy over the state-of-the-art semi-supervised methods in terms of two commonly used evaluation metrics.
</details>
<details>
<summary>摘要</summary>
半指导学习降低了需要大量标注的像素级数据来进行图像分割的需求，通过利用无标注数据来推导模型预测。无标注数据的预测结果可能不可靠，因此通常采用不确定性感知方法来慢慢地学习有意义和可靠的预测结果。然而，不确定性估计方法通常需要对每个训练步骤进行多次模型预测，这是计算昂贵的。此外，这些不确定性地图只 capture pixel级差异，没有考虑全局信息。本工作提出了一种新的不确定性估计方法，通过利用分割masks中的全局信息来改进 segmentation uncertainty estimation。具体来说，首先学习了一种可靠的分割表示，以模型可用的分割masks来表示可能的分割结果。然后，通过将新的分割预测映射到该可靠的分割表示中，得到了一个具有 анатомиче可靠性的分割结果。与这个可靠的分割结果的差异可以用来估计图像分割的下一个像素级uncertainty，从而进一步引导分割网络。我们的方法只需要单个的模型预测，可以快速地计算不确定性，而不需要多次模型预测。我们对两个公共可用的分割数据集进行评估，即cardiac MRIs中的左大脏分割数据集和abdominal CTs中的多器官分割数据集。我们的半导学习方法在两个常用的评估指标上提高了分割精度，比前STATE-OF-THE-ART semi-supervised方法更高。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Data-as-Validation"><a href="#Synthetic-Data-as-Validation" class="headerlink" title="Synthetic Data as Validation"></a>Synthetic Data as Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16052">http://arxiv.org/abs/2310.16052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fiu-airlab/Next-Generation-Airline-Data-Exchange-Simulator">https://github.com/fiu-airlab/Next-Generation-Airline-Data-Exchange-Simulator</a></li>
<li>paper_authors: Qixin Hu, Alan Yuille, Zongwei Zhou</li>
<li>For: The paper is written to explore the use of synthetic data for early cancer detection in computed tomography (CT) volumes, with a focus on improving the robustness of AI models in identifying very tiny liver tumors.* Methods: The paper uses synthetic data to generate and superimpose tumors onto healthy organs in CT volumes, creating an extensive dataset for validation. The authors also propose a continual learning framework that continuously trains AI models on a stream of out-domain data with synthetic tumors.* Results: The paper shows that using synthetic data for validation can improve AI robustness in both in-domain and out-domain test sets. Specifically, the DSC score for liver tumor segmentation improves from 26.7% to 34.5% when evaluated on an in-domain dataset and from 31.1% to 35.4% on an out-domain dataset, with significant improvements in identifying very tiny liver tumors.<details>
<summary>Abstract</summary>
This study leverages synthetic data as a validation set to reduce overfitting and ease the selection of the best model in AI development. While synthetic data have been used for augmenting the training set, we find that synthetic data can also significantly diversify the validation set, offering marked advantages in domains like healthcare, where data are typically limited, sensitive, and from out-domain sources (i.e., hospitals). In this study, we illustrate the effectiveness of synthetic data for early cancer detection in computed tomography (CT) volumes, where synthetic tumors are generated and superimposed onto healthy organs, thereby creating an extensive dataset for rigorous validation. Using synthetic data as validation can improve AI robustness in both in-domain and out-domain test sets. Furthermore, we establish a new continual learning framework that continuously trains AI models on a stream of out-domain data with synthetic tumors. The AI model trained and validated in dynamically expanding synthetic data can consistently outperform models trained and validated exclusively on real-world data. Specifically, the DSC score for liver tumor segmentation improves from 26.7% (95% CI: 22.6%-30.9%) to 34.5% (30.8%-38.2%) when evaluated on an in-domain dataset and from 31.1% (26.0%-36.2%) to 35.4% (32.1%-38.7%) on an out-domain dataset. Importantly, the performance gain is particularly significant in identifying very tiny liver tumors (radius < 5mm) in CT volumes, with Sensitivity improving from 33.1% to 55.4% on an in-domain dataset and 33.9% to 52.3% on an out-domain dataset, justifying the efficacy in early detection of cancer. The application of synthetic data, from both training and validation perspectives, underlines a promising avenue to enhance AI robustness when dealing with data from varying domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AI-Alignment-and-Social-Choice-Fundamental-Limitations-and-Policy-Implications"><a href="#AI-Alignment-and-Social-Choice-Fundamental-Limitations-and-Policy-Implications" class="headerlink" title="AI Alignment and Social Choice: Fundamental Limitations and Policy Implications"></a>AI Alignment and Social Choice: Fundamental Limitations and Policy Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16048">http://arxiv.org/abs/2310.16048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhilash Mishra</li>
<li>for: 本研究旨在 investigating the challenges of building reinforcement learning with human feedback (RLHF) systems that respect democratic norms.</li>
<li>methods: 本paper使用社会选择理论的不可能性结果，以及研究RLHF系统如何对各个个人的价值观进行对齐。</li>
<li>results: 研究发现，使用RLHF系统对所有个人的价值观进行对齐是不可能的，而且需要对特定用户群进行对齐。 Here’s the full version of the three points in Traditional Chinese:</li>
<li>for: 本研究旨在探讨RLHF系统如何对民主 norms进行对齐。</li>
<li>methods: 本paper使用社会选择理论的不可能性结果，以及研究RLHF系统如何对各个个人的价值观进行对齐。</li>
<li>results: 研究发现，使用RLHF系统对所有个人的价值观进行对齐是不可能的，而且需要对特定用户群进行对齐。<details>
<summary>Abstract</summary>
Aligning AI agents to human intentions and values is a key bottleneck in building safe and deployable AI applications. But whose values should AI agents be aligned with? Reinforcement learning with human feedback (RLHF) has emerged as the key framework for AI alignment. RLHF uses feedback from human reinforcers to fine-tune outputs; all widely deployed large language models (LLMs) use RLHF to align their outputs to human values. It is critical to understand the limitations of RLHF and consider policy challenges arising from these limitations. In this paper, we investigate a specific challenge in building RLHF systems that respect democratic norms. Building on impossibility results in social choice theory, we show that, under fairly broad assumptions, there is no unique voting protocol to universally align AI systems using RLHF through democratic processes. Further, we show that aligning AI agents with the values of all individuals will always violate certain private ethical preferences of an individual user i.e., universal AI alignment using RLHF is impossible. We discuss policy implications for the governance of AI systems built using RLHF: first, the need for mandating transparent voting rules to hold model builders accountable. Second, the need for model builders to focus on developing AI agents that are narrowly aligned to specific user groups.
</details>
<details>
<summary>摘要</summary>
aligning AI agents with human intentions and values is a key bottleneck in building safe and deployable AI applications. But whose values should AI agents be aligned with? reinforcement learning with human feedback (RLHF) has emerged as the key framework for AI alignment. RLHF uses feedback from human reinforcers to fine-tune outputs; all widely deployed large language models (LLMs) use RLHF to align their outputs to human values. It is critical to understand the limitations of RLHF and consider policy challenges arising from these limitations. In this paper, we investigate a specific challenge in building RLHF systems that respect democratic norms. Building on impossibility results in social choice theory, we show that, under fairly broad assumptions, there is no unique voting protocol to universally align AI systems using RLHF through democratic processes. Further, we show that aligning AI agents with the values of all individuals will always violate certain private ethical preferences of an individual user i.e., universal AI alignment using RLHF is impossible. We discuss policy implications for the governance of AI systems built using RLHF: first, the need for mandating transparent voting rules to hold model builders accountable. Second, the need for model builders to focus on developing AI agents that are narrowly aligned to specific user groups.Here's the translation in Traditional Chinese:对于人工智能应用来说，与人类意图和价值观点相互整合是一个关键瓶颈。但是谁的价值应该与AI代理人相整合？人工增强学习with人类反馈（RLHF）已经成为AI整合的关键框架。RLHF通过人类反馈来细化输出，所有广泛部署的大型自然语言模型（LLMs）都使用RLHF来对人类价值进行整合。理解RLHF的限制和考虑由这些限制导致的政策挑战是非常重要。在这篇论文中，我们investigates a specific challenge in building RLHF systems that respect democratic norms。基于社会选择理论中的不可能结果，我们显示，在很广泛的假设下，通过民主过程使用RLHFUnique voting protocol是不可能的。此外，我们显示，通过RLHF来整合AI代理人与所有个人的价值相整合会违反某些个人私人的伦理偏好。我们讨论RLHF建设的政策影响：首先，需要制定透明的投票规则，以便举证模型建立者的责任。第二，模型建设者需要专注于开发特定用户群的狭频整合AI代理人。
</details></li>
</ul>
<hr>
<h2 id="Woodpecker-Hallucination-Correction-for-Multimodal-Large-Language-Models"><a href="#Woodpecker-Hallucination-Correction-for-Multimodal-Large-Language-Models" class="headerlink" title="Woodpecker: Hallucination Correction for Multimodal Large Language Models"></a>Woodpecker: Hallucination Correction for Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16045">http://arxiv.org/abs/2310.16045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bradyfu/woodpecker">https://github.com/bradyfu/woodpecker</a></li>
<li>paper_authors: Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen</li>
<li>for:  mitigate hallucinations in Multimodal Large Language Models (MLLMs)</li>
<li>methods:  training-free method named Woodpecker, consisting of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction</li>
<li>results: 30.66%&#x2F;24.33% improvement in accuracy over the baseline MiniGPT-4&#x2F;mPLUG-Owl on the POPE benchmark, and the source code is released at <a target="_blank" rel="noopener" href="https://github.com/BradyFU/Woodpecker">https://github.com/BradyFU/Woodpecker</a>.<details>
<summary>Abstract</summary>
Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.
</details>
<details>
<summary>摘要</summary>
《抖抖幻觉：一种免 instrucion 的方法》Introduction: Multimodal Large Language Models (MLLMs) 的发展受到幻觉的困扰，幻觉指的是生成文本与图像内容不一致。为了缓解幻觉，现有研究主要采用 instruction-tuning 方法，需要重新训练模型。在这篇论文中，我们开辟了一个不同的方向，提出了一种无需 instrucion 的方法—— Woodpecker。就像一只鹊鸟护理树木，Woodpecker 可以从生成文本中提取和修正幻觉。Method: Woodpecker 包括五个阶段：关键概念提取、问题构造、视觉知识验证、视觉声明生成和幻觉修正。在后续救恤方式下，Woodpecker 可以轻松地服务于不同的 MLLMs，同时可以通过访问不同阶段的输出来解释。我们对 Woodpecker 进行了量化和质量的评估，并显示了这新的思维方式的巨大潜力。在 POPE 测试准则下，我们的方法与基eline MiniGPT-4/mPLUG-Owl 的基eline之间具有30.66%/24.33%的提升。Code Release: Woodpecker 的源代码已经在 GitHub 上发布，可以通过 <https://github.com/BradyFU/Woodpecker> 访问。Note: * 幻觉（Hallucination）指的是生成文本与图像内容不一致的现象。* MLLMs 是多模态大型语言模型的缩写。*  instrucion-tuning 是指重新训练模型以适应特定数据的方法。
</details></li>
</ul>
<hr>
<h2 id="WebWISE-Web-Interface-Control-and-Sequential-Exploration-with-Large-Language-Models"><a href="#WebWISE-Web-Interface-Control-and-Sequential-Exploration-with-Large-Language-Models" class="headerlink" title="WebWISE: Web Interface Control and Sequential Exploration with Large Language Models"></a>WebWISE: Web Interface Control and Sequential Exploration with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16042">http://arxiv.org/abs/2310.16042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heyi Tao, Sethuraman T V, Michal Shlapentokh-Rothman, Derek Hoiem</li>
<li>for: 这篇论文是用一种大型自然语言模型（LLM）来自动执行网络软件任务的。</li>
<li>methods: 这种方法使用筛选的文档对象模型（DOM）元素作为观察，并按照步骤进行任务，逐步生成小程序基于当前观察。</li>
<li>results: 我们的WebWISE方法可以在MiniWob++ bencmark上达到类似或更好的性能，只需要一个在场示例。<details>
<summary>Abstract</summary>
The paper investigates using a Large Language Model (LLM) to automatically perform web software tasks using click, scroll, and text input operations. Previous approaches, such as reinforcement learning (RL) or imitation learning, are inefficient to train and task-specific. Our method uses filtered Document Object Model (DOM) elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations. We use in-context learning, either benefiting from a single manually provided example, or an automatically generated example based on a successful zero-shot trial. We evaluate the proposed method on the MiniWob++ benchmark. With only one in-context example, our WebWISE method achieves similar or better performance than other methods that require many demonstrations or trials.
</details>
<details>
<summary>摘要</summary>
文章研究使用大型自然语言模型（LLM）来自动完成网络软件任务，使用点击、滚动和文本输入操作。先前的方法，如强化学习（RL）或模仿学习，训练不efficient。我们的方法使用筛选后的文档对象模型（DOM）元素作为观察，逐步执行任务，基于当前观察sequentially generating small programs。我们使用在场学习，从单一提供的人工示例或自动生成的零例示例中受益。我们在MiniWob++测试准则上评估了我们的WebWISE方法。只需一个在场示例，我们的方法可以与其他需要多个示例或尝试的方法匹配或超越其表现。
</details></li>
</ul>
<hr>
<h2 id="Instruct-and-Extract-Instruction-Tuning-for-On-Demand-Information-Extraction"><a href="#Instruct-and-Extract-Instruction-Tuning-for-On-Demand-Information-Extraction" class="headerlink" title="Instruct and Extract: Instruction Tuning for On-Demand Information Extraction"></a>Instruct and Extract: Instruction Tuning for On-Demand Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16040">http://arxiv.org/abs/2310.16040</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yzjiao/on-demand-ie">https://github.com/yzjiao/on-demand-ie</a></li>
<li>paper_authors: Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Heng Ji, Jiawei Han</li>
<li>for: 本研究旨在提供一种基于大语言模型的个性化信息EXTRACTION系统，以满足非专家用户的长尾特殊EXTRACTION需求。</li>
<li>methods: 我们提出了一种新的 paradigm，称为“按需信息EXTRACTION”，以满足实际世界用户的个性化需求。我们的任务是根据用户的指令EXTRACTION相关文本中所需的内容，并将其提供在结构化表格格式中。</li>
<li>results: 我们在InstructIE benchmark上进行了广泛的评估，并显示了ODIE模型在相同大小模型中substantially outperform existing open-source models。<details>
<summary>Abstract</summary>
Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction - a classic task in natural language processing - most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users. To address this, we propose a novel paradigm, termed On-Demand Information Extraction, to fulfill the personalized demands of real-world users. Our task aims to follow the instructions to extract the desired content from the associated text and present it in a structured tabular format. The table headers can either be user-specified or inferred contextually by the model. To facilitate research in this emerging area, we present a benchmark named InstructIE, inclusive of both automatically generated training data, as well as the human-annotated test set. Building on InstructIE, we further develop an On-Demand Information Extractor, ODIE. Comprehensive evaluations on our benchmark reveal that ODIE substantially outperforms the existing open-source models of similar size. Our code and dataset are released on https://github.com/yzjiao/On-Demand-IE.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型具有 instrucion-following 能力，打开了更广泛的用户群体。然而，在信息提取任务中，大多数任务特定系统无法与长尾随机提取用 caso 对非专家用户进行好匹配。为解决这个问题，我们提出了一种新的思路，称为需求导向信息提取（On-Demand Information Extraction），以满足实际世界用户的个性化需求。我们的任务是根据用户提供的指令，从关联的文本中提取需要的内容，并将其格式化为结构化的表格形式。表头可以由用户指定，或者由模型Contextually inferred。为促进这个新兴领域的研究，我们提供了一个名为 InstructIE 的benchmark，其包括自动生成的training数据，以及人工注释的测试集。基于 InstructIE，我们进一步开发了一个需求导向信息提取器（ODIE）。我们的评估表明，ODIE在相同大小模型中显著超越了现有的开源模型。我们的代码和数据集在 <https://github.com/yzjiao/On-Demand-IE> 上发布。
</details></li>
</ul>
<hr>
<h2 id="What’s-Left-Concept-Grounding-with-Logic-Enhanced-Foundation-Models"><a href="#What’s-Left-Concept-Grounding-with-Logic-Enhanced-Foundation-Models" class="headerlink" title="What’s Left? Concept Grounding with Logic-Enhanced Foundation Models"></a>What’s Left? Concept Grounding with Logic-Enhanced Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16035">http://arxiv.org/abs/2310.16035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joyhsu0504/left">https://github.com/joyhsu0504/left</a></li>
<li>paper_authors: Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Jiajun Wu</li>
<li>for: 本研究旨在提出一种基于逻辑的跨领域基础模型（LEFT），可以智能地把概念考据到不同领域中。</li>
<li>methods: LEFT使用了大语言模型（LLM）和域特定固定模型（GTM），通过一种可微分的逻辑语言来执行程序。</li>
<li>results: LEFT在四个领域（2D图像、3D场景、人体动作和机器人抓取）中显示出了强大的理解能力，能够解决复杂的任务，包括训练时未看到的任务，并且可以轻松应用于新领域。<details>
<summary>Abstract</summary>
Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like "left" can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.
</details>
<details>
<summary>摘要</summary>
最近的工作，如VisProg和ViperGPT，已经开发了基于大语言模型（LLM）的视觉逻辑基础模型。然而，这些基础模型只能在有限的领域中运行，如2D图像，而不能充分利用语言的抽象概念，如“左”。这种推理只能在新领域中学习和适应已经训练过的模型。我们提出了逻辑增强基础模型（LEFT），一个统一框架，可以在不同领域和任务中学习和理解概念。LEFT的LLM интерпрета器输出一个基于普适逻辑语言的程序表示，这种语言在所有领域和任务中共享。然后，LEFT的执行器将使用可调整的领域特定的固定模块执行程序。我们示示LEFT在四个领域中灵活地学习概念，并且在各种复杂的任务中表现出强大的逻辑能力，包括一些在训练时未经看到的任务。此外，LEFT可以轻松应用于新的领域。
</details></li>
</ul>
<hr>
<h2 id="Finetuning-Offline-World-Models-in-the-Real-World"><a href="#Finetuning-Offline-World-Models-in-the-Real-World" class="headerlink" title="Finetuning Offline World Models in the Real World"></a>Finetuning Offline World Models in the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16029">http://arxiv.org/abs/2310.16029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fyhMer/fowm">https://github.com/fyhMer/fowm</a></li>
<li>paper_authors: Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, Xiaolong Wang</li>
<li>for: 这个研究想要将模型基础RL（world models）训练在真实机器上，并且将其应用于新任务中。</li>
<li>methods: 这个研究使用了禁止线RL（offline RL）框架，将RL策略训练在预存的数据集上，而不需要在线上互动。在训练过程中，我们将模型训练到在线上执行的构思中，以免于在新任务中发生扩展错误。</li>
<li>results: 我们的方法可以在许多类型的视动控制任务中实现几次训练即可以见和未见的任务，甚至在有限的预存数据集上进行训练。我们提供了许多类型的视动控制任务的评估结果，并且提供了相关的代码和数据。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) is notoriously data-inefficient, which makes training on a real robot difficult. While model-based RL algorithms (world models) improve data-efficiency to some extent, they still require hours or days of interaction to learn skills. Recently, offline RL has been proposed as a framework for training RL policies on pre-existing datasets without any online interaction. However, constraining an algorithm to a fixed dataset induces a state-action distribution shift between training and inference, and limits its applicability to new tasks. In this work, we seek to get the best of both worlds: we consider the problem of pretraining a world model with offline data collected on a real robot, and then finetuning the model on online data collected by planning with the learned model. To mitigate extrapolation errors during online interaction, we propose to regularize the planner at test-time by balancing estimated returns and (epistemic) model uncertainty. We evaluate our method on a variety of visuo-motor control tasks in simulation and on a real robot, and find that our method enables few-shot finetuning to seen and unseen tasks even when offline data is limited. Videos, code, and data are available at https://yunhaifeng.com/FOWM .
</details>
<details>
<summary>摘要</summary>
强化学习（RL）被认为是数据不充分的，这使得在真实机器人上进行训练变得困难。而使用世界模型（世界模型）的RL算法可以有一定程度的改善数据效率，但它们仍然需要数小时或数天的交互来学习技能。在最近，无线RL作为一个框架，它可以在已有数据集上训练RL策略而无需在线交互。然而，将算法约束在固定的数据集上会导致状态动作分布shift，限制其应用于新任务。在这个工作中，我们寻求获得两个世界的优点：我们考虑使用已有的世界模型在线上采集的数据进行预训练，然后在线上采集的数据上进行微调。为了减少在线交互时的推断误差，我们提议在测试时对计划进行规则化，将估计的返回和（知识）模型不确定性平衡。我们在视觉动作控制任务上进行了多种实验和在真实机器人上进行了实验，发现我们的方法可以在几个步骤内完成seen和unseen任务。视频、代码和数据可以在 <https://yunhaifeng.com/FOWM> 中获取。
</details></li>
</ul>
<hr>
<h2 id="What-Algorithms-can-Transformers-Learn-A-Study-in-Length-Generalization"><a href="#What-Algorithms-can-Transformers-Learn-A-Study-in-Length-Generalization" class="headerlink" title="What Algorithms can Transformers Learn? A Study in Length Generalization"></a>What Algorithms can Transformers Learn? A Study in Length Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16028">http://arxiv.org/abs/2310.16028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, Preetum Nakkiran</li>
<li>for: 这种研究旨在探讨Transformer模型在算法任务上的总能力范围，以及它们是否可以学习真正的算法来解决问题。</li>
<li>methods: 我们使用RASP（Weiss等，2021）编程语言，并提出了RASP总则：Transformer模型在任务上能够长期泛化，如果该任务可以通过一个短的RASP程序来解决，并且该程序适用于所有输入长度。</li>
<li>results: 我们发现，这个总则能够准确预测Transformer模型在算法任务上的泛化性能，并且我们可以根据这个总则提高传统难度的任务（如平衡和加法）的泛化性能。在理论方面，我们给出了一个简单的示例，表明Abbe等（2023）的”最小度 interpolator”模型不能正确预测Transformer模型的异常行为，而我们的总则则可以正确预测。总之，我们的工作为compositional generalization机制和Transformer模型的算法能力提供了新的视角。<details>
<summary>Abstract</summary>
Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021) -- a programming language designed for the computational model of a Transformer -- and introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically improve generalization performance on traditionally hard tasks (such as parity and addition). On the theoretical side, we give a simple example where the "min-degree-interpolator" model of learning from Abbe et al. (2023) does not correctly predict Transformers' out-of-distribution behavior, but our conjecture does. Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.
</details>
<details>
<summary>摘要</summary>
大型语言模型会表现出意想不到的总结普遍性特性，但同时也对许多简单的逻辑任务显示困难。这提出了Transformer模型是否和何时学习真实的算法来解决任务的问题。我们在特定的设置下研究Transformer模型的能力范围。我们利用Weiss等人（2021）提出的RASP编程语言——一种基于Transformer计算模型的编程语言——并提出了RASP总结假设：Transformer模型在任务上具有长度总结能力，如果任务可以通过一个短的RASP程序来解决，并且这个程序适用于所有输入长度。这个简单的假设奇怪地捕捉了大多数已知的长度总结任务。此外，我们利用我们的发现来大幅提高传统难度任务（如加法和平衡）的总结性能。在理论上，我们给出了一个简单的例子，证明Abbe等人（2023）的“最小度 interpolator”模型不能正确预测Transformer模型的外部数据行为，而我们的假设则能够正确预测。总之，我们的工作提供了一种新的视角来理解compositional总结和Transformer模型的算法能力。
</details></li>
</ul>
<hr>
<h2 id="Physically-Explainable-Deep-Learning-for-Convective-Initiation-Nowcasting-Using-GOES-16-Satellite-Observations"><a href="#Physically-Explainable-Deep-Learning-for-Convective-Initiation-Nowcasting-Using-GOES-16-Satellite-Observations" class="headerlink" title="Physically Explainable Deep Learning for Convective Initiation Nowcasting Using GOES-16 Satellite Observations"></a>Physically Explainable Deep Learning for Convective Initiation Nowcasting Using GOES-16 Satellite Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16015">http://arxiv.org/abs/2310.16015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Da Fan, Steven J. Greybush, David John Gagne II, Eugene E. Clothiaux</li>
<li>for: 预测气象预报模型和现有的nowcasting算法中的气象发展初始化（CI）问题仍然是一个挑战。</li>
<li>methods: 本研究使用物体基于概率深度学习模型预测CI，使用多核心infrared GOES-R卫星观测数据。</li>
<li>results: 深度学习模型在领先时间至1小时之间显著超越了经典逻辑模型，尤其是false alarm ratio。通过案例研究，深度学习模型表现了云和湿度特征在多个水平的依赖关系。模型解释结果显示模型决策过程中的不同基线的重要性。<details>
<summary>Abstract</summary>
Convection initiation (CI) nowcasting remains a challenging problem for both numerical weather prediction models and existing nowcasting algorithms. In this study, object-based probabilistic deep learning models are developed to predict CI based on multichannel infrared GOES-R satellite observations. The data come from patches surrounding potential CI events identified in Multi-Radar Multi-Sensor Doppler weather radar products over the Great Plains region from June and July 2020 and June 2021. An objective radar-based approach is used to identify these events. The deep learning models significantly outperform the classical logistic model at lead times up to 1 hour, especially on the false alarm ratio. Through case studies, the deep learning model exhibits the dependence on the characteristics of clouds and moisture at multiple levels. Model explanation further reveals the model's decision-making process with different baselines. The explanation results highlight the importance of moisture and cloud features at different levels depending on the choice of baseline. Our study demonstrates the advantage of using different baselines in further understanding model behavior and gaining scientific insights.
</details>
<details>
<summary>摘要</summary>
干湿物理学中的干湿物理学问题（CI）预测仍然是数值天气预测模型和现有预测算法的挑战。在本研究中，我们开发了基于多通道红外GOES-R卫星观测数据的对象概率深度学习模型，以预测CI。数据来自于2020年6月至7月和2021年6月在大陆地区的多普勒多感器气象雷达产品中拟合成的潜在CI事件。我们采用了对象雷达基本法来标识这些事件。深度学习模型在预测领域的领先时间（最长1小时）中显著超过了经典逻辑模型，特别是false alarm ratio。通过案例研究，深度学习模型表现出云和湿度特征的依赖关系，并且模型解释结果透视了不同基准下模型决策过程中的云和湿度特征的重要性。这些结果表明了不同基准下模型的行为和科学意义。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-imaging-inverse-problem-with-SA-Roundtrip-prior-via-HMC-pCN-sampler"><a href="#Bayesian-imaging-inverse-problem-with-SA-Roundtrip-prior-via-HMC-pCN-sampler" class="headerlink" title="Bayesian imaging inverse problem with SA-Roundtrip prior via HMC-pCN sampler"></a>Bayesian imaging inverse problem with SA-Roundtrip prior via HMC-pCN sampler</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17817">http://arxiv.org/abs/2310.17817</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qjy415417122/sa-roundtrip">https://github.com/qjy415417122/sa-roundtrip</a></li>
<li>paper_authors: Jiayu Qian, Yuanyuan Liu, Jingya Yang, Qingping Zhou</li>
<li>for: 用于解决科学和工程领域的图像反向问题</li>
<li>methods: 使用深度生成先验学习积分分布，并在描述数据的自注意结构内嵌入探索机制</li>
<li>results: 比较现有方法表现出色，在CT重建和MNIST数据集上实现了稳定和准确的点估计，同时提供了精确的不确定性评估<details>
<summary>Abstract</summary>
Bayesian inference with deep generative prior has received considerable interest for solving imaging inverse problems in many scientific and engineering fields. The selection of the prior distribution is learned from, and therefore an important representation learning of, available prior measurements. The SA-Roundtrip, a novel deep generative prior, is introduced to enable controlled sampling generation and identify the data's intrinsic dimension. This prior incorporates a self-attention structure within a bidirectional generative adversarial network. Subsequently, Bayesian inference is applied to the posterior distribution in the low-dimensional latent space using the Hamiltonian Monte Carlo with preconditioned Crank-Nicolson (HMC-pCN) algorithm, which is proven to be ergodic under specific conditions. Experiments conducted on computed tomography (CT) reconstruction with the MNIST and TomoPhantom datasets reveal that the proposed method outperforms state-of-the-art comparisons, consistently yielding a robust and superior point estimator along with precise uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
bayesian 推理 WITH deep 生成假设 已经受到了许多科学和工程领域的广泛关注，用于解决几何逆问题。假设分布的选择是从可用的假设测量学习的，因此是很重要的假设学习。SA-Roundtrip，一种新的深度生成假设，是用于实现控制的抽样生成和识别数据的自然维度。这个假设包含了自我注意结构在 bidirectional 生成对抗网络中。接下来，bayesian 推理是应用到在低维度的潜在空间中的 posterior 分布上，使用 Hamilton Monte Carlo with preconditioned Crank-Nicolson (HMC-pCN) 算法，这是在特定条件下证明为ergodic。实验在 computed tomography (CT) 重建中使用 MNIST 和 TomoPhantom 数据集，显示了提案的方法在比较之上表现出Robust和superior的点估计，同时精确地评估不确定性。
</details></li>
</ul>
<hr>
<h2 id="Human-in-the-Loop-Task-and-Motion-Planning-for-Imitation-Learning"><a href="#Human-in-the-Loop-Task-and-Motion-Planning-for-Imitation-Learning" class="headerlink" title="Human-in-the-Loop Task and Motion Planning for Imitation Learning"></a>Human-in-the-Loop Task and Motion Planning for Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16014">http://arxiv.org/abs/2310.16014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Mandlekar, Caelan Garrett, Danfei Xu, Dieter Fox</li>
<li>for: 本研究旨在开发一种名为人类在循环控制（HITL）任务和运动规划（TAMP）系统，用于教育机器人执行复杂的搏动任务。</li>
<li>methods: 该系统使用了TAMP-gat Control机制，允许人类操作员在机器人 Fleet 中进行数据采集，并将采集的人类数据与仿冒学框架结合，以训练TAMP-gat 策略。</li>
<li>results: 与传统电动操作系统相比，HITL-TAMP 可以更高效地采集数据，并且可以从非专家电动操作数据中训练高效的机器人。研究中收集了2.1K demos，并在12种接触度高、时间长的任务中实现了近乎完美的机器人。<details>
<summary>Abstract</summary>
Imitation learning from human demonstrations can teach robots complex manipulation skills, but is time-consuming and labor intensive. In contrast, Task and Motion Planning (TAMP) systems are automated and excel at solving long-horizon tasks, but they are difficult to apply to contact-rich tasks. In this paper, we present Human-in-the-Loop Task and Motion Planning (HITL-TAMP), a novel system that leverages the benefits of both approaches. The system employs a TAMP-gated control mechanism, which selectively gives and takes control to and from a human teleoperator. This enables the human teleoperator to manage a fleet of robots, maximizing data collection efficiency. The collected human data is then combined with an imitation learning framework to train a TAMP-gated policy, leading to superior performance compared to training on full task demonstrations. We compared HITL-TAMP to a conventional teleoperation system -- users gathered more than 3x the number of demos given the same time budget. Furthermore, proficient agents (75\%+ success) could be trained from just 10 minutes of non-expert teleoperation data. Finally, we collected 2.1K demos with HITL-TAMP across 12 contact-rich, long-horizon tasks and show that the system often produces near-perfect agents. Videos and additional results at https://hitltamp.github.io .
</details>
<details>
<summary>摘要</summary>
人类示范学习可以教育机器人执行复杂的搬运任务，但是时间和劳动力成本高。相比之下，任务和动作规划（TAMP）系统是自动化的，但是它们在触感丰富任务上表现不佳。本文描述了一种新的人类在循环（HITL-TAMP）系统，它利用了两种方法的优点。该系统使用了 TAMP-gated 控制机制，允许人类操作员在机器人队列中进行数据采集管理。采集的人类数据与一种模仿学习框架结合，以训练一个 TAMP-gated 政策，从而实现更高的性能。我们比较了 HITL-TAMP 和传统的 теле操作系统，发现用户可以在同一时间预算内收集更多的示范数据（至少3倍）。此外，我们还发现，只需要10分钟的非专家电操作数据，可以训练出75%以上的成功率的高效代理。最后，我们收集了12种Contact-rich、long-horizon任务的2100个示范数据，并证明该系统可以生成近似完美的代理。详细结果和视频可以在 <https://hitltamp.github.io> 查看。
</details></li>
</ul>
<hr>
<h2 id="Dissecting-In-Context-Learning-of-Translations-in-GPTs"><a href="#Dissecting-In-Context-Learning-of-Translations-in-GPTs" class="headerlink" title="Dissecting In-Context Learning of Translations in GPTs"></a>Dissecting In-Context Learning of Translations in GPTs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15987">http://arxiv.org/abs/2310.15987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikas Raunak, Hany Hassan Awadalla, Arul Menezes</li>
<li>for: 这项研究旨在更好地理解在语音翻译中使用大语言模型（LLMs）的几个示例选择。</li>
<li>methods: 这项研究使用了对高质量、 Domain 中的示例进行偏移来更好地理解翻译中的几个示例选择。</li>
<li>results: 研究发现，源侧偏移几乎没有影响，而目标偏移可以很快地降低翻译质量，这说明在翻译中，输出文本分布是提供最重要的学习信号。他们提出了一种名为 Zero-Shot-Context 的方法，可以自动地添加这种信号在零示例提示中。研究显示，这种方法可以超越 GPT-3 的零示例翻译性能，甚至与几个示例提示的翻译性能相匹配。<details>
<summary>Abstract</summary>
Most of the recent work in leveraging Large Language Models (LLMs) such as GPT-3 for Machine Translation (MT) has focused on selecting the few-shot samples for prompting. In this work, we try to better understand the role of demonstration attributes for the in-context learning of translations through perturbations of high-quality, in-domain demonstrations. We find that asymmetric perturbation of the source-target mappings yield vastly different results. We show that the perturbation of the source side has surprisingly little impact, while target perturbation can drastically reduce translation quality, suggesting that it is the output text distribution that provides the most important learning signal during in-context learning of translations. We propose a method named Zero-Shot-Context to add this signal automatically in Zero-Shot prompting. We demonstrate that it improves upon the zero-shot translation performance of GPT-3, even making it competitive with few-shot prompted translations.
</details>
<details>
<summary>摘要</summary>
大多数最近的研究把大型自然语言模型（LLM）如GPT-3应用于机器翻译（MT）都集中在选择少量示例的推示中。在这个工作中，我们尝试更好地理解在上下文学习翻译时示例特征的作用。我们发现源目标映射的偏移会导致极其不同的结果，而目标偏移可以很快地降低翻译质量，表明在翻译上下文学习中，输出文本分布提供了最重要的学习信号。我们提出一种名为Zero-Shot-Context的方法，可以自动添加这种信号在零批示中。我们示示了这种方法可以超越GPT-3的零批示翻译性能，甚至与几批示推示的翻译性能竞争。
</details></li>
</ul>
<hr>
<h2 id="Graph-Deep-Learning-for-Time-Series-Forecasting"><a href="#Graph-Deep-Learning-for-Time-Series-Forecasting" class="headerlink" title="Graph Deep Learning for Time Series Forecasting"></a>Graph Deep Learning for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15978">http://arxiv.org/abs/2310.15978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Andrea Cini, Ivan Marisca, Daniele Zambon, Cesare Alippi</li>
<li>for: 本研究旨在提供一个系统的方法ologique框架，以便对时间序列集合进行Graph-based深度学习预测。</li>
<li>methods: 本研究使用的方法包括对时间序列集合进行Graph-based的深度学习预测，并提供了一个系统的设计原则和性能评估方法。</li>
<li>results: 本研究的结果显示，Graph-based深度学习预测方法可以实现高精度的预测，并且可以处理大规模的时间序列集合。此外，本研究还提供了一些设计原则和实践建议，以便帮助研究者在实际应用中使用这些方法。<details>
<summary>Abstract</summary>
Graph-based deep learning methods have become popular tools to process collections of correlated time series. Differently from traditional multivariate forecasting methods, neural graph-based predictors take advantage of pairwise relationships by conditioning forecasts on a (possibly dynamic) graph spanning the time series collection. The conditioning can take the form of an architectural inductive bias on the neural forecasting architecture, resulting in a family of deep learning models called spatiotemporal graph neural networks. Such relational inductive biases enable the training of global forecasting models on large time-series collections, while at the same time localizing predictions w.r.t. each element in the set (i.e., graph nodes) by accounting for local correlations among them (i.e., graph edges). Indeed, recent theoretical and practical advances in graph neural networks and deep learning for time series forecasting make the adoption of such processing frameworks appealing and timely. However, most of the studies in the literature focus on proposing variations of existing neural architectures by taking advantage of modern deep learning practices, while foundational and methodological aspects have not been subject to systematic investigation. To fill the gap, this paper aims to introduce a comprehensive methodological framework that formalizes the forecasting problem and provides design principles for graph-based predictive models and methods to assess their performance. At the same time, together with an overview of the field, we provide design guidelines, recommendations, and best practices, as well as an in-depth discussion of open challenges and future research directions.
</details>
<details>
<summary>摘要</summary>
GRaph-based deep learning methods have become popular tools to process collections of correlated time series. Differently from traditional multivariate forecasting methods, neural graph-based predictors take advantage of pairwise relationships by conditioning forecasts on a (possibly dynamic) graph spanning the time series collection. The conditioning can take the form of an architectural inductive bias on the neural forecasting architecture, resulting in a family of deep learning models called spatiotemporal graph neural networks. Such relational inductive biases enable the training of global forecasting models on large time-series collections, while at the same time localizing predictions w.r.t. each element in the set (i.e., graph nodes) by accounting for local correlations among them (i.e., graph edges). Indeed, recent theoretical and practical advances in graph neural networks and deep learning for time series forecasting make the adoption of such processing frameworks appealing and timely. However, most of the studies in the literature focus on proposing variations of existing neural architectures by taking advantage of modern deep learning practices, while foundational and methodological aspects have not been subject to systematic investigation. To fill the gap, this paper aims to introduce a comprehensive methodological framework that formalizes the forecasting problem and provides design principles for graph-based predictive models and methods to assess their performance. At the same time, together with an overview of the field, we provide design guidelines, recommendations, and best practices, as well as an in-depth discussion of open challenges and future research directions.Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Accented-Speech-Recognition-With-Accent-specific-Codebooks"><a href="#Accented-Speech-Recognition-With-Accent-specific-Codebooks" class="headerlink" title="Accented Speech Recognition With Accent-specific Codebooks"></a>Accented Speech Recognition With Accent-specific Codebooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15970">http://arxiv.org/abs/2310.15970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csalt-research/accented-codebooks-asr">https://github.com/csalt-research/accented-codebooks-asr</a></li>
<li>paper_authors: Darshan Prabhu, Preethi Jyothi, Sriram Ganapathy, Vinit Unni</li>
<li>For: 本研究旨在提高现代自动话语识别（ASR）系统对不同口音的表现，尤其是对于未经投入的口音。* Methods: 我们提出了一种新的口音适应方法，使用跨注意力的学习码库集。这些可学习的码库集中包含了口音特有的信息，并与ASR编码层结合使用。* Results: 我们在Mozilla Common Voice多口音 dataset上进行了实验，结果显示，我们的提议方法可以在训练过程中提高英语口音表现（最多提高37%的单词错误率），同时在未见口音测试数据上也可以获得显著改善（最多提高5%的单词错误率）。此外，我们还发现在零基础转移设置下，我们的方法可以在L2Artic dataset上实现显著改善。与其他基于口音对抗训练的方法进行比较，我们的方法也表现出优异。<details>
<summary>Abstract</summary>
Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare the performance with other approaches based on accent adversarial training.
</details>
<details>
<summary>摘要</summary>
听力口音对现代自动语音识别（ASR）系统 pose  significiant 挑战。听力口音不均衡的性能下降是ASR的包容性采用的严重障碍。在这项工作中，我们提议一种基于 cross-attention 的新的口音适应方法，用于结构ASR系统。这些可学习的codebook捕捉了口音特定信息，并在ASR编码层中集成。模型在口音英语语音上训练，测试数据包含不同于训练数据中的口音。在Mozilla Common Voice多口音集合上，我们表明我们的提议方法可以在 seen 口音上实现 significiant 性能提升（最高达37%的字典错误率下降），以及不seen 口音上（最高达5%的字典错误率下降）。此外，我们还证明了零战斗转移设置下的好处。此外，我们还与口音对抗训练方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-with-Large-Language-Models-for-Recommendation"><a href="#Representation-Learning-with-Large-Language-Models-for-Recommendation" class="headerlink" title="Representation Learning with Large Language Models for Recommendation"></a>Representation Learning with Large Language Models for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15950">http://arxiv.org/abs/2310.15950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkuds/rlmrec">https://github.com/hkuds/rlmrec</a></li>
<li>paper_authors: Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang</li>
<li>for: 这个论文的目的是提高现有的推荐系统，使其能够更好地捕捉用户的偏好和行为。</li>
<li>methods: 这篇论文使用了语言模型（LLM）来强化现有的ID-based推荐系统，并通过跨视图对Alignment来协调语言模型和共同关系信号的semantic空间。</li>
<li>results: 在我们的评估中，RLMRec可以增强现有的推荐模型，并且可以降低噪声和偏见的影响。我们的实现代码可以在<a target="_blank" rel="noopener" href="https://github.com/HKUDS/RLMRec%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HKUDS/RLMRec上获取。</a><details>
<summary>Abstract</summary>
Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals, develops a user/item profiling paradigm empowered by LLMs, and aligns the semantic space of LLMs with the representation space of collaborative relational signals through a cross-view alignment framework. This work further establish a theoretical foundation demonstrating that incorporating textual signals through mutual information maximization enhances the quality of representations. In our evaluation, we integrate RLMRec with state-of-the-art recommender models, while also analyzing its efficiency and robustness to noise data. Our implementation codes are available at https://github.com/HKUDS/RLMRec.
</details>
<details>
<summary>摘要</summary>
现代推荐系统已经受到深度学习和图内存网络的影响，特别是在捕捉用户-项目关系的复杂性方面做出了重要进步。然而，这些图基的推荐器很多依赖ID数据，可能忽略用户和项目之间的文本信息，从而导致学习的表示变得更加粗糙。此外，使用隐式反馈数据也会引入噪声和偏见问题，对用户喜好学习造成挑战。而将大型自然语言模型（LLM）integrated into传统ID基的推荐器已经吸引了关注，但是这些挑战需要解决，例如可扩展性问题、仅依靠文本信息的局限性和提交输入约束。为了解决这些挑战，我们提出了一个模型无关的框架RLMRec，该框架通过将LLM-empowered representation learning与现有推荐器集成，以捕捉用户行为和喜好的细腻层次。RLMRec利用文本信号作为辅助信号，开发了基于LLM的用户/项目 profiling方法，并通过交叉视图对齐框架将LLM的 semantic space与现有的共同关系信号的表示空间相匹配。本研究还建立了基于在加入文本信号的mutual information最大化的理论基础，证明了将文本信号integrated into推荐系统可以提高表示质量。在我们的评估中，我们将RLMRec与当今最先进的推荐模型结合，同时分析其效率和Robustness to noise data。我们的实现代码可以在https://github.com/HKUDS/RLMRec上获取。
</details></li>
</ul>
<hr>
<h2 id="Combining-Behaviors-with-the-Successor-Features-Keyboard"><a href="#Combining-Behaviors-with-the-Successor-Features-Keyboard" class="headerlink" title="Combining Behaviors with the Successor Features Keyboard"></a>Combining Behaviors with the Successor Features Keyboard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15940">http://arxiv.org/abs/2310.15940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wilka Carvalho, Andre Saraiva, Angelos Filos, Andrew Kyle Lampinen, Loic Matthey, Richard L. Lewis, Honglak Lee, Satinder Singh, Danilo J. Rezende, Daniel Zoran</li>
<li>for: 这篇论文目标是提出一种基于Successor Features（SF）和Generalized Policy Improvement（GPI）的行为知识传递方法，以便在新任务环境中快速学习。</li>
<li>methods: 该方法使用了Successor Features Keyboard（SFK）和Categorical Successor Feature Approximator（CSFA）两种算法来实现行为知识传递。CSFA是一种新的学习算法，可以同时发现状态特征和任务编码。</li>
<li>results: 通过SFK和CSFA，该论文在一个复杂的3D环境中实现了行为知识传递，并且比基于转移学习的基准方法更快速地传递到长期任务。此外，CSFA比其他SF approximator方法更能够在大规模任务中发现与SF&amp;GPI相容的表示。<details>
<summary>Abstract</summary>
The Option Keyboard (OK) was recently proposed as a method for transferring behavioral knowledge across tasks. OK transfers knowledge by adaptively combining subsets of known behaviors using Successor Features (SFs) and Generalized Policy Improvement (GPI). However, it relies on hand-designed state-features and task encodings which are cumbersome to design for every new environment. In this work, we propose the "Successor Features Keyboard" (SFK), which enables transfer with discovered state-features and task encodings. To enable discovery, we propose the "Categorical Successor Feature Approximator" (CSFA), a novel learning algorithm for estimating SFs while jointly discovering state-features and task encodings. With SFK and CSFA, we achieve the first demonstration of transfer with SFs in a challenging 3D environment where all the necessary representations are discovered. We first compare CSFA against other methods for approximating SFs and show that only CSFA discovers representations compatible with SF&GPI at this scale. We then compare SFK against transfer learning baselines and show that it transfers most quickly to long-horizon tasks.
</details>
<details>
<summary>摘要</summary>
“Option Keyboard（OK）最近被提议作为知识传递方法。OK通过适应性地组合已知行为使用成功特征（SF）和通用政策改进（GPI）来传递知识。然而，它依赖于手动设计的状态特征和任务编码，这些编码是为每个新环境设计的困难。在这种工作中，我们提出了“Successor Features Keyboard”（SFK），它允许通过发现状态特征和任务编码来传递知识。为实现发现，我们提出了“ categorical Successor Feature Approximator”（CSFA），一种新的学习算法，可以在同时发现状态特征和任务编码中 estimating SF。与SFK和CSFA相比，我们发现SFK可以在长期任务中传递最快。”Note that Simplified Chinese is used in this translation, as it is the more widely used standard for Chinese writing in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="E-Sparse-Boosting-the-Large-Language-Model-Inference-through-Entropy-based-N-M-Sparsity"><a href="#E-Sparse-Boosting-the-Large-Language-Model-Inference-through-Entropy-based-N-M-Sparsity" class="headerlink" title="E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity"></a>E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15929">http://arxiv.org/abs/2310.15929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, Zhanhui Kang</li>
<li>for: 提高 Large Language Models (LLMs) 的 Generative AI 性能，解决传统遍历方法在 LLMs 中的不可持续性问题。</li>
<li>methods: 利用隐藏状态特征的信息熵，设计一种名为 E-Sparse 的减少精度度量，以提高 N:M 稀缺性的准确性。E-Sparse 使用信息 ricness 来优化通道重要性，并采用了一些新的技术来实现：（1）通过增加参数权重和输入特征norm的信息熵来增强精度度量，并不需要修改剩下的权重。（2）设计全局简单混合和本地块混合来快速优化信息分布，并妥协 LLMS 的精度和内存占用。</li>
<li>results: E-Sparse 在 LLaMA 家族和 OPT 模型上实现了显著提高模型推理速度（最高达 1.53 倍）和内存占用减少（最高达 43.52%），与稀缺模型的精度损失可以接受。<details>
<summary>Abstract</summary>
Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere GPUs. Extensive experiments on the LLaMA family and OPT models show that E-Sparse can significantly speed up the model inference over the dense model (up to 1.53X) and obtain significant memory saving (up to 43.52%), with acceptable accuracy loss.
</details>
<details>
<summary>摘要</summary>
传统的剪枝方法在大语言模型（LLM）中的生成AI中存在困难，因为它们的训练过程是不可持预算的，计算负担也很大。我们首次在LLM中引入隐藏状态特征的信息熵，并将其作为剪枝度量设计metric，称之为E-Sparse。E-Sparse利用隐藏状态特征的信息质量来优化频道重要性，并采用了一些新的技术来实现：1. 通过将参数权重和输入特征norm中的信息熵添加到剪枝度量中，以提高N:M稀疏的精度。2. 设计了全局简单混淆和局部块混淆，以快速调整信息分布，并有效地处理LLMs的精度下降。E-Sparse实现为Sparse-GEMM在FasterTransformer上，并在NVIDIA Ampere GPU上运行。广泛的实验表明，E-Sparse可以在LLMs中提高模型推理速度（最高达1.53倍），并获得显著的内存储存减少（最高达43.52%），同时保持了可接受的精度损失。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Mechanisms-for-Factual-Recall-in-Language-Models"><a href="#Characterizing-Mechanisms-for-Factual-Recall-in-Language-Models" class="headerlink" title="Characterizing Mechanisms for Factual Recall in Language Models"></a>Characterizing Mechanisms for Factual Recall in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15910">http://arxiv.org/abs/2310.15910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinan Yu, Jack Merullo, Ellie Pavlick</li>
<li>for: 研究语言模型（LM）在练习时如何处理不同来源的信息冲突。</li>
<li>methods: 使用分布式和机制性的方法来研究LM的行为，包括测量LM使用counterfactual前缀（例如“波兰首都是伦敦”）抹除预练知识。</li>
<li>results: 发现在Pythia和GPT2模型中，训练国家“波兰”和在context中城市“伦敦”的频率高度影响LM的使用counterfactual的可能性。通过在运行时缩放单个注意头的值向量来控制使用context answer的可能性，可以提高生成context answer的时间为88%。这些研究贡献到了控制模型行为的方法，并提供了一种proof of concept。<details>
<summary>Abstract</summary>
Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations. Specifically, we measure the proportion of the time an LM will use a counterfactual prefix (e.g., "The capital of Poland is London") to overwrite what it learned in pretraining ("Warsaw"). On Pythia and GPT2, the training frequency of both the query country ("Poland") and the in-context city ("London") highly affect the models' likelihood of using the counterfactual. We then use head attribution to identify individual attention heads that either promote the memorized answer or the in-context answer in the logits. By scaling up or down the value vector of these heads, we can control the likelihood of using the in-context answer on new data. This method can increase the rate of generating the in-context answer to 88\% of the time simply by scaling a single head at runtime. Our work contributes to a body of evidence showing that we can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）经常需要将在预训练中记忆的信息与新的上下文中的信息结合在一起。这两个来源可能会有冲突， causing competition within the model, 并且不清楚如何使模型解决这种冲突。在一个关于世界首都的数据集上，我们调查了分布式和机制性的权重决定LM行为在这些情况下。specifically，我们测量了LM使用counterfactual prefix（例如，"Poland的首都是London")来覆盖预训练中学习的答案（"Warsaw")的比例。在Pythia和GPT2上，训练国家("Poland")和上下文城市("London")的频率高度影响模型的使用counterfactual的可能性。然后，我们使用头归因来确定具有推荐 memorized answer 或 in-context answer 的个体注意头。通过在运行时缩放这些头的值向量，我们可以控制在新数据上使用 in-context answer 的可能性。这种方法可以将在新数据上使用 in-context answer 的比例提高到88%，只需在运行时缩放单个头。我们的工作贡献到了控制模型行为的方法，并提供了一种实验证明的证明。
</details></li>
</ul>
<hr>
<h2 id="Is-Probing-All-You-Need-Indicator-Tasks-as-an-Alternative-to-Probing-Embedding-Spaces"><a href="#Is-Probing-All-You-Need-Indicator-Tasks-as-an-Alternative-to-Probing-Embedding-Spaces" class="headerlink" title="Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding Spaces"></a>Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15905">http://arxiv.org/abs/2310.15905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tal Levy, Omer Goldman, Reut Tsarfaty</li>
<li>for: This paper is written to explore the use of indicator tasks for evaluating the information encoded in word embeddings, and to demonstrate the advantages of using indicator tasks over traditional probing methods.</li>
<li>methods: The paper uses two test cases to demonstrate the effectiveness of indicator tasks: one dealing with gender debiasing and another with the erasure of morphological information from embedding spaces.</li>
<li>results: The paper shows that the application of a suitable indicator provides a more accurate picture of the information captured and removed compared to probes, and thus concludes that indicator tasks should be implemented and taken into consideration when eliciting information from embedded representations.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨word embedding中信息的评估方法，并通过两个测试 случа件来证明使用指标任务的优势。</li>
<li>methods: 这篇论文使用两个测试 случа件来证明指标任务的效果：一个 gender 偏见减少的测试 случа件和另一个 morphological information 从 embedding space 中除去的测试cases。</li>
<li>results: 这篇论文表明，使用合适的指标可以更准确地捕捉 embedding space 中 captured 和 removed 的信息，而不是使用 probing 方法。因此，这篇论文认为，指标任务应该被实施并考虑在 embedded representation 中提取信息时使用。<details>
<summary>Abstract</summary>
The ability to identify and control different kinds of linguistic information encoded in vector representations of words has many use cases, especially for explainability and bias removal. This is usually done via a set of simple classification tasks, termed probes, to evaluate the information encoded in the embedding space. However, the involvement of a trainable classifier leads to entanglement between the probe's results and the classifier's nature. As a result, contemporary works on probing include tasks that do not involve training of auxiliary models. In this work we introduce the term indicator tasks for non-trainable tasks which are used to query embedding spaces for the existence of certain properties, and claim that this kind of tasks may point to a direction opposite to probes, and that this contradiction complicates the decision on whether a property exists in an embedding space. We demonstrate our claims with two test cases, one dealing with gender debiasing and another with the erasure of morphological information from embedding spaces. We show that the application of a suitable indicator provides a more accurate picture of the information captured and removed compared to probes. We thus conclude that indicator tasks should be implemented and taken into consideration when eliciting information from embedded representations.
</details>
<details>
<summary>摘要</summary>
“可以识别和控制不同类型的语言信息对字 vector 表示的编码有很多用途，特别是 для explainability 和偏见除除。通常透过一系列简单的分类任务，称为探针，来评估嵌入空间中的信息。但是，将探针与搜寻器的性质推导关系会导致嵌入空间中的信息被探针所探索的结果混合在一起。为了解决这个问题，现代工作中通常使用不需要训练助手模型的任务。在这篇文章中，我们引入了“仪器任务”这个概念，用于在嵌入空间中询问特定的特性是否存在。我们声称，这种任务可能会与探针相反的方向，并且这个对应关系会复杂化嵌入空间中的信息决定。我们透过两个测试案例，一个是关于性别偏见的消除和另一个是关于嵌入空间中的 morphological 信息的消除，以示出适当的仪器可以提供更加精确的信息捕捉和移除比探针。因此，我们结论到，仪器任务应该被实现和考虑在嵌入表示中提取信息时。”
</details></li>
</ul>
<hr>
<h2 id="AdaptiX-–-A-Transitional-XR-Framework-for-Development-and-Evaluation-of-Shared-Control-Applications-in-Assistive-Robotics"><a href="#AdaptiX-–-A-Transitional-XR-Framework-for-Development-and-Evaluation-of-Shared-Control-Applications-in-Assistive-Robotics" class="headerlink" title="AdaptiX – A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics"></a>AdaptiX – A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15887">http://arxiv.org/abs/2310.15887</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxpascher/AdaptiX">https://github.com/maxpascher/AdaptiX</a></li>
<li>paper_authors: Max Pascher, Felix Ferdinand Goldau, Kirill Kronhardt, Udo Frese, Jens Gerken</li>
<li>for: 本研究旨在提高辅助技术的可用性和普及度，通过结合人机共同控制的概念，提高用户自主性，并在高解像力 simulate 环境中评估和开发共同控制应用程序。</li>
<li>methods: 本研究使用了开源的 AdaptiX XR 框架，可以快速开发和评估共同控制应用程序，无需真实的 física robotic arm。它可以轻松扩展，以满足特定研究需求，并可以通过 ROS 集成控制真实 robotic arm。</li>
<li>results: 本研究通过 AdaptiX 框架进行了评估和开发共同控制应用程序，并在虚拟现实环境中进行了多种人机共同控制方法的研究。结果表明，AdaptiX 可以帮助提高用户自主性，并减少用户与软件控制之间的差异。<details>
<summary>Abstract</summary>
With the ongoing efforts to empower people with mobility impairments and the increase in technological acceptance by the general public, assistive technologies, such as collaborative robotic arms, are gaining popularity. Yet, their widespread success is limited by usability issues, specifically the disparity between user input and software control along the autonomy continuum. To address this, shared control concepts provide opportunities to combine the targeted increase of user autonomy with a certain level of computer assistance. This paper presents the free and open-source AdaptiX XR framework for developing and evaluating shared control applications in a high-resolution simulation environment. The initial framework consists of a simulated robotic arm with an example scenario in Virtual Reality (VR), multiple standard control interfaces, and a specialized recording/replay system. AdaptiX can easily be extended for specific research needs, allowing Human-Robot Interaction (HRI) researchers to rapidly design and test novel interaction methods, intervention strategies, and multi-modal feedback techniques, without requiring an actual physical robotic arm during the early phases of ideation, prototyping, and evaluation. Also, a Robot Operating System (ROS) integration enables the controlling of a real robotic arm in a PhysicalTwin approach without any simulation-reality gap. Here, we review the capabilities and limitations of AdaptiX in detail and present three bodies of research based on the framework. AdaptiX can be accessed at https://adaptix.robot-research.de.
</details>
<details>
<summary>摘要</summary>
随着人们身体障碍的 empowerment 和技术的普及，协助技术，如合作 робоptic arms，在广泛的应用中受到欢迎。然而，其广泛的成功受限于使用问题，具体是用户输入和软件控制之间的差异，在自主性维度上。 To address this, shared control concepts can combine the targeted increase of user autonomy with a certain level of computer assistance. This paper presents the free and open-source AdaptiX XR framework for developing and evaluating shared control applications in a high-resolution simulation environment. The initial framework consists of a simulated robotic arm with an example scenario in Virtual Reality (VR), multiple standard control interfaces, and a specialized recording/replay system. AdaptiX can easily be extended for specific research needs, allowing Human-Robot Interaction (HRI) researchers to rapidly design and test novel interaction methods, intervention strategies, and multi-modal feedback techniques, without requiring an actual physical robotic arm during the early phases of ideation, prototyping, and evaluation. Additionally, a Robot Operating System (ROS) integration enables the controlling of a real robotic arm in a PhysicalTwin approach without any simulation-reality gap. Here, we review the capabilities and limitations of AdaptiX in detail and present three bodies of research based on the framework. AdaptiX can be accessed at https://adaptix.robot-research.de.
</details></li>
</ul>
<hr>
<h2 id="KirchhoffNet-A-Circuit-Bridging-Message-Passing-and-Continuous-Depth-Models"><a href="#KirchhoffNet-A-Circuit-Bridging-Message-Passing-and-Continuous-Depth-Models" class="headerlink" title="KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models"></a>KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15872">http://arxiv.org/abs/2310.15872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengqi Gao, Fan-Keng Sun, Duane S. Boning</li>
<li>for: 这篇论文的目的是介绍一种基于电子电路基本原理的神经网络模型，称之为KirchhoffNet，它与消息传递神经网络和连续深度网络有紧密的联系。</li>
<li>methods: 作者使用Kirchhoff的电流法则来引入一种独特的神经网络模型，这种模型不需要传统层次结构（如卷积、聚合和线性层）却可以达到98.86%的测试准确率在MNIST dataset，与当前最佳实践（SOTA）结果相当。</li>
<li>results: 作者证明了KirchhoffNet在硬件方面具有潜在的优势，它可以通过物理实现为analog电子电路，而不需要使用GPU。此外，作者还证明了KirchhoffNet的前向计算总是在1&#x2F;f秒钟内完成，这意味着无论KirchhoffNet的参数数量如何，它都可以在1&#x2F;f秒钟内完成计算。这一特点为实现ultra-大规模神经网络提供了一个有前途的技术。<details>
<summary>Abstract</summary>
In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteristic introduces a promising technology for implementing ultra-large-scale neural networks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们利用电子电路中的一个基本原理，基希夫托的电流法则，来引入一种新的神经网络模型，我们称之为基希夫托网络（KirchhoffNet）。基希夫托网络与消息传递神经网络和连续深度网络有紧密的联系。我们示示了，即使没有传统层（如核心抽取、聚合或线性层），基希夫托网络仍能达到98.86%的测试准确率在MNIST数据集上，与状态的最佳实践（SOTA）结果相当。这使得基希夫托网络更加吸引人，因为它在硬件方面具有潜在的优势。当代深度神经网络通常在GPU上部署。相比之下，基希夫托网络可以被物理实现为一个分析电子电路。此外，我们证明了基希夫托网络的前向计算总是在1/f秒钟内完成，其中f是硬件的频率。这一特点引入了可能实现超大规模神经网络的有望技术。
</details></li>
</ul>
<hr>
<h2 id="CP-BCS-Binary-Code-Summarization-Guided-by-Control-Flow-Graph-and-Pseudo-Code"><a href="#CP-BCS-Binary-Code-Summarization-Guided-by-Control-Flow-Graph-and-Pseudo-Code" class="headerlink" title="CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code"></a>CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16853">http://arxiv.org/abs/2310.16853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du, Peiyu Liu, Shouling Ji, Wenhai Wang</li>
<li>for: 本研究的目的是提供一种能够自动生成binary函数摘要的方法，以便进行反向工程学。</li>
<li>methods: 本研究使用了一种基于控制流图和伪代码的 binary code summarization 框架，称为CP-BCS。该框架利用了双向指令级别控制流图和伪代码，以便充分利用 Assembly 代码的 semantics。</li>
<li>results: 对于3种不同的 binary 优化级别（O1、O2、O3）和3种不同的计算机架构（X86、X64、ARM），CP-BCS 的评估结果表明，它能够显著提高反向工程学的效率。<details>
<summary>Abstract</summary>
Automatically generating function summaries for binaries is an extremely valuable but challenging task, since it involves translating the execution behavior and semantics of the low-level language (assembly code) into human-readable natural language. However, most current works on understanding assembly code are oriented towards generating function names, which involve numerous abbreviations that make them still confusing. To bridge this gap, we focus on generating complete summaries for binary functions, especially for stripped binary (no symbol table and debug information in reality). To fully exploit the semantics of assembly code, we present a control flow graph and pseudo code guided binary code summarization framework called CP-BCS. CP-BCS utilizes a bidirectional instruction-level control flow graph and pseudo code that incorporates expert knowledge to learn the comprehensive binary function execution behavior and logic semantics. We evaluate CP-BCS on 3 different binary optimization levels (O1, O2, and O3) for 3 different computer architectures (X86, X64, and ARM). The evaluation results demonstrate CP-BCS is superior and significantly improves the efficiency of reverse engineering.
</details>
<details>
<summary>摘要</summary>
自动生成函数摘要 для二进制文件是一项非常有价值但具有挑战性的任务，因为它涉及翻译二进制语言（assembly code）的执行行为和semantics into human-readable natural language。然而，现有的大多数工作都集中在生成函数名称上，这些名称充满缩写，使得它们仍然混乱不清。为了弥补这一差距，我们将注意点在生成完整的函数摘要，特别是对于剥离二进制（没有符号表和调试信息）。为了全面利用二进制语言的 semantics，我们提出了一种控制流图和pseudo code导航 binary code摘要框架，称为CP-BCS。CP-BCS使用双向指令级别控制流图和pseudo code，并通过专家知识来学习二进制函数执行行为和逻辑semantics。我们对3个不同的二进制优化级别（O1、O2和O3）和3种不同的计算机架构（X86、X64和ARM）进行了评估。评估结果表明，CP-BCS在逆向工程中提高了效率。
</details></li>
</ul>
<hr>
<h2 id="Topology-aware-Debiased-Self-supervised-Graph-Learning-for-Recommendation"><a href="#Topology-aware-Debiased-Self-supervised-Graph-Learning-for-Recommendation" class="headerlink" title="Topology-aware Debiased Self-supervised Graph Learning for Recommendation"></a>Topology-aware Debiased Self-supervised Graph Learning for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15858">http://arxiv.org/abs/2310.15858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/malajikuai/tdsgl">https://github.com/malajikuai/tdsgl</a></li>
<li>paper_authors: Lei Han, Hui Yan, Zhicheng Qiao</li>
<li>for: 提高推荐系统的准确率和效果</li>
<li>methods: 基于图的自适应自监学习（TDSGL），利用交互数据反映用户购买意愿和物品特点，构建强类对比，提高模型的泛化能力</li>
<li>results: 在三个公共数据集上实验显示，提出的模型在与状态艺术模型进行比较时达到了显著的提高，代码可以在<a target="_blank" rel="noopener" href="https://github.com/malajikuai/TDSGL%E4%B8%AD%E8%8E%B7%E5%8F%96">https://github.com/malajikuai/TDSGL中获取</a><details>
<summary>Abstract</summary>
In recommendation, graph-based Collaborative Filtering (CF) methods mitigate the data sparsity by introducing Graph Contrastive Learning (GCL). However, the random negative sampling strategy in these GCL-based CF models neglects the semantic structure of users (items), which not only introduces false negatives (negatives that are similar to anchor user (item)) but also ignores the potential positive samples. To tackle the above issues, we propose Topology-aware Debiased Self-supervised Graph Learning (TDSGL) for recommendation, which constructs contrastive pairs according to the semantic similarity between users (items). Specifically, since the original user-item interaction data commendably reflects the purchasing intent of users and certain characteristics of items, we calculate the semantic similarity between users (items) on interaction data. Then, given a user (item), we construct its negative pairs by selecting users (items) which embed different semantic structures to ensure the semantic difference between the given user (item) and its negatives. Moreover, for a user (item), we design a feature extraction module that converts other semantically similar users (items) into an auxiliary positive sample to acquire a more informative representation. Experimental results show that the proposed model outperforms the state-of-the-art models significantly on three public datasets. Our model implementation codes are available at https://github.com/malajikuai/TDSGL.
</details>
<details>
<summary>摘要</summary>
在推荐中，基于图的共同推荐（CF）方法可以减轻数据稀缺性，通过引入图像对比学习（GCL）。然而，Random Negative Sampling策略在这些GCL基于CF模型中忽略用户（item）的 semantic structure，不仅引入假的负样本（与锚用户（item）相似的负样本），还忽略了可能的正样本。为了解决这些问题，我们提出了 topology-aware debiased self-supervised graph learning（TDSGL）模型，它根据用户（item）的semantic similarity来构建对比对。具体来说，我们根据原始的用户-item交互数据可以得到用户（item）的购买意愿和一些特征，然后计算用户（item）的semantic similarity。接下来，给一个用户（item），我们选择semantic structure不同的用户（item）作为负样本，以确保与给定用户（item）的semantic差异。此外，为一个用户（item），我们设计了一个特征提取模块，将其他semantic相似的用户（item）转换为auxiliary positive sample，以获得更加有用的表示。实验结果显示，我们提出的模型在三个公共数据集上与当前状态的模型显著超越。我们的模型实现代码可以在https://github.com/malajikuai/TDSGL中找到。
</details></li>
</ul>
<hr>
<h2 id="Using-Artificial-French-Data-to-Understand-the-Emergence-of-Gender-Bias-in-Transformer-Language-Models"><a href="#Using-Artificial-French-Data-to-Understand-the-Emergence-of-Gender-Bias-in-Transformer-Language-Models" class="headerlink" title="Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models"></a>Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15852">http://arxiv.org/abs/2310.15852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lina Conti, Guillaume Wisniewski</li>
<li>for: 这个研究探究了神经语言模型如何自然地学习语言特性，包括 gender 信息的捕捉和使用规则。</li>
<li>methods: 研究使用了一个基于 PCDF 的人工语料库，控制了训练数据中 gender 分布，以确定模型是否正确地捕捉 gender 信息，或者受到 gender 偏见。</li>
<li>results: 研究发现，当模型在训练时受到 sufficient 的 gender 信息时，它们可以正确地捕捉 gender 信息并遵循使用规则。但是，当 gender 信息不充分时，模型可能受到 gender 偏见。<details>
<summary>Abstract</summary>
Numerous studies have demonstrated the ability of neural language models to learn various linguistic properties without direct supervision. This work takes an initial step towards exploring the less researched topic of how neural models discover linguistic properties of words, such as gender, as well as the rules governing their usage. We propose to use an artificial corpus generated by a PCFG based on French to precisely control the gender distribution in the training data and determine under which conditions a model correctly captures gender information or, on the contrary, appears gender-biased.
</details>
<details>
<summary>摘要</summary>
多个研究已经证明神经语言模型可以不直接监督学习不同语言性质。这项工作尝试了对较少研究的话语性质的探索，包括单词的性别信息以及其使用规则。我们提议使用基于法语的PCFG生成人工词库，以控制训练数据中性别分布的精度，并确定模型在捕捉性别信息时是否正确或偏向一方。
</details></li>
</ul>
<hr>
<h2 id="Posterior-Estimation-for-Dynamic-PET-imaging-using-Conditional-Variational-Inference"><a href="#Posterior-Estimation-for-Dynamic-PET-imaging-using-Conditional-Variational-Inference" class="headerlink" title="Posterior Estimation for Dynamic PET imaging using Conditional Variational Inference"></a>Posterior Estimation for Dynamic PET imaging using Conditional Variational Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15850">http://arxiv.org/abs/2310.15850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofeng Liu, Thibault Marin, Tiss Amal, Jonghye Woo, Georges El Fakhri, Jinsong Ouyang</li>
<li>for: 这个研究旨在高效地估计动态PET成像中的质量参数 posterior distribution，基于时间活动曲线的测量。</li>
<li>methods: 该研究使用了深度学习框架，通过引入幂变量来抵消前向模型中的信息损失，然后使用conditional variational autoencoder（CVAE）估计 posterior。</li>
<li>results: 研究表明，CVAE-based方法可以高效地估计质量参数 posterior distribution，并且与MCMC sampling相比，其效果更好，特别是在低维度数据（单个脑区）下。<details>
<summary>Abstract</summary>
This work aims efficiently estimating the posterior distribution of kinetic parameters for dynamic positron emission tomography (PET) imaging given a measurement of time of activity curve. Considering the inherent information loss from parametric imaging to measurement space with the forward kinetic model, the inverse mapping is ambiguous. The conventional (but expensive) solution can be the Markov Chain Monte Carlo (MCMC) sampling, which is known to produce unbiased asymptotical estimation. We propose a deep-learning-based framework for efficient posterior estimation. Specifically, we counteract the information loss in the forward process by introducing latent variables. Then, we use a conditional variational autoencoder (CVAE) and optimize its evidence lower bound. The well-trained decoder is able to infer the posterior with a given measurement and the sampled latent variables following a simple multivariate Gaussian distribution. We validate our CVAE-based method using unbiased MCMC as the reference for low-dimensional data (a single brain region) with the simplified reference tissue model.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a deep-learning-based framework for efficient posterior estimation. Specifically, we introduce latent variables to counteract the information loss in the forward process. Then, we use a conditional variational autoencoder (CVAE) to optimize the evidence lower bound. The well-trained decoder is able to infer the posterior with a given measurement and the sampled latent variables, following a simple multivariate Gaussian distribution.To validate our CVAE-based method, we use unbiased MCMC as the reference for low-dimensional data (a single brain region) with the simplified reference tissue model. Our results show that the CVAE-based method can provide accurate and efficient posterior estimation for dynamic PET imaging.
</details></li>
</ul>
<hr>
<h2 id="Grid-Frequency-Forecasting-in-University-Campuses-using-Convolutional-LSTM"><a href="#Grid-Frequency-Forecasting-in-University-Campuses-using-Convolutional-LSTM" class="headerlink" title="Grid Frequency Forecasting in University Campuses using Convolutional LSTM"></a>Grid Frequency Forecasting in University Campuses using Convolutional LSTM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16071">http://arxiv.org/abs/2310.16071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aneesh Sathe, Wen Ren Yang</li>
<li>for: 这种paper的目的是提出一种基于Convolutional Neural Networks (CNN)和Long Short-Term Memory (LSTM)网络的强化时间序列预测模型，以提高电网的可靠性和灵活性。</li>
<li>methods: 这种方法使用了Convolutional LSTM (ConvLSTM)模型，通过训练每个学生会大楼的自适应模型，以适应各自的时间序列数据。同时，一种ensemble模型也是形成，以汇集各个大楼的预测结果，提供整体的预测结果。</li>
<li>results: 实验结果表明，提出的方法在预测电网频率方面表现出色，比传统预测技术更高的精度和稳定性。 metrics such as Mean Square Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) 都表明了这种方法的优势。<details>
<summary>Abstract</summary>
The modern power grid is facing increasing complexities, primarily stemming from the integration of renewable energy sources and evolving consumption patterns. This paper introduces an innovative methodology that harnesses Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks to establish robust time series forecasting models for grid frequency. These models effectively capture the spatiotemporal intricacies inherent in grid frequency data, significantly enhancing prediction accuracy and bolstering power grid reliability. The research explores the potential and development of individualized Convolutional LSTM (ConvLSTM) models for buildings within a university campus, enabling them to be independently trained and evaluated for each building. Individual ConvLSTM models are trained on power consumption data for each campus building and forecast the grid frequency based on historical trends. The results convincingly demonstrate the superiority of the proposed models over traditional forecasting techniques, as evidenced by performance metrics such as Mean Square Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). Additionally, an Ensemble Model is formulated to aggregate insights from the building-specific models, delivering comprehensive forecasts for the entire campus. This approach ensures the privacy and security of power consumption data specific to each building.
</details>
<details>
<summary>摘要</summary>
现代电力网络面临着加大复杂性，主要来自于可再生能源资源的整合和消耗模式的变化。这篇论文提出了一种创新的方法，利用卷积神经网络（CNN）和长短期记忆网络（LSTM）建立了可靠的时间序列预测模型，以提高电力网络的可靠性。研究探讨了建筑物pecific Convolutional LSTM（ConvLSTM）模型的可能性和发展，通过对每座建筑物的电力消耗数据进行独立训练和评估，以预测电力网络频率。结果证明了提议的模型在传统预测技术的基础上具有明显的优势，如 Mean Square Error（MSE）、Mean Absolute Error（MAE）和Mean Absolute Percentage Error（MAPE）等性能指标。此外，一个Ensemble Model也被构建，以汇集建筑物特定的预测结果，为整个校园提供全面的预测。这种方法保障了每座建筑物的电力消耗数据的隐私和安全。
</details></li>
</ul>
<hr>
<h2 id="Clinical-Decision-Support-System-for-Unani-Medicine-Practitioners"><a href="#Clinical-Decision-Support-System-for-Unani-Medicine-Practitioners" class="headerlink" title="Clinical Decision Support System for Unani Medicine Practitioners"></a>Clinical Decision Support System for Unani Medicine Practitioners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18361">http://arxiv.org/abs/2310.18361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haider Sultan, Hafiza Farwa Mahmood, Noor Fatima, Marriyam Nadeem, Talha Waheed</li>
<li>for: 这个研究旨在开发一个基于网络的临床决策支持系统，以帮助新手的医生更准确地诊断疾病，并提供远程医疗服务。</li>
<li>methods: 这个系统使用了现代人工智能技术，包括决策树、深度学习和自然语言处理，并将患者症状输入到网络上，然后自动分析和生成潜在疾病列表。</li>
<li>results: 这个系统可以帮助医生更快速和准确地诊断疾病，提高患者满意度和医疗效果，同时也可以减少医疗成本和提高医疗资源的利用率。<details>
<summary>Abstract</summary>
Like other fields of Traditional Medicines, Unani Medicines have been found as an effective medical practice for ages. It is still widely used in the subcontinent, particularly in Pakistan and India. However, Unani Medicines Practitioners are lacking modern IT applications in their everyday clinical practices. An Online Clinical Decision Support System may address this challenge to assist apprentice Unani Medicines practitioners in their diagnostic processes. The proposed system provides a web-based interface to enter the patient's symptoms, which are then automatically analyzed by our system to generate a list of probable diseases. The system allows practitioners to choose the most likely disease and inform patients about the associated treatment options remotely. The system consists of three modules: an Online Clinical Decision Support System, an Artificial Intelligence Inference Engine, and a comprehensive Unani Medicines Database. The system employs advanced AI techniques such as Decision Trees, Deep Learning, and Natural Language Processing. For system development, the project team used a technology stack that includes React, FastAPI, and MySQL. Data and functionality of the application is exposed using APIs for integration and extension with similar domain applications. The novelty of the project is that it addresses the challenge of diagnosing diseases accurately and efficiently in the context of Unani Medicines principles. By leveraging the power of technology, the proposed Clinical Decision Support System has the potential to ease access to healthcare services and information, reduce cost, boost practitioner and patient satisfaction, improve speed and accuracy of the diagnostic process, and provide effective treatments remotely. The application will be useful for Unani Medicines Practitioners, Patients, Government Drug Regulators, Software Developers, and Medical Researchers.
</details>
<details>
<summary>摘要</summary>
如其他传统医学一样，欧奈医学在历史上已经证明自己是一种有效的医疗方式。它仍然广泛使用在亚洲子半岛，特别是在巴基斯坦和印度。然而，欧奈医学实践者缺乏现代IT应用程序在日常临床实践中。一个在线临床决策支持系统可能解决这个挑战，并帮助新手欧奈医学实践者在诊断过程中更加准确和效率。该系统提供了一个网络化界面，让医生输入症状，然后自动由我们的系统分析，生成潜在疾病列表。系统允许医生选择最有可能的疾病，并通过在线向患者提供相关的治疗方案。系统包括三个模块：在线临床决策支持系统、人工智能推理引擎和欧奈医学数据库。系统运用先进的AI技术，如决策树、深度学习和自然语言处理。为系统开发，项目团队使用了技术栈，包括React、FastAPI和MySQL。数据和功能的暴露使用API，以便与类似领域应用程序集成和扩展。该项目的创新之处在于，它通过应用技术来解决欧奈医学原则下诊断疾病的挑战。通过利用技术，该提案的临床决策支持系统具有扩大健康服务和信息的访问权，降低成本，提高医生和患者满意度，提高诊断过程的速度和准确性，并提供远程治疗方案。该应用程序对欧奈医学实践者、患者、政府药品监管部门、软件开发者和医学研究人员都将有用。
</details></li>
</ul>
<hr>
<h2 id="A-Diffusion-Weighted-Graph-Framework-for-New-Intent-Discovery"><a href="#A-Diffusion-Weighted-Graph-Framework-for-New-Intent-Discovery" class="headerlink" title="A Diffusion Weighted Graph Framework for New Intent Discovery"></a>A Diffusion Weighted Graph Framework for New Intent Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15836">http://arxiv.org/abs/2310.15836</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yibai-shi/dwgf">https://github.com/yibai-shi/dwgf</a></li>
<li>paper_authors: Wenkai Shi, Wenbin An, Feng Tian, Qinghua Zheng, QianYing Wang, Ping Chen</li>
<li>for: 本研究旨在用有限量的标注数据来识别新和已知意图 from 无标注数据，并提供更充分和可靠的监督信号。</li>
<li>methods: 我们提出了一种新的Diffusion Weighted Graph Framework (DWGF)，可以捕捉数据中的语义相似性和结构关系，并生成更加充分和可靠的监督信号。</li>
<li>results: 我们的方法在多个 benchmark 数据集上的所有评价指标上都达到了state-of-the-art 水平。<details>
<summary>Abstract</summary>
New Intent Discovery (NID) aims to recognize both new and known intents from unlabeled data with the aid of limited labeled data containing only known intents. Without considering structure relationships between samples, previous methods generate noisy supervisory signals which cannot strike a balance between quantity and quality, hindering the formation of new intent clusters and effective transfer of the pre-training knowledge. To mitigate this limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to capture both semantic similarities and structure relationships inherent in data, enabling more sufficient and reliable supervisory signals. Specifically, for each sample, we diffuse neighborhood relationships along semantic paths guided by the nearest neighbors for multiple hops to characterize its local structure discriminately. Then, we sample its positive keys and weigh them based on semantic similarities and local structures for contrastive learning. During inference, we further propose Graph Smoothing Filter (GSF) to explicitly utilize the structure relationships to filter high-frequency noise embodied in semantically ambiguous samples on the cluster boundary. Extensive experiments show that our method outperforms state-of-the-art models on all evaluation metrics across multiple benchmark datasets. Code and data are available at https://github.com/yibai-shi/DWGF.
</details>
<details>
<summary>摘要</summary>
新意探索（NID）目标是从无标签数据中识别新和已知意图，帮助于限量标注数据中只包含已知意图。过去的方法生成了不稳定的超级监督信号，无法保持新意群和传输预训练知识的效果。为了解决这一限制，我们提出了一种新的扩散加重图框架（DWGF），可以捕捉数据中的语义相似性和结构关系，生成更充分和可靠的监督信号。具体来说，对每个样本，我们将邻居关系扩散到语义路径上，由最近邻居 guid 的多个跳步来特征化其地方结构。然后，我们将其正样例选择，并根据语义相似性和地方结构进行权重调整。在推理过程中，我们还提出了图像缓和筛选器（GSF），可以直接利用结构关系来过滤semantically ambiguous的样本。广泛的实验表明，我们的方法在多个评价指标上都超过了当前的模型。代码和数据可以在https://github.com/yibai-shi/DWGF中获取。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Aorta-Segmentation-with-Heavily-Augmented-High-Resolution-3-D-ResUNet-Contribution-to-the-SEG-A-Challenge"><a href="#Automatic-Aorta-Segmentation-with-Heavily-Augmented-High-Resolution-3-D-ResUNet-Contribution-to-the-SEG-A-Challenge" class="headerlink" title="Automatic Aorta Segmentation with Heavily Augmented, High-Resolution 3-D ResUNet: Contribution to the SEG.A Challenge"></a>Automatic Aorta Segmentation with Heavily Augmented, High-Resolution 3-D ResUNet: Contribution to the SEG.A Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15827">http://arxiv.org/abs/2310.15827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mwod/sega_mw_2023">https://github.com/mwod/sega_mw_2023</a></li>
<li>paper_authors: Marek Wodzinski, Henning Müller</li>
<li>for: 这个论文主要目标是提出一种自动推断三维医疗图像中的大动脉分割方法，以解决医疗图像中动脉分割的难题。</li>
<li>methods: 该方法基于深度编码器-解码器架构，并且假设数据预处理和扩展对于深度架构的性能更为重要，特别是在低数据范围内。因此，该方法基于变体的卷积核网络。</li>
<li>results: 该方法在所有测试 caso中取得了 dice 分数大于0.9，并且在所有参与者中得分最高，在临床评估、量化结果和三维瓷砾质量等方面分别获得了第一、第四和第三名。ources code、预训练模型和算法在 Grand-Challenge 平台上提供开源。<details>
<summary>Abstract</summary>
Automatic aorta segmentation from 3-D medical volumes is an important yet difficult task. Several factors make the problem challenging, e.g. the possibility of aortic dissection or the difficulty with segmenting and annotating the small branches. This work presents a contribution by the MedGIFT team to the SEG.A challenge organized during the MICCAI 2023 conference. We propose a fully automated algorithm based on deep encoder-decoder architecture. The main assumption behind our work is that data preprocessing and augmentation are much more important than the deep architecture, especially in low data regimes. Therefore, the solution is based on a variant of traditional convolutional U-Net. The proposed solution achieved a Dice score above 0.9 for all testing cases with the highest stability among all participants. The method scored 1st, 4th, and 3rd in terms of the clinical evaluation, quantitative results, and volumetric meshing quality, respectively. We freely release the source code, pretrained model, and provide access to the algorithm on the Grand-Challenge platform.
</details>
<details>
<summary>摘要</summary>
自动从三维医疗量子中分类索引静脉是一项重要但困难的任务。几种因素使得这个问题复杂，例如索引分化或者分类和注释小支流的困难。本文是MedGIFT团队对SEG.A挑战的贡献，于MICCAI 2023会议上举行。我们提议一种完全自动的深度编码器-解码器架构。我们假设数据预处理和扩展是在低数据情况下更重要的，特别是在低数据情况下。因此，我们的解决方案基于变种的传统 convolutional U-Net。我们的方法在测试案例中的 Dice 分数超过 0.9，并且在所有参与者中显示出最高稳定性。在临床评估、量化结果和卷积质量方面，我们的方法分别获得了第一名、第四名和第三名。我们开源了源代码、预训练模型，并在 Grand-Challenge 平台上提供了算法访问权限。
</details></li>
</ul>
<hr>
<h2 id="Rosetta-Stone-at-KSAA-RD-Shared-Task-A-Hop-From-Language-Modeling-To-Word–Definition-Alignment"><a href="#Rosetta-Stone-at-KSAA-RD-Shared-Task-A-Hop-From-Language-Modeling-To-Word–Definition-Alignment" class="headerlink" title="Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word–Definition Alignment"></a>Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word–Definition Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15823">http://arxiv.org/abs/2310.15823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed ElBakry, Mohamed Gabr, Muhammad ElNokrashy, Badr AlKhamissi</li>
<li>for: 本文旨在提供一种用于解决阿拉伯语”tip-of-the-tongue”现象的词典查询工具。</li>
<li>methods: 本文使用了一个 ensemble of 阿拉伯语BERT模型，通过对定义进行预处理并使用 average 方法生成word embedding。</li>
<li>results: 本文在两个子任务中都达到了最高分，并且在两个子任务中的结果表明了 ensemble 方法的可靠性和稳定性。<details>
<summary>Abstract</summary>
A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the "Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each model within the ensemble. In contrast, the most effective solution for the second subtask involves translating the English test definitions into Arabic and applying them to the finetuned models originally trained for the first subtask. This straightforward method achieves the highest score across both subtasks.
</details>
<details>
<summary>摘要</summary>
一个反字典是一种工具，允许用户根据提供的定义、含义或描述找到一个词。这种技术在不同的场景中具有很大的价值，帮助语言学习者只有word的描述而没有其标识，也有助于作家寻找精准的术语。这些场景通常被称为“舌尖上的灵感”（Tip-of-the-Tongue，TOT）现象。在这个工作中，我们介绍了我们对阿拉伯语反字典共享任务的赢利解决方案。这个任务的目标是从阿拉伯语定义中提取一个阿拉伯语词的向量表示。共享任务包括两个不同的子任务：第一个使用阿拉伯语定义作为输入，第二个使用英语定义。对于第一个子任务，我们的方法是使用预处理的阿拉伯语BERT模型 ensemble，将给定定义生成word embedding。最终的表示是通过ensemble中每个模型的输出均值来获得。相反，对于第二个子任务，我们发现简单地将英语测试定义翻译成阿拉伯语，然后将其应用于原始预处理的阿拉伯语BERT模型中得到了最高分。
</details></li>
</ul>
<hr>
<h2 id="Discriminator-Guidance-for-Autoregressive-Diffusion-Models"><a href="#Discriminator-Guidance-for-Autoregressive-Diffusion-Models" class="headerlink" title="Discriminator Guidance for Autoregressive Diffusion Models"></a>Discriminator Guidance for Autoregressive Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15817">http://arxiv.org/abs/2310.15817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filip Ekström Kelvinius, Fredrik Lindsten</li>
<li>for: 这个论文是用来描述如何使用抑制器导向抽象扩散模型进行生成的。</li>
<li>methods: 这个论文使用了一个抑制器来导向一个抽象扩散过程，以前这种方法已经在连续扩散模型中使用过。这里的作者 derive了在离散 caso 中使用抑制器和预训练生成模型的方法。</li>
<li>results: 作者通过对分子图生成任务进行测试，发现使用抑制器可以提高生成性能。<details>
<summary>Abstract</summary>
We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discrimiator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.
</details>
<details>
<summary>摘要</summary>
我们介绍了推导者导向的抽象扩散模型。在这篇文章中，我们 derivates了使用推导者与预训生成模型在碎散 случа的使用方法。首先，我们显示了使用最佳推导者可以正确地训练预训生成模型，并允许精准地抽取背景数据分布中的样本。其次，为了考虑实际情况中使用不Optimal推导者的情况，我们 derivates了一个序列 Монте卡洛 Algorithms，它在生成过程中逐步地考虑推导者的预测结果。我们在生成分子图的任务上试用了这些方法，并证明了推导者可以提高生成性能。
</details></li>
</ul>
<hr>
<h2 id="DALE-Generative-Data-Augmentation-for-Low-Resource-Legal-NLP"><a href="#DALE-Generative-Data-Augmentation-for-Low-Resource-Legal-NLP" class="headerlink" title="DALE: Generative Data Augmentation for Low-Resource Legal NLP"></a>DALE: Generative Data Augmentation for Low-Resource Legal NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15799">http://arxiv.org/abs/2310.15799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sreyan88/dale">https://github.com/sreyan88/dale</a></li>
<li>paper_authors: Sreyan Ghosh, Chandra Kiran Evuru, Sonal Kumar, S Ramaneswaran, S Sakshi, Utkarsh Tyagi, Dinesh Manocha</li>
<li>for: 提供了一个新的、有效的生成数据增强框架，用于低资源法律自然语言处理（Legal NLP）。</li>
<li>methods: 使用Encoder-Decoder语言模型，在 selective masking 基础上进行预训练，以获得法律语言特有的知识和语言使用表达。</li>
<li>results: 在13个数据集和6个任务中，DALE对低资源法律自然语言处理任务进行了有效的增强，与基线相比，DALE的表现提高1%-50%。<details>
<summary>Abstract</summary>
We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel unsupervised text denoising objective based on selective masking - our masking strategy exploits the domain-specific language characteristics of templatized legal documents to mask collocated spans of text. Denoising these spans helps DALE acquire knowledge about legal concepts, principles, and language usage. Consequently, it develops the ability to generate coherent and diverse augmentations with novel contexts. Finally, DALE performs conditional generation to generate synthetic augmentations for low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13 datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our baselines, including LLMs, qualitatively and quantitatively, with improvements of 1%-50%.
</details>
<details>
<summary>摘要</summary>
我们介绍DALE，一种新的有效的生成数据扩展框架，用于低资源法律自然语言处理（Legal NLP）。DALE解决了现有框架对法律文档生成有效数据扩展的挑战，因为法律语言具有特殊词汇和复杂的语法、语义和语法结构，而且不能仅通过重句来生成有效的数据扩展。为了解决这个问题，DALE采用了一个 Encoder-Decoder 语言模型，并在其前置训练时使用一种新的无监督文本净化目标函数，基于选择性遮盖。我们的遮盖策略利用了特定领域的法律文档语言特征， selectively 遮盖了协调的文本段。这种净化帮助DALE获得了法律概念、原则和语言使用的知识。因此，DALE可以生成具有新Context的coherent和多样化的扩展。最后，DALE进行了conditional生成，为低资源法律 NLP 任务生成了 sintetic的扩展。我们在13个数据集和6个任务上demonstrate了DALE的效果，与基线相比，DALE表现出1%-50%的提升。
</details></li>
</ul>
<hr>
<h2 id="Random-Entity-Quantization-for-Parameter-Efficient-Compositional-Knowledge-Graph-Representation"><a href="#Random-Entity-Quantization-for-Parameter-Efficient-Compositional-Knowledge-Graph-Representation" class="headerlink" title="Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation"></a>Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15797">http://arxiv.org/abs/2310.15797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaangl/randomquantization">https://github.com/jiaangl/randomquantization</a></li>
<li>paper_authors: Jiaang Li, Quan Wang, Yi Liu, Licheng Zhang, Zhendong Mao</li>
<li>for: 这篇论文关注在知识图（KG）表示学习中的实体表示问题，尤其是现有的KG Embedding（KGE）方法在扩展性上存在挑战。</li>
<li>methods: 该论文提出了一种新的方法，即随机实体量化，即对实体进行随机分配小型编码图表。</li>
<li>results: 研究发现，随机实体量化可以达到类似于现有策略的效果，并且通过分析 entropy 和 Jaccard 距离，解释了这种现象。<details>
<summary>Abstract</summary>
Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents entities by composing entity-corresponding codewords matched from predefined small-scale codebooks. We refer to the process of obtaining corresponding codewords of each entity as entity quantization, for which previous works have designed complicated strategies. Surprisingly, this paper shows that simple random entity quantization can achieve similar results to current strategies. We analyze this phenomenon and reveal that entity codes, the quantization outcomes for expressing entities, have higher entropy at the code level and Jaccard distance at the codeword level under random entity quantization. Therefore, different entities become more easily distinguished, facilitating effective KG representation. The above results show that current quantization strategies are not critical for KG representation, and there is still room for improvement in entity distinguishability beyond current strategies. The code to reproduce our results is available at https://github.com/JiaangL/RandomQuantization.
</details>
<details>
<summary>摘要</summary>
知识图（KG）下 Representation Learning 是下游任务的关键。目前主流方法是知识图嵌入（KGE），它使用独立的向量表示实体，但面临缩放挑战。近期研究提出了一种减少参数的方法，即使用预定的小规模码表来表示实体。我们称之为实体量化，之前的工作已经设计了复杂的策略。很奇怪的是，这篇论文显示了Random Entity Quantization可以达到类似的效果。我们分析了这个现象，发现实体代码，即表示实体的量化结果，在Random Entity Quantization中具有更高的熵度和Jacard距离。因此，不同的实体更容易区分，从而实现有效的KG表示。上述结果表明，当前的量化策略并不是KG表示的关键因素，还有可能提高实体区分性的空间。代码可以在https://github.com/JiaangL/RandomQuantization中找到。
</details></li>
</ul>
<hr>
<h2 id="Improving-generalization-in-large-language-models-by-learning-prefix-subspaces"><a href="#Improving-generalization-in-large-language-models-by-learning-prefix-subspaces" class="headerlink" title="Improving generalization in large language models by learning prefix subspaces"></a>Improving generalization in large language models by learning prefix subspaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15793">http://arxiv.org/abs/2310.15793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Falissard, Vincent Guigue, Laure Soulier</li>
<li>for: 这篇论文主要关注大语言模型（LLMs）在缺乏数据的情况下（也称为“几个样本”学习设定）进行精细调整。</li>
<li>methods: 该论文提出了一种基于神经网络空间的方法，用于提高LLMs的通用能力。这种优化方法，原来在计算机视觉领域中引入，通过同时优化参数空间中的整个简单кс（simplex）来提高模型的通用能力。然而，将这种方法应用于大型、预训练的变换器 pose 些挑战，主要是由于它们具有大量参数和固定参数初始化方法。我们表明，“参数效率微调”（PEFT）方法是与这种原始方法完全兼容的，并提议在参数空间中学习整个简单кс中的连续前缀。</li>
<li>results: 我们在一个基于GLUE bencmark的变换器上进行了测试，并显示了我们的贡献对于“几个样本”学习设定下的平均性能具有提高。实现可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/Liloulou/prefix_subspace">https://github.com/Liloulou/prefix_subspace</a><details>
<summary>Abstract</summary>
This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as the "few-shot" learning setting). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Its adaptation to massive, pretrained transformers, however, poses some challenges. First, their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT) methods, however, are perfectly compatible with this original approach, and propose to learn entire simplex of continuous prefixes. We test our method on a variant of the GLUE benchmark adapted to the few-shot learning setting, and show that both our contributions jointly lead to a gain in average performances compared to sota methods. The implementation can be found at the following link: https://github.com/Liloulou/prefix_subspace
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇文章关注大型语言模型（LLMs）在缺乏数据情况下（也称为“几个样本学习”）的细化。我们提出一种方法，通过神经网络子空间来增强LLMs的泛化能力。这种优化方法，在计算机视觉领域已经引入，目的是通过 JOINT 优化整个参数空间中的模型，以提高模型的泛化性。但是对于大型预训练转换器来说，它们的许多参数和确定性参数初始化方式使得它们不适合原始的子空间方法。我们在这篇文章中显示，使用 "Parameter Efficient Fine-Tuning"（PEFT）方法可以与原始方法相结合，并提议学习整个继承refix的连续前缀。我们在一个基于 GLUE benchmark 的修改版本上测试了我们的方法，并显示了我们的贡献的共同作用导致了与 SOTA 方法相比的平均性能提高。实现可以在以下链接中找到：<https://github.com/Liloulou/prefix_subspace>
</details></li>
</ul>
<hr>
<h2 id="Guiding-LLM-to-Fool-Itself-Automatically-Manipulating-Machine-Reading-Comprehension-Shortcut-Triggers"><a href="#Guiding-LLM-to-Fool-Itself-Automatically-Manipulating-Machine-Reading-Comprehension-Shortcut-Triggers" class="headerlink" title="Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers"></a>Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18360">http://arxiv.org/abs/2310.18360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mosh0110/guiding-llm">https://github.com/mosh0110/guiding-llm</a></li>
<li>paper_authors: Mosh Levy, Shauli Ravfogel, Yoav Goldberg</li>
<li>for: 这篇论文探讨了LLMs在机器阅读理解（MRC）系统中的应用，以及短cuts Mechanisms在其可靠性上的影响。</li>
<li>methods: 作者分析了LLMs作为编辑和读者两个角度，并提出了一个框架来引导编辑添加可能的短cut triggers。使用GPT4作为编辑，研究发现GPT4可以成功地编辑出Trigger短cut。</li>
<li>results: 研究发现，即使使用 capable LLMs，也可以使用短cut知识来欺骗LLMs。另外，发现GPT4可以被自己的编辑（15%的F1下降）。这些发现表明LLMs在短cut manipulate下存在潜在的漏洞。作者发布了ShortcutQA数据集，供未来研究使用。<details>
<summary>Abstract</summary>
Recent applications of LLMs in Machine Reading Comprehension (MRC) systems have shown impressive results, but the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability. We analyze the problem from two angles: LLMs as editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer questions based on the edited text. We introduce a framework that guides an editor to add potential shortcuts-triggers to samples. Using GPT4 as the editor, we find it can successfully edit trigger shortcut in samples that fool LLMs. Analysing LLMs as readers, we observe that even capable LLMs can be deceived using shortcut knowledge. Strikingly, we discover that GPT4 can be deceived by its own edits (15% drop in F1). Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a curated dataset generated by our framework for future research.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近，大语言模型（LLMs）在机器阅读理解（MRC）系统中的应用有所成就，但使用快捷途径，基于true标签的特征相关性，被视为可能的可靠性问题。我们从两个角度分析问题：LLMs作为编辑，受导向编辑文本以诱导LLMs;以及LLMs作为读者，根据编辑后的文本回答问题。我们提出了一个框架，帮助编辑添加可能的快捷途径触发器到样本中。使用GPT4作为编辑，我们发现它可以成功地编辑诱导LLMs的快捷途径。我们分析LLMs作为读者时，发现即使有能力的LLMs也可以被使用快捷途径知识欺骗。另外，我们发现GPT4可以被自己的编辑（15%下降）欺骗。我们的发现表明LLMs具有快捷途径 manipulate的潜在弱点。我们发布了ShortcutQA数据集，用于未来的研究。
</details></li>
</ul>
<hr>
<h2 id="SequenceMatch-Revisiting-the-design-of-weak-strong-augmentations-for-Semi-supervised-learning"><a href="#SequenceMatch-Revisiting-the-design-of-weak-strong-augmentations-for-Semi-supervised-learning" class="headerlink" title="SequenceMatch: Revisiting the design of weak-strong augmentations for Semi-supervised learning"></a>SequenceMatch: Revisiting the design of weak-strong augmentations for Semi-supervised learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15787">http://arxiv.org/abs/2310.15787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khanh-Binh Nguyen</li>
<li>for: 本研究的目的是提出一种高效的 semi-supervised learning (SSL) 方法，以增强模型在半有标注数据上的训练。</li>
<li>methods: 本方法使用多种数据增强，其中包括一种中等增强，以减少模型对半有标注数据的过拟合。此外，本方法还定义了两种不同的一致性约束，以适应不同的预测情况。</li>
<li>results: 对于标准的 CIFAR-10&#x2F;100、SVHN 和 STL-10 测试集，SequenceMatch 减少了模型的训练时间和数据量，同时保持高度的准确率。此外，SequenceMatch 在 ImageNet 大规模测试集上也达到了新的州OF-THE-ART 水平，其error rate 为 38.46%。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) has become popular in recent years because it allows the training of a model using a large amount of unlabeled data. However, one issue that many SSL methods face is the confirmation bias, which occurs when the model is overfitted to the small labeled training dataset and produces overconfident, incorrect predictions. To address this issue, we propose SequenceMatch, an efficient SSL method that utilizes multiple data augmentations. The key element of SequenceMatch is the inclusion of a medium augmentation for unlabeled data. By taking advantage of different augmentations and the consistency constraints between each pair of augmented examples, SequenceMatch helps reduce the divergence between the prediction distribution of the model for weakly and strongly augmented examples. In addition, SequenceMatch defines two different consistency constraints for high and low-confidence predictions. As a result, SequenceMatch is more data-efficient than ReMixMatch, and more time-efficient than both ReMixMatch ($\times4$) and CoMatch ($\times2$) while having higher accuracy. Despite its simplicity, SequenceMatch consistently outperforms prior methods on standard benchmarks, such as CIFAR-10/100, SVHN, and STL-10. It also surpasses prior state-of-the-art methods by a large margin on large-scale datasets such as ImageNet, with a 38.46\% error rate. Code is available at https://github.com/beandkay/SequenceMatch.
</details>
<details>
<summary>摘要</summary>
隐supervised learning（SSL）在最近几年内变得越来越流行，因为它允许使用大量无标签数据来训练模型。然而，许多SSL方法面临的问题是确认偏见，这种情况下模型将被适应小标注训练集，并生成过自信的错误预测。为解决这个问题，我们提出了SequenceMatch方法，它利用多种数据增强。SequenceMatch的关键元素是将媒体增强加到无标签数据中。通过利用不同的增强和每对增强的约束相互之间的一致性，SequenceMatch可以减少模型预测结果的分化。此外，SequenceMatch还定义了高和低自信预测的两种一致约束。因此，SequenceMatch比RemixMatch更有效率，比RemixMatch（×4）和Comatch（×2）更快速，同时具有更高的准确率。尽管简单，SequenceMatch在标准标准benchmark上一直保持领先，包括CIFAR-10/100、SVHN和STL-10。它还在大规模数据集上超越了先前的状态rut bearing， erreur rate为38.46％。代码可以在https://github.com/beandkay/SequenceMatch上找到。
</details></li>
</ul>
<hr>
<h2 id="3D-Masked-Autoencoders-for-Enhanced-Privacy-in-MRI-Scans"><a href="#3D-Masked-Autoencoders-for-Enhanced-Privacy-in-MRI-Scans" class="headerlink" title="3D Masked Autoencoders for Enhanced Privacy in MRI Scans"></a>3D Masked Autoencoders for Enhanced Privacy in MRI Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15778">http://arxiv.org/abs/2310.15778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lennart Alexander Van der Goten, Kevin Smith</li>
<li>for: 防止MRI扫描数据泄露个人隐私信息</li>
<li>methods: 使用Masked Autoencoders和Generative Adversarial Networks（GAN）进行数据隐私处理</li>
<li>results: 提出CP-MAE模型，可以在下游任务中提高隐私处理性能，同时可以生成高分辨率($256^3$)的扫描数据 Synthesize<details>
<summary>Abstract</summary>
MRI scans provide valuable medical information, however they also contain sensitive and personally identifiable information (PII) that needs to be protected. Whereas MRI metadata is easily sanitized, MRI image data is a privacy risk because it contains information to render highly-realistic 3D visualizations of a patient's head, enabling malicious actors to possibly identify the subject by cross-referencing a database. Data anonymization and de-identification is concerned with ensuring the privacy and confidentiality of individuals' personal information. Traditional MRI de-identification methods remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This comes at the expense of introducing a domain shift that can throw off downstream analyses. Recently, a GAN-based approach was proposed to de-identify a patient's scan by remodeling it (e.g. changing the face) rather than by removing parts. In this work, we propose CP-MAE, a model that de-identifies the face using masked autoencoders and that outperforms all previous approaches in terms of downstream task performance as well as de-identification. With our method we are able to synthesize scans of resolution up to $256^3$ (previously 128 cubic) which constitutes an eight-fold increase in the number of voxels. Using our construction we were able to design a system that exhibits a highly robust training stage, making it easy to fit the network on novel data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MindLLM-Pre-training-Lightweight-Large-Language-Model-from-Scratch-Evaluations-and-Domain-Applications"><a href="#MindLLM-Pre-training-Lightweight-Large-Language-Model-from-Scratch-Evaluations-and-Domain-Applications" class="headerlink" title="MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications"></a>MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15777">http://arxiv.org/abs/2310.15777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang Liu, Heyan Huang, Yang Gao</li>
<li>for: 这 paper 的目的是开发轻量级大语言模型，以提高模型的效率和可扩展性，并应对训练和部署大模型的高成本和资源短缺。</li>
<li>methods: 这 paper 使用了自scratch 训练的双语大语言模型 MindLLM，并提供了 1.3 亿和 3 亿参数的模型。文章还介绍了数据构造、模型体系、评估和应用等方面的经验。</li>
<li>results: MindLLM 可以与其他更大的开源模型在一些公共Benchmark上匹配或超越其表现，而且文章还提出了一种针对更小的模型进行调整 instrucion 的框架，以提高其能力。此外，文章还探讨了 MindLLM 在法律和金融等特定领域的应用。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the performance of other open-source larger models on some public benchmarks. We also introduce an innovative instruction tuning framework tailored for smaller models to enhance their capabilities efficiently. Moreover, we explore the application of MindLLM in specific vertical domains such as law and finance, underscoring the agility and adaptability of our lightweight models.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经展示出惊人的性能， marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the performance of other open-source larger models on some public benchmarks. We also introduce an innovative instruction tuning framework tailored for smaller models to enhance their capabilities efficiently. Moreover, we explore the application of MindLLM in specific vertical domains such as law and finance, underscoring the agility and adaptability of our lightweight models.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Causal-Understanding-of-Why-Users-Share-Hate-Speech-on-Social-Media"><a href="#Causal-Understanding-of-Why-Users-Share-Hate-Speech-on-Social-Media" class="headerlink" title="Causal Understanding of Why Users Share Hate Speech on Social Media"></a>Causal Understanding of Why Users Share Hate Speech on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15772">http://arxiv.org/abs/2310.15772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominique Geissler, Abdurahman Maarouf, Stefan Feuerriegel</li>
<li>for: 这篇论文旨在探讨用户在社交媒体上分享仇恨言论的原因，以及如何预测和防止这种情况。</li>
<li>methods: 这篇论文采用了一种新的三步 causal 框架，包括减少选择性的偏见、使用倒排比例分数模型 latent 感知用户对仇恨言论的抵触程度，以及基于这些感知值模型 causal 效应。</li>
<li>results: 研究发现，有 fewer followers、fewer friends 和 fewer posts 的用户更容易分享仇恨言论，而 younger accounts 则更 unlikely to do so。这些结论可以帮助检测可能会发生危害的用户，并设计有效的预防策略。<details>
<summary>Abstract</summary>
Hate speech on social media threatens the mental and physical well-being of individuals and is further responsible for real-world violence. An important driver behind the spread of hate speech and thus why hateful posts can go viral are reshares, yet little is known about why users reshare hate speech. In this paper, we present a comprehensive, causal analysis of the user attributes that make users reshare hate speech. However, causal inference from observational social media data is challenging, because such data likely suffer from selection bias, and there is further confounding due to differences in the vulnerability of users to hate speech. We develop a novel, three-step causal framework: (1) We debias the observational social media data by applying inverse propensity scoring. (2) We use the debiased propensity scores to model the latent vulnerability of users to hate speech as a latent embedding. (3) We model the causal effects of user attributes on users' probability of sharing hate speech, while controlling for the latent vulnerability of users to hate speech. Compared to existing baselines, a particular strength of our framework is that it models causal effects that are non-linear, yet still explainable. We find that users with fewer followers, fewer friends, and fewer posts share more hate speech. Younger accounts, in return, share less hate speech. Overall, understanding the factors that drive users to share hate speech is crucial for detecting individuals at risk of engaging in harmful behavior and for designing effective mitigation strategies.
</details>
<details>
<summary>摘要</summary>
仇恨言语在社交媒体上威胁个人的心理和身体健康，并导致实际暴力。散布仇恨言语的重要驱动力是重分享，然而现有少知为何用户会重分享仇恨言语。在这篇论文中，我们提供了一个完整的 causal 分析，探讨用户特征是如何导致他们重分享仇恨言语。然而，从观察社交媒体数据中进行 causal 推断是困难的，因为这些数据可能受到选择偏见的影响，并且存在用户敏感性差异，导致不同用户对仇恨言语的敏感程度不同。我们开发了一种新的三步 causal 框架：1. 我们使用反报分析来减少社交媒体数据的选择偏见。2. 我们使用减少后的报分数来模型用户对仇恨言语的敏感程度，并将其转换为一个隐藏的嵌入。3. 我们使用隐藏嵌入来模型用户特征对重分享仇恨言语的 causal 影响，同时控制用户对仇恨言语的敏感程度。相比现有基线，我们的框架的一个重要优势是可以模型非线性的 causal 效应，同时仍然可以解释。我们发现，分享仇恨言语的用户通常有 fewer followers、fewer friends 和 fewer posts，而年轻的帐户则更少分享仇恨言语。总之，理解用户分享仇恨言语的因素非常重要，以便检测可能产生危害行为的个人，并设计有效的缓解策略。
</details></li>
</ul>
<hr>
<h2 id="Debiasing-calibrating-and-improving-Semi-supervised-Learning-performance-via-simple-Ensemble-Projector"><a href="#Debiasing-calibrating-and-improving-Semi-supervised-Learning-performance-via-simple-Ensemble-Projector" class="headerlink" title="Debiasing, calibrating, and improving Semi-supervised Learning performance via simple Ensemble Projector"></a>Debiasing, calibrating, and improving Semi-supervised Learning performance via simple Ensemble Projector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15764">http://arxiv.org/abs/2310.15764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/beandkay/epass">https://github.com/beandkay/epass</a></li>
<li>paper_authors: Khanh-Binh Nguyen</li>
<li>for: 本研究旨在提高现有的对联训练 semi-supervised learning 框架的性能，而不是 introduce 更多的网络组件和训练过程。</li>
<li>methods: 该 paper 提出了一种简单的方法，名为 Ensemble Projectors Aided for Semi-supervised Learning (EPASS)，它是通过改进学习的嵌入来提高 semi-supervised learning 的性能。</li>
<li>results: EPASS 可以提高泛化、强化特征表示和提高性能，例如，在 SimMatch 和 CoMatch  datasets 上，EPASS 可以提高基eline 的 semi-supervised learning 性能 by 39.47%&#x2F;31.39%&#x2F;24.70% top-1 error rate，并且可以在不同的方法、网络架构和 dataset 上获得一致的提高。<details>
<summary>Abstract</summary>
Recent studies on semi-supervised learning (SSL) have achieved great success. Despite their promising performance, current state-of-the-art methods tend toward increasingly complex designs at the cost of introducing more network components and additional training procedures. In this paper, we propose a simple method named Ensemble Projectors Aided for Semi-supervised Learning (EPASS), which focuses mainly on improving the learned embeddings to boost the performance of the existing contrastive joint-training semi-supervised learning frameworks. Unlike standard methods, where the learned embeddings from one projector are stored in memory banks to be used with contrastive learning, EPASS stores the ensemble embeddings from multiple projectors in memory banks. As a result, EPASS improves generalization, strengthens feature representation, and boosts performance. For instance, EPASS improves strong baselines for semi-supervised learning by 39.47\%/31.39\%/24.70\% top-1 error rate, while using only 100k/1\%/10\% of labeled data for SimMatch, and achieves 40.24\%/32.64\%/25.90\% top-1 error rate for CoMatch on the ImageNet dataset. These improvements are consistent across methods, network architectures, and datasets, proving the general effectiveness of the proposed methods. Code is available at https://github.com/beandkay/EPASS.
</details>
<details>
<summary>摘要</summary>
最近的半监督学习（SSL）研究已经取得了很大的成功。虽然现有的状态之arteMethods tend to increasingly complex designs at the cost of introducing more network components and additional training procedures. 在这篇论文中，我们提出了一种简单的方法named Ensemble Projectors Aided for Semi-supervised Learning（EPASS），它主要是通过改进学习的嵌入来提高现有的对比训练半监督学习框架的性能。 unlike standard methods, where the learned embeddings from one projector are stored in memory banks to be used with contrastive learning, EPASS stores the ensemble embeddings from multiple projectors in memory banks. 因此，EPASS可以提高总体化，强化特征表示，并提高性能。例如，EPASS可以使用100k/1\%/10\%的标注数据对SimMatch进行训练，并实现了ImageNet数据集上的40.24\%/32.64\%/25.90\%的top-1错误率，而使用标注数据的数量和网络架构和数据集的变化，这些改进是一致的，证明了提posed方法的通用效果。代码可以在https://github.com/beandkay/EPASS中找到。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Language-Models-into-Direct-Speech-Translation-An-Inference-Time-Solution-to-Control-Gender-Inflection"><a href="#Integrating-Language-Models-into-Direct-Speech-Translation-An-Inference-Time-Solution-to-Control-Gender-Inflection" class="headerlink" title="Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection"></a>Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15752">http://arxiv.org/abs/2310.15752</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlt-mt/fbk-fairseq">https://github.com/hlt-mt/fbk-fairseq</a></li>
<li>paper_authors: Dennis Fucci, Marco Gaido, Sara Papi, Mauro Cettolo, Matteo Negri, Luisa Bentivogli</li>
<li>for: 这篇论文主要是为了提高语音翻译（ST）系统中的 speaker-related gender inflection 控制，以提高翻译的准确性和可靠性。</li>
<li>methods: 这篇论文提出了一种在运行时使用 external language model（LM）来控制 ST 系统中的 speaker-related gender inflection 的方法，而不需要对模型进行专门的重新训练。</li>
<li>results: 根据实验结果，这种方法可以在 en-&gt;es&#x2F;fr&#x2F;it 等语言对话中提高 gender accuracy 的表现，相比基eline模型和最佳训练时间缓解策略，可以提高31.0和1.6个点的准确率。此外，当 speaker 的 vocal trait 与 gender 存在冲突时，这种方法的提高还更加明显，可以提高32.0和3.4个点的准确率。<details>
<summary>Abstract</summary>
When translating words referring to the speaker, speech translation (ST) systems should not resort to default masculine generics nor rely on potentially misleading vocal traits. Rather, they should assign gender according to the speakers' preference. The existing solutions to do so, though effective, are hardly feasible in practice as they involve dedicated model re-training on gender-labeled ST data. To overcome these limitations, we propose the first inference-time solution to control speaker-related gender inflections in ST. Our approach partially replaces the (biased) internal language model (LM) implicitly learned by the ST decoder with gender-specific external LMs. Experiments on en->es/fr/it show that our solution outperforms the base models and the best training-time mitigation strategy by up to 31.0 and 1.6 points in gender accuracy, respectively, for feminine forms. The gains are even larger (up to 32.0 and 3.4) in the challenging condition where speakers' vocal traits conflict with their gender.
</details>
<details>
<summary>摘要</summary>
当翻译话语时，ST系统 shouldn't resort to default masculine generics 也不可以 rely on potentially misleading vocal traits。而是应该根据Speaker的偏好 assign gender。现有的解决方案，虽然有效，但在实践中困难可行，需要重新训练 gender-labeled ST 数据。为了缓解这些限制，我们提出了首个在推理时控制 speaker-related gender inflections的解决方案。我们的方法部分地将（偏见的）STdecoder内部的语言模型（LM）被换成 gender-specific external LMs。实验表明，我们的解决方案在 en->es/fr/it 等语言之间对比 base models 和最佳训练时mitigation strategy 的性能有31.0和1.6点的提升，即 gender accuracy 的提升。这些提升还更大（达到32.0和3.4）在 speakers' vocal traits 与 gender 之间的复杂情况下。
</details></li>
</ul>
<hr>
<h2 id="The-Hyperdimensional-Transform-a-Holographic-Representation-of-Functions"><a href="#The-Hyperdimensional-Transform-a-Holographic-Representation-of-Functions" class="headerlink" title="The Hyperdimensional Transform: a Holographic Representation of Functions"></a>The Hyperdimensional Transform: a Holographic Representation of Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16065">http://arxiv.org/abs/2310.16065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pieter Dewulf, Michiel Stock, Bernard De Baets</li>
<li>For: The paper introduces a new type of integral transform called the hyperdimensional transform, which maps square-integrable functions into noise-robust, holographic, high-dimensional representations called hyperdimensional vectors.* Methods: The paper uses a set of stochastic, orthogonal basis functions to approximate a function as a linear combination of random functions, and defines the hyperdimensional transform and its inverse.* Results: The paper discusses general transform-related properties such as uniqueness, approximation properties of the inverse transform, and the representation of integrals and derivatives, and provides straightforward and easily understandable code for computing the transform and solving differential equations.<details>
<summary>Abstract</summary>
Integral transforms are invaluable mathematical tools to map functions into spaces where they are easier to characterize. We introduce the hyperdimensional transform as a new kind of integral transform. It converts square-integrable functions into noise-robust, holographic, high-dimensional representations called hyperdimensional vectors. The central idea is to approximate a function by a linear combination of random functions. We formally introduce a set of stochastic, orthogonal basis functions and define the hyperdimensional transform and its inverse. We discuss general transform-related properties such as its uniqueness, approximation properties of the inverse transform, and the representation of integrals and derivatives. The hyperdimensional transform offers a powerful, flexible framework that connects closely with other integral transforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it provides theoretical foundations and new insights for the field of hyperdimensional computing, a computing paradigm that is rapidly gaining attention for efficient and explainable machine learning algorithms, with potential applications in statistical modelling and machine learning. In addition, we provide straightforward and easily understandable code, which can function as a tutorial and allows for the reproduction of the demonstrated examples, from computing the transform to solving differential equations.
</details>
<details>
<summary>摘要</summary>
Integral transforms are incredibly useful mathematical tools for mapping functions into spaces where they are easier to characterize. We introduce the hyperdimensional transform as a new type of integral transform. It converts square-integrable functions into noise-robust, holographic, high-dimensional representations called hyperdimensional vectors. The central idea is to approximate a function by a linear combination of random functions. We formally introduce a set of stochastic, orthogonal basis functions and define the hyperdimensional transform and its inverse. We discuss general transform-related properties such as its uniqueness, approximation properties of the inverse transform, and the representation of integrals and derivatives. The hyperdimensional transform offers a powerful, flexible framework that connects closely with other integral transforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it provides theoretical foundations and new insights for the field of hyperdimensional computing, a computing paradigm that is rapidly gaining attention for efficient and explainable machine learning algorithms, with potential applications in statistical modeling and machine learning. In addition, we provide straightforward and easily understandable code, which can function as a tutorial and allows for the reproduction of the demonstrated examples, from computing the transform to solving differential equations.Here's the translation in Traditional Chinese:Integral transforms are incredibly useful mathematical tools for mapping functions into spaces where they are easier to characterize. We introduce the hyperdimensional transform as a new type of integral transform. It converts square-integrable functions into noise-robust, holographic, high-dimensional representations called hyperdimensional vectors. The central idea is to approximate a function by a linear combination of random functions. We formally introduce a set of stochastic, orthogonal basis functions and define the hyperdimensional transform and its inverse. We discuss general transform-related properties such as its uniqueness, approximation properties of the inverse transform, and the representation of integrals and derivatives. The hyperdimensional transform offers a powerful, flexible framework that connects closely with other integral transforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it provides theoretical foundations and new insights for the field of hyperdimensional computing, a computing paradigm that is rapidly gaining attention for efficient and explainable machine learning algorithms, with potential applications in statistical modeling and machine learning. In addition, we provide straightforward and easily understandable code, which can function as a tutorial and allows for the reproduction of the demonstrated examples, from computing the transform to solving differential equations.
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Linear-Transformers"><a href="#Recurrent-Linear-Transformers" class="headerlink" title="Recurrent Linear Transformers"></a>Recurrent Linear Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15719">http://arxiv.org/abs/2310.15719</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subho406/Recurrent-Linear-Transformers">https://github.com/subho406/Recurrent-Linear-Transformers</a></li>
<li>paper_authors: Subhojeet Pramanik, Esraa Elelimy, Marlos C. Machado, Adam White</li>
<li>for: 本文目的是提出一种基于循环自注意力机制的 alternating attention 机制，以提高 transformer 模型在 reinforcement learning  задача中的应用可行性。</li>
<li>methods: 本文使用了一种基于循环自注意力机制的 alternating attention 机制，该机制可以独立地进行启发成本计算，同时能够有效地利用长距离依赖关系。</li>
<li>results: 对于 reinforcement learning 问题，本文的方法与 state-of-the-art 方法 GTrXL 相比，推理成本下降至少 40%，并且减少了内存使用量超过 50%。此外，本文的方法在 harder tasks 上比 GTrXL 表现更好，提高了更多于 37%。<details>
<summary>Abstract</summary>
The self-attention mechanism in the transformer architecture is capable of capturing long-range dependencies and it is the main reason behind its effectiveness in processing sequential data. Nevertheless, despite their success, transformers have two significant drawbacks that still limit their broader applicability: (1) In order to remember past information, the self-attention mechanism requires access to the whole history to be provided as context. (2) The inference cost in transformers is expensive. In this paper we introduce recurrent alternatives to the transformer self-attention mechanism that offer a context-independent inference cost, leverage long-range dependencies effectively, and perform well in practice. We evaluate our approaches in reinforcement learning problems where the aforementioned computational limitations make the application of transformers nearly infeasible. We quantify the impact of the different components of our architecture in a diagnostic environment and assess performance gains in 2D and 3D pixel-based partially-observable environments. When compared to a state-of-the-art architecture, GTrXL, inference in our approach is at least 40% cheaper while reducing memory use in more than 50%. Our approach either performs similarly or better than GTrXL, improving more than 37% upon GTrXL performance on harder tasks.
</details>
<details>
<summary>摘要</summary>
transformer 架构中的自注意机制可以 capture 长距离依赖关系，这是它处理序列数据的效果的主要原因。然而，尽管它们成功， transformer 还有两个主要的缺点，这些缺点还限制了它们的更广泛应用：1. 为了记忆过去的信息，自注意机制需要提供全部历史作为 Context。2. transformer 的推理成本贵。在这篇论文中，我们介绍了 transformer 自注意机制的循环alternative，这些alternative可以提供context-independent的推理成本，高效地利用长距离依赖关系，并在实践中表现良好。我们在 reinforcement learning 问题中评估了我们的方法，这些问题的计算限制使 transformer 在实际应用中几乎无法实现。我们评估了不同组件的影响，并在二维和三维像素基于部分可见环境中评估性能。与 state-of-the-art 架构 GTrXL 相比，我们的方法的推理成本至少减少了40%，同时减少了内存使用量超过50%。我们的方法在更难的任务上表现相对或更好于 GTrXL，提高了More than 37% 的 GTrXL 性能。
</details></li>
</ul>
<hr>
<h2 id="Solving-large-flexible-job-shop-scheduling-instances-by-generating-a-diverse-set-of-scheduling-policies-with-deep-reinforcement-learning"><a href="#Solving-large-flexible-job-shop-scheduling-instances-by-generating-a-diverse-set-of-scheduling-policies-with-deep-reinforcement-learning" class="headerlink" title="Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning"></a>Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15706">http://arxiv.org/abs/2310.15706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Imanol Echeverria, Maialen Murua, Roberto Santana</li>
<li>for: 提出了一种方法可以针对大规模的Job Shop优化调度问题（FJSSP）。</li>
<li>methods: 使用图ael Neural Networks（GNN）模型FJSSP，并提出了两种方法来提高推理的稳定性：生成多个调度策略并限制其使用派单规则（DRs）。</li>
<li>results: 对于大规模的FJSSP实例，该方法比派单规则和三种 latest deep reinforcement learning方法获得更好的结果。<details>
<summary>Abstract</summary>
The Flexible Job Shop Scheduling Problem (FJSSP) has been extensively studied in the literature, and multiple approaches have been proposed within the heuristic, exact, and metaheuristic methods. However, the industry's demand to be able to respond in real-time to disruptive events has generated the necessity to be able to generate new schedules within a few seconds. Among these methods, under this constraint, only dispatching rules (DRs) are capable of generating schedules, even though their quality can be improved. To improve the results, recent methods have been proposed for modeling the FJSSP as a Markov Decision Process (MDP) and employing reinforcement learning to create a policy that generates an optimal solution assigning operations to machines. Nonetheless, there is still room for improvement, particularly in the larger FJSSP instances which are common in real-world scenarios. Therefore, the objective of this paper is to propose a method capable of robustly solving large instances of the FJSSP. To achieve this, we propose a novel way of modeling the FJSSP as an MDP using graph neural networks. We also present two methods to make inference more robust: generating a diverse set of scheduling policies that can be parallelized and limiting them using DRs. We have tested our approach on synthetically generated instances and various public benchmarks and found that our approach outperforms dispatching rules and achieves better results than three other recent deep reinforcement learning methods on larger FJSSP instances.
</details>
<details>
<summary>摘要</summary>
flexible job shop scheduling problem (FJSSP) 已经在文献中得到了广泛的研究，多种方法在迭代、精确和元迭代方法中提出了多种方法。然而，业务需要在实时应对突发事件，因此需要在几秒钟内生成新的计划。在这些方法中，只有派送规则（DRs）可以生成计划，尽管它们的质量可以进行改进。为了改进结果，latest methods 把 FJSSP 模型为Markov Decision Process (MDP)，并使用 reinforcement learning 创建一个政策，以确定操作分配给机器。然而，在更大的 FJSSP 实例中仍然有很多的空间进行改进。因此，本文的目标是提出一种可以坚定地解决大型 FJSSP 实例的方法。为了实现这一目标，我们提出了一种新的 MDP 模型使用图 neural network。我们还提出了两种方法来使推理更加稳定：生成多种调度策略，并使用 DRs 限制它们。我们对 synthetically generated instances 和多个公共benchmark 进行了测试，并发现我们的方法在更大的 FJSSP 实例中比派送规则和三种最新的深度学习方法更好的结果。
</details></li>
</ul>
<hr>
<h2 id="Learning-based-Scheduling-for-Information-Accuracy-and-Freshness-in-Wireless-Networks"><a href="#Learning-based-Scheduling-for-Information-Accuracy-and-Freshness-in-Wireless-Networks" class="headerlink" title="Learning-based Scheduling for Information Accuracy and Freshness in Wireless Networks"></a>Learning-based Scheduling for Information Accuracy and Freshness in Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15705">http://arxiv.org/abs/2310.15705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hitesh Gudwani</li>
<li>for: 本文关注一个多源单渠道监测系统，其中每个源都测量时间变化的量，但是这些量的准确性和传输成功率都不确定。</li>
<li>methods: 作者使用多臂抽奖问题的变体来模型调度问题，并对四种标准抽奖策略进行比较：ETC、$\epsilon$-贪婪、UCB和TS。此外，作者还提供了ETC和$\epsilon$-贪婪策略的分析保证。</li>
<li>results: 作者通过 simulations 和分析来证明，ETC 策略和 $\epsilon$-贪婪策略在该系统中表现最佳，而 UCB 和 TS 策略则表现较差。此外，作者还提供了系统的下界性能。<details>
<summary>Abstract</summary>
We consider a system of multiple sources, a single communication channel, and a single monitoring station. Each source measures a time-varying quantity with varying levels of accuracy and one of them sends its update to the monitoring station via the channel. The probability of success of each attempted communication is a function of the source scheduled for transmitting its update. Both the probability of correct measurement and the probability of successful transmission of all the sources are unknown to the scheduler. The metric of interest is the reward received by the system which depends on the accuracy of the last update received by the destination and the Age-of-Information (AoI) of the system. We model our scheduling problem as a variant of the multi-arm bandit problem with sources as different arms. We compare the performance of all $4$ standard bandit policies, namely, ETC, $\epsilon$-greedy, UCB, and TS suitably adjusted to our system model via simulations. In addition, we provide analytical guarantees of $2$ of these policies, ETC, and $\epsilon$-greedy. Finally, we characterize the lower bound on the cumulative regret achievable by any policy.
</details>
<details>
<summary>摘要</summary>
我们考虑一个多源多通道单一监控站的系统。每个源都会测量一个时间 varying 的量，并且其中一个source会将更新发送到监控站 via 通道。各 source 的尝试通信成功率是对应的 schedule 的函数。各源的正确测量和通信成功率都是未知的。我们的评估问题是一种多臂钟点问题，其中每个臂是各个源。我们通过实验和分析 guarantees 评估不同的政策的表现。特别是，我们评估了4种标准钟点策略， namely ETC、 $\epsilon $-greedy、UCB 和 TS，并且提供了适当的适配。此外，我们还提供了任何政策的下限 regret 可以实现的Lower bound。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automated-Recipe-Genre-Classification-using-Semi-Supervised-Learning"><a href="#Towards-Automated-Recipe-Genre-Classification-using-Semi-Supervised-Learning" class="headerlink" title="Towards Automated Recipe Genre Classification using Semi-Supervised Learning"></a>Towards Automated Recipe Genre Classification using Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15693">http://arxiv.org/abs/2310.15693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazmus Sakib, G. M. Shahariar, Md. Mohsinul Kabir, Md. Kamrul Hasan, Hasan Mahmud</li>
<li>for: 这个研究的目的是提供一个大规模的Cooking recipe dataset，以便对食谱进行分类和识别。</li>
<li>methods: 这个研究使用了两种Named Entity Recognition（NER）提取工具，并将食谱描述中的各种名称都EXTRACTED出来。</li>
<li>results: 研究结果显示，使用传统机器学习、深度学习和预训练语言模型可以对食谱进行分类，并达到了98.6%的总准确率。 investigate发现，标题特征在分类种类中发挥了更重要的作用。<details>
<summary>Abstract</summary>
Sharing cooking recipes is a great way to exchange culinary ideas and provide instructions for food preparation. However, categorizing raw recipes found online into appropriate food genres can be challenging due to a lack of adequate labeled data. In this study, we present a dataset named the ``Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking Recipe Dataset" that contains two million culinary recipes labeled in respective categories with extended named entities extracted from recipe descriptions. This collection of data includes various features such as title, NER, directions, and extended NER, as well as nine different labels representing genres including bakery, drinks, non-veg, vegetables, fast food, cereals, meals, sides, and fusions. The proposed pipeline named 3A2M+ extends the size of the Named Entity Recognition (NER) list to address missing named entities like heat, time or process from the recipe directions using two NER extraction tools. 3A2M+ dataset provides a comprehensive solution to the various challenging recipe-related tasks, including classification, named entity recognition, and recipe generation. Furthermore, we have demonstrated traditional machine learning, deep learning and pre-trained language models to classify the recipes into their corresponding genre and achieved an overall accuracy of 98.6\%. Our investigation indicates that the title feature played a more significant role in classifying the genre.
</details>
<details>
<summary>摘要</summary>
共享cooking recipe是一个好方式来交换culinary ideas和提供食物预制 instrucciones。然而，将raw recipefound online分类到合适的食物种类中可以是一个挑战，因为缺乏足够的标注数据。在这项研究中，我们提出了名为“Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking Recipe Dataset”的数据集，其包含2000万个culinary recipe，每个recipe都被标注到相应的食物种类中，并且包含了多种特征，如标题、NER、 instrucciones、扩展NER等，以及九个不同的标签，表示不同的食物种类，包括烘焙、饮料、非肉食、蔬菜、快餐、粮食、主食、侧菜和混合。我们提出的管道名为3A2M+，通过两个NER提取工具来扩展recipe directions中缺失的名称实体。3A2M+数据集提供了一个完整的解决方案 для多种挑战的recipe-related任务，包括分类、名称实体识别和recipe生成。此外，我们使用传统机器学习、深度学习和预训练语言模型来分类recipes，并达到了98.6%的总准确率。我们的调查表明，标题特征在分类种类中发挥了更重要的作用。
</details></li>
</ul>
<hr>
<h2 id="Improving-Biomedical-Abstractive-Summarisation-with-Knowledge-Aggregation-from-Citation-Papers"><a href="#Improving-Biomedical-Abstractive-Summarisation-with-Knowledge-Aggregation-from-Citation-Papers" class="headerlink" title="Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers"></a>Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15684">http://arxiv.org/abs/2310.15684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tangg555/biomed-sum">https://github.com/tangg555/biomed-sum</a></li>
<li>paper_authors: Chen Tang, Shun Wang, Tomas Goldsack, Chenghua Lin</li>
<li>for: 本研究旨在提高生物医学文献摘要的语言模型性能，通过 integrate 文献引用中的域专业知识。</li>
<li>methods: 我们提出了一种新的注意力机制，用于将域专业知识集成到语言模型中，以便通过文献内容和相关知识来生成摘要。</li>
<li>results: 我们的模型在生物医学摘要 tasks 上表现出优秀的result，至于比基eline方法有substantial 的提升。<details>
<summary>Abstract</summary>
Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensive experiments demonstrate that our model outperforms state-of-the-art approaches and achieves substantial improvements in abstractive biomedical text summarisation.
</details>
<details>
<summary>摘要</summary>
摘要抽象从生物医学文献中获得特有的领域特有特征，包括专业性语言风格和生物医学术语，这使得现有语言模型在生物医学抽象摘要中表现不佳，因为缺乏领域专业知识。这篇论文旨在提高生物医学抽象摘要中语言模型的表现，通过收集外部文献中的参照纸引入领域专业知识。我们提议一种注意力机制的参照纸聚合模型，使得神经网络可以通过文献内容和相关知识来生成摘要。此外，我们构建了大规模的生物医学摘要数据集，作为我们的研究基础。广泛的实验表明，我们的模型在生物医学抽象摘要方面表现出色，与现有方法相比，得到了显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Recent-Advances-in-Multi-modal-3D-Scene-Understanding-A-Comprehensive-Survey-and-Evaluation"><a href="#Recent-Advances-in-Multi-modal-3D-Scene-Understanding-A-Comprehensive-Survey-and-Evaluation" class="headerlink" title="Recent Advances in Multi-modal 3D Scene Understanding: A Comprehensive Survey and Evaluation"></a>Recent Advances in Multi-modal 3D Scene Understanding: A Comprehensive Survey and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15676">http://arxiv.org/abs/2310.15676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinjie Lei, Zixuan Wang, Feng Chen, Guoqing Wang, Peng Wang, Yang Yang</li>
<li>for: 这篇论文主要是为了探讨多Modal 3D场景理解的最新进展和发展趋势。</li>
<li>methods: 该论文使用了多种多Modal 3D方法，包括3D+2D多camera图像和3D+语言文本描述。</li>
<li>results: 论文对多Modal 3D方法的比较和分析，并提供了一些实验结果和深入分析。<details>
<summary>Abstract</summary>
Multi-modal 3D scene understanding has gained considerable attention due to its wide applications in many areas, such as autonomous driving and human-computer interaction. Compared to conventional single-modal 3D understanding, introducing an additional modality not only elevates the richness and precision of scene interpretation but also ensures a more robust and resilient understanding. This becomes especially crucial in varied and challenging environments where solely relying on 3D data might be inadequate. While there has been a surge in the development of multi-modal 3D methods over past three years, especially those integrating multi-camera images (3D+2D) and textual descriptions (3D+language), a comprehensive and in-depth review is notably absent. In this article, we present a systematic survey of recent progress to bridge this gap. We begin by briefly introducing a background that formally defines various 3D multi-modal tasks and summarizes their inherent challenges. After that, we present a novel taxonomy that delivers a thorough categorization of existing methods according to modalities and tasks, exploring their respective strengths and limitations. Furthermore, comparative results of recent approaches on several benchmark datasets, together with insightful analysis, are offered. Finally, we discuss the unresolved issues and provide several potential avenues for future research.
</details>
<details>
<summary>摘要</summary>
多modal 3D场景理解在过去三年内获得了广泛关注，因为它在各个领域，如自动驾驶和人机交互，有广泛的应用。与传统的单modal 3D理解相比，引入一个额外的模式不仅提高了场景理解的丰富性和精度，而且确保了更加可靠和抗障的理解。这在多样化和挑战性的环境中变得特别重要。随着过去三年内多modal 3D方法的开发，特别是将多个相机图像（3D+2D）和文本描述（3D+语言）结合起来的方法，尚未有一篇系统性的审查。在这篇文章中，我们提供了一个系统性的评估，以填补这一空白。我们首先介绍了背景，正式定义了不同的3D多modal任务，并总结了它们的内在挑战。接着，我们提出了一种新的分类方法，根据模式和任务进行了详细的分类，探讨它们的优势和局限性。此外，我们还提供了一些最近的方法在多个标准数据集上的比较结果，并进行了深入的分析。最后，我们讨论了未解决的问题，并提出了一些未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="Confounder-Balancing-in-Adversarial-Domain-Adaptation-for-Pre-Trained-Large-Models-Fine-Tuning"><a href="#Confounder-Balancing-in-Adversarial-Domain-Adaptation-for-Pre-Trained-Large-Models-Fine-Tuning" class="headerlink" title="Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning"></a>Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16062">http://arxiv.org/abs/2310.16062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan, Xiangping Wu</li>
<li>For: This paper proposes a method for adversarial domain adaptation (ADA) with confounder balancing for pre-trained large models (PLMs) fine-tuning.* Methods: The proposed method, ADA-CBF, includes a PLM as the foundation model for a feature extractor, a domain classifier, and a confounder classifier, which are jointly trained with an adversarial loss.* Results: Compared to existing ADA methods, ADA-CBF can correctly identify confounders in domain-invariant features and eliminate confounder biases in the extracted features from PLMs. Empirical results on natural language processing and computer vision downstream tasks show that ADA-CBF outperforms the newest GPT-4, LLaMA2, ViT, and ADA methods.Here is the text in Simplified Chinese:* For: 这篇论文提出了针对预训练大型模型（PLMs）精细调教时的反对频道适应（ADA）方法，以帮助将知识从源频道传递到目标频道。* Methods: ADA-CBF方法包括PLM作为基础模型，用于特征提取器、频道分类器和干扰分类器，这些模型通过反对损失进行联合训练。* Results: 与现有ADA方法相比，ADA-CBF方法可以正确地识别频道 invariant 中的干扰因素，从而消除PLMs中提取的干扰因素。实验结果表明，ADA-CBF方法在自然语言处理和计算机视觉下游任务上表现出色，超过了最新的GPT-4、LLaMA2、ViT和ADA方法。<details>
<summary>Abstract</summary>
The excellent generalization, contextual learning, and emergence abilities in the pre-trained large models (PLMs) handle specific tasks without direct training data, making them the better foundation models in the adversarial domain adaptation (ADA) methods to transfer knowledge learned from the source domain to target domains. However, existing ADA methods fail to account for the confounder properly, which is the root cause of the source data distribution that differs from the target domains. This study proposes an adversarial domain adaptation with confounder balancing for PLMs fine-tuning (ADA-CBF). The ADA-CBF includes a PLM as the foundation model for a feature extractor, a domain classifier and a confounder classifier, and they are jointly trained with an adversarial loss. This loss is designed to improve the domain-invariant representation learning by diluting the discrimination in the domain classifier. At the same time, the adversarial loss also balances the confounder distribution among source and unmeasured domains in training. Compared to existing ADA methods, ADA-CBF can correctly identify confounders in domain-invariant features, thereby eliminating the confounder biases in the extracted features from PLMs. The confounder classifier in ADA-CBF is designed as a plug-and-play and can be applied in the confounder measurable, unmeasurable, or partially measurable environments. Empirical results on natural language processing and computer vision downstream tasks show that ADA-CBF outperforms the newest GPT-4, LLaMA2, ViT and ADA methods.
</details>
<details>
<summary>摘要</summary>
这些预训练大型模型（PLM）具有出色的泛化、上下文学习和 emergence 能力，可以在不直接使用训练数据的情况下处理特定任务，使其成为更好的基础模型在逆转Domain adaptation（ADA）方法中传递知识从源频道到目标频道。然而，现有的 ADA 方法未能正确考虑隐藏因素（confounder），这是源数据分布的根本原因。这项研究提出了基于 PLM 的逆转Domain adaptation with confounder balancing（ADA-CBF）。ADA-CBF 包括 PLM 作为基础模型的特征提取器、频道分类器和隐藏因素分类器，这些模块在jointly 训练的 adversarial loss 中进行学习。这种损失设计可以提高频道无关的表示学习，同时也在训练中均衡隐藏因素的分布。相比现有的 ADA 方法，ADA-CBF 可以正确识别隐藏因素在频道无关的特征中，从而消除 PLM 提取的隐藏因素偏见。ADA-CBF 的隐藏因素分类器设计为可插入的 plug-and-play，可以在测量、未测量或部分测量环境中应用。实验结果表明，ADA-CBF 在自然语言处理和计算机视觉下渠道任务上表现出色，超过了最新的 GPT-4、LLaMA2、ViT 和 ADA 方法。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Detection-of-LLMs-Generated-Content"><a href="#A-Survey-on-Detection-of-LLMs-Generated-Content" class="headerlink" title="A Survey on Detection of LLMs-Generated Content"></a>A Survey on Detection of LLMs-Generated Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15654">http://arxiv.org/abs/2310.15654</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xianjun-yang/awesome_papers_on_llms_detection">https://github.com/xianjun-yang/awesome_papers_on_llms_detection</a></li>
<li>paper_authors: Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda Petzold, William Yang Wang, Wei Cheng</li>
<li>For: The paper aims to provide a comprehensive survey of existing detection strategies and benchmarks for identifying content generated by advanced large language models (LLMs), and to identify key challenges and prospects in the field.* Methods: The paper scrutinizes the differences between existing detection strategies and benchmarks, and advocates for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs.* Results: The paper provides a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content. The relevant papers are summarized and will be consistently updated at a specific GitHub repository.<details>
<summary>Abstract</summary>
The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content. The relevant papers are summarized and will be consistently updated at https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.
</details>
<details>
<summary>摘要</summary>
大量高级自然语言模型（LLMs）的发展，如ChatGPT，已经导致了假内容生成的增加，对媒体、网络安全、公共讨论和教育等领域产生了重要影响。因此，检测LLMs生成的内容的能力变得非常重要。我们想要提供一个详细的检测策略和标准 benchmark的概述，分析它们之间的差异，并确定领域中关键的挑战和前景，并提倡更加适应性和可靠性的模型，以提高检测精度。此外，我们认为需要采取多方面的方法来防止不同类型的攻击，以对抗LLMs的攻击能力的快速发展。根据我们所知，这是LLMs生成内容检测领域的首次全面评价。我们希望这篇文章能为研究人员和实践者提供一份广泛的理解LLMs生成内容检测领域的现状，并作为参考，帮助他们在LLMs占据主导地位的时代保持数字信息的完整性。相关论文将在https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git中进行系统性的汇总和更新。
</details></li>
</ul>
<hr>
<h2 id="Career-Path-Prediction-using-Resume-Representation-Learning-and-Skill-based-Matching"><a href="#Career-Path-Prediction-using-Resume-Representation-Learning-and-Skill-based-Matching" class="headerlink" title="Career Path Prediction using Resume Representation Learning and Skill-based Matching"></a>Career Path Prediction using Resume Representation Learning and Skill-based Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15636">http://arxiv.org/abs/2310.15636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens-Joris Decorte, Jeroen Van Hautte, Johannes Deleu, Chris Develder, Thomas Demeester</li>
<li>for: 预测工作者的下一步职业发展，包括职业变革预测和内部职业移动预测。</li>
<li>methods: 利用工作经验段落中的文本描述来预测下一步职业发展，包括一种基于 ESCO 职业标签的数据集，以及一种基于文本描述的 represencing learning 方法（CareerBERT）。</li>
<li>results: 在使用数据集进行训练后，提出了一种基于技能的预测模型和一种基于文本描述的预测模型，它们分别达到了 35.24% 和 39.61% 的 recall@10 性能指标，而将这两种模型结合使用的 Hybrid 方法则达到了 43.01% 的 recall@10 性能指标。<details>
<summary>Abstract</summary>
The impact of person-job fit on job satisfaction and performance is widely acknowledged, which highlights the importance of providing workers with next steps at the right time in their career. This task of predicting the next step in a career is known as career path prediction, and has diverse applications such as turnover prevention and internal job mobility. Existing methods to career path prediction rely on large amounts of private career history data to model the interactions between job titles and companies. We propose leveraging the unexplored textual descriptions that are part of work experience sections in resumes. We introduce a structured dataset of 2,164 anonymized career histories, annotated with ESCO occupation labels. Based on this dataset, we present a novel representation learning approach, CareerBERT, specifically designed for work history data. We develop a skill-based model and a text-based model for career path prediction, which achieve 35.24% and 39.61% recall@10 respectively on our dataset. Finally, we show that both approaches are complementary as a hybrid approach achieves the strongest result with 43.01% recall@10.
</details>
<details>
<summary>摘要</summary>
“人职匹配对工作满意度和性能的影响广泛被认可，这 highlights 提供工作者合适的下一步时间的重要性。预测工作者的下一步职业是职业发展预测的任务，具有吸引人的应用场景，如防止辞职和内部职位转移。现有的职业发展预测方法基于大量私人职业历史数据来模型职业标签和公司之间的互动。我们提议利用工作经历部分的未曾被利用的文本描述。我们提供了2,164个匿名化的职业历史记录，并将其注解为 ESCO 职业标签。基于这个数据集，我们提出了一种新的表征学习方法，称为 CareerBERT，专门为工作历史数据设计。我们开发了一种基于技能的模型和一种基于文本的模型，以预测职业发展。两种方法分别 achiev 35.24% 和 39.61% 的回归@10 的结果。最后，我们表明这两种方法是补充的，混合方法实现最强的结果，即 43.01% 的回归@10。”
</details></li>
</ul>
<hr>
<h2 id="Using-Slisemap-to-interpret-physical-data"><a href="#Using-Slisemap-to-interpret-physical-data" class="headerlink" title="Using Slisemap to interpret physical data"></a>Using Slisemap to interpret physical data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15610">http://arxiv.org/abs/2310.15610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauri Seppäläinen, Anton Björklund, Vitus Besel, Kai Puolamäki</li>
<li>for: 这篇论文是用于描述一种新的 manifold visualization 方法，叫做 Slise，以及其在物理和化学 dataset 上的应用。</li>
<li>methods: 这篇论文使用了 Slisemap，它是一种结合 manifold visualization 和可解释人工智能的方法，用于调查黑盒机器学习模型和复杂模拟器的决策过程。 Slisemap 可以找到一个嵌入，使得数据项的相似本地解释被分组在一起，从而提供了黑盒模型的不同行为的总览。</li>
<li>results: 在这篇论文中， authors 使用了 Slisemap 在物理数据上进行了评估，并发现了 Slisemap 可以帮助找到 meaningful 信息，包括分类和回归模型在数据集上的表现。<details>
<summary>Abstract</summary>
Manifold visualisation techniques are commonly used to visualise high-dimensional datasets in physical sciences. In this paper we apply a recently introduced manifold visualisation method, called Slise, on datasets from physics and chemistry. Slisemap combines manifold visualisation with explainable artificial intelligence. Explainable artificial intelligence is used to investigate the decision processes of black box machine learning models and complex simulators. With Slisemap we find an embedding such that data items with similar local explanations are grouped together. Hence, Slisemap gives us an overview of the different behaviours of a black box model. This makes Slisemap into a supervised manifold visualisation method, where the patterns in the embedding reflect a target property. In this paper we show how Slisemap can be used and evaluated on physical data and that Slisemap is helpful in finding meaningful information on classification and regression models trained on these datasets.
</details>
<details>
<summary>摘要</summary>
manifold 视觉技术广泛应用于物理科学中高维数据的可视化。在这篇论文中，我们使用最近引入的抽象可视化方法，即Slise，对物理和化学数据集进行可视化。Slisemap 结合抽象智能和可解释性，用于调查黑盒机器学习模型和复杂模拟器的决策过程。通过Slisemap，我们可以找到一个嵌入，使得数据项与相似的本地解释集成在一起。因此，Slisemap 为我们提供了一种监督的抽象可视化方法，其中抽象的特征嵌入反映了目标属性。在这篇论文中，我们示例了如何使用 Slisemap 可视化和评估物理数据集，并证明 Slisemap 对分类和回归模型的训练数据进行可视化有助于找到有意义的信息。
</details></li>
</ul>
<hr>
<h2 id="tagE-Enabling-an-Embodied-Agent-to-Understand-Human-Instructions"><a href="#tagE-Enabling-an-Embodied-Agent-to-Understand-Human-Instructions" class="headerlink" title="tagE: Enabling an Embodied Agent to Understand Human Instructions"></a>tagE: Enabling an Embodied Agent to Understand Human Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15605">http://arxiv.org/abs/2310.15605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csarkar/tage">https://github.com/csarkar/tage</a></li>
<li>paper_authors: Chayan Sarkar, Avik Mitra, Pradip Pramanick, Tapas Nayak</li>
<li>for: 本研究旨在提高智能代理人（embodied agent）理解自然语言（NLU）指令，以便在人工智能（AI）与人类交互时更加准确地理解人类的意图。</li>
<li>methods: 本研究提出了一种新的task和argument grounding for Embodied agents（tagE）系统，该系统使用了一种新型的神经网络模型，可以从自然语言中提取复杂任务指令中的多个任务和其相应的Arguments。</li>
<li>results: 实验结果表明，tagE系统比基eline模型表现出色，能够更好地理解人类的意图并将其映射到机器人的已知技能集和环境中的物品上。<details>
<summary>Abstract</summary>
Natural language serves as the primary mode of communication when an intelligent agent with a physical presence engages with human beings. While a plethora of research focuses on natural language understanding (NLU), encompassing endeavors such as sentiment analysis, intent prediction, question answering, and summarization, the scope of NLU directed at situations necessitating tangible actions by an embodied agent remains limited. The inherent ambiguity and incompleteness inherent in natural language present challenges for intelligent agents striving to decipher human intention. To tackle this predicament head-on, we introduce a novel system known as task and argument grounding for Embodied agents (tagE). At its core, our system employs an inventive neural network model designed to extract a series of tasks from complex task instructions expressed in natural language. Our proposed model adopts an encoder-decoder framework enriched with nested decoding to effectively extract tasks and their corresponding arguments from these intricate instructions. These extracted tasks are then mapped (or grounded) to the robot's established collection of skills, while the arguments find grounding in objects present within the environment. To facilitate the training and evaluation of our system, we have curated a dataset featuring complex instructions. The results of our experiments underscore the prowess of our approach, as it outperforms robust baseline models.
</details>
<details>
<summary>摘要</summary>
自然语言作为智能代理人与人类交流的主要模式，而许多研究集中在自然语言理解（NLU）方面，涵盖了情感分析、意图预测、问答和概要概述等领域。然而，针对需要物理行为的情况，NLU的应用范围仍然很有限。自然语言的抽象和缺失使得智能代理人很难理解人类意图。为了解决这个问题，我们介绍了一种新的系统——任务和论点固定 для具体代理人（tagE）。我们的系统使用了一种创新的神经网络模型，可以从自然语言中提取复杂任务指令中的多个任务和其相应的论点。我们的提议的模型采用了Encoder-Decoder框架，并添加了嵌入式编码来有效地提取任务和论点。这些提取的任务将被映射到机器人已经建立的技能集中，而论点则会在环境中找到对应的物品。为了让我们的系统受到训练和评估，我们已经制作了一个复杂指令的数据集。实验结果表明，我们的方法在与强大基准模型进行比较时表现出色。
</details></li>
</ul>
<hr>
<h2 id="Emergent-Communication-in-Interactive-Sketch-Question-Answering"><a href="#Emergent-Communication-in-Interactive-Sketch-Question-Answering" class="headerlink" title="Emergent Communication in Interactive Sketch Question Answering"></a>Emergent Communication in Interactive Sketch Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15597">http://arxiv.org/abs/2310.15597</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mediabrain-sjtu/ecisqa">https://github.com/mediabrain-sjtu/ecisqa</a></li>
<li>paper_authors: Zixing Lei, Yiming Zhang, Yuxin Xiong, Siheng Chen</li>
<li>for: 本研究旨在通过图画和解释器学习沟通，并评估多轮交互对人工智能沟通的影响。</li>
<li>methods: 本研究提出了一个新的互动图问答任务，两名合作player通过图画回答一个图像的问题，并采用了一种新的有效的互动EC系统，可以实现问题答案准确率、图画复杂度和人类可读性的有效平衡。</li>
<li>results: 实验结果和人类评价表明，多轮交互机制可以提高目标和有效的人工智能沟通，并且具有良好的人类可读性。<details>
<summary>Abstract</summary>
Vision-based emergent communication (EC) aims to learn to communicate through sketches and demystify the evolution of human communication. Ironically, previous works neglect multi-round interaction, which is indispensable in human communication. To fill this gap, we first introduce a novel Interactive Sketch Question Answering (ISQA) task, where two collaborative players are interacting through sketches to answer a question about an image in a multi-round manner. To accomplish this task, we design a new and efficient interactive EC system, which can achieve an effective balance among three evaluation factors, including the question answering accuracy, drawing complexity and human interpretability. Our experimental results including human evaluation demonstrate that multi-round interactive mechanism facilitates targeted and efficient communication between intelligent agents with decent human interpretability.
</details>
<details>
<summary>摘要</summary>
视觉基于的演进通信（EC）目标是通过绘画学习沟通，并推翻人类沟通的演进。却意外地，先前的工作忽略了多轮互动，这是人类沟通中不可或缺的一部分。为了填补这一漏洞，我们首先介绍了一种新的互动绘画问答（ISQA）任务，其中两名合作的玩家通过绘画回答一个图像的问题，以多轮的方式进行交互。为了完成这项任务，我们设计了一种新的高效的互动EC系统，可以实现三个评价因素的有效平衡，包括问题答案准确率、绘画复杂度和人类可解释性。我们的实验结果，包括人类评价，表明了多轮互动机制可以使智能代理人与人类之间的沟通更加准确和高效。
</details></li>
</ul>
<hr>
<h2 id="Retrieval-based-Knowledge-Transfer-An-Effective-Approach-for-Extreme-Large-Language-Model-Compression"><a href="#Retrieval-based-Knowledge-Transfer-An-Effective-Approach-for-Extreme-Large-Language-Model-Compression" class="headerlink" title="Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression"></a>Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15594">http://arxiv.org/abs/2310.15594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Ran Lucien Wang, Rui Yan</li>
<li>for: 实现大规模预训语言模型（LLMs）在实际应用中的部署，解决大型模型的问题。</li>
<li>methods: 提出了一种新的压缩方法 Retrieval-based Knowledge Transfer（RetriKT），可以将 LLMS 中的知识转移到非常小型的模型（例如 1%）中。</li>
<li>results: 实际实验结果显示，提出的方法可以将小型模型的性能提高，通过将 LLMS 中的知识转移到小型模型中，并且运用软题调整和 proximal policy optimization（PPO）优化学习方法。<details>
<summary>Abstract</summary>
Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%). In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques are employed. Extensive experiments are conducted on low-resource tasks from SuperGLUE and GLUE benchmarks. The results demonstrate that the proposed approach significantly enhances the performance of small-scale models by leveraging the knowledge from LLMs.
</details>
<details>
<summary>摘要</summary>
Our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, we employ soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques. We conduct extensive experiments on low-resource tasks from SuperGLUE and GLUE benchmarks. The results show that the proposed approach significantly enhances the performance of small-scale models by leveraging the knowledge from LLMs.
</details></li>
</ul>
<hr>
<h2 id="Detecting-Intentional-AIS-Shutdown-in-Open-Sea-Maritime-Surveillance-Using-Self-Supervised-Deep-Learning"><a href="#Detecting-Intentional-AIS-Shutdown-in-Open-Sea-Maritime-Surveillance-Using-Self-Supervised-Deep-Learning" class="headerlink" title="Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning"></a>Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15586">http://arxiv.org/abs/2310.15586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Bernabé, Arnaud Gotlieb, Bruno Legeard, Dusica Marijan, Frank Olaf Sem-Jacobsen, Helge Spieker</li>
<li>for: 检测非法活动，如非法捕鱼或贩卖不法物品。</li>
<li>methods: 使用自动标识系统（AIS）消息，并使用深度学习技术和变换器模型进行自我监督学习，实时处理AIS消息，可处理每月超过500亿消息和60000艘船舶的路径轨迹。</li>
<li>results: 可以准确地检测AIS消息不当失 reception，并且可以 rediscover已经检测到的意外AIS终端停机。<details>
<summary>Abstract</summary>
In maritime traffic surveillance, detecting illegal activities, such as illegal fishing or transshipment of illicit products is a crucial task of the coastal administration. In the open sea, one has to rely on Automatic Identification System (AIS) message transmitted by on-board transponders, which are captured by surveillance satellites. However, insincere vessels often intentionally shut down their AIS transponders to hide illegal activities. In the open sea, it is very challenging to differentiate intentional AIS shutdowns from missing reception due to protocol limitations, bad weather conditions or restricting satellite positions. This paper presents a novel approach for the detection of abnormal AIS missing reception based on self-supervised deep learning techniques and transformer models. Using historical data, the trained model predicts if a message should be received in the upcoming minute or not. Afterwards, the model reports on detected anomalies by comparing the prediction with what actually happens. Our method can process AIS messages in real-time, in particular, more than 500 Millions AIS messages per month, corresponding to the trajectories of more than 60 000 ships. The method is evaluated on 1-year of real-world data coming from four Norwegian surveillance satellites. Using related research results, we validated our method by rediscovering already detected intentional AIS shutdowns.
</details>
<details>
<summary>摘要</summary>
在海上交通监控中，检测违法活动，如非法渔业或贩卖黑市商品是海岸管理部门的关键任务。在开海上，一需要依靠自动识别系统（AIS）消息，由船舶上的扩展器传输，由监控卫星接收。然而，不诚实的船舶经常故意关闭AIS扩展器，以隐藏违法活动。在开海上，区分意外的AIS缺失和意外关闭非常困难。这篇论文提出了一种新的偏差检测方法，基于自动学习技术和变换器模型。使用历史数据，训练好的模型预测下一分钟内是否应该接收AIS消息。之后，模型对检测到的异常进行报告，通过与实际情况进行比较。我们的方法可以处理AIS消息在实时，特别是每月超过5亿条消息，对应60000艘船舶的轨迹。我们的方法在4年的实际数据上进行了评估，使用相关研究成果， validate our method by rediscovering already detected intentional AIS shutdowns。
</details></li>
</ul>
<hr>
<h2 id="CONTRASTE-Supervised-Contrastive-Pre-training-With-Aspect-based-Prompts-For-Aspect-Sentiment-Triplet-Extraction"><a href="#CONTRASTE-Supervised-Contrastive-Pre-training-With-Aspect-based-Prompts-For-Aspect-Sentiment-Triplet-Extraction" class="headerlink" title="CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction"></a>CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15577">http://arxiv.org/abs/2310.15577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nitkannen/contraste">https://github.com/nitkannen/contraste</a></li>
<li>paper_authors: Rajdeep Mukherjee, Nithish Kannen, Saurabh Kumar Pandey, Pawan Goyal</li>
<li>for: 提高多个ABSA任务下推理性能</li>
<li>methods: 使用对比学习增强ASTE性能，并提出多任务结合方法</li>
<li>results: 实现新的ASTE最佳Result，并通过细致的ablation study证明每个提案的重要性<details>
<summary>Abstract</summary>
Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus on developing more efficient fine-tuning techniques for the task. Instead, our motivation is to come up with a generic approach that can improve the downstream performances of multiple ABSA tasks simultaneously. Towards this, we present CONTRASTE, a novel pre-training strategy using CONTRastive learning to enhance the ASTE performance. While we primarily focus on ASTE, we also demonstrate the advantage of our proposed technique on other ABSA tasks such as ACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion, sentiment) triplets, first, we design aspect-based prompts with corresponding sentiments masked. We then (pre)train an encoder-decoder model by applying contrastive learning on the decoder-generated aspect-aware sentiment representations of the masked terms. For fine-tuning the model weights thus obtained, we then propose a novel multi-task approach where the base encoder-decoder model is combined with two complementary modules, a tagging-based Opinion Term Detector, and a regression-based Triplet Count Estimator. Exhaustive experiments on four benchmark datasets and a detailed ablation study establish the importance of each of our proposed components as we achieve new state-of-the-art ASTE results.
</details>
<details>
<summary>摘要</summary>
现有的对应情感三元EXTRACTION（ASTE）研究主要集中在发展更高效的精细化训练技术上。而我们的动机是为了开发一个通用的方法，可以同时提高多个ABSA任务的下游性能。为此，我们提出了CONTRASTE，一种基于对照学习的新预训练策略。我们主要关注ASTE，但我们也证明了我们的提案方法在其他ABSA任务，如ACOS、TASD和AESC上的优势。对于一句话和其相关的（方面、意见、情感）三元，我们首先设计了方面基于的提示，并将相应的情感覆盖。然后，我们使用对照学习 trains一个encoder-decoder模型，并透过对decoder生成的方面对应情感表现进行对照学习。接着，我们提出了一个新的多任务方法，将基础encoder-decoder模型与两个辅助模组（一个是基于标签的意见 терміن列表，另一个是基于回归的三元数据列表）结合。我们对四个 benchmark 数据集进行了详细的实验和ablation研究，并证明了我们的每个提案元件的重要性，得到了新的state-of-the-art ASTE结果。
</details></li>
</ul>
<hr>
<h2 id="DeSIQ-Towards-an-Unbiased-Challenging-Benchmark-for-Social-Intelligence-Understanding"><a href="#DeSIQ-Towards-an-Unbiased-Challenging-Benchmark-for-Social-Intelligence-Understanding" class="headerlink" title="DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding"></a>DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18359">http://arxiv.org/abs/2310.18359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao-Yu Guo, Yuan-Fang Li, Gholamreza Haffari</li>
<li>for: 本研究旨在检验社交智能benchmark datasets的准确性。</li>
<li>methods: 我们采用了一种全面的方法来研究社交智能benchmark datasets的准确性，包括对社交智能问题的分析和模型的评估。</li>
<li>results: 我们发现社交智能benchmark datasets存在严重的偏见，可以由一个moderately strong的语言模型学习到无 Context或问题的关系，并且我们提出了一个新的benchmark dataset（DeSIQ），它可以减少了原始社交智能benchmark datasets中的偏见。<details>
<summary>Abstract</summary>
Social intelligence is essential for understanding and reasoning about human expressions, intents and interactions. One representative benchmark for its study is Social Intelligence Queries (Social-IQ), a dataset of multiple-choice questions on videos of complex social interactions. We define a comprehensive methodology to study the soundness of Social-IQ, as the soundness of such benchmark datasets is crucial to the investigation of the underlying research problem. Our analysis reveals that Social-IQ contains substantial biases, which can be exploited by a moderately strong language model to learn spurious correlations to achieve perfect performance without being given the context or even the question. We introduce DeSIQ, a new challenging dataset, constructed by applying simple perturbations to Social-IQ. Our empirical analysis shows DeSIQ significantly reduces the biases in the original Social-IQ dataset. Furthermore, we examine and shed light on the effect of model size, model style, learning settings, commonsense knowledge, and multi-modality on the new benchmark performance. Our new dataset, observations and findings open up important research questions for the study of social intelligence.
</details>
<details>
<summary>摘要</summary>
社会智能是人类表达、意图和互动理解的关键。一个代表性的研究 benchmark 是社交智能问答 (Social-IQ)，这是一个多选问题 dataset 集合，涵盖了复杂的社交互动。我们定义了一种全面的方法ологи？来研究 Social-IQ 的听实性。我们的分析发现，Social-IQ 存在严重的偏见，可以由一个 Moderately strong 语言模型学习伪造关系，以达到完美性无需 Context 或问题。我们引入了 DeSIQ，一个新的挑战性 dataset，通过简单的变换来修正 Social-IQ 中的偏见。我们的实验分析表明，DeSIQ 有效减少了 Social-IQ 中的偏见。此外，我们还检查了模型大小、模型风格、学习设置、常识知识和多模态对新 benchmark 性能的影响。我们的新 dataset、观察和发现开 up了重要的研究问题，用于研究社会智能。
</details></li>
</ul>
<hr>
<h2 id="SteloCoder-a-Decoder-Only-LLM-for-Multi-Language-to-Python-Code-Translation"><a href="#SteloCoder-a-Decoder-Only-LLM-for-Multi-Language-to-Python-Code-Translation" class="headerlink" title="SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation"></a>SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15539">http://arxiv.org/abs/2310.15539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sade-adrien/stelocoder">https://github.com/sade-adrien/stelocoder</a></li>
<li>paper_authors: Jialing Pan, Adrien Sadé, Jin Kim, Eric Soriano, Guillem Sole, Sylvain Flamant</li>
<li>for: 这个论文的目的是提出一个基于 StarCoder 的 Decoder-only LLM，用于多种程式语言到 Python 的程式码转换。</li>
<li>methods: 这个模型使用 Mixture-of-Experts 技术和 Low-Rank Adaptive Method，将 StarCoder 模型扩展为多任务处理。</li>
<li>results: 这个模型在 XLCoST 数据集上实现了73.76 CodeBLEU 分数，比领先者至少高出3.5。<details>
<summary>Abstract</summary>
With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance training efficiency in terms of time, we adopt curriculum learning strategy and use self-instruct data for efficient fine-tuning. As a result, each expert takes only 6 hours to train on one single 80Gb A100 HBM. With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76 CodeBLEU score in multi-programming language-to-Python translation, surpassing the top performance from the leaderboard by at least 3.5. This accomplishment is attributed to only 45M extra parameters with StarCoder as the backbone and 32 hours of valid training on one 80GB A100 HBM. The source code is release here: https://github.com/sade-adrien/SteloCoder.
</details>
<details>
<summary>摘要</summary>
Recently, there has been a focus on Large Language Models (LLMs), and both StarCoder (Li et al., 2023) and Code Llama (Rozi`ere et al., 2023) have shown impressive performance in code generation. However, there is still room for improvement in code translation functionality with efficient training techniques. In response, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. Specifically, SteloCoder can translate C++, C#, JavaScript, Java, or PHP code to Python without specifying the input programming language. We modified the StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique with five experts and a gating network for multi-task handling. The experts are obtained by fine-tuning StarCoder. We use a Low-Rank Adaptive Method (LoRA) technique to limit each expert size to only 0.06% of the number of StarCoder's parameters. To improve training efficiency in terms of time, we adopt a curriculum learning strategy and use self-instruct data for efficient fine-tuning. As a result, each expert can be trained on one single 80Gb A100 HBM in just 6 hours. With experiments on XLCoST datasets, SteloCoder achieves an average CodeBLEU score of 73.76 in multi-programming language-to-Python translation, surpassing the top performance from the leaderboard by at least 3.5. This accomplishment is attributed to only 45M extra parameters with StarCoder as the backbone and 32 hours of valid training on one 80GB A100 HBM. The source code is available at: https://github.com/sade-adrien/SteloCoder.
</details></li>
</ul>
<hr>
<h2 id="Generative-and-Contrastive-Paradigms-Are-Complementary-for-Graph-Self-Supervised-Learning"><a href="#Generative-and-Contrastive-Paradigms-Are-Complementary-for-Graph-Self-Supervised-Learning" class="headerlink" title="Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning"></a>Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15523">http://arxiv.org/abs/2310.15523</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyx11112/GCMAE">https://github.com/wyx11112/GCMAE</a></li>
<li>paper_authors: Yuxiang Wang, Xiao Yan, Chuang Hu, Fangcheng Fu, Wentao Zhang, Hao Wang, Shuo Shang, Jiawei Jiang</li>
<li>for: 本研究旨在提出一种基于自适应学习的图像自我超visentinig方法，即图像对比学习（CL）和压缩学习（MAE）两种方法的统一框架，以提高图像自我超visentinig的性能。</li>
<li>methods: 本研究提出了一种名为图像对比压缩学习（GCMAE）的统一框架，其包括一个MAE分支和一个CL分支，两个分支共享一个编码器，这使得MAE分支可以利用CL分支中提取的全局信息。此外，为了让GCMAE捕捉全局图像结构，研究人员们提出了一种强制GCMAE重建整个相对性矩阵的方法，而不是只重建部分掩码的edge как在现有工作中。</li>
<li>results: 研究人员们对四个流行的图像任务（i.e., 节点类别、节点划分、链接预测和图像类别）进行了测试，并与14种现有的基准值进行了比较。结果显示，GCMAE在这些任务中的性能都很好，最大的性能提升比为3.2%相比最佳基准值。<details>
<summary>Abstract</summary>
For graph self-supervised learning (GSSL), masked autoencoder (MAE) follows the generative paradigm and learns to reconstruct masked graph edges or node features. Contrastive Learning (CL) maximizes the similarity between augmented views of the same graph and is widely used for GSSL. However, MAE and CL are considered separately in existing works for GSSL. We observe that the MAE and CL paradigms are complementary and propose the graph contrastive masked autoencoder (GCMAE) framework to unify them. Specifically, by focusing on local edges or node features, MAE cannot capture global information of the graph and is sensitive to particular edges and features. On the contrary, CL excels in extracting global information because it considers the relation between graphs. As such, we equip GCMAE with an MAE branch and a CL branch, and the two branches share a common encoder, which allows the MAE branch to exploit the global information extracted by the CL branch. To force GCMAE to capture global graph structures, we train it to reconstruct the entire adjacency matrix instead of only the masked edges as in existing works. Moreover, a discrimination loss is proposed for feature reconstruction, which improves the disparity between node embeddings rather than reducing the reconstruction error to tackle the feature smoothing problem of MAE. We evaluate GCMAE on four popular graph tasks (i.e., node classification, node clustering, link prediction, and graph classification) and compare with 14 state-of-the-art baselines. The results show that GCMAE consistently provides good accuracy across these tasks, and the maximum accuracy improvement is up to 3.2% compared with the best-performing baseline.
</details>
<details>
<summary>摘要</summary>
For 自我指导学习（GSSL），假设自动编码器（MAE）遵循生成概念，学习缺失图 edges 或节点特征的重建。相对学习（CL） maximizes 图中的相似性，并广泛应用于 GSSL。然而，MAE 和 CL 在现有的工作中是分开的。我们发现 MAE 和 CL 的概念是补偿的，我们提议一个图自相关假设自动编码器（GCMAE）框架来统一它们。具体来说，MAE 因为关注本地 edges 或节点特征，无法捕捉图的全局信息，而且受到特定的 edges 和特征的影响。相反，CL 因为考虑图之间的关系，能够提取全局信息。因此，我们在 GCMAE 中采用 MAE 支持和 CL 支持，两者共享一个编码器，使 MAE 支持 CL 支持的全局信息。此外，我们训练 GCMAE 可以重建整个相关矩阵，而不是只重建缺失 edges  как在现有的工作中。此外，我们还提出了一种分类损失，以提高节点特征的重建。我们在四个流行的图任务（即节点分类、节点划分、链接预测和图分类）上评估 GCMAE，并与 14 个当前的基eline进行比较。结果显示，GCMAE 在这些任务上具有良好的准确率，最大的准确率提升为 3.2% 相比最佳基eline。
</details></li>
</ul>
<hr>
<h2 id="KITAB-Evaluating-LLMs-on-Constraint-Satisfaction-for-Information-Retrieval"><a href="#KITAB-Evaluating-LLMs-on-Constraint-Satisfaction-for-Information-Retrieval" class="headerlink" title="KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval"></a>KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15511">http://arxiv.org/abs/2310.15511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marah I Abdin, Suriya Gunasekar, Varun Chandrasekaran, Jerry Li, Mert Yuksekgonul, Rahee Ghosh Peshawaria, Ranjita Naik, Besmira Nushi</li>
<li>for: 本研究旨在探讨现代模型是否能够回答信息检索查询（如“ san diego 的冰淇淋店”），以及这类查询是否只能通过网络搜索或知识库解决。</li>
<li>methods: 本研究使用了现代语言模型（LLMs）来初步研究这类查询的能力。</li>
<li>results: 研究发现，许多现有的检索标准不能够评估模型的约束满足能力，而且现有的检索数据集也存在许多问题。研究者们提供了一个新的数据集（KITAB），以及一种相关的动态数据收集和约束验证方法，以便在其他作者的数据上进行类似的测试。经过扩展的实验表明，在不含上下文的情况下，模型会表现出严重的局限性，包括不相关信息、错误信息和不完整性，这些局限性随着信息的流行程度减少而加剧。<details>
<summary>Abstract</summary>
We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., 'a list of ice cream shops in San Diego'). In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes across dimensions such as information popularity, constraint types, and context availability. Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases. While context availability mitigates irrelevant information, it is not helpful for satisfying constraints, identifying fundamental barriers to constraint satisfaction. We open source our contributions to foster further research on improving constraint satisfaction abilities of future models.
</details>
<details>
<summary>摘要</summary>
我们研究现代模型对信息检索查询（如“圣地亚哥的冰淇淋店列表”）的能力。在过去，这些查询被视为只能通过网络搜索或知识库解决的任务。然而，大型自然语言模型（LLM）在这个任务中已经表现出初步的能力。然而，现有的检索标准 benchmark 中有很多不够或者不能测试制约能力的问题。为了解决这些问题，我们提出了 KITAB，一个新的数据集，用于测试语言模型的制约能力。KITAB 包含了More than 600 作者和 13,000 个查询，并且提供了一种相关的动态数据收集和制约验证方法，用于获取类似的测试数据 для其他作者。我们在 GPT4 和 GPT3.5 上进行了扩展的实验，用于描述和解除不同维度的失败模式，包括信息受欢迎程度、制约类型和上下文可用性。结果表明，在没有上下文时，模型会表现出严重的限制，包括无关信息、错误信息和不完整性，这些问题在信息受欢迎程度下降时加剧。然而，上下文可用性可以减少无关信息，但是不能满足制约。我们将我们的贡献开源，以促进未来模型的制约能力改进。
</details></li>
</ul>
<hr>
<h2 id="Robust-Representation-Learning-for-Unified-Online-Top-K-Recommendation"><a href="#Robust-Representation-Learning-for-Unified-Online-Top-K-Recommendation" class="headerlink" title="Robust Representation Learning for Unified Online Top-K Recommendation"></a>Robust Representation Learning for Unified Online Top-K Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15492">http://arxiv.org/abs/2310.15492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minfang Lu, Yuchen Jiang, Huihui Dong, Qi Li, Ziru Xu, Yuanlin Liu, Lixia Wu, Haoyuan Hu, Han Zhu, Yuning Jiang, Jian Xu, Bo Zheng</li>
<li>For: The paper focuses on improving the efficiency of online recommendation systems in large-scale industrial e-commerce, particularly in delivering highly relevant item&#x2F;content advertising that caters to diverse business scenarios.* Methods: The proposed method employs robust representation learning, including domain adversarial learning and multi-view Wasserstein distribution learning, to learn robust representations that ensure data fairness. The method balances conflicting objectives through homoscedastic uncertainty weights and orthogonality constraints.* Results: The proposed method is effective in tackling the challenges of multi-domain matching and retrieving top-k advertisements from multi-entity advertisements across different domains. Various experiments validate the effectiveness and rationality of the proposed method, which has been successfully deployed online to serve real business scenarios.<details>
<summary>Abstract</summary>
In large-scale industrial e-commerce, the efficiency of an online recommendation system is crucial in delivering highly relevant item/content advertising that caters to diverse business scenarios. However, most existing studies focus solely on item advertising, neglecting the significance of content advertising. This oversight results in inconsistencies within the multi-entity structure and unfair retrieval. Furthermore, the challenge of retrieving top-k advertisements from multi-entity advertisements across different domains adds to the complexity. Recent research proves that user-entity behaviors within different domains exhibit characteristics of differentiation and homogeneity. Therefore, the multi-domain matching models typically rely on the hybrid-experts framework with domain-invariant and domain-specific representations. Unfortunately, most approaches primarily focus on optimizing the combination mode of different experts, failing to address the inherent difficulty in optimizing the expert modules themselves. The existence of redundant information across different domains introduces interference and competition among experts, while the distinct learning objectives of each domain lead to varying optimization challenges among experts. To tackle these issues, we propose robust representation learning for the unified online top-k recommendation. Our approach constructs unified modeling in entity space to ensure data fairness. The robust representation learning employs domain adversarial learning and multi-view wasserstein distribution learning to learn robust representations. Moreover, the proposed method balances conflicting objectives through the homoscedastic uncertainty weights and orthogonality constraints. Various experiments validate the effectiveness and rationality of our proposed method, which has been successfully deployed online to serve real business scenarios.
</details>
<details>
<summary>摘要</summary>
大规模工业电商中，在线推荐系统的效率是关键，可以帮助提供高度相关的商品/内容广告，满足不同的业务场景。然而，大多数现有研究仅关注Item广告，忽略了内容广告的重要性。这种忽略会导致多元结构中的不一致性和不公正性。另外，从多个Domains中检索Top-k广告的挑战更加复杂。现有研究表明，用户-Entity行为在不同Domains中具有差异和同一性特征。因此，多Domain匹配模型通常采用Hybrid-Experts框架，并使用域不同和域相同的表示。然而，大多数方法主要关注多个专家的组合方式优化，而忽略了专家模块的内在困难。在不同Domains中存在重复信息的问题会导致专家之间的竞争和干扰，而每个Domain的学习目标也会导致专家模块的优化挑战。为了解决这些问题，我们提出了robust表示学习方法，以确保数据公平。我们的方法在Entity空间建立了统一的模型，并使用域对抗学习和多视图沃氏分布学习来学习Robust表示。此外，我们的方法通过Homoscedastic不确定量和正交约束来均衡矛盾目标。经过多个实验 validate了我们提出的方法的有效性和合理性，并在实际业务场景中成功部署。
</details></li>
</ul>
<hr>
<h2 id="NuTrea-Neural-Tree-Search-for-Context-guided-Multi-hop-KGQA"><a href="#NuTrea-Neural-Tree-Search-for-Context-guided-Multi-hop-KGQA" class="headerlink" title="NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA"></a>NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15484">http://arxiv.org/abs/2310.15484</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlvlab/nutrea">https://github.com/mlvlab/nutrea</a></li>
<li>paper_authors: Hyeong Kyu Choi, Seunghun Lee, Jaewon Chu, Hyunwoo J. Kim</li>
<li>for: 本研究旨在提高多跳知识图问答（KGQA）任务的性能，使得可以使用自然语言问题来检索知识图（KG）中的节点。</li>
<li>methods: 我们提出了一种基于搜索的图神经网络模型（GNN），即神经搜索（NuTrea），它可以更好地考虑知识图的全局背景。我们采用了一种消息传递机制，可以增强过去的嵌入。此外，我们还引入了一种关系频率反Entity频率（RF-IEF）节点嵌入，可以更好地描述不确定的知识图节点。</li>
<li>results: 我们通过对三个主要多跳KGQA测试集进行实验，证明了我们的方法的普遍有效性。此外，我们还进行了广泛的分析，以证明我们的方法的表达能力和稳定性。总的来说，NuTrea 提供了一种可以使用自然语言问题来检索知识图中节点的强大工具。代码可以在<a target="_blank" rel="noopener" href="https://github.com/mlvlab/NuTrea">https://github.com/mlvlab/NuTrea</a> 上获取。<details>
<summary>Abstract</summary>
Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves retrieving nodes from a knowledge graph (KG) to answer natural language questions. Recent GNN-based approaches formulate this task as a KG path searching problem, where messages are sequentially propagated from the seed node towards the answer nodes. However, these messages are past-oriented, and they do not consider the full KG context. To make matters worse, KG nodes often represent proper noun entities and are sometimes encrypted, being uninformative in selecting between paths. To address these problems, we propose Neural Tree Search (NuTrea), a tree search-based GNN model that incorporates the broader KG context. Our model adopts a message-passing scheme that probes the unreached subtree regions to boost the past-oriented embeddings. In addition, we introduce the Relation Frequency-Inverse Entity Frequency (RF-IEF) node embedding that considers the global KG context to better characterize ambiguous KG nodes. The general effectiveness of our approach is demonstrated through experiments on three major multi-hop KGQA benchmark datasets, and our extensive analyses further validate its expressiveness and robustness. Overall, NuTrea provides a powerful means to query the KG with complex natural language questions. Code is available at https://github.com/mlvlab/NuTrea.
</details>
<details>
<summary>摘要</summary>
多跳知识图问答（KGQA）是一个检索知识图（KG）中节点，以回答自然语言问题的任务。现代GNN基于的方法将这个任务视为知识图路径搜索问题，其中消息从种子节点向答题节点进行顺序传递。然而，这些消息是过去oriented，没有考虑整个KG上下文。另外，KG节点经常表示特定名词实体，有时会隐藏信息，从而困难在选择路径。为了解决这些问题，我们提出了神经树搜索（NuTrea）模型，它采用了树搜索的方法，并在搜索过程中采用了消息传递机制，以提高过去oriented的嵌入。此外，我们还引入了关系频率对反应实体频率（RF-IEF）节点嵌入，以更好地描述不确定的KG节点。我们的方法在三个主要多跳KGQA标准数据集上进行了实验，并通过了广泛的分析，以证明其表达力和稳定性。总之，NuTrea提供了一种可以用于查询KG的复杂自然语言问题的强大工具。代码可以在https://github.com/mlvlab/NuTrea上找到。
</details></li>
</ul>
<hr>
<h2 id="AutoDiff-combining-Auto-encoder-and-Diffusion-model-for-tabular-data-synthesizing"><a href="#AutoDiff-combining-Auto-encoder-and-Diffusion-model-for-tabular-data-synthesizing" class="headerlink" title="AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing"></a>AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15479">http://arxiv.org/abs/2310.15479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Namjoon Suh, Xiaofeng Lin, Din-Yin Hsieh, Merhdad Honarkhah, Guang Cheng</li>
<li>for: 本研究使用扩散模型生成 синтетиче表格数据，解决了表格数据生成中异构特征的问题。</li>
<li>methods: 我们使用 auto-encoder 架构来处理异构特征，并与现有的表格生成器进行比较。</li>
<li>results: 我们在 15 个公共数据集上进行了实验，发现我们的模型能够准确地捕捉特征之间的相关性，并在下游任务中表现良好。<details>
<summary>Abstract</summary>
Diffusion model has become a main paradigm for synthetic data generation in many subfields of modern machine learning, including computer vision, language model, or speech synthesis. In this paper, we leverage the power of diffusion model for generating synthetic tabular data. The heterogeneous features in tabular data have been main obstacles in tabular data synthesis, and we tackle this problem by employing the auto-encoder architecture. When compared with the state-of-the-art tabular synthesizers, the resulting synthetic tables from our model show nice statistical fidelities to the real data, and perform well in downstream tasks for machine learning utilities. We conducted the experiments over 15 publicly available datasets. Notably, our model adeptly captures the correlations among features, which has been a long-standing challenge in tabular data synthesis. Our code is available upon request and will be publicly released if paper is accepted.
</details>
<details>
<summary>摘要</summary>
Diffusion model已成为现代机器学习许多子领域的主要思想，包括计算机视觉、语言模型和声音生成等。在这篇论文中，我们利用分散模型为生成合成表格数据。表格数据中具有多种特征的差异是现代表格数据生成的主要障碍，我们通过使用自编码器架构解决这个问题。与现有的表格生成器相比，我们的模型能够很好地保持与真实数据的统计准确性，并在下游任务中表现出色。我们在15个公共可用的数据集上进行了实验。值得一提的是，我们的模型能够很好地捕捉表格数据中特征之间的相关性，这是现代表格数据生成中长期的挑战。我们的代码可以在请求时提供，并将在纸上接受后公开发布。
</details></li>
</ul>
<hr>
<h2 id="A-Communication-Theory-Perspective-on-Prompting-Engineering-Methods-for-Large-Language-Models"><a href="#A-Communication-Theory-Perspective-on-Prompting-Engineering-Methods-for-Large-Language-Models" class="headerlink" title="A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models"></a>A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18358">http://arxiv.org/abs/2310.18358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanfeng Song, Yuanqin He, Xuefang Zhao, Hanlin Gu, Di Jiang, Haijun Yang, Lixin Fan, Qiang Yang</li>
<li>for: 本文旨在提供一种新的 perspective 来审查现有的 prompting 方法，并帮助读者更深入地理解现有的发展趋势。</li>
<li>methods: 本文使用了communication theory 框架来 illustrate 现有的 prompting 方法，并分析了四种典型任务的发展趋势。</li>
<li>results: 本文提出了一些可能的 future research directions ，可以帮助开发更好的 prompting 方法。<details>
<summary>Abstract</summary>
The springing up of Large Language Models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm. Along this line of research endeavors in the area, LLM-based prompting methods have attracted much attention, partially due to the technological advantages brought by prompt engineering (PE) as well as the underlying NLP principles disclosed by various prompting methods. Traditional supervised learning usually requires training a model based on labeled data and then making predictions. In contrast, PE methods directly use the powerful capabilities of existing LLMs (i.e., GPT-3 and GPT-4) via composing appropriate prompts, especially under few-shot or zero-shot scenarios. Facing the abundance of studies related to the prompting and the ever-evolving nature of this field, this article aims to (i) illustrate a novel perspective to review existing PE methods, within the well-established communication theory framework; (ii) facilitate a better/deeper understanding of developing trends of existing PE methods used in four typical tasks; (iii) shed light on promising research directions for future PE methods.
</details>
<details>
<summary>摘要</summary>
春季 LLM 的出现导致社区从单一任务领域的自然语言处理（NLP）研究转移到整体端到端多任务学习模式。在这一研究领域，基于 LLM 的提示方法吸引了很多注意力，部分是因为提示工程（PE）技术的优势以及不同提示方法下的 NLP 原理的披露。传统的超级vised学习通常需要基于标注数据进行训练，然后进行预测。与此相反，PE 方法直接利用现有的 LLM（如 GPT-3 和 GPT-4）的强大能力，通过编写合适的提示，特别是在少量或零量场景下。面对各种提示和这一领域的不断演化，这篇文章的目标是：（i）在通信理论框架下绘制一种新的观点来评估现有的 PE 方法；（ii）促进更深刻的理解现有 PE 方法在四种典型任务的发展趋势；（iii）突出未来 PE 方法的探索方向。
</details></li>
</ul>
<hr>
<h2 id="Empowering-Distributed-Solutions-in-Renewable-Energy-Systems-and-Grid-Optimization"><a href="#Empowering-Distributed-Solutions-in-Renewable-Energy-Systems-and-Grid-Optimization" class="headerlink" title="Empowering Distributed Solutions in Renewable Energy Systems and Grid Optimization"></a>Empowering Distributed Solutions in Renewable Energy Systems and Grid Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15468">http://arxiv.org/abs/2310.15468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mohammadi, Ali Mohammadi</li>
<li>for: 这项研究探讨了电力业务从中心化到分散式的转变，尤其是如何通过机器学习（ML）技术推动可再生能源和改善电网管理。</li>
<li>methods: 这项研究使用了各种机器学习模型，如人工神经网络、支持向量机和决策树，以预测可再生能源生产和消耗。此外，还使用了数据处理技术，如数据分割、 нормализация、分解和简化，以提高预测精度。</li>
<li>results: 该研究发现，通过将大数据和机器学习应用于智能电网，可以提高能源效率、更好地应对需求，并更好地 интеグрировать可再生能源资源。然而，还需要解决大量数据处理、保障网络安全和获得专业知识等挑战。<details>
<summary>Abstract</summary>
This study delves into the shift from centralized to decentralized approaches in the electricity industry, with a particular focus on how machine learning (ML) advancements play a crucial role in empowering renewable energy sources and improving grid management. ML models have become increasingly important in predicting renewable energy generation and consumption, utilizing various techniques like artificial neural networks, support vector machines, and decision trees. Furthermore, data preprocessing methods, such as data splitting, normalization, decomposition, and discretization, are employed to enhance prediction accuracy.   The incorporation of big data and ML into smart grids offers several advantages, including heightened energy efficiency, more effective responses to demand, and better integration of renewable energy sources. Nevertheless, challenges like handling large data volumes, ensuring cybersecurity, and obtaining specialized expertise must be addressed. The research investigates various ML applications within the realms of solar energy, wind energy, and electric distribution and storage, illustrating their potential to optimize energy systems. To sum up, this research demonstrates the evolving landscape of the electricity sector as it shifts from centralized to decentralized solutions through the application of ML innovations and distributed decision-making, ultimately shaping a more efficient and sustainable energy future.
</details>
<details>
<summary>摘要</summary>
The integration of big data and ML into smart grids offers several advantages, including improved energy efficiency, more effective demand response, and better integration of renewable energy sources. However, challenges such as managing large data volumes, ensuring cybersecurity, and obtaining specialized expertise must be addressed.The research examines various ML applications within the realms of solar energy, wind energy, and electric distribution and storage, demonstrating their potential to optimize energy systems. For instance, ML can be used to predict solar and wind power output, optimize energy storage systems, and manage electricity distribution networks.Overall, this study illustrates the evolving landscape of the electricity sector as it shifts from centralized to decentralized solutions through the application of ML innovations and distributed decision-making, ultimately leading to a more efficient and sustainable energy future.
</details></li>
</ul>
<hr>
<h2 id="UI-Layout-Generation-with-LLMs-Guided-by-UI-Grammar"><a href="#UI-Layout-Generation-with-LLMs-Guided-by-UI-Grammar" class="headerlink" title="UI Layout Generation with LLMs Guided by UI Grammar"></a>UI Layout Generation with LLMs Guided by UI Grammar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15455">http://arxiv.org/abs/2310.15455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwen Lu, Ziang Tong, Qinyi Zhao, Chengzhi Zhang, Toby Jia-Jun Li</li>
<li>for:  investigate the use of Large Language Models (LLMs) for UI layout generation</li>
<li>methods:  propose a novel approach called UI grammar to represent the hierarchical structure of UI screens and guide the generative capacities of LLMs</li>
<li>results:  initial experiments with GPT-4 showed promising capability of LLMs to produce high-quality user interfaces via in-context learning, and preliminary comparative study suggested the potential of the grammar-based approach in improving the quality of generative results.Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 这份位置论文是要研究 Large Language Models (LLMs) 在用户界面 (UI) 布局生成方面的应用。</li>
<li>methods: 我们提出了一种新的方法，即 UI 语法，来表示 UI 画面中的层次结构，并将其用来引导 LLMs 的生成能力。</li>
<li>results: 我们的初步实验显示，使用 GPT-4 可以通过内容学习获得高质量的用户界面，而且我们的初步比较研究显示，语法基本方法在特定方面的生成结果质量上有潜在的改善。<details>
<summary>Abstract</summary>
The recent advances in Large Language Models (LLMs) have stimulated interest among researchers and industry professionals, particularly in their application to tasks concerning mobile user interfaces (UIs). This position paper investigates the use of LLMs for UI layout generation. Central to our exploration is the introduction of UI grammar -- a novel approach we proposed to represent the hierarchical structure inherent in UI screens. The aim of this approach is to guide the generative capacities of LLMs more effectively and improve the explainability and controllability of the process. Initial experiments conducted with GPT-4 showed the promising capability of LLMs to produce high-quality user interfaces via in-context learning. Furthermore, our preliminary comparative study suggested the potential of the grammar-based approach in improving the quality of generative results in specific aspects.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PromptInfuser-How-Tightly-Coupling-AI-and-UI-Design-Impacts-Designers’-Workflows"><a href="#PromptInfuser-How-Tightly-Coupling-AI-and-UI-Design-Impacts-Designers’-Workflows" class="headerlink" title="PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers’ Workflows"></a>PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers’ Workflows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15435">http://arxiv.org/abs/2310.15435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savvas Petridis, Michael Terry, Carrie J. Cai</li>
<li>for: 这篇研究探讨了如何将AI示范和UI设计联系起来，以提高设计师的工作效率和 prototype 的可信度。</li>
<li>methods: 研究人员开发了一个名为PromptInfuser的 Figma 插件，可以让设计师将 UI 元素与提示连接起来，实现半功能的 mockup。</li>
<li>results: 在14名设计师 participated 的研究中，PromptInfuser 被视为比现有的 AI 示范工作流程更有用，可以更好地传达产品想法，生成更加实际地表现出想像中的产品，并且更有效率地进行示范。<details>
<summary>Abstract</summary>
Prototyping AI applications is notoriously difficult. While large language model (LLM) prompting has dramatically lowered the barriers to AI prototyping, designers are still prototyping AI functionality and UI separately. We investigate how coupling prompt and UI design affects designers' workflows. Grounding this research, we developed PromptInfuser, a Figma plugin that enables users to create semi-functional mockups, by connecting UI elements to the inputs and outputs of prompts. In a study with 14 designers, we compare PromptInfuser to designers' current AI-prototyping workflow. PromptInfuser was perceived to be significantly more useful for communicating product ideas, more capable of producing prototypes that realistically represent the envisioned artifact, more efficient for prototyping, and more helpful for anticipating UI issues and technical constraints. PromptInfuser encouraged iteration over prompt and UI together, which helped designers identify UI and prompt incompatibilities and reflect upon their total solution. Together, these findings inform future systems for prototyping AI applications.
</details>
<details>
<summary>摘要</summary>
probiotyping AI 应用非常困难。 although large language model (LLM) 提示大大降低了 AI  probiotyping 的门槛，Designers 仍然在 AI 功能和 UI 设计之间进行分离的探索。我们研究了如何将提示和 UI 设计结合，对设计者的工作流程产生影响。为了实践这些研究，我们开发了 PromptInfuser，一款 Figma 插件，允许用户通过将 UI 元素连接到提示的输入和输出来创建半功能的 mockup。在 14 名设计者参与的研究中，我们比较了 PromptInfuser 和设计者当前的 AI 探索工作流程。结果显示，PromptInfuser 被视为更有用于传达产品想法，更能生成符合想像中的 artifact 的 mockup，更高效的探索，并更有助于预测 UI 问题和技术约束。PromptInfuser 促进了提示和 UI 的同步 iterate，帮助设计者更好地了解他们的总解决方案，并反思提示和 UI 之间的不兼容性。这些发现可以帮助未来的 AI 探索系统。
</details></li>
</ul>
<hr>
<h2 id="ConstitutionMaker-Interactively-Critiquing-Large-Language-Models-by-Converting-Feedback-into-Principles"><a href="#ConstitutionMaker-Interactively-Critiquing-Large-Language-Models-by-Converting-Feedback-into-Principles" class="headerlink" title="ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles"></a>ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15428">http://arxiv.org/abs/2310.15428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savvas Petridis, Ben Wedin, James Wexler, Aaron Donsbach, Mahima Pushkarna, Nitesh Goyal, Carrie J. Cai, Michael Terry</li>
<li>for: 这个研究的目的是开发一种可以帮助用户通过自然语言反馈来调整LLM输出的工具，以便更好地控制chatbot的行为。</li>
<li>methods: 这个研究使用了用户反馈的自然语言来生成 constitution，以便用于控制chatbot的行为。</li>
<li>results: 研究发现用户可以使用ConstitutionMaker工具来更好地控制chatbot的行为，并且可以更加快速地将自己的反馈转换成明确的原则。此外，用户还可以使用这个工具来更好地表达自己的想法和反馈，并且可以更加有效地控制chatbot的行为。<details>
<summary>Abstract</summary>
Large language model (LLM) prompting is a promising new approach for users to create and customize their own chatbots. However, current methods for steering a chatbot's outputs, such as prompt engineering and fine-tuning, do not support users in converting their natural feedback on the model's outputs to changes in the prompt or model. In this work, we explore how to enable users to interactively refine model outputs through their feedback, by helping them convert their feedback into a set of principles (i.e. a constitution) that dictate the model's behavior. From a formative study, we (1) found that users needed support converting their feedback into principles for the chatbot and (2) classified the different principle types desired by users. Inspired by these findings, we developed ConstitutionMaker, an interactive tool for converting user feedback into principles, to steer LLM-based chatbots. With ConstitutionMaker, users can provide either positive or negative feedback in natural language, select auto-generated feedback, or rewrite the chatbot's response; each mode of feedback automatically generates a principle that is inserted into the chatbot's prompt. In a user study with 14 participants, we compare ConstitutionMaker to an ablated version, where users write their own principles. With ConstitutionMaker, participants felt that their principles could better guide the chatbot, that they could more easily convert their feedback into principles, and that they could write principles more efficiently, with less mental demand. ConstitutionMaker helped users identify ways to improve the chatbot, formulate their intuitive responses to the model into feedback, and convert this feedback into specific and clear principles. Together, these findings inform future tools that support the interactive critiquing of LLM outputs.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）提示是一种有前途的新方法，允许用户创建和自定义自己的 chatbot。然而，现有的输出方法，如提示工程和精度调整，不支持用户将自然反馈转化为提示或模型更新。在这种工作中，我们explore了如何让用户通过反馈来精炼模型输出，并帮助他们将反馈转化为一组原则（即宪法），定义模型的行为。从一项形成研究中，我们发现：1. 用户需要支持将反馈转化为原则，以便控制 chatbot。2. 用户希望的不同原则类型，包括具体的问题和答案。基于这些发现，我们开发了宪法制作器（ConstitutionMaker），一种可供用户在自然语言反馈下，将反馈转化为原则。宪法制作器包括三种反馈模式：自然语言反馈、自动生成反馈和重写回复。每种反馈模式都会自动生成一个原则，并将其插入到 chatbot 的提示中。在14名参与者参与的用户研究中，我们比较了宪法制作器与一个减少版本，其中用户需要手动编写原则。与减少版本相比，参与者表示使用宪法制作器可以更好地控制 chatbot，更容易将反馈转化为原则，并可以更高效地编写原则，减少心理压力。宪法制作器帮助用户找到 chatbot 的问题，将自然反馈转化为可以更好地引导模型的反馈，并将反馈转化为具体和明确的原则。这些发现可以指导未来的工具，以支持 LLM 输出的交互批判。
</details></li>
</ul>
<hr>
<h2 id="LoRAShear-Efficient-Large-Language-Model-Structured-Pruning-and-Knowledge-Recovery"><a href="#LoRAShear-Efficient-Large-Language-Model-Structured-Pruning-and-Knowledge-Recovery" class="headerlink" title="LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery"></a>LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18356">http://arxiv.org/abs/2310.18356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, Luming Liang</li>
<li>for: 本研究目的是提出一种高效的语言模型结构剪裁方法，以降低大型语言模型的计算成本。</li>
<li>methods: 本方法首先生成了LoRA模块之间的依赖关系图，以发现最小 removable 结构并分析知识分布。然后，它进行了逐步结构剪裁 LoRA 适配器，并启用了内置的知识传递以更好地保留红利模块中的信息。</li>
<li>results: 经过numerical experiment，这种方法可以在只使用一个GPU内部的几个GPU天内减少大型语言模型的占用空间，并且只减少了1.0%的性能。此外，这种方法还可以至少20%的计算成本。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance degradation and significantly outperforms state-of-the-arts. The source code will be available at https://github.com/microsoft/lorashear.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经改变人工智能领域的景观，但它们的巨大大小也带来了计算成本的挑战。我们介绍LoRAShear，一种新的高效方法，可以结构删除LLMs并恢复知识。对于一般的LLMs，LoRAShear首先创建LoRA模组之间的依赖关系图以发现最小移除结构，并分析知识的分布。然后，它逐渐进行进构式删除LoRA拓展，并启用内置的知识转移，以更好地保留在缩减结构中遗留的信息。为了恢复在删除过程中遗失的知识，LoRAShear详细研究并提出了动态精度调整方案和动态数据适配器，以有效地缩小性能差距和全模型。 num = num results表明，仅使用一个GPU，在几个GPU天的时间内，LoRAShear已经成功地将LLMs的印识缩减了20%，仅增加1.0%的性能损失，并明显超过了状态顶峰的方法。源代码将会在https://github.com/microsoft/lorashear上公开。
</details></li>
</ul>
<hr>
<h2 id="FANToM-A-Benchmark-for-Stress-testing-Machine-Theory-of-Mind-in-Interactions"><a href="#FANToM-A-Benchmark-for-Stress-testing-Machine-Theory-of-Mind-in-Interactions" class="headerlink" title="FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions"></a>FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15421">http://arxiv.org/abs/2310.15421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skywalker023/fantom">https://github.com/skywalker023/fantom</a></li>
<li>paper_authors: Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, Maarten Sap</li>
<li>for: 这个论文是为了测试理解人类思维的语言模型（LLM）。</li>
<li>methods: 该论文使用了一个新的benchmark方法called FANToM，通过问答测试LLM的理解人类思维的能力。</li>
<li>results: 研究发现，现状的LLM模型在Answering questions时表现较差，与人类的思维能力相比。<details>
<summary>Abstract</summary>
Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
</details>
<details>
<summary>摘要</summary>
theory of mind (ToM) 评估目前主要是通过使用无反应的故事进行测试，这些故事自然lack interactivity。我们介绍了FANToM，一个新的 benchmark，用于在信息不均衡对话上测试 ToM，通过问答来强制测试。我们的 benchmark  drew upon 心理理论中的重要需求和评估大语言模型 (LLMs) 中必要的实证考虑。特别是，我们设计了多种问题，需要同样的理解来识别LLMs中的幻觉或false sense of ToM能力。我们显示了FANToM是现状顶尖 LLMs 所表现出 significatively worse than humans，包括链式思维或细化。
</details></li>
</ul>
<hr>
<h2 id="Fractal-Landscapes-in-Policy-Optimization"><a href="#Fractal-Landscapes-in-Policy-Optimization" class="headerlink" title="Fractal Landscapes in Policy Optimization"></a>Fractal Landscapes in Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15418">http://arxiv.org/abs/2310.15418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Wang, Sylvia Herbert, Sicun Gao</li>
<li>for: 本研究旨在探讨policy gradient方法在连续状态空间下的深度学习控制问题中的一种限制。</li>
<li>methods: 本研究使用了chaos theory和非准确分析的技术，分析了策略优化目标函数的马克思列普涅夫准则和欧几里得级数。</li>
<li>results: 研究发现，在某些类型的MDP中，策略优化的优化地形可能具有非准确或复杂的特征，导致无法估算梯度。通过实际实验，研究证明了这种情况的发生。<details>
<summary>Abstract</summary>
Policy gradient lies at the core of deep reinforcement learning (RL) in continuous domains. Despite much success, it is often observed in practice that RL training with policy gradient can fail for many reasons, even on standard control problems with known solutions. We propose a framework for understanding one inherent limitation of the policy gradient approach: the optimization landscape in the policy space can be extremely non-smooth or fractal for certain classes of MDPs, such that there does not exist gradient to be estimated in the first place. We draw on techniques from chaos theory and non-smooth analysis, and analyze the maximal Lyapunov exponents and H\"older exponents of the policy optimization objectives. Moreover, we develop a practical method that can estimate the local smoothness of objective function from samples to identify when the training process has encountered fractal landscapes. We show experiments to illustrate how some failure cases of policy optimization can be explained by such fractal landscapes.
</details>
<details>
<summary>摘要</summary>
政策梯度位于深度奖励学习（RL）中的核心，即使在标准控制问题上有知解的解决方案，RL 训练仍然可能失败。我们提出了一种框架来理解政策梯度方法的内在限制：在策略空间中的优化景观可能是某些类型的 Markov  declaim Problem （MDP） 非凡的，无法计算梯度。我们Draw on chaos theory and non-smooth analysis techniques, and analyze the maximal Lyapunov exponents and Holder exponents of the policy optimization objectives. Moreover, we develop a practical method that can estimate the local smoothness of the objective function from samples to identify when the training process has encountered fractal landscapes. We show experiments to illustrate how some failure cases of policy optimization can be explained by such fractal landscapes.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The Traditional Chinese writing system is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Nominality-Score-Conditioned-Time-Series-Anomaly-Detection-by-Point-Sequential-Reconstruction"><a href="#Nominality-Score-Conditioned-Time-Series-Anomaly-Detection-by-Point-Sequential-Reconstruction" class="headerlink" title="Nominality Score Conditioned Time Series Anomaly Detection by Point&#x2F;Sequential Reconstruction"></a>Nominality Score Conditioned Time Series Anomaly Detection by Point&#x2F;Sequential Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15416">http://arxiv.org/abs/2310.15416</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andrewlai61616/npsr">https://github.com/andrewlai61616/npsr</a></li>
<li>paper_authors: Chih-Yu Lai, Fan-Keng Sun, Zhengqi Gao, Jeffrey H. Lang, Duane S. Boning</li>
<li>for: 本研究旨在提出一种基于点 reconstruction 和序列 reconstruction 的无监测时间序异常检测方法，以解决时间序异常检测中的复杂性和多样性问题。</li>
<li>methods: 本研究提出了一个点 reconstruction 模型和一个序列 reconstruction 模型，用于检测点异常和Contextual异常。Point reconstruction 模型使用一个点异常检测器来评估点异常，而序列 reconstruction 模型使用一个序列异常检测器来评估点和Contextual异常。</li>
<li>results: 对于several public datasets进行了广泛的实验研究，结果表明，提出的方法在时间序异常检测中比大多数现有的基准方法表现更好。<details>
<summary>Abstract</summary>
Time series anomaly detection is challenging due to the complexity and variety of patterns that can occur. One major difficulty arises from modeling time-dependent relationships to find contextual anomalies while maintaining detection accuracy for point anomalies. In this paper, we propose a framework for unsupervised time series anomaly detection that utilizes point-based and sequence-based reconstruction models. The point-based model attempts to quantify point anomalies, and the sequence-based model attempts to quantify both point and contextual anomalies. Under the formulation that the observed time point is a two-stage deviated value from a nominal time point, we introduce a nominality score calculated from the ratio of a combined value of the reconstruction errors. We derive an induced anomaly score by further integrating the nominality score and anomaly score, then theoretically prove the superiority of the induced anomaly score over the original anomaly score under certain conditions. Extensive studies conducted on several public datasets show that the proposed framework outperforms most state-of-the-art baselines for time series anomaly detection.
</details>
<details>
<summary>摘要</summary>
时间序列异常检测是因为复杂多变的异常模式而具有挑战性。一个主要困难在于模型时间相关关系以确定上下文异常，同时保持点异常检测的准确性。在这篇论文中，我们提出一种无监督时间序列异常检测框架，该框架利用点基模型和序列基模型来进行重建。点基模型尝试量化点异常，而序列基模型尝试量化点和上下文异常。我们在认为观察到的时间点为假值的两个阶段偏移后，引入了一个 Nominality 分数，该分数来自重建错误的合计值。我们还提出了一个潜在异常分数，并经过理论证明其在某些条件下超过原始异常分数的优越性。我们在多个公共数据集上进行了广泛的研究，并证明了我们的框架在大多数现状监督模型的基础上显著超越。
</details></li>
</ul>
<hr>
<h2 id="Mind-the-Gap-Between-Conversations-for-Improved-Long-Term-Dialogue-Generation"><a href="#Mind-the-Gap-Between-Conversations-for-Improved-Long-Term-Dialogue-Generation" class="headerlink" title="Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation"></a>Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15415">http://arxiv.org/abs/2310.15415</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qzx7/mindthetime">https://github.com/qzx7/mindthetime</a></li>
<li>paper_authors: Qiang Zhang, Jason Naradowsky, Yusuke Miyao</li>
<li>for: 这篇论文目的是让对话模型意识到时间的概念，并在不同的时间间隔下进行对话。</li>
<li>methods: 作者提出了一个名为GapChat的多会话对话集，其中每个会话之间的时间间隔不同。模型在接受时间信息后，对时间和事件进展进行了不同的表示。</li>
<li>results: 人工评估表明，在评价对话 relevance 和信息吸收方面，意识时间的模型表现更好。<details>
<summary>Abstract</summary>
Knowing how to end and resume conversations over time is a natural part of communication, allowing for discussions to span weeks, months, or years. The duration of gaps between conversations dictates which topics are relevant and which questions to ask, and dialogue systems which do not explicitly model time may generate responses that are unnatural. In this work we explore the idea of making dialogue models aware of time, and present GapChat, a multi-session dialogue dataset in which the time between each session varies. While the dataset is constructed in real-time, progress on events in speakers' lives is simulated in order to create realistic dialogues occurring across a long timespan. We expose time information to the model and compare different representations of time and event progress. In human evaluation we show that time-aware models perform better in metrics that judge the relevance of the chosen topics and the information gained from the conversation.
</details>
<details>
<summary>摘要</summary>
知道如何结束和续续会话是通信的自然部分，允许对话 span 周月年。对话系统不显式考虑时间可能生成不自然的响应。在这项工作中，我们探讨将对话模型意识到时间的想法，并提出了 GapChat，一个多期对话集。在这个数据集中，每个会话之间的时间间隔不同。虽然数据集是在实时构建的，但speakers的生活进程的进步是通过模拟来创建真实的对话发生在长时间间隔。我们暴露了时间信息给模型，并比较了不同的时间和事件进度表示。在人类评估中，我们显示时间意识的模型在评估对话中话题的相关性和获得的信息的 метриках中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Conventions-for-Human-AI-Collaboration"><a href="#Diverse-Conventions-for-Human-AI-Collaboration" class="headerlink" title="Diverse Conventions for Human-AI Collaboration"></a>Diverse Conventions for Human-AI Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15414">http://arxiv.org/abs/2310.15414</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Stanford-ILIAD/Diverse-Conventions">https://github.com/Stanford-ILIAD/Diverse-Conventions</a></li>
<li>paper_authors: Bidipta Sarkar, Andy Shih, Dorsa Sadigh</li>
<li>for: 提高多代理人游戏中的合作性和多样性，使得玩家可以协调共享策略而不需要显式交流。</li>
<li>methods: 使用自适应奖励学习和权衡策略来优化合作策略，并通过权衡策略和前一次发现的策略之间的冲突来生成多样的协议。</li>
<li>results: 在多种多样的合作游戏中，包括Overcooked，技术可以超越人类水平，并且能够适应人类的协议。<details>
<summary>Abstract</summary>
Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce \emph{mixed-play}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits of our technique on various multi-agent collaborative games, including Overcooked, and find that our technique can adapt to the conventions of humans, surpassing human-level performance when paired with real users.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce \emph{mixed-play}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits of our technique on various multi-agent collaborative games, including Overcooked, and find that our technique can adapt to the conventions of humans, surpassing human-level performance when paired with real users.">>  Here's the translation in Simplified Chinese:<<SYS>>合作多代理游戏中，共谱是关键，它允许玩家在不Explicit Communication的情况下协调共同策略。然而，标准的多代理学习技术，如自我玩家，会导致不多样化的共谱，这会导致与新合作伙伴交互时的泛化性强度不高。在这项工作中，我们提出了一种技术来生成多样化的共谱，通过（1）在自我玩家中最大化奖励，而（2）在已经发现的共谱中最小化奖励，以便使共谱具有semantically different的特征。为确保学习的策略在恶性优化的cross-play中保持良好的行为，我们引入了杂合玩家（mixed-play），其中，初始状态由自我玩家和cross-play转移中随机选择，并且玩家学习从这个初始状态中 maximize自我玩家奖励。我们对多种多代理合作游戏，包括Overcooked，进行分析，发现我们的技术可以适应人类的共谱，并且在与真实用户配对时超过人类水平的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/24/cs.AI_2023_10_24/" data-id="cloq1wl1a00657o88h5f095nn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/24/cs.CL_2023_10_24/" class="article-date">
  <time datetime="2023-10-24T11:00:00.000Z" itemprop="datePublished">2023-10-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/24/cs.CL_2023_10_24/">cs.CL - 2023-10-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GlotLID-Language-Identification-for-Low-Resource-Languages"><a href="#GlotLID-Language-Identification-for-Low-Resource-Languages" class="headerlink" title="GlotLID: Language Identification for Low-Resource Languages"></a>GlotLID: Language Identification for Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16248">http://arxiv.org/abs/2310.16248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cisnlp/glotsparse">https://github.com/cisnlp/glotsparse</a></li>
<li>paper_authors: Amir Hossein Kargaran, Ayyoob Imani, François Yvon, Hinrich Schütze</li>
<li>for: 本研究的目的是提供一个可靠、高效的语言识别模型，以推动低资源语言的人工智能技术的普及和提高。</li>
<li>methods: 本研究使用了一种基于对照的语言识别模型，并运用了一些特殊的技术来解决低资源语言的问题，例如：错误的资料metadata、高资源语言的泄漏、等等。</li>
<li>results: 本研究的结果显示，GlotLID-M模型能够实现与先前的模型比较的高度匹配，并且在调整F1和错误率之间取得平衡。此外，研究也显示了一些低资源语言的特殊挑战，例如：metadata错误、语言泄漏、等等。<details>
<summary>Abstract</summary>
Several recent papers have published good solutions for language identification (LID) for about 300 high-resource and medium-resource languages. However, there is no LID available that (i) covers a wide range of low-resource languages, (ii) is rigorously evaluated and reliable and (iii) efficient and easy to use. Here, we publish GlotLID-M, an LID model that satisfies the desiderata of wide coverage, reliability and efficiency. It identifies 1665 languages, a large increase in coverage compared to prior work. In our experiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and NLLB) when balancing F1 and false positive rate (FPR). We analyze the unique challenges that low-resource LID poses: incorrect corpus metadata, leakage from high-resource languages, difficulty separating closely related languages, handling of macrolanguage vs varieties and in general noisy data. We hope that integrating GlotLID-M into dataset creation pipelines will improve quality and enhance accessibility of NLP technology for low-resource languages and cultures. GlotLID-M model, code, and list of data sources are available: https://github.com/cisnlp/GlotLID.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ZzzGPT-An-Interactive-GPT-Approach-to-Enhance-Sleep-Quality"><a href="#ZzzGPT-An-Interactive-GPT-Approach-to-Enhance-Sleep-Quality" class="headerlink" title="ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality"></a>ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16242">http://arxiv.org/abs/2310.16242</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marwahalaofi/ubicomp23-student-challenge">https://github.com/marwahalaofi/ubicomp23-student-challenge</a></li>
<li>paper_authors: Yonchanok Khaokaew, Thuc Hanh Nguyen, Kaixin Ji, Hiruni Kegalle, Marwah Alaofi</li>
<li>for: 这篇论文旨在提供有用的睡眠预测和反馈，以提高用户的睡眠质量。</li>
<li>methods: 该论文使用两stage框架，结合大量自然语言模型（LLMs），以提供准确的睡眠预测和有用的反馈。</li>
<li>results: 该研究使用GLOBEM数据集和生成的 sintetic数据，显示使用XGBoost模型可以提高预测的准确性。<details>
<summary>Abstract</summary>
In today's world, sleep quality is pivotal for overall well-being. While wearable sensors offer real-time monitoring, they often lack actionable insights, leading to user abandonment. This paper delves into the role of technology in understanding sleep patterns. We introduce a two-stage framework, utilizing Large Language Models (LLMs), aiming to provide accurate sleep predictions with actionable feedback. Leveraging the GLOBEM dataset and synthetic data from LLMs, we highlight enhanced results with models like XGBoost. Our approach merges advanced machine learning with user-centric design, blending scientific accuracy with practicality.
</details>
<details>
<summary>摘要</summary>
今天的世界中，睡眠质量对总体健康非常重要。虽然佩戴式感知器可以提供实时监测，但它们经常缺乏有用的反馈，导致用户废弃。这篇论文探讨技术在理解睡眠模式方面的作用。我们介绍了一个两个阶段的框架，利用大语言模型（LLMs），以提供准确的睡眠预测和有用的反馈。我们利用GLOBEM数据集和LLMs生成的 sintetic数据，显示了加强的结果，如XGBoost模型。我们的方法结合了先进的机器学习技术和用户中心的设计，将科学准确性融合到实用中。
</details></li>
</ul>
<hr>
<h2 id="Mixture-of-Linguistic-Experts-Adapters-for-Improving-and-Interpreting-Pre-trained-Language-Models"><a href="#Mixture-of-Linguistic-Experts-Adapters-for-Improving-and-Interpreting-Pre-trained-Language-Models" class="headerlink" title="Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models"></a>Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16240">http://arxiv.org/abs/2310.16240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raymond Li, Gabriel Murray, Giuseppe Carenini</li>
<li>for: 这篇论文旨在将两个流行的研究领域融合到预训练语言模型中，通过在PEFT设置下进行参数有效的微调。</li>
<li>methods: 我们提出了一种将并行适配器模块编码不同语言结构的混合多语言专家架构，使用Gumbel-Softmax门控制每层模型中每个专家的重要性。为降低参数数量，我们先在 fixes 小数量的步骤上训练模型，然后根据专家的重要性分数进行预测。</li>
<li>results: 我们的方法可以与其他PEFT方法相比，在相同的参数数量下达到更高的性能水平。此外，我们还提供了额外的分析，以便对每层模型选择的专家进行深入的探究。<details>
<summary>Abstract</summary>
In this work, we propose a method that combines two popular research areas by injecting linguistic structures into pre-trained language models in the parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel adapter modules encoding different linguistic structures are combined using a novel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates are used to determine the importance of these modules at each layer of the model. To reduce the number of parameters, we first train the model for a fixed small number of steps before pruning the experts based on their importance scores. Our experiment results with three different pre-trained models show that our approach can outperform state-of-the-art PEFT methods with a comparable number of parameters. In addition, we provide additional analysis to examine the experts selected by each model at each layer to provide insights for future studies.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种方法，该方法将两个流行研究领域结合在一起，通过在预训练语言模型的参数高效调教（PEFT）设置中注入语言结构。我们的方法使用一种新的混合语言专家架构，其中Gumbel-Softmax门控制每层模型中每个专家的重要性。为了减少参数数量，我们首先在fixed小数目步骤上训练模型，然后根据每个专家的重要性分数进行专家折叠。我们的实验结果表明，我们的方法可以与同样数量的参数 OUTPERFORM现状的PEFT方法。此外，我们还提供了进一步的分析，以便为未来研究提供洞察。
</details></li>
</ul>
<hr>
<h2 id="TiC-CLIP-Continual-Training-of-CLIP-Models"><a href="#TiC-CLIP-Continual-Training-of-CLIP-Models" class="headerlink" title="TiC-CLIP: Continual Training of CLIP Models"></a>TiC-CLIP: Continual Training of CLIP Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16226">http://arxiv.org/abs/2310.16226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri</li>
<li>for: This paper is written for training vision-language models on time-continuous data, specifically to address the problem of continually training large foundation models without retraining from scratch.</li>
<li>methods: The paper introduces the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models, including TiC-DataCompt, TiC-YFCC, and TiC-RedCaps, which contain over 12.7B timestamped image-text pairs spanning 9 years (2014-2022). The paper also introduces a simple rehearsal-based approach for efficiently training models on time-continuous data.</li>
<li>results: The paper shows that OpenAI’s CLIP (trained on data up to 2020) loses approximately 8% zero-shot accuracy on the curated retrieval task from 2021-2022 compared with more recently trained models in the OpenCLIP repository. The paper also demonstrates that the simple rehearsal-based approach can reduce compute by 2.5 times compared to the standard practice of retraining from scratch.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了训练视觉语言模型而写的，具体来说是为了解决大型基础模型不断 retraining 的问题。</li>
<li>methods: 这篇论文引入了首个 web-scale 时间连续 (TiC) 测试 benchmark  для训练视觉语言模型，包括 TiC-DataCompt、TiC-YFCC 和 TiC-RedCaps，这些 benchmark 包含了9年(2014-2022) 时间内的12.7亿个时间戳image-text对。论文还提出了一种简单的回忆型方法来高效地训练模型。</li>
<li>results: 论文显示，OpenAI的 CLIP (训练到2020年的数据) 在 authors 精心制定的检索任务上减少了约8%的零shot准确率，与 OpenCLIP 库中更近期训练的模型相比。论文还证明了这种简单的回忆型方法可以将计算量减少为2.5倍。<details>
<summary>Abstract</summary>
Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the last checkpoint and replays old data reduces compute by $2.5\times$ when compared to the standard practice of retraining from scratch.
</details>
<details>
<summary>摘要</summary>
维护大型基础模型的最新数据是昂贵的。为了避免不断 retraining 的高成本，需要不断训练这些模型。这个问题被加剧了由于大规模 continual learning 的缺乏标准 benchamarks 和基准。我们发布了首个 web-scale Time-Continual（TiC）benchmarks，包括 TiC-DataCompt、TiC-YFCC 和 TiC-RedCaps，共包含12.7亿个时间戳图像文本对。我们首先使用我们的benchmarks来推动不同的动态评估，以测试时间 robustness 的现有模型。我们发现 OpenAI 的 CLIP（训练数据止2020年）在我们精心编辑的检索任务上的零shot准确率下降了约8%，与更近期训练的 OpenCLIP 存储库中的模型相比。然后我们研究如何有效地在时间连续的数据上训练模型。我们发现一种简单的待机执行方法，通过从上一个检查点继续训练并重复旧数据，可以将计算量减少了2.5倍，相比于标准的重新训练从零开始的做法。
</details></li>
</ul>
<hr>
<h2 id="Background-Summarization-of-Event-Timelines"><a href="#Background-Summarization-of-Event-Timelines" class="headerlink" title="Background Summarization of Event Timelines"></a>Background Summarization of Event Timelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16197">http://arxiv.org/abs/2310.16197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adithya Pratapa, Kevin Small, Markus Dreyer<br>for: 新闻事件的 concise 概述是一项自然语言处理任务的挑战。而新闻记者通常会编辑时间线，以便强调关键的子事件，但新来的读者可能会面临困难catching up with 新闻事件的历史背景。本文提出了背景新闻概述任务，该任务的目的是为每个时间步骤的新闻事件提供相关的前一系列事件的背景概述。我们构建了一个数据集，通过将现有的时间线数据集合并请求人工标注员为每个时间步骤的新闻事件编写背景概述。我们建立了强大的基eline性能，并提出了一种关注点 variant 来生成背景概述。为评估背景概述质量，我们提出了一种问题回答 metric，即背景用户分数（BUS），该分数测量一个当前事件时间步骤的问题中，背景概述是否能够回答。我们的实验表明，经过 fine-tuning 的 Flan-T5 系统以及 GPT-3.5 的强大零 shot 性能。<details>
<summary>Abstract</summary>
Generating concise summaries of news events is a challenging natural language processing task. While journalists often curate timelines to highlight key sub-events, newcomers to a news event face challenges in catching up on its historical context. In this paper, we address this need by introducing the task of background news summarization, which complements each timeline update with a background summary of relevant preceding events. We construct a dataset by merging existing timeline datasets and asking human annotators to write a background summary for each timestep of each news event. We establish strong baseline performance using state-of-the-art summarization systems and propose a query-focused variant to generate background summaries. To evaluate background summary quality, we present a question-answering-based evaluation metric, Background Utility Score (BUS), which measures the percentage of questions about a current event timestep that a background summary answers. Our experiments show the effectiveness of instruction fine-tuned systems such as Flan-T5, in addition to strong zero-shot performance using GPT-3.5.
</details>
<details>
<summary>摘要</summary>
传送新闻事件简要摘要是一项自然语言处理任务，具有挑战性。虽然记者们经常摘要时间线以便强调关键事件，但新手 faced with a news event often faces challenges in understanding its historical context. 在这篇论文中，我们解决这个需求，我们引入背景新闻摘要任务，每个时间点的新闻事件都需要一个背景摘要，涵盖有关的前一系列事件。我们构建了一个数据集，将现有的时间线数据集合并让人工标注者为每个时间点的新闻事件写一个背景摘要。我们建立了强大的基线性能，使用现有的摘要系统，并提出了一种关注点variant来生成背景摘要。为评估背景摘要质量，我们提出了一个问题回答 metric，背景用于评估器（Background Utility Score，BUS），该指标测量一个当前事件时间点的背景摘要是否能回答有关该事件的问题。我们的实验表明，在训练过程中使用 Flan-T5 等系统可以实现有效的辅助 fine-tuning，同时 Zero-shot 性能也非常强。
</details></li>
</ul>
<hr>
<h2 id="BLP-2023-Task-2-Sentiment-Analysis"><a href="#BLP-2023-Task-2-Sentiment-Analysis" class="headerlink" title="BLP 2023 Task 2: Sentiment Analysis"></a>BLP 2023 Task 2: Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16183">http://arxiv.org/abs/2310.16183</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blp-workshop/blp_task2">https://github.com/blp-workshop/blp_task2</a></li>
<li>paper_authors: Md. Arid Hasan, Firoj Alam, Anika Anjum, Shudipta Das, Afiyat Anjum</li>
<li>for: 这个研究是为了探讨社交媒体文本中的 sentiment 检测问题，以便更好地理解用户对产品或服务的看法。</li>
<li>methods: 这个研究使用了多种方法，包括传统机器学习模型、预先训练模型的 fine-tuning、以及大语言模型（LLMs）在零或少数shot设置下的使用。</li>
<li>results: 这个研究共收到了71名参与者的参与，其中29个 коман队在开发阶段提交了系统，并在评估阶段提交了30个系统。总共有597个运行被提交。然而，共有15个团队提交了系统描述文献。<details>
<summary>Abstract</summary>
We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, a total of 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a brief overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the research community, to foster further research in this domain
</details>
<details>
<summary>摘要</summary>
我们提供了BLP情感共享任务的概述，这是BLP 2023工作坊的一部分，并与EMNLP 2023相位。这个任务的定义是社交媒体文本中的情感检测。这个任务吸引了71名参与者的关注，其中29个团队在开发阶段和评估阶段分别提交了597次运行。然而，总共15个团队提交了系统描述论文。参与者们的提交的方法包括经典机器学习模型、精度调整预先训练模型以及在零和几个附加设置下使用大语言模型（LLMs）。在这篇论文中，我们提供了任务设置的详细资料，包括数据集开发和评估设置。此外，我们还提供了参与者们提交的系统的简要概述。所有的数据集和评估脚本都已经公开发布给研究社区，以促进这个领域的进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Hidden-Citations-Obscure-True-Impact-in-Science"><a href="#Hidden-Citations-Obscure-True-Impact-in-Science" class="headerlink" title="Hidden Citations Obscure True Impact in Science"></a>Hidden Citations Obscure True Impact in Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16181">http://arxiv.org/abs/2310.16181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyi Meng, Onur Varol, Albert-László Barabási</li>
<li>for: 本研究旨在探讨科学家如何使用文献来评估发现的影响，并发现了隐藏的参考。</li>
<li>methods: 研究者采用了无监督可解释机器学习方法，对每篇论文的全文进行分析，系统地发现隐藏的参考。</li>
<li>results: 研究发现，对于影响大的发现，隐藏的参考数量更多于公开的参考数量，不受发表venue和学科影响。此外，隐藏参考的存在不是受到公开参考数量的影响，而是受到文献中对话的度量。<details>
<summary>Abstract</summary>
References, the mechanism scientists rely on to signal previous knowledge, lately have turned into widely used and misused measures of scientific impact. Yet, when a discovery becomes common knowledge, citations suffer from obliteration by incorporation. This leads to the concept of hidden citation, representing a clear textual credit to a discovery without a reference to the publication embodying it. Here, we rely on unsupervised interpretable machine learning applied to the full text of each paper to systematically identify hidden citations. We find that for influential discoveries hidden citations outnumber citation counts, emerging regardless of publishing venue and discipline. We show that the prevalence of hidden citations is not driven by citation counts, but rather by the degree of the discourse on the topic within the text of the manuscripts, indicating that the more discussed is a discovery, the less visible it is to standard bibliometric analysis. Hidden citations indicate that bibliometric measures offer a limited perspective on quantifying the true impact of a discovery, raising the need to extract knowledge from the full text of the scientific corpus.
</details>
<details>
<summary>摘要</summary>
科学家们常采用参考来示previous knowledge，但现在这些参考变成了广泛使用且不当地用于科学影响的标准。当发现成为通用知识时，参考会被包含在内化。这会导致隐藏引用的概念，表示文献中的明文资料 credit。我们利用不监督可解释机器学习对每篇论文全文进行系统地寻找隐藏引用。我们发现，对影响发现的隐藏引用数量比引用数量更多，不受出版平台和学科影响。我们表明，隐藏引用的存在不是基于引用数量，而是基于文献中话题的讨论程度， indicating that the more discussed is a discovery, the less visible it is to standard bibliometric analysis。隐藏引用表明， bibliometric measures 只能提供科学发现的有限影响量，需要从科学文献中提取知识。
</details></li>
</ul>
<hr>
<h2 id="WojoodNER-2023-The-First-Arabic-Named-Entity-Recognition-Shared-Task"><a href="#WojoodNER-2023-The-First-Arabic-Named-Entity-Recognition-Shared-Task" class="headerlink" title="WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task"></a>WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16153">http://arxiv.org/abs/2310.16153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa Jarrar, Muhammad Abdul-Mageed, Mohammed Khalilia, Bashar Talafha, AbdelRahim Elmadany, Nagham Hamad, Alaa’ Omar</li>
<li>for: 本文主要关注阿拉伯语命名实体识别（NER）任务，提供了新的NER数据集（i.e., Wojood），并定义了用于促进不同NER方法比较的meaningful的子任务。</li>
<li>methods: 本文使用了45个团队参与了共同任务，其中11个团队参与了测试阶段。specifically, 11 teams participated in FlatNER, while 8 teams tackled NestedNER。</li>
<li>results: 赢得奖winning teams achieved F1 scores of 91.96 and 93.73 in FlatNER and NestedNER, respectively.<details>
<summary>Abstract</summary>
We present WojoodNER-2023, the first Arabic Named Entity Recognition (NER) Shared Task. The primary focus of WojoodNER-2023 is on Arabic NER, offering novel NER datasets (i.e., Wojood) and the definition of subtasks designed to facilitate meaningful comparisons between different NER approaches. WojoodNER-2023 encompassed two Subtasks: FlatNER and NestedNER. A total of 45 unique teams registered for this shared task, with 11 of them actively participating in the test phase. Specifically, 11 teams participated in FlatNER, while $8$ teams tackled NestedNER. The winning teams achieved F1 scores of 91.96 and 93.73 in FlatNER and NestedNER, respectively.
</details>
<details>
<summary>摘要</summary>
我们现在介绍WojoodNER-2023，这是第一个阿拉伯语命名实体识别（NER）共同任务。WojoodNER-2023的主要焦点是阿拉伯语NER，提供了新的NER数据集（即Wojood），以及为不同NER方法进行比较有意义的子任务定义。WojoodNER-2023包括了两个子任务：FlatNER和NestedNER。总共45个团队 регистри过这个共同任务，其中11个团队参加了测试阶段。特别是，11个团队参加了FlatNER，而8个团队解决了NestedNER。赢利的团队在FlatNER和NestedNER中的F1分数分别为91.96和93.73。
</details></li>
</ul>
<hr>
<h2 id="Can-You-Follow-Me-Testing-Situational-Understanding-in-ChatGPT"><a href="#Can-You-Follow-Me-Testing-Situational-Understanding-in-ChatGPT" class="headerlink" title="Can You Follow Me? Testing Situational Understanding in ChatGPT"></a>Can You Follow Me? Testing Situational Understanding in ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16135">http://arxiv.org/abs/2310.16135</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangalan123/situationaltesting">https://github.com/yangalan123/situationaltesting</a></li>
<li>paper_authors: Chenghao Yang, Allyson Ettinger</li>
<li>for: 这篇论文的目的是检验人工智能代理人（ChatGPT）的情景理解（Situational Understanding，SU）能力，以确定其是否具备人类样式的对话能力。</li>
<li>methods: 作者们使用了一个新的synthetic environment来测试ChatGPT的SU能力，通过评估模型在不同环境下的表现来评估其能力。</li>
<li>results: 研究发现，尽管ChatGPT在对话任务上表现出色，但它在保持正确的环境状态方面存在问题。研究人员发现，ChatGPT的表现受到各种因素的影响，包括模型的忘记率和假的更新。这些发现表明，ChatGPT目前还不具备坚定的情景理解能力。<details>
<summary>Abstract</summary>
Understanding sentence meanings and updating information states appropriately across time -- what we call "situational understanding" (SU) -- is a critical ability for human-like AI agents. SU is essential in particular for chat models, such as ChatGPT, to enable consistent, coherent, and effective dialogue between humans and AI. Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs), but the extent and causes of these limitations are not well understood, and capabilities of current chat-based models in this domain have not been explored. In this work we tackle these questions, proposing a novel synthetic environment for SU testing which allows us to do controlled and systematic testing of SU in chat-oriented models, through assessment of models' ability to track and enumerate environment states. Our environment also allows for close analysis of dynamics of model performance, to better understand underlying causes for performance patterns. We apply our test to ChatGPT, the state-of-the-art chatbot, and find that despite the fundamental simplicity of the task, the model's performance reflects an inability to retain correct environment states across time. Our follow-up analyses suggest that performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to hallucinated updates -- including updates that artificially inflate accuracies. Our findings suggest overall that ChatGPT is not currently equipped for robust tracking of situation states, and that trust in the impressive dialogue performance of ChatGPT comes with risks. We release the codebase for reproducing our test environment, as well as all prompts and API responses from ChatGPT, at https://github.com/yangalan123/SituationalTesting.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>人类智能代理机器人需要具备"情境理解"（SU）能力，以便在时间上更新信息状态。chatGPT等 chatbot需要这种能力，以实现人机对话的一致、 coherent 和有效。previous works已经发现了一些 SU 限制在非 chatbot 大语言模型（LLMs）中，但这些限制的EXTENT和原因还不够了解。此外，当前的 chat-based 模型在这个领域的能力还没有得到探索。在这项工作中，我们提出了一种新的情境测试环境，以控制和系统地测试 chat-oriented 模型的 SU 能力。我们通过评估模型的环境状态跟踪和总结来进行测试。我们在这个环境中测试了 chatGPT，发现它的性能表现出了无法保持正确的环境状态的问题。我们的跟进分析表明，chatGPT 的性能下降的主要原因是它没有持续性的内容快照（although it can access the full dialogue history），并且容易受到幻想的更新的影响。我们的发现建议 chatGPT 目前不具备 Robust 的情境跟踪能力，并且对它的印象性对话性能有风险。我们在 GitHub 上发布了测试环境的代码基本，以及所有的提示和 API 响应，可以在 <https://github.com/yangalan123/SituationalTesting> 中下载。
</details></li>
</ul>
<hr>
<h2 id="GenKIE-Robust-Generative-Multimodal-Document-Key-Information-Extraction"><a href="#GenKIE-Robust-Generative-Multimodal-Document-Key-Information-Extraction" class="headerlink" title="GenKIE: Robust Generative Multimodal Document Key Information Extraction"></a>GenKIE: Robust Generative Multimodal Document Key Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16131">http://arxiv.org/abs/2310.16131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/glasgow-ai4biomed/genkie">https://github.com/glasgow-ai4biomed/genkie</a></li>
<li>paper_authors: Panfeng Cao, Ye Wang, Qiang Zhang, Zaiqiao Meng</li>
<li>for: 提高扫描文档中关键信息提取精度</li>
<li>methods: 提出一种新的生成型终端模型（GenKIE），利用多modal编码器将视觉、布局和文本特征嵌入，并使用decoder生成需要的输出</li>
<li>results: 广泛实验表明，GenKIE在不同类型的文档上具有良好的泛化能力，并实现了状态之最的结果，同时模型也具有自动纠正OCR错误的能力。<details>
<summary>Abstract</summary>
Key information extraction (KIE) from scanned documents has gained increasing attention because of its applications in various domains. Although promising results have been achieved by some recent KIE approaches, they are usually built based on discriminative models, which lack the ability to handle optical character recognition (OCR) errors and require laborious token-level labelling. In this paper, we propose a novel generative end-to-end model, named GenKIE, to address the KIE task. GenKIE is a sequence-to-sequence multimodal generative model that utilizes multimodal encoders to embed visual, layout and textual features and a decoder to generate the desired output. Well-designed prompts are leveraged to incorporate the label semantics as the weakly supervised signals and entice the generation of the key information. One notable advantage of the generative model is that it enables automatic correction of OCR errors. Besides, token-level granular annotation is not required. Extensive experiments on multiple public real-world datasets show that GenKIE effectively generalizes over different types of documents and achieves state-of-the-art results. Our experiments also validate the model's robustness against OCR errors, making GenKIE highly applicable in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
针对扫描文档中的关键信息提取（Key Information Extraction，KIE）问题，随着不同领域的应用，拥有增加的关注。虽然一些最新的KIE方法已经实现了可观的成果，但是这些方法通常是基于分类模型，缺乏对光学字符识别（OCR）错误的处理能力，同时需要繁琐的单个字符标注。在这篇论文中，我们提出了一种新的生成型终端模型，名为GenKIE，用于解决KIE任务。GenKIE是一种序列到序列多Modal生成模型，利用多Modal编码器将视觉、格式和文本特征编码，并使用解码器生成需要的输出。Well-designed prompts被利用来把标签 semantics 作为弱样本标注，让生成器自动生成关键信息。一个GenKIE的优点是它可以自动更正 OCR 错误。此外，单个字符精度标注不是必要的。广泛的实验表明，GenKIE可以高效地泛化到不同类型的文档，并 achieve 状态的最佳结果。我们的实验还证明了模型对 OCR 错误的Robustness，使得GenKIE在实际应用中非常可靠。
</details></li>
</ul>
<hr>
<h2 id="Octopus-A-Multitask-Model-and-Toolkit-for-Arabic-Natural-Language-Generation"><a href="#Octopus-A-Multitask-Model-and-Toolkit-for-Arabic-Natural-Language-Generation" class="headerlink" title="Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation"></a>Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16127">http://arxiv.org/abs/2310.16127</a></li>
<li>repo_url: None</li>
<li>paper_authors: AbdelRahim Elmadany, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed</li>
<li>for: 这个研究是为了开发一个可以处理广泛任务的阿拉伯文本生成工具组。</li>
<li>methods: 这个研究使用了一种新的阿拉伯文本转换模型，名为AraT5v2，并在该模型上进行了训练。该训练包括了不同的预训练策略，包括无超给、有超给、共同预训练等。</li>
<li>results: 根据研究结果，这个新的模型在与其他比较基eline的模型进行比较时，具有了大幅度的提升。此外，研究者还开发了一个名为Octopus的Python基本包和命令行工具组，可以帮助开发者快速地进行阿拉伯文本生成任务。<details>
<summary>Abstract</summary>
Understanding Arabic text and generating human-like responses is a challenging endeavor. While many researchers have proposed models and solutions for individual problems, there is an acute shortage of a comprehensive Arabic natural language generation toolkit that is capable of handling a wide range of tasks. In this work, we present a novel Arabic text-to-text Transformer model, namely AraT5v2. Our new model is methodically trained on extensive and diverse data, utilizing an extended sequence length of 2,048 tokens. We explore various pretraining strategies including unsupervised, supervised, and joint pertaining, under both single and multitask settings. Our models outperform competitive baselines with large margins. We take our work one step further by developing and publicly releasing Octopus, a Python-based package and command-line toolkit tailored for eight Arabic generation tasks all exploiting a single model. We release the models and the toolkit on our public repository.
</details>
<details>
<summary>摘要</summary>
理解阿拉伯文本和生成人类化响应是一项复杂的任务。虽然许多研究人员已经提出了模型和解决方案，但是总的来说是缺乏一个全面的阿拉伯自然语言生成工具包，可以处理广泛的任务。在这项工作中，我们提出了一种新的阿拉伯文本-文本变换器模型，即AraT5v2。我们的新模型通过对广泛和多样化的数据进行系统训练，并使用扩展的序列长度2048个元素。我们研究了不同的预训练策略，包括无监督、监督、共同预训练等，在单任务和多任务 setting下进行了测试。我们的模型在比较基eline上表现出了明显的优势。为了进一步推动这项工作，我们还开发了一个名为Octopus的Python基本包和命令行工具集，用于执行八种阿拉伯生成任务，均基于单个模型。我们将模型和工具集公开发布到我们的公共存储库上。
</details></li>
</ul>
<hr>
<h2 id="NADI-2023-The-Fourth-Nuanced-Arabic-Dialect-Identification-Shared-Task"><a href="#NADI-2023-The-Fourth-Nuanced-Arabic-Dialect-Identification-Shared-Task" class="headerlink" title="NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task"></a>NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16117">http://arxiv.org/abs/2310.16117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Abdul-Mageed, AbdelRahim Elmadany, Chiyu Zhang, El Moatez Billah Nagoudi, Houda Bouamor, Nizar Habash</li>
<li>for: 本研究的目的是提高阿拉伯语自然语言处理的状态作图，通过创造合作的研究团队在标准化的环境下竞争。</li>
<li>methods: 本研究使用了多种方法，包括 диалект识别和 machine translation。</li>
<li>results: 研究结果显示，三个子任务仍然具有挑战性，并且鼓励未来的研究。winning teams的成绩为87.27 F1、14.76 Bleu和21.10 Bleu，分别在三个子任务中。<details>
<summary>Abstract</summary>
We describe the findings of the fourth Nuanced Arabic Dialect Identification Shared Task (NADI 2023). The objective of NADI is to help advance state-of-the-art Arabic NLP by creating opportunities for teams of researchers to collaboratively compete under standardized conditions. It does so with a focus on Arabic dialects, offering novel datasets and defining subtasks that allow for meaningful comparisons between different approaches. NADI 2023 targeted both dialect identification (Subtask 1) and dialect-to-MSA machine translation (Subtask 2 and Subtask 3). A total of 58 unique teams registered for the shared task, of whom 18 teams have participated (with 76 valid submissions during test phase). Among these, 16 teams participated in Subtask 1, 5 participated in Subtask 2, and 3 participated in Subtask 3. The winning teams achieved 87.27   F1 on Subtask 1, 14.76 Bleu in Subtask 2, and 21.10 Bleu in Subtask 3, respectively. Results show that all three subtasks remain challenging, thereby motivating future work in this area. We describe the methods employed by the participating teams and briefly offer an outlook for NADI.
</details>
<details>
<summary>摘要</summary>
我团队描述了第四届细腻阿拉伯语言标注分享任务（NADI 2023）的发现。NADI 的目标是通过共同竞争的标准化条件来进步阿拉伯语言处理技术。它专注于阿拉伯方言，提供了新的数据集和定义了可比较的子任务，以便进行有意义的对不同方法的比较。NADI 2023 targeted both dialect identification (Subtask 1) and dialect-to-MSA machine translation (Subtask 2 and Subtask 3). A total of 58 unique teams registered for the shared task, of whom 18 teams participated (with 76 valid submissions during the test phase). Among these, 16 teams participated in Subtask 1, 5 participated in Subtask 2, and 3 participated in Subtask 3. The winning teams achieved 87.27% F1 on Subtask 1, 14.76 Bleu in Subtask 2, and 21.10 Bleu in Subtask 3, respectively. Results show that all three subtasks remain challenging, thereby motivating future work in this area. We describe the methods employed by the participating teams and briefly offer an outlook for NADI.
</details></li>
</ul>
<hr>
<h2 id="Locally-Differentially-Private-Document-Generation-Using-Zero-Shot-Prompting"><a href="#Locally-Differentially-Private-Document-Generation-Using-Zero-Shot-Prompting" class="headerlink" title="Locally Differentially Private Document Generation Using Zero Shot Prompting"></a>Locally Differentially Private Document Generation Using Zero Shot Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16111">http://arxiv.org/abs/2310.16111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saiteja Utpala, Sara Hooker, Pin Yu Chen</li>
<li>for: 防止推导语言模型隐私风险</li>
<li>methods: 提出了一种 мест化差异隐私机制 called DP-Prompt，利用预训练大语言模型和零戳训练提示来对作者解匿攻击进行防御，并最小化下游性能的影响</li>
<li>results: 对IMDB数据集进行测试，DP-Prompt（与ChatGPT结合）可以完美地恢复清洁情感F1分数，同时对于静止攻击者而言，可以实现46%的作者识别F1分数减少，对于适应攻击者而言，可以实现26%的减少<details>
<summary>Abstract</summary>
Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\% reduction in author identification F1 score against static attackers and a 26\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.
</details>
<details>
<summary>摘要</summary>
多数研究已经强调了预训练大型自然语言模型的隐私风险。然而，我们的研究呈现了一种独特的视角，即预训练大型自然语言模型可以有效地增进隐私保护。我们提出了一种本地差分隐私机制called DP-Prompt，利用了预训练大型自然语言模型和零扩展提示的力量，对抗作者去掌握攻击而减少下游实用性的影响。当DP-Prompt与强大的语言模型如ChatGPT（gpt-3.5）一起使用时，我们观察到了对抗攻击者去掌握的成功率下降，而且与现有方法相比，DP-Prompt具有更简单的设计。例如，在IMDB dataset中，DP-Prompt（与ChatGPT）可以完美地恢复清洁的 sentiment F1 分数，同时对于静态攻击者实现46%的减少，对于适应性攻击者实现26%的减少。我们在六个开源大型自然语言模型（最大参数70亿）上进行了广泛的实验，以分析不同的隐私Utility贸易。
</details></li>
</ul>
<hr>
<h2 id="CR-COPEC-Causal-Rationale-of-Corporate-Performance-Changes-to-Learn-from-Financial-Reports"><a href="#CR-COPEC-Causal-Rationale-of-Corporate-Performance-Changes-to-Learn-from-Financial-Reports" class="headerlink" title="CR-COPEC: Causal Rationale of Corporate Performance Changes to Learn from Financial Reports"></a>CR-COPEC: Causal Rationale of Corporate Performance Changes to Learn from Financial Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16095">http://arxiv.org/abs/2310.16095</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cr-copec/cr-copec">https://github.com/cr-copec/cr-copec</a></li>
<li>paper_authors: Ye Eun Chun, Sunjae Kwon, Kyunghwan Sohn, Nakwon Sung, Junyoup Lee, Byungki Seo, Kevin Compher, Seung-won Hwang, Jaesik Choi</li>
<li>for: 这 paper 是为了构建一个大规模域适化 causal 句子数据集，用于检测公司财务性能变化。</li>
<li>methods: 这 paper 使用了 10-K 年度报告中专家的 causal 分析，以满足 accounting 标准。 dataset 可以广泛地用于个人投资者和分析师作为投资决策的材料资源，无需阅读大量文档。</li>
<li>results: 这 paper 在 twelve 个industry 中考虑了不同的特征，因此可以在不同的industry中分辨 causal 句子。 authors 还提供了 dataset 的构建和分析，以及实验代码。<details>
<summary>Abstract</summary>
In this paper, we introduce CR-COPEC called Causal Rationale of Corporate Performance Changes from financial reports. This is a comprehensive large-scale domain-adaptation causal sentence dataset to detect financial performance changes of corporate. CR-COPEC contributes to two major achievements. First, it detects causal rationale from 10-K annual reports of the U.S. companies, which contain experts' causal analysis following accounting standards in a formal manner. This dataset can be widely used by both individual investors and analysts as material information resources for investing and decision making without tremendous effort to read through all the documents. Second, it carefully considers different characteristics which affect the financial performance of companies in twelve industries. As a result, CR-COPEC can distinguish causal sentences in various industries by taking unique narratives in each industry into consideration. We also provide an extensive analysis of how well CR-COPEC dataset is constructed and suited for classifying target sentences as causal ones with respect to industry characteristics. Our dataset and experimental codes are publicly available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们引入了CR-COPEC，即财务报表中公司业绩变化的原因分析 dataset。这是一个广泛的领域适应性 causal 句据集，用于检测公司财务业绩变化。CR-COPEC 在两个主要成果方面做出了贡献。首先，它从美国公司的10-K年度报告中检测出了专家们根据会计标准进行形式化的 causal 分析。这个数据集可以被广泛使用于投资和决策过程中，无需阅读所有文档。其次，它仔细考虑了不同领域对公司财务业绩的影响，因此可以在不同领域中分辨出 causal 句。我们还提供了对 CR-COPEC 数据集的广泛分析和适用于分类目标句的研究。我们的数据集和实验代码都公开可用。
</details></li>
</ul>
<hr>
<h2 id="MuSR-Testing-the-Limits-of-Chain-of-thought-with-Multistep-Soft-Reasoning"><a href="#MuSR-Testing-the-Limits-of-Chain-of-thought-with-Multistep-Soft-Reasoning" class="headerlink" title="MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning"></a>MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16049">http://arxiv.org/abs/2310.16049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zayne-sprague/musr">https://github.com/zayne-sprague/musr</a></li>
<li>paper_authors: Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett</li>
<li>for: 评估大语言模型（LLM）的理性能力</li>
<li>methods: 使用链式思维提示技术和自然语言生成算法</li>
<li>results: 评估多步软理解任务的语言模型表现不佳，需要进一步改进<details>
<summary>Abstract</summary>
While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经具备了一些技术，如链式思维提示，表现出了很好的能力。然而，LLM在复杂的设定下进行Robust reasoning仍然弱点。然而，评估LLM的理解是困难的，因为系统的能力不断提高，而对于逻辑推理的 benchmark数据集仍然保持不变。我们介绍了 MuSR，一个用于评估语言模型多步软件逻辑任务的自然语言 narative。这个数据集有两个重要特点：首先，它通过一种新的 neurosymbolic 生成算法来生成复杂的逻辑任务，可以挑战 GPT-4（例如，谋杀 MYSTERY 约1000个单词长），并可以进一步扩展为更有能力的 LLM 发布。其次，我们的数据集实例是自然语言 narative，与实际世界的 reasoning Domain相对应，这使得它比其他生成的 benchmark 更加具有挑战性和真实性，同时也可以让人类评估员解决高度准确。我们对这些数据集进行了一系列LLM和提示技术的评估，并描述了链式思维在这些任务中的缺陷。
</details></li>
</ul>
<hr>
<h2 id="Visual-Cropping-Improves-Zero-Shot-Question-Answering-of-Multimodal-Large-Language-Models"><a href="#Visual-Cropping-Improves-Zero-Shot-Question-Answering-of-Multimodal-Large-Language-Models" class="headerlink" title="Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models"></a>Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16033">http://arxiv.org/abs/2310.16033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saccharomycetes/visual_crop_zsvqa">https://github.com/saccharomycetes/visual_crop_zsvqa</a></li>
<li>paper_authors: Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski</li>
<li>for: 这 paper  investigate  multimodal Large Language Models (LLMs) 在视觉问答 (VQA) 任务中的限制，特别是它们是否可以正确地捕捉图像中的小型细节。</li>
<li>methods: 这 paper 使用了 multimodal LLMs 和人工视觉截割来提高 VQA 任务的性能。</li>
<li>results: 研究发现，multimodal LLMs 在图像中的小型细节捕捉能力很弱，其 zero-shot 性能与问题中图像Subject的大小有直接关系，随着Subject的尺寸增大，性能下降至 46%。此外，研究发现，人工视觉截割可以有效地改善 multimodal LLMs 的性能。<details>
<summary>Abstract</summary>
Multimodal Large Language Models (LLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether multimodal LLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to $46\%$ with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose three automatic visual cropping methods as inference time mechanisms to improve the zero-shot performance of multimodal LLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that multimodal LLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. Our code and data are publicly available.
</details>
<details>
<summary>摘要</summary>
多modal大语言模型（LLMs）在视觉问答（VQA）任务上最近获得了promising的零shot准确率——一个影响多个下渠应用和领域的基本任务。考虑到这些模型的广泛应用的潜力，因此调查其对不同图像和问题属性的局限性是非常重要的。在这项工作中，我们调查多modal LLMs是否可以正确地捕捉图像中的小 Details和大 Details。具体来说，我们发现其零shot准确率在回答视觉问题时对图像中的视觉主题大小具有极高的敏感性，随着主题大小的增加，准确率可以下降至46%。此外，我们发现这种效果是 causal，因为人类视觉cropping可以明显减轻其对大小的敏感性。 inspirited by the usefulness of human cropping，我们提出了三种自动视觉cropping方法作为infere时机制来提高多modal LLMs的零shot性能。我们在四个popular VQA dataset上研究了这些方法的效果，并在VQAv2 dataset上进行了一些精细Visual Details的子集研究。我们的发现表明：多modal LLMs在细节敏感的VQA应用中应用should be cautious，而visual cropping是一个有前途的方向来提高其零shot性能。我们的代码和数据公开可用。
</details></li>
</ul>
<hr>
<h2 id="Mixture-of-Tokens-Efficient-LLMs-through-Cross-Example-Aggregation"><a href="#Mixture-of-Tokens-Efficient-LLMs-through-Cross-Example-Aggregation" class="headerlink" title="Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation"></a>Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15961">http://arxiv.org/abs/2310.15961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Szymon Antoniak, Sebastian Jaszczur, Michał Krutul, Maciej Pióro, Jakub Krajewski, Jan Ludziejewski, Tomasz Odrzygóźdź, Marek Cygan</li>
<li>for: 提高Transformer模型的参数计数，保持训练和推理成本的MoE模型。</li>
<li>methods: 使用 feed-forward layer中的一些专家来代表所有token，但这会导致训练不稳定和专家使用不均匀。</li>
<li>results: 提出了一种新的Mixture of Tokens模型，可以保持MoE模型的好处而不是经受以上问题，通过将token从不同的示例混合而不是路由它们到专家。<details>
<summary>Abstract</summary>
Despite the promise of Mixture of Experts (MoE) models in increasing parameter counts of Transformer models while maintaining training and inference costs, their application carries notable drawbacks. The key strategy of these models is to, for each processed token, activate at most a few experts - subsets of an extensive feed-forward layer. But this approach is not without its challenges. The operation of matching experts and tokens is discrete, which makes MoE models prone to issues like training instability and uneven expert utilization. Existing techniques designed to address these concerns, such as auxiliary losses or balance-aware matching, result either in lower model performance or are more difficult to train. In response to these issues, we propose Mixture of Tokens, a fully-differentiable model that retains the benefits of MoE architectures while avoiding the aforementioned difficulties. Rather than routing tokens to experts, this approach mixes tokens from different examples prior to feeding them to experts, enabling the model to learn from all token-expert combinations. Importantly, this mixing can be disabled to avoid mixing of different sequences during inference. Crucially, this method is fully compatible with both masked and causal Large Language Model training and inference.
</details>
<details>
<summary>摘要</summary>
尽管混合专家（MoE）模型在提高Transformer模型参数数量的同时保持训练和推理成本的承诺，但它们的应用还存在一些困难。这些模型的关键策略是，对每个处理的单词，只启用最多几个专家——Feed-Forward层中的子集。但这种方法存在许多问题，如训练不稳定和专家不均衡使用。现有的技术，如辅助损失或平衡感知匹配，可以解决这些问题，但它们会导致模型性能下降或更难以训练。为了解决这些问题，我们提议 Mixture of Tokens，一种完全可导的模型，保留MoE架构的优点而避免上述困难。而不是路由单词到专家，这种方法将不同的单词混合在一起，以便模型可以学习所有单词-专家组合。这种混合可以被禁用，以避免在推理过程中混合不同的序列。此外，这种方法与 маsked和 causal Large Language Model 的训练和推理完全兼容。
</details></li>
</ul>
<hr>
<h2 id="NoteChat-A-Dataset-of-Synthetic-Doctor-Patient-Conversations-Conditioned-on-Clinical-Notes"><a href="#NoteChat-A-Dataset-of-Synthetic-Doctor-Patient-Conversations-Conditioned-on-Clinical-Notes" class="headerlink" title="NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes"></a>NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15959">http://arxiv.org/abs/2310.15959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junda Wang, Zonghai Yao, Zhichao Yang, Huixue Zhou, Rumeng Li, Xun Wang, Yucheng Xu, Hong Yu</li>
<li>for: 这个论文旨在Automating the creation of clinical records drafted by doctors after each patient’s visit, using language models to reduce the workload of doctors.</li>
<li>methods: 该论文提出了一种名为NoteChat的多agger扩展模型，利用大型自然语言模型（LLMs）生成医生与病人之间的合作对话，并且可以根据医疗记录来conditioning。NoteChat包括规划、角色扮演和精炼模块。</li>
<li>results: 对于NoteChat，我们提供了全自动和人工评估，与当前状态的模型进行比较，包括OpenAI的ChatGPT和GPT-4。结果表明，NoteChat可以生成高质量的医生与病人之间的合作对话，从而探索人工智能在医疗领域的潜在应用。这是多个LLMs合作完成基于医疗记录的医生与病人对话的第一个实例，为人工智能在医疗领域的发展提供了promising的可能性。<details>
<summary>Abstract</summary>
The detailed clinical records drafted by doctors after each patient's visit are crucial for medical practitioners and researchers. Automating the creation of these notes with language models can reduce the workload of doctors. However, training such models can be difficult due to the limited public availability of conversations between patients and doctors. In this paper, we introduce NoteChat, a cooperative multi-agent framework leveraging Large Language Models (LLMs) for generating synthetic doctor-patient conversations conditioned on clinical notes. NoteChat consists of Planning, Roleplay, and Polish modules. We provide a comprehensive automatic and human evaluation of NoteChat, comparing it with state-of-the-art models, including OpenAI's ChatGPT and GPT-4. Results demonstrate that NoteChat facilitates high-quality synthetic doctor-patient conversations, underscoring the untapped potential of LLMs in healthcare. This work represents the first instance of multiple LLMs cooperating to complete a doctor-patient conversation conditioned on clinical notes, offering promising avenues for the intersection of AI and healthcare
</details>
<details>
<summary>摘要</summary>
医生 после每次诊断的细节记录是医疗干部和研究人员的关键资料。使用语言模型自动生成这些笔记可以减轻医生的工作负担。然而，训练这些模型可以困难，因为医生和病人之间的对话非常有限。在这篇论文中，我们介绍NoteChat，一种合作多代理框架，利用大型语言模型（LLMs）生成基于医疗笔记的 sintetic 医生-病人对话。NoteChat包括规划、角色扮演和磨练模块。我们提供了完整的自动和人工评估NoteChat，与现有模型，包括OpenAI的ChatGPT和GPT-4进行比较。结果表明，NoteChat可以生成高质量的 sintetic 医生-病人对话，强调了人工智能在医疗领域的潜在价值。这是首次多个LLMs合作完成基于医疗笔记的医生-病人对话，提供了潜在的人工智能和医疗领域的交叉点。
</details></li>
</ul>
<hr>
<h2 id="This-is-not-a-Dataset-A-Large-Negation-Benchmark-to-Challenge-Large-Language-Models"><a href="#This-is-not-a-Dataset-A-Large-Negation-Benchmark-to-Challenge-Large-Language-Models" class="headerlink" title="This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models"></a>This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15941">http://arxiv.org/abs/2310.15941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hitz-zentroa/this-is-not-a-dataset">https://github.com/hitz-zentroa/this-is-not-a-dataset</a></li>
<li>paper_authors: Iker García-Ferrero, Begoña Altuna, Javier Álvez, Itziar Gonzalez-Dios, German Rigau</li>
<li>for: 这 paper 是为了解释 LLMs 对否定语言的理解能力的研究。</li>
<li>methods: 该 paper 使用了一个大量自动生成的描述句子数据集，用于测试 LLMs 的总结和推理能力。</li>
<li>results: 研究发现， LLMs 在处理负面句子时表现不佳，通常仅仅依靠 superficiale 的cue。 fine-tuning 模型可以提高其表现，但negation 理解和总结仍然存在挑战。<details>
<summary>Abstract</summary>
Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型（LLMs）已经显示出了一定程度的语法知识和总结能力，但它们对否定的解释仍然存在困难。我们尝试解释LLMs理解否定的原因。我们创建了一个大型 semi-自动生成的描述句子集，包含约400,000个描述句子，其中否定存在约2/3的句子中，具有不同的形式。我们使用了我们的数据集和最大可用的开放LLMs进行零容量方法来评估这些模型的总结和推理能力，并对一些模型进行了微调以评估否定理解是否可以被训练。我们的发现表明，虽然LLMs在有Affirmative句子上表现出色，但它们对Negative句子表示困难，并且缺乏深入的否定理解，通常仅仅依靠 superficies 的cue。虽然微调模型可以提高其表现，但否定理解的总体化问题仍然存在，这 highlights  continue challenges of LLMs  regarding negation understanding and generalization.我们的数据集和代码公开available。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-based-Sentence-Encoders-Implicitly-Weight-Informative-Words"><a href="#Contrastive-Learning-based-Sentence-Encoders-Implicitly-Weight-Informative-Words" class="headerlink" title="Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words"></a>Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15921">http://arxiv.org/abs/2310.15921</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kuriyan1204/sentence-encoder-word-weighting">https://github.com/kuriyan1204/sentence-encoder-word-weighting</a></li>
<li>paper_authors: Hiroto Kurita, Goro Kobayashi, Sho Yokoi, Kentaro Inui</li>
<li>for: 这篇论文旨在提高句子编码器的性能，通过对比损失进行微调。</li>
<li>methods: 这篇论文使用了对比学习，并通过 тео리тиче和实验方法显示了模型在对比学习过程中对字符的重要性的影响。</li>
<li>results: 实验结果表明，对比 fine-tuning 会使模型对 informative 字符进行更多的重要性赋值，而不重要的字符则 receive 更少的重要性。<details>
<summary>Abstract</summary>
The performance of sentence encoders can be significantly improved through the simple practice of fine-tuning using contrastive loss. A natural question arises: what characteristics do models acquire during contrastive learning? This paper theoretically and experimentally shows that contrastive-based sentence encoders implicitly weight words based on information-theoretic quantities; that is, more informative words receive greater weight, while others receive less. The theory states that, in the lower bound of the optimal value of the contrastive learning objective, the norm of word embedding reflects the information gain associated with the distribution of surrounding words. We also conduct comprehensive experiments using various models, multiple datasets, two methods to measure the implicit weighting of models (Integrated Gradients and SHAP), and two information-theoretic quantities (information gain and self-information). The results provide empirical evidence that contrastive fine-tuning emphasizes informative words.
</details>
<details>
<summary>摘要</summary>
通过简单的对比损失精度调整，可以大幅提高句子编码器的性能。一个自然的问题是：模型在对比学习中所获得的特征是什么？这篇论文通过理论和实验表明，对比学习基于句子编码器会隐式地根据信息理论量来Weight词语，即更有信息的词语会 receiving 更大的权重，而其他词语则 receiving 更小的权重。我们的理论表明，在最优化对比学习目标下的下界中，词语 embedding 的 нор幅反映了周围词语分布中的信息增加。我们还进行了多种模型、多个数据集、两种测量模型（集成梯度和SHAP）以及两种信息理论量（信息增加和自信息）的实验。结果证明了，对比精度调整会强调有用的词语。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-Creates-Task-Vectors"><a href="#In-Context-Learning-Creates-Task-Vectors" class="headerlink" title="In-Context Learning Creates Task Vectors"></a>In-Context Learning Creates Task Vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15916">http://arxiv.org/abs/2310.15916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/roeehendel/icl_task_vectors">https://github.com/roeehendel/icl_task_vectors</a></li>
<li>paper_authors: Roee Hendel, Mor Geva, Amir Globerson</li>
<li>for: 该研究探讨了宏语言模型（LLM）中的协同学习（ICL） paradigma的下面机制。</li>
<li>methods: 该研究使用了一系列实验来证明ICL学习的函数结构往往非常简单，即将训练集($S$)映射到一个单个任务向量($\boldsymbol{\theta}(S)$)，然后使用这个任务向量来修饰transformer模型生成输出。</li>
<li>results: 该研究通过对多种模型和任务进行广泛的实验来支持上述声明，并证明ICL可以压缩训练集($S$)到一个单个任务向量($\boldsymbol{\theta}(S)$)，从而使用这个任务向量来修饰transformer模型生成输出。<details>
<summary>Abstract</summary>
In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the "standard" machine learning framework, where one uses a training set $S$ to find a best-fitting function $f(x)$ in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query $x$ and a single "task vector" calculated from the training set. Thus, ICL can be seen as compressing $S$ into a single task vector $\boldsymbol{\theta}(S)$ and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.
</details>
<details>
<summary>摘要</summary>
受Contextual learning（ICL）在大语言模型（LLM）中emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the "standard" machine learning framework, where one uses a training set $S$ to find a best-fitting function $f(x)$ in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query $x$ and a single "task vector" calculated from the training set. Thus, ICL can be seen as compressing $S$ into a single task vector $\boldsymbol{\theta}(S)$ and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.Here's the translation in Traditional Chinese:受Contextual learning（ICL）在大语言模型（LLM）中emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the "standard" machine learning framework, where one uses a training set $S$ to find a best-fitting function $f(x)$ in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query $x$ and a single "task vector" calculated from the training set. Thus, ICL can be seen as compressing $S$ into a single task vector $\boldsymbol{\theta}(S)$ and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.
</details></li>
</ul>
<hr>
<h2 id="Do-Stochastic-Parrots-have-Feelings-Too-Improving-Neural-Detection-of-Synthetic-Text-via-Emotion-Recognition"><a href="#Do-Stochastic-Parrots-have-Feelings-Too-Improving-Neural-Detection-of-Synthetic-Text-via-Emotion-Recognition" class="headerlink" title="Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition"></a>Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15904">http://arxiv.org/abs/2310.15904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alanagiasi/emoplmsynth">https://github.com/alanagiasi/emoplmsynth</a></li>
<li>paper_authors: Alan Cowap, Yvette Graham, Jennifer Foster</li>
<li>for:  This paper aims to address the issue of identifying synthetic text generated by high-performance generative AI models, specifically by leveraging the emotional content present in human-authored text.</li>
<li>methods:  The authors fine-tune pre-trained language models (PLMs) on emotion to develop an emotionally-aware detector, which is tested on various synthetic text generators, model sizes, datasets, and domains.</li>
<li>results:  The emotionally-aware detector achieves significant improvements in identifying synthetic text, particularly when compared to ChatGPT, reinforcing the potential of emotion as a signal for identifying synthetic text.<details>
<summary>Abstract</summary>
Recent developments in generative AI have shone a spotlight on high-performance synthetic text generation technologies. The now wide availability and ease of use of such models highlights the urgent need to provide equally powerful technologies capable of identifying synthetic text. With this in mind, we draw inspiration from psychological studies which suggest that people can be driven by emotion and encode emotion in the text they compose. We hypothesize that pretrained language models (PLMs) have an affective deficit because they lack such an emotional driver when generating text and consequently may generate synthetic text which has affective incoherence i.e. lacking the kind of emotional coherence present in human-authored text. We subsequently develop an emotionally aware detector by fine-tuning a PLM on emotion. Experiment results indicate that our emotionally-aware detector achieves improvements across a range of synthetic text generators, various sized models, datasets, and domains. Finally, we compare our emotionally-aware synthetic text detector to ChatGPT in the task of identification of its own output and show substantial gains, reinforcing the potential of emotion as a signal to identify synthetic text. Code, models, and datasets are available at https: //github.com/alanagiasi/emoPLMsynth
</details>
<details>
<summary>摘要</summary>
近期的生成AI发展把关注高性能的合成文本生成技术。现在这些模型的广泛可用性和使用的容易度，强调了需要提供相应的技术来识别合成文本。基于心理学研究，我们发现人们在编写文本时会受到情感驱动，并且在文本中编码情感。我们 hypothesize that 预训练语言模型（PLM）缺乏情感驱动，因此可能会生成无情感含量的合成文本，即情感无 coherence。我们随后开发了一种情感意识检测器，通过细化 PLM 来实现。实验结果表明，我们的情感意识检测器在不同的合成文本生成器、模型大小、数据集和领域中均显示出改进。最后，我们与 ChatGPT 进行了对自己输出的识别任务，并显示了substantial 的提高，证明情感可以作为识别合成文本的信号。代码、模型和数据集可以在 <https://github.com/alanagiasi/emoPLMsynth> 上获取。
</details></li>
</ul>
<hr>
<h2 id="BianQue-Balancing-the-Questioning-and-Suggestion-Ability-of-Health-LLMs-with-Multi-turn-Health-Conversations-Polished-by-ChatGPT"><a href="#BianQue-Balancing-the-Questioning-and-Suggestion-Ability-of-Health-LLMs-with-Multi-turn-Health-Conversations-Polished-by-ChatGPT" class="headerlink" title="BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT"></a>BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15896">http://arxiv.org/abs/2310.15896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scutcyr/bianque">https://github.com/scutcyr/bianque</a></li>
<li>paper_authors: Yirong Chen, Zhenyu Wang, Xiaofen Xing, huimin zheng, Zhipei Xu, Kai Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, Xiangmin Xu</li>
<li>For: The paper aims to improve the chain of questioning (CoQ) of large language models (LLMs) in providing personalized and effective health suggestions.* Methods: The proposed BianQue model is a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus, which includes multiple turns of questioning and health suggestions polished by ChatGPT.* Results: Experimental results demonstrate that the proposed BianQue can simultaneously balance the capabilities of both questioning and health suggestions, which will help promote the research and application of LLMs in the field of proactive health.Here is the information in Simplified Chinese text:* For: 本研究目的是提高大语言模型（LLMs）的问题链（CoQ），以提供个性化和有效的健康建议。* Methods: 我们提出的 BianQue 模型是基于 ChatGLM 的 LLM，通过自构建的健康对话集 BianQueCorpus 进行 fine-tuning，该集包括多个问题和健康建议的循环对话，并通过 ChatGPT 进行精细调整。* Results: 实验结果表明，我们的 BianQue 可以同时保持问题和健康建议的能力，从而推动 LLMs 在健康预防领域的研究和应用。<details>
<summary>Abstract</summary>
Large language models (LLMs) have performed well in providing general and extensive health suggestions in single-turn conversations, exemplified by systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning. In real-world medical consultations, doctors usually employ a series of iterative inquiries to comprehend the patient's condition thoroughly, enabling them to provide effective and personalized suggestions subsequently, which can be defined as chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health suggestions polished by ChatGPT. Experimental results demonstrate that the proposed BianQue can simultaneously balance the capabilities of both questioning and health suggestions, which will help promote the research and application of LLMs in the field of proactive health.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)大型语言模型（LLMs）在单转对话中表现良好，如ChatGPT、ChatGLM、ChatDoctor、DoctorGLM等系统。然而，用户在单转提供的信息有限，导致生成的建议没有充分个性化和目标化，需要用户独立选择有用的部分。这主要由单转问题的缺失引起。在实际医疗咨询中，医生通常采用多个迭代问题来深入了解患者的情况，以便提供有效和个性化的建议，这可以定义为链条问题（CoQ） для LLMs。为了提高LLMs的CoQ，我们提出 BianQue，基于ChatGLM的LLM，通过自动构建的健康对话集 BianQueCorpus 进行微调，该集包含多个问题和健康建议的多个转换。实验结果表明，我们的 BianQue 可以同时保持问题和健康建议的能力，这将有助于推动LLMs在健康预防领域的研究和应用。
</details></li>
</ul>
<hr>
<h2 id="A-Contextualized-Real-Time-Multimodal-Emotion-Recognition-for-Conversational-Agents-using-Graph-Convolutional-Networks-in-Reinforcement-Learning"><a href="#A-Contextualized-Real-Time-Multimodal-Emotion-Recognition-for-Conversational-Agents-using-Graph-Convolutional-Networks-in-Reinforcement-Learning" class="headerlink" title="A Contextualized Real-Time Multimodal Emotion Recognition for Conversational Agents using Graph Convolutional Networks in Reinforcement Learning"></a>A Contextualized Real-Time Multimodal Emotion Recognition for Conversational Agents using Graph Convolutional Networks in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18363">http://arxiv.org/abs/2310.18363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fathima Abdul Rahman, Guang Lu</li>
<li>for: 本研究旨在提高对话代理人的情感认知能力，以提供更加人性化的交互体验。</li>
<li>methods: 本研究使用图像、视频和文本模式的图 convolutional neural network（GCN）和奖励学习（RL）来实现情感认知。</li>
<li>results: 对比其他状态当前模型，conER-GRL模型在IEMOCAP数据集上表现出了优于其他模型的情感认知能力。<details>
<summary>Abstract</summary>
Owing to the recent developments in Generative Artificial Intelligence (GenAI) and Large Language Models (LLM), conversational agents are becoming increasingly popular and accepted. They provide a human touch by interacting in ways familiar to us and by providing support as virtual companions. Therefore, it is important to understand the user's emotions in order to respond considerately. Compared to the standard problem of emotion recognition, conversational agents face an additional constraint in that recognition must be real-time. Studies on model architectures using audio, visual, and textual modalities have mainly focused on emotion classification using full video sequences that do not provide online features. In this work, we present a novel paradigm for contextualized Emotion Recognition using Graph Convolutional Network with Reinforcement Learning (conER-GRL). Conversations are partitioned into smaller groups of utterances for effective extraction of contextual information. The system uses Gated Recurrent Units (GRU) to extract multimodal features from these groups of utterances. More importantly, Graph Convolutional Networks (GCN) and Reinforcement Learning (RL) agents are cascade trained to capture the complex dependencies of emotion features in interactive scenarios. Comparing the results of the conER-GRL model with other state-of-the-art models on the benchmark dataset IEMOCAP demonstrates the advantageous capabilities of the conER-GRL architecture in recognizing emotions in real-time from multimodal conversational signals.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SoK-Memorization-in-General-Purpose-Large-Language-Models"><a href="#SoK-Memorization-in-General-Purpose-Large-Language-Models" class="headerlink" title="SoK: Memorization in General-Purpose Large Language Models"></a>SoK: Memorization in General-Purpose Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18362">http://arxiv.org/abs/2310.18362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, Robert West</li>
<li>for: 本研究旨在探讨大语言模型（LLM）在各种应用中的发展，以及LLM memorization的问题。</li>
<li>methods: 本研究使用了一种新的分类方法，以描述LLM中的 memorization 类型，包括 verbatim text、事实、想法、算法、写作风格、分布性和对齐目标。</li>
<li>results: 研究发现，LLM可以记忆短语和概念，但也可能记忆文本中的具体信息和写作风格。这些记忆可能导致隐私和安全问题，同时也可能提高模型的性能。研究还揭示了LLM的各种问题，如模型行为定义和排序算法的影响。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are advancing at a remarkable pace, with myriad applications under development. Unlike most earlier machine learning models, they are no longer built for one specific application but are designed to excel in a wide range of tasks. A major part of this success is due to their huge training datasets and the unprecedented number of model parameters, which allow them to memorize large amounts of information contained in the training data. This memorization goes beyond mere language, and encompasses information only present in a few documents. This is often desirable since it is necessary for performing tasks such as question answering, and therefore an important part of learning, but also brings a whole array of issues, from privacy and security to copyright and beyond. LLMs can memorize short secrets in the training data, but can also memorize concepts like facts or writing styles that can be expressed in text in many different ways. We propose a taxonomy for memorization in LLMs that covers verbatim text, facts, ideas and algorithms, writing styles, distributional properties, and alignment goals. We describe the implications of each type of memorization - both positive and negative - for model performance, privacy, security and confidentiality, copyright, and auditing, and ways to detect and prevent memorization. We further highlight the challenges that arise from the predominant way of defining memorization with respect to model behavior instead of model weights, due to LLM-specific phenomena such as reasoning capabilities or differences between decoding algorithms. Throughout the paper, we describe potential risks and opportunities arising from memorization in LLMs that we hope will motivate new research directions.
</details>
<details>
<summary>摘要</summary>
LLMs 可以记忆短语、事实、写作风格、分布性、对齐目标等。我们提出了 LLMs 的记忆分类，并描述了每种记忆的正面和负面影响，包括模型性能、隐私、安全、版权等方面。我们还描述了如何检测和预防记忆。然而，由于 LLMs 的特殊性，如推理能力或decoding算法的差异，我们需要更加注意记念的定义方式。在这篇论文中，我们描述了 LLMs 的记忆所带来的风险和机遇，希望能够激发新的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Self-Guard-Empower-the-LLM-to-Safeguard-Itself"><a href="#Self-Guard-Empower-the-LLM-to-Safeguard-Itself" class="headerlink" title="Self-Guard: Empower the LLM to Safeguard Itself"></a>Self-Guard: Empower the LLM to Safeguard Itself</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15851">http://arxiv.org/abs/2310.15851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei Lin, Kam-Fai Wong</li>
<li>for: 防止Large Language Model（LLM）被破坏攻击，避免LLM生成危险内容的负面社会影响。</li>
<li>methods: 提出了两种主要方法来解决破坏攻击：安全培训和安全保护。安全培训通过进一步训练LLM提高其安全性，而安全保护则通过外部模型或筛选器来防止危险输出。但是，安全培训在新型攻击的适应性有限，常会导致模型性能下降，而安全保护的帮助有限。</li>
<li>results: Self-Guard方法可以强化LLM对危险内容识别的能力，并使LLM能够自动检测和避免破坏攻击。实验结果表明，Self-Guard方法具有Robust性和稳定性。在坏CASE分析中，我们发现LLM occasional提供无害回答危险查询。此外，我们还评估了LLM的总能力之前和之后安全培训，证明Self-Guard方法不会导致LLM性能下降。在敏感测试中，Self-Guard方法不仅避免了LLM偏敏的问题，而且还可以减轻这种问题。<details>
<summary>Abstract</summary>
The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experiment has demonstrated that Self-Guard is robust against jailbreak attacks. In the bad case analysis, we find that LLM occasionally provides harmless responses to harmful queries. Additionally, we evaluated the general capabilities of the LLM before and after safety training, providing evidence that Self-Guard does not result in the LLM's performance degradation. In sensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM but also can even mitigate this issue.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的安全措施可以被绕过，从而生成危险的内容。这种LLM的诈骗使得社会产生了负面的后果。目前，有两种主要方法来解决绑架攻击：安全训练和安全措施。安全训练是对LLM进行进一步训练，以增强其安全性。而安全措施则是通过对LLM的输出进行外部模型或节流的实现，以防止危险的输出。但是，安全训练受到新的攻击类型的限制，而且常会导致模型性能下降。安全措施则被证明是有限的帮助。为了解决这些问题，我们提出了一个新的方法 called Self-Guard，它结合了安全训练和安全措施的优点。Self-Guard包括两个阶段。在第一阶段，我们将增强LLM的危险内容评估能力。在第二阶段，我们将 instrucLLM 通过自己的回应来预防危险输出。实验结果表明，Self-Guard具有对绑架攻击的Robust性。在坏情况分析中，我们发现LLM occasional提供无害回应危险查询。此外，我们还评估了LLM的一般能力之前和之后安全训练，证明Self-Guard不会导致LLM的性能下降。在敏感测试中，Self-Guard不仅能避免对LLM的过敏化，而且甚至可以缓和这个问题。
</details></li>
</ul>
<hr>
<h2 id="Unnatural-language-processing-How-do-language-models-handle-machine-generated-prompts"><a href="#Unnatural-language-processing-How-do-language-models-handle-machine-generated-prompts" class="headerlink" title="Unnatural language processing: How do language models handle machine-generated prompts?"></a>Unnatural language processing: How do language models handle machine-generated prompts?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15829">http://arxiv.org/abs/2310.15829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Corentin Kervadec, Francesca Franzon, Marco Baroni</li>
<li>for: 这篇论文主要是为了研究语言模型推荐的优化问题。</li>
<li>methods: 这篇论文使用了自动生成的token序列来检测语言模型的响应。</li>
<li>results: 研究发现，自动生成的token序列可以 Routinely outperform manually crafted prompts，并且可以让模型表现出不同的响应模式。<details>
<summary>Abstract</summary>
Language model prompt optimization research has shown that semantically and grammatically well-formed manually crafted prompts are routinely outperformed by automatically generated token sequences with no apparent meaning or syntactic structure, including sequences of vectors from a model's embedding space. We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions. We study the behavior of models of different sizes in multiple semantic tasks in response to both continuous and discrete machine-generated prompts, and compare it to the behavior in response to human-generated natural-language prompts. Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles. We provide preliminary insight into the nature of the units activated by different prompt types, suggesting that only natural language prompts recruit a genuinely linguistic circuit.
</details>
<details>
<summary>摘要</summary>
Language model 提示优化研究显示，通常有意义和 grammatical 的手动生成的提示将被自动生成的Token序列所超越，包括模型的 embedding 空间中的Vector序列。我们使用机器生成的提示来探索模型对不同类型的输入的响应。我们研究不同大小的模型在多个semantic任务中对于连续和离散机器生成的提示的响应，并与人类生成的自然语言提示进行比较。即使生成相同的输出，机器生成和人类提示仍会触发不同的网络处理路径，包括不同的混乱度、注意力和输出 entropy 分布、和不同的单元活动profile。我们提供了初步的启示，表明只有自然语言提示会激活真正的语言环路。
</details></li>
</ul>
<hr>
<h2 id="Generative-Language-Models-Exhibit-Social-Identity-Biases"><a href="#Generative-Language-Models-Exhibit-Social-Identity-Biases" class="headerlink" title="Generative Language Models Exhibit Social Identity Biases"></a>Generative Language Models Exhibit Social Identity Biases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15819">http://arxiv.org/abs/2310.15819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Sander van der Linden, Jon Roozenbeek</li>
<li>for: 这个研究探讨了现代大语言模型是否具有基本社会身份偏见，以及这些偏见是如何从人类语言模型中学习的。</li>
<li>methods: 研究人员使用51个大语言模型进行调查，发现大多数基础语言模型和一些指导练习模型在完成句子时表现出明显的团队positive和对外group negative的偏见。</li>
<li>results: 研究发现，通过对模型在练习数据中含有更多团队positive或对外group negative句子的控制，可以使模型表现出更高的团队solidarity和更大的对外group hostility。此外，从练习数据中去除团队positive或对外group negative句子也可以减少模型的偏见。这些结果表明，现代大语言模型具有基本社会身份偏见，并且可以通过精心制定练习数据来减少这些偏见。<details>
<summary>Abstract</summary>
The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. In this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. We find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g., "We are..."). A comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. To investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the United States Democrat-Republican divide. Doing so resulted in the models exhibiting a marked increase in ingroup solidarity and an even greater increase in outgroup hostility. Furthermore, removing either ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning data leads to a significant reduction in both ingroup solidarity and outgroup hostility, suggesting that biases can be reduced by removing biased training data. Our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data. Our results have practical implications for creating less biased large-language models and further underscore the need for more research into user interactions with LLMs to prevent potential bias reinforcement in humans.
</details>
<details>
<summary>摘要</summary>
现代大型语言模型的流行性带来了对这些模型可能学习到的偏见的担忧。在这项研究中，我们调查了51个大型语言模型是否具有社会偏见。我们发现，大多数基础语言模型和一些特定任务练习模型在完成句子时表现出了明显的内群团结和外群敌对偏见。与人类文本相比，这些模型的偏见水平可能相当或更高。为了探索这些偏见的来源，我们在模型微调过程中采用了不同的群体偏见训练数据，并观察到模型在美国民主党和共和党的分化下表现出了明显的内群团结和外群敌对偏见。此外，从微调数据中移除内群团结或外群敌对的句子后，模型中的偏见有显著减少的趋势，这表明可以通过修改训练数据来减少偏见。我们的发现表明现代大型语言模型具有基本的社会标识偏见，并且可以通过精心修改训练数据来减少这些偏见。这些结论有实际意义，可以帮助创建更少偏见的大型语言模型，并且更加重要的是，防止人类与LLM之间的偏见循环。
</details></li>
</ul>
<hr>
<h2 id="BLESS-Benchmarking-Large-Language-Models-on-Sentence-Simplification"><a href="#BLESS-Benchmarking-Large-Language-Models-on-Sentence-Simplification" class="headerlink" title="BLESS: Benchmarking Large Language Models on Sentence Simplification"></a>BLESS: Benchmarking Large Language Models on Sentence Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15773">http://arxiv.org/abs/2310.15773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zurichnlp/bless">https://github.com/zurichnlp/bless</a></li>
<li>paper_authors: Tannon Kew, Alison Chi, Laura Vásquez-Rodríguez, Sweta Agrawal, Dennis Aumiller, Fernando Alva-Manchego, Matthew Shardlow</li>
<li>for: 这个论文的目的是为了测试最新的大语言模型（LLMs）在文本简化（TS）任务上的性能，以及这些模型是否可以解决这个复杂的任务。</li>
<li>methods: 这个论文使用了44种不同的大语言模型，包括不同的大小、结构、预训练方法和可访问性。这些模型在三个不同的领域（Wikipedia、新闻和医学）上进行了三种不同的测试集。</li>
<li>results: 研究发现，最佳的LLMs，即使没有专门为TS进行训练，也能与当前TS基线一样好。此外，研究还发现了一些模型在执行编辑操作方面的多样性和创新性。这个性能评估将成为未来TS方法和评价度量的开发资源。<details>
<summary>Abstract</summary>
We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art large language models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS, perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics.
</details>
<details>
<summary>摘要</summary>
我们提出了BLESS，一个全面的性能评测标准，用于评测最新的大语言模型（LLMs）在文本简化（TS）任务上的表现。我们对44种不同的模型进行了评测，这些模型之间有不同的大小、结构、预训练方法和可访问性。我们使用三个来自不同领域（Wikipedia、新闻和医学）的测试集，在几个shot设定下进行了评测。我们的分析包括一系列自动度量器以及大规模的量化分析，以评估不同模型在TS任务上的表现。此外，我们还进行了一些手动质量分析，以更好地评估模型生成的简化结果质量。我们的评估结果表明，最佳的LLMs，即使没有直接针对TS进行训练，也能够与当前TS基准集成比肩。此外，我们发现某些LLMs在生成简化结果时拥有更广泛和多样化的编辑操作。我们的性能评测标准将作为未来TS方法和评价度量器的开发资源。
</details></li>
</ul>
<hr>
<h2 id="Learning-From-Free-Text-Human-Feedback-–-Collect-New-Datasets-Or-Extend-Existing-Ones"><a href="#Learning-From-Free-Text-Human-Feedback-–-Collect-New-Datasets-Or-Extend-Existing-Ones" class="headerlink" title="Learning From Free-Text Human Feedback – Collect New Datasets Or Extend Existing Ones?"></a>Learning From Free-Text Human Feedback – Collect New Datasets Or Extend Existing Ones?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15758">http://arxiv.org/abs/2310.15758</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ukplab/emnlp2023-learning-from-free-text-human-feedback">https://github.com/ukplab/emnlp2023-learning-from-free-text-human-feedback</a></li>
<li>paper_authors: Dominic Petrak, Nafise Sadat Moosavi, Ye Tian, Nikolai Rozanov, Iryna Gurevych</li>
<li>for: 这篇论文的目的是研究对话系统学习自由文本人类反馈的可能性，以及使用现成对话数据集进行增强。</li>
<li>methods: 该论文使用了现成对话数据集，包括MultiWoZ、SGD、BABI、PersonaChat、Wizards-of-Wikipedia以及Self-Feeding Chatbot的人机分割。然后，通过对这些数据集中的自由文本人类反馈进行分类，derive出新的对话数据集的分类法。最后，用三种现状前景语言生成模型进行response生成，以评估包括这些数据集的影响。</li>
<li>results: 该论文的结果显示，包括MultiWoZ、SGD、BABI、PersonaChat、Wizards-of-Wikipedia以及Self-Feeding Chatbot的现成对话数据集中，自由文本人类反馈的类型和频率各不相同。此外，通过使用新的分类法，可以对对话数据集进行更好的增强。此外，通过使用三种现状前景语言生成模型进行response生成，可以评估包括这些数据集的影响。<details>
<summary>Abstract</summary>
Learning from free-text human feedback is essential for dialog systems, but annotated data is scarce and usually covers only a small fraction of error types known in conversational AI. Instead of collecting and annotating new datasets from scratch, recent advances in synthetic dialog generation could be used to augment existing dialog datasets with the necessary annotations. However, to assess the feasibility of such an effort, it is important to know the types and frequency of free-text human feedback included in these datasets. In this work, we investigate this question for a variety of commonly used dialog datasets, including MultiWoZ, SGD, BABI, PersonaChat, Wizards-of-Wikipedia, and the human-bot split of the Self-Feeding Chatbot. Using our observations, we derive new taxonomies for the annotation of free-text human feedback in dialogs and investigate the impact of including such data in response generation for three SOTA language generation models, including GPT-2, LLAMA, and Flan-T5. Our findings provide new insights into the composition of the datasets examined, including error types, user response types, and the relations between them.
</details>
<details>
<summary>摘要</summary>
学习从自由文本人类反馈是对对话系统的关键，但已经标注的数据匮乏，通常只覆盖了对话AI中的一小部分错误类型。相反，利用现有对话数据的同时生成技术可以增强对话数据的标注。然而，以评估这种努力的可行性为目的，我们需要了解这些数据集中自由文本人类反馈的类型和频率。在这种工作中，我们对多个常用的对话数据集进行调查，包括MultiWoZ、SGD、BABI、PersonaChat、Wizards-of-Wikipedia以及自然语言生成模型的人类分裂。通过我们的观察，我们 derivates新的对话自由文本人类反馈的分类法，并 investigate如何在响应生成中包含这些数据的影响。我们的发现提供了对这些数据集的详细了解，包括错误类型、用户反馈类型以及它们之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Do-Differences-in-Values-Influence-Disagreements-in-Online-Discussions"><a href="#Do-Differences-in-Values-Influence-Disagreements-in-Online-Discussions" class="headerlink" title="Do Differences in Values Influence Disagreements in Online Discussions?"></a>Do Differences in Values Influence Disagreements in Online Discussions?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15757">http://arxiv.org/abs/2310.15757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m0re4u/value-disagreement">https://github.com/m0re4u/value-disagreement</a></li>
<li>paper_authors: Michiel van der Meer, Piek Vossen, Catholijn M. Jonker, Pradeep K. Murukannaiah</li>
<li>for: 本研究旨在 investigating 在 онлайн讨论中的不同意见是否与个人价值观有关，并探讨如何使用现有的模型来估算个人价值观。</li>
<li>methods: 本研究使用现有的语言模型来估算在线讨论中的个人价值观，然后将估算的价值观集成成价值 profil。 finally, 研究者使用人类标注的一致标签来评估价值 profil 的准确性。</li>
<li>results: 研究发现，在某些情况下，个人价值观的不同程度与不同意见有直接关系。此外，包含价值信息在一致预测中能够提高性能。<details>
<summary>Abstract</summary>
Disagreements are common in online discussions. Disagreement may foster collaboration and improve the quality of a discussion under some conditions. Although there exist methods for recognizing disagreement, a deeper understanding of factors that influence disagreement is lacking in the literature. We investigate a hypothesis that differences in personal values are indicative of disagreement in online discussions. We show how state-of-the-art models can be used for estimating values in online discussions and how the estimated values can be aggregated into value profiles. We evaluate the estimated value profiles based on human-annotated agreement labels. We find that the dissimilarity of value profiles correlates with disagreement in specific cases. We also find that including value information in agreement prediction improves performance.
</details>
<details>
<summary>摘要</summary>
互联网讨论中的不同意见是常见的。一些情况下，不同意见可能会促进协作并提高讨论的质量。然而，文献中对不同意见的影响因素还没有进行深入的研究。我们提出一种假设，即在线讨论中的个人价值差异是不同意见的指标。我们使用现有的模型来估算在线讨论中的价值，并将估算的价值聚合成价值profile。我们根据人类标注的一致标签来评估价值profile的评估结果。我们发现，价值profile的不同程度与specific cases中的不同意见相关。此外，包含价值信息在一致预测中可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="Failures-Pave-the-Way-Enhancing-Large-Language-Models-through-Tuning-free-Rule-Accumulation"><a href="#Failures-Pave-the-Way-Enhancing-Large-Language-Models-through-Tuning-free-Rule-Accumulation" class="headerlink" title="Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation"></a>Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15746">http://arxiv.org/abs/2310.15746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thunlp-mt/tran">https://github.com/thunlp-mt/tran</a></li>
<li>paper_authors: Zeyuan Yang, Peng Li, Yang Liu</li>
<li>for: 提高大型语言模型（LLM）的性能</li>
<li>methods: 使用自适应规则积累（Tuning-free Rule Accumulation，TRAN）框架，让 LLM 从错误案例中学习并改进性能</li>
<li>results: 实验表明，Compared with recent baselines, TRAN 可以提高 LLM 的性能的大幅度。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have showcased impressive performance. However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes. In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving their performance by learning from previous mistakes. Considering data arrives sequentially, LLMs gradually accumulate rules from incorrect cases, forming a rule collection. These rules are then utilized by the LLMs to avoid making similar mistakes when processing subsequent inputs. Moreover, the rules remain independent of the primary prompts, seamlessly complementing prompt design strategies. Experimentally, we show that TRAN improves over recent baselines by a large margin.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）有表现出色。然而，由于它们无法捕捉预测项目之间的关系，这些冻结的 LLM 总是重复相似的错误。在这个工作中，我们提出了我们的调整-自由规律积累（TRAN）框架，帮助 LLM 改善其表现。对于预测项目的给定，LLM 会逐渐累累集合错误的规律，形成一个规律集合。这些规律可以让 LLM 在处理后续的输入时避免重复错误。此外，这些规律与主要提示无关，可以与提示设计策略相融合。实验结果显示，TRAN 比最近的基eline高得多。
</details></li>
</ul>
<hr>
<h2 id="RAPL-A-Relation-Aware-Prototype-Learning-Approach-for-Few-Shot-Document-Level-Relation-Extraction"><a href="#RAPL-A-Relation-Aware-Prototype-Learning-Approach-for-Few-Shot-Document-Level-Relation-Extraction" class="headerlink" title="RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction"></a>RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15743">http://arxiv.org/abs/2310.15743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiao Meng, Xuming Hu, Aiwei Liu, Shu’ang Li, Fukun Ma, Yawen Yang, Lijie Wen</li>
<li>for: 这篇论文主要用于提高几shot文档关系提取中的Semantic关系识别精度。</li>
<li>methods: 该方法使用度量基于的元学习框架，通过建立类prototype来进行分类。</li>
<li>results: 对于两个FSDLRE benchmark中的多种设置，该方法比前state-of-the-art方法提高了2.61%的$F_1$值。<details>
<summary>Abstract</summary>
How to identify semantic relations among entities in a document when only a few labeled documents are available? Few-shot document-level relation extraction (FSDLRE) is crucial for addressing the pervasive data scarcity problem in real-world scenarios. Metric-based meta-learning is an effective framework widely adopted for FSDLRE, which constructs class prototypes for classification. However, existing works often struggle to obtain class prototypes with accurate relational semantics: 1) To build prototype for a target relation type, they aggregate the representations of all entity pairs holding that relation, while these entity pairs may also hold other relations, thus disturbing the prototype. 2) They use a set of generic NOTA (none-of-the-above) prototypes across all tasks, neglecting that the NOTA semantics differs in tasks with different target relation types. In this paper, we propose a relation-aware prototype learning method for FSDLRE to strengthen the relational semantics of prototype representations. By judiciously leveraging the relation descriptions and realistic NOTA instances as guidance, our method effectively refines the relation prototypes and generates task-specific NOTA prototypes. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches by average 2.61% $F_1$ across various settings of two FSDLRE benchmarks.
</details>
<details>
<summary>摘要</summary>
如何在文档中IdentifyEntities的 semantic relations When only a few labeled documents are available? 几shot文档关系抽取 (FSDLRE) 是解决实际场景中数据缺乏的问题的关键。 度量基于的meta学是一种广泛采用的效果的框架，它在文档级别上构建分类的类prototype。然而，现有的工作frequently struggle to obtain class prototype with accurate relational semantics：1. 为目标关系类型构建类 prototype，它们将所有包含该关系的实体对的表示拟合在一起，但这些实体对可能也包含其他关系，从而干扰 prototype。2. 使用所有任务中的通用 NOTA (None-of-the-above) 类型的代表，忽略了不同目标关系类型下的 NOTA semantics 的差异。在这篇论文中，我们提出了一种关系意识 prototype learning 方法，用于加强文档中的关系semantics。我们judiciously leveraging relation descriptions和realistic NOTA instances as guidance，our method effectively refines the relation prototypes and generates task-specific NOTA prototypes。经过extensive experiments demonstrate that our method outperforms state-of-the-art approaches by average 2.61% $F_1$ across various settings of two FSDLRE benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Variator-Accelerating-Pre-trained-Models-with-Plug-and-Play-Compression-Modules"><a href="#Variator-Accelerating-Pre-trained-Models-with-Plug-and-Play-Compression-Modules" class="headerlink" title="Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules"></a>Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15724">http://arxiv.org/abs/2310.15724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thunlp/compression-plugin">https://github.com/thunlp/compression-plugin</a></li>
<li>paper_authors: Chaojun Xiao, Yuqi Luo, Wenbin Zhang, Pengle Zhang, Xu Han, Yankai Lin, Zhengyan Zhang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou</li>
<li>for: 提高 NLP 任务的计算效率，减少 Parameters 的大小和计算成本。</li>
<li>methods: 使用可插入的压缩插件，通过压缩多个隐藏向量为一个压缩多个隐藏向量为一个，并在原始 PLM 中固定训练。</li>
<li>results: 在七个数据集上 validate 了 Variator 的有效性，可以Save 53% 的计算成本，仅占 Parameters 的 0.9%，性能下降低于 2%。<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original PLMs frozen. Different from traditional model acceleration methods, which compress PLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a growing number of tasks. We validate the effectiveness of Variator on seven datasets. Experimental results show that Variator can save 53% computational costs using only 0.9% additional parameters with a performance drop of less than 2%. Moreover, when the model scales to billions of parameters, Variator matches the strong performance of uncompressed PLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Re-Temp-Relation-Aware-Temporal-Representation-Learning-for-Temporal-Knowledge-Graph-Completion"><a href="#Re-Temp-Relation-Aware-Temporal-Representation-Learning-for-Temporal-Knowledge-Graph-Completion" class="headerlink" title="Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion"></a>Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15722">http://arxiv.org/abs/2310.15722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunze Wang, Soyeon Caren Han, Josiah Poon</li>
<li>for: 预测未来事物中缺失的实体（Temporal Knowledge Graph Completion under extrapolation setting）</li>
<li>methods: 利用显式时间嵌入和跳过信息流来避免无关信息的泄露，并 introduce two-phase forward propagation method to prevent information leakage</li>
<li>results: 在六个TKGC（extrapolation）数据集上比其他八个最新的状态艺术模型表现出色，具体 result 见 paper 中的表格<details>
<summary>Abstract</summary>
Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting aims to predict the missing entity from a fact in the future, posing a challenge that aligns more closely with real-world prediction problems. Existing research mostly encodes entities and relations using sequential graph neural networks applied to recent snapshots. However, these approaches tend to overlook the ability to skip irrelevant snapshots according to entity-related relations in the query and disregard the importance of explicit temporal information. To address this, we propose our model, Re-Temp (Relation-Aware Temporal Representation Learning), which leverages explicit temporal embedding as input and incorporates skip information flow after each timestamp to skip unnecessary information for prediction. Additionally, we introduce a two-phase forward propagation method to prevent information leakage. Through the evaluation on six TKGC (extrapolation) datasets, we demonstrate that our model outperforms all eight recent state-of-the-art models by a significant margin.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过提供答案，我将文本翻译成简化中文。</SYS>>temporal knowledge graph completion（TKGC）下的推断任务是预测未来中的缺失实体，这种任务更加接近实际预测问题。现有研究大都使用逻辑图神经网络对最近的快照进行编码，但这些方法通常忽略了查询中实体相关的关系，以及时间信息的直接表达。为了解决这个问题，我们提出了我们的模型Re-Temp（关系意识时间表示学习），它利用显式的时间嵌入作为输入，并在每个时间戳点后进行跳过不必要信息的流动。此外，我们还提出了一种两阶段前进协议，以避免信息泄露。经过六个TKGC（推断）数据集的评估，我们示出了我们的模型在八个最新的状态艺术模型中具有显著的超越。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-of-Task-Specific-Language-Models-for-Brain-Encoding"><a href="#Ensemble-of-Task-Specific-Language-Models-for-Brain-Encoding" class="headerlink" title="Ensemble of Task-Specific Language Models for Brain Encoding"></a>Ensemble of Task-Specific Language Models for Brain Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15720">http://arxiv.org/abs/2310.15720</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jr-john/ensemble_brain_encoders">https://github.com/jr-john/ensemble_brain_encoders</a></li>
<li>paper_authors: Sanjai Kumaran, Arvindh Arun, Jerrin John</li>
<li>for: 用于提高语言模型对大脑响应的预测性能</li>
<li>methods: 使用10种流行的自然语言处理任务的表示学习来转移学习，并创建了一个ensemble模型</li>
<li>results: 比基eline平均提高10%的性能 across all ROIs<details>
<summary>Abstract</summary>
Language models have been shown to be rich enough to encode fMRI activations of certain Regions of Interest in our Brains. Previous works have explored transfer learning from representations learned for popular natural language processing tasks for predicting brain responses. In our work, we improve the performance of such encoders by creating an ensemble model out of 10 popular Language Models (2 syntactic and 8 semantic). We beat the current baselines by 10% on average across all ROIs through our ensembling methods.
</details>
<details>
<summary>摘要</summary>
Language models有可能编码certain Regions of Interest（ROI）的fMRI活动。 previous works曾经explored transfer learning from representations学习的 popular natural language processing tasks来预测brain responses。在我们的工作中，我们提高了such encoders的性能by creating an ensemble model out of 10 popular Language Models（2 syntactic和8 semantic）。我们在所有ROIs上 average beat the current baselines by 10%。Note: "Regions of Interest" (ROIs) are specific areas of the brain that are being studied. In this text, the author is referring to the ability of language models to encode the activity of these areas based on fMRI scans.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Biomedical-Lay-Summarisation-with-External-Knowledge-Graphs"><a href="#Enhancing-Biomedical-Lay-Summarisation-with-External-Knowledge-Graphs" class="headerlink" title="Enhancing Biomedical Lay Summarisation with External Knowledge Graphs"></a>Enhancing Biomedical Lay Summarisation with External Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15702">http://arxiv.org/abs/2310.15702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tgoldsack1/enhancing_biomedical_lay_summarisation_with_external_knowledge_graphs">https://github.com/tgoldsack1/enhancing_biomedical_lay_summarisation_with_external_knowledge_graphs</a></li>
<li>paper_authors: Tomas Goldsack, Zhihao Zhang, Chen Tang, Carolina Scarton, Chenghua Lin</li>
<li>for: 这篇论文主要是为了提供一种自动生成简要摘要的方法，以便将专业技术文章简化成普通读者可以理解的语言。</li>
<li>methods: 这篇论文使用了知识图来提高自动生成简要摘要的效果，并系统地研究了三种不同的方法，每种方法targeting一个不同的encoder-decoder模型结构。</li>
<li>results: 结果表明，通过 интеGRATING graph-based domain knowledge，可以很大地提高自动生成简要摘要的可读性和技术概念的解释。<details>
<summary>Abstract</summary>
Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay audience. We address this issue by augmenting eLife, an existing biomedical lay summarisation dataset, with article-specific knowledge graphs, each containing detailed information on relevant biomedical concepts. Using both automatic and human evaluations, we systematically investigate the effectiveness of three different approaches for incorporating knowledge graphs within lay summarisation models, with each method targeting a distinct area of the encoder-decoder model architecture. Our results confirm that integrating graph-based domain knowledge can significantly benefit lay summarisation by substantially increasing the readability of generated text and improving the explanation of technical concepts.
</details>
<details>
<summary>摘要</summary>
précédentes approches pour la résumé automatique sont exclusivement dépendantes de l'article de source qui, étant écrit pour un public technique (par exemple, des chercheurs), est peu probable de définir explicitement tous les concepts techniques ou d'indiquer toutes les informations de fond pertinentes pour un public lay. Nous résolvons ce problème en augmentant eLife, un dataset existant de résumé lay biomédical, avec des graphiques de connaissances spécifiques à l'article, chacune contenant des informations détaillées sur les concepts biomédicaux pertinents. En utilisant des évaluations automatiques et humaines, nous étudions de manière systématique l'efficacité de trois approches différentes pour intégrer des connaissances graphiques dans les modèles de résumé lay, chacune ciblant une région distincte de l'architecture encoder-decoder. Nos résultats confirment que l'intégration de la connaissance domainale basée sur les graphes peut considérablement améliorer la lisibilité du texte généré et l'explication des concepts techniques.
</details></li>
</ul>
<hr>
<h2 id="COPF-Continual-Learning-Human-Preference-through-Optimal-Policy-Fitting"><a href="#COPF-Continual-Learning-Human-Preference-through-Optimal-Policy-Fitting" class="headerlink" title="COPF: Continual Learning Human Preference through Optimal Policy Fitting"></a>COPF: Continual Learning Human Preference through Optimal Policy Fitting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15694">http://arxiv.org/abs/2310.15694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu</li>
<li>for: 提高预训练语言模型（LM） conform to human preferences</li>
<li>methods: 使用Reinforcement Learning from Human Feedback（RLHF）方法，但不需要每次新的查询或反馈都进行全量重新训练</li>
<li>results: 实验结果表明，COPF方法在不同任务和领域中一直与人类偏好保持一致，并且超越了强大的Continuous learning（CL）基elines<details>
<summary>Abstract</summary>
The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability with RLHF to learn from unlabeled data, making it flexible for continual preference learning. Our experimental results show that COPF outperforms strong Continuous learning (CL) baselines when it comes to consistently aligning with human preferences on different tasks and domains.
</details>
<details>
<summary>摘要</summary>
RLHF（人类反馈学习强化）技术是通常用于改进预训练语言模型（LM）的方法，以提高其遵循人类偏好的能力。然而，现有RLHF基于LM的模型每次新增查询或反馈都需要全面重新训练，这会成为一项具有挑战性的任务，因为人类偏好可能在不同的领域或任务中发生变化。重新训练LM对于许多实际应用场景来说具有困难和费时的问题，同时也存在数据隐私问题。为解决这个限制，我们提出了一种新方法 called Continual Optimal Policy Fitting（COPF），其中我们使用Monte Carlo方法来估算一系列的优化政策，然后不断地使用功能规则来适应政策序列。COPF只需一次学习阶段，不需要复杂的强化学习。这种方法与RLHF一样可以学习从无标签数据中，这使其具有适应不断改变的偏好的灵活性。我们的实验结果表明，COPF在不同任务和领域中一直适应人类偏好的表现比强大的连续学习（CL）基eline更好。
</details></li>
</ul>
<hr>
<h2 id="Creating-a-silver-standard-for-patent-simplification"><a href="#Creating-a-silver-standard-for-patent-simplification" class="headerlink" title="Creating a silver standard for patent simplification"></a>Creating a silver standard for patent simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15689">http://arxiv.org/abs/2310.15689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slvcsl/patentsilverstandard">https://github.com/slvcsl/patentsilverstandard</a></li>
<li>paper_authors: Silvia Casola, Alberto Lavelli, Horacio Saggion</li>
<li>for: 本文提出了一种自动简化专利文本的方法，以便提高专利文本的访问性和机器可读性。</li>
<li>methods: 本文使用了一种基于人工智能的自动生成简化字典，并使用了特定的筛选器来生成 cleaner 的译文集。</li>
<li>results: 人工评估表明，生成的简化译文集具有 grammaticality、准确性和简洁性。<details>
<summary>Abstract</summary>
Patents are legal documents that aim at protecting inventions on the one hand and at making technical knowledge circulate on the other. Their complex style -- a mix of legal, technical, and extremely vague language -- makes their content hard to access for humans and machines and poses substantial challenges to the information retrieval community. This paper proposes an approach to automatically simplify patent text through rephrasing. Since no in-domain parallel simplification data exist, we propose a method to automatically generate a large-scale silver standard for patent sentences. To obtain candidates, we use a general-domain paraphrasing system; however, the process is error-prone and difficult to control. Thus, we pair it with proper filters and construct a cleaner corpus that can successfully be used to train a simplification system. Human evaluation of the synthetic silver corpus shows that it is considered grammatical, adequate, and contains simple sentences.
</details>
<details>
<summary>摘要</summary>
专利文档是法律文档，旨在一方面保护发明，另一方面让技术知识流通。它们的复杂风格——包括法律、技术和极其抽象语言——使得它们的内容困难 для人类和机器访问，对信息检索社区提出了重大挑战。这篇论文提议自动简化专利文本的方法，由于没有相关领域的平行简化数据，我们提议自动生成大规模的银色标准集。为获得候选者，我们使用通用领域重叠系统，但这个过程存在误差和难以控制的问题。因此，我们对其进行过滤，并构建了一个更加清晰的集合，可以成功地用于培训简化系统。人工评估该银色集表示，它具有正确的格式、充分的表达和简单的句子。
</details></li>
</ul>
<hr>
<h2 id="Prevalence-and-prevention-of-large-language-model-use-in-crowd-work"><a href="#Prevalence-and-prevention-of-large-language-model-use-in-crowd-work" class="headerlink" title="Prevalence and prevention of large language model use in crowd work"></a>Prevalence and prevention of large language model use in crowd work</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15683">http://arxiv.org/abs/2310.15683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Veniamin Veselovsky, Manoel Horta Ribeiro, Philip Cozzolino, Andrew Gordon, David Rothschild, Robert West</li>
<li>for: The paper is written to investigate the use of large language models (LLMs) among crowd workers and to develop targeted mitigation strategies to reduce LLM use.</li>
<li>methods: The paper uses a text summarization task where workers were not directed in any way regarding their LLM use, and compares the estimated prevalence of LLM use with and without targeted mitigation strategies. The paper also conducts secondary analyses to explore the impact of LLM use on the quality and homogeneity of responses.</li>
<li>results: The paper finds that targeted mitigation strategies can significantly reduce, but not eliminate, LLM use among crowd workers. The paper also finds that LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data. Additionally, the paper finds that preventing LLM use may be at odds with obtaining high-quality responses.<details>
<summary>Abstract</summary>
We show that the use of large language models (LLMs) is prevalent among crowd workers, and that targeted mitigation strategies can significantly reduce, but not eliminate, LLM use. On a text summarization task where workers were not directed in any way regarding their LLM use, the estimated prevalence of LLM use was around 30%, but was reduced by about half by asking workers to not use LLMs and by raising the cost of using them, e.g., by disabling copy-pasting. Secondary analyses give further insight into LLM use and its prevention: LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data. At the same time, preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use LLMs, summaries contained fewer keywords carrying essential information. Our estimates will likely change as LLMs increase in popularity or capabilities, and as norms around their usage change. Yet, understanding the co-evolution of LLM-based tools and users is key to maintaining the validity of research done using crowdsourcing, and we provide a critical baseline before widespread adoption ensues.
</details>
<details>
<summary>摘要</summary>
我们表明大语言模型（LLM）在观众工作者中的使用是普遍的，并且目标的 Mitigation Strategies 可以大幅降低，但不能完全消除 LLM 的使用。在一个文本摘要任务中，工作者没有任何指导，LLM 的使用率约为 30%，但通过请求工作者不使用 LLM 和提高使用它们的成本，例如禁用复制键，可以大幅降低 LLM 的使用率，约从 30% 降至 15%。次要分析显示 LLM 使用对人类（而不是模型）的行为有高质量但同质的回应，这可能对研究造成伤害，并且对未来由观众集成的数据训练的模型造成负面影响。同时，防止 LLM 使用可能与获得高质量回应相抵触，例如当请求工作者不使用 LLM 时，摘要中的关键词数量减少。我们的估计将在 LLM 的普及度和能力增加，以及使用 norms 的改变时改变。但是，理解 LLM 基本的工具和用户之间的共演是维护透过观众集成所进行的研究的有效性的关键。我们提供了一个基本的估计，以便在大规模的采用前，我们可以更好地理解 LLM 的影响。
</details></li>
</ul>
<hr>
<h2 id="How-Much-Context-Does-My-Attention-Based-ASR-System-Need"><a href="#How-Much-Context-Does-My-Attention-Based-ASR-System-Need" class="headerlink" title="How Much Context Does My Attention-Based ASR System Need?"></a>How Much Context Does My Attention-Based ASR System Need?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15672">http://arxiv.org/abs/2310.15672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robflynnyh/long-context-asr">https://github.com/robflynnyh/long-context-asr</a></li>
<li>paper_authors: Robert Flynn, Anton Ragni</li>
<li>for: 这个研究旨在检验在语音识别任务中使用更长的听录上下文时的效果。</li>
<li>methods: 这些实验使用了 dense-attention 基于的语音和语言模型，并在不同的上下文长度（5秒至1小时）下进行训练和评估。</li>
<li>results: 研究发现，使用约80秒的听录上下文可以获得14.9%的相对提升，并通过批处理搜索与长上下文转换器语言模型组合来实现长上下文语音识别系统，与当前状态前进竞争。<details>
<summary>Abstract</summary>
For the task of speech recognition, the use of more than 30 seconds of acoustic context during training is uncommon, and under-investigated in literature. In this work, we examine the effect of scaling the sequence length used to train/evaluate (dense-attention based) acoustic and language models on speech recognition performance. For these experiments a dataset of roughly 100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets Earnings-22 and Tedlium demonstrate a benefit from training with around 80 seconds of acoustic context, showing up to a 14.9% relative improvement from a limited context baseline. Furthermore, we perform a system combination with long-context transformer language models via beam search for a fully long-context ASR system, with results that are competitive with the current state-of-the-art.
</details>
<details>
<summary>摘要</summary>
For the task of speech recognition, using more than 30 seconds of acoustic context during training is rare and under-investigated in literature. In this work, we study the effect of scaling the sequence length used to train/evaluate (dense-attention based) acoustic and language models on speech recognition performance. For these experiments, we use a dataset of approximately 100,000 pseudo-labeled Spotify podcasts, with context lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets Earnings-22 and Tedlium show a benefit from training with around 80 seconds of acoustic context, with up to a 14.9% relative improvement from a limited context baseline. Furthermore, we perform a system combination with long-context transformer language models via beam search for a fully long-context ASR system, with results that are competitive with the current state-of-the-art.
</details></li>
</ul>
<hr>
<h2 id="Expression-Syntax-Information-Bottleneck-for-Math-Word-Problems"><a href="#Expression-Syntax-Information-Bottleneck-for-Math-Word-Problems" class="headerlink" title="Expression Syntax Information Bottleneck for Math Word Problems"></a>Expression Syntax Information Bottleneck for Math Word Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15664">http://arxiv.org/abs/2310.15664</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/menik1126/math_esib">https://github.com/menik1126/math_esib</a></li>
<li>paper_authors: Jing Xiong, Chengming Li, Min Yang, Xiping Hu, Bin Hu</li>
<li>for:  automatic solving of mathematical questions in texts</li>
<li>methods:  Expression Syntax Information Bottleneck (ESIB) method based on variational information bottleneck, with self-distillation loss to improve generalization and generate more diverse expressions</li>
<li>results:  state-of-the-art results and more diverse solutions on two large-scale benchmarks<details>
<summary>Abstract</summary>
Math Word Problems (MWP) aims to automatically solve mathematical questions given in texts. Previous studies tend to design complex models to capture additional information in the original text so as to enable the model to gain more comprehensive features. In this paper, we turn our attention in the opposite direction, and work on how to discard redundant features containing spurious correlations for MWP. To this end, we design an Expression Syntax Information Bottleneck method for MWP (called ESIB) based on variational information bottleneck, which extracts essential features of expression syntax tree while filtering latent-specific redundancy containing syntax-irrelevant features. The key idea of ESIB is to encourage multiple models to predict the same expression syntax tree for different problem representations of the same problem by mutual learning so as to capture consistent information of expression syntax tree and discard latent-specific redundancy. To improve the generalization ability of the model and generate more diverse expressions, we design a self-distillation loss to encourage the model to rely more on the expression syntax information in the latent space. Experimental results on two large-scale benchmarks show that our model not only achieves state-of-the-art results but also generates more diverse solutions. The code is available.
</details>
<details>
<summary>摘要</summary>
mathematical word problems (MWP) 目的是自动解决在文本中提供的数学问题。在前一些研究中，旨在设计复杂的模型，以便 capture额外信息在原始文本中，以便模型可以获得更加全面的特征。在这篇论文中，我们弯我们的注意力向另一个方向，并努力如何抛弃 redundancy 中的偶极特征，以便为 MWP 提供更好的解决方案。为此，我们提出了一种基于变量信息瓶颈的表达 syntax 信息瓶颈方法（ESIB），该方法可以提取表达 syntax 树中的重要特征，同时过滤 latent-specific 的偶极特征。ESIB 的关键思想是通过互学习来鼓励多个模型对不同的问题表示进行同一个表达 syntax 树的预测，以便捕捉表达 syntax 树中的一致信息，并抛弃 latent-specific 的偶极特征。为了提高模型的通用能力和生成更多的表达，我们还设计了一种自我混合损失，以便让模型更加依赖于表达 syntax 信息在潜在空间中。实验结果表明，我们的模型不仅达到了当前最佳 результаados，还能够生成更多的解决方案。代码可用。
</details></li>
</ul>
<hr>
<h2 id="CoAnnotating-Uncertainty-Guided-Work-Allocation-between-Human-and-Large-Language-Models-for-Data-Annotation"><a href="#CoAnnotating-Uncertainty-Guided-Work-Allocation-between-Human-and-Large-Language-Models-for-Data-Annotation" class="headerlink" title="CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation"></a>CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15638">http://arxiv.org/abs/2310.15638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salt-nlp/coannotating">https://github.com/salt-nlp/coannotating</a></li>
<li>paper_authors: Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy F. Chen, Zhengyuan Liu, Diyi Yang</li>
<li>for: 本研究旨在提出一种新的人机共约注解模型（CoAnnotating），用于大规模的不结构化文本注解。</li>
<li>methods: 该模型利用了机器学习模型的不确定性来估算机器模型的注解能力。</li>
<li>results: 实验结果表明，CoAnnotating可以有效地划分工作，并且在不同的数据集上达到21%的性能提升。<details>
<summary>Abstract</summary>
Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>自然语言处理（NLP）中，注释数据扮演了关键角色，在训练模型和评估其性能方面都是非常重要的。在最近的大语言模型（LLMs）中，models如ChatGPT显示了零开始能力，在许多文本注释任务上表现相当于或 même exceeding human annotators。这些LLMs可以作为手动注释的替代方案，因为其成本较低，可扩展性较高。然而，有限的工作没有利用LLMs作为补充注释者，也没有探索如何将注释工作分配给人类和LLMs以实现质量和成本目标。我们提出了CoAnnotating，一种新的人类-LLM共同注释文本的框架。在这个框架下，我们利用uncertainty来估计LLMs的注释能力。我们的实验表明，CoAnnotating是一种有效的方法，可以在不同的数据集上分配工作，并且比Random baseline提高性能达21%。 For code implementation, see <https://github.com/SALT-NLP/CoAnnotating>.
</details></li>
</ul>
<hr>
<h2 id="Tips-for-making-the-most-of-64-bit-architectures-in-langage-design-libraries-or-garbage-collection"><a href="#Tips-for-making-the-most-of-64-bit-architectures-in-langage-design-libraries-or-garbage-collection" class="headerlink" title="Tips for making the most of 64-bit architectures in langage design, libraries or garbage collection"></a>Tips for making the most of 64-bit architectures in langage design, libraries or garbage collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15632">http://arxiv.org/abs/2310.15632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benoît Sonntag, Dominique Colnet</li>
<li>for: 该论文探讨了如何利用64位处理器的低级编程可能性，以提高计算速度和内存利用率。</li>
<li>methods: 论文提出了三个具体的例子，包括实现多精度整数库、使用UTF-8字符串索引和优化垃圾回收器。</li>
<li>results: 论文的例子显示了在计算速度和内存利用率方面的性能提升。<details>
<summary>Abstract</summary>
The 64-bit architectures that have become standard today offer unprecedented low-level programming possibilities. For the first time in the history of computing, the size of address registers far exceeded the physical capacity of their bus.After a brief reminder of the possibilities offered by the small size of addresses compared to the available 64 bits,we develop three concrete examples of how the vacant bits of these registers can be used.Among these examples, two of them concern the implementation of a library for a new statically typed programming language.Firstly, the implementation of multi-precision integers, with the aim of improving performance in terms of both calculation speed and RAM savings.The second example focuses on the library's handling of UTF-8 character strings.Here, the idea is to make indexing easier by ignoring the physical size of each UTF-8 characters.Finally, the third example is a possible enhancement of garbage collectors, in particular the mark \& sweep for the object marking phase.
</details>
<details>
<summary>摘要</summary>
现代64位架构已成为标准，提供了前所未有的低级编程可能性。这是计算史上首次，地址 Register 的大小超过了总线的物理容量。我们从小地址比例64位中的可用 bits的角度出发，提出三个具体的例子，其中两个是实现一种新的静态类型编程语言的库。首先，我们实现了多精度整数，以提高计算速度和内存占用量。第二个例子是库处理 UTF-8 字符串，以便更容易地索引。在这个例子中，我们忽略了每个 UTF-8 字符的物理大小，使索引更加简单。最后，第三个例子是对垃圾收集器的优化，特别是对对象标识阶段的 mark & sweep 垃圾收集器。
</details></li>
</ul>
<hr>
<h2 id="Machine-Translation-for-Nko-Tools-Corpora-and-Baseline-Results"><a href="#Machine-Translation-for-Nko-Tools-Corpora-and-Baseline-Results" class="headerlink" title="Machine Translation for Nko: Tools, Corpora and Baseline Results"></a>Machine Translation for Nko: Tools, Corpora and Baseline Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15612">http://arxiv.org/abs/2310.15612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moussa Koulako Bala Doumbouya, Baba Mamadi Diané, Solo Farabado Cissé, Djibrila Diané, Abdoulaye Sow, Séré Moussa Doumbouya, Daouda Bangoura, Fodé Moriba Bayo, Ibrahima Sory 2. Condé, Kalo Mory Diané, Chris Piech, Christopher Manning</li>
<li>for:  Addressing the lack of usable machine translation systems for Nko, a language spoken by tens of millions of people across multiple West African countries.</li>
<li>methods:  Developed a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available.</li>
<li>results:  Presented a novel collaborative parallel text curation software (Friallel), expanded the FLoRes-200 and NLLB-Seed corpora with high-quality Nko translations, and developed a collection of trilingual and bilingual corpora (nicolingua-0005) with over 3 million Nko words. The best model scored 30.83 English-Nko chrF++ on FLoRes-devtest.<details>
<summary>Abstract</summary>
Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the best model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.
</details>
<details>
<summary>摘要</summary>
Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available.(1) Friallel: 一款新的合作平行文本纠正软件，包含质量控制通过复制编辑工作流程。(2) FLoRes-200和NLLB-Seed corpora的扩展，包含2,009和6,193个高质量的Nko翻译和204种其他语言的平行翻译。(3) nicolingua-0005：一个包含130,850个平行段和更多than 3 million Nko单词的多语言和双语 corpora集。(4) 使用最佳模型，得到了30.83的英文-Nko chrF++ 成绩在FLoRes-devtest上。
</details></li>
</ul>
<hr>
<h2 id="MUSER-A-Multi-View-Similar-Case-Retrieval-Dataset"><a href="#MUSER-A-Multi-View-Similar-Case-Retrieval-Dataset" class="headerlink" title="MUSER: A Multi-View Similar Case Retrieval Dataset"></a>MUSER: A Multi-View Similar Case Retrieval Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15602">http://arxiv.org/abs/2310.15602</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thulawtech/muser">https://github.com/thulawtech/muser</a></li>
<li>paper_authors: Qingquan Li, Yiran Hu, Feng Yao, Chaojun Xiao, Zhiyuan Liu, Maosong Sun, Weixing Shen</li>
<li>for: 提高司法公平性，开发智能法律应用程序</li>
<li>methods: 多视角相似度测量、全面法律元素标注</li>
<li>results:  incorporating 法律元素可以提高相似案件模型的表现，但还需要继续解决MUSER中的挑战。Here’s a brief explanation of each point:</li>
<li>for: The paper is written to improve the fairness of the judicial system by developing a smart legal application.</li>
<li>methods: The paper uses a multi-view similarity measurement and comprehensive legal element annotation to evaluate the similarity between cases.</li>
<li>results: Incorporating legal elements can improve the performance of similar case retrieval models, but there are still challenges to be addressed in the MUSER dataset.<details>
<summary>Abstract</summary>
Similar case retrieval (SCR) is a representative legal AI application that plays a pivotal role in promoting judicial fairness. However, existing SCR datasets only focus on the fact description section when judging the similarity between cases, ignoring other valuable sections (e.g., the court's opinion) that can provide insightful reasoning process behind. Furthermore, the case similarities are typically measured solely by the textual semantics of the fact descriptions, which may fail to capture the full complexity of legal cases from the perspective of legal knowledge. In this work, we present MUSER, a similar case retrieval dataset based on multi-view similarity measurement and comprehensive legal element with sentence-level legal element annotations. Specifically, we select three perspectives (legal fact, dispute focus, and law statutory) and build a comprehensive and structured label schema of legal elements for each of them, to enable accurate and knowledgeable evaluation of case similarities. The constructed dataset originates from Chinese civil cases and contains 100 query cases and 4,024 candidate cases. We implement several text classification algorithms for legal element prediction and various retrieval methods for retrieving similar cases on MUSER. The experimental results indicate that incorporating legal elements can benefit the performance of SCR models, but further efforts are still required to address the remaining challenges posed by MUSER. The source code and dataset are released at https://github.com/THUlawtech/MUSER.
</details>
<details>
<summary>摘要</summary>
相似案件检索（SCR）是法律人工智能应用的代表之一，对法院公正发挥重要作用。然而，现有的 SCR 数据集仅基于事实描述部分进行相似性评估，忽略其他有价值的部分（例如法院意见），这可能导致失去有价值的推理过程。另外，案例相似性通常基于事实描述部分的文本 semantics 进行评估，这可能不能捕捉法律案例的全面性。在这种情况下，我们提出了 MUSER，一个基于多视角相似度测量和全面的法律元素的similar case retrieval数据集。具体来说，我们选择了三个视角（法律事实、争议重点和法律法规），并建立了每个视角的结构化标签 schema 以便准确和知识化评估案例的相似性。构建的数据集来自中国民事案例，包含100个查询案例和4,024个候选案例。我们实现了多种文本分类算法以便法律元素预测，以及多种检索方法以便检索相似案例。实验结果表明，包含法律元素可以提高 SCR 模型的性能，但还需要进一步努力以 Address  MUSER 中的剩下挑战。数据集和代码可以在 <https://github.com/THUlawtech/MUSER> 获取。
</details></li>
</ul>
<hr>
<h2 id="ScanDL-A-Diffusion-Model-for-Generating-Synthetic-Scanpaths-on-Texts"><a href="#ScanDL-A-Diffusion-Model-for-Generating-Synthetic-Scanpaths-on-Texts" class="headerlink" title="ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts"></a>ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15587">http://arxiv.org/abs/2310.15587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dili-lab/scandl">https://github.com/dili-lab/scandl</a></li>
<li>paper_authors: Lena S. Bolliger, David R. Reich, Patrick Haller, Deborah N. Jakobi, Paul Prasse, Lena A. Jäger</li>
<li>for: 这个论文的目的是为了研究人类语言处理的认知机制，以及利用眼动数据进行语言相关的机器学习任务。</li>
<li>methods: 这篇论文使用了一种基于扩散过程的数据驱动的眼动数据生成模型，称为ScanDL，以生成人类化的扫描路径。模型利用预训练的单词表示和共同嵌入句子和固定序列，以捕捉多模态的句子和眼动之间的互动。</li>
<li>results: 作者在几个数据集上进行了内部和跨数据集的评估，并证明了ScanDL在生成眼动数据方面表现出色，大大超过了当前的状态艺术。此外，作者还进行了广泛的心理语言分析，证明ScanDL能够展现出人类化的读取行为。<details>
<summary>Abstract</summary>
Eye movements in reading play a crucial role in psycholinguistic research studying the cognitive mechanisms underlying human language processing. More recently, the tight coupling between eye movements and cognition has also been leveraged for language-related machine learning tasks such as the interpretability, enhancement, and pre-training of language models, as well as the inference of reader- and text-specific properties. However, scarcity of eye movement data and its unavailability at application time poses a major challenge for this line of research. Initially, this problem was tackled by resorting to cognitive models for synthesizing eye movement data. However, for the sole purpose of generating human-like scanpaths, purely data-driven machine-learning-based methods have proven to be more suitable. Following recent advances in adapting diffusion processes to discrete data, we propose ScanDL, a novel discrete sequence-to-sequence diffusion model that generates synthetic scanpaths on texts. By leveraging pre-trained word representations and jointly embedding both the stimulus text and the fixation sequence, our model captures multi-modal interactions between the two inputs. We evaluate ScanDL within- and across-dataset and demonstrate that it significantly outperforms state-of-the-art scanpath generation methods. Finally, we provide an extensive psycholinguistic analysis that underlines the model's ability to exhibit human-like reading behavior. Our implementation is made available at https://github.com/DiLi-Lab/ScanDL.
</details>
<details>
<summary>摘要</summary>
阅读时的眼动对心理语言研究具有关键性。更 reciently, 眼动和认知之间的紧密关系也被用于语言相关的机器学习任务，如语言模型的可读性、加强和预训练，以及文本和读者特有的属性的推断。然而，眼动数据的罕见和应用时无法获取的问题成为了这一研究领域的主要挑战。初始地，这个问题被解决了通过启用认知模型来生成眼动数据。然而，为了生成人类化的扫描路径，数据驱动的机器学习基本方法证明更加适用。基于最近的扩散过程的适应，我们提出了ScanDL，一种新的离散序列到序列扩散模型，可以生成人工智能化的扫描路径。通过将预训练word表示和扫描序列共同嵌入，我们的模型捕捉了文本和扫描序列之间的多模态交互。我们在不同的数据集上进行了 dentro- y across-dataset 的评估，并证明ScanDL在生成扫描路径方面显著超越了现有的扫描路径生成方法。最后，我们进行了广泛的心理语言分析，并证明ScanDL能够展现出人类化的阅读行为。我们的实现可以在https://github.com/DiLi-Lab/ScanDL 上找到。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Representations-for-Teacher-Guided-Compositional-Visual-Reasoning"><a href="#Multimodal-Representations-for-Teacher-Guided-Compositional-Visual-Reasoning" class="headerlink" title="Multimodal Representations for Teacher-Guided Compositional Visual Reasoning"></a>Multimodal Representations for Teacher-Guided Compositional Visual Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15585">http://arxiv.org/abs/2310.15585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wafa Aissa, Marin Ferecatu, Michel Crucianu</li>
<li>for: 提高图像问答模型的效果和可读性</li>
<li>methods: 使用大规模交叉模式Encoder获取特征，并利用学习导师约束来改善模型的训练方法</li>
<li>results: 通过增加交叉模式特征和改进训练方法，实现图像问答模型的性能和可读性之间的平衡<details>
<summary>Abstract</summary>
Neural Module Networks (NMN) are a compelling method for visual question answering, enabling the translation of a question into a program consisting of a series of reasoning sub-tasks that are sequentially executed on the image to produce an answer. NMNs provide enhanced explainability compared to integrated models, allowing for a better understanding of the underlying reasoning process. To improve the effectiveness of NMNs we propose to exploit features obtained by a large-scale cross-modal encoder. Also, the current training approach of NMNs relies on the propagation of module outputs to subsequent modules, leading to the accumulation of prediction errors and the generation of false answers. To mitigate this, we introduce an NMN learning strategy involving scheduled teacher guidance. Initially, the model is fully guided by the ground-truth intermediate outputs, but gradually transitions to an autonomous behavior as training progresses. This reduces error accumulation, thus improving training efficiency and final performance.We demonstrate that by incorporating cross-modal features and employing more effective training techniques for NMN, we achieve a favorable balance between performance and transparency in the reasoning process.
</details>
<details>
<summary>摘要</summary>
神经模块网络（NMN）是一种吸引人的方法 для视觉问答，允许将问题转化为一系列的逻辑子任务，并在图像上顺序执行以生成答案。NMN提供了提高可读性的优势，allowing for a better understanding of the underlying reasoning process. To improve the effectiveness of NMNs, we propose to exploit features obtained by a large-scale cross-modal encoder. In addition, the current training approach of NMNs relies on the propagation of module outputs to subsequent modules, leading to the accumulation of prediction errors and the generation of false answers. To mitigate this, we introduce an NMN learning strategy involving scheduled teacher guidance. Initially, the model is fully guided by the ground-truth intermediate outputs, but gradually transitions to an autonomous behavior as training progresses. This reduces error accumulation, thus improving training efficiency and final performance.We demonstrate that by incorporating cross-modal features and employing more effective training techniques for NMN, we achieve a favorable balance between performance and transparency in the reasoning process.
</details></li>
</ul>
<hr>
<h2 id="POE-Process-of-Elimination-for-Multiple-Choice-Reasoning"><a href="#POE-Process-of-Elimination-for-Multiple-Choice-Reasoning" class="headerlink" title="POE: Process of Elimination for Multiple Choice Reasoning"></a>POE: Process of Elimination for Multiple Choice Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15575">http://arxiv.org/abs/2310.15575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kasmasvan/poe">https://github.com/kasmasvan/poe</a></li>
<li>paper_authors: Chenkai Ma, Xinya Du</li>
<li>for: 提高自然语言处理器在多选理智任务中的表现</li>
<li>methods: 提出了一种两步评分方法，称为过程排除（POE），首先对每个选项进行评分，然后根据评分结果排除看上去错误的选项，并从剩下的选项中进行最终预测。</li>
<li>results: 在8种理智任务上进行零例试验，证明POE方法的有效性，并且发现POE方法尤其适合逻辑理智任务。此外，还进行了masks的分析，并证明POE方法可以应用于少量示例设定和大语言模型（LLMs）如ChatGPT。<details>
<summary>Abstract</summary>
Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As humans often first eliminate wrong options before picking the final correct answer, we argue a similar two-step strategy can make LMs better at these tasks. To this end, we present the Process of Elimination (POE), a two-step scoring method. In the first step, POE scores each option, and eliminates seemingly wrong options. In the second step, POE masks these wrong options, and makes the final prediction from the remaining options. Zero-shot experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a following analysis finds our method to be especially performant on logical reasoning tasks. We further analyze the effect of masks, and show that POE applies to few-shot settings and large language models (LLMs) like ChatGPT.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）可以在多选问题上进行上下文学习，但是选项在这些任务中往往被对待相同。人类通常会先消除错误的选项，然后选择最终正确的答案。我们认为类似的两步策略可以使LM更好地处理这些任务。为此，我们提出了消除过程（POE），它是一种两步评分方法。在第一步，POE对每个选项进行评分，并消除看起来错误的选项。在第二步，POE隐藏这些错误的选项，然后从剩下的选项中进行最终预测。在零容量实验中，POE在8个理解任务上表现出色，并且分析发现，POE在逻辑理解任务上表现特别出色。我们还分析了mask的效果，并证明POE适用于少量上下文和大语言模型（LLM）如ChatGPT。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-Processing-for-Drug-Discovery-Knowledge-Graphs-promises-and-pitfalls"><a href="#Natural-Language-Processing-for-Drug-Discovery-Knowledge-Graphs-promises-and-pitfalls" class="headerlink" title="Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls"></a>Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15572">http://arxiv.org/abs/2310.15572</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Charles G. Jeynes, Tim James, Matthew Corney</li>
<li>for: 该论文旨在探讨使用自然语言处理（NLP）技术来挖掘科学文献中的不结构化文本，以帮助建立知识图谱（KG）以激发药物发现。</li>
<li>methods: 该论文使用NLP技术来自动提取文本中的数据，以增强知识图谱中的数据。</li>
<li>results: 该论文指出，使用NLP技术可以自动提取数据从数百万个文档中，但也存在许多可能的坏处，如命名实体识别和 ontology 链接错误，这些错误可能导致错误的推论和结论。<details>
<summary>Abstract</summary>
Building and analysing knowledge graphs (KGs) to aid drug discovery is a topical area of research. A salient feature of KGs is their ability to combine many heterogeneous data sources in a format that facilitates discovering connections. The utility of KGs has been exemplified in areas such as drug repurposing, with insights made through manual exploration and modelling of the data. In this article, we discuss promises and pitfalls of using natural language processing (NLP) to mine unstructured text typically from scientific literature as a data source for KGs. This draws on our experience of initially parsing structured data sources such as ChEMBL as the basis for data within a KG, and then enriching or expanding upon them using NLP. The fundamental promise of NLP for KGs is the automated extraction of data from millions of documents a task practically impossible to do via human curation alone. However, there are many potential pitfalls in NLP-KG pipelines such as incorrect named entity recognition and ontology linking all of which could ultimately lead to erroneous inferences and conclusions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Visually-Grounded-Continual-Language-Learning-with-Selective-Specialization"><a href="#Visually-Grounded-Continual-Language-Learning-with-Selective-Specialization" class="headerlink" title="Visually Grounded Continual Language Learning with Selective Specialization"></a>Visually Grounded Continual Language Learning with Selective Specialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15571">http://arxiv.org/abs/2310.15571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyra Ahrens, Lennart Bengtson, Jae Hee Lee, Stefan Wermter</li>
<li>for: 本研究旨在提供Language-informed continual learning中模型特性的扩展分析，以便控制特定任务学习和总体知识的平衡。</li>
<li>methods: 本研究使用了多种特性分析和模型评估策略，包括两个新引入的 диагностические数据集，以及多种模型结构和特性分析。</li>
<li>results: 研究结果表明，选择策略对于Language-informed continual learning中的模型特性具有重要作用，并且提出了一些简单易行的方法，以超越常见的 continual learning 基elines。<details>
<summary>Abstract</summary>
A desirable trait of an artificial agent acting in the visual world is to continually learn a sequence of language-informed tasks while striking a balance between sufficiently specializing in each task and building a generalized knowledge for transfer. Selective specialization, i.e., a careful selection of model components to specialize in each task, is a strategy to provide control over this trade-off. However, the design of selection strategies requires insights on the role of each model component in learning rather specialized or generalizable representations, which poses a gap in current research. Thus, our aim with this work is to provide an extensive analysis of selection strategies for visually grounded continual language learning. Due to the lack of suitable benchmarks for this purpose, we introduce two novel diagnostic datasets that provide enough control and flexibility for a thorough model analysis. We assess various heuristics for module specialization strategies as well as quantifiable measures for two different types of model architectures. Finally, we design conceptually simple approaches based on our analysis that outperform common continual learning baselines. Our results demonstrate the need for further efforts towards better aligning continual learning algorithms with the learning behaviors of individual model parts.
</details>
<details>
<summary>摘要</summary>
文本中的愿望是一个人工智能在视觉世界中不断学习一串语言指导的任务，同时保持学习任务之间的平衡。选择性特殊化是一种策略，可以为这个贸易做出控制。然而，选择策略的设计需要了解模型组件在学习特定或通用表示方面的角色，这种空白在当前研究中存在。因此，我们的目标是通过广泛的分析来探讨选择策略的选择。由于没有适合的标准准确量表 для这种目的，我们引入了两个新的诊断数据集，以便对模型进行完善的分析。我们评估了多种模块特殊化策略，以及两种不同的模型架构中的量化度量。最后，我们设计了简单易于实现的方法，并超越常见的 kontinuierliche learning 基线。我们的结果表明需要更多的努力来更好地对照 continual learning 算法和模型组件之间的学习行为进行对应。
</details></li>
</ul>
<hr>
<h2 id="MuLMS-A-Multi-Layer-Annotated-Text-Corpus-for-Information-Extraction-in-the-Materials-Science-Domain"><a href="#MuLMS-A-Multi-Layer-Annotated-Text-Corpus-for-Information-Extraction-in-the-Materials-Science-Domain" class="headerlink" title="MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain"></a>MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15569">http://arxiv.org/abs/2310.15569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timo Pierre Schrader, Matteo Finco, Stefan Grünewald, Felix Hildebrand, Annemarie Friedrich</li>
<li>for: 本文旨在提供一个新的数据集，用于支持材料科学领域的研究。</li>
<li>methods: 本文使用了多种机器学习模型，包括命名实体识别、关系检测和帧结构识别等多种任务。</li>
<li>results: 研究发现，通过多任务训练和利用相关的资源，可以获得竞争力强的模型性能。<details>
<summary>Abstract</summary>
Keeping track of all relevant recent publications and experimental results for a research area is a challenging task. Prior work has demonstrated the efficacy of information extraction models in various scientific areas. Recently, several datasets have been released for the yet understudied materials science domain. However, these datasets focus on sub-problems such as parsing synthesis procedures or on sub-domains, e.g., solid oxide fuel cells. In this resource paper, we present MuLMS, a new dataset of 50 open-access articles, spanning seven sub-domains of materials science. The corpus has been annotated by domain experts with several layers ranging from named entities over relations to frame structures. We present competitive neural models for all tasks and demonstrate that multi-task training with existing related resources leads to benefits.
</details>
<details>
<summary>摘要</summary>
监控最新相关发表文章和实验结果是研究领域中的一项具有挑战性的任务。先前的工作已经证明了信息EXTRACTION模型在不同的科学领域中的效果。最近，物理科学领域内的一些数据集已经发布。然而，这些数据集都是关注子问题，如 sintesis过程解析或特定子领域，如固体燃料电池。在这篇资源文章中，我们介绍了MuLMS数据集，包含50篇开放访问文章，覆盖了物理科学七个子领域。这个corpus已经由域专家注释了多层，从名称实体到关系到帧结构。我们提出了竞争力强的神经网络模型，并示出了多任务训练与现有相关资源的合作带来的 beneficial effect。
</details></li>
</ul>
<hr>
<h2 id="TCRA-LLM-Token-Compression-Retrieval-Augmented-Large-Language-Model-for-Inference-Cost-Reduction"><a href="#TCRA-LLM-Token-Compression-Retrieval-Augmented-Large-Language-Model-for-Inference-Cost-Reduction" class="headerlink" title="TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction"></a>TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15556">http://arxiv.org/abs/2310.15556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming Qian</li>
<li>For: The paper aims to mitigate the cost of deploying commercial retrieval-augmented large language models (LLMs) by proposing a token compression scheme.* Methods: The proposed token compression scheme includes two methods: summarization compression and semantic compression. The first method uses a T5-based model fine-tuned by datasets generated using self-instruct, while the second method removes words with lower impact on the semantic.* Results: The proposed methods are evaluated using a dataset called Food-Recommendation DB (FRDB) focusing on food recommendation for women around pregnancy period or infants. The results show that the summarization compression can reduce 65% of the retrieval token size with further 0.3% improvement on the accuracy, while the semantic compression provides a more flexible way to trade-off the token size with performance, with a 20% reduction in token size and only 1.6% drop in accuracy.<details>
<summary>Abstract</summary>
Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB) focusing on food recommendation for women around pregnancy period or infants. Our summarization compression can reduce 65% of the retrieval token size with further 0.3% improvement on the accuracy; semantic compression provides a more flexible way to trade-off the token size with performance, for which we can reduce the token size by 20% with only 1.6% of accuracy drop.
</details>
<details>
<summary>摘要</summary>
The first method uses a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths, reducing the token size by summarizing the content. The second method further compresses the token size by removing words with lower impact on the semantic. To evaluate the effectiveness of the proposed methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB) that focuses on food recommendations for women during pregnancy or infancy.Our summarization compression can reduce the retrieval token size by 65% with a further 0.3% improvement in accuracy, while the semantic compression provides a more flexible way to trade-off the token size with performance, reducing the token size by 20% with only a 1.6% drop in accuracy.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Multilinguality-in-Transformer-Models-Exploring-Language-Specificity-in-Feed-Forward-Networks"><a href="#Unveiling-Multilinguality-in-Transformer-Models-Exploring-Language-Specificity-in-Feed-Forward-Networks" class="headerlink" title="Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks"></a>Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15552">http://arxiv.org/abs/2310.15552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunit Bhattacharya, Ondrej Bojar</li>
<li>for: 这个论文的目的是研究Transformer模型中的Feed-Forward模块是一个键值记忆集合，keys学习了各个输入pattern的特定模式，values则将这些’记忆’的输出组合起来预测下一个token。</li>
<li>methods: 这个论文使用了Transformer模型的pretraining和autoregressive预测方法，并通过使用并行 corpora来验证自己的假设。</li>
<li>results: 研究发现，模型的输入和输出层更加具有语言特有的行为，而中间层则更加具有语言共享的特征。<details>
<summary>Abstract</summary>
Recent research suggests that the feed-forward module within Transformers can be viewed as a collection of key-value memories, where the keys learn to capture specific patterns from the input based on the training examples. The values then combine the output from the 'memories' of the keys to generate predictions about the next token. This leads to an incremental process of prediction that gradually converges towards the final token choice near the output layers. This interesting perspective raises questions about how multilingual models might leverage this mechanism. Specifically, for autoregressive models trained on two or more languages, do all neurons (across layers) respond equally to all languages? No! Our hypothesis centers around the notion that during pretraining, certain model parameters learn strong language-specific features, while others learn more language-agnostic (shared across languages) features. To validate this, we conduct experiments utilizing parallel corpora of two languages that the model was initially pretrained on. Our findings reveal that the layers closest to the network's input or output tend to exhibit more language-specific behaviour compared to the layers in the middle.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)近期研究表明，Transformer 中的 feed-forward 模块可以视为一个包含键值记忆的集合，其中键学习 capture 输入中特定的模式，基于训练示例。值们则将 ' memories ' 中的输出 combine 以生成下一个token的预测。这种逐步进行的预测过程，逐渐 converges 到输出层的最终token选择。这种有趣的视角提出了关于多语言模型如何利用这种机制的问题。例如，对于使用两个或更多语言的 autoregressive 模型，所有神经元（在层次上）是否对所有语言具有相同的响应？不是！我们的假设是，在预训练时，模型参数会学习强度语言特有的特征，而其他参数则会学习更加语言共享的特征。为验证这一假设，我们通过使用两种语言的并行 corpus 进行实验。我们的发现是，输入层和输出层的层次比较接近的神经元具有更强的语言特有行为，与中间层的神经元相比。
</details></li>
</ul>
<hr>
<h2 id="Improving-Language-Models-Meaning-Understanding-and-Consistency-by-Learning-Conceptual-Roles-from-Dictionary"><a href="#Improving-Language-Models-Meaning-Understanding-and-Consistency-by-Learning-Conceptual-Roles-from-Dictionary" class="headerlink" title="Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary"></a>Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15541">http://arxiv.org/abs/2310.15541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Myeongjun Erik Jang, Thomas Lukasiewicz</li>
<li>for: 本研究旨在解决现代预训练语言模型（PLM）的不信worthiness问题，即生成不一致的预测问题，这会导致生成不同的预测结果，但是表达的意思相同。</li>
<li>methods: 我们提出了一种实用的方法，通过加强PLM的意识性来解决这个问题。我们基于字典中的词义对的概念角色理论来让PLM学习准确的意思。然后，我们提出了一种高效的参数集成技术，可以快速地将学习到的概念关系与PLM的预训练知识结合起来。</li>
<li>results: 我们的实验结果表明，我们的方法可以同时改善多种一致性，快速地集成知识，并可以应用于其他语言。<details>
<summary>Abstract</summary>
The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. Previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue. However, their usage is limited, because they consume expensive training resources for large-sized PLMs and can only handle a certain consistency type. To this end, we propose a practical approach that alleviates the inconsistent behaviour issue by fundamentally improving PLMs' meaning awareness. Based on the conceptual role theory, our method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary. Next, we propose an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with PLMs' pre-trained knowledge. Our experimental results reveal that the approach can concurrently improve multiple types of consistency, enables efficient knowledge integration, and easily applies to other languages.
</details>
<details>
<summary>摘要</summary>
当代预训练语言模型（PLM）的非人类行为是信worthiness的主要原因。一种 striking 的现象是生成不一致的预测，这会产生逻辑矛盾的结果，如生成表达同义文本时不同的预测或违反逻辑规则。先前的研究使用了数据扩展或特殊的损失函数来缓解问题，但它们的使用有限，因为它们需要大量的训练资源，只能处理一定的一致性类型。为此，我们提出了一种实用的方法，可以根本改善 PLM 的意识意识。基于概念角色理论，我们的方法使 PLM 能够准确地捕捉意思，通过学习词定义对的精准关系。然后，我们提出了一种高效的参数集成技术，可以在 PLM 的预训练知识基础之上更新只需要一些额外参数。我们的实验结果表明，该方法可以同时改善多种一致性，实现高效的知识集成，并且容易应用于其他语言。
</details></li>
</ul>
<hr>
<h2 id="MarkQA-A-large-scale-KBQA-dataset-with-numerical-reasoning"><a href="#MarkQA-A-large-scale-KBQA-dataset-with-numerical-reasoning" class="headerlink" title="MarkQA: A large scale KBQA dataset with numerical reasoning"></a>MarkQA: A large scale KBQA dataset with numerical reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15517">http://arxiv.org/abs/2310.15517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cdhx/markqa">https://github.com/cdhx/markqa</a></li>
<li>paper_authors: Xiang Huang, Sitao Cheng, Yuheng Bao, Shanshan Huang, Yuzhong Qu</li>
<li>for: 本研究对知识库问答（KBQA）的进一步发展进行了尝试，特别是对数字reasoning进行了更多的探索。</li>
<li>methods: 本文提出了一个新的任务，即NR-KBQA，它需要某些问题的解决需要进行多个跳步逻辑和数字逻辑。作者们提出了一种逻辑表示形式，即Python的PyQL，用于表示数字逻辑问题的解决过程。</li>
<li>results: 对MarkQA数据集进行了一些state-of-the-art QA方法的实验，结果显示，复杂的数字逻辑在KBQA中具有很大的挑战。<details>
<summary>Abstract</summary>
While question answering over knowledge bases (KBQA) has shown progress in addressing factoid questions, KBQA with numerical reasoning remains relatively unexplored. In this paper, we focus on the complex numerical reasoning in KBQA and propose a new task, NR-KBQA, which necessitates the ability to perform both multi-hop reasoning and numerical reasoning. We design a logic form in Python format called PyQL to represent the reasoning process of numerical reasoning questions. To facilitate the development of NR-KBQA, we present a large dataset called MarkQA, which is automatically constructed from a small set of seeds. Each question in MarkQA is equipped with its corresponding SPARQL query, alongside the step-by-step reasoning process in the QDMR format and PyQL program. Experimental results of some state-of-the-art QA methods on the MarkQA show that complex numerical reasoning in KBQA faces great challenges.
</details>
<details>
<summary>摘要</summary>
while 问答 sobre bases de conocimiento (KBQA) ha demostrado progreso en responder preguntas de hecho, el KBQA con razonamiento numérico permanece relativamente inexplorado. En este artículo, nos enfocamos en el razonamiento numérico complejo en KBQA y propusimos una nueva tarea, NR-KBQA, que requiere la habilidad de realizar tanto razonamiento en múltiples pasos como razonamiento numérico. Diseñamos una forma lógica en formato Python llamada PyQL para representar el proceso de razonamiento de preguntas de números. Para facilitar el desarrollo de NR-KBQA, presentamos una gran base de datos llamada MarkQA, que se construyó automáticamente a partir de un conjunto pequeño de semillas. Cada pregunta en MarkQA está equipada con su correspondiente consulta SPARQL, así como el proceso de razonamiento paso a paso en el formato QDMR y el programa PyQL. Los resultados experimentales de algunos métodos de QA estado-de-arte en MarkQA muestran que el razonamiento numérico complejo en KBQA enfrenta grandes desafíos.
</details></li>
</ul>
<hr>
<h2 id="Fighting-Fire-with-Fire-The-Dual-Role-of-LLMs-in-Crafting-and-Detecting-Elusive-Disinformation"><a href="#Fighting-Fire-with-Fire-The-Dual-Role-of-LLMs-in-Crafting-and-Detecting-Elusive-Disinformation" class="headerlink" title="Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation"></a>Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15515">http://arxiv.org/abs/2310.15515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya Rohatgi, Dongwon Lee</li>
<li>for: 防止大语言模型（LLM）被负用（即生成大规模危害和误导性内容）。</li>
<li>methods: 提议一种“战火与火”（F3）策略，利用现代LLM的生成和总结能力对人类写的和LLM生成的假信息进行抗击。</li>
<li>results: 在广泛的实验中，我们发现GPT-3.5-turbo在适用零极学习上下文Semantic Reasoning技术的情况下，对各种数据集（包括原始和假数据集）具有显著的优势，其精度在68-72%之间，而其他自定义和精度调整的假信息检测器则显示负性。<details>
<summary>Abstract</summary>
Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (.i.e, generating large-scale harmful and misleading content). To combat this emerging risk of LLMs, we propose a novel "Fighting Fire with Fire" (F3) strategy that harnesses modern LLMs' generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3.
</details>
<details>
<summary>摘要</summary>
最近，大型自然语言模型（LLM）的普遍性和破坏性带来了关于其可能被滥用（例如，生成大规模危害和误导性内容）的担忧。为了解决这种emerging risk，我们提出了一种“战火与火”（F3）策略，利用现代LLM的生成和emergent reasoning能力来对人写的和LLM生成的假信息进行反击。首先，我们利用GPT-3.5-turbo来生成真实和假的LLM生成的内容，通过重写和扰动预 prompts来分别生成假和真实的内容。其次，我们运用零批学内存推理技术，通过cloze预 prompts来判断真实和假的文章和新闻报道。在我们的广泛实验中，我们发现GPT-3.5-turbo在各种数据集上表现出零批学的优势，其中GPT-3.5-turbo在各种distribution和out-of-distribution数据集上都能够达到68-72%的准确率，与之前经过定制和精度调整的假信息检测器不同，其准确率在下降。我们的代码库和数据集可以在https://github.com/mickeymst/F3中获取。
</details></li>
</ul>
<hr>
<h2 id="A-Joint-Matrix-Factorization-Analysis-of-Multilingual-Representations"><a href="#A-Joint-Matrix-Factorization-Analysis-of-Multilingual-Representations" class="headerlink" title="A Joint Matrix Factorization Analysis of Multilingual Representations"></a>A Joint Matrix Factorization Analysis of Multilingual Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15513">http://arxiv.org/abs/2310.15513</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zsquaredz/joint_multilingual_analysis">https://github.com/zsquaredz/joint_multilingual_analysis</a></li>
<li>paper_authors: Zheng Zhao, Yftah Ziser, Bonnie Webber, Shay B. Cohen</li>
<li>for: This paper is written to analyze the representations learned by multilingual pre-trained models and study how they encode morphosyntactic information.</li>
<li>methods: The authors use joint matrix factorization as an alternative to probing to compare the latent representations of multilingual and monolingual models.</li>
<li>results: The authors find variations in the encoding of morphosyntactic information across upper and lower layers, with category-specific differences influenced by language properties. They also find strong associations between the factorization outputs and performance across different cross-lingual tasks.<details>
<summary>Abstract</summary>
We present an analysis tool based on joint matrix factorization for comparing latent representations of multilingual and monolingual models. An alternative to probing, this tool allows us to analyze multiple sets of representations in a joint manner. Using this tool, we study to what extent and how morphosyntactic features are reflected in the representations learned by multilingual pre-trained models. We conduct a large-scale empirical study of over 33 languages and 17 morphosyntactic categories. Our findings demonstrate variations in the encoding of morphosyntactic information across upper and lower layers, with category-specific differences influenced by language properties. Hierarchical clustering of the factorization outputs yields a tree structure that is related to phylogenetic trees manually crafted by linguists. Moreover, we find the factorization outputs exhibit strong associations with performance observed across different cross-lingual tasks. We release our code to facilitate future research.
</details>
<details>
<summary>摘要</summary>
我们提出了基于共同矩阵因子化的分析工具，用于比较多语言和单语言模型的潜在表示。这是探测工具的一种代替方式，允许我们同时分析多个表示集。使用这种工具，我们研究了多语言预训练模型学习的 morphosyntactic 特征如何反映在其中。我们进行了33种语言和17种 morphosyntactic 类别的大规模实验研究。我们的发现表明，在上下层之间存在不同的 morphosyntactic 编码方式，并且这些差异受到语言属性的影响。使用层次 clustering 分析输出得到的树结构与手动制作的语言树相关。此外，我们发现了因素分解输出与不同的语言交互任务中的性能强相关。我们发布了我们的代码，以便将来的研究。
</details></li>
</ul>
<hr>
<h2 id="TRAMS-Training-free-Memory-Selection-for-Long-range-Language-Modeling"><a href="#TRAMS-Training-free-Memory-Selection-for-Long-range-Language-Modeling" class="headerlink" title="TRAMS: Training-free Memory Selection for Long-range Language Modeling"></a>TRAMS: Training-free Memory Selection for Long-range Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15494">http://arxiv.org/abs/2310.15494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lwaekfjlk/trams">https://github.com/lwaekfjlk/trams</a></li>
<li>paper_authors: Haofei Yu, Cunxiang wang, Yue Zhang, Wei Bi</li>
<li>for: 提高Transformer架构在长距离语言模型方面的性能</li>
<li>methods: 提出了一种协助选择 calculus 计算中参与者的简单指标，以提高Transformer架构的长距离语言模型性能</li>
<li>results: 在Word-levelbenchmark（WikiText-103）和Character-level benchmark（enwik8）上测试了该方法，并获得了提高性能的结果，不需要额外训练或添加参数。<details>
<summary>Abstract</summary>
The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.
</details>
<details>
<summary>摘要</summary>
transformer 架构在许多人工智能模型中具有重要作用，但它在长距离语言模型化方面仍面临挑战。虽然有一些特定的 transformer 架构被设计来解决长距离依赖关系的问题，但现有方法如 transformer-xl 受到高比例的不有效内存带来困扰。在这项研究中，我们提出了一种插件化策略，称为 TRAining-free Memory Selection (TRAMS)，该策略选择基于一个简单度量的 tokens 参与计算注意力。这种策略允许我们保留与当前查询具有高注意力的 tokens，并忽略其他 tokens。我们在 word-level 约束（WikiText-103）和 character-level 约束（enwik8）上测试了我们的方法，结果表明我们的方法可以提高性能，无需进行额外训练或添加额外参数。
</details></li>
</ul>
<hr>
<h2 id="CRaSh-Clustering-Removing-and-Sharing-Enhance-Fine-tuning-without-Full-Large-Language-Model"><a href="#CRaSh-Clustering-Removing-and-Sharing-Enhance-Fine-tuning-without-Full-Large-Language-Model" class="headerlink" title="CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model"></a>CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15477">http://arxiv.org/abs/2310.15477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/CRaSh">https://github.com/TsinghuaC3I/CRaSh</a></li>
<li>paper_authors: Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xinwei Long, Bowen Zhou</li>
<li>for: 本研究旨在探讨如何使用Offsite-Tuning（OFT）技术来提高中央化大语言模型（LLM）的泛化能力，同时保护私人数据的隐私。</li>
<li>methods: 本研究使用了Empirical Analysis和CRaSh（Clustering, Removing, and Sharing）训练自由策略来探索LLM层次结构和表达的特性，以及OFT的效果。</li>
<li>results: 研究发现LLM具有层次结构，并且在不同层次上存在表达和中间预测的变化。此外，CRaSh训练自由策略可以大幅提高OFT性能，并且研究发现了基于损失函数的优化的优化方法。<details>
<summary>Abstract</summary>
Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning (OFT), a representative technique that transfers transformer blocks between centralized LLMs and downstream emulators. Given the limited understanding of the underlying mechanism of OFT, we perform an empirical analysis on LLMs from the perspectives of representation and functional similarity. Interestingly, our findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands. Simultaneously, we note subtle but potentially significant changes in representation and intermediate predictions across the layers. Inspired by these observations, we propose CRaSh, involving Clustering, Removing, and Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh significantly boosts performance of OFT with billions of parameters. Furthermore, we investigate the optimal solutions yielded by fine-tuning with and without full model through the lens of loss landscape. Our findings demonstrate a linear connectivity among these optima falling over the same basin, thereby highlighting the effectiveness of CRaSh and OFT. The source code is publicly available at https://github.com/TsinghuaC3I/CRaSh.
</details>
<details>
<summary>摘要</summary>
征文报告：现代大语言模型（LLM）的调教技术已被认为是提高 LLM 的通用能力的有效方法。然而，当将公共可访问的中央 LLM 调教私人指令数据时，隐私问题是不可避免的。直接将参数化模块 между模型传递是一种可能的方法，但它的影响和效果需要进一步的探索。本文关注 Offsite-Tuning（OFT）技术，该技术将 transformer 块 между中央 LLM 和下游模拟器传递。由于 OFT 的深层理解尚未得到充分的研究，我们进行了empirical分析 LLM 的表示和功能相似性。我们发现 LLM 具有一种唯一的模块结构，该结构随模型大小增加而出现。此外，我们注意到在层次中存在潜在重要的表示和中间预测变化。 inspirited by these observations，我们提出了 CRaSh，即 Clustering, Removing, and Sharing 训练free的策略，以 derive improved emulators from LLMs。CRaSh 可以显著提高 OFT 的性能，并且我们通过对 fine-tuning with 和 without full model 的研究，发现这些优点在同一个极值附近。我们的发现表明 CRaSh 和 OFT 的效果。源代码可以在 https://github.com/TsinghuaC3I/CRaSh 上获取。
</details></li>
</ul>
<hr>
<h2 id="Continual-Event-Extraction-with-Semantic-Confusion-Rectification"><a href="#Continual-Event-Extraction-with-Semantic-Confusion-Rectification" class="headerlink" title="Continual Event Extraction with Semantic Confusion Rectification"></a>Continual Event Extraction with Semantic Confusion Rectification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15470">http://arxiv.org/abs/2310.15470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nju-websoft/SCR">https://github.com/nju-websoft/SCR</a></li>
<li>paper_authors: Zitao Wang, Xinyi Wang, Wei Hu</li>
<li>for: 本研究旨在提取不断出现的事件信息，并避免忘记现象。</li>
<li>methods: 我们提出了一种新的不断事件提取模型，使用semantic confusion rectification来纠正事件类型的含义混乱。我们为每句文本添加 Pseudo标签，以减轻semantic confusion。此外，我们还将当前和前一个模型之间的重要知识传递，以提高事件类型的理解。</li>
<li>results: 我们的模型在不平衡数据集上表现出色，比基eline模型更高效。<details>
<summary>Abstract</summary>
We study continual event extraction, which aims to extract incessantly emerging event information while avoiding forgetting. We observe that the semantic confusion on event types stems from the annotations of the same text being updated over time. The imbalance between event types even aggravates this issue. This paper proposes a novel continual event extraction model with semantic confusion rectification. We mark pseudo labels for each sentence to alleviate semantic confusion. We transfer pivotal knowledge between current and previous models to enhance the understanding of event types. Moreover, we encourage the model to focus on the semantics of long-tailed event types by leveraging other associated types. Experimental results show that our model outperforms state-of-the-art baselines and is proficient in imbalanced datasets.
</details>
<details>
<summary>摘要</summary>
我们研究不断发生的事件提取，旨在不断出现的事件信息的提取，而避免忘记。我们发现事件类型的含义混乱来自文本更新过程中的注释。事件类型的偏度更进一步加剧了这个问题。这篇论文提出了一种新的不断事件提取模型，用于纠正含义混乱。我们为每句话添加 Pseudo 标签，以减轻含义混乱。我们将当前和前一个模型之间的核心知识传递，以提高事件类型的理解。此外，我们采用其他相关类型来强化长尾事件类型的含义。实验结果表明，我们的模型在不良数据集中表现出色，超越了当前的基elines。
</details></li>
</ul>
<hr>
<h2 id="The-Janus-Interface-How-Fine-Tuning-in-Large-Language-Models-Amplifies-the-Privacy-Risks"><a href="#The-Janus-Interface-How-Fine-Tuning-in-Large-Language-Models-Amplifies-the-Privacy-Risks" class="headerlink" title="The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks"></a>The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15469">http://arxiv.org/abs/2310.15469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang, Liya Su, XiaoFeng Wang, Haixu Tang</li>
<li>for: 这项研究旨在探讨 Large Language Models (LLMs) 在训练过程中是否可能泄露个人隐私信息。</li>
<li>methods: 该研究使用了 OpenAI 的 GPT-3.5 模型，并通过构建一个 PII assoiciation 任务来探讨 LLM 是否可能泄露个人隐私信息。</li>
<li>results: 研究发现，通过一个非常小的 PII 数据集进行微调，LLM 可以从原本不可能泄露 PII 的状态转变为可以泄露大量隐私信息的状态。这种攻击方式被称为 Janus 攻击。<details>
<summary>Abstract</summary>
The era post-2018 marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue, called the Janus attack. In the attack, one can construct a PII association task, whereby an LLM is fine-tuned using a minuscule PII dataset, to potentially reinstate and reveal concealed PIIs. Our findings indicate that, with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from being impermeable to PII extraction to a state where they divulge a substantial proportion of concealed PII. This research, through its deep dive into the Janus attack vector, underscores the imperative of navigating the intricate interplay between LLM utility and privacy preservation.
</details>
<details>
<summary>摘要</summary>
post-2018年代marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue, called the Janus attack. In the attack, one can construct a PII association task, whereby an LLM is fine-tuned using a minuscule PII dataset, to potentially reinstate and reveal concealed PIIs. Our findings indicate that, with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from being impermeable to PII extraction to a state where they divulge a substantial proportion of concealed PII. This research, through its deep dive into the Janus attack vector, underscores the imperative of navigating the intricate interplay between LLM utility and privacy preservation.
</details></li>
</ul>
<hr>
<h2 id="Interpreting-Answers-to-Yes-No-Questions-in-User-Generated-Content"><a href="#Interpreting-Answers-to-Yes-No-Questions-in-User-Generated-Content" class="headerlink" title="Interpreting Answers to Yes-No Questions in User-Generated Content"></a>Interpreting Answers to Yes-No Questions in User-Generated Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15464">http://arxiv.org/abs/2310.15464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivam Mathur, Keun Hee Park, Dhivya Chinnappa, Saketh Kotamraju, Eduardo Blanco</li>
<li>for: 这篇论文是关于如何解释社交媒体上的问答问题的。</li>
<li>methods: 论文使用了4442个Twitter上的问答对，并分析了答案中的语言特征，以及无法解释的答案。</li>
<li>results: 研究发现，大型语言模型尚未解决了这个问题，即使精度和混合其他 corpora。<details>
<summary>Abstract</summary>
Interpreting answers to yes-no questions in social media is difficult. Yes and no keywords are uncommon, and the few answers that include them are rarely to be interpreted what the keywords suggest. In this paper, we present a new corpus of 4,442 yes-no question-answer pairs from Twitter. We discuss linguistic characteristics of answers whose interpretation is yes or no, as well as answers whose interpretation is unknown. We show that large language models are far from solving this problem, even after fine-tuning and blending other corpora for the same problem but outside social media.
</details>
<details>
<summary>摘要</summary>
社交媒体中答复问答的解释困难。yes和no关键词罕见，答案中很少包含这些词语，而且这些词语的解释并不总是简单。在这篇论文中，我们提供了4,442个yes-no问答对的新词汇库，并讨论了答案的语言特征，以及未知的解释。我们显示了大型自然语言模型，即使经过练习和混合其他词汇库，仍未能解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="Facilitating-Self-Guided-Mental-Health-Interventions-Through-Human-Language-Model-Interaction-A-Case-Study-of-Cognitive-Restructuring"><a href="#Facilitating-Self-Guided-Mental-Health-Interventions-Through-Human-Language-Model-Interaction-A-Case-Study-of-Cognitive-Restructuring" class="headerlink" title="Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring"></a>Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15461">http://arxiv.org/abs/2310.15461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashish Sharma, Kevin Rushton, Inna Wanyin Lin, Theresa Nguyen, Tim Althoff</li>
<li>for: 本研究旨在探讨人工智能语言模型如何支持自主心理健康 intervención.</li>
<li>methods: 我们采用了一种基于证据的心理治疗技巧——思想重构，作为案例研究。我们在一个大型心理健康网站上进行了一项IRB批准的随机场景研究，并设计了一个使用语言模型支持人们完成各种思想重构步骤的系统。</li>
<li>results: 我们发现，我们的系统对67%的参与者产生了正面的情感强度影响，并帮助65%的参与者超越负面思想。虽然青少年报告的result relatively worse,但我们发现可以通过简化语言模型生成来提高总效果和公平性。<details>
<summary>Abstract</summary>
Self-guided mental health interventions, such as "do-it-yourself" tools to learn and practice coping strategies, show great promise to improve access to mental health care. However, these interventions are often cognitively demanding and emotionally triggering, creating accessibility barriers that limit their wide-scale implementation and adoption. In this paper, we study how human-language model interaction can support self-guided mental health interventions. We take cognitive restructuring, an evidence-based therapeutic technique to overcome negative thinking, as a case study. In an IRB-approved randomized field study on a large mental health website with 15,531 participants, we design and evaluate a system that uses language models to support people through various steps of cognitive restructuring. Our findings reveal that our system positively impacts emotional intensity for 67% of participants and helps 65% overcome negative thoughts. Although adolescents report relatively worse outcomes, we find that tailored interventions that simplify language model generations improve overall effectiveness and equity.
</details>
<details>
<summary>摘要</summary>
自顾式精神健康互助 intervención,如"DIY"工具来学习和实践抗压力策略,显示了很大的托管难以实施和普及潜在。在这篇论文中，我们研究了人类语言模型交互如何支持自顾式精神健康互助。我们使用了证据基础的认知修剪技巧，即超越负面思维，作为一个案例研究。在一个获得IRB批准的随机场景学习中，我们设计并评估了一个使用语言模型支持人们进行多个步骤的认知修剪步骤。我们的发现表明，我们的系统对67%的参与者有积极的情感影响，并帮助65%的人超越负面思维。虽然青少年报告的结果相对较差，但我们发现了针对语言模型生成简化的个性化 intervención，可以提高总效果和公平性。
</details></li>
</ul>
<hr>
<h2 id="K-HATERS-A-Hate-Speech-Detection-Corpus-in-Korean-with-Target-Specific-Ratings"><a href="#K-HATERS-A-Hate-Speech-Detection-Corpus-in-Korean-with-Target-Specific-Ratings" class="headerlink" title="K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings"></a>K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15439">http://arxiv.org/abs/2310.15439</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssu-humane/k-haters">https://github.com/ssu-humane/k-haters</a></li>
<li>paper_authors: Chaewon Park, Soohwan Kim, Kyubyong Park, Kunwoo Park</li>
<li>for: 这个研究的目的是开发一个针对韩语 hate speech 检测的新词库，以提高现有的 hate speech 检测模型的准确性和可靠性。</li>
<li>methods: 这个研究使用了192000篇韩语新闻评论，并对其进行了target-specific offensiveness rating。此外，研究还采用了认知反射测试来评估annotations的质量。</li>
<li>results: 研究发现，使用lowest test scores的annotations可能会导致模型对特定目标群体进行偏袋的预测，并且准确性较低。这个研究对韩语 hate speech 检测领域的NLG研究做出了贡献，并提供了一个大规模的韩语 hate speech 词库。<details>
<summary>Abstract</summary>
Numerous datasets have been proposed to combat the spread of online hate. Despite these efforts, a majority of these resources are English-centric, primarily focusing on overt forms of hate. This research gap calls for developing high-quality corpora in diverse languages that also encapsulate more subtle hate expressions. This study introduces K-HATERS, a new corpus for hate speech detection in Korean, comprising approximately 192K news comments with target-specific offensiveness ratings. This resource is the largest offensive language corpus in Korean and is the first to offer target-specific ratings on a three-point Likert scale, enabling the detection of hate expressions in Korean across varying degrees of offensiveness. We conduct experiments showing the effectiveness of the proposed corpus, including a comparison with existing datasets. Additionally, to address potential noise and bias in human annotations, we explore a novel idea of adopting the Cognitive Reflection Test, which is widely used in social science for assessing an individual's cognitive ability, as a proxy of labeling quality. Findings indicate that annotations from individuals with the lowest test scores tend to yield detection models that make biased predictions toward specific target groups and are less accurate. This study contributes to the NLP research on hate speech detection and resource construction. The code and dataset can be accessed at https://github.com/ssu-humane/K-HATERS.
</details>
<details>
<summary>摘要</summary>
众多数据集已经提出以抗击在线仇恨的扩展。然而，大多数这些资源都是英语中心，主要关注于显着的仇恨表达。这个研究 gap 需要开发多语言高质量 corpora，同时包括更加柔和的仇恨表达。本研究介绍 K-HATERS，一个新的仇恨语言检测 corpora 在韩语中，包含约192万条新闻评论，每个评论具有目标特定的不宽容度评分。这是韩语中最大的不宽容度 corpora，也是第一个提供目标特定的三点 Likert 等级评分，以便在不同程度的不宽容度下检测韩语中的仇恨表达。我们进行了实验，证明了提posed corpus 的有效性，包括与现有数据集进行比较。此外，为了减少人工标注中的噪音和偏见，我们explore了一种新的想法，即采用社会科学中广泛使用的认知反射测试作为标注质量的代理。结果表明，由lowest test scores的人进行标注的模型倾向于特定目标群体的报告，并且准确性较低。这种研究对 NLP 领域的仇恨语言检测和资源建构做出了贡献。可以通过 GitHub 上的https://github.com/ssu-humane/K-HATERS 获取代码和数据集。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-for-Enhanced-Product-Descriptions-in-eCommerce"><a href="#Leveraging-Large-Language-Models-for-Enhanced-Product-Descriptions-in-eCommerce" class="headerlink" title="Leveraging Large Language Models for Enhanced Product Descriptions in eCommerce"></a>Leveraging Large Language Models for Enhanced Product Descriptions in eCommerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18357">http://arxiv.org/abs/2310.18357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianghong Zhou, Bo Liu, Jhalak Nilesh Acharya Yao Hong, Kuang-chih Lee, Musen Wen</li>
<li>for: 提高电商搜索可见性和用户参与度，提高销售额和用户满意度</li>
<li>methods: 使用LLAMA 2.0 7B语言模型自动生成产品描述，并对模型进行域名特定语言特征和电商细节的定制，以提高其在销售和用户参与方面的实用性</li>
<li>results: 系统可以减少人工劳动量，同时提高搜索可见性和用户 clicks， validate the effectiveness of our approach using multiple evaluation metrics such as NDCG, customer click-through rates, and human assessments.<details>
<summary>Abstract</summary>
In the dynamic field of eCommerce, the quality and comprehensiveness of product descriptions are pivotal for enhancing search visibility and customer engagement. Effective product descriptions can address the 'cold start' problem, align with market trends, and ultimately lead to increased click-through rates. Traditional methods for crafting these descriptions often involve significant human effort and may lack both consistency and scalability. This paper introduces a novel methodology for automating product description generation using the LLAMA 2.0 7B language model. We train the model on a dataset of authentic product descriptions from Walmart, one of the largest eCommerce platforms. The model is then fine-tuned for domain-specific language features and eCommerce nuances to enhance its utility in sales and user engagement. We employ multiple evaluation metrics, including NDCG, customer click-through rates, and human assessments, to validate the effectiveness of our approach. Our findings reveal that the system is not only scalable but also significantly reduces the human workload involved in creating product descriptions. This study underscores the considerable potential of large language models like LLAMA 2.0 7B in automating and optimizing various facets of eCommerce platforms, offering significant business impact, including improved search functionality and increased sales.
</details>
<details>
<summary>摘要</summary>
在电商领域中，产品描述的质量和完整性对搜索可见性和客户参与度有着重要的影响。有效的产品描述可以解决冷启动问题，遵循市场趋势，最终导致更高的点击率。传统的产品描述创作方法通常需要大量的人工劳动，并且可能缺乏一致性和可扩展性。这篇论文提出了一种使用 LLMA 2.0 7B 语言模型自动生成产品描述的新方法。我们在 Walmart 的实际产品描述数据集上训练了模型，然后对域pecific语言特征和电商特点进行了微调，以提高其在销售和用户参与度方面的实用性。我们使用了多种评价指标，包括 NDCG、客户点击率和人类评价，来验证我们的方法的有效性。我们的发现表明，系统不仅可扩展，还可以减少人工劳动时间。这篇研究表明了大语言模型 LIKELLMA 2.0 7B 在电商平台上自动化和优化各种方面的潜在业务影响，包括改善搜索功能和提高销售。
</details></li>
</ul>
<hr>
<h2 id="What-Makes-it-Ok-to-Set-a-Fire-Iterative-Self-distillation-of-Contexts-and-Rationales-for-Disambiguating-Defeasible-Social-and-Moral-Situations"><a href="#What-Makes-it-Ok-to-Set-a-Fire-Iterative-Self-distillation-of-Contexts-and-Rationales-for-Disambiguating-Defeasible-Social-and-Moral-Situations" class="headerlink" title="What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations"></a>What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15431">http://arxiv.org/abs/2310.15431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kavel Rao, Liwei Jiang, Valentina Pyatkin, Yuling Gu, Niket Tandon, Nouha Dziri, Faeze Brahman, Yejin Choi</li>
<li>for: 这个论文的目的是提出一种叫做抵让的道德理由，用于在真实的生活场景中更好地表达人类的道德判断。</li>
<li>methods: 这篇论文使用了一种迭代自适应学习的方法，从GPT-3的少量无结构的种子知识开始，然后通过自己的模型进行自适应学习，并用人类判断和NLI进行筛选和自我学习，以获得更高质量的任务数据。</li>
<li>results: 这篇论文通过这种方法获得了一个高质量的数据集，名为δ-规则箴言（delta-Rules-of-Thumb），包含1.2万个 Entry of contextualizations和理由，用于115 thousand个可 defeasible moral actions。这个数据集的有效性和多样性得到了人类 annotators 的评价，85.9%-99.8% 的时间内。使用这个数据集， authors 还获得了一个高质量的学生模型，比所有 intermediate 学生模型都高得多。<details>
<summary>Abstract</summary>
Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios.   We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. To elicit high-quality task data, we take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality). This process yields a student model that produces defeasible contexts with improved validity, diversity, and defeasibility. From this model we distill a high-quality dataset, \delta-Rules-of-Thumb, of 1.2M entries of contextualizations and rationales for 115K defeasible moral actions rated highly by human annotators 85.9% to 99.8% of the time. Using \delta-RoT we obtain a final student model that wins over all intermediate student models by a notable margin.
</details>
<details>
<summary>摘要</summary>
道德或伦理判断强烈取决于特定情境下发生。理解不同程度的抵觐性上下文化（即更强化或减弱行动的道德可接受度）是 kritical  то accurately represent 人类的场景下的细腻和复杂的道德判断。我们引入抵觐的道德理解任务：提供场景和理由，使得行动更加道德可接受或更加不道德。为了获得高质量任务数据，我们采用迭代自适应法，从小量无结构的 GPT-3 种子知识开始，然后每次 alternate  между (1) 自适应法 ; (2) 人类判断训练的批评模型和 NLI （以提高有效性和多样性）; (3) 自我学习（以增强愿望的数据质量）。这个过程产生了一个学生模型，该模型生成的抵觐上下文和理由具有提高的有效性、多样性和抵觐性。从这个模型中，我们提取了一个高质量数据集， delta-Rules-of-Thumb，包含 1.2 万个上下文和理由，用于 115 万个可抵觐的道德行动，被人类标注者评分为 85.9% 到 99.8% 的时间。使用 delta-RoT，我们获得了一个最终的学生模型，该模型在所有 intermediate 学生模型之上胜出了显著的差距。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Sentiment-Leveraging-Topic-Metrics-for-Political-Stance-Classification"><a href="#Beyond-Sentiment-Leveraging-Topic-Metrics-for-Political-Stance-Classification" class="headerlink" title="Beyond Sentiment: Leveraging Topic Metrics for Political Stance Classification"></a>Beyond Sentiment: Leveraging Topic Metrics for Political Stance Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15429">http://arxiv.org/abs/2310.15429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihong Qi</li>
<li>for: 本研究旨在替代和补充 sentiment 分析，准确地反映文本中的政治立场和结构。</li>
<li>methods: 本研究使用 Bestvater 和 Monroe (2023) 提供的三个数据集，利用 BERTopic 方法抽取凝结话题，并使用这些话题作为偏好变量进行立场分类。</li>
<li>results: 实验结果显示，BERTopic 可以提高凝结分数（coherence scores）by 17.07% to 54.20%  Comparing to traditional方法如 Dirichlet Allocation (LDA) 和 Non-negative Matrix Factorization (NMF)，并且 topic metrics 在立场分类中表现更高，可以提高性能 by as much as 18.95%。<details>
<summary>Abstract</summary>
Sentiment analysis, widely critiqued for capturing merely the overall tone of a corpus, falls short in accurately reflecting the latent structures and political stances within texts. This study introduces topic metrics, dummy variables converted from extracted topics, as both an alternative and complement to sentiment metrics in stance classification. By employing three datasets identified by Bestvater and Monroe (2023), this study demonstrates BERTopic's proficiency in extracting coherent topics and the effectiveness of topic metrics in stance classification. The experiment results show that BERTopic improves coherence scores by 17.07% to 54.20% when compared to traditional approaches such as Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), prevalent in earlier political science research. Additionally, our results indicate topic metrics outperform sentiment metrics in stance classification, increasing performance by as much as 18.95%. Our findings suggest topic metrics are especially effective for context-rich texts and corpus where stance and sentiment correlations are weak. The combination of sentiment and topic metrics achieve an optimal performance in most of the scenarios and can further address the limitations of relying solely on sentiment as well as the low coherence score of topic metrics.
</details>
<details>
<summary>摘要</summary>
“对文本的情感分析，受到了许多批评，因为它只能捕捉文本的总趋势，而不能准确地反映文本中的隐藏结构和政治立场。这项研究提出了话题指标，即从提取出来的话题中生成的假变量，作为对 sentiment 指标的代替和补充。通过使用最近的 Bestvater 和 Monroe (2023) 所提出的三个数据集，本研究示出 BERTopic 在提取有 coherence 的话题方面的能力，以及话题指标在立场分类中的效果。实验结果表明，BERTopic 在比较于传统方法（如 Dirichlet Allocation 和 Non-negative Matrix Factorization）时，提高 coherence 分数 by 17.07% 到 54.20%。此外，我们的结果还表明，话题指标在立场分类中表现更好，提高性能 by 18.95%。我们的发现表明，话题指标在Context-rich 文本和 corpus 中 especial 有用，并且 combining sentiment 和话题指标可以达到最佳性能，并且可以解决受 sentiment 指标和话题指标低 coherence 分数的限制。”
</details></li>
</ul>
<hr>
<h2 id="The-Mason-Alberta-Phonetic-Segmenter-A-forced-alignment-system-based-on-deep-neural-networks-and-interpolation"><a href="#The-Mason-Alberta-Phonetic-Segmenter-A-forced-alignment-system-based-on-deep-neural-networks-and-interpolation" class="headerlink" title="The Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation"></a>The Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15425">http://arxiv.org/abs/2310.15425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masonphonlab/maps_paper_code">https://github.com/masonphonlab/maps_paper_code</a></li>
<li>paper_authors: Matthew C. Kelley, Scott James Perry, Benjamin V. Tucker</li>
<li>for: 这个论文主要是为了提出一种新的神经网络基于的强制对齐系统（MAPS），以测试两种可能的改进方法。</li>
<li>methods: 这个系统使用神经网络作为强制对齐的模型，并采用了两种改进方法：一是将声学模型视为标记任务而不是分类任务，二是使用插值技术来允许边界更精确。</li>
<li>results: 与比较体系 Montreal Forced Aligner 相比，使用插值技术的系统在测试集上增加了27.92%的边界 Within 10 ms。然而，使用标记任务的方法并不总是有所改进。这个研究还探讨了声学模型的训练任务和输出目标的问题，并提出了重新思考如何分 segment  speech 本身的可能性。<details>
<summary>Abstract</summary>
Forced alignment systems automatically determine boundaries between segments in speech data, given an orthographic transcription. These tools are commonplace in phonetics to facilitate the use of speech data that would be infeasible to manually transcribe and segment. In the present paper, we describe a new neural network-based forced alignment system, the Mason-Alberta Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two possible improvements we pursue for forced alignment systems. The first is treating the acoustic model in a forced aligner as a tagging task, rather than a classification task, motivated by the common understanding that segments in speech are not truly discrete and commonly overlap. The second is an interpolation technique to allow boundaries more precise than the common 10 ms limit in modern forced alignment systems. We compare configurations of our system to a state-of-the-art system, the Montreal Forced Aligner. The tagging approach did not generally yield improved results over the Montreal Forced Aligner. However, a system with the interpolation technique had a 27.92% increase relative to the Montreal Forced Aligner in the amount of boundaries within 10 ms of the target on the test set. We also reflect on the task and training process for acoustic modeling in forced alignment, highlighting how the output targets for these models do not match phoneticians' conception of similarity between phones and that reconciliation of this tension may require rethinking the task and output targets or how speech itself should be segmented.
</details>
<details>
<summary>摘要</summary>
受限Alignment系统会自动确定在语音数据中的分割边界， givent an orthographic transcription。这些工具在 fonetics 中很普遍，以便使用不可靠的手动转录和分割语音数据。在这篇文章中，我们描述了一个新的 нейрон网络基于的受限Alignment系统，即 Mason-Alberta 音频分 segmenter (MAPS)。MAPS 分配器 serves as a testbed for two possible improvements we pursue for forced alignment systems。第一个是在受限Alignment系统中对音频模型进行标记任务，而不是分类任务，这是因为在语音中的分割不是真正独立的，通常 overlap。第二个是一种 interpolate 技术，以Allow boundaries more precise than the common 10 ms limit in modern forced alignment systems。我们与 state-of-the-art system， Montreal Forced Aligner 进行比较。 tagging 方法不一般提高resultsover Montreal Forced Aligner。然而，一种含 interpolate 技术的系统在 test set 上有 27.92% 的提高，相对于 Montreal Forced Aligner。我们还反思了 acoustic modeling 在受限Alignment中的任务和训练过程，并 highlighted how the output targets for these models do not match phoneticians' conception of similarity between phones，这可能需要重新考虑 speech 自身的 segmentation。
</details></li>
</ul>
<hr>
<h2 id="Let-the-Pretrained-Language-Models-“Imagine”-for-Short-Texts-Topic-Modeling"><a href="#Let-the-Pretrained-Language-Models-“Imagine”-for-Short-Texts-Topic-Modeling" class="headerlink" title="Let the Pretrained Language Models “Imagine” for Short Texts Topic Modeling"></a>Let the Pretrained Language Models “Imagine” for Short Texts Topic Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15420">http://arxiv.org/abs/2310.15420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang</li>
<li>for: 寻找短文档中隐藏的 semantics，Addressing the data-sparsity issue in short-text topic modeling.</li>
<li>methods: 使用 pre-trained language models (PLMs) 将短文档扩展到更长的序列，并提供一种简单的解决方案来减少 PLMs 生成的噪音文本影响。</li>
<li>results: 在多个实际场景下，模型可以大幅提高短文档主题模型的性能，并超越现有的模型。<details>
<summary>Abstract</summary>
Topic models are one of the compelling methods for discovering latent semantics in a document collection. However, it assumes that a document has sufficient co-occurrence information to be effective. However, in short texts, co-occurrence information is minimal, which results in feature sparsity in document representation. Therefore, existing topic models (probabilistic or neural) mostly fail to mine patterns from them to generate coherent topics. In this paper, we take a new approach to short-text topic modeling to address the data-sparsity issue by extending short text into longer sequences using existing pre-trained language models (PLMs). Besides, we provide a simple solution extending a neural topic model to reduce the effect of noisy out-of-topics text generation from PLMs. We observe that our model can substantially improve the performance of short-text topic modeling. Extensive experiments on multiple real-world datasets under extreme data sparsity scenarios show that our models can generate high-quality topics outperforming state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
In this paper, we take a new approach to short-text topic modeling to address the data-sparsity issue by extending short texts into longer sequences using existing pre-trained language models (PLMs). Additionally, we provide a simple solution for reducing the effect of noisy out-of-topics text generation from PLMs by extending a neural topic model. Our experiments on multiple real-world datasets under extreme data sparsity scenarios show that our models can generate high-quality topics that outperform state-of-the-art models.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/24/cs.CL_2023_10_24/" data-id="cloq1wl3p00da7o88gerob8au" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/24/cs.LG_2023_10_24/" class="article-date">
  <time datetime="2023-10-24T10:00:00.000Z" itemprop="datePublished">2023-10-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/24/cs.LG_2023_10_24/">cs.LG - 2023-10-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Task-Grouping-for-Automated-Multi-Task-Machine-Learning-via-Task-Affinity-Prediction"><a href="#Task-Grouping-for-Automated-Multi-Task-Machine-Learning-via-Task-Affinity-Prediction" class="headerlink" title="Task Grouping for Automated Multi-Task Machine Learning via Task Affinity Prediction"></a>Task Grouping for Automated Multi-Task Machine Learning via Task Affinity Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16241">http://arxiv.org/abs/2310.16241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afiya Ayman, Ayan Mukhopadhyay, Aron Laszka</li>
<li>for: 本研究旨在找到优化多任务学习（MTL）模型性能的任务组合方法。</li>
<li>methods: 研究者使用了四个通用的 benchmark 数据集，对于神经网络基于的 MTL 模型进行研究，并identified inherent task features和单任务学习（STL）特征，以预测任务组合是否可以通过 MTL 进行学习。</li>
<li>results: 研究者提出了一种随机搜索算法，使用预测器来最小化 MTL 训练的数量，并在四个 benchmark 数据集上 demonstarted 该方法可以找到更好的任务组合，比较 existed 的基eline 方法更高。<details>
<summary>Abstract</summary>
When a number of similar tasks have to be learned simultaneously, multi-task learning (MTL) models can attain significantly higher accuracy than single-task learning (STL) models. However, the advantage of MTL depends on various factors, such as the similarity of the tasks, the sizes of the datasets, and so on; in fact, some tasks might not benefit from MTL and may even incur a loss of accuracy compared to STL. Hence, the question arises: which tasks should be learned together? Domain experts can attempt to group tasks together following intuition, experience, and best practices, but manual grouping can be labor-intensive and far from optimal. In this paper, we propose a novel automated approach for task grouping. First, we study the affinity of tasks for MTL using four benchmark datasets that have been used extensively in the MTL literature, focusing on neural network-based MTL models. We identify inherent task features and STL characteristics that can help us to predict whether a group of tasks should be learned together using MTL or if they should be learned independently using STL. Building on this predictor, we introduce a randomized search algorithm, which employs the predictor to minimize the number of MTL trainings performed during the search for task groups. We demonstrate on the four benchmark datasets that our predictor-driven search approach can find better task groupings than existing baseline approaches.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a novel automated approach for task grouping. First, we analyze the affinity of tasks for MTL using four benchmark datasets that have been widely used in the MTL literature, focusing on neural network-based MTL models. We identify inherent task features and STL characteristics that can help predict whether a group of tasks should be learned together using MTL or if they should be learned independently using STL. Building on this predictor, we introduce a randomized search algorithm that employs the predictor to minimize the number of MTL trainings performed during the search for task groups. We demonstrate on the four benchmark datasets that our predictor-driven search approach can find better task groupings than existing baseline approaches.
</details></li>
</ul>
<hr>
<h2 id="Attention-Based-Ensemble-Pooling-for-Time-Series-Forecasting"><a href="#Attention-Based-Ensemble-Pooling-for-Time-Series-Forecasting" class="headerlink" title="Attention-Based Ensemble Pooling for Time Series Forecasting"></a>Attention-Based Ensemble Pooling for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16231">http://arxiv.org/abs/2310.16231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/awikner/denpool">https://github.com/awikner/denpool</a></li>
<li>paper_authors: Dhruvit Patel, Alexander Wikner</li>
<li>for: 降低时间序列预测模型偏见</li>
<li>methods: 使用 ensemble 预测模型并将其输出汇总为ensemble预测</li>
<li>results: 在非站ARY Lorenz ‘63方程的多步预测中表现出色，但在COVID-19每周病例死亡的一步预测中不一定比现有的ensemble pooling表现更好。Here’s the breakdown of each point:</li>
<li>for: 降低时间序列预测模型偏见 (What the paper is written for: reducing bias in time-series forecasting models)</li>
<li>methods: 使用 ensemble 预测模型并将其输出汇总为ensemble预测 (What methods the paper uses: using an ensemble of predictive models and pooling their output)</li>
<li>results: 在非站ARY Lorenz ‘63方程的多步预测中表现出色，但在COVID-19每周病例死亡的一步预测中不一定比现有的ensemble pooling表现更好。 (What results the paper gets: excellent performance in multi-step forecasting of the non-stationary Lorenz ‘63 equation, but not consistently better than existing ensemble pooling in one-step forecasting of COVID-19 weekly incident deaths)<details>
<summary>Abstract</summary>
A common technique to reduce model bias in time-series forecasting is to use an ensemble of predictive models and pool their output into an ensemble forecast. In cases where each predictive model has different biases, however, it is not always clear exactly how each model forecast should be weighed during this pooling. We propose a method for pooling that performs a weighted average over candidate model forecasts, where the weights are learned by an attention-based ensemble pooling model. We test this method on two time-series forecasting problems: multi-step forecasting of the dynamics of the non-stationary Lorenz `63 equation, and one-step forecasting of the weekly incident deaths due to COVID-19. We find that while our model achieves excellent valid times when forecasting the non-stationary Lorenz `63 equation, it does not consistently perform better than the existing ensemble pooling when forecasting COVID-19 weekly incident deaths.
</details>
<details>
<summary>摘要</summary>
一种常见的减少模型偏见技术在时间序列预测中是使用一个ensemble的预测模型，并将其输出Pool到一个ensemble预测中。在每个预测模型具有不同偏见的情况下，不一定是如何对每个模型预测应该进行权重的。我们提议一种使用注意力基于的ensemblePooling模型来学习 weights。我们在两个时间序列预测问题上测试了这种方法：非站点性 Lorenz '63 方程的多步预测和 COVID-19 每周病例死亡的一步预测。我们发现，虽然我们的模型在预测非站点性 Lorenz '63 方程时表现出色，但在预测 COVID-19 每周病例死亡时不一定能够超越现有的ensemblePooling。
</details></li>
</ul>
<hr>
<h2 id="Poison-is-Not-Traceless-Fully-Agnostic-Detection-of-Poisoning-Attacks"><a href="#Poison-is-Not-Traceless-Fully-Agnostic-Detection-of-Poisoning-Attacks" class="headerlink" title="Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks"></a>Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16224">http://arxiv.org/abs/2310.16224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinglong Chang, Katharina Dost, Gillian Dobbie, Jörg Wicker</li>
<li>for: 这个论文是为了检测机器学习模型中的攻击。</li>
<li>methods: 这个检测方法使用了一个全新的无预设框架，名为DIVA，它可以对于潜在的毒素资料集进行检测。DIVA 基于Classifier的精度差异 між受毒和清洁资料集来检测攻击。</li>
<li>results: 在这个论文中，DIVA 方法在面对标签转换攻击时得到了良好的结果，能够妥善地检测并识别攻击。<details>
<summary>Abstract</summary>
The performance of machine learning models depends on the quality of the underlying data. Malicious actors can attack the model by poisoning the training data. Current detectors are tied to either specific data types, models, or attacks, and therefore have limited applicability in real-world scenarios. This paper presents a novel fully-agnostic framework, DIVA (Detecting InVisible Attacks), that detects attacks solely relying on analyzing the potentially poisoned data set. DIVA is based on the idea that poisoning attacks can be detected by comparing the classifier's accuracy on poisoned and clean data and pre-trains a meta-learner using Complexity Measures to estimate the otherwise unknown accuracy on a hypothetical clean dataset. The framework applies to generic poisoning attacks. For evaluation purposes, in this paper, we test DIVA on label-flipping attacks.
</details>
<details>
<summary>摘要</summary>
Machine learning 模型的性能取决于训练数据的质量。恶意攻击者可以让训练数据中掺入假数据，从而影响模型的性能。现有的检测器受限于特定数据类型、模型或攻击方式，因此在实际场景中有有限的应用。这篇论文提出了一种全新的无关数据类型和模型的检测框架，名为DIVA（检测隐藏攻击）。DIVA基于的想法是，攻击者可以通过比较污染和干净数据集中分类器的准确率来检测攻击。DIVA使用复杂度度量来估算干净数据集中模型的准确率，并在这个假设中预训练一个元学习器。该框架适用于普通的污染攻击。为评估目的，在这篇论文中，我们对标签替换攻击进行测试。
</details></li>
</ul>
<hr>
<h2 id="Performance-Tuning-for-GPU-Embedded-Systems-Machine-Learning-based-and-Analytical-Model-driven-Tuning-Methodologies"><a href="#Performance-Tuning-for-GPU-Embedded-Systems-Machine-Learning-based-and-Analytical-Model-driven-Tuning-Methodologies" class="headerlink" title="Performance Tuning for GPU-Embedded Systems: Machine-Learning-based and Analytical Model-driven Tuning Methodologies"></a>Performance Tuning for GPU-Embedded Systems: Machine-Learning-based and Analytical Model-driven Tuning Methodologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16214">http://arxiv.org/abs/2310.16214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Perez Dieguez, Margarita Amor Lopez</li>
<li>for: 本文旨在提高GPU嵌入式系统的性能，以满足实时或时间consuming应用的需求。</li>
<li>methods: 本文提出了两种优化方法，一是分析模型驱动的优化方法，另一是基于机器学习（ML）的优化方法。</li>
<li>results: 对各种并行前缀操作（FFT、扫描 primitives、三角系统解）的表现进行了性能分析，并提供了开发者和研究人员可以参考的实践指导。<details>
<summary>Abstract</summary>
GPU-embedded systems have gained popularity across various domains due to their efficient power consumption. However, in order to meet the demands of real-time or time-consuming applications running on these systems, it is crucial for them to be tuned to exhibit high performance. This paper addresses the issue by developing and comparing two tuning methodologies on GPU-embedded systems, and also provides performance insights for developers and researchers seeking to optimize applications running on these architectures. We focus on parallel prefix operations, such as FFT, scan primitives, and tridiagonal system solvers, which are performance-critical components in many applications. The study introduces an analytical model-driven tuning methodology and a Machine Learning (ML)-based tuning methodology. We evaluate the performance of the two tuning methodologies for different parallel prefix implementations of the BPLG library in an NVIDIA Jetson system, and compare their performance to the ones achieved through an exhaustive search. The findings shed light on the best strategies for handling the open challenge of performance portability for major computational patterns among server and embedded devices, providing practical guidance for offline and online tuning. We also address the existing gap in performance studies for parallel computational patterns in GPU-embedded systems by comparing the BPLG performance against other state-of-the-art libraries, including CUSPARSE, CUB, and CUFFT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ELM-Ridge-Regression-Boosting"><a href="#ELM-Ridge-Regression-Boosting" class="headerlink" title="ELM Ridge Regression Boosting"></a>ELM Ridge Regression Boosting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16209">http://arxiv.org/abs/2310.16209</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Andrecut</li>
<li>for: 提高Extreme Learning Machine（ELM）的分类性能和Robustness。</li>
<li>methods: 使用Boosting方法对Ridge Regression（RR）方法进行改进。</li>
<li>results: 提高了ELM的分类性能和Robustness。In simplified Chinese, the three key points would be:</li>
<li>for: 提高ELM的分类性能和Robustness。</li>
<li>methods: 使用Boosting方法对RR方法进行改进。</li>
<li>results: 提高了ELM的分类性能和Robustness。<details>
<summary>Abstract</summary>
We discuss a boosting approach for the Ridge Regression (RR) method, with applications to the Extreme Learning Machine (ELM), and we show that the proposed method significantly improves the classification performance and robustness of ELMs.
</details>
<details>
<summary>摘要</summary>
我们讨论了一种扩充方法，用于ridge回归(RR)方法，并应用于极限学习机(ELM)中。我们显示，提案的方法可以显著提高ELM的分类性能和可靠性。Here's the word-for-word translation:我们讨论了一种扩充方法，用于ridge回归(RR)方法，并应用于极限学习机(ELM)中。我们显示，提案的方法可以显著提高ELM的分类性能和可靠性。Note that the word "ridge" is translated as "ridge回归" (ridge regression) in Simplified Chinese, and "extreme learning machine" is translated as "极限学习机" (extreme learning machine).
</details></li>
</ul>
<hr>
<h2 id="Efficient-deep-data-assimilation-with-sparse-observations-and-time-varying-sensors"><a href="#Efficient-deep-data-assimilation-with-sparse-observations-and-time-varying-sensors" class="headerlink" title="Efficient deep data assimilation with sparse observations and time-varying sensors"></a>Efficient deep data assimilation with sparse observations and time-varying sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16187">http://arxiv.org/abs/2310.16187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dl-wg/vivid">https://github.com/dl-wg/vivid</a></li>
<li>paper_authors: Sibo Cheng, Che Liu, Yike Guo, Rossella Arcucci<br>for:* 这个论文是为了提出一种新的变量数据整合（DA）方法，用于处理高维动力系统中的不Structured观测数据。methods:* 这种新的DA方法称为Voronoi-tessellation Inverse operator for VariatIonal Data assimilation（VIVID），它 integration了深度学习（DL） inverse operator到数据整合目标函数中。* VIVID使用 Voronoi-tessellation和卷积神经网络来处理稀疏、不结构的观测数据，并且可以轻松地与Proper Orthogonal Decomposition（POD）结合，实现一个综合的减少维度数据整合方案。results:* 在一个流体动力系统中的数值实验中，VIVID可以明显超过现有的DA和DL算法。* VIVID的稳定性也被证明，通过对不同水平的先验错误、不同数量的探测器和数据整合错误 covariance的使用进行评估。<details>
<summary>Abstract</summary>
Variational Data Assimilation (DA) has been broadly used in engineering problems for field reconstruction and prediction by performing a weighted combination of multiple sources of noisy data. In recent years, the integration of deep learning (DL) techniques in DA has shown promise in improving the efficiency and accuracy in high-dimensional dynamical systems. Nevertheless, existing deep DA approaches face difficulties in dealing with unstructured observation data, especially when the placement and number of sensors are dynamic over time. We introduce a novel variational DA scheme, named Voronoi-tessellation Inverse operator for VariatIonal Data assimilation (VIVID), that incorporates a DL inverse operator into the assimilation objective function. By leveraging the capabilities of the Voronoi-tessellation and convolutional neural networks, VIVID is adept at handling sparse, unstructured, and time-varying sensor data. Furthermore, the incorporation of the DL inverse operator establishes a direct link between observation and state space, leading to a reduction in the number of minimization steps required for DA. Additionally, VIVID can be seamlessly integrated with Proper Orthogonal Decomposition (POD) to develop an end-to-end reduced-order DA scheme, which can further expedite field reconstruction. Numerical experiments in a fluid dynamics system demonstrate that VIVID can significantly outperform existing DA and DL algorithms. The robustness of VIVID is also accessed through the application of various levels of prior error, the utilization of varying numbers of sensors, and the misspecification of error covariance in DA.
</details>
<details>
<summary>摘要</summary>
“Variational Data Assimilation（DA）在工程问题中广泛应用于场景重建和预测，通过对多种噪声数据进行权重组合。在过去几年，将深度学习（DL）技术integrated into DA中的潜在优化高维动态系统的效率和准确性。然而，现有的深度DA方法在处理不结构化观测数据时存在困难，特别是当感知器的位置和数量在时间上是动态变化的。我们介绍了一种新的variational DA方案，名为Voronoi-tessellation Inverse operator for VariatIonal Data assimilation（VIVID），它通过在权重组合中引入深度反向运算来解决这些问题。通过利用Voronoi-tessellation和卷积神经网络，VIVID能够处理稀疏、不结构化和时间变化的感知数据。此外，将深度反向运算integrated into DA目标函数可以直接连接观测和状态空间，从而减少DA的最小化步骤数量。此外，VIVID可以轻松地与Proper Orthogonal Decomposition（POD）结合，开发一个端到端减少DA方案，可以进一步加速场景重建。数值实验在流体动力系统中表明，VIVID可以明显超越现有的DA和DL算法。此外，我们还评估了VIVID的稳定性，通过在不同水平的先验错误、不同数量的感知器和DA中错误协方差的情况下进行测试。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Image-Segmentation-using-U-Net-Architecture-for-Powder-X-ray-Diffraction-Images"><a href="#Image-Segmentation-using-U-Net-Architecture-for-Powder-X-ray-Diffraction-Images" class="headerlink" title="Image Segmentation using U-Net Architecture for Powder X-ray Diffraction Images"></a>Image Segmentation using U-Net Architecture for Powder X-ray Diffraction Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16186">http://arxiv.org/abs/2310.16186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Howard Yanxon, Eric Roberts, Hannah Parraga, James Weng, Wenqian Xu, Uta Ruett, Alexander Hexemer, Petrus Zwart, Nickolas Schwarz</li>
<li>for: 这个论文是为了探讨材料功能设备中的晶体结构的研究。</li>
<li>methods: 该论文提出了一种使用深度学习卷积神经网络方法来识别实验XRD图像中的假象。</li>
<li>results: 研究结果表明，U-Nets可以在测试数据集上保持92.4%的准确率，同时减少了False Positive的平均值34%，并减少了对假象的分析时间超过50%。<details>
<summary>Abstract</summary>
Scientific researchers frequently use the in situ synchrotron high-energy powder X-ray diffraction (XRD) technique to examine the crystallographic structures of materials in functional devices such as rechargeable battery materials. We propose a method for identifying artifacts in experimental XRD images. The proposed method uses deep learning convolutional neural network architectures, such as tunable U-Nets to identify the artifacts. In particular, the predicted artifacts are evaluated against the corresponding ground truth (manually implemented) using the overall true positive rate or recall. The result demonstrates that the U-Nets can consistently produce great recall performance at 92.4% on the test dataset, which is not included in the training, with a 34% reduction in average false positives in comparison to the conventional method. The U-Nets also reduce the time required to identify and separate artifacts by more than 50%. Furthermore, the exclusion of the artifacts shows major changes in the integrated 1D XRD pattern, enhancing further analysis of the post-processing XRD data.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:科学研究人员 часто使用坐垦同步троン高能粉末X射 diffraction（XRD）技术来研究功能设备中材料的 кристал化结构。我们提出了用深度学习 convolutional neural network 架构来Identify experimental XRD 图像中的 artifacts的方法。特别是，预测的 artifacts 将与相应的ground truth（手动实现的）进行评估，并使用 true positive rate 或 recall 来评估预测的性能。结果显示，U-Nets 可以在测试集上保持92.4%的回归性能，与传统方法相比下降34%的假阳性率。U-Nets 还可以降低识别和分离 artifacts 所需的时间超过50%。此外，排除 artifacts 会导致整个1D XRD 图像中的积分呈现出明显的变化，进一步促进了后处理 XRD 数据的分析。
</details></li>
</ul>
<hr>
<h2 id="On-the-Convergence-and-Sample-Complexity-Analysis-of-Deep-Q-Networks-with-ε-Greedy-Exploration"><a href="#On-the-Convergence-and-Sample-Complexity-Analysis-of-Deep-Q-Networks-with-ε-Greedy-Exploration" class="headerlink" title="On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $ε$-Greedy Exploration"></a>On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $ε$-Greedy Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16173">http://arxiv.org/abs/2310.16173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Zhang, Hongkang Li, Meng Wang, Miao Liu, Pin-Yu Chen, Songtao Lu, Sijia Liu, Keerthiram Murugesan, Subhajit Chaudhury</li>
<li>for: 提供了深度强化学习中DQN($\epsilon$-资源探索)的理论理解。</li>
<li>methods: 使用目标网络和经验回放来获得不偏的MSBE估计，并提供了第一个实际 Setting中DQN的 тео리тиче converge和样本复杂度分析。</li>
<li>results: 证明了一种循环过程，其中 decaying $\epsilon$  converge to the optimal Q-value function geometrically，而且 higher level of $\epsilon$  values 增加了整体的整合区域，但是降低了循环速度，相反，lower level of $\epsilon$ values 减少了整合区域，但是提高了循环速度。实验证明了我们的理论成果。<details>
<summary>Abstract</summary>
This paper provides a theoretical understanding of Deep Q-Network (DQN) with the $\varepsilon$-greedy exploration in deep reinforcement learning. Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored. First, the exploration strategy is either impractical or ignored in the existing analysis. Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training the Q-network. However, the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly overparameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the practical setting of DQNs with $\epsilon$-greedy policy. We prove an iterative procedure with decaying $\epsilon$ converges to the optimal Q-value function geometrically. Moreover, a higher level of $\epsilon$ values enlarges the region of convergence but slows down the convergence, while the opposite holds for a lower level of $\epsilon$ values. Experiments justify our established theoretical insights on DQNs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fine-tuning-Pre-trained-Models-for-Robustness-Under-Noisy-Labels"><a href="#Fine-tuning-Pre-trained-Models-for-Robustness-Under-Noisy-Labels" class="headerlink" title="Fine tuning Pre trained Models for Robustness Under Noisy Labels"></a>Fine tuning Pre trained Models for Robustness Under Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17668">http://arxiv.org/abs/2310.17668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumyeong Ahn, Sihyeon Kim, Jongwoo Ko, Se-Young Yun</li>
<li>for: 这篇论文的目的是为了解决养分拥有误标的训练集时，机器学习模型的性能如何受到影响。</li>
<li>methods: 这篇论文使用了对于误标数据的研究，以及对于这些误标数据的处理和范例。</li>
<li>results: 这篇论文的结果显示，使用TURN算法可以实现高效的误标数据处理，并且可以增强预训模型在不同 benchmark 上的表现。<details>
<summary>Abstract</summary>
The presence of noisy labels in a training dataset can significantly impact the performance of machine learning models. To tackle this issue, researchers have explored methods for Learning with Noisy Labels to identify clean samples and reduce the influence of noisy labels. However, constraining the influence of a certain portion of the training dataset can result in a reduction in overall generalization performance. To alleviate this, recent studies have considered the careful utilization of noisy labels by leveraging huge computational resources. Therefore, the increasing training cost necessitates a reevaluation of efficiency. In other areas of research, there has been a focus on developing fine-tuning techniques for large pre-trained models that aim to achieve both high generalization performance and efficiency. However, these methods have mainly concentrated on clean datasets, and there has been limited exploration of the noisy label scenario. In this research, our aim is to find an appropriate way to fine-tune pre-trained models for noisy labeled datasets. To achieve this goal, we investigate the characteristics of pre-trained models when they encounter noisy datasets. Through empirical analysis, we introduce a novel algorithm called TURN, which robustly and efficiently transfers the prior knowledge of pre-trained models. The algorithm consists of two main steps: (1) independently tuning the linear classifier to protect the feature extractor from being distorted by noisy labels, and (2) reducing the noisy label ratio and fine-tuning the entire model based on the noise-reduced dataset to adapt it to the target dataset. The proposed algorithm has been extensively tested and demonstrates efficient yet improved denoising performance on various benchmarks compared to previous methods.
</details>
<details>
<summary>摘要</summary>
训练数据中噪音标签的存在可能对机器学习模型的性能产生深见影响。为了解决这问题，研究人员已经探索了学习噪音标签的方法，以避免噪音标签的影响。然而，限制噪音标签的影响可能会导致总体的适应性下降。为了解决这问题，最近的研究已经考虑了大量计算资源的利用，以减少噪音标签的影响。因此，随着训练成本的增加，我们需要重新评估效率。在其他研究领域，研究人员已经对大型预训练模型进行了细化调整，以达到高适应性和高效率的目标。然而，这些方法主要集中在干净数据上进行调整，而噪音标签场景得到的研究很少。在这项研究中，我们的目标是找到适合的方法来调整预训练模型，以适应噪音标签数据。为了实现这个目标，我们进行了employm empirical分析，并提出了一种名为TURN的新算法。TURN算法包括两个主要步骤：（1）独立地调整线性分类器，以防止噪音标签对特征提取器的影响，和（2）通过减少噪音标签比例，并在减少后进行 fine-tuning，以适应目标数据。我们对TURN算法进行了广泛的测试，并证明了它可以有效地、高效率地除噪。
</details></li>
</ul>
<hr>
<h2 id="Brainchop-Next-Generation-Web-Based-Neuroimaging-Application"><a href="#Brainchop-Next-Generation-Web-Based-Neuroimaging-Application" class="headerlink" title="Brainchop: Next Generation Web-Based Neuroimaging Application"></a>Brainchop: Next Generation Web-Based Neuroimaging Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16162">http://arxiv.org/abs/2310.16162</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuroneural/brainchop">https://github.com/neuroneural/brainchop</a></li>
<li>paper_authors: Mohamed Masoud, Pratyush Reddy, Farfalla Hu, Sergey Plis</li>
<li>for: This paper is written for researchers and practitioners in the field of neuroimaging, particularly those interested in whole brain preprocessing and segmentation using deep learning models.</li>
<li>methods: The paper uses a pre-trained full-brain deep learning model to perform volumetric analysis of structural MRI data directly within the browser, without requiring technical expertise or intricate setup procedures. The MeshNet architecture is used to enable client-side processing for volumetric data.</li>
<li>results: The paper evaluates the performance of the Brainchop tool across various software and hardware configurations, demonstrating the practicality of client-side processing for volumetric data even within the resource-constrained environment of web browsers. The results show that Brainchop offers multiple benefits, including scalability, low latency, user-friendly operation, cross-platform compatibility, and enhanced accessibility.<details>
<summary>Abstract</summary>
Performing volumetric image processing directly within the browser, particularly with medical data, presents unprecedented challenges compared to conventional backend tools. These challenges arise from limitations inherent in browser environments, such as constrained computational resources and the availability of frontend machine learning libraries. Consequently, there is a shortage of neuroimaging frontend tools capable of providing comprehensive end-to-end solutions for whole brain preprocessing and segmentation while preserving end-user data privacy and residency. In light of this context, we introduce Brainchop (http://www.brainchop.org) as a groundbreaking in-browser neuroimaging tool that enables volumetric analysis of structural MRI using pre-trained full-brain deep learning models, all without requiring technical expertise or intricate setup procedures. Beyond its commitment to data privacy, this frontend tool offers multiple features, including scalability, low latency, user-friendly operation, cross-platform compatibility, and enhanced accessibility. This paper outlines the processing pipeline of Brainchop and evaluates the performance of models across various software and hardware configurations. The results demonstrate the practicality of client-side processing for volumetric data, owing to the robust MeshNet architecture, even within the resource-constrained environment of web browsers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将三维图像处理直接在浏览器中进行，特别是在医疗数据上，存在前所未有的挑战。这些挑战来自浏览器环境的限制，如计算资源的紧张和前端机器学习库的可用性。因此，存在一个缺乏神经成像前端工具，可以提供全 bran 的整体端到端解决方案，保持用户数据隐私和存储。在这个 контексте，我们介绍 Brainchop（http://www.brainchop.org），一种创新的在浏览器中的神经成像工具，可以使用预训练的全 bran 深度学习模型进行结构 MRI 的三维分析，无需技术专业知识或复杂的设置过程。 Brainchop 除了保持用户数据隐私外，还具有规模、延迟低、易用操作、跨平台兼容和更好的可访问性等多个特点。本文介绍 Brainchop 的处理管道和模型性能的评估，结果表明在web浏览器的资源环境中，可以通过强大的 MeshNet 架构实现Client-side 处理三维数据的实用性。
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Curse-of-Dimensionality-in-Deep-Neural-Networks-by-Learning-Invariant-Representations"><a href="#Breaking-the-Curse-of-Dimensionality-in-Deep-Neural-Networks-by-Learning-Invariant-Representations" class="headerlink" title="Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations"></a>Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16154">http://arxiv.org/abs/2310.16154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Petrini</li>
<li>for: 这个论文旨在探讨深度学习模型的理论基础，具体来说是研究深度学习模型如何从数据中学习有用的特征，以及这些模型如何在高维数据中学习函数。</li>
<li>methods: 该论文采用了实验室方法，结合物理启发式的小型模型，以研究和解释深度学习系统中复杂的行为。</li>
<li>results: 该论文发现了深度学习模型的效果听起来是受数据结构的支持，而不是受数据量的支持。此外，不同的建筑均可以利用不同的数据结构，从而提高模型的性能。<details>
<summary>Abstract</summary>
Artificial intelligence, particularly the subfield of machine learning, has seen a paradigm shift towards data-driven models that learn from and adapt to data. This has resulted in unprecedented advancements in various domains such as natural language processing and computer vision, largely attributed to deep learning, a special class of machine learning models. Deep learning arguably surpasses traditional approaches by learning the relevant features from raw data through a series of computational layers.   This thesis explores the theoretical foundations of deep learning by studying the relationship between the architecture of these models and the inherent structures found within the data they process. In particular, we ask What drives the efficacy of deep learning algorithms and allows them to beat the so-called curse of dimensionality-i.e. the difficulty of generally learning functions in high dimensions due to the exponentially increasing need for data points with increased dimensionality? Is it their ability to learn relevant representations of the data by exploiting their structure? How do different architectures exploit different data structures? In order to address these questions, we push forward the idea that the structure of the data can be effectively characterized by its invariances-i.e. aspects that are irrelevant for the task at hand.   Our methodology takes an empirical approach to deep learning, combining experimental studies with physics-inspired toy models. These simplified models allow us to investigate and interpret the complex behaviors we observe in deep learning systems, offering insights into their inner workings, with the far-reaching goal of bridging the gap between theory and practice.
</details>
<details>
<summary>摘要</summary>
人工智能，尤其是机器学习的一个子领域，已经经历了一种 Paradigm shift ，把注重于数据驱动的模型作为核心。这种 shift 导致了各个领域的不同领域，如自然语言处理和计算机视觉等领域，具有压倒性的进步，主要归功于深度学习，一种特殊的机器学习模型。深度学习可能超越传统方法，因为它可以从原始数据中学习相关的特征。这个论文探讨了深度学习的理论基础，研究了深度学习模型和数据之间的关系。具体来说，我们问：深度学习算法的效果是什么？它能够在高维度上学习函数吗？深度学习模型如何利用数据的结构来学习有用的表示？不同的架构如何利用不同的数据结构？为了回答这些问题，我们推进了一种理论，即数据的结构可以被有效地Characterized by its invariances，即不重要的特征。我们的方法采取了实验室的方法，结合了物理启发的简单模型。这些简单的模型允许我们investigate和 interpret 深度学习系统中的复杂行为，提供了对其内部工作的深入理解，以期 bridge 理论和实践之间的差距。
</details></li>
</ul>
<hr>
<h2 id="FLTrojan-Privacy-Leakage-Attacks-against-Federated-Language-Models-Through-Selective-Weight-Tampering"><a href="#FLTrojan-Privacy-Leakage-Attacks-against-Federated-Language-Models-Through-Selective-Weight-Tampering" class="headerlink" title="FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering"></a>FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16152">http://arxiv.org/abs/2310.16152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Kang Gu, Najrin Sultana, Shagufta Mehnaz</li>
<li>for: 本研究旨在探讨 federated learning（FL）中隐私泄露的问题，具体来说是在语言模型中实现隐私保护。</li>
<li>methods: 本研究使用了两个新发现，即模型Snapshot在中间轮次可能导致更大的隐私泄露，以及模型中选择性的权重可以增加隐私泄露的风险。</li>
<li>results: 研究发现，使用最佳方法可以提高会员推理精度29%，并达到70%的隐私数据重建率，质量明显超过现有攻击方法。<details>
<summary>Abstract</summary>
Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even without any cooperation from the server. Our best-performing method improves the membership inference recall by 29% and achieves up to 70% private data reconstruction, evidently outperforming existing attacks with stronger assumptions of adversary capabilities.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 已成为许多技术应用程序的关键组件，包括语音模型 - where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward, and existing attacks only aim to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even without any cooperation from the server. Our best-performing method improves the membership inference recall by 29% and achieves up to 70% private data reconstruction, evidently outperforming existing attacks with stronger assumptions of adversary capabilities.
</details></li>
</ul>
<hr>
<h2 id="A-Risk-Averse-Framework-for-Non-Stationary-Stochastic-Multi-Armed-Bandits"><a href="#A-Risk-Averse-Framework-for-Non-Stationary-Stochastic-Multi-Armed-Bandits" class="headerlink" title="A Risk-Averse Framework for Non-Stationary Stochastic Multi-Armed Bandits"></a>A Risk-Averse Framework for Non-Stationary Stochastic Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19821">http://arxiv.org/abs/2310.19821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reda Alami, Mohammed Mahfoud, Mastane Achab</li>
<li>for: 该文章目的是提出一种适应非站台环境的可靠多臂抓拍算法框架，以优化在健康保险或金融等高波动环境中的学习问题。</li>
<li>methods: 该框架基于多种常见的风险度量，将多个家族的多臂抓拍算法映射到风险敏感的设定中。此外，该框架还使用重启 Bayesian 线性时间变化检测算法（R-BOCPD）和一个可调的强制探索策略来检测每个臂的地方（本地）变化。</li>
<li>results: 虽然该框架在理论上的 finite-time 保证和 asymptotic  regret bound 是 $\tilde O(\sqrt{K_T T})$，但在实际应用中，它在 synthetic 和实际环境中表现出色，并能够有效地处理风险敏感和非站台环境。<details>
<summary>Abstract</summary>
In a typical stochastic multi-armed bandit problem, the objective is often to maximize the expected sum of rewards over some time horizon $T$. While the choice of a strategy that accomplishes that is optimal with no additional information, it is no longer the case when provided additional environment-specific knowledge. In particular, in areas of high volatility like healthcare or finance, a naive reward maximization approach often does not accurately capture the complexity of the learning problem and results in unreliable solutions. To tackle problems of this nature, we propose a framework of adaptive risk-aware strategies that operate in non-stationary environments. Our framework incorporates various risk measures prevalent in the literature to map multiple families of multi-armed bandit algorithms into a risk-sensitive setting. In addition, we equip the resulting algorithms with the Restarted Bayesian Online Change-Point Detection (R-BOCPD) algorithm and impose a (tunable) forced exploration strategy to detect local (per-arm) switches. We provide finite-time theoretical guarantees and an asymptotic regret bound of order $\tilde O(\sqrt{K_T T})$ up to time horizon $T$ with $K_T$ the total number of change-points. In practice, our framework compares favorably to the state-of-the-art in both synthetic and real-world environments and manages to perform efficiently with respect to both risk-sensitivity and non-stationarity.
</details>
<details>
<summary>摘要</summary>
在一般的随机多臂抓拍问题中，目标通常是在某个时间戳 $T$ 上 maximize 的预期奖励总和。而在具有环境特定知识的情况下，一个简单的奖励最大化方法通常不能准确捕捉学习问题的复杂性，导致不可靠的解决方案。为解决这类问题，我们提出了一个适应风险感知策略框架。我们的框架将多种通用风险度量 integrate 到非站台环境中，将多个家族的多臂抓拍算法映射到风险敏感设定下。此外，我们还具有Restarted Bayesian Online Change-Point Detection（R-BOCPD）算法和（可调）强制探索策略，以探测每个臂上的地方（本地）交替。我们提供了finite-time理论保证和 asymptotic  regret bound of order $\tilde O(\sqrt{K_T T})$ up to time horizon $T$ with $K_T$ 是总共变化点数。在实践中，我们的框架与当前状态的最佳实践相比，在 sintetic 和实际环境中表现出色，并能够有效地处理风险敏感和非站台环境。
</details></li>
</ul>
<hr>
<h2 id="Online-Thermal-Field-Prediction-for-Metal-Additive-Manufacturing-of-Thin-Walls"><a href="#Online-Thermal-Field-Prediction-for-Metal-Additive-Manufacturing-of-Thin-Walls" class="headerlink" title="Online Thermal Field Prediction for Metal Additive Manufacturing of Thin Walls"></a>Online Thermal Field Prediction for Metal Additive Manufacturing of Thin Walls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16125">http://arxiv.org/abs/2310.16125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Tang, M. Rahmani Dehaghani, Pouyan Sajadi, Shahriar Bakrani Balani, Akshay Dhalpe, Suraj Panicker, Di Wu, Eric Coatanea, G. Gary Wang</li>
<li>For: This paper aims to study a practical issue in metal AM, specifically how to predict the thermal field of yet-to-print parts online when only a few sensors are available.* Methods: The paper proposes an online thermal field prediction method using mapping and reconstruction, which incorporates an artificial neural network and a reduced order model (ROM) to estimate the temperature profiles of points on the yet-to-print layer.* Results: The proposed method can construct the thermal field of a yet-to-print layer within 0.1 seconds on a low-cost desktop, and has acceptable generalization capability in most cases from lower layers to higher layers in the same simulation and from one simulation to a new simulation on different AM process parameters.<details>
<summary>Abstract</summary>
This paper aims to study a practical issue in metal AM, i.e., how to predict the thermal field of yet-to-print parts online when only a few sensors are available. This work proposes an online thermal field prediction method using mapping and reconstruction, which could be integrated into a metal AM process for online performance control. Based on the similarity of temperature curves (curve segments of a temperature profile of one point), the thermal field mapping applies an artificial neural network to estimate the temperature curves of points on the yet-to-print layer from measured temperatures of certain points on the previously printed layer. With measured/predicted temperature profiles of several points on the same layer, the thermal field reconstruction proposes a reduced order model (ROM) to construct the temperature profiles of all points on the same layer, which could be used to build the temperature field of the entire layer. The training of ROM is performed with an extreme learning machine (ELM) for computational efficiency. Fifteen wire arc AM experiments and nine simulations are designed for thin walls with a fixed length and unidirectional printing of each layer. The test results indicate that the proposed prediction method could construct the thermal field of a yet-to-print layer within 0.1 seconds on a low-cost desktop. Meanwhile, the method has acceptable generalization capability in most cases from lower layers to higher layers in the same simulation and from one simulation to a new simulation on different AM process parameters. More importantly, after fine-tuning the proposed method with limited experimental data, the relative errors of all predicted temperature profiles on a new experiment are sufficiently small, demonstrating the applicability and generalization of the proposed thermal field prediction method in online applications for metal AM.
</details>
<details>
<summary>摘要</summary>
The method uses artificial neural networks to estimate the temperature curves of points on the yet-to-print layer based on measured temperatures of certain points on the previously printed layer. With measured/predicted temperature profiles of several points on the same layer, a reduced order model (ROM) is constructed to build the temperature field of the entire layer. The ROM is trained with an extreme learning machine (ELM) for computational efficiency.Experiments and simulations were conducted on thin walls with a fixed length and unidirectional printing of each layer. The results show that the proposed prediction method can construct the thermal field of a yet-to-print layer within 0.1 seconds on a low-cost desktop, and has acceptable generalization capability in most cases from lower layers to higher layers in the same simulation and from one simulation to a new simulation on different AM process parameters. After fine-tuning the method with limited experimental data, the relative errors of all predicted temperature profiles on a new experiment were sufficiently small, demonstrating the applicability and generalization of the proposed thermal field prediction method in online applications for metal AM.
</details></li>
</ul>
<hr>
<h2 id="Anchor-Space-Optimal-Transport-Accelerating-Batch-Processing-of-Multiple-OT-Problems"><a href="#Anchor-Space-Optimal-Transport-Accelerating-Batch-Processing-of-Multiple-OT-Problems" class="headerlink" title="Anchor Space Optimal Transport: Accelerating Batch Processing of Multiple OT Problems"></a>Anchor Space Optimal Transport: Accelerating Batch Processing of Multiple OT Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16123">http://arxiv.org/abs/2310.16123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianming Huang, Xun Su, Zhongxi Fang, Hiroyuki Kasai</li>
<li>for: 多个似然分布的批处理解决方案</li>
<li>methods: 使用共享anchor点空间来捕捉分布的共同特征，并提出三种方法来学习anchor空间，每种方法都有应用背景</li>
<li>results: 实验表明，提出的方法可以大幅降低计算时间，同时保持可接受的近似性表现<details>
<summary>Abstract</summary>
The optimal transport (OT) theory provides an effective way to compare probability distributions on a defined metric space, but it suffers from cubic computational complexity. Although the Sinkhorn's algorithm greatly reduces the computational complexity of OT solutions, the solutions of multiple OT problems are still time-consuming and memory-comsuming in practice. However, many works on the computational acceleration of OT are usually based on the premise of a single OT problem, ignoring the potential common characteristics of the distributions in a mini-batch. Therefore, we propose a translated OT problem designated as the anchor space optimal transport (ASOT) problem, which is specially designed for batch processing of multiple OT problem solutions. For the proposed ASOT problem, the distributions will be mapped into a shared anchor point space, which learns the potential common characteristics and thus help accelerate OT batch processing. Based on the proposed ASOT, the Wasserstein distance error to the original OT problem is proven to be bounded by ground cost errors. Building upon this, we propose three methods to learn an anchor space minimizing the distance error, each of which has its application background. Numerical experiments on real-world datasets show that our proposed methods can greatly reduce computational time while maintaining reasonable approximation performance.
</details>
<details>
<summary>摘要</summary>
Optimal transport（OT）理论提供了一种有效地比较概率分布在定义的度量空间上，但它受到立方体计算复杂性的限制。虽然斯inkelhorn算法可以大幅降低OT解决方案的计算复杂性，但在实践中，多个OT问题的解决仍然占用了大量的时间和内存。然而，许多关于OT计算加速的研究通常基于单个OT问题的假设，忽略了多个分布在一个批处理中的共同特征。因此，我们提出了一个名为anchor space optimal transport（ASOT）问题的翻译问题，这是专门为批处理多个OT问题的解决而设计的。在我们的ASOT问题中，分布将被映射到共享的anchor点空间中，这里学习了可能共同的特征，从而帮助加速OT批处理。根据我们的ASOT问题，我们证明了对原OT问题的沃asserstein距离错误是由地面成本错误约束的。基于这个结论，我们提出了三种方法来学习一个anchor空间，以降低距离错误，每种方法都有其应用背景。在实际数据上进行的数值实验表明，我们的提出方法可以大幅减少计算时间，同时保持合理的近似性。
</details></li>
</ul>
<hr>
<h2 id="19-Parameters-Is-All-You-Need-Tiny-Neural-Networks-for-Particle-Physics"><a href="#19-Parameters-Is-All-You-Need-Tiny-Neural-Networks-for-Particle-Physics" class="headerlink" title="19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics"></a>19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16121">http://arxiv.org/abs/2310.16121</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abogatskiy/pelican-nano">https://github.com/abogatskiy/pelican-nano</a></li>
<li>paper_authors: Alexander Bogatskiy, Timothy Hoffman, Jan T. Offermann</li>
<li>for: 这 paper 是为了探讨快速 neural network 架构的可行性，用于低延迟任务 such as triggering。</li>
<li>methods: 这 paper 使用了一种最近的 Lorentz-和 permutation-symmetric 架构 PELICAN，并提供了具有只 19 个可训练参数的实例，可以与 generic 架构相比而言而出色的表现。</li>
<li>results: 论文表明，PELICAN 架构可以在 top quark jet 分类任务中表现更好，并且只需要很少的参数数量。<details>
<summary>Abstract</summary>
As particle accelerators increase their collision rates, and deep learning solutions prove their viability, there is a growing need for lightweight and fast neural network architectures for low-latency tasks such as triggering. We examine the potential of one recent Lorentz- and permutation-symmetric architecture, PELICAN, and present its instances with as few as 19 trainable parameters that outperform generic architectures with tens of thousands of parameters when compared on the binary classification task of top quark jet tagging.
</details>
<details>
<summary>摘要</summary>
为了满足加速器的冲突速率的提高和深度学习解决方案的可行性，现有一个增长的需求是轻量级快速的神经网络架构，用于低延迟任务such as 触发。我们研究了最近的 Lorentz 和 permutation 相对symmetric架构PELICAN，并提供了具有只有19个可训练参数的实例，与通用架构数万个参数相比，在极高速批处理任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Compressed-representation-of-brain-genetic-transcription"><a href="#Compressed-representation-of-brain-genetic-transcription" class="headerlink" title="Compressed representation of brain genetic transcription"></a>Compressed representation of brain genetic transcription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16113">http://arxiv.org/abs/2310.16113</a></li>
<li>repo_url: None</li>
<li>paper_authors: James K Ruffle, Henry Watkins, Robert J Gray, Harpreet Hyare, Michel Thiebaut de Schotten, Parashkev Nachev</li>
<li>for: 本研究旨在提供一种高效的方法来压缩大规模的脑组织表达数据，以便更好地探索脑组织结构和功能。</li>
<li>methods: 本研究使用了多种常用的线性和非线性方法，包括PCA、kernel PCA、NMF、t-SNE、UMAP以及深度自编码，以实现数据压缩。</li>
<li>results: 研究结果表明，使用深度自编码可以获得最高的重建精度、结构准确性和预测utilty，因此支持使用深度自编码来表示脑组织表达数据。<details>
<summary>Abstract</summary>
The architecture of the brain is too complex to be intuitively surveyable without the use of compressed representations that project its variation into a compact, navigable space. The task is especially challenging with high-dimensional data, such as gene expression, where the joint complexity of anatomical and transcriptional patterns demands maximum compression. Established practice is to use standard principal component analysis (PCA), whose computational felicity is offset by limited expressivity, especially at great compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas transcription data, here we systematically compare compressed representations based on the most widely supported linear and non-linear methods-PCA, kernel PCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding (t-SNE), uniform manifold approximation and projection (UMAP), and deep auto-encoding-quantifying reconstruction fidelity, anatomical coherence, and predictive utility with respect to signalling, microstructural, and metabolic targets. We show that deep auto-encoders yield superior representations across all metrics of performance and target domains, supporting their use as the reference standard for representing transcription patterns in the human brain.
</details>
<details>
<summary>摘要</summary>
人脑的建筑物理太复杂，无法直观探索，需要使用压缩表示方法将其变化 проек到一个紧凑可 navigate 空间中。特别是在高维数据，如基因表达，其结合型复杂性和谱系强度需要最大压缩。现有的做法是使用标准的主成分分析（PCA），它的计算方便性受到限制，特别是在大压缩比例时。使用整个脑、每个 voxel 的 Allan 脑 Atlases 的整个转录数据，我们系统地比较了压缩表示方法，包括 PCA、kernel PCA、非正式矩阵分解（NMF）、t-Stochastic neighbor embedding（t-SNE）、Uniform manifold approximation and projection（UMAP）以及深度自编码。我们发现深度自编码可以在所有性能指标和目标领域中获得最佳表示，支持它作为人脑转录模式的参照标准。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Learning-over-Wireless-Networks-with-Broadcast-Based-Subgraph-Sampling"><a href="#Decentralized-Learning-over-Wireless-Networks-with-Broadcast-Based-Subgraph-Sampling" class="headerlink" title="Decentralized Learning over Wireless Networks with Broadcast-Based Subgraph Sampling"></a>Decentralized Learning over Wireless Networks with Broadcast-Based Subgraph Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16106">http://arxiv.org/abs/2310.16106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Pérez Herrera, Zheng Chen, Erik G. Larsson</li>
<li>for: 本文关注分布式学习无线网络上的通信方面，使用consensus-based分布式随机梯度下降（D-SGD）。考虑到网络中信息交换过程中的实际通信成本或延迟，我们的目标是使得算法快速 konvergence， measured by improvement per transmission slot。</li>
<li>methods: 我们提议了一种高效的通信框架BASS，用于D-SGD在无线网络上的广播传输和抽样。在每个迭代中，我们活化多个不干扰的节点广播模型更新到其邻居。这些subsets是随机变化的，与网络连接性相关，并且受到通信成本限制（例如，每次迭代的平均传输槽数）。在consensus更新步骤中，只有双向链接被保留，以保持通信对称。</li>
<li>results: 与现有的链接计划方法相比，无线通信频道的自然广播特性可以在同样的传输槽数下提高分布式学习的 konvergence 速度。<details>
<summary>Abstract</summary>
This work centers on the communication aspects of decentralized learning over wireless networks, using consensus-based decentralized stochastic gradient descent (D-SGD). Considering the actual communication cost or delay caused by in-network information exchange in an iterative process, our goal is to achieve fast convergence of the algorithm measured by improvement per transmission slot. We propose BASS, an efficient communication framework for D-SGD over wireless networks with broadcast transmission and probabilistic subgraph sampling. In each iteration, we activate multiple subsets of non-interfering nodes to broadcast model updates to their neighbors. These subsets are randomly activated over time, with probabilities reflecting their importance in network connectivity and subject to a communication cost constraint (e.g., the average number of transmission slots per iteration). During the consensus update step, only bi-directional links are effectively preserved to maintain communication symmetry. In comparison to existing link-based scheduling methods, the inherent broadcasting nature of wireless channels offers intrinsic advantages in speeding up convergence of decentralized learning by creating more communicated links with the same number of transmission slots.
</details>
<details>
<summary>摘要</summary>
To achieve this, we propose BASS, an efficient communication framework for D-SGD over wireless networks with broadcast transmission and probabilistic subgraph sampling. In each iteration, we activate multiple subsets of non-interfering nodes to broadcast model updates to their neighbors. These subsets are randomly activated over time, with probabilities reflecting their importance in network connectivity and subject to a communication cost constraint (e.g., the average number of transmission slots per iteration). During the consensus update step, only bi-directional links are effectively preserved to maintain communication symmetry.Compared to existing link-based scheduling methods, the inherent broadcasting nature of wireless channels offers intrinsic advantages in speeding up convergence of decentralized learning by creating more communicated links with the same number of transmission slots.
</details></li>
</ul>
<hr>
<h2 id="Locally-Differentially-Private-Gradient-Tracking-for-Distributed-Online-Learning-over-Directed-Graphs"><a href="#Locally-Differentially-Private-Gradient-Tracking-for-Distributed-Online-Learning-over-Directed-Graphs" class="headerlink" title="Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs"></a>Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16105">http://arxiv.org/abs/2310.16105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqin Chen, Yongqiang Wang</li>
<li>for:  solves the tradeoff between learning accuracy and privacy in distributed online learning over directed graphs.</li>
<li>methods:  proposes a locally differentially private gradient tracking based distributed online learning algorithm that ensures rigorous local differential privacy and converges to the exact optimal solution.</li>
<li>results:  outperforms existing counterparts in both training and testing accuracies, with guaranteed finite cumulative privacy budget even when the number of iterations tends to infinity.<details>
<summary>Abstract</summary>
Distributed online learning has been proven extremely effective in solving large-scale machine learning problems over streaming data. However, information sharing between learners in distributed learning also raises concerns about the potential leakage of individual learners' sensitive data. To mitigate this risk, differential privacy, which is widely regarded as the "gold standard" for privacy protection, has been widely employed in many existing results on distributed online learning. However, these results often face a fundamental tradeoff between learning accuracy and privacy. In this paper, we propose a locally differentially private gradient tracking based distributed online learning algorithm that successfully circumvents this tradeoff. We prove that the proposed algorithm converges in mean square to the exact optimal solution while ensuring rigorous local differential privacy, with the cumulative privacy budget guaranteed to be finite even when the number of iterations tends to infinity. The algorithm is applicable even when the communication graph among learners is directed. To the best of our knowledge, this is the first result that simultaneously ensures learning accuracy and rigorous local differential privacy in distributed online learning over directed graphs. We evaluate our algorithm's performance by using multiple benchmark machine-learning applications, including logistic regression of the "Mushrooms" dataset and CNN-based image classification of the "MNIST" and "CIFAR-10" datasets, respectively. The experimental results confirm that the proposed algorithm outperforms existing counterparts in both training and testing accuracies.
</details>
<details>
<summary>摘要</summary>
分布式在线学习已经证明可以非常有效地解决大规模机器学习问题，但是learners之间的信息共享也会使得个人学习者敏感数据泄露的风险增加。为了解决这个风险，分布式在线学习中广泛采用了广泛被视为“金标准”的隐私保护技术——差分隐私。然而，这些结果通常面临着学习精度和隐私之间的基本负担。在这篇论文中，我们提议一种基于差分隐私的梯度追踪分布式在线学习算法，该算法可以绕过学习精度和隐私之间的负担。我们证明该算法在mean square拟合到了准确的优质解，同时坚持rigorous的本地差分隐私，并且 garanttees the cumulative privacy budget是有限的，即使训练迭代数趋向于无穷。该算法适用于 directed communication graph中的learners。到目前为止，这是分布式在线学习中首先同时保证学习精度和rigorous的本地差分隐私的结果。我们通过多个benchmark机器学习应用程序进行性能评估，包括“蘑菇”数据集上的逻辑回归和“MNIST”和“CIFAR-10”数据集上的CNN图像分类，分别得出了良好的训练和测试准确率。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Bandits-for-Evaluating-and-Improving-Inventory-Control-Policies"><a href="#Contextual-Bandits-for-Evaluating-and-Improving-Inventory-Control-Policies" class="headerlink" title="Contextual Bandits for Evaluating and Improving Inventory Control Policies"></a>Contextual Bandits for Evaluating and Improving Inventory Control Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16096">http://arxiv.org/abs/2310.16096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dean Foster, Randy Jia, Dhruv Madeka</li>
<li>for:  addresses the periodic review inventory control problem with nonstationary random demand, lost sales, and stochastic vendor lead times.</li>
<li>methods:  uses a light-weight contextual bandit-based algorithm to evaluate and occasionally tweak policies.</li>
<li>results:  achieves favorable guarantees both theoretically and in empirical studies.Here’s the full translation in Simplified Chinese:</li>
<li>for: 本文addresses periodic review inventory control problem with nonstationary random demand, lost sales, and stochastic vendor lead times.</li>
<li>methods: 使用轻量级上下文ual bandit-based算法来评估和 occasional tweak policies.</li>
<li>results:  achieves favorable guarantees both theoretically and in empirical studies.<details>
<summary>Abstract</summary>
Solutions to address the periodic review inventory control problem with nonstationary random demand, lost sales, and stochastic vendor lead times typically involve making strong assumptions on the dynamics for either approximation or simulation, and applying methods such as optimization, dynamic programming, or reinforcement learning. Therefore, it is important to analyze and evaluate any inventory control policy, in particular to see if there is room for improvement. We introduce the concept of an equilibrium policy, a desirable property of a policy that intuitively means that, in hindsight, changing only a small fraction of actions does not result in materially more reward. We provide a light-weight contextual bandit-based algorithm to evaluate and occasionally tweak policies, and show that this method achieves favorable guarantees, both theoretically and in empirical studies.
</details>
<details>
<summary>摘要</summary>
解决 periodic review 存储控制问题，包括非站ARY random demand，lost sales，和随机供应商延迟，通常需要做强大的假设，并使用优化、动态Programming或强化学习方法。因此，重要的是分析和评估存储控制策略，以确定是否有可能进行改进。我们介绍了平衡策略，这是一种策略的感知性质，意味着在后悔中，只需要改变一小部分的行动，不会导致更多的奖励。我们提供了一种轻量级上下文ual bandit-based算法来评估和偶尔调整策略，并证明了这种方法在理论和实际研究中都有有利的保证。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Scalable-Framework-for-Neural-Population-Decoding"><a href="#A-Unified-Scalable-Framework-for-Neural-Population-Decoding" class="headerlink" title="A Unified, Scalable Framework for Neural Population Decoding"></a>A Unified, Scalable Framework for Neural Population Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16046">http://arxiv.org/abs/2310.16046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Azabou, Vinam Arora, Venkataramana Ganesh, Ximeng Mao, Santosh Nachimuthu, Michael J. Mendelson, Blake Richards, Matthew G. Perich, Guillaume Lajoie, Eva L. Dyer</li>
<li>for: 这个论文的目的是提出一种基于深度学习的方法，用于分析神经活动的大规模数据。</li>
<li>methods: 这个方法使用了跨注意力和PerceiverIO底层，将神经活动的population dynamics tokenized成一个有效的表示，并使用这个表示来预训练模型。</li>
<li>results: 作者在七只非human Primates的158个不同会话中，使用了大规模的数据集和100个小时的录制，预训练了一个大规模多会话模型。在不同的任务中， authorsshowed that their pretrained model can be rapidly adapted to new, unseen sessions with unspecified neuron correspondence, achieving few-shot performance with minimal labels.<details>
<summary>Abstract</summary>
Our ability to use deep learning approaches to decipher neural activity would likely benefit from greater scale, in terms of both model size and datasets. However, the integration of many neural recordings into one unified model is challenging, as each recording contains the activity of different neurons from different individual animals. In this paper, we introduce a training framework and architecture designed to model the population dynamics of neural activity across diverse, large-scale neural recordings. Our method first tokenizes individual spikes within the dataset to build an efficient representation of neural events that captures the fine temporal structure of neural activity. We then employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural population activities. Utilizing this architecture and training framework, we construct a large-scale multi-session model trained on large datasets from seven nonhuman primates, spanning over 158 different sessions of recording from over 27,373 neural units and over 100 hours of recordings. In a number of different tasks, we demonstrate that our pretrained model can be rapidly adapted to new, unseen sessions with unspecified neuron correspondence, enabling few-shot performance with minimal labels. This work presents a powerful new approach for building deep learning tools to analyze neural data and stakes out a clear path to training at scale.
</details>
<details>
<summary>摘要</summary>
我们可能通过深入学习方法解读神经活动受惠于更大的规模，包括模型大小和数据集。然而，将多个神经记录集成为一个统一的模型是挑战，因为每个记录包含不同动物的不同神经元的活动。在这篇论文中，我们介绍了一个训练框架和架构，用于模型神经活动范围内的人工智能动态。我们首先将数据集中的每个冲击分解成精细的时间结构，然后使用对话式和PerceiverIO脊梁来构建封装了神经人类活动的秘密tokenization。使用这个框架和训练方法，我们建立了一个大规模多Session模型，训练在7个非人类 Primates上，共计158个不同的Session，记录了27,373个神经元和100个小时的记录。在一些不同任务中，我们示示了我们预训练模型可以快速适应新、未看过的Session，并且只需少量标签。这项工作提出了一种新的深入学习工具来分析神经数据，并铺开了训练在大规模的道路。
</details></li>
</ul>
<hr>
<h2 id="TimewarpVAE-Simultaneous-Time-Warping-and-Representation-Learning-of-Trajectories"><a href="#TimewarpVAE-Simultaneous-Time-Warping-and-Representation-Learning-of-Trajectories" class="headerlink" title="TimewarpVAE: Simultaneous Time-Warping and Representation Learning of Trajectories"></a>TimewarpVAE: Simultaneous Time-Warping and Representation Learning of Trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16027">http://arxiv.org/abs/2310.16027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/travers-rhodes/timewarpvae">https://github.com/travers-rhodes/timewarpvae</a></li>
<li>paper_authors: Travers Rhodes, Daniel D. Lee</li>
<li>for: 本研究旨在学习复杂任务中人类示例路径的有效表示。</li>
<li>methods: 该研究提出了一种完全可导的拟合推断算法 TimewarpVAE，它通过动态时间拟合（DTW）同时学习时间变化和空间变化的纬度因素。</li>
<li>results: 研究结果表明，TimewarpVAE算法可以在小手写和餐刀捏持任务上学习有意义的时间对齐和空间变化的表示，并且可以生成符合 semantics的新路径。<details>
<summary>Abstract</summary>
Human demonstrations of trajectories are an important source of training data for many machine learning problems. However, the difficulty of collecting human demonstration data for complex tasks makes learning efficient representations of those trajectories challenging. For many problems, such as for handwriting or for quasistatic dexterous manipulation, the exact timings of the trajectories should be factored from their spatial path characteristics. In this work, we propose TimewarpVAE, a fully differentiable manifold-learning algorithm that incorporates Dynamic Time Warping (DTW) to simultaneously learn both timing variations and latent factors of spatial variation. We show how the TimewarpVAE algorithm learns appropriate time alignments and meaningful representations of spatial variations in small handwriting and fork manipulation datasets. Our results have lower spatial reconstruction test error than baseline approaches and the learned low-dimensional representations can be used to efficiently generate semantically meaningful novel trajectories.
</details>
<details>
<summary>摘要</summary>
人类示例路径是许多机器学习问题的重要训练数据源。然而，收集复杂任务的人类示例数据的困难性使得学习这些路径的有效表示困难。例如，手写或 quasi-静止的手部操作中，路径的具体时间 shouldn't 被从其空间轨迹特征中分离。在这种情况下，我们提出了 TimewarpVAE，一种完全可导的推广学习算法，其中包含动态时间扭曲（DTW），以同时学习时间变化和空间变化的秘密因素。我们展示了 TimewarpVAE 算法如何学习合适的时间对齐和有意义的空间变化表示。我们的结果在小手写和铲 manipulate 数据集上具有较低的空间重建测试错误，并且学习的低维度表示可以高效地生成具有 semantic 意义的新路径。
</details></li>
</ul>
<hr>
<h2 id="Practical-Computational-Power-of-Linear-Transformers-and-Their-Recurrent-and-Self-Referential-Extensions"><a href="#Practical-Computational-Power-of-Linear-Transformers-and-Their-Recurrent-and-Self-Referential-Extensions" class="headerlink" title="Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions"></a>Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16076">http://arxiv.org/abs/2310.16076</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idsia/fwp-formal-lang">https://github.com/idsia/fwp-formal-lang</a></li>
<li>paper_authors: Kazuki Irie, Róbert Csordás, Jürgen Schmidhuber</li>
<li>for: 这项研究探讨了基于批处理器的循环神经网络（RNN）的计算能力层次结构，以及在实时和有限精度假设下的RNN架构。</li>
<li>methods: 本研究使用自动回归转换器，即线性转换器（LT）或快速Weight程序（FWP），这些模型可以看作是固定大小状态的RNN序列处理器，同时也可以表示为当前流行的自注意网络。</li>
<li>results: 我们的实验表明，许多对标准转换器的结果直接适用于LTs&#x2F;FWPs，而且扩展FWP可以超越LT的一些局限性，例如在枚举问题上进行泛化。我们的代码公开。<details>
<summary>Abstract</summary>
Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs). LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state, while they can also be expressed as the now-popular self-attention networks. We show that many well-known results for the standard Transformer directly transfer to LTs/FWPs. Our formal language recognition experiments demonstrate how recently proposed FWP extensions such as recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of the LT, e.g., allowing for generalisation on the parity problem. Our code is public.
</details>
<details>
<summary>摘要</summary>
近期研究计算力学的回归神经网络（RNN）表明了一个RNN架构层次结构，基于实时和有限精度假设。我们研究了自动回归转换器（LT），它们是特殊的因为它们可以视为固定大小的状态Sequence processor，同时也可以表示为目前受欢迎的自注意网络。我们证明了许多标准Transformer的结果直接适用于LTs/FWPs。我们的正式语言识别实验表明了 reciprocal FWP和自referential weight matrix的扩展可以超越LT的一些局限性，例如解决基本问题的泛化问题。我们的代码公开。
</details></li>
</ul>
<hr>
<h2 id="MLFMF-Data-Sets-for-Machine-Learning-for-Mathematical-Formalization"><a href="#MLFMF-Data-Sets-for-Machine-Learning-for-Mathematical-Formalization" class="headerlink" title="MLFMF: Data Sets for Machine Learning for Mathematical Formalization"></a>MLFMF: Data Sets for Machine Learning for Mathematical Formalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16005">http://arxiv.org/abs/2310.16005</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ul-fmf/mlfmf-data">https://github.com/ul-fmf/mlfmf-data</a></li>
<li>paper_authors: Andrej Bauer, Matej Petković, Ljupčo Todorovski</li>
<li>for: 本研究用于开发一个收集数据集，用于对ormalization of mathematics with proof assistants进行benchmarking。</li>
<li>methods: 本研究使用了Agda和Lean两种证明助手中的library，通过EXTRACTING网络和s-expressions来 Represent each library in two ways。</li>
<li>results: 本研究reported基线结果，使用了标准图 embeddings、word embeddings、tree ensembles和instance-based learning算法。 MLFMF数据集提供了对formalized mathematics的numerous machine learningapproaches的Solid benchmarking支持。<details>
<summary>Abstract</summary>
We introduce MLFMF, a collection of data sets for benchmarking recommendation systems used to support formalization of mathematics with proof assistants. These systems help humans identify which previous entries (theorems, constructions, datatypes, and postulates) are relevant in proving a new theorem or carrying out a new construction. Each data set is derived from a library of formalized mathematics written in proof assistants Agda or Lean. The collection includes the largest Lean~4 library Mathlib, and some of the largest Agda libraries: the standard library, the library of univalent mathematics Agda-unimath, and the TypeTopology library. Each data set represents the corresponding library in two ways: as a heterogeneous network, and as a list of s-expressions representing the syntax trees of all the entries in the library. The network contains the (modular) structure of the library and the references between entries, while the s-expressions give complete and easily parsed information about every entry. We report baseline results using standard graph and word embeddings, tree ensembles, and instance-based learning algorithms. The MLFMF data sets provide solid benchmarking support for further investigation of the numerous machine learning approaches to formalized mathematics. The methodology used to extract the networks and the s-expressions readily applies to other libraries, and is applicable to other proof assistants. With more than $250\,000$ entries in total, this is currently the largest collection of formalized mathematical knowledge in machine learnable format.
</details>
<details>
<summary>摘要</summary>
我们介绍MLFMF，一个收集数据集用于评估推荐系统的 formalized mathematics  benchmarking 集合。这些系统可以帮助人类identify新的证明（ theorem、construction、数据类型和假设）和新的构造是否有 relevance 在证明新的证明或完成新的构造。每个数据集来自于Agda或Lean证明助手中的库，包括最大的Lean4库Mathlib，以及一些最大的Agda库：标准库、Agda-unimath库和TypeTopology库。每个数据集表示对应的库在两种方式：为heterogeneous网络和所有entry的Syntax树表示。网络包含库中的（模块）结构和entry之间的引用，而Syntax树表示了每个entry的完整和可读性。我们报告了使用标准图 embeddings、word embeddings、树集和实例学习算法的基准结果。MLFMF数据集为further investigation of numerous machine learning approaches to formalized mathematics提供了坚实的 benchmarking 支持。我们使用的方法可以轻松应用于其他库和证明助手，现已有超过250000个入库。这是目前最大的 formalized mathematical knowledge 在机器学习化Format中的集合。
</details></li>
</ul>
<hr>
<h2 id="White-box-Compiler-Fuzzing-Empowered-by-Large-Language-Models"><a href="#White-box-Compiler-Fuzzing-Empowered-by-Large-Language-Models" class="headerlink" title="White-box Compiler Fuzzing Empowered by Large Language Models"></a>White-box Compiler Fuzzing Empowered by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15991">http://arxiv.org/abs/2310.15991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyuan Yang, Yinlin Deng, Runyu Lu, Jiayi Yao, Jiawei Liu, Reyhaneh Jabbarvand, Lingming Zhang</li>
<li>for:  compiler testing, specifically white-box fuzzing of compiler optimizations</li>
<li>methods:  uses Large Language Models (LLMs) with source-code information to generate high-quality tests for exercising deep optimizations</li>
<li>results:  found 96 bugs, with 80 confirmed as previously unknown and 51 already fixed, demonstrating the effectiveness of WhiteFox in discovering previously undiscovered compiler bugs.<details>
<summary>Abstract</summary>
Compiler correctness is crucial, as miscompilation falsifying the program behaviors can lead to serious consequences. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates tests without sufficient understanding of internal compiler behaviors. As such, they often fail to construct programs to exercise conditions of intricate optimizations. Meanwhile, traditional white-box techniques are computationally inapplicable to the giant codebase of compilers. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, prompting LLMs with compiler source-code information remains a missing piece of research in compiler testing.   To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization. WhiteFox adopts a dual-model framework: (i) an analysis LLM examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) a generation LLM produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are used as feedback to further enhance the test generation on the fly. Our evaluation on four popular compilers shows that WhiteFox can generate high-quality tests to exercise deep optimizations requiring intricate conditions, practicing up to 80 more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 96 bugs, with 80 confirmed as previously unknown and 51 already fixed. Beyond compiler testing, WhiteFox can also be adapted for white-box fuzzing of other complex, real-world software systems in general.
</details>
<details>
<summary>摘要</summary>
compiler Correctness 非常重要，因为 mistCompilation 可能会导致程序行为错误。在文献中，对于 Compiler 的检测已经进行了广泛的研究。然而， Compiler fuzzing 仍然是一项挑战：现有的技术主要集中在黑盒和灰盒 fuzzing 中，生成测试程序不具备内存compiler内部行为的充分理解。此外，传统的白盒技术对于编译器的代码库来说是计算不可行的。现有的研究表明， Large Language Models (LLMs) 在代码生成/理解任务中表现出色，并在黑盒 fuzzing 中达到了状态对照性。然而，在编译器测试中，LLMs 被提取source code 信息的研究还没有进行。为此，我们提出了 WhiteFox，首个使用 LLMs 和 source code 信息进行白盒编译器测试的工具。WhiteFox 采用了两个模型框架：（i）分析 LLM 对低级优化源代码进行分析，并生成高级测试程序可以触发优化;（ii）生成 LLM 根据总结的需求生成测试程序。此外，我们还使用优化触发测试作为反馈，以进一步改进测试生成。我们对四种流行的编译器进行评估，发现 WhiteFox 可以生成高质量的测试程序，激活深入的优化需求。至今，WhiteFox 共发现了96个bug，其中80个已经确认为新发现的问题，51个已经修复。除了编译器测试外，WhiteFox 还可以适用于白盒 fuzzing 其他复杂、实际世界软件系统。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Traffic-Simulation-A-Comprehensive-Review"><a href="#Data-driven-Traffic-Simulation-A-Comprehensive-Review" class="headerlink" title="Data-driven Traffic Simulation: A Comprehensive Review"></a>Data-driven Traffic Simulation: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15975">http://arxiv.org/abs/2310.15975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Chen, Meixin Zhu, Hao Yang, Xuesong Wang, Yinhai Wang</li>
<li>For: This paper aims to review current research efforts and provide a futuristic perspective on data-driven microscopic traffic simulation for autonomous vehicles.* Methods: The paper discusses various methods, including imitation learning, reinforcement learning, generative learning, and deep learning, and evaluates their advantages and disadvantages.* Results: The paper provides a comprehensive evaluation of existing challenges and future research directions in data-driven microscopic traffic simulation for autonomous vehicles.Here is the same information in Simplified Chinese text:* For: 这篇论文目标是对当前的数据驱动微型交通模拟技术进行评估和未来发展规划。* Methods: 论文讨论了各种方法，包括仿制学习、奖励学习、生成学习和深度学习，并评估了它们的优缺点。* Results: 论文提供了数据驱动微型交通模拟技术的现有挑战和未来研究方向的全面评估。<details>
<summary>Abstract</summary>
Autonomous vehicles (AVs) have the potential to significantly revolutionize society by providing a secure and efficient mode of transportation. Recent years have witnessed notable advance-ments in autonomous driving perception and prediction, but the challenge of validating the performance of AVs remains largely unresolved. Data-driven microscopic traffic simulation has be-come an important tool for autonomous driving testing due to 1) availability of high-fidelity traffic data; 2) its advantages of ena-bling large-scale testing and scenario reproducibility; and 3) its potential in reactive and realistic traffic simulation. However, a comprehensive review of this topic is currently lacking. This pa-per aims to fill this gap by summarizing relevant studies. The primary objective of this paper is to review current research ef-forts and provide a futuristic perspective that will benefit future developments in the field. It introduces the general issues of data-driven traffic simulation and outlines key concepts and terms. After overviewing traffic simulation, various datasets and evalua-tion metrics commonly used are reviewed. The paper then offers a comprehensive evaluation of imitation learning, reinforcement learning, generative and deep learning methods, summarizing each and analyzing their advantages and disadvantages in detail. Moreover, it evaluates the state-of-the-art, existing challenges, and future research directions.
</details>
<details>
<summary>摘要</summary>
自动驾驶车（AV）有可能重大改变社会，提供安全有效的交通方式。过去几年，自动驾驶驱动技术得到了显著的进步，但AV的性能验证仍然存在重要的挑战。基于数据驱动的微型交通 simulate 成为了自动驾驶测试中的重要工具，因为它们具有以下优势：1）高品质交通数据的可 availability; 2）大规模测试和enario reproducibility的优势; 3）可以实现反应性和真实的交通 simulate。然而，这个领域的全面回顾仍然缺失。这篇论文的主要目标是审查当前的研究努力，并为未来发展提供未来性的视角。它介绍了一般问题，并 outline 关键概念和术语。然后，它综述了不同的数据集和评价指标，并进行了详细的分析和评价。此外，它还评估了现状、挑战和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Sign-based-Random-Reshuffling-Algorithms-for-Nonconvex-Optimization"><a href="#Convergence-of-Sign-based-Random-Reshuffling-Algorithms-for-Nonconvex-Optimization" class="headerlink" title="Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex Optimization"></a>Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15976">http://arxiv.org/abs/2310.15976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Qin, Zhishuai Liu, Pan Xu</li>
<li>for: 这个论文主要针对非对称优化问题，提出了一种基于随机排序的签名SGD算法，并证明了其 converge 性。</li>
<li>methods: 该论文使用了随机排序（SignRR）、变量减少（SignRVR）和均值更新（SignRVM）等方法，并证明了它们的 converge 性。</li>
<li>results: 该论文通过实验证明，随机排序签名SGD算法可以与现有的基准相匹配或超越它们。<details>
<summary>Abstract</summary>
signSGD is popular in nonconvex optimization due to its communication efficiency. Yet, existing analyses of signSGD rely on assuming that data are sampled with replacement in each iteration, contradicting the practical implementation where data are randomly reshuffled and sequentially fed into the algorithm. We bridge this gap by proving the first convergence result of signSGD with random reshuffling (SignRR) for nonconvex optimization. Given the dataset size $n$, the number of epochs of data passes $T$, and the variance bound of a stochastic gradient $\sigma^2$, we show that SignRR has the same convergence rate $O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ as signSGD \citep{bernstein2018signsgd}. We then present SignRVR and SignRVM, which leverage variance-reduced gradients and momentum updates respectively, both converging at $O(\log(nT)/\sqrt{nT})$. In contrast with the analysis of signSGD, our results do not require an extremely large batch size in each iteration to be of the same order as the total number of iterations \citep{bernstein2018signsgd} or the signs of stochastic and true gradients match element-wise with a minimum probability of 1/2 \citep{safaryan2021stochastic}. We also extend our algorithms to cases where data are distributed across different machines, yielding dist-SignRVR and dist-SignRVM, both converging at $O(\log(n_0T)/\sqrt{n_0T})$, where $n_0$ is the dataset size of a single machine. We back up our theoretical findings through experiments on simulated and real-world problems, verifying that randomly reshuffled sign methods match or surpass existing baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate text into Simplified Chinese<</SYS>>signSGD 受欢迎在非对称优化中，因为它的通信效率高。然而，现有的 signSGD 分析假设数据在每个迭代中随机抽取，与实际情况不符，即数据随机排序并且按照顺序feed into the algorithm。我们将这一悖论 bridge 这一悖论，通过证明 signSGD 随机排序（SignRR）在非对称优化中的首次收敛结果。给出 dataset 大小 $n$，迭代数据 passes  $T$， Stochastic gradient 的方差 bound $\sigma^2$，我们显示 SignRR 的收敛速率为 $O(\frac{\log(nT)}{\sqrt{nT} + \|\sigma\|_1)$，与 signSGD 相同。然后，我们提出 SignRVR 和 SignRVM，它们都使用减少噪声的 gradient 和摩托UPDATE 分别收敛，它们的收敛速率为 $O(\frac{\log(nT)}{\sqrt{nT}$。与 signSGD 分析不同，我们的结果不需要在每个迭代中批处理大小与总迭代数量之间的很大批处理大小。我们还扩展我们的算法，以适应数据分布在不同机器上的情况，得到 dist-SignRVR 和 dist-SignRVM，它们的收敛速率为 $O(\frac{\log(n_0T)}{\sqrt{n_0T}$，其中 $n_0$ 是单机dataset 大小。我们通过对模拟和实际问题进行实验，证明 randomly reshuffled sign 方法与现有的基准相匹配或甚至超越。
</details></li>
</ul>
<hr>
<h2 id="Minimax-Forward-and-Backward-Learning-of-Evolving-Tasks-with-Performance-Guarantees"><a href="#Minimax-Forward-and-Backward-Learning-of-Evolving-Tasks-with-Performance-Guarantees" class="headerlink" title="Minimax Forward and Backward Learning of Evolving Tasks with Performance Guarantees"></a>Minimax Forward and Backward Learning of Evolving Tasks with Performance Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15974">http://arxiv.org/abs/2310.15974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/machinelearningbcam/imrcs-for-incremental-learning-neurips-2023">https://github.com/machinelearningbcam/imrcs-for-incremental-learning-neurips-2023</a></li>
<li>paper_authors: Verónica Álvarez, Santiago Mazuelas, Jose A. Lozano</li>
<li>for: 这篇论文是为了解决随时间推移而改变的任务序列中的分类任务。</li>
<li>methods: 本论文提出了一种叫做增量最小最大协方差分类器（IMRC），可以充分利用前向和后向学习，并考虑任务序列中任务之间的演进。</li>
<li>results: 实验结果显示，IMRC可以对于几乎无samples的情况下，实现明显的性能提升。<details>
<summary>Abstract</summary>
For a sequence of classification tasks that arrive over time, it is common that tasks are evolving in the sense that consecutive tasks often have a higher similarity. The incremental learning of a growing sequence of tasks holds promise to enable accurate classification even with few samples per task by leveraging information from all the tasks in the sequence (forward and backward learning). However, existing techniques developed for continual learning and concept drift adaptation are either designed for tasks with time-independent similarities or only aim to learn the last task in the sequence. This paper presents incremental minimax risk classifiers (IMRCs) that effectively exploit forward and backward learning and account for evolving tasks. In addition, we analytically characterize the performance improvement provided by forward and backward learning in terms of the tasks' expected quadratic change and the number of tasks. The experimental evaluation shows that IMRCs can result in a significant performance improvement, especially for reduced sample sizes.
</details>
<details>
<summary>摘要</summary>
For a sequence of classification tasks that arrive over time, it is common that tasks are evolving in the sense that consecutive tasks often have a higher similarity. The incremental learning of a growing sequence of tasks holds promise to enable accurate classification even with few samples per task by leveraging information from all the tasks in the sequence (forward and backward learning). However, existing techniques developed for continual learning and concept drift adaptation are either designed for tasks with time-independent similarities or only aim to learn the last task in the sequence. This paper presents incremental minimax risk classifiers (IMRCs) that effectively exploit forward and backward learning and account for evolving tasks. In addition, we analytically characterize the performance improvement provided by forward and backward learning in terms of the tasks' expected quadratic change and the number of tasks. The experimental evaluation shows that IMRCs can result in a significant performance improvement, especially for reduced sample sizes.Here's the translation in Traditional Chinese:在时间序列中涌现的课题中，常有课题之间的相似性增加。对于这种情况，将批量学习应用到不断增长的课题序列上，可以实现仅使用少量样本进行精确的分类。然而，现有的持续学习和概念漂移适应技术 either 对任务之间的相似性为时间独立或只是针对最后一个任务进行学习。本文则提出了增量最大危险风险分类器（IMRC），可以有效地利用前向和后向学习，并考虑到任务的演化。此外，我们也 analytically Characterize 增量学习的性能改善，包括任务预期的quadratic change 和任务数量。实验评估显示，IMRC 可以对于仅使用少量样本进行分类时，实现重要的性能改善。
</details></li>
</ul>
<hr>
<h2 id="Constructing-and-Machine-Learning-Calabi-Yau-Five-folds"><a href="#Constructing-and-Machine-Learning-Calabi-Yau-Five-folds" class="headerlink" title="Constructing and Machine Learning Calabi-Yau Five-folds"></a>Constructing and Machine Learning Calabi-Yau Five-folds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15966">http://arxiv.org/abs/2310.15966</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Alawadhi, D. Angella, A. Leonardo, T. Schettini Gherardini</li>
<li>for: 这个论文目的是构建所有可能的完全交叉Calabi-Yau五维空间，使用四个或更少的复抽象 проекive空间，并对这些空间进行研究。</li>
<li>methods: 作者使用了configuration matrix的方法，并对这些空间进行计数。他们还计算了这些空间中的cohomological数据，并将其存储在 dataset 中。</li>
<li>results: 作者发现了2375个不同的Hodge diamond，并对这些数据进行了supervised机器学习，使用类ifier和回归神经网络。他们发现，可以非常有效地预测h^{1,1}的值，并且accuracy达96%。对于h^{1,4},h^{2,3}和η，也发现了非常高的R^2 Score，但是精度较低，因为这些值的范围很大。<details>
<summary>Abstract</summary>
We construct all possible complete intersection Calabi-Yau five-folds in a product of four or less complex projective spaces, with up to four constraints. We obtain $27068$ spaces, which are not related by permutations of rows and columns of the configuration matrix, and determine the Euler number for all of them. Excluding the $3909$ product manifolds among those, we calculate the cohomological data for $12433$ cases, i.e. $53.7 \%$ of the non-product spaces, obtaining $2375$ different Hodge diamonds. The dataset containing all the above information is available at https://www.dropbox.com/scl/fo/z7ii5idt6qxu36e0b8azq/h?rlkey=0qfhx3tykytduobpld510gsfy&dl=0 . The distributions of the invariants are presented, and a comparison with the lower-dimensional analogues is discussed. Supervised machine learning is performed on the cohomological data, via classifier and regressor (both fully connected and convolutional) neural networks. We find that $h^{1,1}$ can be learnt very efficiently, with very high $R^2$ score and an accuracy of $96\%$, i.e. $96 \%$ of the predictions exactly match the correct values. For $h^{1,4},h^{2,3}, \eta$, we also find very high $R^2$ scores, but the accuracy is lower, due to the large ranges of possible values.
</details>
<details>
<summary>摘要</summary>
我们建构所有可能的完整交叉Calabi-Yau五维流体，在四个或更少的复数射影空间中，并将其条件为最多四个。我们获得了27068个空间，其中3909个是产生的流体，我们计算了这些流体的 cohomological 数据，获得了2375个不同的Hodge几何。我们提供了这些数据的Dataset，可以在https://www.dropbox.com/scl/fo/z7ii5idt6qxu36e0b8azq/h?rlkey=0qfhx3tykytduobpld510gsfy&dl=0 获取。我们分析了这些数据的分布，并与低维类型的数据进行比较。我们还使用了supervised机器学习，使用分类器和回归（both fully connected和 convolutional）神经网络，以学习cohomological数据。我们发现，可以非常高效地学习 $h^{1,1}$，其 $R^2$ 分数和准确率均非常高，分别为96%和96%。对于 $h^{1,4},h^{2,3}, \eta$，我们也发现了非常高的 $R^2$ 分数，但准确率较低，因为这些数据的可能的值幅度较大。
</details></li>
</ul>
<hr>
<h2 id="Weighted-Distance-Nearest-Neighbor-Condensing"><a href="#Weighted-Distance-Nearest-Neighbor-Condensing" class="headerlink" title="Weighted Distance Nearest Neighbor Condensing"></a>Weighted Distance Nearest Neighbor Condensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15951">http://arxiv.org/abs/2310.15951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lee-Ad Gottlieb, Timor Sharabi, Roi Weiss</li>
<li>for: 这篇论文研究了加权距离最近邻居减去问题，即对减去集中分配权重，然后根据权重距离最近邻居来标注新点。</li>
<li>methods: 本论文提出了一种新的减去模型，并研究了其理论性质。具体来说，作者们使用权重距离来标注新点，并证明了这种方法可以比标准最近邻居规则更好地减去。</li>
<li>results: 作者们在论文中提出了一种减去启发式，并证明了这种启发式是极大likelihood估计的极限情况。此外，他们还进行了一些实验，并证明了这种方法在实际应用中的表现非常良好。<details>
<summary>Abstract</summary>
The problem of nearest neighbor condensing has enjoyed a long history of study, both in its theoretical and practical aspects. In this paper, we introduce the problem of weighted distance nearest neighbor condensing, where one assigns weights to each point of the condensed set, and then new points are labeled based on their weighted distance nearest neighbor in the condensed set.   We study the theoretical properties of this new model, and show that it can produce dramatically better condensing than the standard nearest neighbor rule, yet is characterized by generalization bounds almost identical to the latter. We then suggest a condensing heuristic for our new problem. We demonstrate Bayes consistency for this heuristic, and also show promising empirical results.
</details>
<details>
<summary>摘要</summary>
这个 nearest neighbor 压缩问题有很长的历史研究，包括理论和实践方面。在这篇论文中，我们引入了一个新的weighted distance nearest neighbor压缩模型，其中每个点都有一个权重，然后根据这些权重计算的距离最近的点在压缩集中。我们研究了这个新模型的理论性质，并证明它可以比标准的 nearest neighbor 规则更好地压缩，但是其泛化 bound 几乎与后者一样。我们then propose了一种压缩启发法，并证明了这种启发法的拜访统计准确性。最后，我们还提供了一些实验结果，表明这种方法的潜在实用性。
</details></li>
</ul>
<hr>
<h2 id="ABKD-Graph-Neural-Network-Compression-with-Attention-Based-Knowledge-Distillation"><a href="#ABKD-Graph-Neural-Network-Compression-with-Attention-Based-Knowledge-Distillation" class="headerlink" title="ABKD: Graph Neural Network Compression with Attention-Based Knowledge Distillation"></a>ABKD: Graph Neural Network Compression with Attention-Based Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15938">http://arxiv.org/abs/2310.15938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anshul Ahluwalia, Rohit Das, Payman Behnam, Alind Khare, Pan Li, Alexey Tumanov<br>for: 这个论文的目的是提出一种基于注意力的知识填充（ABKD）方法，用于压缩图像网络（GNNs），以提高压缩后的准确率。methods: 这个论文使用了知识填充（KD）方法，并使用注意力机制来选择重要的教师-学生层对。results:  compared to现有的方法，这个方法可以实现更高的压缩率（32.3倍），同时保持相对较高的准确率（1.79%提升）。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have proven to be quite versatile for a variety of applications, including recommendation systems, fake news detection, drug discovery, and even computer vision. Due to the expanding size of graph-structured data, GNN models have also increased in complexity, leading to substantial latency issues. This is primarily attributed to the irregular structure of graph data and its access pattern into memory. The natural solution to reduce latency is to compress large GNNs into small GNNs. One way to do this is via knowledge distillation (KD). However, most KD approaches for GNNs only consider the outputs of the last layers and do not consider the outputs of the intermediate layers of the GNNs; these layers may contain important inductive biases indicated by the graph structure. To address this shortcoming, we propose a novel KD approach to GNN compression that we call Attention-Based Knowledge Distillation (ABKD). ABKD is a KD approach that uses attention to identify important intermediate teacher-student layer pairs and focuses on aligning their outputs. ABKD enables higher compression of GNNs with a smaller accuracy dropoff compared to existing KD approaches. On average, we achieve a 1.79% increase in accuracy with a 32.3x compression ratio on OGBN-Mag, a large graph dataset, compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 已经证明非常适用于多种应用程序，包括推荐系统、假新闻检测、药物发现和计算机视觉。由于图数据的规模不断扩大，GNN 模型也在复杂度方面不断提高，导致了明显的延迟问题。这主要归结于图数据的不规则结构和访问模式。以减少延迟为目的，可以将大型 GNN 压缩成小型 GNN。一种可行的方法是通过知识传播（KD）。然而，现有的 KD 方法只考虑 GNN 的最后层输出，而忽略了中间层的输出，这些层可能包含图结构中重要的导向信息。为了解决这个缺陷，我们提出了一种新的 KD 方法，称之为注意力基本知识传播（ABKD）。ABKD 是一种使用注意力来确定重要的教师生Student层对对应的输出的准确性。ABKD 可以更好地压缩 GNN，并且相比现有的 KD 方法，具有更高的压缩率和更低的准确性下降。在 OGBN-Mag 大图数据集上，我们平均 achieved 1.79% 的准确性提高，与状态机器的 KD 方法相比，具有 32.3 倍的压缩率。
</details></li>
</ul>
<hr>
<h2 id="Online-Robust-Mean-Estimation"><a href="#Online-Robust-Mean-Estimation" class="headerlink" title="Online Robust Mean Estimation"></a>Online Robust Mean Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15932">http://arxiv.org/abs/2310.15932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel M. Kane, Ilias Diakonikolas, Hanshen Xiao, Sihan Liu</li>
<li>for: 本文研究高维度 robust mean estimation 问题在在线 Setting下。 Specifically, we consider a scenario where $n$ sensor 测量一些常见、持续进行的现象。 At each time step $t&#x3D;1,2,\ldots,T$, the $i^{th}$ sensor reports its readings $x^{(i)}_t$ for that time step. The algorithm must then commit to its estimate $\mu_t$ for the true mean value of the process at time $t$.</li>
<li>methods: 我们使用 online 算法来计算一个好的 approximation $\mu$ 来近似 true mean 值 $\mu^\ast :&#x3D; \mathbf{E}[X]$. We assume that most of the sensors observe independent samples from some common distribution $X$, but an $\epsilon$-fraction of them may instead behave maliciously.</li>
<li>results: 我们证明了两个主要结论。 First, if the uncorrupted samples satisfy the standard condition of $(\epsilon,\delta)$-stability, we give an efficient online algorithm that outputs estimates $\mu_t$, $t \in [T],$ such that with high probability it holds that $|\mu-\mu^\ast|_2 &#x3D; O(\delta \log(T))$. We note that this error bound is nearly competitive with the best offline algorithms, which would achieve $\ell_2$-error of $O(\delta)$. Our second main result shows that with additional assumptions on the input (most notably that $X$ is a product distribution) there are inefficient algorithms whose error does not depend on $T$ at all.<details>
<summary>Abstract</summary>
We study the problem of high-dimensional robust mean estimation in an online setting. Specifically, we consider a scenario where $n$ sensors are measuring some common, ongoing phenomenon. At each time step $t=1,2,\ldots,T$, the $i^{th}$ sensor reports its readings $x^{(i)}_t$ for that time step. The algorithm must then commit to its estimate $\mu_t$ for the true mean value of the process at time $t$. We assume that most of the sensors observe independent samples from some common distribution $X$, but an $\epsilon$-fraction of them may instead behave maliciously. The algorithm wishes to compute a good approximation $\mu$ to the true mean $\mu^\ast := \mathbf{E}[X]$. We note that if the algorithm is allowed to wait until time $T$ to report its estimate, this reduces to the well-studied problem of robust mean estimation. However, the requirement that our algorithm produces partial estimates as the data is coming in substantially complicates the situation.   We prove two main results about online robust mean estimation in this model. First, if the uncorrupted samples satisfy the standard condition of $(\epsilon,\delta)$-stability, we give an efficient online algorithm that outputs estimates $\mu_t$, $t \in [T],$ such that with high probability it holds that $\|\mu-\mu^\ast\|_2 = O(\delta \log(T))$, where $\mu = (\mu_t)_{t \in [T]}$. We note that this error bound is nearly competitive with the best offline algorithms, which would achieve $\ell_2$-error of $O(\delta)$. Our second main result shows that with additional assumptions on the input (most notably that $X$ is a product distribution) there are inefficient algorithms whose error does not depend on $T$ at all.
</details>
<details>
<summary>摘要</summary>
我们研究了一个高维Robust Mean Estimation问题在在线 Setting中。 Specifically, we consider a scenario where $n$ sensor are measuring some common, ongoing phenomenon. At each time step $t=1,2,\ldots,T$, the $i^{th}$ sensor reports its readings $x^{(i)}_t$ for that time step. The algorithm must then commit to its estimate $\mu_t$ for the true mean value of the process at time $t$. We assume that most of the sensors observe independent samples from some common distribution $X$, but an $\epsilon$-fraction of them may instead behave maliciously. The algorithm wishes to compute a good approximation $\mu$ to the true mean $\mu^\ast := \mathbf{E}[X]$. We note that if the algorithm is allowed to wait until time $T$ to report its estimate, this reduces to the well-studied problem of robust mean estimation. However, the requirement that our algorithm produces partial estimates as the data is coming in substantially complicates the situation.  我们证明了在这种模型中的两个主要结果。 first, if the uncorrupted samples satisfy the standard condition of $( \epsilon, \delta )$-stability, we give an efficient online algorithm that outputs estimates $\mu_t$, $t \in [T],$ such that with high probability it holds that $\| \mu - \mu^\ast \|_2 = O( \delta \log(T) )$, where $\mu = ( \mu_t )_{t \in [T]}$. We note that this error bound is nearly competitive with the best offline algorithms, which would achieve $\ell_2$-error of $O( \delta )$. Our second main result shows that with additional assumptions on the input (most notably that $X$ is a product distribution) there are inefficient algorithms whose error does not depend on $T$ at all.
</details></li>
</ul>
<hr>
<h2 id="Climate-Change-Impact-on-Agricultural-Land-Suitability-An-Interpretable-Machine-Learning-Based-Eurasia-Case-Study"><a href="#Climate-Change-Impact-on-Agricultural-Land-Suitability-An-Interpretable-Machine-Learning-Based-Eurasia-Case-Study" class="headerlink" title="Climate Change Impact on Agricultural Land Suitability: An Interpretable Machine Learning-Based Eurasia Case Study"></a>Climate Change Impact on Agricultural Land Suitability: An Interpretable Machine Learning-Based Eurasia Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15912">http://arxiv.org/abs/2310.15912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valeriy Shevchenko, Daria Taniushkina, Aleksander Lukashevich, Aleksandr Bulkin, Roland Grinis, Kirill Kovalev, Veronika Narozhnaia, Nazar Sotiriadi, Alexander Krenke, Yury Maximov</li>
<li>for: 预测 климатиче变化对农业适应性的影响，以便为政策制定人员提供有用的决策指南，以避免人道主义危机。</li>
<li>methods: 使用机器学习方法预测不同碳排放enario下农业适应性的风险，并通过全面的特征重要性分析，揭示了气候和地形特征对农业适应性的影响。</li>
<li>results: 该研究实现了Remarkable accuracy，为政策制定人员提供了有价值的决策指南，以避免人道主义危机。<details>
<summary>Abstract</summary>
The United Nations has identified improving food security and reducing hunger as essential components of its sustainable development goals. As of 2021, approximately 828 million people worldwide are experiencing hunger and malnutrition, with numerous fatalities reported. Climate change significantly impacts agricultural land suitability, potentially leading to severe food shortages and subsequent social and political conflicts. To address this pressing issue, we have developed a machine learning-based approach to predict the risk of substantial land suitability degradation and changes in irrigation patterns. Our study focuses on Central Eurasia, a region burdened with economic and social challenges.   This study represents a pioneering effort in utilizing machine learning methods to assess the impact of climate change on agricultural land suitability under various carbon emissions scenarios. Through comprehensive feature importance analysis, we unveil specific climate and terrain characteristics that exert influence on land suitability. Our approach achieves remarkable accuracy, offering policymakers invaluable insights to facilitate informed decisions aimed at averting a humanitarian crisis, including strategies such as the provision of additional water and fertilizers. This research underscores the tremendous potential of machine learning in addressing global challenges, with a particular emphasis on mitigating hunger and malnutrition.
</details>
<details>
<summary>摘要</summary>
联合国认为，提高食品安全和减少饥饿是可持续发展目标的重要组成部分。截至2021年，全球约有828万人在饥饿和营养不良的情况下生活，有多起死亡报告。气候变化对农业适应性的影响是重要的，可能导致严重的食品短缺和社会和政治冲突。为解决这一问题，我们开发了一种基于机器学习的方法，以预测气候变化对农业适应性的影响。我们的研究对中亚欧洲进行了应用，这是一个受经济和社会挑战的地区。本研究是机器学习方法在评估气候变化对农业适应性的影响方面的先驱之作。通过全面的特征重要性分析，我们揭示了气候和地形特征对适应性的影响。我们的方法具有remarkable的准确性，为政策制定人员提供了有价值的信息，以便采取积极措施，如提供额外的水和肥料，以避免人道主义危机。这项研究强调机器学习在解决全球问题方面的潜在力量，特别是减少饥饿和营养不良的挑战。
</details></li>
</ul>
<hr>
<h2 id="Neural-Collapse-in-Multi-label-Learning-with-Pick-all-label-Loss"><a href="#Neural-Collapse-in-Multi-label-Learning-with-Pick-all-label-Loss" class="headerlink" title="Neural Collapse in Multi-label Learning with Pick-all-label Loss"></a>Neural Collapse in Multi-label Learning with Pick-all-label Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15903">http://arxiv.org/abs/2310.15903</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heimine/nc_mlab">https://github.com/heimine/nc_mlab</a></li>
<li>paper_authors: Pengyu Li, Yutong Wang, Xiao Li, Qing Qu</li>
<li>for: 本文研究了深度神经网络在多类别分类任务中的彩色折衣（NC）现象，并扩展到多标签学习任务。</li>
<li>methods: 本文使用了先前的工作 restriction to multi-class classification setting中发现的NC现象的扩展，并证明了一种泛化的NC现象在多标签学习中存在。</li>
<li>results: 本文发现了一种新的 combinatorial 性，称为“标签均值”性质，其表明在多标签样本中，每个标签的特征类别均值是各个标签的特征类别均值的扩展。此外，本文也证明了全球优化结果，表明在UFM下，pick-all-label cross entropy risk的全球优化结果是唯一的。<details>
<summary>Abstract</summary>
We study deep neural networks for the multi-label classification (MLab) task through the lens of neural collapse (NC). Previous works have been restricted to the multi-class classification setting and discovered a prevalent NC phenomenon comprising of the following properties for the last-layer features: (i) the variability of features within every class collapses to zero, (ii) the set of feature means form an equi-angular tight frame (ETF), and (iii) the last layer classifiers collapse to the feature mean upon some scaling. We generalize the study to multi-label learning, and prove for the first time that a generalized NC phenomenon holds with the "pick-all-label'' formulation. Under the natural analog of the unconstrained feature model (UFM), we establish that the only global classifier of the pick-all-label cross entropy loss display the same ETF geometry which further collapse to multiplicity-1 feature class means. Besides, we discover a combinatorial property in generalized NC which is unique for multi-label learning that we call ``tag-wise average'' property, where the feature class-means of samples with multiple labels are scaled average of the feature class-means of single label tags. Theoretically, we establish global optimality result for the pick-all-label cross-entropy risk for the UFM. Additionally, We also provide empirical evidence to support our investigation into training deep neural networks on multi-label datasets, resulting in improved training efficiency.
</details>
<details>
<summary>摘要</summary>
我们研究深度神经网络对多类别分类任务（MLab）的学习过程中的神经垮坏（NC）现象。先前的研究都是对多类别分类任务进行研究，发现了一种常见的NC现象，其特征是：（i）每个类别内的特征变量归一化到零，（ii）最后层特征均值组成一个等角紧凑矩阵（ETF），以及（iii）最后层分类器归一化到特征均值。我们扩展了研究到多类别学习任务，并证明了一种普遍的NC现象在“选取所有标签”（pick-all-label）形式下存在。在自然的无约束特征模型（UFM）下，我们证明了全球分类器的pick-all-label权重概率分布具有相同的ETF几何结构，并且归一化到多重特征类别均值。此外，我们发现了多类别学习中特有的“标签均值”性质，我们称之为“标签均值”性质，即每个样本的多个标签的特征类别均值是多个标签的特征类别均值的扩展。我们 theoretically 证明了pick-all-label权重概率的全球最优性Result，并且我们还提供了实验证明，证明在训练深度神经网络在多类别数据上的效率提高。
</details></li>
</ul>
<hr>
<h2 id="Cross-feature-Contrastive-Loss-for-Decentralized-Deep-Learning-on-Heterogeneous-Data"><a href="#Cross-feature-Contrastive-Loss-for-Decentralized-Deep-Learning-on-Heterogeneous-Data" class="headerlink" title="Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data"></a>Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15890">http://arxiv.org/abs/2310.15890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aparna-aketi/cross_feature_contrastive_loss">https://github.com/aparna-aketi/cross_feature_contrastive_loss</a></li>
<li>paper_authors: Sai Aparna Aketi, Kaushik Roy</li>
<li>for: 这篇论文的目的是提出一种基于对比损失的无中心学习方法，以应对实际分布式数据中的数据不均匀性。</li>
<li>methods: 这篇论文使用的方法包括了无中心学习算法，以及基于对比损失的数据自由知识传递技术。</li>
<li>results: 实验结果显示，这篇论文的提案方法可以在各种计算机视觉数据集（CIFAR-10、CIFAR-100、时尚MINST、Imagenette、ImageNet）、模型架构（ResNet、Inception）和网络架构（FC、CNN）上显示出超过0.2-4%的提升（相较于其他现有的无中心学习方法）。<details>
<summary>Abstract</summary>
The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (0.2-4% improvement in test accuracy) compared to other existing techniques for decentralized learning on heterogeneous data.
</details>
<details>
<summary>摘要</summary>
现代的分布式学习算法大多假设数据分布是独立同分布（IID）。然而，在实际应用场景中，分布式数据集可能具有明显不同的数据分布。在这项工作中，我们提出了一种新的分布式学习方法，通过对不同特征进行数据解压缩，提高性能。特征是指一对邻居代理的特征（即模型参数之间的最后隐藏层活动）。我们通过对各种计算机视觉数据集（CIFAR-10、CIFAR-100、时尚MINST、Imagenette和ImageNet）、模型架构和网络结构进行广泛的实验，证明了我们的方法的有效性。我们的实验结果表明，我们的方法可以与其他现有的分布式学习方法相比，在不同数据分布情况下提高测试准确率（0.2-4%）。
</details></li>
</ul>
<hr>
<h2 id="State-Sequences-Prediction-via-Fourier-Transform-for-Representation-Learning"><a href="#State-Sequences-Prediction-via-Fourier-Transform-for-Representation-Learning" class="headerlink" title="State Sequences Prediction via Fourier Transform for Representation Learning"></a>State Sequences Prediction via Fourier Transform for Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15888">http://arxiv.org/abs/2310.15888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miralab-ustc/rl-spf">https://github.com/miralab-ustc/rl-spf</a></li>
<li>paper_authors: Mingxuan Ye, Yufei Kuang, Jie Wang, Rui Yang, Wengang Zhou, Houqiang Li, Feng Wu</li>
<li>for: 提高深度强化学习（RL）的数据效率，增强RL在复杂控制任务中的性能。</li>
<li>methods: 利用 Fourier 变换来提取状态序列中的下行结构信息，并通过这种信息来学习高效表示。</li>
<li>results: 实验表明，提案的方法可以在样本效率和性能两个方面超过一些现有的算法。<details>
<summary>Abstract</summary>
While deep reinforcement learning (RL) has been demonstrated effective in solving complex control tasks, sample efficiency remains a key challenge due to the large amounts of data required for remarkable performance. Existing research explores the application of representation learning for data-efficient RL, e.g., learning predictive representations by predicting long-term future states. However, many existing methods do not fully exploit the structural information inherent in sequential state signals, which can potentially improve the quality of long-term decision-making but is difficult to discern in the time domain. To tackle this problem, we propose State Sequences Prediction via Fourier Transform (SPF), a novel method that exploits the frequency domain of state sequences to extract the underlying patterns in time series data for learning expressive representations efficiently. Specifically, we theoretically analyze the existence of structural information in state sequences, which is closely related to policy performance and signal regularity, and then propose to predict the Fourier transform of infinite-step future state sequences to extract such information. One of the appealing features of SPF is that it is simple to implement while not requiring storage of infinite-step future states as prediction targets. Experiments demonstrate that the proposed method outperforms several state-of-the-art algorithms in terms of both sample efficiency and performance.
</details>
<details>
<summary>摘要</summary>
深度强化学习（RL）已经在复杂控制任务中得到了证明，但是样本效率仍然是一个关键挑战，因为需要大量数据来获得出色的性能。现有研究利用表示学习来提高数据效率的RL，例如通过预测长期未来状态来学习预测性表示。然而，许多现有方法没有充分利用状态序列中的结构信息，这可能会提高长期决策质量，但是在时间频谱中很难发现。为解决这个问题，我们提出了状态序列预测via傅里叶变换（SPF），一种新的方法，它利用状态序列的频谱频率来提取下表示的基本特征，从而学习表达式的表示。我们 theoretically 分析了状态序列中的结构信息，与策略性能和信号规律性 closely 相关，然后提议预测无穷步未来状态序列的傅里叶变换来提取这些信息。SPF 的一个吸引人的特点是它简单易行，不需要存储无穷步未来状态作为预测目标。实验表明，提案的方法在样本效率和性能两个方面都超过了一些现有算法。
</details></li>
</ul>
<hr>
<h2 id="Using-Causality-Aware-Graph-Neural-Networks-to-Predict-Temporal-Centralities-in-Dynamic-Graphs"><a href="#Using-Causality-Aware-Graph-Neural-Networks-to-Predict-Temporal-Centralities-in-Dynamic-Graphs" class="headerlink" title="Using Causality-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs"></a>Using Causality-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15865">http://arxiv.org/abs/2310.15865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franziska Heeg, Ingo Scholtes</li>
<li>for: 该论文旨在 Addressing the issue of computationally expensive path-based centrality calculation in temporal graphs, and exploring the application of De Bruijn Graph Neural Networks (DBGNN) for predicting temporal path-based centralities in time series data.</li>
<li>methods: 该论文使用了 De Bruijn Graph Neural Networks (DBGNN) 来预测 temporal 图的路径基于中心性。</li>
<li>results: 该论文在 13 个 temporal 图 dataset 上进行实验，并显示了 DBGNN 可以 considerably improve the prediction of both betweenness and closeness centrality compared to static Graph Convolutional Neural Network.<details>
<summary>Abstract</summary>
Node centralities play a pivotal role in network science, social network analysis, and recommender systems. In temporal data, static path-based centralities like closeness or betweenness can give misleading results about the true importance of nodes in a temporal graph. To address this issue, temporal generalizations of betweenness and closeness have been defined that are based on the shortest time-respecting paths between pairs of nodes. However, a major issue of those generalizations is that the calculation of such paths is computationally expensive. Addressing this issue, we study the application of De Bruijn Graph Neural Networks (DBGNN), a causality-aware graph neural network architecture, to predict temporal path-based centralities in time series data. We experimentally evaluate our approach in 13 temporal graphs from biological and social systems and show that it considerably improves the prediction of both betweenness and closeness centrality compared to a static Graph Convolutional Neural Network.
</details>
<details>
<summary>摘要</summary>
节点中心性在网络科学、社交网络分析和推荐系统中扮演着关键性的角色。在时间数据中，静态路径基于中心性如距离或中间性可能会导致 mistakenly 高估节点的重要性。为解决这个问题，我们定义了时间概念的betweenness和closeness中心性的扩展，它们基于时间尊重的最短路径 между pair of nodes。然而，这些扩展的计算具有高计算成本。为此，我们研究了使用 De Bruijn 图解决方案（DBGNN），一种具有 causality-awareness 的图解决方案，来预测时间序列数据中的时间路径基于中心性。我们对 13 个时间图从生物和社会系统进行实验，并证明我们的方法可以在预测 betweenness 和 closeness 中心性方面提高比 static 图CONvolutional Neural Network 的表现。
</details></li>
</ul>
<hr>
<h2 id="Improving-Event-Time-Prediction-by-Learning-to-Partition-the-Event-Time-Space"><a href="#Improving-Event-Time-Prediction-by-Learning-to-Partition-the-Event-Time-Space" class="headerlink" title="Improving Event Time Prediction by Learning to Partition the Event Time Space"></a>Improving Event Time Prediction by Learning to Partition the Event Time Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15853">http://arxiv.org/abs/2310.15853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Jimmy Hickey, Ricardo Henao, Daniel Wojdyla, Michael Pencina, Matthew M. Engelhard</li>
<li>for: 本研究旨在提高现有方法的存活分析方法，通过预测每个预先指定的时间间隔中事件发生的概率，以提高预测性能，特别是当数据充沛时。</li>
<li>methods: 本研究使用了一种方法，即通过学习数据中的切点来定义一个有限数量的时间间隔，以便在存活分析中进行更好的预测。</li>
<li>results: 在两个 simulated datasets 中，我们能够回归到下samples的切点，并在三个实际的观察数据集中显示了提高的预测性能。此外，我们还表明了该方法可以帮助临床决策人员更好地选择适合每个任务的时间间隔，以提高预测性能。<details>
<summary>Abstract</summary>
Recently developed survival analysis methods improve upon existing approaches by predicting the probability of event occurrence in each of a number pre-specified (discrete) time intervals. By avoiding placing strong parametric assumptions on the event density, this approach tends to improve prediction performance, particularly when data are plentiful. However, in clinical settings with limited available data, it is often preferable to judiciously partition the event time space into a limited number of intervals well suited to the prediction task at hand. In this work, we develop a method to learn from data a set of cut points defining such a partition. We show that in two simulated datasets, we are able to recover intervals that match the underlying generative model. We then demonstrate improved prediction performance on three real-world observational datasets, including a large, newly harmonized stroke risk prediction dataset. Finally, we argue that our approach facilitates clinical decision-making by suggesting time intervals that are most appropriate for each task, in the sense that they facilitate more accurate risk prediction.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:现在的生存分析方法已经超越了现有的方法，它可以预测每个预先指定的时间间隔中事件发生的概率。通过不对事件密度做强制的参数假设，这种方法在数据充沛的情况下往往会提高预测性能。然而，在医疗设置中具有有限的数据时，通常是选择judiciously事件时间空间中的一个有限数量的间隔，以适应预测任务。在这个工作中，我们开发了一种从数据中学习分割点的方法，以定义这些间隔。我们在两个随机数据集中能够回归到下面的生成模型。然后，我们在三个实际观察数据集上进行了改进预测性能。最后，我们认为我们的方法可以促进临床决策，因为它们可以为每个任务提供时间间隔，这些时间间隔可以更好地预测风险。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Temporal-Hypergraph-Neural-Network-for-Traffic-Forecasting"><a href="#Spatial-Temporal-Hypergraph-Neural-Network-for-Traffic-Forecasting" class="headerlink" title="Spatial-Temporal Hypergraph Neural Network for Traffic Forecasting"></a>Spatial-Temporal Hypergraph Neural Network for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16070">http://arxiv.org/abs/2310.16070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengzhi Yao, Zhi Li, Junbo Wang</li>
<li>for:  traffic forecasting in Intelligent Transportation Systems, to implement rich and varied transportation applications and bring convenient transportation services to people based on collected traffic data.</li>
<li>methods:  combines road network topology and traffic dynamics to capture high-order spatio-temporal dependencies in traffic data, using a spatial module and a temporal module, including an adaptive MixHop hypergraph ODE network and a hyperedge evolving ODE network.</li>
<li>results:  superior performance compared to various baselines, as demonstrated through extensive experiments conducted on four real-world traffic datasets.<details>
<summary>Abstract</summary>
Traffic forecasting, which benefits from mobile Internet development and position technologies, plays a critical role in Intelligent Transportation Systems. It helps to implement rich and varied transportation applications and bring convenient transportation services to people based on collected traffic data. Most existing methods usually leverage graph-based deep learning networks to model the complex road network for traffic forecasting shallowly. Despite their effectiveness, these methods are generally limited in fully capturing high-order spatial dependencies caused by road network topology and high-order temporal dependencies caused by traffic dynamics. To tackle the above issues, we focus on the essence of traffic system and propose STHODE: Spatio-Temporal Hypergraph Neural Ordinary Differential Equation Network, which combines road network topology and traffic dynamics to capture high-order spatio-temporal dependencies in traffic data. Technically, STHODE consists of a spatial module and a temporal module. On the one hand, we construct a spatial hypergraph and leverage an adaptive MixHop hypergraph ODE network to capture high-order spatial dependencies. On the other hand, we utilize a temporal hypergraph and employ a hyperedge evolving ODE network to capture high-order temporal dependencies. Finally, we aggregate the outputs of stacked STHODE layers to mutually enhance the prediction performance. Extensive experiments conducted on four real-world traffic datasets demonstrate the superior performance of our proposed model compared to various baselines.
</details>
<details>
<summary>摘要</summary>
traffic 预测，受移动互联网发展和位置技术启发，在智能交通系统中扮演了关键角色。它帮助实施丰富多样的交通应用程序，为人们提供了基于收集的交通数据的便捷交通服务。现有方法通常利用图形基的深度学习网络来模型复杂的公路网络，尽管它们有效，但通常只能 superficiety 捕捉公路网络的高级空间相互关系和交通动态的高级时间相互关系。为了解决以上问题，我们关注交通系统的核心，并提出了 STHODE：空间-时间多重图神经方程网络，它将公路网络和交通动态联系起来，捕捉交通数据中的高级空间-时间相互关系。技术上来说，STHODE包括空间模块和时间模块。一方面，我们构建了空间多重图，并利用自适应 MixHop 多重图 ODE 网络来捕捉高级空间相互关系。另一方面，我们利用时间多重图，并使用超过度演化 ODE 网络来捕捉高级时间相互关系。最后，我们将堆叠的 STHODE 层输出进行互相增强预测性能。我们在四个实际交通数据集上进行了广泛的实验，结果表明我们的提出的模型在对比多种基准模型时表现出色。
</details></li>
</ul>
<hr>
<h2 id="Localization-of-Small-Leakages-in-Water-Distribution-Networks-using-Concept-Drift-Explanation-Methods"><a href="#Localization-of-Small-Leakages-in-Water-Distribution-Networks-using-Concept-Drift-Explanation-Methods" class="headerlink" title="Localization of Small Leakages in Water Distribution Networks using Concept Drift Explanation Methods"></a>Localization of Small Leakages in Water Distribution Networks using Concept Drift Explanation Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15830">http://arxiv.org/abs/2310.15830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valerie Vaquet, Fabian Hinder, Kathrin Lammers, Jonas Vaquet, Barbara Hammer</li>
<li>for: 该研究旨在帮助找到和地图水 Distribution 网络中的漏斗，以减少水资源的损失。</li>
<li>methods: 该研究使用压力测量来进行漏斗检测和地图。首先，漏斗在水 Distribution 网络中被模型化为 bayesian 网络，然后通过分析系统动态来连接漏斗问题和概念漂移。</li>
<li>results: 实验结果表明，使用模型基的解释可以帮助在有限信息 circumstance 中找到漏斗。<details>
<summary>Abstract</summary>
Facing climate change the already limited availability of drinking water will decrease in the future rendering drinking water an increasingly scarce resource. Considerable amounts of it are lost through leakages in water transportation and distribution networks. Leakage detection and localization are challenging problems due to the complex interactions and changing demands in water distribution networks. Especially small leakages are hard to pinpoint yet their localization is vital to avoid water loss over long periods of time. While there exist different approaches to solving the tasks of leakage detection and localization, they are relying on various information about the system, e.g. real-time demand measurements and the precise network topology, which is an unrealistic assumption in many real-world scenarios. In contrast, this work attempts leakage localization using pressure measurements only. For this purpose, first, leakages in the water distribution network are modeled employing Bayesian networks, and the system dynamics are analyzed. We then show how the problem is connected to and can be considered through the lens of concept drift. In particular, we argue that model-based explanations of concept drift are a promising tool for localizing leakages given limited information about the network. The methodology is experimentally evaluated using realistic benchmark scenarios.
</details>
<details>
<summary>摘要</summary>
面对气候变化，未来 drinking water 的可用性将更加紧缩，这将导致 drinking water 成为一种渐渐减少的资源。现有大量 drinking water 通过水交通和分配网络的泄漏而丢失。泄漏探测和定位是一项具有挑战性的问题，因为水分配网络的复杂交互和变化的需求。特别是小泄漏难以寻址，但其localization是非常重要，以避免长时间内泄漏水。现有不同的方法来解决泄漏探测和定位问题，但它们均基于不同的信息，如实时需求测量和网络topology的精确知识，这在许多实际场景中是不可能的假设。相反，这项工作使用压力测量来进行泄漏定位。为此，我们首先使用 Bayesian 网络模型了泄漏在水分配网络中，然后分析系统动态。我们 THEN 显示了如何通过概念漂移来连接这个问题，并证明了模型基于解释是一种有 Promise 的工具来定位泄漏。该方法在实际benchmark场景下进行了实验性评估。
</details></li>
</ul>
<hr>
<h2 id="One-or-Two-Things-We-know-about-Concept-Drift-–-A-Survey-on-Monitoring-Evolving-Environments"><a href="#One-or-Two-Things-We-know-about-Concept-Drift-–-A-Survey-on-Monitoring-Evolving-Environments" class="headerlink" title="One or Two Things We know about Concept Drift – A Survey on Monitoring Evolving Environments"></a>One or Two Things We know about Concept Drift – A Survey on Monitoring Evolving Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15826">http://arxiv.org/abs/2310.15826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Hinder, Valerie Vaquet, Barbara Hammer</li>
<li>for: 本研究对于概念漂移在无监督数据流中进行了文献综述，这个Setting particularly relevant for monitoring和异常检测，这些任务和挑战在工程中直接适用。</li>
<li>methods: 本文提供了一种系统的文献综述，同时还提供了precise的数学定义，以及基于 Parametric artificial datasets的标准化实验，allowing for direct comparison of different strategies for detection and localization。</li>
<li>results: 本研究提供了一种系统的文献综述，并对不同的探测和定位策略进行了系统的分析和评价，从而为实际场景中的应用提供了指南。 Additionally, the paper discusses the emerging topic of explaining concept drift.<details>
<summary>Abstract</summary>
The world surrounding us is subject to constant change. These changes, frequently described as concept drift, influence many industrial and technical processes. As they can lead to malfunctions and other anomalous behavior, which may be safety-critical in many scenarios, detecting and analyzing concept drift is crucial. In this paper, we provide a literature review focusing on concept drift in unsupervised data streams. While many surveys focus on supervised data streams, so far, there is no work reviewing the unsupervised setting. However, this setting is of particular relevance for monitoring and anomaly detection which are directly applicable to many tasks and challenges in engineering. This survey provides a taxonomy of existing work on drift detection. Besides, it covers the current state of research on drift localization in a systematic way. In addition to providing a systematic literature review, this work provides precise mathematical definitions of the considered problems and contains standardized experiments on parametric artificial datasets allowing for a direct comparison of different strategies for detection and localization. Thereby, the suitability of different schemes can be analyzed systematically and guidelines for their usage in real-world scenarios can be provided. Finally, there is a section on the emerging topic of explaining concept drift.
</details>
<details>
<summary>摘要</summary>
世界周围的变化是不断发生的，这些变化通常被称为概念融合（concept drift），它们影响了多种工业和技术过程。由于这些变化可能会导致机器人员和其他不正常行为，因此检测和分析概念融合是非常重要的。在这篇论文中，我们提供了关于概念融合在无监督数据流中的文献综述。虽然许多调查集中了监督数据流，但是这个设置尤其关键对监测和异常检测，这些任务直接适用于许多工程应用。本文提供了概念融合检测的taxonomy，并对现有研究进行了系统性的概述。此外，本文还提供了精确的数学定义，以及参考 datasets的标准实验，以便对不同策略的检测和定位进行直接比较。因此，可以系统地分析不同方案的适用性，并提供实际应用场景中的指南。最后，文章还包括一节关于解释概念融合的新趋势。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-dimensionality-reduction-then-and-now-AIMs-for-dissipative-PDEs-in-the-ML-era"><a href="#Nonlinear-dimensionality-reduction-then-and-now-AIMs-for-dissipative-PDEs-in-the-ML-era" class="headerlink" title="Nonlinear dimensionality reduction then and now: AIMs for dissipative PDEs in the ML era"></a>Nonlinear dimensionality reduction then and now: AIMs for dissipative PDEs in the ML era</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15816">http://arxiv.org/abs/2310.15816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleni D. Koronaki, Nikolaos Evangelou, Cristina P. Martin-Linares, Edriss S. Titi, Ioannis G. Kevrekidis</li>
<li>for: 这个研究报告是关于构建分布式动力系统的减少预算模型（ROMs）的一系列纯数据驱动的工作流程。</li>
<li>methods: 这些ROMs是基于 Approximate Inertial Manifolds（AIMs）理论的数据助け模型，特别是garcia-archilla、novo和titi所提出的后处理加erkin方法。这种方法的应用范围可以扩展到不知道正确的潜在变量的情况下，使用机器学习工具来避免精度退化的问题。</li>
<li>results: 该方法可以表达ROMs为（a）理论（Fourier积分）、（b）线性数据驱动（POD模式）和&#x2F;或（c）非线性数据驱动（Diffusion Maps）坐标。此外，还描述了黑盒和（理论 Informed和数据修正的）灰盒模型，后者是因为逐个 Galerkin 投影不准确而不可以进行后处理。通过使用 Chafee-Infante 反应扩散和 Kuramoto-Sivashinsky 损失偏微分方程来证明和成功测试整个框架。<details>
<summary>Abstract</summary>
This study presents a collection of purely data-driven workflows for constructing reduced-order models (ROMs) for distributed dynamical systems. The ROMs we focus on, are data-assisted models inspired by, and templated upon, the theory of Approximate Inertial Manifolds (AIMs); the particular motivation is the so-called post-processing Galerkin method of Garcia-Archilla, Novo and Titi. Its applicability can be extended: the need for accurate truncated Galerkin projections and for deriving closed-formed corrections can be circumvented using machine learning tools. When the right latent variables are not a priori known, we illustrate how autoencoders as well as Diffusion Maps (a manifold learning scheme) can be used to discover good sets of latent variables and test their explainability. The proposed methodology can express the ROMs in terms of (a) theoretical (Fourier coefficients), (b) linear data-driven (POD modes) and/or (c) nonlinear data-driven (Diffusion Maps) coordinates. Both Black-Box and (theoretically-informed and data-corrected) Gray-Box models are described; the necessity for the latter arises when truncated Galerkin projections are so inaccurate as to not be amenable to post-processing. We use the Chafee-Infante reaction-diffusion and the Kuramoto-Sivashinsky dissipative partial differential equations to illustrate and successfully test the overall framework.
</details>
<details>
<summary>摘要</summary>
In this study, we focus on the use of machine learning tools to circumvent the need for accurate truncated Galerkin projections and to derive closed-formed corrections in the construction of ROMs. Specifically, we use autoencoders and Diffusion Maps to discover good sets of latent variables and to test their explainability. The proposed methodology is able to express the ROMs in terms of different types of coordinates, including theoretical (Fourier coefficients), linear data-driven (POD modes), and nonlinear data-driven (Diffusion Maps) coordinates. Both Black-Box and Gray-Box models are described, with the necessity for the latter arising when the truncated Galerkin projections are inaccurate and not amenable to post-processing.The study demonstrates the applicability of the proposed methodology on two examples: the Chafee-Infante reaction-diffusion equation and the Kuramoto-Sivashinsky dissipative partial differential equation. The results show that the methodology is able to successfully construct ROMs for these systems, and that the use of machine learning tools can improve the accuracy and efficiency of the ROM construction process.
</details></li>
</ul>
<hr>
<h2 id="Good-Better-Best-Self-Motivated-Imitation-Learning-for-noisy-Demonstrations"><a href="#Good-Better-Best-Self-Motivated-Imitation-Learning-for-noisy-Demonstrations" class="headerlink" title="Good Better Best: Self-Motivated Imitation Learning for noisy Demonstrations"></a>Good Better Best: Self-Motivated Imitation Learning for noisy Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15815">http://arxiv.org/abs/2310.15815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye Yuan, Xin Li, Yong Heng, Leiji Zhang, MingZhong Wang</li>
<li>for: 本文目的是探讨如何使用自我驱动的imitasion learning（SMILE）方法，排除具有低级别专家知识的示范行为，从而帮助学习者更好地学习专家示范行为。</li>
<li>methods: 本文使用了Diffusion Models的前向和反向过程，模拟示范行为中的专家知识散布，然后利用这些信息来预测示范行为中的扩散步数，并在自我驱动的方式下筛选掉不符合当前策略的示范行为。</li>
<li>results: 经验证明，SMILE方法可以在具有噪音示范行为的情况下，有效地学习专家示范行为，并且可以准确地排除不符合当前策略的示范行为。<details>
<summary>Abstract</summary>
Imitation Learning (IL) aims to discover a policy by minimizing the discrepancy between the agent's behavior and expert demonstrations. However, IL is susceptible to limitations imposed by noisy demonstrations from non-expert behaviors, presenting a significant challenge due to the lack of supplementary information to assess their expertise. In this paper, we introduce Self-Motivated Imitation LEarning (SMILE), a method capable of progressively filtering out demonstrations collected by policies deemed inferior to the current policy, eliminating the need for additional information. We utilize the forward and reverse processes of Diffusion Models to emulate the shift in demonstration expertise from low to high and vice versa, thereby extracting the noise information that diffuses expertise. Then, the noise information is leveraged to predict the diffusion steps between the current policy and demonstrators, which we theoretically demonstrate its equivalence to their expertise gap. We further explain in detail how the predicted diffusion steps are applied to filter out noisy demonstrations in a self-motivated manner and provide its theoretical grounds. Through empirical evaluations on MuJoCo tasks, we demonstrate that our method is proficient in learning the expert policy amidst noisy demonstrations, and effectively filters out demonstrations with expertise inferior to the current policy.
</details>
<details>
<summary>摘要</summary>
自适应学习（IL）目的是找到一个策略，以尽可能地减少代理者行为与专家示范之间的差异。然而，IL 受到非专家行为的示范噪声的限制，这是一个主要挑战，因为缺乏补充信息来评估他们的专业程度。在这篇论文中，我们介绍了一种名为自适应模仿学习（SMILE）的方法，可以逐渐排除由当前策略评估为非专业的示范集，不需要额外信息。我们利用了升降 diffusion 模型的前后进程，模拟示范专业程度从低到高和倒数，从而提取噪声信息。然后，我们利用这些噪声信息预测示范者与当前策略之间的扩散步骤，我们 theoretically 证明其等价于专业差距。我们进一步解释了在实际评估中如何应用预测的扩散步骤来自适应地排除噪声示范，并提供了其理论基础。通过在 MuJoCo 任务上的实际评估，我们证明了我们的方法可以在噪声示范下学习专家策略，并有效地排除不符合当前策略的示范。
</details></li>
</ul>
<hr>
<h2 id="Amortised-Inference-in-Neural-Networks-for-Small-Scale-Probabilistic-Meta-Learning"><a href="#Amortised-Inference-in-Neural-Networks-for-Small-Scale-Probabilistic-Meta-Learning" class="headerlink" title="Amortised Inference in Neural Networks for Small-Scale Probabilistic Meta-Learning"></a>Amortised Inference in Neural Networks for Small-Scale Probabilistic Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15786">http://arxiv.org/abs/2310.15786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Ashman, Tommy Rochussen, Adrian Weller</li>
<li>for: 用于实现 task-specific BNNs 的 Bayesian inference</li>
<li>methods: 使用 inducing point variational approximation，将 inducing inputs 替换为实际数据，并使用权重网络（inference network）进行权重学习</li>
<li>results: 实现了对 task-specific BNNs 的权重学习 Bayesian inference<details>
<summary>Abstract</summary>
The global inducing point variational approximation for BNNs is based on using a set of inducing inputs to construct a series of conditional distributions that accurately approximate the conditionals of the true posterior distribution. Our key insight is that these inducing inputs can be replaced by the actual data, such that the variational distribution consists of a set of approximate likelihoods for each datapoint. This structure lends itself to amortised inference, in which the parameters of each approximate likelihood are obtained by passing each datapoint through a meta-model known as the inference network. By training this inference network across related datasets, we can meta-learn Bayesian inference over task-specific BNNs.
</details>
<details>
<summary>摘要</summary>
全球启发点可变近似方法是基于使用一组启发输入来构建一系列条件分布，准确地近似真实 posterior 分布的 conditionals。我们的关键发现是这些启发输入可以被取代为实际数据，从而变量分布包含一系列的近似可能性分布，每个数据点的参数可以通过一个称为推理网络的meta-模型来获得。通过训练这个推理网络在相关的数据集上，我们可以meta-学习 bayesian推理。
</details></li>
</ul>
<hr>
<h2 id="Robust-Learning-via-Conditional-Prevalence-Adjustment"><a href="#Robust-Learning-via-Conditional-Prevalence-Adjustment" class="headerlink" title="Robust Learning via Conditional Prevalence Adjustment"></a>Robust Learning via Conditional Prevalence Adjustment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15766">http://arxiv.org/abs/2310.15766</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mnhng/CoPA">https://github.com/mnhng/CoPA</a></li>
<li>paper_authors: Minh Nguyen, Alan Q. Wang, Heejong Kim, Mert R. Sabuncu</li>
<li>for: 这个研究旨在解决健康领域资料中的潜在联合问题，该联合问题的存在可能导致深度学习模型在未见过的站点上失败。</li>
<li>methods: 这个研究提出了一种名为CoPA（Conditional Prevalence-Adjustment）的方法，该方法假设（1）生成机制是稳定的，即标签Y和偏导变量Z生成X，以及（2）每个站点E中的不稳定 conditional prevalence完全accounts for X和Y之间的不稳定相互关联。</li>
<li>results: 实验结果显示CoPA可以比基本reference beat competitive baselines。<details>
<summary>Abstract</summary>
Healthcare data often come from multiple sites in which the correlations between confounding variables can vary widely. If deep learning models exploit these unstable correlations, they might fail catastrophically in unseen sites. Although many methods have been proposed to tackle unstable correlations, each has its limitations. For example, adversarial training forces models to completely ignore unstable correlations, but doing so may lead to poor predictive performance. Other methods (e.g. Invariant risk minimization [4]) try to learn domain-invariant representations that rely only on stable associations by assuming a causal data-generating process (input X causes class label Y ). Thus, they may be ineffective for anti-causal tasks (Y causes X), which are common in computer vision. We propose a method called CoPA (Conditional Prevalence-Adjustment) for anti-causal tasks. CoPA assumes that (1) generation mechanism is stable, i.e. label Y and confounding variable(s) Z generate X, and (2) the unstable conditional prevalence in each site E fully accounts for the unstable correlations between X and Y . Our crucial observation is that confounding variables are routinely recorded in healthcare settings and the prevalence can be readily estimated, for example, from a set of (Y, Z) samples (no need for corresponding samples of X). CoPA can work even if there is a single training site, a scenario which is often overlooked by existing methods. Our experiments on synthetic and real data show CoPA beating competitive baselines.
</details>
<details>
<summary>摘要</summary>
医疗数据经常来自多个站点，其中 correlate 变量之间的相关性可能很大。如果深度学习模型利用这些不稳定相关性，它们可能在未看过的站点上失败 catastrophically。虽然许多方法已经提出来解决不稳定相关性问题，但每种方法都有其限制。例如，对抗性训练 forces 模型完全忽略不稳定相关性，但这可能导致预测性能差。其他方法（例如不变 risk minimization [4]）尝试学习域不变的表示，它们假设数据生成过程是 causal 的（输入 X 导致类别标签 Y）。因此，它们可能无效于反 causal 任务（Y 导致 X），这些任务在计算机视觉中很常见。我们提出了一种方法called CoPA（conditional Prevalence-Adjustment），这种方法适用于反 causal 任务。CoPA 假设（1）生成机制是稳定的，即标签 Y 和干扰变量 Z 生成 X，以及（2）每个站点 E 中的不稳定 conditional prevalence  completly accounts for 不稳定相关性 между X 和 Y。我们的关键观察是，干扰变量通常在医疗设置中记录，并且 prevalence 可以从 (Y, Z) 样本中Ready estimation（无需对应的 X 样本）。CoPA 可以在单个训练站点上工作，这种情况frequently overlooked by existing methods。我们在 sintetic 和实际数据上进行了实验，CoPA 可以击败竞争基线。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Single-Cell-RNA-Sequencing-with-Topological-Nonnegative-Matrix-Factorization"><a href="#Analyzing-Single-Cell-RNA-Sequencing-with-Topological-Nonnegative-Matrix-Factorization" class="headerlink" title="Analyzing Single Cell RNA Sequencing with Topological Nonnegative Matrix Factorization"></a>Analyzing Single Cell RNA Sequencing with Topological Nonnegative Matrix Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15744">http://arxiv.org/abs/2310.15744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuta Hozumi, Guo-Wei Wei</li>
<li>for: 这个论文旨在探讨单个细胞RNA sequencing（scRNA-seq）数据的统计分析方法，尤其是非正定矩阵分解（NMF）方法在scRNA-seq数据中的应用。</li>
<li>methods: 这篇论文提出了两种持续拉普拉斯regularized NMF方法，即topological NMF（TNMF）和robust topological NMF（rTNMF），并在12个数据集中进行了比较性研究，结果显示这两种方法在NMF-based方法中具有显著优势。</li>
<li>results: 研究人员通过使用TNMF和rTNMF方法对流行的Uniform Manifold Approximation and Projection（UMAP）和t-distributed stochastic neighbor embedding（t-SNE）进行了可视化，并证明了这两种方法在scRNA-seq数据中的应用可以带来更好的结果。<details>
<summary>Abstract</summary>
Single-cell RNA sequencing (scRNA-seq) is a relatively new technology that has stimulated enormous interest in statistics, data science, and computational biology due to the high dimensionality, complexity, and large scale associated with scRNA-seq data. Nonnegative matrix factorization (NMF) offers a unique approach due to its meta-gene interpretation of resulting low-dimensional components. However, NMF approaches suffer from the lack of multiscale analysis. This work introduces two persistent Laplacian regularized NMF methods, namely, topological NMF (TNMF) and robust topological NMF (rTNMF). By employing a total of 12 datasets, we demonstrate that the proposed TNMF and rTNMF significantly outperform all other NMF-based methods. We have also utilized TNMF and rTNMF for the visualization of popular Uniform Manifold Approximation and Projection (UMAP) and t-distributed stochastic neighbor embedding (t-SNE).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Diffusion-Models-for-ECG-Imputation-with-an-Augmented-Template-Prior"><a href="#Improving-Diffusion-Models-for-ECG-Imputation-with-an-Augmented-Template-Prior" class="headerlink" title="Improving Diffusion Models for ECG Imputation with an Augmented Template Prior"></a>Improving Diffusion Models for ECG Imputation with an Augmented Template Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15742">http://arxiv.org/abs/2310.15742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Jenkins, Zehua Chen, Fu Siong Ng, Danilo Mandic</li>
<li>for: 提高心电图（ECG）中缺失值的填充和预测精度，使用 probabilistic time-series models。</li>
<li>methods: 使用 template-guided denoising diffusion probabilistic model（PulseDiff），conditioned on informative prior for range of health conditions，包括个体特征和心跳水平的Variation。</li>
<li>results: 对PTBXL数据集进行实验，显示 PulseDiff 方法可以提高 DDPMs 基线模型 CSDI 和 SSSD$^{S4}$ 的性能，并且可以安全地使用医疗知识来提供先前的 priors。 合并 SSDD$^{S4}$ 模型时，PulseDiff 方法在短时间缺失数据下表现出色，与长时间缺失数据下的性能相当。<details>
<summary>Abstract</summary>
Pulsative signals such as the electrocardiogram (ECG) are extensively collected as part of routine clinical care. However, noisy and poor-quality recordings, leading to missing values, are a major issue for signals collected using mobile health systems, decreasing the signal quality and affecting the automated downstream tasks. Recent studies have explored imputation of missing values for ECG with probabilistic time-series models. Nevertheless, in comparison with the deterministic models, their performance is still limited, as the variations across subjects and heart-beat relationships are not explicitly considered in the training objective. In this work, to improve the ECG imputation and forecasting accuracy with probabilistic models, we present an template-guided denoising diffusion probabilistic model, PulseDiff, which is conditioned an informative prior for a range of health conditions. Specifically, 1) we first extract a subject-level pulsative template from the observation as an informative prior of missing values, which captures the personal characteristics; 2) we then add beat-level stochastic shift terms on the template for prior augmentation, which considers the beat-level variance of positioning and amplitude; 3) we finally design a confidence score to consider the health condition of subject, which ensures our prior is provided in a safe way. Experiments with the PTBXL dataset reveal PulseDiff improves the performance of two strong DDPMs baseline models, CSDI and SSSD$^{S4}$, verifying our method guides the generation of DDPMs while managing the uncertainty. When combining with SSSD$^{S4}$, our PulseDiff method outperforms the leading deterministic model for short-interval missing data and is comparable for long-interval data loss.
</details>
<details>
<summary>摘要</summary>
脉冲信号如电子心脉agram（ECG）在 Routine 临床护理中广泛采集。然而，手持设备采集的信号具有噪声和低质量，导致数据损失，这会降低信号质量并影响下游任务的自动化。Recent studies  explored imputation of missing values for ECG with probabilistic time-series models. However, their performance is still limited, as they do not explicitly consider the variations across subjects and heart-beat relationships in the training objective. In this work, we aim to improve the ECG imputation and forecasting accuracy with probabilistic models by presenting a template-guided denoising diffusion probabilistic model, PulseDiff, which is conditioned on an informative prior for a range of health conditions. Specifically, we first extract a subject-level pulsative template from the observation as an informative prior of missing values, which captures the personal characteristics; then, we add beat-level stochastic shift terms on the template for prior augmentation, which considers the beat-level variance of positioning and amplitude; finally, we design a confidence score to consider the health condition of the subject, which ensures our prior is provided in a safe way. Experiments with the PTBXL dataset reveal that PulseDiff improves the performance of two strong DDPMs baseline models, CSDI and SSSD$^{S4}$, verifying our method guides the generation of DDPMs while managing the uncertainty. When combining with SSSD$^{S4}$, our PulseDiff method outperforms the leading deterministic model for short-interval missing data and is comparable for long-interval data loss.
</details></li>
</ul>
<hr>
<h2 id="Causal-Representation-Learning-Made-Identifiable-by-Grouping-of-Observational-Variables"><a href="#Causal-Representation-Learning-Made-Identifiable-by-Grouping-of-Observational-Variables" class="headerlink" title="Causal Representation Learning Made Identifiable by Grouping of Observational Variables"></a>Causal Representation Learning Made Identifiable by Grouping of Observational Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15709">http://arxiv.org/abs/2310.15709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiroshi Morioka, Aapo Hyvärinen</li>
<li>for: The paper is written for learning a causal model for hidden features in a data-driven manner, specifically addressing the problem of Causal Representation Learning (CRL) which is ill-posed and difficult to identify.</li>
<li>methods: The paper proposes a novel approach to CRL that is based on weak constraints and does not require temporal structure, intervention, or weak supervision. The approach uses assuming the observational mixing exhibits a suitable grouping of the observational variables.</li>
<li>results: The paper shows that the proposed approach is statistically consistent and experimentally demonstrates superior CRL performances compared to state-of-the-art baselines. Additionally, the paper demonstrates the robustness of the approach against latent confounders and causal cycles.<details>
<summary>Abstract</summary>
A topic of great current interest is Causal Representation Learning (CRL), whose goal is to learn a causal model for hidden features in a data-driven manner. Unfortunately, CRL is severely ill-posed since it is a combination of the two notoriously ill-posed problems of representation learning and causal discovery. Yet, finding practical identifiability conditions that guarantee a unique solution is crucial for its practical applicability. Most approaches so far have been based on assumptions on the latent causal mechanisms, such as temporal causality, or existence of supervision or interventions; these can be too restrictive in actual applications. Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision. The approach is based assuming the observational mixing exhibits a suitable grouping of the observational variables. We also propose a novel self-supervised estimation framework consistent with the model, prove its statistical consistency, and experimentally show its superior CRL performances compared to the state-of-the-art baselines. We further demonstrate its robustness against latent confounders and causal cycles.
</details>
<details>
<summary>摘要</summary>
Currently, a topic of great interest is Causal Representation Learning (CRL), which aims to learn a causal model for hidden features in a data-driven manner. However, CRL is severely ill-posed because it combines the two notoriously ill-posed problems of representation learning and causal discovery. Finding practical identifiability conditions that guarantee a unique solution is crucial for its practical applicability. Most existing approaches rely on assumptions about the latent causal mechanisms, such as temporal causality or the existence of supervision or interventions, which can be too restrictive in real-world applications.In this study, we propose a novel approach to identifiability based on weak constraints that do not require temporal structure, intervention, or weak supervision. The approach is based on the assumption that the observational mixing exhibits a suitable grouping of the observational variables. We also propose a novel self-supervised estimation framework that is consistent with the model, prove its statistical consistency, and experimentally show its superior CRL performance compared to state-of-the-art baselines. Furthermore, we demonstrate its robustness against latent confounders and causal cycles.
</details></li>
</ul>
<hr>
<h2 id="Fixed-Budget-Real-Valued-Combinatorial-Pure-Exploration-of-Multi-Armed-Bandit"><a href="#Fixed-Budget-Real-Valued-Combinatorial-Pure-Exploration-of-Multi-Armed-Bandit" class="headerlink" title="Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit"></a>Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15681">http://arxiv.org/abs/2310.15681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shintaro Nakamura, Masashi Sugiyama</li>
<li>for: 这个论文是关于实值 combinatorial 纯探索多臂抓拍机的 fixed-budget 设定下的研究。</li>
<li>methods: 论文首先介绍了 Combinatorial Successive Assignment (CSA) 算法，这是第一个可以在 exponentially large 的动作类型下标识最佳动作的算法。 然后，论文还介绍了另一种名为 Minimax Combinatorial Successive Accepts and Rejects (Minimax-CombSAR) 算法，用于对动作类型的大小为 polynomial 的情况，并证明其是最佳的。</li>
<li>results: 论文通过对先前方法进行实验比较，显示了自己的算法的优越性。<details>
<summary>Abstract</summary>
We study the real-valued combinatorial pure exploration of the multi-armed bandit in the fixed-budget setting. We first introduce the Combinatorial Successive Asign (CSA) algorithm, which is the first algorithm that can identify the best action even when the size of the action class is exponentially large with respect to the number of arms. We show that the upper bound of the probability of error of the CSA algorithm matches a lower bound up to a logarithmic factor in the exponent. Then, we introduce another algorithm named the Minimax Combinatorial Successive Accepts and Rejects (Minimax-CombSAR) algorithm for the case where the size of the action class is polynomial, and show that it is optimal, which matches a lower bound. Finally, we experimentally compare the algorithms with previous methods and show that our algorithm performs better.
</details>
<details>
<summary>摘要</summary>
我们研究了实数值的 combinatorial 纯查探多枪仓设定下的多枪仓问题。我们首先介绍了 Combinatorial Successive Assignment（CSA）算法，这是第一个可以确定最佳动作，即使动作类型数量为对数几何函数而言是非常大的算法。我们表明了 CSA 算法的上界错误概率与下界差分Logarithmic factor。然后，我们介绍了另一种名为 Minimax Combinatorial Successive Accepts and Rejects（Minimax-CombSAR）算法，用于情况下动作类型数量是 polynomial，并证明其是优化的，与下界匹配。最后，我们对 previous methods 进行了实验比较，并证明我们的算法表现更好。Note: "多枪仓" (dà qiàng chuī) is a Chinese term that refers to the multi-armed bandit problem.
</details></li>
</ul>
<hr>
<h2 id="Interactive-Generalized-Additive-Model-and-Its-Applications-in-Electric-Load-Forecasting"><a href="#Interactive-Generalized-Additive-Model-and-Its-Applications-in-Electric-Load-Forecasting" class="headerlink" title="Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting"></a>Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15662">http://arxiv.org/abs/2310.15662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linxiao Yang, Rui Ren, Xinyue Gu, Liang Sun</li>
<li>for: 预测电力负荷，帮助电力系统规划和管理。</li>
<li>methods: 我们提出了一种可互动的 generalized additive model（GAM），利用划分式线性函数，可以在限量数据或缺乏数据情况下提高预测性能。</li>
<li>results: 在公共benchmark和电力数据集上，我们的可互动GAM比现有状态对照方法高效，并且在极端天气事件情况下也表现良好。我们已经将这种模型集成到我们的eforecaster产品中，并创建了一个User-friendly的Web应用程序。<details>
<summary>Abstract</summary>
Electric load forecasting is an indispensable component of electric power system planning and management. Inaccurate load forecasting may lead to the threat of outages or a waste of energy. Accurate electric load forecasting is challenging when there is limited data or even no data, such as load forecasting in holiday, or under extreme weather conditions. As high-stakes decision-making usually follows after load forecasting, model interpretability is crucial for the adoption of forecasting models. In this paper, we propose an interactive GAM which is not only interpretable but also can incorporate specific domain knowledge in electric power industry for improved performance. This boosting-based GAM leverages piecewise linear functions and can be learned through our efficient algorithm. In both public benchmark and electricity datasets, our interactive GAM outperforms current state-of-the-art methods and demonstrates good generalization ability in the cases of extreme weather events. We launched a user-friendly web-based tool based on interactive GAM and already incorporated it into our eForecaster product, a unified AI platform for electricity forecasting.
</details>
<details>
<summary>摘要</summary>
电力负荷预测是电力系统规划和管理中不可或缺的一部分。不准确的电力预测可能会导致供电停顺或能源浪费。正确的电力预测在有限数据或缺乏数据情况下是挑战。例如，在假日或极端天气情况下， load forecasting 难以准确。因为高飞度决策通常会随后进行load forecasting， therefore model interpretability是预测模型的采用的关键因素。在这篇论文中，我们提出了一种可交互的Generalized Additive Model（GAM），不仅可 interpretability，而且可以包含特定领域知识以提高性能。这种boosting-based GAM使用分割线性函数，可以通过我们的效果algorithm learn。在公共 benchmark和电力数据集上，我们的交互式 GAM 超过当前状态艺术方法，并在极端天气事件情况下展现了良好的总体化能力。我们已经将交互式 GAM 集成到我们的 eForecaster 产品中，这是一个统一的 AI 平台 для电力预测。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Traffic-Prediction-with-Learnable-Filter-Module"><a href="#Enhancing-Traffic-Prediction-with-Learnable-Filter-Module" class="headerlink" title="Enhancing Traffic Prediction with Learnable Filter Module"></a>Enhancing Traffic Prediction with Learnable Filter Module</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16063">http://arxiv.org/abs/2310.16063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanshao Zhu, Yongchao Ye, Xiangyu Zhao, James J. Q. Yu</li>
<li>for: 提高交通预测模型的准确率， addresses the challenge of modeling future traffic conditions by proposing a learnable filter module to adaptively filter out noise in traffic data.</li>
<li>methods: 使用 Fourier 变换将数据转移到频域，并基于频谱特征对噪声进行过滤。然后使用反 Fourier 变换将过滤后的数据转换回时域。</li>
<li>results: 实验结果表明，提出的模块可以有效地 Mitigate 噪声，提高交通预测性能。<details>
<summary>Abstract</summary>
Modeling future traffic conditions often relies heavily on complex spatial-temporal neural networks to capture spatial and temporal correlations, which can overlook the inherent noise in the data. This noise, often manifesting as unexpected short-term peaks or drops in traffic observation, is typically caused by traffic accidents or inherent sensor vibration. In practice, such noise can be challenging to model due to its stochastic nature and can lead to overfitting risks if a neural network is designed to learn this behavior. To address this issue, we propose a learnable filter module to filter out noise in traffic data adaptively. This module leverages the Fourier transform to convert the data to the frequency domain, where noise is filtered based on its pattern. The denoised data is then recovered to the time domain using the inverse Fourier transform. Our approach focuses on enhancing the quality of the input data for traffic prediction models, which is a critical yet often overlooked aspect in the field. We demonstrate that the proposed module is lightweight, easy to integrate with existing models, and can significantly improve traffic prediction performance. Furthermore, we validate our approach with extensive experimental results on real-world datasets, showing that it effectively mitigates noise and enhances prediction accuracy.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)模型未来交通情况经常利用复杂的空间-时间神经网络来捕捉空间和时间相关性，这可能会忽略数据中的随机噪声。这种噪声，通常表现为交通事故或感知器振荡所引起的意外短期峰值或下降，会导致模型过拟合。为解决这问题，我们提出了一个可学习的筛波模块，可以适应性地筛除交通数据中的噪声。这个模块利用快推转换数据到频率域，然后根据噪声的模式进行筛选。筛选后的数据再用逆快推转换回时间域。我们的方法注重提高交通预测模型的输入数据质量，这是预测领域中很重要的一点，尤其是在实际应用中。我们的实验结果表明，我们的方法可以减少噪声，提高预测精度。
</details></li>
</ul>
<hr>
<h2 id="Momentum-Gradient-based-Untargeted-Attack-on-Hypergraph-Neural-Networks"><a href="#Momentum-Gradient-based-Untargeted-Attack-on-Hypergraph-Neural-Networks" class="headerlink" title="Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks"></a>Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15656">http://arxiv.org/abs/2310.15656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Chen, Stjepan Picek, Zhonglin Ye, Zhaoyang Wang, Haixing Zhao</li>
<li>For: 这个论文的目的是研究隐藏图论文模型（HGNNs）对于隐藏图相关任务的应用。由于HGNNs具有高阶表示能力，因此它们在各种任务上表现出色。然而，深度学习模型在攻击下可能会受到影响，而大多数研究都集中在图神经网络（GNNs）上，对于HGNNs的攻击研究还很少。这篇论文试图填补这个空白。* Methods: 这篇论文提出了一种新的HGNNs攻击模型，称为MGHGA，用于无目标攻击。MGHGA的核心思想是通过修改节点特征来攻击HGNNs。具体来说，MGHGA包括两个部分：特征选择和特征修改。在特征选择模块中，我们使用了沃尔tz势力Gradient机制来选择攻击节点的特征。在特征修改模块中，我们使用了两种特征生成方法（直接修改和签号Gradient）来使MGHGA适用于离散和连续数据集。* Results: 我们在五个benchmark数据集上进行了广泛的实验，以验证MGHGA在节点和视对象分类任务中的攻击性能。结果显示，MGHGA在比基线模型高2%的平均提高了性能。<details>
<summary>Abstract</summary>
Hypergraph Neural Networks (HGNNs) have been successfully applied in various hypergraph-related tasks due to their excellent higher-order representation capabilities. Recent works have shown that deep learning models are vulnerable to adversarial attacks. Most studies on graph adversarial attacks have focused on Graph Neural Networks (GNNs), and the study of adversarial attacks on HGNNs remains largely unexplored. In this paper, we try to reduce this gap. We design a new HGNNs attack model for the untargeted attack, namely MGHGA, which focuses on modifying node features. We consider the process of HGNNs training and use a surrogate model to implement the attack before hypergraph modeling. Specifically, MGHGA consists of two parts: feature selection and feature modification. We use a momentum gradient mechanism to choose the attack node features in the feature selection module. In the feature modification module, we use two feature generation approaches (direct modification and sign gradient) to enable MGHGA to be employed on discrete and continuous datasets. We conduct extensive experiments on five benchmark datasets to validate the attack performance of MGHGA in the node and the visual object classification tasks. The results show that MGHGA improves performance by an average of 2% compared to the than the baselines.
</details>
<details>
<summary>摘要</summary>
希PEREP Neural Networks (HGNNs) 已经成功应用于多个几何グラフ関系的任务中，因为它具有出色的高阶表现能力。 recent works have shown that deep learning models are vulnerable to adversarial attacks. most studies on graph adversarial attacks have focused on Graph Neural Networks (GNNs), and the study of adversarial attacks on HGNNs remains largely unexplored. in this paper, we try to reduce this gap. we design a new HGNNs attack model for the untargeted attack, namely MGHGA, which focuses on modifying node features. we consider the process of HGNNs training and use a surrogate model to implement the attack before hypergraph modeling. specifically, MGHGA consists of two parts: feature selection and feature modification. we use a momentum gradient mechanism to choose the attack node features in the feature selection module. in the feature modification module, we use two feature generation approaches (direct modification and sign gradient) to enable MGHGA to be employed on discrete and continuous datasets. we conduct extensive experiments on five benchmark datasets to validate the attack performance of MGHGA in the node and the visual object classification tasks. the results show that MGHGA improves performance by an average of 2% compared to the baselines.
</details></li>
</ul>
<hr>
<h2 id="Deceptive-Fairness-Attacks-on-Graphs-via-Meta-Learning"><a href="#Deceptive-Fairness-Attacks-on-Graphs-via-Meta-Learning" class="headerlink" title="Deceptive Fairness Attacks on Graphs via Meta Learning"></a>Deceptive Fairness Attacks on Graphs via Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15653">http://arxiv.org/abs/2310.15653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiank2/fate">https://github.com/jiank2/fate</a></li>
<li>paper_authors: Jian Kang, Yinglong Xia, Ross Maciejewski, Jiebo Luo, Hanghang Tong</li>
<li>for: 本研究旨在Answering the question of how to launch poisoning attacks on graph learning models to exacerbate bias in a deceptive manner.</li>
<li>methods: 本研究使用bi-level优化问题和meta学习基础结构（FATE）来实现攻击。FATE 可以应用于不同的公平定义和图学习模型，以及任意的操作修改。</li>
<li>results: 实验结果表明，FATE 可以在真实世界数据集上增强图神经网络的偏见，无论是否考虑公平性。此外，FATE 还可以维持下游任务的有用性。本研究提供了对不公正公平图学习的抗 adversarial 性的新的理解，并可能为未来的研究提供指导。<details>
<summary>Abstract</summary>
We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity and individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust and fair graph learning in future studies.
</details>
<details>
<summary>摘要</summary>
我们研究欺骗性公平攻击图进行回答：如何通过欺骗攻击图学模型，扩大偏见？我们通过二级优化问题回答这个问题，并提出了一种基于元学习的框架名为FATE。FATE在不同的公平定义和图学模型之间都是广泛应用的，同时还可以针对各种杂化操作进行配置。我们进一步实现FATE来攻击统计均衡和个人公平在图神经网络上。我们在实际世界数据集上进行了广泛的实验评估，结果表明FATE可以在不考虑公平情况下或者考虑公平情况下增强图神经网络的偏见，同时保持下游任务的实用性。我们希望这篇论文可以提供关于公平 graph learning 的抗攻击性和未来研究的灵感。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Convolutional-Neural-Networks-as-Efficient-Pre-trained-Audio-Models"><a href="#Dynamic-Convolutional-Neural-Networks-as-Efficient-Pre-trained-Audio-Models" class="headerlink" title="Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio Models"></a>Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15648">http://arxiv.org/abs/2310.15648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fschmid56/efficientat">https://github.com/fschmid56/efficientat</a></li>
<li>paper_authors: Florian Schmid, Khaled Koutini, Gerhard Widmer</li>
<li>for: 这个论文的目的是提高大规模的音频数据集上的音频标签任务的效果，并且比对传统的卷积神经网络（CNN）和变换器（Transformer）的性能。</li>
<li>methods: 该论文使用了知识填充（Knowledge Distillation）技术，将Transformer知识填充到有限的效率卷积神经网络（efficient CNN）中，以提高其性能。此外，该论文还引入了动态卷积块（dynamic convolutions）、非线性（non-linearities）和注意机制（attention mechanisms），以提高效率卷积神经网络的性能。</li>
<li>results: 实验结果表明，引入的动态卷积块和非线性等技术可以提高效率卷积神经网络的性能，并且在AudioSet和多个下游任务上达到或超过Transformer的性能。此外，该论文还发现，动态卷积块和非线性等技术可以提高下游任务的灵活性和泛化能力。<details>
<summary>Abstract</summary>
The introduction of large-scale audio datasets, such as AudioSet, paved the way for Transformers to conquer the audio domain and replace CNNs as the state-of-the-art neural network architecture for many tasks. Audio Spectrogram Transformers are excellent at exploiting large datasets, creating powerful pre-trained models that surpass CNNs when fine-tuned on downstream tasks. However, current popular Audio Spectrogram Transformers are demanding in terms of computational complexity compared to CNNs. Recently, we have shown that, by employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch up with and even outperform Transformers on large datasets. In this work, we extend this line of research and increase the capacity of efficient CNNs by introducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic convolutions and attention mechanisms. We show that these dynamic CNNs outperform traditional efficient CNNs, in terms of the performance-complexity trade-off and parameter efficiency, at the task of audio tagging on the large-scale AudioSet. Our experiments further indicate that the introduced dynamic CNNs achieve better performance on downstream tasks and scale up well, attaining Transformer performance and even outperforming them on AudioSet and several downstream tasks.
</details>
<details>
<summary>摘要</summary>
大量的声音数据集，如AudioSet，开创了Transformer在声音领域的统治之路，取代了CNN作为下游任务的状态态程最佳神经网络架构。声音柱图Transformer具有大量数据集的利用能力，创造了强大的预训练模型，在下游任务上精度训练时超过CNN。然而，当前流行的声音柱图Transformer在计算复杂性方面较高，与CNN相比。在最近的研究中，我们发现，通过Transformer-to-CNN知识储存学习，可以使得高效的CNNcatch up与和超过Transformers在大规模数据集上。在这项工作中，我们扩展这一研究，提高高效CNN的容量，通过引入动态非线性、动态核算和注意机制来构建动态CNN块。我们示出，这些动态CNN可以在性能和复杂度之间取得更好的平衡点，并且在AudioSet和多个下游任务上达到或超过Transformer的性能。我们的实验还表明，引入的动态CNN可以在更多的下游任务上进行扩展，并且可以在AudioSet和多个下游任务上保持稳定的性能。
</details></li>
</ul>
<hr>
<h2 id="Light-up-that-Droid-On-the-Effectiveness-of-Static-Analysis-Features-against-App-Obfuscation-for-Android-Malware-Detection"><a href="#Light-up-that-Droid-On-the-Effectiveness-of-Static-Analysis-Features-against-App-Obfuscation-for-Android-Malware-Detection" class="headerlink" title="Light up that Droid! On the Effectiveness of Static Analysis Features against App Obfuscation for Android Malware Detection"></a>Light up that Droid! On the Effectiveness of Static Analysis Features against App Obfuscation for Android Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15645">http://arxiv.org/abs/2310.15645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Borja Molina-Coronado, Antonio Ruggia, Usue Mori, Alessio Merlo, Alexander Mendiburu, Jose Miguel-Alonso</li>
<li>for: 本研究旨在探讨针对Android平台的Machine Learning（ML）恶意软件检测器是否能够抵抗增强难以理解的软件（obfuscation）的影响。</li>
<li>methods: 本研究使用了多种常见的静态分析特征，包括代码字串、API调用记录、函数名称和参数等，并使用了多种静态分析工具来检测和分析软件的增强难以理解。</li>
<li>results: 研究发现，增强难以理解可以对静态分析特征产生一定的影响，但是certain features still retain their validity for ML malware detection even in the presence of obfuscation。基于这些发现，本研究提出了一种robust against obfuscation的ML恶意软件检测器，并比当前状态的检测器表现更高。<details>
<summary>Abstract</summary>
Malware authors have seen obfuscation as the mean to bypass malware detectors based on static analysis features. For Android, several studies have confirmed that many anti-malware products are easily evaded with simple program transformations. As opposed to these works, ML detection proposals for Android leveraging static analysis features have also been proposed as obfuscation-resilient. Therefore, it needs to be determined to what extent the use of a specific obfuscation strategy or tool poses a risk for the validity of ML malware detectors for Android based on static analysis features. To shed some light in this regard, in this article we assess the impact of specific obfuscation techniques on common features extracted using static analysis and determine whether the changes are significant enough to undermine the effectiveness of ML malware detectors that rely on these features. The experimental results suggest that obfuscation techniques affect all static analysis features to varying degrees across different tools. However, certain features retain their validity for ML malware detection even in the presence of obfuscation. Based on these findings, we propose a ML malware detector for Android that is robust against obfuscation and outperforms current state-of-the-art detectors.
</details>
<details>
<summary>摘要</summary>
malware作者们看到了隐藏为了绕过基于静态分析特征的反毒软件。 Android 上有许多研究证明了许多反毒软件可以轻松地被隐藏。相比之下， ML 检测建议 для Android  leveraging 静态分析特征也已经被提出侔避免这种情况。因此，需要决定使用特定隐藏策略或工具对 Android 基于静态分析特征的 ML 毒软件检测器的有效性对确。为了解释这问题，这篇文章评估了具体隐藏技术对常用的静态分析特征的影响，并决定这些变化是否足以导致 ML 毒软件检测器的无效性。实验结果表明隐藏技术对不同工具的静态分析特征产生了不同的影响，但certain 特征在隐藏情况下仍然保持其有效性。基于这些发现，我们提出了一个robust  against 隐藏的 ML 毒软件检测器，并与现有的州�� ell-of-the-art 检测器进行比较。
</details></li>
</ul>
<hr>
<h2 id="Guaranteed-Coverage-Prediction-Intervals-with-Gaussian-Process-Regression"><a href="#Guaranteed-Coverage-Prediction-Intervals-with-Gaussian-Process-Regression" class="headerlink" title="Guaranteed Coverage Prediction Intervals with Gaussian Process Regression"></a>Guaranteed Coverage Prediction Intervals with Gaussian Process Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15641">http://arxiv.org/abs/2310.15641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harris Papadopoulos</li>
<li>for: 提高 Gaussian Process Regression（GPR）中的预测不确定性估计的准确性，以便更好地评估模型的性能。</li>
<li>methods: 基于 Conformal Prediction（CP）机器学习框架，对 GPR 进行扩展，以 garantizar 预测interval的覆盖率达到所需的水平，即使模型是完全错误的。</li>
<li>results: 在实验中，提出的方法比现有方法更为有效，可以更好地评估模型的性能。<details>
<summary>Abstract</summary>
Gaussian Process Regression (GPR) is a popular regression method, which unlike most Machine Learning techniques, provides estimates of uncertainty for its predictions. These uncertainty estimates however, are based on the assumption that the model is well-specified, an assumption that is violated in most practical applications, since the required knowledge is rarely available. As a result, the produced uncertainty estimates can become very misleading; for example the prediction intervals (PIs) produced for the 95\% confidence level may cover much less than 95\% of the true labels. To address this issue, this paper introduces an extension of GPR based on a Machine Learning framework called, Conformal Prediction (CP). This extension guarantees the production of PIs with the required coverage even when the model is completely misspecified. The proposed approach combines the advantages of GPR with the valid coverage guarantee of CP, while the performed experimental results demonstrate its superiority over existing methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Contextual-directed-acyclic-graphs"><a href="#Contextual-directed-acyclic-graphs" class="headerlink" title="Contextual directed acyclic graphs"></a>Contextual directed acyclic graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15627">http://arxiv.org/abs/2310.15627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ryan-thompson/contextualdag.jl">https://github.com/ryan-thompson/contextualdag.jl</a></li>
<li>paper_authors: Ryan Thompson, Edwin V. Bonilla, Robert Kohn</li>
<li>for: 本研究旨在解决由观测数据提供的导向无环图（DAG）结构估计问题，具体是在各个个体基础上的图结构异同。</li>
<li>methods: 本研究使用神经网络来映射各个个体的Contextual特征到一个权重adjacency矩阵表示的DAG结构。神经网络具有一个新的投影层，使输出矩阵 sparse并满足最近发展的异环性特征。</li>
<li>results: 我们的实验表明，新的方法可以成功地回归真实的上下文特定的图结构，而existingsapproaches失败。<details>
<summary>Abstract</summary>
Estimating the structure of directed acyclic graphs (DAGs) from observational data remains a significant challenge in machine learning. Most research in this area concentrates on learning a single DAG for the entire population. This paper considers an alternative setting where the graph structure varies across individuals based on available "contextual" features. We tackle this contextual DAG problem via a neural network that maps the contextual features to a DAG, represented as a weighted adjacency matrix. The neural network is equipped with a novel projection layer that ensures the output matrices are sparse and satisfy a recently developed characterization of acyclicity. We devise a scalable computational framework for learning contextual DAGs and provide a convergence guarantee and an analytical gradient for backpropagating through the projection layer. Our experiments suggest that the new approach can recover the true context-specific graph where existing approaches fail.
</details>
<details>
<summary>摘要</summary>
estimate  directed acyclic graphs (DAGs) from observational data remains a significant challenge in machine learning. Most research in this area concentrates on learning a single DAG for the entire population. This paper considers an alternative setting where the graph structure varies across individuals based on available "contextual" features. We tackle this contextual DAG problem via a neural network that maps the contextual features to a DAG, represented as a weighted adjacency matrix. The neural network is equipped with a novel projection layer that ensures the output matrices are sparse and satisfy a recently developed characterization of acyclicity. We devise a scalable computational framework for learning contextual DAGs and provide a convergence guarantee and an analytical gradient for backpropagating through the projection layer. Our experiments suggest that the new approach can recover the true context-specific graph where existing approaches fail.Here's the text with some additional information about the translation:I used Google Translate to translate the text into Simplified Chinese. However, I noticed that the translation did not capture some of the technical terms and phrases used in the original text. Therefore, I made some adjustments to the translation to ensure that the meaning and context of the text are preserved.In particular, I replaced "directed acyclic graphs" with "指向无环图" (which is the literal translation of "directed acyclic graphs" in Simplified Chinese), and I replaced "contextual features" with "上下文特征" (which is a more common term used in machine learning to refer to features that are specific to a particular context or population). I also replaced "acyclic" with "无环" (which is the literal translation of "acyclic" in Simplified Chinese), and I added the word "特定" (which means "specific" or "context-specific") to the phrase "context-specific graph" to emphasize that the graph structure varies across individuals based on their specific context.Overall, I hope that the translation is helpful and accurate, and I apologize for any errors or inaccuracies that may remain.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Split-Federated-Learning-over-Wireless-Communication-Networks"><a href="#Accelerating-Split-Federated-Learning-over-Wireless-Communication-Networks" class="headerlink" title="Accelerating Split Federated Learning over Wireless Communication Networks"></a>Accelerating Split Federated Learning over Wireless Communication Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15584">http://arxiv.org/abs/2310.15584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ce Xu, Jinxuan Li, Yuan Liu, Yushi Ling, Miaowen Wen</li>
<li>for: 这篇论文旨在提高深度神经网络（DNN）在内存限制的Edge设备上的应用，但DNN的参数数量和计算复杂度使得它实现困难。</li>
<li>methods: 这篇论文提出了一种模型分割&#x2F;拆分的方法，将DNN分成两部分，分别在设备和服务器上进行培训或推导。</li>
<li>results: 论文的实验结果显示，这种方法可以将系统延迟降至最低，同时提高准确性。<details>
<summary>Abstract</summary>
The development of artificial intelligence (AI) provides opportunities for the promotion of deep neural network (DNN)-based applications. However, the large amount of parameters and computational complexity of DNN makes it difficult to deploy it on edge devices which are resource-constrained. An efficient method to address this challenge is model partition/splitting, in which DNN is divided into two parts which are deployed on device and server respectively for co-training or co-inference. In this paper, we consider a split federated learning (SFL) framework that combines the parallel model training mechanism of federated learning (FL) and the model splitting structure of split learning (SL). We consider a practical scenario of heterogeneous devices with individual split points of DNN. We formulate a joint problem of split point selection and bandwidth allocation to minimize the system latency. By using alternating optimization, we decompose the problem into two sub-problems and solve them optimally. Experiment results demonstrate the superiority of our work in latency reduction and accuracy improvement.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的发展提供了深度神经网络（DNN）基于应用的推广机会。然而，DNN的参数量和计算复杂性使其在具有限制的边缘设备上部署困难。一种有效的方法是模型分割/拆分，其将DNN分成两部分，一部在设备上部署，另一部在服务器上部署，以进行合作训练或合作推理。在这篇论文中，我们考虑了一个分布式学习（FL）框架，该框架结合了分布式学习的并行训练机制和拆分学习（SL）的模型结构。我们考虑了一个实际的异构设备场景，其中每个设备有自己的拆分点。我们形式化了一个分布式系统延迟最小化问题，并使用交叉优化法解决这个问题。实验结果表明，我们的方法在延迟减少和准确率提高方面具有优势。
</details></li>
</ul>
<hr>
<h2 id="Identifiable-Latent-Polynomial-Causal-Models-Through-the-Lens-of-Change"><a href="#Identifiable-Latent-Polynomial-Causal-Models-Through-the-Lens-of-Change" class="headerlink" title="Identifiable Latent Polynomial Causal Models Through the Lens of Change"></a>Identifiable Latent Polynomial Causal Models Through the Lens of Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15580">http://arxiv.org/abs/2310.15580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi</li>
<li>for: 本研究旨在探讨 causal representation learning 如何揭示隐藏的高级 causal 表示，并提供可靠的确认方法。</li>
<li>methods: 本文提出了一种基于 latent causal variable 的变化分析方法，以确保 causal 模型的可靠性。</li>
<li>results: 本研究发现了一种扩展 latent causal models 的方法，包括非线性 causal 关系和不同的噪音分布。此外，本文还提出了一种新的 empirical estimation 方法，并通过实验验证了其理论成果。<details>
<summary>Abstract</summary>
Causal representation learning aims to unveil latent high-level causal representations from observed low-level data. One of its primary tasks is to provide reliable assurance of identifying these latent causal models, known as identifiability. A recent breakthrough explores identifiability by leveraging the change of causal influences among latent causal variables across multiple environments \citep{liu2022identifying}. However, this progress rests on the assumption that the causal relationships among latent causal variables adhere strictly to linear Gaussian models. In this paper, we extend the scope of latent causal models to involve nonlinear causal relationships, represented by polynomial models, and general noise distributions conforming to the exponential family. Additionally, we investigate the necessity of imposing changes on all causal parameters and present partial identifiability results when part of them remains unchanged. Further, we propose a novel empirical estimation method, grounded in our theoretical finding, that enables learning consistent latent causal representations. Our experimental results, obtained from both synthetic and real-world data, validate our theoretical contributions concerning identifiability and consistency.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: causal representation learning aims to reveal latent high-level causal representations from observed low-level data. One of its primary tasks is to provide reliable assurance of identifying these latent causal models, known as identifiability. A recent breakthrough explores identifiability by leveraging the change of causal influences among latent causal variables across multiple environments. However, this progress rests on the assumption that the causal relationships among latent causal variables adhere strictly to linear Gaussian models. In this paper, we extend the scope of latent causal models to involve nonlinear causal relationships, represented by polynomial models, and general noise distributions conforming to the exponential family. Additionally, we investigate the necessity of imposing changes on all causal parameters and present partial identifiability results when part of them remains unchanged. Further, we propose a novel empirical estimation method, grounded in our theoretical finding, that enables learning consistent latent causal representations. Our experimental results, obtained from both synthetic and real-world data, validate our theoretical contributions concerning identifiability and consistency.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="From-Oja’s-Algorithm-to-the-Multiplicative-Weights-Update-Method-with-Applications"><a href="#From-Oja’s-Algorithm-to-the-Multiplicative-Weights-Update-Method-with-Applications" class="headerlink" title="From Oja’s Algorithm to the Multiplicative Weights Update Method with Applications"></a>From Oja’s Algorithm to the Multiplicative Weights Update Method with Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15559">http://arxiv.org/abs/2310.15559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Garber</li>
<li>for: 这个论文主要针对的是在随机 principaal component analysis 中研究的在线算法 Oja 算法。</li>
<li>methods: 这个论文提出了一个简单 yet novel 的观察，即当应用到任何（不一定是随机）矩阵序列中，只要这些矩阵具有共同特征向量，那么 Oja 算法的 regret 可以直接关于 multiplicative weights update 方法的 regret 的 bound。</li>
<li>results: 论文提出了几个应用于 $\reals^n$ 上的二次形式优化问题，其中包括随机 principaal component analysis。<details>
<summary>Abstract</summary>
Oja's algorithm is a well known online algorithm studied mainly in the context of stochastic principal component analysis. We make a simple observation, yet to the best of our knowledge a novel one, that when applied to a any (not necessarily stochastic) sequence of symmetric matrices which share common eigenvectors, the regret of Oja's algorithm could be directly bounded in terms of the regret of the well known multiplicative weights update method for the problem of prediction with expert advice. Several applications to optimization with quadratic forms over the unit sphere in $\reals^n$ are discussed.
</details>
<details>
<summary>摘要</summary>
“Oja的算法是一种著名的在线算法，主要在随机主成分分析中研究。我们作出了一个简单的观察，即当应用于任何（不一定是随机的）对称矩阵序列，这些矩阵共享共同特征向量时，Oja的算法的 regret可以直接 bounds 为multiplicative weights更新方法的 regret，用于预测专家建议问题。我们还讨论了在 Unit 球上二元函数优化问题的几种应用。”Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Transfer-learning-for-day-ahead-load-forecasting-a-case-study-on-European-national-electricity-demand-time-series"><a href="#Transfer-learning-for-day-ahead-load-forecasting-a-case-study-on-European-national-electricity-demand-time-series" class="headerlink" title="Transfer learning for day-ahead load forecasting: a case study on European national electricity demand time series"></a>Transfer learning for day-ahead load forecasting: a case study on European national electricity demand time series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15555">http://arxiv.org/abs/2310.15555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandros-Menelaos Tzortzis, Sotiris Pelekis, Evangelos Spiliotis, Spiros Mouzakitis, John Psarras, Dimitris Askounis</li>
<li>for: 这个研究的目的是提高短期负载预测（STLF）的精度，并 investigate the performance of transfer learning（TL）in STLF.</li>
<li>methods: 本研究使用了一个流行的神经网络模型（NN），并进行了一个 clustering 分析来找出Series Similarity。</li>
<li>results: 研究结果显示，TL 可以比 convential approach 更高的精度，特别是当使用 clustering 分析时。<details>
<summary>Abstract</summary>
Short-term load forecasting (STLF) is crucial for the daily operation of power grids. However, the non-linearity, non-stationarity, and randomness characterizing electricity demand time series renders STLF a challenging task. Various forecasting approaches have been proposed for improving STLF, including neural network (NN) models which are trained using data from multiple electricity demand series that may not necessary include the target series. In the present study, we investigate the performance of this special case of STLF, called transfer learning (TL), by considering a set of 27 time series that represent the national day-ahead electricity demand of indicative European countries. We employ a popular and easy-to-implement NN model and perform a clustering analysis to identify similar patterns among the series and assist TL. In this context, two different TL approaches, with and without the clustering step, are compiled and compared against each other as well as a typical NN training setup. Our results demonstrate that TL can outperform the conventional approach, especially when clustering techniques are considered.
</details>
<details>
<summary>摘要</summary>
In this study, we investigate the performance of transfer learning (TL) in STLF by using a set of 27 time series that represent the national day-ahead electricity demand of indicative European countries. We employ a popular and easy-to-implement NN model and perform a clustering analysis to identify similar patterns among the series and assist TL. We compare two different TL approaches, with and without the clustering step, against each other and a typical NN training setup.Our results show that TL can outperform the conventional approach, especially when clustering techniques are considered. By leveraging the similarities among the time series, TL can improve the accuracy of STLF and provide more reliable predictions for power grid operations.
</details></li>
</ul>
<hr>
<h2 id="Algorithmic-Regularization-in-Tensor-Optimization-Towards-a-Lifted-Approach-in-Matrix-Sensing"><a href="#Algorithmic-Regularization-in-Tensor-Optimization-Towards-a-Lifted-Approach-in-Matrix-Sensing" class="headerlink" title="Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing"></a>Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15549">http://arxiv.org/abs/2310.15549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziye Ma, Javad Lavaei, Somayeh Sojoudi</li>
<li>for: 研究Gradient Descent（GD）在机器学习模型中的泛化能力，尤其是在矩阵优化问题中。</li>
<li>methods: 使用GD方法优化矩阵优化问题，特别是在 lifted matrix sensing 框架中。</li>
<li>results: 研究发现，对于 sufficient small initialization scale，GD 可以导致矩阵变为约等于rank-1矩阵，并且得到了稳定的解。这些结论 highlights the importance of tensor parametrization of matrix sensing, in combination with first-order methods, in achieving global optimality in such problems.<details>
<summary>Abstract</summary>
Gradient descent (GD) is crucial for generalization in machine learning models, as it induces implicit regularization, promoting compact representations. In this work, we examine the role of GD in inducing implicit regularization for tensor optimization, particularly within the context of the lifted matrix sensing framework. This framework has been recently proposed to address the non-convex matrix sensing problem by transforming spurious solutions into strict saddles when optimizing over symmetric, rank-1 tensors. We show that, with sufficiently small initialization scale, GD applied to this lifted problem results in approximate rank-1 tensors and critical points with escape directions. Our findings underscore the significance of the tensor parametrization of matrix sensing, in combination with first-order methods, in achieving global optimality in such problems.
</details>
<details>
<summary>摘要</summary>
“梯度下降（GD）在机器学习模型中扮演着关键的角色，它会隐藏式地导致正规化，实现紧凑的表示。在这个工作中，我们研究GD在维度优化中的角色，特别是在 matrix sensing 框架下。这个框架最近被提出，以解决非齐形矩阵感知问题，并转换了假解答为绝对点状态。我们发现，将GD应用到这个升高的问题，则可以获得约等于矩阵的紧凑矩阵和潜在绝对点。我们的发现表明，矩阵参数化和首项方法的结合，可以实现这些问题的全球最佳解。”Note: Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. It is different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Symmetry-preserving-graph-attention-network-to-solve-routing-problems-at-multiple-resolutions"><a href="#Symmetry-preserving-graph-attention-network-to-solve-routing-problems-at-multiple-resolutions" class="headerlink" title="Symmetry-preserving graph attention network to solve routing problems at multiple resolutions"></a>Symmetry-preserving graph attention network to solve routing problems at multiple resolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15543">http://arxiv.org/abs/2310.15543</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hysonlab/multires-np-hard">https://github.com/hysonlab/multires-np-hard</a></li>
<li>paper_authors: Cong Dao Tran, Thong Bach, Truong Son Hy</li>
<li>for: 解决Travelling Salesperson Problems (TSPs) 和 Vehicle Routing Problems (VRPs) 的精度和计算时间问题，通过采用机器学习 (ML) 方法。</li>
<li>methods: 提出了首次完全具有对称性的模型和训练方法，能够解决 combinatorial problems。同时，我们还提出了一种Multiresolution scheme和Equivariant Graph Attention network (mEGAT) 架构，可以充分利用图的多尺度结构， especial for large and long-range graphs。</li>
<li>results: 对比于现有基eline，我们的模型表现出了显著的改善，并证明了对称保持和多尺度是解决 combinatorial problems 的关键因素。我们的代码公开 disponível于 GitHub 上（<a target="_blank" rel="noopener" href="https://github.com/HySonLab/Multires-NP-hard%EF%BC%89%E3%80%82">https://github.com/HySonLab/Multires-NP-hard）。</a><details>
<summary>Abstract</summary>
Travelling Salesperson Problems (TSPs) and Vehicle Routing Problems (VRPs) have achieved reasonable improvement in accuracy and computation time with the adaptation of Machine Learning (ML) methods. However, none of the previous works completely respects the symmetries arising from TSPs and VRPs including rotation, translation, permutation, and scaling. In this work, we introduce the first-ever completely equivariant model and training to solve combinatorial problems. Furthermore, it is essential to capture the multiscale structure (i.e. from local to global information) of the input graph, especially for the cases of large and long-range graphs, while previous methods are limited to extracting only local information that can lead to a local or sub-optimal solution. To tackle the above limitation, we propose a Multiresolution scheme in combination with Equivariant Graph Attention network (mEGAT) architecture, which can learn the optimal route based on low-level and high-level graph resolutions in an efficient way. In particular, our approach constructs a hierarchy of coarse-graining graphs from the input graph, in which we try to solve the routing problems on simple low-level graphs first, then utilize that knowledge for the more complex high-level graphs. Experimentally, we have shown that our model outperforms existing baselines and proved that symmetry preservation and multiresolution are important recipes for solving combinatorial problems in a data-driven manner. Our source code is publicly available at https://github.com/HySonLab/Multires-NP-hard
</details>
<details>
<summary>摘要</summary>
旅行销售人员问题 (TSP) 和车辆路径问题 (VRP) 已经通过机器学习 (ML) 方法实现了一定的改进，但是之前的所有工作都没有完全尊重 TSP 和 VRP 中出现的对称性，包括旋转、平移、Permutation 和缩放。在这项工作中，我们介绍了第一个完全对称的模型和训练方法，用于解决 combinatorial 问题。此外，我们认为需要捕捉输入图的多尺度结构（即从本地到全局信息），特别是在大型和长距离图中，而前一些方法只能提取本地信息，可能导致本地或不优解。为解决这些限制，我们提出了一种多尺度 schemes 和对称图注意力网络 (mEGAT) 架构，可以高效地学习输入图的优化路径。具体来说，我们的方法构建了输入图的层次结构，从输入图中构建一系列粗化图，并在这些粗化图上解决路径问题。我们首先在粗化图上解决路径问题，然后利用该知识来解决更复杂的高级图。实验证明，我们的模型已经超过了现有的基eline，并证明对称保持和多尺度是解决 combinatorial 问题的重要配方。我们的源代码可以在 <https://github.com/HySonLab/Multires-NP-hard> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Amplification-for-Matrix-Mechanisms"><a href="#Privacy-Amplification-for-Matrix-Mechanisms" class="headerlink" title="Privacy Amplification for Matrix Mechanisms"></a>Privacy Amplification for Matrix Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15526">http://arxiv.org/abs/2310.15526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher A. Choquette-Choo, Arun Ganesh, Thomas Steinke, Abhradeep Thakurta</li>
<li>for: 这个论文的目的是为了提供一种能够适用于新的状态流行算法的隐私抽象分析方法。</li>
<li>methods: 这个论文使用的方法是基于随机选择数据的隐私抽象分析，以提供更加紧张的隐私保证。</li>
<li>results: 这个论文提出了一种名为”MMCC”的算法，可以对任何generic matrix mechanism进行隐私抽象分析。MMCC的分析结果几乎是最佳的，随着epsilondrawing to zero，其分析结果接近下界。此外，这个论文还证明了可以通过conditioning来分析相关输出，从而使得隐私抽象分析可以应用于新的状态流行算法。<details>
<summary>Abstract</summary>
Privacy amplification exploits randomness in data selection to provide tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's success in machine learning, but, is not readily applicable to the newer state-of-the-art algorithms. This is because these algorithms, known as DP-FTRL, use the matrix mechanism to add correlated noise instead of independent noise as in DP-SGD.   In this paper, we propose "MMCC", the first algorithm to analyze privacy amplification via sampling for any generic matrix mechanism. MMCC is nearly tight in that it approaches a lower bound as $\epsilon\to0$. To analyze correlated outputs in MMCC, we prove that they can be analyzed as if they were independent, by conditioning them on prior outputs. Our "conditional composition theorem" has broad utility: we use it to show that the noise added to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with amplification. Our amplification algorithm also has practical empirical utility: we show it leads to significant improvement in the privacy-utility trade-offs for DP-FTRL algorithms on standard benchmarks.
</details>
<details>
<summary>摘要</summary>
“隐私增强”利用数据选择的随机性提供更紧密的隐私保证（DP）。这个分析是DP-SGD的成功之关键，但是不能directly应用于最新的state-of-the-art算法。这是因为这些算法，称为DP-FTRL，使用矩阵机制来添加相关的随机变数而不是独立的随机变数，如DP-SGD。在这篇论文中，我们提出“MMCC”，第一个可以分析隐私增强通过抽样的任何矩阵机制。MMCC几乎是紧致的，随着$\epsilon$趋向0，它接近下界。在分析相关的输出时，我们证明可以将它们视为独立的，通过对它们的先前输出进行条件。我们称之为“条件汇总定理”，它具有广泛的实用性：我们使用它来显示DP-FTRL中添加到二元树DP-FTRL的随机变数可以对应DP-SGD中的随机变数。我们的增强算法也有实际的实验实用性：我们显示它对DP-FTRL算法的隐私-功能贡献进行了重要的改善。
</details></li>
</ul>
<hr>
<h2 id="On-the-Inherent-Privacy-Properties-of-Discrete-Denoising-Diffusion-Models"><a href="#On-the-Inherent-Privacy-Properties-of-Discrete-Denoising-Diffusion-Models" class="headerlink" title="On the Inherent Privacy Properties of Discrete Denoising Diffusion Models"></a>On the Inherent Privacy Properties of Discrete Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15524">http://arxiv.org/abs/2310.15524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongzhe Wei, Eleonora Kreačić, Haoyu Wang, Haoteng Yin, Eli Chien, Vamsi K. Potluru, Pan Li</li>
<li>for: 该论文旨在探讨对散列模型（DDMs）的散列数据生成的隐私保护性。</li>
<li>methods: 该论文采用了数学理论来描述DDMs的隐私保护性，具体来说是对每个数据点进行分别的散列Diffusion Models（pDP）的隐私泄露分析，以提供数据预处理方法来减少DDMs生成的隐私风险。</li>
<li>results: 该论文的研究结果表明，在使用DDMs生成散列数据时，隐私保护性可以在不同的数据规模下得到保障，并且隐私泄露会随着散列率的增加而减少。此外，该论文还通过实验验证了其理论结论，并在真实世界的数据集上进行了验证。<details>
<summary>Abstract</summary>
Privacy concerns have led to a surge in the creation of synthetic datasets, with diffusion models emerging as a promising avenue. Although prior studies have performed empirical evaluations on these models, there has been a gap in providing a mathematical characterization of their privacy-preserving capabilities. To address this, we present the pioneering theoretical exploration of the privacy preservation inherent in discrete diffusion models (DDMs) for discrete dataset generation. Focusing on per-instance differential privacy (pDP), our framework elucidates the potential privacy leakage for each data point in a given training dataset, offering insights into data preprocessing to reduce privacy risks of the synthetic dataset generation via DDMs. Our bounds also show that training with $s$-sized data points leads to a surge in privacy leakage from $(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pure noise to the synthetic clean data phase, and a faster decay in diffusion coefficients amplifies the privacy guarantee. Finally, we empirically verify our theoretical findings on both synthetic and real-world datasets.
</details>
<details>
<summary>摘要</summary>
隐私问题的增加导致了人工数据集的创造，扩散模型emerging as a promising avenue。 Although prior studies have performed empirical evaluations on these models, there has been a gap in providing a mathematical characterization of their privacy-preserving capabilities. To address this, we present the pioneering theoretical exploration of the privacy preservation inherent in discrete diffusion models (DDMs) for discrete dataset generation. Focusing on per-instance differential privacy (pDP), our framework elucidates the potential privacy leakage for each data point in a given training dataset, offering insights into data preprocessing to reduce privacy risks of the synthetic dataset generation via DDMs. Our bounds also show that training with $s$-sized data points leads to a surge in privacy leakage from $( \epsilon, \mathcal{O}( \frac{1}{s^2\epsilon}) )$-pDP to $( \epsilon, \mathcal{O}( \frac{1}{s\epsilon}) )$-pDP during the transition from the pure noise to the synthetic clean data phase, and a faster decay in diffusion coefficients amplifies the privacy guarantee. Finally, we empirically verify our theoretical findings on both synthetic and real-world datasets.Note: Please note that the translation is in Simplified Chinese, which is one of the two standardized Chinese writing systems. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Graph-Attention-based-Deep-Reinforcement-Learning-for-solving-the-Chinese-Postman-Problem-with-Load-dependent-costs"><a href="#Graph-Attention-based-Deep-Reinforcement-Learning-for-solving-the-Chinese-Postman-Problem-with-Load-dependent-costs" class="headerlink" title="Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs"></a>Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15516">http://arxiv.org/abs/2310.15516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hysonlab/chinese_postman_problem">https://github.com/hysonlab/chinese_postman_problem</a></li>
<li>paper_authors: Cong Dao Tran, Truong Son Hy</li>
<li>for:  solves the Chinese Postman Problem with load-dependent costs (CPP-LC) using a novel deep reinforcement learning (DRL) framework.</li>
<li>methods:  proposes a DRL model consisting of an encoder and decoder to address the CPP-LC challenge effectively, and a bio-inspired meta-heuristic solution based on Evolutionary Algorithm (EA).</li>
<li>results:  outperforms existing meta-heuristic methods such as Iterative Local Search (ILS) and Variable Neighborhood Search (VNS) regarding both solution quality and running time, and gives the best solution quality with much more running time compared to EA.Here is the summary in Traditional Chinese:</li>
<li>for: 解决中文邮差问题（CPP-LC）使用 Deep Reinforcement Learning（DRL）框架。</li>
<li>methods: 提出一个基于 DRL 的数组码（encoder）和解码（decoder）来解决 CPP-LC 挑战，以及一个生物灵感的 meta-heuristic 解决方案基于进化算法（EA）。</li>
<li>results: 与 exist 的 meta-heuristic 方法（如迭代本地搜索（ILS）和Variable Neighborhood Search（VNS））相比，在解决 CPP-LC 中获得更好的解决方案和更快的执行时间，并且与 EA 相比，获得最佳解决方案，但执行时间较长。<details>
<summary>Abstract</summary>
Recently, Deep reinforcement learning (DRL) models have shown promising results in solving routing problems. However, most DRL solvers are commonly proposed to solve node routing problems, such as the Traveling Salesman Problem (TSP). Meanwhile, there has been limited research on applying neural methods to arc routing problems, such as the Chinese Postman Problem (CPP), since they often feature irregular and complex solution spaces compared to TSP. To fill these gaps, this paper proposes a novel DRL framework to address the CPP with load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc routing problem with load constraints. The novelty of our method is two-fold. First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential model. Subsequently, we introduce an autoregressive model based on DRL, namely Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge effectively. Such a framework allows the DRL model to work efficiently and scalably to arc routing problems. Furthermore, we propose a new bio-inspired meta-heuristic solution based on Evolutionary Algorithm (EA) for CPP-LC. Extensive experiments show that Arc-DRL outperforms existing meta-heuristic methods such as Iterative Local Search (ILS) and Variable Neighborhood Search (VNS) proposed by (Corberan et al., 2018) on large benchmark datasets for CPP-LC regarding both solution quality and running time; while the EA gives the best solution quality with much more running time. We release our C++ implementations for metaheuristics such as EA, ILS and VNS along with the code for data generation and our generated data at https://github.com/HySonLab/Chinese_Postman_Problem
</details>
<details>
<summary>摘要</summary>
近期，深度强化学习（DRL）模型已经在路径问题上显示了扎实的成果。然而，大多数DRL解决方案都是针对节点路径问题，如旅行销售人问题（TSP）。同时，对于弧路径问题，如中国邮政员问题（CPP），有限的研究尝试使用神经网络方法。为了填补这些漏洞，这篇论文提出了一种新的DRL框架，用于解决CPP-LC（Corberan et al., 2018），这是一种复杂的弧路径问题，具有负荷依赖成本。我们的创新在两个方面：1. 我们将CPP-LC转化为Markov决策过程（MDP）sequential模型。2. 我们引入了基于DRL的自适应模型，即弧路径模型（Arc-DRL），其包括编码器和解码器，以有效地解决CPP-LC挑战。这种框架使得DRL模型可以有效地和扩展地应用于弧路径问题。此外，我们还提出了一种基于进化算法（EA）的新生物启发式méta-希望解方法，用于CPP-LC。我们在大量的 benchmark 数据集上进行了广泛的实验，结果显示，Arc-DRL在解决CPP-LC问题时，不仅在解决质量和运行时间两个方面都超过了现有的méta-希望方法，如迭代本地搜索（ILS）和变化邻居搜索（VNS），而且EA提供了最佳的解决质量，但需要更多的运行时间。我们在https://github.com/HySonLab/Chinese_Postman_Problem中发布了我们在metaheuristics、EA、ILS和VNS等方面的C++实现，以及我们生成的数据和代码。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Survival-Analysis-for-Heart-Failure-Risk-Prediction"><a href="#Interpretable-Survival-Analysis-for-Heart-Failure-Risk-Prediction" class="headerlink" title="Interpretable Survival Analysis for Heart Failure Risk Prediction"></a>Interpretable Survival Analysis for Heart Failure Risk Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15472">http://arxiv.org/abs/2310.15472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mike Van Ness, Tomas Bosschieter, Natasha Din, Andrew Ambrosy, Alexander Sandhu, Madeleine Udell</li>
<li>for: 这篇论文是针对医疗研究中的存生分析问题（time-to-event analysis），专门针对医院数据库中的心血管疾病风险评估。</li>
<li>methods: 这篇论文提出了一个新的存生分析管线，融合了机器学习和解释性分析，以提高存生分析的精度和可读性。管线包括改进的存生堆压法、ControlBurn 的特征选择和解释性加速机制。</li>
<li>results: 这篇论文使用大规模医院数据库，预测心血管疾病的风险，并 achieve state-of-the-art 性能。同时，管线还提供了有趣和新的风险因素统计，对于医疗应用有很好的帮助。<details>
<summary>Abstract</summary>
Survival analysis, or time-to-event analysis, is an important and widespread problem in healthcare research. Medical research has traditionally relied on Cox models for survival analysis, due to their simplicity and interpretability. Cox models assume a log-linear hazard function as well as proportional hazards over time, and can perform poorly when these assumptions fail. Newer survival models based on machine learning avoid these assumptions and offer improved accuracy, yet sometimes at the expense of model interpretability, which is vital for clinical use. We propose a novel survival analysis pipeline that is both interpretable and competitive with state-of-the-art survival models. Specifically, we use an improved version of survival stacking to transform a survival analysis problem to a classification problem, ControlBurn to perform feature selection, and Explainable Boosting Machines to generate interpretable predictions. To evaluate our pipeline, we predict risk of heart failure using a large-scale EHR database. Our pipeline achieves state-of-the-art performance and provides interesting and novel insights about risk factors for heart failure.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a novel survival analysis pipeline that balances interpretability and competitive performance with state-of-the-art survival models. Our pipeline consists of three key components:1. Survival stacking: We use an improved version of survival stacking to transform the survival analysis problem into a classification problem.2. ControlBurn: We employ ControlBurn for feature selection to identify the most relevant features for predicting the risk of heart failure.3. Explainable Boosting Machines: We use Explainable Boosting Machines to generate interpretable predictions and provide novel insights into risk factors for heart failure.To evaluate our pipeline, we use a large-scale EHR database to predict the risk of heart failure. Our results show that our pipeline achieves state-of-the-art performance and provides interesting and novel insights into risk factors for heart failure.In summary, our proposed survival analysis pipeline offers a balance between interpretability and competitive performance, making it a valuable tool for healthcare researchers and clinicians.
</details></li>
</ul>
<hr>
<h2 id="EKGNet-A-10-96μW-Fully-Analog-Neural-Network-for-Intra-Patient-Arrhythmia-Classification"><a href="#EKGNet-A-10-96μW-Fully-Analog-Neural-Network-for-Intra-Patient-Arrhythmia-Classification" class="headerlink" title="EKGNet: A 10.96μW Fully Analog Neural Network for Intra-Patient Arrhythmia Classification"></a>EKGNet: A 10.96μW Fully Analog Neural Network for Intra-Patient Arrhythmia Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15466">http://arxiv.org/abs/2310.15466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benyamin Haghi, Lin Ma, Sahin Lale, Anima Anandkumar, Azita Emami</li>
<li>for: 这篇论文是用于开发一个低功耗且精准的电子心脏病诊断系统，特别是用于识别不同类型的心脏病。</li>
<li>methods: 这篇论文提出了一个整合式的方法，结合了分析计算和深度学习，用于识别电子心脏病（ECG）的不同类型。它提出了一个名为EKGNet的硬件高效的完全分析型心脏病识别架构，这个架构可以实现高精度和低功耗。</li>
<li>results: 实验结果显示，这个方法在PhysioNet的MIT-BIH和PTB诊断数据集上取得了平均的平衡精度为95%和94.25%，用于内部患者的心脏病识别和myocardial infarction（MI）识别。这个创新的方法具有优秀的可转移性和应用性，可以用于开发低功耗且精准的心脏病诊断系统。<details>
<summary>Abstract</summary>
We present an integrated approach by combining analog computing and deep learning for electrocardiogram (ECG) arrhythmia classification. We propose EKGNet, a hardware-efficient and fully analog arrhythmia classification architecture that archives high accuracy with low power consumption. The proposed architecture leverages the energy efficiency of transistors operating in the subthreshold region, eliminating the need for analog-to-digital converters (ADC) and static random access memory (SRAM). The system design includes a novel analog sequential Multiply-Accumulate (MAC) circuit that mitigates process, supply voltage, and temperature variations. Experimental evaluations on PhysioNet's MIT-BIH and PTB Diagnostics datasets demonstrate the effectiveness of the proposed method, achieving average balanced accuracy of 95% and 94.25% for intra-patient arrhythmia classification and myocardial infarction (MI) classification, respectively. This innovative approach presents a promising avenue for developing low-power arrhythmia classification systems with enhanced accuracy and transferability in biomedical applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一种集成的方法，通过结合分析计算和深度学习来进行电cardiogram（ECG）动 irregularity分类。我们提出了EKGNet，一种具有高精度低功耗的完全分析动 irregularity分类架构。这种架构利用了晶体管在低阈值区工作的能源效率，从而消除了ADC和SRAM的需求。系统设计包括一种新的分析顺序Multiply-Accumulate（MAC）电路，以降低过程、供电电压和温度变化的影响。实验评估PhysioNet的MIT-BIH和PTB诊断数据集表明，提出的方法有效，实现平均权衡精度为95%和94.25%，分别用于患者内部动 irregularity分类和心肺病（MI）分类。这一创新方法对生物医学应用中的低功耗动 irregularity分类系统具有扩展精度和可贷 Transferability。
</details></li>
</ul>
<hr>
<h2 id="Private-Learning-with-Public-Features"><a href="#Private-Learning-with-Public-Features" class="headerlink" title="Private Learning with Public Features"></a>Private Learning with Public Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15454">http://arxiv.org/abs/2310.15454</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Walid Krichene, Nicolas Mayoraz, Steffen Rendle, Shuang Song, Abhradeep Thakurta, Li Zhang</li>
<li>for: 这个论文研究了一类Private Learning问题，其中数据是私有和公共特征的Join。这种情况通常出现在个性化推荐或广告预测中，其中个人相关的特征是敏感信息，而物品相关的特征（如电影或歌曲推荐或广告）不需要保护。问题是公共特征的存在下，私人算法可以达到更高的利用吗？我们给出了一个答案，即多个编码器模型中的一个编码器可以操作公共特征。</li>
<li>methods: 我们开发了新的算法，利用这种分离来只保护必要的充分统计（而不是添加噪声到梯度）。这种方法在线性回归中有保证的利用提升，并在两个标准私人推荐 benchmark 上达到了状态的艺术，证明了适应私人-公共特征分离的方法的重要性。</li>
<li>results: 我们的实验结果表明，在两个标准私人推荐 benchmark 上，我们的方法可以达到状态的艺术，并且在线性回归中有保证的利用提升。这证明了适应私人-公共特征分离的方法的重要性。<details>
<summary>Abstract</summary>
We study a class of private learning problems in which the data is a join of private and public features. This is often the case in private personalization tasks such as recommendation or ad prediction, in which features related to individuals are sensitive, while features related to items (the movies or songs to be recommended, or the ads to be shown to users) are publicly available and do not require protection. A natural question is whether private algorithms can achieve higher utility in the presence of public features. We give a positive answer for multi-encoder models where one of the encoders operates on public features. We develop new algorithms that take advantage of this separation by only protecting certain sufficient statistics (instead of adding noise to the gradient). This method has a guaranteed utility improvement for linear regression, and importantly, achieves the state of the art on two standard private recommendation benchmarks, demonstrating the importance of methods that adapt to the private-public feature separation.
</details>
<details>
<summary>摘要</summary>
我们研究一类private学习问题，其数据是private和public特征的Join。这经常发生在private个性化任务中，如推荐或广告预测，个人相关的特征是敏感信息，而Item（电影或歌曲的推荐或用户展示广告）相关的特征公开可用并不需要保护。自然地出现一个问题：private算法在公共特征的存在下是否可以 дости得更高的实用性。我们给出了一个答案，即多个encoder模型中的一个encoder操作于公共特征。我们开发了新的算法，利用这种分离来只保护特定的充分统计（而不是添加噪声到梯度）。这种方法在线性回归中有保证的实用性提升，并在两个标准的private推荐benchmark上达到了state of the art，证明了对private-public特征分离的方法的重要性。
</details></li>
</ul>
<hr>
<h2 id="General-Identifiability-and-Achievability-for-Causal-Representation-Learning"><a href="#General-Identifiability-and-Achievability-for-Causal-Representation-Learning" class="headerlink" title="General Identifiability and Achievability for Causal Representation Learning"></a>General Identifiability and Achievability for Causal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15450">http://arxiv.org/abs/2310.15450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bvarici/score-general-id-crl">https://github.com/bvarici/score-general-id-crl</a></li>
<li>paper_authors: Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Ali Tajer<br>for:This paper focuses on developing a method for causal representation learning (CRL) under a general nonparametric causal latent model and a general transformation model.methods:The method uses two hard uncoupled interventions per node in the latent causal graph to establish identifiability and achievability results. The algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables.results:The paper guarantees perfect recovery of the latent causal model and variables under uncoupled interventions, and recovers the existing identifiability result for two hard coupled interventions. The method does not require additional faithfulness assumptions when observational data is available.<details>
<summary>Abstract</summary>
This paper focuses on causal representation learning (CRL) under a general nonparametric causal latent model and a general transformation model that maps the latent data to the observational data. It establishes \textbf{identifiability} and \textbf{achievability} results using two hard \textbf{uncoupled} interventions per node in the latent causal graph. Notably, one does not know which pair of intervention environments have the same node intervened (hence, uncoupled environments). For identifiability, the paper establishes that perfect recovery of the latent causal model and variables is guaranteed under uncoupled interventions. For achievability, an algorithm is designed that uses observational and interventional data and recovers the latent causal model and variables with provable guarantees for the algorithm. This algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables. The analysis, additionally, recovers the existing identifiability result for two hard \textbf{coupled} interventions, that is when metadata about the pair of environments that have the same node intervened is known. It is noteworthy that the existing results on non-parametric identifiability require assumptions on interventions and additional faithfulness assumptions. This paper shows that when observational data is available, additional faithfulness assumptions are unnecessary.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-accelerated-first-order-regularized-momentum-descent-ascent-algorithm-for-stochastic-nonconvex-concave-minimax-problems"><a href="#An-accelerated-first-order-regularized-momentum-descent-ascent-algorithm-for-stochastic-nonconvex-concave-minimax-problems" class="headerlink" title="An accelerated first-order regularized momentum descent ascent algorithm for stochastic nonconvex-concave minimax problems"></a>An accelerated first-order regularized momentum descent ascent algorithm for stochastic nonconvex-concave minimax problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15448">http://arxiv.org/abs/2310.15448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiling Zhang, Zi Xu</li>
<li>for: 解决随机非凸-凹最小最大值问题</li>
<li>methods: 使用加速的首项规则化慢步驱动算法（FORMDA）</li>
<li>results: 算法的迭代复杂度为 $\tilde{\mathcal{O}(\varepsilon ^{-6.5})$，可以在单循环算法中解决随机非凸-凹最小最大值问题，并且实现了最佳复杂度下限。<details>
<summary>Abstract</summary>
Stochastic nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose an accelerated first-order regularized momentum descent ascent algorithm (FORMDA) for solving stochastic nonconvex-concave minimax problems. The iteration complexity of the algorithm is proved to be $\tilde{\mathcal{O}(\varepsilon ^{-6.5})$ to obtain an $\varepsilon$-stationary point, which achieves the best-known complexity bound for single-loop algorithms to solve the stochastic nonconvex-concave minimax problems under the stationarity of the objective function.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Dynamics-in-Linear-VAE-Posterior-Collapse-Threshold-Superfluous-Latent-Space-Pitfalls-and-Speedup-with-KL-Annealing"><a href="#Learning-Dynamics-in-Linear-VAE-Posterior-Collapse-Threshold-Superfluous-Latent-Space-Pitfalls-and-Speedup-with-KL-Annealing" class="headerlink" title="Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing"></a>Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15440">http://arxiv.org/abs/2310.15440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuma Ichikawa, Koji Hukushima</li>
<li>for: 这个论文的目的是解决变量自动机 (VAE) 面临的一个知名问题，即随机 posterior 通常与先验密切相关，导致表示学习质量受损。</li>
<li>methods: 该论文提出了一个可调参数 $\beta$ 和一种宽泛化这个参数的策略，称为 KL 渐进。</li>
<li>results: 研究发现，在输入维度很大的情况下，VAE 的学习动力会 converges 到一个决定性过程，从而可以进行详细的泛化误差分析。此外，分析还表明，VAE 初期学习杂合表示，逐渐取得不杂合表示。在特定的学习期间，当 $\beta$ 超过一定的阈值时， posterior 坍塌变得不可避免。此外，过量的尺度变量会导致背景噪声的过拟合，从而影响总体化学习和学习速度。<details>
<summary>Abstract</summary>
Variational autoencoders (VAEs) face a notorious problem wherein the variational posterior often aligns closely with the prior, a phenomenon known as posterior collapse, which hinders the quality of representation learning. To mitigate this problem, an adjustable hyperparameter $\beta$ and a strategy for annealing this parameter, called KL annealing, are proposed. This study presents a theoretical analysis of the learning dynamics in a minimal VAE. It is rigorously proved that the dynamics converge to a deterministic process within the limit of large input dimensions, thereby enabling a detailed dynamical analysis of the generalization error. Furthermore, the analysis shows that the VAE initially learns entangled representations and gradually acquires disentangled representations. A fixed-point analysis of the deterministic process reveals that when $\beta$ exceeds a certain threshold, posterior collapse becomes inevitable regardless of the learning period. Additionally, the superfluous latent variables for the data-generative factors lead to overfitting of the background noise; this adversely affects both generalization and learning convergence. The analysis further unveiled that appropriately tuned KL annealing can accelerate convergence.
</details>
<details>
<summary>摘要</summary>
“短变自动encoder（VAEs）面临一种著名的问题，即变量 posterior frequently aligns closely with the prior，导致 representation learning 质量受损。为了解决这个问题，一个可调参数 $\beta$ 和一种冷却这个参数的策略，称为 KL 冷却，被提出。本研究对一个最小化 VAE 的学习动态进行了理论分析。通过证明了动态在大输入维度下 converges to a deterministic process，从而允许详细分析泛化错误。此外，分析表明 VAE 初始化时学习杂合的表示，逐渐获得分离的表示。一种固定点分析表明，当 $\beta$ 超过某个阈值时， posterior collapse 变得不可避免，无论学习期间如何。此外，杂合的幂 variable 对数据生成因素的过度适应，导致训练过程中的过拟合和泛化错误。研究还发现，合适地调整 KL 冷却可以加速 converge。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Off-Policy-Evaluation-for-Large-Action-Spaces-via-Policy-Convolution"><a href="#Off-Policy-Evaluation-for-Large-Action-Spaces-via-Policy-Convolution" class="headerlink" title="Off-Policy Evaluation for Large Action Spaces via Policy Convolution"></a>Off-Policy Evaluation for Large Action Spaces via Policy Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15433">http://arxiv.org/abs/2310.15433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, Julian McAuley</li>
<li>for: 评估和优化新政策时需要开发准确的非策略估计器，以避免分布偏移问题。</li>
<li>methods: 我们介绍了Policy Convolution（PC）家族的估计器，利用行动嵌入来策略性卷积 logging 和目标策略，从而控制偏移的偏移。</li>
<li>results: 我们在 synthetic 和 benchmark 数据集上进行了实验，发现 PC 可以在策略差异较大的情况下提供remarkable的mean squared error（MSE）改进，最多达到5-6个数量级。<details>
<summary>Abstract</summary>
Developing accurate off-policy estimators is crucial for both evaluating and optimizing for new policies. The main challenge in off-policy estimation is the distribution shift between the logging policy that generates data and the target policy that we aim to evaluate. Typically, techniques for correcting distribution shift involve some form of importance sampling. This approach results in unbiased value estimation but often comes with the trade-off of high variance, even in the simpler case of one-step contextual bandits. Furthermore, importance sampling relies on the common support assumption, which becomes impractical when the action space is large. To address these challenges, we introduce the Policy Convolution (PC) family of estimators. These methods leverage latent structure within actions -- made available through action embeddings -- to strategically convolve the logging and target policies. This convolution introduces a unique bias-variance trade-off, which can be controlled by adjusting the amount of convolution. Our experiments on synthetic and benchmark datasets demonstrate remarkable mean squared error (MSE) improvements when using PC, especially when either the action space or policy mismatch becomes large, with gains of up to 5 - 6 orders of magnitude over existing estimators.
</details>
<details>
<summary>摘要</summary>
发展准确的脱政策估计器是评估和优化新政策的关键。脱政策估计的主要挑战在于数据生成政策和目标政策之间的分布偏移。通常，对偏移分布进行重要样本抽样是一种解决方法，但这种方法通常会带来高异常度，即使在一步上下文抽象带its。此外，重要样本抽样假设共同支持，当行动空间较大时变得不实际。为解决这些挑战，我们介绍了政策卷积（PC）家族估计器。这些方法利用行动中的隐藏结构（通过行动嵌入）进行策略性卷积，这种卷积引入了唯一的偏移-异常度质量trade-off，可以通过调整卷积的量控制。我们在synthetic和benchmark数据集上进行了实验，demonstrate PC在大的行动空间或政策偏移时实现了非常出色的均方差Error（MSE）改进，比现有估计器更高达5-6个数量级。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/24/cs.LG_2023_10_24/" data-id="cloq1wl8h00rj7o888198gy0a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/24/eess.IV_2023_10_24/" class="article-date">
  <time datetime="2023-10-24T09:00:00.000Z" itemprop="datePublished">2023-10-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/24/eess.IV_2023_10_24/">eess.IV - 2023-10-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Comparative-Study-of-Variational-Autoencoders-Normalizing-Flows-and-Score-based-Diffusion-Models-for-Electrical-Impedance-Tomography"><a href="#A-Comparative-Study-of-Variational-Autoencoders-Normalizing-Flows-and-Score-based-Diffusion-Models-for-Electrical-Impedance-Tomography" class="headerlink" title="A Comparative Study of Variational Autoencoders, Normalizing Flows, and Score-based Diffusion Models for Electrical Impedance Tomography"></a>A Comparative Study of Variational Autoencoders, Normalizing Flows, and Score-based Diffusion Models for Electrical Impedance Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15831">http://arxiv.org/abs/2310.15831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adahfbch/dgm-eit">https://github.com/adahfbch/dgm-eit</a></li>
<li>paper_authors: Huihui Wang, Guixian Xu, Qingping Zhou</li>
<li>for:  This study aims to investigate the potential of deep generative models (DGMs) in learning implicit regularizers for Electrical Impedance Tomography (EIT) imaging.</li>
<li>methods:  The study uses three DGMs - variational autoencoder networks, normalizing flow, and score-based diffusion model - to learn implicit regularizers for EIT inverse problems.</li>
<li>results:  The study shows that no single method consistently outperforms the others across all settings, and the conditional normalizing flow model (CNF) exhibits the best generalization in low-level noise, while the conditional score-based diffusion model (CSD*) demonstrates the best generalization in high-level noise settings.<details>
<summary>Abstract</summary>
Electrical Impedance Tomography (EIT) is a widely employed imaging technique in industrial inspection, geophysical prospecting, and medical imaging. However, the inherent nonlinearity and ill-posedness of EIT image reconstruction present challenges for classical regularization techniques, such as the critical selection of regularization terms and the lack of prior knowledge. Deep generative models (DGMs) have been shown to play a crucial role in learning implicit regularizers and prior knowledge. This study aims to investigate the potential of three DGMs-variational autoencoder networks, normalizing flow, and score-based diffusion model-to learn implicit regularizers in learning-based EIT imaging. We first introduce background information on EIT imaging and its inverse problem formulation. Next, we propose three algorithms for performing EIT inverse problems based on corresponding DGMs. Finally, we present numerical and visual experiments, which reveal that (1) no single method consistently outperforms the others across all settings, and (2) when reconstructing an object with 2 anomalies using a well-trained model based on a training dataset containing 4 anomalies, the conditional normalizing flow model (CNF) exhibits the best generalization in low-level noise, while the conditional score-based diffusion model (CSD*) demonstrates the best generalization in high-level noise settings. We hope our preliminary efforts will encourage other researchers to assess their DGMs in EIT and other nonlinear inverse problems.
</details>
<details>
<summary>摘要</summary>
电气阻抗成像（EIT）是广泛应用的成像技术在工业检查、地球物理探测和医疗成像中。然而，EIT图像重建中的内在非线性和不稳定性使得经典正则化技术难以应用。深度生成模型（DGM）已经被证明可以扮演重要的隐式正则化和先知知识角色。本研究旨在调查DGM在学习基于EIT成像的情况下是否能够学习隐式正则化。我们首先介绍EIT成像和其反向问题的背景信息。然后，我们提出了基于不同DGM的三种算法来解决EIT反向问题。最后，我们通过数值和视觉实验发现，(1) 不同的方法在不同的设定下并不一定能够优于其他方法，(2) 使用训练集中包含4个畸形的模型来重建含2个畸形的物体时，在低噪声设定下， conditional normalizing flow模型（CNF）表现最佳，而在高噪声设定下， conditional score-based diffusion模型（CSD*) 表现最佳。我们希望我们的初步努力能够激励其他研究人员通过自己的DGM来探索EIT和其他非线性反向问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/24/eess.IV_2023_10_24/" data-id="cloq1wlfd018g7o88e0zu1pvx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/24/eess.SP_2023_10_24/" class="article-date">
  <time datetime="2023-10-24T08:00:00.000Z" itemprop="datePublished">2023-10-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/24/eess.SP_2023_10_24/">eess.SP - 2023-10-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Resource-Allocation-for-UAV-Assisted-Industrial-IoT-User-with-Finite-Blocklength"><a href="#Resource-Allocation-for-UAV-Assisted-Industrial-IoT-User-with-Finite-Blocklength" class="headerlink" title="Resource Allocation for UAV-Assisted Industrial IoT User with Finite Blocklength"></a>Resource Allocation for UAV-Assisted Industrial IoT User with Finite Blocklength</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16211">http://arxiv.org/abs/2310.16211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atefeh Rezaei, Ata Khalili, Falko Dressler</li>
<li>for: 提高下链信息传输的稳定性和效率，使用无人机（UAV）增强下链信息传输系统。</li>
<li>methods: 使用非对称多接入（NOMA）技术，分两个阶段进行信息传输：第一阶段是控制器向UAV和IIoT设备传输信息，第二阶段是UAV解码并向IIoT设备传输信息。</li>
<li>results: 提出一种基于AOapproach的优化策略，可以有效地 mitigate非对称性问题，并且在 simulations中与基eline schemes进行比较，结果表明该方法可以下降DEP，并且具有高效率和稳定性。<details>
<summary>Abstract</summary>
We consider a relay system empowered by an unmanned aerial vehicle (UAV) that facilitates downlink information delivery while adhering to finite blocklength requirements. The setup involves a remote controller transmitting information to both a UAV and an industrial Internet of Things (IIoT) or remote device, employing the non-orthogonal multiple access (NOMA) technique in the first phase. Subsequently, the UAV decodes and forwards this information to the remote device in the second phase. Our primary objective is to minimize the decoding error probability (DEP) at the remote device, which is influenced by the DEP at the UAV. To achieve this goal, we optimize the blocklength, transmission power, and location of the UAV. However, the underlying problem is highly non-convex and generally intractable to be solved directly. To overcome this challenge, we adopt an alternative optimization (AO) approach and decompose the original problem into three sub-problems. This approach leads to a sub-optimal solution, which effectively mitigates the non-convexity issue. In our simulations, we compare the performance of our proposed algorithm with baseline schemes. The results reveal that the proposed framework outperforms the baseline schemes, demonstrating its superiority in achieving lower DEP at the remote device. Furthermore, the simulation results illustrate the rapid convergence of our proposed algorithm, indicating its efficiency and effectiveness in solving the optimization problem.
</details>
<details>
<summary>摘要</summary>
我们考虑了一种由无人机（UAV） empowered的 relay 系统，该系统可以在 finite blocklength 的限制下提供下行信息传输。系统包括一个远程控制器，该控制器将信息传输到 UAV 和一个工业互联网对象（IIoT）或远程设备，使用非正交多存取（NOMA）技术在第一阶段。在第二阶段，UAV 将这些信息解码并转发给远程设备。我们的主要目标是最小化译码错误概率（DEP）在远程设备上，该错误概率受 UAV 的 DEP 影响。为了实现这个目标，我们优化块长度、传输功率和 UAV 的位置。然而，这个问题具有非对称和紧难直接解决的特点。为了解决这个挑战，我们采用了一种代替优化（AO）方法，将原始问题分解成三个子问题。这种方法导致一个差不多的解决方案，有效地减少了非对称性问题。在我们的 simulate 中，我们与基线方案进行比较。结果表明，我们的提posed 框架在 DEP 上具有更低的性能，表明其在远程设备上的superiority。此外，我们的 simulate 结果表明我们的算法具有快速收敛的特点，这表明它在解决优化问题时的效率和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Systematic-Physics-Compliant-Analysis-of-Over-the-Air-Channel-Equalization-in-RIS-Parametrized-Wireless-Networks-on-Chip"><a href="#Systematic-Physics-Compliant-Analysis-of-Over-the-Air-Channel-Equalization-in-RIS-Parametrized-Wireless-Networks-on-Chip" class="headerlink" title="Systematic Physics-Compliant Analysis of Over-the-Air Channel Equalization in RIS-Parametrized Wireless Networks-on-Chip"></a>Systematic Physics-Compliant Analysis of Over-the-Air Channel Equalization in RIS-Parametrized Wireless Networks-on-Chip</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16195">http://arxiv.org/abs/2310.16195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean Tapie, Hugo Prod’homme, Mohammadreza F. Imani, Philipp del Hougne</li>
<li>for: 这篇论文的目的是提出一种用于快速预测干扰环境中无线网络的模型，以便更好地利用快速模拟和优化技术来提高无线网络的性能。</li>
<li>methods: 这篇论文使用了一种基于物理学的减少基数模型，该模型可以通过一次全波矢量优化来预测干扰环境中无线网络的响应。</li>
<li>results: 该论文的结果表明，使用这种减少基数模型可以快速预测干扰环境中无线网络的响应，并且可以系统地研究各种干扰环境下的无线网络性能。此外，论文还提出了一种用于优化多个在芯处理器上的无线链接的同时优化策略。<details>
<summary>Abstract</summary>
Wireless networks-on-chip (WNoCs) are an enticing complementary interconnect technology for multi-core chips but face severe resource constraints. Being limited to simple on-off-keying modulation, the reverberant nature of the chip enclosure imposes limits on allowed modulation speeds in sight of inter-symbol interference, casting doubts on the competitiveness of WNoCs as interconnect technology. Fortunately, this vexing problem was recently overcome by parametrizing the on-chip radio environment with a reconfigurable intelligent surface (RIS). By suitably configuring the RIS, selected channel impulse responses (CIRs) can be tuned to be (almost) pulse-like despite rich scattering thanks to judiciously tailored multi-bounce path interferences. However, the exploration of this "over-the-air" (OTA) equalization is thwarted by (i) the overwhelming complexity of the propagation environment, and (ii) the non-linear dependence of the CIR on the RIS configuration, requiring a costly and lengthy full-wave simulation for every optimization step. Here, we show that a reduced-basis physics-compliant model for RIS-parametrized WNoCs can be calibrated with a single full-wave simulation. Thereby, we unlock the possibility of predicting the CIR for any RIS configuration almost instantaneously without any additional full-wave simulation. We leverage this new tool to systematically explore OTA equalization in RIS-parametrized WNoCs regarding the optimal choice of delay time for the RIS-shaped CIR's peak. We also study the simultaneous optimization of multiple on-chip wireless links for broadcasting. Looking forward, the introduced tools will enable the efficient exploration of various types of OTA analog computing in RIS-parametrized WNoCs.
</details>
<details>
<summary>摘要</summary>
无线网络在芯片（WNoCs）是一种吸引人的补充连接技术，但它们面临严重的资源约束。由于使用简单的ON-OFF键调制，芯片封包的噪声环境会限制允许的调制速率，从而影响WNoCs的竞争力。幸运的是，这个问题已经得到了解决，通过使用可配置的智能表面（RIS）来Parametrize the on-chip radio environment。通过适当配置RIS，可以通过 selecing channel impulse responses（CIRs）来实现（近乎）普液化调制，即使在丰富的散射下。然而，这种“空中”（OTA）平衡的探索被阻塞了（1）压倒性的宽泛环境，以及（2）RIS配置的非线性依赖关系，需要每个优化步骤都需要昂贵和时间consuming的全波 Simulation。在这里，我们展示了一种基于物理的减少基准模型，可以在RIS参数化的WNoCs中快速地Calibrate  the CIR for any RIS configuration。通过这种新工具，我们可以在不需要额外的全波 Simulation 的情况下，快速地预测CIR для任何RIS配置。我们利用这种新工具来系统地探索OTA平衡在RIS参数化的WNoCs中的最佳选择延迟时间。我们还研究了在多个芯片无线连接中同时优化的问题。在未来，我们将引入的工具将允许我们高效地探索不同类型的OTA分析计算在RIS参数化的WNoCs中。
</details></li>
</ul>
<hr>
<h2 id="Codebook-based-Uplink-Transmission-Enhancement-in-5G-Advanced-Sub-band-Precoding"><a href="#Codebook-based-Uplink-Transmission-Enhancement-in-5G-Advanced-Sub-band-Precoding" class="headerlink" title="Codebook-based Uplink Transmission Enhancement in 5G Advanced: Sub-band Precoding"></a>Codebook-based Uplink Transmission Enhancement in 5G Advanced: Sub-band Precoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16137">http://arxiv.org/abs/2310.16137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Cao, Yahia Shabara, Parisa Cheraghi</li>
<li>for: 提高 fifth-generation (5G) 移动设备的上行性能 (UL) 性能。</li>
<li>methods: 使用全协同天线端口进行单层传输，并分析SB预编程选择标准和设计改进的UL代码书。</li>
<li>results: 通过链级模拟，发现使用ULSB预编程可以提高BLER错误率至少2dB，并且显示UL性能增幅受SB大小选择和相对相位偏移多样性的影响。<details>
<summary>Abstract</summary>
The transformative enhancements of fifth-generation (5G) mobile devices bring about new challenges to achieve better uplink (UL) performance. Particularly, in codebook-based transmission, the wide-band (WB) precoding and the legacy UL codebook may become main bottlenecks for higher efficient data transmission. In this paper, we investigate the codebook-based UL single-layer transmission performance using fully coherent antenna ports in the context of sub-band (SB) precoding. We analyze the SB precoder selection criteria and design an UL codebook used for SB precoding by increasing the number of relative phase shifts of each port. Via link-level simulations, we verify that the UL SB precoding can improve up to 2 dB performance gain in terms of the block error rate (BLER) compared with the UL WB precoding which is the current UL precoding scheme. We also show that UL performance gain is sensitive to the SB size selection as well as the relative phase shift diversity.
</details>
<details>
<summary>摘要</summary>
fifth-generation (5G) 移动设备的转变性丰富化带来了新的挑战以实现更好的上行（UL）性能。特别是在编码库（codebook）基础上的传输中，宽频（WB）预编码和传统的UL编码库可能成为更高效数据传输的主要瓶颈。在这篇论文中，我们调查了使用完全协同天线端口的UL单层传输性能。我们分析了SB预编码选择标准和设计了用于SB预编码的UL编码库。通过链级模拟，我们证明了UL SB预编码可以提高至2dB的BLER错误率相对于当前的UL WB预编码方案。我们还表明UL性能提升的敏感性受SB大小选择以及相对频shift多样性的影响。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Energy-Efficiency-for-Reconfigurable-Intelligent-Surfaces-with-Practical-Power-Models"><a href="#Enhancing-Energy-Efficiency-for-Reconfigurable-Intelligent-Surfaces-with-Practical-Power-Models" class="headerlink" title="Enhancing Energy Efficiency for Reconfigurable Intelligent Surfaces with Practical Power Models"></a>Enhancing Energy Efficiency for Reconfigurable Intelligent Surfaces with Practical Power Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15901">http://arxiv.org/abs/2310.15901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyi Li, Jida Zhang, Jieao Zhu, Shi Jin, Linglong Dai</li>
<li>for: 本研究旨在提高智能表面协助下的无线通信系统的能效性 (EE)，并且解决了先前的研究中忽略了智能表面元素上的PIN晶体的ON和OFF状态之间的能效性差异问题。</li>
<li>methods: 本研究使用了实际的能源模型，考虑了智能表面元素上PIN晶体的ON和OFF状态之间的能效性差异，并根据此模型提出了更加准确的EE优化问题。然而，这个问题是非对称的和杂合整数性质的，对优化poses了挑战。为解决这个问题，本研究使用了有效的替换优化算法框架，分别优化了基站和智能表面的扩展抽象预测器。</li>
<li>results:  тео理分析表明，本研究可以减少原始问题的复杂性，从多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段减少到多阶段<details>
<summary>Abstract</summary>
Reconfigurable intelligent surfaces (RISs) are widely considered a promising technology for future wireless communication systems. As an important indicator of RIS-assisted communication systems in green wireless communications, energy efficiency (EE) has recently received intensive research interest as an optimization target. However, most previous works have ignored the different power consumption between ON and OFF states of the PIN diodes attached to each RIS element. This oversight results in extensive unnecessary power consumption and reduction of actual EE due to the inaccurate power model. To address this issue, in this paper, we first utilize a practical power model for a RIS-assisted multi-user multiple-input single-output (MU-MISO) communication system, which takes into account the difference in power dissipation caused by ON-OFF states of RIS's PIN diodes. Based on this model, we formulate a more accurate EE optimization problem. However, this problem is non-convex and has mixed-integer properties, which poses a challenge for optimization. To solve the problem, an effective alternating optimization (AO) algorithm framework is utilized to optimize the base station and RIS beamforming precoder separately. To obtain the essential RIS beamforming precoder, we develop two effective methods based on maximum gradient search and SDP relaxation respectively. Theoretical analysis shows the exponential complexity of the original problem has been reduced to polynomial complexity. Simulation results demonstrate that the proposed algorithm outperforms the existing ones, leading to a significant increase in EE across a diverse set of scenarios.
</details>
<details>
<summary>摘要</summary>
可重配置智能表面（RIS）技术被广泛认为是未来无线通信系统的优秀技术之一。作为RIS协助通信系统中绿色无线通信的重要指标，能效率（EE）在最近几年内受到了投入研究的广泛关注。然而，大多数前一些工作忽视了RIS元素上的PIN晶体的ON和OFF状态之间的能量消耗差异，这会导致无必要的能量浪费和EE的减少。为解决这个问题，在这篇论文中，我们首先采用了RIS协助多用户多输入单出口（MU-MISO）通信系统的实用能量模型，该模型考虑了RIS元素上PIN晶体的ON-OFF状态所导致的能量消耗差异。基于这个模型，我们形ulated了一个更加精准的EE优化问题。然而，这个问题具有非核心和杂合属性，这会对优化带来挑战。为解决这个问题，我们采用了有效的交互式优化（AO）算法框架，以分别优化基站和RIS的扫描频率和扫描方向。为了获得 essence RIS扫描频率和扫描方向，我们开发了两种有效的方法，包括最大峰值搜索和SDP缓和方法。理论分析表明，原始问题的几何复杂度已经降低到了多项式复杂度。实验结果表明，我们的算法在多种场景下比现有算法更高效，导致EE的提高。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Modeling-and-Analysis-of-Transmission-Error-in-Harmonic-Drive-Systems-Nonlinear-Dynamics-Error-Modeling-and-Compensation-Techniques"><a href="#Data-Driven-Modeling-and-Analysis-of-Transmission-Error-in-Harmonic-Drive-Systems-Nonlinear-Dynamics-Error-Modeling-and-Compensation-Techniques" class="headerlink" title="Data-Driven Modeling and Analysis of Transmission Error in Harmonic Drive Systems: Nonlinear Dynamics, Error Modeling, and Compensation Techniques"></a>Data-Driven Modeling and Analysis of Transmission Error in Harmonic Drive Systems: Nonlinear Dynamics, Error Modeling, and Compensation Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15875">http://arxiv.org/abs/2310.15875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju Wu, Philippe Louis Schuchert, Alireza Karimi<br>for:  This paper aims to improve the precision performance of harmonic drive systems (HDS) by data-driven modeling and analysis of the system’s kinematic errors.methods:  The authors use Lagrange equations to derive the HDS dynamics, and develop various linear and nonlinear models to predict the kinematic errors. They also propose novel compensation mechanisms and policies, including nonlinear model predictive control and frequency loop-shaping.results:  The best-performing model, based on a nonlinear neural network, achieves over 98% accuracy for one-step predictions on both the training and validation data sets. The authors also analyze the feedback loop to select the controller for vibration mitigation. The main contributions of the paper include the nonlinear dynamics derivation, data-driven nonlinear modeling of flexible kinematic errors, repeatable experiment design, and proposed novel compensation mechanism and policies.<details>
<summary>Abstract</summary>
Harmonic drive systems (HDS) are high-precision robotic transmissions featuring compact size and high gear ratios. However, issues like kinematic transmission errors hamper their precision performance. This article focuses on data-driven modeling and analysis of an HDS to improve kinematic error compensation. The background introduces HDS mechanics, nonlinear attributes, and modeling approaches from literature. The HDS dynamics are derived using Lagrange equations. Experiments under aggressive conditions provide training data exhibiting deterministic patterns. Various linear and nonlinear models have been developed. The best-performing model, based on a nonlinear neural network, achieves over 98\% accuracy for one-step predictions on both the training and validation data sets. A phenomenological model separates the kinematic error into a periodic pure part and flexible part. Apart from implementation of estimated transmission error injection compensation, novel compensation mechanisms policies for the kinematic error are analyzed and proposed, including nonlinear model predictive control and frequency loop-shaping. The feedback loop is analyzed to select the controller for vibration mitigation. Main contributions include the nonlinear dynamics derivation, data-driven nonlinear modeling of flexible kinematic errors, repeatable experiment design, and proposed novel compensation mechanism and policies. Future work involves using physics-informed neural networks, sensitivity analysis, full life-cycle monitoring, and extracting physical laws directly from data.
</details>
<details>
<summary>摘要</summary>
响应驱动系统（HDS）是高精度机器人传动系统，具有高比例和减小的尺寸。然而， transmit  errors 妨碍它们的精度性表现。这篇文章关注数据驱动模型化和分析HDS，以改善传动误差补偿。文章首先介绍HDS的机械特性和非线性属性，以及从文献中所获取的模型方法。然后，通过拉格朗日方程来 derivate HDS 的动力学。在严格条件下进行的实验提供了训练数据，显示出具有决定性模式的特征。 varous linear 和非线性模型已经被开发出来，并且基于非线性神经网络的最佳性能模型达到了98%以上的准确性。在这篇文章中，我们采用了以下主要贡献：1. 非线性动力学 derivation2. 数据驱动非线性模型化的弹性传动误差3. 重复性实验设计4. 提出了新的补偿机制和策略，包括非线性预测控制和频率征 Loop-shaping5. 反馈循环分析，选择合适的控制器 для防止振荡未来的工作包括使用physics-informed neural networks、敏感分析、全生命周期监测和直接从数据中提取物理法则。
</details></li>
</ul>
<hr>
<h2 id="A-High-Performance-and-Low-Complexity-5G-LDPC-Decoder-Algorithm-and-Implementation"><a href="#A-High-Performance-and-Low-Complexity-5G-LDPC-Decoder-Algorithm-and-Implementation" class="headerlink" title="A High-Performance and Low-Complexity 5G LDPC Decoder: Algorithm and Implementation"></a>A High-Performance and Low-Complexity 5G LDPC Decoder: Algorithm and Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15801">http://arxiv.org/abs/2310.15801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqing Ren, Hassan Harb, Yifei Shen, Alexios Balatsoukas-Stimming, Andreas Burg</li>
<li>for: 这个论文的目的是为5G新Radio（NR）设计低密度差错检查（LDPC）解码算法和相关的 Very Large-Scale Integration（VLSI）实现。</li>
<li>methods: 这个论文提出了一种高性能低复杂度LDPC解码算法，以满足5G的要求。具体来说，作者提出了一种扩展了调整最小和（GA-MS）解码算法，可以在硬件中实现准确的信号传输。</li>
<li>results: 作者通过数字实验表明，提出的固定点GAMS算法与浮点数BP解码算法在不同的5G标准规范下具有只有0.1dB的差异。此外，作者还实现了一个可重新配置的5G NR LDPC解码器，使用GA-MS解码算法，并采用多种数据压缩和近似技术来减少解码器的内存开销。<details>
<summary>Abstract</summary>
5G New Radio (NR) has stringent demands on both performance and complexity for the design of low-density parity-check (LDPC) decoding algorithms and corresponding VLSI implementations. Furthermore, decoders must fully support the wide range of all 5G NR blocklengths and code rates, which is a significant challenge. In this paper, we present a high-performance and low-complexity LDPC decoder, tailor-made to fulfill the 5G requirements. First, to close the gap between belief propagation (BP) decoding and its approximations in hardware, we propose an extension of adjusted min-sum decoding, called generalized adjusted min-sum (GA-MS) decoding. This decoding algorithm flexibly truncates the incoming messages at the check node level and carefully approximates the non-linear functions of BP decoding to balance the error-rate and hardware complexity. Numerical results demonstrate that the proposed fixed-point GAMS has only a minor gap of 0.1 dB compared to floating-point BP under various scenarios of 5G standard specifications. Secondly, we present a fully reconfigurable 5G NR LDPC decoder implementation based on GA-MS decoding. Given that memory occupies a substantial portion of the decoder area, we adopt multiple data compression and approximation techniques to reduce 42.2% of the memory overhead. The corresponding 28nm FD-SOI ASIC decoder has a core area of 1.823 mm2 and operates at 895 MHz. It is compatible with all 5G NR LDPC codes and achieves a peak throughput of 24.42 Gbps and a maximum area efficiency of 13.40 Gbps/mm2 at 4 decoding iterations.
</details>
<details>
<summary>摘要</summary>
5G新Radio（NR）具有严格的性能和复杂度要求对低密度差异检查（LDPC）解码算法和相关的VLSI实现。此外，解码器必须完全支持5G NR块长和编码率的广泛范围，这是一项重要挑战。在这篇论文中，我们提出了一种高性能低复杂度LDPC解码器，专门用于满足5G的要求。首先，我们提出了一种通过修改信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级别的信道节点级�
</details></li>
</ul>
<hr>
<h2 id="Observation-of-Damped-Oscillations-in-Chemical-Quantum-Magnetic-Interactions"><a href="#Observation-of-Damped-Oscillations-in-Chemical-Quantum-Magnetic-Interactions" class="headerlink" title="Observation of Damped Oscillations in Chemical-Quantum-Magnetic Interactions"></a>Observation of Damped Oscillations in Chemical-Quantum-Magnetic Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15775">http://arxiv.org/abs/2310.15775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luana Hildever, Thiago Ferro, José Holanda</li>
<li>for: 这个论文是关于新型的化学-量子-磁学互动现象的报道。</li>
<li>methods: 这个研究使用了Fe3O4&#x2F;PANI nanostructure，并通过考察这种结构的电子韧度和磁矩来观察和测量化学-量子-磁学互动的行为。</li>
<li>results: 研究发现，在某些条件下，Fe3O4&#x2F;PANI nanostructure会具有双值性效应，导致化学部分的振荡，而量子和磁部分则由双值性效应控制。此外，通过互动测量，研究人员发现了一种受限的抑制的响应，符合抑制的响应的行为。<details>
<summary>Abstract</summary>
Fundamental interactions are the basis of the most diverse phenomena in science that allow the dazzling of possible applications. In this work, we report a new interaction, which we call chemical-quantum-magnetic interaction. This interaction arises due to the difference in valence that the Fe3O4/PANI nanostructure acquires under certain conditions. In this study, PANI activates the chemical part of the oscillations, leaving the quantum and magnetic part for the double valence effect and consequently for changing the number of spins of the nanostructure sites. We also observed using interaction measurements that chemical-quantum-magnetic interactions oscillate in a subcritical regime satisfying the behavior of a damped harmonic oscillator.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neuromorphic-Sampling-of-Sparse-Signals"><a href="#Neuromorphic-Sampling-of-Sparse-Signals" class="headerlink" title="Neuromorphic Sampling of Sparse Signals"></a>Neuromorphic Sampling of Sparse Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15750">http://arxiv.org/abs/2310.15750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abijith Jagannath Kamath, Chandra Sekhar Seelamantula</li>
<li>for: 这 paper 是关于 neuromorphic sampling 技术的研究，用于实现低功耗、高 Dynamical range 和高时间分辨率的视觉传感器。</li>
<li>methods: 这 paper 使用 sampling-theoretic 方法，基于 neuromorphic sensing 和时间基准扩展，通过高分辨率 spectral estimation 方法进行参数估计，实现完美信号重建。</li>
<li>results: 研究人员通过实验证明，可以通过高精度的 spectral estimation 方法和 kernel-based sampling 方法，实现完美信号重建，并且可以在多通道情况下进行多个输入多个输出（MIMO）和单输入多输出（SIMO）的信号参数估计。<details>
<summary>Abstract</summary>
Neuromorphic sampling is a bioinspired and opportunistic analog-to-digital conversion technique, where the measurements are recorded only when there is a significant change in the signal amplitude. Neuromorphic sampling has paved the way for a new class of vision sensors called event cameras or dynamic vision sensors (DVS), which consume low power, accommodate a high-dynamic range, and provide sparse measurements with high temporal resolution making it convenient for downstream inference tasks. In this paper, we consider neuromorphic sensing of signals with a finite rate of innovation (FRI), including a stream of Dirac impulses, sum of weighted and time-shifted pulses, and piecewise-polynomial functions. We consider a sampling-theoretic approach and leverage the close connection between neuromorphic sensing and time-based sampling, where the measurements are encoded temporally. Using Fourier-domain analysis, we show that perfect signal reconstruction is possible via parameter estimation using high-resolution spectral estimation methods. We develop a kernel-based sampling approach, which allows for perfect reconstruction with a sample complexity equal to the rate of innovation of the signal. We provide sufficient conditions on the parameters of the neuromorphic encoder for perfect reconstruction. Furthermore, we extend the analysis to multichannel neuromorphic sampling of FRI signals, in the single-input multi-output (SIMO) and multi-input multi-output (MIMO) configurations. We show that the signal parameters can be jointly estimated using multichannel measurements. Experimental results are provided to substantiate the theoretical claims.
</details>
<details>
<summary>摘要</summary>
《神经omorphic sampling：一种基于生物体的 opportunistic扫描技术》，我们使用神经omorphic sampling技术来捕捉信号，只在信号强度发生显著变化时进行扫描。这种技术具有低功耗、高 Dinamic range 和高时间分辨率等优点，使得后续的推理任务变得更加便捷。在这篇论文中，我们考虑了具有有限速度创新（FRI）信号的神经omorphic捕捉，包括束缚 Dirac 冲击、重量加权和时间偏移的冲击和分割 polynomials 函数。我们采用抽象理论方法，利用神经omorphic捕捉和时间基的关系，通过时间编码来编码测量。使用傅里叶分析，我们表明可以通过高分辨率spectral estimation方法来进行参数估计，从而实现完美的信号重建。我们开发了基于核函数的抽象 sampling 方法，可以在样本复杂度等于创新率的情况下实现完美的重建。我们还提供了神经omorphic捕捉器参数的充分条件，以确保完美的重建。此外，我们扩展了分析到多通道神经omorphic捕捉的 FRI 信号，包括单输入多输出（SIMO）和多输入多输出（MIMO）配置。我们表明可以通过多通道测量来共同估计信号参数。实验结果用于证明理论声明。
</details></li>
</ul>
<hr>
<h2 id="User-Clustering-for-Coexistence-between-Near-field-and-Far-field-Communications"><a href="#User-Clustering-for-Coexistence-between-Near-field-and-Far-field-Communications" class="headerlink" title="User Clustering for Coexistence between Near-field and Far-field Communications"></a>User Clustering for Coexistence between Near-field and Far-field Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15707">http://arxiv.org/abs/2310.15707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaidi Wang, Zhiguo Ding, George K. Karagiannidis</li>
<li>for: 本研究 investigate 近距离（NF）和远距离（FF）通信的共存，其中多个FF用户被归类为NF用户的扩散的磁场上服务，通过非对称多接入（NOMA）技术。</li>
<li>methods: 本研究提出了三种不同的突击抑制（SIC）解码策略，并将总比特率最大化问题转化为一个 clustering 问题，并且使用了 overlap coalitional game 来设计 clustering 算法。</li>
<li>results: 实验结果表明，提出的归类策略能够显著提高考虑系统的总比特率，而提出的策略可以实现不同的平衡 между 总比特率和公平性。<details>
<summary>Abstract</summary>
This letter investigates the coexistence between near-field (NF) and far-field (FF) communications, where multiple FF users are clustered to be served on the beams of legacy NF users, via non-orthogonal multiple access (NOMA). Three different successive interference cancellation (SIC) decoding strategies are proposed and a sum rate maximization problem is formulated to optimize the assignment and decoding order. The beam allocation problem is further reformulated as an overlapping coalitional game, which facilitates the the design of the proposed clustering algorithm. The optimal decoding order in each cluster is also derived, which can be integrated into the proposed clustering. Simulation results demonstrate that the proposed clustering algorithm is able to significantly improve the sum rate of the considered system, and the developed strategies achieve different trade-offs between sum rate and fairness.
</details>
<details>
<summary>摘要</summary>
这封信函数研究了靠近场（NF）和远场（FF）通信的共存问题，其中多个FF用户被集中服务在传统NF用户的扩散中，通过非对称多访问（NOMA）。提出了三种不同的successive interference cancellation（SIC）解码策略，并将总比特率最大化问题编制为优化分配和解码顺序的问题。扩散分配问题被再次编制为覆盖合作游戏，这使得设计提案的集群算法变得更加容易。最佳解码顺序在每个分组中也得出，可以与集群分配结合使用。实验结果表明，提案的集群算法可以明显提高考虑系统的总比特率，而提出的策略可以实现不同的比特率和公平性之间的质量。
</details></li>
</ul>
<hr>
<h2 id="Exploitation-des-propri-e-t-e-s-de-saturation-synaptique-pour-obtenir-un-neurone-a-fr-e-quence-sp-e-cifique"><a href="#Exploitation-des-propri-e-t-e-s-de-saturation-synaptique-pour-obtenir-un-neurone-a-fr-e-quence-sp-e-cifique" class="headerlink" title="Exploitation des propri{é}t{é}s de saturation synaptique pour obtenir un neurone {à} fr{é}quence sp{é}cifique"></a>Exploitation des propri{é}t{é}s de saturation synaptique pour obtenir un neurone {à} fr{é}quence sp{é}cifique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15635">http://arxiv.org/abs/2310.15635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Marthe, Claire Goursaud</li>
<li>for: 解决 IoT 应用中的能源消耗问题，特别是 Micro-controller 的过高能耗。</li>
<li>methods: 使用生物体启发的 Interacting Synapses 技术，实现时间滤波。</li>
<li>results: 提出一种在 analog 频域中处理时间序列的新方法，并研究 Parameters 的适应。<details>
<summary>Abstract</summary>
Energy consumption remains the main limiting factors in many promising IoT applications. In particular, micro-controllers consume far too much power. In order to overcome this problem, new circuit designs have been proposed and the use of spiking neurons and analog computing has emerged as it allows a very significant consumption reduction. However, working in the analog domain brings difficulty to handle the sequential processing of incoming signals as is needed in many use cases.In this paper, we propose to use a bio-inspired phenomenon called Interacting Synapses to produce a time filter. We propose a model of synapses that makes the neuron fire for a specific range of delays between two incoming spikes, but not react when this Inter-Spike Timing is not in that range. We study the parameters of the model to understand how to adapt the Inter-Spike Timing. The originality of the paper is to propose a new way, in the analog domain, to deal with temporal sequences.
</details>
<details>
<summary>摘要</summary>
“能源消耗仍然是许多有前途的IoT应用中的主要限制因素。特别是微控制器的能量消耗相对较高。为了解决这个问题，新的电路设计被提议，以及使用脉冲神经和分析计算。然而，在分析频域工作带来了处理进入信号的顺序处理的困难。在这篇论文中，我们提出使用生物体启发的现象——交互式 synapses 来生成时间筛选器。我们提出一种 synapses 模型，使得神经元在某个时间范围内的两个脉冲间隔时间不同，但是不会响应其他时间范围内的脉冲。我们研究模型参数，以便适应交互式时间调整。本文的原创性在于，在分析频域中，提出了一种新的方法来处理时间序列。”Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="3D-Multi-Target-Localization-Via-Intelligent-Reflecting-Surface-Protocol-and-Analysis"><a href="#3D-Multi-Target-Localization-Via-Intelligent-Reflecting-Surface-Protocol-and-Analysis" class="headerlink" title="3D Multi-Target Localization Via Intelligent Reflecting Surface: Protocol and Analysis"></a>3D Multi-Target Localization Via Intelligent Reflecting Surface: Protocol and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15574">http://arxiv.org/abs/2310.15574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Hua, Guangji Chen, Shaodan Ma, Chau Yuen, Hing Cheung So</li>
<li>for: 这篇论文的目的是研究基于多智能反射表（IRS）的三维多Target地位定位系统。</li>
<li>methods: 论文使用了多种方法，包括控制IRS的打开&#x2F;关状态，以实现三维地位定位。</li>
<li>results:  simulation results demonstrate the effectiveness of the proposed scheme, and sub-meter-level positioning accuracy can be achieved.<details>
<summary>Abstract</summary>
With the emerging environment-aware applications, ubiquitous sensing is expected to play a key role in future networks. In this paper, we study a 3-dimensional (3D) multi-target localization system where multiple intelligent reflecting surfaces (IRSs) are applied to create virtual line-of-sight (LoS) links that bypass the base station (BS) and targets. To fully unveil the fundamental limit of IRS for sensing, we first study a single-target-single-IRS case and propose a novel \textit{two-stage localization protocol} by controlling the on/off state of IRS. To be specific, in the IRS-off stage, we derive the Cram\'{e}r-Rao bound (CRB) of the azimuth/elevation direction-of-arrival (DoA) of the BS-target link and design a DoA estimator based on the MUSIC algorithm. In the IRS-on stage, the CRB of the azimuth/elevation DoA of the IRS-target link is derived and a simple DoA estimator based on the on-grid IRS beam scanning method is proposed. Particularly, the impact of echo signals reflected by IRS from different paths on sensing performance is analyzed. Moreover, we prove that the single-beam of the IRS is not capable of sensing, but it can be achieved with \textit{multi-beam}. Based on the two obtained DoAs, the 3D single-target location is constructed. We then extend to the multi-target-multi-IRS case and propose an \textit{IRS-adaptive sensing protocol} by controlling the on/off state of multiple IRSs, and a multi-target localization algorithm is developed. Simulation results demonstrate the effectiveness of our scheme and show that sub-meter-level positioning accuracy can be achieved.
</details>
<details>
<summary>摘要</summary>
With the emerging environment-aware applications, ubiquitous sensing is expected to play a key role in future networks. In this paper, we study a 3-dimensional (3D) multi-target localization system where multiple intelligent reflecting surfaces (IRSs) are applied to create virtual line-of-sight (LoS) links that bypass the base station (BS) and targets. To fully unveil the fundamental limit of IRS for sensing, we first study a single-target-single-IRS case and propose a novel \textit{two-stage localization protocol} by controlling the on/off state of IRS. To be specific, in the IRS-off stage, we derive the Cramér-Rao bound (CRB) of the azimuth/elevation direction-of-arrival (DoA) of the BS-target link and design a DoA estimator based on the MUSIC algorithm. In the IRS-on stage, the CRB of the azimuth/elevation DoA of the IRS-target link is derived and a simple DoA estimator based on the on-grid IRS beam scanning method is proposed. Particularly, the impact of echo signals reflected by IRS from different paths on sensing performance is analyzed. Moreover, we prove that the single-beam of the IRS is not capable of sensing, but it can be achieved with \textit{multi-beam}. Based on the two obtained DoAs, the 3D single-target location is constructed. We then extend to the multi-target-multi-IRS case and propose an \textit{IRS-adaptive sensing protocol} by controlling the on/off state of multiple IRSs, and a multi-target localization algorithm is developed. Simulation results demonstrate the effectiveness of our scheme and show that sub-meter-level positioning accuracy can be achieved.
</details></li>
</ul>
<hr>
<h2 id="Reconfigurable-Intelligent-Surface-Based-Receive-Generalized-Spatial-Modulation-Design"><a href="#Reconfigurable-Intelligent-Surface-Based-Receive-Generalized-Spatial-Modulation-Design" class="headerlink" title="Reconfigurable Intelligent Surface-Based Receive Generalized Spatial Modulation Design"></a>Reconfigurable Intelligent Surface-Based Receive Generalized Spatial Modulation Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15566">http://arxiv.org/abs/2310.15566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinghao Guo, Hanjiang Hong, Yin Xu, Yi-yan Wu, Dazhi He, Wenjun Zhang</li>
<li>for: 本文提出了一种基于智能表面的收发复制模式（RGSM）方案，用于提高未来无线通信的高效性。</li>
<li>methods: 该方案使用智能表面（RIS）的群控制器来实现选择接收天线和相位幂分多址（PSK）模ulation，并通过修改元素的激活状态来实现幂分多址（APSK）模ulation。</li>
<li>results: 比对与现有的RIS-aided receive generalized space shift keying（RIS-RGSSK）方案，该方案在同样的bit error rate（BER）性能下具有更好的性能，并且结果表明，在低幂分多址（PSK）下，该方案会在低幂分多址（PSK）下表现更好，而在高幂分多址（APSK）下，该方案会表现更好。<details>
<summary>Abstract</summary>
In this paper, the receive generalized spatial modulation (RGSM) scheme with reconfigurable intelligent surfaces (RIS) assistance is proposed. The RIS group controllers change the reflected phases of the RIS elements to achieve the selection of receive antennas and phase shift keying (PSK) modulation, and the amplitudes of the received symbols are adjusted by changing the activation states of the elements to achieve amplitude phase shift keying (APSK) modulation. Compared with the existing RIS-aided receive generalized space shift keying (RIS-RGSSK) scheme, the proposed scheme realizes that the selected antennas respectively receive different modulation symbols, and only adds the process to control the modulated phases and the activation states of elements. The proposed scheme has better bit error rate (BER) performance than the RIS-RGSSK scheme at the same rate. In addition, the results show that for low modulation orders, the proposed scheme will perform better with PSK, while for high modulation order, APSK is better. The proposed scheme is a promising scheme for future wireless communication to achieve high-efficiency.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，提出了基于扩展通用空间调制（RGSM）的接收方案，即使用智能表面（RIS）的协助。RIS组控制器通过改变反射的相位来实现选择接收天线和相位幅调制（PSK）调制，并通过改变元素的激活状态来调整接收符号的幅度调制（APSK）。与现有的RIS协助的接收扩展空间幅移键（RIS-RGSSK）方案相比，提议方案可以实现每个接收天线分别接收不同的调制符号，并且只需控制调制相位和元素激活状态。提议方案的比较优秀性比RIS-RGSSK方案更好，特别是在同样的速率下，它的比特错误率（BER）性能更高。此外，结果显示，当低调制次数时，提议方案会在PSK上表现更好，而在高调制次数时，APSK会更好。这个方案是未来无线通信技术的一个有前途的方案。
</details></li>
</ul>
<hr>
<h2 id="Capacity-based-Spatial-Modulation-Constellation-and-Pre-scaling-Design"><a href="#Capacity-based-Spatial-Modulation-Constellation-and-Pre-scaling-Design" class="headerlink" title="Capacity-based Spatial Modulation Constellation and Pre-scaling Design"></a>Capacity-based Spatial Modulation Constellation and Pre-scaling Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15565">http://arxiv.org/abs/2310.15565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinghao Guo, Hanjiang Hong, Yin Xu, Yi-yan Wu, Dazhi He, Wenjun Zhang</li>
<li>for: 提高SM系统的性能，不需要渠道状态返回（CSI）反馈。</li>
<li>methods: 使用非均匀抽象（NUC）和预scaling系数优化设计方案，对SM系统进行优化。</li>
<li>results: 对多输入单出口（MISO）系统 WITH Rayleigh canal 进行优化，提高了SM系统的性能，并且可以作为未来6G技术来实现高效性。<details>
<summary>Abstract</summary>
Spatial Modulation (SM) can utilize the index of the transmit antenna (TA) to transmit additional information. In this paper, to improve the performance of SM, a non-uniform constellation (NUC) and pre-scaling coefficients optimization design scheme is proposed. The bit-interleaved coded modulation (BICM) capacity calculation formula of SM system is firstly derived. The constellation and pre-scaling coefficients are optimized by maximizing the BICM capacity without channel state information (CSI) feedback. Optimization results are given for the multiple-input-single-output (MISO) system with Rayleigh channel. Simulation result shows the proposed scheme provides a meaningful performance gain compared to conventional SM system without CSI feedback. The proposed optimization design scheme can be a promising technology for future 6G to achieve high-efficiency.
</details>
<details>
<summary>摘要</summary>
空间调制（SM）可以利用发射天线（TA）的指标来传输额外信息。在本文中，为提高SM表现，一种非均匀定点（NUC）和预scaling系数优化设计方案被提议。SM系统的bit-拼接oded模ulation（BICM）容量计算公式首先得到。定点和预scaling系数通过最大化BICM容量而不需要通道状态信息（CSI）反馈进行优化。对多输入单出口（MISO）系统的RAYLEIGH通道进行优化结果。实验结果表明，提议的方案比非CSI反馈的SM系统提供了更高的表现。这种优化设计方案可能成为未来6G高效技术的亮点。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-driven-Meta-learning-for-CSI-Feedback"><a href="#Knowledge-driven-Meta-learning-for-CSI-Feedback" class="headerlink" title="Knowledge-driven Meta-learning for CSI Feedback"></a>Knowledge-driven Meta-learning for CSI Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15548">http://arxiv.org/abs/2310.15548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Xiao, Wenqiang Tian, Wendong Liu, Jiajia Guo, Zhi Zhang, Shi Jin, Zhihua Shi, Li Guo, Jia Shen</li>
<li>for: 提高大规模多输入多输出系统中的减法状态信息反馈精度，通过深度学习（DL）技术进行改进。</li>
<li>methods: 提出了一种基于知识驱动的元学习方法，其中DL模型在元训练阶段初始化的meta模型能够在目标 retrained 阶段快速趋向于新enario。特别是，不需要训练大量来自不同enario的数据，而是基于通信频率特征的内在知识来构建meta任务环境，并将目标任务数据集进行了可brie� augmentation。</li>
<li>results: 通过仪器实验结果显示，提出的方法能够在反馈性能和训练时间两个方面具有优势。<details>
<summary>Abstract</summary>
Accurate and effective channel state information (CSI) feedback is a key technology for massive multiple-input and multiple-output systems. Recently, deep learning (DL) has been introduced for CSI feedback enhancement through massive collected training data and lengthy training time, which is quite costly and impractical for realistic deployment. In this article, a knowledge-driven meta-learning approach is proposed, where the DL model initialized by the meta model obtained from meta training phase is able to achieve rapid convergence when facing a new scenario during target retraining phase. Specifically, instead of training with massive data collected from various scenarios, the meta task environment is constructed based on the intrinsic knowledge of spatial-frequency characteristics of CSI for meta training. Moreover, the target task dataset is also augmented by exploiting the knowledge of statistical characteristics of wireless channel, so that the DL model can achieve higher performance with small actually collected dataset and short training time. In addition, we provide analyses of rationale for the improvement yielded by the knowledge in both phases. Simulation results demonstrate the superiority of the proposed approach from the perspective of feedback performance and convergence speed.
</details>
<details>
<summary>摘要</summary>
Accurate and effective channel state information (CSI) feedback is a key technology for massive multiple-input and multiple-output systems. Recently, deep learning (DL) has been introduced for CSI feedback enhancement through massive collected training data and lengthy training time, which is quite costly and impractical for realistic deployment. In this article, a knowledge-driven meta-learning approach is proposed, where the DL model initialized by the meta model obtained from meta training phase is able to achieve rapid convergence when facing a new scenario during target retraining phase. Specifically, instead of training with massive data collected from various scenarios, the meta task environment is constructed based on the intrinsic knowledge of spatial-frequency characteristics of CSI for meta training. Moreover, the target task dataset is also augmented by exploiting the knowledge of statistical characteristics of wireless channel, so that the DL model can achieve higher performance with small actually collected dataset and short training time. In addition, we provide analyses of rationale for the improvement yielded by the knowledge in both phases. Simulation results demonstrate the superiority of the proposed approach from the perspective of feedback performance and convergence speed.Here's the translation in Traditional Chinese:精准且有效的通道状态信息（CSI）反馈是许多入出发点多载波系统的关键技术。近期，深度学习（DL）已经被引入用于CSI反馈增强，通过大量收集的训练数据和长时间的训练时间，但这是实际部署中非常昂贵和不实际的。在本文中，我们提出了知识驱动的meta学习方法，其中DL模型由meta模型在meta训练阶段取得的知识初始化。在target重训阶段，DL模型能够从新的enario面对快速融合，而不需要大量的训练数据和长时间训练。此外，我们还提供了两个阶段中知识的分析，以解释增强的原因。 simulation results表明，提案的方法具有较好的反馈性和融合速度。
</details></li>
</ul>
<hr>
<h2 id="LDPC-Decoding-with-Degree-Specific-Neural-Message-Weights-and-RCQ-Decoding"><a href="#LDPC-Decoding-with-Degree-Specific-Neural-Message-Weights-and-RCQ-Decoding" class="headerlink" title="LDPC Decoding with Degree-Specific Neural Message Weights and RCQ Decoding"></a>LDPC Decoding with Degree-Specific Neural Message Weights and RCQ Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15483">http://arxiv.org/abs/2310.15483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linfang Wang, Caleb Terrill, Richard Wesel, Dariush Divsalar<br>for: This paper proposes a family of weight-sharing schemes for low-density parity-check (LDPC) codes that use the same weight for edges with the same check node degree and&#x2F;or variable node degree, reducing neural network complexity and storage requirements.methods: The paper combines degree-specific neural weights with a reconstruction-computation-quantization (RCQ) decoder to produce a weighted RCQ (W-RCQ) decoder, and identifies and resolves a gradient explosion issue that can arise when training neural LDPC decoders.results: The paper shows that node-degree-based weight-sharing can deliver the same performance as using distinct weights for each node, and the W-RCQ decoder with node-degree-based weight sharing has a reduced hardware requirement compared with the original RCQ decoder.<details>
<summary>Abstract</summary>
Recently, neural networks have improved MinSum message-passing decoders for low-density parity-check (LDPC) codes by multiplying or adding weights to the messages, where the weights are determined by a neural network. The neural network complexity to determine distinct weights for each edge is high, often limiting the application to relatively short LDPC codes. Furthermore, storing separate weights for every edge and every iteration can be a burden for hardware implementations. To reduce neural network complexity and storage requirements, this paper proposes a family of weight-sharing schemes that use the same weight for edges that have the same check node degree and/or variable node degree. Our simulation results show that node-degree-based weight-sharing can deliver the same performance requiring distinct weights for each node.   This paper also combines these degree-specific neural weights with a reconstruction-computation-quantization (RCQ) decoder to produce a weighted RCQ (W-RCQ) decoder. The W-RCQ decoder with node-degree-based weight sharing has a reduced hardware requirement compared with the original RCQ decoder. As an additional contribution, this paper identifies and resolves a gradient explosion issue that can arise when training neural LDPC decoders.
</details>
<details>
<summary>摘要</summary>
近些年，神经网络已经改进了MinSum消息传递解码器，用于低密度约束Check（LDPC）编码器。在这些解码器中，神经网络会对消息 multiply 或 add weights，其中 weights 的确定由神经网络。然而，神经网络Complexity determining distinct weights for each edge is high，通常只能应用于相对短LDPC编码器。此外，每个边和每个迭代都需要独立存储 weights 的存储要求是硬件实现的负担。为了减少神经网络复杂性和存储要求，本文提出了一家weight-sharing方案，该方案使用同样的weight для每个检查节点度和/或变量节点度相同的边。我们的实验结果表明，基于节点度的weight-sharing可以实现与每个节点具有独立 weights 的性能。此外，本文还将这种度量Specific neural weights与 reconstruction-computation-quantization（RCQ）解码器结合，生成一个weighted RCQ（W-RCQ）解码器。W-RCQ解码器采用node-degree-based weight sharing，与原始RCQ解码器相比，具有更低的硬件需求。作为其他贡献，本文还识别并解决了在培育神经LDPC解码器时可能出现的梯度爆炸问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/24/eess.SP_2023_10_24/" data-id="cloq1wlgv01c77o8802vm842d" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/23/cs.SD_2023_10_23/" class="article-date">
  <time datetime="2023-10-23T15:00:00.000Z" itemprop="datePublished">2023-10-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/23/cs.SD_2023_10_23/">cs.SD - 2023-10-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GESI-Gammachirp-Envelope-Similarity-Index-for-Predicting-Intelligibility-of-Simulated-Hearing-Loss-Sounds"><a href="#GESI-Gammachirp-Envelope-Similarity-Index-for-Predicting-Intelligibility-of-Simulated-Hearing-Loss-Sounds" class="headerlink" title="GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds"></a>GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15399">http://arxiv.org/abs/2310.15399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayako Yamamoto, Toshio Irino, Fuki Miyazaki, Honoka Tamaru<br>for: 这个研究旨在开发一种新的听力可读性指标（OIM），以便预测normal hearing（NH）听众对 simulate hearing loss（HL）声音的听力可读性（SI）。methods: 这个研究使用了一种新的方法，即Gammachirp Envelope Similarity Index（GESI），该方法使用了gammachirp filterbank（GCFB）、模ulation filterbank和extended cosine similarity measure来计算SI指标。GESI可以接受参照声音和测试声音的水平不均衡，并反映listeners的听力水平如 audiogram 上所示。results: 研究发现，GESI可以在四个SI实验中预测mean和个人SI值，而其他传统的OIMs（STOI、ESTOI、MBSTOI和HASPI）没有预测SI的能力。GESI也可以根据听众的个人听力状况来预测SI。<details>
<summary>Abstract</summary>
We proposed a new objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), which can predict the speech intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners. GESI is an intrusive method that computes the SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. GESI can accept the level asymmetry of the reference and test sounds and reflect the HI listener's hearing level as it appears on the audiogram. A unique feature of GESI is its ability to incorporate an individual participant's listening condition into the SI prediction. We conducted four SI experiments on male and female speech sounds in both laboratory and crowdsourced remote environments. We then evaluated GESI and the conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI, for their ability to predict mean and individual SI values with and without the use of simulated HL sounds. GESI outperformed the other OIMs in all evaluations. STOI, ESTOI, and MBSTOI did not predict SI at all, even when using the simulated HL sounds. HASPI did not predict the difference between the laboratory and remote experiments on male speech sounds and the individual SI values. GESI may provide a first step toward SI prediction for individual HI listeners whose HL is caused solely by peripheral dysfunction.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的对象智能度量标 (OIM)，即γ折衔幂响同比指标 (GESI)，可以预测正常聆听者 (NH) 对 simulate 听力损伤 (HL) 声音的语音 inteligibilidad (SI)。GESI 是一种侵入性的方法，使用γ折衔幂 filterbank (GCFB)、模拟 filterbank 和扩展 косину similarity measure 来计算 SI 指标。GESI 可以接受参考和测试声音的水平差异，并反映听力测试图中的听力水平。GESI 的一个独特特点是可以将参与者的听力条件 incorporated 到 SI 预测中。我们在男女speech声音上进行了四个 SI 实验，分别在实验室和 Remote 环境中进行。然后，我们评估了 GESI 和传统 OIMs，STOI、ESTOI、MBSTOI 和 HASPI，它们在使用 simulate HL 声音时预测 Mean 和个体 SI 值的能力。GESI 在所有评估中表现出色，而 STOI、ESTOI 和 MBSTOI 没有预测 SI 一样，甚至在使用 simulate HL 声音时也无法预测。HASPI 不能预测男子speech声音在实验室和 Remote 环境之间的差异。GESI 可能为听力损伤仅由 péripheral dysfunction 引起的个体 HI listeners 提供了第一步 toward SI 预测。
</details></li>
</ul>
<hr>
<h2 id="8-8-4-Formalizing-Time-Units-to-Handle-Symbolic-Music-Durations"><a href="#8-8-4-Formalizing-Time-Units-to-Handle-Symbolic-Music-Durations" class="headerlink" title="8+8&#x3D;4: Formalizing Time Units to Handle Symbolic Music Durations"></a>8+8&#x3D;4: Formalizing Time Units to Handle Symbolic Music Durations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14952">http://arxiv.org/abs/2310.14952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanouil Karystinaios, Francesco Foscarin, Florent Jacquemard, Masahiko Sakai, Satoshi Tojo, Gerhard Widmer</li>
<li>for: 这个论文关注符号音乐作品中的 Nominal 持续时间（音符和停止符），以及如何在计算机应用程序中方便地处理这些。</li>
<li>methods: 作者提议使用直接与音乐Sheet中图形符号相关的时间单位，并提供了一组常用的计算音乐应用程序中的操作。</li>
<li>results: 作者将这种时间单位和常用方法整合到了一个数学框架中，并讨论了一些实际应用场景，并指出了在数据类型和计算数量方面，该系统可以提高处理效率。<details>
<summary>Abstract</summary>
This paper focuses on the nominal durations of musical events (notes and rests) in a symbolic musical score, and on how to conveniently handle these in computer applications. We propose the usage of a temporal unit that is directly related to the graphical symbols in musical scores and pair this with a set of operations that cover typical computations in music applications. We formalize this time unit and the more commonly used approach in a single mathematical framework, as semirings, algebraic structures that enable an abstract description of algorithms/processing pipelines. We then discuss some practical use cases and highlight when our system can improve such pipelines by making them more efficient in terms of data type used and the number of computations.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文关注 симвоlic musical score中的 Nominal duration of musical events (notes and rests)，以及如何在计算机应用程序中方便处理这些事件。我们提议使用 directly related to the graphical symbols in musical scores的 temporal unit，并与常用的算法/处理管道中的操作集成一起。我们将这个时间单位和常用的方法集成到一个数学框架中，使用semirings，这些数学结构可以描述算法/处理管道的抽象描述。然后我们讨论了一些实际应用场景，并 highlighted when our system can improve such pipelines by making them more efficient in terms of data type used and the number of computations.
</details></li>
</ul>
<hr>
<h2 id="Intuitive-Multilingual-Audio-Visual-Speech-Recognition-with-a-Single-Trained-Model"><a href="#Intuitive-Multilingual-Audio-Visual-Speech-Recognition-with-a-Single-Trained-Model" class="headerlink" title="Intuitive Multilingual Audio-Visual Speech Recognition with a Single-Trained Model"></a>Intuitive Multilingual Audio-Visual Speech Recognition with a Single-Trained Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14946">http://arxiv.org/abs/2310.14946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joanna Hong, Se Jin Park, Yong Man Ro</li>
<li>for: 本研究旨在开发一种多语言 audio-visual speech recognition系统，以便在多语言环境中提高语音识别精度。</li>
<li>methods: 我们提出了一种基于人类语言认知系统的方法，通过分别捕捉语言之间的共同特征和差异来识别输入语音的语言类别。我们在大量预训练的 audio-visual 表示模型中加入了提示细化技术，使网络可以同时识别语音和语言类别。</li>
<li>results: 我们的方法可以在多语言 audio-visual speech recognition任务中实现高效精度的识别，而无需建立语言特定的模型。这些结果表明，我们的方法可以提高多语言 audio-visual speech recognition系统的Robustness和效率。<details>
<summary>Abstract</summary>
We present a novel approach to multilingual audio-visual speech recognition tasks by introducing a single model on a multilingual dataset. Motivated by a human cognitive system where humans can intuitively distinguish different languages without any conscious effort or guidance, we propose a model that can capture which language is given as an input speech by distinguishing the inherent similarities and differences between languages. To do so, we design a prompt fine-tuning technique into the largely pre-trained audio-visual representation model so that the network can recognize the language class as well as the speech with the corresponding language. Our work contributes to developing robust and efficient multilingual audio-visual speech recognition systems, reducing the need for language-specific models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来处理多语言音视频演说识别任务，那是通过在多语言数据集上引入单一模型。我们受到人类认知系统的启发，人们可以无需注意或指导，直观地分辨不同语言。因此，我们提议一种可以从语言输入speech中捕捉语言类别的模型，同时还能识别相应的语言。为此，我们在广泛预训练的音视频表示模型中加入了提示细调技术，使网络可以同时识别语言类别和相应的speech。我们的工作对于开发高效、可靠的多语言音视频演说识别系统做出了贡献，降低了语言特定模型的需求。
</details></li>
</ul>
<hr>
<h2 id="Audio-Visual-Speaker-Tracking-Progress-Challenges-and-Future-Directions"><a href="#Audio-Visual-Speaker-Tracking-Progress-Challenges-and-Future-Directions" class="headerlink" title="Audio-Visual Speaker Tracking: Progress, Challenges, and Future Directions"></a>Audio-Visual Speaker Tracking: Progress, Challenges, and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14778">http://arxiv.org/abs/2310.14778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinzheng Zhao, Yong Xu, Xinyuan Qian, Davide Berghi, Peipei Wu, Meng Cui, Jianyuan Sun, Philip J. B. Jackson, Wenwu Wang</li>
<li>for: 本文提供了一个全面的室内Audio-visual speaker tracking的综述，包括 Bayesian 筛选器家族和获取音视频测量方法。</li>
<li>methods: 本文使用 Bayesian 筛选器家族和深度学习技术来解决数据归一化、音视频融合和跟踪管理等问题。</li>
<li>results: 本文对 AV16.3 数据集的现有跟踪器进行了总结，并讨论了深度学习技术对测量提取和状态估计的影响。<details>
<summary>Abstract</summary>
Audio-visual speaker tracking has drawn increasing attention over the past few years due to its academic values and wide application. Audio and visual modalities can provide complementary information for localization and tracking. With audio and visual information, the Bayesian-based filter can solve the problem of data association, audio-visual fusion and track management. In this paper, we conduct a comprehensive overview of audio-visual speaker tracking. To our knowledge, this is the first extensive survey over the past five years. We introduce the family of Bayesian filters and summarize the methods for obtaining audio-visual measurements. In addition, the existing trackers and their performance on AV16.3 dataset are summarized. In the past few years, deep learning techniques have thrived, which also boosts the development of audio visual speaker tracking. The influence of deep learning techniques in terms of measurement extraction and state estimation is also discussed. At last, we discuss the connections between audio-visual speaker tracking and other areas such as speech separation and distributed speaker tracking.
</details>
<details>
<summary>摘要</summary>
simplified chinese translation:听音视频说话人跟踪在过去几年来得到了越来越多的关注，这主要归功于其学术价值和广泛的应用领域。听音和视频Modalities可以提供补充信息，用于定位和跟踪。通过听音和视频信息， Bayesian 筛选器可以解决数据关联、听音视频融合和跟踪管理问题。在这篇论文中，我们提供了听音视频说话人跟踪的全面概述。据我们所知，这是过去五年来第一篇详细的报告。我们介绍了 Bayesian 筛选器的家族和获得听音视频测量的方法。此外，我们还总结了现有的跟踪器和其在 AV16.3 数据集上的性能。在过去几年中，深度学习技术得到了大量应用，这也促进了听音视频说话人跟踪的发展。深度学习技术在测量提取和状态估计方面的影响也得到了讨论。最后，我们讨论了听音视频说话人跟踪和其他领域之间的连接，如语音分离和分布式说话人跟踪。</sys>Note: Simplified Chinese translation is based on the standardized form of Chinese used in mainland China. The translation may vary depending on the region or dialect.
</details></li>
</ul>
<hr>
<h2 id="Acoustic-BPE-for-Speech-Generation-with-Discrete-Tokens"><a href="#Acoustic-BPE-for-Speech-Generation-with-Discrete-Tokens" class="headerlink" title="Acoustic BPE for Speech Generation with Discrete Tokens"></a>Acoustic BPE for Speech Generation with Discrete Tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14580">http://arxiv.org/abs/2310.14580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feiyu Shen, Yiwei Guo, Chenpeng Du, Xie Chen, Kai Yu</li>
<li>for: 用于提高语音生成器的速度和语法捕捉能力</li>
<li>methods: 使用字对编码法（BPE）对声音token进行编码，以减少序列长度并利用tokentsequence中的词法信息</li>
<li>results: 通过对语音生成器进行了全面的调查，证明了acoustic BPE可以提高语音生成器的听起来更自然和语法更正确，并提出了一种新的重新分配方法来选择最佳的人工语音。<details>
<summary>Abstract</summary>
Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.
</details>
<details>
<summary>摘要</summary>
几种自动学习模型中的不连续音频标记已经在语音生成中得到了广泛的应用。然而，直接使用音频标记会导致序列模型遇到长度问题，同时还需要模型建立音频标记之间的相关性，这会复杂化模型化过程。为解决这个问题，我们提出了音频BPE，它利用字对编码法将常见音频标记模式编码。音频BPE可以缩短序列长度，同时利用音频标记序列中的先前 morphological 信息，从而使模型化过程更加简单。通过对一个使用音频BPE训练的语音语言模型进行了广泛的调查，我们证明了它具有 faster inference 和 improved syntax capturing 能力。此外，我们还提出了一种新的重新分配方法，可以在rich-diversity TTS 系统中选择最佳的 sintetic 语音。实验证明，重新分配选择与人类偏好高度相符，这 highlights 音频BPE 的潜在应用在其他语音生成任务中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/23/cs.SD_2023_10_23/" data-id="cloq1wlb900yl7o88fxolc4js" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/10/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/88/">88</a><a class="extend next" rel="next" href="/page/12/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">117</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">68</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">50</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
