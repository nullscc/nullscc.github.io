
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/3/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/eess.IV_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T09:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/eess.IV_2023_11_19/">eess.IV - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Classification-of-Radio-Galaxies-with-trainable-COSFIRE-filters"><a href="#Classification-of-Radio-Galaxies-with-trainable-COSFIRE-filters" class="headerlink" title="Classification of Radio Galaxies with trainable COSFIRE filters"></a>Classification of Radio Galaxies with trainable COSFIRE filters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11286">http://arxiv.org/abs/2311.11286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Ndungu, Trienko Grobler, Stefan J. Wijnholds Dimka Karastoyanova, George Azzopardi</li>
<li>for:  radio galaxy classification</li>
<li>methods: COSFIRE filters (explainable, learning-free, rotation-tolerant, efficient)</li>
<li>results: achieved an average accuracy rate of 93.36%, outperformed contemporary deep learning models, better computational performance ($\sim$20$\times$ fewer operations)Here’s the full text in Simplified Chinese:for: 这个研究旨在为 радио галактик类型分类做出一种有效的方法。methods: 我们使用了COSFIRE滤波器，它具有适应形态和方向的能力，同时具有解释性、学习自由、旋转快速等优点。results: 我们在一个标准的 радио галактик数据集上进行了实验，包括1180个训练样本和404个测试样本。结果显示，我们的方法实现了93.36%的平均准确率，超越了当前的深度学习模型，并在这个数据集上达到了最佳的成绩。此外，COSFIRE滤波器在计算性能方面也表现出了优势，相比同精度的 denseNet 模型，它的计算量只需20倍以下。这些发现证明了 COSFIRE 滤波器在 радио галактик类型分类中的效iveness。<details>
<summary>Abstract</summary>
Radio galaxies exhibit a rich diversity of characteristics and emit radio emissions through a variety of radiation mechanisms, making their classification into distinct types based on morphology a complex challenge. To address this challenge effectively, we introduce an innovative approach for radio galaxy classification using COSFIRE filters. These filters possess the ability to adapt to both the shape and orientation of prototype patterns within images. The COSFIRE approach is explainable, learning-free, rotation-tolerant, efficient, and does not require a huge training set. To assess the efficacy of our method, we conducted experiments on a benchmark radio galaxy data set comprising of 1180 training samples and 404 test samples. Notably, our approach achieved an average accuracy rate of 93.36\%. This achievement outperforms contemporary deep learning models, and it is the best result ever achieved on this data set. Additionally, COSFIRE filters offer better computational performance, $\sim$20$\times$ fewer operations than the DenseNet-based competing method (when comparing at the same accuracy). Our findings underscore the effectiveness of the COSFIRE filter-based approach in addressing the complexities associated with radio galaxy classification. This research contributes to advancing the field by offering a robust solution that transcends the orientation challenges intrinsic to radio galaxy observations. Our method is versatile in that it is applicable to various image classification approaches.
</details>
<details>
<summary>摘要</summary>
Radio галактики表现出丰富多样性，通过多种辐射机制发射电波，这使得它们的分类成为一项复杂的挑战。为了解决这个挑战，我们提出了一种创新的电波 галакти子分类方法使用COSFIRE筛选器。这些筛选器具有适应形状和orientation的图像模式的能力。COSFIRE方法是可解释的、学习无需、旋转快速、高效的，并不需要庞大的训练集。为评估我们的方法的有效性，我们在一个标准电波 галакти子数据集上进行了实验，该数据集包括1180个训练样本和404个测试样本。结果显示，我们的方法实现了93.36%的准确率，这比当今的深度学习模型高出了一些，同时也是这个数据集上的最佳成绩。此外，COSFIRE筛选器在计算性能方面比 denseNet 基于的竞争方法更好，在同等准确率下，它们的操作数量只需 $\sim$20 $\times$  fewer。我们的发现证明了 COSFIRE 筛选器基于的方法在电波 галакти子分类中的效iveness。这种方法不仅可以应用于电波 галакти子分类，还可以扩展到其他图像分类领域。
</details></li>
</ul>
<hr>
<h2 id="Wireless-Regional-Imaging-through-Reconfigurable-Intelligent-Surfaces-Passive-Mode"><a href="#Wireless-Regional-Imaging-through-Reconfigurable-Intelligent-Surfaces-Passive-Mode" class="headerlink" title="Wireless Regional Imaging through Reconfigurable Intelligent Surfaces: Passive Mode"></a>Wireless Regional Imaging through Reconfigurable Intelligent Surfaces: Passive Mode</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11222">http://arxiv.org/abs/2311.11222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuhai Wang, Chun Wang, Rujing Xiong, Zhengyu Wang, Tiebin Mi, Robert Caiming Qiu</li>
<li>for: 这篇论文旨在提出一种基于多个反射器的无线成像框架，以解决分布式感知网络的问题。</li>
<li>methods: 该系统使用可变相位调制的多个反射器（RIS）来生成随机的反射 patrern，使接收器能够在定义的空间区域（SoI）中捕捉信号。</li>
<li>results: 论文提出了一种基于多个反射器的线性成像通道模型，并提出了一种计算成像理论来恢复SOI中信号强度分布。furthermore, the paper proposes an amplitude-only imaging algorithm to mitigate the problem of phase unpredictability. Finally, the performance of the imaging algorithm is verified by proof-of-concept experiments under reasonable parameter settings.<details>
<summary>Abstract</summary>
In this paper, we propose a multi-RIS-aided wireless imaging framework in 3D facing the distributed placement of multi-sensor networks. The system creates a randomized reflection pattern by adjusting the RIS phase shift, enabling the receiver to capture signals within the designated space of interest (SoI). Firstly, a multi-RIS-aided linear imaging channel modeling is proposed. We introduce a theoretical framework of computational imaging to recover the signal strength distribution of the SOI. For the RIS-aided imaging system, the impact of multiple parameters on the performance of the imaging system is analyzed. The simulation results verify the correctness of the proposal. Furthermore, we propose an amplitude-only imaging algorithm for the RIS-aided imaging system to mitigate the problem of phase unpredictability. Finally, the performance verification of the imaging algorithm is carried out by proof of concept experiments under reasonable parameter settings.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于多个反射层设备（RIS）的无线影像框架，面临分布式感知网络的分布式部署。系统通过调整RIS阶段偏移，使接收器能够在指定空间 интере（SoI）内捕捉信号。首先，我们提出了一种基于多个RIS的线性影像通道模型。我们介绍了一种计算影像理论框架，以回收SOI中信号强度分布。对于RIS协助影像系统，我们分析了多个参数对系统性能的影响。实验结果证明了我们的提议的正确性。此外，我们提出了一种幂only影像算法，以 Mitigate phase unpredictability问题。最后，我们验证了影像算法的性能通过理想参数设置的证明。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/eess.IV_2023_11_19/" data-id="clpxp6ccl01dnee88bfd0dv79" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/eess.SP_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T08:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/eess.SP_2023_11_19/">eess.SP - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Rethinking-Integrated-Sensing-and-Communication-When-Near-Field-Meets-Wideband"><a href="#Rethinking-Integrated-Sensing-and-Communication-When-Near-Field-Meets-Wideband" class="headerlink" title="Rethinking Integrated Sensing and Communication: When Near Field Meets Wideband"></a>Rethinking Integrated Sensing and Communication: When Near Field Meets Wideband</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11416">http://arxiv.org/abs/2311.11416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaolin Wang, Xidong Mu, Yuanwei Liu</li>
<li>for: 本研究重新审视了融合感知通信（ISAC）系统在近场区域内部 while 利用大带宽。</li>
<li>methods: 我们首先揭示了宽频感知通信（S&amp;C）通道的基本特征，并强调在远场到近场区域的转变中发生的两个基本变化。</li>
<li>results: near-field region可以实现宽频-like S&amp;C 功能，包括信号复用和距离探测，以及在高速移动 S&amp;C 场景中实现不可能的宽频功能。<details>
<summary>Abstract</summary>
This article re-examines integrated sensing and communication (ISAC) systems operating in the near-field region of a large antenna array while exploiting a large bandwidth. We first reveal the fundamental characteristics of wideband sensing and communication (S&C) channels and highlight the key changes that occur during the transition from the far-field to the near-field region. Specifically, there are two fundamental changes in the near-field region: strong angular-delay correlation and element-specific Doppler frequencies. It is highlighted that the near-field effect can enable the wideband-like S&C functionalities in terms of signal multiplexing and range sensing due to the strong angular-delay correlation, thus allowing the trading of large antenna arrays for large bandwidths. Furthermore, it also introduces the wideband-unattainable functionalities in high mobility S&C scenarios by leveraging the element-specific Doppler frequencies. We then delineate certain paradigm shifts in thinking required to advance toward near-field wideband ISAC systems, with a particular emphasis on resource allocation, antenna array arrangement, and transceiver architecture. Lastly, some other promising directions are discussed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vital-Signs-Estimation-Using-a-26-GHz-Multi-Beam-Communication-Testbed"><a href="#Vital-Signs-Estimation-Using-a-26-GHz-Multi-Beam-Communication-Testbed" class="headerlink" title="Vital Signs Estimation Using a 26 GHz Multi-Beam Communication Testbed"></a>Vital Signs Estimation Using a 26 GHz Multi-Beam Communication Testbed</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11275">http://arxiv.org/abs/2311.11275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miquel Sellés Valls, Sofie Pollin, Ying Wang, Rizqi Hersyandika, Andre Kokkeler, Yang Miao</li>
<li>for: 这个论文旨在开发一种基于26GHz多束通信测试床的血气监测管理系统，以实现无接触的人体血气监测。</li>
<li>methods: 该系统使用多束通信技术，并利用空间排序的扫描方式来估计呼吸和心跳速率。在单人场景下，使用频率域干扰滤波来提高性能，而在多人场景下，使用K-means算法来提取多个人的血气监测数据。</li>
<li>results: 测试结果显示，在单人场景下， breath rate和heart rate的估计 error 可以达到2 beat&#x2F;分钟以下，而在多人场景下，可以分别提取出每个人的血气监测数据。这些结果表明，该系统可以准确地检测人体血气监测，并且可以在不同的场景下进行无接触的监测。<details>
<summary>Abstract</summary>
This paper presents a novel pipeline for vital sign monitoring using a 26 GHz multi-beam communication testbed. In context of Joint Communication and Sensing (JCAS), the advanced communication capability at millimeter-wave bands is comparable to the radio resource of radars and is promising to sense the surrounding environment. Being able to communicate and sense the vital sign of humans present in the environment will enable new vertical services of telecommunication, i.e., remote health monitoring. The proposed processing pipeline leverages spatially orthogonal beams to estimate the vital sign - breath rate and heart rate - of single and multiple persons in static scenarios from the raw Channel State Information samples. We consider both monostatic and bistatic sensing scenarios. For monostatic scenario, we employ the phase time-frequency calibration and Discrete Wavelet Transform to improve the performance compared to the conventional Fast Fourier Transform based methods. For bistatic scenario, we use K-means clustering algorithm to extract multi-person vital signs due to the distinct frequency-domain signal feature between single and multi-person scenarios. The results show that the estimated breath rate and heart rate reach below 2 beats per minute (bpm) error compared to the reference captured by on-body sensor for the single-person monostatic sensing scenario with body-transceiver distance up to 2 m, and the two-person bistatic sensing scenario with BS-UE distance up to 4 m. The presented work does not optimize the OFDM waveform parameters for sensing; it demonstrates a promising JCAS proof-of-concept in contact-free vital sign monitoring using mmWave multi-beam communication systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Link-Streams-as-a-Generalization-of-Graphs-and-Time-Series"><a href="#Link-Streams-as-a-Generalization-of-Graphs-and-Time-Series" class="headerlink" title="Link Streams as a Generalization of Graphs and Time Series"></a>Link Streams as a Generalization of Graphs and Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11187">http://arxiv.org/abs/2311.11187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esteban Bautista, Matthieu Latapy</li>
<li>for: 本文旨在探讨链流的ormalism的开发，具体来说是将链流扩展到时间序列之上。</li>
<li>methods: 本文使用了图论的扩展来建立链流的正式形式，并将时间序列扩展到关系维度。</li>
<li>results: 本文表明了链流可以看作是时间序列的扩展，并开展了链流上的信号处理概念的扩展。<details>
<summary>Abstract</summary>
A link stream is a set of possibly weighted triplets (t, u, v) modeling that u and v interacted at time t. Link streams offer an effective model for datasets containing both temporal and relational information, making their proper analysis crucial in many applications. They are commonly regarded as sequences of graphs or collections of time series. Yet, a recent seminal work demonstrated that link streams are more general objects of which graphs are only particular cases. It therefore started the construction of a dedicated formalism for link streams by extending graph theory. In this work, we contribute to the development of this formalism by showing that link streams also generalize time series. In particular, we show that a link stream corresponds to a time-series extended to a relational dimension, which opens the door to also extend the framework of signal processing to link streams. We therefore develop extensions of numerous signal concepts to link streams: from elementary ones like energy, correlation, and differentiation, to more advanced ones like Fourier transform and filters.
</details>
<details>
<summary>摘要</summary>
一个链流是一组可能权重的三元组（t, u, v），表示在时间t上u和v之间的交互。链流提供了包含时间和关系信息的数据集模型，其分析在许多应用中非常重要。它们通常被视为时间序列或图集。然而，一篇最近的著名论文表明，链流是图集的更一般的对象。因此，它开始了链流的专门 формальismus的建构，通过扩展图论。在这项工作中，我们贡献了链流 formalism的发展，并证明了链流也拓展了时间序列。具体来说，一个链流对应于一个扩展到关系维度的时间序列，这开门了将信号处理框架扩展到链流的可能性。因此，我们开发了链流上许多信号概念的扩展：从基础的一些如能量、相关性和导数，到更高级的一些如傅里叶变换和筛选器。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/eess.SP_2023_11_19/" data-id="clpxp6cei01i5ee884k1o3gm3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/cs.CV_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T13:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/cs.CV_2023_11_18/">cs.CV - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Diverse-Shape-Completion-via-Style-Modulated-Generative-Adversarial-Networks"><a href="#Diverse-Shape-Completion-via-Style-Modulated-Generative-Adversarial-Networks" class="headerlink" title="Diverse Shape Completion via Style Modulated Generative Adversarial Networks"></a>Diverse Shape Completion via Style Modulated Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11184">http://arxiv.org/abs/2311.11184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wesley Khademi, Li Fuxin</li>
<li>For: 本文提出了一种新的 Conditional Generative Adversarial Network (CGAN)，用于完成部分观察到的三维对象的形状。* Methods: 该网络使用了风格修饰来实现多种可能的完成，并通过提取完整形状的风格代码来实现更好的完成。它还引入了多尺度多样性罚款和批判器，以避免 conditional mode collapse 并在不需要多个真实完成的前提下训练。* Results: 对多个 sintetic 和实际数据集进行评估，该方法能够有效地尊重部分观察，同时获得更多的多样性在完成中。<details>
<summary>Abstract</summary>
Shape completion aims to recover the full 3D geometry of an object from a partial observation. This problem is inherently multi-modal since there can be many ways to plausibly complete the missing regions of a shape. Such diversity would be indicative of the underlying uncertainty of the shape and could be preferable for downstream tasks such as planning. In this paper, we propose a novel conditional generative adversarial network that can produce many diverse plausible completions of a partially observed point cloud. To enable our network to produce multiple completions for the same partial input, we introduce stochasticity into our network via style modulation. By extracting style codes from complete shapes during training, and learning a distribution over them, our style codes can explicitly carry shape category information leading to better completions. We further introduce diversity penalties and discriminators at multiple scales to prevent conditional mode collapse and to train without the need for multiple ground truth completions for each partial input. Evaluations across several synthetic and real datasets demonstrate that our method achieves significant improvements in respecting the partial observations while obtaining greater diversity in completions.
</details>
<details>
<summary>摘要</summary>
shape completion 目标是从部分观察到的3D形状恢复完整的形状。这是一个多Modal的问题，因为可以有多种可能性地完成部分观察到的区域。这种多样性表示形状的下面 uncertainty 和可能性，这对下游任务 such as 规划来说是有利的。在这篇论文中，我们提出了一种新的 conditional 生成 Adversarial network，可以生成多种可能性的完整的点云。为了使我们的网络可以生成同一个部分输入多个完整的结果，我们在网络中引入了随机性。在训练中，我们从完整的形状中提取了style code，学习了这些代码的分布，这些代码可以显式地携带形状类别信息，从而得到更好的 completions。我们还引入了多个缩放因子和权重来避免 conditional 模式崩溃和不需要多个真实的完整结果来训练。在多个 sintetic 和实际的数据集上进行了评估，我们的方法在尊重部分观察的同时获得了更大的多样性。
</details></li>
</ul>
<hr>
<h2 id="Active-Prompt-Learning-in-Vision-Language-Models"><a href="#Active-Prompt-Learning-in-Vision-Language-Models" class="headerlink" title="Active Prompt Learning in Vision Language Models"></a>Active Prompt Learning in Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11178">http://arxiv.org/abs/2311.11178</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Jihwan Bang, Sumyeong Ahn, Jae-Gil Lee</li>
<li>for: 本研究旨在适应预训练视觉语言模型（VLM）在活动学习框架下进行适应。</li>
<li>methods: 我们提出了一种新的活动学习框架，称为PCB，以适应预训练VLM。PCB利用VLM的知识提供偏好，以解决标签选择的不均衡问题。</li>
<li>results: 我们在七个实际世界数据集上进行了实验，结果表明PCB比普通的活动学习和随机抽样方法表现更好。<details>
<summary>Abstract</summary>
Pre-trained Vision Language Models (VLMs) have demonstrated notable progress in various zero-shot tasks, such as classification and retrieval. Despite their performance, because improving performance on new tasks requires task-specific knowledge, their adaptation is essential. While labels are needed for the adaptation, acquiring them is typically expensive. To overcome this challenge, active learning, a method of achieving a high performance by obtaining labels for a small number of samples from experts, has been studied. Active learning primarily focuses on selecting unlabeled samples for labeling and leveraging them to train models. In this study, we pose the question, "how can the pre-trained VLMs be adapted under the active learning framework?" In response to this inquiry, we observe that (1) simply applying a conventional active learning framework to pre-trained VLMs even may degrade performance compared to random selection because of the class imbalance in labeling candidates, and (2) the knowledge of VLMs can provide hints for achieving the balance before labeling. Based on these observations, we devise a novel active learning framework for VLMs, denoted as PCB. To assess the effectiveness of our approach, we conduct experiments on seven different real-world datasets, and the results demonstrate that PCB surpasses conventional active learning and random sampling methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LOSTU-Fast-Scalable-and-Uncertainty-Aware-Triangulation"><a href="#LOSTU-Fast-Scalable-and-Uncertainty-Aware-Triangulation" class="headerlink" title="LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation"></a>LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11171">http://arxiv.org/abs/2311.11171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sébastien Henry, John A. Christian</li>
<li>for: 提供一种快速、可扩展、统计优化的三角分解方法（LOSTU），用于结构从运动（SfM）管道中的点云三角分解。</li>
<li>methods: 该方法基于最近的发现，并且不同于传统的$L_2$三角分解方法，可以考虑3D点云不确定性。</li>
<li>results: LOSTU可以减少3D重建错误，并且可以更快速于Levenberg-Marquardt优化方案。<details>
<summary>Abstract</summary>
Triangulation algorithms often aim to minimize the reprojection ($L_2$) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional $L_2$ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes.
</details>
<details>
<summary>摘要</summary>
通常，三角化算法的目标是减少($L_2$)  reprojection 错误，但这只提供了无摄像头参数或摄像头姿态错误时的最大可能性估计。尽管最近有一些进步，仍有许多结构从运动（SfM）管道使用老的三角化算法。这项工作利用最近的发现，提供一种快速、可扩展、统计优化的三角化方法，称为LOSTU。结果显示，LOSTU 常常生成较低的3D重建错误，并且可以更多的点 successfully triangulated。此外，LOSTU 还可以比 Levenberg-Marquardt（或类似）优化方案更快。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Feature-Extractors-for-Reinforcement-Learning-Based-Semiconductor-Defect-Localization"><a href="#Benchmarking-Feature-Extractors-for-Reinforcement-Learning-Based-Semiconductor-Defect-Localization" class="headerlink" title="Benchmarking Feature Extractors for Reinforcement Learning-Based Semiconductor Defect Localization"></a>Benchmarking Feature Extractors for Reinforcement Learning-Based Semiconductor Defect Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11145">http://arxiv.org/abs/2311.11145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enrique Dehaerne, Bappaditya Dey, Sandip Halder, Stefan De Gendt</li>
<li>for: 测试和检测半导体板件中的缺陷</li>
<li>methods: 使用深度强化学习（RL）方法进行缺陷定位，并评估不同的特征提取器的效果</li>
<li>results: 评估18个代理人在不同的特征提取器下进行定位缺陷的效果，并讨论RL基本框架在半导体缺陷定位中的优点和缺点。<details>
<summary>Abstract</summary>
As semiconductor patterning dimensions shrink, more advanced Scanning Electron Microscopy (SEM) image-based defect inspection techniques are needed. Recently, many Machine Learning (ML)-based approaches have been proposed for defect localization and have shown impressive results. These methods often rely on feature extraction from a full SEM image and possibly a number of regions of interest. In this study, we propose a deep Reinforcement Learning (RL)-based approach to defect localization which iteratively extracts features from increasingly smaller regions of the input image. We compare the results of 18 agents trained with different feature extractors. We discuss the advantages and disadvantages of different feature extractors as well as the RL-based framework in general for semiconductor defect localization.
</details>
<details>
<summary>摘要</summary>
As semiconductor patterning dimensions shrink, more advanced Scanning Electron Microscopy (SEM) image-based defect inspection techniques are needed. Recently, many Machine Learning (ML)-based approaches have been proposed for defect localization and have shown impressive results. These methods often rely on feature extraction from a full SEM image and possibly a number of regions of interest. In this study, we propose a deep Reinforcement Learning (RL)-based approach to defect localization which iteratively extracts features from increasingly smaller regions of the input image. We compare the results of 18 agents trained with different feature extractors. We discuss the advantages and disadvantages of different feature extractors as well as the RL-based framework in general for semiconductor defect localization.Here's the text in Traditional Chinese:为了应对半导体 Patterning 的缩小，需要更进一步的 Scanning Electron Microscopy (SEM) 图像基于抗错方法。最近，许多 Machine Learning (ML) 基于方法已经被提出来进行抗错定位，并且获得了优异的结果。这些方法通常将特征提取自全SEM图像以及可能的一些区域区域。在这一研究中，我们提出了一个深度强化学习 (RL) 基于的抗错定位方法，这个方法会逐步提取从输入图像中的特征，并且将其与RL网络进行结合。我们将训练 18 个代理人使用不同的特征提取器，并且比较它们的结果。我们会讨论不同的特征提取器优点和缺点，以及RL 基于架构的一般优点和缺点。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Uncertainty-in-Landslide-Segmentation-Models"><a href="#Estimating-Uncertainty-in-Landslide-Segmentation-Models" class="headerlink" title="Estimating Uncertainty in Landslide Segmentation Models"></a>Estimating Uncertainty in Landslide Segmentation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11138">http://arxiv.org/abs/2311.11138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savinay Nagendra, Chaopeng Shen, Daniel Kifer</li>
<li>for: 这个论文的目的是为了提供高质量、大规模的滥覆区域风险地区数据集，以便进行防范和减轻滥覆的准备和防控工作。</li>
<li>methods: 这篇论文使用了深度学习模型进行滥覆分割（像素标注），并评估了多种不需要建立新的架构的方法来评估像素级别的不确定性。</li>
<li>results: 实验结果表明，使用测试时数据拟合法（Test-Time Augmentation）方法可以在不同的模型和指标下提供最高质量的不确定性评估结果。<details>
<summary>Abstract</summary>
Landslides are a recurring, widespread hazard. Preparation and mitigation efforts can be aided by a high-quality, large-scale dataset that covers global at-risk areas. Such a dataset currently does not exist and is impossible to construct manually. Recent automated efforts focus on deep learning models for landslide segmentation (pixel labeling) from satellite imagery. However, it is also important to characterize the uncertainty or confidence levels of such segmentations. Accurate and robust uncertainty estimates can enable low-cost (in terms of manual labor) oversight of auto-generated landslide databases to resolve errors, identify hard negative examples, and increase the size of labeled training data. In this paper, we evaluate several methods for assessing pixel-level uncertainty of the segmentation. Three methods that do not require architectural changes were compared, including Pre-Threshold activations, Monte-Carlo Dropout and Test-Time Augmentation -- a method that measures the robustness of predictions in the face of data augmentation. Experimentally, the quality of the latter method was consistently higher than the others across a variety of models and metrics in our dataset.
</details>
<details>
<summary>摘要</summary>
landslide 是一种常 recurs 的、广泛存在的威胁。 prep 和 mitigation efforts 可以通过高质量、大规模的数据集来得到支持。目前没有这样的数据集，并且无法手动构建。 latest 自动化努力是使用深度学习模型进行滥舟分割（像素标注），但也重要是确定这些分割的不确定性或信任水平。准确和可靠的不确定性估计可以减少人工劳动成本，以便对自动生成的滥舟数据库进行低成本监督，解决错误、识别硬例外并增加标注训练数据的大小。在这篇论文中，我们评估了一些方法来评估像素级别的不确定性。我们比较了三种方法，包括 Pre-Threshold 活动、Monte-Carlo Dropout 和 Test-Time Augmentation。实验表明，后一种方法在不同的模型和指标上表现了最高的质量。
</details></li>
</ul>
<hr>
<h2 id="Invariant-based-Mapping-of-Space-During-General-Motion-of-an-Observer"><a href="#Invariant-based-Mapping-of-Space-During-General-Motion-of-an-Observer" class="headerlink" title="Invariant-based Mapping of Space During General Motion of an Observer"></a>Invariant-based Mapping of Space During General Motion of an Observer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11130">http://arxiv.org/abs/2311.11130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan D. Yepes, Daniel Raviv</li>
<li>for: 这个论文探讨了基于视觉运动的 invariants，导致一个新的即时领域，在这个领域中，站ARY environment被看作是不变的，即使图像在摄像机运动中不断改变，并且可以探测和避免特定子空间中的障碍物，以及检测运动 objetcs。</li>
<li>methods: 这个论文使用了非线性函数， derivated from measurable optical flow，这些函数与三维几何 invariants相连接。</li>
<li>results: 作者通过实验和实践，证明了这种方法可以在具有相机运动的情况下，保持图像的stationary environment不变，并且可以检测和分类运动 объекcs。<details>
<summary>Abstract</summary>
This paper explores visual motion-based invariants, resulting in a new instantaneous domain where: a) the stationary environment is perceived as unchanged, even as the 2D images undergo continuous changes due to camera motion, b) obstacles can be detected and potentially avoided in specific subspaces, and c) moving objects can potentially be detected. To achieve this, we make use of nonlinear functions derived from measurable optical flow, which are linked to geometric 3D invariants.   We present simulations involving a camera that translates and rotates relative to a 3D object, capturing snapshots of the camera projected images. We show that the object appears unchanged in the new domain over time. We process real data from the KITTI dataset and demonstrate how to segment space to identify free navigational regions and detect obstacles within a predetermined subspace. Additionally, we present preliminary results, based on the KITTI dataset, on the identification and segmentation of moving objects, as well as the visualization of shape constancy.   This representation is straightforward, relying on functions for the simple de-rotation of optical flow. This representation only requires a single camera, it is pixel-based, making it suitable for parallel processing, and it eliminates the necessity for 3D reconstruction techniques.
</details>
<details>
<summary>摘要</summary>
a) The stationary environment is perceived as unchanged, even as the 2D images undergo continuous changes due to camera motion.b) Obstacles can be detected and potentially avoided in specific subspaces.c) Moving objects can potentially be detected.To achieve this, we use nonlinear functions derived from measurable optical flow, which are linked to geometric 3D invariants. We present simulations involving a camera that translates and rotates relative to a 3D object, capturing snapshots of the camera-projected images. We show that the object appears unchanged in the new domain over time.We process real data from the KITTI dataset and demonstrate how to segment space to identify free navigational regions and detect obstacles within a predetermined subspace. Additionally, we present preliminary results, based on the KITTI dataset, on the identification and segmentation of moving objects, as well as the visualization of shape constancy.This representation is straightforward, relying on functions for the simple de-rotation of optical flow. This representation only requires a single camera, it is pixel-based, making it suitable for parallel processing, and it eliminates the necessity for 3D reconstruction techniques.
</details></li>
</ul>
<hr>
<h2 id="SecondPose-SE-3-Consistent-Dual-Stream-Feature-Fusion-for-Category-Level-Pose-Estimation"><a href="#SecondPose-SE-3-Consistent-Dual-Stream-Feature-Fusion-for-Category-Level-Pose-Estimation" class="headerlink" title="SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation"></a>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11125">http://arxiv.org/abs/2311.11125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yamei Chen, Yan Di, Guangyao Zhai, Fabian Manhardt, Chenyangguang Zhang, Ruida Zhang, Federico Tombari, Nassir Navab, Benjamin Busam</li>
<li>for: 这篇研究目的是估计物体的6D姿势和3D大小，特别是面对着巨量内类形态的挑战。</li>
<li>methods: 本研究使用物体特有的几何特征，与DINOv2的semantic数据组合，实现了SE(3)-不变的几何特征抽出，并与DINOv2的特征进行对点对焦，实现了对焦对应的物体表现。</li>
<li>results: 实验结果显示，SecondPose在NOCS-REAL275上比前一代最高12.4%提高，并在更复杂的HouseCat6D上仍然超越其他竞争对手。<details>
<summary>Abstract</summary>
Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of capturing this variation. To address this issue, we present SecondPose, a novel approach integrating object-specific geometric features with semantic category priors from DINOv2. Leveraging the advantage of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin. The code will be released soon.
</details>
<details>
<summary>摘要</summary>
Category-level object pose estimation aims to predict the 6D pose and 3D size of objects from known categories, but it often struggles with large intra-class shape variation. Existing methods using mean shapes often fail to capture this variation. To address this issue, we propose SecondPose, a novel approach that integrates object-specific geometric features with semantic category priors from DINOv2. By leveraging the advantages of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% improvement over the state-of-the-art. Moreover, on the more complex HouseCat6D dataset, which provides photometrically challenging objects, SecondPose still outperforms other competitors by a large margin. The code will be released soon.Here's the translation in Traditional Chinese:Category-level object pose estimation aims to predict the 6D pose and 3D size of objects from known categories, but it often struggles with large intra-class shape variation. Existing methods using mean shapes often fail to capture this variation. To address this issue, we propose SecondPose, a novel approach that integrates object-specific geometric features with semantic category priors from DINOv2. By leveraging the advantages of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% improvement over the state-of-the-art. Moreover, on the more complex HouseCat6D dataset, which provides photometrically challenging objects, SecondPose still outperforms other competitors by a large margin. The code will be released soon.
</details></li>
</ul>
<hr>
<h2 id="ShapeMaker-Self-Supervised-Joint-Shape-Canonicalization-Segmentation-Retrieval-and-Deformation"><a href="#ShapeMaker-Self-Supervised-Joint-Shape-Canonicalization-Segmentation-Retrieval-and-Deformation" class="headerlink" title="ShapeMaker: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation"></a>ShapeMaker: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11106">http://arxiv.org/abs/2311.11106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Di, Chenyangguang Zhang, Chaowei Wang, Ruida Zhang, Guangyao Zhai, Yanyan Li, Bowen Fu, Xiangyang Ji, Shan Gao</li>
<li>for: 本文提出了一种自助学习框架ShapeMaker，用于同时进行形态均衡化、分割、检索和变换等四个高度相关的过程。</li>
<li>methods: 本文使用了一种独特的自助学习方法，同时具有分割、检索和变换等功能。具体来说，首先从 partially-observed 对象中提取出点级 affine-invariant 特征，然后利用这些特征预测semantically consistent的部分分 segmentation和对应的部分中心。接着，使用了一种轻量级的检索模块，将每个部分的特征集成为其检索 токен，然后与一个预设的数据库中的源形进行比较，以确定最接近的形状。最后，使用了一种基于部分中心的神经网络围栏变换模块，将检索到的形状与输入对象进行精准匹配。</li>
<li>results: 实验表明，ShapeMaker 在 Synthetic 数据集 PartNet、ComplementMe 和实际数据集 Scan2CAD 上表现出色，与竞争者相比，具有显著的优势。<details>
<summary>Abstract</summary>
In this paper, we present ShapeMaker, a unified self-supervised learning framework for joint shape canonicalization, segmentation, retrieval and deformation. Given a partially-observed object in an arbitrary pose, we first canonicalize the object by extracting point-wise affine-invariant features, disentangling inherent structure of the object with its pose and size. These learned features are then leveraged to predict semantically consistent part segmentation and corresponding part centers. Next, our lightweight retrieval module aggregates the features within each part as its retrieval token and compare all the tokens with source shapes from a pre-established database to identify the most geometrically similar shape. Finally, we deform the retrieved shape in the deformation module to tightly fit the input object by harnessing part center guided neural cage deformation. The key insight of ShapeMaker is the simultaneous training of the four highly-associated processes: canonicalization, segmentation, retrieval, and deformation, leveraging cross-task consistency losses for mutual supervision. Extensive experiments on synthetic datasets PartNet, ComplementMe, and real-world dataset Scan2CAD demonstrate that ShapeMaker surpasses competitors by a large margin. Codes will be released soon.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了ShapeMaker，一个独立学习框架，用于同时进行形态均衡化、分割、检索和变换。给定一个部分可见的物体，我们首先使用点精度不变的特征提取方法，提取物体的内在结构，并与姿态和大小相关。这些学习的特征然后用于预测相同分割和相应的中心点。接着，我们的轻量级检索模块将每个分割的特征作为检索token进行聚合，并将所有token与源形状库中的形状进行比较，以确定最接近的形状。最后，我们使用中心导航神经网络扭曲模块将检索到的形状与输入物体进行紧密匹配。ShapeMaker的关键思想是同时培养四个高度相关的过程：均衡化、分割、检索和变换，通过交叉任务一致损失来互相超级视图。我们在PartNet、ComplementMe和Scan2CAD等 sintetic数据集上进行了广泛的实验，结果显示ShapeMaker在与竞争对手进行比较时，具有很大的优势。代码即将发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-Out-of-Distribution-Robustness-of-Foundation-Models-in-Medical-Image-Segmentation"><a href="#On-the-Out-of-Distribution-Robustness-of-Foundation-Models-in-Medical-Image-Segmentation" class="headerlink" title="On the Out of Distribution Robustness of Foundation Models in Medical Image Segmentation"></a>On the Out of Distribution Robustness of Foundation Models in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11096">http://arxiv.org/abs/2311.11096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duy Minh Ho Nguyen, Tan Ngoc Pham, Nghiem Tuong Diep, Nghi Quoc Phan, Quang Pham, Vinh Tong, Binh T. Nguyen, Ngan Hoang Le, Nhat Ho, Pengtao Xie, Daniel Sonntag, Mathias Niepert</li>
<li>for: 本研究旨在探讨基础模型在医学图像分割任务中的鲁棒性，以便更好地适应不同的分布转移。</li>
<li>methods: 我们使用了各种基础模型，包括ViT和DeiT，并对其进行了微调。</li>
<li>results: 我们的实验结果表明，基础模型在不同的频率上具有更高的鲁棒性，并且我们还发展了一种新的 bayesian 不确定性估计方法，可以用于评估模型在不同数据集上的性能。<details>
<summary>Abstract</summary>
Constructing a robust model that can effectively generalize to test samples under distribution shifts remains a significant challenge in the field of medical imaging. The foundational models for vision and language, pre-trained on extensive sets of natural image and text data, have emerged as a promising approach. It showcases impressive learning abilities across different tasks with the need for only a limited amount of annotated samples. While numerous techniques have focused on developing better fine-tuning strategies to adapt these models for specific domains, we instead examine their robustness to domain shifts in the medical image segmentation task. To this end, we compare the generalization performance to unseen domains of various pre-trained models after being fine-tuned on the same in-distribution dataset and show that foundation-based models enjoy better robustness than other architectures. From here, we further developed a new Bayesian uncertainty estimation for frozen models and used them as an indicator to characterize the model's performance on out-of-distribution (OOD) data, proving particularly beneficial for real-world applications. Our experiments not only reveal the limitations of current indicators like accuracy on the line or agreement on the line commonly used in natural image applications but also emphasize the promise of the introduced Bayesian uncertainty. Specifically, lower uncertainty predictions usually tend to higher out-of-distribution (OOD) performance.
</details>
<details>
<summary>摘要</summary>
建立一个坚固的模型，使其能够有效地泛化到测试样本下的分布变化，是领域医学影像处理中的一大挑战。基础模型，在大量自然图像和文本数据上进行预训练，已经出现为一种有前途的方法。它在不同任务上展示出了卓越的学习能力，只需要有限量的标注样本。虽然许多技术专注于开发更好的细化策略，以适应特定领域，但我们则研究基础模型在医学图像分割任务中的Robustness。为此，我们比较了不同预训练模型在未看到的领域中的泛化性能，并发现基础模型在这个方面表现出了更好的Robustness。此外，我们还开发了一种新的 bayesian uncertainty estimation 方法，并用其作为指标，来评估模型在未分布（OOD）数据上的性能。我们的实验结果不仅揭示了现有的指标，如精度在线或者线上协调率在自然图像应用中的局限性，而且还强调了我们引入的 bayesian uncertainty 的承诺。具体来说， Lower uncertainty predictions 通常与更高的OOD性能相关。
</details></li>
</ul>
<hr>
<h2 id="LightBTSeg-A-lightweight-breast-tumor-segmentation-model-using-ultrasound-images-via-dual-path-joint-knowledge-distillation"><a href="#LightBTSeg-A-lightweight-breast-tumor-segmentation-model-using-ultrasound-images-via-dual-path-joint-knowledge-distillation" class="headerlink" title="LightBTSeg: A lightweight breast tumor segmentation model using ultrasound images via dual-path joint knowledge distillation"></a>LightBTSeg: A lightweight breast tumor segmentation model using ultrasound images via dual-path joint knowledge distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11086">http://arxiv.org/abs/2311.11086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongjiang Guo, Shengwen Wang, Hao Dang, Kangle Xiao, Yaru Yang, Wenpei Liu, Tongtong Liu, Yiying Wan</li>
<li>for: 这个研究的目的是为了提高乳腺癌检测的精确性，以提高乳腺癌的早期诊断和治疗。</li>
<li>methods: 这个研究使用了一种名为LightBTSeg的双路共同知识传授框架，它利用了双教师模型来表达乳腺癌的细部特征。</li>
<li>results: 实验结果显示，LightBTSeg在乳腺癌检测中表现出色，比其他Counterparts更高精确。<details>
<summary>Abstract</summary>
The accurate segmentation of breast tumors is an important prerequisite for lesion detection, which has significant clinical value for breast tumor research. The mainstream deep learning-based methods have achieved a breakthrough. However, these high-performance segmentation methods are formidable to implement in clinical scenarios since they always embrace high computation complexity, massive parameters, slow inference speed, and huge memory consumption. To tackle this problem, we propose LightBTSeg, a dual-path joint knowledge distillation framework, for lightweight breast tumor segmentation. Concretely, we design a double-teacher model to represent the fine-grained feature of breast ultrasound according to different semantic feature realignments of benign and malignant breast tumors. Specifically, we leverage the bottleneck architecture to reconstruct the original Attention U-Net. It is regarded as a lightweight student model named Simplified U-Net. Then, the prior knowledge of benign and malignant categories is utilized to design the teacher network combined dual-path joint knowledge distillation, which distills the knowledge from cumbersome benign and malignant teachers to a lightweight student model. Extensive experiments conducted on breast ultrasound images (Dataset BUSI) and Breast Ultrasound Dataset B (Dataset B) datasets demonstrate that LightBTSeg outperforms various counterparts.
</details>
<details>
<summary>摘要</summary>
importante para la detección de lesiones en el diagnóstico de tumores mamarios. Los métodos basados en aprendizaje profundo mainstream han logrado un avance significativo. Sin embargo, estos métodos de alta performance son difíciles de implementar en escenarios clínicos debido a su complejidad computacional alta, parámetros masivos, velocidad de inferencia lenta y consumo de memoria grande. Para abordar este problema, propusimos LightBTSeg, un marco de distilación de conocimiento dual-path, para segmentación de tumores mamarios livianos.En detalle, diseñamos un modelo doble-maestro para representar la característica fine-grained de la ecografía de mama según diferentes realineaciones semánticas de tumores benignos y malignos. Utilizamos la arquitectura de bottleneck para reconstruir la red Attention U-Net original. Se considera un modelo estudiantil liviano llamado Simplified U-Net. Luego, utilizamos la información previa de categorías benignas y malignas para diseñar la red maestra combinada con distilación de conocimiento dual-path, que transmite el conocimiento de los maestros pesados benignos y malignos a un modelo estudiantil liviano.Los experimentos extensivos realizados en las imágenes de ecografía de mama (dataset BUSI) y el dataset Breast Ultrasound Dataset B (dataset B) demostraron que LightBTSeg supera a sus pares.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Transformer-Based-Segmentation-for-Breast-Cancer-Diagnosis-using-Auto-Augmentation-and-Search-Optimisation-Techniques"><a href="#Enhancing-Transformer-Based-Segmentation-for-Breast-Cancer-Diagnosis-using-Auto-Augmentation-and-Search-Optimisation-Techniques" class="headerlink" title="Enhancing Transformer-Based Segmentation for Breast Cancer Diagnosis using Auto-Augmentation and Search Optimisation Techniques"></a>Enhancing Transformer-Based Segmentation for Breast Cancer Diagnosis using Auto-Augmentation and Search Optimisation Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11065">http://arxiv.org/abs/2311.11065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leon Hamnett, Mary Adewunmi, Modinat Abayomi, Kayode Raheem, Fahad Ahmed<br>for:This paper aims to improve the accuracy and robustness of breast cancer cell segmentation in histology slides using automated image augmentation selection and search optimization strategies.methods:The proposed methodology combines RandAugment with Tree-based Parzen Estimator to identify optimal values for image augmentations and their associated parameters, leading to enhanced segmentation performance.results:The proposed methodology leads to segmentation models that are more resilient to variations in histology slides while maintaining high levels of segmentation performance, with improved segmentation of the tumour class compared to previous research. The best result after applying the augmentations is a Dice Score of 84.08 and an IoU score of 72.54 when segmenting the tumour class.<details>
<summary>Abstract</summary>
Breast cancer remains a critical global health challenge, necessitating early and accurate detection for effective treatment. This paper introduces a methodology that combines automated image augmentation selection (RandAugment) with search optimisation strategies (Tree-based Parzen Estimator) to identify optimal values for the number of image augmentations and the magnitude of their associated augmentation parameters, leading to enhanced segmentation performance. We empirically validate our approach on breast cancer histology slides, focusing on the segmentation of cancer cells. A comparative analysis of state-of-the-art transformer-based segmentation models is conducted, including SegFormer, PoolFormer, and MaskFormer models, to establish a comprehensive baseline, before applying the augmentation methodology. Our results show that the proposed methodology leads to segmentation models that are more resilient to variations in histology slides whilst maintaining high levels of segmentation performance, and show improved segmentation of the tumour class when compared to previous research. Our best result after applying the augmentations is a Dice Score of 84.08 and an IoU score of 72.54 when segmenting the tumour class. The primary contribution of this paper is the development of a methodology that enhances segmentation performance while ensuring model robustness to data variances. This has significant implications for medical practitioners, enabling the development of more effective machine learning models for clinical applications to identify breast cancer cells from histology slides. Furthermore, the codebase accompanying this research will be released upon publication. This will facilitate further research and application development based on our methodology, thereby amplifying its impact.
</details>
<details>
<summary>摘要</summary>
乳癌仍然是全球健康挑战之一，需要早期精准的检测以实现有效的治疗。本文介绍一种方法，将自动图像增强选择（RandAugment）与搜索优化策略（Tree-based Parzen Estimator）结合，以确定图像增强数量和相关增强参数的优化值，以提高分 segmentation性能。我们对乳癌 histology 胶卷进行了实验，专注于癌细胞分 segmentation。我们进行了现有 transformer 基本模型的比较分析，包括 SegFormer、PoolFormer 和 MaskFormer 模型，以建立全面的基准。我们的结果表明，提posed方法可以提高模型对数据变化的抗性，保持高水平的分 segmentation性能，并在识别癌细胞方面显示了改进的分 segmentation性能。我们的最佳结果是 Dice 分数84.08和 IoU 分数72.54，分 segmentation癌细胞类。本文的主要贡献是开发了一种能够提高分 segmentation性能的同时保持模型对数据变化的抗性的方法，这有着重要的医疗应用。此外，本文的代码库将在出版时发布，以便进一步的研究和应用开发，从而增强其影响。
</details></li>
</ul>
<hr>
<h2 id="HIDRO-VQA-High-Dynamic-Range-Oracle-for-Video-Quality-Assessment"><a href="#HIDRO-VQA-High-Dynamic-Range-Oracle-for-Video-Quality-Assessment" class="headerlink" title="HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment"></a>HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11059">http://arxiv.org/abs/2311.11059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreshth Saini, Avinab Saha, Alan C. Bovik</li>
<li>for: 这篇论文旨在提供高动态范围视频质量评估（VQA）模型，用于提供高精度的HDR视频质量评估。</li>
<li>methods: 该模型使用自动生成的相似性推训策略，将SDR视频中的质量感知特征转移到HDR视频中，无需标注。</li>
<li>results: 研究发现，通过自动生成的相似性推训策略，可以将SDR视频中的质量感知特征转移到HDR视频中，并在LIVE-HDR VQA数据库上达到了状态之最好的性能。<details>
<summary>Abstract</summary>
We introduce HIDRO-VQA, a no-reference (NR) video quality assessment model designed to provide precise quality evaluations of High Dynamic Range (HDR) videos. HDR videos exhibit a broader spectrum of luminance, detail, and color than Standard Dynamic Range (SDR) videos. As HDR content becomes increasingly popular, there is a growing demand for video quality assessment (VQA) algorithms that effectively address distortions unique to HDR content. To address this challenge, we propose a self-supervised contrastive fine-tuning approach to transfer quality-aware features from the SDR to the HDR domain, utilizing unlabeled HDR videos. Our findings demonstrate that self-supervised pre-trained neural networks on SDR content can be further fine-tuned in a self-supervised setting using limited unlabeled HDR videos to achieve state-of-the-art performance on the only publicly available VQA database for HDR content, the LIVE-HDR VQA database. Moreover, our algorithm can be extended to the Full Reference VQA setting, also achieving state-of-the-art performance. Our code is available publicly at https://github.com/avinabsaha/HIDRO-VQA.
</details>
<details>
<summary>摘要</summary>
我们介绍HIDRO-VQA，一个不受参考（NR）影像质量评估模型，旨在为高动态范围（HDR）影像提供精确的质量评估。HDR影像比标准动态范围（SDR）影像更具宽频谱、细节和颜色，随着HDR内容的普及，需要一种能有效地处理HDR内容的质量评估算法。为解决这个挑战，我们提议一种基于自我超级vised contrastive fine-tuning的方法，将SDR频谱中的质量感知特征转移到HDR频谱中，使用有限的无标注HDR影像进行自我超级vised fine-tuning。我们的发现表明，可以在自我超级vised Setting中使用SDR内容的自我超级vised预训练网络，通过有限的无标注HDR影像进行自我超级vised fine-tuning，以达到LIVE-HDR VQA数据库中的最新纪录。此外，我们的算法还可以扩展到全参考VQA Setting，也达到了最新纪录。我们的代码可以在https://github.com/avinabsaha/HIDRO-VQA上获得。
</details></li>
</ul>
<hr>
<h2 id="Hyperbolic-Space-with-Hierarchical-Margin-Boosts-Fine-Grained-Learning-from-Coarse-Labels"><a href="#Hyperbolic-Space-with-Hierarchical-Margin-Boosts-Fine-Grained-Learning-from-Coarse-Labels" class="headerlink" title="Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels"></a>Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11019">http://arxiv.org/abs/2311.11019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu-Lin Xu, Yifan Sun, Faen Zhang, Anqi Xu, Xiu-Shen Wei, Yi Yang</li>
<li>for: 这篇论文的目的是提出一种新的方法，用于从粗略标签中学习细化嵌入。</li>
<li>methods: 该方法使用了一种新的嵌入方法，将视觉嵌入映射到一个希пербо利空间中，并在这个空间中应用一种层次cosine margin方式来增强嵌入的推理能力。</li>
<li>results: 经过广泛的实验，该方法在五个 benchmark 数据集上达到了最佳效果，超过了竞争方法的表现。<details>
<summary>Abstract</summary>
Learning fine-grained embeddings from coarse labels is a challenging task due to limited label granularity supervision, i.e., lacking the detailed distinctions required for fine-grained tasks. The task becomes even more demanding when attempting few-shot fine-grained recognition, which holds practical significance in various applications. To address these challenges, we propose a novel method that embeds visual embeddings into a hyperbolic space and enhances their discriminative ability with a hierarchical cosine margins manner. Specifically, the hyperbolic space offers distinct advantages, including the ability to capture hierarchical relationships and increased expressive power, which favors modeling fine-grained objects. Based on the hyperbolic space, we further enforce relatively large/small similarity margins between coarse/fine classes, respectively, yielding the so-called hierarchical cosine margins manner. While enforcing similarity margins in the regular Euclidean space has become popular for deep embedding learning, applying it to the hyperbolic space is non-trivial and validating the benefit for coarse-to-fine generalization is valuable. Extensive experiments conducted on five benchmark datasets showcase the effectiveness of our proposed method, yielding state-of-the-art results surpassing competing methods.
</details>
<details>
<summary>摘要</summary>
学习细腻嵌入从粗略标签的挑战 task  Due to limited label granularity supervision, i.e., lacking the detailed distinctions required for fine-grained tasks. The task becomes even more demanding when attempting few-shot fine-grained recognition, which holds practical significance in various applications. To address these challenges, we propose a novel method that embeds visual embeddings into a hyperbolic space and enhances their discriminative ability with a hierarchical cosine margins manner. Specifically, the hyperbolic space offers distinct advantages, including the ability to capture hierarchical relationships and increased expressive power, which favors modeling fine-grained objects. Based on the hyperbolic space, we further enforce relatively large/small similarity margins between coarse/fine classes, respectively, yielding the so-called hierarchical cosine margins manner. While enforcing similarity margins in the regular Euclidean space has become popular for deep embedding learning, applying it to the hyperbolic space is non-trivial and validating the benefit for coarse-to-fine generalization is valuable. Extensive experiments conducted on five benchmark datasets showcase the effectiveness of our proposed method, yielding state-of-the-art results surpassing competing methods.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Improving-Adversarial-Transferability-by-Stable-Diffusion"><a href="#Improving-Adversarial-Transferability-by-Stable-Diffusion" class="headerlink" title="Improving Adversarial Transferability by Stable Diffusion"></a>Improving Adversarial Transferability by Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11017">http://arxiv.org/abs/2311.11017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayang Liu, Siyu Zhu, Siyuan Liang, Jie Zhang, Han Fang, Weiming Zhang, Ee-Chien Chang</li>
<li>For: The paper is written to explore the potential of leveraging data generated by Stable Diffusion to boost adversarial transferability in the black-box scenario.* Methods: The paper introduces a novel attack method called Stable Diffusion Attack Method (SDAM), which incorporates samples generated by Stable Diffusion to augment input images. Additionally, the paper proposes a fast variant of SDAM to reduce computational overhead while preserving high adversarial transferability.* Results: The paper demonstrates that the proposed method outperforms state-of-the-art baselines by a substantial margin. The approach is also compatible with existing transfer-based attacks to further enhance adversarial transferability.<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are susceptible to adversarial examples, which introduce imperceptible perturbations to benign samples, deceiving DNN predictions. While some attack methods excel in the white-box setting, they often struggle in the black-box scenario, particularly against models fortified with defense mechanisms. Various techniques have emerged to enhance the transferability of adversarial attacks for the black-box scenario. Among these, input transformation-based attacks have demonstrated their effectiveness. In this paper, we explore the potential of leveraging data generated by Stable Diffusion to boost adversarial transferability. This approach draws inspiration from recent research that harnessed synthetic data generated by Stable Diffusion to enhance model generalization. In particular, previous work has highlighted the correlation between the presence of both real and synthetic data and improved model generalization. Building upon this insight, we introduce a novel attack method called Stable Diffusion Attack Method (SDAM), which incorporates samples generated by Stable Diffusion to augment input images. Furthermore, we propose a fast variant of SDAM to reduce computational overhead while preserving high adversarial transferability. Our extensive experimental results demonstrate that our method outperforms state-of-the-art baselines by a substantial margin. Moreover, our approach is compatible with existing transfer-based attacks to further enhance adversarial transferability.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:深度神经网络 (DNN) 容易受到攻击，这些攻击引入不可见的杂音，误导 DNN 预测。虽然一些攻击方法在白盒设置下表现出色，但在黑盒设置下，它们经常遇到困难，特别是面临了防御机制。不同的技术已经出现，以增强黑盒攻击的传输性。 Among them, input transformation-based attacks have shown their effectiveness. 在这篇文章中，我们探索了使用 Stable Diffusion 生成的数据来提高攻击的传输性。这种方法启发于最近的研究，通过 Stable Diffusion 生成的数据来提高模型通用性。以前的研究已经表明，当存在真实数据和生成数据时，模型的通用性会得到改善。基于这一点，我们提出了一种新的攻击方法，即 Stable Diffusion Attack Method (SDAM)，它利用 Stable Diffusion 生成的样本来补充输入图像。此外，我们还提出了一种快速的 SDAM variant，以降低计算开销而保持高的攻击传输性。我们的实验结果表明，我们的方法在比较之下大幅超越了状态当前的基准值。此外，我们的方法与现有的传输基于攻击相容，可以进一步提高攻击的传输性。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Event-RGBD-Neural-SLAM"><a href="#Implicit-Event-RGBD-Neural-SLAM" class="headerlink" title="Implicit Event-RGBD Neural SLAM"></a>Implicit Event-RGBD Neural SLAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11013">http://arxiv.org/abs/2311.11013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Delin Qu, Chi Yan, Dong Wang, Jie Yin, Dan Xu, Bin Zhao, Xuelong Li</li>
<li>for: 这种paper的目的是提出一种基于事件RGBD的启发式SLAM框架，以解决非理想enario中的问题，如运动模糊和灯光变化，从而提高tracking和mapping的精度和稳定性。</li>
<li>methods: 这种方法使用了可微分CRF渲染技术，通过共享辐射场来生成独特的RGB和事件摄像头数据，并通过学习一个统一的启发式表示来优化 captured事件和RGBD监视。此外，基于事件的时间差性，我们提出了一种时间聚合优化策略，使用事件的连续差异约束来提高跟踪准确性和稳定性。</li>
<li>results: 我们在6个场景、17个序列的实验中，证明了我们的方法可以在不同的挑战环境中高效地处理运动模糊和灯光变化，并且与现有最佳方法进行比较，在跟踪ATE和mappingACC中具有更高的精度和稳定性。<details>
<summary>Abstract</summary>
Implicit neural SLAM has achieved remarkable progress recently. Nevertheless, existing methods face significant challenges in non-ideal scenarios, such as motion blur or lighting variation, which often leads to issues like convergence failures, localization drifts, and distorted mapping. To address these challenges, we propose $\textbf{EN-SLAM}$, the first event-RGBD implicit neural SLAM framework, which effectively leverages the high rate and high dynamic range advantages of event data for tracking and mapping. Specifically, EN-SLAM proposes a differentiable CRF (Camera Response Function) rendering technique to generate distinct RGB and event camera data via a shared radiance field, which is optimized by learning a unified implicit representation with the captured event and RGBD supervision. Moreover, based on the temporal difference property of events, we propose a temporal aggregating optimization strategy for the event joint tracking and global bundle adjustment, capitalizing on the consecutive difference constraints of events, significantly enhancing tracking accuracy and robustness. Finally, we construct the simulated dataset $\textbf{DEV-Indoors}$ and real captured dataset $\textbf{DEV-Reals}$ containing 6 scenes, 17 sequences with practical motion blur and lighting changes for evaluations. Experimental results show that our method outperforms the SOTA methods in both tracking ATE and mapping ACC with a real-time $17$ FPS in various challenging environments. The code and dataset will be released upon the paper publication.
</details>
<details>
<summary>摘要</summary>
Recently, implicit neural SLAM has made significant progress. However, existing methods still face challenges in non-ideal scenarios, such as motion blur or lighting variation, which often leads to issues like convergence failures, localization drifts, and distorted mapping. To address these challenges, we propose $\textbf{EN-SLAM}$, the first event-RGBD implicit neural SLAM framework, which effectively leverages the high rate and high dynamic range advantages of event data for tracking and mapping. Specifically, EN-SLAM proposes a differentiable CRF (Camera Response Function) rendering technique to generate distinct RGB and event camera data via a shared radiance field, which is optimized by learning a unified implicit representation with the captured event and RGBD supervision. Moreover, based on the temporal difference property of events, we propose a temporal aggregating optimization strategy for the event joint tracking and global bundle adjustment, capitalizing on the consecutive difference constraints of events, significantly enhancing tracking accuracy and robustness. Finally, we construct the simulated dataset $\textbf{DEV-Indoors}$ and real captured dataset $\textbf{DEV-Reals}$ containing 6 scenes, 17 sequences with practical motion blur and lighting changes for evaluations. Experimental results show that our method outperforms the SOTA methods in both tracking ATE and mapping ACC with a real-time $17$ FPS in various challenging environments. The code and dataset will be released upon the paper publication.Here's the translation in Traditional Chinese:过去的几年，隐式神经 SLAM 已经取得了非常的进步。然而，现有的方法在非理想的enario中仍然面临着问题，例如运动模糊或照明变化，这经常导致整合失败、位置漂移和投影变形的问题。为了解决这些问题，我们提出了 $\textbf{EN-SLAM}$，第一个事件-RGBD 隐式神经 SLAM 框架。EN-SLAM 使用了可微的 CRF (Camera Response Function) 渲染技术生成了不同的 RGB 和事件摄像机数据，并且透过学习一个统一的隐式表现来对于捕捉的事件和 RGBD 进行超参。此外，基于事件的时间差异性，我们提出了一个时间聚合优化策略，具体来说是在事件统一追踪和全局统一调整中，运用了 consecutive difference 的条件来增强追踪精度和Robustness。最后，我们建立了 $\textbf{DEV-Indoors}$ 和 $\textbf{DEV-Reals}$ 两个实验 dataset，包括 6 个scene，17 个序列，实际上具有了实验模糊和照明变化。实验结果显示，我们的方法在追踪 ATE 和投影 ACC 方面具有了 SOTA 的表现，并且在多种挑战性环境中实现了实时 $17$ FPS。代码和dataset 将在论文发表时释出。
</details></li>
</ul>
<hr>
<h2 id="Learning-Scene-Context-Without-Images"><a href="#Learning-Scene-Context-Without-Images" class="headerlink" title="Learning Scene Context Without Images"></a>Learning Scene Context Without Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10998">http://arxiv.org/abs/2311.10998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirreza Rouhi, David Han</li>
<li>for: 教学机器人场景知识，以便它们更有效地与环境交互，预测或预测不可见在视觉场景中的对象。</li>
<li>methods: 提出了一种基于 transformer 的新方法 $LMOD$（标签基于缺失对象检测），通过注意机制教会机器人场景知识。该方法不需要实际图像，只需要图像集标签。</li>
<li>results: 研究表明，通过基于标签学习场景关系，可以通过自注意机制学习场景知识，并且该知识可以提高其他视觉基于对象检测算法的性能。<details>
<summary>Abstract</summary>
Teaching machines of scene contextual knowledge would enable them to interact more effectively with the environment and to anticipate or predict objects that may not be immediately apparent in their perceptual field. In this paper, we introduce a novel transformer-based approach called $LMOD$ ( Label-based Missing Object Detection) to teach scene contextual knowledge to machines using an attention mechanism. A distinctive aspect of the proposed approach is its reliance solely on labels from image datasets to teach scene context, entirely eliminating the need for the actual image itself. We show how scene-wide relationships among different objects can be learned using a self-attention mechanism. We further show that the contextual knowledge gained from label based learning can enhance performance of other visual based object detection algorithm.
</details>
<details>
<summary>摘要</summary>
教机器人场景知识会使其更有效地与环境交互，预测或预测未在视觉范围内出现的对象。在这篇论文中，我们介绍了一种新的变换器基本方法，称为$LMOD$（标签基本缺失检测），用于教机器人场景知识。这种方法异常之处在于它完全不需要图像本身，只需要图像的标签。我们示示了如何使用自我注意机制来学习场景中对象之间的关系。我们进一步示示了通过标签学习获得的Contextual知识可以提高其他视觉基于对象检测算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-and-Accurate-Visual-Prompting"><a href="#Towards-Robust-and-Accurate-Visual-Prompting" class="headerlink" title="Towards Robust and Accurate Visual Prompting"></a>Towards Robust and Accurate Visual Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10992">http://arxiv.org/abs/2311.10992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Li, Liangzhi Li, Zhouqiang Jiang, Bowen Wang</li>
<li>for: 本文研究了使用Visual Prompting（VP）在视觉任务中，并解释了robust模型下的VP表现是否会受到数据集的影响。</li>
<li>methods: 本文使用了一种新的技术名为Prompt Boundary Loose（PBL），以提高标准精度下的视觉提示表现，而不会失去对抗 robustness。</li>
<li>results: 广泛的实验结果表明，我们的方法可以在不同的数据集上提高标准精度和对抗 robustness。<details>
<summary>Abstract</summary>
Visual prompting, an efficient method for transfer learning, has shown its potential in vision tasks. However, previous works focus exclusively on VP from standard source models, it is still unknown how it performs under the scenario of a robust source model: Whether a visual prompt derived from a robust model can inherit the robustness while suffering from the generalization performance decline, albeit for a downstream dataset that is different from the source dataset? In this work, we get an affirmative answer of the above question and give an explanation on the visual representation level. Moreover, we introduce a novel technique named Prompt Boundary Loose (PBL) to effectively mitigates the suboptimal results of visual prompt on standard accuracy without losing (or even significantly improving) its adversarial robustness when using a robust model as source model. Extensive experiments across various datasets show that our findings are universal and demonstrate the significant benefits of our proposed method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Visual 提示，一种高效的转移学习方法，在视觉任务中表现出了潜在的潜力。然而，先前的工作都是基于标准源模型进行了研究，而不是知道robust模型下的Visual 提示是否能够继承鲁棒性，而在不同的下游数据集上受到较大的泛化性下降？在这个研究中，我们得到了上述问题的积极答案，并对Visual 提示的视觉表示进行了解释。此外，我们还提出了一种名为Prompt Boundary Loose（PBL）的新技术，可以有效地 mitigate 视觉提示在标准准确性下的不佳结果，而不失去（或甚至进一步提高）对 robust 模型的鲁棒性。通过对多个数据集进行了广泛的实验，我们的发现表明了universal的特点，并demonstrated  Visual 提示的重要性和PBL的有效性。Note: "robust" in Chinese is "鲁棒" (lùbù), and "standard" is "标准" (biāozhāng).
</details></li>
</ul>
<hr>
<h2 id="Expanding-Scene-Graph-Boundaries-Fully-Open-vocabulary-Scene-Graph-Generation-via-Visual-Concept-Alignment-and-Retention"><a href="#Expanding-Scene-Graph-Boundaries-Fully-Open-vocabulary-Scene-Graph-Generation-via-Visual-Concept-Alignment-and-Retention" class="headerlink" title="Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention"></a>Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10988">http://arxiv.org/abs/2311.10988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zuyao Chen, Jinlin Wu, Zhen Lei, Zhaoxiang Zhang, Changwen Chen</li>
<li>for: 提供一种结构化表示方法，用于许多计算机视觉应用程序中。</li>
<li>methods: 基于变换器架构，学习视觉概念对应关系，以扩展已知对象和关系类别。</li>
<li>results: 在视觉 génome 测试benchmark上，提出一种全开 vocabulary SGG方法，实现了不确定对象和关系类别的承认。<details>
<summary>Abstract</summary>
Scene Graph Generation (SGG) offers a structured representation critical in many computer vision applications. Traditional SGG approaches, however, are limited by a closed-set assumption, restricting their ability to recognize only predefined object and relation categories. To overcome this, we categorize SGG scenarios into four distinct settings based on the node and edge: Closed-set SGG, Open Vocabulary (object) Detection-based SGG (OvD-SGG), Open Vocabulary Relation-based SGG (OvR-SGG), and Open Vocabulary Detection + Relation-based SGG (OvD+R-SGG). While object-centric open vocabulary SGG has been studied recently, the more challenging problem of relation-involved open-vocabulary SGG remains relatively unexplored. To fill this gap, we propose a unified framework named OvSGTR towards fully open vocabulary SGG from a holistic view. The proposed framework is an end-toend transformer architecture, which learns a visual-concept alignment for both nodes and edges, enabling the model to recognize unseen categories. For the more challenging settings of relation-involved open vocabulary SGG, the proposed approach integrates relation-aware pre-training utilizing image-caption data and retains visual-concept alignment through knowledge distillation. Comprehensive experimental results on the Visual Genome benchmark demonstrate the effectiveness and superiority of the proposed framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Make-Pixels-Dance-High-Dynamic-Video-Generation"><a href="#Make-Pixels-Dance-High-Dynamic-Video-Generation" class="headerlink" title="Make Pixels Dance: High-Dynamic Video Generation"></a>Make Pixels Dance: High-Dynamic Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10982">http://arxiv.org/abs/2311.10982</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/makepixelsdance/makepixelsdance.github.io">https://github.com/makepixelsdance/makepixelsdance.github.io</a></li>
<li>paper_authors: Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, Hang Li</li>
<li>for: 本研究旨在提高人工智能中的视频生成技术，特别是生成具有复杂场景和细腻动作的动画视频。</li>
<li>methods: 本研究提出了一种基于扩散模型的新方法，称为PixelDance，该方法在视频生成过程中结合文本指令和图像指令。</li>
<li>results: 经验表明，PixelDance在使用公共数据进行训练后，能够生成具有复杂场景和细腻动作的视频，并设置了新的标准 для视频生成。<details>
<summary>Abstract</summary>
Creating high-dynamic videos such as motion-rich actions and sophisticated visual effects poses a significant challenge in the field of artificial intelligence. Unfortunately, current state-of-the-art video generation methods, primarily focusing on text-to-video generation, tend to produce video clips with minimal motions despite maintaining high fidelity. We argue that relying solely on text instructions is insufficient and suboptimal for video generation. In this paper, we introduce PixelDance, a novel approach based on diffusion models that incorporates image instructions for both the first and last frames in conjunction with text instructions for video generation. Comprehensive experimental results demonstrate that PixelDance trained with public data exhibits significantly better proficiency in synthesizing videos with complex scenes and intricate motions, setting a new standard for video generation.
</details>
<details>
<summary>摘要</summary>
创造高动态视频，如有很多动作和复杂视觉效果，是人工智能领域的一大挑战。现有的现场最佳实践，主要集中在文本到视频生成，往往会生成视频剪辑件的动作较少，即使保持高精度。我们认为，仅仅根据文本指令是不够和不优化的 для视频生成。在这篇论文中，我们介绍PixelDance，一种基于扩散模型的新方法，将文本指令和图像指令结合使用，用于视频生成。我们的实验结果表明，PixelDance通过使用公共数据进行训练，可以生成视频中的复杂场景和细腻动作，创造出新的标准 для视频生成。
</details></li>
</ul>
<hr>
<h2 id="Structure-Aware-Sparse-View-X-ray-3D-Reconstruction"><a href="#Structure-Aware-Sparse-View-X-ray-3D-Reconstruction" class="headerlink" title="Structure-Aware Sparse-View X-ray 3D Reconstruction"></a>Structure-Aware Sparse-View X-ray 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10959">http://arxiv.org/abs/2311.10959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Cai, Jiahao Wang, Alan Yuille, Zongwei Zhou, Angtian Wang</li>
<li>for: 提高 sparse-view X-ray 3D 重建的精度和效率</li>
<li>methods: 使用 Line Segment-based Transformer (Lineformer) 和 Masked Local-Global (MLG) 照样策略</li>
<li>results: 在 X3D  dataset 上，SAX-NeRF 比前一代 NeRF-based 方法提高 12.56 和 2.49 dB 在新视图合成和 CT 重建中<details>
<summary>Abstract</summary>
X-ray, known for its ability to reveal internal structures of objects, is expected to provide richer information for 3D reconstruction than visible light. Yet, existing neural radiance fields (NeRF) algorithms overlook this important nature of X-ray, leading to their limitations in capturing structural contents of imaged objects. In this paper, we propose a framework, Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer (Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal structures of objects in 3D space by modeling the dependencies within each line segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray sampling strategy to extract contextual and geometric information in 2D projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray applications. Experiments on X3D show that SAX-NeRF surpasses previous NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT reconstruction. Code, models, and data will be released at https://github.com/caiyuanhao1998/SAX-NeRF
</details>
<details>
<summary>摘要</summary>
X射线，知名于对物体内部结构的显示，对于3D重建提供更加丰富的信息。然而，现有的神经辐射场（NeRF）算法忽略了这一重要特点，导致它们在捕捉图像对象结构的能力有限。在这篇论文中，我们提出了一个框架，即结构意识X射线神经辐射场（SAX-NeRF），用于稀疏视角X射线3D重建。首先，我们设计了一种基于线段的转换器（Lineformer）作为SAX-NeRF的核心。Lineformer可以在3D空间中捕捉物体的内部结构，并且通过模型每个线段之间的依赖关系来捕捉物体的3D结构。其次，我们提出了一种面积掩码本地全球（MLG）照明策略，以EXTRACTContextual和Geometric信息在2D投影中。此外，我们收集了更加广泛的X3D数据集，覆盖更多的X射线应用场景。实验表明，SAX-NeRF在X3D数据集上超过了之前的NeRF基于方法， Novel View Synthesis和CT重建方面的表现提高12.56和2.49 dB。代码、模型和数据将在https://github.com/caiyuanhao1998/SAX-NeRF上发布。
</details></li>
</ul>
<hr>
<h2 id="NAS-ASDet-An-Adaptive-Design-Method-for-Surface-Defect-Detection-Network-using-Neural-Architecture-Search"><a href="#NAS-ASDet-An-Adaptive-Design-Method-for-Surface-Defect-Detection-Network-using-Neural-Architecture-Search" class="headerlink" title="NAS-ASDet: An Adaptive Design Method for Surface Defect Detection Network using Neural Architecture Search"></a>NAS-ASDet: An Adaptive Design Method for Surface Defect Detection Network using Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10952">http://arxiv.org/abs/2311.10952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenrong Wang, Bin Li, Weifeng Li, Shuanlong Niu, Wang Miao, Tongzhi Niu</li>
<li>for: 这个研究旨在找到一个自动生成适合Surface Defect Detection任务的神经网络架构，以提高工业场景中的检测精度和效率。</li>
<li>methods: 本研究使用Neural Architecture Search（NAS）技术，搭配一个适应性搜寻空间，以自动生成适合Surface Defect Detection任务的神经网络架构。搜寻空间包括重复排列的基本新细胞，以及可搜寻的注意操作。进一步地，使用一种进步搜寻策略和深度超级运算，以更快速地和更好地探索搜寻空间。</li>
<li>results: 实验结果显示，提案的方法可以实现高性能和轻量级的检测网络，并且与其他竞争方法相比，包括手动设计和NAS方法，具有更好的性能和较小的网络模型大小。<details>
<summary>Abstract</summary>
Deep convolutional neural networks (CNNs) have been widely used in surface defect detection. However, no CNN architecture is suitable for all detection tasks and designing effective task-specific requires considerable effort. The neural architecture search (NAS) technology makes it possible to automatically generate adaptive data-driven networks. Here, we propose a new method called NAS-ASDet to adaptively design network for surface defect detection. First, a refined and industry-appropriate search space that can adaptively adjust the feature distribution is designed, which consists of repeatedly stacked basic novel cells with searchable attention operations. Then, a progressive search strategy with a deep supervision mechanism is used to explore the search space faster and better. This method can design high-performance and lightweight defect detection networks with data scarcity in industrial scenarios. The experimental results on four datasets demonstrate that the proposed method achieves superior performance and a relatively lighter model size compared to other competitive methods, including both manual and NAS-based approaches.
</details>
<details>
<summary>摘要</summary>
深度卷积神经网络 (CNN) 已广泛应用于表面缺陷检测中。然而，没有任何 CNN 架构适合所有检测任务，设计有效的任务特定网络需要较大的努力。神经网络搜索 (NAS) 技术使得可以自动生成适应数据驱动的网络。我们提出了一种新的方法called NAS-ASDet，用于适应性地设计检测网络。首先，我们设计了一个精细化和适用于工业场景的搜索空间，这个空间由重叠的基本新细胞组成，每个细胞具有搜索注意操作。然后，我们使用一种进步的搜索策略和深度超级视图机制，以更快和更好地探索搜索空间。这种方法可以在工业场景中的数据稀缺情况下设计高性能且轻量级的缺陷检测网络。实验结果表明，我们的方法可以在四个数据集上实现superior的性能，并且与其他竞争方法相比，模型的大小更轻量级。
</details></li>
</ul>
<hr>
<h2 id="Single-shot-Phase-Retrieval-from-a-Fractional-Fourier-Transform-Perspective"><a href="#Single-shot-Phase-Retrieval-from-a-Fractional-Fourier-Transform-Perspective" class="headerlink" title="Single-shot Phase Retrieval from a Fractional Fourier Transform Perspective"></a>Single-shot Phase Retrieval from a Fractional Fourier Transform Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10950">http://arxiv.org/abs/2311.10950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixiao Yang, Ran Tao, Kaixuan Wei, Jun Shi</li>
<li>for: 复原普通频率图像 (Classical Phase Retrieval)</li>
<li>methods: 融合FrFT测量模型与自愿学习重建方法</li>
<li>results: 实现单一测量复原 (Single-shot Phase Retrieval) 和获得高品质图像 (High-quality Images)<details>
<summary>Abstract</summary>
The realm of classical phase retrieval concerns itself with the arduous task of recovering a signal from its Fourier magnitude measurements, which are fraught with inherent ambiguities. A single-exposure intensity measurement is commonly deemed insufficient for the reconstruction of the primal signal, given that the absent phase component is imperative for the inverse transformation. In this work, we present a novel single-shot phase retrieval paradigm from a fractional Fourier transform (FrFT) perspective, which involves integrating the FrFT-based physical measurement model within a self-supervised reconstruction scheme. Specifically, the proposed FrFT-based measurement model addresses the aliasing artifacts problem in the numerical calculation of Fresnel diffraction, featuring adaptability to both short-distance and long-distance propagation scenarios. Moreover, the intensity measurement in the FrFT domain proves highly effective in alleviating the ambiguities of phase retrieval and relaxing the previous conditions on oversampled or multiple measurements in the Fourier domain. Furthermore, the proposed self-supervised reconstruction approach harnesses the fast discrete algorithm of FrFT alongside untrained neural network priors, thereby attaining preeminent results. Through numerical simulations, we demonstrate that both amplitude and phase objects can be effectively retrieved from a single-shot intensity measurement using the proposed approach and provide a promising technique for support-free coherent diffraction imaging.
</details>
<details>
<summary>摘要</summary>
经典阶段恢复的领域涉及于从傅里尔变换（Fourier Transform）中获取信号，但这在存在内在幂等性的情况下是一项困难的任务。单个曝光量测量通常被视为不够于重建原始信号，因为缺失的相位组件是重建过程中的关键因素。在这种情况下，我们提出了一种基于分解傅里尔变换（FrFT）的新的单极恢复方法。我们在这种方法中将FrFT基于物理测量模型集成到了一种自适应恢复方案中。具体来说，我们的FrFT基于测量模型解决了数值计算幂等噪声的问题，并且可以适应短距离和长距离传播enario。此外，在FrFT域中测量INTENSITY的方法具有减轻恢复ambiguities和放弃先前需要多个测量或扩展的观测的优点。此外，我们的自适应恢复方法利用FrFT快速简洁算法和未训练神经网络约束，实现了突出的结果。通过数值实验，我们示示了单极测量INTENSITY可以有效地从单个曝光量测量中获取恢复信号，并提供了一种支持自由幂 diffraction imaging 的有力的技术。
</details></li>
</ul>
<hr>
<h2 id="Jenga-Stacking-Based-on-6D-Pose-Estimation-for-Architectural-Form-Finding-Process"><a href="#Jenga-Stacking-Based-on-6D-Pose-Estimation-for-Architectural-Form-Finding-Process" class="headerlink" title="Jenga Stacking Based on 6D Pose Estimation for Architectural Form Finding Process"></a>Jenga Stacking Based on 6D Pose Estimation for Architectural Form Finding Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10918">http://arxiv.org/abs/2311.10918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixun Huang</li>
<li>for: 这篇论文主要是为了探讨当前最新的6D pose estimation方法的现状，以及在不同的建筑设计场景下应用pose estimation方法的选择。</li>
<li>methods: 本论文通过对最新的Gen6d研究进行评估，对当前开放集成方法进行质量评估，包括应用级别、预测速度、遮挡性、准确率、环境干扰等方面的评估。</li>
<li>results: 本论文通过对6D pose estimation方法的综合评估，发现在应用级别和预测速度方面有所改进空间，对于遮挡性和环境干扰的防止仍然存在一定的挑战。同时，通过与建筑风景环境评估结合，提出了一种可质量的建筑设计方法。<details>
<summary>Abstract</summary>
This paper includes a review of current state of the art 6d pose estimation methods, as well as a discussion of which pose estimation method should be used in two types of architectural design scenarios. Taking the latest pose estimation research Gen6d as an example, we make a qualitative assessment of the current openset methods in terms of application level, prediction speed, resistance to occlusion, accuracy, resistance to environmental interference, etc. In addition, we try to combine 6D pose estimation and building wind environment assessment to create tangible architectural design approach, we discuss the limitations of the method and point out the direction in which 6d pose estimation is eager to progress in this scenario.
</details>
<details>
<summary>摘要</summary>
这篇论文包括当前最佳6D姿态估计方法的回顾，以及在两类建筑设计场景中应用哪种姿态估计方法的讨论。以latest pose estimation research Gen6d为例，我们对当前开放集成方法进行质量评估，包括应用水平、预测速度、遮挡耐受度、准确率、环境干扰耐受度等方面。此外，我们尝试将6D姿态估计与建筑风险环境评估结合，创造可触媒建筑设计方法，并讨论这种方法的局限性和进一步发展方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/cs.CV_2023_11_18/" data-id="clpxp6c2m00neee88cfgp8fir" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/cs.AI_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T12:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/cs.AI_2023_11_18/">cs.AI - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Morphology-Enhanced-CAM-Guided-SAM-for-weakly-supervised-Breast-Lesion-Segmentation"><a href="#Morphology-Enhanced-CAM-Guided-SAM-for-weakly-supervised-Breast-Lesion-Segmentation" class="headerlink" title="Morphology-Enhanced CAM-Guided SAM for weakly supervised Breast Lesion Segmentation"></a>Morphology-Enhanced CAM-Guided SAM for weakly supervised Breast Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11176">http://arxiv.org/abs/2311.11176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuexin18/morseg-cam-sam">https://github.com/yuexin18/morseg-cam-sam</a></li>
<li>paper_authors: Xin Yue, Qing Zhao, Jianqiang Li, Xiaoling Liu, Changwei Song, Suqin Liu, Guanghui Fu</li>
<li>for: 这个研究的目的是提出一个新的测脉检测标准，以帮助早期识别乳腺癌。</li>
<li>methods: 这个方法使用了一个基于 morphology 的测脉检测模型，并且使用了 Class Activation Map (CAM) 来精确地定位悖论。</li>
<li>results: 这个方法可以获得比较好的效果，其 Dice 分数为 74.39%，与超参考方法相比，其 Hausdorff 距离为 24.27，较低。<details>
<summary>Abstract</summary>
Breast cancer diagnosis challenges both patients and clinicians, with early detection being crucial for effective treatment. Ultrasound imaging plays a key role in this, but its utility is hampered by the need for precise lesion segmentation-a task that is both time-consuming and labor-intensive. To address these challenges, we propose a new framework: a morphology-enhanced, Class Activation Map (CAM)-guided model, which is optimized using a computer vision foundation model known as SAM. This innovative framework is specifically designed for weakly supervised lesion segmentation in early-stage breast ultrasound images. Our approach uniquely leverages image-level annotations, which removes the requirement for detailed pixel-level annotation. Initially, we perform a preliminary segmentation using breast lesion morphology knowledge. Following this, we accurately localize lesions by extracting semantic information through a CAM-based heatmap. These two elements are then fused together, serving as a prompt to guide the SAM in performing refined segmentation. Subsequently, post-processing techniques are employed to rectify topological errors made by the SAM. Our method not only simplifies the segmentation process but also attains accuracy comparable to supervised learning methods that rely on pixel-level annotation. Our framework achieves a Dice score of 74.39% on the test set, demonstrating compareable performance with supervised learning methods. Additionally, it outperforms a supervised learning model, in terms of the Hausdorff distance, scoring 24.27 compared to Deeplabv3+'s 32.22. These experimental results showcase its feasibility and superior performance in integrating weakly supervised learning with SAM. The code is made available at: https://github.com/YueXin18/MorSeg-CAM-SAM.
</details>
<details>
<summary>摘要</summary>
乳癌诊断带来挑战，特别是在早期发现是关键。ultrasound imaging在这方面发挥着关键作用，但是需要精准的肿体分割，这是时间consuming和劳动密集的任务。为了解决这些挑战，我们提出了一个新的框架：一种基于形态学的Class Activation Map（CAM）引导模型，通过一个已知的计算机视觉基础模型（SAM）进行优化。这种创新的框架专门用于早期乳癌ultrasound图像中的弱类supervised lesion segmentation。我们的方法利用图像级别的标注，从而消除了精准的像素级别标注的需求。首先，我们进行初步分 segmentation，基于乳腺癌形态知识。然后，我们准确地找到肿体，通过提取semantic信息，并使用CAM基于的热图来准确地Localize lesions。这两个元素之后被融合，作为SAM进行精确的分 segmentation的指引。最后，我们使用post-processing技术来修正SAM中的topological错误。我们的方法不仅简化了分 segmentation过程，还可以达到与超级vised learning方法相同的准确性。我们的框架在测试集上达到了74.39%的Dice分数，与超级vised learning方法相当。此外，它还在 Hausdorff distance上 OUTPERFORMS Deeplabv3+，分别为24.27和32.22。这些实验结果表明了我们的框架在结合弱类学习与SAM的可行性和性能优势。代码可以在以下链接获取：https://github.com/YueXin18/MorSeg-CAM-SAM。
</details></li>
</ul>
<hr>
<h2 id="Best-uses-of-ChatGPT-and-Generative-AI-for-computer-science-research"><a href="#Best-uses-of-ChatGPT-and-Generative-AI-for-computer-science-research" class="headerlink" title="Best uses of ChatGPT and Generative AI for computer science research"></a>Best uses of ChatGPT and Generative AI for computer science research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11175">http://arxiv.org/abs/2311.11175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo C. Garrido-Merchan<br>for: This paper explores the diverse applications of ChatGPT and other generative AI technologies in computer science academic research, with a focus on using these tools to boost the productivity of computer research scientists.methods: The paper highlights innovative uses of generative AI, such as brainstorming research ideas, aiding in the drafting and styling of academic papers, and assisting in the synthesis of state-of-the-art sections.results: The paper makes recommendations for using generative AI to improve the productivity of computer research scientists, including using these tools for synthetic data creation, research methodology, and mentorship, as well as for task organization and article quality assessment. Additionally, the paper explores the capabilities of generative AI in disseminating ideas, generating images and audio, text transcription, and engaging with editors.<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (AI), particularly tools like OpenAI's popular ChatGPT, is reshaping the landscape of computer science research. Used wisely, these tools can boost the productivity of a computer research scientist. This paper provides an exploration of the diverse applications of ChatGPT and other generative AI technologies in computer science academic research, making recommendations about the use of Generative AI to make more productive the role of the computer research scientist, with the focus of writing new research papers. We highlight innovative uses such as brainstorming research ideas, aiding in the drafting and styling of academic papers and assisting in the synthesis of state-of-the-art section. Further, we delve into using these technologies in understanding interdisciplinary approaches, making complex texts simpler, and recommending suitable academic journals for publication. Significant focus is placed on generative AI's contributions to synthetic data creation, research methodology, and mentorship, as well as in task organization and article quality assessment. The paper also addresses the utility of AI in article review, adapting texts to length constraints, constructing counterarguments, and survey development. Moreover, we explore the capabilities of these tools in disseminating ideas, generating images and audio, text transcription, and engaging with editors. We also describe some non-recommended uses of generative AI for computer science research, mainly because of the limitations of this technology.
</details>
<details>
<summary>摘要</summary>
生成人工智能（AI），特别是开源AI的ChatGPT等工具，正在计算机科学研究领域发挥重要作用。如果用得当，这些工具可以提高计算机研究科学家的生产力。本文通过探讨生成AI在计算机科学学术研究中的多种应用，并提出使用生成AI来提高计算机研究科学家的作用，主要是为了写新的研究论文。我们指出了使用这些技术的创新用途，如讨论研究主题、帮助撰写和编写学术论文、协助合并state-of-the-art部分。此外，我们还探讨了这些技术在跨学科approach、简化复杂文本、建议适合发表学术刊物等方面的应用。在生成数据创造、研究方法、导师、任务组织和文章质量评估等方面，生成AI做出了重要贡献。此外，我们还探讨了AI在文章审查、文章修改、调查开发等方面的应用。此外，我们还探讨了这些工具在传播想法、生成图像和音频、文本笔记、与编辑器交互等方面的能力。最后，我们还讨论了生成AI在计算机科学研究中的一些不建议使用情况，主要是因为这些技术的限制。
</details></li>
</ul>
<hr>
<h2 id="Deep-Coherence-Learning-An-Unsupervised-Deep-Beamformer-for-High-Quality-Single-Plane-Wave-Imaging-in-Medical-Ultrasound"><a href="#Deep-Coherence-Learning-An-Unsupervised-Deep-Beamformer-for-High-Quality-Single-Plane-Wave-Imaging-in-Medical-Ultrasound" class="headerlink" title="Deep Coherence Learning: An Unsupervised Deep Beamformer for High Quality Single Plane Wave Imaging in Medical Ultrasound"></a>Deep Coherence Learning: An Unsupervised Deep Beamformer for High Quality Single Plane Wave Imaging in Medical Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11169">http://arxiv.org/abs/2311.11169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunwoo Cho, Seongjun Park, Jinbum Kang, Yangmo Yoo</li>
<li>for: 这个研究旨在提高单束波图像成像（PWI）的高框率和新临床应用。</li>
<li>methods: 这篇研究提出了一种新的无监督学习方法，即深度同步学习（DL）基本构成（DL-DCL），以提高单束波图像成像质量。</li>
<li>results: 实验和phantom研究表明，提出的DL-DCL方法可以与传统的多束波图像成像（DMAS）和传统的深度学习（DAS）相比，具有更高的空间和对比分辨率。<details>
<summary>Abstract</summary>
Plane wave imaging (PWI) in medical ultrasound is becoming an important reconstruction method with high frame rates and new clinical applications. Recently, single PWI based on deep learning (DL) has been studied to overcome lowered frame rates of traditional PWI with multiple PW transmissions. However, due to the lack of appropriate ground truth images, DL-based PWI still remains challenging for performance improvements. To address this issue, in this paper, we propose a new unsupervised learning approach, i.e., deep coherence learning (DCL)-based DL beamformer (DL-DCL), for high-quality single PWI. In DL-DCL, the DL network is trained to predict highly correlated signals with a unique loss function from a set of PW data, and the trained DL model encourages high-quality PWI from low-quality single PW data. In addition, the DL-DCL framework based on complex baseband signals enables a universal beamformer. To assess the performance of DL-DCL, simulation, phantom and in vivo studies were conducted with public datasets, and it was compared with traditional beamformers (i.e., DAS with 75-PWs and DMAS with 1-PW) and other DL-based methods (i.e., supervised learning approach with 1-PW and generative adversarial network (GAN) with 1-PW). From the experiments, the proposed DL-DCL showed comparable results with DMAS with 1-PW and DAS with 75-PWs in spatial resolution, and it outperformed all comparison methods in contrast resolution. These results demonstrated that the proposed unsupervised learning approach can address the inherent limitations of traditional PWIs based on DL, and it also showed great potential in clinical settings with minimal artifacts.
</details>
<details>
<summary>摘要</summary>
单波探射图像重建（PWI）在医疗超声中成为重要的重建方法，具有高帧率和新的临床应用。近期，基于深度学习（DL）的单波PWI被研究以超越传统PWI的帧率降低。然而，由于缺乏适当的基准图像，DL基础的PWI仍然具有挑战性。为解决这问题，本文提出了一新的无监督学习方法——深度协调学习（DCL）基础的DL扁平变数（DL-DCL），以提高单波PWI的质量。在DCL-DCL中，DL网络被训练来预测基于单波探射数据的高度相关的信号，并且训练DL模型以从低质量单波探射数据中获得高质量PWI。此外，DCL-DCL框架基于复杂的基带信号，实现了通用的扁平变数。为评估DCL-DCL的性能，本文进行了遮 simulation、实验和生体研究，并与传统扁平变数（i.e., DAS with 75-PWs和DMAS with 1-PW）和其他DL基础方法（i.e., 监督学习方法with 1-PW和生成 adversarial network（GAN）with 1-PW）进行比较。实验结果显示，提案的DCL-DCL与DMAS with 1-PW和DAS with 75-PWs相似的 spatial resolution，并且与所有比较方法相比，具有更高的contrast resolution。这些结果显示了提案的无监督学习方法可以解决传统PWI基于DL的内在限制，并且在临床设置中具有最小的错误。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Exposure-Bias-in-Discriminator-Guided-Diffusion-Models"><a href="#Mitigating-Exposure-Bias-in-Discriminator-Guided-Diffusion-Models" class="headerlink" title="Mitigating Exposure Bias in Discriminator Guided Diffusion Models"></a>Mitigating Exposure Bias in Discriminator Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11164">http://arxiv.org/abs/2311.11164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos</li>
<li>for: 提高Diffusion Models生成图像质量</li>
<li>methods:  incorporating an auxiliary term derived from a discriminator network, modifying the sampling approach</li>
<li>results:  Achieving an FID score of 1.73 on the unconditional CIFAR-10 dataset, outperforming the current state-of-the-art.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
Diffusion Models have demonstrated remarkable performance in image generation. However, their demanding computational requirements for training have prompted ongoing efforts to enhance the quality of generated images through modifications in the sampling process. A recent approach, known as Discriminator Guidance, seeks to bridge the gap between the model score and the data score by incorporating an auxiliary term, derived from a discriminator network. We show that despite significantly improving sample quality, this technique has not resolved the persistent issue of Exposure Bias and we propose SEDM-G++, which incorporates a modified sampling approach, combining Discriminator Guidance and Epsilon Scaling. Our proposed approach outperforms the current state-of-the-art, by achieving an FID score of 1.73 on the unconditional CIFAR-10 dataset.
</details>
<details>
<summary>摘要</summary>
Diffusion Models 在图像生成中表现出色，但它们的训练需要高度计算能力，这导致了不断尝试改进生成图像质量的方法。一种最新的方法是通过抽象网络来提供一个辅助项，以bridging模型分数和数据分数之间的差距。我们发现，尽管显著提高样本质量，但这种技术并未解决持续存在的曝光偏见问题。我们提出了SEDM-G++，它将 combine Discriminator Guidance 和 Epsilon Scaling 两种技术，并实现了 current state-of-the-art 的 FID 分数（1.73）在无条件 CIFAR-10 数据集上。
</details></li>
</ul>
<hr>
<h2 id="Contextualizing-Internet-Memes-Across-Social-Media-Platforms"><a href="#Contextualizing-Internet-Memes-Across-Social-Media-Platforms" class="headerlink" title="Contextualizing Internet Memes Across Social Media Platforms"></a>Contextualizing Internet Memes Across Social Media Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11157">http://arxiv.org/abs/2311.11157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav Joshi, Filip Ilievski, Luca Luceri</li>
<li>for: 本研究目的是寻找互联网趋势的表达方式，即互联网趋势的媒体表达形式。</li>
<li>methods: 本研究使用了一种名为“知识图”的semantic repository of knowledge，将互联网趋势的表达形式与知识图中的内容进行对比，从而识别和映射互联网趋势。</li>
<li>results: 研究发现，可以通过对互联网趋势的表达形式与知识图中的内容进行对比，来识别和映射互联网趋势。此外，研究还发现了不同平台上的趋势的差异和流行的趋势，以及一些常见的趋势渠道和子Reddit。最后，研究还示出了如何使用知识图来提供社交媒体上趋势的上下文。<details>
<summary>Abstract</summary>
Internet memes have emerged as a novel format for communication and expressing ideas on the web. Their fluidity and creative nature are reflected in their widespread use, often across platforms and occasionally for unethical or harmful purposes. While computational work has already analyzed their high-level virality over time and developed specialized classifiers for hate speech detection, there have been no efforts to date that aim to holistically track, identify, and map internet memes posted on social media. To bridge this gap, we investigate whether internet memes across social media platforms can be contextualized by using a semantic repository of knowledge, namely, a knowledge graph. We collect thousands of potential internet meme posts from two social media platforms, namely Reddit and Discord, and perform an extract-transform-load procedure to create a data lake with candidate meme posts. By using vision transformer-based similarity, we match these candidates against the memes cataloged in a recently released knowledge graph of internet memes, IMKG. We provide evidence that memes published online can be identified by mapping them to IMKG. We leverage this grounding to study the prevalence of memes on different platforms, discover popular memes, and select common meme channels and subreddits. Finally, we illustrate how the grounding can enable users to get context about memes on social media thanks to their link to the knowledge graph.
</details>
<details>
<summary>摘要</summary>
互联网趣闻在互联网上作为一种新的交流和表达方式得到广泛的应用。它们的流动性和创新性使其在不同的平台上广泛使用， occasional 用于不道德或有害的目的。虽然计算工作已经分析了这些趣闻的时间权 virality 和开发了特殊的 hate speech 检测器，但到目前为止没有任何尝试将互联网趣闻在社交媒体上 Contextualized 。为了bridging这个差距，我们investigate 了 whether 互联网趣闻在社交媒体平台上可以通过使用一个semantic repository of knowledge，即知识 graphs 来contextualize。我们收集了 thousands 的 potential 互联网趣闻帖子从两个社交媒体平台，namely Reddit 和 Discord，并perform 了一个 extract-transform-load 过程，以创建一个数据湖包含候选趣闻帖子。通过使用视力 transformer 基于相似性，我们将这些候选者与 IMKG 中 cataloged 的趣闻进行匹配。我们提供了证据，证明在线上发布的趣闻可以被IMKG 中的趣闻映射。我们利用这种固定来研究不同平台上趣闻的流行程度，发现 популяр的趣闻，并选择常见的趣闻渠道和 subreddits。最后，我们示例了如何通过这种固定，使用户在社交媒体上获得趣闻的CONTEXT。
</details></li>
</ul>
<hr>
<h2 id="A-Principled-Framework-for-Knowledge-enhanced-Large-Language-Model"><a href="#A-Principled-Framework-for-Knowledge-enhanced-Large-Language-Model" class="headerlink" title="A Principled Framework for Knowledge-enhanced Large Language Model"></a>A Principled Framework for Knowledge-enhanced Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11135">http://arxiv.org/abs/2311.11135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saizhuo Wang, Zhihan Liu, Zhaoran Wang, Jian Guo</li>
<li>for: 提高LLMs的深度分析能力和可靠性，使其能够应用于重要场景。</li>
<li>methods: 提出了一种严格设计的框架，通过 anchoring knowledge 和 closed-loop reasoning 来提高 LLMs 的分析能力。</li>
<li>results: 通过分析框架的各个组成部分，证明了 LLMs 的理解能力的提高，并且在定义的假设下提供了理论保证。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are versatile, yet they often falter in tasks requiring deep and reliable reasoning due to issues like hallucinations, limiting their applicability in critical scenarios. This paper introduces a rigorously designed framework for creating LLMs that effectively anchor knowledge and employ a closed-loop reasoning process, enhancing their capability for in-depth analysis. We dissect the framework to illustrate the contribution of each component to the LLMs' performance, offering a theoretical assurance of improved reasoning under well-defined assumptions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Bayesian-Neural-Networks-A-Min-Max-Game-Framework"><a href="#Bayesian-Neural-Networks-A-Min-Max-Game-Framework" class="headerlink" title="Bayesian Neural Networks: A Min-Max Game Framework"></a>Bayesian Neural Networks: A Min-Max Game Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11126">http://arxiv.org/abs/2311.11126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junping Hong, Ercan Engin Kuruoglu</li>
<li>for: 这个论文主要是为了探讨bayesian neural networks的应用和关系。</li>
<li>methods: 这个论文使用了bayesian neural networks和variational inference来训练模型，并将其формализова为一个MINIMAX游戏问题。</li>
<li>results: 实验结果表明，这个方法和现有的closed-loop transcription neural network相当，并且可以提供另一种 bayesian neural networks的视角。<details>
<summary>Abstract</summary>
Bayesian neural networks use random variables to describe the neural networks rather than deterministic neural networks and are mostly trained by variational inference which updates the mean and variance at the same time. Here, we formulate the Bayesian neural networks as a minimax game problem. We do the experiments on the MNIST data set and the primary result is comparable to the existing closed-loop transcription neural network. Finally, we reveal the connections between Bayesian neural networks and closed-loop transcription neural networks, and show our framework is rather practical, and provide another view of Bayesian neural networks.
</details>
<details>
<summary>摘要</summary>
bayesian neural networks 使用Random variable来描述神经网络，而不是决定性神经网络，通常通过变量推导来训练。在这里，我们将 bayesian neural networks 表示为最小化游戏问题。我们在 MNIST 数据集上进行实验，主要结果与现有的关闭环路译写神经网络相当。最后，我们描述 bayesian neural networks 和关闭环路译写神经网络之间的联系，并表明我们的框架是实用的，并提供了 bayesian neural networks 另一种视图。Here's the translation breakdown:* Bayesian neural networks ( bayesian neural networks ) - 概率神经网络* use random variables ( 使用Random variable ) - 使用随机变量* to describe the neural networks ( 描述神经网络 ) - 描述神经网络* rather than deterministic neural networks ( 而不是决定性神经网络 ) - 而不是决定性神经网络* and are mostly trained by variational inference ( 通常通过变量推导来训练 ) - 通常通过变量推导来训练* We formulate the Bayesian neural networks as a minimax game problem ( 在这里，我们将 bayesian neural networks 表示为最小化游戏问题 ) - 将 bayesian neural networks 表示为最小化游戏问题* We do the experiments on the MNIST data set ( 我们在 MNIST 数据集上进行实验 ) - 在 MNIST 数据集上进行实验* and the primary result is comparable to the existing closed-loop transcription neural network ( 主要结果与现有的关闭环路译写神经网络相当 ) - 主要结果与现有的关闭环路译写神经网络相当* Finally, we reveal the connections between Bayesian neural networks and closed-loop transcription neural networks ( 最后，我们描述 bayesian neural networks 和关闭环路译写神经网络之间的联系 ) - 最后，我们描述 bayesian neural networks 和关闭环路译写神经网络之间的联系* and show our framework is rather practical ( 并表明我们的框架是实用的 ) - 并表明我们的框架是实用的* and provide another view of Bayesian neural networks ( 并提供了 bayesian neural networks 另一种视图 ) - 并提供了 bayesian neural networks 另一种视图
</details></li>
</ul>
<hr>
<h2 id="An-Improved-Neural-Network-Model-Based-On-CNN-Using-For-Fruit-Sugar-Degree-Detection"><a href="#An-Improved-Neural-Network-Model-Based-On-CNN-Using-For-Fruit-Sugar-Degree-Detection" class="headerlink" title="An Improved Neural Network Model Based On CNN Using For Fruit Sugar Degree Detection"></a>An Improved Neural Network Model Based On CNN Using For Fruit Sugar Degree Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11120">http://arxiv.org/abs/2311.11120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyang Deng, Xin Wen, Zhan Gao</li>
<li>for: 这个论文是用于检测水果糖分的检测方法。</li>
<li>methods: 该论文使用了人工神经网络模型，其中低层为多层感知器(MLP)，中间层为2维相关矩阵层，高层为卷积神经网络(CNN)层。此外，论文还使用了振荡分解(WD)来降维特征，以及进化算法(GA)来找到优秀特征。</li>
<li>results: 论文通过对水果谱спектrum数据进行处理和分析，并比较了不同的神经网络模型和传统参数选择方法的效果。结果表明，使用人工神经网络模型可以准确地检测水果糖分，并且比传统参数选择方法更高效。此外，论文还提出了一种基于数据标准差(STD)的新评价标准，用于评价检测性能。<details>
<summary>Abstract</summary>
Artificial Intelligence(AI) widely applies in Image Classification and Recognition, Text Understanding and Natural Language Processing, which makes great progress. In this paper, we introduced AI into the fruit quality detection field. We designed a fruit sugar degree regression model using an Artificial Neural Network based on spectra of fruits within the visible/near-infrared(V/NIR)range. After analysis of fruit spectra, we innovatively proposed a new neural network structure: low layers consist of a Multilayer Perceptron(MLP), a middle layer is a 2-dimensional correlation matrix layer, and high layers consist of several Convolutional Neural Network(CNN) layers. In this study, we used fruit sugar value as a detection target, collecting two fruits called Gan Nan Navel and Tian Shan Pear as samples, doing experiments respectively, and comparing their results. We used Analysis of Variance(ANOVA) to evaluate the reliability of the dataset we collected. Then, we tried multiple strategies to process spectrum data, evaluating their effects. In this paper, we tried to add Wavelet Decomposition(WD) to reduce feature dimensions and a Genetic Algorithm(GA) to find excellent features. Then, we compared Neural Network models with traditional Partial Least Squares(PLS) based models. We also compared the neural network structure we designed(MLP-CNN) with other traditional neural network structures. In this paper, we proposed a new evaluation standard derived from dataset standard deviation(STD) for evaluating detection performance, validating the viability of using an artificial neural network model to do fruit sugar degree nondestructive detection.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）广泛应用于图像分类和识别、自然语言处理和文本理解等领域，带来了很大的进步。在这篇论文中，我们将AI应用于水果质量检测领域。我们设计了一种基于人工神经网络的水果糖度预测模型，使用水果在可见/近红外（V/NIR）谱spectra进行分析。经过分析水果谱spectra后，我们创新地提出了一种新的神经网络结构：低层为多层感知网络（MLP），中层为2维相关矩阵层，高层为几个卷积神经网络（CNN）层。在这个研究中，我们使用水果糖度值作为检测目标，采集了两种水果样本——芜南柑和天山梨，进行了分别的实验，并比较了其结果。我们使用分布式 Анализа variance（ANOVA）评估数据集的可靠性。然后，我们尝试了多种处理谱数据的策略，评估其效果。在这篇论文中，我们尝试了使用扩展特征矩阵（WD）减少特征维度，以及使用进化算法（GA）找到优秀的特征。然后，我们比较了神经网络模型与传统的部分最小平方（PLS）基于模型。我们还比较了我们设计的神经网络结构（MLP-CNN）与其他传统神经网络结构。在这篇论文中，我们提出了一种基于数据集标准差（STD）的新评价标准，用于评估检测性能。这些结果validate了使用人工神经网络模型进行水果糖度非 destruktive检测的可能性。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Speech-Emotion-Recognition-and-Recommender-Systems-for-Negative-Emotion-Handling-in-Therapy-Chatbots"><a href="#Utilizing-Speech-Emotion-Recognition-and-Recommender-Systems-for-Negative-Emotion-Handling-in-Therapy-Chatbots" class="headerlink" title="Utilizing Speech Emotion Recognition and Recommender Systems for Negative Emotion Handling in Therapy Chatbots"></a>Utilizing Speech Emotion Recognition and Recommender Systems for Negative Emotion Handling in Therapy Chatbots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11116">http://arxiv.org/abs/2311.11116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farideh Majidi, Marzieh Bahrami<br>for: 提高访问者情绪支持和提供人类化同情methods: 使用语音感知技术和语音感受识别（SER）技术，并使用Convolutional Neural Network（CNN）模型和ShEMO数据集来准确地探测和分类负面情绪，包括恐慌、恐惧和悲伤。同时，开发了一个推荐系统，利用SER模型的输出生成个性化的情绪管理建议，并使用 GloVe 和 LSTM 模型来实现这一点。results:  SER 模型在语音信号中探测和分类负面情绪的验证准确率为 88%，而推荐模型在生成个性化的情绪管理建议方面的准确率为 98%。通过 integrate 文本到语音模型 GlowTTS，能够为用户提供人类化的同情和建议，并在英文和波斯语中提供多语言支持。<details>
<summary>Abstract</summary>
Emotional well-being significantly influences mental health and overall quality of life. As therapy chatbots become increasingly prevalent, their ability to comprehend and respond empathetically to users' emotions remains limited. This paper addresses this limitation by proposing an approach to enhance therapy chatbots with auditory perception, enabling them to understand users' feelings and provide human-like empathy. The proposed method incorporates speech emotion recognition (SER) techniques using Convolutional Neural Network (CNN) models and the ShEMO dataset to accurately detect and classify negative emotions, including anger, fear, and sadness. The SER model achieves a validation accuracy of 88%, demonstrating its effectiveness in recognizing emotional states from speech signals. Furthermore, a recommender system is developed, leveraging the SER model's output to generate personalized recommendations for managing negative emotions, for which a new bilingual dataset was generated as well since there is no such dataset available for this task. The recommender model achieves an accuracy of 98% by employing a combination of global vectors for word representation (GloVe) and LSTM models. To provide a more immersive and empathetic user experience, a text-to-speech model called GlowTTS is integrated, enabling the therapy chatbot to audibly communicate the generated recommendations to users in both English and Persian. The proposed approach offers promising potential to enhance therapy chatbots by providing them with the ability to recognize and respond to users' emotions, ultimately improving the delivery of mental health support for both English and Persian-speaking users.
</details>
<details>
<summary>摘要</summary>
情感健康对于心理健康和全面生活质量具有重要影响。随着咨询虚拟助手在使用人数的增加，它们的理解和回应用户情感的能力仍然有限。本文解决这一问题，提出一种增强咨询虚拟助手的方法，使其能够通过听觉感知，理解用户的情感状况，并提供人类化同情。本方法利用了支持Vector（SER）技术，使用Convolutional Neural Network（CNN）模型和ShEMO数据集，准确地检测和分类负面情感，包括恐惧、愤怒和悲伤。SER模型的验证准确率达88%，证明其能够从语音信号中准确地检测情感状况。此外，我们还开发了一个推荐系统，利用SER模型的输出，为用户提供个性化的情感管理建议。我们生成了一个新的双语数据集，以便在这个任务上进行训练。推荐模型使用了GloVe word表示模型和LSTM模型，实现了98%的准确率。为提供更加投入和同情的用户体验，我们还 интегрирова了一个名为GlowTTS的文本读取模型，使咨询虚拟助手能够通过语音方式传达给用户建议，并在英文和波斯语中进行同时传达。本方法的提议具有推动咨询虚拟助手提供更好的情感管理支持的潜在能力。
</details></li>
</ul>
<hr>
<h2 id="Environment-Aware-Dynamic-Graph-Learning-for-Out-of-Distribution-Generalization"><a href="#Environment-Aware-Dynamic-Graph-Learning-for-Out-of-Distribution-Generalization" class="headerlink" title="Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization"></a>Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11114">http://arxiv.org/abs/2311.11114</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ringbdstack/eagle">https://github.com/ringbdstack/eagle</a></li>
<li>paper_authors: Haonan Yuan, Qingyun Sun, Xingcheng Fu, Ziwei Zhang, Cheng Ji, Hao Peng, Jianxin Li</li>
<li>for: This paper focuses on improving the out-of-distribution (OOD) generalization of dynamic graph neural networks (DGNNs) by modeling complex coupled environments and exploiting spatio-temporal invariant patterns.</li>
<li>methods: The proposed Environment-Aware dynamic Graph LEarning (EAGLE) framework includes an environment-aware EA-DGNN to model environments, an environment instantiation mechanism to diversify environments, and an invariant pattern recognition mechanism to discriminate spatio-temporal invariant patterns for OOD prediction.</li>
<li>results: The proposed EAGLE framework achieves superior performance compared to state-of-the-art baselines under distribution shifts, demonstrating its effectiveness in improving OOD generalization on dynamic graphs.<details>
<summary>Abstract</summary>
Dynamic graph neural networks (DGNNs) are increasingly pervasive in exploiting spatio-temporal patterns on dynamic graphs. However, existing works fail to generalize under distribution shifts, which are common in real-world scenarios. As the generation of dynamic graphs is heavily influenced by latent environments, investigating their impacts on the out-of-distribution (OOD) generalization is critical. However, it remains unexplored with the following two major challenges: (1) How to properly model and infer the complex environments on dynamic graphs with distribution shifts? (2) How to discover invariant patterns given inferred spatio-temporal environments? To solve these challenges, we propose a novel Environment-Aware dynamic Graph LEarning (EAGLE) framework for OOD generalization by modeling complex coupled environments and exploiting spatio-temporal invariant patterns. Specifically, we first design the environment-aware EA-DGNN to model environments by multi-channel environments disentangling. Then, we propose an environment instantiation mechanism for environment diversification with inferred distributions. Finally, we discriminate spatio-temporal invariant patterns for out-of-distribution prediction by the invariant pattern recognition mechanism and perform fine-grained causal interventions node-wisely with a mixture of instantiated environment samples. Experiments on real-world and synthetic dynamic graph datasets demonstrate the superiority of our method against state-of-the-art baselines under distribution shifts. To the best of our knowledge, we are the first to study OOD generalization on dynamic graphs from the environment learning perspective.
</details>
<details>
<summary>摘要</summary>
现代图学 neural networks (DGNNs) 在利用空间-时间模式方面越来越普遍。然而，现有的工作无法适应分布变化，这是现实世界中的常见情况。由于生成动态图的过程受到隐藏环境的影响，研究这些环境对于 OUT-OF-DISTRIBUTION (OOD) 通用性的影响是 kritical。然而，这还没有得到充分研究，主要有两个挑战：1. 如何正确地模型和推理动态图上复杂的环境下的分布变化？2. 如何在推理出的空间-时间环境中发现不变Pattern？为解决这些挑战，我们提出了一种新的 Environment-Aware 动态图学 LEarning (EAGLE) 框架，用于 OOD 通用性。具体来说，我们首先设计了环境意识 EA-DGNN，用于模型环境。然后，我们提出了环境实例化机制，用于实现环境多样性。最后，我们通过不变Pattern认识机制来识别OOD情况，并通过精细的 causal interventions 来进行精细的节点修饰。实验结果表明，我们的方法在真实世界和synthetic dynamic graph dataset上表现出优于状态的基eline under distribution shifts。到目前为止，我们是第一个从环境学习角度研究 OOD 通用性在动态图上。
</details></li>
</ul>
<hr>
<h2 id="varepsilon-fractional-Core-Stability-in-Hedonic-Games"><a href="#varepsilon-fractional-Core-Stability-in-Hedonic-Games" class="headerlink" title="$\varepsilon$-fractional Core Stability in Hedonic Games"></a>$\varepsilon$-fractional Core Stability in Hedonic Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11101">http://arxiv.org/abs/2311.11101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Fioravanti, Michele Flammini, Bojana Kodric, Giovanna Varricchio<br>for:This paper focuses on the problem of coalition formation in hedonic games, where agents are strategic and have individual preferences. The goal is to find a stable coalition structure that satisfies some form of stability, such as core-stability.methods:The paper proposes a new notion of $\varepsilon$-fractional core-stability, which allows for a fraction of coalitions to core-block, and designs efficient algorithms to find such partitions for two fundamental classes of hedonic games. The paper also explores the use of probabilistic sampling to learn valuations and compute outcomes that are $\varepsilon$-fractional core-stable.results:The paper shows that the proposed notion of $\varepsilon$-fractional core-stability can guarantee both existence and polynomial-time computation, and provides efficient algorithms for finding such partitions in two fundamental classes of hedonic games. The paper also gives positive and negative results on which distributions allow for the efficient computation of outcomes that are $\varepsilon$-fractional core-stable with arbitrarily high confidence in a PAC-learning fashion.<details>
<summary>Abstract</summary>
Hedonic Games (HGs) are a classical framework modeling coalition formation of strategic agents guided by their individual preferences. According to these preferences, it is desirable that a coalition structure (i.e. a partition of agents into coalitions) satisfies some form of stability. The most well-known and natural of such notions is arguably core-stability. Informally, a partition is core-stable if no subset of agents would like to deviate by regrouping in a so-called core-blocking coalition. Unfortunately, core-stable partitions seldom exist and even when they do, it is often computationally intractable to find one. To circumvent these problems, we propose the notion of $\varepsilon$-fractional core-stability, where at most an $\varepsilon$-fraction of all possible coalitions is allowed to core-block. It turns out that such a relaxation may guarantee both existence and polynomial-time computation. Specifically, we design efficient algorithms returning an $\varepsilon$-fractional core-stable partition, with $\varepsilon$ exponentially decreasing in the number of agents, for two fundamental classes of HGs: Simple Fractional and Anonymous. From a probabilistic point of view, being the definition of $\varepsilon$-fractional core equivalent to requiring that uniformly sampled coalitions core-block with probability lower than $\varepsilon$, we further extend the definition to handle more complex sampling distributions. Along this line, when valuations have to be learned from samples in a PAC-learning fashion, we give positive and negative results on which distributions allow the efficient computation of outcomes that are $\varepsilon$-fractional core-stable with arbitrarily high confidence.
</details>
<details>
<summary>摘要</summary>
хедонис游戏（HG）是一种经典的框架模型，用于模型策略性agent coalition formation。根据这些 preferences，一个 coalition structure（即agent分配到不同的 coalition）满足一种形式的稳定性是感兴趣的。最常见和最自然的这种概念是核稳定性。 informally, a partition is core-stable if no subset of agents would like to deviate by regrouping in a so-called core-blocking coalition。 unfortunately, core-stable partitions seldom exist and even when they do, it is often computationally intractable to find one。 to circumvent these problems, we propose the notion of ε-fractional core-stability, where at most an ε-fraction of all possible coalitions is allowed to core-block。 it turns out that such a relaxation may guarantee both existence and polynomial-time computation。 specifically, we design efficient algorithms returning an ε-fractional core-stable partition, with ε exponentially decreasing in the number of agents, for two fundamental classes of HGs：simple fractional and anonymous。 from a probabilistic point of view, being the definition of ε-fractional core equivalent to requiring that uniformly sampled coalitions core-block with probability lower than ε， we further extend the definition to handle more complex sampling distributions。 along this line, when valuations have to be learned from samples in a PAC-learning fashion, we give positive and negative results on which distributions allow the efficient computation of outcomes that are ε-fractional core-stable with arbitrarily high confidence。
</details></li>
</ul>
<hr>
<h2 id="Introducing-NCL-SM-A-Fully-Annotated-Dataset-of-Images-from-Human-Skeletal-Muscle-Biopsies"><a href="#Introducing-NCL-SM-A-Fully-Annotated-Dataset-of-Images-from-Human-Skeletal-Muscle-Biopsies" class="headerlink" title="Introducing NCL-SM: A Fully Annotated Dataset of Images from Human Skeletal Muscle Biopsies"></a>Introducing NCL-SM: A Fully Annotated Dataset of Images from Human Skeletal Muscle Biopsies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11099">http://arxiv.org/abs/2311.11099</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atifkhanncl/ncl-sm">https://github.com/atifkhanncl/ncl-sm</a></li>
<li>paper_authors: Atif Khan, Conor Lawless, Amy Vincent, Charlotte Warren, Valeria Di Leo, Tiago Gomes, A. Stephen McGough</li>
<li>for: 这个论文的目的是提供一个高质量的生物快照数据集，用于开发自动化、精准、可重复地分析单元细胞（SM）组织图像。</li>
<li>methods: 这篇论文使用了生物快照技术，并提供了高质量的生物快照数据集，包括46个人体制备样本和生物Marker的检测结果。</li>
<li>results: 这篇论文发现了一个全新的、高质量的生物快照数据集，可以用于开发自动化、精准、可重复地分析SM组织图像。这个数据集包括超过50,000个手动分割的肌细胞（myofibers），并且对每个myofibers进行了高质量的检查和标注。<details>
<summary>Abstract</summary>
Single cell analysis of skeletal muscle (SM) tissue is a fundamental tool for understanding many neuromuscular disorders. For this analysis to be reliable and reproducible, identification of individual fibres within microscopy images (segmentation) of SM tissue should be precise. There is currently no tool or pipeline that makes automatic and precise segmentation and curation of images of SM tissue cross-sections possible. Biomedical scientists in this field rely on custom tools and general machine learning (ML) models, both followed by labour intensive and subjective manual interventions to get the segmentation right. We believe that automated, precise, reproducible segmentation is possible by training ML models. However, there are currently no good quality, publicly available annotated imaging datasets available for ML model training. In this paper we release NCL-SM: a high quality bioimaging dataset of 46 human tissue sections from healthy control subjects and from patients with genetically diagnosed muscle pathology. These images include $>$ 50k manually segmented muscle fibres (myofibres). In addition we also curated high quality myofibres and annotated reasons for rejecting low quality myofibres and regions in SM tissue images, making this data completely ready for downstream analysis. This, we believe, will pave the way for development of a fully automatic pipeline that identifies individual myofibres within images of tissue sections and, in particular, also classifies individual myofibres that are fit for further analysis.
</details>
<details>
<summary>摘要</summary>
单元细胞分析对骨附肌（SM）组织是基本工具，用于理解许多神经肌肉疾病。为了使这种分析可靠和重复，单元细胞内的图像分割（segmentation）需要精准。目前没有任何工具或管道可以自动地对SM组织横截面图像进行精准分割和摘要。生物医学科学家在这个领域依赖于自定义工具和通用机器学习（ML）模型，然后进行劳动 INTENSIVE 和主观的手动干预，以确保分割是正确的。我们认为，通过训练ML模型，可以实现自动、精准、可重复的分割。然而，目前没有一个良好的、公共可用的批处理图像数据集，用于ML模型训练。在这篇论文中，我们发布NCL-SM：一个高质量生物影像数据集，包括46名健康控制者和被遗传诊断的肌肉疾病患者的人类组织横截面图像。这些图像包含> 50k个手动分割的肌肉元（myofibres）。此外，我们还精心准备了高质量的肌肉元和SM组织图像的批处理结果，并对低质量的肌肉元和SM组织图像进行了描述，使这些数据完全准备好进行下游分析。我们认为，这将开创出一个完全自动的分析管道，可以在SM组织图像中自动地标识和分割各个肌肉元，并在特定情况下还可以对各个肌肉元进行分类。
</details></li>
</ul>
<hr>
<h2 id="Radiology-Report-Generation-Using-Transformers-Conditioned-with-Non-imaging-Data"><a href="#Radiology-Report-Generation-Using-Transformers-Conditioned-with-Non-imaging-Data" class="headerlink" title="Radiology Report Generation Using Transformers Conditioned with Non-imaging Data"></a>Radiology Report Generation Using Transformers Conditioned with Non-imaging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11097">http://arxiv.org/abs/2311.11097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nurbanu Aksoy, Nishant Ravikumar, Alejandro F Frangi<br>for: 这个研究旨在提高医疗影像解释的效率，并且使用多 modal 的资料来增强 radiology 报告生成。methods: 这个研究使用了一个新的多Modal transformer 网络，将静脉肺 X 光像和相关的病人人口资料集成，生成具体化的 radiology 报告。results: 根据评估指标，包括病人人口资料，使用提案的方法可以将 radiology 报告质量提高，相比基准网络使用静脉肺 X 光像alone。<details>
<summary>Abstract</summary>
Medical image interpretation is central to most clinical applications such as disease diagnosis, treatment planning, and prognostication. In clinical practice, radiologists examine medical images and manually compile their findings into reports, which can be a time-consuming process. Automated approaches to radiology report generation, therefore, can reduce radiologist workload and improve efficiency in the clinical pathway. While recent deep-learning approaches for automated report generation from medical images have seen some success, most studies have relied on image-derived features alone, ignoring non-imaging patient data. Although a few studies have included the word-level contexts along with the image, the use of patient demographics is still unexplored. This paper proposes a novel multi-modal transformer network that integrates chest x-ray (CXR) images and associated patient demographic information, to synthesise patient-specific radiology reports. The proposed network uses a convolutional neural network to extract visual features from CXRs and a transformer-based encoder-decoder network that combines the visual features with semantic text embeddings of patient demographic information, to synthesise full-text radiology reports. Data from two public databases were used to train and evaluate the proposed approach. CXRs and reports were extracted from the MIMIC-CXR database and combined with corresponding patients' data MIMIC-IV. Based on the evaluation metrics used including patient demographic information was found to improve the quality of reports generated using the proposed approach, relative to a baseline network trained using CXRs alone. The proposed approach shows potential for enhancing radiology report generation by leveraging rich patient metadata and combining semantic text embeddings derived thereof, with medical image-derived visual features.
</details>
<details>
<summary>摘要</summary>
医疗图像解读是许多临床应用的核心，包括疾病诊断、治疗规划和前景预测。在临床实践中，医生会 manually examining医疗图像并编写报告，这可能是一项时间消耗的过程。因此，自动化医学报告生成方法可以减轻医生的工作负担，提高临床流程的效率。 although recent deep learning approaches for automated report generation from medical images have shown some success, most studies have relied on image-derived features alone, ignoring non-imaging patient data。在这些研究中，只有一些研究包括了图像上的字符级别上下文，但是使用患者的人口数据仍然是一个未explored的领域。这篇论文提出了一种新的多Modal transformer网络，该网络将 integrate chest x-ray (CXR) 图像和相关的患者人口数据，以生成个性化的医学报告。该网络使用卷积神经网络提取 CXR 图像中的视觉特征，并使用基于 transformer 的 encoder-decoder 网络将这些视觉特征与患者人口数据相结合，以生成全文医学报告。使用两个公共数据库进行训练和评估，包括 MIMIC-CXR 数据库和 MIMIC-IV 数据库。根据评估指标，包括患者人口数据，使用该提案的方法生成的报告质量相比基线网络训练用 CXR 图像alone 提高。该提案表明可以通过利用丰富的患者Metadata和基于 semantic text embeddings  derivated thereof，与医疗图像中的视觉特征结合，提高医学报告生成的质量。
</details></li>
</ul>
<hr>
<h2 id="Deep-Tensor-Network"><a href="#Deep-Tensor-Network" class="headerlink" title="Deep Tensor Network"></a>Deep Tensor Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11091">http://arxiv.org/abs/2311.11091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carpedm20/DCGAN-tensorflow">https://github.com/carpedm20/DCGAN-tensorflow</a></li>
<li>paper_authors: Yifan Zhang</li>
<li>for: 本文探讨了矩阵category的基本原理，通过矩阵产品的universal property，开拓了深度网络架构中新的方法ология。</li>
<li>methods: 本文的主要贡献是提出了矩阵注意力和矩阵互动机制，这是一种利用矩阵category提高深度网络计算效率和表达能力的新方法，甚至可以推广到量子领域。</li>
<li>results: 本文的实验结果表明，通过矩阵注意力和矩阵互动机制，可以增强深度网络的计算效率和表达能力，并且可以应用于量子领域。<details>
<summary>Abstract</summary>
In this paper, we delve into the foundational principles of tensor categories, harnessing the universal property of the tensor product to pioneer novel methodologies in deep network architectures. Our primary contribution is the introduction of the Tensor Attention and Tensor Interaction Mechanism, a groundbreaking approach that leverages the tensor category to enhance the computational efficiency and the expressiveness of deep networks, and can even be generalized into the quantum realm.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探索了矩阵类别的基本原理，利用矩阵产品的 universality 性来开拓深度网络体系的新方法。我们的主要贡献是提出了矩阵注意力和矩阵互动机制，这是一种创新的方法，利用矩阵类别来提高深度网络的计算效率和表达力，甚至可以扩展到量子领域。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Images-An-Integrative-Multi-modal-Approach-to-Chest-X-Ray-Report-Generation"><a href="#Beyond-Images-An-Integrative-Multi-modal-Approach-to-Chest-X-Ray-Report-Generation" class="headerlink" title="Beyond Images: An Integrative Multi-modal Approach to Chest X-Ray Report Generation"></a>Beyond Images: An Integrative Multi-modal Approach to Chest X-Ray Report Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11090">http://arxiv.org/abs/2311.11090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nurbanu Aksoy, Serge Sharoff, Selcuk Baser, Nishant Ravikumar, Alejandro F Frangi</li>
<li>for: 这个论文目的是为了自动生成医疗影像报告，描述医疗影像中的发现。</li>
<li>methods: 该论文提出了一种基于多Modal深度学习网络的方法，结合了结构化患者数据（如生物指标和症状）和不结构化医疗记录，以生成胸部X射线报告。我们引入了一种conditioned cross-multi-head注意力模块，以融合这些不同数据模式，bridging semantic gap between visual and textual data。</li>
<li>results: 实验表明，通过结合多 modal 数据，比单依图像数据获得了显著提高。此外，我们的模型在ROUGE-L指标上达到了相关的state-of-the-art 模型之上。此外，我们还使用了人工评估和临床semantic相似度测量，并与word-overlap指标进行深度的量化分析。<details>
<summary>Abstract</summary>
Image-to-text radiology report generation aims to automatically produce radiology reports that describe the findings in medical images. Most existing methods focus solely on the image data, disregarding the other patient information accessible to radiologists. In this paper, we present a novel multi-modal deep neural network framework for generating chest X-rays reports by integrating structured patient data, such as vital signs and symptoms, alongside unstructured clinical notes.We introduce a conditioned cross-multi-head attention module to fuse these heterogeneous data modalities, bridging the semantic gap between visual and textual data. Experiments demonstrate substantial improvements from using additional modalities compared to relying on images alone. Notably, our model achieves the highest reported performance on the ROUGE-L metric compared to relevant state-of-the-art models in the literature. Furthermore, we employed both human evaluation and clinical semantic similarity measurement alongside word-overlap metrics to improve the depth of quantitative analysis. A human evaluation, conducted by a board-certified radiologist, confirms the model's accuracy in identifying high-level findings, however, it also highlights that more improvement is needed to capture nuanced details and clinical context.
</details>
<details>
<summary>摘要</summary>
radiology report生成旨在自动生成医疗影像中的找到结果。大多数现有方法只是专注于图像数据，忽略了可以 accessible 的患者信息。在这篇论文中，我们提出了一种新的多modal deep neural network框架，用于生成胸部X射线报告，并将结构化患者数据，如生物指标和症状，与不结构化医疗记录相结合。我们提出了一种 conditioned cross-multi-head attention模块，以融合这些不同数据模式，跨越视觉和文本数据之间的semantic gap。实验结果表明，通过使用更多的modalities，可以获得显著提高。特别是，我们的模型在ROUGE-L指标上的表现比 relevante state-of-the-art 模型更高。此外，我们还使用了人类评估和临床semantic相似度测量，与word-overlap指标共同进行深入的量化分析。人类评估，由美国医学会资具认证的放射学专家进行评估，表明模型能够准确地确定高级结果，但也表明需要进一步改进，以捕捉细节和临床上下文。
</details></li>
</ul>
<hr>
<h2 id="Combining-EEG-and-NLP-Features-for-Predicting-Students’-Lecture-Comprehension-using-Ensemble-Classification"><a href="#Combining-EEG-and-NLP-Features-for-Predicting-Students’-Lecture-Comprehension-using-Ensemble-Classification" class="headerlink" title="Combining EEG and NLP Features for Predicting Students’ Lecture Comprehension using Ensemble Classification"></a>Combining EEG and NLP Features for Predicting Students’ Lecture Comprehension using Ensemble Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11088">http://arxiv.org/abs/2311.11088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phantharach Natnithikarat, Theerawit Wilaiprasitporn, Supavit Kongwudhikunakorn</li>
<li>for: 这个研究用于探讨学生在课堂教学中的理解水平，并提出一种分类框架来预测学生在两个任务中的困惑和答案正确性。</li>
<li>methods: 这个研究使用了生物маркер和自然语言处理技术来检测学生的课堂理解水平。生物маркер技术用于检测学生的脑电活动，而自然语言处理技术用于分析学生的语言表达。这些技术结合使用以提取 инте格рирован的特征，以便更好地预测学生的理解水平。</li>
<li>results: 实验结果显示，这种分类框架可以比基eline更高的准确率预测学生的困惑和答案正确性。在两个任务中，这种框架的F1分数最高达0.65和0.78，表明使用这种方法可以提高分类性能。此外，还使用了学生自我报告的困惑评分作为一个Integrated特征，以进一步提高分类性能。<details>
<summary>Abstract</summary>
Electroencephalography (EEG) and Natural Language Processing (NLP) can be applied for education to measure students' comprehension in classroom lectures; currently, the two measures have been used separately. In this work, we propose a classification framework for predicting students' lecture comprehension in two tasks: (i) students' confusion after listening to the simulated lecture and (ii) the correctness of students' responses to the post-lecture assessment. The proposed framework includes EEG and NLP feature extraction, processing, and classification. EEG and NLP features are extracted to construct integrated features obtained from recorded EEG signals and sentence-level syntactic analysis, which provide information about specific biomarkers and sentence structures. An ensemble stacking classification method -- a combination of multiple individual models that produces an enhanced predictive model -- is studied to learn from the features to make predictions accurately. Furthermore, we also utilized subjective confusion ratings as another integrated feature to enhance classification performance. By doing so, experiment results show that this framework performs better than the baselines, which achieved F1 up to 0.65 for predicting confusion and 0.78 for predicting correctness, highlighting that utilizing this has helped improve the classification performance.
</details>
<details>
<summary>摘要</summary>
电子脑波图像（EEG）和自然语言处理（NLP）可以应用于教育，以测量学生在课堂讲解中的理解水平。目前，这两种测量方法都是分开使用的。在这项工作中，我们提议一种分类框架，用于预测学生在两个任务中的课堂理解水平：（i）学生听完模拟课程后的混乱程度，以及（ii）学生完成后评估测验中的答案正确性。提议的框架包括EEG和NLP特征提取、处理和分类。EEG和NLP特征都是从记录的EEG信号和句子水平语法分析中提取出来的，它们提供了特定的生物标志和句子结构信息。我们还利用了学生主观的混乱评分作为另一个整合特征，以提高分类性能。通过这样做，实验结果显示，这个框架比基线表现更好，其F1分数可达0.65，用于预测混乱，以及0.78，用于预测正确性，这表明使用这个框架可以提高分类性能。
</details></li>
</ul>
<hr>
<h2 id="ECLM-Efficient-Edge-Cloud-Collaborative-Learning-with-Continuous-Environment-Adaptation"><a href="#ECLM-Efficient-Edge-Cloud-Collaborative-Learning-with-Continuous-Environment-Adaptation" class="headerlink" title="ECLM: Efficient Edge-Cloud Collaborative Learning with Continuous Environment Adaptation"></a>ECLM: Efficient Edge-Cloud Collaborative Learning with Continuous Environment Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11083">http://arxiv.org/abs/2311.11083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Zhuang, Zhenzhe Zheng, Yunfeng Shao, Bingshuai Li, Fan Wu, Guihai Chen</li>
<li>For: The paper is written for developing an edge-cloud collaborative learning framework for rapid model adaptation in dynamic edge environments.* Methods: The paper proposes a novel block-level model decomposition design to decompose the original large cloud model into multiple combinable modules, and an end-to-end learning framework that incorporates the modular model design into an efficient model adaptation pipeline.* Results: The paper achieves significant improvements in model performance (18.89% accuracy increase) and resource efficiency (7.12x communication cost reduction) in adapting models to dynamic edge environments by efficiently collaborating the edge and the cloud models.<details>
<summary>Abstract</summary>
Pervasive mobile AI applications primarily employ one of the two learning paradigms: cloud-based learning (with powerful large models) or on-device learning (with lightweight small models). Despite their own advantages, neither paradigm can effectively handle dynamic edge environments with frequent data distribution shifts and on-device resource fluctuations, inevitably suffering from performance degradation. In this paper, we propose ECLM, an edge-cloud collaborative learning framework for rapid model adaptation for dynamic edge environments. We first propose a novel block-level model decomposition design to decompose the original large cloud model into multiple combinable modules. By flexibly combining a subset of the modules, this design enables the derivation of compact, task-specific sub-models for heterogeneous edge devices from the large cloud model, and the seamless integration of new knowledge learned on these devices into the cloud model periodically. As such, ECLM ensures that the cloud model always provides up-to-date sub-models for edge devices. We further propose an end-to-end learning framework that incorporates the modular model design into an efficient model adaptation pipeline including an offline on-cloud model prototyping and training stage, and an online edge-cloud collaborative adaptation stage. Extensive experiments over various datasets demonstrate that ECLM significantly improves model performance (e.g., 18.89% accuracy increase) and resource efficiency (e.g., 7.12x communication cost reduction) in adapting models to dynamic edge environments by efficiently collaborating the edge and the cloud models.
</details>
<details>
<summary>摘要</summary>
通用移动AI应用主要采用云基本学习（强大大模型）或设备内学习（轻量级小模型）两种学习方法。尽管它们各有优点，但是 neither paradigm can effectively handle动态边缘环境中的数据分布变化和设备资源波动， resulting in performance degradation. In this paper, we propose ECLM, an edge-cloud collaborative learning framework for rapid model adaptation in dynamic edge environments. We first propose a novel block-level model decomposition design to decompose the original large cloud model into multiple combinable modules. By flexibly combining a subset of the modules, this design enables the derivation of compact, task-specific sub-models for heterogeneous edge devices from the large cloud model, and the seamless integration of new knowledge learned on these devices into the cloud model periodically. As such, ECLM ensures that the cloud model always provides up-to-date sub-models for edge devices. We further propose an end-to-end learning framework that incorporates the modular model design into an efficient model adaptation pipeline including an offline on-cloud model prototyping and training stage, and an online edge-cloud collaborative adaptation stage. Extensive experiments over various datasets demonstrate that ECLM significantly improves model performance (e.g., 18.89% accuracy increase) and resource efficiency (e.g., 7.12x communication cost reduction) in adapting models to dynamic edge environments by efficiently collaborating the edge and the cloud models.
</details></li>
</ul>
<hr>
<h2 id="DSCom-A-Data-Driven-Self-Adaptive-Community-Based-Framework-for-Influence-Maximization-in-Social-Networks"><a href="#DSCom-A-Data-Driven-Self-Adaptive-Community-Based-Framework-for-Influence-Maximization-in-Social-Networks" class="headerlink" title="DSCom: A Data-Driven Self-Adaptive Community-Based Framework for Influence Maximization in Social Networks"></a>DSCom: A Data-Driven Self-Adaptive Community-Based Framework for Influence Maximization in Social Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11080">http://arxiv.org/abs/2311.11080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Zuo, Haojia Sun, Yongyi Hu, Jianxiong Guo, Xiaofeng Gao<br>for: This paper aims to address the data-driven version of influence maximization, where the diffusion model is not given and needs to be inferred from the history cascades.methods: The paper proposes a machine learning-based framework called DSCom, which leverages node attributes to estimate the closeness between connected nodes and overcome the influence overlap problem.results: The proposed algorithm is evaluated through empirical experiments with parameterized diffusion models based on real-world social networks, showing its efficiency and effectiveness.Here’s the Chinese version:for: 这篇论文主要解决了数据驱动版本的影响最大化问题，其中diffusion模型未提供，需要从历史扩散中推断。methods: 该论文提出了基于机器学习的DSCom框架，利用节点特征来估计连接节点的相互关系，并通过自similarity matrix来解决因果重叠问题。results: 该算法经验测试了基于实际社交网络的参数化扩散模型，证明其效率和有效性。<details>
<summary>Abstract</summary>
Influence maximization aims to find a subset of seeds that maximize the influence spread under a given budget. In this paper, we mainly address the data-driven version of this problem, where the diffusion model is not given but needs to be inferred from the history cascades. Several previous works have addressed this topic in a statistical way and provided efficient algorithms with theoretical guarantee. However, in their settings, though the diffusion parameters are inferred, they still need users to preset the diffusion model, which can be an intractable problem in real-world practices. In this paper, we reformulate the problem on the attributed network and leverage the node attributes to estimate the closeness between the connected nodes. Specifically, we propose a machine learning-based framework, named DSCom, to address this problem in a heuristic way. Under this framework, we first infer the users' relationship from the diffusion dataset through attention mechanism and then leverage spectral clustering to overcome the influence overlap problem in the lack of exact diffusion formula. Compared to the previous theoretical works, we carefully designed empirical experiments with parameterized diffusion models based on real-world social networks, which prove the efficiency and effectiveness of our algorithm.
</details>
<details>
<summary>摘要</summary>
“影响 maximization 目标是找到一个最大化影响的种子子集，在给定的预算下。在这篇论文中，我们主要关注数据驱动的版本问题，即 diffusion 模型不是给定的，而是从历史扩散中推断出来。先前的一些工作已经Addressed this topic in a statistical way, providing efficient algorithms with theoretical guarantee. However, in their settings, the diffusion parameters are inferred, but users still need to preset the diffusion model, which can be an intractable problem in real-world practices.在这篇论文中，我们将问题 reformulate 到 attributed network 上，并利用节点特征来估计连接节点之间的距离。specifically，我们提出了一种机器学习基于的框架，named DSCom，来解决这个问题。在这个框架下，我们首先通过注意力机制从扩散数据集中推断用户之间的关系，然后利用 спектраль聚类来超越扩散影响的问题，具有缺乏准确扩散方程的情况下。与先前的理论工作相比，我们在实际实验中谨慎地设计了参数化的扩散模型，基于实际的社交网络数据，这证明了我们的算法的效率和有效性。”
</details></li>
</ul>
<hr>
<h2 id="Adapters-A-Unified-Library-for-Parameter-Efficient-and-Modular-Transfer-Learning"><a href="#Adapters-A-Unified-Library-for-Parameter-Efficient-and-Modular-Transfer-Learning" class="headerlink" title="Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning"></a>Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11077">http://arxiv.org/abs/2311.11077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clifton Poth, Hannah Sterz, Indraneil Paul, Sukannya Purkayastha, Leon Engländer, Timo Imhof, Ivan Vulić, Sebastian Ruder, Iryna Gurevych, Jonas Pfeiffer</li>
<li>For: 这篇论文是为了推广更有效率的转移学习方法，提供了一个开源库（Adapters），可以实现参数高效和弹性的转移学习。* Methods: 这篇论文使用了10种多样化的adapter方法，并将它们集成到一个简单的接口中，提供了使用方便和可配置的方式。* Results: 论文透过评估这些adapter方法的性能，证明了这个库的可行性和弹性，并且比较了它们与传统 fine-tuning 方法的性能。<details>
<summary>Abstract</summary>
We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library's efficacy by evaluating its performance against full fine-tuning on various NLP tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个名为“Adapter”的开源库，这个库可以实现parameter-efficient和modular的转移学习在大型自然语言模型中。我们统一了10种不同的adapter方法，并将它们集成到了一个简单的界面中，这使得研究者和实践者可以轻松地使用和自定义adapter。我们显示了这个库的性能，与完整的 fine-tuning 进行比较，并证明了它在不同的NLP任务中的表现。“Adapter”提供了一个强大的工具，用于解决传统 fine-tuning 方法的挑战，并促进更有效和自定义的转移学习。这个库可以在https://adapterhub.ml/adapters中下载。
</details></li>
</ul>
<hr>
<h2 id="Community-Aware-Efficient-Graph-Contrastive-Learning-via-Personalized-Self-Training"><a href="#Community-Aware-Efficient-Graph-Contrastive-Learning-via-Personalized-Self-Training" class="headerlink" title="Community-Aware Efficient Graph Contrastive Learning via Personalized Self-Training"></a>Community-Aware Efficient Graph Contrastive Learning via Personalized Self-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11073">http://arxiv.org/abs/2311.11073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuecheng Li, Yanming Hu, Lele Fu, Chuan Chen, Lei Yang, Zibin Zheng</li>
<li>for: The paper is written for community detection tasks in graph-structured data, and it proposes a novel framework called Community-aware Efficient Graph Contrastive Learning (CEGCL) to jointly learn community partition and node representations in an end-to-end manner.</li>
<li>methods: The proposed CEGCL framework uses a personalized self-training (PeST) strategy for unsupervised scenarios, which enables the model to capture precise community-level personalized information in a graph. Additionally, the aligned graph clustering (AlGC) is employed to obtain the community partition.</li>
<li>results: The paper demonstrates the effectiveness of the proposed CEGCL model for community detection both theoretically and experimentally. Extensive experimental results show that CEGCL exhibits state-of-the-art performance on three benchmark datasets with different scales.<details>
<summary>Abstract</summary>
In recent years, graph contrastive learning (GCL) has emerged as one of the optimal solutions for various supervised tasks at the node level. However, for unsupervised and structure-related tasks such as community detection, current GCL algorithms face difficulties in acquiring the necessary community-level information, resulting in poor performance. In addition, general contrastive learning algorithms improve the performance of downstream tasks by increasing the number of negative samples, which leads to severe class collision and unfairness of community detection. To address above issues, we propose a novel Community-aware Efficient Graph Contrastive Learning Framework (CEGCL) to jointly learn community partition and node representations in an end-to-end manner. Specifically, we first design a personalized self-training (PeST) strategy for unsupervised scenarios, which enables our model to capture precise community-level personalized information in a graph. With the benefit of the PeST, we alleviate class collision and unfairness without sacrificing the overall model performance. Furthermore, the aligned graph clustering (AlGC) is employed to obtain the community partition. In this module, we align the clustering space of our downstream task with that in PeST to achieve more consistent node embeddings. Finally, we demonstrate the effectiveness of our model for community detection both theoretically and experimentally. Extensive experimental results also show that our CEGCL exhibits state-of-the-art performance on three benchmark datasets with different scales.
</details>
<details>
<summary>摘要</summary>
Recently, graph contrastive learning (GCL) has emerged as one of the optimal solutions for various supervised tasks at the node level. However, for unsupervised and structure-related tasks such as community detection, current GCL algorithms have difficulty obtaining the necessary community-level information, resulting in poor performance. In addition, general contrastive learning algorithms improve the performance of downstream tasks by increasing the number of negative samples, which leads to severe class collision and unfairness of community detection. To address these issues, we propose a novel Community-aware Efficient Graph Contrastive Learning Framework (CEGCL) to jointly learn community partition and node representations in an end-to-end manner. Specifically, we first design a personalized self-training (PeST) strategy for unsupervised scenarios, which enables our model to capture precise community-level personalized information in a graph. With the benefit of the PeST, we alleviate class collision and unfairness without sacrificing the overall model performance. Furthermore, the aligned graph clustering (AlGC) is employed to obtain the community partition. In this module, we align the clustering space of our downstream task with that in PeST to achieve more consistent node embeddings. Finally, we demonstrate the effectiveness of our model for community detection both theoretically and experimentally. Extensive experimental results also show that our CEGCL exhibits state-of-the-art performance on three benchmark datasets with different scales.
</details></li>
</ul>
<hr>
<h2 id="SBTRec-A-Transformer-Framework-for-Personalized-Tour-Recommendation-Problem-with-Sentiment-Analysis"><a href="#SBTRec-A-Transformer-Framework-for-Personalized-Tour-Recommendation-Problem-with-Sentiment-Analysis" class="headerlink" title="SBTRec- A Transformer Framework for Personalized Tour Recommendation Problem with Sentiment Analysis"></a>SBTRec- A Transformer Framework for Personalized Tour Recommendation Problem with Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11071">http://arxiv.org/abs/2311.11071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngai Lam Ho, Roy Ka-Wei Lee, Kwan Hui Lim</li>
<li>for: 提供个性化的旅游点赞推荐，帮助旅行者更好地规划旅游路线和探索当地热点。</li>
<li>methods: 基于BERT的旅游点序列推荐算法，利用用户检查到和上传照片来理解POI访问和距离之间的关系，并通过情感分析提高推荐准确性。</li>
<li>results: 与基eline算法进行比较，SBTRec实现了平均F1分数61.45%，表明其在序列预测任务中表现出色。<details>
<summary>Abstract</summary>
When traveling to an unfamiliar city for holidays, tourists often rely on guidebooks, travel websites, or recommendation systems to plan their daily itineraries and explore popular points of interest (POIs). However, these approaches may lack optimization in terms of time feasibility, localities, and user preferences. In this paper, we propose the SBTRec algorithm: a BERT-based Trajectory Recommendation with sentiment analysis, for recommending personalized sequences of POIs as itineraries. The key contributions of this work include analyzing users' check-ins and uploaded photos to understand the relationship between POI visits and distance. We introduce SBTRec, which encompasses sentiment analysis to improve recommendation accuracy by understanding users' preferences and satisfaction levels from reviews and comments about different POIs. Our proposed algorithms are evaluated against other sequence prediction methods using datasets from 8 cities. The results demonstrate that SBTRec achieves an average F1 score of 61.45%, outperforming baseline algorithms.   The paper further discusses the flexibility of the SBTRec algorithm, its ability to adapt to different scenarios and cities without modification, and its potential for extension by incorporating additional information for more reliable predictions. Overall, SBTRec provides personalized and relevant POI recommendations, enhancing tourists' overall trip experiences. Future work includes fine-tuning personalized embeddings for users, with evaluation of users' comments on POIs,~to further enhance prediction accuracy.
</details>
<details>
<summary>摘要</summary>
The paper further discusses the flexibility of the SBTRec algorithm, its ability to adapt to different scenarios and cities without modification, and its potential for extension by incorporating additional information for more reliable predictions. Overall, SBTRec provides personalized and relevant POI recommendations, enhancing tourists' overall trip experiences. Future work includes fine-tuning personalized embeddings for users, with evaluation of users' comments on POIs, to further enhance prediction accuracy.Translated into Simplified Chinese:旅游者在前往未知城市度假时，经常依靠旅游指南、旅游网站或推荐系统，以制定日程和探索流行点 интере斯（POIs）。然而，这些方法可能缺乏时间可行性、地点和用户偏好的优化。在这篇论文中，我们提出了SBTRec算法：一种基于BERT的 trajectory recommendation，具有情感分析，用于建议个性化POIs的顺序。我们的主要贡献包括分析用户检查到和上传照片，以理解POI访问和距离之间的关系。我们引入SBTRec，它包括情感分析，以提高推荐准确性。我们的提出的算法与其他序列预测方法进行比较，使用8个城市的数据。结果显示，SBTRec实现了平均F1分数为61.45%，超过基线算法。论文进一步讨论了SBTRec算法的灵活性，它可以适应不同的情况和城市，无需修改。此外，它还有扩展的潜在，通过添加更多信息，以提高预测的可靠性。总的来说，SBTRec提供了个性化和相关的POI推荐，提高旅游者的总体旅行体验。未来的工作包括个性化用户的嵌入调整，通过评估用户对POIs的评论，进一步提高预测准确性。
</details></li>
</ul>
<hr>
<h2 id="AIMS-EREA-–-A-framework-for-AI-accelerated-Innovation-of-Materials-for-Sustainability-–-for-Environmental-Remediation-and-Energy-Applications"><a href="#AIMS-EREA-–-A-framework-for-AI-accelerated-Innovation-of-Materials-for-Sustainability-–-for-Environmental-Remediation-and-Energy-Applications" class="headerlink" title="AIMS-EREA – A framework for AI-accelerated Innovation of Materials for Sustainability – for Environmental Remediation and Energy Applications"></a>AIMS-EREA – A framework for AI-accelerated Innovation of Materials for Sustainability – for Environmental Remediation and Energy Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11060">http://arxiv.org/abs/2311.11060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarson Roy Pratihar, Deepesh Pai, Manaswita Nag</li>
<li>For: 可以用于综合考虑多种可能性和结构，快速找到适合的绿色材料，以满足可持续发展的能源和环境修复应用。* Methods: 基于密度函数理论（DFT）和其他理论，以及人工智能技术，可以快速和效率地对可能性进行筛选和预测，从而降低实验室synthesis和分析过程中的努力和成本。* Results: 通过 combing best of breed of Material Science theory with the power of Generative AI, 可以快速和高效地找到适合的绿色材料，并且可以避免生产危险副产品的可能性。<details>
<summary>Abstract</summary>
Many environmental remediation and energy applications (conversion and storage) for sustainability need design and development of green novel materials. Discovery processes of such novel materials are time taking and cumbersome due to large number of possible combinations and permutations of materials structures. Often theoretical studies based on Density Functional Theory (DFT) and other theories, coupled with Simulations are conducted to narrow down sample space of candidate materials, before conducting laboratory-based synthesis and analytical process. With the emergence of artificial intelligence (AI), AI techniques are being tried in this process too to ease out simulation time and cost. However tremendous values of previously published research from various parts of the world are still left as labor-intensive manual effort and discretion of individual researcher and prone to human omissions. AIMS-EREA is our novel framework to blend best of breed of Material Science theory with power of Generative AI to give best impact and smooth and quickest discovery of material for sustainability. This also helps to eliminate the possibility of production of hazardous residues and bye-products of the reactions. AIMS-EREA uses all available resources -- Predictive and Analytical AI on large collection of chemical databases along with automated intelligent assimilation of deep materials knowledge from previously published research works through Generative AI. We demonstrate use of our own novel framework with an example, how this framework can be successfully applied to achieve desired success in development of thermoelectric material for waste heat conversion.
</details>
<details>
<summary>摘要</summary>
多种环境恢复和能源应用（转化和存储）需要设计和开发绿色新材料。发现这些新材料的过程是时间consuming和复杂，因为有很多可能的组合和排序结构。经常通过密度函数理论（DFT）和其他理论，加上计算机模拟，来缩小实验室合成和分析过程中的样本空间。随着人工智能（AI）的出现，AI技术也在这个过程中使用，以减少计算时间和成本。然而，大量前期发表的研究成果仍然受到劳动密集和个人研究者的主观性的影响，容易出现人类缺失。我们的AIMS-EREA框架通过融合材料科学理论和生成AI的力量，为可持续发展提供了最佳影响和最快速的材料发现。此外，它还可以消除生产过程中可能产生的危险副产品。AIMS-EREA利用了所有可用资源——预测和分析AI在大量化学数据库中，以及自动智能吸收深入材料知识从前期发表的研究作品中。我们示例如如何使用我们的框架成功应用于废热电转换材料的开发。
</details></li>
</ul>
<hr>
<h2 id="Designing-Interpretable-ML-System-to-Enhance-Trustworthy-AI-in-Healthcare-A-Systematic-Review-of-the-Last-Decade-to-A-Proposed-Robust-Framework"><a href="#Designing-Interpretable-ML-System-to-Enhance-Trustworthy-AI-in-Healthcare-A-Systematic-Review-of-the-Last-Decade-to-A-Proposed-Robust-Framework" class="headerlink" title="Designing Interpretable ML System to Enhance Trustworthy AI in Healthcare: A Systematic Review of the Last Decade to A Proposed Robust Framework"></a>Designing Interpretable ML System to Enhance Trustworthy AI in Healthcare: A Systematic Review of the Last Decade to A Proposed Robust Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11055">http://arxiv.org/abs/2311.11055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elham Nasarian, Roohallah Alizadehsani, U. Rajendra Acharyac, d Kwok-Leung Tsui<br>for: This paper aims to review and discuss the processes and challenges of interpretable machine learning (IML) and explainable AI (XAI) in healthcare, with a focus on quality control and the importance of robust interpretability.methods: The paper uses a systematic literature review approach, searching PubMed, Scopus, and Web of Science databases using specific strings to identify relevant studies. The IML process is classified into three stages: data pre-processing interpretability, interpretable modeling, and post-processing interpretability.results: The paper provides experimental results to establish the importance of robust interpretability in healthcare, and offers insights for creating communicable clinician-AI tools. The survey also introduces a step-by-step roadmap for implementing XAI in clinical applications, addressing existing gaps and acknowledging XAI model limitations.<details>
<summary>Abstract</summary>
AI-based medical technologies, including wearables, telemedicine, LLMs, and digital care twins, significantly impact healthcare. Ensuring AI results are accurate and interpretable is crucial, especially for clinicians. This paper reviews processes and challenges of interpretable ML (IML) and explainable AI (XAI) in healthcare. Objectives include reviewing XAI processes, methods, applications, and challenges, with a focus on quality control. The IML process is classified into data pre-processing interpretability, interpretable modeling, and post-processing interpretability. The paper aims to establish the importance of robust interpretability in healthcare through experimental results, providing insights for creating communicable clinician-AI tools. Research questions, eligibility criteria, and goals were identified following PRISMA and PICO methods. PubMed, Scopus, and Web of Science were systematically searched using specific strings. The survey introduces a step-by-step roadmap for implementing XAI in clinical applications, addressing existing gaps and acknowledging XAI model limitations.
</details>
<details>
<summary>摘要</summary>
人工智能技术在医疗领域的应用，包括智能服务器、远程医疗、语言模型和数字护理双，对医疗业产生了深远的影响。为确保人工智能结果准确和可解释，特别是 для临床医生，在医疗领域中确保可解释的机器学习（IML）和可解释人工智能（XAI）的过程和挑战是非常重要。本文将对可解释ML（IML）和可解释人工智能（XAI）在医疗领域的过程和挑战进行了评估。包括数据预处理可解释、可解释模型和后处理可解释在内的IML过程将被分类。本文的目标是通过实验结果证明可Robust可解释在医疗领域的重要性，并为创建可通信的医生-AI工具提供了新的发现。根据PRISMA和PICO方法，我们定义了研究问题、适用性标准和目标。通过对PubMed、Scopus和Web of Science等数据库进行系统性搜索，我们使用特定的搜索串检索相关文献。本文将提供一个步骤并进的路线图，以帮助实施XAI在临床应用中，并解决现有的坑害和XAI模型的限制。
</details></li>
</ul>
<hr>
<h2 id="Orca-2-Teaching-Small-Language-Models-How-to-Reason"><a href="#Orca-2-Teaching-Small-Language-Models-How-to-Reason" class="headerlink" title="Orca 2: Teaching Small Language Models How to Reason"></a>Orca 2: Teaching Small Language Models How to Reason</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11045">http://arxiv.org/abs/2311.11045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, Ahmed Awadallah</li>
<li>for: 本研究旨在探讨如何通过改进训练信号来提高小型语言模型（LM）的逻辑能力。</li>
<li>methods: 研究人员使用了不同的解释Trace来训练小型LM，并研究了不同的解释策略（如步骤解释、记忆然后生成、记忆然后解释生成等），以帮助模型选择最佳解释策略以适应不同任务。</li>
<li>results: Orca 2在15个多样化的 benchmarck 上表现出优于同类型模型和大型模型的 Zero-shot 表现，并在复杂任务中达到了与大型模型相当或更好的水平。<details>
<summary>Abstract</summary>
Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. We open-source Orca 2 to encourage further research on the development, evaluation, and alignment of smaller LMs.
</details>
<details>
<summary>摘要</summary>
鲸鱼1从 ricSignals 学习，如解释迹象，使其在 BigBench Hard 和 AGIEval 上表现出色。在 Orca 2 中，我们继续探索如何提高训练信号可以提高小LMs 的理解能力。研究小LMs 的训练通常采用了模仿学习，以复制更强大的模型的输出。我们认为过分强调模仿可能会限制小LMs 的潜力。我们尝试教育小LMs 使用不同的解决方案来解决不同任务，可能与更大的模型不同。例如，更大的模型可能会提供一个直接回答复杂任务的方法，而小LMs 可能没有同样的容量。在 Orca 2 中，我们教育模型多种理解技巧（步骤、回忆然后生成、回忆然后生成、直接回答等）。更重要的是，我们努力帮助模型学习选择每个任务最有效的解决方案。我们使用了15种多样化的benchmark（相当于100个任务和36,000个唯一提问）来评估 Orca 2。结果显示，Orca 2 在复杂任务中表现出色，并在零容量情况下与5-10倍大的模型相当或更好的表现。我们将 Orca 2 开源，以便进一步研究小LMs 的发展、评估和对齐。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Data-Generation-for-Bridging-Sim2Real-Gap-in-a-Production-Environment"><a href="#Synthetic-Data-Generation-for-Bridging-Sim2Real-Gap-in-a-Production-Environment" class="headerlink" title="Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment"></a>Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11039">http://arxiv.org/abs/2311.11039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parth Rawal, Mrunal Sompura, Wolfgang Hintze</li>
<li>for: 这篇论文的目的是提出一种基于模拟数据生成的方法，用于帮助机器人在生产环境中使用人工智能。</li>
<li>methods: 论文使用的方法包括基于模拟数据的生成和组合，以帮助减少实际环境和模拟环境之间的差异。</li>
<li>results: 试验结果表明，使用这些基本方法的组合可以在生产环境中减少模拟到现实之间的差异，从而提高机器人在生产中的表现。<details>
<summary>Abstract</summary>
Synthetic data is being used lately for training deep neural networks in computer vision applications such as object detection, object segmentation and 6D object pose estimation. Domain randomization hereby plays an important role in reducing the simulation to reality gap. However, this generalization might not be effective in specialized domains like a production environment involving complex assemblies. Either the individual parts, trained with synthetic images, are integrated in much larger assemblies making them indistinguishable from their counterparts and result in false positives or are partially occluded just enough to give rise to false negatives. Domain knowledge is vital in these cases and if conceived effectively while generating synthetic data, can show a considerable improvement in bridging the simulation to reality gap. This paper focuses on synthetic data generation procedures for parts and assemblies used in a production environment. The basic procedures for synthetic data generation and their various combinations are evaluated and compared on images captured in a production environment, where results show up to 15% improvement using combinations of basic procedures. Reducing the simulation to reality gap in this way can aid to utilize the true potential of robot assisted production using artificial intelligence.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Center-Audio-Video-Intelligence-on-Device-DAVID-–-An-Edge-AI-Platform-for-Smart-Toys"><a href="#Data-Center-Audio-Video-Intelligence-on-Device-DAVID-–-An-Edge-AI-Platform-for-Smart-Toys" class="headerlink" title="Data Center Audio&#x2F;Video Intelligence on Device (DAVID) – An Edge-AI Platform for Smart-Toys"></a>Data Center Audio&#x2F;Video Intelligence on Device (DAVID) – An Edge-AI Platform for Smart-Toys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11030">http://arxiv.org/abs/2311.11030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Cosache, Francisco Salgado, Cosmin Rotariu, George Sterpu, Rishabh Jain, Peter Corcoran</li>
<li>for: 这份论文主要用于介绍一种 Edge AI 平台，即 DAVID Smart-Toy 平台，该平台包含了高级低功耗数据处理的神经推理模型，并与相关的图像或音频感知器一起嵌入在设备中。</li>
<li>methods: 该平台使用了神经推理模型进行数据处理，并提供了在设备内部进行文本识别和语音生成功能。</li>
<li>results: 该平台可以根据用户的语音和面部表达进行识别和 интерпретаción，并且具有嵌入式的数据保护功能，以保障用户的隐私。<details>
<summary>Abstract</summary>
An overview is given of the DAVID Smart-Toy platform, one of the first Edge AI platform designs to incorporate advanced low-power data processing by neural inference models co-located with the relevant image or audio sensors. There is also on-board capability for in-device text-to-speech generation. Two alternative embodiments are presented: a smart Teddy-bear, and a roving dog-like robot. The platform offers a speech-driven user interface and can observe and interpret user actions and facial expressions via its computer vision sensor node. A particular benefit of this design is that no personally identifiable information passes beyond the neural inference nodes thus providing inbuilt compliance with data protection regulations.
</details>
<details>
<summary>摘要</summary>
TEXT这里提供了DAVID智能玩具平台的总览，这是首先采用进步低功耗神经推论模型与相应的图像或语音感应器集成的 Edge AI 平台设计。它还具有内置的文本转语音功能。这两个版本中的一个是聪明的 teddy bear，另一个是一只行走的狗like 机器人。这个平台具有语音驱动的用户界面，可以通过计算机视觉感应器监测和解读用户的动作和表情。特别的是，这个设计不会将个人识别信息传递到神经推论节点以外，因此提供了内置的数据保护规定的实现。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Geometric-Data-Augmentations-to-Mitigate-Distribution-Shifts-in-Pollen-Classification-from-Microscopic-Images"><a href="#Geometric-Data-Augmentations-to-Mitigate-Distribution-Shifts-in-Pollen-Classification-from-Microscopic-Images" class="headerlink" title="Geometric Data Augmentations to Mitigate Distribution Shifts in Pollen Classification from Microscopic Images"></a>Geometric Data Augmentations to Mitigate Distribution Shifts in Pollen Classification from Microscopic Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11029">http://arxiv.org/abs/2311.11029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nam Cao, Olga Saukh</li>
<li>for: 本研究旨在解决机器学习模型在真实应用场景中的精度下降问题，具体是对野外采集的蜕花粉样图像使用低成本摄像头摄取器进行分类。</li>
<li>methods: 我们利用了领域知识，即形态特征对准精确蜕花识别非常重要，并引入了两种新的几何图像增强技术来减少模型在训练和测试数据集之间的准确度差距。特别是，我们表明了Tenengrad和ImageToSketch筛选器能够很好地补做形态和文本信息，而不会让模型受到不重要的细节的干扰。</li>
<li>results: 我们进行了广泛的评估，并证明了 geometric 增强技术可以在不同的模型架构上提供一定的改进，其中最高达14%。此外，我们还进行了减少采集的滤波器和图像增强器的综合评估，并证明了我们的几何增强技术在文献中的评分最高。<details>
<summary>Abstract</summary>
Distribution shifts are characterized by differences between the training and test data distributions. They can significantly reduce the accuracy of machine learning models deployed in real-world scenarios. This paper explores the distribution shift problem when classifying pollen grains from microscopic images collected in the wild with a low-cost camera sensor. We leverage the domain knowledge that geometric features are highly important for accurate pollen identification and introduce two novel geometric image augmentation techniques to significantly narrow the accuracy gap between the model performance on the train and test datasets. In particular, we show that Tenengrad and ImageToSketch filters are highly effective to balance the shape and texture information while leaving out unimportant details that may confuse the model. Extensive evaluations on various model architectures demonstrate a consistent improvement of the model generalization to field data of up to 14% achieved by the geometric augmentation techniques when compared to a wide range of standard image augmentations. The approach is validated through an ablation study using pollen hydration tests to recover the shape of dry pollen grains. The proposed geometric augmentations also receive the highest scores according to the affinity and diversity measures from the literature.
</details>
<details>
<summary>摘要</summary>
分布Shift问题是指训练和测试数据之间的差异，可能导致机器学习模型在实际应用场景中的精度下降。本文探讨了在野外采集的杂花粉胞微scopic图像中的分布Shift问题，并利用域知识，认为 геометрические特征对于准确的杂花识别非常重要。我们引入了两种新的地形图像增强技术，以减少模型在训练和测试数据集之间的精度差距。特别是，我们表明了Tenengrad和ImageToSketch筛选器在平衡形态和文本信息的同时，留下无关重要信息的能力，可以提高模型的总体性能。我们进行了对多种模型架构的广泛评估，并证明了地形增强技术可以在 field data 上提高模型总体性能达到14%。我们还进行了一项ablation study，用气压测试来恢复干燥杂花粉胞的形状，以验证我们的方法。此外，我们的地形增强技术也在文献中得到了最高的评分。
</details></li>
</ul>
<hr>
<h2 id="Lesion-Search-with-Self-supervised-Learning"><a href="#Lesion-Search-with-Self-supervised-Learning" class="headerlink" title="Lesion Search with Self-supervised Learning"></a>Lesion Search with Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11014">http://arxiv.org/abs/2311.11014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristin Qi, Jiali Cheng, Daniel Haehn</li>
<li>for: 帮助临床专业人员更快地查找相似图像，不需要手动标注。</li>
<li>methods: 使用自动学习的对比学习（SimCLR）来实现内容基于图像检索（CBIR），并使用Generalized-mean（GeM）池化和L2normalization来分类疾病类型和检索相似图像。</li>
<li>results: 实现了提高的表现。 Additionally, 开发了一个开源的图像分析和检索应用程序，易于集成，可能对临床专业人员的日常活动产生潜在支持。<details>
<summary>Abstract</summary>
Content-based image retrieval (CBIR) with self-supervised learning (SSL) accelerates clinicians' interpretation of similar images without manual annotations. We develop a CBIR from the contrastive learning SimCLR and incorporate a generalized-mean (GeM) pooling followed by L2 normalization to classify lesion types and retrieve similar images before clinicians' analysis. Results have shown improved performance. We additionally build an open-source application for image analysis and retrieval. The application is easy to integrate, relieving manual efforts and suggesting the potential to support clinicians' everyday activities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multiple-View-Geometry-Transformers-for-3D-Human-Pose-Estimation"><a href="#Multiple-View-Geometry-Transformers-for-3D-Human-Pose-Estimation" class="headerlink" title="Multiple View Geometry Transformers for 3D Human Pose Estimation"></a>Multiple View Geometry Transformers for 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10983">http://arxiv.org/abs/2311.10983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Liao, Jialiang Zhu, Chunyu Wang, Han Hu, Steven L. Waslander</li>
<li>for: 提高多视图3D人姿估计中Transformers的3D理解能力</li>
<li>methods: 提出了一种新的混合模型MVGFormer，包括不可变的geometry模块和可变的appearance模块，可以同时处理视角依赖的3D任务和图像信号</li>
<li>results: 对于域内和域外设置，模型均表现出优于当前状态艺法，特别是在域外设置下表现出了明显的优势，并且可以在新的摄像头和几何学上进行普适化。<details>
<summary>Abstract</summary>
In this work, we aim to improve the 3D reasoning ability of Transformers in multi-view 3D human pose estimation. Recent works have focused on end-to-end learning-based transformer designs, which struggle to resolve geometric information accurately, particularly during occlusion. Instead, we propose a novel hybrid model, MVGFormer, which has a series of geometric and appearance modules organized in an iterative manner. The geometry modules are learning-free and handle all viewpoint-dependent 3D tasks geometrically which notably improves the model's generalization ability. The appearance modules are learnable and are dedicated to estimating 2D poses from image signals end-to-end which enables them to achieve accurate estimates even when occlusion occurs, leading to a model that is both accurate and generalizable to new cameras and geometries. We evaluate our approach for both in-domain and out-of-domain settings, where our model consistently outperforms state-of-the-art methods, and especially does so by a significant margin in the out-of-domain setting. We will release the code and models: https://github.com/XunshanMan/MVGFormer.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们目标是提高 transformer 在多视图三维人姿估计中的3D理解能力。先前的工作主要集中于终端学习基于 transformer 的设计，它们在 occlusion 时具有准确地理信息的解决能力有限。相比之下，我们提议一种新的混合模型，MVGFormer，它包括一系列具有学习自由的geometry模块和学习端到端的 appearance模块，这些模块在执行循环方式下组织。geometry模块负责所有视点依赖的3D任务，可以大幅提高模型的泛化能力。appearance模块通过 directly 从图像信号中学习2D姿势，可以在 occlusion 时达到高度准确的估计，导致模型具有高度准确和泛化性。我们在域内和域外设置下评估了我们的方法，其中我们的方法在域外设置下一直保持领先，特别是在 occlusion 情况下，我们的方法表现出了明显的优势。我们将在 GitHub 上发布代码和模型：https://github.com/XunshanMan/MVGFormer。
</details></li>
</ul>
<hr>
<h2 id="HungerGist-An-Interpretable-Predictive-Model-for-Food-Insecurity"><a href="#HungerGist-An-Interpretable-Predictive-Model-for-Food-Insecurity" class="headerlink" title="HungerGist: An Interpretable Predictive Model for Food Insecurity"></a>HungerGist: An Interpretable Predictive Model for Food Insecurity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10953">http://arxiv.org/abs/2311.10953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongsu Ahn, Muheng Yan, Yu-Ru Lin, Zian Wang<br>for: This paper aims to address the critical need for advanced early warning systems to combat escalating food insecurity in Africa, which is caused by factors such as war, climate change, and poverty.methods: The paper introduces a multi-task deep learning model called “HungerGist” that utilizes news texts and natural language processing (NLP) techniques to analyze and predict food insecurity.results: The model outperforms the baseline method trained on both traditional risk factors and human-curated keywords, and has the ability to detect critical texts that contain interpretable signals known as “gists.” Additionally, the approach has the potential to reveal latent factors that would otherwise remain concealed in unstructured texts.<details>
<summary>Abstract</summary>
The escalating food insecurity in Africa, caused by factors such as war, climate change, and poverty, demonstrates the critical need for advanced early warning systems. Traditional methodologies, relying on expert-curated data encompassing climate, geography, and social disturbances, often fall short due to data limitations, hindering comprehensive analysis and potential discovery of new predictive factors. To address this, this paper introduces "HungerGist", a multi-task deep learning model utilizing news texts and NLP techniques. Using a corpus of over 53,000 news articles from nine African countries over four years, we demonstrate that our model, trained solely on news data, outperforms the baseline method trained on both traditional risk factors and human-curated keywords. In addition, our method has the ability to detect critical texts that contain interpretable signals known as "gists." Moreover, our examination of these gists indicates that this approach has the potential to reveal latent factors that would otherwise remain concealed in unstructured texts.
</details>
<details>
<summary>摘要</summary>
“非洲的食品不安全升级，由于战争、气候变化和贫困等因素，表明了高度需要先进早期警示系统。传统的方法，依靠专家手动维护的数据，包括气候、地理和社会冲击，经常因数据限制而受到限制，阻碍了全面分析和潜在的新预测因素的发现。为解决这个问题，本文介绍了“饥饿精”，一种多任务深度学习模型，使用新闻文本和自然语言处理技术。使用9个非洲国家的4年新闻文章 corps（总计53,000篇），我们表明了我们的模型，通过新闻数据进行训练，比基eline方法（基于传统风险因素和人工标记）更高效。此外，我们的方法还能探测关键的新闻文本，含有可解释的信号，称为“精”。此外，我们的研究表明，这种方法有潜在的发现隐藏在未结构化文本中的因素的潜力。”
</details></li>
</ul>
<hr>
<h2 id="RecExplainer-Aligning-Large-Language-Models-for-Recommendation-Model-Interpretability"><a href="#RecExplainer-Aligning-Large-Language-Models-for-Recommendation-Model-Interpretability" class="headerlink" title="RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability"></a>RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10947">http://arxiv.org/abs/2311.10947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie</li>
<li>for: 这个论文的目的是提出一种新的模型解释方法，使用大语言模型（LLM）作为庸服务器模型的代理模型，以便更好地理解和解释推荐模型的行为。</li>
<li>methods: 该方法使用了三种Alignment方法：行为对齐、意图对齐和混合对齐。行为对齐在语言空间中表示用户喜好和物品信息为文本，以学习推荐模型的行为；意图对齐在推荐模型的latent空间中工作，使用用户和物品表示来理解模型的行为；混合对齐组合了语言和latent空间进行对齐训练。</li>
<li>results: 经过测试，该方法能够有效地使LLMs理解推荐模型的行为，并生成高度可信的推荐解释。<details>
<summary>Abstract</summary>
Recommender systems are widely used in various online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often lack interpretability, making them less reliable and transparent for both users and developers. With the emergence of large language models (LLMs), we find that their capabilities in language expression, knowledge-aware reasoning, and instruction following are exceptionally powerful. Based on this, we propose a new model interpretation approach for recommender systems, by using LLMs as surrogate models and learn to mimic and comprehend target recommender models. Specifically, we introduce three alignment methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to learn the recommendation model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces for alignment training. To demonstrate the effectiveness of our methods, we conduct evaluation from two perspectives: alignment effect, and explanation generation ability on three public datasets. Experimental results indicate that our approach effectively enables LLMs to comprehend the patterns of recommendation models and generate highly credible recommendation explanations.
</details>
<details>
<summary>摘要</summary>
推荐系统在线服务中广泛应用，尤其是基于嵌入式模型，因其可以表达复杂的信号。然而，这些模型通常缺乏可读性，使得用户和开发者对其不可靠和透明度感到不满。随着大语言模型（LLMs）的出现，我们发现它们在语言表达、知识感知和指令遵循方面具有极高的能力。基于这一点，我们提出了一种新的推荐系统模型解释方法，通过使用 LLMS 作为代理模型，并学习模仿和理解目标推荐模型的行为。 Specifically, we introduce three alignment methods: 行为对齐、意图对齐和混合对齐。行为对齐在语言空间中表示用户偏好和物品信息为文本，以学习推荐模型的行为;意图对齐在推荐模型的latent空间中使用用户和物品表示，以理解模型的行为;混合对齐将语言和latent空间进行对齐训练。为证明我们的方法的有效性，我们从两个角度进行评估：对齐效果和解释生成能力，并在三个公共数据集上进行实验。实验结果表明，我们的方法可以有效地使 LLMS 理解推荐模型的模式，并生成高可信度的推荐解释。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Bayes-Framework-for-Open-Domain-Dialogue-Generation"><a href="#An-Empirical-Bayes-Framework-for-Open-Domain-Dialogue-Generation" class="headerlink" title="An Empirical Bayes Framework for Open-Domain Dialogue Generation"></a>An Empirical Bayes Framework for Open-Domain Dialogue Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10945">http://arxiv.org/abs/2311.10945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Yang Lee, Kong Aik Lee, Woon-Seng Gan</li>
<li>for: 开发一个可以与人类用户进行意义性对话的开放领域对话机器人。</li>
<li>methods: 使用预训练语言模型和 bayesian 方法来建立一个 Bayesian 开放领域对话机器人。</li>
<li>results: BODEB  Framework 在多样性和协调性两个方面都达到了更好的结果，比较于Variational Frameworks。<details>
<summary>Abstract</summary>
To engage human users in meaningful conversation, open-domain dialogue agents are required to generate diverse and contextually coherent dialogue. Despite recent advancements, which can be attributed to the usage of pretrained language models, the generation of diverse and coherent dialogue remains an open research problem. A popular approach to address this issue involves the adaptation of variational frameworks. However, while these approaches successfully improve diversity, they tend to compromise on contextual coherence. Hence, we propose the Bayesian Open-domain Dialogue with Empirical Bayes (BODEB) framework, an empirical bayes framework for constructing an Bayesian open-domain dialogue agent by leveraging pretrained parameters to inform the prior and posterior parameter distributions. Empirical results show that BODEB achieves better results in terms of both diversity and coherence compared to variational frameworks.
</details>
<details>
<summary>摘要</summary>
要让人工智能对话机器人与人进行有意义的对话，开放领域对话代理需要生成多样化和上下文相关的对话。尽管最近的进步可以归功于预训练语言模型的使用，但生成多样化和上下文相关的对话仍然是一个开放的研究问题。一种受欢迎的方法来解决这个问题是适应变量框架。然而，这些方法通常会牺牲上下文相关性。因此，我们提出了概率开放领域对话框架（BODEB），一种基于预训练参数的 bayesian 开放领域对话代理。实际结果表明，BODEB 在多样性和上下文相关性两个方面都比变量框架更好。
</details></li>
</ul>
<hr>
<h2 id="Practical-Estimation-of-Ensemble-Accuracy"><a href="#Practical-Estimation-of-Ensemble-Accuracy" class="headerlink" title="Practical Estimation of Ensemble Accuracy"></a>Practical Estimation of Ensemble Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10940">http://arxiv.org/abs/2311.10940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Simi Haber, Yonatan Wexler</li>
<li>for: 这个论文是为了提出一种可行的方法来估计多个分类器的共同力量，而不需要标签信息，因此可以在无监督学习的大规模数据集上进行实际应用。</li>
<li>methods: 该方法基于一种新的 combinatorial bound 来估计分类器的集合准确率，这个 bound 可以在样本数量linear增长的情况下高效地近似，从而实现一个高效地寻找高 JOIN 准确率的分类器集合的方法。</li>
<li>results: 作者在popular的大规模人脸识别数据集上进行了实验，并证明了该方法的可行性和实用性，同时该方法可以在无监督学习的情况下提供高度的精度和稳定性。<details>
<summary>Abstract</summary>
Ensemble learning combines several individual models to obtain better generalization performance. In this work we present a practical method for estimating the joint power of several classifiers which differs from existing approaches by {\em not relying on labels}, hence enabling the work in unsupervised setting of huge datasets. It differs from existing methods which define a "diversity measure".   The heart of the method is a combinatorial bound on the number of mistakes the ensemble is likely to make. The bound can be efficiently approximated in time linear in the number of samples. Thus allowing an efficient search for a combination of classifiers that are likely to produce higher joint accuracy. Moreover, having the bound applicable to unlabeled data makes it both accurate and practical in modern setting of unsupervised learning. We demonstrate the method on popular large-scale face recognition datasets which provide a useful playground for fine-grain classification tasks using noisy data over many classes.   The proposed framework fits neatly in trending practices of unsupervised learning. It is a measure of the inherent independence of a set of classifiers not relying on extra information such as another classifier or labeled data.
</details>
<details>
<summary>摘要</summary>
ensemble learning可以提高总体化性能，在这个工作中我们提出了一种实用的方法，不同于现有方法，这种方法不需要标签，因此可以在无标签 dataset 上进行学习。它与现有方法不同，这种方法定义一个“多样性度量”。 ensemble learning 的核心思想是一种可以有效地估计多个分类器的结合力的 bounds，这个 bounds 可以在样本数 linear 时间内efficiently  aproximated。因此，可以有效地搜索一组可能 producen higher 的 joint accuracy 的分类器组合。此外，由于这个 bounds 适用于无标签数据，这使得它在现代无supervised learning 中具有准确性和实用性。我们在流行的大规模人脸识别 dataset 上进行了示例，这些 dataset 提供了一个有用的游戏场景，用于精细的分类任务，使用噪音数据。our proposed framework 适合当前流行的无supervised learning 做法，它是一种不依赖于其他分类器或标签数据的独立性度量。
</details></li>
</ul>
<hr>
<h2 id="Case-Repositories-Towards-Case-Based-Reasoning-for-AI-Alignment"><a href="#Case-Repositories-Towards-Case-Based-Reasoning-for-AI-Alignment" class="headerlink" title="Case Repositories: Towards Case-Based Reasoning for AI Alignment"></a>Case Repositories: Towards Case-Based Reasoning for AI Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10934">http://arxiv.org/abs/2311.10934</a></li>
<li>repo_url: None</li>
<li>paper_authors: K. J. Kevin Feng, Quan Ze, Chen, Inyoung Cheong, King Xia, Amy X. Zhang</li>
<li>for: 该论文旨在提出一种基于情感reasoning（CBR）的AI对齐方法，以解决在不同个人和社区中存在复杂和uncertain的社会问题时，AI系统如何对齐。</li>
<li>methods: 该论文提出了一种结合案例研究（CBR）的方法，包括收集seed例题（问题），召集领域专家来确定领域特定的关键维度，使用自然语言处理技术生成不同的案例，并通过公众参与来评判和改进案例。</li>
<li>results: 该论文认为，通过建立一个包含多种案例的案例库，可以帮助AI系统对齐，同时也可以提供一个让人们进行道德反思的平台。<details>
<summary>Abstract</summary>
Case studies commonly form the pedagogical backbone in law, ethics, and many other domains that face complex and ambiguous societal questions informed by human values. Similar complexities and ambiguities arise when we consider how AI should be aligned in practice: when faced with vast quantities of diverse (and sometimes conflicting) values from different individuals and communities, with whose values is AI to align, and how should AI do so? We propose a complementary approach to constitutional AI alignment, grounded in ideas from case-based reasoning (CBR), that focuses on the construction of policies through judgments on a set of cases. We present a process to assemble such a case repository by: 1) gathering a set of ``seed'' cases -- questions one may ask an AI system -- in a particular domain from discussions in online communities, 2) eliciting domain-specific key dimensions for cases through workshops with domain experts, 3) using LLMs to generate variations of cases not seen in the wild, and 4) engaging with the public to judge and improve cases. We then discuss how such a case repository could assist in AI alignment, both through directly acting as precedents to ground acceptable behaviors, and as a medium for individuals and communities to engage in moral reasoning around AI
</details>
<details>
<summary>摘要</summary>
法律、伦理和其他领域的教学经常作为智能机器的准则核心，面临着复杂和uncertain的社会问题，受到人类价值观的框架。在实践中，AI的Alignment也面临着类似的复杂性和uncertainty，问题是AI与 whose values should it align, and how should it do so? We propose a complementary approach to constitutional AI alignment, based on ideas from case-based reasoning (CBR), which focuses on constructing policies through judgments on a set of cases. We present a process to assemble such a case repository by:1. 收集域 especific seed cases（问题可以对AI系统提问）from online community discussions,2. 通过域专家组织工作shop elicit domain-specific key dimensions for cases,3. 使用LLMs生成不同from wild variations of cases,4. 与公众交流，评估和改进cases.然后，我们讨论了如何使用这个案例库来帮助AI的Alignment，包括直接作为行为的根据，以及作为人们和社区们在AI的伦理思考中的媒介。
</details></li>
</ul>
<hr>
<h2 id="Representing-visual-classification-as-a-linear-combination-of-words"><a href="#Representing-visual-classification-as-a-linear-combination-of-words" class="headerlink" title="Representing visual classification as a linear combination of words"></a>Representing visual classification as a linear combination of words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10933">http://arxiv.org/abs/2311.10933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lotterlab/task_word_explainability">https://github.com/lotterlab/task_word_explainability</a></li>
<li>paper_authors: Shobhit Agarwal, Yevgeniy R. Semenov, William Lotter</li>
<li>for: 这个论文的目的是解释深度学习模型在医疗领域中的决策过程，并提供一种基于视觉和语言的解释策略。</li>
<li>methods: 该论文使用了一种视觉语言模型来identify语言基于描述器，并使用这些描述器来描述视觉分类任务的决策过程。</li>
<li>results: 研究发现，使用这种解释策略可以提供一些与临床知识相符的描述器，并且可以帮助非专业人员完成一些特殊的医疗任务。同时，研究还发现了公共数据集中存在一些”短cut连接”的问题。<details>
<summary>Abstract</summary>
Explainability is a longstanding challenge in deep learning, especially in high-stakes domains like healthcare. Common explainability methods highlight image regions that drive an AI model's decision. Humans, however, heavily rely on language to convey explanations of not only "where" but "what". Additionally, most explainability approaches focus on explaining individual AI predictions, rather than describing the features used by an AI model in general. The latter would be especially useful for model and dataset auditing, and potentially even knowledge generation as AI is increasingly being used in novel tasks. Here, we present an explainability strategy that uses a vision-language model to identify language-based descriptors of a visual classification task. By leveraging a pre-trained joint embedding space between images and text, our approach estimates a new classification task as a linear combination of words, resulting in a weight for each word that indicates its alignment with the vision-based classifier. We assess our approach using two medical imaging classification tasks, where we find that the resulting descriptors largely align with clinical knowledge despite a lack of domain-specific language training. However, our approach also identifies the potential for 'shortcut connections' in the public datasets used. Towards a functional measure of explainability, we perform a pilot reader study where we find that the AI-identified words can enable non-expert humans to perform a specialized medical task at a non-trivial level. Altogether, our results emphasize the potential of using multimodal foundational models to deliver intuitive, language-based explanations of visual tasks.
</details>
<details>
<summary>摘要</summary>
explainability 是深度学习中长期的挑战，尤其在医疗领域。常见的解释方法会强调图像区域，帮助人类理解 AI 模型做出的决定。然而，人类主要通过语言来传达解释，不仅包括 "where"，还包括 "what"。此外，大多数解释方法都是解释个别 AI 预测，而不是描述 AI 模型在总体上使用的特征。后者尤其有用于模型和数据集 Auditing，以及可能even 生成知识，因为 AI 在新任务中使用的情况在增加。在这里，我们提出了一种解释策略，使用视觉语言模型来identify图像中的语言基于描述器。我们利用预训练的共同 embedding空间，以图像和文本的 JOINT  embedding space，我们的方法可以将新的分类任务看作是一种线性组合的 слова，从而得到一个对应于每个单词的权重，这个权重指示单词与视觉基于分类器的Alignment。我们通过两个医疗成像分类任务进行评估，发现我们的方法可以获得与临床知识相当的描述器，即使没有域 específico 语言培训。然而，我们的方法还发现了公共数据集中的 "短cut 连接" 问题。为了评估函数性的解释度量，我们进行了一个 Pilot 读者研究，发现 AI 标识的单词可以帮助非专业人员在特殊医疗任务中达到非常轻量级的性能。总之，我们的结果强调了使用多Modal 基础模型来提供直观的语言基于解释，以便更好地理解视觉任务。
</details></li>
</ul>
<hr>
<h2 id="Cognitive-bias-in-large-language-models-Cautious-optimism-meets-anti-Panglossian-meliorism"><a href="#Cognitive-bias-in-large-language-models-Cautious-optimism-meets-anti-Panglossian-meliorism" class="headerlink" title="Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism"></a>Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10932">http://arxiv.org/abs/2311.10932</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Thorstad</li>
<li>for: 本文旨在探讨大语言模型中的偏见问题，特别是在不公正对待少数群体方面。</li>
<li>methods: 本文使用了现代语言模型评估方法，并对现有的偏见研究进行了探讨和分析。</li>
<li>results: 本文指出，现有的大语言模型可能存在一些偏见问题，并提出了一些途径来减少这些偏见。同时，本文也探讨了人类认知偏见的哲学意义以及模型偏见的原因。<details>
<summary>Abstract</summary>
Traditional discussions of bias in large language models focus on a conception of bias closely tied to unfairness, especially as affecting marginalized groups. Recent work raises the novel possibility of assessing the outputs of large language models for a range of cognitive biases familiar from research in judgment and decisionmaking. My aim in this paper is to draw two lessons from recent discussions of cognitive bias in large language models: cautious optimism about the prevalence of bias in current models coupled with an anti-Panglossian willingness to concede the existence of some genuine biases and work to reduce them. I draw out philosophical implications of this discussion for the rationality of human cognitive biases as well as the role of unrepresentative data in driving model biases.
</details>
<details>
<summary>摘要</summary>
传统的大语言模型偏见讨论围绕着不公正性，特别是对弱势群体产生影响。 current work 提出了评估大语言模型输出的多种认知偏见的新可能性，这些偏见 Familiar from research on judgment and decision-making. My goal in this paper is to draw two lessons from recent discussions of cognitive bias in large language models: cautious optimism about the prevalence of bias in current models, combined with an anti-Panglossian willingness to acknowledge the existence of some genuine biases and work to reduce them. I will draw out the philosophical implications of this discussion for human cognitive biases and the role of unrepresentative data in driving model biases.Note: "Panglossian" refers to the tendency to overlook or downplay the existence of negative aspects or biases, named after the character Dr. Pangloss in Voltaire's Candide. The term "anti-Panglossian" is used to describe a willingness to acknowledge and address such biases.
</details></li>
</ul>
<hr>
<h2 id="CAMRA-Copilot-for-AMR-Annotation"><a href="#CAMRA-Copilot-for-AMR-Annotation" class="headerlink" title="CAMRA: Copilot for AMR Annotation"></a>CAMRA: Copilot for AMR Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10928">http://arxiv.org/abs/2311.10928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jon Z. Cai, Shafiuddin Rehan Ahmed, Julia Bonn, Kristin Wright-Bettner, Martha Palmer, James H. Martin</li>
<li>for: 本研究旨在开发一个基于网络语言文本的抽象含义表示（AMR）创建工具——CAMRA（编程语言类型的 AMR 编辑器）。</li>
<li>methods: CAMRA 使用了一种新的方法，将 AMR 注释视为编程语言中的编程，通过利用编程语言的概念，帮助用户更好地理解和使用 AMR 注释。</li>
<li>results: CAMRA 可以快速和准确地生成 AMR 注释，并且可以帮助用户更好地理解和使用 Propbank 角色集。Here’s the breakdown of each point in English:</li>
<li>for: The paper is aimed at developing a tool for creating Abstract Meaning Representations (AMR) from natural language text, called CAMRA (a programming language-like AMR editor).</li>
<li>methods: CAMRA uses a novel approach that treats AMR annotation as coding in programming languages, leveraging the familiarity of programming paradigms to help users better understand and use AMR annotation.</li>
<li>results: CAMRA can quickly and accurately generate AMR annotation, and can also help users better understand and use Propbank role sets.<details>
<summary>Abstract</summary>
In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a cutting-edge web-based tool designed for constructing Abstract Meaning Representation (AMR) from natural language text. CAMRA offers a novel approach to deep lexical semantics annotation such as AMR, treating AMR annotation akin to coding in programming languages. Leveraging the familiarity of programming paradigms, CAMRA encompasses all essential features of existing AMR editors, including example lookup, while going a step further by integrating Propbank roleset lookup as an autocomplete feature within the tool. Notably, CAMRA incorporates AMR parser models as coding co-pilots, greatly enhancing the efficiency and accuracy of AMR annotators. To demonstrate the tool's capabilities, we provide a live demo accessible at: https://camra.colorado.edu
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 CAMRA（抽象含义表示语言编辑器），一款前沿的网页式工具，用于从自然语言文本中构建抽象含义表示（AMR）。CAMRA 采用了一种新的方法来进行深层次语义注释，将 AMR 注释视为编程语言中的编程。利用编程语言的概念互联，CAMRA 包含了所有现有 AMR 编辑器中的重要功能，包括示例查询，同时还添加了基于 Propbank 角色集Lookup 的自动完成功能。另外，CAMRA 还 integrate了 AMR 解析模型，以增强 AMR 注释的效率和准确性。为证明工具的能力，我们提供了一个实时示例，可以在以下链接中访问：https://camra.colorado.edu。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Product-Classification-for-Customs"><a href="#Explainable-Product-Classification-for-Customs" class="headerlink" title="Explainable Product Classification for Customs"></a>Explainable Product Classification for Customs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10922">http://arxiv.org/abs/2311.10922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunji Lee, Sihyeon Kim, Sundong Kim, Soyeon Jung, Heeja Kim, Meeyoung Cha</li>
<li>for: 这研究旨在提供一个可解释的决策支持模型，帮助海关官员分配国际通用商品代码（HS码）。</li>
<li>methods: 该模型使用了机器学习算法，并提供了可解释的理由文档，以帮助海关官员理解和采纳建议。</li>
<li>results: 研究结果表明，该模型对925个困难子类别中的前三个建议准确率为93.9%。用户研究也表明，该算法提供的可解释建议和理由文档可以有效减少海关官员的分类审核时间和劳动。<details>
<summary>Abstract</summary>
The task of assigning internationally accepted commodity codes (aka HS codes) to traded goods is a critical function of customs offices. Like court decisions made by judges, this task follows the doctrine of precedent and can be nontrivial even for experienced officers. Together with the Korea Customs Service (KCS), we propose a first-ever explainable decision supporting model that suggests the most likely subheadings (i.e., the first six digits) of the HS code. The model also provides reasoning for its suggestion in the form of a document that is interpretable by customs officers. We evaluated the model using 5,000 cases that recently received a classification request. The results showed that the top-3 suggestions made by our model had an accuracy of 93.9\% when classifying 925 challenging subheadings. A user study with 32 customs experts further confirmed that our algorithmic suggestions accompanied by explainable reasonings, can substantially reduce the time and effort taken by customs officers for classification reviews.
</details>
<details>
<summary>摘要</summary>
customs offices 负责分配国际接受的商品代码（即HS码）是一项关键的任务。这种任务与法律判决一样，按照前例进行，而且对经验丰富的官员也可能是非常困难的。我们与韩国海关服务（KCS）合作，提出了一种首次出现的可解释决策支持模型，该模型建议商品的可能性最高的六位HS码。此外，模型还提供了其建议的解释，以文档的形式，可以由海关官员理解。我们对5000个最近获得分类请求的案例进行了评估，结果显示，我们模型的top3建议的准确率为93.9%，对925个困难的子标签进行分类。一个用户研究中，32名海关专家确认了我们的算法建议和可解释的理由，可以减少海关官员对分类审核的时间和努力。
</details></li>
</ul>
<hr>
<h2 id="Compact-and-Intuitive-Airfoil-Parameterization-Method-through-Physics-aware-Variational-Autoencoder"><a href="#Compact-and-Intuitive-Airfoil-Parameterization-Method-through-Physics-aware-Variational-Autoencoder" class="headerlink" title="Compact and Intuitive Airfoil Parameterization Method through Physics-aware Variational Autoencoder"></a>Compact and Intuitive Airfoil Parameterization Method through Physics-aware Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10921">http://arxiv.org/abs/2311.10921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Eop Kang, Dawoon Lee, Kwanjung Yee</li>
<li>for:  optimize the design of high-performance aircraft airfoils</li>
<li>methods:  uses physics-aware variational autoencoder to parameterize airfoil shape</li>
<li>results:  produces smooth and non-intersecting airfoils with improved feasibility and intuitiveness<details>
<summary>Abstract</summary>
Airfoil shape optimization plays a critical role in the design of high-performance aircraft. However, the high-dimensional nature of airfoil representation causes the challenging problem known as the "curse of dimensionality". To overcome this problem, numerous airfoil parameterization methods have been developed, which can be broadly classified as polynomial-based and data-driven approaches. Each of these methods has desirable characteristics such as flexibility, parsimony, feasibility, and intuitiveness, but a single approach that encompasses all of these attributes has yet to be found. For example, polynomial-based methods struggle to balance parsimony and flexibility, while data-driven methods lack in feasibility and intuitiveness. In recent years, generative models, such as generative adversarial networks and variational autoencoders, have shown promising potential in airfoil parameterization. However, these models still face challenges related to intuitiveness due to their black-box nature. To address this issue, we developed a novel airfoil parameterization method using physics-aware variational autoencoder. The proposed method not only explicitly separates the generation of thickness and camber distributions to produce smooth and non-intersecting airfoils, thereby improving feasibility, but it also directly aligns its latent dimensions with geometric features of the airfoil, significantly enhancing intuitiveness. Finally, extensive comparative studies were performed to demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
高性能飞机设计中，飞机翼形参数化具有核心作用。然而，飞机翼形表示的维度高度带来“维度咒数”问题，这种问题很难解决。为了缓解这个问题，许多飞机翼形参数化方法已经开发出来，可以分为多项式基于的方法和数据驱动方法。每种方法都具有便利、简洁、可行性和直观性等特点，但是一种方法同时具有所有这些特点还没有被发现。例如，多项式基于的方法很难平衡简洁和灵活性，而数据驱动方法则缺乏可行性和直观性。在最近几年，生成模型，如生成敌方网络和变量自动编码器，在飞机翼形参数化中表现出了潜在的潜力。然而，这些模型仍然面临直观性问题，因为它们的黑盒结构。为了解决这个问题，我们开发了一种新的飞机翼形参数化方法，使用物理意识的变量自动编码器。我们的方法不仅能够明确分离thickness和camber分布的生成，从而提高可行性，而且直接将其缺失的维度与飞机翼形的几何特征直接对应，从而明显提高直观性。最后，我们进行了广泛的比较研究，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-and-Mitigating-Classification-Errors-Through-Interpretable-Token-Patterns"><a href="#Understanding-and-Mitigating-Classification-Errors-Through-Interpretable-Token-Patterns" class="headerlink" title="Understanding and Mitigating Classification Errors Through Interpretable Token Patterns"></a>Understanding and Mitigating Classification Errors Through Interpretable Token Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10920">http://arxiv.org/abs/2311.10920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael A. Hedderich, Jonas Fischer, Dietrich Klakow, Jilles Vreeken</li>
<li>For: 本文旨在Characterizing NLP类型错误，以提供global和可读的描述，以便改进NLP类型错误。* Methods: 本文提出了一种基于最小描述长度原则的方法，可以找到 Correct和错误预测之间的 TokenPatterns。* Results: 实验表明，该方法能够成功地找到ground truth，即使数据集很大且词汇表很大。在VQA和NERcase study中，该方法能够提供明确和行动可能的错误描述。<details>
<summary>Abstract</summary>
State-of-the-art NLP methods achieve human-like performance on many tasks, but make errors nevertheless. Characterizing these errors in easily interpretable terms gives insight into whether a classifier is prone to making systematic errors, but also gives a way to act and improve the classifier. We propose to discover those patterns of tokens that distinguish correct and erroneous predictions as to obtain global and interpretable descriptions for arbitrary NLP classifiers. We formulate the problem of finding a succinct and non-redundant set of such patterns in terms of the Minimum Description Length principle. Through an extensive set of experiments, we show that our method, Premise, performs well in practice. Unlike existing solutions, it recovers ground truth, even on highly imbalanced data over large vocabularies. In VQA and NER case studies, we confirm that it gives clear and actionable insight into the systematic errors made by NLP classifiers.
</details>
<details>
<summary>摘要</summary>
现代NLPT方法可以达到人类水平的性能在许多任务上，但仍会出错。Characterizing这些错误的方式可以提供权威的错误分布，并且可以用来改进分类器。我们提议使用token的 patrerns来 отличи correct和erroneous预测。我们将这问题转化为最小描述长度原理来解决。经过广泛的实验，我们发现我们的方法Premise在实践中表现良好。与现有的解决方案不同，它可以在大词汇和强相关性的数据上恢复真实的描述。在VQA和NER例子中，我们证明了它可以提供清晰和行动可能的NLPT分类器的系统性错误的理解。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/cs.AI_2023_11_18/" data-id="clpxp6bxm007lee88gthtalg5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/cs.CL_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T11:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/cs.CL_2023_11_18/">cs.CL - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Experts-in-the-Loop-Establishing-an-Effective-Workflow-in-Crafting-Privacy-Q-A"><a href="#Experts-in-the-Loop-Establishing-an-Effective-Workflow-in-Crafting-Privacy-Q-A" class="headerlink" title="Experts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q&amp;A"></a>Experts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q&amp;A</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11161">http://arxiv.org/abs/2311.11161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Kolagar, Anna Katharina Leschanowsky, Birgit Popp</li>
<li>for: 保护用户隐私，法律领域世界各地强调数据处理的透明度。</li>
<li>methods: 提议一种动态工作流程，将隐私政策转化为隐私问答对，使隐私政策通过对话AI访问。</li>
<li>results: 促进多学科协作，法律专家和对话设计师之间的合作，同时考虑使用大语言模型的生成能力和相关挑战。<details>
<summary>Abstract</summary>
Privacy policies play a vital role in safeguarding user privacy as legal jurisdictions worldwide emphasize the need for transparent data processing. While the suitability of privacy policies to enhance transparency has been critically discussed, employing conversational AI systems presents unique challenges in informing users effectively. In this position paper, we propose a dynamic workflow for transforming privacy policies into privacy question-and-answer (Q&A) pairs to make privacy policies easily accessible through conversational AI. Thereby, we facilitate interdisciplinary collaboration among legal experts and conversation designers, while also considering the utilization of large language models' generative capabilities and addressing associated challenges. Our proposed workflow underscores continuous improvement and monitoring throughout the construction of privacy Q&As, advocating for comprehensive review and refinement through an experts-in-the-loop approach.
</details>
<details>
<summary>摘要</summary>
《隐私政策在保护用户隐私方面发挥了关键作用，世界各地法律领域都强调数据处理的透明度。虽然透明度是隐私政策的重要方面，但是在使用对话AI系统时，它们带来了一些独特的挑战，用于有效地 Informing 用户。在这篇位点纸中，我们提出了一种动态工作流程，将隐私政策转换成隐私问答对，以便通过对话AI系统访问隐私政策。这样，我们促进了法律专家和对话设计师之间的跨学科合作，同时也考虑了大语言模型的生成能力和相关挑战。我们的提议的工作流程强调了不断改进和监测，在建立隐私问答时，强调了专家征考和反复修改的方式。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Inclusiveness-of-Artificial-Intelligence-Software-in-Enhancing-Project-Management-Efficiency-–-A-Review"><a href="#Evaluating-the-Inclusiveness-of-Artificial-Intelligence-Software-in-Enhancing-Project-Management-Efficiency-–-A-Review" class="headerlink" title="Evaluating the Inclusiveness of Artificial Intelligence Software in Enhancing Project Management Efficiency – A Review"></a>Evaluating the Inclusiveness of Artificial Intelligence Software in Enhancing Project Management Efficiency – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11159">http://arxiv.org/abs/2311.11159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Alevizos, Ilias Georgousis, Akebu Simasiku, Sotiria Karypidou, Antonis Messinis</li>
<li>for: 本研究旨在探讨技术integrated project management中的包容性和效率之间的关系，以及如何通过技术来提高项目成果。</li>
<li>methods: 本研究使用了定义和测量包容性的方法，以及评估技术的可转化潜力。</li>
<li>results: 研究发现，通过技术支持，项目管理中的包容性可以得到 significan improvement，但需要注意技术的可行性和伦理考虑。<details>
<summary>Abstract</summary>
The rise of advanced technology in project management (PM) highlights a crucial need for inclusiveness. This work examines the enhancement of both inclusivity and efficiency in PM through technological integration, focusing on defining and measuring inclusiveness. This approach illuminates how inclusivity-centered technology can significantly elevate project outcomes. The research navigates through the challenges of achieving inclusivity, mainly biases in learning databases and the design process of these technologies, assessment of transformative potential of these technologies, particularly in automating tasks like data collection and analysis, thus enabling managers to prioritize human-centric aspects of projects. However, the integration of such technology transcends efficiency, indicating a paradigm shift in understanding their societal roles. This shift necessitates a new approach in the development of these systems to prevent perpetuating social inequalities. We proposed a methodology involving criteria development for evaluating the inclusiveness and effectiveness of these technologies. This methodical approach is vital to comprehensively address the challenges and limitations inherent in these systems. Emphasizing the importance of inclusivity, the study advocates for a balance between technological advancement and ethical considerations, calling for a holistic understanding and regulation. In conclusion, the paper underscores that while these technologies can significantly improve outcomes, their mindful integration, ensuring inclusivity, is paramount. This exploration into the ethical and practical aspects of technology in PM contributes to a more informed and balanced approach within the field.
</details>
<details>
<summary>摘要</summary>
高技术的普及在项目管理（PM）领域 highlights 一个关键的需求：包容性。这项研究探讨了通过技术集成提高包容性和效率在 PM 中的可能性，关注定义和测量包容性。这种方法揭示了包容性 centered 技术在项目成果上可以产生显著提高。研究探讨了实现包容性的挑战，主要是学习数据库和设计过程中的偏见，以及这些技术的变革潜力，特别是自动化数据采集和分析任务，以便管理人员能够更好地注重项目人文方面。然而，技术的集成不仅提高效率，还表示一种社会角色转变。这种转变需要一种新的开发系统的方法，以避免社会不平等的持续。我们提出了一种包括对包容性和效果的评价 criterion 的方法ологи。这种系统atic 方法是必要的，以全面 Address 这些系统的挑战和限制。强调包容性的重要性，这篇论文呼吁对技术进步和伦理考虑进行平衡，寻求一种整体的理解和规范。 study 的结论是，虽然这些技术可以提高成果，但是注意其包容性的集成是关键。这种探讨技术和伦理的平衡在 PM 领域中做出了更加 Informed 和平衡的方法论。
</details></li>
</ul>
<hr>
<h2 id="Vashantor-A-Large-scale-Multilingual-Benchmark-Dataset-for-Automated-Translation-of-Bangla-Regional-Dialects-to-Bangla-Language"><a href="#Vashantor-A-Large-scale-Multilingual-Benchmark-Dataset-for-Automated-Translation-of-Bangla-Regional-Dialects-to-Bangla-Language" class="headerlink" title="Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated Translation of Bangla Regional Dialects to Bangla Language"></a>Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated Translation of Bangla Regional Dialects to Bangla Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11142">http://arxiv.org/abs/2311.11142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad</li>
<li>for: 这个研究的目的是填充在过去的研究中缺失的班加礼语言方言到标准班加礼语言的翻译。</li>
<li>methods: 该研究使用了mT5和BanglaT5模型来翻译地方语言到标准班加礼语言，以及mBERT和Bangla-bert-base来准确地探测地方语言的来源地区。</li>
<li>results: 实验结果显示，мы门司нг班加礼语言方言的 Bleu 分数为69.06，而奇图加礼语言方言的 Bleu 分数为36.75。我们还发现，мы门司нг班加礼语言方言的平均单词错误率为0.1548，而奇图加礼语言方言的平均单词错误率为0.3385。Region detection的准确率为85.86%和84.36%。这是首次对班加礼语言方言到班加礼语言机器翻译进行大规模的调查。<details>
<summary>Abstract</summary>
The Bangla linguistic variety is a fascinating mix of regional dialects that adds to the cultural diversity of the Bangla-speaking community. Despite extensive study into translating Bangla to English, English to Bangla, and Banglish to Bangla in the past, there has been a noticeable gap in translating Bangla regional dialects into standard Bangla. In this study, we set out to fill this gap by creating a collection of 32,500 sentences, encompassing Bangla, Banglish, and English, representing five regional Bangla dialects. Our aim is to translate these regional dialects into standard Bangla and detect regions accurately. To achieve this, we proposed models known as mT5 and BanglaT5 for translating regional dialects into standard Bangla. Additionally, we employed mBERT and Bangla-bert-base to determine the specific regions from where these dialects originated. Our experimental results showed the highest BLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score of 36.75 for Chittagong regional dialects. We also observed the lowest average word error rate of 0.1548 for Mymensingh regional dialects and the highest of 0.3385 for Chittagong regional dialects. For region detection, we achieved an accuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first large-scale investigation of Bangla regional dialects to Bangla machine translation. We believe our findings will not only pave the way for future work on Bangla regional dialects to Bangla machine translation, but will also be useful in solving similar language-related challenges in low-resource language conditions.
</details>
<details>
<summary>摘要</summary>
“孟加拉语言变体是一种非常有趣的地域 диалект混合，增加了孟加拉语言社区的文化多样性。尽管过去对孟加拉语言到英语、英语到孟加拉语言和孟加拉语言到英语的翻译已经进行了广泛的研究，但是还没有尝试翻译孟加拉语言地域 диалект到标准孟加拉语言。在这项研究中，我们决定填补这一空白，创建了32500句孟加拉语言、孟加拉语言混合和英语 sentences，代表五个孟加拉语言地域 диалект。我们的目标是将这些地域 диаLECTS翻译成标准孟加拉语言，并准确地检测这些地域的来源。为此，我们提出了名为mT5和BanglaT5的模型，以及mBERT和Bangla-bert-base来确定这些地域的来源。我们的实验结果显示，最高的BLEU分数为69.06，来自米亚尼斯希的地域 диаLECTS，最低的BLEU分数为36.75，来自切图格的地域 диаLECTS。我们还观察到，米亚尼斯希的地域 диаLECTS的平均单词错误率最低，为0.1548，而切图格的地域 диаLECTS的平均单词错误率最高，为0.3385。对地域检测，我们获得了85.86%的准确率，使用Bangla-bert-base，和84.36%的准确率，使用mBERT。这是对孟加拉语言地域 диаLECTS到孟加拉语言机器翻译的首次大规模调查。我们认为，我们的发现将不 только开阔未来对孟加拉语言地域 диаLECTS到孟加拉语言机器翻译的道路，还将在低资源语言条件下解决类似语言相关的挑战。”
</details></li>
</ul>
<hr>
<h2 id="Why-Is-My-Prompt-Getting-Worse-Rethinking-Regression-Testing-for-Evolving-LLM-APIs"><a href="#Why-Is-My-Prompt-Getting-Worse-Rethinking-Regression-Testing-for-Evolving-LLM-APIs" class="headerlink" title="(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs"></a>(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11123">http://arxiv.org/abs/2311.11123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanqin Ma, Chenyang Yang, Christian Kästner</li>
<li>for: 本研究旨在探讨如何对不断发展的语言模型API进行 regression testing，以适应应用程序开发人员在使用LMMs时遇到的问题。</li>
<li>methods: 本研究采用了一种CASE研究方法，通过实践案例研究了LLM APIs的更新和 deprecation 对应用程序的影响。</li>
<li>results: 研究发现， traditional testingapproaches 不能满足 regression testing LLMs，因为LLM APIs的不同correctness notions、prompt brittleness和non-determinism。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regression testing for evolving LLM APIs. We argue that regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Responsible-AI-Considerations-in-Text-Summarization-Research-A-Review-of-Current-Practices"><a href="#Responsible-AI-Considerations-in-Text-Summarization-Research-A-Review-of-Current-Practices" class="headerlink" title="Responsible AI Considerations in Text Summarization Research: A Review of Current Practices"></a>Responsible AI Considerations in Text Summarization Research: A Review of Current Practices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11103">http://arxiv.org/abs/2311.11103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Lu Liu, Meng Cao, Su Lin Blodgett, Jackie Chi Kit Cheung, Alexandra Olteanu, Adam Trischler</li>
<li>for: 本研究旨在探讨文本摘要 tasks 中可能存在的责任AI问题，包括可能的风险和影响，以及研究者是否考虑到可能的利益相互作用。</li>
<li>methods: 本研究使用质性分析方法对333篇ACL Anthology中的文本摘要研究进行多轮分析，以探讨研究和报道实践中的责任AI问题。</li>
<li>results: 研究发现大多数论文不考虑可能的利益相互作用和下游影响，导致研究的可能性和可靠性受到限制。基于这些结果，我们提出了实践和研究方向，以促进责任AI在文本摘要领域的发展。<details>
<summary>Abstract</summary>
AI and NLP publication venues have increasingly encouraged researchers to reflect on possible ethical considerations, adverse impacts, and other responsible AI issues their work might engender. However, for specific NLP tasks our understanding of how prevalent such issues are, or when and why these issues are likely to arise, remains limited. Focusing on text summarization -- a common NLP task largely overlooked by the responsible AI community -- we examine research and reporting practices in the current literature. We conduct a multi-round qualitative analysis of 333 summarization papers from the ACL Anthology published between 2020-2022. We focus on how, which, and when responsible AI issues are covered, which relevant stakeholders are considered, and mismatches between stated and realized research goals. We also discuss current evaluation practices and consider how authors discuss the limitations of both prior work and their own work. Overall, we find that relatively few papers engage with possible stakeholders or contexts of use, which limits their consideration of potential downstream adverse impacts or other responsible AI issues. Based on our findings, we make recommendations on concrete practices and research directions.
</details>
<details>
<summary>摘要</summary>
AI和自然语言处理（NLP）发表文章的场景越来越多地鼓励研究人员考虑可能的伦理考虑因素、不良影响和负责任AI问题他们的工作可能会带来。然而，对于特定的NLP任务，我们对这些问题的流行程度、出现时间和发生原因仍然很限制。关注文本概要---一种常见的NLP任务，负责AI社区很少关注的领域---我们对当前文献的研究和报道做了多 round的质量分析。我们关注怎样、何时和为什么负责AI问题被考虑，哪些关键参与者被考虑，以及文献目标与实际实施的差异。我们还讨论当前的评价方法，并考虑作者如何评估先前工作和自己的研究的局限性。总之，我们发现大多数文章没有考虑可能的利益相关者或使用场景，这限制了他们对可能的下游不良影响或其他负责任AI问题的考虑。根据我们的发现，我们提出了具体的实践和研究方向。
</details></li>
</ul>
<hr>
<h2 id="Bit-Cipher-–-A-Simple-yet-Powerful-Word-Representation-System-that-Integrates-Efficiently-with-Language-Models"><a href="#Bit-Cipher-–-A-Simple-yet-Powerful-Word-Representation-System-that-Integrates-Efficiently-with-Language-Models" class="headerlink" title="Bit Cipher – A Simple yet Powerful Word Representation System that Integrates Efficiently with Language Models"></a>Bit Cipher – A Simple yet Powerful Word Representation System that Integrates Efficiently with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11012">http://arxiv.org/abs/2311.11012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Zhao, Jake Ryland Williams</li>
<li>for: 本研究旨在提出一种新的词表示系统，以提高词向量的计算效率和解释力。</li>
<li>methods: 本研究使用了一种新的Bit-cipher算法来训练词向量，该算法不需要反propagation，并且可以利用文本上的上下文信息和紧凑的维度减少技术来提供强大的解释性。</li>
<li>results: 研究表明，使用Bit-cipher算法可以快速训练高效的词向量，并且可以在不同的文本任务中达到竞争性的表现。此外，研究还发现了在折衔批处理和练习中使用cipher embedding层可以加速模型的训练过程，并且可以提高模型的优化性。<details>
<summary>Abstract</summary>
While Large Language Models (LLMs) become ever more dominant, classic pre-trained word embeddings sustain their relevance through computational efficiency and nuanced linguistic interpretation. Drawing from recent studies demonstrating that the convergence of GloVe and word2vec optimizations all tend towards log-co-occurrence matrix variants, we construct a novel word representation system called Bit-cipher that eliminates the need of backpropagation while leveraging contextual information and hyper-efficient dimensionality reduction techniques based on unigram frequency, providing strong interpretability, alongside efficiency. We use the bit-cipher algorithm to train word vectors via a two-step process that critically relies on a hyperparameter -- bits -- that controls the vector dimension. While the first step trains the bit-cipher, the second utilizes it under two different aggregation modes -- summation or concatenation -- to produce contextually rich representations from word co-occurrences. We extend our investigation into bit-cipher's efficacy, performing probing experiments on part-of-speech (POS) tagging and named entity recognition (NER) to assess its competitiveness with classic embeddings like word2vec and GloVe. Additionally, we explore its applicability in LM training and fine-tuning. By replacing embedding layers with cipher embeddings, our experiments illustrate the notable efficiency of cipher in accelerating the training process and attaining better optima compared to conventional training paradigms. Experiments on the integration of bit-cipher embedding layers with Roberta, T5, and OPT, prior to or as a substitute for fine-tuning, showcase a promising enhancement to transfer learning, allowing rapid model convergence while preserving competitive performance.
</details>
<details>
<summary>摘要</summary>
While Large Language Models (LLMs) become ever more dominant, classic pre-trained word embeddings sustain their relevance through computational efficiency and nuanced linguistic interpretation. Drawing from recent studies demonstrating that the convergence of GloVe and word2vec optimizations all tend towards log-co-occurrence matrix variants, we construct a novel word representation system called Bit-cipher that eliminates the need of backpropagation while leveraging contextual information and hyper-efficient dimensionality reduction techniques based on unigram frequency, providing strong interpretability, alongside efficiency. We use the bit-cipher algorithm to train word vectors via a two-step process that critically relies on a hyperparameter -- bits -- that controls the vector dimension. While the first step trains the bit-cipher, the second utilizes it under two different aggregation modes -- summation or concatenation -- to produce contextually rich representations from word co-occurrences. We extend our investigation into bit-cipher's efficacy, performing probing experiments on part-of-speech (POS) tagging and named entity recognition (NER) to assess its competitiveness with classic embeddings like word2vec and GloVe. Additionally, we explore its applicability in LM training and fine-tuning. By replacing embedding layers with cipher embeddings, our experiments illustrate the notable efficiency of cipher in accelerating the training process and attaining better optima compared to conventional training paradigms. Experiments on the integration of bit-cipher embedding layers with Roberta, T5, and OPT, prior to or as a substitute for fine-tuning, showcase a promising enhancement to transfer learning, allowing rapid model convergence while preserving competitive performance.
</details></li>
</ul>
<hr>
<h2 id="Joyful-Joint-Modality-Fusion-and-Graph-Contrastive-Learning-for-Multimodal-Emotion-Recognition"><a href="#Joyful-Joint-Modality-Fusion-and-Graph-Contrastive-Learning-for-Multimodal-Emotion-Recognition" class="headerlink" title="Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition"></a>Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11009">http://arxiv.org/abs/2311.11009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyuan Li, Yusong Wang, Kotaro Funakoshi, Manabu Okumura</li>
<li>for: 本文targets at multimodal emotion recognition, aiming to recognize emotions for each utterance of multiple modalities, and has important applications in human-machine interaction.</li>
<li>methods: 本文提出了一种joint modality fusion and graph contrastive learning方法，即Joyful方法，其中multimodality fusion、contrastive learning和情感认知 jointly optimized. specifically, a new multimodal fusion mechanism is designed to provide deep interaction and fusion between global contextual and uni-modal specific features. Additionally, a graph contrastive learning framework with inter-view and intra-view contrastive losses is introduced to learn more distinguishable representations for samples with different sentiments.</li>
<li>results: 根据三个benchmark datasets的实验结果，Joyful方法 achieved state-of-the-art (SOTA) performance compared to all baselines.<details>
<summary>Abstract</summary>
Multimodal emotion recognition aims to recognize emotions for each utterance of multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse uni-modal features in a dialogue. Furthermore, with the number of graph layers increasing, they easily fall into over-smoothing. In this paper, we propose a method for joint modality fusion and graph contrastive learning for multimodal emotion recognition (Joyful), where multimodality fusion, contrastive learning, and emotion recognition are jointly optimized. Specifically, we first design a new multimodal fusion mechanism that can provide deep interaction and fusion between the global contextual and uni-modal specific features. Then, we introduce a graph contrastive learning framework with inter-view and intra-view contrastive losses to learn more distinguishable representations for samples with different sentiments. Extensive experiments on three benchmark datasets indicate that Joyful achieved state-of-the-art (SOTA) performance compared to all baselines.
</details>
<details>
<summary>摘要</summary>
多模态情感识别目标是识别对话中每个语音的多种情感，受到人机交互应用的关注。现有的图structured方法无法同时展示对话中全局上下文特征和多模态特征的地方特征。此外，随着图层数量的增加，它们容易陷入过度熔合。本文提出了一种对多模态情感识别进行联合多modalité拟合和图相对学习的方法（Joyful），其中多模态拟合、对比学习和情感识别被联合优化。 Specifically，我们首先设计了一种新的多模态拟合机制，可以提供深入的交互和拟合全局上下文特征和单模态特征。然后，我们引入了一个图相对学习框架，包括对视和自视对比损失来学习更加 distinguishable的表示。经验表明，Joyful在三个基准数据集上达到了所有基准的最佳性能（SOTA）。
</details></li>
</ul>
<hr>
<h2 id="Gendec-A-Machine-Learning-based-Framework-for-Gender-Detection-from-Japanese-Names"><a href="#Gendec-A-Machine-Learning-based-Framework-for-Gender-Detection-from-Japanese-Names" class="headerlink" title="Gendec: A Machine Learning-based Framework for Gender Detection from Japanese Names"></a>Gendec: A Machine Learning-based Framework for Gender Detection from Japanese Names</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11001">http://arxiv.org/abs/2311.11001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duong Tien Pham, Luan Thanh Nguyen</li>
<li>for: 这个研究旨在创建一个日本名字性别检测数据集，以及一个基于多种方法的名字性别检测框架，以便更好地理解日本人名中的性别信息。</li>
<li>methods: 该研究使用了多种方法，包括传统的机器学习技术和最新的传输学习模型，以检测日本名字中的性别信息。</li>
<li>results: 研究预计可以准确地预测日本名字中的性别信息，并且可以应用于多个领域。<details>
<summary>Abstract</summary>
Every human has their own name, a fundamental aspect of their identity and cultural heritage. The name often conveys a wealth of information, including details about an individual's background, ethnicity, and, especially, their gender. By detecting gender through the analysis of names, researchers can unlock valuable insights into linguistic patterns and cultural norms, which can be applied to practical applications. Hence, this work presents a novel dataset for Japanese name gender detection comprising 64,139 full names in romaji, hiragana, and kanji forms, along with their biological genders. Moreover, we propose Gendec, a framework for gender detection from Japanese names that leverages diverse approaches, including traditional machine learning techniques or cutting-edge transfer learning models, to predict the gender associated with Japanese names accurately. Through a thorough investigation, the proposed framework is expected to be effective and serve potential applications in various domains.
</details>
<details>
<summary>摘要</summary>
每个人有自己的名字，是他们的身份和文化遗产的基本组成部分。名字通常包含了许多信息，如个人背景、民族和特别是性别。通过对名字进行分析，研究人员可以获得价值的信息，包括语言模式和文化规范，这些信息可以应用于实际应用。因此，本文提出了一个新的日本名字性别检测集合，包括64,139个全名（拼音、平仮名和汉字形式），以及他们的生物性别。此外，我们提出了一个基于多种方法的日本名字性别检测框架，包括传统机器学习技术和前沿技术的转移学习模型，以准确预测日本名字中的性别。经过仔细调查，我们预期该框架能够实现效果，并可以在多个领域应用。
</details></li>
</ul>
<hr>
<h2 id="Behavior-Optimized-Image-Generation"><a href="#Behavior-Optimized-Image-Generation" class="headerlink" title="Behavior Optimized Image Generation"></a>Behavior Optimized Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10995">http://arxiv.org/abs/2311.10995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Varun Khurana, Yaman K Singla, Jayakumar Subramanian, Rajiv Ratn Shah, Changyou Chen, Zhiqiang Xu, Balaji Krishnamurthy</li>
<li>for: 这篇论文的目的是如何在图像生成过程中嵌入目标行为的知识，以创造不仅更加美观的图像，还能够提高图像的表现力。</li>
<li>methods: 这篇论文提出了一种名为BoigLLM的语言模型，该模型能够理解图像内容和用户行为。BoigLLM知道图像需要如何看起来，以达到某些需要的KPI。此外， authors还提出了一种扩散型模型（BoigSD），用于对BoigLLM定义的奖励进行调整。</li>
<li>results: 作者们表明，BoigLLM可以比13倍大的GPT-3.5和GPT-4模型在这个任务中表现更好，表明这些当前状态的模型可以理解图像，但缺乏实际世界中图像的信息。此外， authors还发现，通过使用BoigBench数据集，可以对图像生成和理解进行更多的研究。<details>
<summary>Abstract</summary>
The last few years have witnessed great success on image generation, which has crossed the acceptance thresholds of aesthetics, making it directly applicable to personal and commercial applications. However, images, especially in marketing and advertising applications, are often created as a means to an end as opposed to just aesthetic concerns. The goal can be increasing sales, getting more clicks, likes, or image sales (in the case of stock businesses). Therefore, the generated images need to perform well on these key performance indicators (KPIs), in addition to being aesthetically good. In this paper, we make the first endeavor to answer the question of "How can one infuse the knowledge of the end-goal within the image generation process itself to create not just better-looking images but also "better-performing'' images?''. We propose BoigLLM, an LLM that understands both image content and user behavior. BoigLLM knows how an image should look to get a certain required KPI. We show that BoigLLM outperforms 13x larger models such as GPT-3.5 and GPT-4 in this task, demonstrating that while these state-of-the-art models can understand images, they lack information on how these images perform in the real world. To generate actual pixels of behavior-conditioned images, we train a diffusion-based model (BoigSD) to align with a proposed BoigLLM-defined reward. We show the performance of the overall pipeline on two datasets covering two different behaviors: a stock dataset with the number of forward actions as the KPI and a dataset containing tweets with the total likes as the KPI, denoted as BoigBench. To advance research in the direction of utility-driven image generation and understanding, we release BoigBench, a benchmark dataset containing 168 million enterprise tweets with their media, brand account names, time of post, and total likes.
</details>
<details>
<summary>摘要</summary>
过去几年，图像生成领域取得了巨大的成功，已经突破了艺术性的接受reshold，使其直接适用于个人和商业应用。然而，在市场营销和广告应用中，图像 oftentimes 是为了达到一个目标而创建的，而不是仅仅是艺术意义。因此，生成的图像需要在关键性能指标（KPI）上表现良好，除了外观好。在这篇论文中，我们做出了第一次的尝试，回答“如何在图像生成过程中植入目标的知识，以创造不仅更好看的图像，而且“更好表现”的图像？”我们提出了 BoigLLM，一个理解图像内容和用户行为的 LLM。BoigLLM 知道一个图像需要如何看起来，以达到某个需要的 KPI。我们表明，BoigLLM 在这项任务上超过 13 倍大的模型，如 GPT-3.5 和 GPT-4，表明这些当前顶尖模型可以理解图像，但缺乏实际世界中图像的信息。为生成 Conditioned 的实际像素，我们训练了一个扩散基于模型（BoigSD），以与 BoigLLM 定义的奖励相对。我们展示了整个管道在两个 dataset 上的表现，其中一个是一个股票 dataset，其中的 KPI 是前进动作的数量，另一个是一个包含 tweet 的 dataset，其中的 KPI 是总喜欢数。为了推动研究在Utility-driven 图像生成和理解方面，我们发布了 BoigBench，一个包含 168 万个企业微博，它们的媒体、 bran account 名称、发布时间和总喜欢数。
</details></li>
</ul>
<hr>
<h2 id="Journey-of-Hallucination-minimized-Generative-AI-Solutions-for-Financial-Decision-Makers"><a href="#Journey-of-Hallucination-minimized-Generative-AI-Solutions-for-Financial-Decision-Makers" class="headerlink" title="Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers"></a>Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10961">http://arxiv.org/abs/2311.10961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sohini Roychowdhury</li>
<li>for: 这项研究的目的是设计降低幻觉的大语言模型（LLM）解决方案，以便在金融领域的决策者使用。</li>
<li>methods: 本研究使用了三个主要阶段：评估、缩放和LLM进化，以确保 chatbot 、自动报告和警示的可靠性和质量。</li>
<li>results: 研究表明，通过使用人类反馈来控制幻觉，可以提高 chatbot 的可靠性和质量。同时，通过使用数据 Ansatz 技术，可以生成高质量的自动报告和警示。<details>
<summary>Abstract</summary>
Generative AI has significantly reduced the entry barrier to the domain of AI owing to the ease of use and core capabilities of automation, translation, and intelligent actions in our day to day lives. Currently, Large language models (LLMs) that power such chatbots are being utilized primarily for their automation capabilities for software monitoring, report generation etc. and for specific personalized question answering capabilities, on a limited scope and scale. One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual. Hallucinations are primarily caused by biased training data, ambiguous prompts and inaccurate LLM parameters, and they majorly occur while combining mathematical facts with language-based context. Thus, monitoring and controlling for hallucinations becomes necessary when designing solutions that are meant for decision makers. In this work we present the three major stages in the journey of designing hallucination-minimized LLM-based solutions that are specialized for the decision makers of the financial domain, namely: prototyping, scaling and LLM evolution using human feedback. These three stages and the novel data to answer generation modules presented in this work are necessary to ensure that the Generative AI chatbots, autonomous reports and alerts are reliable and high-quality to aid key decision-making processes.
</details>
<details>
<summary>摘要</summary>
生成AI已经大幅降低了AI领域的入口难度，因为它们提供了易于使用的核心功能，包括自动化、翻译和智能行为，在我们日常生活中。目前，大型语言模型（LLM）正在主要用于其自动化能力，例如软件监控、报告生成等，以及特定的个性化问答能力，但范围和规模受限。现有的LLM家族存在一个主要的问题，即“幻见”（hallucinations），即因训练数据偏见、模糊的提示和不准确的LLM参数而导致的不准确回答。因此，在设计针对决策者的LLM基于解决方案时，监控和控制幻见变得必要。在这项工作中，我们介绍了针对决策者的LLM基于解决方案设计的三个主要阶段：原型、扩大和LLM进化，以及在这三个阶段中使用人类反馈来确保生成AI聊天机器人、自动生成报告和警示高质量和可靠，以帮助决策过程。
</details></li>
</ul>
<hr>
<h2 id="Deception-Detection-from-Linguistic-and-Physiological-Data-Streams-Using-Bimodal-Convolutional-Neural-Networks"><a href="#Deception-Detection-from-Linguistic-and-Physiological-Data-Streams-Using-Bimodal-Convolutional-Neural-Networks" class="headerlink" title="Deception Detection from Linguistic and Physiological Data Streams Using Bimodal Convolutional Neural Networks"></a>Deception Detection from Linguistic and Physiological Data Streams Using Bimodal Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10944">http://arxiv.org/abs/2311.10944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panfeng Li, Mohamed Abouelenien, Rada Mihalcea</li>
<li>for: 这篇论文探讨了用 convolutional neural networks 进行多模态骗学检测的应用。</li>
<li>methods: 作者使用了一个由104名参与者回答两个话题的数据集，从这些数据中提取了语言和生理学特征，以建立和训练神经网络模型。</li>
<li>results: 作者提出了一种将多 modalities 融合的卷积神经网络模型，并证明了这种方法在多模态骗学检测中的优势。此外，作者还比较了这种方法与之前的多模态骗学检测方法，发现其在有限数据情况下也能够达到更高的检测精度。<details>
<summary>Abstract</summary>
Deception detection is gaining increasing interest due to ethical and security concerns. This paper explores the application of convolutional neural networks for the purpose of multimodal deception detection. We use a dataset built by interviewing 104 subjects about two topics, with one truthful and one falsified response from each subject about each topic. In particular, we make three main contributions. First, we extract linguistic and physiological features from this data to train and construct the neural network models. Second, we propose a fused convolutional neural network model using both modalities in order to achieve an improved overall performance. Third, we compare our new approach with earlier methods designed for multimodal deception detection. We find that our system outperforms regular classification methods; our results indicate the feasibility of using neural networks for deception detection even in the presence of limited amounts of data.
</details>
<details>
<summary>摘要</summary>
骗子检测在当前有越来越多的关注，这主要归功于伦理和安全问题。本文探讨了使用卷积神经网络进行多Modal骗子检测的应用。我们使用了104名参与者对两个话题进行了面试，每个话题有一个真实的和一个假的回答。特别是，我们从这些数据中提取了语言和生物学特征，用于训练和构建神经网络模型。我们的主要贡献如下：1. 我们提出了一种基于多Modal的卷积神经网络模型，以实现更好的总体性能。2. 我们对之前的多Modal骗子检测方法进行了比较，并发现我们的系统在数据量有限的情况下表现更好。我们的研究表明，使用卷积神经网络进行多Modal骗子检测是可行的，即使是在数据量有限的情况下。
</details></li>
</ul>
<hr>
<h2 id="Partially-Randomizing-Transformer-Weights-for-Dialogue-Response-Diversity"><a href="#Partially-Randomizing-Transformer-Weights-for-Dialogue-Response-Diversity" class="headerlink" title="Partially Randomizing Transformer Weights for Dialogue Response Diversity"></a>Partially Randomizing Transformer Weights for Dialogue Response Diversity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10943">http://arxiv.org/abs/2311.10943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Yang Lee, Kong Aik Lee, Woon-Seng Gan</li>
<li>for: 提高开放领域对话的响应多样性</li>
<li>methods: 使用固定layer的权重初始化和随机 initialize</li>
<li>results: 与先前的方法相比，PaRaFormer的性能相似，不增加训练或推理难度，也不增加模型的复杂度<details>
<summary>Abstract</summary>
Despite recent progress in generative open-domain dialogue, the issue of low response diversity persists. Prior works have addressed this issue via either novel objective functions, alternative learning approaches such as variational frameworks, or architectural extensions such as the Randomized Link (RL) Transformer. However, these approaches typically entail either additional difficulties during training/inference, or a significant increase in model size and complexity. Hence, we propose the \underline{Pa}rtially \underline{Ra}ndomized trans\underline{Former} (PaRaFormer), a simple extension of the transformer which involves freezing the weights of selected layers after random initialization. Experimental results reveal that the performance of the PaRaformer is comparable to that of the aforementioned approaches, despite not entailing any additional training difficulty or increase in model complexity.
</details>
<details>
<summary>摘要</summary>
尽管最近的开放领域对话生成技术已经做出了 significi cant 进步，但问题仍然存在低响应多样性。先前的方法通常通过新的目标函数、不同的学习方法如变量框架，或者架构扩展如随机链接（RL）转换器来解决这个问题。然而，这些方法通常会在训练/推理过程中增加额外难度，或者模型的大小和复杂性会增加。因此，我们提出了PaRaFormer，一种简单的 transformer 扩展，它通过随机初始化选择层的 weights 并冻结它们来解决低响应多样性问题。实验结果表明，PaRaFormer 的性能与先前的方法相当，而无需额外的训练困难或模型的复杂度增加。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/cs.CL_2023_11_18/" data-id="clpxp6c0000fdee88eewq0vee" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/cs.LG_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T10:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/cs.LG_2023_11_18/">cs.LG - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dueling-Optimization-with-a-Monotone-Adversary"><a href="#Dueling-Optimization-with-a-Monotone-Adversary" class="headerlink" title="Dueling Optimization with a Monotone Adversary"></a>Dueling Optimization with a Monotone Adversary</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11185">http://arxiv.org/abs/2311.11185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avrim Blum, Meghal Gupta, Gene Li, Naren Sarayu Manoj, Aadirupa Saha, Yuanyuan Yang</li>
<li>for: 这篇论文研究了一种叫做对抗优化的问题，即在一个含有反对抗的环境中，设计一个在线算法，以找到一个最小值 $\mathbf{x}^{*}$，其中 $f\colon X \to \mathbb{R}$ 是一个函数， $X \subseteq \mathbb{R}^d$ 是一个空间。</li>
<li>methods: 该论文使用了一种随机化算法，可以在很多种自然的函数 $f$ 和空间 $X$ 上实现。</li>
<li>results: 该论文的主要结果是，该算法可以在 $O(d)$ 的成本和 $O(d\log(1&#x2F;\varepsilon)^2)$ 的迭代复杂度下找到一个 $\varepsilon$-优点。此外，该算法的对 $d$ 的依赖性是可证明为是最佳的，即任何随机算法都必须在 $d$ 上付出 $\Omega(d)$ 的成本和迭代复杂度。<details>
<summary>Abstract</summary>
We introduce and study the problem of dueling optimization with a monotone adversary, which is a generalization of (noiseless) dueling convex optimization. The goal is to design an online algorithm to find a minimizer $\mathbf{x}^{*}$ for a function $f\colon X \to \mathbb{R}$, where $X \subseteq \mathbb{R}^d$. In each round, the algorithm submits a pair of guesses, i.e., $\mathbf{x}^{(1)}$ and $\mathbf{x}^{(2)}$, and the adversary responds with any point in the space that is at least as good as both guesses. The cost of each query is the suboptimality of the worse of the two guesses; i.e., ${\max} \left( f(\mathbf{x}^{(1)}), f(\mathbf{x}^{(2)}) \right) - f(\mathbf{x}^{*})$. The goal is to minimize the number of iterations required to find an $\varepsilon$-optimal point and to minimize the total cost (regret) of the guesses over many rounds. Our main result is an efficient randomized algorithm for several natural choices of the function $f$ and set $X$ that incurs cost $O(d)$ and iteration complexity $O(d\log(1/\varepsilon)^2)$. Moreover, our dependence on $d$ is asymptotically optimal, as we show examples in which any randomized algorithm for this problem must incur $\Omega(d)$ cost and iteration complexity.
</details>
<details>
<summary>摘要</summary>
我们介绍和研究对对抗式优化问题，即一个对抗者对函数 $f\colon X \to \mathbb{R}$ 的一个通用扩展。我们的目标是设计一个在线 Algorithm 以找到 $X \subseteq \mathbb{R}^d$ 中的最小值 $\mathbf{x}^{*}$。在每个回合中，Algorithm 会提交两个猜测，即 $\mathbf{x}^{(1)}$ 和 $\mathbf{x}^{(2)}$，对抗者则回传任何在空间中的任何点，其至少对两个猜测都是最佳的。每次猜测的成本是两个猜测中的较差一个的成本，即 $\max \left( f(\mathbf{x}^{(1)}), f(\mathbf{x}^{(2)}) \right) - f(\mathbf{x}^{*})$。我们的主要结果是一个高效的随机化算法，可以在多个自然的函数 $f$ 和集合 $X$ 上实现cost $O(d)$ 和迭代复杂度 $O(d\log(1/\varepsilon)^2)$。此外，我们的 $d$ 依赖性是对抗数学optimal，我们提供了一些示例，证明任何随机化算法 для这个问题必须有 $\Omega(d)$ 成本和迭代复杂度。
</details></li>
</ul>
<hr>
<h2 id="Exponentially-Convergent-Algorithms-for-Supervised-Matrix-Factorization"><a href="#Exponentially-Convergent-Algorithms-for-Supervised-Matrix-Factorization" class="headerlink" title="Exponentially Convergent Algorithms for Supervised Matrix Factorization"></a>Exponentially Convergent Algorithms for Supervised Matrix Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11182">http://arxiv.org/abs/2311.11182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ljw9510/smf">https://github.com/ljw9510/smf</a></li>
<li>paper_authors: Joowon Lee, Hanbaek Lyu, Weixin Yao</li>
<li>for: 使用Supervised Matrix Factorization（SMF）同时实现特征提取和分类任务，解决高维数据中的挑战。</li>
<li>methods: 提出了一种新的框架，将SMF视为低维矩阵估计问题，并提出了一种高效的算法，可在初始化的假设下，在轻量级的假设下提供快速拟合全局最小化 objective 的 garantia。</li>
<li>results: 通过应用于多种SMF类型问题，成功地鉴别了许多不同类型的肿瘤相关基因组。<details>
<summary>Abstract</summary>
Supervised matrix factorization (SMF) is a classical machine learning method that simultaneously seeks feature extraction and classification tasks, which are not necessarily a priori aligned objectives. Our goal is to use SMF to learn low-rank latent factors that offer interpretable, data-reconstructive, and class-discriminative features, addressing challenges posed by high-dimensional data. Training SMF model involves solving a nonconvex and possibly constrained optimization with at least three blocks of parameters. Known algorithms are either heuristic or provide weak convergence guarantees for special cases. In this paper, we provide a novel framework that 'lifts' SMF as a low-rank matrix estimation problem in a combined factor space and propose an efficient algorithm that provably converges exponentially fast to a global minimizer of the objective with arbitrary initialization under mild assumptions. Our framework applies to a wide range of SMF-type problems for multi-class classification with auxiliary features. To showcase an application, we demonstrate that our algorithm successfully identified well-known cancer-associated gene groups for various cancers.
</details>
<details>
<summary>摘要</summary>
“超级vised矩阵因子（SMF）是一种经典的机器学习方法，同时寻求特征提取和分类任务，这两个任务可能并不是先验 aligned 目标。我们想使用 SMF 学习低级别的秘密因子，以提供可解释、数据重建和类别分类的特征，解决高维数据带来的挑战。SMF 模型的训练过程 involve 解决非拟合和可能受限制的优化问题，其中至少有三个块的参数。知名的算法是 Either HEURISTIC 或提供弱 convergence 保证的特殊情况。在这篇论文中，我们提出了一种新的框架，将 SMF 视为一种低级别矩阵估计问题，并提出了一种高效的算法，可在任意初始化下，在轻微假设下提供可证明的对象目标的极限值 global minimizer ，并且在数据中心化的情况下提供了一个稳定的初始化方法。我们的框架适用于多类分类问题中的 SMF-type 问题，并且在不同的肿瘤类型中成功地识别了许多知名的肿瘤相关基因组。”
</details></li>
</ul>
<hr>
<h2 id="Nonsmooth-Projection-Free-Optimization-with-Functional-Constraints"><a href="#Nonsmooth-Projection-Free-Optimization-with-Functional-Constraints" class="headerlink" title="Nonsmooth Projection-Free Optimization with Functional Constraints"></a>Nonsmooth Projection-Free Optimization with Functional Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11180">http://arxiv.org/abs/2311.11180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kamiarasgari/Nonsmooth-Projection-Free-Optimization-with-Functional-Constraints">https://github.com/kamiarasgari/Nonsmooth-Projection-Free-Optimization-with-Functional-Constraints</a></li>
<li>paper_authors: Kamiar Asgari, Michael J. Neely</li>
<li>for: 这个论文提出了一种基于偏导数的非光滑凸优化算法，可以处理具有一般凸函数不等式约束的非光滑优化问题，而不需要进行可行集 проекции。</li>
<li>methods: 该算法使用了一种简单的分离方案，并使用了一个新的拉格朗日积分更新规则。</li>
<li>results: 该算法可以在 $\mathcal{O}(\epsilon^{-2})$ 迭代中获得 $\epsilon$-次优化解决方案，每迭代只需要一个 (可能不准确) 线性最小化询问（LMO）和一个 (可能不准确) 偏导数计算。这种性能与现有的下界具有相同性。<details>
<summary>Abstract</summary>
This paper presents a subgradient-based algorithm for constrained nonsmooth convex optimization that does not require projections onto the feasible set. While the well-established Frank-Wolfe algorithm and its variants already avoid projections, they are primarily designed for smooth objective functions. In contrast, our proposed algorithm can handle nonsmooth problems with general convex functional inequality constraints. It achieves an $\epsilon$-suboptimal solution in $\mathcal{O}(\epsilon^{-2})$ iterations, with each iteration requiring only a single (potentially inexact) Linear Minimization Oracle (LMO) call and a (possibly inexact) subgradient computation. This performance is consistent with existing lower bounds. Similar performance is observed when deterministic subgradients are replaced with stochastic subgradients. In the special case where there are no functional inequality constraints, our algorithm competes favorably with a recent nonsmooth projection-free method designed for constraint-free problems. Our approach utilizes a simple separation scheme in conjunction with a new Lagrange multiplier update rule.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种基于梯度的准则逼近算法，用于非规范凸优化问题，不需要进行可行集的投影。与已知的Frank-Wolfe算法和其变体不同，我们的提议的算法可以处理具有一般凸函数不等式约束的非准确问题。它可以在 $\mathcal{O}(\epsilon^{-2})$ 迭代内 achieve an $\epsilon$-下界解，每迭代只需要一个 (可能不准确) 线性最小化函数 oracle (LMO) 调用和 (可能不准确) 梯度计算。这个性能与现有的下界具有相同的性能。此外，当使用权重函数时，我们的算法也可以达到类似的性能。在特殊情况下，当没有函数不等式约束时，我们的算法与一种最近的准则逼近方法，设计用于无约束问题，竞争得到。我们的方法使用了一种简单的分离方案，并使用了一个新的拉格朗日积分规则。
</details></li>
</ul>
<hr>
<h2 id="Low-Precision-Floating-Point-for-Efficient-On-Board-Deep-Neural-Network-Processing"><a href="#Low-Precision-Floating-Point-for-Efficient-On-Board-Deep-Neural-Network-Processing" class="headerlink" title="Low-Precision Floating-Point for Efficient On-Board Deep Neural Network Processing"></a>Low-Precision Floating-Point for Efficient On-Board Deep Neural Network Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11172">http://arxiv.org/abs/2311.11172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cédric Gernigon, Silviu-Ioan Filip, Olivier Sentieys, Clément Coggiola, Mickaël Bruno</li>
<li>for: 降低高分辨率地球观测卫星系统中的下载链速率</li>
<li>methods: 使用在机器上的深度学习来压缩数据</li>
<li>results: 使用6比特浮点数字quantization可以和单精度浮点数字相比，无需 significannot accuracy degradation<details>
<summary>Abstract</summary>
One of the major bottlenecks in high-resolution Earth Observation (EO) space systems is the downlink between the satellite and the ground. Due to hardware limitations, on-board power limitations or ground-station operation costs, there is a strong need to reduce the amount of data transmitted. Various processing methods can be used to compress the data. One of them is the use of on-board deep learning to extract relevant information in the data. However, most ground-based deep neural network parameters and computations are performed using single-precision floating-point arithmetic, which is not adapted to the context of on-board processing. We propose to rely on quantized neural networks and study how to combine low precision (mini) floating-point arithmetic with a Quantization-Aware Training methodology. We evaluate our approach with a semantic segmentation task for ship detection using satellite images from the Airbus Ship dataset. Our results show that 6-bit floating-point quantization for both weights and activations can compete with single-precision without significant accuracy degradation. Using a Thin U-Net 32 model, only a 0.3% accuracy degradation is observed with 6-bit minifloat quantization (a 6-bit equivalent integer-based approach leads to a 0.5% degradation). An initial hardware study also confirms the potential impact of such low-precision floating-point designs, but further investigation at the scale of a full inference accelerator is needed before concluding whether they are relevant in a practical on-board scenario.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)一个主要瓶颈在高分辨率地球观测（EO）空间系统中是地面和卫星之间的下载链。由于硬件限制、机载电力限制或地面站操作成本，有强需要减少数据传输量。Various processing methods can be used to compress the data. One of them is the use of on-board deep learning to extract relevant information in the data. However, most ground-based deep neural network parameters and computations are performed using single-precision floating-point arithmetic, which is not adapted to the context of on-board processing. We propose to rely on quantized neural networks and study how to combine low precision (mini) floating-point arithmetic with a Quantization-Aware Training methodology. We evaluate our approach with a semantic segmentation task for ship detection using satellite images from the Airbus Ship dataset. Our results show that 6-bit floating-point quantization for both weights and activations can compete with single-precision without significant accuracy degradation. Using a Thin U-Net 32 model, only a 0.3% accuracy degradation is observed with 6-bit minifloat quantization (a 6-bit equivalent integer-based approach leads to a 0.5% degradation). An initial hardware study also confirms the potential impact of such low-precision floating-point designs, but further investigation at the scale of a full inference accelerator is needed before concluding whether they are relevant in a practical on-board scenario.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Machine-Learning-Models-for-Quantum-Error-Correction"><a href="#Benchmarking-Machine-Learning-Models-for-Quantum-Error-Correction" class="headerlink" title="Benchmarking Machine Learning Models for Quantum Error Correction"></a>Benchmarking Machine Learning Models for Quantum Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11167">http://arxiv.org/abs/2311.11167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Fu, Yue Zhao</li>
<li>for: 本研究的目的是强调机器学习（ML）在量子错误恢复（QEC）中的应用，以及在量子计算机系统中实现稳定的量子计算机系统。</li>
<li>methods: 本研究使用了七种当前最佳的深度学习算法，包括卷积神经网络、图神经网络和图变换器，以评估Machine Learning在QEC中捕捉远程 ancilla qubits 的依赖关系的能力。</li>
<li>results: 研究发现，通过扩大接受区域来利用远程 ancilla qubits 中的信息，可以提高 QEC 的准确率。例如，与 CNN 相比，U-Net 可以提高准确率约50%。此外，研究还提供了一个全面的分析，以便未来的研究。<details>
<summary>Abstract</summary>
Quantum Error Correction (QEC) is one of the fundamental problems in quantum computer systems, which aims to detect and correct errors in the data qubits within quantum computers. Due to the presence of unreliable data qubits in existing quantum computers, implementing quantum error correction is a critical step when establishing a stable quantum computer system. Recently, machine learning (ML)-based approaches have been proposed to address this challenge. However, they lack a thorough understanding of quantum error correction. To bridge this research gap, we provide a new perspective to understand machine learning-based QEC in this paper. We find that syndromes in the ancilla qubits result from errors on connected data qubits, and distant ancilla qubits can provide auxiliary information to rule out some incorrect predictions for the data qubits. Therefore, to detect errors in data qubits, we must consider the information present in the long-range ancilla qubits. To the best of our knowledge, machine learning is less explored in the dependency relationship of QEC. To fill the blank, we curate a machine learning benchmark to assess the capacity to capture long-range dependencies for quantum error correction. To provide a comprehensive evaluation, we evaluate seven state-of-the-art deep learning algorithms spanning diverse neural network architectures, such as convolutional neural networks, graph neural networks, and graph transformers. Our exhaustive experiments reveal an enlightening trend: By enlarging the receptive field to exploit information from distant ancilla qubits, the accuracy of QEC significantly improves. For instance, U-Net can improve CNN by a margin of about 50%. Finally, we provide a comprehensive analysis that could inspire future research in this field. We will release the code when the paper is published.
</details>
<details>
<summary>摘要</summary>
量子错误修复（QEC）是量子计算机系统中的基本问题，旨在检测和修复数据QUUBITS中的错误。由于现有的量子计算机中的数据QUUBITS不可靠，实施量子错误修复是建立稳定量子计算机系统的关键步骤。近年来，基于机器学习（ML）的方法被提议用于解决这个挑战。然而，这些方法缺乏量子错误修复的深入理解。为了填补这个研究漏洞，我们在这篇论文中提供了一新的视角，发现在 ancilla qubits 中的症状是由数据QUUBITS中的错误引起的，并且远离 ancilla qubits 可以提供辅助信息，以排除一些错误的预测。因此，为检测数据QUUBITS 中的错误，我们必须考虑 ancilla qubits 中的信息。在量子错误修复中，机器学习的应用较少，因此我们在这个领域进行了一项机器学习benchmark的创建，以评估机器学习算法的捕捉远程依赖关系能力。我们对七种state-of-the-art深度学习算法进行了广泛的实验，包括卷积神经网络、图神经网络和图变换器。我们的广泛实验发现，通过扩大感知场，以利用远离 ancilla qubits 中的信息，量子错误修复的准确率得到了显著提高。例如，U-Net 可以提高 CNN 的准确率约50%。最后，我们对这一结论进行了全面的分析，以便对这一领域的未来研究提供指导。我们将在论文发表时释放代码。
</details></li>
</ul>
<hr>
<h2 id="On-the-Hardness-of-Learning-to-Stabilize-Linear-Systems"><a href="#On-the-Hardness-of-Learning-to-Stabilize-Linear-Systems" class="headerlink" title="On the Hardness of Learning to Stabilize Linear Systems"></a>On the Hardness of Learning to Stabilize Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11151">http://arxiv.org/abs/2311.11151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong Zeng, Zexiang Liu, Zhe Du, Necmiye Ozay, Mario Sznaier</li>
<li>for: 这 paper 是研究 linear time-invariant systems 的 stabilization 问题的，具体来说是研究学习这类系统的统计困难性。</li>
<li>methods: 这 paper 使用了学习理论和 robust control 的思想，研究了一种类型的系统在不同维度下的学习难度。</li>
<li>results: 研究发现，这类系统的学习难度会随着系统维度的增加而增加 exponentially，即使这些系统可以容易地被识别。<details>
<summary>Abstract</summary>
Inspired by the work of Tsiamis et al. \cite{tsiamis2022learning}, in this paper we study the statistical hardness of learning to stabilize linear time-invariant systems. Hardness is measured by the number of samples required to achieve a learning task with a given probability. The work in \cite{tsiamis2022learning} shows that there exist system classes that are hard to learn to stabilize with the core reason being the hardness of identification. Here we present a class of systems that can be easy to identify, thanks to a non-degenerate noise process that excites all modes, but the sample complexity of stabilization still increases exponentially with the system dimension. We tie this result to the hardness of co-stabilizability for this class of systems using ideas from robust control.
</details>
<details>
<summary>摘要</summary>
根据 Tsiavmis 等人的研究 \cite{tsiamis2022learning}, 在这篇论文中我们研究了线性时间不变系统学习稳定性的统计困难性。困难性是由于学习任务中的概率。 Tsiavmis 等人的研究显示存在一些系统类型具有稳定性学习困难的核心原因，那就是识别困难。在这篇论文中，我们提出了一种系统类型，它具有非零噪变数驱动所有模式，但是稳定性学习的样本复杂性还是随系统维度的幂函数增长。我们将这结果与这种系统的稳定性困难性进行连结，使用了稳定控制的想法。
</details></li>
</ul>
<hr>
<h2 id="Auxiliary-Losses-for-Learning-Generalizable-Concept-based-Models"><a href="#Auxiliary-Losses-for-Learning-Generalizable-Concept-based-Models" class="headerlink" title="Auxiliary Losses for Learning Generalizable Concept-based Models"></a>Auxiliary Losses for Learning Generalizable Concept-based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11108">http://arxiv.org/abs/2311.11108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ivaxi0s/coop-cbm">https://github.com/ivaxi0s/coop-cbm</a></li>
<li>paper_authors: Ivaxi Sheth, Samira Ebrahimi Kahou</li>
<li>for: 提高模型透明度和性能，增强模型的可理解性。</li>
<li>methods: 提出协作概念瓶颈模型（coop-CBM），采用概念归一化损失（COL）来促进概念表示的分离和减少内部概念距离。</li>
<li>results: 在实际数据集上进行了图像分类任务的广泛实验，并研究了不同分布Shift Setting下模型的性能。结果显示，我们提出的方法可以在所有分布Shift Setting下达到最高精度，甚至超过黑盒模型的最高概念精度。<details>
<summary>Abstract</summary>
The increasing use of neural networks in various applications has lead to increasing apprehensions, underscoring the necessity to understand their operations beyond mere final predictions. As a solution to enhance model transparency, Concept Bottleneck Models (CBMs) have gained popularity since their introduction. CBMs essentially limit the latent space of a model to human-understandable high-level concepts. While beneficial, CBMs have been reported to often learn irrelevant concept representations that consecutively damage model performance. To overcome the performance trade-off, we propose cooperative-Concept Bottleneck Model (coop-CBM). The concept representation of our model is particularly meaningful when fine-grained concept labels are absent. Furthermore, we introduce the concept orthogonal loss (COL) to encourage the separation between the concept representations and to reduce the intra-concept distance. This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL. We also study the performance of coop-CBM models under various distributional shift settings. We show that our proposed method achieves higher accuracy in all distributional shift settings even compared to the black-box models with the highest concept accuracy.
</details>
<details>
<summary>摘要</summary>
随着神经网络在不同应用领域的使用越来越广泛，人们对神经网络的运作更加积极要进行了解。为了增强模型的透明度，概念瓶颈模型（CBM）在引入后得到了广泛的关注。CBM通过限制神经网络的幂辐空间来带来人类可理解的高级概念表示。虽然有利，但CBM经常学习不直观的概念表示，这会导致模型性能下降。为了解决性能和概念表示之间的负相关性，我们提议协同概念瓶颈模型（coop-CBM）。我们的模型中的概念表示在细化概念标签缺失时特别有意义。此外，我们引入了概念正交损失（COL），以鼓励概念表示之间的分离和减少内部概念距离。本文通过对实际世界数据集进行了广泛的实验，包括CUB、AwA2、CelebA和TIL等图像分类任务。我们还研究了coop-CBM模型在不同分布shift设置下的性能。我们的提议方法在所有分布shift设置下都实现了更高的准确率，包括黑盒模型的最高概念准确率。
</details></li>
</ul>
<hr>
<h2 id="Flat-Minima-in-Linear-Estimation-and-an-Extended-Gauss-Markov-Theorem"><a href="#Flat-Minima-in-Linear-Estimation-and-an-Extended-Gauss-Markov-Theorem" class="headerlink" title="Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem"></a>Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11093">http://arxiv.org/abs/2311.11093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Segert</li>
<li>for: 本研究考虑了线性估计问题，并提出了一种基于GAUSS-MARKOV theorem的扩展，允许偏差运算符不等于零，但是受限于一个矩阵 нор 的Schatten类型。</li>
<li>methods: 本文使用了优化估计器的简单和显式公式，并在核和spectral norms（包括 Frobenius case）中 derivation。</li>
<li>results: 通过对多种Random matrix ensembles的分析和 simulations studies，本文显示了cross-validated Nuclear和Spectral regressors可以在一些情况下超越Ridge。<details>
<summary>Abstract</summary>
We consider the problem of linear estimation, and establish an extension of the Gauss-Markov theorem, in which the bias operator is allowed to be non-zero but bounded with respect to a matrix norm of Schatten type. We derive simple and explicit formulas for the optimal estimator in the cases of Nuclear and Spectral norms (with the Frobenius case recovering ridge regression). Additionally, we analytically derive the generalization error in multiple random matrix ensembles, and compare with Ridge regression. Finally, we conduct an extensive simulation study, in which we show that the cross-validated Nuclear and Spectral regressors can outperform Ridge in several circumstances.
</details>
<details>
<summary>摘要</summary>
我们考虑了线性估计问题，并提出了允许报 bias 运算符不为零，但是对矩阵 нор 类型的 Schatten 类型做bounded的扩展。我们得到了简单明确的优化者公式，包括核心和 спектраль norm 两种情况（带有 Frobenius 情况，相当于ridge regression）。此外，我们也derived了多种Random Matrix ensemble的泛化误差，并与ridge regression进行比较。最后，我们进行了大量的实验研究，并证明了在某些情况下，cross-validate的核心和 спектраль回归可以超越ridge。Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Compositional-Fusion-of-Signals-in-Data-Embedding"><a href="#Compositional-Fusion-of-Signals-in-Data-Embedding" class="headerlink" title="Compositional Fusion of Signals in Data Embedding"></a>Compositional Fusion of Signals in Data Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11085">http://arxiv.org/abs/2311.11085</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZhijinGuo/Compositional-Fusion-of-Signals-in-Data-Embedding">https://github.com/ZhijinGuo/Compositional-Fusion-of-Signals-in-Data-Embedding</a></li>
<li>paper_authors: Zhijin Guo, Zhaozhen Xu, Martha Lewis, Nello Cristianini<br>for: 本研究旨在探讨人工智能中的嵌入，即将符号结构转换为固定维度的向量，从而实际上将多个信号融合在一起。methods: 本研究提出了两种方法：首先是相关性检测法，测量知道特征和嵌入之间的相关性，其次是加法融合检测法，视嵌入为各个特征的向量的总和。results: 应用这两种方法后，Word2Vec中的嵌入发现 combining semantic and morphological signals，BERT句 embeddings可以分解为各个单词vector的主语、谓语和 objet。在知识图基本推荐系统中，用户嵌入，即使没有训练民主数据，仍然表现出年龄和性别的信号。 这种研究表明，嵌入是多种信号的融合，从Word2Vec组件到知识图中的人类特征提示。<details>
<summary>Abstract</summary>
Embeddings in AI convert symbolic structures into fixed-dimensional vectors, effectively fusing multiple signals. However, the nature of this fusion in real-world data is often unclear. To address this, we introduce two methods: (1) Correlation-based Fusion Detection, measuring correlation between known attributes and embeddings, and (2) Additive Fusion Detection, viewing embeddings as sums of individual vectors representing attributes.   Applying these methods, word embeddings were found to combine semantic and morphological signals. BERT sentence embeddings were decomposed into individual word vectors of subject, verb and object. In the knowledge graph-based recommender system, user embeddings, even without training on demographic data, exhibited signals of demographics like age and gender.   This study highlights that embeddings are fusions of multiple signals, from Word2Vec components to demographic hints in graph embeddings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这里使用对应的字 embeddings 转换为简化中文。</SYS>>原文：Embeddings in AI 将 символічні结构转换为固定维度的 вектор，实际上融合多个讯号。但在实际数据中，这种融合的性质往往不明确。为了解决这个问题，我们提出了两种方法：1. 相似性检测法，检测知道的特征和对应的 embedding 之间的相似性。2. 总和检测法，视对应的 embedding 为各个特征的总和，将它们视为各个特征的表现。通过这两种方法，我们发现了单词嵌入在 Word2Vec 中的 Semantic 和 Morphological 信号都会被融合在一起。BERT 的句子嵌入则可以被分解为单词的主语、词汇和宾语嵌入。在基于知识库的推荐系统中，用户嵌入，即使没有训练过demographic数据，也会显示出年龄和性别的信号。这些研究表明，对应的嵌入是多种信号的融合，从 Word2Vec 的成分到知识库中的推荐系统中的 demographic 信号。
</details></li>
</ul>
<hr>
<h2 id="The-Persian-Piano-Corpus-A-Collection-Of-Instrument-Based-Feature-Extracted-Data-Considering-Dastgah"><a href="#The-Persian-Piano-Corpus-A-Collection-Of-Instrument-Based-Feature-Extracted-Data-Considering-Dastgah" class="headerlink" title="The Persian Piano Corpus: A Collection Of Instrument-Based Feature Extracted Data Considering Dastgah"></a>The Persian Piano Corpus: A Collection Of Instrument-Based Feature Extracted Data Considering Dastgah</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11074">http://arxiv.org/abs/2311.11074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parsa Rasouli, Azam Bastanfard</li>
<li>for: 这篇论文的目的是提供一个完整的波斯钢琴资料库，以便在后续的研究中更好地理解波斯音乐和钢琴在其中的角色。</li>
<li>methods: 这篇论文使用了工具类方法，包括钢琴labels和完整的元数据，以提供对波斯音乐模式（Dastgah）的覆盖性研究。</li>
<li>results: 这篇论文提供了2022年波斯钢琴曲目的特征EXTRACT，以便在后续的研究中更好地理解波斯音乐和钢琴在其中的角色。<details>
<summary>Abstract</summary>
The research in the field of music is rapidly growing, and this trend emphasizes the need for comprehensive data. Though researchers have made an effort to contribute their own datasets, many data collections lack the requisite inclusivity for comprehensive study because they are frequently focused on particular components of music or other specific topics. We have endeavored to address data scarcity by employing an instrument-based approach to provide a complete corpus related to the Persian piano. Our piano corpus includes relevant labels for Persian music mode (Dastgah) and comprehensive metadata, allowing for utilization in various popular research areas. The features extracted from 2022 Persian piano pieces in The Persian Piano Corpus (PPC) have been collected and made available to researchers, aiming for a more thorough understanding of Persian music and the role of the piano in it in subsequent steps.
</details>
<details>
<summary>摘要</summary>
研究领域内的音乐研究正在快速增长，这种趋势强调了全面的数据需求。虽然研究人员努力提供自己的数据集，但许多数据集缺乏包括性，因为它们 часто专注于特定的音乐组成部分或其他特定话题。我们尝试 Address 数据缺乏的问题，采用了 Musical Instrument 基本方法，以提供完整的波斯钢琴相关数据集。我们的钢琴数据集包括波斯音乐模式（Dastgah）相关的标签，以及完整的元数据，以便在各种流行的研究领域中使用。我们从2022年波斯钢琴曲目中提取了特征，并将其作为研究者的工具提供，以便在后续步骤中更好地理解波斯音乐和钢琴在其中的角色。
</details></li>
</ul>
<hr>
<h2 id="Tactics2D-A-Multi-agent-Reinforcement-Learning-Environment-for-Driving-Decision-making"><a href="#Tactics2D-A-Multi-agent-Reinforcement-Learning-Environment-for-Driving-Decision-making" class="headerlink" title="Tactics2D: A Multi-agent Reinforcement Learning Environment for Driving Decision-making"></a>Tactics2D: A Multi-agent Reinforcement Learning Environment for Driving Decision-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11058">http://arxiv.org/abs/2311.11058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/woodoxen/tactics2d">https://github.com/woodoxen/tactics2d</a></li>
<li>paper_authors: Yueyuan Li, Songan Zhang, Mingyang Jiang, Xingyuan Chen, Ming Yang</li>
<li>for: 这篇论文的目的是提供一个轻松使用的多智能体学习库，用于开发自动驾驶决策算法。</li>
<li>methods: 该库包含了多种交通场景，以及具备多感知功能和交通规则违反检测的环境。它还提供了一个基线测试方法，并且具有高度可模块化和自定义化的特点。</li>
<li>results: 该库可以帮助研究人员快速开发和测试决策算法，以便更好地研究自动驾驶技术。<details>
<summary>Abstract</summary>
Tactics2D is an open-source multi-agent reinforcement learning library with a Python backend. Its goal is to provide a convenient toolset for researchers to develop decision-making algorithms for autonomous driving. The library includes diverse traffic scenarios implemented as gym-based environments equipped with multi-sensory capabilities and violation detection for traffic rules. Additionally, it features a reinforcement learning baseline tested with reasonable evaluation metrics. Tactics2D is highly modular and customizable. The source code of Tactics2D is available at https://github.com/WoodOxen/Tactics2D.
</details>
<details>
<summary>摘要</summary>
《战略2D》是一个开源的多代理人强化学习库，Python后端。它的目标是为研究人员提供一个便捷的工具集，以开发自适应驾驶决策算法。库包括各种交通场景，通过gym环境实现了多感知功能和规则违反检测。此外，它还提供了一个基线测试，并且高度可 modify 和定制。《战略2D》的源代码可以在 GitHub 上找到：https://github.com/WoodOxen/Tactics2D。
</details></li>
</ul>
<hr>
<h2 id="Challenges-in-data-based-geospatial-modeling-for-environmental-research-and-practice"><a href="#Challenges-in-data-based-geospatial-modeling-for-environmental-research-and-practice" class="headerlink" title="Challenges in data-based geospatial modeling for environmental research and practice"></a>Challenges in data-based geospatial modeling for environmental research and practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11057">http://arxiv.org/abs/2311.11057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana Koldasbayeva, Polina Tregubova, Mikhail Gasanov, Alexey Zaytsev, Anna Petrovskaia, Evgeny Burnaev</li>
<li>for: 这篇论文主要写于地球观测数据的数据科学应用，尤其是用于环境研究中的地球空间模型使用机器学习（ML）。</li>
<li>methods: 论文详细介绍了地球空间模型中常见的特点和挑战，如数据不均衡、空间自相关、预测错误、模型泛化、域特点和不确定性估计。</li>
<li>results: 论文总结了解决这些挑战的技术和 популяр编程工具，以及地球空间智能在环境应用中的前景。<details>
<summary>Abstract</summary>
With the rise of electronic data, particularly Earth observation data, data-based geospatial modelling using machine learning (ML) has gained popularity in environmental research. Accurate geospatial predictions are vital for domain research based on ecosystem monitoring and quality assessment and for policy-making and action planning, considering effective management of natural resources. The accuracy and computation speed of ML has generally proved efficient. However, many questions have yet to be addressed to obtain precise and reproducible results suitable for further use in both research and practice. A better understanding of the ML concepts applicable to geospatial problems enhances the development of data science tools providing transparent information crucial for making decisions on global challenges such as biosphere degradation and climate change. This survey reviews common nuances in geospatial modelling, such as imbalanced data, spatial autocorrelation, prediction errors, model generalisation, domain specificity, and uncertainty estimation. We provide an overview of techniques and popular programming tools to overcome or account for the challenges. We also discuss prospects for geospatial Artificial Intelligence in environmental applications.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation is written in Traditional Chinese, which is used in Taiwan and Hong Kong.Please note that the translation is done by a machine and may not be perfect, and there may be some nuances or cultural references that are not fully captured.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Simulators-for-Autonomous-Driving-Taxonomy-Challenges-and-Evaluation-Metrics"><a href="#A-Survey-of-Simulators-for-Autonomous-Driving-Taxonomy-Challenges-and-Evaluation-Metrics" class="headerlink" title="A Survey of Simulators for Autonomous Driving: Taxonomy, Challenges, and Evaluation Metrics"></a>A Survey of Simulators for Autonomous Driving: Taxonomy, Challenges, and Evaluation Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11056">http://arxiv.org/abs/2311.11056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueyuan Li, Wei Yuan, Weihao Yan, Qiyuan Shen, Chunxiang Wang, Ming Yang</li>
<li>For: This paper provides an in-depth review of simulators for autonomous driving, with a focus on their evolution, functionalities, and limitations.* Methods: The paper classifies simulators based on their functions, including traffic flow simulators, vehicle dynamics simulators, scenario editors, sensory data generators, and driving strategy validators. It also explores commercial and open-source simulators and evaluates their performance using qualitative and quantitative metrics.* Results: The paper identifies the primary limitations of simulators as fidelity and efficiency concerns and proposes solutions such as enhancing adverse weather simulation, automated map reconstruction, and interactive traffic participants. It also explores headless simulation and multiple-speed simulation techniques to improve the realism and efficiency of simulators.<details>
<summary>Abstract</summary>
Simulators have irreplaceable importance for the research and development of autonomous driving. Besides saving resources, labor, and time, simulation is the only feasible way to reproduce many severe accident scenarios. Despite their widespread adoption across academia and industry, there is an absence in the evolutionary trajectory of simulators and critical discourse on their limitations.   To bridge the gap in research, this paper conducts an in-depth review of simulators for autonomous driving. It delineates the three-decade development into three stages: specialized development period, gap period, and comprehensive development, from which it detects a trend of implementing comprehensive functionalities and open-source accessibility. Then it classifies the simulators by functions, identifying five categories: traffic flow simulator, vehicle dynamics simulator, scenario editor, sensory data generator, and driving strategy validator. Simulators that amalgamate diverse features are defined as comprehensive simulators. By investigating commercial and open-source simulators, this paper reveals that the critical issues faced by simulators primarily revolve around fidelity and efficiency concerns. This paper justifies that enhancing the realism of adverse weather simulation, automated map reconstruction, and interactive traffic participants will bolster credibility. Concurrently, headless simulation and multiple-speed simulation techniques will exploit the theoretic advantages. Moreover, this paper delves into potential solutions for the identified issues. It explores qualitative and quantitative evaluation metrics to assess the simulator's performance. This paper guides users to find suitable simulators efficiently and provides instructive suggestions for developers to improve simulator efficacy purposefully.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于自动驾驶研发中的研究和开发，模拟器具有不可或缺的重要性。除了节省资源、劳动力和时间之外，模拟是重创各种严重事故场景的唯一可行方式。尽管在学术和业界中广泛应用，但模拟器的演化征程和批判性讨论却缺乏。 为了填补这些研究的空白，本文进行了自动驾驶模拟器的深入综述。它将三十年的发展分成三个阶段：特殊开发期、阶段差期和综合开发，从中探测到了实现广泛功能和开源可访问性的趋势。然后它将模拟器按功能分类，确定了五种类别：交通流模拟器、车辆动力模拟器、enario编辑器、感知数据生成器和驾驶策略验证器。拥有多种功能的模拟器被定义为综合模拟器。通过商业和开源模拟器的调查，本文发现了模拟器的主要问题是准确性和效率问题。本文认为，提高风暴天气模拟、自动地图重建和互动交通参与者会增强信息的准确性。同时，无头模拟和多速模拟技术可以实现理论上的优势。此外，本文还探讨了模拟器问题的解决方案。它提出了评估模拟器性能的量化和质量评价指标，并为用户寻找适合的模拟器提供了有用的指导。为开发者提高模拟器效果，本文还提供了有价值的建议。
</details></li>
</ul>
<hr>
<h2 id="DenseNet-and-Support-Vector-Machine-classifications-of-major-depressive-disorder-using-vertex-wise-cortical-features"><a href="#DenseNet-and-Support-Vector-Machine-classifications-of-major-depressive-disorder-using-vertex-wise-cortical-features" class="headerlink" title="DenseNet and Support Vector Machine classifications of major depressive disorder using vertex-wise cortical features"></a>DenseNet and Support Vector Machine classifications of major depressive disorder using vertex-wise cortical features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11046">http://arxiv.org/abs/2311.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir Belov, Tracy Erwin-Grabner, Ling-Li Zeng, Christopher R. K. Ching, Andre Aleman, Alyssa R. Amod, Zeynep Basgoze, Francesco Benedetti, Bianca Besteher, Katharina Brosch, Robin Bülow, Romain Colle, Colm G. Connolly, Emmanuelle Corruble, Baptiste Couvy-Duchesne, Kathryn Cullen, Udo Dannlowski, Christopher G. Davey, Annemiek Dols, Jan Ernsting, Jennifer W. Evans, Lukas Fisch, Paola Fuentes-Claramonte, Ali Saffet Gonul, Ian H. Gotlib, Hans J. Grabe, Nynke A. Groenewold, Dominik Grotegerd, Tim Hahn, J. Paul Hamilton, Laura K. M. Han, Ben J Harrison, Tiffany C. Ho, Neda Jahanshad, Alec J. Jamieson, Andriana Karuk, Tilo Kircher, Bonnie Klimes-Dougan, Sheri-Michelle Koopowitz, Thomas Lancaster, Ramona Leenings, Meng Li, David E. J. Linden, Frank P. MacMaster, David M. A. Mehler, Susanne Meinert, Elisa Melloni, Bryon A. Mueller, Benson Mwangi, Igor Nenadić, Amar Ojha, Yasumasa Okamoto, Mardien L. Oudega, Brenda W. J. H. Penninx, Sara Poletti, Edith Pomarol-Clotet, Maria J. Portella, Elena Pozzi, Joaquim Radua, Elena Rodríguez-Cano, Matthew D. Sacchet, Raymond Salvador, Anouk Schrantee, Kang Sim, Jair C. Soares, Aleix Solanes, Dan J. Stein, Frederike Stein, Aleks Stolicyn, Sophia I. Thomopoulos, Yara J. Toenders, Aslihan Uyar-Demir, Eduard Vieta, Yolanda Vives-Gilabert, Henry Völzke, Martin Walter, Heather C. Whalley, Sarah Whittle, Nils Winter, Katharina Wittfeld, Margaret J. Wright, Mon-Ju Wu, Tony T. Yang, Carlos Zarate, Dick J. Veltman, Lianne Schmaal, Paul M. Thompson, Roberto Goya-Maldonado</li>
<li>for: 这个研究是为了检测主要抑郁疾病（MDD）是否有 morphological alterations in the brain 的问题。</li>
<li>methods: 这个研究使用了深度学习工具来分析神经成像数据，并使用了 DenseNet 和 Support Vector Machine（SVM）两种类型的分类器。</li>
<li>results: 研究发现，不 matter which classifier is used, the integration of vertex-wise morphometric features did not lead to differentiability between MDD and healthy controls（HC），并且site effect also exists。 Therefore, the study suggests that MDD classification on this combination of features and classifiers is unfeasible。<details>
<summary>Abstract</summary>
Major depressive disorder (MDD) is a complex psychiatric disorder that affects the lives of hundreds of millions of individuals around the globe. Even today, researchers debate if morphological alterations in the brain are linked to MDD, likely due to the heterogeneity of this disorder. The application of deep learning tools to neuroimaging data, capable of capturing complex non-linear patterns, has the potential to provide diagnostic and predictive biomarkers for MDD. However, previous attempts to demarcate MDD patients and healthy controls (HC) based on segmented cortical features via linear machine learning approaches have reported low accuracies. In this study, we used globally representative data from the ENIGMA-MDD working group containing an extensive sample of people with MDD (N=2,772) and HC (N=4,240), which allows a comprehensive analysis with generalizable results. Based on the hypothesis that integration of vertex-wise cortical features can improve classification performance, we evaluated the classification of a DenseNet and a Support Vector Machine (SVM), with the expectation that the former would outperform the latter. As we analyzed a multi-site sample, we additionally applied the ComBat harmonization tool to remove potential nuisance effects of site. We found that both classifiers exhibited close to chance performance (balanced accuracy DenseNet: 51%; SVM: 53%), when estimated on unseen sites. Slightly higher classification performance (balanced accuracy DenseNet: 58%; SVM: 55%) was found when the cross-validation folds contained subjects from all sites, indicating site effect. In conclusion, the integration of vertex-wise morphometric features and the use of the non-linear classifier did not lead to the differentiability between MDD and HC. Our results support the notion that MDD classification on this combination of features and classifiers is unfeasible.
</details>
<details>
<summary>摘要</summary>
Major Depressive Disorder (MDD) 是一种复杂的心理疾病，影响全球数百万人的生活。尽管研究人员今天仍然debatewhether morphological alterations in the brain are linked to MDD, but the application of deep learning tools to neuroimaging data has the potential to provide diagnostic and predictive biomarkers for MDD. However, previous attempts to distinguish MDD patients and healthy controls (HC) based on segmented cortical features via linear machine learning approaches have reported low accuracies.在这个研究中，我们使用了ENIGMA-MDD工作组的全球代表性数据集（N=2,772）和HC（N=4,240），可以进行全面的分析并得到普遍可靠的结果。基于假设集成 vertex-wise cortical features可以提高分类性能，我们评估了DenseNet和Support Vector Machine (SVM)两种类器，期望前者能够超越后者。由于我们分析了多个站点的数据，我们还应用了ComBat协调工具来除掉可能的站点效应。我们发现，无论使用DenseNet或SVM类器，在未看过的站点上估计时，两者的准确率都接近机会准确率（balanced accuracy DenseNet: 51%; SVM: 53%）。然而，当分割folds包含所有站点时，两者的准确率（balanced accuracy DenseNet: 58%; SVM: 55%）提高了一些， indicating that site effect played a role.结论：在这种 combinaton of features and classifiers 上，不可能 diferenciate MDD and HC。our results support the notion that MDD classification on this combination of features and classifiers is unfeasible.
</details></li>
</ul>
<hr>
<h2 id="SORTAD-Self-Supervised-Optimized-Random-Transformations-for-Anomaly-Detection-in-Tabular-Data"><a href="#SORTAD-Self-Supervised-Optimized-Random-Transformations-for-Anomaly-Detection-in-Tabular-Data" class="headerlink" title="SORTAD: Self-Supervised Optimized Random Transformations for Anomaly Detection in Tabular Data"></a>SORTAD: Self-Supervised Optimized Random Transformations for Anomaly Detection in Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11018">http://arxiv.org/abs/2311.11018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Hay, Pablo Liberman</li>
<li>for: 这篇研究目的是为了开发一个自主式异常探测方法，用于检测表格资料中的异常。</li>
<li>methods: 这篇研究使用了随机变数的应用，并使用这些预测的变数来检测异常。</li>
<li>results: 研究获得了当前顶尖的结果，在多个常用的异常探测数据集上都取得了良好的成绩，并且在所有测试数据集上也取得了总体良好的结果。<details>
<summary>Abstract</summary>
We consider a self-supervised approach to anomaly detection in tabular data. Random transformations are applied to the data, and then each transformation is identified based on its output. These predicted transformations are used to identify anomalies. In tabular data this approach faces many challenges that are related to the uncorrelated nature of the data. These challenges affect the transformations that should be used, as well as the use of their predictions. To this end, we propose SORTAD, a novel algorithm that is tailor-made to solve these challenges. SORTAD optimally chooses random transformations that help the classification process, and have a scoring function that is more sensitive to the changes in the transformations classification prediction encountered in tabular data. SORTAD achieved state-of-the-art results on multiple commonly used anomaly detection data sets, as well as in the overall results across all data sets tested.
</details>
<details>
<summary>摘要</summary>
我们考虑了一种自助学习方法 для异常检测在表格数据中。在这种方法中，随机变换被应用于数据，然后每个变换被预测。这些预测的变换被用来标识异常。在表格数据中，这种方法遇到了许多与不相关性相关的挑战。这些挑战影响了应用的变换以及其预测的用途。为此，我们提出了SORTAD算法，这是特制的解决这些挑战的算法。SORTAD优选随机变换，帮助分类过程，并且有一个更敏感的变换分类预测值的评分函数。SORTAD在多个常用的异常检测数据集上达到了状态的最佳结果，以及在所有数据集上的总结果。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Convergence-Guarantees-for-a-General-Class-of-Score-Based-Generative-Models"><a href="#Wasserstein-Convergence-Guarantees-for-a-General-Class-of-Score-Based-Generative-Models" class="headerlink" title="Wasserstein Convergence Guarantees for a General Class of Score-Based Generative Models"></a>Wasserstein Convergence Guarantees for a General Class of Score-Based Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11003">http://arxiv.org/abs/2311.11003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Gao, Hoang M. Nguyen, Lingjiong Zhu</li>
<li>for: 这 paper 是为了提供Score-based generative models（SGMs）的convrgence guarantees，以便在各种应用中达到state-of-the-art性能。</li>
<li>methods: 这 paper 使用了一种通用的 SGMs 类型，assuming accurate score estimates和smooth log-concave data distribution，并特定了几种具体的 forward processes，包括一些 newly proposed 的模型。</li>
<li>results: 这 paper 提供了一个 upper bound 的 iteration complexity  для每个模型，并提供了一个 lower bound 当数据分布是 Gaussian。 numerically, 这 paper 通过对 CIFAR-10 上的 unconditional image generation 进行实验，发现实验结果与理论预测相一致，并且模型使用我们 newly proposed forward processes 可以超越现有模型。<details>
<summary>Abstract</summary>
Score-based generative models (SGMs) is a recent class of deep generative models with state-of-the-art performance in many applications. In this paper, we establish convergence guarantees for a general class of SGMs in 2-Wasserstein distance, assuming accurate score estimates and smooth log-concave data distribution. We specialize our result to several concrete SGMs with specific choices of forward processes modelled by stochastic differential equations, and obtain an upper bound on the iteration complexity for each model, which demonstrates the impacts of different choices of the forward processes. We also provide a lower bound when the data distribution is Gaussian. Numerically, we experiment SGMs with different forward processes, some of which are newly proposed in this paper, for unconditional image generation on CIFAR-10. We find that the experimental results are in good agreement with our theoretical predictions on the iteration complexity, and the models with our newly proposed forward processes can outperform existing models.
</details>
<details>
<summary>摘要</summary>
score-based生成模型（SGM）是一种最近的深度生成模型，在许多应用场景中表现出色。在这篇论文中，我们证明了SGM在2-Wasserstein距离下的收敛保证，假设批处数据分布是准确的评估值和光滑凹陷分布。我们对特定的SGM进行特化，并得到了每个模型的迭代复杂度上限，这 demonstartes了不同的前进过程选择对模型的影响。我们还提供了 Gaussian 分布时的下界。 numerically, we experiment SGMs with different forward processes, some of which are newly proposed in this paper, for unconditional image generation on CIFAR-10. We find that the experimental results are in good agreement with our theoretical predictions on the iteration complexity, and the models with our newly proposed forward processes can outperform existing models.Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="BrainZ-BP-A-Non-invasive-Cuff-less-Blood-Pressure-Estimation-Approach-Leveraging-Brain-Bio-impedance-and-Electrocardiogram"><a href="#BrainZ-BP-A-Non-invasive-Cuff-less-Blood-Pressure-Estimation-Approach-Leveraging-Brain-Bio-impedance-and-Electrocardiogram" class="headerlink" title="BrainZ-BP: A Non-invasive Cuff-less Blood Pressure Estimation Approach Leveraging Brain Bio-impedance and Electrocardiogram"></a>BrainZ-BP: A Non-invasive Cuff-less Blood Pressure Estimation Approach Leveraging Brain Bio-impedance and Electrocardiogram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10996">http://arxiv.org/abs/2311.10996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bf-yang/brainz-bp">https://github.com/bf-yang/brainz-bp</a></li>
<li>paper_authors: Bufang Yang, Le Liu, Wenxuan Wu, Mengliang Zhou, Hongxing Liu, Xinbao Ning</li>
<li>for: 这个研究旨在探讨利用大脑 bio-impedance（BIOZ）测量血压（BP）的可能性，并提出了一种新的无捕获压力测量方法（BrainZ-BP）。</li>
<li>methods: 这个研究使用了两个电极位于前后脑骨的安排，测量大脑 BIOZ，并提取了脉冲传输时间和脑 BIOZ 形态特征等特征，并将其传输给四种回归模型进行 BP 估计。</li>
<li>results: 研究结果表明，Random Forest 回归模型的 Mean Absolute Error、Root Mean Square Error 和 Correlation Coefficient 分别为 2.17 mmHg、3.91 mmHg 和 0.90  для systolic pressure 估计，并为 1.71 mmHg、3.02 mmHg 和 0.89  для diastolic pressure 估计。这些结果表明 BrainZ-BP 可以准确地估计血压。<details>
<summary>Abstract</summary>
Accurate and continuous blood pressure (BP) monitoring is essential to the early prevention of cardiovascular diseases. Non-invasive and cuff-less BP estimation algorithm has gained much attention in recent years. Previous studies have demonstrated that brain bio-impedance (BIOZ) is a promising technique for non-invasive intracranial pressure (ICP) monitoring. Clinically, treatment for patients with traumatic brain injuries (TBI) requires monitoring the ICP and BP of patients simultaneously. Estimating BP by brain BIOZ directly can reduce the number of sensors attached to the patients, thus improving their comfort. To address the issues, in this study, we explore the feasibility of leveraging brain BIOZ for BP estimation and propose a novel cuff-less BP estimation approach called BrainZ-BP. Two electrodes are placed on the forehead and occipital bone of the head in the anterior-posterior direction for brain BIOZ measurement. Various features including pulse transit time and morphological features of brain BIOZ are extracted and fed into four regression models for BP estimation. Results show that the mean absolute error, root mean square error, and correlation coefficient of random forest regression model are 2.17 mmHg, 3.91 mmHg, and 0.90 for systolic pressure estimation, and are 1.71 mmHg, 3.02 mmHg, and 0.89 for diastolic pressure estimation. The presented BrainZ-BP can be applied in the brain BIOZ-based ICP monitoring scenario to monitor BP simultaneously.
</details>
<details>
<summary>摘要</summary>
Accurate and continuous blood pressure (BP) monitoring is essential to the early prevention of cardiovascular diseases. Non-invasive and cuff-less BP estimation algorithm has gained much attention in recent years. Previous studies have demonstrated that brain bio-impedance (BIOZ) is a promising technique for non-invasive intracranial pressure (ICP) monitoring. Clinically, treatment for patients with traumatic brain injuries (TBI) requires monitoring the ICP and BP of patients simultaneously. Estimating BP by brain BIOZ directly can reduce the number of sensors attached to the patients, thus improving their comfort. To address the issues, in this study, we explore the feasibility of leveraging brain BIOZ for BP estimation and propose a novel cuff-less BP estimation approach called BrainZ-BP. Two electrodes are placed on the forehead and occipital bone of the head in the anterior-posterior direction for brain BIOZ measurement. Various features including pulse transit time and morphological features of brain BIOZ are extracted and fed into four regression models for BP estimation. Results show that the mean absolute error, root mean square error, and correlation coefficient of random forest regression model are 2.17 mmHg, 3.91 mmHg, and 0.90 for systolic pressure estimation, and are 1.71 mmHg, 3.02 mmHg, and 0.89 for diastolic pressure estimation. The presented BrainZ-BP can be applied in the brain BIOZ-based ICP monitoring scenario to monitor BP simultaneously.Here's a word-for-word translation of the text into Simplified Chinese:精准和不间断的血压监测是预防心血管疾病的关键。非侵入式和无捕血压估算算法在最近几年内得到了广泛关注。先前的研究表明，脑 bio-impedance（BIOZ）是非侵入式 intracranial pressure（ICP）监测的一个可靠的技术。临床上，对抢救性脑 травuma（TBI）患者的治疗需要同时监测ICP和血压。通过脑 BIOZ 直接估算血压可以降低患者身上的感测器数量，从而改善他们的 комфор度。为解决这些问题，本研究提出了利用脑 BIOZ 进行血压估算的可能性，并提出了一种新的无捕血压估算方法，即 BrainZ-BP。在脑 BIOZ 测量中，两个电极被安置在头颈部的前后方向上，用于测量脑 BIOZ。从脑 BIOZ 中提取了多种特征，包括脉搏传输时间和脑 BIOZ 的形态特征，并将其传输给四种回归模型进行血压估算。结果显示，Random Forest 回归模型的平均绝对误差、根圆平方误差和相关系数分别为2.17 mmHg、3.91 mmHg和0.90 для systolic pressure 估算，分别为1.71 mmHg、3.02 mmHg和0.89 для diastolic pressure 估算。所提出的 BrainZ-BP 可以在脑 BIOZ 基于 ICP 监测场景中进行同时监测血压。
</details></li>
</ul>
<hr>
<h2 id="EdgeFM-Leveraging-Foundation-Model-for-Open-set-Learning-on-the-Edge"><a href="#EdgeFM-Leveraging-Foundation-Model-for-Open-set-Learning-on-the-Edge" class="headerlink" title="EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge"></a>EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10986">http://arxiv.org/abs/2311.10986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bufang Yang, Lixing He, Neiwen Ling, Zhenyu Yan, Guoliang Xing, Xian Shuai, Xiaozhe Ren, Xin Jiang</li>
<li>for: 这个研究旨在探讨如何将深度学习（Deep Learning）模型实现在资源有限的边缘设备（IoT devices）上，并且在不同的环境和任务下保持模型的一致性。</li>
<li>methods: 本研究提出了一个名为EdgeFM的边缘云合作系统，通过选择上传无标的数据来询问云上的基础模型（Foundation Models，FMs），并且自动在 runtime 进行模型交替，以应对数据不确定性和网络动态变化。</li>
<li>results: 根据三个公共数据集和两个自行收集的数据集进行评估，EdgeFM 可以将终端延迟时间降低到3.2倍，并且与基eline相比，实现了34.3%的准确度提升。<details>
<summary>Abstract</summary>
Deep Learning (DL) models have been widely deployed on IoT devices with the help of advancements in DL algorithms and chips. However, the limited resources of edge devices make these on-device DL models hard to be generalizable to diverse environments and tasks. Although the recently emerged foundation models (FMs) show impressive generalization power, how to effectively leverage the rich knowledge of FMs on resource-limited edge devices is still not explored. In this paper, we propose EdgeFM, a novel edge-cloud cooperative system with open-set recognition capability. EdgeFM selectively uploads unlabeled data to query the FM on the cloud and customizes the specific knowledge and architectures for edge models. Meanwhile, EdgeFM conducts dynamic model switching at run-time taking into account both data uncertainty and dynamic network variations, which ensures the accuracy always close to the original FM. We implement EdgeFM using two FMs on two edge platforms. We evaluate EdgeFM on three public datasets and two self-collected datasets. Results show that EdgeFM can reduce the end-to-end latency up to 3.2x and achieve 34.3% accuracy increase compared with the baseline.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）模型已广泛部署在物联网设备上，由于DL算法和芯片的进步。然而，边缘设备的限制资源使得这些边缘DL模型难以在多样环境和任务中具有普适性。虽然最近出现的基础模型（FM）表现出了很好的泛化能力，但是如何有效地利用边缘设备的贫乏资源来激活FM的知识仍然不是研究的主要方向。在本文中，我们提出了EdgeFM，一种边缘云合作系统，具有开放集合识别能力。EdgeFM在云端查询FM的基础上，选择上传无标签数据，并自适应边缘模型的特定知识和结构。同时，EdgeFM在运行时进行动态模型交换，考虑到数据不确定性和动态网络变化，以确保精度总是相对于原FM做出最佳化。我们使用了两个FM在两个边缘平台进行实现EdgeFM。我们对EdgeFM进行了三个公共数据集和两个自收集数据集的测试。结果表明，EdgeFM可以将终端延迟减少至3.2倍，并实现对基准值的34.3%的准确率提升。
</details></li>
</ul>
<hr>
<h2 id="Polynomial-Time-Solutions-for-ReLU-Network-Training-A-Complexity-Classification-via-Max-Cut-and-Zonotopes"><a href="#Polynomial-Time-Solutions-for-ReLU-Network-Training-A-Complexity-Classification-via-Max-Cut-and-Zonotopes" class="headerlink" title="Polynomial-Time Solutions for ReLU Network Training: A Complexity Classification via Max-Cut and Zonotopes"></a>Polynomial-Time Solutions for ReLU Network Training: A Complexity Classification via Max-Cut and Zonotopes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10972">http://arxiv.org/abs/2311.10972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Wang, Mert Pilanci</li>
<li>for:  investigate the complexity of training a two-layer ReLU neural network with weight decay regularization</li>
<li>methods:  using a standard cone-constrained convex program and developing a randomized algorithm</li>
<li>results:  prove that the hardness of approximation of ReLU networks mirrors the complexity of the Max-Cut problem, and develop polynomial-time approximation guarantees for certain categories of datasets<details>
<summary>Abstract</summary>
We investigate the complexity of training a two-layer ReLU neural network with weight decay regularization. Previous research has shown that the optimal solution of this problem can be found by solving a standard cone-constrained convex program. Using this convex formulation, we prove that the hardness of approximation of ReLU networks not only mirrors the complexity of the Max-Cut problem but also, in certain special cases, exactly corresponds to it. In particular, when $\epsilon\leq\sqrt{84/83}-1\approx 0.006$, we show that it is NP-hard to find an approximate global optimizer of the ReLU network objective with relative error $\epsilon$ with respect to the objective value. Moreover, we develop a randomized algorithm which mirrors the Goemans-Williamson rounding of semidefinite Max-Cut relaxations. To provide polynomial-time approximations, we classify training datasets into three categories: (i) For orthogonal separable datasets, a precise solution can be obtained in polynomial-time. (ii) When there is a negative correlation between samples of different classes, we give a polynomial-time approximation with relative error $\sqrt{\pi/2}-1\approx 0.253$. (iii) For general datasets, the degree to which the problem can be approximated in polynomial-time is governed by a geometric factor that controls the diameter of two zonotopes intrinsic to the dataset. To our knowledge, these results present the first polynomial-time approximation guarantees along with first hardness of approximation results for regularized ReLU networks.
</details>
<details>
<summary>摘要</summary>
我们研究具有权重减权化的两层ReLU神经网络的训练复杂性。先前的研究表明，这个问题的优化解决方案可以通过标准的 cone-constrained 几何 програм约束来找到。使用这个几何形式，我们证明了ReLU网络的困难性不仅和Max-Cut问题的复杂性相同，而且在某些特殊情况下，甚至与其相同。具体来说，当 $\epsilon\leq\sqrt{84/83}-1\approx 0.006$ 时，我们显示了一个NP困难的问题：在Relative Error $\epsilon$ 下，不可能在 polynomial-time 内找到ReLU网络目标函数的approximate全局最优值。此外，我们开发了一种随机化的算法，它类似于Goemans-Williamson 的半definite Max-Cut 缩放。为了提供 polynomial-time approxiamtion，我们将训练数据分为三类：（i）对吸引式分割数据进行精确解决，可以在 polynomial-time 内完成。（ii）当不同类别样本之间存在负相关性时，我们提供了一种 polynomial-time approxiamtion，其相对误差为 $\sqrt{\pi/2}-1\approx 0.253$。（iii）对一般数据集，问题的approxiamtion程度由数据集的径向因子控制，这个因子控制两个zonotope 的径向。我们认为这些结果是 regularized ReLU 神经网络的首个 polynomial-time approximation guarantee 和首个困难性 results。
</details></li>
</ul>
<hr>
<h2 id="Learning-Deterministic-Finite-Automata-from-Confidence-Oracles"><a href="#Learning-Deterministic-Finite-Automata-from-Confidence-Oracles" class="headerlink" title="Learning Deterministic Finite Automata from Confidence Oracles"></a>Learning Deterministic Finite Automata from Confidence Oracles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10963">http://arxiv.org/abs/2311.10963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wilson Wu</li>
<li>for: 学习一个确定性Finite Automaton（DFA）从一个信任函数($Q$)中。</li>
<li>methods: 使用一个信任函数($Q$)，该函数对字符串$x\in\Sigma^*$返回一个分数，表示该字符串是否在目标语言($L$)中。</li>
<li>results: 学习一个DFA表示，该表示保留了信任函数($Q$)中的信息，并且与该函数在高度信任的地方匹配紧密。<details>
<summary>Abstract</summary>
We discuss the problem of learning a deterministic finite automaton (DFA) from a confidence oracle. That is, we are given access to an oracle $Q$ with incomplete knowledge of some target language $L$ over an alphabet $\Sigma$; the oracle maps a string $x\in\Sigma^*$ to a score in the interval $[-1,1]$ indicating its confidence that the string is in the language. The interpretation is that the sign of the score signifies whether $x\in L$, while the magnitude $|Q(x)|$ represents the oracle's confidence. Our goal is to learn a DFA representation of the oracle that preserves the information that it is confident in. The learned DFA should closely match the oracle wherever it is highly confident, but it need not do this when the oracle is less sure of itself.
</details>
<details>
<summary>摘要</summary>
我们讨论一个 deterministic finite automaton (DFA) 的学习问题， Specifically, we are given access to an oracle $Q$ with incomplete knowledge of some target language $L$ over an alphabet $\Sigma$; the oracle maps a string $x\in\Sigma^*$ to a score in the interval $[-1,1]$ indicating its confidence that the string is in the language. The interpretation is that the sign of the score signifies whether $x\in L$, while the magnitude $|Q(x)|$ represents the oracle's confidence. Our goal is to learn a DFA representation of the oracle that preserves the information that it is confident in. The learned DFA should closely match the oracle wherever it is highly confident, but it need not do this when the oracle is less sure of itself.Here's the translation breakdown:* "deterministic finite automaton" (DFA) 被翻译为 "确定型 finite automaton" (CFAs)* "confidence oracle" 被翻译为 "信任 oracle"* "target language" 被翻译为 "目标语言"* "alphabet" 被翻译为 "字母"* "score" 被翻译为 "分数"* "interval" 被翻译为 "区间"* "sign" 被翻译为 "符号"* "magnitude" 被翻译为 "大小"* "preserves" 被翻译为 "保持"* "closely match" 被翻译为 "匹配"Note that the translation is in Simplified Chinese, which is the most widely used variety of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Classification-Methods-Based-on-Machine-Learning-for-the-Analysis-of-Fetal-Health-Data"><a href="#Classification-Methods-Based-on-Machine-Learning-for-the-Analysis-of-Fetal-Health-Data" class="headerlink" title="Classification Methods Based on Machine Learning for the Analysis of Fetal Health Data"></a>Classification Methods Based on Machine Learning for the Analysis of Fetal Health Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10962">http://arxiv.org/abs/2311.10962</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferzaad/Diabetes">https://github.com/ferzaad/Diabetes</a></li>
<li>paper_authors: Binod Regmi, Chiranjibi Shah</li>
<li>for: This paper aims to assess the classification performance of various machine learning models for fetal health analysis.</li>
<li>methods: The authors use machine learning models such as SVM, RF, and TabNet, as well as dimensionality reduction techniques like PCA and LDA.</li>
<li>results: The TabNet model achieves a classification accuracy of 94.36% on a fetal health dataset, demonstrating the effectiveness of machine learning-based techniques for fetal health analysis.<details>
<summary>Abstract</summary>
The persistent battle to decrease childhood mortality serves as a commonly employed benchmark for gauging advancements in the field of medicine. Globally, the under-5 mortality rate stands at approximately 5 million, with a significant portion of these deaths being avoidable. Given the significance of this problem, Machine learning-based techniques have emerged as a prominent tool for assessing fetal health. In this work, we have analyzed the classification performance of various machine learning models for fetal health analysis. Classification performance of various machine learning models, such as support vector machine (SVM), random forest(RF), and attentive interpretable tabular learning (TabNet) have been assessed on fetal health. Moreover, dimensionality reduction techniques, such as Principal component analysis (PCA) and Linear discriminant analysis (LDA) have been implemented to obtain better classification performance with less number of features. A TabNet model on a fetal health dataset provides a classification accuracy of 94.36%. In general, this technology empowers doctors and healthcare experts to achieve precise fetal health classification and identify the most influential features in the process.
</details>
<details>
<summary>摘要</summary>
persistent battle to decrease childhood mortality serves as a commonly employed benchmark for gauging advancements in the field of medicine。 globally, the under-5 mortality rate stands at approximately 5 million, with a significant portion of these deaths being avoidable。 given the significance of this problem, machine learning-based techniques have emerged as a prominent tool for assessing fetal health。 in this work, we have analyzed the classification performance of various machine learning models for fetal health analysis。 classification performance of various machine learning models, such as support vector machine (SVM), random forest (RF), and attentive interpretable tabular learning (TabNet) have been assessed on fetal health。 moreover, dimensionality reduction techniques, such as principal component analysis (PCA) and linear discriminant analysis (LDA) have been implemented to obtain better classification performance with less number of features。 a TabNet model on a fetal health dataset provides a classification accuracy of 94.36%。 in general, this technology empowers doctors and healthcare experts to achieve precise fetal health classification and identify the most influential features in the process。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Taxonomic-analysis-of-asteroids-with-artificial-neural-networks"><a href="#Taxonomic-analysis-of-asteroids-with-artificial-neural-networks" class="headerlink" title="Taxonomic analysis of asteroids with artificial neural networks"></a>Taxonomic analysis of asteroids with artificial neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10954">http://arxiv.org/abs/2311.10954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nanping Luo, Xiaobin Wang, Shenghong Gu, Antti Penttilä, Karri Muinonen, Yisi Liu</li>
<li>for:  asteroid taxonomy and composition analysis</li>
<li>methods:  artificial neural networks (ANNs) and spectral data from the Chinese Space Survey telescope (CSST)</li>
<li>results:  higher than 92% accuracy in asteroid classification using the ANN tool, reasonable predictions for known taxonomic labels, and potential application for analyzing CSST asteroid spectra in the future.Here’s the simplified Chinese text:</li>
<li>for: asteroid的分类和组成分析</li>
<li>methods: 人工神经网络（ANNs）和中国空间探测 telescope（CSST）的spectral数据</li>
<li>results:  ANN工具的准确率高于92%，对已知分类标签的预测结果是合理的，并且可能应用于未来分析CSST asteroid spectrum。<details>
<summary>Abstract</summary>
We study the surface composition of asteroids with visible and/or infrared spectroscopy. For example, asteroid taxonomy is based on the spectral features or multiple color indices in visible and near-infrared wavelengths. The composition of asteroids gives key information to understand their origin and evolution. However, we lack compositional information for faint asteroids due to limits of ground-based observational instruments. In the near future, the Chinese Space Survey telescope (CSST) will provide multiple colors and spectroscopic data for asteroids of apparent magnitude brighter than 25 mag and 23 mag, respectively. For the aim of analysis of the CSST spectroscopic data, we applied an algorithm using artificial neural networks (ANNs) to establish a preliminary classification model for asteroid taxonomy according to the design of the survey module of CSST. Using the SMASS II spectra and the Bus-Binzel taxonomy system, our ANN classification tool composed of 5 individual ANNs is constructed, and the accuracy of this classification system is higher than 92 %. As the first application of our ANN tool, 64 spectra of 42 asteroids obtained in 2006 and 2007 by us with the 2.16-m telescope in the Xinglong station (Observatory Code 327) of National Astronomical Observatory of China are analyzed. The predicted labels of these spectra using our ANN tool are found to be reasonable when compared to their known taxonomic labels. Considering the accuracy and stability, our ANN tool can be applied to analyse the CSST asteroid spectra in the future.
</details>
<details>
<summary>摘要</summary>
我们研究小行星表面成分，使用可见和近红外谱学观测。例如，小行星分类是基于谱spectral特征或多色指数在可见和近红外波长上。小行星的成分提供了关键信息以解释它们的起源和演化。但我们对暗淡小行星的 compositional 信息缺乏。未来，中国空间探测 telescope（CSST）将提供多种颜色和谱学数据，用于 asteroids 的 apparent magnitude  brighter than 25 mag 和 23 mag 。为了分析 CSST 谱学数据的目的，我们采用了一种使用人工神经网络（ANNs）的算法，以建立一个初步的分类模型，以便根据 CSST 的设计模块进行 asteroid 分类。使用 SMASS II 谱和 Bus-Binzel 分类系统，我们的 ANN 分类工具由 5 个个 ANNS 组成，其准确率高于 92 %。作为我们 ANN 工具的首次应用，我们分析了 2006 和 2007 年我们使用 2.16-m  telescope 在中国天文台（Observatory Code 327）的 Xinglong 站进行的 64 个spectrum 数据，并发现这些spectrum 的预测标签使用我们 ANN 工具是合理的，与知道的分类标签相比。考虑准确和稳定，我们的 ANN 工具可以在未来用于分析 CSST 小行星谱数据。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Data-Driven-and-Knowledge-Driven-Approaches-for-Safety-Critical-Scenario-Generation-in-Automated-Vehicle-Validation"><a href="#Bridging-Data-Driven-and-Knowledge-Driven-Approaches-for-Safety-Critical-Scenario-Generation-in-Automated-Vehicle-Validation" class="headerlink" title="Bridging Data-Driven and Knowledge-Driven Approaches for Safety-Critical Scenario Generation in Automated Vehicle Validation"></a>Bridging Data-Driven and Knowledge-Driven Approaches for Safety-Critical Scenario Generation in Automated Vehicle Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10937">http://arxiv.org/abs/2311.10937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunkun Hao, Lu Liu, Wen Cui, Jianxing Zhang, Songyang Yan, Yuxi Pan, Zijiang Yang</li>
<li>for: The paper is written to address the challenges of validating automated driving vehicles (ADV) in safety-critical scenarios, and to propose a scenario generation framework called BridgeGen that can effectively generate diverse safety-critical scenarios for ADV development and performance evaluations.</li>
<li>methods: The paper uses both data-driven and knowledge-driven scenario generation methods, and introduces an ontology-based approach to model the five scenario layers in the operational design domain (ODD). The paper also develops an optimized scenario generation toolkit that combines traditional optimization and reinforcement learning schemes.</li>
<li>results: The paper conducts extensive experiments using the Carla simulator and demonstrates the effectiveness of BridgeGen in generating diverse safety-critical scenarios for ADV. The results show that BridgeGen can efficiently generate safety-critical scenarios that are not easily achievable by existing methods.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解决自动驾驶汽车（ADV）在安全关键场景中的验证问题，并提出一种名为 BridgeGen 的场景生成框架，用于 ADV 的开发和性能评估。</li>
<li>methods: 论文使用了数据驱动和知识驱动的场景生成方法，并提出了基于 ontology 的方法来模型五个场景层次。论文还开发了一个优化场景生成工具包，其结合了传统优化和强化学习方案。</li>
<li>results: 论文通过使用 Carla  simulateur 进行了广泛的实验，并证明了 BridgeGen 可以有效地生成多样化的安全关键场景。结果表明，BridgeGen 可以高效地生成安全关键场景，而这些场景不易由现有的方法实现。<details>
<summary>Abstract</summary>
Automated driving vehicles~(ADV) promise to enhance driving efficiency and safety, yet they face intricate challenges in safety-critical scenarios. As a result, validating ADV within generated safety-critical scenarios is essential for both development and performance evaluations. This paper investigates the complexities of employing two major scenario-generation solutions: data-driven and knowledge-driven methods. Data-driven methods derive scenarios from recorded datasets, efficiently generating scenarios by altering the existing behavior or trajectories of traffic participants but often falling short in considering ADV perception; knowledge-driven methods provide effective coverage through expert-designed rules, but they may lead to inefficiency in generating safety-critical scenarios within that coverage. To overcome these challenges, we introduce BridgeGen, a safety-critical scenario generation framework, designed to bridge the benefits of both methodologies. Specifically, by utilizing ontology-based techniques, BridgeGen models the five scenario layers in the operational design domain (ODD) from knowledge-driven methods, ensuring broad coverage, and incorporating data-driven strategies to efficiently generate safety-critical scenarios. An optimized scenario generation toolkit is developed within BridgeGen. This expedites the crafting of safety-critical scenarios through a combination of traditional optimization and reinforcement learning schemes. Extensive experiments conducted using Carla simulator demonstrate the effectiveness of BridgeGen in generating diverse safety-critical scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Short-term-Volatility-Estimation-for-High-Frequency-Trades-using-Gaussian-processes-GPs"><a href="#Short-term-Volatility-Estimation-for-High-Frequency-Trades-using-Gaussian-processes-GPs" class="headerlink" title="Short-term Volatility Estimation for High Frequency Trades using Gaussian processes (GPs)"></a>Short-term Volatility Estimation for High Frequency Trades using Gaussian processes (GPs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10935">http://arxiv.org/abs/2311.10935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonard Mushunje, Maxwell Mashasha, Edina Chandiwana</li>
<li>For: The paper aims to improve short-term volatility and return forecasting for high-frequency trades by combining numeric and probabilistic models.* Methods: The paper uses a combination of Gaussian Processes (GPs) and a Numerical market prediction (NMP) model to make one-day-ahead volatility forecasts. The NMP model is used to correct the stock price data, and a Censored GP is used to model the relationship between the corrected stock prices and returns.* Results: The paper evaluates the forecasting errors using implied and estimated data.Here’s the simplified Chinese text for the three information points:* For: 这篇论文目的是为高频交易提高短期涨落风险和回报预测。* Methods: 论文使用GP和NMP模型组合来实现一天前的涨落风险预测。NMP模型用于correcting股票价格数据，而Censored GP用于模型corrected股票价格和回报之间的关系。* Results: 论文使用implied和estimated数据来评估预测误差。<details>
<summary>Abstract</summary>
The fundamental theorem behind financial markets is that stock prices are intrinsically complex and stochastic. One of the complexities is the volatility associated with stock prices. Volatility is a tendency for prices to change unexpectedly [1]. Price volatility is often detrimental to the return economics, and thus, investors should factor it in whenever making investment decisions, choices, and temporal or permanent moves. It is, therefore, crucial to make necessary and regular short and long-term stock price volatility forecasts for the safety and economics of investors returns. These forecasts should be accurate and not misleading. Different models and methods, such as ARCH GARCH models, have been intuitively implemented to make such forecasts. However, such traditional means fail to capture the short-term volatility forecasts effectively. This paper, therefore, investigates and implements a combination of numeric and probabilistic models for short-term volatility and return forecasting for high-frequency trades. The essence is that one-day-ahead volatility forecasts were made with Gaussian Processes (GPs) applied to the outputs of a Numerical market prediction (NMP) model. Firstly, the stock price data from NMP was corrected by a GP. Since it is not easy to set price limits in a market due to its free nature and randomness, a Censored GP was used to model the relationship between the corrected stock prices and returns. Forecasting errors were evaluated using the implied and estimated data.
</details>
<details>
<summary>摘要</summary>
金融市场的基本定理是股票价格本身具有内在的复杂性和随机性。其中一种复杂性是股票价格的波动性，波动性通常对于投资者的返报有负面影响，因此投资者应该在做投资决策时考虑波动性。为保证投资者的返报安全和经济，因此需要在REGULAR basis上进行短期和长期股票价格波动性预测。这些预测应该准确无误。传统方法和模型，如ARCH GARCH模型，已经被应用来进行这些预测，但它们在短期波动性预测上并不准确。这篇论文因此调查和实施了一种组合 numeric和 probabilistic 模型来进行短期波动性和回报预测。其中一种方法是使用 Gaussian Processes (GPs) 来对 numerics 市场预测模型（NMP）的输出进行预测。首先，股票价格数据从NMP中被修正了一个GP。由于市场的自由和随机性，使用 Censored GP 模型来模型修正后的股票价格和回报之间的关系。预测错误被评估使用实际和预测数据。
</details></li>
</ul>
<hr>
<h2 id="Near-Optimal-Fair-Resource-Allocation-for-Strategic-Agents-without-Money-A-Data-Driven-Approach"><a href="#Near-Optimal-Fair-Resource-Allocation-for-Strategic-Agents-without-Money-A-Data-Driven-Approach" class="headerlink" title="Near-Optimal Fair Resource Allocation for Strategic Agents without Money: A Data-Driven Approach"></a>Near-Optimal Fair Resource Allocation for Strategic Agents without Money: A Data-Driven Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10927">http://arxiv.org/abs/2311.10927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sihan Zeng, Sujay Bhatt, Eleonora Kreacic, Parisa Hassanzadeh, Alec Koppel, Sumitra Ganesh</li>
<li>for: 学习基于的公平分配机制设计，使用相对公平（PF）作为标准。</li>
<li>methods: 使用复杂的技术 inspirited by differentiable convex programming literature，计算PF机制的利用性。</li>
<li>results: 提出了一种能够快速计算PF机制的利用性的方法，并通过控制交易OFF来实现高度公平的分配。<details>
<summary>Abstract</summary>
We study learning-based design of fair allocation mechanisms for divisible resources, using proportional fairness (PF) as a benchmark. The learning setting is a significant departure from the classic mechanism design literature, in that, we need to learn fair mechanisms solely from data. In particular, we consider the challenging problem of learning one-shot allocation mechanisms -- without the use of money -- that incentivize strategic agents to be truthful when reporting their valuations. It is well-known that the mechanism that directly seeks to optimize PF is not incentive compatible, meaning that the agents can potentially misreport their preferences to gain increased allocations. We introduce the notion of "exploitability" of a mechanism to measure the relative gain in utility from misreport, and make the following important contributions in the paper: (i) Using sophisticated techniques inspired by differentiable convex programming literature, we design a numerically efficient approach for computing the exploitability of the PF mechanism. This novel contribution enables us to quantify the gap that needs to be bridged to approximate PF via incentive compatible mechanisms. (ii) Next, we modify the PF mechanism to introduce a trade-off between fairness and exploitability. By properly controlling this trade-off using data, we show that our proposed mechanism, ExPF-Net, provides a strong approximation to the PF mechanism while maintaining low exploitability. This mechanism, however, comes with a high computational cost. (iii) To address the computational challenges, we propose another mechanism ExS-Net, which is end-to-end parameterized by a neural network. ExS-Net enjoys similar (slightly inferior) performance and significantly accelerated training and inference time performance. (iv) Extensive numerical simulations demonstrate the robustness and efficacy of the proposed mechanisms.
</details>
<details>
<summary>摘要</summary>
我们研究基于学习的公平分配机制设计，使用比例公平（PF）作为 referent。学习设定是传统机制设计文献中的一个重要 departure，因为我们需要从数据中学习公平的机制。具体来说，我们考虑到具有挑战性的问题：学习一次分配机制——不使用金钱——导致战略性代表者 truthfully 报告他们的价值。已知PF直接寻求最佳化机制是不可吸引的，代表代表者可能会隐藏他们的 preference以获得更多的分配。我们引入了机制的“滥用”（exploitability）来衡量代表者可以从misreport中获得的优化。我们在文中做以下重要贡献：(i) 使用 differential convex programming 文献中的专门技术，我们设计了一个精确的方法来 Compute 机制的滥用。这个新的贡献使我们能够量化PF机制和吸引机制之间的差异。(ii) 我们将PF机制修改，以引入公平和滥用之间的变数。通过对数据进行控制，我们显示了我们的提案机制ExPF-Net可以将PF机制作为近似，同时保持低滥用。这个机制，然而，具有高计算成本。(iii) 为了解决计算问题，我们提出了另一个机制ExS-Net，这个机制是由神经网 Parametrize 的。ExS-Net 具有相似（微scopically inferior）的性能，并且具有明显提高的训练和测试时间性能。(iv) 我们的实验结果显示了我们的提案机制具有优良的Robustness和效用性。
</details></li>
</ul>
<hr>
<h2 id="PACOL-Poisoning-Attacks-Against-Continual-Learners"><a href="#PACOL-Poisoning-Attacks-Against-Continual-Learners" class="headerlink" title="PACOL: Poisoning Attacks Against Continual Learners"></a>PACOL: Poisoning Attacks Against Continual Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10919">http://arxiv.org/abs/2311.10919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huayu Li, Gregory Ditzler</li>
<li>for: 本研究旨在探讨 kontinual learning 系统在不可靠的来源中被恶意攻击的情况，并提出一种新的数据毒化攻击方法，称为 kontinual learning 攻击（PACOL）。</li>
<li>methods: 本研究使用了 Generative Replay 和 regularization-based kontinual learning 方法，并进行了广泛的实验来评估这些方法在攻击下的抵抗能力。</li>
<li>results: 研究发现，常用的 Generative Replay 和 regularization-based kontinual learning 方法都容易受到攻击，特别是 label-flipping 和 PACOL 等攻击方法可以让 kontinual learning 系统忘记已经学习的任务。<details>
<summary>Abstract</summary>
Continual learning algorithms are typically exposed to untrusted sources that contain training data inserted by adversaries and bad actors. An adversary can insert a small number of poisoned samples, such as mislabeled samples from previously learned tasks, or intentional adversarial perturbed samples, into the training datasets, which can drastically reduce the model's performance. In this work, we demonstrate that continual learning systems can be manipulated by malicious misinformation and present a new category of data poisoning attacks specific for continual learners, which we refer to as {\em Poisoning Attacks Against Continual Learners} (PACOL). The effectiveness of labeling flipping attacks inspires PACOL; however, PACOL produces attack samples that do not change the sample's label and produce an attack that causes catastrophic forgetting. A comprehensive set of experiments shows the vulnerability of commonly used generative replay and regularization-based continual learning approaches against attack methods. We evaluate the ability of label-flipping and a new adversarial poison attack, namely PACOL proposed in this work, to force the continual learning system to forget the knowledge of a learned task(s). More specifically, we compared the performance degradation of continual learning systems trained on benchmark data streams with and without poisoning attacks. Moreover, we discuss the stealthiness of the attacks in which we test the success rate of data sanitization defense and other outlier detection-based defenses for filtering out adversarial samples.
</details>
<details>
<summary>摘要</summary>
continuous learning algorithms 通常会被不良来源攻击，这些来源包括由 adversary 和坏 actor 插入的训练数据。一个 adversary 可以插入一小数量的毒害样本，如先前学习的任务中的杂乱标注样本或者 adversarial 扰动样本，这些样本可以导致模型的性能下降很快。在这项工作中，我们展示了 continual learning 系统可以被恶意诡射的，并提出了一种新的数据毒害攻击，称为 continual learning 中的毒害攻击（PACOL）。PACOL 的攻击样本不会改变样本的标签，但会导致模型忘记已经学习的知识。我们对常用的生成回馈和常规化基于 continual learning 的方法进行了完整的实验，并证明了这些方法对于攻击方法的抵触性。我们还比较了标签旋转攻击和我们在这项工作中提出的新的 adversarial 毒害攻击（PACOL）的性能下降情况，以及在不同的数据流中对 continual learning 系统的影响。此外，我们还讨论了这些攻击的隐蔽性，包括测试攻击成功率和其他基于异常检测的防御机制是否能够过滤恶意样本。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/cs.LG_2023_11_18/" data-id="clpxp6c5d00vaee880l4l2mys" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/eess.SP_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T08:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/eess.SP_2023_11_18/">eess.SP - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="BeamSync-Over-The-Air-Synchronization-for-Distributed-Massive-MIMO-Systems"><a href="#BeamSync-Over-The-Air-Synchronization-for-Distributed-Massive-MIMO-Systems" class="headerlink" title="BeamSync: Over-The-Air Synchronization for Distributed Massive MIMO Systems"></a>BeamSync: Over-The-Air Synchronization for Distributed Massive MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11070">http://arxiv.org/abs/2311.11070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Unnikrishnan Kunnath Ganesan, Rimalapudi Sarvendranath, Erik G. Larsson</li>
<li>for: 这个论文的目的是探讨分布式大量多输入多出力（MIMO）系统中多个地理上分开的访问点（AP）同时通信用户，利用多antenna协同MIMO处理和分布式设置的各种优势。</li>
<li>methods: 这篇论文使用了一种新的投影同步协调方法，named BeamSync，以协调多个地理上分开的AP进行同步。这种同步方法不需要发送任何测量数据到中央处理器（CPU）通过前方HL。</li>
<li>results:  simulation结果表明，使用提议的BeamSync方法可以提高性能，当AP中天线数量 Doubles 时，性能提高3dB。此外，这种方法也与传统的束 formaiting技术相比较好。<details>
<summary>Abstract</summary>
In distributed massive multiple-input multiple-output (MIMO) systems, multiple geographically separated access points (APs) communicate simultaneously with a user, leveraging the benefits of multi-antenna coherent MIMO processing and macro-diversity gains from the distributed setups. However, time and frequency synchronization of the multiple APs is crucial to achieve good performance and enable joint precoding. In this paper, we analyze the synchronization requirement among multiple APs from a reciprocity perspective, taking into account the multiplicative impairments caused by mismatches in radio frequency (RF) hardware. We demonstrate that a phase calibration of reciprocity-calibrated APs is sufficient for the joint coherent transmission of data to the user. To achieve synchronization, we propose a novel over-the-air synchronization protocol, named BeamSync, to calibrate the geographically separated APs without sending any measurements to the central processing unit (CPU) through fronthaul. We show that sending the synchronization signal in the dominant direction of the channel between APs is optimal. Additionally, we derive the optimal phase and frequency offset estimators. Simulation results indicate that the proposed BeamSync method enhances performance by 3 dB when the number of antennas at the APs is doubled. Moreover, the method performs well compared to traditional beamforming techniques.
</details>
<details>
<summary>摘要</summary>
在分布式巨大多输入多输出（MIMO）系统中，多个地理上分开的访问点（AP）同时与用户通信，利用多antenna干扰MIMO处理和macro-多样性收益。然而，多个AP的时间和频率同步是需要达到良好性能和启用联合预编码的关键。在这篇论文中，我们从reciprocity角度分析了多个AP之间的同步需求，考虑了 radio频率硬件匹配不准的乘数性质。我们示出，只需要在reciprocity-calibrated APs中进行相位准化，即可实现联合整合数据传输到用户。为实现同步，我们提出了一种新的无需中央处理单元（CPU）通过前段传输的空中同步协议，名为BeamSync。我们发现，在AP之间通信道的主导方向上发送同步信号是优化的。此外，我们 derive了最佳相位和频率偏移估计器。实验结果表明，我们提出的BeamSync方法可以在APantenna数量两倍时提高性能，并且与传统的扫描方法相比，其性能较好。
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-for-FAS-assisted-Multiuser-mmWave-Systems"><a href="#Channel-Estimation-for-FAS-assisted-Multiuser-mmWave-Systems" class="headerlink" title="Channel Estimation for FAS-assisted Multiuser mmWave Systems"></a>Channel Estimation for FAS-assisted Multiuser mmWave Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11041">http://arxiv.org/abs/2311.11041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Xu, Gui Zhou, Kai-Kit Wong, Wee Kiat New, Chao Wang, Chan-Byoung Chae, Ross Murch, Shi Jin, Yangyang Zhang</li>
<li>for: 这个论文targets the challenge of channel estimation in a multiuser millimeter-wave (mmWave) time-division duplexing (TDD) system.</li>
<li>methods: 该论文提议了一种low-sample-size sparse channel reconstruction (L3SCR)方法，利用mmWave通道的稀疏卷积特点来重建渠道状态信息 (CSI)。</li>
<li>results:  simulations results show that the proposed method can obtain precise CSI with minimal hardware switching and pilot overhead, leading to a system sum-rate that approaches the upper bound achievable with perfect CSI.<details>
<summary>Abstract</summary>
This letter investigates the challenge of channel estimation in a multiuser millimeter-wave (mmWave) time-division duplexing (TDD) system. In this system, the base station (BS) employs a multi-antenna uniform linear array (ULA), while each mobile user is equipped with a fluid antenna system (FAS). Accurate channel state information (CSI) plays a crucial role in the precise placement of antennas in FAS. Traditional channel estimation methods designed for fixed-antenna systems are inadequate due to the high dimensionality of FAS. To address this issue, we propose a low-sample-size sparse channel reconstruction (L3SCR) method, capitalizing on the sparse propagation paths characteristic of mmWave channels. In this approach, each fluid antenna only needs to switch and measure the channel at a few specific locations. By observing this reduced-dimensional data, we can effectively extract angular and gain information related to the sparse channel, enabling us to reconstruct the full CSI. Simulation results demonstrate that our proposed method allows us to obtain precise CSI with minimal hardware switching and pilot overhead. As a result, the system sum-rate approaches the upper bound achievable with perfect CSI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/eess.SP_2023_11_18/" data-id="clpxp6ceh01i3ee88doya6iys" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/17/cs.SD_2023_11_17/" class="article-date">
  <time datetime="2023-11-17T15:00:00.000Z" itemprop="datePublished">2023-11-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/17/cs.SD_2023_11_17/">cs.SD - 2023-11-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MSPB-a-longitudinal-multi-sensor-dataset-with-phenotypic-trait-measurements-from-honey-bees"><a href="#MSPB-a-longitudinal-multi-sensor-dataset-with-phenotypic-trait-measurements-from-honey-bees" class="headerlink" title="MSPB: a longitudinal multi-sensor dataset with phenotypic trait measurements from honey bees"></a>MSPB: a longitudinal multi-sensor dataset with phenotypic trait measurements from honey bees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10876">http://arxiv.org/abs/2311.10876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhu00121/MSPB-webpage">https://github.com/zhu00121/MSPB-webpage</a></li>
<li>paper_authors: Yi Zhu, Mahsa Abdollahi, Ségolène Maucourt, Nico Coallier, Heitor R. Guimarães, Pierre Giovenazzo, Tiago H. Falk</li>
<li>for: 这个论文是为了提供多种现象特征测量和蜂群学专家标注的蜂巢数据集，以便更广泛的分析。</li>
<li>methods: 该论文使用了多种感知器和温度传感器收集数据，并对数据进行预处理和分析。</li>
<li>results: 论文提供了蜂巢数据集的分布和感知器数据的视觉化，并基于感知器数据分析和机器学习实现了冬季死亡预测、蜂巢人口估计和活蜂后繁殖的应用。<details>
<summary>Abstract</summary>
We present a longitudinal multi-sensor dataset collected from honey bee colonies (Apis mellifera) with rich phenotypic measurements. Data were continuously collected between May-2020 and April-2021 from 53 hives located at two apiaries in Qu\'ebec, Canada. The sensor data included audio features, temperature, and relative humidity. The phenotypic measurements contained beehive population, number of brood cells (eggs, larva and pupa), Varroa destructor infestation levels, defensive and hygienic behaviors, honey yield, and winter mortality. Our study is amongst the first to provide a wide variety of phenotypic trait measurements annotated by apicultural science experts, which facilitate a broader scope of analysis. We first summarize the data collection procedure, sensor data pre-processing steps, and data composition. We then provide an overview of the phenotypic data distribution as well as a visualization of the sensor data patterns. Lastly, we showcase several hive monitoring applications based on sensor data analysis and machine learning, such as winter mortality prediction, hive population estimation, and the presence of an active and laying queen.
</details>
<details>
<summary>摘要</summary>
我们提供了一个长期多感器数据集，从加拿大魁北克省五十三个坛巢中收集到的蜜蜂（Apis mellifera）的数据。数据从2020年5月到2021年4月连续收集，来自两个 apiary 地点。感器数据包括声音特征、温度和相对湿度。现象特征测量包括坛巢人口、卵、幼虫和蛹数量、Varroa destructor 感染水平、防御和卫生行为、蜜产量和冬季死亡率。我们的研究是 amongst the first 提供了蜜蜂生物学专家 annotated 多种现象特征测量，使得更广泛的分析 scope 可行。我们首先介绍数据采集过程、感器数据预处理步骤和数据结构。然后我们提供现象特征数据分布的概述以及感器数据模式的视觉化。最后，我们展示了基于感器数据分析和机器学习的坛巢监测应用，如冬季死亡预测、坛巢人口估计和活跃和繁殖 queen 存在。
</details></li>
</ul>
<hr>
<h2 id="Retrieval-Augmented-Generation-of-Symbolic-Music-with-LLMs"><a href="#Retrieval-Augmented-Generation-of-Symbolic-Music-with-LLMs" class="headerlink" title="Retrieval Augmented Generation of Symbolic Music with LLMs"></a>Retrieval Augmented Generation of Symbolic Music with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10384">http://arxiv.org/abs/2311.10384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Jonason, Luca Casini, Carl Thomé, Bob L. T. Sturm</li>
<li>for: 用于音乐生成</li>
<li>methods: 使用检索系统选择相关示例</li>
<li>results: 在对用户进行对话时，音乐生成初步结果显示投入效果，特别是考虑到实现的可能性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We explore the use of large language models (LLMs) for music generation using a retrieval system to select relevant examples. We find promising initial results for music generation in a dialogue with the user, especially considering the ease with which such a system can be implemented. The code is available online.
</details>
<details>
<summary>摘要</summary>
我们研究使用大型自然语言模型（LLM） для музыкаль生成，使用检索系统选择相关的示例。我们发现在与用户对话中进行音乐生成初果很有前途，特别是考虑到这种系统的实现非常容易。代码在线上可用。Note: "音乐生成" (yīn yuè chàng jì) is a term used in China to refer to the generation of music using machine learning or other computational methods.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/17/cs.SD_2023_11_17/" data-id="clpxp6c7z012mee88dlnhf0mh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_11_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/17/eess.AS_2023_11_17/" class="article-date">
  <time datetime="2023-11-17T14:00:00.000Z" itemprop="datePublished">2023-11-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/17/eess.AS_2023_11_17/">eess.AS - 2023-11-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GhostVec-A-New-Threat-to-Speaker-Privacy-of-End-to-End-Speech-Recognition-System"><a href="#GhostVec-A-New-Threat-to-Speaker-Privacy-of-End-to-End-Speech-Recognition-System" class="headerlink" title="GhostVec: A New Threat to Speaker Privacy of End-to-End Speech Recognition System"></a>GhostVec: A New Threat to Speaker Privacy of End-to-End Speech Recognition System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10689">http://arxiv.org/abs/2311.10689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojiao Chen, Sheng Li, Jiyi Li, Hao Huang, Yang Cao, Liang He</li>
<li>for: 这篇论文旨在探讨语音识别系统中的隐私问题，具体来说是攻击者可以通过访问语音识别系统来提取说话人的信息。</li>
<li>methods: 这篇论文提出了一种简单高效的攻击方法，即GhostVec，可以在基于 transformer 的语音识别系统中提取说话人的信息，无需外部的说话人验证系统或自然的人声为参考。</li>
<li>results: 实验结果表明，GhostVec 可以将 transformer 基于语音识别系统中的说话人信息提取出来，并且可以达到 10.83% EER 和 0.47 minDCF 的水平，这表明了提案的方法的效iveness。<details>
<summary>Abstract</summary>
Speaker adaptation systems face privacy concerns, for such systems are trained on private datasets and often overfitting. This paper demonstrates that an attacker can extract speaker information by querying speaker-adapted speech recognition (ASR) systems. We focus on the speaker information of a transformer-based ASR and propose GhostVec, a simple and efficient attack method to extract the speaker information from an encoder-decoder-based ASR system without any external speaker verification system or natural human voice as a reference. To make our results quantitative, we pre-process GhostVec using singular value decomposition (SVD) and synthesize it into waveform. Experiment results show that the synthesized audio of GhostVec reaches 10.83\% EER and 0.47 minDCF with target speakers, which suggests the effectiveness of the proposed method. We hope the preliminary discovery in this study to catalyze future speech recognition research on privacy-preserving topics.
</details>
<details>
<summary>摘要</summary>
喋 speaker adaptation 系统面临着隐私问题，因为这些系统通常是基于私人数据集训练的，并且容易过拟合。这篇论文显示，攻击者可以通过询问 speaker-adapted 语音识别（ASR）系统来提取speaker信息。我们关注于基于 transformer 的 ASR 系统中的 speaker 信息，并提出了 GhostVec，一种简单而高效的攻击方法，可以在 encoder-decoder 结构的 ASR 系统中提取speaker 信息，不需要外部的 speaker 验证系统或自然的人声作为参考。为了让我们的结果变量，我们使用了特征值分解（SVD）来预处理 GhostVec，并将其转换成波形。实验结果表明， synthesized audio 的 GhostVec 达到了 10.83% EER 和 0.47 minDCF 的目标 speaker，这表明了我们提出的方法的有效性。我们希望这一初步发现可以推动未来的语音识别研究，尤其是在隐私保护方面。
</details></li>
</ul>
<hr>
<h2 id="Reprogramming-Self-supervised-Learning-based-Speech-Representations-for-Speaker-Anonymization"><a href="#Reprogramming-Self-supervised-Learning-based-Speech-Representations-for-Speaker-Anonymization" class="headerlink" title="Reprogramming Self-supervised Learning-based Speech Representations for Speaker Anonymization"></a>Reprogramming Self-supervised Learning-based Speech Representations for Speaker Anonymization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10664">http://arxiv.org/abs/2311.10664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojiao Chen, Sheng Li, Jiyi Li, Hao Huang, Yang Cao, Liang He</li>
<li>for: 隐藏说话人的身份，特别是使用自动生成学（SSL）模型时需要巨量的计算资源。</li>
<li>methods: 提出一种高效、参数灵活的说话人隐藏方法，基于最新的端到端模型重程技术。首先提取大型SSL模型中的说话人表示，并将其重程为一个 Pseudo 领域，以隐藏说话人的身份。</li>
<li>results: 在 VoicePrivacy Challenge (VPC) 2022 数据集上进行了广泛的实验，证明了我们的提出的参数灵活学习隐藏方法的效iveness。同时，我们的方法可以在隐藏过程中减少计算资源的消耗。<details>
<summary>Abstract</summary>
Current speaker anonymization methods, especially with self-supervised learning (SSL) models, require massive computational resources when hiding speaker identity. This paper proposes an effective and parameter-efficient speaker anonymization method based on recent End-to-End model reprogramming technology. To improve the anonymization performance, we first extract speaker representation from large SSL models as the speaker identifies. To hide the speaker's identity, we reprogram the speaker representation by adapting the speaker to a pseudo domain. Extensive experiments are carried out on the VoicePrivacy Challenge (VPC) 2022 datasets to demonstrate the effectiveness of our proposed parameter-efficient learning anonymization methods. Additionally, while achieving comparable performance with the VPC 2022 strong baseline 1.b, our approach consumes less computational resources during anonymization.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "speaker anonymization" is translated as "声音隐私" (shēng diàn yǐn bì)* "self-supervised learning" is translated as "自我超vision" (zì wǒ chāo wén)* "End-to-End model reprogramming" is translated as "端到端模型重写" (dìan dào dian módel zhòng xiǎng)* "pseudo domain" is translated as "假领域" (jiǎ lǐng yì)* "computational resources" is translated as "计算资源" (jìsuàn zhīyuán)Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="LE-SSL-MOS-Self-Supervised-Learning-MOS-Prediction-with-Listener-Enhancement"><a href="#LE-SSL-MOS-Self-Supervised-Learning-MOS-Prediction-with-Listener-Enhancement" class="headerlink" title="LE-SSL-MOS: Self-Supervised Learning MOS Prediction with Listener Enhancement"></a>LE-SSL-MOS: Self-Supervised Learning MOS Prediction with Listener Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10656">http://arxiv.org/abs/2311.10656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zili Qi, Xinhui Hu, Wangjin Zhou, Sheng Li, Hao Wu, Jian Lu, Xinkang Xu<br>for:This paper proposes a novel fusion model for MOS (Mean Opinion Score) prediction that combines supervised and unsupervised approaches to improve the accuracy of predicting subjective evaluations for speech synthesis systems, especially on out-of-domain test sets.methods:The proposed fusion model uses a combination of supervised and unsupervised techniques, including pre-trained self-supervised learning models, fine-tuning of unit language models, and ensemble learning with ASR confidence.results:The experimental results on the VoiceMOS Challenge 2023 show that the proposed LE-SSL-MOS system achieves better performance than the baseline, with an absolute improvement of 13% on the noisy and enhanced speech track. The system ranked 1st and 2nd, respectively, in the French speech synthesis track and the challenge’s noisy and enhanced speech track.<details>
<summary>Abstract</summary>
Recently, researchers have shown an increasing interest in automatically predicting the subjective evaluation for speech synthesis systems. This prediction is a challenging task, especially on the out-of-domain test set. In this paper, we proposed a novel fusion model for MOS prediction that combines supervised and unsupervised approaches. In the supervised aspect, we developed an SSL-based predictor called LE-SSL-MOS. The LE-SSL-MOS utilizes pre-trained self-supervised learning models and further improves prediction accuracy by utilizing the opinion scores of each utterance in the listener enhancement branch. In the unsupervised aspect, two steps are contained: we fine-tuned the unit language model (ULM) using highly intelligible domain data to improve the correlation of an unsupervised metric - SpeechLMScore. Another is that we utilized ASR confidence as a new metric with the help of ensemble learning. To our knowledge, this is the first architecture that fuses supervised and unsupervised methods for MOS prediction. With these approaches, our experimental results on the VoiceMOS Challenge 2023 show that LE-SSL-MOS performs better than the baseline. Our fusion system achieved an absolute improvement of 13% over LE-SSL-MOS on the noisy and enhanced speech track. Our system ranked 1st and 2nd, respectively, in the French speech synthesis track and the challenge's noisy and enhanced speech track.
</details>
<details>
<summary>摘要</summary>
近些时候，研究人员对自动预测语音合成系统的主观评价有增加的兴趣。这个预测任务，尤其是在域外测试集上是一项挑战。在这篇论文中，我们提出了一种新的融合模型，用于MOS预测。我们的LE-SSL-MOS模型 combining supervised和Unsupervised方法。在supervised方面，我们开发了基于自我超vised学习模型的SSL-based predictor。在无supervised方面，我们finetune了unit语言模型（ULM），使其与高度可识别的频谱数据进行更好的对应。此外，我们还使用ASR确idence作为一个新的度量，并通过ensemble学习来利用其。到我们所知，这是首个将supervised和Unsupervised方法融合的MOS预测架构。我们的实验结果表明，LE-SSL-MOS在VoiceMOS Challenge 2023中表现出色，与基准相比，LE-SSL-MOS在噪音和加强的语音轨道上具有13%的绝对改进。我们的融合系统在法语语音合成轨道和挑战的噪音和加强语音轨道上分别 ranking 1st和2nd。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/17/eess.AS_2023_11_17/" data-id="clpxp6c9k016hee88gawaa60h" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/17/cs.CV_2023_11_17/" class="article-date">
  <time datetime="2023-11-17T13:00:00.000Z" itemprop="datePublished">2023-11-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/17/cs.CV_2023_11_17/">cs.CV - 2023-11-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Closely-Spaced-Object-Classification-Using-MuyGPyS"><a href="#Closely-Spaced-Object-Classification-Using-MuyGPyS" class="headerlink" title="Closely-Spaced Object Classification Using MuyGPyS"></a>Closely-Spaced Object Classification Using MuyGPyS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10904">http://arxiv.org/abs/2311.10904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kerianne Pruett, Nathan McNaughton, Michael Schneider</li>
<li>for: 这篇论文的目的是提高地面空间域意识（SDA）算法的精度，以更好地检测和识别靠近的空间 объек。</li>
<li>methods: 这篇论文使用了泛函分布（Gaussian process）python包MuyGPyS进行靠近 объек的分类，并研究了分类精度与角分度和亮度差的关系。</li>
<li>results: 论文发现，使用泛函分布分类方法可以在更为困难的情况下提高分类精度，并且比传统机器学习方法更有优势。<details>
<summary>Abstract</summary>
Accurately detecting rendezvous and proximity operations (RPO) is crucial for understanding how objects are behaving in the space domain. However, detecting closely-spaced objects (CSO) is challenging for ground-based optical space domain awareness (SDA) algorithms as two objects close together along the line-of-sight can appear blended as a single object within the point-spread function (PSF) of the optical system. Traditional machine learning methods can be useful for differentiating between singular objects and closely-spaced objects, but many methods require large training sample sizes or high signal-to-noise conditions. The quality and quantity of realistic data make probabilistic classification methods a superior approach, as they are better suited to handle these data inadequacies. We present CSO classification results using the Gaussian process python package, MuyGPyS, and examine classification accuracy as a function of angular separation and magnitude difference between the simulated satellites. This orbit-independent analysis is done on highly accurate simulated SDA images that emulate realistic ground-based commercial-of-the-shelf (COTS) optical sensor observations of CSOs. We find that MuyGPyS outperforms traditional machine learning methods, especially under more challenging circumstances.
</details>
<details>
<summary>摘要</summary>
准确探测 rendezvous 和 proximity operations（RPO）是 espacial domain 中对 объек的行为理解的关键。然而， closely-spaced objects（CSO）的探测对地面上的 optical space domain awareness（SDA）算法是挑战的，因为两个 объек在视线上几乎相同的距离可以在 optic 系统的 point-spread function（PSF）中被混合为一个单一的 объек。传统的机器学习方法可以用于分 differentiating between singular objects and closely-spaced objects，但这些方法通常需要大量的训练样本或高的信号噪声比。 probablistic classification methods 是一种更加适合的方法，因为它们可以更好地处理这些数据不足。我们使用 Gaussian process python 包 MuyGPyS 进行 CSO 类别结果，并分析类别精度与两个 simulated satellite 的角度差和亮度差之间的关系。这是一种 orbit-independent 的分析，基于高度准确的 simulated SDA 图像，这些图像模拟了商用的 COTS 光学感知器观测 CSOs。我们发现 MuyGPyS 在更加挑战的情况下表现更好，特别是在更低的信号噪声比下。
</details></li>
</ul>
<hr>
<h2 id="OCT2Confocal-3D-CycleGAN-based-Translation-of-Retinal-OCT-Images-to-Confocal-Microscopy"><a href="#OCT2Confocal-3D-CycleGAN-based-Translation-of-Retinal-OCT-Images-to-Confocal-Microscopy" class="headerlink" title="OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to Confocal Microscopy"></a>OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to Confocal Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10902">http://arxiv.org/abs/2311.10902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Tian, Nantheera Anantrasirichai, Lindsay Nicholson, Alin Achim</li>
<li>for:  bridging the gap between in vivo OCT and ex vivo confocal microscopy imaging</li>
<li>methods:  developed a 3D CycleGAN framework for unsupervised translation of in vivo OCT to ex vivo confocal microscopy images</li>
<li>results:  effectively translates between 3D medical data domains, capturing vascular, textural, and cellular details with precision, outperforming existing methods despite limited data.<details>
<summary>Abstract</summary>
Optical coherence tomography (OCT) and confocal microscopy are pivotal in retinal imaging, each presenting unique benefits and limitations. In vivo OCT offers rapid, non-invasive imaging but can be hampered by clarity issues and motion artifacts. Ex vivo confocal microscopy provides high-resolution, cellular detailed color images but is invasive and poses ethical concerns and potential tissue damage. To bridge these modalities, we developed a 3D CycleGAN framework for unsupervised translation of in vivo OCT to ex vivo confocal microscopy images. Applied to our OCT2Confocal dataset, this framework effectively translates between 3D medical data domains, capturing vascular, textural, and cellular details with precision. This marks the first attempt to exploit the inherent 3D information of OCT and translate it into the rich, detailed color domain of confocal microscopy. Assessed through quantitative and qualitative metrics, the 3D CycleGAN framework demonstrates commendable image fidelity and quality, outperforming existing methods despite the constraints of limited data. This non-invasive generation of retinal confocal images has the potential to further enhance diagnostic and monitoring capabilities in ophthalmology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Point-Cloud-Self-supervised-Learning-via-3D-to-Multi-view-Masked-Autoencoder"><a href="#Point-Cloud-Self-supervised-Learning-via-3D-to-Multi-view-Masked-Autoencoder" class="headerlink" title="Point Cloud Self-supervised Learning via 3D to Multi-view Masked Autoencoder"></a>Point Cloud Self-supervised Learning via 3D to Multi-view Masked Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10887">http://arxiv.org/abs/2311.10887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhimin Chen, Yingwei Li, Longlong Jing, Liang Yang, Bing Li</li>
<li>for: 本研究目的是提出一种基于多视图特征的3D自助学习方法，以便更深入地理解3D结构。</li>
<li>methods: 我们提出了一种使用3D做masked autoencoder来全面利用多视图特征，并使用编码的 токен来生成原始点云和多视图深度图像。</li>
<li>results: 我们的方法在不同任务和设定下表现出色，并且在3D物体分类、少样本学习、部分分割和3D物体检测等多种下游任务中具有较高的表现。<details>
<summary>Abstract</summary>
In recent years, the field of 3D self-supervised learning has witnessed significant progress, resulting in the emergence of Multi-Modality Masked AutoEncoders (MAE) methods that leverage both 2D images and 3D point clouds for pre-training. However, a notable limitation of these approaches is that they do not fully utilize the multi-view attributes inherent in 3D point clouds, which is crucial for a deeper understanding of 3D structures. Building upon this insight, we introduce a novel approach employing a 3D to multi-view masked autoencoder to fully harness the multi-modal attributes of 3D point clouds. To be specific, our method uses the encoded tokens from 3D masked point clouds to generate original point clouds and multi-view depth images across various poses. This approach not only enriches the model's comprehension of geometric structures but also leverages the inherent multi-modal properties of point clouds. Our experiments illustrate the effectiveness of the proposed method for different tasks and under different settings. Remarkably, our method outperforms state-of-the-art counterparts by a large margin in a variety of downstream tasks, including 3D object classification, few-shot learning, part segmentation, and 3D object detection. Code will be available at: https://github.com/Zhimin-C/Multiview-MAE
</details>
<details>
<summary>摘要</summary>
近年来，3D自适应学习领域内，多模式做法（MAE）方法得到了显著进步，利用了2D图像和3D点云进行预训练。然而，这些方法并不完全利用3D点云中的多视角特征，这是深入理解3D结构的关键。基于这一点，我们提出了一种新的方法，使用3D到多视角做法（MAE）来全面利用3D点云的多模式特征。具体来说，我们的方法使用3D做法掩码图像中的编码符号来生成原始点云和多视角深度图像。这种方法不仅扩大了模型对几何结构的理解，还利用了点云的内在多模式特征。我们的实验表明，提议的方法在不同任务和设置下具有显著的优势，比如3D物体分类、几何学学习、部分分割和3D物体检测等多个下游任务。代码将在：https://github.com/Zhimin-C/Multiview-MAE 上提供。
</details></li>
</ul>
<hr>
<h2 id="A-Video-Based-Activity-Classification-of-Human-Pickers-in-Agriculture"><a href="#A-Video-Based-Activity-Classification-of-Human-Pickers-in-Agriculture" class="headerlink" title="A Video-Based Activity Classification of Human Pickers in Agriculture"></a>A Video-Based Activity Classification of Human Pickers in Agriculture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10885">http://arxiv.org/abs/2311.10885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishesh Pal, Antonio C. Leite, Jon G. O. Gjevestad, Pål J. From</li>
<li>for: This paper aims to improve the efficiency and productivity of harvesting operations in farming systems by developing an intelligent robotic system that can monitor human behavior, identify ongoing activities, and anticipate the worker’s needs.</li>
<li>methods: The proposed solution uses a combination of Mask Region-based Convolutional Neural Network (Mask R-CNN) for object detection, optical flow for motion estimation, and newly added statistical attributes of flow motion descriptors (Correlation Sensitivity, CS) to classify human activities in different agricultural scenarios.</li>
<li>results: The proposed framework is tested on in-house collected datasets from various crop fields, including strawberry polytunnels and apple tree orchards, and shows satisfactory results amidst challenges such as lighting variation, blur, and occlusions. The framework is evaluated using sensitivity, specificity, and accuracy measures, and the results demonstrate the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
In farming systems, harvesting operations are tedious, time- and resource-consuming tasks. Based on this, deploying a fleet of autonomous robots to work alongside farmworkers may provide vast productivity and logistics benefits. Then, an intelligent robotic system should monitor human behavior, identify the ongoing activities and anticipate the worker's needs. In this work, the main contribution consists of creating a benchmark model for video-based human pickers detection, classifying their activities to serve in harvesting operations for different agricultural scenarios. Our solution uses the combination of a Mask Region-based Convolutional Neural Network (Mask R-CNN) for object detection and optical flow for motion estimation with newly added statistical attributes of flow motion descriptors, named as Correlation Sensitivity (CS). A classification criterion is defined based on the Kernel Density Estimation (KDE) analysis and K-means clustering algorithm, which are implemented upon in-house collected dataset from different crop fields like strawberry polytunnels and apple tree orchards. The proposed framework is quantitatively analyzed using sensitivity, specificity, and accuracy measures and shows satisfactory results amidst various dataset challenges such as lighting variation, blur, and occlusions.
</details>
<details>
<summary>摘要</summary>
在农业系统中，收割操作是耗时耗源的任务。基于这点，投入一支自动驾驶机器人工作 alongside 农工可能提供广泛的生产力和物流利好。然后，一个智能机器人系统应该监测人类行为，识别当前活动并预测工作者的需求。在这种工作中，我们的主要贡献是创建一个视频基于人员检测的benchmark模型，并将其应用于不同的农业场景。我们的解决方案利用了掩模区域基于卷积神经网络（Mask R-CNN） для对象检测和运动场景的估计，并添加了新的统计特征，即相关敏感度（CS）。我们定义了一个基于饱和概率分布（KDE）分析和K-means归一化算法的分类准则，并在自己收集的 dataset 上进行了测试。我们的提出的框架在不同的 dataset 挑战下，如光线变化、模糊和遮挡等，也表现出了满意的结果。
</details></li>
</ul>
<hr>
<h2 id="Pre-to-Post-Contrast-Breast-MRI-Synthesis-for-Enhanced-Tumour-Segmentation"><a href="#Pre-to-Post-Contrast-Breast-MRI-Synthesis-for-Enhanced-Tumour-Segmentation" class="headerlink" title="Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation"></a>Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10879">http://arxiv.org/abs/2311.10879</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richardobi/pre_post_synthesis">https://github.com/richardobi/pre_post_synthesis</a></li>
<li>paper_authors: Richard Osuala, Smriti Joshi, Apostolia Tsirikoglou, Lidia Garrucho, Walter H. L. Pinaya, Oliver Diaz, Karim Lekadir</li>
<li>for: This paper aims to explore the feasibility of producing synthetic contrast enhancements for dynamic contrast-enhanced MRI (DCE-MRI) using a generative adversarial network (GAN).</li>
<li>methods: The authors use a GAN to translate pre-contrast T1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI sequence. They also introduce a Scaled Aggregate Measure (SAMe) to evaluate the quality of synthetic data.</li>
<li>results: The generated DCE-MRI data are assessed using quantitative image quality metrics and applied to the downstream task of 3D breast tumour segmentation. The results show that the synthetic data can enhance the robustness of breast tumour segmentation models via data augmentation.Here’s the summary in Simplified Chinese:</li>
<li>for: 这个研究旨在探讨使用生成 adversarial 网络 (GAN) 生成动态增强MRI (DCE-MRI) 的增强剂。</li>
<li>methods: 作者使用 GAN 将预contrast T1-weighted fat-saturated breast MRI 翻译成其对应的第一个 DCE-MRI 序列。他们还引入了一种Scale Aggregate Measure (SAMe) 评估生成数据的质量。</li>
<li>results: 生成的 DCE-MRI 数据通过量化图像质量指标进行评估，并应用于3D breast tumour segmentation 下游任务。结果表明，生成数据可以通过数据增强提高breast tumour segmentation 模型的Robustness。<details>
<summary>Abstract</summary>
Despite its benefits for tumour detection and treatment, the administration of contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated with a range of issues, including their invasiveness, bioaccumulation, and a risk of nephrogenic systemic fibrosis. This study explores the feasibility of producing synthetic contrast enhancements by translating pre-contrast T1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI sequence leveraging the capabilities of a generative adversarial network (GAN). Additionally, we introduce a Scaled Aggregate Measure (SAMe) designed for quantitatively evaluating the quality of synthetic data in a principled manner and serving as a basis for selecting the optimal generative model. We assess the generated DCE-MRI data using quantitative image quality metrics and apply them to the downstream task of 3D breast tumour segmentation. Our results highlight the potential of post-contrast DCE-MRI synthesis in enhancing the robustness of breast tumour segmentation models via data augmentation. Our code is available at https://github.com/RichardObi/pre_post_synthesis.
</details>
<details>
<summary>摘要</summary>
尽管对肿瘤检测和治疗具有优点，但是在动态增强磁共振成像（DCE-MRI）中 administraiting 对比剂具有一系列问题，包括其涉及性、堆积和肾生成性综合症风险。本研究探讨使用生成对抗网络（GAN）将预对磁共振成像（T1）转换为对应的第一个DCE-MRI序列的可能性，并引入一种量化评价生成数据的标准尺度（SAMe）。我们评估生成的DCE-MRI数据使用量化图像质量指标，并应用其到下游任务——三维乳腺肿瘤分割。我们的结果表明，通过数据增强，可以提高乳腺肿瘤分割模型的Robustness。我们的代码可以在https://github.com/RichardObi/pre_post_synthesis上获取。
</details></li>
</ul>
<hr>
<h2 id="Multi-entity-Video-Transformers-for-Fine-Grained-Video-Representation-Learning"><a href="#Multi-entity-Video-Transformers-for-Fine-Grained-Video-Representation-Learning" class="headerlink" title="Multi-entity Video Transformers for Fine-Grained Video Representation Learning"></a>Multi-entity Video Transformers for Fine-Grained Video Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10873">http://arxiv.org/abs/2311.10873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/video_rep_learning">https://github.com/facebookresearch/video_rep_learning</a></li>
<li>paper_authors: Matthew Walmer, Rose Kanjirathinkal, Kai Sheng Tai, Keyur Muzumdar, Taipeng Tian, Abhinav Shrivastava</li>
<li>for: 本研究旨在提高视频表示学习中的时间粒度表示学习，以便在时间很密集的任务中生成帧对帧的表示。</li>
<li>methods: 我们提出了一种自我监督的方法，通过在时间管道中更好地 интеGRATE空间信息来提高 transformer 架构的设计。我们的 Multi-entity Video Transformer（MV-Former）架构使用了无监督的 ViT 特征，并采用了多种策略来最大化提取的特征的utilty，而不需要细化 ViT 背部网络。这包括一种可学习的空间符号池化策略，用于从每帧中提取多个关键区域的特征。</li>
<li>results: 我们的实验表明，MV-Former 不仅超过了先前的自我监督方法，还超过了一些使用额外监督或训练数据的先前工作。当与 Kinetics-400 的额外训练数据结合使用时，MV-Former 又得到了进一步的性能提升。MV-Former 的代码可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
The area of temporally fine-grained video representation learning aims to generate frame-by-frame representations for temporally dense tasks. In this work, we advance the state-of-the-art for this area by re-examining the design of transformer architectures for video representation learning. A salient aspect of our self-supervised method is the improved integration of spatial information in the temporal pipeline by representing multiple entities per frame. Prior works use late fusion architectures that reduce frames to a single dimensional vector before any cross-frame information is shared, while our method represents each frame as a group of entities or tokens. Our Multi-entity Video Transformer (MV-Former) architecture achieves state-of-the-art results on multiple fine-grained video benchmarks. MV-Former leverages image features from self-supervised ViTs, and employs several strategies to maximize the utility of the extracted features while also avoiding the need to fine-tune the complex ViT backbone. This includes a Learnable Spatial Token Pooling strategy, which is used to identify and extract features for multiple salient regions per frame. Our experiments show that MV-Former not only outperforms previous self-supervised methods, but also surpasses some prior works that use additional supervision or training data. When combined with additional pre-training data from Kinetics-400, MV-Former achieves a further performance boost. The code for MV-Former is available at https://github.com/facebookresearch/video_rep_learning.
</details>
<details>
<summary>摘要</summary>
traditional Chinese:Temporally fine-grained video representation learning的领域目标是产生每帧的frame-by-frame表现，以便在时间紧密的任务中进行分析。在这个工作中，我们提高了时间精细video representation learning的状态艺术，通过重新评估 transformer架构的设计。我们的自我超vised方法之一的特点是在时间管线中更好地融合空间信息，通过每帧都 représent multiple entities或token。对于先前的works，他们使用输出frames的单一维度vector，然后在cross-frame信息交互之前进行简化，而我们的方法则是在每帧中represent多个entity或token。我们的 Multi-entity Video Transformer（MV-Former）架构在多个精细video benchmark上 achieved state-of-the-art results。MV-Former leverages自我超vised ViTs的图像特征，并运用多种策略来提高提取的特征之 utility，同时避免繁杂的 ViT 背部bone fine-tuning。包括学习的空间Token Pooling策略，用于在每帧中识别和提取多个焦点区域的特征。我们的实验显示，MV-Former不仅超过了先前的自我超vised方法，而且还超过了一些使用额外supervision或训练数据的先前工作。当与Kinetics-400的额外训练数据结合时，MV-Former又得到了进一步的性能提升。MV-Former的代码可以在https://github.com/facebookresearch/video_rep_learning 获取。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Digital-Rock-Image-Segmentation-with-a-Fine-Tuned-Segment-Anything-Model"><a href="#Zero-Shot-Digital-Rock-Image-Segmentation-with-a-Fine-Tuned-Segment-Anything-Model" class="headerlink" title="Zero-Shot Digital Rock Image Segmentation with a Fine-Tuned Segment Anything Model"></a>Zero-Shot Digital Rock Image Segmentation with a Fine-Tuned Segment Anything Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10865">http://arxiv.org/abs/2311.10865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyang Ma, Xupeng He, Shuyu Sun, Bicheng Yan, Hyung Kwak, Jun Gao</li>
<li>for: 这研究旨在提高石油和天然气提取效率，通过精准的图像分割，提高数字岩石模型的精度，进一步推进数字岩石物理学的理解。</li>
<li>methods: 这研究使用Meta AI的Segment Anything Model（SAM），并对其进行了微调，以优化参数和处理大规模图像，从而提高准确性。</li>
<li>results: 实验结果表明，微调后的SAM模型（RockSAM）在岩石CT&#x2F;SEM图像分割中表现出色，能够生成高质量的mask，从而提高数字岩石图像分析的效率和准确性。<details>
<summary>Abstract</summary>
Accurate image segmentation is crucial in reservoir modelling and material characterization, enhancing oil and gas extraction efficiency through detailed reservoir models. This precision offers insights into rock properties, advancing digital rock physics understanding. However, creating pixel-level annotations for complex CT and SEM rock images is challenging due to their size and low contrast, lengthening analysis time. This has spurred interest in advanced semi-supervised and unsupervised segmentation techniques in digital rock image analysis, promising more efficient, accurate, and less labour-intensive methods. Meta AI's Segment Anything Model (SAM) revolutionized image segmentation in 2023, offering interactive and automated segmentation with zero-shot capabilities, essential for digital rock physics with limited training data and complex image features. Despite its advanced features, SAM struggles with rock CT/SEM images due to their absence in its training set and the low-contrast nature of grayscale images. Our research fine-tunes SAM for rock CT/SEM image segmentation, optimizing parameters and handling large-scale images to improve accuracy. Experiments on rock CT and SEM images show that fine-tuning significantly enhances SAM's performance, enabling high-quality mask generation in digital rock image analysis. Our results demonstrate the feasibility and effectiveness of the fine-tuned SAM model (RockSAM) for rock images, offering segmentation without extensive training or complex labelling.
</details>
<details>
<summary>摘要</summary>
准确的图像分割是重要的在沟口模型和物质Characterization中，提高油气抽取效率的细节沟口模型。这种精度提供了岩石性质的启示，进而提高数字岩石物理理解。然而，为复杂的CT和SEM岩石图像创建像素级注释是困难的，因为它们的大小和低对比度，使分析时间增加。这种情况推动了数字岩石图像分析中进阶半supervised和无监督分割技术的兴趣，提供更高效、准确和 menos labor-intensive的方法。Meta AI的Segment Anything Model（SAM）在2023年革命化图像分割，提供了交互式和自动化分割，零 shot能力，对数字岩石物理进行有限训练数据和复杂图像特征是必需的。尽管它具有先进的特征，但SAM在岩石CT/SEM图像上很困难，因为它们缺失在其训练集中，以及图像的低对比度。我们的研究根据SAM进行了精度调整和大规模图像处理，以提高准确性。实验表明，精度调整可以大幅提高SAM的性能，使得高质量的面 Generation在数字岩石图像分析中。我们的结果证明了RockSAM模型的可行性和效果，为岩石图像分割提供了无需广泛训练或复杂标注的选择。
</details></li>
</ul>
<hr>
<h2 id="WATUNet-A-Deep-Neural-Network-for-Segmentation-of-Volumetric-Sweep-Imaging-Ultrasound"><a href="#WATUNet-A-Deep-Neural-Network-for-Segmentation-of-Volumetric-Sweep-Imaging-Ultrasound" class="headerlink" title="WATUNet: A Deep Neural Network for Segmentation of Volumetric Sweep Imaging Ultrasound"></a>WATUNet: A Deep Neural Network for Segmentation of Volumetric Sweep Imaging Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10857">http://arxiv.org/abs/2311.10857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donya Khaledyan, Thomas J. Marini, Avice OConnell, Steven Meng, Jonah Kan, Galen Brennan, Yu Zhao, Timothy M. Baran, Kevin J. Parker</li>
<li>for: 这个论文是为了提高乳腺癌诊断技术的研究。</li>
<li>methods: 这个研究使用了一种新的方法，即量子扫描干扰图像（VSI），它可以让不具备专业训练的操作员捕捉高质量的超声图像。这个方法与深度学习技术结合，可能地改善乳腺癌诊断的准确率、时间和成本，并提高病人的结果。</li>
<li>results: 这个研究使用了一种新的分割模型，即波峰注意网络（WATUNet），以提高分割模型的性能。研究结果表明，与其他深度网络相比，这个模型在两个 dataset 上的分割结果显著优于其他。在 VSI dataset 上，该模型的 dice 系数和 F1 分数分别为 0.94 和 0.94，而在公共 dataset 上，其分别为 0.93 和 0.94。<details>
<summary>Abstract</summary>
Objective. Limited access to breast cancer diagnosis globally leads to delayed treatment. Ultrasound, an effective yet underutilized method, requires specialized training for sonographers, which hinders its widespread use. Approach. Volume sweep imaging (VSI) is an innovative approach that enables untrained operators to capture high-quality ultrasound images. Combined with deep learning, like convolutional neural networks (CNNs), it can potentially transform breast cancer diagnosis, enhancing accuracy, saving time and costs, and improving patient outcomes. The widely used UNet architecture, known for medical image segmentation, has limitations, such as vanishing gradients and a lack of multi-scale feature extraction and selective region attention. In this study, we present a novel segmentation model known as Wavelet_Attention_UNet (WATUNet). In this model, we incorporate wavelet gates (WGs) and attention gates (AGs) between the encoder and decoder instead of a simple connection to overcome the limitations mentioned, thereby improving model performance. Main results. Two datasets are utilized for the analysis. The public "Breast Ultrasound Images" (BUSI) dataset of 780 images and a VSI dataset of 3818 images. Both datasets contained segmented lesions categorized into three types: no mass, benign mass, and malignant mass. Our segmentation results show superior performance compared to other deep networks. The proposed algorithm attained a Dice coefficient of 0.94 and an F1 score of 0.94 on the VSI dataset and scored 0.93 and 0.94 on the public dataset, respectively.
</details>
<details>
<summary>摘要</summary>
Approach:  to address this issue, we propose a novel approach called volume sweep imaging (VSI), which enables untrained operators to capture high-quality ultrasound images. We also use deep learning, specifically convolutional neural networks (CNNs), to improve breast cancer diagnosis. However, existing UNet architectures have limitations, such as vanishing gradients and a lack of multi-scale feature extraction and selective region attention.To overcome these limitations, we present a novel segmentation model called Wavelet_Attention_UNet (WATUNet). Our model incorporates wavelet gates (WGs) and attention gates (AGs) between the encoder and decoder, which improves model performance.Main results:  we evaluate our model on two datasets: the public "Breast Ultrasound Images" (BUSI) dataset of 780 images and a VSI dataset of 3818 images. Both datasets contain segmented lesions categorized into three types: no mass, benign mass, and malignant mass. Our segmentation results show superior performance compared to other deep networks. On the VSI dataset, the proposed algorithm attained a Dice coefficient of 0.94 and an F1 score of 0.94, while on the public dataset, it scored 0.93 and 0.94, respectively.
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-of-3D-Object-Detection-by-Density-Resampling"><a href="#Domain-Generalization-of-3D-Object-Detection-by-Density-Resampling" class="headerlink" title="Domain Generalization of 3D Object Detection by Density-Resampling"></a>Domain Generalization of 3D Object Detection by Density-Resampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10845">http://arxiv.org/abs/2311.10845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuangzhi Li, Lei Ma, Xingyu Li</li>
<li>for: 提高3D对象检测的通用性，增强对不同频谱域的检测性能。</li>
<li>methods: 我们提出了一种基于单个频谱的泛化（SDG）方法，包括一种新的数据增强方法和一种多任务学习策略。数据增强方法是物理感知基于的density-based增强方法，可以减少因点云密度而导致的性能下降。学习策略方面，我们开发了一种多任务学习方法，在源Training期间，除了主要的标准检测任务之外，还使用了一个辅助的自动化3D场景恢复任务，以提高Encoder对背景和前景细节的理解，从而提高对象检测的准确率。</li>
<li>results: 我们的方法在多个检测任务上（包括“车”, “人”和“自行车”检测）取得了优于state-of-the-art SDG方法和不supervised频谱适应方法的性能。代码将公开发布。<details>
<summary>Abstract</summary>
Point-cloud-based 3D object detection suffers from performance degradation when encountering data with novel domain gaps. To tackle it, the single-domain generalization (SDG) aims to generalize the detection model trained in a limited single source domain to perform robustly on unexplored domains. In this paper, we propose an SDG method to improve the generalizability of 3D object detection to unseen target domains. Unlike prior SDG works for 3D object detection solely focusing on data augmentation, our work introduces a novel data augmentation method and contributes a new multi-task learning strategy in the methodology. Specifically, from the perspective of data augmentation, we design a universal physical-aware density-based data augmentation (PDDA) method to mitigate the performance loss stemming from diverse point densities. From the learning methodology viewpoint, we develop a multi-task learning for 3D object detection: during source training, besides the main standard detection task, we leverage an auxiliary self-supervised 3D scene restoration task to enhance the comprehension of the encoder on background and foreground details for better recognition and detection of objects. Furthermore, based on the auxiliary self-supervised task, we propose the first test-time adaptation method for domain generalization of 3D object detection, which efficiently adjusts the encoder's parameters to adapt to unseen target domains during testing time, to further bridge domain gaps. Extensive cross-dataset experiments covering "Car", "Pedestrian", and "Cyclist" detections, demonstrate our method outperforms state-of-the-art SDG methods and even overpass unsupervised domain adaptation methods under some circumstances. The code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
“点云基于3D对象检测中的性能逐渐下降，这是因为数据域隔阶产生的域外挑战。为解决这问题，单域泛化（SDG）寻求将在有限单个源域中训练的检测模型扩展到未探索的域中进行稳健的检测。在这篇论文中，我们提出了一种SDG方法，以提高3D对象检测的泛化性能。与先前的SDG方法不同的是，我们不仅通过数据扩展来解决问题，还提出了一种新的多任务学习策略。”“从数据扩展的角度来看，我们设计了一种物理相关的点云数据扩展（PDDA）方法，以避免因点云密度差异而导致的性能下降。从学习方法的角度来看，我们开发了一种多任务学习方法，在源训练期间，除了主要的标准检测任务之外，我们还利用一个自动导向的3D场景恢复任务来增强encoder对背景和前景细节的理解，以便更好地识别和检测对象。”“此外，基于自动导向任务，我们提出了第一个测试时适应方法，用于域泛化3D对象检测的适应。在测试时，我们可以快速调整encoder的参数，以适应未探索的目标域，从而减少域外隔阶。广泛的跨数据集实验表明，我们的方法超过了状态艺术SDG方法，甚至在某些情况下超过了无监督域泛化方法。代码将在公共上公布。”
</details></li>
</ul>
<hr>
<h2 id="SelfEval-Leveraging-the-discriminative-nature-of-generative-models-for-evaluation"><a href="#SelfEval-Leveraging-the-discriminative-nature-of-generative-models-for-evaluation" class="headerlink" title="SelfEval: Leveraging the discriminative nature of generative models for evaluation"></a>SelfEval: Leveraging the discriminative nature of generative models for evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10708">http://arxiv.org/abs/2311.10708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sai Saketh Rambhatla, Ishan Misra</li>
<li>for: 这个论文旨在描述一种自动化评估文本图像生成模型的方法，以评估这些模型在多modal文本图像分类任务中的性能。</li>
<li>methods: 该方法使用文本图像生成模型计算图像的可能性，以直接应用于分类任务。</li>
<li>results: 该方法可以自动评估文本图像生成模型的性能，并且可以评估模型在 attribute binding、颜色识别、计数、形状识别和空间理解等任务中的表现。此外，该方法还可以评估生成模型在 Winoground 图像分数任务中的性能，并与权威评价相当。<details>
<summary>Abstract</summary>
In this work, we show that text-to-image generative models can be 'inverted' to assess their own text-image understanding capabilities in a completely automated manner.   Our method, called SelfEval, uses the generative model to compute the likelihood of real images given text prompts, making the generative model directly applicable to discriminative tasks.   Using SelfEval, we repurpose standard datasets created for evaluating multimodal text-image discriminative models to evaluate generative models in a fine-grained manner: assessing their performance on attribute binding, color recognition, counting, shape recognition, spatial understanding.   To the best of our knowledge SelfEval is the first automated metric to show a high degree of agreement for measuring text-faithfulness with the gold-standard human evaluations across multiple models and benchmarks.   Moreover, SelfEval enables us to evaluate generative models on challenging tasks such as Winoground image-score where they demonstrate competitive performance to discriminative models.   We also show severe drawbacks of standard automated metrics such as CLIP-score to measure text faithfulness on benchmarks such as DrawBench, and how SelfEval sidesteps these issues.   We hope SelfEval enables easy and reliable automated evaluation for diffusion models.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们显示了文本到图像生成模型可以被"反转"来评估它们自己的文本-图像理解能力，这是一种完全自动化的方法。  我们称之为SelfEval，它使用生成模型来计算文本提示给出的真实图像的概率，使生成模型直接适用于分类任务。  使用SelfEval，我们可以将标准的评估多媒体文本-图像模型的数据集重新用于评估生成模型，并且可以细化地评估它们的性能，包括Attribute binding、颜色识别、计数、形态识别和空间理解。  根据我们所知，SelfEval是首个自动化指标，能够与人工评估的标准金属板高度一致度的评估文本 faithfulness 多种模型和基准。  此外，SelfEval可以评估生成模型在Winoground图像分数任务上的竞争性表现，并且可以解决标准自动化指标如CLIP-score在DrawBench任务上的问题。  我们希望SelfEval可以帮助执行扩散模型的自动化评估。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Representation-Learning-by-Alternating-Unimodal-Adaptation"><a href="#Multimodal-Representation-Learning-by-Alternating-Unimodal-Adaptation" class="headerlink" title="Multimodal Representation Learning by Alternating Unimodal Adaptation"></a>Multimodal Representation Learning by Alternating Unimodal Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10707">http://arxiv.org/abs/2311.10707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Zhang, Jaehong Yoon, Mohit Bansal, Huaxiu Yao</li>
<li>for: addressing the challenge of dominant modalities in multimodal learning, improving performance in scenarios with complete and missing modalities</li>
<li>methods: alternating unimodal learning, shared head with continuous optimization, gradient modification mechanism for preventing information loss, test-time uncertainty-based model fusion</li>
<li>results: superior performance compared to prior approaches in extensive experiments on five diverse datasets<details>
<summary>Abstract</summary>
Multimodal learning, which integrates data from diverse sensory modes, plays a pivotal role in artificial intelligence. However, existing multimodal learning methods often struggle with challenges where some modalities appear more dominant than others during multimodal learning, resulting in suboptimal performance. To address this challenge, we propose MLA (Multimodal Learning with Alternating Unimodal Adaptation). MLA reframes the conventional joint multimodal learning process by transforming it into an alternating unimodal learning process, thereby minimizing interference between modalities. Simultaneously, it captures cross-modal interactions through a shared head, which undergoes continuous optimization across different modalities. This optimization process is controlled by a gradient modification mechanism to prevent the shared head from losing previously acquired information. During the inference phase, MLA utilizes a test-time uncertainty-based model fusion mechanism to integrate multimodal information. Extensive experiments are conducted on five diverse datasets, encompassing scenarios with complete modalities and scenarios with missing modalities. These experiments demonstrate the superiority of MLA over competing prior approaches.
</details>
<details>
<summary>摘要</summary>
多模式学习，它将多种感知模式的数据集成在人工智能中扮演着关键角色。然而，现有的多模式学习方法经常遇到一些模式在多模式学习中显得更加主导地，导致表现下降。为解决这个挑战，我们提议了MLA（多模式学习与交换单模式适应）。MLA将传统的共同多模式学习过程重新框定为交换单模式学习过程，从而减少模式之间的干扰。同时，它通过共享头来捕捉交叉模式交互，并在不同模式之间进行不断的优化。这个优化过程由梯度修正机制控制，以防止共享头失去先前获得的信息。在推断阶段，MLA通过测试时间不确定性基于模型融合机制来集成多模式信息。经验表明，MLA在五种多样化的数据集上比前一种方法更高效。
</details></li>
</ul>
<hr>
<h2 id="SplatArmor-Articulated-Gaussian-splatting-for-animatable-humans-from-monocular-RGB-videos"><a href="#SplatArmor-Articulated-Gaussian-splatting-for-animatable-humans-from-monocular-RGB-videos" class="headerlink" title="SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos"></a>SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10812">http://arxiv.org/abs/2311.10812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohit Jena, Ganesh Subramanian Iyer, Siddharth Choudhary, Brandon Smith, Pratik Chaudhari, James Gee</li>
<li>for: 这篇论文旨在提出一种新的人体模型恢复方法，以便实现可控的人体synthesis。</li>
<li>methods: 该方法使用3D Gaussians来 parameterize人体模型，并通过扩展SMPL几何体的皮肤来定义人体的动作。此外，该方法还使用SE(3)场来捕捉人体的 pose-dependent效果，并使用神经颜色场来提供颜色Regularization和3D超vision。</li>
<li>results: 该方法可以提供高质量的人体模型，并且可以在ZJU MoCap和People Snapshot数据集上达到惊人的效果。这些结果表明，Gaussian splatting是一种有趣的代替方法，可以使用笔制 primitives来实现人体synthesis，不会面临非 differentiability和优化问题。<details>
<summary>Abstract</summary>
We propose SplatArmor, a novel approach for recovering detailed and animatable human models by `armoring' a parameterized body model with 3D Gaussians. Our approach represents the human as a set of 3D Gaussians within a canonical space, whose articulation is defined by extending the skinning of the underlying SMPL geometry to arbitrary locations in the canonical space. To account for pose-dependent effects, we introduce a SE(3) field, which allows us to capture both the location and anisotropy of the Gaussians. Furthermore, we propose the use of a neural color field to provide color regularization and 3D supervision for the precise positioning of these Gaussians. We show that Gaussian splatting provides an interesting alternative to neural rendering based methods by leverging a rasterization primitive without facing any of the non-differentiability and optimization challenges typically faced in such approaches. The rasterization paradigms allows us to leverage forward skinning, and does not suffer from the ambiguities associated with inverse skinning and warping. We show compelling results on the ZJU MoCap and People Snapshot datasets, which underscore the effectiveness of our method for controllable human synthesis.
</details>
<details>
<summary>摘要</summary>
我们提出了SplatArmor，一种新的方法，用于recovering detailed和可动的人体模型。我们的方法使用3D Gaussian来“armor”一个参数化的体型模型，并将人体表示为一组3D Gaussian在一个坐标系中。这个坐标系的定义是通过扩展SMPLGeometry的皮肤来任意位置在坐标系中的扩展。为了考虑pose-dependent效果，我们引入了SE(3)场，以便捕捉Gaussian的位置和方向。此外，我们还提出了使用神经颜色场来提供颜色规则和3D超vision来精确位置Gaussian。我们表明，Gaussian splatting提供了一种有趣的代替方法，而不是基于神经网络渲染方法。这种渲染方法不受非导数和优化问题的影响，并且不受人体 inverse skinning和扭曲的困扰。我们在ZJU MoCap和People Snapshot数据集上展示了吸引人的结果，这些结果表明了我们的方法的可控性和可行性。
</details></li>
</ul>
<hr>
<h2 id="SpACNN-LDVAE-Spatial-Attention-Convolutional-Latent-Dirichlet-Variational-Autoencoder-for-Hyperspectral-Pixel-Unmixing"><a href="#SpACNN-LDVAE-Spatial-Attention-Convolutional-Latent-Dirichlet-Variational-Autoencoder-for-Hyperspectral-Pixel-Unmixing" class="headerlink" title="SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet Variational Autoencoder for Hyperspectral Pixel Unmixing"></a>SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet Variational Autoencoder for Hyperspectral Pixel Unmixing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10701">http://arxiv.org/abs/2311.10701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soham Chitnis, Kiran Mantripragada, Faisal Z. Qureshi</li>
<li>For: The paper proposes a method for hyperspectral unmixing, which aims to separate the pure spectral signals of underlying materials (endmembers) and their proportions (abundances) in a hyperspectral image (HSI).* Methods: The proposed method builds upon the Latent Dirichlet Variational Autoencoder (LDVAE) and incorporates an isotropic convolutional neural network (CNN) encoder with spatial attention to leverage the spatial information present in the HSI.* Results: The proposed method was evaluated on four datasets (Samson, Hydice Urban, Cuprite, and OnTech-HSI-Syn-21) and showed improvement in endmember extraction and abundance estimation by incorporating spatial information. The model was also trained on synthetic data and evaluated on real-world data for the Cuprite dataset, demonstrating the transfer learning paradigm.<details>
<summary>Abstract</summary>
The Hyperspectral Unxming problem is to find the pure spectral signal of the underlying materials (endmembers) and their proportions (abundances). The proposed method builds upon the recently proposed method, Latent Dirichlet Variational Autoencoder (LDVAE). It assumes that abundances can be encoded as Dirichlet Distributions while mixed pixels and endmembers are represented by Multivariate Normal Distributions. However, LDVAE does not leverage spatial information present in an HSI; we propose an Isotropic CNN encoder with spatial attention to solve the hyperspectral unmixing problem. We evaluated our model on Samson, Hydice Urban, Cuprite, and OnTech-HSI-Syn-21 datasets. Our model also leverages the transfer learning paradigm for Cuprite Dataset, where we train the model on synthetic data and evaluate it on real-world data. We are able to observe the improvement in the results for the endmember extraction and abundance estimation by incorporating the spatial information. Code can be found at https://github.com/faisalqureshi/cnn-ldvae
</details>
<details>
<summary>摘要</summary>
“干扰干扰干扰干扰”问题是找到背景材料（终端成员）的纯 spectral 信号和它们的含量（充足）。我们的方法建立在近期提出的方法，潜在 Dirichlet 自动化学习（LDVAE）之上。它假设了含量可以被编码为 Dirichlet 分布，混合像素和终端成员则是 Multivariate Normal 分布。但是， LDVAE 不利用高spectral 图像中的空间信息，我们提议使用ISO特征层 CNN 编码器和空间注意力来解决干扰问题。我们在 Samson、Hydice Urban、Cuprite 和 OnTech-HSI-Syn-21  dataset 上评估了我们的模型，并且利用了转移学习 paradigma 在 Cuprite dataset 上训练模型，然后评估在实际数据上。我们发现了在 incorporating 空间信息时，可以提高终端EXTRACTION 和含量估测的结果。代码可以在 GitHub 上找到：https://github.com/faisalqureshi/cnn-ldvae。
</details></li>
</ul>
<hr>
<h2 id="Versatile-Medical-Image-Segmentation-Learned-from-Multi-Source-Datasets-via-Model-Self-Disambiguation"><a href="#Versatile-Medical-Image-Segmentation-Learned-from-Multi-Source-Datasets-via-Model-Self-Disambiguation" class="headerlink" title="Versatile Medical Image Segmentation Learned from Multi-Source Datasets via Model Self-Disambiguation"></a>Versatile Medical Image Segmentation Learned from Multi-Source Datasets via Model Self-Disambiguation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10696">http://arxiv.org/abs/2311.10696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyang Chen, Hao Zheng, Yuemeng Li, Yuncong Ma, Liang Ma, Hongming Li, Yong Fan</li>
<li>for: 本研究旨在开发一种可靠、多Modal的医疗影像分割模型，以便更好地应用于各种医疗设备和协议下收集的影像数据。</li>
<li>methods: 我们采用了可cost-efficient的方法，利用 readily available的部分或甚至是缺失注解的分割标签。我们提出了自ambiguation、专业知识 incorporation和 imbalance mitigation 等策略，以解决不同来源的标签不一致性所带来的挑战。</li>
<li>results: 我们在基于八种不同来源的多Modal数据集上进行了实验，并证明了我们的方法的有效性和超越性。这些结果表明，我们的方法可以更好地利用现有的注解数据，并减少新数据的注解工作，以提高模型的能力。<details>
<summary>Abstract</summary>
A versatile medical image segmentation model applicable to imaging data collected with diverse equipment and protocols can facilitate model deployment and maintenance. However, building such a model typically requires a large, diverse, and fully annotated dataset, which is rarely available due to the labor-intensive and costly data curation. In this study, we develop a cost-efficient method by harnessing readily available data with partially or even sparsely annotated segmentation labels. We devise strategies for model self-disambiguation, prior knowledge incorporation, and imbalance mitigation to address challenges associated with inconsistently labeled data from various sources, including label ambiguity and imbalances across modalities, datasets, and segmentation labels. Experimental results on a multi-modal dataset compiled from eight different sources for abdominal organ segmentation have demonstrated our method's effectiveness and superior performance over alternative state-of-the-art methods, highlighting its potential for optimizing the use of existing annotated data and reducing the annotation efforts for new data to further enhance model capability.
</details>
<details>
<summary>摘要</summary>
一种通用的医疗影像分割模型，可以应用于不同设备和协议收集的影像数据，可以方便模型部署和维护。然而，建立这种模型通常需要一个大、多样化和完全注释的数据集，而这种数据集罕见地可以通过劳动密集和成本高的数据整理获得。在这种研究中，我们开发了一种经济高效的方法，利用可用的数据中具有部分或甚至缺失注释的分割标签。我们提出了自然语言处理技术、先前知识 integrate 和负面优化的策略，以解决来自不同来源的分割标签的不一致和模式、数据集和分割标签之间的差异。实验结果表明，我们的方法在多模态数据集上表现出色，并在与其他当前状态的方法进行比较中表现出超越性，这 highlights 我们的方法的潜在用于现有注释数据的优化和新数据的注释努力的减少，以进一步提高模型的能力。
</details></li>
</ul>
<hr>
<h2 id="3D-TexSeg-Unsupervised-Segmentation-of-3D-Texture-using-Mutual-Transformer-Learning"><a href="#3D-TexSeg-Unsupervised-Segmentation-of-3D-Texture-using-Mutual-Transformer-Learning" class="headerlink" title="3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual Transformer Learning"></a>3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual Transformer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10651">http://arxiv.org/abs/2311.10651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iyyakutti Iyappan Ganapathi, Fayaz Ali, Sajid Javed, Syed Sadaf Ali, Naoufel Werghi</li>
<li>for: 本研究旨在提出一种无监督的3D文本分割方法，用于分割3D模型表面上的文本特征。</li>
<li>methods: 该方法基于变换器模型，包括一个标签生成器和一个清洁器。这两个模型使用三维图像表示法处理 mesh 表面 Facet，并在循环互学学习机制下对其进行标签。</li>
<li>results: 实验结果表明，提出的方法可以在三个公共可用数据集上以外，与标准和state-of-the-art无监督方法相比，并与监督方法相竞争。<details>
<summary>Abstract</summary>
Analysis of the 3D Texture is indispensable for various tasks, such as retrieval, segmentation, classification, and inspection of sculptures, knitted fabrics, and biological tissues. A 3D texture is a locally repeated surface variation independent of the surface's overall shape and can be determined using the local neighborhood and its characteristics. Existing techniques typically employ computer vision techniques that analyze a 3D mesh globally, derive features, and then utilize the obtained features for retrieval or classification. Several traditional and learning-based methods exist in the literature, however, only a few are on 3D texture, and nothing yet, to the best of our knowledge, on the unsupervised schemes. This paper presents an original framework for the unsupervised segmentation of the 3D texture on the mesh manifold. We approach this problem as binary surface segmentation, partitioning the mesh surface into textured and non-textured regions without prior annotation. We devise a mutual transformer-based system comprising a label generator and a cleaner. The two models take geometric image representations of the surface mesh facets and label them as texture or non-texture across an iterative mutual learning scheme. Extensive experiments on three publicly available datasets with diverse texture patterns demonstrate that the proposed framework outperforms standard and SOTA unsupervised techniques and competes reasonably with supervised methods.
</details>
<details>
<summary>摘要</summary>
analysis of 3D texture 是必备的 для多种任务，如采集、分割、分类和生物组织诊断。 3D texture 是一种本地重复的表面变化，不受表面整体形状的影响，可以通过地方 neighborhood 和其特征来确定。现有的技术通常使用计算机视觉技术，分析全球的 3D 网格，提取特征，然后使用获得的特征进行采集或分类。文献中有一些传统的方法和学习基于的方法，但只有一些是3D texture，而没有任何一个是基于无监督方案。本文提出了一个原创的无监督分割方案，将 mesh 表面分割成文本化和非文本化区域，无需先有注释。我们设计了一个基于 transformer 的系统，包括标签生成器和清洁器两部分。两个模型在迭代的互助学习方案中，对 mesh 表面 Facet 的 геометрической图像表示进行标注。我们对三个公共可用的数据集进行了广泛的实验，结果显示，提出的方案可以超过标准和 SOTA 无监督方法，并与指导方法相匹配。
</details></li>
</ul>
<hr>
<h2 id="Self-trained-Panoptic-Segmentation"><a href="#Self-trained-Panoptic-Segmentation" class="headerlink" title="Self-trained Panoptic Segmentation"></a>Self-trained Panoptic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10648">http://arxiv.org/abs/2311.10648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shourya Verma</li>
<li>for: 这个研究的目的是提出一种基于嵌入的自监督掌景分割方法，以便在假数据和无标签数据的情况下提高掌景分割模型的性能。</li>
<li>methods: 该方法使用了一种基于嵌入的自监督方法，通过在Synthetic和Real两个频谱中进行自适应训练，以便在Synthetic和Real两个频谱中进行分割。</li>
<li>results: 研究表明，该方法可以在Synthetic和Real两个频谱中提高掌景分割模型的性能，并且可以在不同的频谱之间进行准确的适应。<details>
<summary>Abstract</summary>
Panoptic segmentation is an important computer vision task which combines semantic and instance segmentation. It plays a crucial role in domains of medical image analysis, self-driving vehicles, and robotics by providing a comprehensive understanding of visual environments. Traditionally, deep learning panoptic segmentation models have relied on dense and accurately annotated training data, which is expensive and time consuming to obtain. Recent advancements in self-supervised learning approaches have shown great potential in leveraging synthetic and unlabelled data to generate pseudo-labels using self-training to improve the performance of instance and semantic segmentation models. The three available methods for self-supervised panoptic segmentation use proposal-based transformer architectures which are computationally expensive, complicated and engineered for specific tasks. The aim of this work is to develop a framework to perform embedding-based self-supervised panoptic segmentation using self-training in a synthetic-to-real domain adaptation problem setting.
</details>
<details>
<summary>摘要</summary>
《泛opeptic segmentation是计算机视觉任务之一，它将semantic segmentation和instance segmentation融合在一起，在医学影像分析、自动驾驶和机器人等领域具有重要作用。传统的深度学习泛opeptic segmentation模型通常需要大量和准确的标注训练数据，这是expensive和时间consuming的。现在的self-supervised learning方法已经 показа出了很好的潜力，可以使用synthetic和无标签数据生成pseudo-labels，以提高instance和semantic segmentation模型的性能。现有的三种方法 для自我超vised泛opeptic segmentation都使用提案based transformer架构，这些架构是 computationally expensive, complicated和engineered for specific tasks。本研究的目标是开发一个框架，可以在synthetic-to-real domain adaptation问题上进行嵌入基于自我超vised泛opeptic segmentation，使用self-training。》Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Astronomical-Images-Quality-Assessment-with-Automated-Machine-Learning"><a href="#Astronomical-Images-Quality-Assessment-with-Automated-Machine-Learning" class="headerlink" title="Astronomical Images Quality Assessment with Automated Machine Learning"></a>Astronomical Images Quality Assessment with Automated Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10617">http://arxiv.org/abs/2311.10617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Parisot, Pierrick Bruneau, Patrik Hitzelberger</li>
<li>for: 这个论文是为了探讨电子助力天文学中图像质量评估的应用。</li>
<li>methods: 该论文使用了自动化机器学习模型来评估天文图像质量。</li>
<li>results: 研究人员通过使用自动化机器学习模型，成功地自动评估了天文图像质量。<details>
<summary>Abstract</summary>
Electronically Assisted Astronomy consists in capturing deep sky images with a digital camera coupled to a telescope to display views of celestial objects that would have been invisible through direct observation. This practice generates a large quantity of data, which may then be enhanced with dedicated image editing software after observation sessions. In this study, we show how Image Quality Assessment can be useful for automatically rating astronomical images, and we also develop a dedicated model by using Automated Machine Learning.
</details>
<details>
<summary>摘要</summary>
电子助力天文学包括使用数字摄像头与望远镜相结合，以显示天体对象的深空图像，这些图像可能通过直接观察无法看到。这种做法生成了大量数据，可以使用专门的图像修复软件进行优化 после观察会。在这项研究中，我们展示了如何使用图像质量评估来自动评分天文图像，并开发了一个专门的自动机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="CA-Jaccard-Camera-aware-Jaccard-Distance-for-Person-Re-identification"><a href="#CA-Jaccard-Camera-aware-Jaccard-Distance-for-Person-Re-identification" class="headerlink" title="CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification"></a>CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10605">http://arxiv.org/abs/2311.10605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyu Chen, Zheyi Fan, Zhaoru Chen, Yixuan Zhu</li>
<li>for: 人体重复识别 (re-ID) 是一个复杂的任务，旨在学习人体特征以实现人体 Retrieval。</li>
<li>methods: 我们提出了一种新的相机意识（CA）Jaccard距离，利用相机信息提高Jaccard距离的可靠性。我们引入了相机意识k-相似最近邻居（CKRNNs）和相机意识本地查询扩展（CLQE）来提高相关邻居的可靠性和考虑相机变化的强制约束。</li>
<li>results: 我们的CA-Jaccard距离是一种简单又有效的距离度量，可以高效地提高人体重复识别方法的可靠性和低计算成本。我们在实验中证明了我们的方法的效果。<details>
<summary>Abstract</summary>
Person re-identification (re-ID) is a challenging task that aims to learn discriminative features for person retrieval. In person re-ID, Jaccard distance is a widely used distance metric, especially in re-ranking and clustering scenarios. However, we discover that camera variation has a significant negative impact on the reliability of Jaccard distance. In particular, Jaccard distance calculates the distance based on the overlap of relevant neighbors. Due to camera variation, intra-camera samples dominate the relevant neighbors, which reduces the reliability of the neighbors by introducing intra-camera negative samples and excluding inter-camera positive samples. To overcome this problem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that leverages camera information to enhance the reliability of Jaccard distance. Specifically, we introduce camera-aware k-reciprocal nearest neighbors (CKRNNs) to find k-reciprocal nearest neighbors on the intra-camera and inter-camera ranking lists, which improves the reliability of relevant neighbors and guarantees the contribution of inter-camera samples in the overlap. Moreover, we propose a camera-aware local query expansion (CLQE) to exploit camera variation as a strong constraint to mine reliable samples in relevant neighbors and assign these samples higher weights in overlap to further improve the reliability. Our CA-Jaccard distance is simple yet effective and can serve as a general distance metric for person re-ID methods with high reliability and low computational cost. Extensive experiments demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
人体重认识（re-ID）是一项具有挑战性的任务，旨在学习人体特征的识别特征。在人体重认识中，Jacard距离是广泛使用的距离度量，特别是在重新排序和聚合场景中。然而，我们发现了相机变化对Jaccard距离的可靠性的印象。具体来说，Jaccard距离根据相机变化引入了内相机负样本和排除了间相机正样本，从而减少了相机变化对Jaccard距离的可靠性。为解决这个问题，我们提出了一种新的相机意识Jacard（CA-Jaccard）距离，利用相机信息来提高Jaccard距离的可靠性。特别是，我们引入相机意识k-最相似邻居（CKRNNs），以找到k-最相似邻居在内相机和间相机排名列表上，从而改善相机变化对Jaccard距离的可靠性。此外，我们提出了相机意识地本查询扩展（CLQE），以利用相机变化作为强制约束，挖掘可靠的样本，并将这些样本在重合中分配更高的权重，以进一步提高可靠性。我们的CA-Jaccard距离简单又有效，可以作为人体重认识方法中的一种高可靠性低计算成本的距离度量。广泛的实验证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Indoor-Localization-Using-Crowdsourced-Radio-Maps"><a href="#Multimodal-Indoor-Localization-Using-Crowdsourced-Radio-Maps" class="headerlink" title="Multimodal Indoor Localization Using Crowdsourced Radio Maps"></a>Multimodal Indoor Localization Using Crowdsourced Radio Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10601">http://arxiv.org/abs/2311.10601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoguang Yi, Xiangyu Wen, Qiyue Xia, Peize Li, Francisco Zampella, Firas Alsehly, Chris Xiaoxuan Lu</li>
<li>for: 这篇论文旨在替代传统的indoorPositioning Systems（IPS）中使用建筑物的floor plans，而是利用人们所持有的智能手机和WiFi启动的Robots所生成的广泛来源Radio Maps。</li>
<li>methods: 这篇论文提出了一种新的框架，该框架利用一种不确定性感知神经网络模型和一种特制的 bayesian融合技术来解决广泛来源Radio Maps的不准确和罕见的问题。</li>
<li>results: 对多个实际场景进行了广泛的评估，结果显示该系统可以减少 ~ 25%的性能提升，与最佳参考值相比。<details>
<summary>Abstract</summary>
Indoor Positioning Systems (IPS) traditionally rely on odometry and building infrastructures like WiFi, often supplemented by building floor plans for increased accuracy. However, the limitation of floor plans in terms of availability and timeliness of updates challenges their wide applicability. In contrast, the proliferation of smartphones and WiFi-enabled robots has made crowdsourced radio maps - databases pairing locations with their corresponding Received Signal Strengths (RSS) - increasingly accessible. These radio maps not only provide WiFi fingerprint-location pairs but encode movement regularities akin to the constraints imposed by floor plans. This work investigates the possibility of leveraging these radio maps as a substitute for floor plans in multimodal IPS. We introduce a new framework to address the challenges of radio map inaccuracies and sparse coverage. Our proposed system integrates an uncertainty-aware neural network model for WiFi localization and a bespoken Bayesian fusion technique for optimal fusion. Extensive evaluations on multiple real-world sites indicate a significant performance enhancement, with results showing ~ 25% improvement over the best baseline
</details>
<details>
<summary>摘要</summary>
室内定位系统（IPS）传统上依靠速度和建筑物的WiFi基础设施，经常补充了建筑物的 floor plan，以提高准确性。然而， floor plan 的有效性和时效性的限制使其广泛应用受到挑战。相比之下，智能手机和WIFI启动的机器人的普及，使得来自众生的广播电子地图 - 将位置对应到接收信号强度（RSS）的数据库 - 变得越来越可 accessible。这些广播电子地图不仅提供WIFI指纹-位置对应，还编码了运动规律，类似于建筑物的制约。这项工作探讨了使用这些广播电子地图作为 floor plan 的替代品在多模态 IPS 中的可能性。我们提出了一种新的框架，以解决广播电子地图的不准确和罕见覆盖问题。我们的提议的系统通过不确定性感知神经网络模型和特定的 Bayesian 融合技术来实现优质融合。对多个实际场景进行了广泛的评估，结果显示了大约 25% 的性能提升，与最佳基eline相比。
</details></li>
</ul>
<hr>
<h2 id="Detection-d’objets-celestes-dans-des-images-astronomiques-par-IA-explicable"><a href="#Detection-d’objets-celestes-dans-des-images-astronomiques-par-IA-explicable" class="headerlink" title="Détection d’objets célestes dans des images astronomiques par IA explicable"></a>Détection d’objets célestes dans des images astronomiques par IA explicable</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10592">http://arxiv.org/abs/2311.10592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Parisot, Mahmoud Jaziri</li>
<li>for: 这个研究是为了自动检测捕获的天体对象（如星系、星云、或球状星团）的存在和位置。</li>
<li>methods: 该研究使用了可解释的人工智能方法来检测捕获的天体对象。</li>
<li>results: 该研究可以自动检测捕获的天体对象的存在和位置，并提供了可解释的结果。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Amateur and professional astronomers can easily capture a large number of deep sky images with recent smart telescopes. However, afterwards verification is still required to check whether the celestial objects targeted are actually visible in the images produced. Depending on the magnitude of the targets, the observation conditions and the time during which the data is captured, it is possible that only stars are present in the images. In this study, we propose an approach based on explainable Artificial Intelligence to automatically detect the presence and position of captured objects. -- --   Gr\^ace \`a l'apport des t\'elescopes automatis\'es grand public, les astronomes amateurs et professionnels peuvent capturer facilement une grande quantit\'e d'images du ciel profond (comme par exemple les galaxies, n\'ebuleuses, ou amas globulaires). N\'eanmoins, une v\'erification reste n\'ecessaire \`a post\'eriori pour v\'erifier si les objets c\'elestes vis\'es sont effectivement visibles dans les images produites: cela d\'epend notamment de la magnitude des cibles, des conditions d'observation mais aussi de la dur\'ee pendant laquelle les donn\'ees sont captur\'ees. Dans cette \'etude, nous proposons une approche bas\'ee sur l'IA explicable pour d\'etecter automatiquement la pr\'esence et la position des objets captur\'es.
</details>
<details>
<summary>摘要</summary>
<<SYS>>现代智能望远镜可以轻松地 capture 大量深空图像，但是需要 posteriori 验证以确认targeted 的星系是否实际存在在图像中。这取决于目标星系的亮度、观测条件以及Capture 时间。在本研究中，我们提出一种基于可解释的人工智能方法来自动检测captured 对象的存在和位置。---感谢公共大众望远镜的贡献，天文爱好者和专业天文学家可以轻松地捕捉大量深空图像，如 галактиcas, 星系、或球状星团。然而，需要 posteriori 验证以确认targeted 的星系是否实际存在在图像中：这取决于目标星系的亮度、观测条件以及Capture 时间。在本研究中，我们提出一种基于可解释的人工智能方法来自动检测captured 对象的存在和位置。
</details></li>
</ul>
<hr>
<h2 id="Human-motion-trajectory-prediction-using-the-Social-Force-Model-for-real-time-and-low-computational-cost-applications"><a href="#Human-motion-trajectory-prediction-using-the-Social-Force-Model-for-real-time-and-low-computational-cost-applications" class="headerlink" title="Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications"></a>Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10582">http://arxiv.org/abs/2311.10582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Gil, Alberto Sanfeliu</li>
<li>for: 这篇论文的目的是提出一种新的人体动量预测模型，即Social Force Generative Adversarial Network (SoFGAN)，用于人机合作任务中的预测人体动量，如陪伴、引导或接近等任务。</li>
<li>methods: 这篇论文使用了Generative Adversarial Network (GAN)和Social Force Model (SFM)组合来生成不同的可能的人体动量，以避免在场景中的碰撞。此外，还添加了一个Conditional Variational Autoencoder (CVAE)模块，以强调目的学习。</li>
<li>results: 根据UCY或BIWI数据集的实验结果表示，我们的方法在预测方面比大多数当前状态的方法更加准确，同时也比其他方法减少了碰撞的风险。此外，我们还实现了在实时无GPU的情况下，使用这种模型进行高质量的预测，而且计算成本较低。<details>
<summary>Abstract</summary>
Human motion trajectory prediction is a very important functionality for human-robot collaboration, specifically in accompanying, guiding, or approaching tasks, but also in social robotics, self-driving vehicles, or security systems. In this paper, a novel trajectory prediction model, Social Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate different plausible people trajectories reducing collisions in a scene. Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to emphasize the destination learning. We show that our method is more accurate in making predictions in UCY or BIWI datasets than most of the current state-of-the-art models and also reduces collisions in comparison to other approaches. Through real-life experiments, we demonstrate that the model can be used in real-time without GPU's to perform good quality predictions with a low computational cost.
</details>
<details>
<summary>摘要</summary>
人体运动轨迹预测是人机合作中非常重要的功能，尤其在陪伴、引导或接近任务中，也在社交机器人、自动驾驶车或安全系统中。在这篇论文中，我们提出了一种新的轨迹预测模型，即社交力生成抗拒网络（SoFGAN）。SoFGAN使用生成抗拒网络（GAN）和社交力模型（SFM）生成不同的可能的人体轨迹，以降低场景中的碰撞。此外，我们添加了一个条件可变自动编码器（CVAE）模块，以强调目标学习。我们表明，我们的方法在UCY或BIWI数据集上的预测比现有的大多数状态对模型更准确，并且与其他方法相比，减少了碰撞。通过实际实验，我们示示了该模型可以在实时无需GPU进行高质量预测，并且计算成本较低。
</details></li>
</ul>
<hr>
<h2 id="SSB-Simple-but-Strong-Baseline-for-Boosting-Performance-of-Open-Set-Semi-Supervised-Learning"><a href="#SSB-Simple-but-Strong-Baseline-for-Boosting-Performance-of-Open-Set-Semi-Supervised-Learning" class="headerlink" title="SSB: Simple but Strong Baseline for Boosting Performance of Open-Set Semi-Supervised Learning"></a>SSB: Simple but Strong Baseline for Boosting Performance of Open-Set Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10572">http://arxiv.org/abs/2311.10572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Fan, Anna Kukleva, Dengxin Dai, Bernt Schiele</li>
<li>for: 本文研究了开放集成学习（SSL）方法在开放集成场景中的应用，即使用不含标签数据来提高模型的泛化性。</li>
<li>methods: 本文提出了一种叫做简单强基线（SSB）的方法，它利用高置信度的 pseudo-标签数据来提高准确分类，并利用非线性变换将特征分配给多任务学习框架中的准确分类和异常检测两个任务。此外，本文还提出了 pseudo-负数据挖掘技术，以进一步提高异常检测性能。</li>
<li>results: 实验表明，SSB 方法可以大幅提高准确分类和异常检测性能，在开放集成场景中准确分类率高达 97.5%，与现有方法相比差距很大。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) methods effectively leverage unlabeled data to improve model generalization. However, SSL models often underperform in open-set scenarios, where unlabeled data contain outliers from novel categories that do not appear in the labeled set. In this paper, we study the challenging and realistic open-set SSL setting, where the goal is to both correctly classify inliers and to detect outliers. Intuitively, the inlier classifier should be trained on inlier data only. However, we find that inlier classification performance can be largely improved by incorporating high-confidence pseudo-labeled data, regardless of whether they are inliers or outliers. Also, we propose to utilize non-linear transformations to separate the features used for inlier classification and outlier detection in the multi-task learning framework, preventing adverse effects between them. Additionally, we introduce pseudo-negative mining, which further boosts outlier detection performance. The three ingredients lead to what we call Simple but Strong Baseline (SSB) for open-set SSL. In experiments, SSB greatly improves both inlier classification and outlier detection performance, outperforming existing methods by a large margin. Our code will be released at https://github.com/YUE-FAN/SSB.
</details>
<details>
<summary>摘要</summary>
semi-supervised learning（SSL）方法可以有效地利用无标签数据来提高模型的泛化性。然而，SSL模型经常在开放集成分类情况下表现不佳，因为无标签数据中可能包含来自新类别的异常数据。在这篇论文中，我们研究了开放集成SSL设定，其目标是同时正确地分类归类数据和检测异常数据。intuitively，归类器应该只在归类数据上训练。然而，我们发现归类性能可以通过包含高信任 Pseudo-标注数据来大幅提高，无论它们是归类数据还是异常数据。此外，我们提议利用非线性变换来分离在多任务学习框架中使用的归类和异常检测的特征，避免它们之间的干扰。此外，我们引入 Pseudo-负样本采集，进一步提高异常检测性能。这三个元素导致我们提出的简单强基线（SSB）方法，在实验中对开放集成SSL方法进行了大幅改进。我们的代码将在https://github.com/YUE-FAN/SSB上发布。
</details></li>
</ul>
<hr>
<h2 id="Phase-Guided-Light-Field-for-Spatial-Depth-High-Resolution-3D-Imaging"><a href="#Phase-Guided-Light-Field-for-Spatial-Depth-High-Resolution-3D-Imaging" class="headerlink" title="Phase Guided Light Field for Spatial-Depth High Resolution 3D Imaging"></a>Phase Guided Light Field for Spatial-Depth High Resolution 3D Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10568">http://arxiv.org/abs/2311.10568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geyou Zhang, Ce Zhu, Kai Liu, Yipeng Liu</li>
<li>for: 提高单shot光场相机的空间分辨率和深度准确性</li>
<li>methods: 使用光学投影机制作高频单相频偏振Pattern，并提出相位导向光场算法以提高光场相机的空间分辨率和深度准确性</li>
<li>results: 实验结果显示，相比现有活动光场方法，提出的方法可以在单shot光场相机上重建3D点云，并且提高了空间分辨率10倍，保持同高深度分辨率，仅需要一组高频单相频偏振Pattern。<details>
<summary>Abstract</summary>
On 3D imaging, light field cameras typically are of single shot, and however, they heavily suffer from low spatial resolution and depth accuracy. In this paper, by employing an optical projector to project a group of single high-frequency phase-shifted sinusoid patterns, we propose a phase guided light field algorithm to significantly improve both the spatial and depth resolutions for off-the-shelf light field cameras. First, for correcting the axial aberrations caused by the main lens of our light field camera, we propose a deformed cone model to calibrate our structured light field system. Second, over wrapped phases computed from patterned images, we propose a stereo matching algorithm, i.e. phase guided sum of absolute difference, to robustly obtain the correspondence for each pair of neighbored two lenslets. Finally, by introducing a virtual camera according to the basic geometrical optics of light field imaging, we propose a reorganization strategy to reconstruct 3D point clouds with spatial-depth high resolution. Experimental results show that, compared with the state-of-the-art active light field methods, the proposed reconstructs 3D point clouds with a spatial resolution of 1280$\times$720 with factors 10$\times$ increased, while maintaining the same high depth resolution and needing merely a single group of high-frequency patterns.
</details>
<details>
<summary>摘要</summary>
对3D成像，光场相机通常是单步的，但它们受到低空间分辨率和深度准确性的压制。在这篇论文中，我们通过使用光学投影机 проек加入一组单高频相位偏移的振荡模式，提出了相位导航光场算法，以提高各种光场相机的空间和深度分辨率。首先，为了正确地纠正主镜的轴向扭曲，我们提出了扭曲杯模型来准确地calibrate我们的结构化光场系统。其次，通过对 Patterned 图像中的卷绕相位进行匹配，我们提出了相位导航差分差分析法，以稳定地获取每对邻居两个镜头的匹配。最后，通过引入基本光学的光场投影机，我们提出了重新组织策略，以重建3D点云的空间深度高分辨率。实验结果表明，相比state-of-the-art的活动光场方法，我们的提议可以重建3D点云，空间分辨率为1280×720，同时保持高深度分辨率，只需要一组高频模式。
</details></li>
</ul>
<hr>
<h2 id="Archtree-on-the-fly-tree-structured-exploration-for-latency-aware-pruning-of-deep-neural-networks"><a href="#Archtree-on-the-fly-tree-structured-exploration-for-latency-aware-pruning-of-deep-neural-networks" class="headerlink" title="Archtree: on-the-fly tree-structured exploration for latency-aware pruning of deep neural networks"></a>Archtree: on-the-fly tree-structured exploration for latency-aware pruning of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10549">http://arxiv.org/abs/2311.10549</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KBerghaus/class_der">https://github.com/KBerghaus/class_der</a></li>
<li>paper_authors: Rémi Ouazan Reboul, Edouard Yvinec, Arnaud Dapogny, Kevin Bailly</li>
<li>for: 这篇论文目的是为了提出一种基于硬件约束的延迟驱动结构化剪辑方法，以提高深度神经网络（DNN）的执行效率。</li>
<li>methods: 这篇论文使用了一种基于树结构的搜索方法，即Archtree，以同时探索多个候选遗传子模型的空间。此外，它还包括在目标硬件上实时估计延迟，以更加准确地考虑硬件约束。</li>
<li>results: 实验结果表明，Archtree方法可以更好地保持原始模型的准确性，同时更好地适应延迟预算。相比之下，现有的状态 искусственный方法都不如Archtree方法。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have become ubiquitous in addressing a number of problems, particularly in computer vision. However, DNN inference is computationally intensive, which can be prohibitive e.g. when considering edge devices. To solve this problem, a popular solution is DNN pruning, and more so structured pruning, where coherent computational blocks (e.g. channels for convolutional networks) are removed: as an exhaustive search of the space of pruned sub-models is intractable in practice, channels are typically removed iteratively based on an importance estimation heuristic. Recently, promising latency-aware pruning methods were proposed, where channels are removed until the network reaches a target budget of wall-clock latency pre-emptively estimated on specific hardware. In this paper, we present Archtree, a novel method for latency-driven structured pruning of DNNs. Archtree explores multiple candidate pruned sub-models in parallel in a tree-like fashion, allowing for a better exploration of the search space. Furthermore, it involves on-the-fly latency estimation on the target hardware, accounting for closer latencies as compared to the specified budget. Empirical results on several DNN architectures and target hardware show that Archtree better preserves the original model accuracy while better fitting the latency budget as compared to existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Recently, promising latency-aware pruning methods have been proposed, where channels are removed until the network reaches a target budget of wall-clock latency pre-emptively estimated on specific hardware. In this paper, we present Archtree, a novel method for latency-driven structured pruning of DNNs. Archtree explores multiple candidate pruned sub-models in parallel in a tree-like fashion, allowing for a better exploration of the search space. Furthermore, it involves on-the-fly latency estimation on the target hardware, accounting for closer latencies as compared to the specified budget.Empirical results on several DNN architectures and target hardware show that Archtree better preserves the original model accuracy while better fitting the latency budget as compared to existing state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Joint-covariance-property-under-geometric-image-transformations-for-spatio-temporal-receptive-fields-according-to-the-generalized-Gaussian-derivative-model-for-visual-receptive-fields"><a href="#Joint-covariance-property-under-geometric-image-transformations-for-spatio-temporal-receptive-fields-according-to-the-generalized-Gaussian-derivative-model-for-visual-receptive-fields" class="headerlink" title="Joint covariance property under geometric image transformations for spatio-temporal receptive fields according to the generalized Gaussian derivative model for visual receptive fields"></a>Joint covariance property under geometric image transformations for spatio-temporal receptive fields according to the generalized Gaussian derivative model for visual receptive fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10543">http://arxiv.org/abs/2311.10543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tony Lindeberg</li>
<li>for: 本研究探讨了自然图像变换对视觉操作的影响，尤其是在计算机视觉和生物视觉中。</li>
<li>methods: 本文使用了geometry image transformations的covariance性质来表达图像操作的稳定性和高层次视觉操作的 invariancy。</li>
<li>results: 本文提出了一种joint covariance property，该性质可以描述不同类型的图像变换如何相互交互，并且提供了匹配输出与下游spatio-temporal receptive fields的参数需要如何变换以实现图像操作的稳定性。<details>
<summary>Abstract</summary>
The influence of natural image transformations on receptive field responses is crucial for modelling visual operations in computer vision and biological vision. In this regard, covariance properties with respect to geometric image transformations in the earliest layers of the visual hierarchy are essential for expressing robust image operations and for formulating invariant visual operations at higher levels. This paper defines and proves a joint covariance property under compositions of spatial scaling transformations, spatial affine transformations, Galilean transformations and temporal scaling transformations, which makes it possible to characterize how different types of image transformations interact with each other. Specifically, the derived relations show how the receptive field parameters need to be transformed, in order to match the output from spatio-temporal receptive fields with the underlying spatio-temporal image transformations.
</details>
<details>
<summary>摘要</summary>
自然图像变换对视觉运算的影响是计算机视觉和生物视觉中关键的一部分。在这种情况下，图像层次结构的最初层的covariance属性对于表达 Robust 图像运算和高层次的抗变换视觉操作是关键的。本文定义并证明了在作用于图像的空间缩放变换、空间乘数变换、加利列安变换和时间缩放变换的复合作用下，covariance属性的共同性。具体来说， derivated 关系表明了感知场参数如何对应于图像变换，以实现匹配下来的输出与背景图像变换。
</details></li>
</ul>
<hr>
<h2 id="Segment-Anything-Model-with-Uncertainty-Rectification-for-Auto-Prompting-Medical-Image-Segmentation"><a href="#Segment-Anything-Model-with-Uncertainty-Rectification-for-Auto-Prompting-Medical-Image-Segmentation" class="headerlink" title="Segment Anything Model with Uncertainty Rectification for Auto-Prompting Medical Image Segmentation"></a>Segment Anything Model with Uncertainty Rectification for Auto-Prompting Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10529">http://arxiv.org/abs/2311.10529</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YichiZhang98/UR-SAM">https://github.com/YichiZhang98/UR-SAM</a></li>
<li>paper_authors: Yichi Zhang, Shiyao Hu, Chen Jiang, Yuan Cheng, Yuan Qi</li>
<li>for: 提高自动提示医疗图像分割的可靠性和Robustness</li>
<li>methods: 使用提高分割性能的指示map和不确定性修正模块</li>
<li>results: 在两个公共的3D医疗图像数据集上，无需额外训练或精度调整，our方法可以进一步提高分割性能，达到最高达10.7%和13.8%的 dice相似度，表明our方法具有效果和广泛的应用前提。<details>
<summary>Abstract</summary>
The introduction of the Segment Anything Model (SAM) has marked a significant advancement in prompt-driven image segmentation. However, SAM's application to medical image segmentation requires manual prompting of target structures to obtain acceptable performance, which is still labor-intensive. Despite attempts of auto-prompting to turn SAM into a fully automatic manner, it still exhibits subpar performance and lacks of reliability in the field of medical imaging. In this paper, we propose UR-SAM, an uncertainty rectified SAM framework to enhance the robustness and reliability for auto-prompting medical image segmentation. Our method incorporates a prompt augmentation module to estimate the distribution of predictions and generate uncertainty maps, and an uncertainty-based rectification module to further enhance the performance of SAM. Extensive experiments on two public 3D medical datasets covering the segmentation of 35 organs demonstrate that without supplementary training or fine-tuning, our method further improves the segmentation performance with up to 10.7 % and 13.8 % in dice similarity coefficient, demonstrating efficiency and broad capabilities for medical image segmentation without manual prompting.
</details>
<details>
<summary>摘要</summary>
《Introduction of Segment Anything Model (SAM) has brought significant advancements in prompt-driven image segmentation. However, applying SAM to medical image segmentation still requires manual prompting of target structures, which is labor-intensive. Despite attempts to turn SAM into a fully automatic manner, its performance is still subpar and unreliable in medical imaging. In this paper, we propose UR-SAM, an uncertainty rectified SAM framework to enhance the robustness and reliability of auto-prompting medical image segmentation. Our method incorporates a prompt augmentation module to estimate the distribution of predictions and generate uncertainty maps, and an uncertainty-based rectification module to further enhance the performance of SAM. Extensive experiments on two public 3D medical datasets covering the segmentation of 35 organs show that our method can improve segmentation performance by up to 10.7% and 13.8% in dice similarity coefficient without supplementary training or fine-tuning, demonstrating efficiency and broad capabilities for medical image segmentation without manual prompting.》Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Removing-Adverse-Volumetric-Effects-From-Trained-Neural-Radiance-Fields"><a href="#Removing-Adverse-Volumetric-Effects-From-Trained-Neural-Radiance-Fields" class="headerlink" title="Removing Adverse Volumetric Effects From Trained Neural Radiance Fields"></a>Removing Adverse Volumetric Effects From Trained Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10523">http://arxiv.org/abs/2311.10523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas L. Teigen, Mauhing Yip, Victor P. Hamran, Vegard Skui, Annette Stahl, Rudolf Mester</li>
<li>for: 本文探讨了使用神经辐射场（NeRF）在雾瑞环境中的应用，并提出了一种方法来除去雾瑞。</li>
<li>methods: 本文提出了一种基于场景全局强度的方法，可以在生成新视图时除去雾瑞。此外，本文还引入了一个新的数据集，用于测试NeRF在雾瑞环境中的性能。</li>
<li>results: 本文通过视频结果示示了使用NeRF来渲染雾瑞环境中的 объек 的 ClearView 效果。<details>
<summary>Abstract</summary>
While the use of neural radiance fields (NeRFs) in different challenging settings has been explored, only very recently have there been any contributions that focus on the use of NeRF in foggy environments. We argue that the traditional NeRF models are able to replicate scenes filled with fog and propose a method to remove the fog when synthesizing novel views. By calculating the global contrast of a scene, we can estimate a density threshold that, when applied, removes all visible fog. This makes it possible to use NeRF as a way of rendering clear views of objects of interest located in fog-filled environments. Additionally, to benchmark performance on such scenes, we introduce a new dataset that expands some of the original synthetic NeRF scenes through the addition of fog and natural environments. The code, dataset, and video results can be found on our project page: https://vegardskui.com/fognerf/
</details>
<details>
<summary>摘要</summary>
traditional NeRF models 可以复制foggy scenes，我们提出了一种方法来从synthesized views中移除fog。通过计算场景的全局对比度，我们可以估算一个density threshold，当应用于场景时，可以完全remove all visible fog。这使得我们可以使用NeRF来渲染fog-filled environments中的 объекts of interest的清晰视图。此外，为了评估这些场景的性能，我们引入了一个新的 dataset，该dataset通过fog和自然环境的添加扩展了一些原始的synthetic NeRF scenes。我们的代码、dataset和视频结果可以在我们项目页面上找到：https://vegardskui.com/fognerf/
</details></li>
</ul>
<hr>
<h2 id="Mind-the-map-Accounting-for-existing-map-information-when-estimating-online-HDMaps-from-sensor-data"><a href="#Mind-the-map-Accounting-for-existing-map-information-when-estimating-online-HDMaps-from-sensor-data" class="headerlink" title="Mind the map! Accounting for existing map information when estimating online HDMaps from sensor data"></a>Mind the map! Accounting for existing map information when estimating online HDMaps from sensor data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10517">http://arxiv.org/abs/2311.10517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hustvl/maptr">https://github.com/hustvl/maptr</a></li>
<li>paper_authors: Rémy Sun, Li Yang, Diane Lingrand, Frédéric Precioso</li>
<li>for: 提高在感知器上的高清晰地图（HDMap）估计，以便减轻自动驾驶系统中HDMap的手动获取成本，并可能扩展其使用范围。</li>
<li>methods: 利用已有地图，提高在线HDMap估计。提出3种有用的现有地图类型（最小主义、噪声、过时），并介绍MapEX框架，该框架通过编码地图元素为查询token，并改进了训练классиic查询基地图估计模型的匹配算法。</li>
<li>results: 在nuScenes数据集上实现了显著的改进，比如使用噪声地图时，MapEX比MapTRv2探测器提高38%，比当前SOTA提高16%。<details>
<summary>Abstract</summary>
Online High Definition Map (HDMap) estimation from sensors offers a low-cost alternative to manually acquired HDMaps. As such, it promises to lighten costs for already HDMap-reliant Autonomous Driving systems, and potentially even spread their use to new systems. In this paper, we propose to improve online HDMap estimation by accounting for already existing maps. We identify 3 reasonable types of useful existing maps (minimalist, noisy, and outdated). We also introduce MapEX, a novel online HDMap estimation framework that accounts for existing maps. MapEX achieves this by encoding map elements into query tokens and by refining the matching algorithm used to train classic query based map estimation models. We demonstrate that MapEX brings significant improvements on the nuScenes dataset. For instance, MapEX - given noisy maps - improves by 38% over the MapTRv2 detector it is based on and by 16% over the current SOTA.
</details>
<details>
<summary>摘要</summary>
“在线高清地图（HDMap）估算从感知器件提供了一种低成本的替代方案，以减轻现有HDMap-依赖的自动驾驶系统的成本，并可能扩展其使用至新的系统。在这篇论文中，我们提议改进在线HDMap估算，考虑现有地图的价值。我们确定了三种有用的现有地图类型（简化、噪音、过时），并引入了MapEX，一种新的在线HDMap估算框架。MapEX通过编码地图元素为查询token，并通过改进基于类传统查询基于地图估算模型的匹配算法来实现。我们示示了MapEX在nuScenes数据集上带来了显著改进，比如，MapEX（基于噪音地图）与MapTRv2探测器相比提高38%，与当前SOTA相比提高16%。”Note: "HDMap" in the text refers to "High-Definition Map".
</details></li>
</ul>
<hr>
<h2 id="A-Framework-of-Landsat-8-Band-Selection-based-on-UMDA-for-Deforestation-Detection"><a href="#A-Framework-of-Landsat-8-Band-Selection-based-on-UMDA-for-Deforestation-Detection" class="headerlink" title="A Framework of Landsat-8 Band Selection based on UMDA for Deforestation Detection"></a>A Framework of Landsat-8 Band Selection based on UMDA for Deforestation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10513">http://arxiv.org/abs/2311.10513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo B. Neto, Paulo R. C. Pedro, Alvaro Fazenda, Fabio A. Faria</li>
<li>for: 这项研究旨在提出一种新的框架，用于监测热带雨林。</li>
<li>methods: 该研究使用分布统计算法（UMDA）选择来自Landstat-8的 спектраль带，以提高对除森林区域的识别。</li>
<li>results: 实验表明，使用最佳组合（651）可以达到90%以上的准确率，并且比其他所有组合相比，效率和效果更高。<details>
<summary>Abstract</summary>
The conservation of tropical forests is a current subject of social and ecological relevance due to their crucial role in the global ecosystem. Unfortunately, millions of hectares are deforested and degraded each year. Therefore, government or private initiatives are needed for monitoring tropical forests. In this sense, this work proposes a novel framework, which uses of distribution estimation algorithm (UMDA) to select spectral bands from Landsat-8 that yield a better representation of deforestation areas to guide a semantic segmentation architecture called DeepLabv3+. In performed experiments, it was possible to find several compositions that reach balanced accuracy superior to 90% in segment classification tasks. Furthermore, the best composition (651) found by UMDA algorithm fed the DeepLabv3+ architecture and surpassed in efficiency and effectiveness all compositions compared in this work.
</details>
<details>
<summary>摘要</summary>
保护热带雨林是当前社会和生态领域的热点话题，因为它们在全球生态系统中扮演了关键角色。然而，每年仍有数百万公顷的雨林被毁灭和侵蚀。因此，政府或私人的倡议是必要的，以监测热带雨林。在这种情况下，本工作提出了一个新的框架，使用分布Estimation算法（UMDA）选择LandSat-8遥感器中的spectral Band，以更好地表示Deforestation区域，并用DeepLabv3+ semanticsegmentation架构进行分类。在实验中，能够找到许多compositions，其中balanced accuracy超过90%的分类任务。此外，最佳Composition（651）由UMDA算法选择，并将DeepLabv3+架构feed，在效率和效果方面超过了所有相比的Compositions。
</details></li>
</ul>
<hr>
<h2 id="A-Relay-System-for-Semantic-Image-Transmission-based-on-Shared-Feature-Extraction-and-Hyperprior-Entropy-Compression"><a href="#A-Relay-System-for-Semantic-Image-Transmission-based-on-Shared-Feature-Extraction-and-Hyperprior-Entropy-Compression" class="headerlink" title="A Relay System for Semantic Image Transmission based on Shared Feature Extraction and Hyperprior Entropy Compression"></a>A Relay System for Semantic Image Transmission based on Shared Feature Extraction and Hyperprior Entropy Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10492">http://arxiv.org/abs/2311.10492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wannian An, Zhicheng Bao, Haotai Liang, Chen Dong, Xiaodong</li>
<li>for: 提高图像重建和修复的质量</li>
<li>methods: 使用共享特征提取技术和幂噪 entropy压缩（HEC）技术</li>
<li>results: 相比其他最新研究方法，提出的系统具有较低的传输开销和更高的 semantic 图像传输性能，特别是在同样的条件下，相对比较方法的多尺度结构相似度（MS-SSIM）高出约0.2。Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 提高图像重建和修复的质量</li>
<li>methods: 使用共享特征提取技术和幂噪 entropy压缩（HEC）技术</li>
<li>results: 相比其他最新研究方法，提出的系统具有较低的传输开销和更高的 semantic 图像传输性能。<details>
<summary>Abstract</summary>
Nowadays, the need for high-quality image reconstruction and restoration is more and more urgent. However, most image transmission systems may suffer from image quality degradation or transmission interruption in the face of interference such as channel noise and link fading. To solve this problem, a relay communication network for semantic image transmission based on shared feature extraction and hyperprior entropy compression (HEC) is proposed, where the shared feature extraction technology based on Pearson correlation is proposed to eliminate partial shared feature of extracted semantic latent feature. In addition, the HEC technology is used to resist the effect of channel noise and link fading and carried out respectively at the source node and the relay node. Experimental results demonstrate that compared with other recent research methods, the proposed system has lower transmission overhead and higher semantic image transmission performance. Particularly, under the same conditions, the multi-scale structural similarity (MS-SSIM) of this system is superior to the comparison method by approximately 0.2.
</details>
<details>
<summary>摘要</summary>
现在，高品质图像重建和修复的需求越来越紧迫。然而，大多数图像传输系统可能会受到频率干扰和链接损坏的影响，导致图像质量下降。为解决这个问题，一种基于共享特征提取和超凡 entropy压缩（HEC）的关键点通信网络 для semantics 图像传输是提出的，其中基于皮尔逊相关性的共享特征提取技术用于消除部分共享特征。此外，HEC技术在源节点和关键节点进行了分别应用，以抵御频率干扰和链接损坏的影响。实验结果表明，相比其他最近的研究方法，提出的系统具有较低的传输 overhead 和较高的semantics 图像传输性能。特别是，在同样的条件下，该系统的多尺度结构相似度（MS-SSIM）比对比方法高出约0.2。
</details></li>
</ul>
<hr>
<h2 id="FRCSyn-Challenge-at-WACV-2024-Face-Recognition-Challenge-in-the-Era-of-Synthetic-Data"><a href="#FRCSyn-Challenge-at-WACV-2024-Face-Recognition-Challenge-in-the-Era-of-Synthetic-Data" class="headerlink" title="FRCSyn Challenge at WACV 2024:Face Recognition Challenge in the Era of Synthetic Data"></a>FRCSyn Challenge at WACV 2024:Face Recognition Challenge in the Era of Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10476">http://arxiv.org/abs/2311.10476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ndido98/frcsyn">https://github.com/ndido98/frcsyn</a></li>
<li>paper_authors: Pietro Melzi, Ruben Tolosana, Ruben Vera-Rodriguez, Minchul Kim, Christian Rathgeb, Xiaoming Liu, Ivan DeAndres-Tame, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia, Weisong Zhao, Xiangyu Zhu, Zheyu Yan, Xiao-Yu Zhang, Jinlin Wu, Zhen Lei, Suvidha Tripathi, Mahak Kothari, Md Haider Zama, Debayan Deb, Bernardo Biesseck, Pedro Vidal, Roger Granada, Guilherme Fickel, Gustavo Führ, David Menotti, Alexander Unnervik, Anjith George, Christophe Ecabert, Hatef Otroshi Shahreza, Parsa Rahimi, Sébastien Marcel, Ioannis Sarridis, Christos Koutlis, Georgia Baltsou, Symeon Papadopoulos, Christos Diou, Nicolò Di Domenico, Guido Borghi, Lorenzo Pellegrini, Enrique Mas-Candela, Ángela Sánchez-Pérez, Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, Mirko Marras</li>
<li>for: 这篇论文旨在探讨面 recognition技术在假数据 Era中的挑战，以及如何通过使用假数据来解决现有技术的限制。</li>
<li>methods: 这篇论文使用了一个国际性的 Face Recognition Challenge in the Era of Synthetic Data (FRCSyn)，以探讨假数据在面 recognition技术中的应用。</li>
<li>results: 根据这篇论文的结果，使用假数据可以有效地解决面 recognition技术中的数据隐私问题、人种偏见、未经见过的场景推理、以及面部pose和 occlusion 等挑战。<details>
<summary>Abstract</summary>
Despite the widespread adoption of face recognition technology around the world, and its remarkable performance on current benchmarks, there are still several challenges that must be covered in more detail. This paper offers an overview of the Face Recognition Challenge in the Era of Synthetic Data (FRCSyn) organized at WACV 2024. This is the first international challenge aiming to explore the use of synthetic data in face recognition to address existing limitations in the technology. Specifically, the FRCSyn Challenge targets concerns related to data privacy issues, demographic biases, generalization to unseen scenarios, and performance limitations in challenging scenarios, including significant age disparities between enrollment and testing, pose variations, and occlusions. The results achieved in the FRCSyn Challenge, together with the proposed benchmark, contribute significantly to the application of synthetic data to improve face recognition technology.
</details>
<details>
<summary>摘要</summary>
尽管全球范围内普及的人脸识别技术表现出色，但还有一些挑战需要更加详细地考虑。这篇文章提供了WACV 2024年举行的人脸识别挑战（FRCSyn）的概述。这是首个使用合成数据探索人脸识别技术的国际挑战。具体来说，FRCSyn挑战旨在解决现有技术中的数据隐私问题、人口偏见、未经见测enario推理和场景复杂性等问题。包括年龄差距、拍摄角度变化、 occlusion等情况下的性能 limitation。FRCSyn挑战的结果，以及提议的标准化程序，对于使用合成数据提升人脸识别技术具有重要 significanse。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-autoencoding-architecture-for-the-simultaneous-generation-of-medical-images-and-corresponding-segmentation-masks"><a href="#End-to-end-autoencoding-architecture-for-the-simultaneous-generation-of-medical-images-and-corresponding-segmentation-masks" class="headerlink" title="End-to-end autoencoding architecture for the simultaneous generation of medical images and corresponding segmentation masks"></a>End-to-end autoencoding architecture for the simultaneous generation of medical images and corresponding segmentation masks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10472">http://arxiv.org/abs/2311.10472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Pierre Vera, Su Ruan</li>
<li>for: 这篇论文的目的是提出一个基于希尔伯特统计力学自适应网络（HVAE）的终端架构，以提高医疗影像分类中的训练数据产生和实验结果。</li>
<li>methods: 这篇论文使用的方法是基于HVAE的终端架构，实现更好的 posterior distribution 推测，并且与传统的Variational Autoencoders（VAE）相比，具有更高的图像生成质量。</li>
<li>results: 这篇论文的结果显示，在拥有少量数据的情况下，这个方法可以超越对抗性模型，实现更好的图像质量和精确的肿瘤标识生成。实验结果显示，这个方法在不同的医疗影像模式下都具有良好的效果。<details>
<summary>Abstract</summary>
Despite the increasing use of deep learning in medical image segmentation, acquiring sufficient training data remains a challenge in the medical field. In response, data augmentation techniques have been proposed; however, the generation of diverse and realistic medical images and their corresponding masks remains a difficult task, especially when working with insufficient training sets. To address these limitations, we present an end-to-end architecture based on the Hamiltonian Variational Autoencoder (HVAE). This approach yields an improved posterior distribution approximation compared to traditional Variational Autoencoders (VAE), resulting in higher image generation quality. Our method outperforms generative adversarial architectures under data-scarce conditions, showcasing enhancements in image quality and precise tumor mask synthesis. We conduct experiments on two publicly available datasets, MICCAI's Brain Tumor Segmentation Challenge (BRATS), and Head and Neck Tumor Segmentation Challenge (HECKTOR), demonstrating the effectiveness of our method on different medical imaging modalities.
</details>
<details>
<summary>摘要</summary>
尽管深度学习在医学图像分割中得到了广泛应用，但获取充足的训练数据仍然是医疗领域的挑战。为应对这些限制，数据增强技术被提出，但生成真实和多样化的医学图像和其相对应的掩码仍然是一项困难任务，特别是在训练集较少的情况下。为解决这些限制，我们提出了基于希尔伯特变量自动机（HVAE）的端到端架构。这种方法可以在训练集较少的情况下提供更好的 posterior distribution 近似，从而提高图像生成质量。我们的方法在数据缺乏情况下比generative adversarial网络（GAN）表现出色，展现出了图像质量和精准肿瘤掩码生成的改进。我们在公共数据集 BRATS 和 HECKTOR 上进行了实验，证明了我们的方法在不同的医学成像模式下的效果。
</details></li>
</ul>
<hr>
<h2 id="Correlation-Distance-Graph-Learning-for-Treatment-Response-Prediction-from-rs-fMRI"><a href="#Correlation-Distance-Graph-Learning-for-Treatment-Response-Prediction-from-rs-fMRI" class="headerlink" title="Correlation-Distance Graph Learning for Treatment Response Prediction from rs-fMRI"></a>Correlation-Distance Graph Learning for Treatment Response Prediction from rs-fMRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10463">http://arxiv.org/abs/2311.10463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/summerwings/cdgin">https://github.com/summerwings/cdgin</a></li>
<li>paper_authors: Xiatian Zhang, Sisi Zheng, Hubert P. H. Shum, Haozheng Zhang, Nan Song, Mingkang Song, Hongxiao Jia</li>
<li>for: 该研究旨在提高resting-state fMRI（rs-fMRI）功能连接分析的严格应用，以推断药物反应。</li>
<li>methods: 该研究提出了一种图学习框架，通过对相似度和距离基于的神经相似度进行集成，以实现更加准确地捕捉脑动态特征，并且可以更好地预测药物反应。</li>
<li>results: 实验结果表明，该方法在 Chronic pain 和 depersonalization disorder 数据集上都有出色的表现，并且超过了当前方法的表现。<details>
<summary>Abstract</summary>
Resting-state fMRI (rs-fMRI) functional connectivity (FC) analysis provides valuable insights into the relationships between different brain regions and their potential implications for neurological or psychiatric disorders. However, specific design efforts to predict treatment response from rs-fMRI remain limited due to difficulties in understanding the current brain state and the underlying mechanisms driving the observed patterns, which limited the clinical application of rs-fMRI. To overcome that, we propose a graph learning framework that captures comprehensive features by integrating both correlation and distance-based similarity measures under a contrastive loss. This approach results in a more expressive framework that captures brain dynamic features at different scales and enables more accurate prediction of treatment response. Our experiments on the chronic pain and depersonalization disorder datasets demonstrate that our proposed method outperforms current methods in different scenarios. To the best of our knowledge, we are the first to explore the integration of distance-based and correlation-based neural similarity into graph learning for treatment response prediction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>resting-state fMRI（rs-fMRI）功能连接（FC）分析为脑区之间关系提供了有价值的信息，但是特定的设计努力用于预测治疗响应从rs-fMRI中仍然有限，这主要归结于Current brain state和下面的机制难以理解，这限制了rs-fMRI在临床应用中的使用。为了解决这个问题，我们提议一种图学学习框架，该框架可以捕捉包括相互关联和距离基于相似度度量在内的全面特征。这种方法具有更加表达力的优点，可以捕捉脑动态特征在不同尺度上，并且可以更准确地预测治疗响应。我们的实验表明，在折磨症和人格分裂症数据集上，我们的提议方法在不同的场景中都超过了当前方法。到目前为止，我们是首次将距离基于和相互关联基于的神经相似度 интегри进图学学习中，以预测治疗响应。
</details></li>
</ul>
<hr>
<h2 id="DeepClean-Machine-Unlearning-on-the-Cheap-by-Resetting-Privacy-Sensitive-Weights-using-the-Fisher-Diagonal"><a href="#DeepClean-Machine-Unlearning-on-the-Cheap-by-Resetting-Privacy-Sensitive-Weights-using-the-Fisher-Diagonal" class="headerlink" title="DeepClean: Machine Unlearning on the Cheap by Resetting Privacy Sensitive Weights using the Fisher Diagonal"></a>DeepClean: Machine Unlearning on the Cheap by Resetting Privacy Sensitive Weights using the Fisher Diagonal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10448">http://arxiv.org/abs/2311.10448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaeli Shi, Najah Ghalyan, Kostis Gourgoulias, John Buford, Sean Moran</li>
<li>for: 保护隐私信息，避免机器学习模型意外吸收和泄露敏感信息。</li>
<li>methods: 使用 Fisher Information Matrix (FIM) 实现选择性忘记，而不需要全面重训练或大量矩阵逆函数计算。</li>
<li>results: 实验表明，我们的算法可以成功忘记任意选择的训练数据subset，并且可以在不同的神经网络架构上实现。<details>
<summary>Abstract</summary>
Machine learning models trained on sensitive or private data can inadvertently memorize and leak that information. Machine unlearning seeks to retroactively remove such details from model weights to protect privacy. We contribute a lightweight unlearning algorithm that leverages the Fisher Information Matrix (FIM) for selective forgetting. Prior work in this area requires full retraining or large matrix inversions, which are computationally expensive. Our key insight is that the diagonal elements of the FIM, which measure the sensitivity of log-likelihood to changes in weights, contain sufficient information for effective forgetting. Specifically, we compute the FIM diagonal over two subsets -- the data to retain and forget -- for all trainable weights. This diagonal representation approximates the complete FIM while dramatically reducing computation. We then use it to selectively update weights to maximize forgetting of the sensitive subset while minimizing impact on the retained subset. Experiments show that our algorithm can successfully forget any randomly selected subsets of training data across neural network architectures. By leveraging the FIM diagonal, our approach provides an interpretable, lightweight, and efficient solution for machine unlearning with practical privacy benefits.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DUA-DA-Distillation-based-Unbiased-Alignment-for-Domain-Adaptive-Object-Detection"><a href="#DUA-DA-Distillation-based-Unbiased-Alignment-for-Domain-Adaptive-Object-Detection" class="headerlink" title="DUA-DA: Distillation-based Unbiased Alignment for Domain Adaptive Object Detection"></a>DUA-DA: Distillation-based Unbiased Alignment for Domain Adaptive Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10437">http://arxiv.org/abs/2311.10437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongchao Feng, Shiwei Li, Yingjie Gao, Ziyue Huang, Yanan Zhang, Qingjie Liu, Yunhong Wang</li>
<li>for: 提高韦度预测的域隔适应性和精度</li>
<li>methods: 使用分立老师模型和目标相关物理网络，对域隔适应对象检测进行准则整合和域准确性提高</li>
<li>results: 在跨域场景下，提高了域隔适应对象检测的精度和一致性，并大幅超越了现有的准则整合方法<details>
<summary>Abstract</summary>
Though feature-alignment based Domain Adaptive Object Detection (DAOD) have achieved remarkable progress, they ignore the source bias issue, i.e. the aligned features are more favorable towards the source domain, leading to a sub-optimal adaptation. Furthermore, the presence of domain shift between the source and target domains exacerbates the problem of inconsistent classification and localization in general detection pipelines. To overcome these challenges, we propose a novel Distillation-based Unbiased Alignment (DUA) framework for DAOD, which can distill the source features towards a more balanced position via a pre-trained teacher model during the training process, alleviating the problem of source bias effectively. In addition, we design a Target-Relevant Object Localization Network (TROLN), which can mine target-related knowledge to produce two classification-free metrics (IoU and centerness). Accordingly, we implement a Domain-aware Consistency Enhancing (DCE) strategy that utilizes these two metrics to further refine classification confidences, achieving a harmonization between classification and localization in cross-domain scenarios. Extensive experiments have been conducted to manifest the effectiveness of this method, which consistently improves the strong baseline by large margins, outperforming existing alignment-based works.
</details>
<details>
<summary>摘要</summary>
尽管基于特征对齐的领域适应物体检测（DAOD）已经取得了显著的进步，但它们忽略了源偏见问题，即对齐的特征更加偏向源频道，导致不优化适应。此外，在源和目标频道之间的频道变化使得检测总体的一致性和地方化准确性受到影响。为了解决这些挑战，我们提出了一种基于凝固的不偏投对适应（DUA）框架，可以在教师模型的 pré-训练过程中通过凝固来减轻源偏见问题。此外，我们设计了一个 Target-Relevant Object Localization Network（TROLN），可以挖掘目标相关知识，生成两个无类别的度量（IoU和中心率）。根据这两个度量，我们实施了域aware的一致性提高策略（DCE），以进一步精细化类别信任度，实现在垂直频道上的一致性。我们进行了广泛的实验，manifestly显示了该方法的有效性， persistently 大幅超越了现有的对齐基本elines。
</details></li>
</ul>
<hr>
<h2 id="Deep-Residual-CNN-for-Multi-Class-Chest-Infection-Diagnosis"><a href="#Deep-Residual-CNN-for-Multi-Class-Chest-Infection-Diagnosis" class="headerlink" title="Deep Residual CNN for Multi-Class Chest Infection Diagnosis"></a>Deep Residual CNN for Multi-Class Chest Infection Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10430">http://arxiv.org/abs/2311.10430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Donghan Kwon, Dohyun Lim, Yoonha Lee, Seung Won Lee</li>
<li>for: 这篇论文旨在开发和评估一种深度卷积神经网络（CNN），用于多类诊断胸部感染病，基于胸部X射影像。</li>
<li>methods: 该模型采用了深度卷积神经网络，通过对不同来源的数据集进行训练和验证，实现了robust的总准确率达93%。</li>
<li>results: 研究发现，不同类别之间存在微妙的差异，尤其是 fibrosis 类别，这反映了自动医疗图像诊断的复杂性和挑战。这些发现可以帮助未来的研究，增强模型在识别图像中更加细腻和复杂的特征方面的性能，以及优化和改进模型的架构和训练过程。<details>
<summary>Abstract</summary>
The advent of deep learning has significantly propelled the capabilities of automated medical image diagnosis, providing valuable tools and resources in the realm of healthcare and medical diagnostics. This research delves into the development and evaluation of a Deep Residual Convolutional Neural Network (CNN) for the multi-class diagnosis of chest infections, utilizing chest X-ray images. The implemented model, trained and validated on a dataset amalgamated from diverse sources, demonstrated a robust overall accuracy of 93%. However, nuanced disparities in performance across different classes, particularly Fibrosis, underscored the complexity and challenges inherent in automated medical image diagnosis. The insights derived pave the way for future research, focusing on enhancing the model's proficiency in classifying conditions that present more subtle and nuanced visual features in the images, as well as optimizing and refining the model architecture and training process. This paper provides a comprehensive exploration into the development, implementation, and evaluation of the model, offering insights and directions for future research and development in the field.
</details>
<details>
<summary>摘要</summary>
深度学习的出现对医疗图像诊断自动化技术带来了 significiant 的推动，提供了valuable 的工具和资源在医疗和医疗诊断领域。这项研究探讨了一种使用深度差分卷积神经网络（CNN）进行多类医疗图像诊断，使用了胸部X射线图像。实施的模型，在基于多个来源的数据集上进行训练和验证，表现了93%的总准确率。然而，不同类型的疾病之间存在了细微的差异，这反映了自动医疗图像诊断的复杂性和挑战。这些发现可以为未来的研究提供方向，例如增强模型对疾病表现更加细微的图像特征的识别能力，以及优化和改进模型的架构和训练过程。本文对模型的开发、实现和评估进行了全面的探讨，为未来的研究和发展提供了新的想法和方向。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-CNN-Model-for-Classification-and-Detection-of-Individuals-Wearing-Face-Mask"><a href="#Deep-Learning-based-CNN-Model-for-Classification-and-Detection-of-Individuals-Wearing-Face-Mask" class="headerlink" title="Deep Learning based CNN Model for Classification and Detection of Individuals Wearing Face Mask"></a>Deep Learning based CNN Model for Classification and Detection of Individuals Wearing Face Mask</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10408">http://arxiv.org/abs/2311.10408</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Chinnaiyan, Iyyappan M, Al Raiyan Shariff A, Kondaveeti Sai, Mallikarjunaiah B M, P Bharath</li>
<li>for: 防止 COVID-19 流行病的扩散，提高安全性，特别是在敏感区域。</li>
<li>methods: 使用深度学习创建一个实时视频和图像中检测面具的模型，包括面部检测和物体检测。</li>
<li>results: 实验结果表明模型在测试数据上具有优秀的准确率。<details>
<summary>Abstract</summary>
In response to the global COVID-19 pandemic, there has been a critical demand for protective measures, with face masks emerging as a primary safeguard. The approach involves a two-fold strategy: first, recognizing the presence of a face by detecting faces, and second, identifying masks on those faces. This project utilizes deep learning to create a model that can detect face masks in real-time streaming video as well as images. Face detection, a facet of object detection, finds applications in diverse fields such as security, biometrics, and law enforcement. Various detector systems worldwide have been developed and implemented, with convolutional neural networks chosen for their superior performance accuracy and speed in object detection. Experimental results attest to the model's excellent accuracy on test data. The primary focus of this research is to enhance security, particularly in sensitive areas. The research paper proposes a rapid image pre-processing method with masks centred on faces. Employing feature extraction and Convolutional Neural Network, the system classifies and detects individuals wearing masks. The research unfolds in three stages: image pre-processing, image cropping, and image classification, collectively contributing to the identification of masked faces. Continuous surveillance through webcams or CCTV cameras ensures constant monitoring, triggering a security alert if a person is detected without a mask.
</details>
<details>
<summary>摘要</summary>
因应全球 COVID-19 大流行，有一个急需的保护措施，面具出现为主要防范手段。该方法包括两个方面策略：首先，识别面具，然后，识别面具上的面。这个项目利用深度学习创建一个在实时流动视频和图像中检测面具的模型。面部检测是对象检测的一个方面，在安全、生物特征、和刑事调查等领域有广泛的应用。全球各地已经开发和实施了多种检测系统， convolutional neural networks（CNN）因其对对象检测的高性能精度和速度而被选择。实验结果证明模型在测试数据上具有优秀的准确率。本研究的主要目标是增强安全，特别是在敏感区域。研究论文提议一种快速的图像预处理方法，将面具围绕面进行中心。通过特征提取和 Convolutional Neural Network，系统可以识别和检测戴着面具的人。研究分三个阶段：图像预处理、图像裁剪和图像分类，共同帮助识别面具。通过持续的网络或 CCTV 摄像头监测，确保不断监测，如果检测到没有面具的人，就触发安全警报。
</details></li>
</ul>
<hr>
<h2 id="Optimized-Deep-Learning-Models-for-AUV-Seabed-Image-Analysis"><a href="#Optimized-Deep-Learning-Models-for-AUV-Seabed-Image-Analysis" class="headerlink" title="Optimized Deep Learning Models for AUV Seabed Image Analysis"></a>Optimized Deep Learning Models for AUV Seabed Image Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10399">http://arxiv.org/abs/2311.10399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajesh Sharma R, Akey Sungheetha, Chinnaiyan R</li>
<li>for: 本研究旨在提供最新的AUV图像处理技术和工具，以帮助更好地理解海底的特征和结构。</li>
<li>methods: 本研究使用了最新的计算机和算法技术，包括图像处理和分析方法，以提高AUV图像的质量和准确性。</li>
<li>results: 研究发现，使用新的AUV图像处理技术和工具，可以提高海底图像的质量和准确性，并且可以更好地理解海底的特征和结构。In English, this means:</li>
<li>for: This study aims to provide the most up-to-date AUV image processing techniques and tools to better understand the characteristics and structure of the seafloor.</li>
<li>methods: The study uses the latest computer and algorithmic techniques, including image processing and analysis methods, to improve the quality and accuracy of AUV images.</li>
<li>results: The research found that using new AUV image processing techniques and tools can improve the quality and accuracy of seafloor images, and provide a better understanding of the seafloor’s characteristics and structure.<details>
<summary>Abstract</summary>
Using autonomous underwater vehicles, or AUVs, has completely changed how we gather data from the ocean floor. AUV innovation has advanced significantly, especially in the analysis of images, due to the increasing need for accurate and efficient seafloor mapping. This blog post provides a detailed summary and comparison of the most current advancements in AUV seafloor image processing. We will go into the realm of undersea technology, covering everything through computer and algorithmic advancements to advances in sensors and cameras. After reading this page through to the end, you will have a solid understanding of the most up-to-date techniques and tools for using AUVs to process seabed photos and how they could further our comprehension of the ocean floor
</details>
<details>
<summary>摘要</summary>
Translation notes:* "AUV" is translated as "自主探测器" (zì zhòu tàn bèi qì) in Simplified Chinese.* "seafloor mapping" is translated as "海底地图" (hǎi dǐ dì tú) in Simplified Chinese.* "underwater technology" is translated as "水下科技" (shuǐ xià kē jì) in Simplified Chinese.* "computer and algorithmic advancements" is translated as "计算机和算法进步" (jì suàn jí hé suān fǎ jìn bo) in Simplified Chinese.* "sensor and camera improvements" is translated as "探测器和摄像头改进" (tàn bèi qì hé diàn yǐng tóu gǎi jì) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Two-Factor-Authentication-Approach-Based-on-Behavior-Patterns-for-Defeating-Puppet-Attacks"><a href="#Two-Factor-Authentication-Approach-Based-on-Behavior-Patterns-for-Defeating-Puppet-Attacks" class="headerlink" title="Two-Factor Authentication Approach Based on Behavior Patterns for Defeating Puppet Attacks"></a>Two-Factor Authentication Approach Based on Behavior Patterns for Defeating Puppet Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10389">http://arxiv.org/abs/2311.10389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Wang, Guyue Li, Zhiming Chu, Haobo Li, Daniele Faccio</li>
<li>for: 这篇论文是为了防止傀儡攻击（puppet attacks）而设计的。</li>
<li>methods: 该方法基于用户行为特征，具体来说是在身份验证过程中两次 consecutively 使用不同的手指点击设备。</li>
<li>results: 该方法可以实现97.87%的准确率和1.89%的false positive rate（FPR）。而且，对于增强傀儡攻击的抵抗力， combining 图像特征和时间特征在PUPGUARD中具有突出的优势。<details>
<summary>Abstract</summary>
Fingerprint traits are widely recognized for their unique qualities and security benefits. Despite their extensive use, fingerprint features can be vulnerable to puppet attacks, where attackers manipulate a reluctant but genuine user into completing the authentication process. Defending against such attacks is challenging due to the coexistence of a legitimate identity and an illegitimate intent. In this paper, we propose PUPGUARD, a solution designed to guard against puppet attacks. This method is based on user behavioral patterns, specifically, the user needs to press the capture device twice successively with different fingers during the authentication process. PUPGUARD leverages both the image features of fingerprints and the timing characteristics of the pressing intervals to establish two-factor authentication. More specifically, after extracting image features and timing characteristics, and performing feature selection on the image features, PUPGUARD fuses these two features into a one-dimensional feature vector, and feeds it into a one-class classifier to obtain the classification result. This two-factor authentication method emphasizes dynamic behavioral patterns during the authentication process, thereby enhancing security against puppet attacks. To assess PUPGUARD's effectiveness, we conducted experiments on datasets collected from 31 subjects, including image features and timing characteristics. Our experimental results demonstrate that PUPGUARD achieves an impressive accuracy rate of 97.87% and a remarkably low false positive rate (FPR) of 1.89%. Furthermore, we conducted comparative experiments to validate the superiority of combining image features and timing characteristics within PUPGUARD for enhancing resistance against puppet attacks.
</details>
<details>
<summary>摘要</summary>
人体指纹特征因其独特性和安全优势而广泛应用。然而，指纹特征可能受到傀儡攻击，攻击者可以让用户不情愿而真实的完成身份验证过程。防御 против这类攻击困难，因为攻击者可以利用合法身份和不良意图的共存。在本文中，我们提出了PUPGUARD解决方案，用于防御傀儡攻击。该方法基于用户行为特征，具体来说是在身份验证过程中，用户需要连续两次使用不同的手指点击捕捉设备。PUPGUARD利用手套特征和点击间隔时间特征，并将这两个特征 fusion 成一维特征向量，然后通过一类一分类器进行分类。这种二因素验证方法强调身份验证过程中的动态行为特征，从而增强对傀儡攻击的安全性。为评估PUPGUARD的效果，我们在31名参与者的数据集上进行了实验。我们的实验结果表明，PUPGUARD的准确率为97.87%，并且false positive rate (FPR)为1.89%。此外，我们还进行了比较实验，以验证PUPGUARD combining 手套特征和点击间隔时间特征可以增强对傀儡攻击的抵抗力。
</details></li>
</ul>
<hr>
<h2 id="Single-Shot-and-Multi-Shot-Feature-Learning-for-Multi-Object-Tracking"><a href="#Single-Shot-and-Multi-Shot-Feature-Learning-for-Multi-Object-Tracking" class="headerlink" title="Single-Shot and Multi-Shot Feature Learning for Multi-Object Tracking"></a>Single-Shot and Multi-Shot Feature Learning for Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10382">http://arxiv.org/abs/2311.10382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhe Li, Sanping Zhou, Zheng Qin, Le Wang, Jinjun Wang, Nanning Zheng<br>for: 本研究旨在提高多目标跟踪（MOT）的精度和可靠性，使其在视频序列中更好地跟踪目标。methods: 本研究提出了一种简单 yet effective的两stage特征学习 парадиг，用于同时学习单击和多击特征，以实现robust的数据关联。在不同的框架中，我们设计了一种单击特征学习模块，以提取每个检测的特征，以便在不同的帧之间进行有效的目标关联。在跟踪过程中，如果某些目标被lost多个帧，我们则设计了一种多击特征学习模块，以提取每个跟踪的特征，以便在长期内重新找到这些丢失的目标。results: 我们的方法在MOT17和MOT20 datasets上实现了显著的改进，并在DanceTrack dataset上达到了当前最佳性能。<details>
<summary>Abstract</summary>
Multi-Object Tracking (MOT) remains a vital component of intelligent video analysis, which aims to locate targets and maintain a consistent identity for each target throughout a video sequence. Existing works usually learn a discriminative feature representation, such as motion and appearance, to associate the detections across frames, which are easily affected by mutual occlusion and background clutter in practice. In this paper, we propose a simple yet effective two-stage feature learning paradigm to jointly learn single-shot and multi-shot features for different targets, so as to achieve robust data association in the tracking process. For the detections without being associated, we design a novel single-shot feature learning module to extract discriminative features of each detection, which can efficiently associate targets between adjacent frames. For the tracklets being lost several frames, we design a novel multi-shot feature learning module to extract discriminative features of each tracklet, which can accurately refind these lost targets after a long period. Once equipped with a simple data association logic, the resulting VisualTracker can perform robust MOT based on the single-shot and multi-shot feature representations. Extensive experimental results demonstrate that our method has achieved significant improvements on MOT17 and MOT20 datasets while reaching state-of-the-art performance on DanceTrack dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MSE-Nets-Multi-annotated-Semi-supervised-Ensemble-Networks-for-Improving-Segmentation-of-Medical-Image-with-Ambiguous-Boundaries"><a href="#MSE-Nets-Multi-annotated-Semi-supervised-Ensemble-Networks-for-Improving-Segmentation-of-Medical-Image-with-Ambiguous-Boundaries" class="headerlink" title="MSE-Nets: Multi-annotated Semi-supervised Ensemble Networks for Improving Segmentation of Medical Image with Ambiguous Boundaries"></a>MSE-Nets: Multi-annotated Semi-supervised Ensemble Networks for Improving Segmentation of Medical Image with Ambiguous Boundaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10380">http://arxiv.org/abs/2311.10380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Wang, Tengjin Weng, Jingyi Wang, Yang Shen, Zhidong Zhao, Yixiu Liu, Pengfei Jiao, Zhiming Cheng, Yaqi Wang</li>
<li>for: 这个研究是为了解决医学影像分类标注 exhibit 对于专家而存在变化，因为医学影像中的物体和背景的分类Boundaries是不明确的。</li>
<li>methods: 我们提出了Multi-annotated Semi-supervised Ensemble Networks (MSE-Nets)，用于从有限多标注和充沛无标注数据中学习分类。我们还引入了网络组别实现增强 (NPCE) 模组和多网络伪素指导 (MNPS) 模组，以便更好地处理多标注数据。</li>
<li>results: 我们的方法可以大大减少需要多标注数据的需求，仅需97.75%的标注数据，并且与最佳对照方法的差距只有Jaccard指数4%。此外，我们的方法在医学影像分类 tasks 上的实验结果表明，与其他仅使用单一标注或合并融合方法相比，我们的方法在医学影像分类 tasks 上表现更好。<details>
<summary>Abstract</summary>
Medical image segmentation annotations exhibit variations among experts due to the ambiguous boundaries of segmented objects and backgrounds in medical images. Although using multiple annotations for each image in the fully-supervised has been extensively studied for training deep models, obtaining a large amount of multi-annotated data is challenging due to the substantial time and manpower costs required for segmentation annotations, resulting in most images lacking any annotations. To address this, we propose Multi-annotated Semi-supervised Ensemble Networks (MSE-Nets) for learning segmentation from limited multi-annotated and abundant unannotated data. Specifically, we introduce the Network Pairwise Consistency Enhancement (NPCE) module and Multi-Network Pseudo Supervised (MNPS) module to enhance MSE-Nets for the segmentation task by considering two major factors: (1) to optimize the utilization of all accessible multi-annotated data, the NPCE separates (dis)agreement annotations of multi-annotated data at the pixel level and handles agreement and disagreement annotations in different ways, (2) to mitigate the introduction of imprecise pseudo-labels, the MNPS extends the training data by leveraging consistent pseudo-labels from unannotated data. Finally, we improve confidence calibration by averaging the predictions of base networks. Experiments on the ISIC dataset show that we reduced the demand for multi-annotated data by 97.75\% and narrowed the gap with the best fully-supervised baseline to just a Jaccard index of 4\%. Furthermore, compared to other semi-supervised methods that rely only on a single annotation or a combined fusion approach, the comprehensive experimental results on ISIC and RIGA datasets demonstrate the superior performance of our proposed method in medical image segmentation with ambiguous boundaries.
</details>
<details>
<summary>摘要</summary>
医学图像分割注释存在专家间的差异，这是因为医学图像中对象和背景的分割Boundaries是不确定的。虽然使用多个注释来训练深度模型已经得到了广泛的研究，但获得大量多注释数据是困难的，因为分割注释需要大量的时间和人力资源，导致大多数图像无法得到任何注释。为解决这个问题，我们提出了多注释 semi-supervised ensemble networks (MSE-Nets)，用于从有限多注释和丰富无注释数据中学习分割。 Specifically, we introduce the network pairwise consistency enhancement (NPCE) module and multi-network pseudo-supervised (MNPS) module to enhance MSE-Nets for the segmentation task by considering two major factors: (1) to optimize the utilization of all accessible multi-annotated data, the NPCE separates (dis)agreement annotations of multi-annotated data at the pixel level and handles agreement and disagreement annotations in different ways, (2) to mitigate the introduction of imprecise pseudo-labels, the MNPS extends the training data by leveraging consistent pseudo-labels from unannotated data. Finally, we improve confidence calibration by averaging the predictions of base networks. 实验结果表明，我们可以降低需要多注释数据的数量为97.75%，并将与最佳完全监督基eline之间的差距降低到只是Jaccard指数4%。此外，与其他半监督方法相比，我们的提议方法在医学图像分割中的边界是不确定的情况下表现出了superior performance。
</details></li>
</ul>
<hr>
<h2 id="Breaking-Temporal-Consistency-Generating-Video-Universal-Adversarial-Perturbations-Using-Image-Models"><a href="#Breaking-Temporal-Consistency-Generating-Video-Universal-Adversarial-Perturbations-Using-Image-Models" class="headerlink" title="Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models"></a>Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10366">http://arxiv.org/abs/2311.10366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hee-Seon Kim, Minji Son, Minbeom Kim, Myung-Joon Kwon, Changick Kim</li>
<li>for: 防御深度学习模型受到攻击的安全性问题在视频分析中变得更加紧迫。特别是Universal Adversarial Perturbation（UAP）对深度学习模型 pose a significant threat, as a single perturbation can mislead deep learning models on entire datasets.</li>
<li>methods: 我们提出了一种新的视频UAP使用图像数据和图像模型。这使得我们可以利用图像数据和图像模型基础的研究来进行视频应用。然而，图像模型对视频中的时间方面的分析有限，这是成功视频攻击的关键。为解决这个挑战，我们引入了Breaking Temporal Consistency（BTC）方法，这是在图像模型中 incorporate temporal information into video attacks 的第一个尝试。我们想要生成攻击视频，其具有与原始视频相反的模式。具体来说，BTC-UAP minimizes the feature similarity between neighboring frames in videos。</li>
<li>results: 我们的方法比现有方法更有效，可以在不同的数据集上达到高效率。其中包括ImageNet、UCF-101和Kinetics-400等数据集。此外，我们的方法适用于视频的不同长度和对时间偏移的抗衡性。<details>
<summary>Abstract</summary>
As video analysis using deep learning models becomes more widespread, the vulnerability of such models to adversarial attacks is becoming a pressing concern. In particular, Universal Adversarial Perturbation (UAP) poses a significant threat, as a single perturbation can mislead deep learning models on entire datasets. We propose a novel video UAP using image data and image model. This enables us to take advantage of the rich image data and image model-based studies available for video applications. However, there is a challenge that image models are limited in their ability to analyze the temporal aspects of videos, which is crucial for a successful video attack. To address this challenge, we introduce the Breaking Temporal Consistency (BTC) method, which is the first attempt to incorporate temporal information into video attacks using image models. We aim to generate adversarial videos that have opposite patterns to the original. Specifically, BTC-UAP minimizes the feature similarity between neighboring frames in videos. Our approach is simple but effective at attacking unseen video models. Additionally, it is applicable to videos of varying lengths and invariant to temporal shifts. Our approach surpasses existing methods in terms of effectiveness on various datasets, including ImageNet, UCF-101, and Kinetics-400.
</details>
<details>
<summary>摘要</summary>
为了使深度学习模型更加普及，对于这些模型的攻击性质也变得越来越重要。特别是对于Universal Adversarial Perturbation（UAP）而言，单个杂散可以诱导深度学习模型对整个数据集进行误导。我们提出了一种新的视频UAP，使用图像数据和图像模型。这使得我们可以利用图像数据和图像模型基础的研究，对于视频应用有更多的优势。然而，图像模型对视频中的时间方面有限制，这是成功视频攻击的关键。为解决这个挑战，我们引入了Breaking Temporal Consistency（BTC）方法，这是在图像模型中引入时间信息的第一次尝试。我们想要生成一些与原始视频相反的攻击视频。特别是，BTC-UAP减少了邻域帧视频特征之间的相似性。我们的方法简单而有效，可以让未看过视频模型进行攻击。此外，它适用于视频的不同长度和不同的时间偏移。我们的方法在不同的数据集上表现出色，包括ImageNet、UCF-101和Kinetics-400等。
</details></li>
</ul>
<hr>
<h2 id="Video-based-Sequential-Bayesian-Homography-Estimation-for-Soccer-Field-Registration"><a href="#Video-based-Sequential-Bayesian-Homography-Estimation-for-Soccer-Field-Registration" class="headerlink" title="Video-based Sequential Bayesian Homography Estimation for Soccer Field Registration"></a>Video-based Sequential Bayesian Homography Estimation for Soccer Field Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10361">http://arxiv.org/abs/2311.10361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul J. Claasen, J. P. de Villiers</li>
<li>for: 提高视频摄像机动态Homography的推断精度</li>
<li>methods: 使用两stage kalman滤波器与tracked keypoints进行BAYESIAN Homography推断</li>
<li>results: 与现有方法相比，提高了homography评估指标的精度，且可以使用现有的键点检测方法进行增强<details>
<summary>Abstract</summary>
A novel Bayesian framework is proposed, which explicitly relates the homography of one video frame to the next through an affine transformation while explicitly modelling keypoint uncertainty. The literature has previously used differential homography between subsequent frames, but not in a Bayesian setting. In cases where Bayesian methods have been applied, camera motion is not adequately modelled, and keypoints are treated as deterministic. The proposed method, Bayesian Homography Inference from Tracked Keypoints (BHITK), employs a two-stage Kalman filter and significantly improves existing methods. Existing keypoint detection methods may be easily augmented with BHITK. It enables less sophisticated and less computationally expensive methods to outperform the state-of-the-art approaches in most homography evaluation metrics. Furthermore, the homography annotations of the WorldCup and TS-WorldCup datasets have been refined using a custom homography annotation tool released for public use. The refined datasets are consolidated and released as the consolidated and refined WorldCup (CARWC) dataset.
</details>
<details>
<summary>摘要</summary>
提出了一种新的 bayesian 框架，将一帧视频与下一帧视频之间的投影关系通过 affine 变换进行显式关联，并且明确模糊关键点的不确定性。过去的文献中使用了差分投影 между 后续帧，但是没有在 bayesian  Setting 中使用。在摄像机运动不充分模型和关键点 treated 为确定的情况下，提出的方法 bayesian homography inference from tracked keypoints (BHITK) 使用了两 stage kalman filter，并有所提高现有方法。现有的关键点检测方法可以轻松地增强 BHITK。这种方法使得不太复杂和计算成本较低的方法能够在大多数投影评价指标中超越现有的状态艺术方法。此外，世界杯和 TS-WorldCup 数据集中的 homography 标注已经通过自定义 homography 标注工具进行了精细化，并将其整合成 consolidated and refined WorldCup (CARWC) 数据集，并将其公开发布。
</details></li>
</ul>
<hr>
<h2 id="Garment-Recovery-with-Shape-and-Deformation-Priors"><a href="#Garment-Recovery-with-Shape-and-Deformation-Priors" class="headerlink" title="Garment Recovery with Shape and Deformation Priors"></a>Garment Recovery with Shape and Deformation Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10356">http://arxiv.org/abs/2311.10356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ren Li, Corentin Dumery, Benoît Guillard, Pascal Fua</li>
<li>for: 提出了一种方法，用于从真实图像中生成真实的服装模型，无论服装的形状或扭formation。</li>
<li>methods: 我们的方法利用了自动生成的数据学习到的形状和扭formation的约束，以准确地捕捉服装的形状和扭formation，包括大型的扭formation。</li>
<li>results: 我们的方法可以准确地回归服装的几何结构，同时也可以生成可以直接用于动画和 simulations 的服装模型。<details>
<summary>Abstract</summary>
While modeling people wearing tight-fitting clothing has made great strides in recent years, loose-fitting clothing remains a challenge. We propose a method that delivers realistic garment models from real-world images, regardless of garment shape or deformation. To this end, we introduce a fitting approach that utilizes shape and deformation priors learned from synthetic data to accurately capture garment shapes and deformations, including large ones. Not only does our approach recover the garment geometry accurately, it also yields models that can be directly used by downstream applications such as animation and simulation.
</details>
<details>
<summary>摘要</summary>
“对于穿着紧身服装的人体模型化在最近几年中已经做出了很大的进步，但是对于润缓服装仍然是一个挑战。我们提出了一种方法，可以将实际拍摄的图像转换为实际的服装模型，无论服装的形状或者扭曲是多大。为此，我们引入了一种适应策略，利用自己 sintetic data 中学习的形状和扭曲假设，实现精准地捕捉服装的形状和扭曲，包括大的一类。不仅我们的方法可以实现实际的服装几何学精准地回传，还能够提供下游应用，如动画和模拟等。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Pseudo-Label-Guided-Data-Fusion-and-Output-Consistency-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Pseudo-Label-Guided-Data-Fusion-and-Output-Consistency-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Pseudo Label-Guided Data Fusion and Output Consistency for Semi-Supervised Medical Image Segmentation"></a>Pseudo Label-Guided Data Fusion and Output Consistency for Semi-Supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10349">http://arxiv.org/abs/2311.10349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ortonwang/plgdf">https://github.com/ortonwang/plgdf</a></li>
<li>paper_authors: Tao Wang, Yuanbin Chen, Xinlin Zhang, Yuanbo Zhou, Junlin Lan, Bizhe Bai, Tao Tan, Min Du, Qinquan Gao, Tong Tong</li>
<li>for: 这个研究是为了提出一个基于Convolutional Neural Networks的 semi-supervised learning架构，以便医疗图像分类任务中使用更少的标签数据。</li>
<li>methods: 这个研究使用了mean teacher network，并提出了一个新的伪标签使用方案，将标签和无标签数据结合以增加资料集。此外，研究者还强制了解释层之间的一致性，并提出了适合评估一致性的损失函数。最后，研究者还加入了一个锋化操作以进一步提高分类的精度。</li>
<li>results: 实验结果显示，PLGDF架构可以将更少的标签数据用于医疗图像分类任务中，同时与六种州分类学习方法进行比较，PLGDF的性能较高。codes这个研究的数据可以在<a target="_blank" rel="noopener" href="https://github.com/ortonwang/PLGDF%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/ortonwang/PLGDF上获得。</a><details>
<summary>Abstract</summary>
Supervised learning algorithms based on Convolutional Neural Networks have become the benchmark for medical image segmentation tasks, but their effectiveness heavily relies on a large amount of labeled data. However, annotating medical image datasets is a laborious and time-consuming process. Inspired by semi-supervised algorithms that use both labeled and unlabeled data for training, we propose the PLGDF framework, which builds upon the mean teacher network for segmenting medical images with less annotation. We propose a novel pseudo-label utilization scheme, which combines labeled and unlabeled data to augment the dataset effectively. Additionally, we enforce the consistency between different scales in the decoder module of the segmentation network and propose a loss function suitable for evaluating the consistency. Moreover, we incorporate a sharpening operation on the predicted results, further enhancing the accuracy of the segmentation.   Extensive experiments on three publicly available datasets demonstrate that the PLGDF framework can largely improve performance by incorporating the unlabeled data. Meanwhile, our framework yields superior performance compared to six state-of-the-art semi-supervised learning methods. The codes of this study are available at https://github.com/ortonwang/PLGDF.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将提供给定文本的简化中文翻译。<</SYS>>基于卷积神经网络的超级vised学习算法在医疗影像 segmentation 任务上成为标准，但它们的效果受到大量标注数据的限制。然而，标注医疗影像集合是一项劳累和时间consuming的过程。 inspirited by semi-supervised算法，我们提出了 PLGDF 框架，该框架基于 Mean Teacher 网络，用于 segmenting 医疗影像，并使用 menos 标注数据。我们提出了一种新的 Pseudo-label 利用方案，该方案将标注和无标注数据结合使用，以增强数据集的规模。此外，我们在解码模块中 enforcing 等效性，并提出了适合评估等效性的损失函数。此外，我们还添加了一种锐化操作，以进一步提高 segmentation 的准确性。广泛的实验表明，PLGDF 框架可以通过 incorporating 无标注数据，大幅提高性能。同时，我们的框架在六种 state-of-the-art  semi-supervised 学习方法中显示出了超越性。codes 这些研究可以在 https://github.com/ortonwang/PLGDF 上获得。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Student-Engagement-in-Online-Learning-through-Facial-Expression-Analysis-and-Complex-Emotion-Recognition-using-Deep-Learning"><a href="#Enhancing-Student-Engagement-in-Online-Learning-through-Facial-Expression-Analysis-and-Complex-Emotion-Recognition-using-Deep-Learning" class="headerlink" title="Enhancing Student Engagement in Online Learning through Facial Expression Analysis and Complex Emotion Recognition using Deep Learning"></a>Enhancing Student Engagement in Online Learning through Facial Expression Analysis and Complex Emotion Recognition using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10343">http://arxiv.org/abs/2311.10343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rekha R Nair, Tina Babu, Pavithra K</li>
<li>for: 该论文目的是提出一种基于深度学习技术的 facial expression 评估方法，以评估在线学习 sessio 中学生的参与度。</li>
<li>methods: 该方法使用了深度学习模型，通过分析学生表情来评估学生的参与度。</li>
<li>results: 实验结果显示，该方法可以准确地分类学生的基本情感状态，并且达到了95%的准确率。<details>
<summary>Abstract</summary>
In response to the COVID-19 pandemic, traditional physical classrooms have transitioned to online environments, necessitating effective strategies to ensure sustained student engagement. A significant challenge in online teaching is the absence of real-time feedback from teachers on students learning progress. This paper introduces a novel approach employing deep learning techniques based on facial expressions to assess students engagement levels during online learning sessions. Human emotions cannot be adequately conveyed by a student using only the basic emotions, including anger, disgust, fear, joy, sadness, surprise, and neutrality. To address this challenge, proposed a generation of four complex emotions such as confusion, satisfaction, disappointment, and frustration by combining the basic emotions. These complex emotions are often experienced simultaneously by students during the learning session. To depict these emotions dynamically,utilized a continuous stream of image frames instead of discrete images. The proposed work utilized a Convolutional Neural Network (CNN) model to categorize the fundamental emotional states of learners accurately. The proposed CNN model demonstrates strong performance, achieving a 95% accuracy in precise categorization of learner emotions.
</details>
<details>
<summary>摘要</summary>
Due to the COVID-19 pandemic, traditional physical classrooms have transitioned to online environments, requiring effective strategies to ensure sustained student engagement. One significant challenge in online teaching is the lack of real-time feedback from teachers on students' learning progress. This paper proposes a novel approach using deep learning techniques based on facial expressions to assess students' engagement levels during online learning sessions.Human emotions cannot be fully conveyed by a student using only the basic emotions, such as anger, disgust, fear, joy, sadness, surprise, and neutrality. To address this challenge, the proposed approach generates four complex emotions, including confusion, satisfaction, disappointment, and frustration, by combining the basic emotions. These complex emotions are often experienced simultaneously by students during the learning session. To depict these emotions dynamically, the proposed approach uses a continuous stream of image frames instead of discrete images.The proposed approach utilizes a Convolutional Neural Network (CNN) model to accurately categorize the fundamental emotional states of learners. The proposed CNN model demonstrates strong performance, achieving a 95% accuracy in precise categorization of learner emotions.Here is the translation in Traditional Chinese:因COVID-19大流行，传统的物理教室转换为在线环境，需要有效的策略来确保学生的持续参与。在线教学中的一个重要挑战是缺乏教师在学生学习过程中的实时反馈。本文提出了一种使用深度学习技术基于表情来评估在线学习Session中学生的参与水平的新方法。人类的情感无法由学生只使用基本情感来完全表达，例如愤怒、厌恶、恐惧、喜悦、悲伤、惊讶和中性。为了解决这个挑战，提出了组合基本情感生成四种复杂情感，包括混乱、满足、失望和沮丧。这些复杂情感经常在学习Session中同时出现。为了显示这些情感的动态变化，提出的方法使用一串无限长的图像框架而不是独立的图像。提出的方法使用一个Convolutional Neural Network（CNN）模型精确地分Category学生的情感状态。提出的CNN模型示出了强大的表现，实现了95%的精确分Category学生情感。
</details></li>
</ul>
<hr>
<h2 id="A2XP-Towards-Private-Domain-Generalization"><a href="#A2XP-Towards-Private-Domain-Generalization" class="headerlink" title="A2XP: Towards Private Domain Generalization"></a>A2XP: Towards Private Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10339">http://arxiv.org/abs/2311.10339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geunhyeok Yu, Hyoseok Hwang</li>
<li>for: 本研究旨在解决深度神经网络（DNNs）在不同领域数据中表现不佳的问题，特别是 Computer Vision 领域。</li>
<li>methods: 本文提出了一种新的方法，即 Attend to eXpert Prompts（A2XP），以解决 DNNs 在不同领域数据中的领域泛化问题。A2XP 包括两个阶段：专家适应和领域泛化。在第一阶段，每个源领域的提问都被优化，以引导模型向优化的方向发展。在第二阶段，两个嵌入器网络被训练，以有效地混合这些专家提问，以达到最佳输出。</li>
<li>results: 我们的广泛实验表明，A2XP 可以与现有的非私有领域泛化方法相比，达到最新的结果。实验结果表明，提出的方法不仅可以解决 DNNs 中的领域泛化问题，还提供了一种隐私保护、高效的解决方案，对 Computer Vision 领域的更广泛应用。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have become pivotal in various fields, especially in computer vision, outperforming previous methodologies. A critical challenge in their deployment is the bias inherent in data across different domains, such as image style, and environmental conditions, leading to domain gaps. This necessitates techniques for learning general representations from biased training data, known as domain generalization. This paper presents Attend to eXpert Prompts (A2XP), a novel approach for domain generalization that preserves the privacy and integrity of the network architecture. A2XP consists of two phases: Expert Adaptation and Domain Generalization. In the first phase, prompts for each source domain are optimized to guide the model towards the optimal direction. In the second phase, two embedder networks are trained to effectively amalgamate these expert prompts, aiming for an optimal output. Our extensive experiments demonstrate that A2XP achieves state-of-the-art results over existing non-private domain generalization methods. The experimental results validate that the proposed approach not only tackles the domain generalization challenge in DNNs but also offers a privacy-preserving, efficient solution to the broader field of computer vision.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cooperative-Perception-with-Learning-Based-V2V-communications"><a href="#Cooperative-Perception-with-Learning-Based-V2V-communications" class="headerlink" title="Cooperative Perception with Learning-Based V2V communications"></a>Cooperative Perception with Learning-Based V2V communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10336">http://arxiv.org/abs/2311.10336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenguang Liu, Yunfei Chen, Jianjun Chen, Ryan Payton, Michael Riley, Shuang-Hua Yang</li>
<li>for: 本文旨在探讨协同感知在自动驾驶中的应用，以缓解单个自动车辆感知的局限性。</li>
<li>methods: 本文使用了不同的融合方法和通信频率损害来评估协同感知的性能。同时，一种新的晚期融合方案被提出，以利用中间特征的稳定性。</li>
<li>results: numerical results表明，中间融合在频率损害较大的情况下比早期融合和晚期融合更加稳定，当SNR大于0dB时。此外，提议的融合方案也超过了使用检测输出的传统晚期融合。 autoencoder也提供了一个好的平衡点，在检测准确率和带宽使用之间。<details>
<summary>Abstract</summary>
Cooperative perception has been widely used in autonomous driving to alleviate the inherent limitation of single automated vehicle perception. To enable cooperation, vehicle-to-vehicle (V2V) communication plays an indispensable role. This work analyzes the performance of cooperative perception accounting for communications channel impairments. Different fusion methods and channel impairments are evaluated. A new late fusion scheme is proposed to leverage the robustness of intermediate features. In order to compress the data size incurred by cooperation, a convolution neural network-based autoencoder is adopted. Numerical results demonstrate that intermediate fusion is more robust to channel impairments than early fusion and late fusion, when the SNR is greater than 0 dB. Also, the proposed fusion scheme outperforms the conventional late fusion using detection outputs, and autoencoder provides a good compromise between detection accuracy and bandwidth usage.
</details>
<details>
<summary>摘要</summary>
合作感知在自动驾驶中广泛应用以减轻单个自动车辆感知的内在限制。为实现合作，车辆间通信（V2V）在无可或缺。这项工作分析了帐户通信频率的影响，并评估不同的混合方法和通信频率。我们提出了一种新的晚期混合方案，以利用中间特征的可靠性。为减少合作所带来的数据大小，我们采用了一种基于卷积神经网络的自适应编码器。数值结果表明，中间混合比早期混合和晚期混合更加鲁棒，当SNR大于0dB时。此外，我们的混合方案比传统的晚期混合使用检测输出更高效，而自适应编码器提供了一个好的平衡 между检测精度和带宽使用。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Multimodal-Fusion-for-Enhanced-Diagnosis-of-Multiple-Retinal-Diseases-in-Ultra-wide-OCTA"><a href="#Leveraging-Multimodal-Fusion-for-Enhanced-Diagnosis-of-Multiple-Retinal-Diseases-in-Ultra-wide-OCTA" class="headerlink" title="Leveraging Multimodal Fusion for Enhanced Diagnosis of Multiple Retinal Diseases in Ultra-wide OCTA"></a>Leveraging Multimodal Fusion for Enhanced Diagnosis of Multiple Retinal Diseases in Ultra-wide OCTA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10331">http://arxiv.org/abs/2311.10331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hwei-hw/m3octa">https://github.com/hwei-hw/m3octa</a></li>
<li>paper_authors: Hao Wei, Peilun Shi, Guitao Bai, Minqing Zhang, Shuangle Li, Wu Yuan</li>
<li>for: 该论文旨在提供 Exceptionally wide scanning range of up to 24 x 20 $mm^{2}$，覆盖 anterior和 posterior regions of the retina 的 Ultra-wide optical coherence tomography angiography (UW-OCTA) 技术。</li>
<li>methods: 该论文提出了一种 cross-modal fusion 框架，利用 multi-modal information for diagnosing multiple diseases。</li>
<li>results: 通过对 M3OCTA 数据集进行了extensive experiments，证明了该方法的效iveness和超越性， both in fixed and varying modalities settings。<details>
<summary>Abstract</summary>
Ultra-wide optical coherence tomography angiography (UW-OCTA) is an emerging imaging technique that offers significant advantages over traditional OCTA by providing an exceptionally wide scanning range of up to 24 x 20 $mm^{2}$, covering both the anterior and posterior regions of the retina. However, the currently accessible UW-OCTA datasets suffer from limited comprehensive hierarchical information and corresponding disease annotations. To address this limitation, we have curated the pioneering M3OCTA dataset, which is the first multimodal (i.e., multilayer), multi-disease, and widest field-of-view UW-OCTA dataset. Furthermore, the effective utilization of multi-layer ultra-wide ocular vasculature information from UW-OCTA remains underdeveloped. To tackle this challenge, we propose the first cross-modal fusion framework that leverages multi-modal information for diagnosing multiple diseases. Through extensive experiments conducted on our openly available M3OCTA dataset, we demonstrate the effectiveness and superior performance of our method, both in fixed and varying modalities settings. The construction of the M3OCTA dataset, the first multimodal OCTA dataset encompassing multiple diseases, aims to advance research in the ophthalmic image analysis community.
</details>
<details>
<summary>摘要</summary>
“ULTRA-WIDE Optical coherence tomography angiography（UW-OCTA）是一种emerging imaging技术，具有优先的优点，包括提供exceptionally wide scanning range，覆盖 anterior和 posterior retina regions，但是目前可用的UW-OCTA数据集存在limited comprehensive hierarchical information和相应的疾病标识。为解决这个限制，我们已经组装了创新的M3OCTA数据集，是第一个多模式（即多层）、多疾病、最宽的field-of-view UW-OCTA数据集。此外，使用ultra-wide ocular vasculature信息的有效利用方法仍然受到挑战。为了解决这个问题，我们提出了首个 Cross-modal fusion框架，利用多modal信息进行疾病诊断。经过了我们公开提供的M3OCTA数据集的广泛实验，我们证明了我们的方法的有效性和superior performance，包括固定和变化modal settings。M3OCTA数据集的建立，是为了进展医疗影像分析社区的研究。”
</details></li>
</ul>
<hr>
<h2 id="TransONet-Automatic-Segmentation-of-Vasculature-in-Computed-Tomographic-Angiograms-Using-Deep-Learning"><a href="#TransONet-Automatic-Segmentation-of-Vasculature-in-Computed-Tomographic-Angiograms-Using-Deep-Learning" class="headerlink" title="TransONet: Automatic Segmentation of Vasculature in Computed Tomographic Angiograms Using Deep Learning"></a>TransONet: Automatic Segmentation of Vasculature in Computed Tomographic Angiograms Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10328">http://arxiv.org/abs/2311.10328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Bagheri Rajeoni, Breanna Pederson, Ali Firooz, Hamed Abdollahi, Andrew K. Smith, Daniel G. Clair, Susan M. Lessner, Homayoun Valafar</li>
<li>for: 这个研究旨在提高Computed Tomographic Angiography（CTA）图像中血管系统的诊断速度和准确率，以便帮助医生更好地诊断和治疗 peripheral arterial disease（PAD）。</li>
<li>methods: 这个研究使用了深度学习技术来分类CTA图像中的血管系统，包括从下颈部分到股骨分支和从下颈部分到膝盖的两个部分。</li>
<li>results: 研究结果表明，使用深度学习技术可以准确地分类CTA图像中的血管系统，其中最高的Dice准确率达到93.5%和80.64%。这些结果表明深度学习技术在诊断血管系统中具有高度的潜在价值和优势。<details>
<summary>Abstract</summary>
Pathological alterations in the human vascular system underlie many chronic diseases, such as atherosclerosis and aneurysms. However, manually analyzing diagnostic images of the vascular system, such as computed tomographic angiograms (CTAs) is a time-consuming and tedious process. To address this issue, we propose a deep learning model to segment the vascular system in CTA images of patients undergoing surgery for peripheral arterial disease (PAD). Our study focused on accurately segmenting the vascular system (1) from the descending thoracic aorta to the iliac bifurcation and (2) from the descending thoracic aorta to the knees in CTA images using deep learning techniques. Our approach achieved average Dice accuracies of 93.5% and 80.64% in test dataset for (1) and (2), respectively, highlighting its high accuracy and potential clinical utility. These findings demonstrate the use of deep learning techniques as a valuable tool for medical professionals to analyze the health of the vascular system efficiently and accurately. Please visit the GitHub page for this paper at https://github.com/pip-alireza/TransOnet.
</details>
<details>
<summary>摘要</summary>
人体血管系统的疾病变化对多种慢性疾病有重要影响，如atherosclerosis和aneurysms。然而，手动分析医学影像诊断图像（如计算机tomography angiography，CTA）是一项时间consuming和繁琐的过程。为解决这个问题，我们提出了一个深度学习模型，用于在患有peripheral arterial disease（PAD）患者的CTA图像中分类血管系统。我们的研究把注意力集中在以下两个方面：1. 从 descending thoracic aorta 到 iliac bifurcation 的血管系统分类（CTA图像），我们使用深度学习技术实现了平均的 Dice 准确率为 93.5%。2. 从 descending thoracic aorta 到 knees 的血管系统分类（CTA图像），我们使用深度学习技术实现了平均的 Dice 准确率为 80.64%。这些结果表明，使用深度学习技术可以作为医疗专业人员分析血管系统的健康效果精准和高效的 valuabe工具。更多信息请参考我们的 GitHub 页面：https://github.com/pip-alireza/TransOnet。
</details></li>
</ul>
<hr>
<h2 id="Learning-transformer-based-heterogeneously-salient-graph-representation-for-multimodal-fusion-classification-of-hyperspectral-image-and-LiDAR-data"><a href="#Learning-transformer-based-heterogeneously-salient-graph-representation-for-multimodal-fusion-classification-of-hyperspectral-image-and-LiDAR-data" class="headerlink" title="Learning transformer-based heterogeneously salient graph representation for multimodal fusion classification of hyperspectral image and LiDAR data"></a>Learning transformer-based heterogeneously salient graph representation for multimodal fusion classification of hyperspectral image and LiDAR data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10320">http://arxiv.org/abs/2311.10320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Yang, Bo Du, Liangpei Zhang</li>
<li>for: 提高多源Remote Sensing图像分类的精度和准确性。</li>
<li>methods: 提出基于 transformer 的不同类型数据图像表示法（THSGR），通过多模式图像抽象和自注意力自适应归一化来提高模型的稳定性和泛化能力。</li>
<li>results: 通过三个 benchmark 数据集的实验和分析，证明提出的方法能够在不同的模式数据上提高分类精度和准确性，并且与其他 SOTA 方法相比具有竞争力。<details>
<summary>Abstract</summary>
Data collected by different modalities can provide a wealth of complementary information, such as hyperspectral image (HSI) to offer rich spectral-spatial properties, synthetic aperture radar (SAR) to provide structural information about the Earth's surface, and light detection and ranging (LiDAR) to cover altitude information about ground elevation. Therefore, a natural idea is to combine multimodal images for refined and accurate land-cover interpretation. Although many efforts have been attempted to achieve multi-source remote sensing image classification, there are still three issues as follows: 1) indiscriminate feature representation without sufficiently considering modal heterogeneity, 2) abundant features and complex computations associated with modeling long-range dependencies, and 3) overfitting phenomenon caused by sparsely labeled samples. To overcome the above barriers, a transformer-based heterogeneously salient graph representation (THSGR) approach is proposed in this paper. First, a multimodal heterogeneous graph encoder is presented to encode distinctively non-Euclidean structural features from heterogeneous data. Then, a self-attention-free multi-convolutional modulator is designed for effective and efficient long-term dependency modeling. Finally, a mean forward is put forward in order to avoid overfitting. Based on the above structures, the proposed model is able to break through modal gaps to obtain differentiated graph representation with competitive time cost, even for a small fraction of training samples. Experiments and analyses on three benchmark datasets with various state-of-the-art (SOTA) methods show the performance of the proposed approach.
</details>
<details>
<summary>摘要</summary>
据收集的不同模式数据可以提供丰富的补充信息，如光谱镜像（HSI）提供了丰富的 spectral-spatial 性质， Synthetic Aperture Radar（SAR）提供了地球表面的结构信息，和光散射和距离测量（LiDAR）提供了高度信息。因此，将多Modal Image 进行合并，可以提高准确的地面解释。虽然许多努力已经尝试过多源 remote sensing 图像分类，但还有三个问题：1）不充分考虑模态不同性，2）丰富的特征和复杂的计算，3）过拟合现象。为了突破这些障碍，本文提出了一种基于 transformer 的多模态焦点图表示（THSGR）方法。首先，一种多模态不同结构图编码器被提出，用于编码不同模式数据中的非欧几何特征。然后，一种自我注意力free的多层核心修饰器被设计，用于效果地和高效地模型长远依赖关系。最后，一种均方桢被提出，以避免过拟合。基于以上结构，提出的方法可以突破模态差异，获得竞争时间成本下的分化图表示，即使只有小部分训练样本。实验和分析基于三个benchmark数据集和多种 state-of-the-art 方法表明了方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Teaching-for-Multiple-Learners"><a href="#Nonparametric-Teaching-for-Multiple-Learners" class="headerlink" title="Nonparametric Teaching for Multiple Learners"></a>Nonparametric Teaching for Multiple Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10318">http://arxiv.org/abs/2311.10318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chen2hang/mint_nonparametricteaching">https://github.com/chen2hang/mint_nonparametricteaching</a></li>
<li>paper_authors: Chen Zhang, Xiaofeng Cao, Weiyang Liu, Ivor Tsang, James Kwok</li>
<li>for: 本研究旨在解决多学生同时学习的非 Parametric 迭代教学问题，这问题受到单个学生教学设定下的现实世界情况启发，在这种情况下，一名教师通常会向多名学生传授知识。</li>
<li>methods: 我们在本研究中提出了一种新的框架–多学生非 Parametric 教学（MINT），其中教师 aimsto 教育多名学生，每名学生都专注于学习一个拟合值模型。为达到这个目标，我们将问题转化为教学一个向量值模型，并从单个学生教学情况下使用的拟合值希尔伯特空间扩展到向量值空间。</li>
<li>results: 我们证明了 MINT 可以在多名学生可以交流的情况下提供显著的教学速度提升，特别是在重复的单个学生教学情况下。此外，我们还进行了广泛的实验来验证 MINT 的实用性和效率。<details>
<summary>Abstract</summary>
We study the problem of teaching multiple learners simultaneously in the nonparametric iterative teaching setting, where the teacher iteratively provides examples to the learner for accelerating the acquisition of a target concept. This problem is motivated by the gap between current single-learner teaching setting and the real-world scenario of human instruction where a teacher typically imparts knowledge to multiple students. Under the new problem formulation, we introduce a novel framework -- Multi-learner Nonparametric Teaching (MINT). In MINT, the teacher aims to instruct multiple learners, with each learner focusing on learning a scalar-valued target model. To achieve this, we frame the problem as teaching a vector-valued target model and extend the target model space from a scalar-valued reproducing kernel Hilbert space used in single-learner scenarios to a vector-valued space. Furthermore, we demonstrate that MINT offers significant teaching speed-up over repeated single-learner teaching, particularly when the multiple learners can communicate with each other. Lastly, we conduct extensive experiments to validate the practicality and efficiency of MINT.
</details>
<details>
<summary>摘要</summary>
我们研究同时教育多个学习者的问题，这是非参数iterative teaching设定下的教师逐步提供示例，以加速目标概念的掌握。这个问题受到单个学习者教学设定下的现实场景启发，在这种场景中，一个教师通常会向多名学生传授知识。在新的问题设定下，我们提出了一个新的框架——多学习者非参数教学（MINT）。在MINT中，教师需要同时教育多名学习者，每名学习者都需要学习一个含有Scalar值的目标模型。为了实现这一点，我们将问题归类为教育一个向量值目标模型，并从单个学习者的场景中使用的scalar值 reproduce kernel Hilbert space扩展了目标模型空间。此外，我们还证明了MINT在多名学习者之间交流的情况下可以得到明显的教学速度增加。最后，我们进行了广泛的实验来验证MINT的实用性和效率。
</details></li>
</ul>
<hr>
<h2 id="MPSeg-Multi-Phase-strategy-for-coronary-artery-Segmentation"><a href="#MPSeg-Multi-Phase-strategy-for-coronary-artery-Segmentation" class="headerlink" title="MPSeg : Multi-Phase strategy for coronary artery Segmentation"></a>MPSeg : Multi-Phase strategy for coronary artery Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10306">http://arxiv.org/abs/2311.10306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonghoe Ku, Yong-Hee Lee, Junsup Shin, In Kyu Lee, Hyun-Woo Kim</li>
<li>for: 这份论文是为了提供一个新的多阶段方法来自动分类心脏arteries的分类，以便更好地评估 cardiovascular disease。</li>
<li>methods: 这篇论文使用了一个多阶段方法，包括分类 Left Coronary Artery (LCA) 和 Right Coronary Artery (RCA)，然后使用专门的集成模型来执行分类任务。在LCA的分类任务中，使用了一个精确的修正模型以更正初始类别预测。</li>
<li>results: 这篇论文在Automatic Region-based Coronary Artery Disease diagnostics using x-ray angiography imagEs (ARCADE) Segmentation Detection Algorithm challenge at MICCAI 2023 中表现了非常出色的效果。<details>
<summary>Abstract</summary>
Accurate segmentation of coronary arteries is a pivotal process in assessing cardiovascular diseases. However, the intricate structure of the cardiovascular system presents significant challenges for automatic segmentation, especially when utilizing methodologies like the SYNTAX Score, which relies extensively on detailed structural information for precise risk stratification. To address these difficulties and cater to this need, we present MPSeg, an innovative multi-phase strategy designed for coronary artery segmentation. Our approach specifically accommodates these structural complexities and adheres to the principles of the SYNTAX Score. Initially, our method segregates vessels into two categories based on their unique morphological characteristics: Left Coronary Artery (LCA) and Right Coronary Artery (RCA). Specialized ensemble models are then deployed for each category to execute the challenging segmentation task. Due to LCA's higher complexity over RCA, a refinement model is utilized to scrutinize and correct initial class predictions on segmented areas. Notably, our approach demonstrated exceptional effectiveness when evaluated in the Automatic Region-based Coronary Artery Disease diagnostics using x-ray angiography imagEs (ARCADE) Segmentation Detection Algorithm challenge at MICCAI 2023.
</details>
<details>
<summary>摘要</summary>
通过精准分割 coronary arteries 是评估心血管疾病的关键过程。然而，心血管系统的复杂结构对自动分割带来了很大挑战，特别是在使用 SYNTAX Score 方法时。为了解决这些困难并满足这种需求，我们提出了 MPSeg，一种创新的多阶段策略，用于 coronary artery 分割。我们的方法特别注意到 cardiovascular 系统的结构特征，并遵循 SYNTAX Score 的原则。我们的方法首先将血管分为两类，根据它们独特的形态特征：左心脏动脉 (LCA) 和右心脏动脉 (RCA)。然后，我们使用专门设计的集合模型来执行分割任务。由于 LCA 的复杂性比 RCA 更高，因此我们使用修正模型来检查并更正初始分类预测结果。值得注意的是，我们的方法在 MICCAI 2023 年的 Automatic Region-based Coronary Artery Disease diagnostics using x-ray angiography imagEs (ARCADE) Segmentation Detection Algorithm 挑战中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-ViT-knowledge-distillation-network-with-style-transfer-normalization-for-colorectal-liver-metastases-survival-prediction"><a href="#Semi-supervised-ViT-knowledge-distillation-network-with-style-transfer-normalization-for-colorectal-liver-metastases-survival-prediction" class="headerlink" title="Semi-supervised ViT knowledge distillation network with style transfer normalization for colorectal liver metastases survival prediction"></a>Semi-supervised ViT knowledge distillation network with style transfer normalization for colorectal liver metastases survival prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10305">http://arxiv.org/abs/2311.10305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed El Amine Elforaici, Emmanuel Montagnon, Francisco Perdigon Romero, William Trung Le, Feryel Azzi, Dominique Trudel, Bich Nguyen, Simon Turcotte, An Tang, Samuel Kadoury</li>
<li>for: 预测肝脏 метаstatic tumor (CLM) 患者的存活率和疾病进展，以便在系统化化学疗apy 的回应基础上提高精准医学进步。</li>
<li>methods: 我们提出了一种综合方法，包括使用生成对抗网络 (GAN) Normalize histology slides 和 semi-supervised 模型进行组织类划分，以及使用注意力机制来重要性评估不同的 Histology slides 区域。</li>
<li>results: 我们的方法在一个临床数据集上进行评估，表现出优于相关方法，c-index 为 0.804 (0.014) 和 0.733 (0.014)  для OS 和 TTR，同时在TRG 分类任务中，我们的方法可以达到 86.9% 到 90.3% 的准确率和 78.5% 到 82.1% 的准确率。<details>
<summary>Abstract</summary>
Colorectal liver metastases (CLM) significantly impact colon cancer patients, influencing survival based on systemic chemotherapy response. Traditional methods like tumor grading scores (e.g., tumor regression grade - TRG) for prognosis suffer from subjectivity, time constraints, and expertise demands. Current machine learning approaches often focus on radiological data, yet the relevance of histological images for survival predictions, capturing intricate tumor microenvironment characteristics, is gaining recognition. To address these limitations, we propose an end-to-end approach for automated prognosis prediction using histology slides stained with H&E and HPS. We first employ a Generative Adversarial Network (GAN) for slide normalization to reduce staining variations and improve the overall quality of the images that are used as input to our prediction pipeline. We propose a semi-supervised model to perform tissue classification from sparse annotations, producing feature maps. We use an attention-based approach that weighs the importance of different slide regions in producing the final classification results. We exploit the extracted features for the metastatic nodules and surrounding tissue to train a prognosis model. In parallel, we train a vision Transformer (ViT) in a knowledge distillation framework to replicate and enhance the performance of the prognosis prediction. In our evaluation on a clinical dataset of 258 patients, our approach demonstrates superior performance with c-indexes of 0.804 (0.014) for OS and 0.733 (0.014) for TTR. Achieving 86.9% to 90.3% accuracy in predicting TRG dichotomization and 78.5% to 82.1% accuracy for the 3-class TRG classification task, our approach outperforms comparative methods. Our proposed pipeline can provide automated prognosis for pathologists and oncologists, and can greatly promote precision medicine progress in managing CLM patients.
</details>
<details>
<summary>摘要</summary>
来自肝脏 метаstatic colorectal cancer (CLM) 对colon cancer 患者有着重要的影响，影响了存生基于系统化化学疗法的回朋答。传统的方法，如肿瘤分化分数 (e.g., 肿瘤变化度 - TRG) 用于预后预测，受到主观性、时间限制和专业知识的限制。目前的机器学习方法通常专注于放射学数据，但是对存生预测的 histological 影像具有重要的特征，因此获得了更多的认可。为了解决这些限制，我们提出了一个端到端的方法，用于自动预测CLM患者的存生预测。我们首先使用生成对抗网络 (GAN) 来 нормалізу slide，以改善图像质量并减少染色变化。然后，我们提出了一个半supervised模型，用于从稀疏标注中进行组织识别，生成特征地图。我们使用注意力机制，让不同的图像区域在生成最终类别结果中获得不同的重要性。我们利用提取的特征来训练存生预测模型。另外，我们在知识传播框架中将视觉Transformers (ViT) 训练来增强和复制存生预测的表现。在我们在258名病例的临床数据集上进行评估时，我们的方法示出了超过86.9%到90.3%的准确率，对于TRG分类 tasks 的准确率为78.5%到82.1%。我们的方法比较方法更高。我们的提案的管道可以为Pathologist和Oncologist提供自动预测，并可以帮助精确医学进步在处理CLM患者。
</details></li>
</ul>
<hr>
<h2 id="BiHRNet-A-Binary-high-resolution-network-for-Human-Pose-Estimation"><a href="#BiHRNet-A-Binary-high-resolution-network-for-Human-Pose-Estimation" class="headerlink" title="BiHRNet: A Binary high-resolution network for Human Pose Estimation"></a>BiHRNet: A Binary high-resolution network for Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10296">http://arxiv.org/abs/2311.10296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhicheng Zhang, Xueyao Sun, Yonghao Dang, Jianqin Yin</li>
<li>for: 本研究旨在提出一个可行的人体pose估计器（BiHRNet），用于实时应用中的人体pose估计。</li>
<li>methods: 本研究使用了二进制神经网络（BNN），并提出了两种技术来减少由网络二进制化所导致的准确性下降：一种是一种新的损失函数，另一种是一种新的信息重建瓶颈（IR Bottleneck）和多槽基块（MS-Block）的设计。</li>
<li>results: 实验结果表明，BiHRNet在MPII数据集上 achieve PCKh 87.9，而在COCO数据集上 achieve 70.8 mAP，这些结果都高于大多数测试的精简型全精度网络。<details>
<summary>Abstract</summary>
Human Pose Estimation (HPE) plays a crucial role in computer vision applications. However, it is difficult to deploy state-of-the-art models on resouce-limited devices due to the high computational costs of the networks. In this work, a binary human pose estimator named BiHRNet(Binary HRNet) is proposed, whose weights and activations are expressed as $\pm$1. BiHRNet retains the keypoint extraction ability of HRNet, while using fewer computing resources by adapting binary neural network (BNN). In order to reduce the accuracy drop caused by network binarization, two categories of techniques are proposed in this work. For optimizing the training process for binary pose estimator, we propose a new loss function combining KL divergence loss with AWing loss, which makes the binary network obtain more comprehensive output distribution from its real-valued counterpart to reduce information loss caused by binarization. For designing more binarization-friendly structures, we propose a new information reconstruction bottleneck called IR Bottleneck to retain more information in the initial stage of the network. In addition, we also propose a multi-scale basic block called MS-Block for information retention. Our work has less computation cost with few precision drop. Experimental results demonstrate that BiHRNet achieves a PCKh of 87.9 on the MPII dataset, which outperforms all binary pose estimation networks. On the challenging of COCO dataset, the proposed method enables the binary neural network to achieve 70.8 mAP, which is better than most tested lightweight full-precision networks.
</details>
<details>
<summary>摘要</summary>
人体姿势估计（HPE）在计算机视觉应用中扮演着关键性的角色。然而，由于现有的状态艺术网络在资源有限的设备上部署困难，因此在这种情况下，一种名为BiHRNet的二进制人体姿势估计器被提出。BiHRNet在保持人体姿势估计的能力的同时，使用了更少的计算资源，通过适应二进制神经网络（BNN）进行减少计算成本。为了减少由网络二进制化引起的准确性下降，本文提出了两种类型的技术。首先，我们提出了一种新的损失函数，它将KL散度损失与AWing损失相结合，以使得二进制网络从其浮点对应的网络获得更全面的输出分布，从而减少由二进制化引起的信息损失。其次，我们提出了一种新的信息重建瓶颈，称为IR瓶颈，以保留在网络的初始阶段更多的信息。此外，我们还提出了一种多尺度基本块，称为MS-Block，以保持更多的信息。我们的方法具有较低的计算成本和减少的精度下降。实验结果表明，BiHRNet在MPII dataset上 achievied PCKh的87.9，超过了所有二进制姿势估计网络。在COCO dataset上，我们的方法使得二进制神经网络在70.8 mAP的情况下获得了更好的表现，超过了大多数测试的轻量级整数网络。
</details></li>
</ul>
<hr>
<h2 id="Text-to-Sticker-Style-Tailoring-Latent-Diffusion-Models-for-Human-Expression"><a href="#Text-to-Sticker-Style-Tailoring-Latent-Diffusion-Models-for-Human-Expression" class="headerlink" title="Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human Expression"></a>Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human Expression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10794">http://arxiv.org/abs/2311.10794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Animesh Sinha, Bo Sun, Anmol Kalia, Arantxa Casanova, Elliot Blanchard, David Yan, Winnie Zhang, Tony Nelli, Jiahui Chen, Hardik Shah, Licheng Yu, Mitesh Kumar Singh, Ankit Ramchandani, Maziar Sanjabi, Sonal Gupta, Amy Bearman, Dhruv Mahajan</li>
<li>for: 该论文 targets 在 distinct domain 中使用 Latent Diffusion Models (LDMs) 进行图像生成，以提高图像质量、描述性和场景多样性。</li>
<li>methods: 该论文使用了Prompt Engineering和Human-in-the-Loop (HITL) Alignment和Style datasets来finetune Emu模型，以提高图像的描述性和风格匹配。</li>
<li>results: 论文的实验结果表明，使用 Style Tailoring 方法可以提高图像质量（14%）、描述性（16.2%）和场景多样性（15.3%），相比于基于 Emu 模型的描述性引擎。<details>
<summary>Abstract</summary>
We introduce Style Tailoring, a recipe to finetune Latent Diffusion Models (LDMs) in a distinct domain with high visual quality, prompt alignment and scene diversity. We choose sticker image generation as the target domain, as the images significantly differ from photorealistic samples typically generated by large-scale LDMs. We start with a competent text-to-image model, like Emu, and show that relying on prompt engineering with a photorealistic model to generate stickers leads to poor prompt alignment and scene diversity. To overcome these drawbacks, we first finetune Emu on millions of sticker-like images collected using weak supervision to elicit diversity. Next, we curate human-in-the-loop (HITL) Alignment and Style datasets from model generations, and finetune to improve prompt alignment and style alignment respectively. Sequential finetuning on these datasets poses a tradeoff between better style alignment and prompt alignment gains. To address this tradeoff, we propose a novel fine-tuning method called Style Tailoring, which jointly fits the content and style distribution and achieves best tradeoff. Evaluation results show our method improves visual quality by 14%, prompt alignment by 16.2% and scene diversity by 15.3%, compared to prompt engineering the base Emu model for stickers generation.
</details>
<details>
<summary>摘要</summary>
我们介绍 Style Tailoring，一种精度调整潜在扩散模型（LDM）的独特领域recipe，以高质量的视觉和文本映射、场景多样性为目标。我们选择了贴纸图像生成作为目标领域，因为这些图像与大规模 LDM 通常生成的 фото真实样式很不同。我们开始 WITH 一个能够文本图像生成的竞争力强的模型，如 Emu，并显示了，通过倚靠写入 photorealistic 模型来生成贴纸会导致文本映射和场景多样性受损。为了缓解这些缺陷，我们首先在弱监督下对 Emu 进行了数百万个贴纸样本的训练，以启动多样性。然后，我们 manually curate 了人类在 loop（HITL）的对应和风格数据集，并进行了进一步的训练，以提高对应和风格的映射。我们发现在这些数据集上进行顺序的训练存在一定的负面关系，即改进对应和风格的映射可能会导致另一个方面的损失。为了解决这个负面关系，我们提出了一种新的训练方法，即 Style Tailoring。我们的方法可以同时调整内容和风格的分布，并实现最佳的负面关系。我们的评估结果表明，相比于基于 Emu 模型的贴纸生成，我们的方法可以提高视觉质量14%, 文本映射16.2%, 场景多样性15.3%。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Pruning-of-Deep-Ensembles-with-Focal-Diversity"><a href="#Hierarchical-Pruning-of-Deep-Ensembles-with-Focal-Diversity" class="headerlink" title="Hierarchical Pruning of Deep Ensembles with Focal Diversity"></a>Hierarchical Pruning of Deep Ensembles with Focal Diversity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10293">http://arxiv.org/abs/2311.10293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanzhao Wu, Ka-Ho Chow, Wenqi Wei, Ling Liu<br>for: 这篇论文旨在提出一种新的深度神经网络集成预选方法，以提高集成的普遍性和可靠性，并且可以有效地降低集成执行的时间和空间成本。methods: 该方法基于三种新的集成预选技术，包括：1) 基于焦点多样性指标的集成预选方法，可以准确地捕捉集成中每个网络的补做能力；2) 基于层次结构的集成预选方法，可以逐层找到低成本高准确性的深度 ensemble；3) 基于多个焦点多样性指标的集成协调方法，可以融合多个焦点多样性指标，以提高集成预选的精度和可靠性。results: 在使用 популяр的 benchmark 数据集上进行测试，我们 demonstarted that the proposed hierarchical ensemble pruning approach can effectively identify high-quality deep ensembles with better generalizability while being more time and space efficient in ensemble decision-making.<details>
<summary>Abstract</summary>
Deep neural network ensembles combine the wisdom of multiple deep neural networks to improve the generalizability and robustness over individual networks. It has gained increasing popularity to study deep ensemble techniques in the deep learning community. Some mission-critical applications utilize a large number of deep neural networks to form deep ensembles to achieve desired accuracy and resilience, which introduces high time and space costs for ensemble execution. However, it still remains a critical challenge whether a small subset of the entire deep ensemble can achieve the same or better generalizability and how to effectively identify these small deep ensembles for improving the space and time efficiency of ensemble execution. This paper presents a novel deep ensemble pruning approach, which can efficiently identify smaller deep ensembles and provide higher ensemble accuracy than the entire deep ensemble of a large number of member networks. Our hierarchical ensemble pruning approach (HQ) leverages three novel ensemble pruning techniques. First, we show that the focal diversity metrics can accurately capture the complementary capacity of the member networks of an ensemble, which can guide ensemble pruning. Second, we design a focal diversity based hierarchical pruning approach, which will iteratively find high quality deep ensembles with low cost and high accuracy. Third, we develop a focal diversity consensus method to integrate multiple focal diversity metrics to refine ensemble pruning results, where smaller deep ensembles can be effectively identified to offer high accuracy, high robustness and high efficiency. Evaluated using popular benchmark datasets, we demonstrate that the proposed hierarchical ensemble pruning approach can effectively identify high quality deep ensembles with better generalizability while being more time and space efficient in ensemble decision making.
</details>
<details>
<summary>摘要</summary>
深度神经网络集成 combines the wisdom of multiple deep neural networks to improve the generalizability and robustness over individual networks. It has gained increasing popularity to study deep ensemble techniques in the deep learning community. Some mission-critical applications utilize a large number of deep neural networks to form deep ensembles to achieve desired accuracy and resilience, which introduces high time and space costs for ensemble execution. However, it still remains a critical challenge whether a small subset of the entire deep ensemble can achieve the same or better generalizability and how to effectively identify these small deep ensembles for improving the space and time efficiency of ensemble execution. This paper presents a novel deep ensemble pruning approach, which can efficiently identify smaller deep ensembles and provide higher ensemble accuracy than the entire deep ensemble of a large number of member networks. Our hierarchical ensemble pruning approach (HQ) leverages three novel ensemble pruning techniques. First, we show that the focal diversity metrics can accurately capture the complementary capacity of the member networks of an ensemble, which can guide ensemble pruning. Second, we design a focal diversity based hierarchical pruning approach, which will iteratively find high quality deep ensembles with low cost and high accuracy. Third, we develop a focal diversity consensus method to integrate multiple focal diversity metrics to refine ensemble pruning results, where smaller deep ensembles can be effectively identified to offer high accuracy, high robustness and high efficiency. Evaluated using popular benchmark datasets, we demonstrate that the proposed hierarchical ensemble pruning approach can effectively identify high quality deep ensembles with better generalizability while being more time and space efficient in ensemble decision making.
</details></li>
</ul>
<hr>
<h2 id="SSASS-Semi-Supervised-Approach-for-Stenosis-Segmentation"><a href="#SSASS-Semi-Supervised-Approach-for-Stenosis-Segmentation" class="headerlink" title="SSASS: Semi-Supervised Approach for Stenosis Segmentation"></a>SSASS: Semi-Supervised Approach for Stenosis Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10281">http://arxiv.org/abs/2311.10281</a></li>
<li>repo_url: None</li>
<li>paper_authors: In Kyu Lee, Junsup Shin, Yong-Hee Lee, Jonghoe Ku, Hyun-Woo Kim</li>
<li>for: 这篇论文是为了帮助医疗专业人员更准确地评估患者的情况，尤其是在 coronary angiography (CAG) 中准确识别 coronary artery stenosis 的疾病风险。</li>
<li>methods: 我们提出了一种 semi-supervised 方法，通过数据扩充和 pseudo-label-based 学习技术，以便更好地处理 coronary artery 的复杂结构和 X-ray 图像中的隐藏噪声。</li>
<li>results: 我们的方法在 Automatic Region-based Coronary Artery Disease diagnostics using x-ray angiography imagEs (ARCADE) Stenosis Detection Algorithm 挑战中表现出色，只需要一个模型，而不需要 ensemble 多个模型。这种成功表明我们的方法具有 Automatic 和高效的特点，可以帮助医疗专业人员更准确地评估患者的情况。<details>
<summary>Abstract</summary>
Coronary artery stenosis is a critical health risk, and its precise identification in Coronary Angiography (CAG) can significantly aid medical practitioners in accurately evaluating the severity of a patient's condition. The complexity of coronary artery structures combined with the inherent noise in X-ray images poses a considerable challenge to this task. To tackle these obstacles, we introduce a semi-supervised approach for cardiovascular stenosis segmentation. Our strategy begins with data augmentation, specifically tailored to replicate the structural characteristics of coronary arteries. We then apply a pseudo-label-based semi-supervised learning technique that leverages the data generated through our augmentation process. Impressively, our approach demonstrated an exceptional performance in the Automatic Region-based Coronary Artery Disease diagnostics using x-ray angiography imagEs (ARCADE) Stenosis Detection Algorithm challenge by utilizing a single model instead of relying on an ensemble of multiple models. This success emphasizes our method's capability and efficiency in providing an automated solution for accurately assessing stenosis severity from medical imaging data.
</details>
<details>
<summary>摘要</summary>
coronary artery stenosis 是一个严重的健康风险，并且准确评估患者的状况的精准识别在 coronary angiography (CAG) 中非常重要。 However, the complexity of coronary artery structures and the inherent noise in X-ray images pose significant challenges to this task. To overcome these challenges, we propose a semi-supervised approach for cardiovascular stenosis segmentation. Our approach begins with data augmentation, specifically tailored to replicate the structural characteristics of coronary arteries. We then apply a pseudo-label-based semi-supervised learning technique that leverages the data generated through our augmentation process. Notably, our approach achieved an exceptional performance in the Automatic Region-based Coronary Artery Disease diagnostics using x-ray angiography imagEs (ARCADE) Stenosis Detection Algorithm challenge by utilizing a single model instead of relying on an ensemble of multiple models. This success highlights our method's capability and efficiency in providing an automated solution for accurately assessing stenosis severity from medical imaging data.Here's the word-for-word translation of the text into Simplified Chinese: coronary artery stenosis 是一个严重的健康风险，并且准确评估患者的状况的精准识别在 coronary angiography (CAG) 中非常重要。然而， coronary artery structures 的复杂性和 X-ray images 中的自然噪音 pose significant challenges to this task. To overcome these challenges, we propose a semi-supervised approach for cardiovascular stenosis segmentation. our approach begins with data augmentation, specifically tailored to replicate the structural characteristics of coronary arteries. We then apply a pseudo-label-based semi-supervised learning technique that leverages the data generated through our augmentation process. notable, our approach achieved an exceptional performance in the Automatic Region-based Coronary Artery Disease diagnostics using x-ray angiography imagEs (ARCADE) Stenosis Detection Algorithm challenge by utilizing a single model instead of relying on an ensemble of multiple models. This success highlights our method's capability and efficiency in providing an automated solution for accurately assessing stenosis severity from medical imaging data.
</details></li>
</ul>
<hr>
<h2 id="Vision-meets-mmWave-Radar-3D-Object-Perception-Benchmark-for-Autonomous-Driving"><a href="#Vision-meets-mmWave-Radar-3D-Object-Perception-Benchmark-for-Autonomous-Driving" class="headerlink" title="Vision meets mmWave Radar: 3D Object Perception Benchmark for Autonomous Driving"></a>Vision meets mmWave Radar: 3D Object Perception Benchmark for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10261">http://arxiv.org/abs/2311.10261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhou Wang, Jen-Hao Cheng, Jui-Te Huang, Sheng-Yao Kuan, Qiqian Fu, Chiming Ni, Shengyu Hao, Gaoang Wang, Guanbin Xing, Hui Liu, Jenq-Neng Hwang</li>
<li>for: 这个论文主要是为了提出一个新的自动驾驶识别系统，利用摄像头和雷达的结合来实现高精度和可靠性。</li>
<li>methods: 这个论文使用了一种新的数据集CRUW3D，包含66万个同步和准确卡口、雷达和LiDAR的帧，以及不同的驾驶enario。另外，这个论文还使用了Radio Frequency（RF）张量来表示雷达数据，这种张量包含了3D位置信息以及空间时间 semantics信息。</li>
<li>results: 这个论文通过对摄像头和雷达数据进行融合，实现了更高的识别精度和可靠性。这种融合方法可以在不同的照明和天气情况下实现更好的性能，并且可以提供更多的semantic信息来支持更高级别的自动驾驶功能。<details>
<summary>Abstract</summary>
Sensor fusion is crucial for an accurate and robust perception system on autonomous vehicles. Most existing datasets and perception solutions focus on fusing cameras and LiDAR. However, the collaboration between camera and radar is significantly under-exploited. The incorporation of rich semantic information from the camera, and reliable 3D information from the radar can potentially achieve an efficient, cheap, and portable solution for 3D object perception tasks. It can also be robust to different lighting or all-weather driving scenarios due to the capability of mmWave radars. In this paper, we introduce the CRUW3D dataset, including 66K synchronized and well-calibrated camera, radar, and LiDAR frames in various driving scenarios. Unlike other large-scale autonomous driving datasets, our radar data is in the format of radio frequency (RF) tensors that contain not only 3D location information but also spatio-temporal semantic information. This kind of radar format can enable machine learning models to generate more reliable object perception results after interacting and fusing the information or features between the camera and radar.
</details>
<details>
<summary>摘要</summary>
感知融合是自动驾驶车辆准确和可靠的感知系统中的关键。大多数现有的数据集和感知解决方案都将关注相机和LiDAR的整合。然而，相机和雷达之间的合作仍然被忽视了。将 ricoh semantic information from the camera and reliable 3D information from the radar fusion 可能实现一个高效、便宜、可搬的解决方案 для 3D 物体感知任务。它还可以在不同的照明或天气驾驶enario中具有更高的可靠性，因为雷达具有 millimeter wave 的特点。在本文中，我们介绍了 CRUW3D 数据集，包括 66 万个同步和准确地测量的相机、雷达和 LiDAR 帧，在不同的驾驶enario中。与其他大规模自动驾驶数据集不同的是，我们的雷达数据以 radio frequency (RF) 张量的形式提供，该张量包含不仅 3D 位置信息，还有空间时间semantic信息。这种雷达格式可以让机器学习模型通过相机和雷达之间的信息或特征交互和融合来生成更可靠的物体感知结果。
</details></li>
</ul>
<hr>
<h2 id="UniMOS-A-Universal-Framework-For-Multi-Organ-Segmentation-Over-Label-Constrained-Datasets"><a href="#UniMOS-A-Universal-Framework-For-Multi-Organ-Segmentation-Over-Label-Constrained-Datasets" class="headerlink" title="UniMOS: A Universal Framework For Multi-Organ Segmentation Over Label-Constrained Datasets"></a>UniMOS: A Universal Framework For Multi-Organ Segmentation Over Label-Constrained Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10251">http://arxiv.org/abs/2311.10251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lw8807001/unimos">https://github.com/lw8807001/unimos</a></li>
<li>paper_authors: Can Li, Sheng Shao, Junyi Qu, Shuchao Pang, Mehmet A. Orgun</li>
<li>for: This paper aims to provide a universal framework for medical image segmentation tasks, which can utilize fully and partially labeled images as well as unlabeled images.</li>
<li>methods: The proposed framework, called UniMOS, uses a Multi-Organ Segmentation (MOS) module over fully&#x2F;partially labeled data as the basenet, and incorporates a semi-supervised training module that combines consistent regularization and pseudolabeling techniques on unlabeled data.</li>
<li>results: The experiments show that the UniMOS framework exhibits excellent performance in several medical image segmentation tasks compared to other advanced methods, and also significantly improves data utilization and reduces annotation cost.Here’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文目的是提供一个通用的医学图像分割框架，可以利用全部和部分标注图像以及无标注图像。</li>
<li>methods: 提议的UniMOS框架使用Multi-Organ Segmentation（MOS）模块来基于全部&#x2F;部分标注数据进行基准，并将一种半supervised训练模块与无标注数据进行结合，该模块使用了一致的常规化和pseudolabeling技术。</li>
<li>results: 实验表明，UniMOS框架在几种医学图像分割任务中表现出色，与其他先进方法相比，也有显著提高数据利用率和减少标注成本。<details>
<summary>Abstract</summary>
Machine learning models for medical images can help physicians diagnose and manage diseases. However, due to the fact that medical image annotation requires a great deal of manpower and expertise, as well as the fact that clinical departments perform image annotation based on task orientation, there is the problem of having fewer medical image annotation data with more unlabeled data and having many datasets that annotate only a single organ. In this paper, we present UniMOS, the first universal framework for achieving the utilization of fully and partially labeled images as well as unlabeled images. Specifically, we construct a Multi-Organ Segmentation (MOS) module over fully/partially labeled data as the basenet and designed a new target adaptive loss. Furthermore, we incorporate a semi-supervised training module that combines consistent regularization and pseudolabeling techniques on unlabeled data, which significantly improves the segmentation of unlabeled data. Experiments show that the framework exhibits excellent performance in several medical image segmentation tasks compared to other advanced methods, and also significantly improves data utilization and reduces annotation cost. Code and models are available at: https://github.com/lw8807001/UniMOS.
</details>
<details>
<summary>摘要</summary>
医疗影像机器学习模型可以帮助医生诊断和管理疾病。然而，由于医疗影像标注需要很大的人力和专业知识，以及临床部门根据任务方向进行标注，导致有 fewer 的医疗影像标注数据和更多的未标注数据，以及许多只标注一个器官的数据集。在这篇论文中，我们提出了UniMOS，第一个可以实现充分利用完全/部分标注图像和未标注图像的框架。具体来说，我们在完全/部分标注数据上构建了多器官分割（MOS）模块作为基础网络，并设计了一种新的目标适应损失函数。此外，我们还将一种semi-supervised Training模块 incorporated into the framework，该模块将在无标注数据上进行一致准确 regularization 和 pseudolabeling 技术，以显著提高未标注数据的分割性能。实验表明，该框架在多个医疗影像分割任务中表现出色，比其他进步方法更好，同时也有利用数据和减少标注成本。代码和模型可以在：https://github.com/lw8807001/UniMOS 中找到。
</details></li>
</ul>
<hr>
<h2 id="Segment-Anything-in-Defect-Detection"><a href="#Segment-Anything-in-Defect-Detection" class="headerlink" title="Segment Anything in Defect Detection"></a>Segment Anything in Defect Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10245">http://arxiv.org/abs/2311.10245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bozhenhhu/DefectSAM">https://github.com/bozhenhhu/DefectSAM</a></li>
<li>paper_authors: Bozhen Hu, Bin Gao, Cheng Tan, Tongle Wu, Stan Z. Li<br>for: 这个研究旨在提高非破坏性测试系统中的缺陷检测精度，使用非接触、安全、高效的检测能力。methods: 本研究使用了一个新的方法，即DefectSAM，基于Segment Anything（SAM）模型，并使用了一个精心组制的实验室实验数据集和专业人员的讯息，以超越现有的州际状态艺术法。results: 实验结果显示，DefectSAM可以优化缺陷检测率，尤其是检测较弱和小型缺陷的能力，并获得更 precisione 的缺陷大小估计。此外，DefectSAM在不同材料上进行了实验，并证明了其在缺陷检测中的可靠性和有效性。<details>
<summary>Abstract</summary>
Defect detection plays a crucial role in infrared non-destructive testing systems, offering non-contact, safe, and efficient inspection capabilities. However, challenges such as low resolution, high noise, and uneven heating in infrared thermal images hinder comprehensive and accurate defect detection. In this study, we propose DefectSAM, a novel approach for segmenting defects on highly noisy thermal images based on the widely adopted model, Segment Anything (SAM)\cite{kirillov2023segany}. Harnessing the power of a meticulously curated dataset generated through labor-intensive lab experiments and valuable prompts from experienced experts, DefectSAM surpasses existing state-of-the-art segmentation algorithms and achieves significant improvements in defect detection rates. Notably, DefectSAM excels in detecting weaker and smaller defects on complex and irregular surfaces, reducing the occurrence of missed detections and providing more accurate defect size estimations. Experimental studies conducted on various materials have validated the effectiveness of our solutions in defect detection, which hold significant potential to expedite the evolution of defect detection tools, enabling enhanced inspection capabilities and accuracy in defect identification.
</details>
<details>
<summary>摘要</summary>
异常检测在红外非破坏测试系统中扮演着关键角色，提供无接触、安全、高效的检测能力。然而，红外热图像中的低分辨率、高噪声和不均匀热辐射等挑战使得全面和准确的异常检测变得困难。本研究提出了基于广泛采用的模型Segment Anything（SAM）的新方法——异常检测（DefectSAM），通过精心制作的劳动atorydataset和经验丰富的专家提示，超越现有状态的最佳分割算法，提高异常检测率。特别是，DefectSAM在复杂和不规则表面上检测弱小异常，降低错过检测的发生率，并提供更准确的异常大小估计。在不同材料上进行的实验研究证明了我们的解决方案在异常检测方面的有效性，这些解决方案具有提高检测工具的精度和准确性，并促进异常 indentification的进步。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/17/cs.CV_2023_11_17/" data-id="clpxp6c2l00ncee886c7752k9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/4/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
