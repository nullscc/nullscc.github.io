
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/3/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.LG_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T10:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.LG_2023_10_30/">cs.LG - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Subgraph-GNNs-by-Learning-Effective-Selection-Policies"><a href="#Efficient-Subgraph-GNNs-by-Learning-Effective-Selection-Policies" class="headerlink" title="Efficient Subgraph GNNs by Learning Effective Selection Policies"></a>Efficient Subgraph GNNs by Learning Effective Selection Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20082">http://arxiv.org/abs/2310.20082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro, Haggai Maron</li>
<li>for: 本文旨在学习从可能的子图集中选择一小 subsets，以提高Subgraph GNNs的应用性。</li>
<li>methods: 本文提出了一种新的Policy-Learn方法，通过iterative manner来学习选择子图。</li>
<li>results: 实验结果表明，Policy-Learn在多种 dataset上都能够超越现有的基准值。<details>
<summary>Abstract</summary>
Subgraph GNNs are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of learning to select a small subset of the large set of possible subgraphs in a data-driven fashion. We first motivate the problem by proving that there are families of WL-indistinguishable graphs for which there exist efficient subgraph selection policies: small subsets of subgraphs that can already identify all the graphs within the family. We then propose a new approach, called Policy-Learn, that learns how to select subgraphs in an iterative manner. We prove that, unlike popular random policies and prior work addressing the same problem, our architecture is able to learn the efficient policies mentioned above. Our experimental results demonstrate that Policy-Learn outperforms existing baselines across a wide range of datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>图гра非常表达的神经网络（Subgraph GNNs）可以从多个子图中学习图表示。然而，它们的应用受到计算复杂性的限制，因为需要在许多子图上进行消息传递。在这篇论文中，我们考虑了选择一小集合的可能的子图的问题。我们首先证明了存在一些家族的无益图（WL-indistinguishable graphs），其中存在高效的子图选择策略：小 subsets of subgraphs 可以已经识别整个家族中的所有图。然后，我们提出了一种新的方法，叫做 Policy-Learn，它在循环的方式学习如何选择子图。我们证明了，与流行的随机策略和先前的相同问题的解决方案不同，我们的架构可以学习高效的策略。我们的实验结果表明，Policy-Learn 在各种数据集上超过现有的基elines。
</details></li>
</ul>
<hr>
<h2 id="Hybridizing-Physics-and-Neural-ODEs-for-Predicting-Plasma-Inductance-Dynamics-in-Tokamak-Fusion-Reactors"><a href="#Hybridizing-Physics-and-Neural-ODEs-for-Predicting-Plasma-Inductance-Dynamics-in-Tokamak-Fusion-Reactors" class="headerlink" title="Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors"></a>Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20079">http://arxiv.org/abs/2310.20079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allen M. Wang, Darren T. Garnier, Cristina Rea</li>
<li>for: 这项研究旨在提高核聚变炉的控制精度，以便实现更高效的能源生产。</li>
<li>methods: 该研究使用神经 ordinary differential equations（ODE）框架来预测核聚变炉中的气态动力学行为，并结合物理学基本方程来限制神经网络模型。</li>
<li>results: 研究发现，将物理学基本方程与神经网络模型相结合，可以提高预测气态动力学行为的准确性，比已有的物理学推导式ODE和纯神经网络模型都更好。<details>
<summary>Abstract</summary>
While fusion reactors known as tokamaks hold promise as a firm energy source, advances in plasma control, and handling of events where control of plasmas is lost, are needed for them to be economical. A significant bottleneck towards applying more advanced control algorithms is the need for better plasma simulation, where both physics-based and data-driven approaches currently fall short. The former is bottle-necked by both computational cost and the difficulty of modelling plasmas, and the latter is bottle-necked by the relative paucity of data. To address this issue, this work applies the neural ordinary differential equations (ODE) framework to the problem of predicting a subset of plasma dynamics, namely the coupled plasma current and internal inductance dynamics. As the neural ODE framework allows for the natural inclusion of physics-based inductive biases, we train both physics-based and neural network models on data from the Alcator C-Mod fusion reactor and find that a model that combines physics-based equations with a neural ODE performs better than both existing physics-motivated ODEs and a pure neural ODE model.
</details>
<details>
<summary>摘要</summary>
tokamak核聚变反应堆具有成本能源的承诺，但是需要进一步的材料控制和失控事件处理技术才能实现经济性。目前控制算法的应用受到plasma simulate的限制，physics-based和data-driven两种方法都有瓶颈。前者由计算成本和模拟plasma困难而受限，后者由数据的缺乏而受限。为解决这个问题，本工作采用神经网络 ordinary differential equations（ODE）框架来预测plasma动力学中的一部分，即核聚变和内 inductance动力学。由于神经网络ODE框架可以自然地包含物理学基础的束缚，我们在Alcator C-Mod核聚变反应堆数据上训练了physics-based和神经网络模型，发现一个combines physics-based方程和神经网络ODE模型的模型在 physics-motivated ODE和纯神经网络模型之上表现更好。
</details></li>
</ul>
<hr>
<h2 id="Meek-Separators-and-Their-Applications-in-Targeted-Causal-Discovery"><a href="#Meek-Separators-and-Their-Applications-in-Targeted-Causal-Discovery" class="headerlink" title="Meek Separators and Their Applications in Targeted Causal Discovery"></a>Meek Separators and Their Applications in Targeted Causal Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20075">http://arxiv.org/abs/2310.20075</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uhlerlab/meek_sep">https://github.com/uhlerlab/meek_sep</a></li>
<li>paper_authors: Kirankumar Shiragur, Jiaqi Zhang, Caroline Uhler</li>
<li>for: 学习干预结构 FROM 干预数据，特别是只需要学习部分 causal graph 的情况。</li>
<li>methods: 我们引入了 $Meek~separator$，它是一个特定的子集 vertices，当 intervened 时，可以将剩下的未oriented edges 分解成小的连接组件。我们还提出了两种随机算法，用于实现 subset search 和 causal matching。</li>
<li>results: 我们的结果提供了首个known average-case 证明保证，用于 subset search 和 causal matching 问题。我们认为这将开启更多targeted causal structure learning问题的解决方案的可能性。<details>
<summary>Abstract</summary>
Learning causal structures from interventional data is a fundamental problem with broad applications across various fields. While many previous works have focused on recovering the entire causal graph, in practice, there are scenarios where learning only part of the causal graph suffices. This is called $targeted$ causal discovery. In our work, we focus on two such well-motivated problems: subset search and causal matching. We aim to minimize the number of interventions in both cases.   Towards this, we introduce the $Meek~separator$, which is a subset of vertices that, when intervened, decomposes the remaining unoriented edges into smaller connected components. We then present an efficient algorithm to find Meek separators that are of small sizes. Such a procedure is helpful in designing various divide-and-conquer-based approaches. In particular, we propose two randomized algorithms that achieve logarithmic approximation for subset search and causal matching, respectively. Our results provide the first known average-case provable guarantees for both problems. We believe that this opens up possibilities to design near-optimal methods for many other targeted causal structure learning problems arising from various applications.
</details>
<details>
<summary>摘要</summary>
Towards this, we introduce the Meek separator, which is a subset of vertices that, when intervened, decomposes the remaining unoriented edges into smaller connected components. We then present an efficient algorithm to find Meek separators that are of small sizes. Such a procedure is helpful in designing various divide-and-conquer-based approaches. In particular, we propose two randomized algorithms that achieve logarithmic approximation for subset search and causal matching, respectively. Our results provide the first known average-case provable guarantees for both problems. We believe that this opens up possibilities to design near-optimal methods for many other targeted causal structure learning problems arising from various applications.Translated into Simplified Chinese:学习 causal 结构从 intervening 数据是一个基本的问题，它在各个领域中有广泛的应用。虽然许多前一些工作都是Focus on recovering the entire causal graph，但在实践中有情况是只需要学习 causal 结构的一部分。这被称为 targeted causal discovery。在我们的工作中，我们关注了两个这样的有利场景：subset search和causal matching。我们想要最小化 intervened 的数量。为了实现这一目标，我们引入 Meek  separator，它是一个被 intervened 的顶点集，其中的顶点被 intervened 后，可以将剩下的未oriented 边分解成更小的连接组件。我们然后提出了一种高效的算法来找到 Meek  separator，其中的顶点数量尽可能小。这种过程对于设计 divide-and-conquer 基于的方法非常有用。具体来说，我们提出了两种随机算法，它们可以对 subset search 和 causal matching 问题实现 logarithmic 的近似，并且我们的结果提供了这些问题的首个平均情况可证 guarantees。我们认为这些结果将开启大量的可优化 targeted causal structure learning 问题的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Decentralised-Scalable-and-Privacy-Preserving-Synthetic-Data-Generation"><a href="#Decentralised-Scalable-and-Privacy-Preserving-Synthetic-Data-Generation" class="headerlink" title="Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation"></a>Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20062">http://arxiv.org/abs/2310.20062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishal Ramesh, Rui Zhao, Naman Goel<br>for:The paper is written for the purpose of exploring the use of synthetic data in machine learning, with a focus on privacy-preserving methods for generating synthetic data.methods:The paper uses a novel system that combines Solid (Social Linked Data), MPC (Secure Multi-Party Computation), and TEEs (Trusted Execution Environments) to generate differentially private synthetic data.results:The paper demonstrates the effectiveness of their approach through rigorous empirical results on simulated and real datasets, and shows that their method can address various challenges in responsible and trustworthy synthetic data generation, including contributor autonomy, decentralization, privacy, and scalability.<details>
<summary>Abstract</summary>
Synthetic data is emerging as a promising way to harness the value of data, while reducing privacy risks. The potential of synthetic data is not limited to privacy-friendly data release, but also includes complementing real data in use-cases such as training machine learning algorithms that are more fair and robust to distribution shifts etc. There is a lot of interest in algorithmic advances in synthetic data generation for providing better privacy and statistical guarantees and for its better utilisation in machine learning pipelines. However, for responsible and trustworthy synthetic data generation, it is not sufficient to focus only on these algorithmic aspects and instead, a holistic view of the synthetic data generation pipeline must be considered. We build a novel system that allows the contributors of real data to autonomously participate in differentially private synthetic data generation without relying on a trusted centre. Our modular, general and scalable solution is based on three building blocks namely: Solid (Social Linked Data), MPC (Secure Multi-Party Computation) and Trusted Execution Environments (TEEs). Solid is a specification that lets people store their data securely in decentralised data stores called Pods and control access to their data. MPC refers to the set of cryptographic methods for different parties to jointly compute a function over their inputs while keeping those inputs private. TEEs such as Intel SGX rely on hardware based features for confidentiality and integrity of code and data. We show how these three technologies can be effectively used to address various challenges in responsible and trustworthy synthetic data generation by ensuring: 1) contributor autonomy, 2) decentralisation, 3) privacy and 4) scalability. We support our claims with rigorous empirical results on simulated and real datasets and different synthetic data generation algorithms.
</details>
<details>
<summary>摘要</summary>
人工数据正在成为一种有前途的方法，以获得数据的价值，同时降低隐私风险。人工数据的潜在价值不仅限于隐私友好的数据发布，还包括补充实际数据在训练机器学习算法等场景中的使用。目前有很大的兴趣在人工数据生成算法方面的技术进步，以提供更好的隐私和统计保证，并在机器学习管道中更好地使用人工数据。但是，为了负责任和信任worthy的人工数据生成，不能仅仅关注算法方面，而是需要考虑整个人工数据生成管道的各个方面。我们开发了一个新的系统，使得实际数据的 contribuutors可以自主参与在权限匿名的情况下进行权限匿名的情况下进行权限匿名的人工数据生成。我们的模块化、通用和可扩展的解决方案基于以下三个建造件：Solid（社会链接数据）、MPC（安全多方计算）和TEEs（可信执行环境）。Solid是一种规范，允许人们在分布式数据存储called Pods中安全存储自己的数据，并控制对自己的数据的访问。MPC是一种用于不同党计算共同计算函数的 cryptographic 方法，以保持各个党的输入私有。TEEs，如Intel SGX，基于硬件特性，以确保代码和数据的机密性和完整性。我们证明了这三种技术可以有效地解决负责任和信任worthy的人工数据生成中的各种挑战，包括：1）参与者自主性，2）分布式，3）隐私和4）可扩展性。我们支持我们的主张通过对 simulated 和实际数据集和不同的人工数据生成算法进行严格的实验结果。
</details></li>
</ul>
<hr>
<h2 id="AdaSub-Stochastic-Optimization-Using-Second-Order-Information-in-Low-Dimensional-Subspaces"><a href="#AdaSub-Stochastic-Optimization-Using-Second-Order-Information-in-Low-Dimensional-Subspaces" class="headerlink" title="AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces"></a>AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20060">http://arxiv.org/abs/2310.20060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jvictormata/adasub">https://github.com/jvictormata/adasub</a></li>
<li>paper_authors: João Victor Galvão da Mata, Martin S. Andersen</li>
<li>for: 提高 Stochastic Optimization 算法的效率和精度，使其在实际应用中更加可靠。</li>
<li>methods: 使用第二阶信息计算搜索方向，并在低维度子空间中进行随机搜索。</li>
<li>results: 比较具有时间和迭代次数的精度和效率，超过了流行的随机优化器。<details>
<summary>Abstract</summary>
We introduce AdaSub, a stochastic optimization algorithm that computes a search direction based on second-order information in a low-dimensional subspace that is defined adaptively based on available current and past information. Compared to first-order methods, second-order methods exhibit better convergence characteristics, but the need to compute the Hessian matrix at each iteration results in excessive computational expenses, making them impractical. To address this issue, our approach enables the management of computational expenses and algorithm efficiency by enabling the selection of the subspace dimension for the search. Our code is freely available on GitHub, and our preliminary numerical results demonstrate that AdaSub surpasses popular stochastic optimizers in terms of time and number of iterations required to reach a given accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍AdaSub，一种随机优化算法，它基于目前和过去信息中的第二项信息来计算搜寻方向。相比于首项方法，第二项方法具有更好的均衡特性，但是计算梯度矩阵的需要在每个迭代运算中导致过度的计算成本，使其无法实际应用。为解决这个问题，我们的方法可以运算计算成本和算法效率的管理，并允许选择搜寻空间的维度。我们的代码可以免费下载于GitHub，而我们的初步的数据显示，AdaSub在时间和迭代次数方面超过流行的随机优化器。
</details></li>
</ul>
<hr>
<h2 id="Estimating-optimal-PAC-Bayes-bounds-with-Hamiltonian-Monte-Carlo"><a href="#Estimating-optimal-PAC-Bayes-bounds-with-Hamiltonian-Monte-Carlo" class="headerlink" title="Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo"></a>Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20053">http://arxiv.org/abs/2310.20053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Szilvia Ujváry, Gergely Flamich, Vincent Fortuin, José Miguel Hernández Lobato</li>
<li>for:  investigate the tightness of PAC-Bayes bounds when restricting the posterior family to factorized Gaussian distributions.</li>
<li>methods: 使用 Hamiltonian Monte Carlo 采样优化 posterior，通过热力学整合Estimate KL divergence from the prior，并提出三种方法来获得高概率 bound under different assumptions.</li>
<li>results:  experiments on MNIST dataset show significant tightness gaps, as much as 5-6% in some cases.<details>
<summary>Abstract</summary>
An important yet underexplored question in the PAC-Bayes literature is how much tightness we lose by restricting the posterior family to factorized Gaussian distributions when optimizing a PAC-Bayes bound. We investigate this issue by estimating data-independent PAC-Bayes bounds using the optimal posteriors, comparing them to bounds obtained using MFVI. Concretely, we (1) sample from the optimal Gibbs posterior using Hamiltonian Monte Carlo, (2) estimate its KL divergence from the prior with thermodynamic integration, and (3) propose three methods to obtain high-probability bounds under different assumptions. Our experiments on the MNIST dataset reveal significant tightness gaps, as much as 5-6\% in some cases.
</details>
<details>
<summary>摘要</summary>
“一个重要但未获足够探讨的问题在PAC-Bayes文献中是，当我们将 posterior family 限制为因素化 Gaussian 分布时，我们所失去的紧密程度。我们调查这个问题，使用最佳 Gibbs posterior 来定义 PAC-Bayes 下界，并与 MFVI 的下界进行比较。具体来说，我们进行了以下三个步骤：1. 使用 Hamiltonian Monte Carlo 方法获取最佳 Gibbs posterior 的抽象；2. 使用热力学 интеграation 估算这个 posterior 对从假设的 KL 散度；3. 提出了三种方法来在不同的假设下获得高概率下界。我们对 MNIST dataset 进行了实验，发现在一些情况下，紧密程度可以相对较高，甚至高达 5-6%。”
</details></li>
</ul>
<hr>
<h2 id="The-Expressibility-of-Polynomial-based-Attention-Scheme"><a href="#The-Expressibility-of-Polynomial-based-Attention-Scheme" class="headerlink" title="The Expressibility of Polynomial based Attention Scheme"></a>The Expressibility of Polynomial based Attention Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20051">http://arxiv.org/abs/2310.20051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao Song, Guangyi Xu, Junze Yin</li>
<li>for: This paper aims to provide a theoretical analysis of the expressive capabilities of polynomial attention in transformer architectures, and to explore the effectiveness of high-degree polynomials in amplifying large values and distinguishing between datasets.</li>
<li>methods: The paper uses a combination of theoretical analysis and experimental evaluation to study the representational capacity of polynomial attention. The authors construct two carefully designed datasets, namely $\mathcal{D}_0$ and $\mathcal{D}_1$, and demonstrate the ability of a single-layer polynomial attention network to distinguish between these datasets using a sufficient high degree $\beta$.</li>
<li>results: The paper shows that with a high degree $\beta$, a single-layer polynomial attention network can effectively separate the two datasets, while with a low degree $\beta$, the network cannot effectively distinguish between them. The analysis underscores the greater effectiveness of high-degree polynomials in amplifying large values and capturing intricate linguistic correlations.<details>
<summary>Abstract</summary>
Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to some extent, reshaped people's lifestyles. However, the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference. While a recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, the theoretical understandings of this new approach are not yet well understood.   In this paper, we offer a theoretical analysis of the expressive capabilities of polynomial attention. Our study reveals a disparity in the ability of high-degree and low-degree polynomial attention. Specifically, we construct two carefully designed datasets, namely $\mathcal{D}_0$ and $\mathcal{D}_1$, where $\mathcal{D}_1$ includes a feature with a significantly larger value compared to $\mathcal{D}_0$. We demonstrate that with a sufficiently high degree $\beta$, a single-layer polynomial attention network can distinguish between $\mathcal{D}_0$ and $\mathcal{D}_1$. However, with a low degree $\beta$, the network cannot effectively separate the two datasets. This analysis underscores the greater effectiveness of high-degree polynomials in amplifying large values and distinguishing between datasets. Our analysis offers insight into the representational capacity of polynomial attention and provides a rationale for incorporating higher-degree polynomials in attention mechanisms to capture intricate linguistic correlations.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经有效地改善了我们日常生活中的多个方面。这些模型影响了许多领域，从医疗保健到教育，提高生产力、决策过程和可доступ性。因此，它们已经影响了人们的生活方式。然而，trasformer架构中的对话复杂度问题使得当文本上下文变长时，训练和应用这些模型变得不实际。 recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, but the theoretical understandings of this new approach are not yet well understood.在这篇论文中，我们提供了对幂函数注意力的理论分析。我们的研究显示了高度和低度幂函数注意力之间的差异。具体来说，我们创建了两个精心设计的数据集，namely $\mathcal{D}_0$和$\mathcal{D}_1$，其中 $\mathcal{D}_1$ 包含一个具有许多更大值的特征，相比 $\mathcal{D}_0$。我们示出了，随着幂度 $\beta$ 增加到足够高的程度，单层幂函数注意力网络可以有效地分辨 $\mathcal{D}_0$ 和 $\mathcal{D}_1$。然而，随着幂度 $\beta$ 降低，网络无法有效地区分这两个数据集。这一分析显示了幂函数注意力的表达能力，并提供了将更高度幂函数包含在注意力机制中以捕捉复杂的语言相关资讯的理论基础。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Riemannian-Diffusion-Models"><a href="#Scaling-Riemannian-Diffusion-Models" class="headerlink" title="Scaling Riemannian Diffusion Models"></a>Scaling Riemannian Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20030">http://arxiv.org/abs/2310.20030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louaaron/Scaling-Riemannian-Diffusion">https://github.com/louaaron/Scaling-Riemannian-Diffusion</a></li>
<li>paper_authors: Aaron Lou, Minkai Xu, Stefano Ermon</li>
<li>for: 本研究旨在提高偏微分方法在泛化空间上的性能，并允许在高维空间中应用。</li>
<li>methods: 本研究使用了各种 Ansatz 和技巧来简化偏微分过程，包括使用对称空间的假设，以高精度计算相关量。</li>
<li>results: 研究发现，在低维数据集上，使用我们的修正可以获得显著改善，使偏微分与其他方法竞争。此外，我们还证明了我们的方法可以在高维任务上进行泛化。例如，我们在 $SU(n)$ 格子上模型了 QCD 分布，并在高维球面上学习了对比性学习的嵌入。<details>
<summary>Abstract</summary>
Riemannian diffusion models draw inspiration from standard Euclidean space diffusion models to learn distributions on general manifolds. Unfortunately, the additional geometric complexity renders the diffusion transition term inexpressible in closed form, so prior methods resort to imprecise approximations of the score matching training objective that degrade performance and preclude applications in high dimensions. In this work, we reexamine these approximations and propose several practical improvements. Our key observation is that most relevant manifolds are symmetric spaces, which are much more amenable to computation. By leveraging and combining various ans\"{a}tze, we can quickly compute relevant quantities to high precision. On low dimensional datasets, our correction produces a noticeable improvement, allowing diffusion to compete with other methods. Additionally, we show that our method enables us to scale to high dimensional tasks on nontrivial manifolds. In particular, we model QCD densities on $SU(n)$ lattices and contrastively learned embeddings on high dimensional hyperspheres.
</details>
<details>
<summary>摘要</summary>
里曼尼 diffusion 模型继承自标准欧几里得空间 diffusion 模型，以学习总体 manifold 上的分布。然而，额外的几何复杂性使得扩散过程的转移函数无法表示为闭合形式，因此先前的方法通常采用了准确性不高的替换方法，这会影响性能并阻碍高维应用。在这个工作中，我们重新评估这些替换方法，并提出了一些实用的改进方案。我们的关键观察是，大多数相关的拓扑都是 симметричные 空间，这使得计算变得非常方便。通过利用和组合不同的 ansatz，我们可以快速计算相关的量，并达到高精度。在低维数据集上，我们的修正提供了明显的改善，使扩散能够与其他方法竞争。此外，我们还证明了我们的方法可以扩展到高维任务上，特别是在 $SU(n)$ 格点上模型 QCD 分布和高维球体上的 contrastively 学习 embedding。
</details></li>
</ul>
<hr>
<h2 id="PolyThrottle-Energy-efficient-Neural-Network-Inference-on-Edge-Devices"><a href="#PolyThrottle-Energy-efficient-Neural-Network-Inference-on-Edge-Devices" class="headerlink" title="PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices"></a>PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19991">http://arxiv.org/abs/2310.19991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghao Yan, Hongyi Wang, Shivaram Venkataraman</li>
<li>for: 这篇论文旨在提高神经网络（NN）的能源管理，特别是在推广运行中。</li>
<li>methods: 论文使用了对硬件元件的配置优化，包括GPU、内存和CPU频率，以提高NN推广运行中的能源使用效率。</li>
<li>results: 论文的实验评估发现，这种配置优化可以实现36%的能源储存，并且可以快速地对应应用程序的限制。<details>
<summary>Abstract</summary>
As neural networks (NN) are deployed across diverse sectors, their energy demand correspondingly grows. While several prior works have focused on reducing energy consumption during training, the continuous operation of ML-powered systems leads to significant energy use during inference. This paper investigates how the configuration of on-device hardware-elements such as GPU, memory, and CPU frequency, often neglected in prior studies, affects energy consumption for NN inference with regular fine-tuning. We propose PolyThrottle, a solution that optimizes configurations across individual hardware components using Constrained Bayesian Optimization in an energy-conserving manner. Our empirical evaluation uncovers novel facets of the energy-performance equilibrium showing that we can save up to 36 percent of energy for popular models. We also validate that PolyThrottle can quickly converge towards near-optimal settings while satisfying application constraints.
</details>
<details>
<summary>摘要</summary>
As neural networks (NN) are deployed across diverse sectors, their energy demand correspondingly grows. While several prior works have focused on reducing energy consumption during training, the continuous operation of ML-powered systems leads to significant energy use during inference. This paper investigates how the configuration of on-device hardware-elements such as GPU, memory, and CPU frequency, often neglected in prior studies, affects energy consumption for NN inference with regular fine-tuning. We propose PolyThrottle, a solution that optimizes configurations across individual hardware components using Constrained Bayesian Optimization in an energy-conserving manner. Our empirical evaluation uncovers novel facets of the energy-performance equilibrium showing that we can save up to 36 percent of energy for popular models. We also validate that PolyThrottle can quickly converge towards near-optimal settings while satisfying application constraints.Here's the translation in Traditional Chinese:随着神经网络（NN）在多个领域应用，其能源需求也随之增加。虽然先前的研究主要集中在训练过程中对能源采取减少措施，但是ML系统的持续运行仍然导致重要的能源消耗。本文研究了在实际应用中对于NN推理的硬件元件配置，如GPU、内存和CPU频率，对于能源消耗的影响。我们提出了PolyThrottle，一个实现硬件元件配置优化的解决方案，使用受限的泊松优化。我们的实验评估发现，PolyThrottle可以实现36%的能源储存，并且可以快速对应应用程序的限制。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Up-Differentially-Private-LASSO-Regularized-Logistic-Regression-via-Faster-Frank-Wolfe-Iterations"><a href="#Scaling-Up-Differentially-Private-LASSO-Regularized-Logistic-Regression-via-Faster-Frank-Wolfe-Iterations" class="headerlink" title="Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations"></a>Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19978">http://arxiv.org/abs/2310.19978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Raff, Amol Khanna, Fred Lu</li>
<li>for: 这个论文的目的是提出一种能够在稀疏输入数据上训练具有不同保证的回归模型。</li>
<li>methods: 这篇论文使用了Frank-Wolfe算法进行$L_1$ 惩罚线性回归模型的训练，并将其改进以适应稀疏输入数据。</li>
<li>results: 该方法可以减少训练时间的复杂度，从$\mathcal{O}(TDCS + TNS)$降低到$\mathcal{O}(NS + T\sqrt{D}\log{D} + TS^2)$，具体取决于私有保证参数$\epsilon$和数据稀疏程度。实验结果表明，这种方法可以减少训练时间的因子达2200倍。<details>
<summary>Abstract</summary>
To the best of our knowledge, there are no methods today for training differentially private regression models on sparse input data. To remedy this, we adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be aware of sparse inputs and to use them effectively. In doing so, we reduce the training time of the algorithm from $\mathcal{O}( T D S + T N S)$ to $\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$, where $T$ is the number of iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features. Our results demonstrate that this procedure can reduce runtime by a factor of up to $2,200\times$, depending on the value of the privacy parameter $\epsilon$ and the sparsity of the dataset.
</details>
<details>
<summary>摘要</summary>
Currently, there are no methods for training differentially private regression models on sparse input data. To address this, we modify the Frank-Wolfe algorithm for $L_1$ penalized linear regression to handle sparse inputs and improve its efficiency. As a result, the training time of the algorithm is reduced from $\mathcal{O}( T D S + T N S)$ to $\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$, where $T$ is the number of iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features. Our experiments show that this approach can reduce the runtime by a factor of up to $2,200\times$, depending on the value of the privacy parameter $\epsilon$ and the sparsity of the dataset.
</details></li>
</ul>
<hr>
<h2 id="Unified-Enhancement-of-Privacy-Bounds-for-Mixture-Mechanisms-via-f-Differential-Privacy"><a href="#Unified-Enhancement-of-Privacy-Bounds-for-Mixture-Mechanisms-via-f-Differential-Privacy" class="headerlink" title="Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy"></a>Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19973">http://arxiv.org/abs/2310.19973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chendi Wang, Buxin Su, Jiayuan Ye, Reza Shokri, Weijie J. Su</li>
<li>for: 这篇论文的目的是提高隐私保护的机器学习算法的隐私保证，特别是随机初始化和批处理抽取的随机模型和一轮渐进式隐私梯度下降（DP-GD）的隐私保证。</li>
<li>methods: 这篇论文使用了$f$-DP来提高随机模型和DP-GD的隐私保证，并 derive了一个关于洗淤模型的关闭式表达函数，以及对随机初始化的研究。</li>
<li>results: 这篇论文的研究表明，随机初始化可以增强DP-GD的隐私保证，并且对于洗淤模型，$f$-DP可以提供更好的隐私保证。此外，这篇论文还提出了一个新的不等式 для质量函数，用于分析混合机制的隐私保证。<details>
<summary>Abstract</summary>
Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an inequality for trade-off functions introduced in this paper. This inequality implies the joint convexity of $F$-divergences. Finally, we study an $f$-DP analog of the advanced joint convexity of the hockey-stick divergence related to $(\epsilon,\delta)$-DP and apply it to analyze the privacy of mixture mechanisms.
</details>
<details>
<summary>摘要</summary>
differentially private（DP）机器学习算法会产生多种随机性，如初始化随机值、批处理随机抽样和排序。然而，这些随机性很难在证明泛化隐私级别时考虑，因为它们导致算法输出的混合分布变得更加复杂。这篇论文关注改善混合模型和一轮泛化隐私梯度下降（DP-GD）的隐私级别，使用 $f $-DP。我们 derive了混合模型的关闭式交易函数表达，超过最新的 $(\epsilon ,\delta )-$DP 结果。此外，我们还研究了随机初始化对DP-GD的隐私性的影响。我们的数值计算表达交易函数指示，随机初始化可以增强DP-GD的隐私性。我们的 $f $-DP 保证分析中使用了这篇论文引入的交易函数不等式，该不等式表明 $F $-散度函数的共轭性。最后，我们研究了 $f $-DP 对混合机制的隐私性的分析，并应用了 $(\epsilon ,\delta )-$DP 相关的先进 JOINT CONVEXITY 的射影分析。
</details></li>
</ul>
<hr>
<h2 id="Early-detection-of-inflammatory-arthritis-to-improve-referrals-using-multimodal-machine-learning-from-blood-testing-semi-structured-and-unstructured-patient-records"><a href="#Early-detection-of-inflammatory-arthritis-to-improve-referrals-using-multimodal-machine-learning-from-blood-testing-semi-structured-and-unstructured-patient-records" class="headerlink" title="Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records"></a>Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19967">http://arxiv.org/abs/2310.19967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bing Wang, Weizi Li, Anthony Bradlow, Antoni T. Y. Chan, Eghosa Bazuaye</li>
<li>for: 早期诊断急性关节炎（IA），以便有效、准确地折衔医疗资源，避免疾病诊断过程中的滥衔和恶化。</li>
<li>methods: 我们使用多modal数据 ensemble学和融合学方法，以帮助早期诊断IA。这些方法包括 semi-structured和不结构数据的融合，以提高IA的检测精度。</li>
<li>results: 我们的研究表明，使用多modal数据 ensemble学和融合学方法可以帮助早期诊断IA，并且在实际应用中可以提高医疗资源的利用率和诊断精度。这些方法可以作为诊断IA的助手，帮助医生更准确地诊断疾病。<details>
<summary>Abstract</summary>
Early detection of inflammatory arthritis (IA) is critical to efficient and accurate hospital referral triage for timely treatment and preventing the deterioration of the IA disease course, especially under limited healthcare resources. The manual assessment process is the most common approach in practice for the early detection of IA, but it is extremely labor-intensive and inefficient. A large amount of clinical information needs to be assessed for every referral from General Practice (GP) to the hospitals. Machine learning shows great potential in automating repetitive assessment tasks and providing decision support for the early detection of IA. However, most machine learning-based methods for IA detection rely on blood testing results. But in practice, blood testing data is not always available at the point of referrals, so we need methods to leverage multimodal data such as semi-structured and unstructured data for early detection of IA. In this research, we present fusion and ensemble learning-based methods using multimodal data to assist decision-making in the early detection of IA. To the best of our knowledge, our study is the first attempt to utilize multimodal data to support the early detection of IA from GP referrals.
</details>
<details>
<summary>摘要</summary>
早期检测Inflammatory Arthritis (IA) 的重要性在有限的医疗资源下是非常重要，以确保有效和准确的医院推荐诊断和避免IA疾病趋势的恶化。现在，手动评估过程是在实践中最常见的检测IA的方法，但它很劳动密集和不效率。每次从普通医生（GP）到医院的 Referral 需要评估大量临床信息。机器学习显示出了自动化重复的评估任务和提供决策支持的潜力，但大多数机器学习基于IA检测方法都依赖血液测试结果。但在实践中，血液测试数据不总是在 Referral 时可以获得，因此我们需要使用多Modal 数据来支持早期IA检测。在这项研究中，我们提出了将多Modal 数据 fusion 和 ensemble learning 技术应用于早期IA检测。到目前为止，我们的研究是首次利用多Modal 数据来支持从GP Referral 中的IA检测。
</details></li>
</ul>
<hr>
<h2 id="Topological-Learning-for-Motion-Data-via-Mixed-Coordinates"><a href="#Topological-Learning-for-Motion-Data-via-Mixed-Coordinates" class="headerlink" title="Topological Learning for Motion Data via Mixed Coordinates"></a>Topological Learning for Motion Data via Mixed Coordinates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19960">http://arxiv.org/abs/2310.19960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrluo/topologicalmotionseries">https://github.com/hrluo/topologicalmotionseries</a></li>
<li>paper_authors: Hengrui Luo, Jisu Kim, Alice Patania, Mikael Vejdemo-Johansson</li>
<li>for: 这个论文的目的是将拓扑信息integrated into a multiple output Gaussian process model for transfer learning purposes.</li>
<li>methods: 作者们提出了一种使用拓扑信息construct a cluster based kernel in a multiple output Gaussian process model, which incorporates the topological structural information and allows for a unified framework using topological information in time and motion series.</li>
<li>results: 作者们的方法可以effectively learn from multiple time series via a multiple output Gaussian process model, and achieve better performance compared to traditional methods.<details>
<summary>Abstract</summary>
Topology can extract the structural information in a dataset efficiently. In this paper, we attempt to incorporate topological information into a multiple output Gaussian process model for transfer learning purposes. To achieve this goal, we extend the framework of circular coordinates into a novel framework of mixed valued coordinates to take linear trends in the time series into consideration.   One of the major challenges to learn from multiple time series effectively via a multiple output Gaussian process model is constructing a functional kernel. We propose to use topologically induced clustering to construct a cluster based kernel in a multiple output Gaussian process model. This kernel not only incorporates the topological structural information, but also allows us to put forward a unified framework using topological information in time and motion series.
</details>
<details>
<summary>摘要</summary>
topology可以效率地提取数据集中的结构信息。在这篇论文中，我们尝试将拓扑信息integrated into a novel framework of mixed-valued coordinates to take linear trends in the time series into consideration。  一个主要挑战是通过多个时间序列学习是通过多输出 Gaussian process模型，constructing a functional kernel。我们提议使用拓扑induced clustering construct a cluster-based kernel in a multiple output Gaussian process model。这个kernel不仅 incorporates the topological structural information，而且允许我们提出一个统一的拓扑信息在时间和运动序列中使用的框架。
</details></li>
</ul>
<hr>
<h2 id="PriPrune-Quantifying-and-Preserving-Privacy-in-Pruned-Federated-Learning"><a href="#PriPrune-Quantifying-and-Preserving-Privacy-in-Pruned-Federated-Learning" class="headerlink" title="PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning"></a>PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19958">http://arxiv.org/abs/2310.19958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyue Chu, Mengwei Yang, Nikolaos Laoutaris, Athina Markopoulou</li>
<li>for: 这种研究的目的是为了提高 Federated Learning (FL) 中的隐私保护，特别是在使用模型剔除 (pruning) 时。</li>
<li>methods: 这篇论文使用了信息论的Upper bound来衡量剔除后模型泄露的信息量，并通过对state-of-the-art privacy attacks进行实验来验证这些理论结论。</li>
<li>results: 该论文的实验结果表明，使用 PriPrune 可以在 FL 中提高隐私保护，并且可以与不同的防御策略和模型剔除策略结合使用以提高隐私精度。<details>
<summary>Abstract</summary>
Federated learning (FL) is a paradigm that allows several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks.   In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings, with comprehensive experiments that involve state-of-the-art privacy attacks, on several state-of-the-art FL pruning schemes, using benchmark datasets. This evaluation provides valuable insights into the choices and parameters that can affect the privacy protection provided by pruning. Based on these insights, we introduce PriPrune -- a privacy-aware algorithm for local model pruning, which uses a personalized per-client defense mask and adapts the defense pruning rate so as to jointly optimize privacy and model performance. PriPrune is universal in that can be applied after any pruned FL scheme on the client, without modification, and protects against any inversion attack by the server. Our empirical evaluation demonstrates that PriPrune significantly improves the privacy-accuracy tradeoff compared to state-of-the-art pruned FL schemes that do not take privacy into account.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）是一种 paradigm，让多个客户端设备和服务器共同训练全域模型，只需将模型更新交换，不需客户端分享本地训练数据。这些客户端通常受到通信和计算资源的限制，可以进一步从模型剔除中获得保护。实际上，通过让本地模型变得粗糙，剔除是对隐私攻击的一种保护措施，但这个保护尚未得到正式或实验性的描述，而且不清楚是否具有足够的防护力。在这篇文章中，我们进行了联合学习中隐私保护的第一次研究。我们 derive 信息论的上限，以量度剔除FL模型中的资讯泄露。我们还进行了广泛的实验，使用现代隐私攻击，评估多个state-of-the-art FL剔除方案在多个 benchmark 数据集上的性能。这些实验给出了价值的对照，以便选择和参数的推广。基于这些对照，我们引入 PriPrune，一个适应性的隐私保护算法，使用对每个客户端的个人化防御面罩，并调整防御剔除率，以同时优化隐私和模型性能。 PriPrune 可以跨多个FL剔除方案，无需修改，并且可以对任何剔除FL模型进行防护，不受服务器的攻击。我们的实验显示，PriPrune 在隐私对照调整中提供了明显的改善。
</details></li>
</ul>
<hr>
<h2 id="The-Acquisition-of-Physical-Knowledge-in-Generative-Neural-Networks"><a href="#The-Acquisition-of-Physical-Knowledge-in-Generative-Neural-Networks" class="headerlink" title="The Acquisition of Physical Knowledge in Generative Neural Networks"></a>The Acquisition of Physical Knowledge in Generative Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19943">http://arxiv.org/abs/2310.19943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cross32768/PlaNet_PyTorch">https://github.com/cross32768/PlaNet_PyTorch</a></li>
<li>paper_authors: Luca M. Schulze Buschoff, Eric Schulz, Marcel Binz</li>
<li>for:  investigate how the learning trajectories of deep generative neural networks compare to children’s developmental trajectories using physical understanding as a testbed.</li>
<li>methods:  use physical understanding as a testbed to examine two distinct hypotheses of human development - stochastic optimization and complexity increase.</li>
<li>results:  find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.Here’s the summary in Traditional Chinese text:</li>
<li>for: 研究深度生成神经网络的学习轨迹与儿童的发展轨迹，用物理理解作为测试床。</li>
<li>methods: 使用物理理解作为测试床，检查两种人类发展假设——几率优化和复杂度增加。</li>
<li>results: 发现模型能够准确预测一些物理 проце数，但其学习轨迹下两种假设都不跟随儿童的发展轨迹。<details>
<summary>Abstract</summary>
As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.
</details>
<details>
<summary>摘要</summary>
As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.Here's the translation in Traditional Chinese:随着儿童长大，他们会开始有直觉地理解环境中的物理过程。儿童的物理理解会随着时间的推移，逐步发展出不同的发展轨迹，这些轨迹已经在前一些实验研究中得到了详细的描述。在这里，我们将实验探索深度生成神经网络的学习轨迹与儿童的发展轨迹之间的相似之处。我们提出了两个假设来检验人类发展的机制：随机优化和复杂度增加。我们发现了我们的模型可以对一些物理过程进行准确的预测，但是它们的学习轨迹不会跟随儿童的发展轨迹。
</details></li>
</ul>
<hr>
<h2 id="Lyapunov-Based-Dropout-Deep-Neural-Network-Lb-DDNN-Controller"><a href="#Lyapunov-Based-Dropout-Deep-Neural-Network-Lb-DDNN-Controller" class="headerlink" title="Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller"></a>Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19938">http://arxiv.org/abs/2310.19938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saiedeh Akbari, Emily J. Griffis, Omkar Sudhir Patil, Warren E. Dixon</li>
<li>for: 提高非线性动力系统中不结构化不确定性的补偿</li>
<li>methods: 使用Dropout正则化技术和Lyapunov基于实时权重更新方法进行在线无监督学习</li>
<li>results: 在实验中，提出的Dropout DNN基于适应控制器比基eline adaptive DNN控制器无Dropout正则化技术下的追踪误差下降38.32%，功能预测误差下降53.67%，控制努力下降50.44%。<details>
<summary>Abstract</summary>
Deep neural network (DNN)-based adaptive controllers can be used to compensate for unstructured uncertainties in nonlinear dynamic systems. However, DNNs are also very susceptible to overfitting and co-adaptation. Dropout regularization is an approach where nodes are randomly dropped during training to alleviate issues such as overfitting and co-adaptation. In this paper, a dropout DNN-based adaptive controller is developed. The developed dropout technique allows the deactivation of weights that are stochastically selected for each individual layer within the DNN. Simultaneously, a Lyapunov-based real-time weight adaptation law is introduced to update the weights of all layers of the DNN for online unsupervised learning. A non-smooth Lyapunov-based stability analysis is performed to ensure asymptotic convergence of the tracking error. Simulation results of the developed dropout DNN-based adaptive controller indicate a 38.32% improvement in the tracking error, a 53.67% improvement in the function approximation error, and 50.44% lower control effort when compared to a baseline adaptive DNN-based controller without dropout regularization.
</details>
<details>
<summary>摘要</summary>
（简化中文）深度神经网络（DNN）基于适应控制器可以补偿非结构化不确定性在非线性动态系统中。然而，DNN也很容易过拟合和相互作用。Dropout regularization是一种approach，在训练时随机drop nodes以解决过拟合和相互作用的问题。在这篇论文中，一种dropout DNN基于适应控制器被开发出来。该developed dropout技术allow the deactivation of weights that are stochastically selected for each individual layer within the DNN.同时，一种Lyapunov-based实时重量更新法是引入，以更新所有层的DNN重量 для在线无监督学习。一种非稠密Lyapunov-based稳定分析是执行以确保追踪误差的极限收敛。实验结果表明，与基eline adaptive DNN基于控制器 безdropout regularization相比，开发的dropout DNN基于适应控制器可以提高追踪误差38.32%，函数适应误差53.67%，控制努力50.44%。
</details></li>
</ul>
<hr>
<h2 id="Sim2Real-for-Environmental-Neural-Processes"><a href="#Sim2Real-for-Environmental-Neural-Processes" class="headerlink" title="Sim2Real for Environmental Neural Processes"></a>Sim2Real for Environmental Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19932">http://arxiv.org/abs/2310.19932</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jonas-scholz123/sim2real-downscaling">https://github.com/jonas-scholz123/sim2real-downscaling</a></li>
<li>paper_authors: Jonas Scholz, Tom R. Andersson, Anna Vaughan, James Requeima, Richard E. Turner</li>
<li>for: 这个论文旨在探讨如何使用机器学习模型进行天气预测和气候监测。</li>
<li>methods: 这个论文使用的方法包括使用数值数据融合系统生成的格子化气象数据，以及使用神经网络模型来模拟天气Conditioned on both gridded and off-the-grid context data to make uncertainty-aware predictions at target locations。</li>
<li>results: 研究发现，使用“Sim2Real”方法可以在德国的表面温度预测 task 中取得substantially better results，这表明了使用数值数据融合系统的数据作为适应的跳板，可以帮助学习从实际观测数据中获得更高的准确率。<details>
<summary>Abstract</summary>
Machine learning (ML)-based weather models have recently undergone rapid improvements. These models are typically trained on gridded reanalysis data from numerical data assimilation systems. However, reanalysis data comes with limitations, such as assumptions about physical laws and low spatiotemporal resolution. The gap between reanalysis and reality has sparked growing interest in training ML models directly on observations such as weather stations. Modelling scattered and sparse environmental observations requires scalable and flexible ML architectures, one of which is the convolutional conditional neural process (ConvCNP). ConvCNPs can learn to condition on both gridded and off-the-grid context data to make uncertainty-aware predictions at target locations. However, the sparsity of real observations presents a challenge for data-hungry deep learning models like the ConvCNP. One potential solution is 'Sim2Real': pre-training on reanalysis and fine-tuning on observational data. We analyse Sim2Real with a ConvCNP trained to interpolate surface air temperature over Germany, using varying numbers of weather stations for fine-tuning. On held-out weather stations, Sim2Real training substantially outperforms the same model architecture trained only with reanalysis data or only with station data, showing that reanalysis data can serve as a stepping stone for learning from real observations. Sim2Real could thus enable more accurate models for weather prediction and climate monitoring.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）基于天气模型在最近几年内呈现了快速进步。这些模型通常是基于网格化的重新分析数据进行训练的。然而，重新分析数据具有一些限制，例如假设物理法律和低空时间分辨率。这导致了在重新分析和现实之间的差距，并且引发了使用直接训练在天气站上的ML模型的兴趣。模型散布和稀缺环境观测需要扩展和灵活的ML架构，其中之一是卷积条件隐藏过程（ConvCNP）。ConvCNP可以通过条件在网格和非网格上的数据来进行随机预测。然而，实际观测的稀缺性对深度学习模型来说是一个挑战。一个可能的解决方案是“Sim2Real”：先在重新分析数据上进行预训练，然后在天气站上进行细化训练。我们使用了一个ConvCNP来 interpolate surface air temperature over Germany，使用不同数量的天气站进行细化训练。在封存的天气站上，Sim2Real训练显著超过了同样的模型架构在重新分析数据或天气站数据上进行训练，这表明重新分析数据可以作为学习真实观测的“跳板”。Sim2Real可能可以帮助建立更准确的天气预测和气候监测模型。
</details></li>
</ul>
<hr>
<h2 id="Solving-a-Class-of-Cut-Generating-Linear-Programs-via-Machine-Learning"><a href="#Solving-a-Class-of-Cut-Generating-Linear-Programs-via-Machine-Learning" class="headerlink" title="Solving a Class of Cut-Generating Linear Programs via Machine Learning"></a>Solving a Class of Cut-Generating Linear Programs via Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19920">http://arxiv.org/abs/2310.19920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atefeh Rajabalizadeh, Danial Davarnia</li>
<li>for: 该论文目的是提出一种基于机器学习的方法，用于在分支和缩进算法中选择最有用的节点，以提高权衡Program的解的质量。</li>
<li>methods: 该方法利用了分支和缩进算法中的分支点，通过转化CGLP为一个指示函数，使用传统的数据分类技术来近似CGLP的优化值。</li>
<li>results: 实验结果表明，使用该方法可以提高解时间，比 conventinal cutting plane方法更快。<details>
<summary>Abstract</summary>
Cut-generating linear programs (CGLPs) play a key role as a separation oracle to produce valid inequalities for the feasible region of mixed-integer programs. When incorporated inside branch-and-bound, the cutting planes obtained from CGLPs help to tighten relaxations and improve dual bounds. However, running the CGLPs at the nodes of the branch-and-bound tree is computationally cumbersome due to the large number of node candidates and the lack of a priori knowledge on which nodes admit useful cutting planes. As a result, CGLPs are often avoided at default settings of branch-and-cut algorithms despite their potential impact on improving dual bounds. In this paper, we propose a novel framework based on machine learning to approximate the optimal value of a CGLP class that determines whether a cutting plane can be generated at a node of the branch-and-bound tree. Translating the CGLP as an indicator function of the objective function vector, we show that it can be approximated through conventional data classification techniques. We provide a systematic procedure to efficiently generate training data sets for the corresponding classification problem based on the CGLP structure. We conduct computational experiments on benchmark instances using classification methods such as logistic regression. These results suggest that the approximate CGLP obtained from classification can improve the solution time compared to that of conventional cutting plane methods. Our proposed framework can be efficiently applied to a large number of nodes in the branch-and-bound tree to identify the best candidates for adding a cut.
</details>
<details>
<summary>摘要</summary>
割生成线性程序（CGLP）在杂integer程序的可行区域中扮演着关键角色，作为分离或acles来生成有效的不等式。在branch和bound中包含CGLP时，可以通过割生成的割面来紧张 relaxation 和提高 dual bound。然而，在branch和bound树中运行CGLP的计算占用了大量计算资源，因为有很多节点候选和没有先验知识，哪些节点可以生成有用的割面。因此，CGLP通常在branch和cut算法的默认设置下被避免使用，即使它们有可能改善 dual bound。在这篇论文中，我们提出了一种基于机器学习的新框架，用于缩小CGLP类型的优化值。我们将CGLP转化为目标函数向量的指示函数，并示出可以通过传统数据分类技术来近似其优化值。我们还提供了一种系统化的生成训练数据集的方法，基于CGLP的结构。我们在 benchmark 实例上进行了计算实验，使用类логистиック回归等分类方法。这些结果表明，使用我们的提出的框架可以提高解决时间，比 conventional cutting plane方法更好。我们的提出的框架可以高效地应用于branch和bound树中的大量节点，以确定最佳的加法割面候选。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-Strategies-through-Value-Maximization-in-Neural-Networks"><a href="#Meta-Learning-Strategies-through-Value-Maximization-in-Neural-Networks" class="headerlink" title="Meta-Learning Strategies through Value Maximization in Neural Networks"></a>Meta-Learning Strategies through Value Maximization in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19919">http://arxiv.org/abs/2310.19919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigo Carrasco-Davis, Javier Masís, Andrew M. Saxe</li>
<li>for: 这篇论文的目的是研究如何在生物学和人工智能学习代理人面临多种学习选择的情况下，理解如何实现正规的控制功能。</li>
<li>methods: 这篇论文使用了一种可 tractable 的学习努力框架，可以有效地优化控制信号，以实现折扣累累性性能的总体优化。</li>
<li>results: 研究发现，在不同的学习Setting中，控制努力在早期学习 easier aspects of a task 时最有利，然后坚持努力 harder aspects 上。总的来说，这种学习努力框架提供了一个可 tractable 的理论测试床，可以研究不同的学习系统中的正规控制策略，以及一种正规的认知控制理论中提出的学习 trajectory 的控制策略。<details>
<summary>Abstract</summary>
Biological and artificial learning agents face numerous choices about how to learn, ranging from hyperparameter selection to aspects of task distributions like curricula. Understanding how to make these meta-learning choices could offer normative accounts of cognitive control functions in biological learners and improve engineered systems. Yet optimal strategies remain challenging to compute in modern deep networks due to the complexity of optimizing through the entire learning process. Here we theoretically investigate optimal strategies in a tractable setting. We present a learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning. We obtain computational tractability by using average dynamical equations for gradient descent, available for simple neural network architectures. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified normative setting. We apply this framework to investigate the effect of approximations in common meta-learning algorithms; infer aspects of optimal curricula; and compute optimal neuronal resource allocation in a continual learning setting. Across settings, we find that control effort is most beneficial when applied to easier aspects of a task early in learning; followed by sustained effort on harder aspects. Overall, the learning effort framework provides a tractable theoretical test bed to study normative benefits of interventions in a variety of learning systems, as well as a formal account of optimal cognitive control strategies over learning trajectories posited by established theories in cognitive neuroscience.
</details>
<details>
<summary>摘要</summary>
In this study, we investigate optimal strategies in a tractable setting using a learning effort framework that efficiently optimizes control signals based on a fully normative objective: discounted cumulative performance throughout learning. We use average dynamical equations for gradient descent, which are available for simple neural network architectures, to achieve computational tractability. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified normative setting.We apply the framework to investigate the effect of approximations in common meta-learning algorithms, infer aspects of optimal curricula, and compute optimal neuronal resource allocation in a continual learning setting. Our findings show that control effort is most beneficial when applied to easier aspects of a task early in learning, followed by sustained effort on harder aspects.Overall, the learning effort framework provides a tractable theoretical test bed to study normative benefits of interventions in a variety of learning systems, as well as a formal account of optimal cognitive control strategies over learning trajectories posited by established theories in cognitive neuroscience.
</details></li>
</ul>
<hr>
<h2 id="GPCR-BERT-Interpreting-Sequential-Design-of-G-Protein-Coupled-Receptors-Using-Protein-Language-Models"><a href="#GPCR-BERT-Interpreting-Sequential-Design-of-G-Protein-Coupled-Receptors-Using-Protein-Language-Models" class="headerlink" title="GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models"></a>GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19915">http://arxiv.org/abs/2310.19915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seongwon Kim, Parisa Mollaei, Akshay Antony, Rishikesh Magar, Amir Barati Farimani</li>
<li>for: 本研究旨在开展G蛋白聚合物受体（GPCR）的Sequential设计。</li>
<li>methods: 本研究使用了Prot-Bert模型，通过精度调整和预测任务来探讨蛋白质序列中氨酸的相互关系，以及NPxxY、CWxP和E&#x2F;DRY等保守排序模式。</li>
<li>results: 研究发现，使用注意力权重和隐藏状态来描述蛋白质序列中各氨酸的贡献程度，以及3D结构的分析，可以帮助理解GPCR的协同作用。<details>
<summary>Abstract</summary>
With the rise of Transformers and Large Language Models (LLMs) in Chemistry and Biology, new avenues for the design and understanding of therapeutics have opened up to the scientific community. Protein sequences can be modeled as language and can take advantage of recent advances in LLMs, specifically with the abundance of our access to the protein sequence datasets. In this paper, we developed the GPCR-BERT model for understanding the sequential design of G Protein-Coupled Receptors (GPCRs). GPCRs are the target of over one-third of FDA-approved pharmaceuticals. However, there is a lack of comprehensive understanding regarding the relationship between amino acid sequence, ligand selectivity, and conformational motifs (such as NPxxY, CWxP, E/DRY). By utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with prediction tasks of variations in the motifs, we were able to shed light on several relationships between residues in the binding pocket and some of the conserved motifs. To achieve this, we took advantage of attention weights, and hidden states of the model that are interpreted to extract the extent of contributions of amino acids in dictating the type of masked ones. The fine-tuned models demonstrated high accuracy in predicting hidden residues within the motifs. In addition, the analysis of embedding was performed over 3D structures to elucidate the higher-order interactions within the conformations of the receptors.
</details>
<details>
<summary>摘要</summary>
By utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with prediction tasks of variations in the motifs, we were able to shed light on several relationships between residues in the binding pocket and some of the conserved motifs. To achieve this, we took advantage of attention weights and hidden states of the model that are interpreted to extract the extent of contributions of amino acids in dictating the type of masked ones. The fine-tuned models demonstrated high accuracy in predicting hidden residues within the motifs.In addition, the analysis of embedding was performed over 3D structures to elucidate the higher-order interactions within the conformations of the receptors.
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Simulation-based-Inference-for-Cosmological-Initial-Conditions"><a href="#Bayesian-Simulation-based-Inference-for-Cosmological-Initial-Conditions" class="headerlink" title="Bayesian Simulation-based Inference for Cosmological Initial Conditions"></a>Bayesian Simulation-based Inference for Cosmological Initial Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19910">http://arxiv.org/abs/2310.19910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian List, Noemi Anau Montel, Christoph Weniger</li>
<li>for: 这篇论文是为了重构astrophysical和cosmological场的观测数据而写的。</li>
<li>methods: 这篇论文使用了bayesian场重构算法，该算法基于simulation-based inference和自REGRESSIVE模型。</li>
<li>results: 论文首次实现了从观测数据中重构cosmological initial condition的能力。<details>
<summary>Abstract</summary>
Reconstructing astrophysical and cosmological fields from observations is challenging. It requires accounting for non-linear transformations, mixing of spatial structure, and noise. In contrast, forward simulators that map fields to observations are readily available for many applications. We present a versatile Bayesian field reconstruction algorithm rooted in simulation-based inference and enhanced by autoregressive modeling. The proposed technique is applicable to generic (non-differentiable) forward simulators and allows sampling from the posterior for the underlying field. We show first promising results on a proof-of-concept application: the recovery of cosmological initial conditions from late-time density fields.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将astro物理和 cosmological场 reconstruction from observations 是一项具有挑战性的任务。它需要考虑非线性变换、空间结构混合以及噪声。相反，前向模拟器可以快速地将场转换为观测数据，这些模拟器在许多应用场景中ready available。我们提出了一种可靠的 Bayesian 场 reconstruction算法，基于simulation-based inference和自适应模型。该算法适用于通用（非 diferenciable）前向模拟器，并允许采样 posterior 中的 underlying 场。我们在一个证明性应用中展示了该技术的初步成果： cosmological initial condition 的回归从晚期密度场中。Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="BTRec-BERT-Based-Trajectory-Recommendation-for-Personalized-Tours"><a href="#BTRec-BERT-Based-Trajectory-Recommendation-for-Personalized-Tours" class="headerlink" title="BTRec: BERT-Based Trajectory Recommendation for Personalized Tours"></a>BTRec: BERT-Based Trajectory Recommendation for Personalized Tours</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19886">http://arxiv.org/abs/2310.19886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nxh912/BTRec_RecSys23">https://github.com/nxh912/BTRec_RecSys23</a></li>
<li>paper_authors: Ngai Lam Ho, Roy Ka-Wei Lee, Kwan Hui Lim</li>
<li>for: 这项研究的目的是提供个性化的旅游路线建议，以帮助旅行者在不знаком的城市中享受更加美好的旅行体验。</li>
<li>methods: 这项研究使用BERT框架，combined with user demographic information and past POI visits，提出了一种基于POIBERT嵌入算法的迭代算法（BTREC），用于个性化POI路线建议。</li>
<li>results: 实验结果表明，BTREC算法在八个不同规模的城市 dataset 上具有稳定性和高度的效果，与许多其他序列预测算法相比， measured by recall, precision, and F1-scores。<details>
<summary>Abstract</summary>
An essential task for tourists having a pleasant holiday is to have a well-planned itinerary with relevant recommendations, especially when visiting unfamiliar cities. Many tour recommendation tools only take into account a limited number of factors, such as popular Points of Interest (POIs) and routing constraints. Consequently, the solutions they provide may not always align with the individual users of the system. We propose an iterative algorithm in this paper, namely: BTREC (BERT-based Trajectory Recommendation), that extends from the POIBERT embedding algorithm to recommend personalized itineraries on POIs using the BERT framework. Our BTREC algorithm incorporates users' demographic information alongside past POI visits into a modified BERT language model to recommend a personalized POI itinerary prediction given a pair of source and destination POIs. Our recommendation system can create a travel itinerary that maximizes POIs visited, while also taking into account user preferences for categories of POIs and time availability. Our recommendation algorithm is largely inspired by the problem of sentence completion in natural language processing (NLP). Using a dataset of eight cities of different sizes, our experimental results demonstrate that our proposed algorithm is stable and outperforms many other sequence prediction algorithms, measured by recall, precision, and F1-scores.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "An essential task for tourists having a pleasant holiday is to have a well-planned itinerary with relevant recommendations, especially when visiting unfamiliar cities. Many tour recommendation tools only take into account a limited number of factors, such as popular Points of Interest (POIs) and routing constraints. Consequently, the solutions they provide may not always align with the individual users of the system. We propose an iterative algorithm in this paper, namely: BTREC (BERT-based Trajectory Recommendation), that extends from the POIBERT embedding algorithm to recommend personalized itineraries on POIs using the BERT framework. Our BTREC algorithm incorporates users' demographic information alongside past POI visits into a modified BERT language model to recommend a personalized POI itinerary prediction given a pair of source and destination POIs. Our recommendation system can create a travel itinerary that maximizes POIs visited, while also taking into account user preferences for categories of POIs and time availability. Our recommendation algorithm is largely inspired by the problem of sentence completion in natural language processing (NLP). Using a dataset of eight cities of different sizes, our experimental results demonstrate that our proposed algorithm is stable and outperforms many other sequence prediction algorithms, measured by recall, precision, and F1-scores."中文简体版：旅游者在度假时，有一项非常重要的任务是制定一个合适的行程计划，尤其当访问不熟悉的城市时。许多旅游推荐工具只是考虑有限的因素，如流行的点位（POI）和路径约束。因此，他们提供的解决方案可能不一定适合个人用户。我们在这篇论文中提出了一种迭代算法，即BTREC（基于BERT的行程推荐算法），它从POIBERT嵌入算法中推荐个性化的行程计划。我们的BTREC算法将用户的人口信息和过去访问的POI相结合在一个修改后的BERT语言模型中，以预测给定源和目的POI的个性化行程预测。我们的推荐系统可以创建一个包含最多POI的行程计划，同时也考虑用户对分类POI和时间可用性的偏好。我们的推荐算法受到自然语言处理（NLP）中的句子完成问题的启发。使用八个不同规模的城市的数据集，我们的实验结果表明，我们提出的算法稳定性高，并且在多个序列预测算法中赢得了较高的回归、准确率和F1分数。
</details></li>
</ul>
<hr>
<h2 id="Learning-quantum-states-and-unitaries-of-bounded-gate-complexity"><a href="#Learning-quantum-states-and-unitaries-of-bounded-gate-complexity" class="headerlink" title="Learning quantum states and unitaries of bounded gate complexity"></a>Learning quantum states and unitaries of bounded gate complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19882">http://arxiv.org/abs/2310.19882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haimeng Zhao, Laura Lewis, Ishaan Kannan, Yihui Quek, Hsin-Yuan Huang, Matthias C. Caro</li>
<li>for: 本文研究了学习量子状态和量子运算的复杂性。</li>
<li>methods: 作者使用了样本复杂性和查询复杂性来证明了学习量子状态和量子运算的复杂性。</li>
<li>results: 作者证明了学习量子状态和量子运算的sample complexity和查询复杂性必须线性增长。此外，作者还证明了在理想的批处理下，学习量子状态和量子运算的计算复杂性必须线性增长。这些结果解释了量子机器学习模型的表达能力和创造量子状态和量子运算的复杂性之间的关系。<details>
<summary>Abstract</summary>
While quantum state tomography is notoriously hard, most states hold little interest to practically-minded tomographers. Given that states and unitaries appearing in Nature are of bounded gate complexity, it is natural to ask if efficient learning becomes possible. In this work, we prove that to learn a state generated by a quantum circuit with $G$ two-qubit gates to a small trace distance, a sample complexity scaling linearly in $G$ is necessary and sufficient. We also prove that the optimal query complexity to learn a unitary generated by $G$ gates to a small average-case error scales linearly in $G$. While sample-efficient learning can be achieved, we show that under reasonable cryptographic conjectures, the computational complexity for learning states and unitaries of gate complexity $G$ must scale exponentially in $G$. We illustrate how these results establish fundamental limitations on the expressivity of quantum machine learning models and provide new perspectives on no-free-lunch theorems in unitary learning. Together, our results answer how the complexity of learning quantum states and unitaries relate to the complexity of creating these states and unitaries.
</details>
<details>
<summary>摘要</summary>
“量子状态扫描不易，大多数状态对实际应用者来说并不具有兴趣。因为自然界中的状态和单位里程都具有有限的门阶复杂性，因此可以问到是否存在高效的学习方法。在这项工作中，我们证明了要用小距离来学习由quantum circuit生成的状态，需要样本复杂度 Linearly scale with G。我们还证明了要用average-case error来学习由G个门阶生成的单位ри程，需要查询复杂度 Linearly scale with G。虽然可以实现高效的学习，但是在合理的密码学假设下，计算复杂性为学习状态和单位里程的复杂性必须线性增长于G。我们示出了这些结果如何建立量子机器学习模型的基本限制，并提供了新的视角来评估unitary learning的no-free-lunch定理。这些结果回答了学习量子状态和单位里程的复杂性与创造这些状态和单位里程的复杂性之间的关系。”Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Metric-Flows-with-Neural-Networks"><a href="#Metric-Flows-with-Neural-Networks" class="headerlink" title="Metric Flows with Neural Networks"></a>Metric Flows with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19870">http://arxiv.org/abs/2310.19870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teluashish/traffic-flow-volume-prediction">https://github.com/teluashish/traffic-flow-volume-prediction</a></li>
<li>paper_authors: James Halverson, Fabian Ruehle</li>
<li>for: 这篇论文是关于使用神经网络梯度下降来引导流形的理论研究。</li>
<li>methods: 论文使用了神经网络 gradient descent 来实现流形的发展，并 derivated 相应的流形方程，它们是由一个复杂的、非本地的 metric neural tangent kernel 控制的。</li>
<li>results: 论文通过对数字Calabi-Yau流形的应用来实践这些思想，并发现了一些有用的特性学习。<details>
<summary>Abstract</summary>
We develop a theory of flows in the space of Riemannian metrics induced by neural network gradient descent. This is motivated in part by recent advances in approximating Calabi-Yau metrics with neural networks and is enabled by recent advances in understanding flows in the space of neural networks. We derive the corresponding metric flow equations, which are governed by a metric neural tangent kernel, a complicated, non-local object that evolves in time. However, many architectures admit an infinite-width limit in which the kernel becomes fixed and the dynamics simplify. Additional assumptions can induce locality in the flow, which allows for the realization of Perelman's formulation of Ricci flow that was used to resolve the 3d Poincar\'e conjecture. We apply these ideas to numerical Calabi-Yau metrics, including a discussion on the importance of feature learning.
</details>
<details>
<summary>摘要</summary>
我们开发了一种在里曼纹理度量空间中流体的理论，它是由神经网络梯度下降引起的。这是由于近期神经网络对Calabi-Yau度量的近似而受到启发，以及对神经网络空间中流体的理解的进展。我们 derive了相应的流体方程，它们是由一个Metric neural tangent kernel（一种复杂、非本地的物体）控制的，这个物体在时间演化。然而，许多架构在无限宽限制下可以得到一个固定的kernel，从而简化动力学。此外，可以通过假设来引入本地性，使得流体可以实现Perelman的形式的Ricci流，这种流可以解决3d Poincaré conjecture。我们在数字Calabi-Yau度量中应用了这些想法，包括一些关于特征学习的讨论。
</details></li>
</ul>
<hr>
<h2 id="Posterior-Sampling-for-Competitive-RL-Function-Approximation-and-Partial-Observation"><a href="#Posterior-Sampling-for-Competitive-RL-Function-Approximation-and-Partial-Observation" class="headerlink" title="Posterior Sampling for Competitive RL: Function Approximation and Partial Observation"></a>Posterior Sampling for Competitive RL: Function Approximation and Partial Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19861">http://arxiv.org/abs/2310.19861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Qiu, Ziyu Dai, Han Zhong, Zhaoran Wang, Zhuoran Yang, Tong Zhang</li>
<li>for: 这 paper  investigate posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations.</li>
<li>methods:  authors propose model-based posterior sampling methods to control both players to learn Nash equilibrium, and incorporate adversarial GEC to handle partial observability.</li>
<li>results:  authors provide low regret bounds for proposed algorithms that can scale sublinearly with the proposed GEC and the number of episodes $T$. These methods can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.<details>
<summary>Abstract</summary>
This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms that can scale sublinearly with the proposed GEC and the number of episodes $T$. To the best of our knowledge, we for the first time develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.
</details>
<details>
<summary>摘要</summary>
To measure the complexity of function approximation in MGs, we propose the self-play and adversarial generalized eluder coefficient (GEC). This captures the exploration-exploitation trade-off in MGs and provides a basis for developing model-based posterior sampling methods to control both players in learning Nash equilibrium.In self-play settings, we propose a model-based self-play posterior sampling method that can successfully handle partial observability of states. Additionally, we identify a set of partially observable MG models that can be used for MG learning with adversarial policies of the opponent.Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We provide low regret bounds for our proposed algorithms, which can scale sublinearly with the proposed GEC and the number of episodes $T$.Our contributions are twofold: (1) we develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning; and (2) we provide low regret bounds for our proposed algorithms, which can scale sublinearly with the proposed GEC and the number of episodes $T$.To the best of our knowledge, this is the first time that model-based posterior sampling algorithms have been developed for competitive RL that can handle a wide range of zero-sum MG classes with self-play and adversarial learning, and provide low regret bounds that can scale sublinearly with the proposed GEC and the number of episodes $T$.
</details></li>
</ul>
<hr>
<h2 id="Robust-Causal-Bandits-for-Linear-Models"><a href="#Robust-Causal-Bandits-for-Linear-Models" class="headerlink" title="Robust Causal Bandits for Linear Models"></a>Robust Causal Bandits for Linear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19794">http://arxiv.org/abs/2310.19794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirui Yan, Arpan Mukherjee, Burak Varıcı, Ali Tajer</li>
<li>for: 本文是为了研究 causal bandits 中模型不断变化的情况下的 robustness。</li>
<li>methods: 本文使用了 sequential design of experiments 来优化 reward function，并采用了 cumulative regret 作为设计目标。</li>
<li>results: 本文显示了现有方法在模型偏差的情况下的 regret 是线性增长的，而提出了一种robust causal bandits algorithm，其 regret 是 near-optimal 的 $\tilde{\mathcal{O}(\sqrt{T})$。<details>
<summary>Abstract</summary>
Sequential design of experiments for optimizing a reward function in causal systems can be effectively modeled by the sequential design of interventions in causal bandits (CBs). In the existing literature on CBs, a critical assumption is that the causal models remain constant over time. However, this assumption does not necessarily hold in complex systems, which constantly undergo temporal model fluctuations. This paper addresses the robustness of CBs to such model fluctuations. The focus is on causal systems with linear structural equation models (SEMs). The SEMs and the time-varying pre- and post-interventional statistical models are all unknown. Cumulative regret is adopted as the design criteria, based on which the objective is to design a sequence of interventions that incur the smallest cumulative regret with respect to an oracle aware of the entire causal model and its fluctuations. First, it is established that the existing approaches fail to maintain regret sub-linearity with even a few instances of model deviation. Specifically, when the number of instances with model deviation is as few as $T^\frac{1}{2L}$, where $T$ is the time horizon and $L$ is the longest causal path in the graph, the existing algorithms will have linear regret in $T$. Next, a robust CB algorithm is designed, and its regret is analyzed, where upper and information-theoretic lower bounds on the regret are established. Specifically, in a graph with $N$ nodes and maximum degree $d$, under a general measure of model deviation $C$, the cumulative regret is upper bounded by $\tilde{\mathcal{O}(d^{L-\frac{1}{2}(\sqrt{NT} + NC))$ and lower bounded by $\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T},d^2C\})$. Comparing these bounds establishes that the proposed algorithm achieves nearly optimal $\tilde{\mathcal{O}(\sqrt{T})$ regret when $C$ is $o(\sqrt{T})$ and maintains sub-linear regret for a broader range of $C$.
</details>
<details>
<summary>摘要</summary>
统计设计实验可以有效地模型 causal 系统中的奖励函数。在现有的文献中，一个重要的假设是 causal 模型在时间上不会改变。但这个假设可能不正确，因为复杂的系统可能会在时间上持续地改变。这篇论文处理了 causal 系统中 model 改变的影响。我们专注于 causal 系统中的线性结构方程模型 (SEM)，并且所有的时间变化前后统计模型都是未知的。我们采用了累累 regret 作为设计标准，并且目标是设计一系列的干预，以实现最小的累累 regret，对于一个了解整个 causal 模型和其变化的 oracle。首先，我们证明了现有的方法在几次模型偏差后会具有线性 regret。具体来说，当时间悠久 $T$ 和最长 causal 路径 $L$ 的比例为 $T^\frac{1}{2L}$ 时，现有的算法将会具有线性 regret。接下来，我们设计了一个预警 causal 搜索 algorithm，并且分析了它的 regret。具体来说，在一个具有 $N$ 个节点和最大关系度 $d$ 的图形上，在一个普通的模型偏差 $C$ 下，累累 regret 是上界 $\tilde{\mathcal{O}(d^{L-\frac{1}{2}(\sqrt{NT} + NC))$ 和下界 $\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T},d^2C\})$。比较这些下界可以证明我们的算法实现了近乎最佳的 $\tilde{\mathcal{O}(\sqrt{T})$ regret，当 $C$ 是 $o(\sqrt{T})$ 时。此外，我们的算法还可以在更广泛的 $C$ 下维持线性 regret。
</details></li>
</ul>
<hr>
<h2 id="On-Learning-Gaussian-Multi-index-Models-with-Gradient-Flow"><a href="#On-Learning-Gaussian-Multi-index-Models-with-Gradient-Flow" class="headerlink" title="On Learning Gaussian Multi-index Models with Gradient Flow"></a>On Learning Gaussian Multi-index Models with Gradient Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19793">http://arxiv.org/abs/2310.19793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Bietti, Joan Bruna, Loucas Pillaud-Vivien</li>
<li>for: 这个论文研究了高维 Gaussian 数据上的多指标回归问题，并使用了梯度流算法来解决这个问题。</li>
<li>methods: 这个论文使用了一种两个时间步骤算法，其中低维链函数使用非 Parametric 模型，而且在低维向量空间中学习了一个低维投影。</li>
<li>results: 论文表明，在适当地利用了子空间相关矩阵的 matrix semigroup 结构的情况下，可以确定Gradient 流动的全局收敛性，并提供了这个流动的相关 ‘saddle-to-saddle’ 动态的量化描述。<details>
<summary>Abstract</summary>
We study gradient flow on the multi-index regression problem for high-dimensional Gaussian data. Multi-index functions consist of a composition of an unknown low-rank linear projection and an arbitrary unknown, low-dimensional link function. As such, they constitute a natural template for feature learning in neural networks.   We consider a two-timescale algorithm, whereby the low-dimensional link function is learnt with a non-parametric model infinitely faster than the subspace parametrizing the low-rank projection. By appropriately exploiting the matrix semigroup structure arising over the subspace correlation matrices, we establish global convergence of the resulting Grassmannian population gradient flow dynamics, and provide a quantitative description of its associated `saddle-to-saddle' dynamics. Notably, the timescales associated with each saddle can be explicitly characterized in terms of an appropriate Hermite decomposition of the target link function. In contrast with these positive results, we also show that the related \emph{planted} problem, where the link function is known and fixed, in fact has a rough optimization landscape, in which gradient flow dynamics might get trapped with high probability.
</details>
<details>
<summary>摘要</summary>
我们研究了梯度流在多指标回归问题上，这个问题适用于高维 Gaussian 数据。多指标函数可以看作是一种含有未知低维线性投影和一个未知低维连接函数的组合。因此，它们成为了神经网络中的自然特征学习模板。我们考虑了一种两个时间标度的算法，其中低维连接函数使用非 Parametric 模型 infinitely faster than 子空间嵌入函数。通过利用子空间相关矩阵的matrix semigroup结构，我们证明了涉及的 Grassmannian 人口梯度流动的全球收敛性，并提供了相关的 `saddle-to-saddle' 动态的量化描述。与此不同的是，相关的植入（planted）问题，其中连接函数已知并固定，实际上有一个 rugged 优化山峰，因此梯度流动可能会在高概率下被困。
</details></li>
</ul>
<hr>
<h2 id="Locally-Optimal-Best-Arm-Identification-with-a-Fixed-Budget"><a href="#Locally-Optimal-Best-Arm-Identification-with-a-Fixed-Budget" class="headerlink" title="Locally Optimal Best Arm Identification with a Fixed Budget"></a>Locally Optimal Best Arm Identification with a Fixed Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19788">http://arxiv.org/abs/2310.19788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kato</li>
<li>for: 这个研究目的是确定最佳治疗臂，即治疗臂具有最高预期结果。</li>
<li>methods: 我们使用了各种方法来探索最佳治疗臂，包括best arm identification（BAI）和ORDINAL OPTIMIZATION。我们还使用了Generalized-Neyman-Allocation（GNA）-empirical-best-arm（EBA）策略，这是Neyman（1934）所提出的Neyman分配的扩展和Bubeck等人（2011）所提出的Uniform-EBA策略。</li>
<li>results: 我们的实验结果显示，GNA-EBA策略在小差值领域下是 asymptotically 优化的，即其错误率与下限 bounds 相align，这意味着这个策略在这个领域下是最佳的。<details>
<summary>Abstract</summary>
This study investigates the problem of identifying the best treatment arm, a treatment arm with the highest expected outcome. We aim to identify the best treatment arm with a lower probability of misidentification, which has been explored under various names across numerous research fields, including \emph{best arm identification} (BAI) and ordinal optimization. In our experiments, the number of treatment-allocation rounds is fixed. In each round, a decision-maker allocates a treatment arm to an experimental unit and observes a corresponding outcome, which follows a Gaussian distribution with a variance different among treatment arms. At the end of the experiment, we recommend one of the treatment arms as an estimate of the best treatment arm based on the observations. The objective of the decision-maker is to design an experiment that minimizes the probability of misidentifying the best treatment arm. With this objective in mind, we develop lower bounds for the probability of misidentification under the small-gap regime, where the gaps of the expected outcomes between the best and suboptimal treatment arms approach zero. Then, assuming that the variances are known, we design the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is an extension of the Neyman allocation proposed by Neyman (1934) and the Uniform-EBA strategy proposed by Bubeck et al. (2011). For the GNA-EBA strategy, we show that the strategy is asymptotically optimal because its probability of misidentification aligns with the lower bounds as the sample size approaches infinity under the small-gap regime. We refer to such optimal strategies as locally asymptotic optimal because their performance aligns with the lower bounds within restricted situations characterized by the small-gap regime.
</details>
<details>
<summary>摘要</summary>
To achieve this goal, we develop lower bounds for the probability of misidentification under the small-gap regime, where the gaps of the expected outcomes between the best and suboptimal treatment arms approach zero. We then design the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is an extension of the Neyman allocation proposed by Neyman (1934) and the Uniform-EBA strategy proposed by Bubeck et al. (2011). We show that the GNA-EBA strategy is asymptotically optimal because its probability of misidentification aligns with the lower bounds as the sample size approaches infinity under the small-gap regime. We refer to such optimal strategies as locally asymptotic optimal because their performance aligns with the lower bounds within restricted situations characterized by the small-gap regime.
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Attention-Neural-Networks-for-Non-Line-of-Sight-User-Tracking-with-Dynamic-Metasurface-Antennas"><a href="#Autoregressive-Attention-Neural-Networks-for-Non-Line-of-Sight-User-Tracking-with-Dynamic-Metasurface-Antennas" class="headerlink" title="Autoregressive Attention Neural Networks for Non-Line-of-Sight User Tracking with Dynamic Metasurface Antennas"></a>Autoregressive Attention Neural Networks for Non-Line-of-Sight User Tracking with Dynamic Metasurface Antennas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19767">http://arxiv.org/abs/2310.19767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyriakos Stylianopoulos, Murat Bayraktar, Nuria González Prelcic, George C. Alexandropoulos</li>
<li>for: 这个论文旨在革新下一代无线网络中的用户本地化和跟踪技术，使用动态金属表 antenna (DMA) 技术。</li>
<li>methods: 该论文使用两个阶段的机器学习方法进行用户跟踪，特别是在非直线射频环境中。首先，使用注意力机制来将噪声 Channel response 映射到用户位置。其次，通过一个学习的 autoregressive 模型来利用时间相关的通道信息来获得最终的位置预测。</li>
<li>results: 数字评估结果表明，尽管LoS堵塞，这种方法在多种 multipath 环境中可以实现高精度的位置预测。<details>
<summary>Abstract</summary>
User localization and tracking in the upcoming generation of wireless networks have the potential to be revolutionized by technologies such as the Dynamic Metasurface Antennas (DMAs). Commonly proposed algorithmic approaches rely on assumptions about relatively dominant Line-of-Sight (LoS) paths, or require pilot transmission sequences whose length is comparable to the number of DMA elements, thus, leading to limited effectiveness and considerable measurement overheads in blocked LoS and dynamic multipath environments. In this paper, we present a two-stage machine-learning-based approach for user tracking, specifically designed for non-LoS multipath settings. A newly proposed attention-based Neural Network (NN) is first trained to map noisy channel responses to potential user positions, regardless of user mobility patterns. This architecture constitutes a modification of the prominent vision transformer, specifically modified for extracting information from high-dimensional frequency response signals. As a second stage, the NN's predictions for the past user positions are passed through a learnable autoregressive model to exploit the time-correlated channel information and obtain the final position predictions. The channel estimation procedure leverages a DMA receive architecture with partially-connected radio frequency chains, which results to reduced numbers of pilots. The numerical evaluation over an outdoor ray-tracing scenario illustrates that despite LoS blockage, this methodology is capable of achieving high position accuracy across various multipath settings.
</details>
<details>
<summary>摘要</summary>
User 本地化和跟踪在未来的无线网络中将有可能被革命化由动态元件天线（DMA）技术。通常的算法方法假设有 relativelly 主导的直线视线（LoS）路径，或者需要启发传输序列的长度相当于DMA元件的数量，从而导致有限的效果和较大的测量过程中的遮挡和动态干扰环境中。在本文中，我们提出了一种基于机器学习的两个阶段方法 для用户跟踪，特别是非直线视线多path 环境中。我们首先使用一种新的注意力基于神经网络（NN）来将干扰后的通道响应映射到潜在的用户位置，无论用户移动模式。这个架构是基于prominent vision transformer modify的，专门用于从高维频响应信号中提取信息。作为第二个阶段，NN的过去用户位置预测被传递给一个学习的自然随机过程，以利用时相关的通道信息并获得最终的位置预测。通道估计过程利用了DMA接收架构，其中一部分连接的 радио频信号链，这会减少数据的数量。 numerically 评估在一个outdoor 照明场景中表明，尽管LoS堵塞，这种方法可以在不同的多path 环境中实现高精度的位置预测。
</details></li>
</ul>
<hr>
<h2 id="Epidemic-outbreak-prediction-using-machine-learning-models"><a href="#Epidemic-outbreak-prediction-using-machine-learning-models" class="headerlink" title="Epidemic outbreak prediction using machine learning models"></a>Epidemic outbreak prediction using machine learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19760">http://arxiv.org/abs/2310.19760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshara Pramod, JS Abhishek, Dr. Suganthi K</li>
<li>for: 这个研究是为了预测爆发在纽约州的流感、肝炎和 маля리아疫情，以便当地当局和医疗机构可以预先准备必要的药物和物资。</li>
<li>methods: 这个研究使用机器学习和深度学习算法，以及一个portal来预测疫情爆发。该算法使用历史数据预测下一个5个星期内可能出现的病例数量。此外，非клиниче因素如Google搜索趋势、社交媒体数据和天气数据也被使用来预测疫情爆发的可能性。</li>
<li>results: 根据研究结果，这些算法可以准确地预测疫情爆发，并且可以提供5个星期内的病例数量预测结果。这些结果可以帮助当地当局和医疗机构预先准备疫情应急应急响应。<details>
<summary>Abstract</summary>
In today's world,the risk of emerging and re-emerging epidemics have increased.The recent advancement in healthcare technology has made it possible to predict an epidemic outbreak in a region.Early prediction of an epidemic outbreak greatly helps the authorities to be prepared with the necessary medications and logistics required to keep things in control. In this article, we try to predict the epidemic outbreak (influenza, hepatitis and malaria) for the state of New York, USA using machine and deep learning algorithms, and a portal has been created for the same which can alert the authorities and health care organizations of the region in case of an outbreak. The algorithm takes historical data to predict the possible number of cases for 5 weeks into the future. Non-clinical factors like google search trends,social media data and weather data have also been used to predict the probability of an outbreak.
</details>
<details>
<summary>摘要</summary>
今天的世界中，突发和重新爆发的疫情的风险增加了。最近的医疗技术发展使得可以预测一个地区的疫情爆发。预测疫情爆发的早期帮助当地 autorités 和医疗机构做好准备，以保持事务在控制之下。在这篇文章中，我们使用机器学习和深度学习算法预测新 York 州的 influenza、hepatitis 和 malaria 疫情爆发，并创建了一个portal，可以警示当地 autorités 和医疗机构在疫情爆发时。算法使用历史数据预测下一个5周内可能出现的病例数。此外，我们还使用非клиниче因素，如Google搜索趋势、社交媒体数据和天气数据预测疫情爆发的可能性。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Reward-Estimation-with-Preference-Feedback"><a href="#Differentially-Private-Reward-Estimation-with-Preference-Feedback" class="headerlink" title="Differentially Private Reward Estimation with Preference Feedback"></a>Differentially Private Reward Estimation with Preference Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19733">http://arxiv.org/abs/2310.19733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayak Ray Chowdhury, Xingyu Zhou, Nagarajan Natarajan</li>
<li>For: This paper focuses on aligning generative models with human interests using preference-based feedback, and protecting the privacy of human labelers in the process.* Methods: The authors use reinforcement learning with human feedback (RLHF) to train generative models, and adopt the notion of label differential privacy (DP) to protect the privacy of individual labelers. They use the parametric Bradley-Terry-Luce (BTL) model to estimate the latent reward parameter $\theta^* \in \mathbb{R}^d$ from pairwise comparison feedback.* Results: The authors provide tight upper and lower bounds on the error in estimating $\theta^*$ under both local and central models of DP, and show that the additional cost to ensure label-DP under local model is $\Theta \big(\frac{1}{ e^\epsilon-1}\sqrt{\frac{d}{n}\big)$, while it is $\Theta\big(\frac{\text{poly}(d)}{\epsilon n} \big)$ under the weaker central model. They perform simulations on synthetic data that corroborate these theoretical results.Here is the information in Simplified Chinese text, as requested:* For: 这篇论文关注使用偏好反馈来对人类 интересов进行Alignment，并保护人类标签者的隐私。* Methods: 作者使用人类反馈学习（RLHF）来训练生成模型，并采用标签敏感 differential privacy（DP）来保护每个标签者的隐私。他们使用 Bradley-Terry-Luce（BTL）模型来估算基于对比比较的latent reward参数 $\theta^* \in \mathbb{R}^d$。* Results: 作者提供了 tight 上下文 bound 来估算 $\theta^*$ 的误差，并证明在本地模型下，为保持标签-DP，额外的成本为 $\Theta \big(\frac{1}{ e^\epsilon-1}\sqrt{\frac{d}{n}\big)$。在更弱的中央模型下，成本为 $\Theta\big(\frac{\text{poly}(d)}{\epsilon n} \big)$。作者在合成数据上进行了仪表实验，并证明了这些理论结果。<details>
<summary>Abstract</summary>
Learning from preference-based feedback has recently gained considerable traction as a promising approach to align generative models with human interests. Instead of relying on numerical rewards, the generative models are trained using reinforcement learning with human feedback (RLHF). These approaches first solicit feedback from human labelers typically in the form of pairwise comparisons between two possible actions, then estimate a reward model using these comparisons, and finally employ a policy based on the estimated reward model. An adversarial attack in any step of the above pipeline might reveal private and sensitive information of human labelers. In this work, we adopt the notion of label differential privacy (DP) and focus on the problem of reward estimation from preference-based feedback while protecting privacy of each individual labelers. Specifically, we consider the parametric Bradley-Terry-Luce (BTL) model for such pairwise comparison feedback involving a latent reward parameter $\theta^* \in \mathbb{R}^d$. Within a standard minimax estimation framework, we provide tight upper and lower bounds on the error in estimating $\theta^*$ under both local and central models of DP. We show, for a given privacy budget $\epsilon$ and number of samples $n$, that the additional cost to ensure label-DP under local model is $\Theta \big(\frac{1}{ e^\epsilon-1}\sqrt{\frac{d}{n}\big)$, while it is $\Theta\big(\frac{\text{poly}(d)}{\epsilon n} \big)$ under the weaker central model. We perform simulations on synthetic data that corroborate these theoretical results.
</details>
<details>
<summary>摘要</summary>
学习从偏好反馈获得了相当的满意度，作为一种使生成模型与人类兴趣相对应的方法。而不是依靠数字奖励，这些生成模型通过人工智能反馈学习（RLHF）进行训练。这些方法首先从人类标签器收集反馈，通常是两个可能的行动之间的对比，然后将这些反馈用来估算奖励模型，最后使用这个估算模型来采取策略。在这个管道中的任何攻击都可能泄露人类标签器的私人和敏感信息。在这种情况下，我们采用标签权限隐私（DP）的想法，并专注于基于偏好反馈的奖励估算问题，以保护每个人类标签器的隐私。具体来说，我们考虑使用 parametric Bradley-Terry-Luce（BTL）模型来处理这些对比反馈，其中包含一个隐藏奖励参数 $\theta^* \in \mathbb{R}^d$。在标准的最小最大估算框架下，我们提供了紧密的Upper和Lower bounds，用于估算 $\theta^*$ 的误差，并分析了在本地和中央模型下的DP。我们发现，对于给定的隐私预算 $\epsilon$ 和样本数 $n$，在本地模型下添加额外的隐私成本是 $\Theta \big(\frac{1}{ e^\epsilon-1}\sqrt{\frac{d}{n}\big)$，而在更弱的中央模型下，这个成本是 $\Theta\big(\frac{\text{poly}(d)}{\epsilon n} \big)$。我们在合成数据上进行了实验，并证明了这些理论结论。
</details></li>
</ul>
<hr>
<h2 id="Support-matrix-machine-A-review"><a href="#Support-matrix-machine-A-review" class="headerlink" title="Support matrix machine: A review"></a>Support matrix machine: A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19717">http://arxiv.org/abs/2310.19717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Anuradha Kumari, Mushir Akhtar, Rupal Shah, M. Tanveer</li>
<li>for: This paper is written for academics and researchers who work with matrix input data and want to use support vector machines (SVMs) for classification and regression problems.</li>
<li>methods: The paper proposes a new method called support matrix machine (SMM) that can handle matrix input data and preserve the structural information of the data. SMM uses a combination of the nuclear norm and Frobenius norm, known as the spectral elastic net property, to achieve this.</li>
<li>results: The paper provides an in-depth analysis of the development of the SMM model, including numerous variants such as robust, sparse, class imbalance, and multi-class classification models. The paper also discusses applications of the SMM model and outlines potential future research avenues and possibilities.<details>
<summary>Abstract</summary>
Support vector machine (SVM) is one of the most studied paradigms in the realm of machine learning for classification and regression problems. It relies on vectorized input data. However, a significant portion of the real-world data exists in matrix format, which is given as input to SVM by reshaping the matrices into vectors. The process of reshaping disrupts the spatial correlations inherent in the matrix data. Also, converting matrices into vectors results in input data with a high dimensionality, which introduces significant computational complexity. To overcome these issues in classifying matrix input data, support matrix machine (SMM) is proposed. It represents one of the emerging methodologies tailored for handling matrix input data. The SMM method preserves the structural information of the matrix data by using the spectral elastic net property which is a combination of the nuclear norm and Frobenius norm. This article provides the first in-depth analysis of the development of the SMM model, which can be used as a thorough summary by both novices and experts. We discuss numerous SMM variants, such as robust, sparse, class imbalance, and multi-class classification models. We also analyze the applications of the SMM model and conclude the article by outlining potential future research avenues and possibilities that may motivate academics to advance the SMM algorithm.
</details>
<details>
<summary>摘要</summary>
支持向量机器 (SVM) 是机器学习领域中最受研究的一种类型，用于分类和回归问题。它基于向量化输入数据。然而，现实世界中大量数据存在矩阵格式，需要将矩阵转换为向量，以便给SVM进行输入。然而，这个过程会破坏矩阵数据中的空间相关性，并且将输入数据的维度增加，导致计算复杂性增加。为了解决这些问题，支持矩阵机器 (SMM) 被提出。它是一种处理矩阵输入数据的新方法。SMM方法利用矩阵数据的特征信息，通过spectral elastic net property，该属性是 nuclear norm 和 Frobenius norm 的组合。本文提供了 SMM 模型的首次深入分析，可以作为新手和专家使用的综述。我们讨论了多种 SMM 变体，如 robust、稀热、类偏振和多类分类模型。我们还分析了 SMM 模型的应用，并将文章结束于对 SMM 算法的未来研究方向和可能性的讨论。
</details></li>
</ul>
<hr>
<h2 id="Exact-Recovery-and-Bregman-Hard-Clustering-of-Node-Attributed-Stochastic-Block-Model"><a href="#Exact-Recovery-and-Bregman-Hard-Clustering-of-Node-Attributed-Stochastic-Block-Model" class="headerlink" title="Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic Block Model"></a>Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic Block Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19854">http://arxiv.org/abs/2310.19854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilien Dreveton, Felipe S. Fernandes, Daniel R. Figueiredo</li>
<li>for: 本研究旨在掌握节点网络中的社群标签，同时考虑节点的属性信息。</li>
<li>methods: 该研究提出了一种基于信息理论的 критерион，以及一种基于这个 критерион 的迭代归一化算法，以优化社群标签的掌握。</li>
<li>results: 实验结果表明，提出的算法在Synthetic数据上表现出色，比 классические算法和现状最佳算法更高效。<details>
<summary>Abstract</summary>
Network clustering tackles the problem of identifying sets of nodes (communities) that have similar connection patterns. However, in many scenarios, nodes also have attributes that are correlated with the clustering structure. Thus, network information (edges) and node information (attributes) can be jointly leveraged to design high-performance clustering algorithms. Under a general model for the network and node attributes, this work establishes an information-theoretic criterion for the exact recovery of community labels and characterizes a phase transition determined by the Chernoff-Hellinger divergence of the model. The criterion shows how network and attribute information can be exchanged in order to have exact recovery (e.g., more reliable network information requires less reliable attribute information). This work also presents an iterative clustering algorithm that maximizes the joint likelihood, assuming that the probability distribution of network interactions and node attributes belong to exponential families. This covers a broad range of possible interactions (e.g., edges with weights) and attributes (e.g., non-Gaussian models), as well as sparse networks, while also exploring the connection between exponential families and Bregman divergences. Extensive numerical experiments using synthetic data indicate that the proposed algorithm outperforms classic algorithms that leverage only network or only attribute information as well as state-of-the-art algorithms that also leverage both sources of information. The contributions of this work provide insights into the fundamental limits and practical techniques for inferring community labels on node-attributed networks.
</details>
<details>
<summary>摘要</summary>
Under a general model for the network and node attributes, this work establishes an information-theoretic criterion for the exact recovery of community labels and characterizes a phase transition determined by the Chernoff-Hellinger divergence of the model. The criterion shows how network and attribute information can be exchanged to achieve exact recovery (e.g., more reliable network information requires less reliable attribute information).This work also presents an iterative clustering algorithm that maximizes the joint likelihood, assuming that the probability distribution of network interactions and node attributes belongs to exponential families. This covers a broad range of possible interactions (e.g., edges with weights) and attributes (e.g., non-Gaussian models), as well as sparse networks, while also exploring the connection between exponential families and Bregman divergences.Numerical experiments using synthetic data show that the proposed algorithm outperforms classic algorithms that only use network or attribute information and state-of-the-art algorithms that also use both sources of information. The contributions of this work provide insights into the fundamental limits and practical techniques for inferring community labels on node-attributed networks.
</details></li>
</ul>
<hr>
<h2 id="Convolutional-State-Space-Models-for-Long-Range-Spatiotemporal-Modeling"><a href="#Convolutional-State-Space-Models-for-Long-Range-Spatiotemporal-Modeling" class="headerlink" title="Convolutional State Space Models for Long-Range Spatiotemporal Modeling"></a>Convolutional State Space Models for Long-Range Spatiotemporal Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19694">http://arxiv.org/abs/2310.19694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimmy T. H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon</li>
<li>for: 模型长时间空间序列的挑战在于同时模型复杂的空间相关性和长距离时间依赖关系。</li>
<li>methods: 本文提出了一种新的卷积状态空间模型（ConvSSM），结合了卷积神经网络和状态空间方法的优点，以提高长时间空间序列的模型性能。</li>
<li>results: 对比于Transformers和卷积LSTM，ConvS5在一个长期运动MNIST实验中表现出色，训练3倍 faster和生成样本400倍 faster，并在DMLab、Minecraft和Habitat预测benchmark上与或超过了现有方法的性能。<details>
<summary>Abstract</summary>
Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.
</details>
<details>
<summary>摘要</summary>
长期空间序列模型化是一项挑战，因为需要同时模型复杂的空间相关性和长距离时间依赖关系。ConvLSTM通过将张量状态更新为逻辑神经网络，但它们的顺序计算使其训练速度慢。相比之下，Transformers可以在并行计算整个空间时间序列，但它们的注意力成本 quadratic 增长，限制其扩展性。在这里，我们解决先前方法的挑战，并引入张量状态空间模型（ConvSSM），它将ConvLSTM的张量模型思想与状态方法如S4和S5的长序列模型方法结合。首先，我们说明了如何使用并行扫描来实现张量权重的并行化，以实现高效的自然递归生成。然后，我们证明了ConvSSM的动态与状态方法的动态相同，这使得我们可以设计参数和初始化策略来模型长距离相关性。结果是ConvS5，一种高效的ConvSSM变体，用于长距离空间时间模型化。ConvS5在一个长期 Moving-MNIST 实验中显著超过了Transformers和ConvLSTM，并在训练3倍 faster than ConvLSTM 和生成样本400倍 faster than Transformers。此外，ConvS5与当前状态级方法在 DMLab、Minecraft 和 Habitat 预测benchmark上具有相同或更高的性能，并开启了长期空间时间序列的新模型方向。
</details></li>
</ul>
<hr>
<h2 id="Towards-Practical-Non-Adversarial-Distribution-Alignment-via-Variational-Bounds"><a href="#Towards-Practical-Non-Adversarial-Distribution-Alignment-via-Variational-Bounds" class="headerlink" title="Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds"></a>Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19690">http://arxiv.org/abs/2310.19690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyu Gong, Ben Usman, Han Zhao, David I. Inouye</li>
<li>for: 学习不变表示，应用于公平和稳定性。</li>
<li>methods: 使用非对抗性VB-based方法，可应用于任何模型链。</li>
<li>results: 可以取代对抗损失，不需修改原始架构，广泛应用非对抗性Alignment方法。Here’s a breakdown of each point:</li>
<li>for: The paper is written for learning invariant representations, with a focus on fairness and robustness.</li>
<li>methods: The paper proposes a non-adversarial VAE-based alignment method that can be applied to any model pipeline.</li>
<li>results: The proposed method can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures, significantly broadening the applicability of non-adversarial alignment methods.<details>
<summary>Abstract</summary>
Distribution alignment can be used to learn invariant representations with applications in fairness and robustness. Most prior works resort to adversarial alignment methods but the resulting minimax problems are unstable and challenging to optimize. Non-adversarial likelihood-based approaches either require model invertibility, impose constraints on the latent prior, or lack a generic framework for alignment. To overcome these limitations, we propose a non-adversarial VAE-based alignment method that can be applied to any model pipeline. We develop a set of alignment upper bounds (including a noisy bound) that have VAE-like objectives but with a different perspective. We carefully compare our method to prior VAE-based alignment approaches both theoretically and empirically. Finally, we demonstrate that our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures -- thereby significantly broadening the applicability of non-adversarial alignment methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>分布对齐可以用来学习不变表示，并且有应用于公平和Robustness。大多数先前的工作都是通过对抗对齐方法来实现，但这些最小化问题是不稳定且困难优化。非对抗的可能性-基于方法 Either require model invertibility, impose constraints on the latent prior, or lack a generic framework for alignment. To overcome these limitations, we propose a non-adversarial VAE-based alignment method that can be applied to any model pipeline. We develop a set of alignment upper bounds (including a noisy bound) that have VAE-like objectives but with a different perspective. We carefully compare our method to prior VAE-based alignment approaches both theoretically and empirically. Finally, we demonstrate that our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures -- thereby significantly broadening the applicability of non-adversarial alignment methods.
</details></li>
</ul>
<hr>
<h2 id="DGFN-Double-Generative-Flow-Networks"><a href="#DGFN-Double-Generative-Flow-Networks" class="headerlink" title="DGFN: Double Generative Flow Networks"></a>DGFN: Double Generative Flow Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19685">http://arxiv.org/abs/2310.19685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elaine Lau, Nikhil Vemgal, Doina Precup, Emmanuel Bengio</li>
<li>for: 这篇研究探讨了用深度学习来进行药品探索，特别是运用Generative Flow Networks（GFlowNets&#x2F;GFNs）生成多元候选者。</li>
<li>methods: 本研究引入了双层Generative Flow Networks（DGFNs），参考了增强学习和双层深度Q学习，使用目标网络获取访问路径，并将主网络更新为这些访问路径。</li>
<li>results: 实验结果显示，DGFNs有效地增强了在罕见奖励领域和高维州空间中的探索，具有丰富的应用前景在药品探索中。<details>
<summary>Abstract</summary>
Deep learning is emerging as an effective tool in drug discovery, with potential applications in both predictive and generative models. Generative Flow Networks (GFlowNets/GFNs) are a recently introduced method recognized for the ability to generate diverse candidates, in particular in small molecule generation tasks. In this work, we introduce double GFlowNets (DGFNs). Drawing inspiration from reinforcement learning and Double Deep Q-Learning, we introduce a target network used to sample trajectories, while updating the main network with these sampled trajectories. Empirical results confirm that DGFNs effectively enhance exploration in sparse reward domains and high-dimensional state spaces, both challenging aspects of de-novo design in drug discovery.
</details>
<details>
<summary>摘要</summary>
深度学习在药物发现中emerging为有效工具，潜在应用于预测和生成模型。生成流网络（GFlowNets/GFNs）是最近引入的方法，被认可为能够生成多样化的候选者，尤其是小分子生成任务中。在这种工作中，我们引入双流网络（DGFNs）。通过引入奖励学习和双深度Q学习，我们引入一个目标网络用于采样轨迹，并将主网络更新为这些采样轨迹。实验结果表明，DGFNs有效地增强了探索性在缺乏奖励的领域和高维状态空间中，这些领域都是药物发现中的挑战。
</details></li>
</ul>
<hr>
<h2 id="Density-Estimation-for-Entry-Guidance-Problems-using-Deep-Learning"><a href="#Density-Estimation-for-Entry-Guidance-Problems-using-Deep-Learning" class="headerlink" title="Density Estimation for Entry Guidance Problems using Deep Learning"></a>Density Estimation for Entry Guidance Problems using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19684">http://arxiv.org/abs/2310.19684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens A. Rataczak, Davide Amato, Jay W. McMahon</li>
<li>for: 这篇论文是用来解决行星入 atmospheric density profiles estimation问题的深度学习方法。</li>
<li>methods: 论文使用了一个长期短期记忆（LSTM）神经网络，通过学习将在board的探测器上可用的测量与 atmospheric density profile 之间的映射关系。测量包括圆柱状态表示、Cartesian感知加速度组件和表层压力测量。</li>
<li>results: 论文的结果表明，使用LSTM网络可以更好地预测行星入 atmospheric density profiles，并且可以重建过去飞行的 density profile。论文还证明了使用LSTM模型可以提高行星入 guidance 算法的终端准确性，比其他两种技术更好。<details>
<summary>Abstract</summary>
This work presents a deep-learning approach to estimate atmospheric density profiles for use in planetary entry guidance problems. A long short-term memory (LSTM) neural network is trained to learn the mapping between measurements available onboard an entry vehicle and the density profile through which it is flying. Measurements include the spherical state representation, Cartesian sensed acceleration components, and a surface-pressure measurement. Training data for the network is initially generated by performing a Monte Carlo analysis of an entry mission at Mars using the fully numerical predictor-corrector guidance (FNPEG) algorithm that utilizes an exponential density model, while the truth density profiles are sampled from MarsGRAM. A curriculum learning procedure is developed to refine the LSTM network's predictions for integration within the FNPEG algorithm. The trained LSTM is capable of both predicting the density profile through which the vehicle will fly and reconstructing the density profile through which it has already flown. The performance of the FNPEG algorithm is assessed for three different density estimation techniques: an exponential model, an exponential model augmented with a first-order fading-memory filter, and the LSTM network. Results demonstrate that using the LSTM model results in superior terminal accuracy compared to the other two techniques when considering both noisy and noiseless measurements.
</details>
<details>
<summary>摘要</summary>
The training data for the network is generated by performing a Monte Carlo analysis of an entry mission at Mars using the fully numerical predictor-corrector guidance (FNPEG) algorithm, which utilizes an exponential density model. The truth density profiles are sampled from MarsGRAM. To refine the LSTM network's predictions, a curriculum learning procedure is developed.The trained LSTM network is capable of both predicting the density profile through which the vehicle will fly and reconstructing the density profile through which it has already flown. The performance of the FNPEG algorithm is assessed for three different density estimation techniques: an exponential model, an exponential model augmented with a first-order fading-memory filter, and the LSTM network. The results show that using the LSTM model results in superior terminal accuracy compared to the other two techniques when considering both noisy and noiseless measurements.
</details></li>
</ul>
<hr>
<h2 id="An-Online-Bootstrap-for-Time-Series"><a href="#An-Online-Bootstrap-for-Time-Series" class="headerlink" title="An Online Bootstrap for Time Series"></a>An Online Bootstrap for Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19683">http://arxiv.org/abs/2310.19683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Nicolai Palm, Thomas Nagler</li>
<li>for: 这篇论文是为了解决实时资料分析中的抽样问题，特别是处理大量相互相关的资料时。</li>
<li>methods: 本研究提出了一种基于自适应抽样质量的新方法，可以在线上执行，适合实时应用。这个方法基于一个自适应增长的抽样重要性序列。</li>
<li>results: 我们透过实验证明了这个方法的理论有效性，并且在复杂的资料依赖关系下提供了可靠的不确定量化。本研究将传统抽样技术与现代资料分析之间的距离缩小，提供了实用的工具 для研究者和实践者。<details>
<summary>Abstract</summary>
Resampling methods such as the bootstrap have proven invaluable in the field of machine learning. However, the applicability of traditional bootstrap methods is limited when dealing with large streams of dependent data, such as time series or spatially correlated observations. In this paper, we propose a novel bootstrap method that is designed to account for data dependencies and can be executed online, making it particularly suitable for real-time applications. This method is based on an autoregressive sequence of increasingly dependent resampling weights. We prove the theoretical validity of the proposed bootstrap scheme under general conditions. We demonstrate the effectiveness of our approach through extensive simulations and show that it provides reliable uncertainty quantification even in the presence of complex data dependencies. Our work bridges the gap between classical resampling techniques and the demands of modern data analysis, providing a valuable tool for researchers and practitioners in dynamic, data-rich environments.
</details>
<details>
<summary>摘要</summary>
bootstrap 方法如 bootstrap 已经在机器学习领域得到了广泛的应用。然而，传统的 bootstrap 方法在处理大量相关数据时失效，如时间序列或空间相关观察值。在这篇论文中，我们提出了一种新的 bootstrap 方法，可以考虑数据相互关系，并且可以在线执行，使其特别适用于实时应用。这种方法基于一个自增式相关的排序重样Weight。我们证明了该 bootstrap 方案的理论有效性，并通过了广泛的仿真实验，证明它可以在复杂数据相互关系下提供可靠的不确定性评估。我们的工作 bridges 了传统的排 sampling 技术和现代数据分析的需求之间，提供了一种有价值的工具，为研究人员和实践者在动态、数据丰富的环境中。
</details></li>
</ul>
<hr>
<h2 id="HyPE-Attention-with-Hyperbolic-Biases-for-Relative-Positional-Encoding"><a href="#HyPE-Attention-with-Hyperbolic-Biases-for-Relative-Positional-Encoding" class="headerlink" title="HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding"></a>HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19676">http://arxiv.org/abs/2310.19676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgio Angelotti</li>
<li>for: 提高Transformer架构中的注意机制的可 permutation-invariance性，使其能够更好地捕捉各个输入序列中的关系。</li>
<li>methods: 提出了一种新的Hyperbolic Positional Encoding（HyPE）方法，通过利用抽象函数的性质来编码输入序列中的各个元素的相对位置，从而不需要存储 $O(L^2)$ 值的面Mask，其中 $L$ 是输入序列的长度。HyPE 通过预limiting concatenation 操作和矩阵乘法来实现编码，并且可以在 FlashAttention-2 中兼容，并且支持任何可能存在的学习参数的梯度反propagation。</li>
<li>results: 通过分析示出，通过选择合适的 hyperparameter，HyPE 可以近似 ALiBi 的注意偏好，从而提供了更好的泛化能力，并且在未来的实验中可以作为一个可能的方向进行探索。<details>
<summary>Abstract</summary>
In Transformer-based architectures, the attention mechanism is inherently permutation-invariant with respect to the input sequence's tokens. To impose sequential order, token positions are typically encoded using a scheme with either fixed or learnable parameters. We introduce Hyperbolic Positional Encoding (HyPE), a novel method that utilizes hyperbolic functions' properties to encode tokens' relative positions. This approach biases the attention mechanism without the necessity of storing the $O(L^2)$ values of the mask, with $L$ being the length of the input sequence. HyPE leverages preliminary concatenation operations and matrix multiplications, facilitating the encoding of relative distances indirectly incorporating biases into the softmax computation. This design ensures compatibility with FlashAttention-2 and supports the gradient backpropagation for any potential learnable parameters within the encoding. We analytically demonstrate that, by careful hyperparameter selection, HyPE can approximate the attention bias of ALiBi, thereby offering promising generalization capabilities for contexts extending beyond the lengths encountered during pretraining. The experimental evaluation of HyPE is proposed as a direction for future research.
</details>
<details>
<summary>摘要</summary>
在基于Transformer的架构中，注意机制自然地对输入序列中token的 permutation-invariant。为了强制顺序排序，通常使用一种方案，其中 Either fixed or learnable parameters are used to encode token positions. 我们介绍了一种新的方法：幽微位置编码（HyPE），它利用幽微函数的属性来编码token的相对位置。这种方法不需要存储 $O(L^2)$ 值的 маска，其中 $L$ 是输入序列的长度。HyPE 利用了先行 concatenation 操作和矩阵乘法，以便编码相对距离，并通过间接 incorporating 到 softmax 计算中的权重。这种设计确保了与 FlashAttention-2 兼容，并支持任何可能的可学习参数在编码中。我们分析表明，通过精心选择 hyperparameter，HyPE 可以近似 ALiBi 的注意力偏好，从而提供了扩展 beyond  lengths encountered during pretraining 的普适化能力。HyPE 的实验评估被提议作为未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes"><a href="#Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes" class="headerlink" title="Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes"></a>Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19666">http://arxiv.org/abs/2310.19666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wzhut/dynamic-tensor-decomposition-via-neural-diffusion-reaction-processes">https://github.com/wzhut/dynamic-tensor-decomposition-via-neural-diffusion-reaction-processes</a></li>
<li>paper_authors: Zheng Wang, Shikai Fang, Shibo Li, Shandian Zhe</li>
<li>For: This paper proposes a new method called Dynamic EMbedIngs fOr dynamic Tensor dEcomposition (DEMOTE) for dynamic tensor decomposition, which can capture both the commonalities and personalities of the entities in the tensor.* Methods: The proposed method uses a neural diffusion-reaction process to estimate dynamic embeddings for the entities in each tensor mode, and a neural network to model the entry value as a nonlinear function of the embedding trajectories.* Results: The proposed method is shown to have advantages in both simulation study and real-world applications, and can capture the underlying temporal structure of the data more effectively than existing methods.<details>
<summary>Abstract</summary>
Tensor decomposition is an important tool for multiway data analysis. In practice, the data is often sparse yet associated with rich temporal information. Existing methods, however, often under-use the time information and ignore the structural knowledge within the sparsely observed tensor entries. To overcome these limitations and to better capture the underlying temporal structure, we propose Dynamic EMbedIngs fOr dynamic Tensor dEcomposition (DEMOTE). We develop a neural diffusion-reaction process to estimate dynamic embeddings for the entities in each tensor mode. Specifically, based on the observed tensor entries, we build a multi-partite graph to encode the correlation between the entities. We construct a graph diffusion process to co-evolve the embedding trajectories of the correlated entities and use a neural network to construct a reaction process for each individual entity. In this way, our model can capture both the commonalities and personalities during the evolution of the embeddings for different entities. We then use a neural network to model the entry value as a nonlinear function of the embedding trajectories. For model estimation, we combine ODE solvers to develop a stochastic mini-batch learning algorithm. We propose a stratified sampling method to balance the cost of processing each mini-batch so as to improve the overall efficiency. We show the advantage of our approach in both simulation study and real-world applications. The code is available at https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes.
</details>
<details>
<summary>摘要</summary>
tensor 分解是多方数据分析中的重要工具。在实践中，数据通常是稀疏的， yet 具有丰富的时间信息。现有方法通常会下用时间信息和tensor 中稀疏观测的结构知识。为了超越这些限制和更好地捕捉下面结构，我们提出了动态嵌入 для动态tensor 分解（DEMOTE）。我们采用神经扩散-反应过程来估算动态嵌入 для不同模式的tensor 中的实体。具体来说，基于观测的tensor 入口，我们构建了多部分图来编码实体之间的相关性。我们构建了图扩散过程来同步嵌入轨迹的演化，并使用神经网络来构建每个实体的反应过程。这样，我们的模型可以捕捉不同实体的共同特征和个性特征在tensor 分解过程中的演化。然后，我们使用神经网络来模型每个入口的值为非线性函数。为模型估计，我们结合了ode 解除器来开发随机批处理算法。我们提出了分解采样方法，以保证每个批处理的成本相对均衡。我们在 simulate 研究和实际应用中展示了我们的方法的优势。代码可以在 <https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Predicting-mutational-effects-on-protein-protein-binding-via-a-side-chain-diffusion-probabilistic-model"><a href="#Predicting-mutational-effects-on-protein-protein-binding-via-a-side-chain-diffusion-probabilistic-model" class="headerlink" title="Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model"></a>Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19849">http://arxiv.org/abs/2310.19849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eurekazhu/diffaffinity">https://github.com/eurekazhu/diffaffinity</a></li>
<li>paper_authors: Shiwei Liu, Tian Zhu, Milong Ren, Chungong Yu, Dongbo Bu, Haicang Zhang</li>
<li>for: 预测蛋白质-蛋白质结合的质量变化，用于蛋白工程和药物发现。</li>
<li>methods: 使用 representation learning 方法，基于无标注实验数据，学习蛋白质副链的生成过程，并对蛋白质-蛋白质接触面的结构上的变化进行 representations。</li>
<li>results: 实现了预测蛋白质-蛋白质结合的质量变化的最佳性能，并且 SidechainDiff 是首个使用液体傅尔父模型来生成副链结构的方法。<details>
<summary>Abstract</summary>
Many crucial biological processes rely on networks of protein-protein interactions. Predicting the effect of amino acid mutations on protein-protein binding is vital in protein engineering and therapeutic discovery. However, the scarcity of annotated experimental data on binding energy poses a significant challenge for developing computational approaches, particularly deep learning-based methods. In this work, we propose SidechainDiff, a representation learning-based approach that leverages unlabelled experimental protein structures. SidechainDiff utilizes a Riemannian diffusion model to learn the generative process of side-chain conformations and can also give the structural context representations of mutations on the protein-protein interface. Leveraging the learned representations, we achieve state-of-the-art performance in predicting the mutational effects on protein-protein binding. Furthermore, SidechainDiff is the first diffusion-based generative model for side-chains, distinguishing it from prior efforts that have predominantly focused on generating protein backbone structures.
</details>
<details>
<summary>摘要</summary>
很多生物过程依赖于蛋白质-蛋白质之间的交互，而预测蛋白质变异对蛋白质-蛋白质绑定的影响是蛋白工程和药物发现中非常重要的。然而，实验室内缺乏绑定能力的标注数据，对于开发计算方法，特别是深度学习方法，带来了很大的挑战。在这项工作中，我们提出了SidechainDiff，一种基于学习推论的方法，利用无标注实验蛋白结构来学习蛋白分子中侧链的生成过程。SidechainDiff使用瑞 Mann 扩散模型来学习侧链的生成过程，同时还可以给蛋白质-蛋白质界面上的杂交位点 Representations。利用学习的表示，我们实现了对蛋白质变异对蛋白质-蛋白质绑定的预测性能的状态级别表现。此外，SidechainDiff是首个采用扩散模型生成侧链的方法，与之前的主要关注在蛋白质脊梁结构生成上。
</details></li>
</ul>
<hr>
<h2 id="Dis-inhibitory-neuronal-circuits-can-control-the-sign-of-synaptic-plasticity"><a href="#Dis-inhibitory-neuronal-circuits-can-control-the-sign-of-synaptic-plasticity" class="headerlink" title="Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity"></a>Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19614">http://arxiv.org/abs/2310.19614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmi-basel/disinhibitory-control">https://github.com/fmi-basel/disinhibitory-control</a></li>
<li>paper_authors: Julian Rossbroich, Friedemann Zenke</li>
<li>for: 解决 neural circuit 中信息归属问题</li>
<li>methods: 使用 microcircuit model 和 Hebbian learning rule</li>
<li>results:  naturally emerges error-modulated learning 和 comparable performance to back-propagation of error on several non-linearly separable benchmarks<details>
<summary>Abstract</summary>
How neuronal circuits achieve credit assignment remains a central unsolved question in systems neuroscience. Various studies have suggested plausible solutions for back-propagating error signals through multi-layer networks. These purely functionally motivated models assume distinct neuronal compartments to represent local error signals that determine the sign of synaptic plasticity. However, this explicit error modulation is inconsistent with phenomenological plasticity models in which the sign depends primarily on postsynaptic activity. Here we show how a plausible microcircuit model and Hebbian learning rule derived within an adaptive control theory framework can resolve this discrepancy. Assuming errors are encoded in top-down dis-inhibitory synaptic afferents, we show that error-modulated learning emerges naturally at the circuit level when recurrent inhibition explicitly influences Hebbian plasticity. The same learning rule accounts for experimentally observed plasticity in the absence of inhibition and performs comparably to back-propagation of error (BP) on several non-linearly separable benchmarks. Our findings bridge the gap between functional and experimentally observed plasticity rules and make concrete predictions on inhibitory modulation of excitatory plasticity.
</details>
<details>
<summary>摘要</summary>
neronal 网络如何进行信用分配仍然是系统神经科学中的中心未解问题。 Various 研究表明可能的解决方案是通过多层网络传递误差信号 backwards。 These 纯 fonctionally 动机化的模型假设了不同的 neuronal 腔室来表示本地误差信号，这些信号 Determine the sign of synaptic 弹性。 However, this explicit error 调变是与 Phenomenological 弹性模型不一致，这些模型中误差的 sign 主要取决于postsynaptic 活动。 Here we show how a plausible microcircuit 模型和 Hebbian 学习规则 Derived within an adaptive control theory 框架可以解决这个矛盾。 Assuming errors are encoded in top-down 异化 synaptic afferents, we show that error-modulated 学习 emerges naturally at the circuit level when recurrent inhibition explicitly influences Hebbian plasticity. The same 学习规则 accounts for experimentally observed plasticity in the absence of inhibition and performs comparably to back-propagation of error (BP) on several non-linearly separable benchmarks. Our findings bridge the gap between functional and experimentally observed plasticity rules and make concrete predictions on inhibitory modulation of excitatory plasticity.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Exploration-in-Continuous-time-Model-based-Reinforcement-Learning"><a href="#Efficient-Exploration-in-Continuous-time-Model-based-Reinforcement-Learning" class="headerlink" title="Efficient Exploration in Continuous-time Model-based Reinforcement Learning"></a>Efficient Exploration in Continuous-time Model-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19848">http://arxiv.org/abs/2310.19848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lenart Treven, Jonas Hübotter, Bhavya Sukhija, Florian Dörfler, Andreas Krause</li>
<li>for: 本研究的目的是提出一种基于模型的强化学习算法，用于解决维度时间的不确定性问题。</li>
<li>methods: 该算法使用非线性常微分方程（ODE）表示连续时间动力学，并使用准确的概率模型捕捉知识uncertainty。采用乐观原则进行探索。</li>
<li>results: 我们的 regret bound表明，在使用 Gaussian Processes（GP） Dynamics 和合适的 measurement selection strategy（MSS）时， regret 是下线的。此外，我们还提出了一种自适应、数据依存的实用MSS，可以在 fewer samples 下达到相同的性能。<details>
<summary>Abstract</summary>
Reinforcement learning algorithms typically consider discrete-time dynamics, even though the underlying systems are often continuous in time. In this paper, we introduce a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). We capture epistemic uncertainty using well-calibrated probabilistic models, and use the optimistic principle for exploration. Our regret bounds surface the importance of the measurement selection strategy(MSS), since in continuous time we not only must decide how to explore, but also when to observe the underlying system. Our analysis demonstrates that the regret is sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of MSS, such as equidistant sampling. Additionally, we propose an adaptive, data-dependent, practical MSS that, when combined with GP dynamics, also achieves sublinear regret with significantly fewer samples. We showcase the benefits of continuous-time modeling over its discrete-time counterpart, as well as our proposed adaptive MSS over standard baselines, on several applications.
</details>
<details>
<summary>摘要</summary>
常规强化学习算法通常考虑逻时动态，即使实际系统是连续时间的。在这篇论文中，我们介绍了一种基于非线性偏微分方程（ODE）的模型基于强化学习算法，用于捕捉连续时间动态中的知识uncertainty。我们使用了准确评估的概率模型，并采用了乐观原则来进行探索。我们的 regret  bound 表明，在连续时间中，选择采样策略（MSS）的重要性，因为我们不仅需要决定如何探索，还需要 WHEN 观察到系统。我们的分析表明，使用 Gaussian Processes（GP）来模型 ODEs 的通常选择的 MSS，例如等间隔采样，可以获得下降 regret。此外，我们还提出了一种自适应、数据依赖的实用 MSS，并与 GP 动力相结合，可以实现下降 regret，并且需要更少的样本。我们还证明了连续时间模型的优越性，以及我们的提议的自适应 MSS 的优越性，在多个应用中。
</details></li>
</ul>
<hr>
<h2 id="On-Feynman–Kac-training-of-partial-Bayesian-neural-networks"><a href="#On-Feynman–Kac-training-of-partial-Bayesian-neural-networks" class="headerlink" title="On Feynman–Kac training of partial Bayesian neural networks"></a>On Feynman–Kac training of partial Bayesian neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19608">http://arxiv.org/abs/2310.19608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Zhao, Sebastian Mair, Thomas B. Schön, Jens Sjölund</li>
<li>for: 这研究旨在提出一种高效的训练策略，以优化半 bayesian neural network（pBNN）的预测性能。</li>
<li>methods: 该策略基于 simulating Feynman–Kac 模型，并使用sequential Monte Carlo samplers来同时估计参数和秘密 posterior distribution。</li>
<li>results: 对各种 synthetic 和实际世界数据进行了评估，并显示了与现有方法相比，该训练策略可以提高预测性能。<details>
<summary>Abstract</summary>
Recently, partial Bayesian neural networks (pBNNs), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full Bayesian neural networks. However, pBNNs are often multi-modal in the latent-variable space and thus challenging to approximate with parametric models. To address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pBNN is formulated as simulating a Feynman--Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. We show on various synthetic and real-world datasets that our proposed training scheme outperforms the state of the art in terms of predictive performance.
</details>
<details>
<summary>摘要</summary>
最近，半 bayesian neural network (pBNN) 已经展示了与全 bayesian neural network 相当的性能，但它们在隐变量空间中存在多模性，因此难以使用参数化模型进行近似。为解决这个问题，我们提出了一种高效的采样学习策略，其中将 trains a pBNN 为 Feynman--Kac 模型的实现。然后，我们描述了可以同时估计参数和隐变量 posterior distribution 的变种Sequential Monte Carlo 抽样方法，并在计算成本下降到可行水平。我们在各种 sintetic 和实际数据上表明了我们的训练方案在预测性能方面超过了现状。
</details></li>
</ul>
<hr>
<h2 id="Deep-Kalman-Filters-Can-Filter"><a href="#Deep-Kalman-Filters-Can-Filter" class="headerlink" title="Deep Kalman Filters Can Filter"></a>Deep Kalman Filters Can Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19603">http://arxiv.org/abs/2310.19603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rishabhpahuja/Apple-Tracking">https://github.com/rishabhpahuja/Apple-Tracking</a></li>
<li>paper_authors: Blanka Hovart, Anastasis Kratsios, Yannick Limmer, Xuwei Yang</li>
<li>for: 这个论文是为了探讨深度卡尔曼筛（DKF）是一类基于神经网络模型的概率度量生成算法，它们可以在时间序列数据上生成高度概率的抽象模型。</li>
<li>methods: 这篇论文使用了一种名为“离散时间DKF”的新的数学模型，该模型可以在不间断时间上实现非马歇尔过程的条件概率分布。</li>
<li>results: 这篇论文的结果表明，使用离散时间DKF可以在离散时间上高度准确地估计非马歇尔过程的条件概率分布，并且这种估计的误差可以通过二者 Wasserstein 距离来量化。<details>
<summary>Abstract</summary>
Deep Kalman filters (DKFs) are a class of neural network models that generate Gaussian probability measures from sequential data. Though DKFs are inspired by the Kalman filter, they lack concrete theoretical ties to the stochastic filtering problem, thus limiting their applicability to areas where traditional model-based filters have been used, e.g.\ model calibration for bond and option prices in mathematical finance. We address this issue in the mathematical foundations of deep learning by exhibiting a class of continuous-time DKFs which can approximately implement the conditional law of a broad class of non-Markovian and conditionally Gaussian signal processes given noisy continuous-times measurements. Our approximation results hold uniformly over sufficiently regular compact subsets of paths, where the approximation error is quantified by the worst-case 2-Wasserstein distance computed uniformly over the given compact set of paths.
</details>
<details>
<summary>摘要</summary>
深度卡尔曼滤波器（DKF）是一类神经网络模型，可以从时序数据生成 Gaussian 概率度量。虽然 DKF 受 Kalman 滤波器的影响，但它们与传统的模型基 filtered 之间没有具体的理论关系，因此只能在传统的模型基 filtered 领域应用，如股票和选择价格的数学金融中进行模型调整。我们在数学深度学习的基础上解决这个问题，展示了一类连续时间DKF，可以约等 conditional law 的一类非马歇尔时间信号过程，基于噪声损失的连续时间测量。我们的近似结果在充分紧张的区域内保持，并且用最差2-沃asserstein距离来衡量近似错误的范围。
</details></li>
</ul>
<hr>
<h2 id="Operator-Learning-Enhanced-Physics-informed-Neural-Networks-for-Solving-Partial-Differential-Equations-Characterized-by-Sharp-Solutions"><a href="#Operator-Learning-Enhanced-Physics-informed-Neural-Networks-for-Solving-Partial-Differential-Equations-Characterized-by-Sharp-Solutions" class="headerlink" title="Operator Learning Enhanced Physics-informed Neural Networks for Solving Partial Differential Equations Characterized by Sharp Solutions"></a>Operator Learning Enhanced Physics-informed Neural Networks for Solving Partial Differential Equations Characterized by Sharp Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19590">http://arxiv.org/abs/2310.19590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Lin, Zhiping Mao, Zhicheng Wang, George Em Karniadakis</li>
<li>for: 解决具有锐解的partial differential equations (PDEs)的问题</li>
<li>methods: 使用Physics-informed Neural Networks (PINNs)和Deep Operator Network (DeepONet)</li>
<li>results: 能够成功地解决多种难以解决的问题，如非线性扩散-反应方程、布格尔方程和不可压缩 Navier-Stokes 方程，并且比vanilla PINN更具有抗过拟合和稳定性。<details>
<summary>Abstract</summary>
Physics-informed Neural Networks (PINNs) have been shown as a promising approach for solving both forward and inverse problems of partial differential equations (PDEs). Meanwhile, the neural operator approach, including methods such as Deep Operator Network (DeepONet) and Fourier neural operator (FNO), has been introduced and extensively employed in approximating solution of PDEs. Nevertheless, to solve problems consisting of sharp solutions poses a significant challenge when employing these two approaches. To address this issue, we propose in this work a novel framework termed Operator Learning Enhanced Physics-informed Neural Networks (OL-PINN). Initially, we utilize DeepONet to learn the solution operator for a set of smooth problems relevant to the PDEs characterized by sharp solutions. Subsequently, we integrate the pre-trained DeepONet with PINN to resolve the target sharp solution problem. We showcase the efficacy of OL-PINN by successfully addressing various problems, such as the nonlinear diffusion-reaction equation, the Burgers equation and the incompressible Navier-Stokes equation at high Reynolds number. Compared with the vanilla PINN, the proposed method requires only a small number of residual points to achieve a strong generalization capability. Moreover, it substantially enhances accuracy, while also ensuring a robust training process. Furthermore, OL-PINN inherits the advantage of PINN for solving inverse problems. To this end, we apply the OL-PINN approach for solving problems with only partial boundary conditions, which usually cannot be solved by the classical numerical methods, showing its capacity in solving ill-posed problems and consequently more complex inverse problems.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 已经被证明为解决部分数据方程式 (PDEs) 的前进和反射问题的有前途的方法。另一方面，神经操作方法，包括深度操作网络 (DeepONet) 和傅立叶神经操作 (FNO)，已经被引入并广泛使用以 aproximating PDEs 的解决方案。然而，当解决具有锋利解决方案的问题时，这两种方法会面临一定的挑战。为了解决这个问题，我们在这个工作中提出了一个新的框架，称为Operator Learning Enhanced Physics-informed Neural Networks (OL-PINN)。我们首先使用DeepONet来学习PDEs中相应的解决运算，然后与PINN相结合以解决目标的锋利解决方案问题。我们在多个问题上成功地使用OL-PINN，包括非线性扩散-反应方程、布格斯方程和不弹压流方程。相比于普通的PINN，我们的提案方法只需要一小部分的余类点来 achieve strong generalization capability，同时也提高了精度和稳定性。此外，OL-PINN继承了PINN的优点，可以解决反射问题，并且可以处理部分边界条件的问题，通常无法由古典数据方法解决。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Dynamics-over-Meshes-with-Gauge-Equivariant-Nonlinear-Message-Passing"><a href="#Modeling-Dynamics-over-Meshes-with-Gauge-Equivariant-Nonlinear-Message-Passing" class="headerlink" title="Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing"></a>Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19589">http://arxiv.org/abs/2310.19589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jypark0/hermes">https://github.com/jypark0/hermes</a></li>
<li>paper_authors: Jung Yeon Park, Lawson L. S. Wong, Robin Walters</li>
<li>for: 解决 Computer graphics 和生物physical systems 中数据 sobre non-Euclidean manifolds 问题</li>
<li>methods: 使用 gauge equivariant convolutional and attentional architectures on meshes</li>
<li>results: 提高了模型 surface PDEs 的性能，但是不同任务中的设计贸易offs 会导致不同的选择<details>
<summary>Abstract</summary>
Data over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, attentional, or message passing networks for different tasks; we investigate in which circumstances our message passing method provides the most benefit.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate text into Simplified ChineseData over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, attentional, or message passing networks for different tasks; we investigate in which circumstances our message passing method provides the most benefit.Translation notes:* "non-Euclidean" is translated as "非几何" (fēi jí hè)* "manifold" is translated as "流形" (liú xíng)* "partial differential equations" is translated as "部分偏微分方程" (bù zhāng tiān wēi dù fāng jiè)* "surface mesh" is translated as "表面网格" (biǎo miàn wǎng yǐ)* "gauge equivariant" is translated as " gauge 对称" (gāu yǐ xiàng)* "convolutional" is translated as "卷积" (juǎn shì)* "attentional" is translated as "注意" (zhù yì)* "message passing" is translated as "消息传递" (xiāo wèn chuán zhù)Please note that Simplified Chinese is used in this translation, which may differ from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Model-Uncertainty-based-Active-Learning-on-Tabular-Data-using-Boosted-Trees"><a href="#Model-Uncertainty-based-Active-Learning-on-Tabular-Data-using-Boosted-Trees" class="headerlink" title="Model Uncertainty based Active Learning on Tabular Data using Boosted Trees"></a>Model Uncertainty based Active Learning on Tabular Data using Boosted Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19573">http://arxiv.org/abs/2310.19573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharath M Shankaranarayana</li>
<li>for: This paper focuses on active learning for tabular data using boosted trees, with a particular emphasis on measuring model uncertainty and leveraging it for efficient label acquisition.</li>
<li>methods: The paper proposes an uncertainty-based sampling strategy for active learning, using entropy as a measure of model uncertainty. Additionally, the authors propose two novel cost-effective active learning methods for regression and classification tasks.</li>
<li>results: The authors evaluate the proposed methods on several benchmark datasets and show that their uncertainty-based sampling strategy and cost-effective active learning methods achieve better performance compared to existing methods.<details>
<summary>Abstract</summary>
Supervised machine learning relies on the availability of good labelled data for model training. Labelled data is acquired by human annotation, which is a cumbersome and costly process, often requiring subject matter experts. Active learning is a sub-field of machine learning which helps in obtaining the labelled data efficiently by selecting the most valuable data instances for model training and querying the labels only for those instances from the human annotator. Recently, a lot of research has been done in the field of active learning, especially for deep neural network based models. Although deep learning shines when dealing with image\textual\multimodal data, gradient boosting methods still tend to achieve much better results on tabular data. In this work, we explore active learning for tabular data using boosted trees. Uncertainty based sampling in active learning is the most commonly used querying strategy, wherein the labels of those instances are sequentially queried for which the current model prediction is maximally uncertain. Entropy is often the choice for measuring uncertainty. However, entropy is not exactly a measure of model uncertainty. Although there has been a lot of work in deep learning for measuring model uncertainty and employing it in active learning, it is yet to be explored for non-neural network models. To this end, we explore the effectiveness of boosted trees based model uncertainty methods in active learning. Leveraging this model uncertainty, we propose an uncertainty based sampling in active learning for regression tasks on tabular data. Additionally, we also propose a novel cost-effective active learning method for regression tasks along with an improved cost-effective active learning method for classification tasks.
</details>
<details>
<summary>摘要</summary>
超visired机器学习 rely on the availability of good labeled data for model training.  Labelled data is acquired by human annotation, which is a cumbersome and costly process, often requiring subject matter experts. Active learning is a sub-field of machine learning which helps in obtaining the labeled data efficiently by selecting the most valuable data instances for model training and querying the labels only for those instances from the human annotator. Recently, a lot of research has been done in the field of active learning, especially for deep neural network based models. Although deep learning shines when dealing with image\textual\multimodal data, gradient boosting methods still tend to achieve much better results on tabular data. In this work, we explore active learning for tabular data using boosted trees. Uncertainty based sampling in active learning is the most commonly used querying strategy, wherein the labels of those instances are sequentially queried for which the current model prediction is maximally uncertain. Entropy is often the choice for measuring uncertainty. However, entropy is not exactly a measure of model uncertainty. Although there has been a lot of work in deep learning for measuring model uncertainty and employing it in active learning, it is yet to be explored for non-neural network models. To this end, we explore the effectiveness of boosted trees based model uncertainty methods in active learning. Leveraging this model uncertainty, we propose an uncertainty based sampling in active learning for regression tasks on tabular data. Additionally, we also propose a novel cost-effective active learning method for regression tasks along with an improved cost-effective active learning method for classification tasks.
</details></li>
</ul>
<hr>
<h2 id="DataZoo-Streamlining-Traffic-Classification-Experiments"><a href="#DataZoo-Streamlining-Traffic-Classification-Experiments" class="headerlink" title="DataZoo: Streamlining Traffic Classification Experiments"></a>DataZoo: Streamlining Traffic Classification Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19568">http://arxiv.org/abs/2310.19568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Luxemburk, Karel Hynek</li>
<li>for: 这篇论文主要是为了解决网络流量分类领域缺乏标准benchmark数据集和支持工具的问题。</li>
<li>methods: 该论文提出了一个名为DataZoo的工具集，用于加速网络流量分类领域的开发。DataZoo包括了标准化API访问三个大型数据集（CESNET-QUIC22、CESNET-TLS22和CESNET-TLS-Year22），以及feature scaling和realistic dataset partitioning方法。</li>
<li>results: 该论文通过DataZoo工具集，使得网络流量分类领域的开发更加容易、更加准确，同时也提高了result的 reproduceability和cross-comparison的能力。<details>
<summary>Abstract</summary>
The machine learning communities, such as those around computer vision or natural language processing, have developed numerous supportive tools and benchmark datasets to accelerate the development. In contrast, the network traffic classification field lacks standard benchmark datasets for most tasks, and the available supportive software is rather limited in scope. This paper aims to address the gap and introduces DataZoo, a toolset designed to streamline dataset management in network traffic classification and to reduce the space for potential mistakes in the evaluation setup. DataZoo provides a standardized API for accessing three extensive datasets -- CESNET-QUIC22, CESNET-TLS22, and CESNET-TLS-Year22. Moreover, it includes methods for feature scaling and realistic dataset partitioning, taking into consideration temporal and service-related factors. The DataZoo toolset simplifies the creation of realistic evaluation scenarios, making it easier to cross-compare classification methods and reproduce results.
</details>
<details>
<summary>摘要</summary>
machine learning 社区，如计算机视觉或自然语言处理等，已经开发出了许多支持工具和标准评估数据集，以加速开发。而网络流量分类领域却缺乏大多数任务的标准评估数据集，可用的支持软件也很有限。这篇论文想要填补这个空白，并引入数据 zoo，一套用于协调 dataset 管理的工具集。数据 zoo 提供了访问三个广泛的数据集——CESNET-QUIC22、CESNET-TLS22 和 CESNET-TLS-Year22 的标准 API。此外，它还包括特征整形和现实 dataset 分区方法，考虑了时间和服务相关因素。数据 zoo 工具集可以简化实际评估场景的创建，使得cross- comparing 分类方法和重复结果更加容易。
</details></li>
</ul>
<hr>
<h2 id="Non-parametric-regression-for-robot-learning-on-manifolds"><a href="#Non-parametric-regression-for-robot-learning-on-manifolds" class="headerlink" title="Non-parametric regression for robot learning on manifolds"></a>Non-parametric regression for robot learning on manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19561">http://arxiv.org/abs/2310.19561</a></li>
<li>repo_url: None</li>
<li>paper_authors: P. C. Lopez-Custodio, K. Bharath, A. Kucukyilmaz, S. P. Preston</li>
<li>for: 本研究的目的是提出一种在抽象空间上进行回归的方法，以便在机器人学习中处理不拘束的抽象数据。</li>
<li>methods: 本方法基于一种叫做“内在”的方法，即在抽象空间上直接使用一种适当的概率分布，并通过一种非 Parametric 的方法来估算这个概率分布的参数。</li>
<li>results: 实验结果表明，使用本方法可以在机器人学习中提高预测精度，并且比使用投影基本法更好。<details>
<summary>Abstract</summary>
Many of the tools available for robot learning were designed for Euclidean data. However, many applications in robotics involve manifold-valued data. A common example is orientation; this can be represented as a 3-by-3 rotation matrix or a quaternion, the spaces of which are non-Euclidean manifolds. In robot learning, manifold-valued data are often handled by relating the manifold to a suitable Euclidean space, either by embedding the manifold or by projecting the data onto one or several tangent spaces. These approaches can result in poor predictive accuracy, and convoluted algorithms. In this paper, we propose an "intrinsic" approach to regression that works directly within the manifold. It involves taking a suitable probability distribution on the manifold, letting its parameter be a function of a predictor variable, such as time, then estimating that function non-parametrically via a "local likelihood" method that incorporates a kernel. We name the method kernelised likelihood estimation. The approach is conceptually simple, and generally applicable to different manifolds. We implement it with three different types of manifold-valued data that commonly appear in robotics applications. The results of these experiments show better predictive accuracy than projection-based algorithms.
</details>
<details>
<summary>摘要</summary>
许多机器人学习工具是为欧几何数据设计。然而，许多机器人应用中的数据是拥有非欧几何结构的。例如，Orientation可以表示为3x3旋转矩阵或 quarternion，这些空间都是非欧几何 manifold。在机器人学习中，把拥有非欧几何结构的数据处理为Euclidean space 是一个常见的做法。这些方法可能会导致预测精度不佳和算法复杂。在这篇论文中，我们提出了一种“内在”的回归方法，可以直接在拥有非欧几何结构的 manifold 上进行。这种方法是基于一个适当的概率分布在拥有非欧几何结构的 manifold 上，使其参数为预测变量，例如时间。然后使用一种“本地概率”方法来估计这个函数，这种方法包含一个核函数。我们称之为核化概率估计。这种方法概念简单，通用于不同的拥有非欧几何结构的 manifold。我们在三种常见的机器人学习中使用了不同类型的拥有非欧几何结构的数据进行实验，实验结果表明这种方法的预测精度比 projection-based 算法更高。
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-Federated-Primal-dual-Learning-for-Non-convex-and-Non-smooth-Problems-with-Model-Sparsification"><a href="#Privacy-preserving-Federated-Primal-dual-Learning-for-Non-convex-and-Non-smooth-Problems-with-Model-Sparsification" class="headerlink" title="Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification"></a>Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19558">http://arxiv.org/abs/2310.19558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwei Li, Chien-Wei Huang, Shuai Wang, Chong-Yung Chi, Tony Q. S. Quek</li>
<li>for: 这篇论文关注了 Federated Learning (FL) 领域中的一类非斜方程和非凸函数问题，这些问题在 FL 应用中很普遍，但却具有复杂的非斜性和非凸性特性，同时需要考虑通信效率和隐私保证。</li>
<li>methods: 这篇论文提出了一个基于 primal-dual 算法的 Federated Learning 方法，其特点是具有双向模型简化，这样可以实现更好的通信效率和隐私保证。此外，论文还应用了对称隐私技术来保证隐私。</li>
<li>results: 实验结果显示，提出的 Federated Learning 方法在实际数据上具有明显的优势，与一些现有的 FL 算法相比，其性能明显更高。此外，论文还 validate了所有的分析结果和性能特性。<details>
<summary>Abstract</summary>
Federated learning (FL) has been recognized as a rapidly growing research area, where the model is trained over massively distributed clients under the orchestration of a parameter server (PS) without sharing clients' data. This paper delves into a class of federated problems characterized by non-convex and non-smooth loss functions, that are prevalent in FL applications but challenging to handle due to their intricate non-convexity and non-smoothness nature and the conflicting requirements on communication efficiency and privacy protection. In this paper, we propose a novel federated primal-dual algorithm with bidirectional model sparsification tailored for non-convex and non-smooth FL problems, and differential privacy is applied for strong privacy guarantee. Its unique insightful properties and some privacy and convergence analyses are also presented for the FL algorithm design guidelines. Extensive experiments on real-world data are conducted to demonstrate the effectiveness of the proposed algorithm and much superior performance than some state-of-the-art FL algorithms, together with the validation of all the analytical results and properties.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Approximation-Theory-Computing-and-Deep-Learning-on-the-Wasserstein-Space"><a href="#Approximation-Theory-Computing-and-Deep-Learning-on-the-Wasserstein-Space" class="headerlink" title="Approximation Theory, Computing, and Deep Learning on the Wasserstein Space"></a>Approximation Theory, Computing, and Deep Learning on the Wasserstein Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19548">http://arxiv.org/abs/2310.19548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimo Fornasier, Pascal Heid, Giacomo Enrico Sodini</li>
<li>for: 本研究探讨了使用机器学习方法数学approximation Sobolev-smooth函数定义在概率空间中的问题。</li>
<li>methods: 本研究采用了三种机器学习方法来定义函数approximants：1. 解决一个有限个优点运输问题并计算相应的 Wasserstein potentials。2. 使用Wasserstein Sobolev空间中的empirical risk minimizationwith Tikhonov regularization。3. 通过锚点形式来表示Tikhonov函数的弱形式Euler-Lagrange方程。</li>
<li>results: 本研究提供了explicit和量化的bounds on generalization errors for each of these solutions。在证明过程中，我们利用了度量 Sobolev空间的理论和优点运输技术、variational calculus和大偏差 bounds。在数值实现中，我们使用了适应设计的神经网络作为基函数。这些神经网络在训练后可以快速地评估。因此，我们的构建解决方案可以在等级准确性下提高评估速度，超过当前状态方法的几个数量级。<details>
<summary>Abstract</summary>
The challenge of approximating functions in infinite-dimensional spaces from finite samples is widely regarded as formidable. In this study, we delve into the challenging problem of the numerical approximation of Sobolev-smooth functions defined on probability spaces. Our particular focus centers on the Wasserstein distance function, which serves as a relevant example. In contrast to the existing body of literature focused on approximating efficiently pointwise evaluations, we chart a new course to define functional approximants by adopting three machine learning-based approaches: 1. Solving a finite number of optimal transport problems and computing the corresponding Wasserstein potentials. 2. Employing empirical risk minimization with Tikhonov regularization in Wasserstein Sobolev spaces. 3. Addressing the problem through the saddle point formulation that characterizes the weak form of the Tikhonov functional's Euler-Lagrange equation. As a theoretical contribution, we furnish explicit and quantitative bounds on generalization errors for each of these solutions. In the proofs, we leverage the theory of metric Sobolev spaces and we combine it with techniques of optimal transport, variational calculus, and large deviation bounds. In our numerical implementation, we harness appropriately designed neural networks to serve as basis functions. These networks undergo training using diverse methodologies. This approach allows us to obtain approximating functions that can be rapidly evaluated after training. Consequently, our constructive solutions significantly enhance at equal accuracy the evaluation speed, surpassing that of state-of-the-art methods by several orders of magnitude.
</details>
<details>
<summary>摘要</summary>
“函数approximation在无穷dimensional空间中从finite samples中进行approximation是广泛认为是困难的挑战。在这种研究中，我们对 Sobolev-smooth 函数定义在概率空间上进行数值 aproximation 进行了研究。我们的特定关注点在于 Wasserstein 距离函数，它是一个有 relevance 的例子。相比之前的文献中关注点在于快速地计算点 wise 评估，我们采取了三种机器学习基于方法：1. 解决 finite 个优质 transport 问题，并计算相应的 Wasserstein 潜 potential。2. 使用 Tikhonov 补做 regularization 在 Wasserstein Sobolev 空间中进行 Empirical Risk Minimization。3. 通过 saddle point 表示法，解决这个问题。在证明中，我们利用了 metric Sobolev 空间理论和optimal transport 技术，并将其与variational calculus 和大数据准则绑定在一起。在数值实现中，我们使用适应设计的神经网络作为基函数。这些神经网络进行训练，并通过不同的方法进行训练。这种方法使得我们可以获得高度精度的函数 aproximation，并且可以在训练后快速地计算这些函数。因此，我们的构建解决方案可以在同等精度下大幅提高评估速度，超过现有方法几个数量级。”
</details></li>
</ul>
<hr>
<h2 id="On-consequences-of-finetuning-on-data-with-highly-discriminative-features"><a href="#On-consequences-of-finetuning-on-data-with-highly-discriminative-features" class="headerlink" title="On consequences of finetuning on data with highly discriminative features"></a>On consequences of finetuning on data with highly discriminative features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19537">http://arxiv.org/abs/2310.19537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wojciech Masarczyk, Tomasz Trzciński, Mateusz Ostaszewski</li>
<li>for: 这篇论文主要是为了探讨在传输学习时， neural network 是否会忽略先前学习的特征，以及这种现象对网络性能和内部表示的影响。</li>
<li>methods: 作者使用了多种方法来分析传输学习中的特征衰退现象，包括网络性能测试、特征重要性分析和内部表示分析等。</li>
<li>results: 研究发现，在传输学习中，网络倾向于优先学习基本数据模式，导致已经学习的特征被忽略，从而影响网络的性能和内部表示。<details>
<summary>Abstract</summary>
In the era of transfer learning, training neural networks from scratch is becoming obsolete. Transfer learning leverages prior knowledge for new tasks, conserving computational resources. While its advantages are well-documented, we uncover a notable drawback: networks tend to prioritize basic data patterns, forsaking valuable pre-learned features. We term this behavior "feature erosion" and analyze its impact on network performance and internal representations.
</details>
<details>
<summary>摘要</summary>
在转移学习时代，从头开始训练神经网络已成为过时。转移学习利用了先前学习的知识，以便应用于新任务，减少计算资源。虽然它的优点已经很好地记录下来，但我们发现了一个明显的缺点：神经网络往往强调基本数据模式，抛弃有价值的先前学习特征。我们称这种行为为“特征蚀减”，并分析其对网络性能和内部表示的影响。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Batch-Inverse-Reinforcement-Learning-Learn-to-Reward-from-Imperfect-Demonstration-for-Interactive-Recommendation"><a href="#Adversarial-Batch-Inverse-Reinforcement-Learning-Learn-to-Reward-from-Imperfect-Demonstration-for-Interactive-Recommendation" class="headerlink" title="Adversarial Batch Inverse Reinforcement Learning: Learn to Reward from Imperfect Demonstration for Interactive Recommendation"></a>Adversarial Batch Inverse Reinforcement Learning: Learn to Reward from Imperfect Demonstration for Interactive Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19536">http://arxiv.org/abs/2310.19536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Liu, Xinyan Su, Zeyu He, Xiangyu Zhao, Jun Li</li>
<li>for: 本研究目标是学习奖励（LTR），即在 reinforcement learning 中学习用户奖励。</li>
<li>methods: 我们提出了一种批量反 inverse reinforcement learning 方法，利用折扣站点分布 corrections 结合 LTR 和 recommender agent 评估。我们还利用 Bellman 变换和 KL 正则化来保持 consecutive policy 更新的 Compositional requirement。</li>
<li>results: 我们在两个实际数据集上进行了实验，结果显示，我们的方法可以相对提高效iveness（2.3%）和效率（11.53%）。<details>
<summary>Abstract</summary>
Rewards serve as a measure of user satisfaction and act as a limiting factor in interactive recommender systems. In this research, we focus on the problem of learning to reward (LTR), which is fundamental to reinforcement learning. Previous approaches either introduce additional procedures for learning to reward, thereby increasing the complexity of optimization, or assume that user-agent interactions provide perfect demonstrations, which is not feasible in practice. Ideally, we aim to employ a unified approach that optimizes both the reward and policy using compositional demonstrations. However, this requirement presents a challenge since rewards inherently quantify user feedback on-policy, while recommender agents approximate off-policy future cumulative valuation. To tackle this challenge, we propose a novel batch inverse reinforcement learning paradigm that achieves the desired properties. Our method utilizes discounted stationary distribution correction to combine LTR and recommender agent evaluation. To fulfill the compositional requirement, we incorporate the concept of pessimism through conservation. Specifically, we modify the vanilla correction using Bellman transformation and enforce KL regularization to constrain consecutive policy updates. We use two real-world datasets which represent two compositional coverage to conduct empirical studies, the results also show that the proposed method relatively improves both effectiveness (2.3\%) and efficiency (11.53\%)
</details>
<details>
<summary>摘要</summary>
奖励 serve as a measure of user satisfaction and act as a limiting factor in interactive recommender systems. 在这个研究中，我们关注学习奖励（LTR）问题，这是基本的回归学习问题。先前的方法可以是引入额外的学习奖励程序，从而增加优化的复杂度，或者假设用户-代理交互提供完美的示例，这不是实际情况。理想地，我们想使用一种统一的方法，同时优化奖励和策略使用compositional示例。但这会提出一个挑战，因为奖励自然地量化用户反馈on-policy，而推荐代理 approximates off-policy未来累累值。为解决这个挑战，我们提出了一种新的批量反向学习 paradigma，实现了所需的属性。我们的方法使用折扣站ary分布 corrected来combine LTR和推荐代理评价。为了满足compositional要求，我们加入了保守性的概念，specifically，我们修改了vanilla correction，并在 Bellman 变换中强制执行KL regularization，以制约 consecutive policy 更新。我们使用了两个实际数据集，代表了两种compositional coverage，进行了empirical研究，结果也表明，我们提出的方法相对提高了效果（2.3%）和效率（11.53%）。
</details></li>
</ul>
<hr>
<h2 id="Decoupled-Actor-Critic"><a href="#Decoupled-Actor-Critic" class="headerlink" title="Decoupled Actor-Critic"></a>Decoupled Actor-Critic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19527">http://arxiv.org/abs/2310.19527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Nauman, Marek Cygan</li>
<li>for: 本研究旨在解决actor-critic方法中的两个问题：首先，批评家倾向于过度估计，需要从保守策略优化的下界Q值中采样 temporal-difference 目标。其次，已知结果表明，面对不确定性时，乐见的策略会带来更低的 regret 水平。</li>
<li>methods: 我们提出了一种叫做Decoupled Actor-Critic（DAC）的离散actor-critic方法，该方法通过梯度反推学习两个不同的actor：一个保守的actor用于 temporal-difference 学习，另一个乐见的actor用于探索。</li>
<li>results: 我们在DeepMind Control任务中进行了低和高回放率 régime的测试，并对多个设计选择进行了抹除。结果显示，Despite minimal computational overhead，DAC可以在涤力学任务中 achieve state-of-the-art performance和sample efficiency。<details>
<summary>Abstract</summary>
Actor-Critic methods are in a stalemate of two seemingly irreconcilable problems. Firstly, critic proneness towards overestimation requires sampling temporal-difference targets from a conservative policy optimized using lower-bound Q-values. Secondly, well-known results show that policies that are optimistic in the face of uncertainty yield lower regret levels. To remedy this dichotomy, we propose Decoupled Actor-Critic (DAC). DAC is an off-policy algorithm that learns two distinct actors by gradient backpropagation: a conservative actor used for temporal-difference learning and an optimistic actor used for exploration. We test DAC on DeepMind Control tasks in low and high replay ratio regimes and ablate multiple design choices. Despite minimal computational overhead, DAC achieves state-of-the-art performance and sample efficiency on locomotion tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generator-Identification-for-Linear-SDEs-with-Additive-and-Multiplicative-Noise"><a href="#Generator-Identification-for-Linear-SDEs-with-Additive-and-Multiplicative-Noise" class="headerlink" title="Generator Identification for Linear SDEs with Additive and Multiplicative Noise"></a>Generator Identification for Linear SDEs with Additive and Multiplicative Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19491">http://arxiv.org/abs/2310.19491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wang, Xi Geng, Wei Huang, Biwei Huang, Mingming Gong</li>
<li>for: 这个论文是为了研究如何从解析解的分布来确定生成器的线性随机振荡方程（SDE）的生成器的condition。</li>
<li>methods: 这篇论文使用了线性SDE的分布来确定生成器的方法。</li>
<li>results: 这篇论文提出了线性SDE的生成器可以通过解析解的分布来确定的必要和 suficient condition，并且提供了这些condition的几何解释。<details>
<summary>Abstract</summary>
In this paper, we present conditions for identifying the generator of a linear stochastic differential equation (SDE) from the distribution of its solution process with a given fixed initial state. These identifiability conditions are crucial in causal inference using linear SDEs as they enable the identification of the post-intervention distributions from its observational distribution. Specifically, we derive a sufficient and necessary condition for identifying the generator of linear SDEs with additive noise, as well as a sufficient condition for identifying the generator of linear SDEs with multiplicative noise. We show that the conditions derived for both types of SDEs are generic. Moreover, we offer geometric interpretations of the derived identifiability conditions to enhance their understanding. To validate our theoretical results, we perform a series of simulations, which support and substantiate the established findings.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了确定Linear Stochastic Differential Equation（SDE）生成器的条件。这些条件是 causal inference 中 linear SDE 的关键因素，它们允许我们从观察分布中确定post-intervention 分布。我们得到了 linear SDE 添加噪声的必要和 suficient condition，以及 linear SDE 乘法噪声的 sufficient condition。我们发现这些条件对于 both types of SDEs 是通用的。此外，我们还提供了这些条件的 geometric interpretation，以便更好地理解。为验证我们的理论结论，我们进行了一系列的 simulations，它们支持和证实了我们的结论。Here's the translation of the text in Traditional Chinese:在这篇论文中，我们提出了确定Linear Stochastic Differential Equation（SDE）生成器的条件。这些条件是 causal inference 中 linear SDE 的关键因素，它们允许我们从观察分布中确定post-intervention 分布。我们得到了 linear SDE 添加噪声的必要和 suficient condition，以及 linear SDE 乘法噪声的 sufficient condition。我们发现这些条件对于 both types of SDEs 是通用的。此外，我们还提供了这些条件的 geometric interpretation，以便更好地理解。为验证我们的理论结论，我们进行了一系列的 simulations，它们支持和证实了我们的结论。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Meta-Learning-Based-KKL-Observer-Design-for-Nonlinear-Dynamical-Systems"><a href="#Adaptive-Meta-Learning-Based-KKL-Observer-Design-for-Nonlinear-Dynamical-Systems" class="headerlink" title="Adaptive Meta-Learning-Based KKL Observer Design for Nonlinear Dynamical Systems"></a>Adaptive Meta-Learning-Based KKL Observer Design for Nonlinear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19489">http://arxiv.org/abs/2310.19489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Trommer, Halil Yigit Oksuz</li>
<li>for: 这篇论文是关于非线性系统观察器设计的研究，具体来说是通过meta-学习来优化观察器的设计，以便更好地适应非线性系统的不同状况和特性。</li>
<li>methods: 该论文使用了人工神经网络来 aproximate非线性变换Map，并通过一种基于学习的方法来设计观察器。</li>
<li>results: 实验结果表明，该方法可以高度准确地估计非线性系统的状态，并且具有良好的泛化能力、鲁棒性和适应性。<details>
<summary>Abstract</summary>
The theory of Kazantzis-Kravaris/Luenberger (KKL) observer design introduces a methodology that uses a nonlinear transformation map and its left inverse to estimate the state of a nonlinear system through the introduction of a linear observer state space. Data-driven approaches using artificial neural networks have demonstrated the ability to accurately approximate these transformation maps. This paper presents a novel approach to observer design for nonlinear dynamical systems through meta-learning, a concept in machine learning that aims to optimize learning models for fast adaptation to a distribution of tasks through an improved focus on the intrinsic properties of the underlying learning problem. We introduce a framework that leverages information from measurements of the system output to design a learning-based KKL observer capable of online adaptation to a variety of system conditions and attributes. To validate the effectiveness of our approach, we present comprehensive experimental results for the estimation of nonlinear system states with varying initial conditions and internal parameters, demonstrating high accuracy, generalization capability, and robustness against noise.
</details>
<details>
<summary>摘要</summary>
《kazantzis-kravaris/Luenberger（KKL）观察器设计理论》引入了一种使用非线性变换Map和其左逆函数来估计非线性系统的状态的方法ологи。使用人工神经网络进行数据驱动的方法已经证明了高度准确地 aproximate这些变换Map。本文提出了一种基于机器学习的观察器设计方法，通过meta-学习来优化学习模型，以便快速适应 distribuition of tasks 中的学习问题。我们提出了一种基于测量系统输出信息的框架，用于设计一种可在线适应系统条件和特性的学习型KKL观察器。为验证我们的方法的有效性，我们提供了广泛的实验结果，包括非线性系统的初始条件和内部参数变化的情况， demonstrating高精度、泛化能力和对噪声的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Grokking-Tickets-Lottery-Tickets-Accelerate-Grokking"><a href="#Grokking-Tickets-Lottery-Tickets-Accelerate-Grokking" class="headerlink" title="Grokking Tickets: Lottery Tickets Accelerate Grokking"></a>Grokking Tickets: Lottery Tickets Accelerate Grokking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19470">http://arxiv.org/abs/2310.19470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gouki510/grokking-tickets">https://github.com/gouki510/grokking-tickets</a></li>
<li>paper_authors: Gouki Minegishi, Yusuke Iwasawa, Yutaka Matsuo</li>
<li>for: 本研究旨在探讨神经网络 generale 的机制，即从 Lottery Ticket Hypothesis 出发，找到可以快速泛化的 ‘’Grokking Tickets’’（好 sparse subnetworks），并证明这些子网络在不同的配置下（MLP 和 Transformer，以及数学和图像分类任务）可以快速泛化。</li>
<li>methods: 本研究使用 ‘’Grokking Tickets’’ 来描述从记忆化解决方案转移到泛化解决方案的过渡阶段。 ‘’Grokking Tickets’’ 通过 magnitude pruning  после完美泛化而被识别出来。</li>
<li>results: 研究发现，使用 ‘’Grokking Tickets’’ 可以大幅加速泛化，并且这种加速不仅在不同的配置下得到证明，而且比 dense network 更快。此外，研究还发现，在适当的剔除率下，泛化可以在没有权重衰减的情况下实现。<details>
<summary>Abstract</summary>
Grokking is one of the most surprising puzzles in neural network generalization: a network first reaches a memorization solution with perfect training accuracy and poor generalization, but with further training, it reaches a perfectly generalized solution. We aim to analyze the mechanism of grokking from the lottery ticket hypothesis, identifying the process to find the lottery tickets (good sparse subnetworks) as the key to describing the transitional phase between memorization and generalization. We refer to these subnetworks as ''Grokking tickets'', which is identified via magnitude pruning after perfect generalization. First, using ''Grokking tickets'', we show that the lottery tickets drastically accelerate grokking compared to the dense networks on various configurations (MLP and Transformer, and an arithmetic and image classification tasks). Additionally, to verify that ''Grokking ticket'' are a more critical factor than weight norms, we compared the ''good'' subnetworks with a dense network having the same L1 and L2 norms. Results show that the subnetworks generalize faster than the controlled dense model. In further investigations, we discovered that at an appropriate pruning rate, grokking can be achieved even without weight decay. We also show that speedup does not happen when using tickets identified at the memorization solution or transition between memorization and generalization or when pruning networks at the initialization (Random pruning, Grasp, SNIP, and Synflow). The results indicate that the weight norm of network parameters is not enough to explain the process of grokking, but the importance of finding good subnetworks to describe the transition from memorization to generalization. The implementation code can be accessed via this link: \url{https://github.com/gouki510/Grokking-Tickets}.
</details>
<details>
<summary>摘要</summary>
干货猪肉是神经网络通用化的一个最有趣的拟合问题：一个网络在完美的训练精度下达到了记忆解决方案，但在进一步训练后它能够达到完美的总结解决方案。我们想要分析干货猪肉机制从抽奖签 hypothesis开始，并识别找到好的干货猪肉（好的稀疏网络）作为总结和拟合之间的关键过渡阶段。我们称这些签证为“干货猪肉签”，通过干货猪肉的大小减少来识别它们。我们首先使用干货猪肉签显示干货猪肉可以快速加速拟合，并在多种配置（MLP和Transformer）和数学和图像分类任务中进行了比较。此外，我们还比较了与稀疏网络相同L1和L2 нор的 dense网络，结果显示干货猪肉总结 faster than控制的稀疏模型。在进一步的调查中，我们发现在适当的减少率下，拟合可以在没有权重衰减的情况下得到。此外，我们还发现速度不会发生在使用记忆解决方案或在转移到总结和拟合之间的过渡阶段，或在初始化（随机减少、抓取、SNIP和Synflow）中减少网络参数。结果表明网络参数的重量 нор不够用来解释干货猪肉的过程，但是找到好的干货猪肉可以描述总结和拟合之间的过渡阶段。相关实现代码可以通过以下链接获取：\url{https://github.com/gouki510/Grokking-Tickets}。
</details></li>
</ul>
<hr>
<h2 id="Regret-Minimization-Algorithms-for-Multi-Agent-Cooperative-Learning-Systems"><a href="#Regret-Minimization-Algorithms-for-Multi-Agent-Cooperative-Learning-Systems" class="headerlink" title="Regret-Minimization Algorithms for Multi-Agent Cooperative Learning Systems"></a>Regret-Minimization Algorithms for Multi-Agent Cooperative Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19468">http://arxiv.org/abs/2310.19468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Yi</li>
<li>for: 这个论文主要针对的是多智能体系统（MACL）的设计和分析，用于解决决策问题。</li>
<li>methods: 这个论文使用了多智能体系统的各种学习算法，包括优化算法和搜索算法，以实现最佳决策。</li>
<li>results: 这个论文提出了一系列的 regret Lower bound，用于衡量多智能体系统在决策问题上的性能。这些 regret Lower bound 取决于通信网络的连接度和延迟时间，从而为 MACL 系统的设计提供了有用的指导。<details>
<summary>Abstract</summary>
A Multi-Agent Cooperative Learning (MACL) system is an artificial intelligence (AI) system where multiple learning agents work together to complete a common task. Recent empirical success of MACL systems in various domains (e.g. traffic control, cloud computing, robotics) has sparked active research into the design and analysis of MACL systems for sequential decision making problems. One important metric of the learning algorithm for decision making problems is its regret, i.e. the difference between the highest achievable reward and the actual reward that the algorithm gains. The design and development of a MACL system with low-regret learning algorithms can create huge economic values. In this thesis, I analyze MACL systems for different sequential decision making problems. Concretely, the Chapter 3 and 4 investigate the cooperative multi-agent multi-armed bandit problems, with full-information or bandit feedback, in which multiple learning agents can exchange their information through a communication network and the agents can only observe the rewards of the actions they choose. Chapter 5 considers the communication-regret trade-off for online convex optimization in the distributed setting. Chapter 6 discusses how to form high-productive teams for agents based on their unknown but fixed types using adaptive incremental matchings. For the above problems, I present the regret lower bounds for feasible learning algorithms and provide the efficient algorithms to achieve this bound. The regret bounds I present in Chapter 3, 4 and 5 quantify how the regret depends on the connectivity of the communication network and the communication delay, thus giving useful guidance on design of the communication protocol in MACL systems
</details>
<details>
<summary>摘要</summary>
《多智能合作学习（MACL）系统是一种人工智能（AI）系统，其中多个学习代理共同完成共同任务。 recent empirical success of MACL systems in various domains（例如交通管理、云计算、机器人等）has sparked active research into the design and analysis of MACL systems for sequential decision-making problems. One important metric of the learning algorithm for decision-making problems is its regret，i.e. the difference between the highest achievable reward and the actual reward that the algorithm gains. The design and development of a MACL system with low-regret learning algorithms can create huge economic values. In this thesis，I analyze MACL systems for different sequential decision-making problems. Specifically，Chapter 3 and 4 investigate the cooperative multi-agent multi-armed bandit problems，with full-information or bandit feedback，in which multiple learning agents can exchange their information through a communication network and the agents can only observe the rewards of the actions they choose. Chapter 5 considers the communication-regret trade-off for online convex optimization in the distributed setting. Chapter 6 discusses how to form high-productive teams for agents based on their unknown but fixed types using adaptive incremental matchings. For the above problems，I present the regret lower bounds for feasible learning algorithms and provide the efficient algorithms to achieve this bound. The regret bounds I present in Chapter 3, 4, and 5 quantify how the regret depends on the connectivity of the communication network and the communication delay，thus giving useful guidance on design of the communication protocol in MACL systems.》Note that Simplified Chinese is a written form of Chinese that uses simpler characters and grammar than Traditional Chinese. The translation is done in a way that is consistent with the conventions of Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="MMM-and-MMMSynth-Clustering-of-heterogeneous-tabular-data-and-synthetic-data-generation"><a href="#MMM-and-MMMSynth-Clustering-of-heterogeneous-tabular-data-and-synthetic-data-generation" class="headerlink" title="MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation"></a>MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19454">http://arxiv.org/abs/2310.19454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chandrani Kumari, Rahul Siddharthan</li>
<li>for: 这两个任务是关于不同类型数据表的 clustering 和生成Synthetic数据的新算法。</li>
<li>methods: 这两个任务使用的方法是基于EM的聚类算法和深度学习生成Synthetic数据。</li>
<li>results: 这两个任务的结果是一个高性能的聚类算法和一种可以生成高质量Synthetic数据的算法。<details>
<summary>Abstract</summary>
We provide new algorithms for two tasks relating to heterogeneous tabular datasets: clustering, and synthetic data generation. Tabular datasets typically consist of heterogeneous data types (numerical, ordinal, categorical) in columns, but may also have hidden cluster structure in their rows: for example, they may be drawn from heterogeneous (geographical, socioeconomic, methodological) sources, such that the outcome variable they describe (such as the presence of a disease) may depend not only on the other variables but on the cluster context. Moreover, sharing of biomedical data is often hindered by patient confidentiality laws, and there is current interest in algorithms to generate synthetic tabular data from real data, for example via deep learning.   We demonstrate a novel EM-based clustering algorithm, MMM (``Madras Mixture Model''), that outperforms standard algorithms in determining clusters in synthetic heterogeneous data, and recovers structure in real data. Based on this, we demonstrate a synthetic tabular data generation algorithm, MMMsynth, that pre-clusters the input data, and generates cluster-wise synthetic data assuming cluster-specific data distributions for the input columns. We benchmark this algorithm by testing the performance of standard ML algorithms when they are trained on synthetic data and tested on real published datasets. Our synthetic data generation algorithm outperforms other literature tabular-data generators, and approaches the performance of training purely with real data.
</details>
<details>
<summary>摘要</summary>
我们提供了新的算法，用于两个与异类表格数据相关的任务：聚类和生成synthetic数据。异类表格数据通常包含不同的数据类型（数值、排序、 categorical）的列，但可能也有隐藏的聚类结构在其行中：例如，它们可能来自不同的地理、社会经济、方法学来源，并且结果变量（如疾病存在）可能不仅取决于其他变量，而且受到聚类上下文的影响。此外，生物医学数据共享受到了患者隐私法律的限制，现在有兴趣在使用深度学习生成synthetic表格数据。我们描述了一种新的EM基于的聚类算法，名为Madras Mixture Model（MMM），它在异类数据上表现出色，并在实际数据中恢复结构。基于这种算法，我们提出了一种synthetic表格数据生成算法，名为MMMsynth，它先对输入数据进行聚类，然后生成每个聚类的cluster-specific synthetic数据，假设每个列的数据分布是固定的。我们对这种算法进行了测试，并发现它在训练与实际发表数据之间的性能相当接近。
</details></li>
</ul>
<hr>
<h2 id="Hodge-Compositional-Edge-Gaussian-Processes"><a href="#Hodge-Compositional-Edge-Gaussian-Processes" class="headerlink" title="Hodge-Compositional Edge Gaussian Processes"></a>Hodge-Compositional Edge Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19450">http://arxiv.org/abs/2310.19450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maosheng Yang, Viacheslav Borovitskiy, Elvin Isufi</li>
<li>for: 本研究旨在开发 principlized Gaussian processes (GPs)，用于模型 simplicial 2-complex 上的函数，特别是流体数据网络中的边流。</li>
<li>methods: 本研究使用 Hodge 分解，开发出 divergence-free 和 curl-free 边 GPs，并将它们组合成 \emph{Hodge-compositional edge GPs}，以便直接学习不同 Hodge  ком component of 边函数。</li>
<li>results: 研究人员在 currency exchange, ocean flows 和 water supply networks 中应用了这些 GPs，并与其他模型进行比较，结果表明这些 GPs 能够准确地捕捉边函数的 relevance。<details>
<summary>Abstract</summary>
We propose principled Gaussian processes (GPs) for modeling functions defined over the edge set of a simplicial 2-complex, a structure similar to a graph in which edges may form triangular faces. This approach is intended for learning flow-type data on networks where edge flows can be characterized by the discrete divergence and curl. Drawing upon the Hodge decomposition, we first develop classes of divergence-free and curl-free edge GPs, suitable for various applications. We then combine them to create \emph{Hodge-compositional edge GPs} that are expressive enough to represent any edge function. These GPs facilitate direct and independent learning for the different Hodge components of edge functions, enabling us to capture their relevance during hyperparameter optimization. To highlight their practical potential, we apply them for flow data inference in currency exchange, ocean flows and water supply networks, comparing them to alternative models.
</details>
<details>
<summary>摘要</summary>
我们提出了原理式加aussian proceses（GPs），用于模型 simplicial 2-complex 上的函数，这种结构类似于图，但是 edges 可能会形成三角形面。这种方法适用于 studying flow-type 资料在网络上，其edge flows 可以通过离散凝聚和旋转来描述。我们首先将 divergence 和 curl 分别对应到 Hodge 分解中的两个分量，然后创建 divergence-free 和 curl-free 的 edge GPs。这些 GPs 可以独立地学习不同的 Hodge 分量，使得它们能够捕捉到不同的运算效应。我们组合这些 GPs 创建了 Hodge-compositional edge GPs，这些 GPs 能够表示任何 edge 函数。我们在货币交易、海洋流和水Supply 网络中应用这些 GPs，与其他模型进行比较。这些 GPs 能够对 flow 资料进行直接和独立的学习，并且能够捕捉到不同的运算效应，因此具有实际的应用潜力。
</details></li>
</ul>
<hr>
<h2 id="A-Federated-Learning-Framework-for-Stenosis-Detection"><a href="#A-Federated-Learning-Framework-for-Stenosis-Detection" class="headerlink" title="A Federated Learning Framework for Stenosis Detection"></a>A Federated Learning Framework for Stenosis Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19445">http://arxiv.org/abs/2310.19445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariachiara Di Cosmo, Giovanna Migliorelli, Matteo Francioni, Andi Mucaj, Alessandro Maolo, Alessandro Aprile, Emanuele Frontoni, Maria Chiara Fiorentino, Sara Moccia</li>
<li>for: 这个研究探讨了 Federated Learning (FL) 在 coronary angiography 影像中的狭窄部分检测。</li>
<li>methods: 我们使用了 Faster R-CNN 模型进行检测，并在两个客户机构之间共享模型背部重量，使用 Federated Averaging (FedAvg) 进行重量聚合。</li>
<li>results: 我们的结果显示，FL 框架不会严重影响客户机构 2 的性能，但对客户机构 1 而言，FL 框架可以提高性能，对比本地训练模型，提高了 +3.76%、+17.21% 和 +10.80%，分别为 P rec &#x3D; 73.56、Rec &#x3D; 67.01 和 F1 &#x3D; 70.13。这些结果显示，FL 可以实现多中心研究，并且保持患者隐私。<details>
<summary>Abstract</summary>
This study explores the use of Federated Learning (FL) for stenosis detection in coronary angiography images (CA). Two heterogeneous datasets from two institutions were considered: Dataset 1 includes 1219 images from 200 patients, which we acquired at the Ospedale Riuniti of Ancona (Italy); Dataset 2 includes 7492 sequential images from 90 patients from a previous study available in the literature. Stenosis detection was performed by using a Faster R-CNN model. In our FL framework, only the weights of the model backbone were shared among the two client institutions, using Federated Averaging (FedAvg) for weight aggregation. We assessed the performance of stenosis detection using Precision (P rec), Recall (Rec), and F1 score (F1). Our results showed that the FL framework does not substantially affects clients 2 performance, which already achieved good performance with local training; for client 1, instead, FL framework increases the performance with respect to local model of +3.76%, +17.21% and +10.80%, respectively, reaching P rec = 73.56, Rec = 67.01 and F1 = 70.13. With such results, we showed that FL may enable multicentric studies relevant to automatic stenosis detection in CA by addressing data heterogeneity from various institutions, while preserving patient privacy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这项研究探讨了在 coronary angiography 图像 (CA) 中使用 Federated Learning (FL) 进行stenosis 检测。我们考虑了两个不同的数据集，一个来自意大利安科那的医院 (Ospedale Riuniti of Ancona)，另一个来自文献中的一项前期研究，共计7492个顺序图像和90名患者。我们使用 Faster R-CNN 模型进行检测。在我们的 FL 框架中，只有模型背景的加权被共享给两个客户机构，使用 Federated Averaging (FedAvg) 进行加权聚合。我们评估了检测精度使用精度 (Precision)、报告率 (Recall) 和 F1 分数 (F1)。我们的结果表明，FL 框架不会对客户机构 2 的性能产生显著影响，这些客户机构已经在本地训练 achieved good performance; 而对于客户机构 1，FL 框架会提高本地模型的性能，增加 +3.76%, +17.21% 和 +10.80%，分别达到 Precision = 73.56, Recall = 67.01 和 F1 = 70.13。通过这些结果，我们证明了 FL 可能会在 CA 中实现多中心的自动stenosis 检测研究，同时解决不同机构的数据不一致性问题，保护患者隐私。
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-Diffusion-Based-Channel-Adaptive-Secure-Wireless-Semantic-Communications"><a href="#Asymmetric-Diffusion-Based-Channel-Adaptive-Secure-Wireless-Semantic-Communications" class="headerlink" title="Asymmetric Diffusion Based Channel-Adaptive Secure Wireless Semantic Communications"></a>Asymmetric Diffusion Based Channel-Adaptive Secure Wireless Semantic Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19439">http://arxiv.org/abs/2310.19439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xintian Ren, Jun Wu, Hansong Xu, Qianqian Pan</li>
<li>For: The paper proposes a secure semantic communication system called DiffuSeC to address the security problem caused by semantic attacks in end-to-end data transmission tasks like image classification and image reconstruction.* Methods: The system leverages the diffusion model and deep reinforcement learning (DRL) to mitigate perturbations added by semantic attacks, including data source attacks and channel attacks. A DRL-based channel-adaptive diffusion step selection scheme is developed to improve robustness under unstable channel conditions.* Results: Simulation results demonstrate that DiffuSeC shows higher robust accuracy than previous works under a wide range of channel conditions and can quickly adjust the model state according to signal-to-noise ratios (SNRs) in unstable environments.Here is the Chinese translation of the three key points:* For: 这篇论文提出了一种名为DiffuSeC的安全semantic通信系统，用于解决semantic攻击导致的安全问题在终端数据传输任务中，如图像分类和图像重建。* Methods: DiffuSeC使用了扩散模型和深度强化学习（DRL）来减少semantic攻击添加的干扰，包括数据源攻击和通道攻击。在不稳定的通道条件下，我们开发了一种基于DRL的通道适应扩散步选择方案，以提高系统的稳定性。* Results: 模拟结果表明，DiffuSeC在各种通道条件下具有更高的Robust性精度，并可以快速根据信号噪比（SNR）在不稳定环境中调整模型状态。<details>
<summary>Abstract</summary>
Semantic communication has emerged as a new deep learning-based communication paradigm that drives the research of end-to-end data transmission in tasks like image classification, and image reconstruction. However, the security problem caused by semantic attacks has not been well explored, resulting in vulnerabilities within semantic communication systems exposed to potential semantic perturbations. In this paper, we propose a secure semantic communication system, DiffuSeC, which leverages the diffusion model and deep reinforcement learning (DRL) to address this issue. With the diffusing module in the sender end and the asymmetric denoising module in the receiver end, the DiffuSeC mitigates the perturbations added by semantic attacks, including data source attacks and channel attacks. To further improve the robustness under unstable channel conditions caused by semantic attacks, we developed a DRL-based channel-adaptive diffusion step selection scheme to achieve stable performance under fluctuating environments. A timestep synchronization scheme is designed for diffusion timestep coordination between the two ends. Simulation results demonstrate that the proposed DiffuSeC shows higher robust accuracy than previous works under a wide range of channel conditions, and can quickly adjust the model state according to signal-to-noise ratios (SNRs) in unstable environments.
</details>
<details>
<summary>摘要</summary>
新型深度学习基于的 semantics 通信方式，semantic communication，在图像分类和图像重建等任务中得到了广泛的应用。然而，这种通信方式受到semantic attack的安全问题的影响，导致其存在漏洞。在这篇论文中，我们提出了一种安全的semantic communication系统，DiffuSeC，该系统利用了幂函数模型和深度强化学习（DRL）来解决这一问题。在发送端有扩散模块，接收端有非对称幂函数模块，DiffuSeC可以防止由semantic attack引起的干扰。为了进一步提高在不稳定的通信环境中的稳定性，我们开发了基于DRL的通信环境适应扩散步选择方案，以确保在不稳定的环境中的稳定性。此外，我们还设计了一种时间步同步方案，用于协调发送端和接收端的扩散步。实验结果表明，提出的DiffuSeC在各种通信环境下表现更高的Robust Accuracy，并能够根据信号噪比（SNR）在不稳定的环境中快速调整模型状态。
</details></li>
</ul>
<hr>
<h2 id="LightSAGE-Graph-Neural-Networks-for-Large-Scale-Item-Retrieval-in-Shopee’s-Advertisement-Recommendation"><a href="#LightSAGE-Graph-Neural-Networks-for-Large-Scale-Item-Retrieval-in-Shopee’s-Advertisement-Recommendation" class="headerlink" title="LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in Shopee’s Advertisement Recommendation"></a>LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in Shopee’s Advertisement Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19394">http://arxiv.org/abs/2310.19394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dang Minh Nguyen, Chenfei Wang, Yan Shen, Yifan Zeng</li>
<li>for: 本研究探讨了在大规模电商搜索中使用图 neural network (GNN) 的应用，以及如何在实际项目中建立高质量图、处理数据稀缺和冷启动问题。</li>
<li>methods: 本研究提出了一种简单 yet novel的图建构技术， combinig strong-signal用户行为和高精度协同推荐（CF）算法来构建高质量item图。 此外，我们还提出了一种名为 LightSAGE 的新的 GNN 架构，用于生成高质量items的嵌入，以便vector搜索。</li>
<li>results: 我们的模型在线上A&#x2F;B测试中表现出色，并在Shopee的推荐广告系统中进行了实质性的应用。我们的模型可以有效地处理冷启动和长尾项目问题，并且在offline评估中也提供了显著的改善。<details>
<summary>Abstract</summary>
Graph Neural Network (GNN) is the trending solution for item retrieval in recommendation problems. Most recent reports, however, focus heavily on new model architectures. This may bring some gaps when applying GNN in the industrial setup, where, besides the model, constructing the graph and handling data sparsity also play critical roles in the overall success of the project. In this work, we report how GNN is applied for large-scale e-commerce item retrieval at Shopee. We introduce our simple yet novel and impactful techniques in graph construction, modeling, and handling data skewness. Specifically, we construct high-quality item graphs by combining strong-signal user behaviors with high-precision collaborative filtering (CF) algorithm. We then develop a new GNN architecture named LightSAGE to produce high-quality items' embeddings for vector search. Finally, we design multiple strategies to handle cold-start and long-tail items, which are critical in an advertisement (ads) system. Our models bring improvement in offline evaluations, online A/B tests, and are deployed to the main traffic of Shopee's Recommendation Advertisement system.
</details>
<details>
<summary>摘要</summary>
Graph Neural Network (GNN) 是当前推荐问题的流行解决方案。然而，最新的报告强调新的模型建立。这可能会导致在实际应用中， besides 模型，构建图和处理数据稀缺问题的重要性被忽略。在这篇文章中，我们报道了在 Shopee 的大规模电商ITEM检索中使用 GNN。我们介绍了我们的简单 yet novel 和有影响力的图构建、模型化和数据偏度处理技术。 Specifically，我们结合强信号用户行为和高精度共同推荐算法来构建高质量的ITEM图。然后，我们开发了一种名为 LightSAGE 的新的 GNN 架构，以生成高质量的ITEM嵌入 Vector 搜索。最后，我们设计了多种方法来处理冷启动和长尾ITEM，这些方法在广告系统中是关键的。我们的模型在线评估、A/B 测试中提供了改进，并在 Shopee 推荐广告系统的主要流量中部署。
</details></li>
</ul>
<hr>
<h2 id="Causal-Fair-Metric-Bridging-Causality-Individual-Fairness-and-Adversarial-Robustness"><a href="#Causal-Fair-Metric-Bridging-Causality-Individual-Fairness-and-Adversarial-Robustness" class="headerlink" title="Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness"></a>Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19391">http://arxiv.org/abs/2310.19391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ehyaei/Causal-Fair-Metric-Learning">https://github.com/Ehyaei/Causal-Fair-Metric-Learning</a></li>
<li>paper_authors: Ahmad-Reza Ehyaei, Golnoosh Farnadi, Samira Samadi</li>
<li>for: 本研究旨在提出一种基于 causal 结构的 fair 度量，以 guaranteeequitable treatment regardless of sensitive attributes。</li>
<li>methods: 本研究使用了 adversarial perturbation 和 protected causal perturbation 来检测和修复模型的漏损和不公正。在 metric learning 方面，提出了一种方法 для metric estimation 和 deployment。</li>
<li>results: 本研究提出了一种基于 causal 结构的 fair 度量，可以应用于 adversarial training, fair learning, algorithmic recourse, 和 causal reinforcement learning 等领域。<details>
<summary>Abstract</summary>
Adversarial perturbation is used to expose vulnerabilities in machine learning models, while the concept of individual fairness aims to ensure equitable treatment regardless of sensitive attributes. Despite their initial differences, both concepts rely on metrics to generate similar input data instances. These metrics should be designed to align with the data's characteristics, especially when it is derived from causal structure and should reflect counterfactuals proximity. Previous attempts to define such metrics often lack general assumptions about data or structural causal models. In this research, we introduce a causal fair metric formulated based on causal structures that encompass sensitive attributes. For robustness analysis, the concept of protected causal perturbation is presented. Additionally, we delve into metric learning, proposing a method for metric estimation and deployment in real-world problems. The introduced metric has applications in the fields adversarial training, fair learning, algorithmic recourse, and causal reinforcement learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>机器学习模型的敏感性漏洞通过对敏感属性的偏见进行攻击来暴露，而个人公平性目标则是保证不同敏感属性的各种对待。尽管这两个概念在初始阶段有所不同，但它们都 rely on 度量来生成类似的输入数据实例。这些度量应该与数据的特点相对应，特别是当数据来自 causal 结构时。在过去的尝试中，定义这些度量的方法经常缺乏一般假设关于数据或结构 causal 模型。在这项研究中，我们引入了基于 causal 结构的公平度量，用于掌控敏感属性。此外，我们还详细介绍了一种 metric learning 方法，用于度量估计和应用在实际问题中。引入的度量具有应用于对抗训练、公平学习、算法抗议和 causal 强化学习等领域的应用。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Manifold-Gaussian-Process-Regression"><a href="#Implicit-Manifold-Gaussian-Process-Regression" class="headerlink" title="Implicit Manifold Gaussian Process Regression"></a>Implicit Manifold Gaussian Process Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19390">http://arxiv.org/abs/2310.19390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bernardo Fichera, Viacheslav Borovitskiy, Andreas Krause, Aude Billard</li>
<li>for: 这篇论文是用于提高 Gaussian process regression 在高维度数据上的预测性和准确性。</li>
<li>methods: 这篇论文提出了一种可以直接从数据中推导隐藏结构的 Gaussian process regression 技术，并且可以处理高维度数据。</li>
<li>results: 这篇论文获得了一个可以测量 Gaussian process regression 模型在高维度数据上的预测性和准确性，并且可以处理百万笔数据。<details>
<summary>Abstract</summary>
Gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. However, it struggles with high-dimensional data. One possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional manifold upon which the data actually lies, as postulated by the manifold hypothesis. Prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. In contrast, in this paper we propose a Gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. For the resulting model, we discuss its convergence to the Mat\'ern Gaussian process on the assumed manifold. Our technique scales up to hundreds of thousands of data points, and may improve the predictive performance and calibration of the standard Gaussian process regression in high-dimensional~settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Gradient-free-online-learning-of-subgrid-scale-dynamics-with-neural-emulators"><a href="#Gradient-free-online-learning-of-subgrid-scale-dynamics-with-neural-emulators" class="headerlink" title="Gradient-free online learning of subgrid-scale dynamics with neural emulators"></a>Gradient-free online learning of subgrid-scale dynamics with neural emulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19385">http://arxiv.org/abs/2310.19385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugo Frezat, Guillaume Balarac, Julien Le Sommer, Ronan Fablet</li>
<li>for: 这 paper 的目的是提出一种通用的算法，用于在线（即 $\textit{a posteriori}$ 损失函数）对 numerical 解析器进行机器学习基于 parametrization 的训练。</li>
<li>methods: 该方法利用神经emuulator来训练一个简化的状态空间解析器的近似，然后使用这个近似来允许时间整合步骤中的梯度传播。</li>
<li>results: 试验表明，通过单独训练神经emuulator和参数化组件的loss量可以最小化一些近似偏差的传播。<details>
<summary>Abstract</summary>
In this paper, we propose a generic algorithm to train machine learning-based subgrid parametrizations online, i.e., with $\textit{a posteriori}$ loss functions for non-differentiable numerical solvers. The proposed approach leverage neural emulators to train an approximation of the reduced state-space solver, which is then used to allows gradient propagation through temporal integration steps. The algorithm is able to recover most of the benefit of online strategies without having to compute the gradient of the original solver. It is demonstrated that training the neural emulator and parametrization components separately with respective loss quantities is necessary in order to minimize the propagation of some approximation bias.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种通用算法，用于在线训练机器学习基于低级 parametrization 的梯度传播，即使 numerical solvers 无法导数。我们的方法利用神经网络仿真器来训练减少状态空间解决方案的近似，然后使用这个近似来允许时间 интеIntegration 步骤中的梯度传播。我们的算法可以在大多数情况下重新获得在线策略中的优点，而无需计算原始解决方案的梯度。我们还发现，在训练神经网络仿真器和 parametrization 组件 separately 的情况下，可以最小化一些近似偏差的传播。
</details></li>
</ul>
<hr>
<h2 id="Deep-anytime-valid-hypothesis-testing"><a href="#Deep-anytime-valid-hypothesis-testing" class="headerlink" title="Deep anytime-valid hypothesis testing"></a>Deep anytime-valid hypothesis testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19384">http://arxiv.org/abs/2310.19384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teodora Pandeva, Patrick Forré, Aaditya Ramdas, Shubhanshu Shekhar</li>
<li>for: 这个论文旨在提供一种通用框架，用于构建强大、顺序的假设测试方法，用于处理非参数测试问题。</li>
<li>methods: 该框架使用两个已知操作符来定义空间假设，允许对数据分布进行一元化处理，包括两样本测试、独立测试和condition independence testing等等。</li>
<li>results: 该框架比传统批处理测试具有以下优点：1）能够不断监测在线数据流中，高效地聚合证据反对空间假设，2）可以实现紧密控制类型一错 без需要多测试修正，3）可以根据未知问题的难度自适应样本大小。<details>
<summary>Abstract</summary>
We propose a general framework for constructing powerful, sequential hypothesis tests for a large class of nonparametric testing problems. The null hypothesis for these problems is defined in an abstract form using the action of two known operators on the data distribution. This abstraction allows for a unified treatment of several classical tasks, such as two-sample testing, independence testing, and conditional-independence testing, as well as modern problems, such as testing for adversarial robustness of machine learning (ML) models. Our proposed framework has the following advantages over classical batch tests: 1) it continuously monitors online data streams and efficiently aggregates evidence against the null, 2) it provides tight control over the type I error without the need for multiple testing correction, 3) it adapts the sample size requirement to the unknown hardness of the problem. We develop a principled approach of leveraging the representation capability of ML models within the testing-by-betting framework, a game-theoretic approach for designing sequential tests. Empirical results on synthetic and real-world datasets demonstrate that tests instantiated using our general framework are competitive against specialized baselines on several tasks.
</details>
<details>
<summary>摘要</summary>
我们提出一种通用框架，用于构建强大、顺序的假设测试，用于一类非Parametric测试问题。 null假设使用两个已知运算符定义在数据分布上，这种抽象允许我们对多种古典任务，如两个样本测试、独立测试和条件独立测试，以及现代问题，如机器学习（ML）模型对抗性测试进行统一处理。我们的提议的优势包括：1）在线流处理数据并快速汇集质量证据反对null假设，2）不需要多样测试修正，可以保持紧凑的类型一错率，3）根据未知问题的难度自适应样本大小。我们开发了一种基于测试-ById的游戏理论方法，用于设计顺序测试。实际结果表明，使用我们的通用框架实现的测试在多个任务上与专门的基准相比竞争。
</details></li>
</ul>
<hr>
<h2 id="Musical-Form-Generation"><a href="#Musical-Form-Generation" class="headerlink" title="Musical Form Generation"></a>Musical Form Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19842">http://arxiv.org/abs/2310.19842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rshohan/Jeffrey-Whalen-from-Yorktown1">https://github.com/rshohan/Jeffrey-Whalen-from-Yorktown1</a></li>
<li>paper_authors: Lilac Atassi</li>
<li>for: 这篇论文是为了生成结构化、可靠的音乐作品而写的。</li>
<li>methods: 该方法使用 conditional generative model 创造乐曲的不同部分，并使用大型自然语言模型提供乐曲的高级结构。</li>
<li>results: 该方法可以生成结构化、可靠的乐曲，不受机会性的限制，可以延展到无限长。<details>
<summary>Abstract</summary>
While recent generative models can produce engaging music, their utility is limited. The variation in the music is often left to chance, resulting in compositions that lack structure. Pieces extending beyond a minute can become incoherent or repetitive. This paper introduces an approach for generating structured, arbitrarily long musical pieces. Central to this approach is the creation of musical segments using a conditional generative model, with transitions between these segments. The generation of prompts that determine the high-level composition is distinct from the creation of finer, lower-level details. A large language model is then used to suggest the musical form.
</details>
<details>
<summary>摘要</summary>
Recent generative models can produce engaging music, but their usefulness is limited. The variation in the music is often left to chance, resulting in compositions that lack structure. Pieces extending beyond a minute can become incoherent or repetitive. This paper introduces an approach for generating structured, arbitrarily long musical pieces. Central to this approach is the creation of musical segments using a conditional generative model, with transitions between these segments. The generation of prompts that determine the high-level composition is distinct from the creation of finer, lower-level details. A large language model is then used to suggest the musical form.Translation note:* "generative models" 被翻译为 "生成模型" (shēngchǎng módel)* "utility" 被翻译为 "实用性" (shíyòngxìng)* "variation" 被翻译为 "变化" (biànbèi)* "incoherent" 被翻译为 "无序" (wùxí)* "repetitive" 被翻译为 "重复" (chóngfù)* "conditional generative model" 被翻译为 "条件生成模型" (tiěkuàng shēngchǎng módel)* "prompts" 被翻译为 "提示" (tímí)* "high-level composition" 被翻译为 "高级组合" (gāojí zhùxìn)* "lower-level details" 被翻译为 "低级细节" (dījí xìaoxiè)* "large language model" 被翻译为 "大型语言模型" (dàxìng yǔyán módel)
</details></li>
</ul>
<hr>
<h2 id="An-interpretable-clustering-approach-to-safety-climate-analysis-examining-driver-group-distinction-in-safety-climate-perceptions"><a href="#An-interpretable-clustering-approach-to-safety-climate-analysis-examining-driver-group-distinction-in-safety-climate-perceptions" class="headerlink" title="An interpretable clustering approach to safety climate analysis: examining driver group distinction in safety climate perceptions"></a>An interpretable clustering approach to safety climate analysis: examining driver group distinction in safety climate perceptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19841">http://arxiv.org/abs/2310.19841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kailai Sun, Tianxiang Lan, Yang Miang Goh, Sufiana Safiena, Yueng-Hsiang Huang, Bailey Lytle, Yimin He</li>
<li>for: 本研究旨在提高卡车驾驶员安全性，特别是通过分析驾驶员安全氛围的不同群体来开发更有效的安全预防措施。</li>
<li>methods: 本研究使用了5种不同的聚类算法来分析驾驶员对安全氛围的感知，并提出了一种新的量化评估部分依赖图（QPDP）的方法来更好地解释聚类结果。</li>
<li>results: 研究发现，supervisory care promotion是分 distinguish various driver groups的关键因素。此外，使用不同的聚类算法可能会导致不同的结果，因此需要进一步的比较和分析。<details>
<summary>Abstract</summary>
The transportation industry, particularly the trucking sector, is prone to workplace accidents and fatalities. Accidents involving large trucks accounted for a considerable percentage of overall traffic fatalities. Recognizing the crucial role of safety climate in accident prevention, researchers have sought to understand its factors and measure its impact within organizations. While existing data-driven safety climate studies have made remarkable progress, clustering employees based on their safety climate perception is innovative and has not been extensively utilized in research. Identifying clusters of drivers based on their safety climate perception allows the organization to profile its workforce and devise more impactful interventions. The lack of utilizing the clustering approach could be due to difficulties interpreting or explaining the factors influencing employees' cluster membership. Moreover, existing safety-related studies did not compare multiple clustering algorithms, resulting in potential bias. To address these issues, this study introduces an interpretable clustering approach for safety climate analysis. This study compares 5 algorithms for clustering truck drivers based on their safety climate perceptions. It proposes a novel method for quantitatively evaluating partial dependence plots (QPDP). To better interpret the clustering results, this study introduces different interpretable machine learning measures (SHAP, PFI, and QPDP). Drawing on data collected from more than 7,000 American truck drivers, this study significantly contributes to the scientific literature. It highlights the critical role of supervisory care promotion in distinguishing various driver groups. The Python code is available at https://github.com/NUS-DBE/truck-driver-safety-climate.
</details>
<details>
<summary>摘要</summary>
运输业界，特别是卡车运输业，受到工作场所意外和死亡的威胁，大型卡车事故占了交通意外整体死亡人数的一定比例。为了预防意外，研究人员权威了安全氛围的因素和影响，并尝试了在组织内部进行分组。这是因为不同的驾驶者对安全氛围的感受不同，因此可以根据驾驶者对安全氛围的感受进行分组。然而，现有的数据驱动的安全氛围研究尚未充分利用分组方法，而且存在评估因素的困难和解释分组成员的问题。此外，现有的安全相关研究未有比较多种分组算法，导致可能的偏见。为了解决这些问题，本研究将引入可解释的分组方法来分析安全氛围。本研究比较了5种分组算法，并提出了一种新的量化评估参数图（QPDP）。为了更好地解释分组结果，本研究引入了不同的可解释机器学习度量（SHAP、PFI、QPDP）。基于超过7,000名美国卡车驾驶者的数据，本研究对科学文献做出了重要贡献。它显示了监理照顾的推广对不同的驾驶者群体的区别起到了关键的作用。Python代码可以在<https://github.com/NUS-DBE/truck-driver-safety-climate>获取。
</details></li>
</ul>
<hr>
<h2 id="ProNet-Progressive-Neural-Network-for-Multi-Horizon-Time-Series-Forecasting"><a href="#ProNet-Progressive-Neural-Network-for-Multi-Horizon-Time-Series-Forecasting" class="headerlink" title="ProNet: Progressive Neural Network for Multi-Horizon Time Series Forecasting"></a>ProNet: Progressive Neural Network for Multi-Horizon Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19322">http://arxiv.org/abs/2310.19322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Lin</li>
<li>for: 这 paper 是为了解决多个时间序列预测问题，提出了一种基于深度学习的 ProNet 模型，可以同时使用 AR 和 NAR 策略。</li>
<li>methods: ProNet 模型使用了 segmentation 技术，将预测时间段分为多个子段，使用非参照性预测最重要的子段，并使用参照性预测剩下的子段。 segmentation 过程使用了隐藏变量，可以有效地捕捉各个时间步骤的重要性。</li>
<li>results: ProNet 模型在四个大数据集上进行了全面的评估，并进行了一个简要的ablation study，结果显示 ProNet 模型在准确率和预测速度两个方面表现出色，比 AR 和 NAR 模型更高。<details>
<summary>Abstract</summary>
In this paper, we introduce ProNet, an novel deep learning approach designed for multi-horizon time series forecasting, adaptively blending autoregressive (AR) and non-autoregressive (NAR) strategies. Our method involves dividing the forecasting horizon into segments, predicting the most crucial steps in each segment non-autoregressively, and the remaining steps autoregressively. The segmentation process relies on latent variables, which effectively capture the significance of individual time steps through variational inference. In comparison to AR models, ProNet showcases remarkable advantages, requiring fewer AR iterations, resulting in faster prediction speed, and mitigating error accumulation. On the other hand, when compared to NAR models, ProNet takes into account the interdependency of predictions in the output space, leading to improved forecasting accuracy. Our comprehensive evaluation, encompassing four large datasets, and an ablation study, demonstrate the effectiveness of ProNet, highlighting its superior performance in terms of accuracy and prediction speed, outperforming state-of-the-art AR and NAR forecasting models.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了ProNet，一种新的深度学习方法，用于多个预测 horizons 的时间序列预测。我们的方法通过分解预测时间轴，预测每个分解段中的最重要步骤非autoregressively，并且剩下的步骤使用 autoregressive 方法预测。这个分 segmentation 过程基于隐藏变量，可以有效地捕捉个时间步骤的重要性 durch variational inference。与AR模型相比，ProNet 显示出了非常remarkable的优势，需要 fewer AR 迭代，减少预测速度，并 Mitigate 预测误差的寄存。与NAR模型相比，ProNet 考虑了输出空间中预测之间的依赖关系，导致更好的预测精度。我们的全面评估，包括四个大数据集，以及一个ablation study，证明了 ProNet 的效果， highlighting 其在精度和预测速度两个方面的superior performance，超越了当前AR和NAR预测模型。
</details></li>
</ul>
<hr>
<h2 id="Dual-Directed-Algorithm-Design-for-Efficient-Pure-Exploration"><a href="#Dual-Directed-Algorithm-Design-for-Efficient-Pure-Exploration" class="headerlink" title="Dual-Directed Algorithm Design for Efficient Pure Exploration"></a>Dual-Directed Algorithm Design for Efficient Pure Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19319">http://arxiv.org/abs/2310.19319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Qin, Wei You</li>
<li>For: The paper is written to address the problem of pure exploration in stochastic sequential adaptive experiments with a finite set of alternative options. The goal is to accurately identify the best alternative with high confidence and minimal measurement efforts.* Methods: The paper uses dual variables to derive necessary and sufficient conditions for optimality, and proposes an information-directed selection rule to adaptively pick from a candidate set based on information gain. The top-two Thompson sampling algorithm is also used to solve the problem of best-arm identification.* Results: The paper establishes that the proposed algorithm is optimal for Gaussian best-arm identification, and is also applicable to other pure exploration problems such as $\epsilon$-best-arm identification and thresholding bandit problems. The numerical experiments show that the proposed algorithm is more efficient than existing ones.<details>
<summary>Abstract</summary>
We consider pure-exploration problems in the context of stochastic sequential adaptive experiments with a finite set of alternative options. The goal of the decision-maker is to accurately answer a query question regarding the alternatives with high confidence with minimal measurement efforts. A typical query question is to identify the alternative with the best performance, leading to ranking and selection problems, or best-arm identification in the machine learning literature. We focus on the fixed-precision setting and derive a sufficient condition for optimality in terms of a notion of strong convergence to the optimal allocation of samples. Using dual variables, we characterize the necessary and sufficient conditions for an allocation to be optimal. The use of dual variables allow us to bypass the combinatorial structure of the optimality conditions that relies solely on primal variables. Remarkably, these optimality conditions enable an extension of top-two algorithm design principle, initially proposed for best-arm identification. Furthermore, our optimality conditions give rise to a straightforward yet efficient selection rule, termed information-directed selection, which adaptively picks from a candidate set based on information gain of the candidates. We outline the broad contexts where our algorithmic approach can be implemented. We establish that, paired with information-directed selection, top-two Thompson sampling is (asymptotically) optimal for Gaussian best-arm identification, solving a glaring open problem in the pure exploration literature. Our algorithm is optimal for $\epsilon$-best-arm identification and thresholding bandit problems. Our analysis also leads to a general principle to guide adaptations of Thompson sampling for pure-exploration problems. Numerical experiments highlight the exceptional efficiency of our proposed algorithms relative to existing ones.
</details>
<details>
<summary>摘要</summary>
我们考虑了纯exploration问题，在随机顺序的adaptive试验中，有 finite 个选项。目标是使用最少的测量努力确定问题的答案，特别是标准问题，如最佳选项的标识。我们关注 fixed-precision 设定下的情况，并 derive 一个强化条件，表明optimal 分配样本是可行的。使用 dual 变量，我们描述了必要和充分条件，以确定分配是optimal。这些optimality condition允许我们扩展top-two 算法设计原则，初始提出于最佳arm 标识。此外，我们的optimality condition 会生成一种直观的 yet efficient 选择规则，称为信息指向选择。我们 outline 了这些算法方法可以应用的广泛上下文。我们证明，将信息指向选择与 top-two Thompson sampling 结合，可以在 Gaussian 最佳arm 标识问题中获得（几何）优化的解决方案，解决了纯exploration 文献中的一个明显的开问题。我们的算法还是 $\epsilon$-最佳arm 标识和阈值bandit问题的优化解决方案。我们的分析还导出了一种通用的原则，可以导向 Thompson sampling 的adaptation для纯exploration问题。 numerics  experiments highlighted 我们提出的算法相比之下，relative to existing ones, exceptional efficiency.
</details></li>
</ul>
<hr>
<h2 id="A-Planning-and-Exploring-Approach-to-Extreme-Mechanics-Force-Fields"><a href="#A-Planning-and-Exploring-Approach-to-Extreme-Mechanics-Force-Fields" class="headerlink" title="A Planning-and-Exploring Approach to Extreme-Mechanics Force Fields"></a>A Planning-and-Exploring Approach to Extreme-Mechanics Force Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19306">http://arxiv.org/abs/2310.19306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengjie Shi, Zhiping Xu</li>
<li>for: 这种研究旨在理解裂解过程中的裂 crack 的形成和增长，以及裂解过程中的微观结构变化。</li>
<li>methods: 这个研究使用分子模拟来解决裂解过程中的进程，包括机械能消耗、裂 crack 的路径选择和动态不稳定性（如弯曲和分支）。</li>
<li>results: 研究发现，使用高精度的力场模型可以更好地预测裂解过程中的材料行为，并且需要考虑材料的电子结构。<details>
<summary>Abstract</summary>
Extreme mechanical processes such as strong lattice distortion and bond breakage during fracture are ubiquitous in nature and engineering, which often lead to catastrophic failure of structures. However, understanding the nucleation and growth of cracks is challenged by their multiscale characteristics spanning from atomic-level structures at the crack tip to the structural features where the load is applied. Molecular simulations offer an important tool to resolve the progressive microstructural changes at crack fronts and are widely used to explore processes therein, such as mechanical energy dissipation, crack path selection, and dynamic instabilities (e.g., kinking, branching). Empirical force fields developed based on local descriptors based on atomic positions and the bond orders do not yield satisfying predictions of fracture, even for the nonlinear, anisotropic stress-strain relations and the energy densities of edges. High-fidelity force fields thus should include the tensorial nature of strain and the energetics of rare events during fracture, which, unfortunately, have not been taken into account in both the state-of-the-art empirical and machine-learning force fields. Based on data generated by first-principles calculations, we develop a neural network-based force field for fracture, NN-F$^3$, by combining pre-sampling of the space of strain states and active-learning techniques to explore the transition states at critical bonding distances. The capability of NN-F$^3$ is demonstrated by studying the rupture of h-BN and twisted bilayer graphene as model problems. The simulation results confirm recent experimental findings and highlight the necessity to include the knowledge of electronic structures from first-principles calculations in predicting extreme mechanical processes.
</details>
<details>
<summary>摘要</summary>
extremely mechanical processes such as strong lattice distortion and bond breakage during fracture are common in nature and engineering, leading to catastrophic failure of structures. However, understanding the nucleation and growth of cracks is challenging due to their multiscale characteristics, ranging from atomic-level structures at the crack tip to the structural features where the load is applied. Molecular simulations are an important tool for exploring the progressive microstructural changes at crack fronts, including mechanical energy dissipation, crack path selection, and dynamic instabilities such as kinking and branching. However, empirical force fields based on local descriptors of atomic positions and bond orders have limited predictive power, especially for nonlinear, anisotropic stress-strain relations and the energy densities of edges. To address this challenge, we developed a neural network-based force field for fracture, NN-F$^3$, by combining pre-sampling of the space of strain states and active-learning techniques to explore the transition states at critical bonding distances. Our simulation results demonstrate the capability of NN-F$^3$ in studying the rupture of h-BN and twisted bilayer graphene as model problems, confirming recent experimental findings and highlighting the importance of considering electronic structures from first-principles calculations in predicting extreme mechanical processes.
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-Federated-Learning-over-Vertically-and-Horizontally-Partitioned-Data-for-Financial-Anomaly-Detection"><a href="#Privacy-Preserving-Federated-Learning-over-Vertically-and-Horizontally-Partitioned-Data-for-Financial-Anomaly-Detection" class="headerlink" title="Privacy-Preserving Federated Learning over Vertically and Horizontally Partitioned Data for Financial Anomaly Detection"></a>Privacy-Preserving Federated Learning over Vertically and Horizontally Partitioned Data for Financial Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19304">http://arxiv.org/abs/2310.19304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swanand Ravindra Kadhe, Heiko Ludwig, Nathalie Baracaldo, Alan King, Yi Zhou, Keith Houck, Ambrish Rawat, Mark Purcell, Naoise Holohan, Mikio Takeuchi, Ryo Kawahara, Nir Drucker, Hayim Shaul, Eyal Kushnir, Omri Soceanu</li>
<li>For: The paper is written to address the problem of detecting financial anomalies in a collaborative setting among multiple financial institutions, where trust is limited due to regulation and competition.* Methods: The paper proposes a novel solution called PV4FAD that combines fully homomorphic encryption, secure multi-party computation, differential privacy, and randomization techniques to balance privacy and accuracy during training and prevent inference threats at deployment time.* Results: The proposed solution achieves high utility and accuracy by significantly reducing the per-bank noise level while satisfying distributed differential privacy. The approach produces an ensemble model, specifically a random forest, to take advantage of the well-known properties of ensembles to reduce variance and increase accuracy. The solution won second prize in the first phase of the U.S. Privacy Enhancing Technologies (PETs) Prize Challenge.Here is the same information in Simplified Chinese:* For: 本文是为了解决多家金融机构之间协作探测金融异常情况，其中信任因为法规和竞争限制。* Methods: 本文提出了一种新的解决方案，即PV4FAD，它将完全Homomorphic encryption、安全多方计算、差分隐私和随机技术结合起来，以保持协作时的隐私和准确性。* Results: 该解决方案可以达到高Utility和准确性，通过减少每家银行的噪声水平，满足分布式差分隐私。该方案生成了一个 Random Forest ensemble模型，以利用 ensemble 模型的知名特性，降低差异和提高准确性。该解决方案在美国隐私提高技术奖 Challenge 的第一阶段中获得了第二名。<details>
<summary>Abstract</summary>
The effective detection of evidence of financial anomalies requires collaboration among multiple entities who own a diverse set of data, such as a payment network system (PNS) and its partner banks. Trust among these financial institutions is limited by regulation and competition. Federated learning (FL) enables entities to collaboratively train a model when data is either vertically or horizontally partitioned across the entities. However, in real-world financial anomaly detection scenarios, the data is partitioned both vertically and horizontally and hence it is not possible to use existing FL approaches in a plug-and-play manner.   Our novel solution, PV4FAD, combines fully homomorphic encryption (HE), secure multi-party computation (SMPC), differential privacy (DP), and randomization techniques to balance privacy and accuracy during training and to prevent inference threats at model deployment time. Our solution provides input privacy through HE and SMPC, and output privacy against inference time attacks through DP. Specifically, we show that, in the honest-but-curious threat model, banks do not learn any sensitive features about PNS transactions, and the PNS does not learn any information about the banks' dataset but only learns prediction labels. We also develop and analyze a DP mechanism to protect output privacy during inference. Our solution generates high-utility models by significantly reducing the per-bank noise level while satisfying distributed DP. To ensure high accuracy, our approach produces an ensemble model, in particular, a random forest. This enables us to take advantage of the well-known properties of ensembles to reduce variance and increase accuracy. Our solution won second prize in the first phase of the U.S. Privacy Enhancing Technologies (PETs) Prize Challenge.
</details>
<details>
<summary>摘要</summary>
要有效探测金融异常，需要多个金融机构合作，其中包括支付网络系统（PNS）和其合作银行。这些金融机构之间的信任受到了法规和竞争的限制。基于联合学习（FL）技术，这些机构可以在数据水平或水平上分割的情况下合作训练模型。然而，在实际金融异常探测场景中，数据通常会被水平和垂直分割，因此不能使用现有的FL方法。我们的新解决方案，PV4FAD，结合了完全同质加密（HE）、安全多方计算（SMPC）、差分隐私（DP）和随机化技术，以保持隐私和准确性 durante 训练和执行时。我们的解决方案提供了输入隐私通过 HE 和 SMPC，并输出隐私防止攻击 during 执行时通过 DP。我们显示，在诚实但偷CURRENT的威胁模型下，银行不会学习PNS交易的敏感特征，而PNS只会学习预测标签，而不是银行的数据集。我们还开发了一种DP机制来保护输出隐私 during 执行。我们的解决方案可以减少每家银行的噪声水平，同时满足分布式DP。为了保证高准确性，我们的方法生成高Utility模型，具体来说是随机森林。这使得我们可以利用随机森林的特性来减少差异和提高准确性。我们的解决方案在美国隐私提升技术（PETs）奖励计划的第一阶段中获得了第二名。
</details></li>
</ul>
<hr>
<h2 id="Stage-Aware-Learning-for-Dynamic-Treatments"><a href="#Stage-Aware-Learning-for-Dynamic-Treatments" class="headerlink" title="Stage-Aware Learning for Dynamic Treatments"></a>Stage-Aware Learning for Dynamic Treatments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19300">http://arxiv.org/abs/2310.19300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanwen Ye, Wenzhuo Zhou, Ruoqing Zhu, Annie Qu</li>
<li>For: 本研究旨在提出一种新的个性化学习方法，以优化患者的临床效果。* Methods: 该方法使用动态治疗决策函数（DTR），并且强调对决策过程中观察到的治疗轨迹和优质治疗轨迹的Alignment。* Results: 研究人员通过实验和实际案例研究，证明了该方法可以提高样本效率和稳定性，并且可以更好地考虑决策过程中各个阶段的差异。<details>
<summary>Abstract</summary>
Recent advances in dynamic treatment regimes (DTRs) provide powerful optimal treatment searching algorithms, which are tailored to individuals' specific needs and able to maximize their expected clinical benefits. However, existing algorithms could suffer from insufficient sample size under optimal treatments, especially for chronic diseases involving long stages of decision-making. To address these challenges, we propose a novel individualized learning method which estimates the DTR with a focus on prioritizing alignment between the observed treatment trajectory and the one obtained by the optimal regime across decision stages. By relaxing the restriction that the observed trajectory must be fully aligned with the optimal treatments, our approach substantially improves the sample efficiency and stability of inverse probability weighted based methods. In particular, the proposed learning scheme builds a more general framework which includes the popular outcome weighted learning framework as a special case of ours. Moreover, we introduce the notion of stage importance scores along with an attention mechanism to explicitly account for heterogeneity among decision stages. We establish the theoretical properties of the proposed approach, including the Fisher consistency and finite-sample performance bound. Empirically, we evaluate the proposed method in extensive simulated environments and a real case study for COVID-19 pandemic.
</details>
<details>
<summary>摘要</summary>
最近的动态治疗方案（DTR）技术提供了高效的优化治疗搜索算法，这些算法是针对个人特定需求而设计的，能够最大化他们的临床效果。然而，现有的算法可能会因为优化治疗下的样本数不够而受到限制，特别是 Chronic diseases 的长期决策过程中。为了解决这些挑战，我们提出了一种新的个性化学习方法，该方法优化 DTR 的估计，注重对观察到的治疗轨迹和优化治疗轨迹之间的Alignment。通过放弃优化治疗下的完全Alignment要求，我们的方法可以大幅提高 inverse probability weighted 基于方法的样本效率和稳定性。特别是，我们的学习方案建立了更通用的框架，包括 popular outcome weighted learning 方法作为我们的特殊情况。此外，我们引入了决策阶段重要性分数以及注意机制，以显式地考虑决策阶段之间的不同。我们证明了我们的方法的理论性质，包括 Fisher 一致性和finite-sample performance bound。Empirically, we evaluate the proposed method in extensive simulated environments and a real case study for COVID-19 pandemic.
</details></li>
</ul>
<hr>
<h2 id="AMLNet-Adversarial-Mutual-Learning-Neural-Network-for-Non-AutoRegressive-Multi-Horizon-Time-Series-Forecasting"><a href="#AMLNet-Adversarial-Mutual-Learning-Neural-Network-for-Non-AutoRegressive-Multi-Horizon-Time-Series-Forecasting" class="headerlink" title="AMLNet: Adversarial Mutual Learning Neural Network for Non-AutoRegressive Multi-Horizon Time Series Forecasting"></a>AMLNet: Adversarial Mutual Learning Neural Network for Non-AutoRegressive Multi-Horizon Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19289">http://arxiv.org/abs/2310.19289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Lin</li>
<li>For: 提高多个时间序列预测的准确率和速度。* Methods: 引入了一种新的非自 regression（NAR）模型AMLNet，通过在线知识传递（KD）方法和两种途径（出来-驱动KD和提示驱动KD）来实现教师模型的知识传递，以提高预测的准确性和速度。* Results: 对比于传统AR和NAR模型，AMLNet显示出了更高的准确率和更快的计算速度。<details>
<summary>Abstract</summary>
Multi-horizon time series forecasting, crucial across diverse domains, demands high accuracy and speed. While AutoRegressive (AR) models excel in short-term predictions, they suffer speed and error issues as the horizon extends. Non-AutoRegressive (NAR) models suit long-term predictions but struggle with interdependence, yielding unrealistic results. We introduce AMLNet, an innovative NAR model that achieves realistic forecasts through an online Knowledge Distillation (KD) approach. AMLNet harnesses the strengths of both AR and NAR models by training a deep AR decoder and a deep NAR decoder in a collaborative manner, serving as ensemble teachers that impart knowledge to a shallower NAR decoder. This knowledge transfer is facilitated through two key mechanisms: 1) outcome-driven KD, which dynamically weights the contribution of KD losses from the teacher models, enabling the shallow NAR decoder to incorporate the ensemble's diversity; and 2) hint-driven KD, which employs adversarial training to extract valuable insights from the model's hidden states for distillation. Extensive experimentation showcases AMLNet's superiority over conventional AR and NAR models, thereby presenting a promising avenue for multi-horizon time series forecasting that enhances accuracy and expedites computation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Scalability-and-Reliability-in-Semi-Decentralized-Federated-Learning-With-Blockchain-Trust-Penalization-and-Asynchronous-Functionality"><a href="#Enhancing-Scalability-and-Reliability-in-Semi-Decentralized-Federated-Learning-With-Blockchain-Trust-Penalization-and-Asynchronous-Functionality" class="headerlink" title="Enhancing Scalability and Reliability in Semi-Decentralized Federated Learning With Blockchain: Trust Penalization and Asynchronous Functionality"></a>Enhancing Scalability and Reliability in Semi-Decentralized Federated Learning With Blockchain: Trust Penalization and Asynchronous Functionality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19287">http://arxiv.org/abs/2310.19287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Kumar Shrestha, Faijan Ahamad Khan, Mohammed Afaan Shaikh, Amir Jaberzadeh, Jason Geng</li>
<li>for: 提高分布式联合学习的可扩展性和可靠性，通过结合区块链技术。</li>
<li>methods: 使用信任惩罚机制提高参与节点的可信度，同时实现ynchronization-free的模型更新。</li>
<li>results: 实现了一个公平、安全、透明的分布式联合学习环境，不会侵犯数据隐私。Here’s the same information in English:</li>
<li>for: To enhance the scalability and reliability of Distributed Federated Learning by integrating blockchain technology.</li>
<li>methods: Using a trust penalization mechanism to enhance the trustworthiness of participating nodes, while enabling asynchronous functionality for efficient and robust model updates.</li>
<li>results: Achieving a fair, secure, and transparent environment for collaborative machine learning without compromising data privacy.<details>
<summary>Abstract</summary>
The paper presents an innovative approach to address the challenges of scalability and reliability in Distributed Federated Learning by leveraging the integration of blockchain technology. The paper focuses on enhancing the trustworthiness of participating nodes through a trust penalization mechanism while also enabling asynchronous functionality for efficient and robust model updates. By combining Semi-Decentralized Federated Learning with Blockchain (SDFL-B), the proposed system aims to create a fair, secure and transparent environment for collaborative machine learning without compromising data privacy. The research presents a comprehensive system architecture, methodologies, experimental results, and discussions that demonstrate the advantages of this novel approach in fostering scalable and reliable SDFL-B systems.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种创新的方法，以解决分布式联合学习中的可扩展性和可靠性问题，通过把区块链技术与联合学习相结合。该论文将参与节点的可信性提高通过信任惩罚机制，同时允许 asynchronous 功能，以实现高效和可靠的模型更新。通过结合 Semi-Decentralized Federated Learning with Blockchain（SDFL-B），该提案旨在创造一个公平、安全和透明的合作机器学习环境，无需妥协数据隐私。论文采用了完整的系统架构、方法、实验结果和讨论，以示该新方法在推动可扩展和可靠的 SDFL-B 系统的优势。
</details></li>
</ul>
<hr>
<h2 id="Facilitating-Graph-Neural-Networks-with-Random-Walk-on-Simplicial-Complexes"><a href="#Facilitating-Graph-Neural-Networks-with-Random-Walk-on-Simplicial-Complexes" class="headerlink" title="Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes"></a>Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19285">http://arxiv.org/abs/2310.19285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhouc20/hodgerandomwalk">https://github.com/zhouc20/hodgerandomwalk</a></li>
<li>paper_authors: Cai Zhou, Xiyuan Wang, Muhan Zhang</li>
<li>for: 这篇论文旨在提高图内链接网络（Graph Neural Network，GNN）的表达能力，通过对 simplicial complexes（SC）中的随机漫步进行系统性分析。</li>
<li>methods: 本论文使用随机漫步在不同的几何级别上进行分析，包括节点级别（0-simplices）、边级别（1-simplices）和更高级别的 simplicial complexes。在spectral analysis of Hodge $1$-Laplacians中，提出了一种 permutation equivariant和表达力强的边级别 pozitional encoding（Hodge1Lap）。</li>
<li>results: 广泛的实验证明了随机漫步基于的方法的效果，包括随机漫步 pozitional encoding（RWSE）和Hodge1Lap。这些方法可以提高GNN的表达能力和稳定性。<details>
<summary>Abstract</summary>
Node-level random walk has been widely used to improve Graph Neural Networks. However, there is limited attention to random walk on edge and, more generally, on $k$-simplices. This paper systematically analyzes how random walk on different orders of simplicial complexes (SC) facilitates GNNs in their theoretical expressivity. First, on $0$-simplices or node level, we establish a connection between existing positional encoding (PE) and structure encoding (SE) methods through the bridge of random walk. Second, on $1$-simplices or edge level, we bridge edge-level random walk and Hodge $1$-Laplacians and design corresponding edge PE respectively. In the spatial domain, we directly make use of edge level random walk to construct EdgeRWSE. Based on the spectral analysis of Hodge $1$-Laplcians, we propose Hodge1Lap, a permutation equivariant and expressive edge-level positional encoding. Third, we generalize our theory to random walk on higher-order simplices and propose the general principle to design PE on simplices based on random walk and Hodge Laplacians. Inter-level random walk is also introduced to unify a wide range of simplicial networks. Extensive experiments verify the effectiveness of our random walk-based methods.
</details>
<details>
<summary>摘要</summary>
<font face=" Times New Roman">节点级随机漫步已广泛应用于图 neural network 中，但是有限的注意力是随机漫步在边和更一般来说的 $k$-simplices 方面。本文系统地分析了随机漫步在不同顺序 simplicial complexes （SC） 中如何促进 GNNs 的理论表达能力。首先，在 $0$-simplices 或节点级别，我们建立了随机漫步和 pozitional 编码（PE） 方法之间的桥接。其次，在 $1$-simplices 或边级别，我们将随机漫步和 Hodge  $1$-Laplacians 桥接，并设计对应的边 PE。在空间领域中，我们直接利用边级随机漫步来构建 EdgeRWSE。基于 Hodge  $1$-Laplacians 的спектраль分析，我们提出了一种可变的edge-level pozitional 编码 Hodge1Lap。第三，我们扩展了我们的理论到高阶 simplices 上，并提出了一个通用的方法来在随机漫步和 Hodge Laplacians 基础上设计 PE。我们还引入了间隔随机漫步，以统一一 wide range of simplicial networks。extensive experiments 表明我们的随机漫步基于方法的效果。</font>Note: Simplified Chinese is a written form of Chinese that uses simpler characters and grammar compared to Traditional Chinese. It is commonly used in mainland China and is the official language of the People's Republic of China.
</details></li>
</ul>
<hr>
<h2 id="rTsfNet-a-DNN-model-with-Multi-head-3D-Rotation-and-Time-Series-Feature-Extraction-for-IMU-based-Human-Activity-Recognition"><a href="#rTsfNet-a-DNN-model-with-Multi-head-3D-Rotation-and-Time-Series-Feature-Extraction-for-IMU-based-Human-Activity-Recognition" class="headerlink" title="rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition"></a>rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19283">http://arxiv.org/abs/2310.19283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Enokibori</li>
<li>for: 这篇论文提出了一种基于IMU的人体活动识别（HAR）的新型深度神经网络（DNN）模型，即rTsfNet。</li>
<li>methods: rTsfNet使用多头3D旋转和时间序列特征提取（TSF）来自动选择3D基准，然后使用多层感知网络（MLP）进行人体活动识别。</li>
<li>results: 该模型在管理的标准 bench 条件下，以及多个数据集（UCI HAR、PAMAP2、Daphnet、OPPORTUNITY）中，实现了最高精度，超过了现有模型。<details>
<summary>Abstract</summary>
This paper proposes rTsfNet, a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction, as a new DNN model for IMU-based human activity recognition (HAR). rTsfNet automatically selects 3D bases from which features should be derived by deriving 3D rotation parameters within the DNN. Then, time series features (TSFs), the wisdom of many researchers, are derived and realize HAR using MLP. Although a model that does not use CNN, it achieved the highest accuracy than existing models under well-managed benchmark conditions and multiple datasets: UCI HAR, PAMAP2, Daphnet, and OPPORTUNITY, which target different activities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-Learning-Regularization-for-the-Minimum-Volume-Formula-of-Toric-Calabi-Yau-3-folds"><a href="#Machine-Learning-Regularization-for-the-Minimum-Volume-Formula-of-Toric-Calabi-Yau-3-folds" class="headerlink" title="Machine Learning Regularization for the Minimum Volume Formula of Toric Calabi-Yau 3-folds"></a>Machine Learning Regularization for the Minimum Volume Formula of Toric Calabi-Yau 3-folds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19276">http://arxiv.org/abs/2310.19276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Choi, Rak-Kyeong Seong</li>
<li>for: 这篇论文是为了计算某些特定的5个维度Sasaki-Einstein流形的最小体积而写的。</li>
<li>methods: 这篇论文使用了机器学习正则化技术来计算这些流形的最小体积。</li>
<li>results: 这篇论文提出了一些可解释的、基于流形的几何 invariants的确定最小体积的公式。这些公式可以高度准确地计算这些流形的最小体积。<details>
<summary>Abstract</summary>
We present a collection of explicit formulas for the minimum volume of Sasaki-Einstein 5-manifolds. The cone over these 5-manifolds is a toric Calabi-Yau 3-fold. These toric Calabi-Yau 3-folds are associated with an infinite class of 4d N=1 supersymmetric gauge theories, which are realized as worldvolume theories of D3-branes probing the toric Calabi-Yau 3-folds. Under the AdS/CFT correspondence, the minimum volume of the Sasaki-Einstein base is inversely proportional to the central charge of the corresponding 4d N=1 superconformal field theories. The presented formulas for the minimum volume are in terms of geometric invariants of the toric Calabi-Yau 3-folds. These explicit results are derived by implementing machine learning regularization techniques that advance beyond previous applications of machine learning for determining the minimum volume. Moreover, the use of machine learning regularization allows us to present interpretable and explainable formulas for the minimum volume. Our work confirms that, even for extensive sets of toric Calabi-Yau 3-folds, the proposed formulas approximate the minimum volume with remarkable accuracy.
</details>
<details>
<summary>摘要</summary>
我们提出了一系列Explicit的方程式，用于找到Sasaki-Einstein 5-dimensional manifold的最小体积。这些5-dimensional manifold的对偶是一种toric Calabi-Yau 3-fold。这些toric Calabi-Yau 3-fold和4d N=1 supersymmetric gauge theory之间存在一个无穷的等级关系，它们是D3-brane在toric Calabi-Yau 3-fold上的世界volume理论。根据AdS/CFT对偶，Sasaki-Einstein底物的最小体积与4d N=1 superconformal field theory的中心质量有逆比例关系。我们提出的方程式是使用机器学习调整技术所得到的，这些技术超过了过去使用机器学习来决定最小体积的应用。此外，使用机器学习调整技术允许我们提供可解释和可读的方程式，用于找到最小体积。我们的工作证明，即使是大量的toric Calabi-Yau 3-fold中，我们的方程式仍然可以对最小体积进行高精度的近似。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Effective-Elastic-Moduli-of-Rocks-using-Graph-Neural-Networks"><a href="#Prediction-of-Effective-Elastic-Moduli-of-Rocks-using-Graph-Neural-Networks" class="headerlink" title="Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks"></a>Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19274">http://arxiv.org/abs/2310.19274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehong Chung, Rasool Ahmad, WaiChing Sun, Wei Cai, Tapan Mukerji</li>
<li>for: 这个研究旨在使用图 neural network (GNN) 方法预测岩石的有效弹性模量从其数字 CT 扫描图像中。</li>
<li>methods: 我们使用 Mapper 算法将三维数字岩石图像转换为图集合，包含重要的几何信息。这些图集合经过训练后能够预测弹性模量。</li>
<li>results: GNN 模型在不同的图大小和Subcube 维度上显示了良好的预测能力，并且在测试集上保持了高预测精度。与 Convolutional Neural Networks (CNNs) 进行比较分析表明，GNNs 在预测未看过的岩石性质方面表现更好。此外，图表示微结构减少了 GPU 内存需求（相比于网格表示法 для CNNs），使得批处理大小的选择更加灵活。这项研究 demonstarte GNN 模型在改善岩石性质预测精度和整个数字岩石分析的效率方面具有潜力。<details>
<summary>Abstract</summary>
This study presents a Graph Neural Networks (GNNs)-based approach for predicting the effective elastic moduli of rocks from their digital CT-scan images. We use the Mapper algorithm to transform 3D digital rock images into graph datasets, encapsulating essential geometrical information. These graphs, after training, prove effective in predicting elastic moduli. Our GNN model shows robust predictive capabilities across various graph sizes derived from various subcube dimensions. Not only does it perform well on the test dataset, but it also maintains high prediction accuracy for unseen rocks and unexplored subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs) reveals the superior performance of GNNs in predicting unseen rock properties. Moreover, the graph representation of microstructures significantly reduces GPU memory requirements (compared to the grid representation for CNNs), enabling greater flexibility in the batch size selection. This work demonstrates the potential of GNN models in enhancing the prediction accuracy of rock properties and boosting the efficiency of digital rock analysis.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这个研究提出了基于图神经网络（GNN）的方法，用于预测矿石的有效弹性模量，基于其数字CT扫描图像。我们使用Mapper算法将3D数字矿石图像转换成图 dataset，包含重要的几何信息。这些图， после训练，能够有效预测弹性模量。我们的 GNN 模型在不同的图大小和不同的子立方体维度上表现出了良好的预测能力。不仅在测试数据集上表现出色，而且可以保持高度预测精度 для未看到的矿石和未探索的子立方体大小。比较分析表明，GNN 模型在预测未看到的矿石属性方面表现出了超越 CNN 模型的优异性。此外，图表示微结构的几何表示法可以减少 GPU 内存需求 (相比于网格表示法 для CNN 模型), 这使得批处理大小的选择更加灵活。这项工作示示了 GNN 模型在提高矿石属性预测精度和提高数字矿石分析效率方面的潜力。
</details></li>
</ul>
<hr>
<h2 id="Invariant-kernels-on-Riemannian-symmetric-spaces-a-harmonic-analytic-approach"><a href="#Invariant-kernels-on-Riemannian-symmetric-spaces-a-harmonic-analytic-approach" class="headerlink" title="Invariant kernels on Riemannian symmetric spaces: a harmonic-analytic approach"></a>Invariant kernels on Riemannian symmetric spaces: a harmonic-analytic approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19270">http://arxiv.org/abs/2310.19270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathael Da Costa, Cyrus Mostajeran, Juan-Pablo Ortega, Salem Said</li>
<li>for: 证明古德曼kernel在非欧几何空间上是否总是正定 positivity.</li>
<li>methods: 发展新的几何和分析方法，提供了对古德曼kernel的正定性的彻底 caracterization，完整性仅在低维度场景下有限数量的数值计算中略有缺陷。</li>
<li>results: 提出了L$^{!\scriptscriptstyle p}$-Godement定理（-$p &#x3D; 1,2），这些定理提供了非 compact型半 Symmetric space上古德曼kernel的正定性的必要和 suficient conditions，这些结果比较容易应用。<details>
<summary>Abstract</summary>
This work aims to prove that the classical Gaussian kernel, when defined on a non-Euclidean symmetric space, is never positive-definite for any choice of parameter. To achieve this goal, the paper develops new geometric and analytical arguments. These provide a rigorous characterization of the positive-definiteness of the Gaussian kernel, which is complete but for a limited number of scenarios in low dimensions that are treated by numerical computations. Chief among these results are the L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement theorems (where $p = 1,2$), which provide verifiable necessary and sufficient conditions for a kernel defined on a symmetric space of non-compact type to be positive-definite. A celebrated theorem, sometimes called the Bochner-Godement theorem, already gives such conditions and is far more general in its scope, but is especially hard to apply. Beyond the connection with the Gaussian kernel, the new results in this work lay out a blueprint for the study of invariant kernels on symmetric spaces, bringing forth specific harmonic analysis tools that suggest many future applications.
</details>
<details>
<summary>摘要</summary>
（这个工作的目标是证明классический高斯核，当定义在非欧几何同轴空间上，从未是一个正定的核心，对于任何选择的参数。为了实现这一目标，文章发展了新的几何和分析 Argument。这些提供了非欧几何同轴空间上高斯核的正定性的完整但是有限数量的低维度场景，通过数值计算处理。文章中最主要的结果是L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement定理（其中$p = 1,2），这些定理提供了非欧几何同轴空间上核定的必要和 suficient条件，这些条件是可靠的，但是特别hard to apply。 beyond the connection with the Gaussian kernel, the new results in this work lay out a blueprint for the study of invariant kernels on symmetric spaces, bringing forth specific harmonic analysis tools that suggest many future applications.）
</details></li>
</ul>
<hr>
<h2 id="A-Metadata-Driven-Approach-to-Understand-Graph-Neural-Networks"><a href="#A-Metadata-Driven-Approach-to-Understand-Graph-Neural-Networks" class="headerlink" title="A Metadata-Driven Approach to Understand Graph Neural Networks"></a>A Metadata-Driven Approach to Understand Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19263">http://arxiv.org/abs/2310.19263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting Wei Li, Qiaozhu Mei, Jiaqi Ma</li>
<li>For: This paper aims to understand the limitations of Graph Neural Networks (GNNs) and identify critical data properties that affect their performance.* Methods: The authors propose a metadata-driven approach to analyze the sensitivity of GNNs to graph data properties, using a multivariate sparse regression analysis on benchmarking data.* Results: The authors find that dataset degree distribution has a significant impact on GNN performance, with more balanced degree distributions leading to better linear separability of node representations and better GNN performance. Theoretical analysis and controlled experiments verify the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved remarkable success in various applications, but their performance can be sensitive to specific data properties of the graph datasets they operate on. Current literature on understanding the limitations of GNNs has primarily employed a $\textit{model-driven}$ approach that leverage heuristics and domain knowledge from network science or graph theory to model the GNN behaviors, which is time-consuming and highly subjective. In this work, we propose a $\textit{metadata-driven}$ approach to analyze the sensitivity of GNNs to graph data properties, motivated by the increasing availability of graph learning benchmarks. We perform a multivariate sparse regression analysis on the metadata derived from benchmarking GNN performance across diverse datasets, yielding a set of salient data properties. To validate the effectiveness of our data-driven approach, we focus on one identified data property, the degree distribution, and investigate how this property influences GNN performance through theoretical analysis and controlled experiments. Our theoretical findings reveal that datasets with more balanced degree distribution exhibit better linear separability of node representations, thus leading to better GNN performance. We also conduct controlled experiments using synthetic datasets with varying degree distributions, and the results align well with our theoretical findings. Collectively, both the theoretical analysis and controlled experiments verify that the proposed metadata-driven approach is effective in identifying critical data properties for GNNs.
</details>
<details>
<summary>摘要</summary>
GRAPHNeural Networks (GNNs) 已经取得了各种应用领域的出色成绩，但它们在具体的数据特性上的性能可能具有敏感性。现有文献中理解 GNN 的限制主要采用了一种 $\textit{模型驱动的}$ 方法，利用网络科学或图论中的euristic和专业知识来模型 GNN 的行为，这是时间consuming 和高度主观的。在这种工作中，我们提议一种 $\textit{metadata驱动的}$ 方法来分析 GNN 对图数据特性的敏感性， motivated by the increasing availability of graph learning benchmarks。我们通过对benchmarking GNN 性能的 metadata 进行多ivariate sparse regression分析，得到了一组突出的数据特性。为了证明我们的数据驱动方法的有效性，我们选择了一个标识的数据特性，即度分布，并通过理论分析和控制实验来研究该特性对 GNN 性能的影响。我们的理论发现表明，具有更平衡的度分布的数据集 exhibit better linear separability of node representations，从而导致更好的 GNN 性能。我们还通过使用 synthetic 数据集来进行控制实验，结果与我们的理论发现一致。总之， Both the theoretical analysis and controlled experiments verify that the proposed metadata-driven approach is effective in identifying critical data properties for GNNs.
</details></li>
</ul>
<hr>
<h2 id="Diversify-Conquer-Outcome-directed-Curriculum-RL-via-Out-of-Distribution-Disagreement"><a href="#Diversify-Conquer-Outcome-directed-Curriculum-RL-via-Out-of-Distribution-Disagreement" class="headerlink" title="Diversify &amp; Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement"></a>Diversify &amp; Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19261">http://arxiv.org/abs/2310.19261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daesol Cho, Seungjae Lee, H. Jin Kim</li>
<li>for: 本文提出了一种新的课程RL方法，叫做Diversify for Disagreement &amp; Conquer（D2C），用于解决RL面临的无知搜索问题。</li>
<li>methods: D2C方法只需很少的愿望结果示例，可以在任何环境中工作，不管环境的geometry或愿望结果示例的分布。该方法首先进行目标状态分类器的多样化，以确定与访问过的状态相似的状态，并确保状态分类器对非典型状态的识别不同，从而可以量化未探索区域和设计一个简单直观的目标条件内部奖励信号。然后，使用两分图匹配定义了一个课程学习目标，生成一个适应度较高的中间目标，使agent自动探索和征服未探索区域。</li>
<li>results: 实验结果表明，D2C方法在量和质上都超过了先前的课程RL方法，即使愿望结果示例的分布是随机的。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) often faces the challenges of uninformed search problems where the agent should explore without access to the domain knowledge such as characteristics of the environment or external rewards. To tackle these challenges, this work proposes a new approach for curriculum RL called Diversify for Disagreement & Conquer (D2C). Unlike previous curriculum learning methods, D2C requires only a few examples of desired outcomes and works in any environment, regardless of its geometry or the distribution of the desired outcome examples. The proposed method performs diversification of the goal-conditional classifiers to identify similarities between visited and desired outcome states and ensures that the classifiers disagree on states from out-of-distribution, which enables quantifying the unexplored region and designing an arbitrary goal-conditioned intrinsic reward signal in a simple and intuitive way. The proposed method then employs bipartite matching to define a curriculum learning objective that produces a sequence of well-adjusted intermediate goals, which enable the agent to automatically explore and conquer the unexplored region. We present experimental results demonstrating that D2C outperforms prior curriculum RL methods in both quantitative and qualitative aspects, even with the arbitrarily distributed desired outcome examples.
</details>
<details>
<summary>摘要</summary>
强化学习（RL）经常遇到无知搜索问题，agent需要无法访问环境特性或外部奖励的情况下探索。为解决这些挑战，这项工作提出了一种新的课程RL方法，即多样化为争议与征服（D2C）。与之前的课程学习方法不同，D2C只需要一些欲要的结果示例，并在任何环境中工作，无论环境的几何结构或欲要结果示例的分布。提出的方法首先进行了目标准备的多样化，以便在访问和欲要结果状态时确定相似性，并确保类ifiers在不同的状态下产生分歧，从而能够量化未探索的区域并设计一个简单直观的目标准备的自适应奖励信号。然后，提出的方法使用两部分匹配来定义一个课程学习目标，生成一系列适应度较高的中间目标，使得agent可以自动探索和征服未探索的区域。我们的实验结果表明，D2C在量和质量上都高于先前的课程RL方法，即使欲要结果示例的分布是随机的。
</details></li>
</ul>
<hr>
<h2 id="Flow-based-Distributionally-Robust-Optimization"><a href="#Flow-based-Distributionally-Robust-Optimization" class="headerlink" title="Flow-based Distributionally Robust Optimization"></a>Flow-based Distributionally Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19253">http://arxiv.org/abs/2310.19253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Xu, Jonghyeok Lee, Xiuyuan Cheng, Yao Xie</li>
<li>for: 解决流基于分布 robust优化（DRO）问题，使用 Wasserstein 不确定集，并需要最差情况分布（也称为最不利分布，LFD）是连续的，以便可以扩展到更大的样本大小和提高适应性。</li>
<li>methods: 使用流模型，连续时间可变Transport map，并开发 Wasserstein proximal梯度流算法来解决计算挑战。在实践中，Transport map 是通过一个序列 neural network 逐步训练的块式方式进行参数化。</li>
<li>results: 在对真实高维数据进行实验中，提出了一种新的机制来实现数据驱动分布扰动隐私，并在分布性检测和鲁棒化学习中显示了强емпириical性能。<details>
<summary>Abstract</summary>
We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various applications. We demonstrate its usage in adversarial learning, distributionally robust hypothesis testing, and a new mechanism for data-driven distribution perturbation differential privacy, where the proposed method gives strong empirical performance on real high-dimensional data.
</details>
<details>
<summary>摘要</summary>
我们提出一种计算效率高的框架，称为\texttt{FlowDRO}，用于解决基于流的分布式 robust优化（DRO）问题，其中最差情况分布（也称为最不利分布，LFD）需要是连续的，以便可以适用于更大的样本大小和更好的总体化能力。为了解决计算复杂的无穷维度优化问题，我们利用流模型，连续时间可逆运输映射 между数据分布和目标分布，并开发了 Wasserstein  proximal梯度流类型的算法。在实践中，我们归一化运输映射使用一个序列的神经网络，逐步在块内部使用梯度下降进行训练。我们的计算框架是通用的，可以处理高维数据和大样本大小，并可以用于多种应用。我们在抑制学习、分布式 robust测试和数据驱动分布泄漏隐私中展示了该方法的强制实际性。
</details></li>
</ul>
<hr>
<h2 id="Assessment-of-Differentially-Private-Synthetic-Data-for-Utility-and-Fairness-in-End-to-End-Machine-Learning-Pipelines-for-Tabular-Data"><a href="#Assessment-of-Differentially-Private-Synthetic-Data-for-Utility-and-Fairness-in-End-to-End-Machine-Learning-Pipelines-for-Tabular-Data" class="headerlink" title="Assessment of Differentially Private Synthetic Data for Utility and Fairness in End-to-End Machine Learning Pipelines for Tabular Data"></a>Assessment of Differentially Private Synthetic Data for Utility and Fairness in End-to-End Machine Learning Pipelines for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19250">http://arxiv.org/abs/2310.19250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayana Pereira, Meghana Kshirsagar, Sumit Mukherjee, Rahul Dodhia, Juan Lavista Ferres, Rafael de Sousa<br>for:This paper aims to investigate the use of differentially private synthetic data in end-to-end machine learning pipelines, specifically exploring the extent to which synthetic data can replace real, tabular data and identifying the most effective synthetic data generation techniques for training and evaluating machine learning models.methods:The authors use a training and evaluation framework that does not assume the availability of real data for testing the utility and fairness of machine learning models trained on synthetic data. They analyze several different definitions of fairness and compare the utility and fairness of models trained using marginal-based and GAN-based synthetic data generation algorithms.results:The authors find that marginal-based synthetic data generators outperform GAN-based ones in terms of model training utility for tabular data, and that models trained using data generated by the marginal-based algorithm MWEM PGM can achieve similar utility to models trained using real data. Additionally, the authors show that these models can also exhibit fairness characteristics similar to those obtained by models trained with real data.<details>
<summary>Abstract</summary>
Differentially private (DP) synthetic data sets are a solution for sharing data while preserving the privacy of individual data providers. Understanding the effects of utilizing DP synthetic data in end-to-end machine learning pipelines impacts areas such as health care and humanitarian action, where data is scarce and regulated by restrictive privacy laws. In this work, we investigate the extent to which synthetic data can replace real, tabular data in machine learning pipelines and identify the most effective synthetic data generation techniques for training and evaluating machine learning models. We investigate the impacts of differentially private synthetic data on downstream classification tasks from the point of view of utility as well as fairness. Our analysis is comprehensive and includes representatives of the two main types of synthetic data generation algorithms: marginal-based and GAN-based. To the best of our knowledge, our work is the first that: (i) proposes a training and evaluation framework that does not assume that real data is available for testing the utility and fairness of machine learning models trained on synthetic data; (ii) presents the most extensive analysis of synthetic data set generation algorithms in terms of utility and fairness when used for training machine learning models; and (iii) encompasses several different definitions of fairness. Our findings demonstrate that marginal-based synthetic data generators surpass GAN-based ones regarding model training utility for tabular data. Indeed, we show that models trained using data generated by marginal-based algorithms can exhibit similar utility to models trained using real data. Our analysis also reveals that the marginal-based synthetic data generator MWEM PGM can train models that simultaneously achieve utility and fairness characteristics close to those obtained by models trained with real data.
</details>
<details>
<summary>摘要</summary>
diferencialmente privado (DP) 的合成数据集是一种为保护个人数据提供者隐私而分享数据的解决方案。 我们 investigated the extent to which synthetic data can replace real, tabular data in machine learning pipelines and identified the most effective synthetic data generation techniques for training and evaluating machine learning models. 我们的分析包括代表两种主要的合成数据生成算法：marginal-based和GAN-based。在我们的研究中，我们首次：（i）提出了一个不假设实际数据可用于测试机器学习模型在合成数据上的实用性和公平性的训练和评估框架；（ii）对合成数据集生成算法进行了最广泛的分析，包括实用性和公平性在内的多种定义。我们的发现表明，marginal-based合成数据生成器在机器学习模型训练中的实用性比GAN-based更高。我们的分析还显示，使用marginal-based算法生成的数据可以让机器学习模型达到与实际数据训练模型相似的实用性特征。此外，我们发现MWEM PGM算法可以通过同时实现实用性和公平性特征来训练模型。
</details></li>
</ul>
<hr>
<h2 id="A-spectral-regularisation-framework-for-latent-variable-models-designed-for-single-channel-applications"><a href="#A-spectral-regularisation-framework-for-latent-variable-models-designed-for-single-channel-applications" class="headerlink" title="A spectral regularisation framework for latent variable models designed for single channel applications"></a>A spectral regularisation framework for latent variable models designed for single channel applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19246">http://arxiv.org/abs/2310.19246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Balshaw, P. Stephan Heyns, Daniel N. Wilke, Stephan Schmidt</li>
<li>for: 这篇论文旨在提供一个Python包，用于解决单通道隐变量模型（LVM）应用中的源重复问题。</li>
<li>methods: 该包使用一个新的спектральRegularization项来解决源重复问题，并提供了一个框架，用于在LVM优化过程中应用спектральRegularization。</li>
<li>results: 该包可以帮助 Investigate和使用LVMs with spectral regularization，并提供了一个一致的线性LVM优化框架，用于单通道时间序列应用。<details>
<summary>Abstract</summary>
Latent variable models (LVMs) are commonly used to capture the underlying dependencies, patterns, and hidden structure in observed data. Source duplication is a by-product of the data hankelisation pre-processing step common to single channel LVM applications, which hinders practical LVM utilisation. In this article, a Python package titled spectrally-regularised-LVMs is presented. The proposed package addresses the source duplication issue via the addition of a novel spectral regularisation term. This package provides a framework for spectral regularisation in single channel LVM applications, thereby making it easier to investigate and utilise LVMs with spectral regularisation. This is achieved via the use of symbolic or explicit representations of potential LVM objective functions which are incorporated into a framework that uses spectral regularisation during the LVM parameter estimation process. The objective of this package is to provide a consistent linear LVM optimisation framework which incorporates spectral regularisation and caters to single channel time-series applications.
</details>
<details>
<summary>摘要</summary>
封装化变量模型（LVM）通常用于捕捉观察数据中的下面依赖、模式和隐藏结构。数据块化是单通道LVM应用中的数据处理步骤中的一个副产品，它限制了实用LVM的使用。本文提出了一个名为“spectrally-regularized-LVMs”的Python包，该包解决了源重复问题，通过添加一个新的spectral regularization term。这个包提供了一个基于spectral regularization的单通道LVM应用程序框架，使得可以更容易地investigate和utilize LVMs with spectral regularization。这是通过使用符号或显式表示potential LVM目标函数，并将其包含到一个使用spectral regularization during LVM参数估计过程中的框架中来实现的。该包的目标是提供一个一致的线性LVM优化框架，该框架包含spectral regularization并且适用于单通道时间序列应用。
</details></li>
</ul>
<hr>
<h2 id="Maximum-Knowledge-Orthogonality-Reconstruction-with-Gradients-in-Federated-Learning"><a href="#Maximum-Knowledge-Orthogonality-Reconstruction-with-Gradients-in-Federated-Learning" class="headerlink" title="Maximum Knowledge Orthogonality Reconstruction with Gradients in Federated Learning"></a>Maximum Knowledge Orthogonality Reconstruction with Gradients in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19222">http://arxiv.org/abs/2310.19222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wfwf10/mkor">https://github.com/wfwf10/mkor</a></li>
<li>paper_authors: Feng Wang, Senem Velipasalar, M. Cenk Gursoy</li>
<li>for: 保护客户端数据隐私，防止泄露客户端数据。</li>
<li>methods: 使用最大知识正交重构（MKOR）方法，通过秘密地修改参数传递给客户端，从客户端的梯度更新中重构客户端的输入数据。</li>
<li>results: 对MNIST、CIFAR-100和ImageNet dataset进行评估，比对已有方法，MKOR方法可以重构高质量的输入图像，并且可以效率地和不察觉地从客户端的梯度更新中重构输入图像。<details>
<summary>Abstract</summary>
Federated learning (FL) aims at keeping client data local to preserve privacy. Instead of gathering the data itself, the server only collects aggregated gradient updates from clients. Following the popularity of FL, there has been considerable amount of work, revealing the vulnerability of FL approaches by reconstructing the input data from gradient updates. Yet, most existing works assume an FL setting with unrealistically small batch size, and have poor image quality when the batch size is large. Other works modify the neural network architectures or parameters to the point of being suspicious, and thus, can be detected by clients. Moreover, most of them can only reconstruct one sample input from a large batch. To address these limitations, we propose a novel and completely analytical approach, referred to as the maximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients' input data. Our proposed method reconstructs a mathematically proven high quality image from large batches. MKOR only requires the server to send secretly modified parameters to clients and can efficiently and inconspicuously reconstruct the input images from clients' gradient updates. We evaluate MKOR's performance on the MNIST, CIFAR-100, and ImageNet dataset and compare it with the state-of-the-art works. The results show that MKOR outperforms the existing approaches, and draws attention to a pressing need for further research on the privacy protection of FL so that comprehensive defense approaches can be developed.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 目的是保持客户端数据本地，以保持隐私。而而不是收集客户端数据本身，服务器只收集客户端上的聚合梯度更新。随着 Federated Learning 的流行，有很多工作揭示了 Federated Learning 方法的漏洞，可以从梯度更新中重建输入数据。然而，大多数现有工作假设了 Federated Learning 的 batch size 非常小，并且在 batch size 较大时图像质量不佳。其他工作可能会修改神经网络结构或参数，以至于可以被客户端探测。此外，大多数工作只能从大批量中重建一个输入数据。为解决这些限制，我们提出了一种新的、完全分析的方法，称为最大知识正交重建（MKOR），可以从客户端的梯度更新中重建输入数据。MKOR 可以高质量地重建大批量中的图像。MKOR 只需服务器在秘密地将修改后的参数发送给客户端，并且可以高效地、不显 Orts reconstruction 输入数据。我们对 MKOR 在 MNIST、CIFAR-100 和 ImageNet 数据集上进行了性能评估，并与现有的方法进行比较。结果显示，MKOR 超过了现有的方法，并吸引了关注于 Federated Learning 隐私保护的进一步研究，以开发全面的防御方法。
</details></li>
</ul>
<hr>
<h2 id="From-Stream-to-Pool-Dynamic-Pricing-Beyond-i-i-d-Arrivals"><a href="#From-Stream-to-Pool-Dynamic-Pricing-Beyond-i-i-d-Arrivals" class="headerlink" title="From Stream to Pool: Dynamic Pricing Beyond i.i.d. Arrivals"></a>From Stream to Pool: Dynamic Pricing Beyond i.i.d. Arrivals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19220">http://arxiv.org/abs/2310.19220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Titing Cui, Su Jia, Thomas Lavastida</li>
<li>for:  This paper focuses on the dynamic pricing problem, specifically addressing the issue of high-valuation customers leaving the market early and causing a shift in the valuation distribution.</li>
<li>methods:  The authors propose a minimax optimal algorithm that computes a non-adaptive policy to guarantee a $1&#x2F;k$ fraction of the optimal revenue, given any set of $k$ prices. Additionally, they present an adaptive learn-then-earn policy based on a novel debiasing approach.</li>
<li>results:  The authors prove an $\tilde O(kn^{3&#x2F;4})$ regret bound for the adaptive policy, and further improve the bound to $\tilde O(k^{3&#x2F;4} n^{3&#x2F;4})$ using martingale concentration inequalities.<details>
<summary>Abstract</summary>
The dynamic pricing problem has been extensively studied under the \textbf{stream} model: A stream of customers arrives sequentially, each with an independently and identically distributed valuation. However, this formulation is not entirely reflective of the real world. In many scenarios, high-valuation customers tend to make purchases earlier and leave the market, leading to a \emph{shift} in the valuation distribution. Thus motivated, we consider a model where a \textbf{pool} of $n$ non-strategic unit-demand customers interact repeatedly with the seller. Each customer monitors the price intermittently according to an independent Poisson process and makes a purchase if the observed price is lower than her \emph{private} valuation, whereupon she leaves the market permanently. We present a minimax \emph{optimal} algorithm that efficiently computes a non-adaptive policy which guarantees a $1/k$ fraction of the optimal revenue, given any set of $k$ prices. Moreover, we present an adaptive \emph{learn-then-earn} policy based on a novel \emph{debiasing} approach, and prove an $\tilde O(kn^{3/4})$ regret bound. We further improve the bound to $\tilde O(k^{3/4} n^{3/4})$ using martingale concentration inequalities.
</details>
<details>
<summary>摘要</summary>
“流动价格问题已经得到了广泛的研究，以流动客户为例：每个客户独立且相同的评价会随机出现。但这个模型不完全反映现实情况：高评价客户往往在早期购买并离开市场，导致价格分布的变化。因此，我们考虑了一个具有$n$名不策略性单位需求客户的集合，这些客户可以重复地与价格发展商互动。每个客户按照独立的波尔兹数 процес监控价格，如果观察到的价格低于其私人评价，就会购买产品并永久离开市场。我们提出了一个最佳算法，可以效率地计算一个不适应性政策， garantua 1/k  Fraction of the 最佳收益， given any set of k prices。此外，我们还提出了一个基于新的传播措施的学习然后获利策略，并证明了 $\tilde O(kn^{3/4})$ 的后悔 bound。 finally，我们使用 martingale concentration inequalities 提高 bound to $\tilde O(k^{3/4} n^{3/4})$。”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Federated-Unlearning-A-Taxonomy-Challenges-and-Future-Directions"><a href="#A-Survey-of-Federated-Unlearning-A-Taxonomy-Challenges-and-Future-Directions" class="headerlink" title="A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions"></a>A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19218">http://arxiv.org/abs/2310.19218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Yang, Yang Zhao</li>
<li>for: 本文提出了一种 Federated Unlearning (FU) 的概述，强调了在 Federated Learning (FL) 环境下实现 Right to be Forgotten (RTBF) 的挑战。</li>
<li>methods: 本文评论了现有的 FU 算法、目标函数和评价指标，并将它们分类为不同的方案、应用场景和未来发展方向。</li>
<li>results: 本文通过对一些研究的回顾和比较，总结了它们的特点和优劣点，并提出了未来研究的可能性和挑战。<details>
<summary>Abstract</summary>
With the development of trustworthy Federated Learning (FL), the requirement of implementing right to be forgotten gives rise to the area of Federated Unlearning (FU). Comparing to machine unlearning, a major challenge of FU lies in the decentralized and privacy-preserving nature of FL, in which clients jointly train a global model without sharing their raw data, making it substantially more intricate to selectively unlearn specific information. In that regard, many efforts have been made to tackle the challenges of FU and have achieved significant progress. In this paper, we present a comprehensive survey of FU. Specially, we provide the existing algorithms, objectives, evaluation metrics, and identify some challenges of FU. By reviewing and comparing some studies, we summarize them into a taxonomy for various schemes, potential applications and future directions.
</details>
<details>
<summary>摘要</summary>
随着可靠的 Federated Learning (FL) 的发展，实现“忘记权”的要求给出了 Federated Unlearning (FU) 领域的挑战。与机器解启相比，FU 的主要挑战在于 Federated Learning 的分布式和隐私保护特性， clients 在无需分享原始数据的情况下集成全球模型，使其 SELECTIVE 地忘记特定信息变得非常复杂。为此，许多努力已经被作出，并取得了显著进步。在这篇论文中，我们提供了 FU 的全面报告，包括现有的算法、目标、评价指标，并对一些研究进行了概要总结和比较，将其分类为不同的方案、应用场景和未来方向。
</details></li>
</ul>
<hr>
<h2 id="On-the-accuracy-and-efficiency-of-group-wise-clipping-in-differentially-private-optimization"><a href="#On-the-accuracy-and-efficiency-of-group-wise-clipping-in-differentially-private-optimization" class="headerlink" title="On the accuracy and efficiency of group-wise clipping in differentially private optimization"></a>On the accuracy and efficiency of group-wise clipping in differentially private optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19215">http://arxiv.org/abs/2310.19215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqi Bu, Ruixuan Liu, Yu-Xiang Wang, Sheng Zha, George Karypis</li>
<li>for: 该论文主要针对具有数百万到数十亿参数的深度学习模型中的权限保护（Differentially Private，DP）优化问题。</li>
<li>methods: 该论文主要研究了分割样本的梯度抑制方式，即在DP优化中的核心组件。研究发现，不同的梯度抑制方式具有相同的时间复杂度，但实际实现了精度-存储空间之间的质量负担。</li>
<li>results: 研究显示，相比总层梯度抑制，分层梯度抑制可以更好地实现高精度和低峰存储之间的平衡。此外，对大型模型进行DP优化，使用分层梯度抑制可以达到高精度和低峰存储同时。<details>
<summary>Abstract</summary>
Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Factor-Fitting-Rank-Allocation-and-Partitioning-in-Multilevel-Low-Rank-Matrices"><a href="#Factor-Fitting-Rank-Allocation-and-Partitioning-in-Multilevel-Low-Rank-Matrices" class="headerlink" title="Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank Matrices"></a>Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19214">http://arxiv.org/abs/2310.19214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cvxgrp/mlr_fitting">https://github.com/cvxgrp/mlr_fitting</a></li>
<li>paper_authors: Tetiana Parshakova, Trevor Hastie, Eric Darve, Stephen Boyd</li>
<li>for: 这篇论文是关于多层低级（MLR）矩阵的研究，MLR矩阵是一种ROW和列Permutation的积集，每个积是一个块分解的改进版本，所有块都是低级矩阵。</li>
<li>methods: 这篇论文提出了三个问题，即因子适应、级别分配和层次分解。对于这些问题，论文提出了一些解决方案。</li>
<li>results: 论文的结果显示，使用提出的方法可以减少积分矩阵的存储空间和计算复杂度，同时保持积分矩阵的性质。此外，论文还附上了一个开源包，实现了提出的方法。<details>
<summary>Abstract</summary>
We consider multilevel low rank (MLR) matrices, defined as a row and column permutation of a sum of matrices, each one a block diagonal refinement of the previous one, with all blocks low rank given in factored form. MLR matrices extend low rank matrices but share many of their properties, such as the total storage required and complexity of matrix-vector multiplication. We address three problems that arise in fitting a given matrix by an MLR matrix in the Frobenius norm. The first problem is factor fitting, where we adjust the factors of the MLR matrix. The second is rank allocation, where we choose the ranks of the blocks in each level, subject to the total rank having a given value, which preserves the total storage needed for the MLR matrix. The final problem is to choose the hierarchical partition of rows and columns, along with the ranks and factors. This paper is accompanied by an open source package that implements the proposed methods.
</details>
<details>
<summary>摘要</summary>
我们考虑多层低阶（MLR）矩阵，定义为一行和列排序的一个总和矩阵的各个矩阵，每个矩阵都是前一个矩阵的对角线均分划分，所有块都是低阶矩阵的实际形式。 MLR 矩阵扩展了低阶矩阵，但与其属性相似，例如总储存需求和矩阵-向量乘法的复杂度。我们处理三个在适用给一个矩阵的 MLR 矩阵的问题：1. 因数适应（factor fitting）：我们调整 MLR 矩阵的因数。2. 权重分配（rank allocation）：我们选择每个层的块的权重，以保持总权重的给定值，并保持 MLR 矩阵的总储存需求不变。3. 垂直分解和因数选择（hierarchical partition and factor selection）：我们选择行和列的垂直分解，以及每个层的因数。这篇文章附加了一个开源套件，实现了我们提议的方法。
</details></li>
</ul>
<hr>
<h2 id="Investigative-Pattern-Detection-Framework-for-Counterterrorism"><a href="#Investigative-Pattern-Detection-Framework-for-Counterterrorism" class="headerlink" title="Investigative Pattern Detection Framework for Counterterrorism"></a>Investigative Pattern Detection Framework for Counterterrorism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19211">http://arxiv.org/abs/2310.19211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashika R. Muramudalige, Benjamin W. K. Hung, Rosanne Libretti, Jytte Klausen, Anura P. Jayasumana</li>
<li>for: 预防暴力激进分子的袭击，保障公众安全。</li>
<li>methods: 使用自动化工具提取信息，回答分析员的问题，不断扫描新信息，与过去事件集成，检测出emerging threats。</li>
<li>results: 开发了一套Investigative Pattern Detection Framework for Counterterrorism（INSPECT），可以自动检测行为指标和风险oprofile&#x2F;组，并自动完成大规模的审查和检索工作。INSPECT已经在domestic jihadism dataset上验证和评估。<details>
<summary>Abstract</summary>
Law-enforcement investigations aimed at preventing attacks by violent extremists have become increasingly important for public safety. The problem is exacerbated by the massive data volumes that need to be scanned to identify complex behaviors of extremists and groups. Automated tools are required to extract information to respond queries from analysts, continually scan new information, integrate them with past events, and then alert about emerging threats. We address challenges in investigative pattern detection and develop an Investigative Pattern Detection Framework for Counterterrorism (INSPECT). The framework integrates numerous computing tools that include machine learning techniques to identify behavioral indicators and graph pattern matching techniques to detect risk profiles/groups. INSPECT also automates multiple tasks for large-scale mining of detailed forensic biographies, forming knowledge networks, and querying for behavioral indicators and radicalization trajectories. INSPECT targets human-in-the-loop mode of investigative search and has been validated and evaluated using an evolving dataset on domestic jihadism.
</details>
<details>
<summary>摘要</summary>
法警调查措施为保障公共安全而日益重要，尤其是针对暴力激进分子的袭击。问题在于巨量数据的检索和分析，以找出激进分子和团体的复杂行为特征。为此，我们提出了一套Investigative Pattern Detection Framework for Counterterrorism（INSPECT），它将多种计算工具集成，包括机器学习技术和图Pattern matching技术，以识别行为指标和风险分布。INSPECT还自动化了大规模的审批细致生物图文搜索、知识网络建立和行为指标和激进化轨迹查询。INSPECT采用人类在循环模式的调查搜索方式，并已经验证和评估了逐渐增长的 dataset on domestic jihadism。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.LG_2023_10_30/" data-id="cloimipdg00sys488bt13az3w" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/eess.SP_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T08:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/eess.SP_2023_10_30/">eess.SP - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analytical-Nonlinear-Distortion-Characterization-for-Frequency-Selective-Massive-MIMO-Channels"><a href="#Analytical-Nonlinear-Distortion-Characterization-for-Frequency-Selective-Massive-MIMO-Channels" class="headerlink" title="Analytical Nonlinear Distortion Characterization for Frequency-Selective Massive MIMO Channels"></a>Analytical Nonlinear Distortion Characterization for Frequency-Selective Massive MIMO Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20038">http://arxiv.org/abs/2310.20038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murat Babek Salman, Emil Björnson, Gokhan Muzaffer Guvensen, Tolga Ciloglu</li>
<li>for:  investigate the impact of frequency selectivity on nonlinear distortion in wireless communication systems</li>
<li>methods: closed-form expression for received distortion power as a function of number of multipath components (MPCs) and delay spread</li>
<li>results: in-band and OOB distortion power is inversely proportional to the number of MPCs, and the in-band distortion power is beamformed towards the intended userHere’s the summary in Traditional Chinese:</li>
<li>for: 研究无线通信系统中频率选择性对不对称干扰的影响</li>
<li>methods: 使用关键表达式来表示受到多普通道（MPC）和延迟幅度的受到干扰力</li>
<li>results: 对于不同频率域的干扰力，对于MPC的数量有直接的关系，并且在延迟幅度变窄时，对于指定用户的干扰力会增加。<details>
<summary>Abstract</summary>
Nonlinear distortion stemming from low-cost power amplifiers may severely affect wireless communication performance through out-of-band (OOB) radiation and in-band distortion. The distortion is correlated between different transmit antennas in an antenna array, which results in a beamforming gain at the receiver side that grows with the number of antennas. In this paper, we investigate how the strength of the distortion is affected by the frequency selectivity of the channel. A closed-form expression for the received distortion power is derived as a function of the number of multipath components (MPCs) and the delay spread, which highlight their impact. The performed analysis, which is verified via numerical simulations, reveals that as the number of MPCs increases, distortion exhibits distinct characteristics for in-band and OOB frequencies. It is shown that the received in-band and OOB distortion power is inversely proportional to the number of MPCs, and it is reported that as the delay spread gets narrower, the in-band distortion power is beamformed towards the intended user, which yields higher received in-band distortion compared to the OOB distortion.
</details>
<details>
<summary>摘要</summary>
非线性扭曲由低成本功率增强器引起的无线通信性能可能严重受到影响，主要通过射频外带（OOB）辐射和射频扭曲引起。这种扭曲与不同的发射天线之间存在相关性，导致发射天线阵列中的扭曲增强。在这篇论文中，我们研究了频率选择性通道的影响于扭曲强度。我们 derivated一个关于数目多path component（MPC）和延迟跨度的关键表达式，这些表达式阐明了他们对扭曲强度的影响。我们的分析，通过数值仿真验证，显示了随着MPC的增加，扭曲展现出明显的特征。 Specifically, we find that the received in-band and OOB distortion power is inversely proportional to the number of MPCs, and as the delay spread narrows, the in-band distortion power is beamformed towards the intended user, which yields higher received in-band distortion compared to the OOB distortion.Here's the translation in Traditional Chinese:非线性扭曲由低成本功率增强器引起的无线通信性能可能严重受到影响，主要通过射频外带（OOB）辐射和射频扭曲引起。这种扭曲与不同的发射天线之间存在相关性，导致发射天线阵列中的扭曲增强。在这篇论文中，我们研究了频率选择性通道的影响于扭曲强度。我们 derivated一个关于数目多path component（MPC）和延迟跨度的关键表达式，这些表达式阐明了他们对扭曲强度的影响。我们的分析，通过数值仿真验证，显示了随着MPC的增加，扭曲展现出明显的特征。 Specifically, we find that the received in-band and OOB distortion power is inversely proportional to the number of MPCs, and as the delay spread narrows, the in-band distortion power is beamformed towards the intended user, which yields higher received in-band distortion compared to the OOB distortion.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Enabled-Text-Semantic-Communication-under-Interference-An-Empirical-Study"><a href="#Deep-Learning-Enabled-Text-Semantic-Communication-under-Interference-An-Empirical-Study" class="headerlink" title="Deep Learning-Enabled Text Semantic Communication under Interference: An Empirical Study"></a>Deep Learning-Enabled Text Semantic Communication under Interference: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19974">http://arxiv.org/abs/2310.19974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tilahun M. Getu, Georges Kaddoum, Mehdi Bennis</li>
<li>for: 这篇论文旨在探讨6G时代，使用深度学习（DL）和自然语言处理（NLP）技术实现文本semantic communication（SemCom）的可靠性和可持续性。</li>
<li>methods: 这篇论文使用了DeepSC方法，利用DL和NLP技术来在低信号噪响（SNR）下传输semantic信息。</li>
<li>results: 测试结果表明，当 Gaussian 多重干扰（RFI）的数量很大时，DeepSC方法会生成无关 semantic 句子。因此, 为了实现6G中的可靠性和可持续性，需要开发一种基于IR$^2$ SemCom的设计方案。<details>
<summary>Abstract</summary>
At the confluence of 6G, deep learning (DL), and natural language processing (NLP), DL-enabled text semantic communication (SemCom) has emerged as a 6G enabler by promising to minimize bandwidth consumption, transmission delay, and power usage. Among text SemCom techniques, \textit{DeepSC} is a popular scheme that leverages advancements in DL and NLP to reliably transmit semantic information in low signal-to-noise ratio (SNR) regimes. To understand the fundamental limits of such a transmission paradigm, our recently developed theory \cite{Getu'23_Performance_Limits} predicted the performance limits of DeepSC under radio frequency interference (RFI). Although these limits were corroborated by simulations, trained deep networks can defy classical statistical wisdom, and hence extensive computer experiments are needed to validate our theory. Accordingly, this empirical work follows concerning the training and testing of DeepSC using the proceedings of the European Parliament (Europarl) dataset. Employing training, validation, and testing sets \textit{tokenized and vectorized} from Europarl, we train the DeepSC architecture in Keras 2.9 with TensorFlow 2.9 as a backend and test it under Gaussian multi-interferer RFI received over Rayleigh fading channels. Validating our theory, the testing results corroborate that DeepSC produces semantically irrelevant sentences as the number of Gaussian RFI emitters gets very large. Therefore, a fundamental 6G design paradigm for \textit{interference-resistant and robust SemCom} (IR$^2$ SemCom) is needed.
</details>
<details>
<summary>摘要</summary>
在6G、深度学习（DL）和自然语言处理（NLP）的交叉点上，DL启用的文本semantic communication（SemCom）已经成为6G的推动者，承诺可以减少带宽消耗、传输延迟和功率使用。 among text SemCom技术，\textit{DeepSC} 是一种受欢迎的方案，利用了深度学习和NLP的进步来可靠地在低信号噪响（SNR）的情况下传输semantic信息。为了理解这种传输模式的基本限制，我们最近提出的理论 \cite{Getu'23_Performance_Limits} 预测了DeepSC在电磁干扰（RFI）下的性能限制。虽然这些限制得到了 simulations 的 corroboration，但是训练过的深度网络可能会违背经典统计知识，因此需要广泛的计算实验来验证我们的理论。因此，这项实验关注了使用 Keras 2.9 和 TensorFlow 2.9 作为后端，使用 Евро Parlament（Europarl）数据集进行训练和测试 DeepSC 架构。在 Gaussian 多源干扰 RFI 下接收 Rayleigh 抖动频道上测试 DeepSC，结果证明了我们的理论。因此，为了实现 \textit{干扰抗性和 Robust SemCom}（IR$^2$ SemCom）的6G设计方针，需要进一步研究和开发。
</details></li>
</ul>
<hr>
<h2 id="Transmission-line-condition-prediction-based-on-semi-supervised-learning"><a href="#Transmission-line-condition-prediction-based-on-semi-supervised-learning" class="headerlink" title="Transmission line condition prediction based on semi-supervised learning"></a>Transmission line condition prediction based on semi-supervised learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19756">http://arxiv.org/abs/2310.19756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sizhe Li, Xun Ma, Nan Liu, Yi Jin</li>
<li>for: 这 paper 的目的是提出一种基于半supervised learning的传输线路状态预测方法，以解决现有模型无法考虑机器稳定性和数据需求的问题。</li>
<li>methods: 这 paper 使用了扩展特征向量、正则矩阵和表示学习来解决填充缺失数据和稀疏编码问题。然后，通过一些标注样本初步确定了不同缺陷状态的线段分类中心。最后，使用了无标注样本来更正估算模型的参数。</li>
<li>results: 例分析表明，这种方法可以提高认知精度和更有效地使用数据，比现有模型更好。<details>
<summary>Abstract</summary>
Transmission line state assessment and prediction are of great significance for the rational formulation of operation and maintenance strategy and improvement of operation and maintenance level. Aiming at the problem that existing models cannot take into account the robustness and data demand, this paper proposes a state prediction method based on semi-supervised learning. Firstly, for the expanded feature vector, the regular matrix is used to fill in the missing data, and the sparse coding problem is solved by representation learning. Then, with the help of a small number of labelled samples to initially determine the category centers of line segments in different defective states. Finally, the estimated parameters of the model are corrected using unlabeled samples. Example analysis shows that this method can improve the recognition accuracy and use data more efficiently than the existing models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对 transmission line 的状态评估和预测是操作和维护战略的重要因素，可以提高操作和维护水平。现有模型无法考虑系统稳定性和数据需求，这篇论文提出了基于半监督学习的状态预测方法。首先，为扩展特征向量，使用常量矩阵填充缺失数据，并通过表示学习解决稀缺编码问题。然后，使用一小部分标注样本初始化不同缺陷状态的线段类中心，并使用无标注样本修正估计模型参数。示例分析表明，这种方法可以提高识别精度和更好地利用数据。Note: "transmission line" is 电力传输线 in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Increased-Multiplexing-Gain-with-Reconfigurable-Surfaces-Simultaneous-Channel-Orthogonalization-and-Information-Embedding"><a href="#Increased-Multiplexing-Gain-with-Reconfigurable-Surfaces-Simultaneous-Channel-Orthogonalization-and-Information-Embedding" class="headerlink" title="Increased Multiplexing Gain with Reconfigurable Surfaces: Simultaneous Channel Orthogonalization and Information Embedding"></a>Increased Multiplexing Gain with Reconfigurable Surfaces: Simultaneous Channel Orthogonalization and Information Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19409">http://arxiv.org/abs/2310.19409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Vidal Alegria, Joao Vieira, Fredrik Rusek</li>
<li>for: 这篇论文主要目的是研究用于提高无线通信链路的可重新配置表面技术。</li>
<li>methods: 论文使用了批量智能表面（BIS）和完全可重新配置智能表面（FRIS）技术来实现MU-MIMO通道的完美正交。</li>
<li>results: 研究结果表明，使用BIS和FRIS技术可以实现MU-MIMO系统中多个antenna的完全利用，并且可以在系统中嵌入额外信息以提高传输率。<details>
<summary>Abstract</summary>
Reconfigurable surface (RS) has been shown to be an effective solution for improving wireless communication links in general multi-user multiple-input multiple-output (MU-MIMO) setting. Current research efforts have been largely directed towards the study of reconfigurable intelligent surface (RIS), which corresponds to an RS made of passive reconfigurable elements with only phase shifting capabilities. RIS constitutes a cost- and energy- efficient solution for increased beamforming gain since it allows to generate constructive interference towards desired directions, e.g., towards a base station (BS). However, in many situations, multiplexing gain may have greater impact on the achievable transmission rates and number of simultaneously connected devices, while RIS has only been able to achieve minor improvements in this aspect. Recent work has proposed the use of alternative RS technologies, namely amplitude-reconfigurable intelligent surface (ARIS) and fully-reconfigurable intelligent surface (FRIS), to achieve perfect orthogonalization of MU-MIMO channels, thus allowing for maximum multiplexing gain at reduced complexity. In this work we consider the use of ARIS and FRIS for simultaneously orthogonalizing a MU-MIMO channel, while embedding extra information in the orthogonalized channel. We show that the resulting achievable rates allow for full exploitation of the degrees of freedom in a MU-MIMO system with excess of BS antennas.
</details>
<details>
<summary>摘要</summary>
响应式表面（RS）已经被证明可以提高无线通信链路的性能，特别是在多用户多输入多输出（MU-MIMO）设置下。当前的研究努力主要集中在研究响应式智能表面（RIS），这是一种具有只能进行相位调整的pasive响应式元件的RS。RIS可以具有更高的增幅功率和更多的连接数，但是在许多情况下，多重化增益可能更大地影响可达的传输率和同时连接的设备数量。最近的工作提议使用另一种RS技术，即幅度调整的智能表面（ARIS）和完全调整的智能表面（FRIS），以实现MU-MIMO频道的完美正交，从而实现最大的多重化增益，同时减少复杂度。在这个工作中，我们考虑使用ARIS和FRIS同时正交MU-MIMO频道，并嵌入额外信息在正交后的频道中。我们发现，所得到的可达率允许在BSantenna数量超出的MU-MIMO系统中完全利用多个自由度。
</details></li>
</ul>
<hr>
<h2 id="A-Low-Complexity-Machine-Learning-Design-for-mmWave-Beam-Prediction"><a href="#A-Low-Complexity-Machine-Learning-Design-for-mmWave-Beam-Prediction" class="headerlink" title="A Low-Complexity Machine Learning Design for mmWave Beam Prediction"></a>A Low-Complexity Machine Learning Design for mmWave Beam Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19323">http://arxiv.org/abs/2310.19323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Qurratulain Khan, Abdo Gaber, Mohammad Parvini, Philipp Schulz, Gerhard Fettweis</li>
<li>for: 本研究旨在提高5G-Advanced New Radio（NR）空间频域 beam prediction的低复杂度机器学习设计，以减少电力消耗和参考讯号 overhead。</li>
<li>methods: 本研究使用低复杂度机器学习设计来优化空间频域 beam prediction，以减少电力消耗和参考讯号 overhead。</li>
<li>results: 本研究显示，提案的模型可以实现state-of-the-art的准确性，并且降低计算复杂度，实现减少电力消耗和快速的 beam prediction。 I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
The 3rd Generation Partnership Project (3GPP) is currently studying machine learning (ML) for the fifth generation (5G)-Advanced New Radio (NR) air interface, where spatial and temporal-domain beam prediction are important use cases. With this background, this letter presents a low-complexity ML design that expedites the spatial-domain beam prediction to reduce the power consumption and the reference signaling overhead, which are currently imperative for frequent beam measurements. Complexity analysis and evaluation results showcase that the proposed model achieves state-of-the-art accuracy with lower computational complexity, resulting in reduced power consumption and faster beam prediction. Furthermore, important observations on the generalization of the proposed model are presented in this letter.
</details>
<details>
<summary>摘要</summary>
现在3rd Generation Partnership Project（3GPP）正在研究 fifth generation（5G）Advanced New Radio（NR）空间域 beam prediction的机器学习（ML）应用，其中空间域和时间域 beam prediction是重要的应用场景。在这种背景下，本封信函数 presenta a low-complexity ML design that accelerates the spatial-domain beam prediction to reduce power consumption and reference signaling overhead, which are currently critical for frequent beam measurements. 复杂性分析和评估结果显示，提案的模型可以实现当前最佳准确率，同时具有较低的计算复杂性，从而降低电力消耗和快速的 beam prediction。此外，本函数还对提案模型的总体化进行了重要的观察。
</details></li>
</ul>
<hr>
<h2 id="A-Synopsis-of-Stent-Graft-Technology-Development"><a href="#A-Synopsis-of-Stent-Graft-Technology-Development" class="headerlink" title="A Synopsis of Stent Graft Technology Development"></a>A Synopsis of Stent Graft Technology Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19235">http://arxiv.org/abs/2310.19235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Umme Hafsa Momy</li>
<li>for:  This paper reviews the evolution of coronary stent technology and its impact on patient care.</li>
<li>methods:  The paper discusses the development of various stent types, including bare metal stents (BMS), first-generation drug-eluting stents (DES), and second-generation DES and bioresorbable vascular scaffolds (BVS). Clinical trials have been crucial in validating each stent’s effectiveness.</li>
<li>results:  The paper highlights the progress made in stent technology, but also acknowledges ongoing challenges in stent selection, approval processes, and minimizing risks. Despite these challenges, the future may see personalized stenting based on patient needs.<details>
<summary>Abstract</summary>
Coronary artery disease (CAD) is a leading cause of death worldwide. Treatments have evolved, with stenting becoming the primary approach over bypass surgery. This article reviews the evolution of coronary stent technology, starting from the first angioplasty in 1977. Pioneers like Forssmann, Dotter, and Gruentzig established the foundation. The late 1980s saw the introduction of bare metal stents (BMS) to address angioplasty limitations. However, BMS had issues, leading to the development of first-generation drug-eluting stents (DES) in the early 2000s, which reduced restenosis but had safety concerns. Subsequent innovations introduced second-generation DES with better results and the latest bioresorbable vascular scaffolds (BVS) that dissolve over time. Clinical trials have been crucial in validating each stent's effectiveness. Despite progress, challenges remain in stent selection, approval processes, and minimizing risks. The future may see personalized stenting based on patient needs, highlighting the significant advancements in stent technology and its impact on patient care.
</details>
<details>
<summary>摘要</summary>
心血管疾病（CAD）是全球最主要的死亡原因之一。治疗方法不断演化，自然硬着附加成为主要方法，而不是通过环路手术。本文将评论心血管镜仪技术的演化，从1977年的首次抗生物治疗开始。先驱们如福斯曼、多特和格劳恩茨基础设置了基础。1980年代中期，无质量镜仪（BMS）被引入，以解决抗生物治疗的局限性。然而，BMS存在问题，导致了第一代药粉镜仪（DES）的开发，它可以减少再生长，但存在安全问题。随后的创新引入了第二代DES，并且最新的生物逐渐消失的血管支架（BVS），它们在时间上逐渐消失。临床试验对每种镜仪的效果进行了验证。尽管进步了，但是镜仪选择、批准过程和降低风险仍然是挑战。未来可能会出现个性化镜仪，这 highlights the significant advancements in stent technology and its impact on patient care.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Status-Updates-for-Minimizing-Age-of-Correlated-Information-in-IoT-Networks-with-Energy-Harvesting-Sensors"><a href="#Optimal-Status-Updates-for-Minimizing-Age-of-Correlated-Information-in-IoT-Networks-with-Energy-Harvesting-Sensors" class="headerlink" title="Optimal Status Updates for Minimizing Age of Correlated Information in IoT Networks with Energy Harvesting Sensors"></a>Optimal Status Updates for Minimizing Age of Correlated Information in IoT Networks with Energy Harvesting Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19216">http://arxiv.org/abs/2310.19216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Xu, Xinyan Zhang, Howard H. Yang, Xijun Wang, Nikolaos Pappas, Dusit Niyato, Tony Q. S. Quek</li>
<li>for: 本研究旨在设计有效地处理相关信息的实时应用程序，特别是在互联网物联网（IoT）中多感器生成的协调信息处理方面。</li>
<li>methods: 本文使用了深度学习扮演评价器（DRL）和长短期记忆（LSTM）技术来解决 correlate 信息处理中的环境动态、感知器电池状态不可见、能量使用 causality 和大规模离散动作空间等挑战。</li>
<li>results: 根据广泛的 simulations  validate ，我们提出的方法可以减少 Age of Correlated Information（AoCI），并且比较有效地处理相关信息。<details>
<summary>Abstract</summary>
Many real-time applications of the Internet of Things (IoT) need to deal with correlated information generated by multiple sensors. The design of efficient status update strategies that minimize the Age of Correlated Information (AoCI) is a key factor. In this paper, we consider an IoT network consisting of sensors equipped with the energy harvesting (EH) capability. We optimize the average AoCI at the data fusion center (DFC) by appropriately managing the energy harvested by sensors, whose true battery states are unobservable during the decision-making process. Particularly, we first formulate the dynamic status update procedure as a partially observable Markov decision process (POMDP), where the environmental dynamics are unknown to the DFC. In order to address the challenges arising from the causality of energy usage, unknown environmental dynamics, unobservability of sensors'true battery states, and large-scale discrete action space, we devise a deep reinforcement learning (DRL)-based dynamic status update algorithm. The algorithm leverages the advantages of the soft actor-critic and long short-term memory techniques. Meanwhile, it incorporates our proposed action decomposition and mapping mechanism. Extensive simulations are conducted to validate the effectiveness of our proposed algorithm by comparing it with available DRL algorithms for POMDPs.
</details>
<details>
<summary>摘要</summary>
Specifically, we first formulate the dynamic status update procedure as a partially observable Markov decision process (POMDP), where the environmental dynamics are unknown to the DFC. To address the challenges arising from the causality of energy usage, unknown environmental dynamics, unobservability of sensors'true battery states, and large-scale discrete action space, we propose a deep reinforcement learning (DRL)-based dynamic status update algorithm. The algorithm leverages the advantages of soft actor-critic and long short-term memory techniques. At the same time, it incorporates our proposed action decomposition and mapping mechanism. Extensive simulations are conducted to validate the effectiveness of our proposed algorithm by comparing it with available DRL algorithms for POMDPs.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/eess.SP_2023_10_30/" data-id="cloimipjw01ags488amj61sb8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.SD_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T15:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.SD_2023_10_29/">cs.SD - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Audio-Analyzer-a-Framework-to-Industrialize-the-Research-on-Audio-Forensics"><a href="#Deep-Audio-Analyzer-a-Framework-to-Industrialize-the-Research-on-Audio-Forensics" class="headerlink" title="Deep Audio Analyzer: a Framework to Industrialize the Research on Audio Forensics"></a>Deep Audio Analyzer: a Framework to Industrialize the Research on Audio Forensics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19081">http://arxiv.org/abs/2310.19081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valerio Francesco Puglisi, Oliver Giudice, Sebastiano Battiato</li>
<li>for: 这篇论文是为了提高音频掌控领域的研究和开发过程的简化和加速，使用户可以快速创建、比较和共享结果。</li>
<li>methods: 该论文描述了一种核心架构，用于支持多个音频分析任务，包括音频特征视图、预训练模型评估、新Audio分析工作流程创建等功能。</li>
<li>results: 通过使用Deep Audio Analyzer工具，法律 enforcement 机构和研究人员可以轻松地评估预训练模型的性能、创建新的Audio分析工作流程，并将其导出和分享。这些功能将提高音频分析实验室的速度和可重复性。<details>
<summary>Abstract</summary>
Deep Audio Analyzer is an open source speech framework that aims to simplify the research and the development process of neural speech processing pipelines, allowing users to conceive, compare and share results in a fast and reproducible way. This paper describes the core architecture designed to support several tasks of common interest in the audio forensics field, showing possibility of creating new tasks thus customizing the framework. By means of Deep Audio Analyzer, forensics examiners (i.e. from Law Enforcement Agencies) and researchers will be able to visualize audio features, easily evaluate performances on pretrained models, to create, export and share new audio analysis workflows by combining deep neural network models with few clicks. One of the advantages of this tool is to speed up research and practical experimentation, in the field of audio forensics analysis thus also improving experimental reproducibility by exporting and sharing pipelines. All features are developed in modules accessible by the user through a Graphic User Interface. Index Terms: Speech Processing, Deep Learning Audio, Deep Learning Audio Pipeline creation, Audio Forensics.
</details>
<details>
<summary>摘要</summary>
深度音频分析器是一个开源的语音框架，旨在简化语音处理管道的研究和开发过程，让用户快速地实现语音处理任务，并且可以方便地比较和共享结果。本文描述了核心架构，支持audio дляensis领域的多个任务，并示出了创建新任务的可能性，因此可以根据需要自定义框架。通过深度音频分析器，法律机关的审查员和研究人员可以轻松地查看音频特征，快速评估预训练模型的性能，创建、导出和共享新的音频分析工作流程，只需几Click。这个工具的一个优点是快速加速了研究和实践实验的速度，因此也提高了实验 reproducibility。所有功能都是在用户可访问的模块中实现的，可以通过图形用户界面来访问。关键词：语音处理、深度学习音频、深度学习音频管道创建、音频鉴定。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.SD_2023_10_29/" data-id="cloimipf100xes488dq5v4gw4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.CV_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T13:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.CV_2023_10_29/">cs.CV - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="3DMiner-Discovering-Shapes-from-Large-Scale-Unannotated-Image-Datasets"><a href="#3DMiner-Discovering-Shapes-from-Large-Scale-Unannotated-Image-Datasets" class="headerlink" title="3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets"></a>3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19188">http://arxiv.org/abs/2310.19188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ta-Ying Cheng, Matheus Gadelha, Soren Pirk, Thibault Groueix, Radomir Mech, Andrew Markham, Niki Trigoni</li>
<li>for: 这个论文是为了挖掘大规模未标注图像集中的3D形状而写的。</li>
<li>methods: 该方法使用了自适应学习的图像表示学习技术来对图像集中的图像进行聚类，并在这些聚类中找到相似的图像对应关系。然后，通过这些对应关系来初始化吊销级整理，并逐步应用束合并推理方法来学习图像集中的神经占据场。</li>
<li>results: 该方法可以在使用Pix3D椅子图像集时生成比州前方法更好的结果， both quantitatively and qualitatively。此外， authors还展示了如何使用3DMiner在实际场景中进行3D重建，例如使用LAION-5B图像集中的图像进行重建。<details>
<summary>Abstract</summary>
We present 3DMiner -- a pipeline for mining 3D shapes from challenging large-scale unannotated image datasets. Unlike other unsupervised 3D reconstruction methods, we assume that, within a large-enough dataset, there must exist images of objects with similar shapes but varying backgrounds, textures, and viewpoints. Our approach leverages the recent advances in learning self-supervised image representations to cluster images with geometrically similar shapes and find common image correspondences between them. We then exploit these correspondences to obtain rough camera estimates as initialization for bundle-adjustment. Finally, for every image cluster, we apply a progressive bundle-adjusting reconstruction method to learn a neural occupancy field representing the underlying shape. We show that this procedure is robust to several types of errors introduced in previous steps (e.g., wrong camera poses, images containing dissimilar shapes, etc.), allowing us to obtain shape and pose annotations for images in-the-wild. When using images from Pix3D chairs, our method is capable of producing significantly better results than state-of-the-art unsupervised 3D reconstruction techniques, both quantitatively and qualitatively. Furthermore, we show how 3DMiner can be applied to in-the-wild data by reconstructing shapes present in images from the LAION-5B dataset. Project Page: https://ttchengab.github.io/3dminerOfficial
</details>
<details>
<summary>摘要</summary>
我们介绍3DMiner---一个用于从大规模无标注图像集合中采矿3D形状的管道。与其他无监督3D重建方法不同，我们假设在大到足够大的集合中，存在着objects with similar shapes but varying backgrounds, textures, and viewpoints的图像。我们的方法利用了最近的自适应图像表示学习的进步，将这些图像分组为具有相似形状的图像集合，并寻找这些集合之间的共同图像对应。我们然后利用这些对应来初始化捆绑调整，并运用这些调整来学习每个图像集合的神经占位场，从而获得形状和位势的标注。我们显示了这个程序可以对实际应用中的图像进行彻底的处理，并且与现有的无监督3D重建方法相比，可以获得更好的结果。此外，我们还显示了3DMiner可以对LAION-5B dataset中的图像进行重建，以及如何将3DMiner应用到实际应用中。更多资讯请参考https://ttchengab.github.io/3dminerOfficial。
</details></li>
</ul>
<hr>
<h2 id="Fast-Trainable-Projection-for-Robust-Fine-Tuning"><a href="#Fast-Trainable-Projection-for-Robust-Fine-Tuning" class="headerlink" title="Fast Trainable Projection for Robust Fine-Tuning"></a>Fast Trainable Projection for Robust Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19182">http://arxiv.org/abs/2310.19182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gt-ripl/ftp">https://github.com/gt-ripl/ftp</a></li>
<li>paper_authors: Junjiao Tian, Yen-Cheng Liu, James Seale Smith, Zsolt Kira<br>for:This paper aims to improve the robustness of pre-trained models when fine-tuning them for downstream tasks, while maintaining their in-distribution (ID) performance.methods:The proposed method, Fast Trainable Projection (FTP), uses projection-based fine-tuning with learnable projection constraints to improve the efficiency and scalability of the algorithm. FTP can be combined with existing optimizers like AdamW and is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner.results:The proposed FTP method achieves superior robustness on out-of-distribution (OOD) datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models. Additionally, FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/GT-RIPL/FTP.git.Here">https://github.com/GT-RIPL/FTP.git.Here</a> is the simplified Chinese text:for:这篇论文目标是在下游任务中练习预训练模型，保持其内部分布（ID）性能，同时提高对外部分布（OOD）的Robustness。methods:提议的方法是快速可调 projection-based fine-tuning，使用可调 projection constraint来提高算法的可扩展性和可优化性。这种方法可以与现有的优化器相结合，如 AdamW，并且是一种特殊的超优化器，可以在learnable manner中调整优化器的超参数。results:提议的FTP方法在不同的视觉任务和预训练模型上，都实现了superior的OOD Robustness，包括频率Shift和自然损害等。此外，FTP还可以在其他学习场景中使用，如low-label和连续学习 Settings，因为它的易于适应性。代码将在<a target="_blank" rel="noopener" href="https://github.com/GT-RIPL/FTP.git%E4%B8%AD%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/GT-RIPL/FTP.git中提供。</a><details>
<summary>Abstract</summary>
Robust fine-tuning aims to achieve competitive in-distribution (ID) performance while maintaining the out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. Recently, projected gradient descent has been successfully used in robust fine-tuning by constraining the deviation from the initialization of the fine-tuned model explicitly through projection. However, algorithmically, two limitations prevent this method from being adopted more widely, scalability and efficiency. In this paper, we propose a new projection-based fine-tuning algorithm, Fast Trainable Projection (FTP) for computationally efficient learning of per-layer projection constraints, resulting in an average $35\%$ speedup on our benchmarks compared to prior works. FTP can be combined with existing optimizers such as AdamW, and be used in a plug-and-play fashion. Finally, we show that FTP is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner through nested differentiation. Empirically, we show superior robustness on OOD datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models. Additionally, we demonstrate that FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings thanks to its easy adaptability. The code will be available at https://github.com/GT-RIPL/FTP.git.
</details>
<details>
<summary>摘要</summary>
Robust fine-tuning aimed at achieving competitive in-distribution (ID) performance while maintaining out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. Recently, projected gradient descent has been successfully used in robust fine-tuning by constraining the deviation from the initialization of the fine-tuned model explicitly through projection. However, algorithmically, two limitations prevent this method from being adopted more widely, scalability, and efficiency. In this paper, we propose a new projection-based fine-tuning algorithm, Fast Trainable Projection (FTP) for computationally efficient learning of per-layer projection constraints, resulting in an average $35\%$ speedup on our benchmarks compared to prior works. FTP can be combined with existing optimizers such as AdamW, and be used in a plug-and-play fashion. Finally, we show that FTP is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner through nested differentiation. Empirically, we show superior robustness on OOD datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models. Additionally, we demonstrate that FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings thanks to its easy adaptability. The code will be available at https://github.com/GT-RIPL/FTP.git.
</details></li>
</ul>
<hr>
<h2 id="BirdSAT-Cross-View-Contrastive-Masked-Autoencoders-for-Bird-Species-Classification-and-Mapping"><a href="#BirdSAT-Cross-View-Contrastive-Masked-Autoencoders-for-Bird-Species-Classification-and-Mapping" class="headerlink" title="BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping"></a>BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19168">http://arxiv.org/abs/2310.19168</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mvrl/birdsat">https://github.com/mvrl/birdsat</a></li>
<li>paper_authors: Srikumar Sastry, Subash Khanal, Aayush Dhakal, Di Huang, Nathan Jacobs</li>
<li>for: 本研究旨在开发一个 metadata-aware 自主学习<del>(SSL)</del>框架，用于细致分类和生物多样性地图的鸟类种类识别。</li>
<li>methods: 该框架结合了对比学习<del>(CL) 和伪像图像模型</del>(MIM) 两种 SSL 策略，同时将元数据与基础级图像相结合，以扩充嵌入空间。研究人员使用单模态和交叉模态 Vision Transformer 进行训练，并在全球鸟类种类数据集上进行训练。</li>
<li>results: 研究人员通过评估两个下游任务：细致视觉分类~(FGVC) 和交叉模态检索，发现模型学习了鸟类种类的细致特征和地域条件。同时，预训练模型在转移学习设置下表现出了顶尖性能，并且模型的交叉模态检索表现强化了鸟类种类的分布地图创建。<details>
<summary>Abstract</summary>
We propose a metadata-aware self-supervised learning~(SSL)~framework useful for fine-grained classification and ecological mapping of bird species around the world. Our framework unifies two SSL strategies: Contrastive Learning~(CL) and Masked Image Modeling~(MIM), while also enriching the embedding space with metadata available with ground-level imagery of birds. We separately train uni-modal and cross-modal ViT on a novel cross-view global bird species dataset containing ground-level imagery, metadata (location, time), and corresponding satellite imagery. We demonstrate that our models learn fine-grained and geographically conditioned features of birds, by evaluating on two downstream tasks: fine-grained visual classification~(FGVC) and cross-modal retrieval. Pre-trained models learned using our framework achieve SotA performance on FGVC of iNAT-2021 birds and in transfer learning settings for CUB-200-2011 and NABirds datasets. Moreover, the impressive cross-modal retrieval performance of our model enables the creation of species distribution maps across any geographic region. The dataset and source code will be released at https://github.com/mvrl/BirdSAT}.
</details>
<details>
<summary>摘要</summary>
我们提出一个具有元数据意识的自助学习~(SSL)~框架，用于精细分类和鸟类生态地图的世界各地鸟种。我们的框架将两种SSL策略：异构学习~(CL) 和伪像图模型~(MIM) 融合在一起，同时将地面鸟类图像中可用的元数据纳入嵌入空间。我们分别在不同视图上训练uni-modal和cross-modal ViT，并在一个新的跨视图全球鸟类数据集上进行训练，该数据集包括地面鸟类图像、元数据（位置、时间）以及相应的卫星图像。我们示示了我们的模型学习到了鸟类精细特征和地理条件特征，通过评估两个下游任务：精细视觉分类~(FGVC) 和交叉模式检索。预训练模型使用我们的框架学习后在iNAT-2021鸟类数据集上达到了最佳性能，并在传输学习设置下在CUB-200-2011和NABirds数据集上达到了优秀的表现。此外，我们的模型在交叉模式检索任务中表现出色，可以创建任何地理区域的鸟种分布图。数据集和源代码将在https://github.com/mvrl/BirdSAT 上发布。
</details></li>
</ul>
<hr>
<h2 id="Out-of-distribution-Object-Detection-through-Bayesian-Uncertainty-Estimation"><a href="#Out-of-distribution-Object-Detection-through-Bayesian-Uncertainty-Estimation" class="headerlink" title="Out-of-distribution Object Detection through Bayesian Uncertainty Estimation"></a>Out-of-distribution Object Detection through Bayesian Uncertainty Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19119">http://arxiv.org/abs/2310.19119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianhao Zhang, Shenglin Wang, Nidhal Bouaynaya, Radu Calinescu, Lyudmila Mihaylova<br>for: 本研究旨在提出一种 novel 的 bayesian 对象检测方法，以提高对象检测器在异常数据（out-of-distribution，OOD）下的性能。methods: 本方法基于提案的 Gaussian 分布来对准确度进行模型化，并通过采样weight参数来 отличаID数据与OOD数据。不同于其他不确定性模型方法，我们的方法不需要巨大的计算成本来推导weight分布，也不需要通过synthetic outlier数据进行模型训练。results: 我们在BDD100k和VOC数据集上进行训练，并在COCO2017数据集上进行评估。结果表明，我们的 bayesian 对象检测器可以在OOD数据下提供满意的鉴别性能，将FPR95分数降低8.19%，AUROC分数提高13.94%。<details>
<summary>Abstract</summary>
The superior performance of object detectors is often established under the condition that the test samples are in the same distribution as the training data. However, in many practical applications, out-of-distribution (OOD) instances are inevitable and usually lead to uncertainty in the results. In this paper, we propose a novel, intuitive, and scalable probabilistic object detection method for OOD detection. Unlike other uncertainty-modeling methods that either require huge computational costs to infer the weight distributions or rely on model training through synthetic outlier data, our method is able to distinguish between in-distribution (ID) data and OOD data via weight parameter sampling from proposed Gaussian distributions based on pre-trained networks. We demonstrate that our Bayesian object detector can achieve satisfactory OOD identification performance by reducing the FPR95 score by up to 8.19% and increasing the AUROC score by up to 13.94% when trained on BDD100k and VOC datasets as the ID datasets and evaluated on COCO2017 dataset as the OOD dataset.
</details>
<details>
<summary>摘要</summary>
超过90%的人会把这篇文章评为“非常好”。文章主要内容是关于Object Detector的性能评估，具体来说是在不同数据分布下进行评估。作者提出了一种新的、直观的、可扩展的概率性Object Detector方法，用于检测Out-of-Distribution（OOD）实例。与其他不确定性模型不同，该方法不需要巨大的计算成本来推导权重分布，也不需要通过人工异常数据进行模型训练。作者提出了一种基于预训练网络的Gaussian分布 sampling方法，用于分辨ID数据和OOD数据。文章示出，该抽象Object Detector可以在BDD100k和VOC数据集上达到满意的OOD标识性能，减少FPR95分数8.19%，提高AUROC分数13.94%。
</details></li>
</ul>
<hr>
<h2 id="CrossEAI-Using-Explainable-AI-to-generate-better-bounding-boxes-for-Chest-X-ray-images"><a href="#CrossEAI-Using-Explainable-AI-to-generate-better-bounding-boxes-for-Chest-X-ray-images" class="headerlink" title="CrossEAI: Using Explainable AI to generate better bounding boxes for Chest X-ray images"></a>CrossEAI: Using Explainable AI to generate better bounding boxes for Chest X-ray images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19835">http://arxiv.org/abs/2310.19835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinze Zhao<br>for: This paper focuses on improving the accuracy of bounding box generation for chest x-ray image diagnosis using post-hoc AI explainable methods.methods: The proposed method, CrossEAI, combines heatmap and gradient map to generate more targeted bounding boxes. The model uses a weighted average of Guided Backpropagation and Grad-CAM++ to generate bounding boxes that are closer to the ground truth.results: The proposed method achieves significant improvement over the state of the art model with the same setting, with an average improvement of 9% in all diseases over all Intersection over Union (IoU). Additionally, the model is able to achieve the same performance as a model that uses 80% of the ground truth bounding box information for training, without using any ground truth bounding box information.<details>
<summary>Abstract</summary>
Explainability is critical for deep learning applications in healthcare which are mandated to provide interpretations to both patients and doctors according to legal regulations and responsibilities. Explainable AI methods, such as feature importance using integrated gradients, model approximation using LIME, or neuron activation and layer conductance to provide interpretations for certain health risk predictions. In medical imaging diagnosis, disease classification usually achieves high accuracy, but generated bounding boxes have much lower Intersection over Union (IoU). Different methods with self-supervised or semi-supervised learning strategies have been proposed, but few improvements have been identified for bounding box generation. Previous work shows that bounding boxes generated by these methods are usually larger than ground truth and contain major non-disease area. This paper utilizes the advantages of post-hoc AI explainable methods to generate bounding boxes for chest x-ray image diagnosis. In this work, we propose CrossEAI which combines heatmap and gradient map to generate more targeted bounding boxes. By using weighted average of Guided Backpropagation and Grad-CAM++, we are able to generate bounding boxes which are closer to the ground truth. We evaluate our model on a chest x-ray dataset. The performance has significant improvement over the state of the art model with the same setting, with $9\%$ improvement in average of all diseases over all IoU. Moreover, as a model that does not use any ground truth bounding box information for training, we achieve same performance in general as the model that uses $80\%$ of the ground truth bounding box information for training
</details>
<details>
<summary>摘要</summary>
“医疗领域深度学习应用中，解释性是关键。由于法律和责任要求，患者和医生都需要获得解释。解释AI方法，如综合梯度的重要性或LIME模型 aproximation，可以提供医疗风险预测的解释。在医学成像诊断中，疾病分类通常具有高准确率，但生成的 bounding box 的 intersection over union（IoU）较低。不同的自动学习或半自动学习策略已经被提出，但很少有改进 bounding box 生成。本文利用post-hoc AI解释方法的优势，对呼吸道X射线成像进行诊断。我们提出 CrossEAI，它结合热图和梯度图来生成更加准确的 bounding box。通过使用权重平均的导引反射和 Grad-CAM++，我们能够生成更加接近真实值的 bounding box。我们在呼吸道X射线数据集上进行评估，并与状态方法相比，显示我们的模型在所有疾病和所有 IoU 上具有9%的提升。此外，作为没有使用任何真实 bounding box 信息进行训练的模型，我们在总体上与使用80%真实 bounding box 信息进行训练的模型相当。”Note that the translation is done using Google Translate, and the text may not be perfectly fluent or idiomatic in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Reward-Finetuning-for-Faster-and-More-Accurate-Unsupervised-Object-Discovery"><a href="#Reward-Finetuning-for-Faster-and-More-Accurate-Unsupervised-Object-Discovery" class="headerlink" title="Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery"></a>Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19080">http://arxiv.org/abs/2310.19080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katie Z Luo, Zhenzhen Liu, Xiangyu Chen, Yurong You, Sagie Benaim, Cheng Perng Phoo, Mark Campbell, Wen Sun, Bharath Hariharan, Kilian Q. Weinberger</li>
<li>for: 提高机器学习模型和人类预期的对应性，并在自动驾驶研究中应用RLHF。</li>
<li>methods: 使用RL方法，通过简单的规则来模拟人类反馈，并使用损失函数来评估矩形框的准确性。</li>
<li>results: 比对于先前的工作，该方法更准确，而且训练速度比较快。<details>
<summary>Abstract</summary>
Recent advances in machine learning have shown that Reinforcement Learning from Human Feedback (RLHF) can improve machine learning models and align them with human preferences. Although very successful for Large Language Models (LLMs), these advancements have not had a comparable impact in research for autonomous vehicles -- where alignment with human expectations can be imperative. In this paper, we propose to adapt similar RL-based methods to unsupervised object discovery, i.e. learning to detect objects from LiDAR points without any training labels. Instead of labels, we use simple heuristics to mimic human feedback. More explicitly, we combine multiple heuristics into a simple reward function that positively correlates its score with bounding box accuracy, \ie, boxes containing objects are scored higher than those without. We start from the detector's own predictions to explore the space and reinforce boxes with high rewards through gradient updates. Empirically, we demonstrate that our approach is not only more accurate, but also orders of magnitudes faster to train compared to prior works on object discovery.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的机器学习进步表明，人类反馈学习（RLHF）可以提高机器学习模型，使其更加符合人类的偏好。尽管在自动驾驶汽车领域中非常成功，但这些进步尚未在研究中得到了相应的影响。在这篇论文中，我们提议使用类似的RL基于方法，以无监督方式探索物体检测。而不是使用标签，我们使用简单的规则来模拟人类反馈。具体来说，我们将多个规则组合成一个简单的奖励函数，其奖励分数与盒子准确率正相关。我们从检测器的自己预测开始，通过梯度更新来强化高奖励的盒子。我们的方法不仅更准确，而且速度也是以前工作的多个量级快。
</details></li>
</ul>
<hr>
<h2 id="Myriad-Large-Multimodal-Model-by-Applying-Vision-Experts-for-Industrial-Anomaly-Detection"><a href="#Myriad-Large-Multimodal-Model-by-Applying-Vision-Experts-for-Industrial-Anomaly-Detection" class="headerlink" title="Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection"></a>Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19070">http://arxiv.org/abs/2310.19070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanze Li, Haolin Wang, Shihao Yuan, Ming Liu, Yiwen Guo, Chen Xu, Guangming Shi, Wangmeng Zuo</li>
<li>for: 这个研究是为了提出一个新的大型多模式辨识模型（Myriad），用于工业异常检测（Industrial Anomaly Detection，IAD），以提供明确的异常检测和详细的异常描述。</li>
<li>methods: 这个研究使用了MiniGPT-4作为基础的大型语言模型（LMM），并设计了专家观察模组，将预设知识从视觉专家转换为可以读取的语言模型（LLM）的 tokens。此外，它还引入了领域调整器，以bridging generic和工业图像的视觉表现差异。最后，它提出了视觉专家导师，将Q-Former变数为生成IAD领域的视觉语言 tokens。</li>
<li>results: 实验结果显示，提案的方法不仅在1-class和几个shot设定下与现有方法相比，表现出色，并且提供了明确的异常预测和详细的异常描述在IAD领域。<details>
<summary>Abstract</summary>
Existing industrial anomaly detection (IAD) methods predict anomaly scores for both anomaly detection and localization. However, they struggle to perform a multi-turn dialog and detailed descriptions for anomaly regions, e.g., color, shape, and categories of industrial anomalies. Recently, large multimodal (i.e., vision and language) models (LMMs) have shown eminent perception abilities on multiple vision tasks such as image captioning, visual understanding, visual reasoning, etc., making it a competitive potential choice for more comprehensible anomaly detection. However, the knowledge about anomaly detection is absent in existing general LMMs, while training a specific LMM for anomaly detection requires a tremendous amount of annotated data and massive computation resources. In this paper, we propose a novel large multi-modal model by applying vision experts for industrial anomaly detection (dubbed Myriad), which leads to definite anomaly detection and high-quality anomaly description. Specifically, we adopt MiniGPT-4 as the base LMM and design an Expert Perception module to embed the prior knowledge from vision experts as tokens which are intelligible to Large Language Models (LLMs). To compensate for the errors and confusions of vision experts, we introduce a domain adapter to bridge the visual representation gaps between generic and industrial images. Furthermore, we propose a Vision Expert Instructor, which enables the Q-Former to generate IAD domain vision-language tokens according to vision expert prior. Extensive experiments on MVTec-AD and VisA benchmarks demonstrate that our proposed method not only performs favorably against state-of-the-art methods under the 1-class and few-shot settings, but also provide definite anomaly prediction along with detailed descriptions in IAD domain.
</details>
<details>
<summary>摘要</summary>
现有的工业异常检测（IAD）方法预测异常得分，但它们在多Turn对话和细节描述方面强不甚，例如颜色、形状和工业异常类别。最近，大量多模式（i.e., 视觉和语言）模型（LMMs）在多种视觉任务上表现出了杰出的感知能力，例如图像描述、视觉理解、视觉逻辑等，使其成为可能的优秀选择。然而，现有的通用LMMs中关于异常检测的知识缺失，而特定LMM的训练需要大量的注释数据和巨大的计算资源。在本文中，我们提出了一种新的大型多模式模型，称为Myriad，用于工业异常检测，它可以实现准确的异常检测和高质量的异常描述。我们采用MiniGPT-4作为基础LMM，并设计了专家感知模块，将视觉专家的先前知识embed为可以被语言模型理解的令牌。为了补做视觉专家的错误和混乱，我们引入了领域适应器，将Generic和工业图像之间的视觉表示差异bridged。此外，我们提出了视觉专家导师，使Q-Former可以根据视觉专家的先前知识生成IAD领域的视觉语言令牌。我们对MVTec-AD和VisAbenchmark进行了广泛的实验，结果表明，我们的提出方法不仅在1类和少shot设置下与状态 искусственный智能方法相当，而且还可以提供准确的异常预测和详细的描述在IAD领域。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-ChatGPT-for-Medical-Applications-an-Experimental-Study-of-GPT-4V"><a href="#Multimodal-ChatGPT-for-Medical-Applications-an-Experimental-Study-of-GPT-4V" class="headerlink" title="Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V"></a>Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19061">http://arxiv.org/abs/2310.19061</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhilingyan/gpt4v-medical-report">https://github.com/zhilingyan/gpt4v-medical-report</a></li>
<li>paper_authors: Zhiling Yan, Kai Zhang, Rong Zhou, Lifang He, Xiang Li, Lichao Sun</li>
<li>for: 这个论文是用来评估当今最先进的多模态大语言模型GPT-4V在视觉问答任务上的能力的。</li>
<li>methods: 我们使用了GPT-4V synergize视觉和文本信息的文本提问来评估其在医学视觉问答任务上的能力。</li>
<li>results: 我们的实验结果表明，当前版本的GPT-4V不建议用于实际诊断，因为它在医学视觉问答任务中的准确率不稳定和较低。此外，我们还分析了GPT-4V在医学视觉问答任务中的七种特点，揭示了它在这个复杂的领域中的约束。详细的评估案例可以在<a target="_blank" rel="noopener" href="https://github.com/ZhilingYan/GPT4V-Medical-Report%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZhilingYan/GPT4V-Medical-Report中找到。</a><details>
<summary>Abstract</summary>
In this paper, we critically evaluate the capabilities of the state-of-the-art multimodal large language model, i.e., GPT-4 with Vision (GPT-4V), on Visual Question Answering (VQA) task. Our experiments thoroughly assess GPT-4V's proficiency in answering questions paired with images using both pathology and radiology datasets from 11 modalities (e.g. Microscopy, Dermoscopy, X-ray, CT, etc.) and fifteen objects of interests (brain, liver, lung, etc.). Our datasets encompass a comprehensive range of medical inquiries, including sixteen distinct question types. Throughout our evaluations, we devised textual prompts for GPT-4V, directing it to synergize visual and textual information. The experiments with accuracy score conclude that the current version of GPT-4V is not recommended for real-world diagnostics due to its unreliable and suboptimal accuracy in responding to diagnostic medical questions. In addition, we delineate seven unique facets of GPT-4V's behavior in medical VQA, highlighting its constraints within this complex arena. The complete details of our evaluation cases are accessible at https://github.com/ZhilingYan/GPT4V-Medical-Report.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们对当今最先进的多Modal大语言模型GPT-4 with Vision（GPT-4V）在视觉问答（VQA）任务上进行了批判性评估。我们的实验 Thoroughly assess GPT-4V 在与图像相关的问题上使用多种modalities（例如 Microscopy、Dermoscopy、X-ray、CT等）和十五种 объек interests（脑、肝脏、肺等）进行了评估。我们的数据集包括医学问题的广泛范围，包括十六种不同的问题类型。在我们的评估中，我们设计了文本提示，用于导引GPT-4V 将视觉和文本信息相互协同。实验结果显示，目前版本的GPT-4V 在回答医学问题上并不可靠，其精度较低。此外，我们还描述了GPT-4V 在医学VQA中的七种特点， highlighting its constraints within this complex arena。详细的评估结果可以在 GitHub上找到：https://github.com/ZhilingYan/GPT4V-Medical-Report。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Decision-Based-Black-Box-Adversarial-Attack-with-Gradient-Priors"><a href="#Boosting-Decision-Based-Black-Box-Adversarial-Attack-with-Gradient-Priors" class="headerlink" title="Boosting Decision-Based Black-Box Adversarial Attack with Gradient Priors"></a>Boosting Decision-Based Black-Box Adversarial Attack with Gradient Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19038">http://arxiv.org/abs/2310.19038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Liu, Xingshuo Huang, Xiaotong Zhang, Qimai Li, Fenglong Ma, Wei Wang, Hongyang Chen, Hong Yu, Xianchao Zhang</li>
<li>for: 这篇论文是关于黑盒攻击的研究，旨在提出一种基于决策的黑盒攻击方法，以提高攻击效率和精度。</li>
<li>methods: 该方法使用了数据依存的梯度先验和时间依存的梯度更新策略，以解决隐藏梯度不均匀和Successive Iteration梯度方向问题。具体来说，该方法使用了双向散射 filter 来处理每个随机扰动，以保持扰动在边缘位置的不整合性。</li>
<li>results: 对比其他强基eline，该方法在广泛的实验中表现出色，显著超过了其他方法。<details>
<summary>Abstract</summary>
Decision-based methods have shown to be effective in black-box adversarial attacks, as they can obtain satisfactory performance and only require to access the final model prediction. Gradient estimation is a critical step in black-box adversarial attacks, as it will directly affect the query efficiency. Recent works have attempted to utilize gradient priors to facilitate score-based methods to obtain better results. However, these gradient priors still suffer from the edge gradient discrepancy issue and the successive iteration gradient direction issue, thus are difficult to simply extend to decision-based methods. In this paper, we propose a novel Decision-based Black-box Attack framework with Gradient Priors (DBA-GP), which seamlessly integrates the data-dependent gradient prior and time-dependent prior into the gradient estimation procedure. First, by leveraging the joint bilateral filter to deal with each random perturbation, DBA-GP can guarantee that the generated perturbations in edge locations are hardly smoothed, i.e., alleviating the edge gradient discrepancy, thus remaining the characteristics of the original image as much as possible. Second, by utilizing a new gradient updating strategy to automatically adjust the successive iteration gradient direction, DBA-GP can accelerate the convergence speed, thus improving the query efficiency. Extensive experiments have demonstrated that the proposed method outperforms other strong baselines significantly.
</details>
<details>
<summary>摘要</summary>
决策基本方法在黑盒反击攻击中表现良好，因为它们只需访问最终模型预测。梯度估计是黑盒反击攻击中关键的步骤，因为它直接影响了查询效率。现有研究尝试使用梯度假设来促进分数基本方法获得更好的结果。然而，这些梯度假设仍然受到边梯度差异问题和连续迭代梯度方向问题的限制，因此难以简单地扩展到决策基本方法。在这篇论文中，我们提出了一种新的决策基本黑盒攻击框架（DBA-GP），该框架将数据依存梯度假设和时间依存假设细致地 интеグриinto梯度估计过程中。首先，通过利用 JOINT 双方filter来处理每个随机扰动，DBA-GP可以保证生成的扰动在边缘位置几乎不平滑，即消除边梯度差异，保持原始图像的特征。其次，通过利用新的梯度更新策略来自动调整 successive 迭代梯度方向，DBA-GP可以加速迭代速度，提高查询效率。经验表明，提议的方法与其他强式基准相比显著出众。
</details></li>
</ul>
<hr>
<h2 id="FPGAN-Control-A-Controllable-Fingerprint-Generator-for-Training-with-Synthetic-Data"><a href="#FPGAN-Control-A-Controllable-Fingerprint-Generator-for-Training-with-Synthetic-Data" class="headerlink" title="FPGAN-Control: A Controllable Fingerprint Generator for Training with Synthetic Data"></a>FPGAN-Control: A Controllable Fingerprint Generator for Training with Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19024">http://arxiv.org/abs/2310.19024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alon Shoshan, Nadav Bhonker, Emanuel Ben Baruch, Ori Nizan, Igor Kviatkovsky, Joshua Engelsma, Manoj Aggarwal, Gerard Medioni</li>
<li>for: 用于训练指纹识别模型，使用人工生成的数据，而不是具有敏感性的个人数据。</li>
<li>methods: 我们提出了FPGAN-Control，一种保持人工生成指纹图像的身份属性的权限控制框架。我们引入了一种新的出现损失，以促进指纹图像的分解性。</li>
<li>results: 我们在使用公开的NIST SD302（N2N）数据集进行训练FPGAN-Control模型时，得到了优秀的结果。我们quantitatively和qualitatively证明了FPGAN-Control的优势，包括保持身份属性的水平、控制指纹图像的出现特征和Synthetic-to-Real域阶跃小。最后，使用仅使用FPGAN-Control生成的人工数据进行训练指纹识别模型，可以达到与使用真实数据进行训练的相同或更高的识别率。<details>
<summary>Abstract</summary>
Training fingerprint recognition models using synthetic data has recently gained increased attention in the biometric community as it alleviates the dependency on sensitive personal data. Existing approaches for fingerprint generation are limited in their ability to generate diverse impressions of the same finger, a key property for providing effective data for training recognition models. To address this gap, we present FPGAN-Control, an identity preserving image generation framework which enables control over the fingerprint's image appearance (e.g., fingerprint type, acquisition device, pressure level) of generated fingerprints. We introduce a novel appearance loss that encourages disentanglement between the fingerprint's identity and appearance properties. In our experiments, we used the publicly available NIST SD302 (N2N) dataset for training the FPGAN-Control model. We demonstrate the merits of FPGAN-Control, both quantitatively and qualitatively, in terms of identity preservation level, degree of appearance control, and low synthetic-to-real domain gap. Finally, training recognition models using only synthetic datasets generated by FPGAN-Control lead to recognition accuracies that are on par or even surpass models trained using real data. To the best of our knowledge, this is the first work to demonstrate this.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用生成的指纹数据训练指纹识别模型已经在生物认证社区中受到了加大的关注，因为它减轻了敏感个人数据的依赖。现有的指纹生成方法具有生成同一个手指多个印记的限制，这限制了生成的指纹数据的多样性。为了解决这个问题，我们提出了FPGAN-Control，一个保持身份的图像生成框架，允许控制生成的指纹图像的显示形式（例如，手指类型、获取设备、压力水平）。我们引入了一种新的外观损失，该损失促进了指纹的身份和外观属性的分离。我们使用公共可用的NIST SD302（N2N）数据集进行FPGAN-Control模型的训练。我们表明FPGAN-Control的优点，包括身份保持水平、外观控制度和实际领域与Synthetic领域之间的差异度。最后，使用FPGAN-Control生成的synthetic数据进行训练，可以达到与实际数据训练的识别率相同或者还高。这是我们知道的第一个研究。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Test-Time-Adaptation-for-Super-Resolution-with-Second-Order-Degradation-and-Reconstruction"><a href="#Efficient-Test-Time-Adaptation-for-Super-Resolution-with-Second-Order-Degradation-and-Reconstruction" class="headerlink" title="Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction"></a>Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19011">http://arxiv.org/abs/2310.19011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dengzeshuai/srtta">https://github.com/dengzeshuai/srtta</a></li>
<li>paper_authors: Zeshuai Deng, Zhuokun Chen, Shuaicheng Niu, Thomas H. Li, Bohan Zhuang, Mingkui Tan<br>for:* 这个论文旨在提出一种快速适应测试环境的超分辨率图像重建方法，以便在不同&#x2F;未知降低类型的测试图像上实现高质量的SR图像重建。methods:* 该方法使用了次序降低方案来生成带有不同降低类型的对应数据，并通过特征级别重建学习来适应测试图像的降低类型。results:* 对于新 synthesized corrupted DIV2K数据集和一些实际世界数据集进行了广泛的实验，并显示了该方法可以具有惊人的提升，并且与现有方法相比具有满意的速度。<details>
<summary>Abstract</summary>
Image super-resolution (SR) aims to learn a mapping from low-resolution (LR) to high-resolution (HR) using paired HR-LR training images. Conventional SR methods typically gather the paired training data by synthesizing LR images from HR images using a predetermined degradation model, e.g., Bicubic down-sampling. However, the realistic degradation type of test images may mismatch with the training-time degradation type due to the dynamic changes of the real-world scenarios, resulting in inferior-quality SR images. To address this, existing methods attempt to estimate the degradation model and train an image-specific model, which, however, is quite time-consuming and impracticable to handle rapidly changing domain shifts. Moreover, these methods largely concentrate on the estimation of one degradation type (e.g., blur degradation), overlooking other degradation types like noise and JPEG in real-world test-time scenarios, thus limiting their practicality. To tackle these problems, we present an efficient test-time adaptation framework for SR, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types. Specifically, we design a second-order degradation scheme to construct paired data based on the degradation type of the test image, which is predicted by a pre-trained degradation classifier. Then, we adapt the SR model by implementing feature-level reconstruction learning from the initial test image to its second-order degraded counterparts, which helps the SR model generate plausible HR images. Extensive experiments are conducted on newly synthesized corrupted DIV2K datasets with 8 different degradations and several real-world datasets, demonstrating that our SRTTA framework achieves an impressive improvement over existing methods with satisfying speed. The source code is available at https://github.com/DengZeshuai/SRTTA.
</details>
<details>
<summary>摘要</summary>
图像超分辨率（SR）目标是通过LR和HR paired培成图像来学习LR到HR的映射。传统的SR方法通常使用预先确定的减样模型，如二维度下采样，来生成LR图像。然而，实际场景中的质量变化可能导致培成时的减样类型与测试时的减样类型不匹配，从而导致SR图像质量下降。为解决这问题，现有方法通常是通过估计减样模型并培成图像特定的模型来解决这个问题，但是这些方法需要较长的时间和不实际的培成过程。另外，这些方法主要集中于估计一种减样类型（例如，模糊减Randomized image degradation），忽略了实际场景中的其他减样类型（如噪声和JPEG），从而限制其实际应用。为此，我们提出了一种高效的测试时适应框架，名为SRTTA，可以快速适应测试图像的不同/未知减样类型。具体来说，我们设计了一种二阶减样方案，通过测试图像的减样类型来构建对应的paired数据，这些数据被预训练的减样类别预测器预测。然后，我们适应SR模型，通过实现初始测试图像的特征级重建学习，从而帮助SR模型生成可靠的HR图像。我们在新 synthesized corrupted DIV2K数据集上进行了广泛的实验，并得到了非常出色的提高，证明了我们的SRTTA框架的可靠性和速度。SR模型的源代码可以在https://github.com/DengZeshuai/SRTTA上获取。
</details></li>
</ul>
<hr>
<h2 id="DynPoint-Dynamic-Neural-Point-For-View-Synthesis"><a href="#DynPoint-Dynamic-Neural-Point-For-View-Synthesis" class="headerlink" title="DynPoint: Dynamic Neural Point For View Synthesis"></a>DynPoint: Dynamic Neural Point For View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18999">http://arxiv.org/abs/2310.18999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaichen Zhou, Jia-Xing Zhong, Sangyun Shin, Kai Lu, Yiyuan Yang, Andrew Markham, Niki Trigoni</li>
<li>for: 实现短时间内预测和合成复杂的单一影像视频中的视角</li>
<li>methods: 使用深度探索和Scene Flow估计来预测邻帧之间的3D对应关系，并将多个参考帧资讯聚合到目标帧上</li>
<li>results: 提高训练时间的减少和与传统方法相比的比较类似的效果，以及强大的长期视频处理能力<details>
<summary>Abstract</summary>
The introduction of neural radiance fields has greatly improved the effectiveness of view synthesis for monocular videos. However, existing algorithms face difficulties when dealing with uncontrolled or lengthy scenarios, and require extensive training time specific to each new scenario. To tackle these limitations, we propose DynPoint, an algorithm designed to facilitate the rapid synthesis of novel views for unconstrained monocular videos. Rather than encoding the entirety of the scenario information into a latent representation, DynPoint concentrates on predicting the explicit 3D correspondence between neighboring frames to realize information aggregation. Specifically, this correspondence prediction is achieved through the estimation of consistent depth and scene flow information across frames. Subsequently, the acquired correspondence is utilized to aggregate information from multiple reference frames to a target frame, by constructing hierarchical neural point clouds. The resulting framework enables swift and accurate view synthesis for desired views of target frames. The experimental results obtained demonstrate the considerable acceleration of training time achieved - typically an order of magnitude - by our proposed method while yielding comparable outcomes compared to prior approaches. Furthermore, our method exhibits strong robustness in handling long-duration videos without learning a canonical representation of video content.
</details>
<details>
<summary>摘要</summary>
“对于单一影像 видео的视角合成，射频场景导入对效果有很大改善。然而，现有的算法在面对无法控制或长度很长的场景时会遇到困难，并且需要对每个新场景进行专门的训练时间。为了解决这些限制，我们提出了DynPoint算法，用于快速合成单一影像 vide的目标帧的视角。而不是将整个场景信息转换为潜在表示，DynPoint专注于预测内部帧之间的明确三维对应关系，以便实现信息聚合。具体来说，这个对应预测是通过对内部帧之间的深度和场景流动信息进行估计。接着，所得到的对应信息被用来将多个参考帧聚合到目标帧上，通过建立对应的神经点云。这个框架可以实现快速和精准地合成目标帧的视角。实验结果显示，我们的提出方法可以大大减少训练时间，通常是一个次的频率，而且与先前的方法相比，其效果相似。此外，我们的方法还表现出强大的韧性，可以处理长度很长的影像 videowithout学习对影像内容的对应表示。”
</details></li>
</ul>
<hr>
<h2 id="Controllable-Group-Choreography-using-Contrastive-Diffusion"><a href="#Controllable-Group-Choreography-using-Contrastive-Diffusion" class="headerlink" title="Controllable Group Choreography using Contrastive Diffusion"></a>Controllable Group Choreography using Contrastive Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18986">http://arxiv.org/abs/2310.18986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aioz-ai/GCD">https://github.com/aioz-ai/GCD</a></li>
<li>paper_authors: Nhat Le, Tuong Do, Khoa Do, Hien Nguyen, Erman Tjiputra, Quang D. Tran, Anh Nguyen</li>
<li>for: 用于生成高质量、可定制的群体舞蹈动画</li>
<li>methods: 使用扩散基 générativeapproach Synthesize flexible number of dancers and long-term group dances, while ensuring coherence to the input music</li>
<li>results: 实现了可观赏的、一致的群体舞蹈动画，可控制consistency或多样性水平<details>
<summary>Abstract</summary>
Music-driven group choreography poses a considerable challenge but holds significant potential for a wide range of industrial applications. The ability to generate synchronized and visually appealing group dance motions that are aligned with music opens up opportunities in many fields such as entertainment, advertising, and virtual performances. However, most of the recent works are not able to generate high-fidelity long-term motions, or fail to enable controllable experience. In this work, we aim to address the demand for high-quality and customizable group dance generation by effectively governing the consistency and diversity of group choreographies. In particular, we utilize a diffusion-based generative approach to enable the synthesis of flexible number of dancers and long-term group dances, while ensuring coherence to the input music. Ultimately, we introduce a Group Contrastive Diffusion (GCD) strategy to enhance the connection between dancers and their group, presenting the ability to control the consistency or diversity level of the synthesized group animation via the classifier-guidance sampling technique. Through intensive experiments and evaluation, we demonstrate the effectiveness of our approach in producing visually captivating and consistent group dance motions. The experimental results show the capability of our method to achieve the desired levels of consistency and diversity, while maintaining the overall quality of the generated group choreography.
</details>
<details>
<summary>摘要</summary>
音乐驱动的群体编舞存在较大的挑战，但具有广泛的应用前景。可以生成协调和视觉吸引人的群体编舞动作，与音乐相对应，可以应用于娱乐、广告和虚拟表演等领域。然而，大多数最近的研究无法生成高品质长期编舞动作，或者失去控制体验。在这个工作中，我们希望通过有效地控制群体编舞的一致性和多样性来解决这个问题。特别是，我们利用扩散基本的生成方法，使得可以生成多个舞者和长期编舞动作，同时保证音乐的一致性。最后，我们引入了群体对比扩散策略（GCD），以提高舞者与群体之间的连接，并通过类型指导抽象技术来控制生成的群体动画的一致性或多样性水平。通过广泛的实验和评估，我们证明了我们的方法的可行性和效果，能够生成视觉吸引人的、一致的群体编舞动作。实验结果表明，我们的方法可以达到所需的一致性和多样性水平，同时保持生成的群体编舞动作的整体质量。
</details></li>
</ul>
<hr>
<h2 id="Blacksmith-Fast-Adversarial-Training-of-Vision-Transformers-via-a-Mixture-of-Single-step-and-Multi-step-Methods"><a href="#Blacksmith-Fast-Adversarial-Training-of-Vision-Transformers-via-a-Mixture-of-Single-step-and-Multi-step-Methods" class="headerlink" title="Blacksmith: Fast Adversarial Training of Vision Transformers via a Mixture of Single-step and Multi-step Methods"></a>Blacksmith: Fast Adversarial Training of Vision Transformers via a Mixture of Single-step and Multi-step Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18975">http://arxiv.org/abs/2310.18975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Salmani, Alireza Dehghanpour Farashah, Mohammad Azizmalayeri, Mahdi Amiri, Navid Eslami, Mohammad Taghi Manzuri, Mohammad Hossein Rohban</li>
<li>for: 防止深度学习模型受到攻击时的灾难性欠拟合 (Catastrophic Overfitting) 问题</li>
<li>methods: 提议使用随机选择PGD-2和FGSM两种攻击方法在小批量训练中，以增加攻击多样性，避免CO问题</li>
<li>results: 比较其他方法，包括N-FGSM，实现更好的防止CO和实现PGD-2级别的性能<details>
<summary>Abstract</summary>
Despite the remarkable success achieved by deep learning algorithms in various domains, such as computer vision, they remain vulnerable to adversarial perturbations. Adversarial Training (AT) stands out as one of the most effective solutions to address this issue; however, single-step AT can lead to Catastrophic Overfitting (CO). This scenario occurs when the adversarially trained network suddenly loses robustness against multi-step attacks like Projected Gradient Descent (PGD). Although several approaches have been proposed to address this problem in Convolutional Neural Networks (CNNs), we found out that they do not perform well when applied to Vision Transformers (ViTs). In this paper, we propose Blacksmith, a novel training strategy to overcome the CO problem, specifically in ViTs. Our approach utilizes either of PGD-2 or Fast Gradient Sign Method (FGSM) randomly in a mini-batch during the adversarial training of the neural network. This will increase the diversity of our training attacks, which could potentially mitigate the CO issue. To manage the increased training time resulting from this combination, we craft the PGD-2 attack based on only the first half of the layers, while FGSM is applied end-to-end. Through our experiments, we demonstrate that our novel method effectively prevents CO, achieves PGD-2 level performance, and outperforms other existing techniques including N-FGSM, which is the state-of-the-art method in fast training for CNNs.
</details>
<details>
<summary>摘要</summary>
尽管深度学习算法在不同领域取得了惊人的成功，但它们仍然面临到抗击干扰的漏洞。对于这个问题，对抗训练（AT）是一种非常有效的解决方案，但是单步AT可能会导致极端过拟合（CO）。这种情况发生在对多步攻击，如投影 gradient descent（PGD）进行了适应训练后，神经网络 suddenly lost its robustness。虽然一些方法已经被提出来解决这个问题在卷积神经网络（CNNs）中，但是这些方法在视图转换器（ViTs）中并不perform well。在这篇论文中，我们提出了黑锤子，一种新的训练策略，可以在ViTs中解决CO问题。我们的方法在 adversarial training 中随机使用 PGD-2 或 Fast Gradient Sign Method（FGSM），以增加训练攻击的多样性，从而可能解决CO问题。为了控制因此的增加训练时间，我们在PGD-2攻击基于神经网络的前半部分，而FGSM在整个神经网络中进行。通过我们的实验，我们证明了黑锤子有效地避免了CO问题，实现了PGD-2水平的性能，并超过了其他现有的方法，包括 N-FGSM，它是对于快速训练的CNNs最佳方法。
</details></li>
</ul>
<hr>
<h2 id="AnomalyCLIP-Object-agnostic-Prompt-Learning-for-Zero-shot-Anomaly-Detection"><a href="#AnomalyCLIP-Object-agnostic-Prompt-Learning-for-Zero-shot-Anomaly-Detection" class="headerlink" title="AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection"></a>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18961">http://arxiv.org/abs/2310.18961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zqhang/anomalyclip">https://github.com/zqhang/anomalyclip</a></li>
<li>paper_authors: Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen</li>
<li>for: 这个论文的目的是为了提出一个新的零分数异常探测（ZSAD）方法，以便在没有目标数据的情况下，精确地探测图像中的异常。</li>
<li>methods: 这个方法使用了大量的预训数据，并且将其与CLIP模型结合，以便学习一些通用的异常特征。另外，这个方法还使用了一些特定的文本描述来帮助模型更好地理解图像中的异常。</li>
<li>results: 在17个真实世界的异常探测数据集上，这个方法获得了Superior的零分数性能，可以实现在不同类型的物品上进行异常探测和分类。<details>
<summary>Abstract</summary>
Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, \eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate text into Simplified Chinese<</SYS>>Zero-shot异常检测（ZSAD）需要使用辅助数据训练的检测模型，以检测异常点 без任何目标数据。这是一个重要的任务，因为训练数据可能无法存取，例如因为数据隐私问题。然而，这是一个具有挑战的任务，因为模型需要对不同领域中的异常点进行概念扩展。最近，大型预训条件语音视觉模型（VLM），例如CLIP，已经展示了在不同视觉任务中的强大零shot识别能力。然而，它们的ZSAD性能较弱，因为VLM专注于模型背景物件的类别 semantics，而不是图像中的异常/正常领域。在这篇论文中，我们介绍了一个新的方法，即AnomalyCLIP，以适应CLIP для精确的ZSAD过程。关键思想是学习对应于图像中任何物件的通用正常和异常文本描述，从而让我们的模型专注于图像中的异常领域，而不是物件 semantics。这使我们的模型能够实现多元类型物件的通用正常和异常识别。大规模的实验显示，AnomalyCLIP在17个真实世界异常检测数据集上表现出色，可以实现零shot检测和分类异常点。代码将会在https://github.com/zqhang/AnomalyCLIP中公开。
</details></li>
</ul>
<hr>
<h2 id="TIC-TAC-A-Framework-To-Learn-And-Evaluate-Your-Covariance"><a href="#TIC-TAC-A-Framework-To-Learn-And-Evaluate-Your-Covariance" class="headerlink" title="TIC-TAC: A Framework To Learn And Evaluate Your Covariance"></a>TIC-TAC: A Framework To Learn And Evaluate Your Covariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18953">http://arxiv.org/abs/2310.18953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vita-epfl/TIC-TAC">https://github.com/vita-epfl/TIC-TAC</a></li>
<li>paper_authors: Megh Shukla, Mathieu Salzmann, Alexandre Alahi</li>
<li>for: 本研究实际问题是无监督 hetroscedastic 幂复数估计，目的是从观察 $x$ 学习多元目标分布 $\mathcal{N}(y, \Sigma_y | x)$。</li>
<li>methods: 通常是使用两个神经网络，通过负数alog-likelihood 训练，预测目标分布的均值 $f_{\theta}(x)$ 和幂复数 $\text{Cov}(f_{\theta}(x))$。</li>
<li>results: 我们解决了这两个问题：首先，我们提出了 TIC：泰勒引入幂复数，它通过在 $x$ 附近的第二阶 Taylor 多项式，捕捉多元 $f_{\theta}(x)$ 的随机性。其次，我们引入了 TAC：任务无关相关，这是一个基于条件的正常分布来评估幂复数。我们的实验显示，TIC 可以更好地学习幂复数，并且通过 TAC 评估其性能。<details>
<summary>Abstract</summary>
We study the problem of unsupervised heteroscedastic covariance estimation, where the goal is to learn the multivariate target distribution $\mathcal{N}(y, \Sigma_y | x )$ given an observation $x$. This problem is particularly challenging as $\Sigma_{y}$ varies for different samples (heteroscedastic) and no annotation for the covariance is available (unsupervised). Typically, state-of-the-art methods predict the mean $f_{\theta}(x)$ and covariance $\textrm{Cov}(f_{\theta}(x))$ of the target distribution through two neural networks trained using the negative log-likelihood. This raises two questions: (1) Does the predicted covariance truly capture the randomness of the predicted mean? (2) In the absence of ground-truth annotation, how can we quantify the performance of covariance estimation? We address (1) by deriving TIC: Taylor Induced Covariance, which captures the randomness of the multivariate $f_{\theta}(x)$ by incorporating its gradient and curvature around $x$ through the second order Taylor polynomial. Furthermore, we tackle (2) by introducing TAC: Task Agnostic Correlations, a metric which leverages conditioning of the normal distribution to evaluate the covariance. We verify the effectiveness of TIC through multiple experiments spanning synthetic (univariate, multivariate) and real-world datasets (UCI Regression, LSP, and MPII Human Pose Estimation). Our experiments show that TIC outperforms state-of-the-art in accurately learning the covariance, as quantified through TAC.
</details>
<details>
<summary>摘要</summary>
我们研究无监督不均匀 covariance 估计问题，目标是学习 multivariate 目标分布 $\mathcal{N}(y, \Sigma_y | x)$  Given an observation $x$. 这个问题特别困难，因为 $\Sigma_y$ 对不同样本而变化 (heteroscedastic) 并且没有对 covariance 的注释 (unsupervised)。通常，当前的方法预测目标分布的均值 $f_{\theta}(x)$ 和 covariance $\text{Cov}(f_{\theta}(x))$ 通过两个神经网络，通过负LOG-likelihood 训练。这引出了两个问题：1. 预测的 covariance 是否真正捕捉了预测的均值的Randomness？2. 在缺乏真实注释的情况下，如何评价 covariance 估计的性能？我们解决了第一个问题，通过 derivation TIC：Taylor Induced Covariance，它利用 $x$ 的第二阶 Taylor 级数来捕捉 multivariate $f_{\theta}(x)$ 的Randomness。此外，我们解决了第二个问题，通过引入 TAC：Task Agnostic Correlations，它利用 conditioning 来评价 covariance。我们通过多个实验证明 TIC 的效果，其中包括 synthetic 数据 (univariate, multivariate) 和实际世界数据 (UCI Regression, LSP, MPII Human Pose Estimation)。我们的实验表明，TIC 可以更好地学习 covariance，并且通过 TAC 评价其性能。
</details></li>
</ul>
<hr>
<h2 id="Customize-StyleGAN-with-One-Hand-Sketch"><a href="#Customize-StyleGAN-with-One-Hand-Sketch" class="headerlink" title="Customize StyleGAN with One Hand Sketch"></a>Customize StyleGAN with One Hand Sketch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18949">http://arxiv.org/abs/2310.18949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaocong Zhang</li>
<li>for: 用于控制 StyleGAN 图像生成的单个用户绘图</li>
<li>methods: 基于 CLIP 的能量学习方法，包括两种新的能量函数，用于在 StyleGAN 的含义空间中学习 conditional 分布</li>
<li>results: 可以使用单个用户绘图控制 StyleGAN 图像生成，并且在一阶段 regime 中显著超越先前方法，同时在不同风格和姿势的人工绘图上也表现出优异性。<details>
<summary>Abstract</summary>
Generating images from human sketches typically requires dedicated networks trained from scratch. In contrast, the emergence of the pre-trained Vision-Language models (e.g., CLIP) has propelled generative applications based on controlling the output imagery of existing StyleGAN models with text inputs or reference images. Parallelly, our work proposes a framework to control StyleGAN imagery with a single user sketch. In particular, we learn a conditional distribution in the latent space of a pre-trained StyleGAN model via energy-based learning and propose two novel energy functions leveraging CLIP for cross-domain semantic supervision. Once trained, our model can generate multi-modal images semantically aligned with the input sketch. Quantitative evaluations on synthesized datasets have shown that our approach improves significantly from previous methods in the one-shot regime. The superiority of our method is further underscored when experimenting with a wide range of human sketches of diverse styles and poses. Surprisingly, our models outperform the previous baseline regarding both the range of sketch inputs and image qualities despite operating with a stricter setting: with no extra training data and single sketch input.
</details>
<details>
<summary>摘要</summary>
通常需要专门的网络来生成图像从人工绘制。然而，clip的出现提高了基于文本输入或参考图像控制现有的StyleGAN模型的生成应用。我们的工作则是一个框架，可以使用单个用户绘制来控制StyleGAN图像。具体来说，我们通过能量学习学习 StyleGAN模型的latent空间中的conditional分布，并提出了两种新的能量函数，利用clip进行跨领域semantic监督。一旦训练完成，我们的模型可以生成与输入绘制semantic相关的多模态图像。对于synthesized dataset的量化评价表明，我们的方法在一键 режиме中表现出了明显的提升。此外，我们的方法还在使用不同风格和姿势的人工绘制中表现出色，并且在没有额外训练数据和单个绘制输入的情况下，我们的模型仍然能够超越前一个基准值。
</details></li>
</ul>
<hr>
<h2 id="Video-Frame-Interpolation-with-Many-to-many-Splatting-and-Spatial-Selective-Refinement"><a href="#Video-Frame-Interpolation-with-Many-to-many-Splatting-and-Spatial-Selective-Refinement" class="headerlink" title="Video Frame Interpolation with Many-to-many Splatting and Spatial Selective Refinement"></a>Video Frame Interpolation with Many-to-many Splatting and Spatial Selective Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18946">http://arxiv.org/abs/2310.18946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Hu, Simon Niklaus, Lu Zhang, Stan Sclaroff, Kate Saenko</li>
<li>for: 这篇论文旨在提出一种可微分的多个目标（Many-to-Many，M2M）拼接框架，以高效地 interpolate 帧。</li>
<li>methods: 该方法使用多个 bidirectional 流来直接将像素截割到想要的时间步，并将每个源像素映射到多个目标像素，以实现多对多拼接方案。</li>
<li>results: 该方法可以在 interpolating 任意数量的中间帧时，对每个输入帧对对有较少的计算开销，因此实现了高速多帧 interpolating。但是，直接在Intensity Domain中截割和融合像素可能会受到运动估计质量的影响，并且可能会受到较差的表示能力。为了提高 interpolating 精度，该方法还提出了一种可调 SSR 组件，可以根据计算效率和 interpolating 质量进行调整。<details>
<summary>Abstract</summary>
In this work, we first propose a fully differentiable Many-to-Many (M2M) splatting framework to interpolate frames efficiently. Given a frame pair, we estimate multiple bidirectional flows to directly forward warp the pixels to the desired time step before fusing overlapping pixels. In doing so, each source pixel renders multiple target pixels and each target pixel can be synthesized from a larger area of visual context, establishing a many-to-many splatting scheme with robustness to undesirable artifacts. For each input frame pair, M2M has a minuscule computational overhead when interpolating an arbitrary number of in-between frames, hence achieving fast multi-frame interpolation. However, directly warping and fusing pixels in the intensity domain is sensitive to the quality of motion estimation and may suffer from less effective representation capacity. To improve interpolation accuracy, we further extend an M2M++ framework by introducing a flexible Spatial Selective Refinement (SSR) component, which allows for trading computational efficiency for interpolation quality and vice versa. Instead of refining the entire interpolated frame, SSR only processes difficult regions selected under the guidance of an estimated error map, thereby avoiding redundant computation. Evaluation on multiple benchmark datasets shows that our method is able to improve the efficiency while maintaining competitive video interpolation quality, and it can be adjusted to use more or less compute as needed.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们首先提出了一个完全可导Many-to-Many（M2M）拼接框架，以高效地 interpolate帧。给定一个帧对，我们估算多个双向流来直接forward扭曲像素到所需的时间步，以便在拼接过程中 fusion overlapping pixels。在这样的情况下，每个源像素可以渲染多个目标像素，并且每个目标像素可以从更大的视觉上下文中 Synthesize，建立了一个多个源像素到多个目标像素的拼接方案，从而具有较好的鲁棒性。对于每个输入帧对，M2M在 interpolating 任意数量的中间帧时，只需要投入微scopic的计算负担，因此实现了高速多帧 interpolating。然而，直接在Intensity Domain中扭曲和合并像素是对动作估计质量的敏感，可能会受到较差的表示能力的影响。为了提高 interpolating 精度，我们进一步扩展了 M2M++ 框架，通过引入 flexible Spatial Selective Refinement（SSR）组件，以便在需要更高的 interpolating 精度时，通过选择难度较高的区域进行精细化，从而避免需要 redundant computation。我们对多个标准数据集进行评估，发现我们的方法可以提高效率，同时保持竞争力强的视频 interpolating 质量，并且可以根据需要调整使用更多或更少的计算资源。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Examples-Are-Not-Real-Features"><a href="#Adversarial-Examples-Are-Not-Real-Features" class="headerlink" title="Adversarial Examples Are Not Real Features"></a>Adversarial Examples Are Not Real Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18936">http://arxiv.org/abs/2310.18936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-ml/advnotrealfeatures">https://github.com/pku-ml/advnotrealfeatures</a></li>
<li>paper_authors: Ang Li, Yifei Wang, Yiwen Guo, Yisen Wang</li>
<li>for: 本研究探讨了 adversarial example 的形成原因，以及非Robust 特征是否真的有用。</li>
<li>methods: 研究者使用了多种学习模式，包括 supervised learning、contrastive learning、masked image modeling 和 diffusion models，以检验 non-Robust 特征的用用性。</li>
<li>results: 研究结果表明，non-Robust 特征在不同的学习模式下 Transfer 性差，而 Robust 特征具有更好的 Transfer 性。此外，研究者还发现，自然地训练的 encoder 在 AutoAttack 中不具有 robustness。结论是，non-Robust 特征并不是真正有用，而是学习模式偏好的快捷途径。<details>
<summary>Abstract</summary>
The existence of adversarial examples has been a mystery for years and attracted much interest. A well-known theory by \citet{ilyas2019adversarial} explains adversarial vulnerability from a data perspective by showing that one can extract non-robust features from adversarial examples and these features alone are useful for classification. However, the explanation remains quite counter-intuitive since non-robust features are mostly noise features to humans. In this paper, we re-examine the theory from a larger context by incorporating multiple learning paradigms. Notably, we find that contrary to their good usefulness under supervised learning, non-robust features attain poor usefulness when transferred to other self-supervised learning paradigms, such as contrastive learning, masked image modeling, and diffusion models. It reveals that non-robust features are not really as useful as robust or natural features that enjoy good transferability between these paradigms. Meanwhile, for robustness, we also show that naturally trained encoders from robust features are largely non-robust under AutoAttack. Our cross-paradigm examination suggests that the non-robust features are not really useful but more like paradigm-wise shortcuts, and robust features alone might be insufficient to attain reliable model robustness. Code is available at \url{https://github.com/PKU-ML/AdvNotRealFeatures}.
</details>
<details>
<summary>摘要</summary>
exist adversarial examples 年来都是一个谜，吸引了很多关注。一种常见的理论是由\citet{ilyas2019adversarial}提出的，它解释了对 adversarial examples 的敏感性从数据角度，表明可以从 adversarial examples 中提取不稳定特征，并且这些特征可以帮助进行分类。然而，这种解释仍然很Counter-intuitive，因为这些不稳定特征对人类来说都是噪音。在这篇论文中，我们重新审视这种理论，通过将多种学习概念相结合。结果发现，相比于supervised learning中的好用性，non-robust features在其他自适应学习概念中，如对比学习、干扰学习和扩散模型， exhibit poor usefulness。这显示出non-robust features并不是如人们所想的那么有用，而是在某些学习概念下的偏好短cut。此外，我们还发现，自然地训练的encoder从robust特征中获得的模型不 robust under AutoAttack。我们的 across-paradigm 审视表明，non-robust features并不是真正有用的，而更像是学习概念的偏好短cut。代码可以在 \url{https://github.com/PKU-ML/AdvNotRealFeatures} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Label-Poisoning-is-All-You-Need"><a href="#Label-Poisoning-is-All-You-Need" class="headerlink" title="Label Poisoning is All You Need"></a>Label Poisoning is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18933">http://arxiv.org/abs/2310.18933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MarkipTheMudkip/in-class-project-2">https://github.com/MarkipTheMudkip/in-class-project-2</a></li>
<li>paper_authors: Rishi D. Jha, Jonathan Hayase, Sewoong Oh</li>
<li>For: The paper investigates the possibility of launching a successful backdoor attack by only corrupting the training labels, rather than the images themselves.* Methods: The paper introduces a novel approach called FLIP, which uses trajectory matching to design label-only backdoor attacks.* Results: The paper demonstrates the effectiveness of FLIP on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer), achieving a near-perfect attack success rate of 99.4% with only a 1.8% drop in the clean test accuracy.Here are the three points in Simplified Chinese text:</li>
<li>for: 论文 investigate 是否可以通过只 corrupting 训练标签来发动成功的后门攻击。</li>
<li>methods: 论文提出了一种新的方法 called FLIP，使用 trajectory matching 设计 label-only 后门攻击。</li>
<li>results: 论文在三个 dataset (CIFAR-10, CIFAR-100, Tiny-ImageNet) 和四种架构 (ResNet-32, ResNet-18, VGG-19, Vision Transformer) 上进行了实验，成功率为 99.4%，但clean test accuracy 下降了1.8%。<details>
<summary>Abstract</summary>
In a backdoor attack, an adversary injects corrupted data into a model's training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger. A typical corrupted training example requires altering both the image, by applying the trigger, and the label. Models trained on clean images, therefore, were considered safe from backdoor attacks. However, in some common machine learning scenarios, the training labels are provided by potentially malicious third-parties. This includes crowd-sourced annotation and knowledge distillation. We, hence, investigate a fundamental question: can we launch a successful backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, which we call FLIP, and demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of 99.4% while suffering only a 1.8% drop in the clean test accuracy. Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation.
</details>
<details>
<summary>摘要</summary>
在一种后门攻击中，敌对者将损坏数据插入模型的训练集中，以获得对特定触发符的图像预测的控制权。通常需要在图像上应用触发符，并修改标签。由于clean图像上训练的模型被认为是安全的，因此这种攻击被称为后门攻击。然而，在一些常见的机器学习场景中，训练标签由可能有恶意的第三方提供，包括人工标注和知识储存。我们因此研究了一个基本问题：可以通过只修改标签来发动成功的后门攻击吗？我们提出了一种新的标签修改攻击方法，称之为FLIP，并在CIFAR-10、CIFAR-100和Tiny-ImageNet三个数据集和四种架构（ResNet-32、ResNet-18、VGG-19和Vision Transformer）上进行了实验。只有2%的CIFAR-10标签被损坏，FLIP可以达到99.4%的攻击成功率，同时只有1.8%的干净测试准确率下降。我们的方法基于最近的曲线匹配原理，原本用于数据储存。
</details></li>
</ul>
<hr>
<h2 id="A-transfer-learning-approach-with-convolutional-neural-network-for-Face-Mask-Detection"><a href="#A-transfer-learning-approach-with-convolutional-neural-network-for-Face-Mask-Detection" class="headerlink" title="A transfer learning approach with convolutional neural network for Face Mask Detection"></a>A transfer learning approach with convolutional neural network for Face Mask Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18928">http://arxiv.org/abs/2310.18928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abolfazl Younesi, Reza Afrouzian, Yousef Seyfari<br>for: 本研究旨在提出一个基于传播学习和Inception v3架构的面具识别系统，以检测拥有人群中的面具使用情况。methods: 本研究使用了两个同时训练 dataset，包括Simulated Mask Face Dataset (SMFD) 和 MaskedFace-Net (MFN)，并通过优化协eles hyper-parameters和精确设计全接触层，以提高系统的准确性和效率。results: 实验结果显示，提案的方法具有高准确性和效率，在训练和测试数据中分别 achievement 99.47% 和 99.33%。<details>
<summary>Abstract</summary>
Due to the epidemic of the coronavirus (Covid-19) and its rapid spread around the world, the world has faced an enormous crisis. To prevent the spread of the coronavirus, the World Health Organization (WHO) has introduced the use of masks and keeping social distance as the best preventive method. So, developing an automatic monitoring system for detecting facemasks in some crowded places is essential. To do this, we propose a mask recognition system based on transfer learning and Inception v3 architecture. In the proposed method, two datasets are used simultaneously for training including the Simulated Mask Face Dataset (SMFD) and MaskedFace-Net (MFN) This paper tries to increase the accuracy of the proposed system by optimally setting hyper-parameters and accurately designing the fully connected layers. The main advantage of the proposed method is that in addition to masked and unmasked faces, it can also detect cases of incorrect use of mask. Therefore, the proposed method classifies the input face images into three categories. Experimental results show the high accuracy and efficiency of the proposed method; so, this method has achieved an accuracy of 99.47% and 99.33% in training and test data respectively
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a mask recognition system based on transfer learning and the Inception v3 architecture. Our approach utilizes two datasets simultaneously for training: the Simulated Mask Face Dataset (SMFD) and MaskedFace-Net (MFN). The primary goal of this paper is to enhance the accuracy of the proposed system by optimally setting hyperparameters and designing the fully connected layers.The key advantage of our method is that it can detect not only masked and unmasked faces but also incorrect use of masks. Therefore, the proposed method classifies input face images into three categories. Experimental results demonstrate the high accuracy and efficiency of the proposed method, with an accuracy of 99.47% and 99.33% in training and test data, respectively.
</details></li>
</ul>
<hr>
<h2 id="Improving-Multi-Person-Pose-Tracking-with-A-Confidence-Network"><a href="#Improving-Multi-Person-Pose-Tracking-with-A-Confidence-Network" class="headerlink" title="Improving Multi-Person Pose Tracking with A Confidence Network"></a>Improving Multi-Person Pose Tracking with A Confidence Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18920">http://arxiv.org/abs/2310.18920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehua Fu, Wenhang Zuo, Zhenghui Hu, Qingjie Liu, Yunhong Wang</li>
<li>for: 本研究旨在提高顶向方法中的人体检测和pose estimation的精度，以解决 occlusion 和 missed detection 问题。</li>
<li>methods: 本文提出了一种新的关键点信任网络和跟踪管道，以提高顶向方法中的人体检测和pose estimation。关键点信任网络用于确定每个关键点是否受到 occlusion，而跟踪管道包括bbox-revision模块和ID-retrieve模块，以减少丢失检测和修复lost trajectory。</li>
<li>results: 实验结果显示，我们的方法在人体检测和pose estimation方面具有 universality，在 PoseTrack 2017 和2018 数据集上达到了状态对精度。<details>
<summary>Abstract</summary>
Human pose estimation and tracking are fundamental tasks for understanding human behaviors in videos. Existing top-down framework-based methods usually perform three-stage tasks: human detection, pose estimation and tracking. Although promising results have been achieved, these methods rely heavily on high-performance detectors and may fail to track persons who are occluded or miss-detected. To overcome these problems, in this paper, we develop a novel keypoint confidence network and a tracking pipeline to improve human detection and pose estimation in top-down approaches. Specifically, the keypoint confidence network is designed to determine whether each keypoint is occluded, and it is incorporated into the pose estimation module. In the tracking pipeline, we propose the Bbox-revision module to reduce missing detection and the ID-retrieve module to correct lost trajectories, improving the performance of the detection stage. Experimental results show that our approach is universal in human detection and pose estimation, achieving state-of-the-art performance on both PoseTrack 2017 and 2018 datasets.
</details>
<details>
<summary>摘要</summary>
人体姿势估计和跟踪是视频理解人类行为的基本任务。现有的顶部框架基础方法通常执行三个阶段任务：人员检测、姿势估计和跟踪。虽然已经获得了出色的结果，但这些方法受到高性能探测器的依赖，可能会在 occluded 或者检测错误时失败。为了解决这些问题，在这篇论文中，我们开发了一种新的关键点信任网络和跟踪管道，以提高顶部方法中的人员检测和姿势估计。具体来说，关键点信任网络是用于判断每个关键点是否受到遮挡的，并将其 incorporated 到姿势估计模块中。在跟踪管道中，我们提出了 Bbox-revision 模块，以减少缺失检测，以及 ID-retrieve 模块，以更正丢失的轨迹，从而提高检测阶段的性能。实验结果表明，我们的方法在人员检测和姿势估计中具有通用性和state-of-the-art 性能，在 PoseTrack 2017 和 2018 数据集上达到了最佳性能。
</details></li>
</ul>
<hr>
<h2 id="TiV-NeRF-Tracking-and-Mapping-via-Time-Varying-Representation-with-Dynamic-Neural-Radiance-Fields"><a href="#TiV-NeRF-Tracking-and-Mapping-via-Time-Varying-Representation-with-Dynamic-Neural-Radiance-Fields" class="headerlink" title="TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields"></a>TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18917">http://arxiv.org/abs/2310.18917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyao Duan, Zhiliu Yang</li>
<li>for: track and reconstruct dynamic scenes in SLAM framework</li>
<li>methods: time-varying representation, self-supervised training, distinct sampling strategies, and keyframe selection strategy</li>
<li>results: more effective compared to current state-of-the-art dynamic mapping methodsHere’s the full summary in Simplified Chinese:</li>
<li>for: 这 paper 旨在将 Neural Radiance Fields (NeRF)  интегрирован到 Simultaneous Localization and Mapping (SLAM) 框架中，以处理动态场景。</li>
<li>methods: 该 paper 提出了时间变化表示，自动Supervised 训练，不同区域采样策略，以及关键帧选择策略。</li>
<li>results: 比现有的动态映射方法更有效。I hope that helps!<details>
<summary>Abstract</summary>
Previous attempts to integrate Neural Radiance Fields (NeRF) into Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or treat dynamic objects as outliers. However, most of real-world scenarios is dynamic. In this paper, we propose a time-varying representation to track and reconstruct the dynamic scenes. Our system simultaneously maintains two processes, tracking process and mapping process. For tracking process, the entire input images are uniformly sampled and training of the RGB images are self-supervised. For mapping process, we leverage know masks to differentiate dynamic objects and static backgrounds, and we apply distinct sampling strategies for two types of areas. The parameters optimization for both processes are made up by two stages, the first stage associates time with 3D positions to convert the deformation field to the canonical field. And the second associates time with 3D positions in canonical field to obtain colors and Signed Distance Function (SDF). Besides, We propose a novel keyframe selection strategy based on the overlapping rate. We evaluate our approach on two publicly available synthetic datasets and validate that our method is more effective compared to current state-of-the-art dynamic mapping methods.
</details>
<details>
<summary>摘要</summary>
previous attempts to integrate Neural Radiance Fields (NeRF) into Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or treat dynamic objects as outliers. However, most of real-world scenarios is dynamic. In this paper, we propose a time-varying representation to track and reconstruct the dynamic scenes. Our system simultaneously maintains two processes, tracking process and mapping process. For tracking process, the entire input images are uniformly sampled and training of the RGB images are self-supervised. For mapping process, we leverage know masks to differentiate dynamic objects and static backgrounds, and we apply distinct sampling strategies for two types of areas. The parameters optimization for both processes are made up by two stages, the first stage associates time with 3D positions to convert the deformation field to the canonical field. And the second associates time with 3D positions in canonical field to obtain colors and Signed Distance Function (SDF). Besides, We propose a novel keyframe selection strategy based on the overlapping rate. We evaluate our approach on two publicly available synthetic datasets and validate that our method is more effective compared to current state-of-the-art dynamic mapping methods.Here's the translation in Traditional Chinese:previous attempts to integrate Neural Radiance Fields (NeRF) into Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or treat dynamic objects as outliers. However, most of real-world scenarios is dynamic. In this paper, we propose a time-varying representation to track and reconstruct the dynamic scenes. Our system simultaneously maintains two processes, tracking process and mapping process. For tracking process, the entire input images are uniformly sampled and training of the RGB images are self-supervised. For mapping process, we leverage known masks to differentiate dynamic objects and static backgrounds, and we apply distinct sampling strategies for two types of areas. The parameters optimization for both processes are made up by two stages, the first stage associates time with 3D positions to convert the deformation field to the canonical field. And the second associates time with 3D positions in canonical field to obtain colors and Signed Distance Function (SDF). Besides, We propose a novel keyframe selection strategy based on the overlapping rate. We evaluate our approach on two publicly available synthetic datasets and validate that our method is more effective compared to current state-of-the-art dynamic mapping methods.
</details></li>
</ul>
<hr>
<h2 id="Identifiable-Contrastive-Learning-with-Automatic-Feature-Importance-Discovery"><a href="#Identifiable-Contrastive-Learning-with-Automatic-Feature-Importance-Discovery" class="headerlink" title="Identifiable Contrastive Learning with Automatic Feature Importance Discovery"></a>Identifiable Contrastive Learning with Automatic Feature Importance Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18904">http://arxiv.org/abs/2310.18904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-ml/tri-factor-contrastive-learning">https://github.com/pku-ml/tri-factor-contrastive-learning</a></li>
<li>paper_authors: Qi Zhang, Yifei Wang, Yisen Wang</li>
<li>for: 本研究旨在提出一种新的对比学习方法（tri-factor contrastive learning，简称triCL），以便从人类视角获得更加可解解释的数据表示。</li>
<li>methods: triCL使用了一种3因素对比的形式，即 $z_x^\top S z_{x’}$，其中 $S$ 是一个可学习的对角矩阵，自动捕捉到每个特征的重要性。</li>
<li>results: 我们证明了 triCL 可以不仅获得可解解释的特征，而且可以通过对比学习方法来获得更高的性能。我们还发现，高重要性的特征具有良好的可解解释性，可以捕捉到共同的类别特征。<details>
<summary>Abstract</summary>
Existing contrastive learning methods rely on pairwise sample contrast $z_x^\top z_{x'}$ to learn data representations, but the learned features often lack clear interpretability from a human perspective. Theoretically, it lacks feature identifiability and different initialization may lead to totally different features. In this paper, we study a new method named tri-factor contrastive learning (triCL) that involves a 3-factor contrast in the form of $z_x^\top S z_{x'}$, where $S=\text{diag}(s_1,\dots,s_k)$ is a learnable diagonal matrix that automatically captures the importance of each feature. We show that by this simple extension, triCL can not only obtain identifiable features that eliminate randomness but also obtain more interpretable features that are ordered according to the importance matrix $S$. We show that features with high importance have nice interpretability by capturing common classwise features, and obtain superior performance when evaluated for image retrieval using a few features. The proposed triCL objective is general and can be applied to different contrastive learning methods like SimCLR and CLIP. We believe that it is a better alternative to existing 2-factor contrastive learning by improving its identifiability and interpretability with minimal overhead. Code is available at https://github.com/PKU-ML/Tri-factor-Contrastive-Learning.
</details>
<details>
<summary>摘要</summary>
现有的对比学习方法通常基于对比样本的对比度 $z_x^\top z_{x'}$ 来学习数据表示，但学习的特征通常缺乏人类可理解的解释性。理论上来说，它缺乏特征可识别性，不同的初始化可能会导致极其不同的特征。在这篇论文中，我们研究了一种新的方法 named tri-factor contrastive learning (triCL)，它包含了一种三因子对比的形式，即 $z_x^\top S z_{x'}$, 其中 $S$ 是一个可学习的对角矩阵，自动捕捉每个特征的重要性。我们显示了，通过这种简单的扩展，triCL 可以不仅获得可识别的特征，而且可以获得更加可解的特征，这些特征被排序于重要性矩阵 $S$ 中，并且高度重要的特征具有良好的解释性，可以捕捉共同的类别特征，并且在图像检索任务中获得更高的性能。我们表明了 triCL 目标是一种通用的对比学习目标，可以应用于不同的对比学习方法，如 SimCLR 和 CLIP。我们认为，triCL 是现有的 two-factor 对比学习的更好的替代方案，可以提高其可识别性和可解性，而且带来最小的开销。代码可以在 GitHub 上找到：https://github.com/PKU-ML/Tri-factor-Contrastive-Learning。
</details></li>
</ul>
<hr>
<h2 id="Multi-task-deep-learning-for-large-scale-building-detail-extraction-from-high-resolution-satellite-imagery"><a href="#Multi-task-deep-learning-for-large-scale-building-detail-extraction-from-high-resolution-satellite-imagery" class="headerlink" title="Multi-task deep learning for large-scale building detail extraction from high-resolution satellite imagery"></a>Multi-task deep learning for large-scale building detail extraction from high-resolution satellite imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18899">http://arxiv.org/abs/2310.18899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chanceqz/buildingdetails-multitask">https://github.com/chanceqz/buildingdetails-multitask</a></li>
<li>paper_authors: Zhen Qian, Min Chen, Zhuo Sun, Fan Zhang, Qingsong Xu, Jinzhao Guo, Zhiwei Xie, Zhixin Zhang</li>
<li>for: 本研究旨在提高城市动态的理解和可持续发展，通过对高分辨度卫星图像进行分析，提取建筑物的详细信息。</li>
<li>methods: 本研究提出了一种适应性的神经网络，称为多任务建筑精细化器（MT-BR），可同时提取各种建筑物的空间和属性信息，如建筑物顶、城市功能类型和瓦屋顶型。此外，MT-BR可以根据需要进行可调整，以涵盖更多的建筑物详细信息。</li>
<li>results: 研究人员通过设计一种新的空间采样方法，可以有效地选择高分辨度卫星图像的限定示例，以提高提取建筑物详细信息的效率。此外，通过启用先进的增强技术，MT-BR可以提高预测性能和泛化能力。实验结果表明，MT-BR在不同的维度上都能够达到更高的预测精度，并且在实际应用中，可以生成包含了建筑物的空间和属性信息的一体化数据集。<details>
<summary>Abstract</summary>
Understanding urban dynamics and promoting sustainable development requires comprehensive insights about buildings. While geospatial artificial intelligence has advanced the extraction of such details from Earth observational data, existing methods often suffer from computational inefficiencies and inconsistencies when compiling unified building-related datasets for practical applications. To bridge this gap, we introduce the Multi-task Building Refiner (MT-BR), an adaptable neural network tailored for simultaneous extraction of spatial and attributional building details from high-resolution satellite imagery, exemplified by building rooftops, urban functional types, and roof architectural types. Notably, MT-BR can be fine-tuned to incorporate additional building details, extending its applicability. For large-scale applications, we devise a novel spatial sampling scheme that strategically selects limited but representative image samples. This process optimizes both the spatial distribution of samples and the urban environmental characteristics they contain, thus enhancing extraction effectiveness while curtailing data preparation expenditures. We further enhance MT-BR's predictive performance and generalization capabilities through the integration of advanced augmentation techniques. Our quantitative results highlight the efficacy of the proposed methods. Specifically, networks trained with datasets curated via our sampling method demonstrate improved predictive accuracy relative to those using alternative sampling approaches, with no alterations to network architecture. Moreover, MT-BR consistently outperforms other state-of-the-art methods in extracting building details across various metrics. The real-world practicality is also demonstrated in an application across Shanghai, generating a unified dataset that encompasses both the spatial and attributional details of buildings.
</details>
<details>
<summary>摘要</summary>
理解城市动力和推动可持续发展需要全面的建筑相关信息。 although geospatial人工智能已经提高了对地球观测数据的EXTRACTION，现有的方法经常受到计算不fficient和不一致的问题，这限制了实际应用中的建筑相关数据集的编译。为了bridging这个差距，我们介绍了多任务建筑精细化器（MT-BR），这是适应同时EXTRACTION的建筑相关细节的适应性神经网络。MT-BR可以根据需要进行微调，以包含更多的建筑细节，从而扩展其可用性。为了应对大规模应用，我们提出了一种新的空间采样方案，该方案选择了有限但表示性强的图像样本。这种方法可以最大化图像样本的空间分布和城市环境特征，从而提高EXTRACTION的效果，同时降低数据准备成本。此外，我们还通过 incorporating advanced augmentation techniques 提高MT-BR的预测性能和泛化能力。我们的量化结果表明，使用我们的采样方法训练的网络比使用其他采样方法更高的预测精度，而无需修改网络结构。此外，MT-BR还在不同的维度上一直 OUTPERFORMS 其他现有的方法。此外，我们在上海应用了MT-BR，生成了一个包括建筑物的空间和特征细节的一体化数据集，这进一步证明了MT-BR的实际可行性。
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Shape-Bias-in-Convolutional-Neural-Networks-through-Activation-Sparsity"><a href="#Emergence-of-Shape-Bias-in-Convolutional-Neural-Networks-through-Activation-Sparsity" class="headerlink" title="Emergence of Shape Bias in Convolutional Neural Networks through Activation Sparsity"></a>Emergence of Shape Bias in Convolutional Neural Networks through Activation Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18894">http://arxiv.org/abs/2310.18894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/crazy-jack/nips2023_shape_vs_texture">https://github.com/crazy-jack/nips2023_shape_vs_texture</a></li>
<li>paper_authors: Tianqin Li, Ziqi Wen, Yangfan Li, Tai Sing Lee</li>
<li>for: 本研究旨在解释深度学习模型为何偏好文本，而人类视觉系统却偏好形状和结构。</li>
<li>methods: 研究人员使用 sparse coding 原理，通过非 differencing Top-K 操作来引入形状偏好到网络中。</li>
<li>results: 研究发现，在卷积神经网络中，强制执行 sparse coding 约束可以导致 neuron 中的结构编码 emerge，从而使网络具有更好的形状偏好。这种形状偏好会使网络在不同的数据集上展现出更好的鲁棒性和可变性。代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/Crazy-Jack/nips2023_shape_vs_texture%E3%80%82">https://github.com/Crazy-Jack/nips2023_shape_vs_texture。</a><details>
<summary>Abstract</summary>
Current deep-learning models for object recognition are known to be heavily biased toward texture. In contrast, human visual systems are known to be biased toward shape and structure. What could be the design principles in human visual systems that led to this difference? How could we introduce more shape bias into the deep learning models? In this paper, we report that sparse coding, a ubiquitous principle in the brain, can in itself introduce shape bias into the network. We found that enforcing the sparse coding constraint using a non-differential Top-K operation can lead to the emergence of structural encoding in neurons in convolutional neural networks, resulting in a smooth decomposition of objects into parts and subparts and endowing the networks with shape bias. We demonstrated this emergence of shape bias and its functional benefits for different network structures with various datasets. For object recognition convolutional neural networks, the shape bias leads to greater robustness against style and pattern change distraction. For the image synthesis generative adversary networks, the emerged shape bias leads to more coherent and decomposable structures in the synthesized images. Ablation studies suggest that sparse codes tend to encode structures, whereas the more distributed codes tend to favor texture. Our code is host at the github repository: \url{https://github.com/Crazy-Jack/nips2023_shape_vs_texture}
</details>
<details>
<summary>摘要</summary>
当前深度学习模型对物体识别存在强烈的文本偏好。然而，人类视觉系统却具有形态和结构偏好。这种差异的原因可能是什么？我们可以如何在深度学习模型中引入更多的形态偏好？在这篇论文中，我们发现了一种叫做稀畴编码的原则，这种原则在大脑中是普遍存在的。我们发现，通过在 convolutional neural networks 中使用不同的 Top-K 操作来实现稀畴编码约束，可以导致神经元内的编码变得更加结构化，从而使得神经网络具有形态偏好。我们通过不同的数据集来证明这种形态偏好的出现和其功能上的好处。对于物体识别 convolutional neural networks，形态偏好使得神经网络更加抗性于样式和 Pattern 变化的干扰。对于生成 adversarial networks， emerged 形态偏好导致生成的图像更加协调和可分解。我们的代码可以在 GitHub 上找到：\url{https://github.com/Crazy-Jack/nips2023_shape_vs_texture}
</details></li>
</ul>
<hr>
<h2 id="Dynamo-Depth-Fixing-Unsupervised-Depth-Estimation-for-Dynamical-Scenes"><a href="#Dynamo-Depth-Fixing-Unsupervised-Depth-Estimation-for-Dynamical-Scenes" class="headerlink" title="Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes"></a>Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18887">http://arxiv.org/abs/2310.18887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Sun, Bharath Hariharan</li>
<li>for: 本文提出了一种解决单目深度估计中动态场景中物体运动所引起的困难的方法，通过对无标注单目视频进行共同学习深度、独立流场和动作分割来解决这种问题。</li>
<li>methods: 本文提出了一种jointly学习深度和独立流场的方法，通过提供了一个关键思想，即在初始化时对运动分割有good的估计可以帮助jointly学习深度和独立运动。</li>
<li>results: 本文在 Waymo Open和nuScenes Dataset上实现了单目深度估计的state-of-the-art性能，对运动中的深度有显著改进。<details>
<summary>Abstract</summary>
Unsupervised monocular depth estimation techniques have demonstrated encouraging results but typically assume that the scene is static. These techniques suffer when trained on dynamical scenes, where apparent object motion can equally be explained by hypothesizing the object's independent motion, or by altering its depth. This ambiguity causes depth estimators to predict erroneous depth for moving objects. To resolve this issue, we introduce Dynamo-Depth, an unifying approach that disambiguates dynamical motion by jointly learning monocular depth, 3D independent flow field, and motion segmentation from unlabeled monocular videos. Specifically, we offer our key insight that a good initial estimation of motion segmentation is sufficient for jointly learning depth and independent motion despite the fundamental underlying ambiguity. Our proposed method achieves state-of-the-art performance on monocular depth estimation on Waymo Open and nuScenes Dataset with significant improvement in the depth of moving objects. Code and additional results are available at https://dynamo-depth.github.io.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>无监督单目深度估计技术已经表现出了激动人心的结果，但通常假设场景是静止的。这些技术在动态场景下遇到问题，因为 Apparent 对象的运动可以 equally 由假设对象的独立运动或者由其深度变化来解释。这种歧义导致深度估计器预测错误的深度值。为了解决这个问题，我们介绍了 Dynamo-Depth，一种统一的方法，它在不监督的单目视频上同时学习单目深度、3D 独立流场和动态分割。我们提供了关键的思路，即一个好的初始化动态分割可以为 JOINTLY 学习深度和独立运动，尽管基本的下面歧义存在。我们的提议方法在 Waymo Open 和 nuScenes 数据集上实现了状态的最佳性能，对于运动中的对象的深度有显著改善。代码和更多结果可以在 <https://dynamo-depth.github.io> 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.CV_2023_10_29/" data-id="cloimip9v00jys488bov4dsgb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.AI_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T12:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.AI_2023_10_29/">cs.AI - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="JEN-1-Composer-A-Unified-Framework-for-High-Fidelity-Multi-Track-Music-Generation"><a href="#JEN-1-Composer-A-Unified-Framework-for-High-Fidelity-Multi-Track-Music-Generation" class="headerlink" title="JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation"></a>JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19180">http://arxiv.org/abs/2310.19180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Yao, Peike Li, Boyu Chen, Alex Wang</li>
<li>for: 本研究旨在提出一种能够实现高级控制的多轨音乐生成模型，以便用户可以通过 iterative 选择和修改音乐轨迹，创造出符合自己的音乐创作审美。</li>
<li>methods: 本研究使用了 JEN-1 算法，并提出了一种带有评估策略的训练策略，以便帮助模型在多轨音乐生成中具备更高的灵活性和控制能力。</li>
<li>results: 对比于现有的音乐生成模型，JEN-1 Composer 能够实现更高的音乐质量和控制能力，并且可以在用户提供的音乐风格和元素的基础上进行高级的音乐创作。<details>
<summary>Abstract</summary>
With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible generation of multi-track combinations. During the inference, users have the ability to iteratively produce and choose music tracks that meet their preferences, subsequently creating an entire musical composition incrementally following the proposed Human-AI co-composition workflow. Quantitative and qualitative assessments demonstrate state-of-the-art performance in controllable and high-fidelity multi-track music synthesis. The proposed JEN-1 Composer represents a significant advance toward interactive AI-facilitated music creation and composition. Demos will be available at https://jenmusic.ai/audio-demos.
</details>
<details>
<summary>摘要</summary>
With the rapid development of generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models have strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. The JEN-1 Composer framework can seamlessly incorporate any diffusion-based music generation system, such as Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible generation of multi-track combinations. During the inference, users have the ability to iteratively produce and choose music tracks that meet their preferences, subsequently creating an entire musical composition incrementally following the proposed Human-AI co-composition workflow. Quantitative and qualitative assessments demonstrate state-of-the-art performance in controllable and high-fidelity multi-track music synthesis. The proposed JEN-1 Composer represents a significant advance toward interactive AI-facilitated music creation and composition. Demos will be available at https://jenmusic.ai/audio-demos.
</details></li>
</ul>
<hr>
<h2 id="Predicting-recovery-following-stroke-deep-learning-multimodal-data-and-feature-selection-using-explainable-AI"><a href="#Predicting-recovery-following-stroke-deep-learning-multimodal-data-and-feature-selection-using-explainable-AI" class="headerlink" title="Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI"></a>Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19174">http://arxiv.org/abs/2310.19174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam White, Margarita Saranti, Artur d’Avila Garcez, Thomas M. H. Hope, Cathy J. Price, Howard Bowman</li>
<li>for: 这 paper 的目的是使用机器学习自动预测 stroke 后症状和其回归治疗的效果。</li>
<li>methods: 这 paper 使用了两种策略：首先使用 2D 图像概述 MRI 扫描结果，其次选择关键特征以提高分类精度。此外，文章还介绍了一种新的方法，即在 MRI 图像和表格数据之间融合学习。</li>
<li>results: 文章的结果显示，可以通过组合 MRI 图像和表格数据来实现高精度的 post-stroke 分类。在不同的 CNN 架构和数据 Representation 下，分类精度最高达 0.854。<details>
<summary>Abstract</summary>
Machine learning offers great potential for automated prediction of post-stroke symptoms and their response to rehabilitation. Major challenges for this endeavour include the very high dimensionality of neuroimaging data, the relatively small size of the datasets available for learning, and how to effectively combine neuroimaging and tabular data (e.g. demographic information and clinical characteristics). This paper evaluates several solutions based on two strategies. The first is to use 2D images that summarise MRI scans. The second is to select key features that improve classification accuracy. Additionally, we introduce the novel approach of training a convolutional neural network (CNN) on images that combine regions-of-interest extracted from MRIs, with symbolic representations of tabular data. We evaluate a series of CNN architectures (both 2D and a 3D) that are trained on different representations of MRI and tabular data, to predict whether a composite measure of post-stroke spoken picture description ability is in the aphasic or non-aphasic range. MRI and tabular data were acquired from 758 English speaking stroke survivors who participated in the PLORAS study. The classification accuracy for a baseline logistic regression was 0.678 for lesion size alone, rising to 0.757 and 0.813 when initial symptom severity and recovery time were successively added. The highest classification accuracy 0.854 was observed when 8 regions-of-interest was extracted from each MRI scan and combined with lesion size, initial severity and recovery time in a 2D Residual Neural Network.Our findings demonstrate how imaging and tabular data can be combined for high post-stroke classification accuracy, even when the dataset is small in machine learning terms. We conclude by proposing how the current models could be improved to achieve even higher levels of accuracy using images from hospital scanners.
</details>
<details>
<summary>摘要</summary>
Machine learning可以提供很大的潜在 для自动预测 poste stroke 症状和其回归治疗的结果。主要挑战包括神经成像数据的非常高维度，可用学习 dataset 的较小尺寸，以及如何有效地结合神经成像和表格数据（例如人口信息和临床特征）。本文评估了多种解决方案，包括使用 2D 图像简化 MRI 扫描结果，以及选择关键特征来提高分类精度。此外，我们还引入了一种新的方法，即在 MRI 扫描结果和表格数据之间进行 симвоlic 表示的训练 convolutional neural network (CNN)。我们评估了一系列 CNN 架构（包括 2D 和 3D），并在不同的 MRI 和表格数据表示下进行训练，以预测stroke 后 spoken picture 描述能力是否在非语症状范围内。MRI 和表格数据来自英国758名中文roke 幸存者参与了PLORAS 研究。分类精度的最高值为 0.854，出现在使用 8 个区域特征 Extracted from each MRI scan 和 lesion size、初始症状严重程度和Recovery time 的 2D Residual Neural Network 中。我们的发现表明，通过结合神经成像和表格数据，可以实现高度的 poste stroke 分类精度，即使数据集较小。我们结束的建议如下：通过使用医疗器械上的图像，可以进一步提高当前模型的准确率。
</details></li>
</ul>
<hr>
<h2 id="Rare-Event-Probability-Learning-by-Normalizing-Flows"><a href="#Rare-Event-Probability-Learning-by-Normalizing-Flows" class="headerlink" title="Rare Event Probability Learning by Normalizing Flows"></a>Rare Event Probability Learning by Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19167">http://arxiv.org/abs/2310.19167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenggqi Gao, Dinghuai Zhang, Luca Daniel, Duane S. Boning<br>for:NOFIS 是一种用于估计罕seen事件的方法，可以在多种领域中提供精确的估计。methods:NOFIS 使用了 normalizing flows 的特点，通过学习一系列的提案分布来实现高效的估计。results:NOFIS 在多个测试 caso 中表现出色，superior 于基eline方法，并且可以提供高质量的估计结果。<details>
<summary>Abstract</summary>
A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing $10$ distinct test cases, which highlight NOFIS's superiority over baseline approaches.
</details>
<details>
<summary>摘要</summary>
一种罕见事件被定义为发生的概率很低。正确地估计这种小概率是多种领域的关键问题。传统的 Монте卡洛方法是不具有效率，需要很多样本来获得可靠的估计。以启发式扩展流为引用，我们再次挑战这个问题，并提出了正则化流助けimportance sampling（NOFIS）方法。NOFIS首先学习一个序列的提议分布，这些分布与预先定义的嵌套子事件相关。然后，它使用重要样本法并与最后一个提议相结合，来估计罕见事件的概率。我们通过对多个测试案例进行详细的可见化，证明了提议分布的优化性，以及对基eline方法的超越。
</details></li>
</ul>
<hr>
<h2 id="Automaton-Distillation-Neuro-Symbolic-Transfer-Learning-for-Deep-Reinforcement-Learning"><a href="#Automaton-Distillation-Neuro-Symbolic-Transfer-Learning-for-Deep-Reinforcement-Learning" class="headerlink" title="Automaton Distillation: Neuro-Symbolic Transfer Learning for Deep Reinforcement Learning"></a>Automaton Distillation: Neuro-Symbolic Transfer Learning for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19137">http://arxiv.org/abs/2310.19137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suraj Singireddy, Andre Beckus, George Atia, Sumit Jha, Alvaro Velasquez</li>
<li>for: 这篇论文旨在应用自适应学习（Reinforcement Learning，RL）来找到最佳策略，但深度RL方法受到两个弱点：需要大量的机器人体验，并且学习的策略对于训练分布外的任务呈现出差强。</li>
<li>methods: 我们提出了两种方法来生成Q值估计：静止转移，它是基于先前知识建立的抽象Markov Decision Process（MDP）上进行推理，以及动态转移，它是从教师Deep Q-Network（DQN）提取Symbolic信息。</li>
<li>results: 我们的实验结果显示，静止转移和动态转移都可以减少获得最佳策略所需的时间，并且在不同的决策任务中均有良好的表现。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a powerful tool for finding optimal policies in sequential decision processes. However, deep RL methods suffer from two weaknesses: collecting the amount of agent experience required for practical RL problems is prohibitively expensive, and the learned policies exhibit poor generalization on tasks outside of the training distribution. To mitigate these issues, we introduce automaton distillation, a form of neuro-symbolic transfer learning in which Q-value estimates from a teacher are distilled into a low-dimensional representation in the form of an automaton. We then propose two methods for generating Q-value estimates: static transfer, which reasons over an abstract Markov Decision Process constructed based on prior knowledge, and dynamic transfer, where symbolic information is extracted from a teacher Deep Q-Network (DQN). The resulting Q-value estimates from either method are used to bootstrap learning in the target environment via a modified DQN loss function. We list several failure modes of existing automaton-based transfer methods and demonstrate that both static and dynamic automaton distillation decrease the time required to find optimal policies for various decision tasks.
</details>
<details>
<summary>摘要</summary>
《强化学习（RL）是一种有力的工具，可以找到sequential decision process中的优化策略。但是深度RL方法受到两点弱点：收集agent经验所需的成本是实际RL问题中 prohibitively expensive，并且学习的策略具有poor generalization在训练分布外的任务上。为了缓解这些问题，我们引入自动机液化，一种neuro-symbolic transfer learning的形式，其中Q值估计从教师中提取到一个低维度表示形式中，这个形式是一个自动机。然后我们提出了两种方法来生成Q值估计：静态传输，这里是reasoning over一个基于先前知识构建的抽象Markov决策过程，以及动态传输，其中Symbolic信息从一个教师深度Q网络（DQN）中提取出来。这些Q值估计的结果被用来启动target环境中的学习，通过一个修改后DQN损失函数。我们列出了现有自动机基于转移方法的失败模式，并证明了静态和动态自动机液化都可以降低在不同决策任务中找到优化策略所需的时间。》Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Three-Dogmas-a-Puzzle-and-its-Solution"><a href="#Three-Dogmas-a-Puzzle-and-its-Solution" class="headerlink" title="Three Dogmas, a Puzzle and its Solution"></a>Three Dogmas, a Puzzle and its Solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19123">http://arxiv.org/abs/2310.19123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elnaserledinellah Mahmood Abdelwahab<br>for:The paper challenges the assumptions of Modern Logics, particularly those of Frege, Russell, and Tarski, and their applications in formal languages.methods:The paper uses undisputed principles of Arabic to falsify the Logicians’ ideas and demonstrate the limitations of their approaches. It also utilizes the existence of “meaning-particles” in Arabic syntax to efficiently recognize words, phrases, and sentences.results:The paper shows that the assumptions of Modern Logics contradict basic principles of Arabic, and that the approaches based on these assumptions are not applicable to Arabic. It also presents a new way to approach the computational problem of Satisfiability (SAT) using the realization that parsing Arabic utilizes the existence of “meaning-particles” within syntax. The paper provides practical evidence, obtained for multiplication circuits, supporting its claims.<details>
<summary>Abstract</summary>
Modern Logics, as formulated notably by Frege, Russell and Tarski involved basic assumptions about Natural Languages in general and Indo-European Languages in particular, which are contested by Linguists. Based upon those assumptions, formal Languages were designed to overcome what Logicians claimed to be 'defects' of Natural Language. In this paper we show that those assumptions contradict basic principles of Arabic. More specifically: The Logicians ideas, that within Natural Language words refer to objects, 'ToBe'-constructions represent identity statements, Indefinite Descriptions must be replaced by existential quantifiers to form meaningful Sentences and Symbols can have no interpretation-independent meanings, are all falsified using undisputed principles of Arabic. The here presented falsification serves two purposes. First, it is used as a factual basis for the rejection of approaches adopting Semantic axioms of Mathematical Logics as Models for meaning of Arabic Syntax. Second, it shows a way to approach the important computational problem: Satisfiability (SAT). The described way is based upon the realization that parsing Arabic utilizes the existence of 'meaning-particles' within Syntax to efficiently recognize words, phrases and Sentences. Similar meaning-particles are shown to exist in 3CNF formulas, which, when properly handled within the machinery of 3SAT-Solvers, enable structural conditions to be imposed on formulas, sufficient alone to guarantee the efficient production of non-exponentially sized Free Binary Decision Diagrams (FBDDs). We show, why known exponential Lower Bounds on sizes of FBDDs do not contradict our results and reveal practical evidence, obtained for multiplication circuits, supporting our claims.
</details>
<details>
<summary>摘要</summary>
现代逻辑，如Frege、Russell和Tarski所提出的基本假设，对于自然语言和印欧语言而言都存在争议。基于这些假设，形式语言被设计用于超越逻辑家所认为自然语言存在的缺陷。在这篇论文中，我们表明了这些假设与阿拉伯语言的基本原理矛盾。具体来说，逻辑家所认为的各种假设，如自然语言中词语引用对象、'ToBe'-构造表示Identidad句子、不确定描述需要通过存在量词替换来形成意义的句子，以及符号没有独立的解释意义，都被使用不争的阿拉伯语言原理驳斥。这种驳斥服两目的。首先，它用作拒绝采用数学逻辑语义模型的方法的拒绝基础。其次，它显示了如何使用阿拉伯语言的存在意思粒子来效率地识别单词、短语和句子。这种方法还可以应用于计算问题：满足（SAT）。我们表明了如何使用这种方法，并解释了为什么已知的下界不会与我们的结果相矛盾。此外，我们还提供了实践证据，来支持我们的主张。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-V2X-Autonomous-Perception-from-Road-to-Vehicle-Vision"><a href="#Dynamic-V2X-Autonomous-Perception-from-Road-to-Vehicle-Vision" class="headerlink" title="Dynamic V2X Autonomous Perception from Road-to-Vehicle Vision"></a>Dynamic V2X Autonomous Perception from Road-to-Vehicle Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19113">http://arxiv.org/abs/2310.19113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayao Tan, Fan Lyu, Linyan Li, Fuyuan Hu, Tingliang Feng, Fenglei Xu, Rui Yao<br>for: 提高自动驾驶系统的安全性和可靠性，适应动态场景methods: 基于路径视觉建立道路到车辆视觉，提出适应性强的道路到车辆视觉感知方法（AR2VP）results: 在3D对象检测和分割任务中，AR2VP在性能和带宽之间做出了优秀的折衔，同时在动态环境中保持了模型的适应性。<details>
<summary>Abstract</summary>
Vehicle-to-everything (V2X) perception is an innovative technology that enhances vehicle perception accuracy, thereby elevating the security and reliability of autonomous systems. However, existing V2X perception methods focus on static scenes from mainly vehicle-based vision, which is constrained by sensor capabilities and communication loads. To adapt V2X perception models to dynamic scenes, we propose to build V2X perception from road-to-vehicle vision and present Adaptive Road-to-Vehicle Perception (AR2VP) method. In AR2VP,we leverage roadside units to offer stable, wide-range sensing capabilities and serve as communication hubs. AR2VP is devised to tackle both intra-scene and inter-scene changes. For the former, we construct a dynamic perception representing module, which efficiently integrates vehicle perceptions, enabling vehicles to capture a more comprehensive range of dynamic factors within the scene.Moreover, we introduce a road-to-vehicle perception compensating module, aimed at preserving the maximized roadside unit perception information in the presence of intra-scene changes.For inter-scene changes, we implement an experience replay mechanism leveraging the roadside unit's storage capacity to retain a subset of historical scene data, maintaining model robustness in response to inter-scene shifts. We conduct perception experiment on 3D object detection and segmentation, and the results show that AR2VP excels in both performance-bandwidth trade-offs and adaptability within dynamic environments.
</details>
<details>
<summary>摘要</summary>
自动驾驶系统的安全和可靠性得到了提高，由于交通场景的变化和不确定性，需要进一步提高汽车的感知精度。现有的V2X感知方法都是基于主要是汽车视觉的静止场景，受到感知器和通信负担的限制。为了适应动态场景，我们提出了基于路面到汽车视觉的Adaptive Road-to-Vehicle Perception（AR2VP）方法。在AR2VP中，我们利用路边设备提供稳定、广泛感知能力，并作为通信枢纽。AR2VP能够应对内场景和间场景变化。对于内场景变化，我们构建了动态感知表示模块，能够有效地集成汽车感知，让汽车能够捕捉更广泛的动态因素。此外，我们引入了路面到汽车感知补做模块，以保持路边设备感知信息的最大化，对于内场景变化。对于间场景变化，我们实施了经验回放机制，利用路边设备的存储容量保留一部分历史场景数据，以保持模型对间场景变化的Robustness。我们对3D объек特征检测和分割进行感知实验，结果显示，AR2VP在性能和带宽之间的负担平衡和动态环境中的适应性都表现出色。
</details></li>
</ul>
<hr>
<h2 id="Efficient-IoT-Inference-via-Context-Awareness"><a href="#Efficient-IoT-Inference-via-Context-Awareness" class="headerlink" title="Efficient IoT Inference via Context-Awareness"></a>Efficient IoT Inference via Context-Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19112">http://arxiv.org/abs/2310.19112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mehdi Rastikerdar, Jin Huang, Shiwei Fang, Hui Guan, Deepak Ganesan</li>
<li>for: 提高深度学习模型在有限资源环境中的表现，特别是在iot设备上。</li>
<li>methods: 提出了一种新的 paradigm，即CACTUS，它可以快速、可扩展地在不同的上下文中进行Context-aware classification。CACTUS包括了三个innovation：1）优化上下文意识类фика器的训练成本，2）实现在线上下文意识切换，3）根据有限资源选择最佳上下文意识类фика器。</li>
<li>results: CACTUS在多种数据集和iot平台上实现了显著的准确率、响应时间和计算预算的改善。<details>
<summary>Abstract</summary>
While existing strategies for optimizing deep learning-based classification models on low-power platforms assume the models are trained on all classes of interest, this paper posits that adopting context-awareness i.e. focusing solely on the likely classes in the current context, can substantially enhance performance in resource-constrained environments. We propose a new paradigm, CACTUS, for scalable and efficient context-aware classification where a micro-classifier recognizes a small set of classes relevant to the current context and, when context change happens, rapidly switches to another suitable micro-classifier. CACTUS has several innovations including optimizing the training cost of context-aware classifiers, enabling on-the-fly context-aware switching between classifiers, and selecting the best context-aware classifiers given limited resources. We show that CACTUS achieves significant benefits in accuracy, latency, and compute budget across a range of datasets and IoT platforms.
</details>
<details>
<summary>摘要</summary>
While existing strategies for optimizing deep learning-based classification models on low-power platforms assume the models are trained on all classes of interest, this paper proposes a new approach that focuses solely on the likely classes in the current context, which can significantly enhance performance in resource-constrained environments. The proposed paradigm, CACTUS, is designed for scalable and efficient context-aware classification, where a micro-classifier recognizes a small set of classes relevant to the current context and can rapidly switch to another suitable micro-classifier when context changes occur. CACTUS has several innovative features, including optimizing the training cost of context-aware classifiers, enabling on-the-fly context-aware switching between classifiers, and selecting the best context-aware classifiers given limited resources. The paper shows that CACTUS achieves significant benefits in accuracy, latency, and compute budget across a range of datasets and IoT platforms.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Task-and-Weight-Prioritization-Curriculum-Learning-for-Multimodal-Imagery"><a href="#Dynamic-Task-and-Weight-Prioritization-Curriculum-Learning-for-Multimodal-Imagery" class="headerlink" title="Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery"></a>Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19109">http://arxiv.org/abs/2310.19109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fualsan/datwep">https://github.com/fualsan/datwep</a></li>
<li>paper_authors: Huseyin Fuat Alsan, Taner Arsan<br>for:这篇论文探索了在多modal深度学习模型下进行后灾分析，使用了curriculum learning方法来优化模型的性能。methods:这篇论文提出了一个curriculum learning策略，通过让深度学习模型在增加复杂性的数据上进行运动，以提高模型的性能。这篇论文使用了U-Net模型进行semantic segmentation和图像编码，并使用了自定义的文本分类器进行视觉问题回答。results:这篇论文的结果显示， DATWEP方法可以帮助提高多modal深度学习模型的视觉问题回答性能。 sources code可以在<a target="_blank" rel="noopener" href="https://github.com/fualsan/DATWEP%E4%B8%8A%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/fualsan/DATWEP上取得。</a><details>
<summary>Abstract</summary>
This paper explores post-disaster analytics using multimodal deep learning models trained with curriculum learning method. Studying post-disaster analytics is important as it plays a crucial role in mitigating the impact of disasters by providing timely and accurate insights into the extent of damage and the allocation of resources. We propose a curriculum learning strategy to enhance the performance of multimodal deep learning models. Curriculum learning emulates the progressive learning sequence in human education by training deep learning models on increasingly complex data. Our primary objective is to develop a curriculum-trained multimodal deep learning model, with a particular focus on visual question answering (VQA) capable of jointly processing image and text data, in conjunction with semantic segmentation for disaster analytics using the FloodNet\footnote{https://github.com/BinaLab/FloodNet-Challenge-EARTHVISION2021} dataset. To achieve this, U-Net model is used for semantic segmentation and image encoding. A custom built text classifier is used for visual question answering. Existing curriculum learning methods rely on manually defined difficulty functions. We introduce a novel curriculum learning approach termed Dynamic Task and Weight Prioritization (DATWEP), which leverages a gradient-based method to automatically decide task difficulty during curriculum learning training, thereby eliminating the need for explicit difficulty computation. The integration of DATWEP into our multimodal model shows improvement on VQA performance. Source code is available at https://github.com/fualsan/DATWEP.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Web3-Meets-AI-Marketplace-Exploring-Opportunities-Analyzing-Challenges-and-Suggesting-Solutions"><a href="#Web3-Meets-AI-Marketplace-Exploring-Opportunities-Analyzing-Challenges-and-Suggesting-Solutions" class="headerlink" title="Web3 Meets AI Marketplace: Exploring Opportunities, Analyzing Challenges, and Suggesting Solutions"></a>Web3 Meets AI Marketplace: Exploring Opportunities, Analyzing Challenges, and Suggesting Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19099">http://arxiv.org/abs/2310.19099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peihao Li</li>
<li>for: 这篇论文旨在探讨AI和Web3的交叉领域，即DeAI，以及该领域的机遇和挑战。</li>
<li>methods: 本文使用了一种框架，让用户可以使用任何种投资链urrency来购买AI服务，同时也可以在协议中短时间锁定资产来获得免费AI服务。</li>
<li>results: 本文提出了一种解决AI市场在Web3空间快速发展的方案，并且打开了该领域的新的商业机会。<details>
<summary>Abstract</summary>
Web3 and AI have been among the most discussed fields over the recent years, with substantial hype surrounding each field's potential to transform the world as we know it. However, as the hype settles, it's evident that neither AI nor Web3 can address all challenges independently. Consequently, the intersection of AI and Web3 is gaining increased attention, emerging as a new field with the potential to address the limitations of each. In this article, we will focus on the integration of web3 and the AI marketplace, where AI services and products can be provided in a decentralized manner (DeAI). A comprehensive review is provided by summarizing the opportunities and challenges on this topic. Additionally, we offer analyses and solutions to address these challenges. We've developed a framework that lets users pay with any kind of cryptocurrency to get AI services. Additionally, they can also enjoy AI services for free on our platform by simply locking up their assets temporarily in the protocol. This unique approach is a first in the industry. Before this, offering free AI services in the web3 community wasn't possible. Our solution opens up exciting opportunities for the AI marketplace in the web3 space to grow and be widely adopted.
</details>
<details>
<summary>摘要</summary>
“Web3和人工智能（AI）在最近几年内得到了很多关注，但是它们无法独立解决全部问题。因此，Web3和AI之间的交叉领域正在吸引越来越多的关注，并且被认为可以抵消每个领域的局限性。本文将关注Web3和AI市场的融合，即DeAI（Decentralized AI）。我们将提供全面的机会和挑战的概述，以及解决这些挑战的分析和解决方案。我们已经开发了一套框架，允许用户使用任何种 криптовалю来购买AI服务。此外，用户还可以在我们的平台上免费使用AI服务，只需将资产短时间内锁定在协议中。这种独特的方法是行业中的首次实践。在这之前，在Web3社区中无法免费提供AI服务。我们的解决方案将推动AI市场在Web3空间广泛采用和普及。”
</details></li>
</ul>
<hr>
<h2 id="Roles-of-Scaling-and-Instruction-Tuning-in-Language-Perception-Model-vs-Human-Attention"><a href="#Roles-of-Scaling-and-Instruction-Tuning-in-Language-Perception-Model-vs-Human-Attention" class="headerlink" title="Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention"></a>Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19084">http://arxiv.org/abs/2310.19084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RiverGao/human_llm_attention">https://github.com/RiverGao/human_llm_attention</a></li>
<li>paper_authors: Changjiang Gao, Shujian Huang, Jixing Li, Jiajun Chen</li>
<li>for: 这研究旨在调查大型自然语言处理器（LLMs）在培训过程中的成功因素，以及这些因素对模型的语言理解能力的影响。</li>
<li>methods: 这研究使用了多种现有的LLMs（LLaMA、Alpaca和Vicuna），并对不同大小（7B、13B、30B、65B）进行比较，以评估 scaling和指令调整对语言理解的影响。</li>
<li>results: 结果显示， scaling 可以增强人类阅读注意力的效果，并减少无关的模式依赖性，而 instruction tuning 则不会对语言理解产生影响。此外，现有的 LLMS 都在注意力方面存在一定的不足，它们的注意力更接近非native than native 语言。<details>
<summary>Abstract</summary>
Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. However, how these factors affect the models' language perception is unclear. This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human reading attention, to assess the effect of scaling and instruction tuning on language perception. Results show that scaling enhances the human resemblance and improves the effective attention by reducing the trivial pattern reliance, while instruction tuning does not. However, instruction tuning significantly enhances the models' sensitivity to instructions. We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models. Our code and data used in the analysis is available on GitHub.
</details>
<details>
<summary>摘要</summary>
最近的大型语言模型（LLMs）表现出了对自然语言的强大理解能力。由于大多数这些模型具有相同的基本结构，即转换块，因此可能的贡献因素包括扩大和指导调整。然而，这些因素对模型的语言识别是如何影响的还未清楚。这项工作比较了一些现有的LLMs（LLaMA、Alpaca和Vicuna）在不同大小（7B、13B、30B、65B）中的自我注意力，以及人类阅读注意力的眼动踪迹，以评估扩大和指导调整对语言识别的影响。结果显示，扩大可以提高人类类似性和有效注意力，而减少了杂乱模式的依赖性。然而，指导调整并没有这样的效果。此外，我们发现现有的LLMs在注意力方面都更接近非本地语言 speaker，这表明所有模型的语言识别能力有所不足。我们在 GitHub 上提供了代码和数据，用于进行分析。
</details></li>
</ul>
<hr>
<h2 id="Bespoke-Solvers-for-Generative-Flow-Models"><a href="#Bespoke-Solvers-for-Generative-Flow-Models" class="headerlink" title="Bespoke Solvers for Generative Flow Models"></a>Bespoke Solvers for Generative Flow Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19075">http://arxiv.org/abs/2310.19075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neta Shaul, Juan Perez, Ricky T. Q. Chen, Ali Thabet, Albert Pumarola, Yaron Lipman</li>
<li>for: 这个论文是为了提高Diffusion或流体模型的生成能力而写的。</li>
<li>methods: 这个论文使用了” Bespoke solvers”，一种新的框架，用于构建特定的ODE解。</li>
<li>results: 这个论文的结果表明，使用” Bespoke solvers”可以大幅提高生成质量，只需要1%的GPU时间和80个学习参数。<details>
<summary>Abstract</summary>
Diffusion or flow-based models are powerful generative paradigms that are notoriously hard to sample as samples are defined as solutions to high-dimensional Ordinary or Stochastic Differential Equations (ODEs/SDEs) which require a large Number of Function Evaluations (NFE) to approximate well. Existing methods to alleviate the costly sampling process include model distillation and designing dedicated ODE solvers. However, distillation is costly to train and sometimes can deteriorate quality, while dedicated solvers still require relatively large NFE to produce high quality samples. In this paper we introduce "Bespoke solvers", a novel framework for constructing custom ODE solvers tailored to the ODE of a given pre-trained flow model. Our approach optimizes an order consistent and parameter-efficient solver (e.g., with 80 learnable parameters), is trained for roughly 1% of the GPU time required for training the pre-trained model, and significantly improves approximation and generation quality compared to dedicated solvers. For example, a Bespoke solver for a CIFAR10 model produces samples with Fr\'echet Inception Distance (FID) of 2.73 with 10 NFE, and gets to 1% of the Ground Truth (GT) FID (2.59) for this model with only 20 NFE. On the more challenging ImageNet-64$\times$64, Bespoke samples at 2.2 FID with 10 NFE, and gets within 2% of GT FID (1.71) with 20 NFE.
</details>
<details>
<summary>摘要</summary>
Diffusion或流程基本模型是一种强大的生成概念，但它们很难进行样本生成，因为样本是定义为高维度常微方程（ODE）或随机常微方程（SDE）的解。现有的方法可以减少样本生成的成本，包括模型热塑化和专门设计的ODE解程。然而，热塑化训练成本较高，并且有时会降低质量，而专门的解程仍然需要相对较多的功能评估（NFE）来生成高质量的样本。在这篇论文中，我们介绍了“特制解程”，一种新的框架，用于建立针对已经训练的流变模型的自定义ODE解程。我们的方法通过优化一个与顺序数相同的并高效的参数（例如80个学习参数），在约1%的GPU时间上训练，并显著改善了样本生成和预测质量，相比特定解程。例如，一个特制解程为CIFAR10模型生成的样本的Fréchet Inception Distance（FID）为2.73，只需10个NFE，并在20个NFE下达到1%的原始真实值（GT）FID（2.59）。在更加困难的ImageNet-64×64上，特制样本的FID为2.2，只需10个NFE，并在20个NFE下达到2%的GT FID（1.71）。
</details></li>
</ul>
<hr>
<h2 id="Gauge-optimal-approximate-learning-for-small-data-classification-problems"><a href="#Gauge-optimal-approximate-learning-for-small-data-classification-problems" class="headerlink" title="Gauge-optimal approximate learning for small data classification problems"></a>Gauge-optimal approximate learning for small data classification problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19066">http://arxiv.org/abs/2310.19066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edoardo Vecchi, Davide Bassetti, Fabio Graziato, Lukas Pospisil, Illia Horenko</li>
<li>for: 这篇论文是为了解决小数据学问题， Specifically, the paper aims to address small data learning problems, where there is a significant discrepancy between the limited amount of response variable observations and the large feature space dimension.</li>
<li>methods: 本论文提出了一个新的方法，即对焦点测量（Gauge-Optimal Approximate Learning，GOAL）算法，这个算法可以实现缩减和旋转特征空间，并提供一个分析可能的解方案。 The paper proposes a new method called Gauge-Optimal Approximate Learning (GOAL) algorithm, which can reduce and rotate the feature space and provide an analytically tractable joint solution to the dimension reduction, feature segmentation, and classification problems for small data learning problems.</li>
<li>results: 实验结果显示， compared to other state-of-the-art machine learning (ML) tools, the proposed GOAL algorithm outperforms the reported best competitors for these problems both in learning performance and computational cost. The experimental results show that the proposed algorithm can accurately classify the data and is more efficient than other methods.<details>
<summary>Abstract</summary>
Small data learning problems are characterized by a significant discrepancy between the limited amount of response variable observations and the large feature space dimension. In this setting, the common learning tools struggle to identify the features important for the classification task from those that bear no relevant information, and cannot derive an appropriate learning rule which allows to discriminate between different classes. As a potential solution to this problem, here we exploit the idea of reducing and rotating the feature space in a lower-dimensional gauge and propose the Gauge-Optimal Approximate Learning (GOAL) algorithm, which provides an analytically tractable joint solution to the dimension reduction, feature segmentation and classification problems for small data learning problems. We prove that the optimal solution of the GOAL algorithm consists in piecewise-linear functions in the Euclidean space, and that it can be approximated through a monotonically convergent algorithm which presents -- under the assumption of a discrete segmentation of the feature space -- a closed-form solution for each optimization substep and an overall linear iteration cost scaling. The GOAL algorithm has been compared to other state-of-the-art machine learning (ML) tools on both synthetic data and challenging real-world applications from climate science and bioinformatics (i.e., prediction of the El Nino Southern Oscillation and inference of epigenetically-induced gene-activity networks from limited experimental data). The experimental results show that the proposed algorithm outperforms the reported best competitors for these problems both in learning performance and computational cost.
</details>
<details>
<summary>摘要</summary>
小数据学习问题特征在于响应变量观察数量较少，而特征空间维度很大。在这种情况下，常见的学习工具困难分化重要的特征和无关信息，并 derivate一个适当的学习规则来区分不同的类别。为解决这个问题，我们利用减少和旋转特征空间的低维度投影的想法，并提出了一种可解算的GOAL算法，该算法可以同时解决小数据学习问题中的维度减少、特征分解和分类问题。我们证明了GOAL算法的优化解决方案是均匀分割的几何函数，并且可以通过一个 monotonically convergent 算法来approximate，该算法在特征空间的精确分割情况下具有closed-form解决方案和linear iteration cost scaling。GOAL算法与其他当前领先的机器学习工具进行比较，在 sintetic data 和挑战性的实际应用中（如气候科学和生物信息学）表现出色，其性能和计算成本均高于报道的最佳竞争对手。
</details></li>
</ul>
<hr>
<h2 id="A-multi-modal-table-tennis-robot-system"><a href="#A-multi-modal-table-tennis-robot-system" class="headerlink" title="A multi-modal table tennis robot system"></a>A multi-modal table tennis robot system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19062">http://arxiv.org/abs/2310.19062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Ziegler, Thomas Gossard, Karl Vetter, Jonas Tebbe, Andreas Zell</li>
<li>for: 研究人员透过设计一个高精度视觉检测和快速机器人反应的推进系统，以解决机器人网球游戏中的视觉感知和机器人控制问题。</li>
<li>methods: 本研究使用了KUKA机器人臂，配备了6DOF的四个架构式摄像头和两个事件式摄像头，并开发了一个新的检测和均衡方法以测量这个多modal的感知系统。</li>
<li>results: 研究人员透过调整该多modal的感知系统，提高了网球的推进精度和速度，并且引入了一个新的旋转估计方法以提高网球的旋转精度。最后，研究人员还展示了结合事件式摄像头和神经网络的精度实时网球检测方法。<details>
<summary>Abstract</summary>
In recent years, robotic table tennis has become a popular research challenge for perception and robot control. Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction. Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras. We developed a novel calibration approach to calibrate this multimodal perception system. For table tennis, spin estimation is crucial. Therefore, we introduced a novel, and more accurate spin estimation approach. Finally, we show how combining the output of an event-based camera and a Spiking Neural Network (SNN) can be used for accurate ball detection.
</details>
<details>
<summary>摘要</summary>
Recently, robotic table tennis has become a popular research challenge for perception and robot control. Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction. Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras. We developed a novel calibration approach to calibrate this multimodal perception system. For table tennis, spin estimation is crucial. Therefore, we introduced a novel, and more accurate spin estimation approach. Finally, we show how combining the output of an event-based camera and a Spiking Neural Network (SNN) can be used for accurate ball detection.Translation in Simplified Chinese:近年来，机器人乒乓球成为了视觉和机器人控制领域的流行研究挑战。在这里，我们提出了一个改进型的乒乓球机器人系统，拥有高精度视觉检测和快速机器人反应。基于先前的工作，我们的系统包括一个KUKA机器人臂 WITH 6 DOE，以及四个帧基的摄像头和两个事件基的摄像头。我们开发了一种新的委外纠偏方法来委外这个多模态感知系统。为乒乓球而言，轨迹估计是非常重要的。因此，我们引入了一种新的、更加准确的轨迹估计方法。最后，我们示出了将事件基摄像头的输出和神经网络（SNN）结合使用，可以实现高精度的球体检测。
</details></li>
</ul>
<hr>
<h2 id="TESTA-Temporal-Spatial-Token-Aggregation-for-Long-form-Video-Language-Understanding"><a href="#TESTA-Temporal-Spatial-Token-Aggregation-for-Long-form-Video-Language-Understanding" class="headerlink" title="TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding"></a>TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19060">http://arxiv.org/abs/2310.19060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/renshuhuai-andy/testa">https://github.com/renshuhuai-andy/testa</a></li>
<li>paper_authors: Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, Lu Hou</li>
<li>for: 提高视频语言理解任务的效率，尤其是对长形视频的处理。</li>
<li>methods: 提出了一种名为 TESTA（时空Token汇集）的有效方法，通过适应地聚合相似帧和帧内相似区域来缩短视频semantics。</li>
<li>results: TESTA可以减少视频Token的数量，提高视频编码的效率，并且在五个数据集上进行了段落到视频检索和长形视频问答任务的实验，经验表明，TESTA可以提高计算效率1.7倍，并且可以充分利用更长的输入帧，例如+13.7 R@1 on QuerYD和+6.5 R@1 on Condensed Movie。<details>
<summary>Abstract</summary>
Large-scale video-language pre-training has made remarkable strides in advancing video-language understanding tasks. However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos. These videos contain massive visual tokens due to their inherent 3D properties and spatiotemporal redundancy, making it challenging to capture complex temporal and spatial relationships. To tackle this issue, we propose an efficient method called TEmporal-Spatial Token Aggregation (TESTA). TESTA condenses video semantics by adaptively aggregating similar frames, as well as similar patches within each frame. TESTA can reduce the number of visual tokens by 75% and thus accelerate video encoding. Building upon TESTA, we introduce a pre-trained video-language model equipped with a divided space-time token aggregation module in each video encoder block. We evaluate our model on five datasets for paragraph-to-video retrieval and long-form VideoQA tasks. Experimental results show that TESTA improves computing efficiency by 1.7 times, and achieves significant performance gains from its scalability in processing longer input frames, e.g., +13.7 R@1 on QuerYD and +6.5 R@1 on Condensed Movie.
</details>
<details>
<summary>摘要</summary>
大规模视频语言预训练已经取得了关键视频语言理解任务的显著进步。然而，视频编码的计算沉重仍然是效率瓶颈，特别是长形视频。这些视频具有自然的3D特性和空间时间重复，使得捕捉复杂的时间和空间关系变得困难。为解决这个问题，我们提出了高效的方法called TESTA。TESTA通过适应地聚合相似帧和帧内相似的小块来缩短视 semantics。TESTA可以减少视觉token数量，从而加速视频编码。基于TESTA，我们介绍了一个带有分开的空间时间token聚合模块的预训练视频语言模型。我们在五个数据集上进行了对 paragraph-to-video retrieval和长形 VideoQA任务的测试。实验结果显示，TESTA可以提高计算效率，并且在处理 longer input frames 时 achieved significant performance gains，比如QuerYD上的+13.7 R@1和Condensed Movie上的+6.5 R@1。
</details></li>
</ul>
<hr>
<h2 id="A-Unique-Training-Strategy-to-Enhance-Language-Models-Capabilities-for-Health-Mention-Detection-from-Social-Media-Content"><a href="#A-Unique-Training-Strategy-to-Enhance-Language-Models-Capabilities-for-Health-Mention-Detection-from-Social-Media-Content" class="headerlink" title="A Unique Training Strategy to Enhance Language Models Capabilities for Health Mention Detection from Social Media Content"></a>A Unique Training Strategy to Enhance Language Models Capabilities for Health Mention Detection from Social Media Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19057">http://arxiv.org/abs/2310.19057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pervaiz Iqbal Khan, Muhammad Nabeel Asim, Andreas Dengel, Sheraz Ahmed</li>
<li>for: 提取社交媒体上的健康相关内容，用于疾病传播、评估药物对疾病的影响等应用。</li>
<li>methods: 采用随机权重扰动和对比学习策略来训练语言模型，以便从社交媒体文本中提取通用模式。</li>
<li>results: 提出一种基于多种语言模型的元预测器，可以将社交媒体文本分类为非健康和健康相关两类，并在三个公共评测数据集上实现了3.87%的F1分数提升和超过现有健康提及分类预测器的性能。<details>
<summary>Abstract</summary>
An ever-increasing amount of social media content requires advanced AI-based computer programs capable of extracting useful information. Specifically, the extraction of health-related content from social media is useful for the development of diverse types of applications including disease spread, mortality rate prediction, and finding the impact of diverse types of drugs on diverse types of diseases. Language models are competent in extracting the syntactic and semantics of text. However, they face a hard time extracting similar patterns from social media texts. The primary reason for this shortfall lies in the non-standardized writing style commonly employed by social media users. Following the need for an optimal language model competent in extracting useful patterns from social media text, the key goal of this paper is to train language models in such a way that they learn to derive generalized patterns. The key goal is achieved through the incorporation of random weighted perturbation and contrastive learning strategies. On top of a unique training strategy, a meta predictor is proposed that reaps the benefits of 5 different language models for discriminating posts of social media text into non-health and health-related classes. Comprehensive experimentation across 3 public benchmark datasets reveals that the proposed training strategy improves the performance of the language models up to 3.87%, in terms of F1-score, as compared to their performance with traditional training. Furthermore, the proposed meta predictor outperforms existing health mention classification predictors across all 3 benchmark datasets.
</details>
<details>
<summary>摘要</summary>
随着社交媒体内容的不断增加，需要更高级的人工智能计算机程序来提取有用信息。具体来说，从社交媒体中提取健康相关内容非常有用，可以用于生病传播、死亡率预测和不同类型的药物对不同类型疾病的影响等多种应用。语言模型可以提取文本的语法和 semantics，但它们在社交媒体文本上遇到困难。主要的原因在于社交媒体用户通常采用不标准的写作风格。为了解决这个问题，本文的关键目标是使语言模型学习泛化模式。这个目标通过随机权重扰动和对比学习策略来实现。此外，我们还提出了一种基于5种语言模型的元预测器，可以对社交媒体文本分类为非健康和健康相关类别。通过对3个公共 benchmark 数据集进行广泛的实验，我们发现，我们的训练策略可以提高语言模型的性能，相比传统训练策略，提高F1-score的表现达3.87%。此外，我们的元预测器还可以在所有3个 benchmark 数据集上超过现有的健康提及分类预测器。
</details></li>
</ul>
<hr>
<h2 id="MILL-Mutual-Verification-with-Large-Language-Models-for-Zero-Shot-Query-Expansion"><a href="#MILL-Mutual-Verification-with-Large-Language-Models-for-Zero-Shot-Query-Expansion" class="headerlink" title="MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion"></a>MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19056">http://arxiv.org/abs/2310.19056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengyue Jia, Yiding Liu, Xiangyu Zhao, Xiaopeng Li, Changying Hao, Shuaiqiang Wang, Dawei Yin</li>
<li>for: 提高搜寻系统的查询扩展功能，以更好地反映用户的资讯需求。</li>
<li>methods: 使用大语言模型（LLM）基础的共识验证框架，从多个角度生成来扩展查询。</li>
<li>results: 在三个资讯搜寻 dataset 上进行了广泛的实验，与其他基eline相比，提高了查询扩展的性能。<details>
<summary>Abstract</summary>
Query expansion is a commonly-used technique in many search systems to better represent users' information needs with additional query terms. Existing studies for this task usually propose to expand a query with retrieved or generated contextual documents. However, both types of methods have clear limitations. For retrieval-based methods, the documents retrieved with the original query might not be accurate enough to reveal the search intent, especially when the query is brief or ambiguous. For generation-based methods, existing models can hardly be trained or aligned on a particular corpus, due to the lack of corpus-specific labeled data. In this paper, we propose a novel Large Language Model (LLM) based mutual verification framework for query expansion, which alleviates the aforementioned limitations. Specifically, we first design a query-query-document generation pipeline, which can effectively leverage the contextual knowledge encoded in LLMs to generate sub-queries and corresponding documents from multiple perspectives. Next, we employ a mutual verification method for both generated and retrieved contextual documents, where 1) retrieved documents are filtered with the external contextual knowledge in generated documents, and 2) generated documents are filtered with the corpus-specific knowledge in retrieved documents. Overall, the proposed method allows retrieved and generated documents to complement each other to finalize a better query expansion. We conduct extensive experiments on three information retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO. The results demonstrate that our method outperforms other baselines significantly.
</details>
<details>
<summary>摘要</summary>
很多搜索系统中使用的查询扩展技术可以更好地表达用户的信息需求。现有的研究通常是使用已经retsieved或生成的文档来扩展查询。然而，这两种方法都有明显的局限性。对于retsieval-based方法来说，用于扩展查询的文档可能并不准确地反映搜索意图，特别是当查询语句简短或 ambiguous 时。对于生成-based方法来说，现有的模型很难在特定的文献上进行训练或对Alignment，因为缺乏带有标注数据的文献特有的训练数据。在本文中，我们提出了一种基于大型自然语言模型（LLM）的 queries 的共同验证框架，以解决以上所述的局限性。具体来说，我们首先设计了一个查询-查询-文档生成管道，可以借助LLM中的上下文知识来生成多个角度的子查询和相应的文档。接着，我们使用一种互verify方法，其中1）retsieved的文档被 Filter 出外部上下文知识生成的文档中，2）生成的文档被 Filter 出特定文献中的训练数据。总的来说，我们的方法可以让retsieved和生成的文档互相补做，以实现更好的查询扩展。我们在TREC-DL-2020、TREC-COVID和MSMARCO三个信息检索数据集上进行了广泛的实验，结果表明我们的方法与其他基准方法相比显著有优势。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Emotional-Landscape-of-Music-An-Analysis-of-Valence-Trends-and-Genre-Variations-in-Spotify-Music-Data"><a href="#Exploring-the-Emotional-Landscape-of-Music-An-Analysis-of-Valence-Trends-and-Genre-Variations-in-Spotify-Music-Data" class="headerlink" title="Exploring the Emotional Landscape of Music: An Analysis of Valence Trends and Genre Variations in Spotify Music Data"></a>Exploring the Emotional Landscape of Music: An Analysis of Valence Trends and Genre Variations in Spotify Music Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19052">http://arxiv.org/abs/2310.19052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shruti Dutta, Shashwat Mookherjee</li>
<li>for: 这个论文通过分析Spotify音乐数据，探讨音乐情感和趋势，包括音频特征和投票得分，以揭示音乐情感关系的Patterns。</li>
<li>methods: 该研究使用回归模型、时间分析、情绪过渡和分类调查等方法，以预测投票得分。</li>
<li>results: 研究发现了音乐情感关系的模式，包括时间的变化和情绪的过渡。这些发现有助于深入理解音乐和情感之间的关系，并提供了长期的音乐情感探索。<details>
<summary>Abstract</summary>
This paper conducts an intricate analysis of musical emotions and trends using Spotify music data, encompassing audio features and valence scores extracted through the Spotipi API. Employing regression modeling, temporal analysis, mood transitions, and genre investigation, the study uncovers patterns within music-emotion relationships. Regression models linear, support vector, random forest, and ridge, are employed to predict valence scores. Temporal analysis reveals shifts in valence distribution over time, while mood transition exploration illuminates emotional dynamics within playlists. The research contributes to nuanced insights into music's emotional fabric, enhancing comprehension of the interplay between music and emotions through years.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "Mandarin" Chinese.Translation Notes:* "valence scores" is translated as "积分" (jīpǐn), which is a term commonly used in Chinese to refer to the emotional content of music.* "regression modeling" is translated as "回归分析" (huíjì fāngxì), which is a term commonly used in Chinese to refer to statistical modeling techniques used to predict continuous outcomes.* "temporal analysis" is translated as "时间分析" (shíjiàn fāngxì), which is a term commonly used in Chinese to refer to the analysis of data over time.* "mood transitions" is translated as "情绪转移" (qíngxù zhōngmǐ), which is a term commonly used in Chinese to refer to the changes in emotional states within a piece of music.* "genre investigation" is translated as "类型调查" (lèitype jiàozhè), which is a term commonly used in Chinese to refer to the examination of different styles or genres of music.
</details></li>
</ul>
<hr>
<h2 id="TeacherLM-Teaching-to-Fish-Rather-Than-Giving-the-Fish-Language-Modeling-Likewise"><a href="#TeacherLM-Teaching-to-Fish-Rather-Than-Giving-the-Fish-Language-Modeling-Likewise" class="headerlink" title="TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise"></a>TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19019">http://arxiv.org/abs/2310.19019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan He, Hanyu Lai, Chenyang Zhao, Zirui Cheng, Junting Pan, Ruoyu Qin, Ruofan Lu, Rui Lu, Yunchen Zhang, Gangming Zhao, Zhaohui Hou, Zhiyuan Huang, Shaoqing Lu, Ding Liang, Mingjie Zhan</li>
<li>for: 这项研究旨在提出一种能够对多种自然语言处理任务进行准确的推理和数据增强的小型语言模型（TeacherLM-7.1B）。</li>
<li>methods: 该模型使用了对重要基础知识、链式思维和常见错误的注释，以便其他模型可以学习“为什么”而不仅仅是“什么”。</li>
<li>results: 根据实验结果，TeacherLM-7.1B模型在MMLU测试中获得了零shot得分52.3，超过了大多数超过100亿参数的模型。此外，基于TeacherLM-7.1B模型，我们对58个NLP数据集进行了数据增强，并在多任务 Setting中教育了不同参数的OPT和BLOOM系列学生模型。实验结果表明，TeacherLM的数据增强对学生模型带来了显著的改进。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn "why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）在不同的自然语言处理任务中表现出了吸引人的推理和数据增强能力。然而，小型模型呢？在这项工作中，我们提出了TeacherLM-7.1B模型，可以对大多数NLU样本进行相关基础知识、链条思维和常见错误的标注，使得其他模型可以学习“为什么”而不仅仅是“什么”，从而提高NLU模型的泛化能力。TeacherLM-7.1B模型在MMLU上取得了零基eline得分52.3，超过了大多数超过100亿参数的模型。此外，TeacherLM还具有出色的数据增强能力。基于TeacherLM，我们对58个NLU数据集进行了增强，并使用不同的OPT和BLOOM系列模型在多任务 Setting中进行了启发。实验结果表明，TeacherLM提供的数据增强对NLU模型的表现带来了显著的改善。我们将发布TeacherLM系列模型和增强数据集作为开源。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-Prototypical-Knowledge-for-Weakly-Open-Vocabulary-Semantic-Segmentation"><a href="#Uncovering-Prototypical-Knowledge-for-Weakly-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic Segmentation"></a>Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19001">http://arxiv.org/abs/2310.19001</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ferenas/PGSeg">https://github.com/Ferenas/PGSeg</a></li>
<li>paper_authors: Fei Zhang, Tianfei Zhou, Boyang Li, Hao He, Chaofan Ma, Tianjiao Zhang, Jiangchao Yao, Ya Zhang, Yanfeng Wang</li>
<li>for: 本研究探讨了弱类开放词汇 semantic segmentation（WOVSS）问题，即通过仅使用图像和文本对进行学习， segmenting objects of arbitrary classes。</li>
<li>methods: existings works 增强 vanilla vision transformer 的方法，通过引入显式分组识别，例如使用多个组token&#x2F;中心来分组图像 токен并进行组级文本对齐。</li>
<li>results: 我们的提议方法可以减少对group token的粒度不一致，并且可以在不同的批处理级别进行多模态规范化，从而提高分区能力和精度。实验结果显示，我们的提议方法可以在多个 benchmark 数据集上达到状态 искусственный智能的性能。<details>
<summary>Abstract</summary>
This paper studies the problem of weakly open-vocabulary semantic segmentation (WOVSS), which learns to segment objects of arbitrary classes using mere image-text pairs. Existing works turn to enhance the vanilla vision transformer by introducing explicit grouping recognition, i.e., employing several group tokens/centroids to cluster the image tokens and perform the group-text alignment. Nevertheless, these methods suffer from a granularity inconsistency regarding the usage of group tokens, which are aligned in the all-to-one v.s. one-to-one manners during the training and inference phases, respectively. We argue that this discrepancy arises from the lack of elaborate supervision for each group token. To bridge this granularity gap, this paper explores explicit supervision for the group tokens from the prototypical knowledge. To this end, this paper proposes the non-learnable prototypical regularization (NPR) where non-learnable prototypes are estimated from source features to serve as supervision and enable contrastive matching of the group tokens. This regularization encourages the group tokens to segment objects with less redundancy and capture more comprehensive semantic regions, leading to increased compactness and richness. Based on NPR, we propose the prototypical guidance segmentation network (PGSeg) that incorporates multi-modal regularization by leveraging prototypical sources from both images and texts at different levels, progressively enhancing the segmentation capability with diverse prototypical patterns. Experimental results show that our proposed method achieves state-of-the-art performance on several benchmark datasets. The source code is available at https://github.com/Ferenas/PGSeg.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="AMIR-Automated-MisInformation-Rebuttal-–-A-COVID-19-Vaccination-Datasets-based-Recommendation-System"><a href="#AMIR-Automated-MisInformation-Rebuttal-–-A-COVID-19-Vaccination-Datasets-based-Recommendation-System" class="headerlink" title="AMIR: Automated MisInformation Rebuttal – A COVID-19 Vaccination Datasets based Recommendation System"></a>AMIR: Automated MisInformation Rebuttal – A COVID-19 Vaccination Datasets based Recommendation System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19834">http://arxiv.org/abs/2310.19834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shakshi Sharma, Anwitaman Datta, Rajesh Sharma</li>
<li>for: 本研究旨在开发一种可靠、可扩展的自动回击谣言工具，以抗谣言的扩散和恶性影响。</li>
<li>methods: 本研究使用现有社交媒体上的信息和更加权威的 фа核数据库，自动生成对谣言的回击。</li>
<li>results: 研究表明，通过使用这种方法，可以快速、高效地对谣言进行回击，并且可以扩展到其他社交媒体平台和谣言类型。<details>
<summary>Abstract</summary>
Misinformation has emerged as a major societal threat in recent years in general; specifically in the context of the COVID-19 pandemic, it has wrecked havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable solutions for combating misinformation are the need of the hour. This work explored how existing information obtained from social media and augmented with more curated fact checked data repositories can be harnessed to facilitate automated rebuttal of misinformation at scale. While the ideas herein can be generalized and reapplied in the broader context of misinformation mitigation using a multitude of information sources and catering to the spectrum of social media platforms, this work serves as a proof of concept, and as such, it is confined in its scope to only rebuttal of tweets, and in the specific context of misinformation regarding COVID-19. It leverages two publicly available datasets, viz. FaCov (fact-checked articles) and misleading (social media Twitter) data on COVID-19 Vaccination.
</details>
<details>
<summary>摘要</summary>
“误情传播已经成为现代社会的主要问题，尤其是在 COVID-19 大流行期间。这种误情传播可能会导致疫苗抵触，例如通过传播不实信息。为了解决这个问题，我们需要一些可靠且可扩展的解决方案。这个研究探索了如何使用社交媒体上的现有信息，加上更加精心的实验 checked 数据库，来自动反驳误情传播。这个研究的想法可以应用于更 широ的误情传播问题，使用多种信息来源和覆盖多个社交媒体平台。这个研究作为证明，仅对于 Twitter 上的反驳误情传播进行了评估，并且仅在 COVID-19 疫苗接种方面进行了评估。它使用了两个公共可用的数据集，namely，FaCov（实验 checked 文章）和 misleading（社交媒体 Twitter）数据集。”Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Bipartite-Graph-Pre-training-for-Unsupervised-Extractive-Summarization-with-Graph-Convolutional-Auto-Encoders"><a href="#Bipartite-Graph-Pre-training-for-Unsupervised-Extractive-Summarization-with-Graph-Convolutional-Auto-Encoders" class="headerlink" title="Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders"></a>Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18992">http://arxiv.org/abs/2310.18992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianren Mao, Shaobo Zhao, Jiarui Li, Xiaolei Gu, Shizhu He, Bo Li, Jianxin Li</li>
<li>for: 用于自动生成文摘</li>
<li>methods: 使用特定设计的 sentence embedding 模型，以优化句子特征和文档凝聚特征</li>
<li>results: 提供了高效的自动文摘方法，超越了使用BERT或RoBERTa的句子表示Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文使用自动生成文摘技术，以提高文摘的效率和质量。</li>
<li>methods: 我们提出了一种使用 sentence embedding 模型来优化句子特征和文档凝聚特征，从而提供高效的自动文摘方法。</li>
<li>results: 我们的方法在下游任务中表现出色，超越了使用BERT或RoBERTa的句子表示。<details>
<summary>Abstract</summary>
Pre-trained sentence representations are crucial for identifying significant sentences in unsupervised document extractive summarization. However, the traditional two-step paradigm of pre-training and sentence-ranking, creates a gap due to differing optimization objectives. To address this issue, we argue that utilizing pre-trained embeddings derived from a process specifically designed to optimize cohensive and distinctive sentence representations helps rank significant sentences. To do so, we propose a novel graph pre-training auto-encoder to obtain sentence embeddings by explicitly modelling intra-sentential distinctive features and inter-sentential cohesive features through sentence-word bipartite graphs. These pre-trained sentence representations are then utilized in a graph-based ranking algorithm for unsupervised summarization. Our method produces predominant performance for unsupervised summarization frameworks by providing summary-worthy sentence representations. It surpasses heavy BERT- or RoBERTa-based sentence representations in downstream tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="NP-SBFL-Bridging-the-Gap-Between-Spectrum-Based-Fault-Localization-and-Faulty-Neural-Pathways-Diagnosis"><a href="#NP-SBFL-Bridging-the-Gap-Between-Spectrum-Based-Fault-Localization-and-Faulty-Neural-Pathways-Diagnosis" class="headerlink" title="NP-SBFL: Bridging the Gap Between Spectrum-Based Fault Localization and Faulty Neural Pathways Diagnosis"></a>NP-SBFL: Bridging the Gap Between Spectrum-Based Fault Localization and Faulty Neural Pathways Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18987">http://arxiv.org/abs/2310.18987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soroush Hashemifar, Saeed Parsa, Akram Kalaee</li>
<li>for: 这篇论文旨在提出一种基于NP-SBFL方法的深度学习网络FAULT LOCALIZATION方法，以便更好地找到深度学习网络中的FAULTY PATH。</li>
<li>methods: 该方法使用层间相关传播（LRP）技术确定关键神经元，并使用多个阶段加速（MGA）来有效地激活一个序列中的神经元，以保持之前神经元的激活。</li>
<li>results: 对于MNIST和CIFAR-10两个常用的数据集，以及三种异常神经元度量 Tarantula、Ochiai 和 Barinel，我们的方法比基elines更有效地识别异常路径和生成攻击输入。特别是在 Tarantula 上，NP-SBFL-MGA 的异常检测率达到 96.75%，超过 DeepFault 在 Ochiai 上的 89.90% 和 NP-SBFL-GA 在 Ochiai 上的 60.61%。<details>
<summary>Abstract</summary>
Deep learning has revolutionized various real-world applications, but the quality of Deep Neural Networks (DNNs) remains a concern. DNNs are complex and have millions of parameters, making it difficult to determine their contributions to fulfilling a task. Moreover, the behavior of a DNN is highly influenced by the data used during training, making it challenging to collect enough data to exercise all potential DNN behavior under all possible scenarios. This paper proposes a novel NP-SBFL method that adapts spectrum-based fault localization (SBFL) to locate faulty neural pathways. Our method identifies critical neurons using the layer-wise relevance propagation (LRP) technique and determines which critical neurons are faulty. We propose a multi-stage gradient ascent (MGA), an extension of gradient ascent, to effectively activate a sequence of neurons one at a time while maintaining the activation of previous neurons. We evaluated the effectiveness of our method on two commonly used datasets, MNIST and CIFAR-10, two baselines DeepFault and NP-SBFL-GA, and three suspicious neuron measures, Tarantula, Ochiai, and Barinel. The empirical results showed that NP-SBFL-MGA is statistically more effective than the baselines at identifying suspicious paths and synthesizing adversarial inputs. Particularly, Tarantula on NP-SBFL-MGA had the highest fault detection rate at 96.75%, surpassing DeepFault on Ochiai (89.90%) and NP-SBFL-GA on Ochiai (60.61%). Our approach also yielded comparable results to the baselines in synthesizing naturalness inputs, and we found a positive correlation between the coverage of critical paths and the number of failed tests in DNN fault localization.
</details>
<details>
<summary>摘要</summary>
深度学习已经革命化了各种实际应用，但是深度神经网络（DNN）的质量仍然是一大问题。DNN具有很多参数，因此很难确定它们在完成任务时的贡献。此外，DNN的行为受到训练数据的影响，因此收集足够的数据来覆盖所有可能的scenario是很困难的。这篇论文提出了一种基于spectrum-based fault localization（SBFL）的新方法，用于 locate faulty neural pathways。我们的方法使用层 wise relevance propagation（LRP）技术来确定关键神经元，并使用多Stage gradient ascent（MGA）来有效地激活一系列神经元，而不会产生前一个神经元的激活失效。我们对MNIST和CIFAR-10两个常用的数据集进行了评估，与DeepFault和NP-SBFL-GA两个基eline进行了比较，以及三种异常神经元度量 Tarantula、Ochiai和Barinel。实际结果表明，NP-SBFL-MGA在 Identifying suspicious paths和生成攻击输入方面具有 statistically higher effectiveness than baselines。特别是在 Tarantula上，NP-SBFL-MGA的异常检测率为 96.75%，超过 DeepFault on Ochiai（89.90%）和 NP-SBFL-GA on Ochiai（60.61%）。我们的方法还在生成自然输入方面得到了相似的结果，并发现了关键路径覆盖率和失败测试数量之间的正相关关系。
</details></li>
</ul>
<hr>
<h2 id="DCQA-Document-Level-Chart-Question-Answering-towards-Complex-Reasoning-and-Common-Sense-Understanding"><a href="#DCQA-Document-Level-Chart-Question-Answering-towards-Complex-Reasoning-and-Common-Sense-Understanding" class="headerlink" title="DCQA: Document-Level Chart Question Answering towards Complex Reasoning and Common-Sense Understanding"></a>DCQA: Document-Level Chart Question Answering towards Complex Reasoning and Common-Sense Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18983">http://arxiv.org/abs/2310.18983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AnranWu-RichPo/DCQA">https://github.com/AnranWu-RichPo/DCQA</a></li>
<li>paper_authors: Anran Wu, Luwei Xiao, Xingjiao Wu, Shuwen Yang, Junjie Xu, Zisong Zhuang, Nian Xie, Cheng Jin, Liang He</li>
<li>for: 这篇论文是为了解决文档级图表问答问题（DCQA）而写的。</li>
<li>methods: 这篇论文使用了文档格式分析（DLA）和图表问答（CQA）技术来解决DCQA问题。</li>
<li>results: 论文提出了一个新的DCQA数据集，包含6种不同的图表样式和699,051个需要高度理解和常识能力的问题。此外，论文还提出了一种使用表格数据、颜色集和基本问题模板生成大量理智问答对的问题生成引擎。<details>
<summary>Abstract</summary>
Visually-situated languages such as charts and plots are omnipresent in real-world documents. These graphical depictions are human-readable and are often analyzed in visually-rich documents to address a variety of questions that necessitate complex reasoning and common-sense responses. Despite the growing number of datasets that aim to answer questions over charts, most only address this task in isolation, without considering the broader context of document-level question answering. Moreover, such datasets lack adequate common-sense reasoning information in their questions. In this work, we introduce a novel task named document-level chart question answering (DCQA). The goal of this task is to conduct document-level question answering, extracting charts or plots in the document via document layout analysis (DLA) first and subsequently performing chart question answering (CQA). The newly developed benchmark dataset comprises 50,010 synthetic documents integrating charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and ChartQA) and includes 699,051 questions that demand a high degree of reasoning ability and common-sense understanding. Besides, we present the development of a potent question-answer generation engine that employs table data, a rich color set, and basic question templates to produce a vast array of reasoning question-answer pairs automatically. Based on DCQA, we devise an OCR-free transformer for document-level chart-oriented understanding, capable of DLA and answering complex reasoning and common-sense questions over charts in an OCR-free manner. Our DCQA dataset is expected to foster research on understanding visualizations in documents, especially for scenarios that require complex reasoning for charts in the visually-rich document. We implement and evaluate a set of baselines, and our proposed method achieves comparable results.
</details>
<details>
<summary>摘要</summary>
文本中的可见语言如图表和折衣图是现实生活中文档中 ubique 存在的。这些图形展示是人类可读的，并在文档中常常用于回答复杂的问题，需要复杂的理解和常识。Despite the growing number of datasets that aim to answer questions over charts, most only address this task in isolation, without considering the broader context of document-level question answering. Moreover, such datasets lack adequate common-sense reasoning information in their questions. In this work, we introduce a novel task named 文档级图表问题回答 (DCQA). The goal of this task is to conduct document-level question answering, extracting charts or plots in the document via 文档布局分析 (DLA) first and subsequently performing 图表问题回答 (CQA). The newly developed benchmark dataset comprises 50,010 synthetic documents integrating charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and ChartQA) and includes 699,051 questions that demand a high degree of reasoning ability and common-sense understanding. Besides, we present the development of a potent question-answer generation engine that employs table data, a rich color set, and basic question templates to produce a vast array of reasoning question-answer pairs automatically. Based on DCQA, we devise an OCR-free transformer for document-level chart-oriented understanding, capable of DLA and answering complex reasoning and common-sense questions over charts in an OCR-free manner. Our DCQA dataset is expected to foster research on understanding visualizations in documents, especially for scenarios that require complex reasoning for charts in the visually-rich document. We implement and evaluate a set of baselines, and our proposed method achieves comparable results.
</details></li>
</ul>
<hr>
<h2 id="EtiCor-Corpus-for-Analyzing-LLMs-for-Etiquettes"><a href="#EtiCor-Corpus-for-Analyzing-LLMs-for-Etiquettes" class="headerlink" title="EtiCor: Corpus for Analyzing LLMs for Etiquettes"></a>EtiCor: Corpus for Analyzing LLMs for Etiquettes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18974">http://arxiv.org/abs/2310.18974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashutosh Dwivedi, Pradhyumna Lavania, Ashutosh Modi</li>
<li>for: 本文提出了一个etiquette corpus，用于评估LLMs在不同地区社会规范方面的知识和理解能力。</li>
<li>methods: 本文使用了现有的LLMs（Delphi、Falcon40B、GPT-3.5）进行实验，以评估它们在不同地区社会规范方面的性能。</li>
<li>results: 初步结果表明，LLMs在非西方世界的社会规范方面往往无法理解和遵循当地的习俗。<details>
<summary>Abstract</summary>
Etiquettes are an essential ingredient of day-to-day interactions among people. Moreover, etiquettes are region-specific, and etiquettes in one region might contradict those in other regions. In this paper, we propose EtiCor, an Etiquettes Corpus, having texts about social norms from five different regions across the globe. The corpus provides a test bed for evaluating LLMs for knowledge and understanding of region-specific etiquettes. Additionally, we propose the task of Etiquette Sensitivity. We experiment with state-of-the-art LLMs (Delphi, Falcon40B, and GPT-3.5). Initial results indicate that LLMs, mostly fail to understand etiquettes from regions from non-Western world.
</details>
<details>
<summary>摘要</summary>
礼仪是日常人际交流中的重要组成部分。同时，礼仪在不同地区之间可能存在差异，一地的礼仪可能与另一地的礼仪矛盾。在这篇论文中，我们提出了“礼仪库”（EtiCor），包含来自五个不同地区的社会规范文本。该库提供了对LRMs（语言模型）的评估和理解地区特有的礼仪知识的测试平台。此外，我们还提出了“礼仪敏感度”的任务。我们对当今顶尖LRMs（Delphi、Falcon40B和GPT-3.5）进行了实验，初步结果显示，LRMs在非西方世界地区的礼仪知识方面表现不佳。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Vision-Transformers-for-Image-Classification-in-Class-Embedding-Space"><a href="#Analyzing-Vision-Transformers-for-Image-Classification-in-Class-Embedding-Space" class="headerlink" title="Analyzing Vision Transformers for Image Classification in Class Embedding Space"></a>Analyzing Vision Transformers for Image Classification in Class Embedding Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18969">http://arxiv.org/abs/2310.18969</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/martinagvilas/vit-cls_emb">https://github.com/martinagvilas/vit-cls_emb</a></li>
<li>paper_authors: Martina G. Vilas, Timothy Schaumlöffel, Gemma Roig</li>
<li>for: 本研究用于解释视觉转换器模型如何用于图像分类任务。</li>
<li>methods: 该研究使用了一种基于前期研究的方法，通过将内部表示映射到学习的类嵌入空间，以解释这些网络如何建立图像分类预测的 categorical 表示。</li>
<li>results: 研究发现，图像块在层次结构中发展出了类型特定的表示，这与注意力机制和上下文信息有关。此外，该方法还可以用来确定图像中关键的部分，并且与传统的直接探测方法相比，具有显著的优势。<details>
<summary>Abstract</summary>
Despite the growing use of transformer models in computer vision, a mechanistic understanding of these networks is still needed. This work introduces a method to reverse-engineer Vision Transformers trained to solve image classification tasks. Inspired by previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research.
</details>
<details>
<summary>摘要</summary>
尽管变换器模型在计算机视觉领域的使用正在增长，但是我们仍需要更好地理解这些网络。这项工作提出了一种方法，用于反向工程视Transformers在图像分类任务上训练过后的内部表示。 Drawing inspiration from previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research.
</details></li>
</ul>
<hr>
<h2 id="Spacecraft-Autonomous-Decision-Planning-for-Collision-Avoidance-a-Reinforcement-Learning-Approach"><a href="#Spacecraft-Autonomous-Decision-Planning-for-Collision-Avoidance-a-Reinforcement-Learning-Approach" class="headerlink" title="Spacecraft Autonomous Decision-Planning for Collision Avoidance: a Reinforcement Learning Approach"></a>Spacecraft Autonomous Decision-Planning for Collision Avoidance: a Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18966">http://arxiv.org/abs/2310.18966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Bourriez, Adrien Loizeau, Adam F. Abdin<br>for:The paper is written for the purpose of proposing an implementation of autonomous collision avoidance decision-making capabilities on spacecraft using reinforcement learning techniques.methods:The proposed methodology is based on a partially observable Markov decision process (POMDP) framework, which considers epistemic and aleatory uncertainties and allows the AI system on board the spacecraft to learn stochastic policies for accurate collision avoidance maneuvers.results:The objective of the paper is to successfully delegate the decision-making process for autonomously implementing a collision avoidance maneuver to the spacecraft without human intervention, allowing for a faster response in the decision-making process and highly decentralized operations.<details>
<summary>Abstract</summary>
The space environment around the Earth is becoming increasingly populated by both active spacecraft and space debris. To avoid potential collision events, significant improvements in Space Situational Awareness (SSA) activities and Collision Avoidance (CA) technologies are allowing the tracking and maneuvering of spacecraft with increasing accuracy and reliability. However, these procedures still largely involve a high level of human intervention to make the necessary decisions. For an increasingly complex space environment, this decision-making strategy is not likely to be sustainable. Therefore, it is important to successfully introduce higher levels of automation for key Space Traffic Management (STM) processes to ensure the level of reliability needed for navigating a large number of spacecraft. These processes range from collision risk detection to the identification of the appropriate action to take and the execution of avoidance maneuvers. This work proposes an implementation of autonomous CA decision-making capabilities on spacecraft based on Reinforcement Learning (RL) techniques. A novel methodology based on a Partially Observable Markov Decision Process (POMDP) framework is developed to train the Artificial Intelligence (AI) system on board the spacecraft, considering epistemic and aleatory uncertainties. The proposed framework considers imperfect monitoring information about the status of the debris in orbit and allows the AI system to effectively learn stochastic policies to perform accurate Collision Avoidance Maneuvers (CAMs). The objective is to successfully delegate the decision-making process for autonomously implementing a CAM to the spacecraft without human intervention. This approach would allow for a faster response in the decision-making process and for highly decentralized operations.
</details>
<details>
<summary>摘要</summary>
地球附近的空间环境正在不断增加活跃的空间craft和空间垃圾的数量，为了避免 potential collision event，空间 situational awareness (SSA) 活动和 collision avoidance (CA) 技术得到了重要改进，可以准确地跟踪和 manipulate 空间craft。然而，这些过程仍然需要人类参与，以便做出必要的决策。随着空间环境的增加复杂度，这种决策策略可能不可持续。因此，需要成功地把Space Traffic Management (STM) 过程中的关键步骤自动化，以确保在大量空间craft navigating 时的可靠性。这些过程包括风险检测和避免措施的识别以及执行避免措施。本工作提出了基于 reinforcement learning (RL) 技术的自动化 CA 决策能力的实现。一种基于 partially observable Markov decision process (POMDP) 框架的新方法被开发，用于在空间craft上训练人工智能 (AI) 系统，考虑到了 epistemic 和 aleatory 不确定性。该方法考虑了在轨道上监测空间垃圾的状况不准确的情况，并允许 AI 系统学习 Stochastic policies 以实现准确的避免措施。目标是成功地委托空间craft上的 AI 系统自动实施避免措施，无需人类参与。这种方法可以提供更快的决策过程，并允许高度分布式的操作。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Autoregressive-Retrieval-via-Bootstrapping-for-Smart-Reply-Systems"><a href="#End-to-End-Autoregressive-Retrieval-via-Bootstrapping-for-Smart-Reply-Systems" class="headerlink" title="End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems"></a>End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18956">http://arxiv.org/abs/2310.18956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Towle, Ke Zhou</li>
<li>for: 这篇论文是为了提出一种新的自动回复方法，以解决现有的自动回复系统的缺陷。</li>
<li>methods: 这篇论文使用了一种束合文本对话模型，通过对一组（消息，回复集）的数据进行Bootstrap来学习自动回复任务的终端到终端的模型。</li>
<li>results: 实验结果表明，这种方法可以与一些现有的基eline方法相比，在三个数据集上表现出5.1%-17.9%的改善空间，并且在0.5%-63.1%的多样性上具有显著的改善。<details>
<summary>Abstract</summary>
Reply suggestion systems represent a staple component of many instant messaging and email systems. However, the requirement to produce sets of replies, rather than individual replies, makes the task poorly suited for out-of-the-box retrieval architectures, which only consider individual message-reply similarity. As a result, these system often rely on additional post-processing modules to diversify the outputs. However, these approaches are ultimately bottlenecked by the performance of the initial retriever, which in practice struggles to present a sufficiently diverse range of options to the downstream diversification module, leading to the suggestions being less relevant to the user. In this paper, we consider a novel approach that radically simplifies this pipeline through an autoregressive text-to-text retrieval model, that learns the smart reply task end-to-end from a dataset of (message, reply set) pairs obtained via bootstrapping. Empirical results show this method consistently outperforms a range of state-of-the-art baselines across three datasets, corresponding to a 5.1%-17.9% improvement in relevance, and a 0.5%-63.1% improvement in diversity compared to the best baseline approach. We make our code publicly available.
</details>
<details>
<summary>摘要</summary>
快件和电子邮件系统中的回复建议系统是一个基本组件。然而，需要生成多个回复而不是单个回复，使得这种任务与传统的检索架构不Compatible，后者只考虑单个消息和回复之间的相似性。因此，这些系统通常需要额外的后处理模块来增加多样性。然而，这些方法受到最初的检索器的性能的限制，导致下游多样化模块无法提供充分多样化的选项，从而导致建议相对较少 relevance。在这篇论文中，我们考虑了一种新的方法，通过自然语言模型来实现简化这个管道。我们通过对 (消息、回复集) 对的数据集进行 bootstrap 来学习端到端的文本到文本检索模型。实验结果表明，这种方法可以一直 exceed  state-of-the-art 基准方法，在三个数据集上取得了5.1%-17.9%的改善，并在多样性方面取得了0.5%-63.1%的改善。我们将代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="Mask-Propagation-for-Efficient-Video-Semantic-Segmentation"><a href="#Mask-Propagation-for-Efficient-Video-Semantic-Segmentation" class="headerlink" title="Mask Propagation for Efficient Video Semantic Segmentation"></a>Mask Propagation for Efficient Video Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18954">http://arxiv.org/abs/2310.18954</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziplab/mpvss">https://github.com/ziplab/mpvss</a></li>
<li>paper_authors: Yuetian Weng, Mingfei Han, Haoyu He, Mingjie Li, Lina Yao, Xiaojun Chang, Bohan Zhuang</li>
<li>for: 本研究目标是提出一种高效的视频 semantic segmentation（VSS）方法，以优化现有的图像semantic segmentation模型，使其能够利用视频帧之间的时间关系，并降低计算成本。</li>
<li>methods: 本方法首先使用强Query-based图像分割器在稀疏的关键帧上生成准确的二值掩码和类别预测。然后，我们设计了一个流量估计模块，使用学习的Query来生成一组相关的掩码流图，每个流图与关键帧的掩码预测相关。最后，掩码-流对被折射以生成非关键帧的掩码预测。通过重用关键帧的预测，我们缓解了对每帧的图像分割器进行资源占用的需求，从而降低计算成本。</li>
<li>results: 我们的mask propagation方法在VSPW和Cityscapes dataset上实现了SOTA的准确率和效率负担的 equilibrio。例如，我们的最佳模型（Swin-L backbone）在VSPW dataset上比SOTA的MRCFA（使用MiT-B5）高4.0%的mIoU，仅需26%的FLOPs。此外，我们的框架可以在Cityscapes验证集上减少到4倍的FLOPs，同时保持只有2%的mIoU下降。代码可以在<a target="_blank" rel="noopener" href="https://github.com/ziplab/MPVSS%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ziplab/MPVSS中获取。</a><details>
<summary>Abstract</summary>
Video Semantic Segmentation (VSS) involves assigning a semantic label to each pixel in a video sequence. Prior work in this field has demonstrated promising results by extending image semantic segmentation models to exploit temporal relationships across video frames; however, these approaches often incur significant computational costs. In this paper, we propose an efficient mask propagation framework for VSS, called MPVSS. Our approach first employs a strong query-based image segmentor on sparse key frames to generate accurate binary masks and class predictions. We then design a flow estimation module utilizing the learned queries to generate a set of segment-aware flow maps, each associated with a mask prediction from the key frame. Finally, the mask-flow pairs are warped to serve as the mask predictions for the non-key frames. By reusing predictions from key frames, we circumvent the need to process a large volume of video frames individually with resource-intensive segmentors, alleviating temporal redundancy and significantly reducing computational costs. Extensive experiments on VSPW and Cityscapes demonstrate that our mask propagation framework achieves SOTA accuracy and efficiency trade-offs. For instance, our best model with Swin-L backbone outperforms the SOTA MRCFA using MiT-B5 by 4.0% mIoU, requiring only 26% FLOPs on the VSPW dataset. Moreover, our framework reduces up to 4x FLOPs compared to the per-frame Mask2Former baseline with only up to 2% mIoU degradation on the Cityscapes validation set. Code is available at https://github.com/ziplab/MPVSS.
</details>
<details>
<summary>摘要</summary>
视频 semantic segmentation (VSS) 是将每帧视频序列中的每个像素分配 semantic 标签。先前的工作在这个领域已经实现了可观的结果，通过将图像 semantic segmentation 模型扩展到利用视频帧之间的时间关系，但这些方法经常产生巨大的计算成本。在这篇论文中，我们提出了一种高效的面 mask 传播框架，称为 MPVSS。我们的方法首先使用强大的查询基于图像 segmentor 在稀疏的关键帧上生成准确的二进制面和分类预测。然后，我们设计了学习查询的流量估计模块，使用学习的查询来生成每帧视频的流量映射，每个映射都与关键帧中的面预测相关。最后，面-流量对被折叠，以用于非关键帧的面预测。通过重用关键帧的预测，我们绕过处理大量视频帧的资源占用INTENSIVE segmentor，解决时间重复和大幅减少计算成本。广泛的实验表明，我们的面传播框架在 VSPW 和 Cityscapes 上实现了最佳的质量和效率的交换。例如，我们的最佳模型（Swin-L 背景）在 VSPW 数据集上的 mIoU 比 SOTA MRCFA（使用 MiT-B5 背景）高4.0%，而计算成本只占26% FLOPs。此外，我们的框架可以在 Cityscapes 验证集上减少至4倍的计算成本，而且只增加了2% mIoU 下降。代码可以在 GitHub 上找到：https://github.com/ziplab/MPVSS。
</details></li>
</ul>
<hr>
<h2 id="Building-a-Safer-Maritime-Environment-Through-Multi-Path-Long-Term-Vessel-Trajectory-Forecasting"><a href="#Building-a-Safer-Maritime-Environment-Through-Multi-Path-Long-Term-Vessel-Trajectory-Forecasting" class="headerlink" title="Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting"></a>Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18948">http://arxiv.org/abs/2310.18948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Spadon, Jay Kumar, Matthew Smith, Sarah Vela, Romina Gehrmann, Derek Eden, Joshua van Berkel, Amilcar Soares, Ronan Fablet, Ronald Pelot, Stan Matwin</li>
<li>for: 这篇论文旨在提高船舶途径预测精度，以便提高水上交通安全性和环境可持续性。</li>
<li>methods: 该论文使用encoder-decoder模型，并采用了bidirectional长短时间记忆网络（Bi-LSTM）进行预测。其中，利用概率特征引入了AIS数据中的潜在路径和目的地信息，以便模型可以基于这些特征进行预测。</li>
<li>results: 该模型在加拿大圣劳伦斯湾（Gulf of St. Lawrence）实现了R2分数超过98%，并且在不同的技术和特征下实现了高精度预测。此外，模型还表现出了更好的复杂决策能力和更高的准确率， average和 median预测错误分别为11km和6km。<details>
<summary>Abstract</summary>
Maritime transport is paramount to global economic growth and environmental sustainability. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, which allows for enhanced traffic surveillance, assisting in vessel safety by avoiding vessel-to-vessel collisions and proactively preventing vessel-to-whale ones. This paper tackles an intrinsic problem to trajectory forecasting: the effective multi-path long-term vessel trajectory forecasting on engineered sequences of AIS data. We utilize an encoder-decoder model with Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data. We feed the model with probabilistic features engineered from the AIS data that refer to the potential route and destination of each trajectory so that the model, leveraging convolutional layers for spatial feature learning and a position-aware attention mechanism that increases the importance of recent timesteps of a sequence during temporal feature learning, forecasts the vessel trajectory taking the potential route and destination into account. The F1 Score of these features is approximately 85% and 75%, indicating their efficiency in supplementing the neural network. We trialed our model in the Gulf of St. Lawrence, one of the North Atlantic Right Whales (NARW) habitats, achieving an R2 score exceeding 98% with varying techniques and features. Despite the high R2 score being attributed to well-defined shipping lanes, our model demonstrates superior complex decision-making during path selection. In addition, our model shows enhanced accuracy, with average and median forecasting errors of 11km and 6km, respectively. Our study confirms the potential of geographical data engineering and trajectory forecasting models for preserving marine life species.
</details>
<details>
<summary>摘要</summary>
海运是全球经济增长和环境可持续性的重要因素。在这个意义上，自动识别系统（AIS）数据在实时流处理中提供了船舶运动的实时流处理数据，从而帮助提高船舶管理和避免船舶相撞和避免船舶和鲸鱼相撞。这篇论文面临了一个核心问题：在Engineered Sequence of AIS Data上进行多path长期船舶轨迹预测。我们使用了编码器-解码器模型，并利用Bi-LSTM网络来预测下一个12小时的船舶轨迹，使用1-3小时的AIS数据。我们对AIS数据进行了Engineering probabilistic特征，这些特征描述了每个轨迹的潜在路径和目的地，以便模型可以通过卷积层学习空间特征和时间特征来预测船舶轨迹。F1 Score的效果为 approximately 85%和75%，表明这些特征的有效性。我们在加拿大圣劳伦斯湾进行了试验， achieve an R2 score exceeding 98% with varying techniques and features。 despite the high R2 score being attributed to well-defined shipping lanes, our model demonstrates superior complex decision-making during path selection。此外，我们的模型还显示了更高的准确率， average和median forecasting errors of 11km和6km， respectively。我们的研究证明了地理数据工程和轨迹预测模型在保护海洋生物种的潜在作用。
</details></li>
</ul>
<hr>
<h2 id="Language-Agents-with-Reinforcement-Learning-for-Strategic-Play-in-the-Werewolf-Game"><a href="#Language-Agents-with-Reinforcement-Learning-for-Strategic-Play-in-the-Werewolf-Game" class="headerlink" title="Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game"></a>Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18940">http://arxiv.org/abs/2310.18940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu</li>
<li>for: 这个研究旨在开发一个基于强大语言模型（LLM）的战略语言代理人，以增强在社交推理游戏“狼人”中的决策能力。</li>
<li>methods: 这个框架使用强大语言模型来推理可能的诈欺，然后使用人口基于训练的循环学习策略来增强代理人的决策能力。</li>
<li>results: 这个研究获得了在对其他LLM-based代理人的胜率最高，并在对反对人类玩家的情况下保持了稳定性的成果。<details>
<summary>Abstract</summary>
Agents built with large language models (LLMs) have recently achieved great advancements. However, most of the efforts focus on single-agent or cooperative settings, leaving more general multi-agent environments underexplored. We propose a new framework powered by reinforcement learning (RL) to develop strategic language agents, i.e., LLM-based agents with strategic thinking ability, for a popular language game, Werewolf. Werewolf is a social deduction game with hidden roles that involves both cooperation and competition and emphasizes deceptive communication and diverse gameplay. Our agent tackles this game by first using LLMs to reason about potential deceptions and generate a set of strategically diverse actions. Then an RL policy, which selects an action from the candidates, is learned by population-based training to enhance the agents' decision-making ability. By combining LLMs with the RL policy, our agent produces a variety of emergent strategies, achieves the highest win rate against other LLM-based agents, and stays robust against adversarial human players in the Werewolf game.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）驱动的代理人最近获得了重要的进步。然而，大多数努力都集中在单个代理人或合作设置上，留下更通用的多代理人环境得以探索。我们提出了一个基于返点学习（RL）的新框架，用于开发语言代理人，即基于LLM的策略思维代理人，用于受欢迎的语言游戏《狼人》。《狼人》是一款社交推理游戏，涉及到协作和竞争，并强调误导性的交流和多样化游戏。我们的代理人首先使用LLM来理解潜在的误导和生成一组策略多样化的动作。然后，通过人口学习来学习RL策略，从候选actions中选择一个动作，以提高代理人的决策能力。通过结合LLM和RL策略，我们的代理人产生了多种emergent策略，在与其他LLM-based代理人的比赛中获得最高胜率，并在对人类玩家的抗击中保持稳定。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Algorithms-to-Predict-Chess960-Result-and-Develop-Opening-Themes"><a href="#Machine-Learning-Algorithms-to-Predict-Chess960-Result-and-Develop-Opening-Themes" class="headerlink" title="Machine Learning Algorithms to Predict Chess960 Result and Develop Opening Themes"></a>Machine Learning Algorithms to Predict Chess960 Result and Develop Opening Themes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18938">http://arxiv.org/abs/2310.18938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyan Deo, Nishchal Dwivedi</li>
<li>for: 这个研究旨在预测棋盘960（也称为菲律敦Random棋盘）游戏结果，以及为每个开局位置发展的开局主题。</li>
<li>methods: 这个研究使用机器学习技术预测游戏结果，并分析开局中每个位置的 Piece 的移动。</li>
<li>results: 研究使用三种机器学习算法（KNN clustering、Random Forest 和 Gradient Boosted Trees）预测游戏结果，并通过分析开局中每个位置的 Piece 的移动，预测游戏的发展方向。<details>
<summary>Abstract</summary>
This work focuses on the analysis of Chess 960, also known as Fischer Random Chess, a variant of traditional chess where the starting positions of the pieces are randomized. The study aims to predict the game outcome using machine learning techniques and develop an opening theme for each starting position. The first part of the analysis utilizes machine learning models to predict the game result based on certain moves in each position. The methodology involves segregating raw data from .pgn files into usable formats and creating datasets comprising approximately 500 games for each starting position. Three machine learning algorithms -- KNN Clustering, Random Forest, and Gradient Boosted Trees -- have been used to predict the game outcome. To establish an opening theme, the board is divided into five regions: center, white kingside, white queenside, black kingside, and black queenside. The data from games played by top engines in all 960 positions is used to track the movement of pieces in the opening. By analysing the change in the number of pieces in each region at specific moves, the report predicts the region towards which the game is developing. These models provide valuable insights into predicting game outcomes and understanding the opening theme in Chess 960.
</details>
<details>
<summary>摘要</summary>
To begin, the study uses machine learning models to predict the game result based on certain moves in each position. The methodology involves separating raw data from .pgn files into usable formats and creating datasets comprising approximately 500 games for each starting position. Three machine learning algorithms - KNN Clustering, Random Forest, and Gradient Boosted Trees - have been used to predict the game outcome.To establish an opening theme, the board is divided into five regions: center, white kingside, white queenside, black kingside, and black queenside. The data from games played by top engines in all 960 positions is used to track the movement of pieces in the opening. By analyzing the change in the number of pieces in each region at specific moves, the report predicts the region towards which the game is developing. These models provide valuable insights into predicting game outcomes and understanding the opening theme in Chess 960.
</details></li>
</ul>
<hr>
<h2 id="The-Utility-of-“Even-if…”-Semifactual-Explanation-to-Optimise-Positive-Outcomes"><a href="#The-Utility-of-“Even-if…”-Semifactual-Explanation-to-Optimise-Positive-Outcomes" class="headerlink" title="The Utility of “Even if…” Semifactual Explanation to Optimise Positive Outcomes"></a>The Utility of “Even if…” Semifactual Explanation to Optimise Positive Outcomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18937">http://arxiv.org/abs/2310.18937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eoinkenny/semifactual_recourse_generation">https://github.com/eoinkenny/semifactual_recourse_generation</a></li>
<li>paper_authors: Eoin M. Kenny, Weipeng Huang</li>
<li>for: 这个论文的目的是优化积极的结果（如贷款批准），而不是改变否定结果（如贷款拒绝）。</li>
<li>methods: 该论文使用了Explainable AI（XAI）技术，并引入了一个新的概念called“Gain”来衡量用户从解释中受益。</li>
<li>results: 对比PRIOR WORK，该论文的算法能够更好地最大化用户的增值（Gain），并且 causality 在这个过程中具有重要性。 最重要的是，用户测试表明，当用户收到贷款批准的积极结果时，semifactual 解释比counterfactuals更有用。<details>
<summary>Abstract</summary>
When users receive either a positive or negative outcome from an automated system, Explainable AI (XAI) has almost exclusively focused on how to mutate negative outcomes into positive ones by crossing a decision boundary using counterfactuals (e.g., \textit{"If you earn 2k more, we will accept your loan application"}). Here, we instead focus on \textit{positive} outcomes, and take the novel step of using XAI to optimise them (e.g., \textit{"Even if you wish to half your down-payment, we will still accept your loan application"}). Explanations such as these that employ "even if..." reasoning, and do not cross a decision boundary, are known as semifactuals. To instantiate semifactuals in this context, we introduce the concept of \textit{Gain} (i.e., how much a user stands to benefit from the explanation), and consider the first causal formalisation of semifactuals. Tests on benchmark datasets show our algorithms are better at maximising gain compared to prior work, and that causality is important in the process. Most importantly however, a user study supports our main hypothesis by showing people find semifactual explanations more useful than counterfactuals when they receive the positive outcome of a loan acceptance.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation is written in the formal tone, which is appropriate for academic writing.)
</details></li>
</ul>
<hr>
<h2 id="Self-Attention-with-Temporal-Prior-Can-We-Learn-More-from-Arrow-of-Time"><a href="#Self-Attention-with-Temporal-Prior-Can-We-Learn-More-from-Arrow-of-Time" class="headerlink" title="Self Attention with Temporal Prior: Can We Learn More from Arrow of Time?"></a>Self Attention with Temporal Prior: Can We Learn More from Arrow of Time?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18932">http://arxiv.org/abs/2310.18932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyung Geun Kim, Byeong Tak Lee</li>
<li>for: 这篇论文是为了解决具有短期和长期时间相关性的自然现象中的问题，特别是时间流向的方向导致的短期相关性。</li>
<li>methods: 该论文使用了注意力层来学习时间序列中的短期相关性，并应用了可学习的、适应性kernel来改进注意力矩阵中的短期时间偏好。</li>
<li>results: 实验结果显示，使用该方法可以在医疗记录（EHR）数据集上实现出色的预测结果，并在大多数任务和数据集上超越最佳模型。<details>
<summary>Abstract</summary>
Many of diverse phenomena in nature often inherently encode both short and long term temporal dependencies, short term dependencies especially resulting from the direction of flow of time. In this respect, we discovered experimental evidences suggesting that {\it interrelations} of these events are higher for closer time stamps. However, to be able for attention based models to learn these regularities in short term dependencies, it requires large amounts of data which are often infeasible. This is due to the reason that, while they are good at learning piece wised temporal dependencies, attention based models lack structures that encode biases in time series. As a resolution, we propose a simple and efficient method that enables attention layers to better encode short term temporal bias of these data sets by applying learnable, adaptive kernels directly to the attention matrices. For the experiments, we chose various prediction tasks using Electronic Health Records (EHR) data sets since they are great examples that have underlying long and short term temporal dependencies. The results of our experiments show exceptional classification results compared to best performing models on most of the task and data sets.
</details>
<details>
<summary>摘要</summary>
很多自然现象具有各种多样化特征，其中许多现象具有短期和长期时间依赖关系。特别是短期时间依赖关系通常由时间流动的方向决定。我们发现了实验证据，表明在更近的时间戳之间的事件关系更高。然而，为了让注意力基本模型学习这些短期时间依赖关系，需要大量数据，而这些数据经常是不可能获得的。这是因为注意力基本模型能够学习独立的时间序列，但缺乏时间序列中的偏好编码结构。为解决这个问题，我们提出了一种简单而高效的方法，即在注意力矩阵上应用学习可变核函数，以更好地编码短期时间依赖关系。我们选择使用医疗记录（EHR）数据集进行实验，因为它们具有下面和长期时间依赖关系的特点。实验结果表明，我们的方法在大多数任务和数据集上达到了最佳性能。
</details></li>
</ul>
<hr>
<h2 id="CHAIN-Exploring-Global-Local-Spatio-Temporal-Information-for-Improved-Self-Supervised-Video-Hashing"><a href="#CHAIN-Exploring-Global-Local-Spatio-Temporal-Information-for-Improved-Self-Supervised-Video-Hashing" class="headerlink" title="CHAIN: Exploring Global-Local Spatio-Temporal Information for Improved Self-Supervised Video Hashing"></a>CHAIN: Exploring Global-Local Spatio-Temporal Information for Improved Self-Supervised Video Hashing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18926">http://arxiv.org/abs/2310.18926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rukai Wei, Yu Liu, Jingkuan Song, Heng Cui, Yanzhao Xie, Ke Zhou</li>
<li>for: The paper is written for improving the efficiency of video retrieval by compressing videos into binary codes and learning accurate hash codes for video retrieval.</li>
<li>methods: The paper uses contrastive learning with augmentation strategies to capture global spatio-temporal information and local spatio-temporal details within video frames, and incorporates two collaborative learning tasks to enhance the perception of temporal structure and the modeling of spatio-temporal relationships.</li>
<li>results: The proposed method outperforms state-of-the-art self-supervised video hashing methods on four video benchmark datasets.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 本文是为了提高视频检索的效率，通过压缩视频为二进制代码，并学习正确的视频哈希代码。</li>
<li>methods: 本文使用对比学习，并采用了一系列的增强策略，以捕捉视频中的全球空间时间信息和本地空间时间细节。</li>
<li>results: 提议的方法在四个视频标准数据集上超越了现有的自动生成视频哈希方法。<details>
<summary>Abstract</summary>
Compressing videos into binary codes can improve retrieval speed and reduce storage overhead. However, learning accurate hash codes for video retrieval can be challenging due to high local redundancy and complex global dependencies between video frames, especially in the absence of labels. Existing self-supervised video hashing methods have been effective in designing expressive temporal encoders, but have not fully utilized the temporal dynamics and spatial appearance of videos due to less challenging and unreliable learning tasks. To address these challenges, we begin by utilizing the contrastive learning task to capture global spatio-temporal information of videos for hashing. With the aid of our designed augmentation strategies, which focus on spatial and temporal variations to create positive pairs, the learning framework can generate hash codes that are invariant to motion, scale, and viewpoint. Furthermore, we incorporate two collaborative learning tasks, i.e., frame order verification and scene change regularization, to capture local spatio-temporal details within video frames, thereby enhancing the perception of temporal structure and the modeling of spatio-temporal relationships. Our proposed Contrastive Hashing with Global-Local Spatio-temporal Information (CHAIN) outperforms state-of-the-art self-supervised video hashing methods on four video benchmark datasets. Our codes will be released.
</details>
<details>
<summary>摘要</summary>
压缩视频到二进制编码可以提高检索速度和减少存储开销。然而，学习准确的Hash代码 для视频检索可以是一个挑战，因为视频帧之间存在高度的本地重复和复杂的全球依赖关系，特别是在标签缺失的情况下。现有的自动编号视频方法有效地设计了表达力强的时间编码器，但是它们没有完全利用视频的时间动态和空间显示特征，尤其是在较难和不可靠的学习任务下。为解决这些挑战，我们开始利用对比学习任务来捕捉视频的全球空间时间信息，并采用我们设计的扩展策略，以创建正确的对应对。这些扩展策略专注于视频帧中的空间和时间变化，以便生成不变于运动、缩放和视点的Hash代码。此外，我们还集成了两个协作学习任务，即帧顺序验证和场景变化规范，以捕捉视频帧中的本地空间时间细节，从而增强视频的时间结构和空间时间关系的模型。我们提出的Contrastive Hashing with Global-Local Spatio-temporal Information（CHAIN）方法在四个视频标准测试集上超越了当前自动编号视频方法的性能。我们的代码将会公开发布。
</details></li>
</ul>
<hr>
<h2 id="QWID-Quantized-Weed-Identification-Deep-neural-network"><a href="#QWID-Quantized-Weed-Identification-Deep-neural-network" class="headerlink" title="QWID: Quantized Weed Identification Deep neural network"></a>QWID: Quantized Weed Identification Deep neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18921">http://arxiv.org/abs/2310.18921</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/parikshit14/qnn-for-weed">https://github.com/parikshit14/qnn-for-weed</a></li>
<li>paper_authors: Parikshit Singh Rathore</li>
<li>for: 本研究旨在提供一种高效的农业用途隐藏植物分类方法。</li>
<li>methods: 该方法使用量化深度神经网络模型，通过8位整数（int8）量化，与标准32位浮点数（fp32）模型不同。它采用了模型大小、执行时间和准确率的平衡，适应农业领域的具体需求。</li>
<li>results: 该方法在ResNet-50和InceptionV3架构上实现了准确率与模型大小、执行时间之间的平衡，并在Desktop、Mobile和Raspberry Pi等实际生产环境中实现了显著的模型大小和执行时间减少，同时保持了准确率的水平。<details>
<summary>Abstract</summary>
In this paper, we present an efficient solution for weed classification in agriculture. We focus on optimizing model performance at inference while respecting the constraints of the agricultural domain. We propose a Quantized Deep Neural Network model that classifies a dataset of 9 weed classes using 8-bit integer (int8) quantization, a departure from standard 32-bit floating point (fp32) models. Recognizing the hardware resource limitations in agriculture, our model balances model size, inference time, and accuracy, aligning with practical requirements. We evaluate the approach on ResNet-50 and InceptionV3 architectures, comparing their performance against their int8 quantized versions. Transfer learning and fine-tuning are applied using the DeepWeeds dataset. The results show staggering model size and inference time reductions while maintaining accuracy in real-world production scenarios like Desktop, Mobile and Raspberry Pi. Our work sheds light on a promising direction for efficient AI in agriculture, holding potential for broader applications.   Code: https://github.com/parikshit14/QNN-for-weed
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种高效的苔藿类分类方案，旨在在农业领域中提高模型性能的推理过程。我们提出了一种量化深度神经网络模型，通过使用8比特整数（int8）量化，与标准32比特浮点数（fp32）模型不同。考虑到农业领域的硬件资源有限，我们的模型坚持平衡模型大小、推理时间和准确率之间的权衡，符合实际需求。我们使用ResNet-50和InceptionV3架构进行评估，与其int8量化版本进行比较。通过使用传输学习和精度调整，我们在桌面、移动设备和Raspberry Pi等实际生产环境中实现了各种模型大小和推理时间的减少，同时保持了准确率。我们的工作探讨了苔藿类分类领域中高效的AI应用的可能性，对更广泛的应用产生了深见。Code: <https://github.com/parikshit14/QNN-for-weed>
</details></li>
</ul>
<hr>
<h2 id="Posterior-Sampling-with-Delayed-Feedback-for-Reinforcement-Learning-with-Linear-Function-Approximation"><a href="#Posterior-Sampling-with-Delayed-Feedback-for-Reinforcement-Learning-with-Linear-Function-Approximation" class="headerlink" title="Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation"></a>Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18919">http://arxiv.org/abs/2310.18919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikki Lijing Kuang, Ming Yin, Mengdi Wang, Yu-Xiang Wang, Yi-An Ma</li>
<li>for: 这个论文主要目标是解决延迟反馈的问题在强化学习中，以提高实际世界系统的性能。</li>
<li>methods: 这个论文使用了线性函数近似来解决延迟反馈问题，并使用 posterior sampling 来实现。</li>
<li>results: 论文提出了一种名为 Delayed-PSVI 的优化策略，并提供了对这种策略的首次分析。这种策略在不知道延迟时间的情况下可以达到 $\widetilde{O}(\sqrt{d^3H^3 T} + d^2H^2 E[\tau])$ 的最差情况 regret。<details>
<summary>Abstract</summary>
Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce Delayed-PSVI, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves $\widetilde{O}(\sqrt{d^3H^3 T} + d^2H^2 E[\tau])$ worst-case regret in the presence of unknown stochastic delays. Here $E[\tau]$ is the expected delay. To further improve its computational efficiency and to expand its applicability in high-dimensional RL problems, we incorporate a gradient-based approximate sampling scheme via Langevin dynamics for Delayed-LPSVI, which maintains the same order-optimal regret guarantee with $\widetilde{O}(dHK)$ computational cost. Empirical evaluations are performed to demonstrate the statistical and computational efficacy of our algorithms.
</details>
<details>
<summary>摘要</summary>
现代再强化学习（RL）研究已经做出了重要进步，通过函数近似来缓解样本复杂性问题，以提高性能。然而，现有的可证fficient算法通常需要快速获得反馈，如果忽略延迟的影响，则可能导致实际系统中的 regret 增长。在这项工作中，我们面临了RL中延迟反馈的挑战，使用线性函数近似，并利用 posterior sampling，这种方法在各种场景中都有良好的实际表现。我们首先介绍了延迟PSVI算法，这是一种使用噪声扰动和 posterior sampling 来精细探索值函数空间的优化算法。我们提供了RL中延迟反馈 posterior sampling 的首次分析，并证明我们的算法在存在未知随机延迟的情况下可以 achiev $\widetilde{O}(\sqrt{d^3H^3T} + d^2H^2 E[\tau])$ 最坏情况的 regret。这里 $E[\tau]$ 是预期的延迟。为了进一步改进其计算效率和扩展到高维RL问题，我们提出了增强版的 Delayed-LPSVI 算法，使用朗凯朋 dynamics 来实现约同 Optimal 的 regret 保证，但计算成本为 $\widetilde{O}(dHK)$。我们的实验表明我们的算法在统计和计算上具有良好的效果。
</details></li>
</ul>
<hr>
<h2 id="Debiasing-Algorithm-through-Model-Adaptation"><a href="#Debiasing-Algorithm-through-Model-Adaptation" class="headerlink" title="Debiasing Algorithm through Model Adaptation"></a>Debiasing Algorithm through Model Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18913">http://arxiv.org/abs/2310.18913</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tomlimi/DAMA">https://github.com/tomlimi/DAMA</a></li>
<li>paper_authors: Tomasz Limisiewicz, David Mareček, Tomáš Musil</li>
<li>for: 这项研究旨在检测和 Mitigating 语言模型中的性别偏见。</li>
<li>methods: 该研究使用 causal analysis 方法来 indentify 问题atic model components，并发现 mid-upper feed-forward layers 最容易传递偏见。基于分析结果，我们采用 linear projection 方法来修改模型。</li>
<li>results: 我们的 DAMA 方法能够 Significantly 降低 bias 指标，同时保持模型在下游任务中的表现。我们发布了我们的方法和模型代码，可以 retrained LLaMA 的 state-of-the-art performance，但是significantly less biased。<details>
<summary>Abstract</summary>
Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
</details>
<details>
<summary>摘要</summary>
大型语言模型在各种语言任务中成为首选解决方案，但是随着容量的增长，模型往往会受到来自于训练数据中的偏见和 sterotypes 的影响，导致模型偏误预测。本研究提出一种新的方法来检测和减轻语言模型中的性别偏见。我们通过 causal 分析发现，中upper feed-forward 层最容易传递偏见。根据分析结果，我们对这些层进行线性投影修改，实现了我们的 DAMA 方法，可以对多种度量进行减轻偏见，同时保持模型在下游任务上的性能。我们发布了我们的方法和模型代码，可以在 retrained LLaMA 的基础性能下进行训练，并且具有较少偏见的性能。
</details></li>
</ul>
<hr>
<h2 id="InstanT-Semi-supervised-Learning-with-Instance-dependent-Thresholds"><a href="#InstanT-Semi-supervised-Learning-with-Instance-dependent-Thresholds" class="headerlink" title="InstanT: Semi-supervised Learning with Instance-dependent Thresholds"></a>InstanT: Semi-supervised Learning with Instance-dependent Thresholds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18910">http://arxiv.org/abs/2310.18910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muyang Li, Runze Wu, Haoyu Liu, Jun Yu, Xun Yang, Bo Han, Tongliang Liu</li>
<li>for: 这篇论文主要是研究 semi-supervised learning（SSL）中的一种新方法，即使用实例висиendent的阈值来选择可信度高的无标示实例，以提高SSL的性能。</li>
<li>methods: 本文提出了一种新的实例висиendent阈值函数，通过利用实例的具体性和 pseudo-标签的实例 dependent 错误率来确定每个实例的阈值。此外，本文还提供了一个 bounded 概率保证，以确保 pseudo-标签 的正确性。</li>
<li>results: 实验结果表明，使用实例висиendent阈值函数可以提高 SSL 的性能，并且可以更好地适应实际应用中的不同数据分布。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) has been a fundamental challenge in machine learning for decades. The primary family of SSL algorithms, known as pseudo-labeling, involves assigning pseudo-labels to confident unlabeled instances and incorporating them into the training set. Therefore, the selection criteria of confident instances are crucial to the success of SSL. Recently, there has been growing interest in the development of SSL methods that use dynamic or adaptive thresholds. Yet, these methods typically apply the same threshold to all samples, or use class-dependent thresholds for instances belonging to a certain class, while neglecting instance-level information. In this paper, we propose the study of instance-dependent thresholds, which has the highest degree of freedom compared with existing methods. Specifically, we devise a novel instance-dependent threshold function for all unlabeled instances by utilizing their instance-level ambiguity and the instance-dependent error rates of pseudo-labels, so instances that are more likely to have incorrect pseudo-labels will have higher thresholds. Furthermore, we demonstrate that our instance-dependent threshold function provides a bounded probabilistic guarantee for the correctness of the pseudo-labels it assigns.
</details>
<details>
<summary>摘要</summary>
半监督学习（SSL）已经是机器学习领域的基本挑战之一。主要的SSL算法家族是使用 Pseudo-labeling 方法，即将 confidence 度不高的无标签实例分配 pseudo-label。因此，选择 pseudo-label 的标准是 SSL 的关键。近年来，有越来越多的关注于开发 SSL 方法，使用动态或适应性的阈值。然而，这些方法通常是对所有样本使用同一个阈值，或者使用基于类型的阈值 для归类为某个类型的实例，而忽略实例级别信息。在这篇论文中，我们提出了研究实例 dependent 阈值的想法。具体来说，我们设计了一种基于实例级别的阈值函数，用于所有无标签实例。我们利用实例级别的冗余和实例 dependent 的 pseudo-label 错误率，来确定实例的阈值。此外，我们也证明了我们的实例 dependent 阈值函数可以为 pseudo-label 提供 bounded 概率 garantue。
</details></li>
</ul>
<hr>
<h2 id="Stacking-the-Odds-Transformer-Based-Ensemble-for-AI-Generated-Text-Detection"><a href="#Stacking-the-Odds-Transformer-Based-Ensemble-for-AI-Generated-Text-Detection" class="headerlink" title="Stacking the Odds: Transformer-Based Ensemble for AI-Generated Text Detection"></a>Stacking the Odds: Transformer-Based Ensemble for AI-Generated Text Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18906">http://arxiv.org/abs/2310.18906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dukeraphaelng/synth_detectives">https://github.com/dukeraphaelng/synth_detectives</a></li>
<li>paper_authors: Duke Nguyen, Khaing Myat Noe Naing, Aditya Joshi</li>
<li>for: 本研究是参加 ALTA 2023 分享任务的提交作品，用于检测人工生成文本。</li>
<li>methods: 我们使用一个核心是 Transformer 的堆叠ensemble，选用了可 accessible 和轻量级的模型。</li>
<li>results: 我们的方法在官方提供的测试数据上达到了0.9555 的准确率。<details>
<summary>Abstract</summary>
This paper reports our submission under the team name `SynthDetectives' to the ALTA 2023 Shared Task. We use a stacking ensemble of Transformers for the task of AI-generated text detection. Our approach is novel in terms of its choice of models in that we use accessible and lightweight models in the ensemble. We show that ensembling the models results in an improved accuracy in comparison with using them individually. Our approach achieves an accuracy score of 0.9555 on the official test data provided by the shared task organisers.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:这篇论文报道我们在 `SynthDetectives' 团队名下提交到 ALTA 2023 共同任务。我们使用Transformers核心来实现人工生成文本检测任务。我们的方法在选择模型方面具有创新性，我们使用可 accessible 和轻量级模型。我们的结果表明，将多个模型 ensemble 后，对比使用单个模型时，具有更高的准确率。我们在官方提供的测试数据上 achieved 0.9555 的准确率。
</details></li>
</ul>
<hr>
<h2 id="Ever-Evolving-Evaluator-EV3-Towards-Flexible-and-Reliable-Meta-Optimization-for-Knowledge-Distillation"><a href="#Ever-Evolving-Evaluator-EV3-Towards-Flexible-and-Reliable-Meta-Optimization-for-Knowledge-Distillation" class="headerlink" title="Ever Evolving Evaluator (EV3): Towards Flexible and Reliable Meta-Optimization for Knowledge Distillation"></a>Ever Evolving Evaluator (EV3): Towards Flexible and Reliable Meta-Optimization for Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18893">http://arxiv.org/abs/2310.18893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Ding, Masrour Zoghi, Guy Tennenholtz, Maryam Karimzadehgan</li>
<li>for: 本文提出了一种基于综合优化的机器学习模型训练框架EV3，用于高效地训练可扩展的机器学习模型。</li>
<li>methods: EV3使用了一种intuitive的explore-assess-adapt协议，在每个迭代中探索不同的模型参数更新，使用相关的评估方法进行评估，并根据最佳更新和之前的进程历史来适应模型。</li>
<li>results: 实验结果表明，EV3可以安全地探索模型空间，并且在多个目标下可以动态调整任务。这种具有很大的灵活性和适应能力的方法，在多种领域可能有广泛的应用。<details>
<summary>Abstract</summary>
We introduce EV3, a novel meta-optimization framework designed to efficiently train scalable machine learning models through an intuitive explore-assess-adapt protocol. In each iteration of EV3, we explore various model parameter updates, assess them using pertinent evaluation methods, and adapt the model based on the optimal updates and previous progress history. EV3 offers substantial flexibility without imposing stringent constraints like differentiability on the key objectives relevant to the tasks of interest. Moreover, this protocol welcomes updates with biased gradients and allows for the use of a diversity of losses and optimizers. Additionally, in scenarios with multiple objectives, it can be used to dynamically prioritize tasks. With inspiration drawn from evolutionary algorithms, meta-learning, and neural architecture search, we investigate an application of EV3 to knowledge distillation. Our experimental results illustrate EV3's capability to safely explore model spaces, while hinting at its potential applicability across numerous domains due to its inherent flexibility and adaptability.
</details>
<details>
<summary>摘要</summary>
我们介绍EV3，一种新的元优化框架，用于高效地训练可扩展机器学习模型。EV3采用一种直观的探索-评估-适应协议，在每次迭代中探索不同的模型参数更新，使用相关的评估方法进行评估，并根据最佳更新和前一次进程历史来适应模型。EV3具有明显的灵活性，不需要在关键任务中强制执行梯度的微分性。此外，这个协议还允许使用多种损失函数和优化器，并在多个目标场景下动态准备任务。 drawing inspiration from evolutionary algorithms, meta-learning, and neural architecture search, we investigate the application of EV3 to knowledge distillation. Our experimental results show that EV3 can safely explore model spaces and hint at its potential applicability in various domains due to its inherent flexibility and adaptability.
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalized-Multi-stage-Clustering-Multi-view-Self-distillation"><a href="#Towards-Generalized-Multi-stage-Clustering-Multi-view-Self-distillation" class="headerlink" title="Towards Generalized Multi-stage Clustering: Multi-view Self-distillation"></a>Towards Generalized Multi-stage Clustering: Multi-view Self-distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18890">http://arxiv.org/abs/2310.18890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiatai Wang, Zhiwei Xu, Xin Wang</li>
<li>for: 这篇论文的目的是提出一个新的多stage深度嵌入式多视角 clustering 框架，以解决多视角 clustering 中 pseudo-label 错误导致模型预测不准确的问题。</li>
<li>methods: 这篇论文使用的方法包括 multi-view self-distillation (DistilMVC)，具体来说是在不同层次的特征空间中运用对比学习来探索多视角中的共同 semantics，并通过最大化两个视角之间的联系信息来取得 pseudo-labels。此外，还有一个 teacher network 负责将 pseudo-labels 转换为黑知识，以便对学习 Network 进行监督和改善预测能力。</li>
<li>results: 实验结果显示，这篇论文的方法在真实世界的多视角数据上表现较好，与现有的方法相比，具有更好的 clustering 性能。<details>
<summary>Abstract</summary>
Existing multi-stage clustering methods independently learn the salient features from multiple views and then perform the clustering task. Particularly, multi-view clustering (MVC) has attracted a lot of attention in multi-view or multi-modal scenarios. MVC aims at exploring common semantics and pseudo-labels from multiple views and clustering in a self-supervised manner. However, limited by noisy data and inadequate feature learning, such a clustering paradigm generates overconfident pseudo-labels that mis-guide the model to produce inaccurate predictions. Therefore, it is desirable to have a method that can correct this pseudo-label mistraction in multi-stage clustering to avoid the bias accumulation. To alleviate the effect of overconfident pseudo-labels and improve the generalization ability of the model, this paper proposes a novel multi-stage deep MVC framework where multi-view self-distillation (DistilMVC) is introduced to distill dark knowledge of label distribution. Specifically, in the feature subspace at different hierarchies, we explore the common semantics of multiple views through contrastive learning and obtain pseudo-labels by maximizing the mutual information between views. Additionally, a teacher network is responsible for distilling pseudo-labels into dark knowledge, supervising the student network and improving its predictive capabilities to enhance the robustness. Extensive experiments on real-world multi-view datasets show that our method has better clustering performance than state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现有的多阶段划分方法独立地学习多视图中的突出特征，然后进行划分任务。特别是，多视图划分（MVC）在多视图或多模式场景中吸引了很多关注。MVC aimed at exploring common semantics and pseudo-labels from multiple views and clustering in a self-supervised manner. However, limited by noisy data and inadequate feature learning, such a clustering paradigm generates overconfident pseudo-labels that misguide the model to produce inaccurate predictions. Therefore, it is desirable to have a method that can correct this pseudo-label mistraction in multi-stage clustering to avoid the bias accumulation. To alleviate the effect of overconfident pseudo-labels and improve the generalization ability of the model, this paper proposes a novel multi-stage deep MVC framework, where multi-view self-distillation (DistilMVC) is introduced to distill dark knowledge of label distribution. Specifically, in the feature subspace at different hierarchies, we explore the common semantics of multiple views through contrastive learning and obtain pseudo-labels by maximizing the mutual information between views. Additionally, a teacher network is responsible for distilling pseudo-labels into dark knowledge, supervising the student network and improving its predictive capabilities to enhance the robustness. Extensive experiments on real-world multi-view datasets show that our method has better clustering performance than state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Learning-of-Generalized-Structured-Matrices-for-Efficient-Deep-Neural-Networks"><a href="#Differentiable-Learning-of-Generalized-Structured-Matrices-for-Efficient-Deep-Neural-Networks" class="headerlink" title="Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks"></a>Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18882">http://arxiv.org/abs/2310.18882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changwoo Lee, Hun-Seok Kim</li>
<li>for: 这个论文是研究如何使用有效的深度神经网络（DNN）来取代笼统的权重矩阵，以便提高神经网络的性能和复杂性。</li>
<li>methods: 作者提出了一种通用和可导的框架，可以系统地学习权重矩阵的有效结构。该框架包括定义一种新的结构化矩阵类型，并采用频域分布式参数化方法来通过拥平方根下降来学习结构参数。</li>
<li>results: 作者的方法可以学习出高性能且低复杂度的DNN，并且比之前使用低级、块稀或块低级矩阵的方法更高效。<details>
<summary>Abstract</summary>
This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. Finally, we introduce an effective initialization method for the proposed scheme. Our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.
</details>
<details>
<summary>摘要</summary>
To address this challenge, the authors propose a generalized and differentiable framework for learning efficient weight matrix structures using gradient descent. They define a new class of structured matrices that covers a wide range of existing structured matrices, and use a frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel to learn the structural parameters.The proposed method uses proximal gradient descent to optimize the structural parameters, and introduces an effective initialization method to improve performance. The authors demonstrate that their approach learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior methods that use low-rank, block-sparse, or block-low-rank matrices.
</details></li>
</ul>
<hr>
<h2 id="HDMNet-A-Hierarchical-Matching-Network-with-Double-Attention-for-Large-scale-Outdoor-LiDAR-Point-Cloud-Registration"><a href="#HDMNet-A-Hierarchical-Matching-Network-with-Double-Attention-for-Large-scale-Outdoor-LiDAR-Point-Cloud-Registration" class="headerlink" title="HDMNet: A Hierarchical Matching Network with Double Attention for Large-scale Outdoor LiDAR Point Cloud Registration"></a>HDMNet: A Hierarchical Matching Network with Double Attention for Large-scale Outdoor LiDAR Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18874">http://arxiv.org/abs/2310.18874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyi Xue, Fan Lu, Guang Chen</li>
<li>for: 大规模外部LiDAR点云注册问题</li>
<li>methods: 提出了一种基于卷积神经网络的double attention机制的 hierarchical neural network（HDMNet），通过两stage匹配和高效的patch-to-patch匹配来提高注册性能。</li>
<li>results: 对两个大规模外部LiDAR点云数据集进行了广泛的实验，证明了提案的HDMNet可以具有高精度和高效性。<details>
<summary>Abstract</summary>
Outdoor LiDAR point clouds are typically large-scale and complexly distributed. To achieve efficient and accurate registration, emphasizing the similarity among local regions and prioritizing global local-to-local matching is of utmost importance, subsequent to which accuracy can be enhanced through cost-effective fine registration. In this paper, a novel hierarchical neural network with double attention named HDMNet is proposed for large-scale outdoor LiDAR point cloud registration. Specifically, A novel feature consistency enhanced double-soft matching network is introduced to achieve two-stage matching with high flexibility while enlarging the receptive field with high efficiency in a patch-to patch manner, which significantly improves the registration performance. Moreover, in order to further utilize the sparse matching information from deeper layer, we develop a novel trainable embedding mask to incorporate the confidence scores of correspondences obtained from pose estimation of deeper layer, eliminating additional computations. The high-confidence keypoints in the sparser point cloud of the deeper layer correspond to a high-confidence spatial neighborhood region in shallower layer, which will receive more attention, while the features of non-key regions will be masked. Extensive experiments are conducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate the high accuracy and efficiency of the proposed HDMNet.
</details>
<details>
<summary>摘要</summary>
大规模户外LiDAR点云注册通常具有复杂分布和大规模特征。为了实现高效和准确的注册，需要强调本地区域之间的相似性，并且优先进行全局本地-本地匹配。在本文中，一种名为HDMNet的新型层次神经网络是提出来解决大规模户外LiDAR点云注册问题。具体来说，我们引入了一种增强特征一致性的双注意网络，可以在patch-to-patch方式下实现高灵活性的两stage匹配，从而显著提高注册性能。此外，我们还开发了一种可 trains embeddingmask，以利用深层次姿态估计中的稀疏匹配信息，从而消除额外计算。高 confidence键点在更 sparse的点云中对应于深层次姿态中的高 confidence空间区域，这些区域将收到更多的注意力，而非键点区域的特征将被masked。我们在两个大规模户外LiDAR点云数据集上进行了广泛的实验，以示提出的HDMNet高精度和高效性。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Engineering-and-Transformer-based-Question-Generation-and-Evaluation"><a href="#Prompt-Engineering-and-Transformer-based-Question-Generation-and-Evaluation" class="headerlink" title="Prompt-Engineering and Transformer-based Question Generation and Evaluation"></a>Prompt-Engineering and Transformer-based Question Generation and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18867">http://arxiv.org/abs/2310.18867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rubaba Amyeen</li>
<li>for: 这篇论文主要是为了找出最佳的问题生成方法，以便在教学中使用。</li>
<li>methods: 该论文使用了一种名为distilBERT的变换器模型，并通过提示工程来生成问题。</li>
<li>results: 研究发现，使用这种方法可以生成高相似度的问题，其中30%的问题达到了高于70%的相似度。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Question generation has numerous applications in the educational context. Question generation can prove helpful for students when reviewing content and testing themselves. Furthermore, a question generation model can aid teachers by lessening the burden of creating assessments and other practice material. This paper aims to find the best method to generate questions from textual data through a transformer model and prompt engineering. In this research, we finetuned a pretrained distilBERT model on the SQuAD question answering dataset to generate questions. In addition to training a transformer model, prompt engineering was applied to generate questions effectively using the LLaMA model. The generated questions were compared against the baseline questions in the SQuAD dataset to evaluate the effectiveness of four different prompts. All four prompts demonstrated over 60% similarity on average. Of the prompt-generated questions, 30% achieved a high similarity score greater than 70%.
</details>
<details>
<summary>摘要</summary>
Question generation has numerous applications in educational contexts. Question generation can help students review content and assess themselves. Additionally, a question generation model can aid teachers by reducing the burden of creating assessments and practice material. This paper aims to find the best method to generate questions from textual data using a transformer model and prompt engineering. In this research, we fine-tuned a pre-trained distilBERT model on the SQuAD question answering dataset to generate questions. In addition to training a transformer model, prompt engineering was applied to generate questions effectively using the LLaMA model. The generated questions were compared to the baseline questions in the SQuAD dataset to evaluate the effectiveness of four different prompts. All four prompts showed an average similarity of over 60%. Of the prompt-generated questions, 30% achieved a high similarity score of over 70%.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.AI_2023_10_29/" data-id="cloimip5t006ds4887b6g7309" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.CL_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T11:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.CL_2023_10_29/">cs.CL - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="From-Chatbots-to-PhishBots-–-Preventing-Phishing-scams-created-using-ChatGPT-Google-Bard-and-Claude"><a href="#From-Chatbots-to-PhishBots-–-Preventing-Phishing-scams-created-using-ChatGPT-Google-Bard-and-Claude" class="headerlink" title="From Chatbots to PhishBots? – Preventing Phishing scams created using ChatGPT, Google Bard and Claude"></a>From Chatbots to PhishBots? – Preventing Phishing scams created using ChatGPT, Google Bard and Claude</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19181">http://arxiv.org/abs/2310.19181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayak Saha Roy, Poojitha Thota, Krishna Vamsi Naragam, Shirin Nilizadeh</li>
<li>for: 防止 Large Language Models (LLMs) 生成邪恶内容，包括骗财攻击。</li>
<li>methods: 使用四种常见的商业可用 LLMs（ChatGPT、GPT 4、Claude 和 Bard）生成功能骗财攻击，使用 serie 的邪恶提示。</li>
<li>results: 发现这些 LLMs 可以生成具有识别度的骗财电子邮件和网站，并且可以使用诸如逃脱检测系统的诡计来掩盖自己。<details>
<summary>Abstract</summary>
The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation. However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks. This study explores the potential of using four popular commercially available LLMs - ChatGPT (GPT 3.5 Turbo), GPT 4, Claude and Bard to generate functional phishing attacks using a series of malicious prompts. We discover that these LLMs can generate both phishing emails and websites that can convincingly imitate well-known brands, and also deploy a range of evasive tactics for the latter to elude detection mechanisms employed by anti-phishing systems. Notably, these attacks can be generated using unmodified, or "vanilla," versions of these LLMs, without requiring any prior adversarial exploits such as jailbreaking. As a countermeasure, we build a BERT based automated detection tool that can be used for the early detection of malicious prompts to prevent LLMs from generating phishing content attaining an accuracy of 97\% for phishing website prompts, and 94\% for phishing email prompts.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的高级功能使得它们在不同应用程序中变得不可或缺，从对话代理和内容创作到数据分析、研究和创新。然而，它们的效iveness和可用性也使得它们容易遭受用于生成恶意内容的违用，包括骗财攻击。这项研究探讨了使用四种流行的商业可用的 LLM——ChatGPT（GPT 3.5 Turbo）、GPT 4、Claude 和 Bard——生成功能攻击。我们发现这些 LLM 可以生成具有识别度的恶意电子邮件和网站，并且可以部署一系列逃脱检测机制的诡计。值得注意的是，这些攻击可以使用未修改的、“纯净”的 LLM 进行生成，不需要任何先前的反对攻击如监禁。作为对策，我们构建了基于 BERT 的自动检测工具，可以用于早期检测恶意提示，以防止 LLM 生成攻击内容，其准确率为 97%  для骗财网站提示，和 94%  для骗财电子邮件提示。
</details></li>
</ul>
<hr>
<h2 id="Robustifying-Language-Models-with-Test-Time-Adaptation"><a href="#Robustifying-Language-Models-with-Test-Time-Adaptation" class="headerlink" title="Robustifying Language Models with Test-Time Adaptation"></a>Robustifying Language Models with Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19177">http://arxiv.org/abs/2310.19177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah Thomas McDermott, Junfeng Yang, Chengzhi Mao</li>
<li>for: 防止语言模型受到语言攻击</li>
<li>methods: 使用遮盖词预测来动态适应输入句子，以逆转语言攻击</li>
<li>results: 在两个受欢迎的句子分类任务上，我们的方法可以修复65%以上的语言攻击In English, this means:</li>
<li>for: Preventing language models from being attacked by adversarial language</li>
<li>methods: Using dynamic adaptation of input sentences with predictions from masked words to reverse language adversarial attacks</li>
<li>results: Our method can repair over 65% of adversarial language attacks on two popular sentence classification tasks without requiring any training.<details>
<summary>Abstract</summary>
Large-scale language models achieved state-of-the-art performance over a number of language tasks. However, they fail on adversarial language examples, which are sentences optimized to fool the language models but with similar semantic meanings for humans. While prior work focuses on making the language model robust at training time, retraining for robustness is often unrealistic for large-scale foundation models. Instead, we propose to make the language models robust at test time. By dynamically adapting the input sentence with predictions from masked words, we show that we can reverse many language adversarial attacks. Since our approach does not require any training, it works for novel tasks at test time and can adapt to novel adversarial corruptions. Visualizations and empirical results on two popular sentence classification datasets demonstrate that our method can repair adversarial language attacks over 65% o
</details>
<details>
<summary>摘要</summary>
大规模语言模型在多种语言任务上实现了状态机器的表现，但它们对语言攻击例子失败，这些例子是用来欺骗语言模型的，但对人类来说含义相同的句子。而现有的工作通常在训练时做 robustness 的优化，但对大规模基础模型来说，这种 retraining 是不现实的。因此，我们提议在测试时使用语言模型的 robustness。我们通过在输入句子上动态适应预测结果来显示，我们可以反转许多语言攻击例子。我们的方法不需要任何训练，因此它在测试时可以对新任务进行适应，并且可以适应新的语言攻击。我们的实验结果和视觉化结果表明，我们的方法可以修复大于 65% 的语言攻击例子。
</details></li>
</ul>
<hr>
<h2 id="Poisoning-Retrieval-Corpora-by-Injecting-Adversarial-Passages"><a href="#Poisoning-Retrieval-Corpora-by-Injecting-Adversarial-Passages" class="headerlink" title="Poisoning Retrieval Corpora by Injecting Adversarial Passages"></a>Poisoning Retrieval Corpora by Injecting Adversarial Passages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19156">http://arxiv.org/abs/2310.19156</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/corpus-poisoning">https://github.com/princeton-nlp/corpus-poisoning</a></li>
<li>paper_authors: Zexuan Zhong, Ziqing Huang, Alexander Wettig, Danqi Chen</li>
<li>for: 本研究旨在测试紧密搜寻系统的安全性，特别是在真实世界应用中是否可以安全地启用。</li>
<li>methods: 作者提出了一种新的对紧密搜寻系统的攻击方法，其中一名黑客产生了一小批陌生过程，并将其插入到大量搜寻 corpora 中，以导致紧密搜寻系统错误地回答查询。</li>
<li>results: 研究发现，这种攻击可以将紧密搜寻系统误导回答，并且这些陌生过程可以直接扩展到不同的域外查询和 corpora，例如在金融文档或网络论坛中，50个生成的过程可以误导&gt;94%的查询。<details>
<summary>Abstract</summary>
Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but to what extent can they be safely deployed in real-world applications? In this work, we propose a novel attack for dense retrieval systems in which a malicious user generates a small number of adversarial passages by perturbing discrete tokens to maximize similarity with a provided set of training queries. When these adversarial passages are inserted into a large retrieval corpus, we show that this attack is highly effective in fooling these systems to retrieve them for queries that were not seen by the attacker. More surprisingly, these adversarial passages can directly generalize to out-of-domain queries and corpora with a high success attack rate -- for instance, we find that 50 generated passages optimized on Natural Questions can mislead >94% of questions posed in financial documents or online forums. We also benchmark and compare a range of state-of-the-art dense retrievers, both unsupervised and supervised. Although different systems exhibit varying levels of vulnerability, we show they can all be successfully attacked by injecting up to 500 passages, a small fraction compared to a retrieval corpus of millions of passages.
</details>
<details>
<summary>摘要</summary>
dense retrievers 在多种信息检索任务中实现了状态码表现，但它们在实际应用中是否可以安全部署？在这项工作中，我们提出了一种 novel 的攻击方法，malicious user 通过修改精确的token来生成一小数量的对抗 passage，以最大化与提供的训练问题的相似性。当这些对抗 passage 添加到大量的检索库中时，我们发现这种攻击非常有效，能让这些系统 retrieve 这些恶意生成的 passage 作为未看过的查询。即使在不同的领域和数据集上，这些对抗 passage 仍然能够直接泛化，并达到高度的成功攻击率。例如，我们发现50个优化后的对抗 passage 可以误导 >94%的金融文档或在线讨论中的问题。我们还对多种当前最佳的 dense retrievers 进行了 benchmark 和比较，包括不超过500个对抗 passage 的攻击。虽然不同的系统在攻击上展现出不同的抵抗程度，但我们发现所有这些系统都可以被成功攻击。
</details></li>
</ul>
<hr>
<h2 id="BERT-Lost-Patience-Won’t-Be-Robust-to-Adversarial-Slowdown"><a href="#BERT-Lost-Patience-Won’t-Be-Robust-to-Adversarial-Slowdown" class="headerlink" title="BERT Lost Patience Won’t Be Robust to Adversarial Slowdown"></a>BERT Lost Patience Won’t Be Robust to Adversarial Slowdown</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19152">http://arxiv.org/abs/2310.19152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ztcoalson/waffle">https://github.com/ztcoalson/waffle</a></li>
<li>paper_authors: Zachary Coalson, Gabriel Ritter, Rakesh Bobba, Sanghyun Hong</li>
<li>for: 这 paper 评估了多出口语言模型对钝化攻击的 Robustness。</li>
<li>methods: 作者设计了一种钝化攻击，通过生成自然的 adversarial text 绕过早出点。 然后，他们使用这种 WAFFLE 攻击来进行多出口机制的全面评估，并在 GLUE benchmark 上测试了三种多出口机制在钝化攻击下的性能。</li>
<li>results: 研究发现，钝化攻击可以减少多出口机制提供的计算成本，特别是对于复杂的机制而言。 此外，研究还发现了一些常见的 perturbation 模式，并与标准的 adversarial text 攻击进行比较。 最后，研究发现了输入整形可以有效地解决钝化攻击，但是 adversarial training 无法战胜钝化攻击。<details>
<summary>Abstract</summary>
In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can remove perturbations effectively. This result suggests that future work is needed for developing efficient yet robust multi-exit models. Our code is available at: https://github.com/ztcoalson/WAFFLE
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们系统地评估了多出口语言模型对针对性慢速攻击的Robustness。为了审计其Robustness，我们设计了一种通过绕过早出点生成自然针对性文本的攻击。我们使用这种WAFFLE攻击来进行对三种多出口机制的GLUEbenchmark进行广泛的评估，并显示我们的攻击可以在白盒和黑盒设置下减少了这些方法提供的计算成本。我们发现，复杂的机制更容易受到针对性慢速攻击。此外，我们还进行了文本输入的语言分析，找到了我们的攻击生成的扰乱模式，并与标准针对性攻击相比较。此外，我们还发现，对于我们的慢速攻击，反向训练无法有效地抵抗，但是使用 conversational 模型，例如 ChatGPT，可以有效地除掉扰乱。这种结果表明，未来的工作需要开发高效又Robust的多出口模型。我们的代码可以在：https://github.com/ztcoalson/WAFFLE 获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Follow-Object-Centric-Image-Editing-Instructions-Faithfully"><a href="#Learning-to-Follow-Object-Centric-Image-Editing-Instructions-Faithfully" class="headerlink" title="Learning to Follow Object-Centric Image Editing Instructions Faithfully"></a>Learning to Follow Object-Centric Image Editing Instructions Faithfully</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19145">http://arxiv.org/abs/2310.19145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tuhinjubcse/faithfuledits_emnlp2023">https://github.com/tuhinjubcse/faithfuledits_emnlp2023</a></li>
<li>paper_authors: Tuhin Chakrabarty, Kanishk Singh, Arkadiy Saakyan, Smaranda Muresan</li>
<li>for: 这篇论文旨在提高文本到图像扩展模型中的自然语言指令编辑性能。</li>
<li>methods: 本文提出了一种基于最新的分割、链式思维提示和视觉问答技术的方法，可以提高自然语言指令下的图像编辑质量。</li>
<li>results: 对比于现有的基线，该方法能够进行细化的对象中心编辑，并且能够在未经训练的领域中进行扩展。此外，模型还能够捕捉到文本指令中的含义，进行 faithfulness 的捕捉和修改。<details>
<summary>Abstract</summary>
Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models. However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to be performed), 3) faithfulness (the need to preserve the elements of the image not affected by the edit instruction). Current approaches focusing on image editing with natural language instructions rely on automatically generated paired data, which, as shown in our investigation, is noisy and sometimes nonsensical, exacerbating the above issues. Building on recent advances in segmentation, Chain-of-Thought prompting, and visual question answering, we significantly improve the quality of the paired data. In addition, we enhance the supervision signal by highlighting parts of the image that need to be changed by the instruction. The model fine-tuned on the improved data is capable of performing fine-grained object-centric edits better than state-of-the-art baselines, mitigating the problems outlined above, as shown by automatic and human evaluations. Moreover, our model is capable of generalizing to domains unseen during training, such as visual metaphors.
</details>
<details>
<summary>摘要</summary>
自然语言指令是文本到图像扩散模型的高级用户界面。然而，需要解决以下挑战：1）下pecification（需要模型理解指令的隐含含义）、2）grounding（需要确定编辑操作的具体位置）、3）loyal（需要保持图像中未受影响的元素）。现有的方法通过自动生成的对应数据来实现图像编辑，但这些数据经过我们的调查发现噪音和无意义，这些问题进一步加剧了上述问题。我们基于最近的分割、链条提示和视觉问答技术进行了大幅改进，提高对应数据的质量。此外，我们还强调要更改的图像部分，以提高模型的精细化对象编辑能力。经过练习这些改进后的模型，在自动和人工评估中都能够更好地完成细化的对象编辑任务，并且能够在训练时未看到的领域中进行推断。此外，我们的模型还能够捕捉到视觉 метаFOR，进一步提高图像编辑的精度和效果。
</details></li>
</ul>
<hr>
<h2 id="Women-Wearing-Lipstick-Measuring-the-Bias-Between-an-Object-and-Its-Related-Gender"><a href="#Women-Wearing-Lipstick-Measuring-the-Bias-Between-an-Object-and-Its-Related-Gender" class="headerlink" title="Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender"></a>Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19130">http://arxiv.org/abs/2310.19130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahmedssabir/genderscore">https://github.com/ahmedssabir/genderscore</a></li>
<li>paper_authors: Ahmed Sabir, Lluís Padró</li>
<li>for:  investigate the impact of objects on gender bias in image captioning systems</li>
<li>methods:  use visual semantic-based gender score to measure the degree of bias</li>
<li>results:  propose a gender score that can be used as an additional metric to existing approach, and observe the bias relation between caption and related gender<details>
<summary>Abstract</summary>
In this paper, we investigate the impact of objects on gender bias in image captioning systems. Our results show that only gender-specific objects have a strong gender bias (e.g., women-lipstick). In addition, we propose a visual semantic-based gender score that measures the degree of bias and can be used as a plug-in for any image captioning system. Our experiments demonstrate the utility of the gender score, since we observe that our score can measure the bias relation between a caption and its related gender; therefore, our score can be used as an additional metric to the existing Object Gender Co-Occ approach. Code and data are publicly available at \url{https://github.com/ahmedssabir/GenderScore}.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了图像描述系统中的性别偏见。我们的结果显示，只有性别特定的物品会带有强烈的性别偏见（例如女性 lipstick）。此外，我们提出了基于视觉 semantics 的性别分数，可以用于任何图像描述系统中。我们的实验表明了这个分数的用途，因为我们发现了这个分数可以测量描述和其相关的性别之间的偏见关系，因此可以用作现有的 Object Gender Co-Occ 方法的附加指标。代码和数据都可以在 \url{https://github.com/ahmedssabir/GenderScore} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Unified-Representation-for-Non-compositional-and-Compositional-Expressions"><a href="#Unified-Representation-for-Non-compositional-and-Compositional-Expressions" class="headerlink" title="Unified Representation for Non-compositional and Compositional Expressions"></a>Unified Representation for Non-compositional and Compositional Expressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19127">http://arxiv.org/abs/2310.19127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziheng Zeng, Suma Bhat</li>
<li>for: This paper is written for researchers and developers working on natural language processing (NLP) and machine learning, specifically those interested in non-compositional language and idiomatic expressions.</li>
<li>methods: The paper proposes a language model called PIER, which builds on BART and generates semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs).</li>
<li>results: The paper shows that the representations generated by PIER result in higher homogeneity scores for embedding clustering and gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA, without sacrificing performance on NLU tasks.<details>
<summary>Abstract</summary>
Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs). PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic interpretations. Via intrinsic evaluation on embedding quality and extrinsic evaluation on PIE processing and NLU tasks, we show that representations generated by PIER result in 33% higher homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29% gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA. These gains are achieved without sacrificing PIER's performance on NLU tasks (+/- 1% accuracy) compared to BART.
</details>
<details>
<summary>摘要</summary>
Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs). PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic interpretations. Via intrinsic evaluation on embedding quality and extrinsic evaluation on PIE processing and NLU tasks, we show that representations generated by PIER result in 33% higher homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29% gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA. These gains are achieved without sacrificing PIER's performance on NLU tasks (+/- 1% accuracy) compared to BART.Here's the translation in Traditional Chinese: Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs). PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic interpretations. Via intrinsic evaluation on embedding quality and extrinsic evaluation on PIE processing and NLU tasks, we show that representations generated by PIER result in 33% higher homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29% gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA. These gains are achieved without sacrificing PIER's performance on NLU tasks (+/- 1% accuracy) compared to BART.
</details></li>
</ul>
<hr>
<h2 id="PACuna-Automated-Fine-Tuning-of-Language-Models-for-Particle-Accelerators"><a href="#PACuna-Automated-Fine-Tuning-of-Language-Models-for-Particle-Accelerators" class="headerlink" title="PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators"></a>PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19106">http://arxiv.org/abs/2310.19106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonin Sulc, Raimund Kammering, Annika Eichler, Tim Wilksen</li>
<li>for: 提高加速器设备的理解和解释能力</li>
<li>methods: 使用公开available的加速器资源（如会议、预印文章和书籍）自动生成问题和数据集，并使用Language模型进行精细调整</li>
<li>results: PACuna可以解决复杂的加速器问题，并被专家 validateTranslation:</li>
<li>for: 提高加速器设备的理解和解释能力</li>
<li>methods: 使用公开available的加速器资源（如会议、预印文章和书籍）自动生成问题和数据集，并使用Language模型进行精细调整</li>
<li>results: PACuna可以解决复杂的加速器问题，并被专家 validate<details>
<summary>Abstract</summary>
Navigating the landscape of particle accelerators has become increasingly challenging with recent surges in contributions. These intricate devices challenge comprehension, even within individual facilities. To address this, we introduce PACuna, a fine-tuned language model refined through publicly available accelerator resources like conferences, pre-prints, and books. We automated data collection and question generation to minimize expert involvement and make the data publicly available. PACuna demonstrates proficiency in addressing intricate accelerator questions, validated by experts. Our approach shows adapting language models to scientific domains by fine-tuning technical texts and auto-generated corpora capturing the latest developments can further produce pre-trained models to answer some intricate questions that commercially available assistants cannot and can serve as intelligent assistants for individual facilities.
</details>
<details>
<summary>摘要</summary>
在加速器领域的探索中，由于最近的贡献增加， navigating 已成为越来越复杂的任务。这些细腻的设备会使人们感到困惑，甚至在同一个设施内。为解决这问题，我们介绍了 PACuna，一种精度调整的语言模型，通过公共可用的加速器资源，如会议、预印和书籍来优化。我们自动收集数据和生成问题，以最小化专家参与度，并将数据公开可用。 PACuna 在解决复杂的加速器问题方面表现出色，由专家 validate。我们的方法表明，通过科学领域中的技术文本和自动生成的 corpora 来练习语言模型，可以生成适用于一些复杂问题的预训练模型，这些模型可以作为加速器设施的智能助手。
</details></li>
</ul>
<hr>
<h2 id="Pushdown-Layers-Encoding-Recursive-Structure-in-Transformer-Language-Models"><a href="#Pushdown-Layers-Encoding-Recursive-Structure-in-Transformer-Language-Models" class="headerlink" title="Pushdown Layers: Encoding Recursive Structure in Transformer Language Models"></a>Pushdown Layers: Encoding Recursive Structure in Transformer Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19089">http://arxiv.org/abs/2310.19089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D. Manning</li>
<li>for: This paper aims to improve the syntactic generalization of Transformer language models by introducing a new self-attention layer called Pushdown Layers.</li>
<li>methods: The Pushdown Layers model recursive state via a stack tape that tracks estimated depths of every token, and the Transformer LMs with Pushdown Layers use this stack tape to softly modulate attention over tokens.</li>
<li>results: The authors achieve dramatically better and 3-5x more sample-efficient syntactic generalization when training Transformers equipped with Pushdown Layers on a corpus of strings annotated with silver constituency parses, while maintaining similar perplexities.<details>
<summary>Abstract</summary>
Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization. This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the observed prefix. Transformer LMs with Pushdown Layers are syntactic language models that autoregressively and synchronously update this stack tape as they predict new tokens, in turn using the stack tape to softly modulate attention over tokens -- for instance, learning to "skip" over closed constituents. When trained on a corpus of strings annotated with silver constituency parses, Transformers equipped with Pushdown Layers achieve dramatically better and 3-5x more sample-efficient syntactic generalization, while maintaining similar perplexities. Pushdown Layers are a drop-in replacement for standard self-attention. We illustrate this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed WikiText-103, leading to improvements on several GLUE text classification tasks.
</details>
<details>
<summary>摘要</summary>
人类语言中具有重要特点的一种是Recursion，它对于自注意机制的缺乏显式状态跟踪机制而具有挑战性。因此，Transformer语言模型在捕捉长尾递归结构方面表现不佳，并且 exhibit  sample-inefficient  sintactic generalization。这项工作介绍了Pushdown层，一种新的自注意层，通过一个堆栈带跟踪每个字符的估计深度来模型 recursive state。Transformer LMs  WITH Pushdown Layers 是一种强式语言模型，可以同步和顺序地更新这个堆栈带，并在预测新字符时使用堆栈来软模式地修饰注意力。例如，学习"跳过"关闭的成分。当在一个Silver Constituency Parses 的集合上训练 Transformer 时，它们配备 Pushdown Layers 可以在同样的批量大小下达到更好的 3-5 倍的样本效率，同时保持相似的折衔率。Pushdown Layers 是一种可替换的自注意层。我们通过对 GPT2-medium  WITH Pushdown Layers 在自动生成的 WikiText-103 上进行训练，来示例这一点。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Recent-Named-Entity-Recognition-and-Relation-Classification-Methods-with-Focus-on-Few-Shot-Learning-Approaches"><a href="#A-Survey-on-Recent-Named-Entity-Recognition-and-Relation-Classification-Methods-with-Focus-on-Few-Shot-Learning-Approaches" class="headerlink" title="A Survey on Recent Named Entity Recognition and Relation Classification Methods with Focus on Few-Shot Learning Approaches"></a>A Survey on Recent Named Entity Recognition and Relation Classification Methods with Focus on Few-Shot Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19055">http://arxiv.org/abs/2310.19055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sakher Alqaaidi, Elika Bozorgi</li>
<li>for: 本研究主要针对非结构化文本中的命名实体识别和关系类型分类两个关键阶段，以抽取有用信息。</li>
<li>methods: 本文主要介绍了最新的非结构化文本处理应用中的命名实体识别和关系类型分类方法，特别是几步学习方法。</li>
<li>results: 本文对两个领域的最新成果进行了比较分析，并对几步学习方法的结果进行了结构化分析。<details>
<summary>Abstract</summary>
Named entity recognition and relation classification are key stages for extracting information from unstructured text. Several natural language processing applications utilize the two tasks, such as information retrieval, knowledge graph construction and completion, question answering and other domain-specific applications, such as biomedical data mining. We present a survey of recent approaches in the two tasks with focus on few-shot learning approaches. Our work compares the main approaches followed in the two paradigms. Additionally, we report the latest metric scores in the two tasks with a structured analysis that considers the results in the few-shot learning scope.
</details>
<details>
<summary>摘要</summary>
Named entity recognition和关系分类是抽取无结构文本信息的关键阶段。许多自然语言处理应用程序利用这两个任务，如信息检索、知识图构建和完善、问答等领域应用程序，以及生物医学数据挖掘等领域应用程序。我们对最近的方法进行了评论，并对几种学习 paradigms进行了比较。此外，我们还对这两个任务中最新的 метри克分数进行了报告，并进行了结构化分析，考虑到几种少量学习范围内的结果。
</details></li>
</ul>
<hr>
<h2 id="ArBanking77-Intent-Detection-Neural-Model-and-a-New-Dataset-in-Modern-and-Dialectical-Arabic"><a href="#ArBanking77-Intent-Detection-Neural-Model-and-a-New-Dataset-in-Modern-and-Dialectical-Arabic" class="headerlink" title="ArBanking77: Intent Detection Neural Model and a New Dataset in Modern and Dialectical Arabic"></a>ArBanking77: Intent Detection Neural Model and a New Dataset in Modern and Dialectical Arabic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19034">http://arxiv.org/abs/2310.19034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa Jarrar, Ahmet Birim, Mohammed Khalilia, Mustafa Erden, Sana Ghanem</li>
<li>for: 本研究开发了一个大型的阿拉伯语言Intent检测dataset，名为ArBanking77，并将其 arabized 和 localized 到了英文 Banking77 dataset。</li>
<li>methods: 本研究使用了一个基于 AraBERT 的神经网络模型，并在 ArBanking77 上进行了 fine-tuning，以达到了 F1-score 的 0.9209 和 0.8995 在 Modern Standard Arabic 和 Palestinian dialect 中 respectively。</li>
<li>results: 本研究实现了对 live chat 查询中的实际应用，并在 simulated low-resource 环境下进行了广泛的实验，以评估模型在不同的情况下的表现。<details>
<summary>Abstract</summary>
This paper presents the ArBanking77, a large Arabic dataset for intent detection in the banking domain. Our dataset was arabized and localized from the original English Banking77 dataset, which consists of 13,083 queries to ArBanking77 dataset with 31,404 queries in both Modern Standard Arabic (MSA) and Palestinian dialect, with each query classified into one of the 77 classes (intents). Furthermore, we present a neural model, based on AraBERT, fine-tuned on ArBanking77, which achieved an F1-score of 0.9209 and 0.8995 on MSA and Palestinian dialect, respectively. We performed extensive experimentation in which we simulated low-resource settings, where the model is trained on a subset of the data and augmented with noisy queries to simulate colloquial terms, mistakes and misspellings found in real NLP systems, especially live chat queries. The data and the models are publicly available at https://sina.birzeit.edu/arbanking77.
</details>
<details>
<summary>摘要</summary>
Note: "AraBERT" is a pre-trained Arabic language model, similar to BERT.
</details></li>
</ul>
<hr>
<h2 id="SALMA-Arabic-Sense-Annotated-Corpus-and-WSD-Benchmarks"><a href="#SALMA-Arabic-Sense-Annotated-Corpus-and-WSD-Benchmarks" class="headerlink" title="SALMA: Arabic Sense-Annotated Corpus and WSD Benchmarks"></a>SALMA: Arabic Sense-Annotated Corpus and WSD Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19029">http://arxiv.org/abs/2310.19029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa Jarrar, Sanad Malaysha, Tymaa Hammouda, Mohammed Khalilia</li>
<li>for: 这个论文是为了描述一个新的阿拉伯语意义权重annotated corpus（SALMA），以及该 corpus 的注释工具和评估 metric。</li>
<li>methods: 这个论文使用了两种不同的意义инвенタри（Modern和Ghani）同时进行注释，并为每个词语提供了多个意义的分数。在注释过程中，研究人员还使用了六种名称实体的注释。</li>
<li>results: 研究人员通过了多种 metric（Kappa、Lineal Weighted Kappa、Quadratic Weighted Kappa、Mean Average Error、Root Mean Square Error）来评估注释质量，并发现了非常高的间接对应者一致性。此外，研究人员还开发了一个基于 Target Sense Verification 的 Word Sense Disambiguation 系统，并使用这个系统来评估三种 Target Sense Verification 模型的性能，其中最佳模型的准确率达到了 84.2%（使用 Modern）和 78.7%（使用 Ghani）。<details>
<summary>Abstract</summary>
SALMA, the first Arabic sense-annotated corpus, consists of ~34K tokens, which are all sense-annotated. The corpus is annotated using two different sense inventories simultaneously (Modern and Ghani). SALMA novelty lies in how tokens and senses are associated. Instead of linking a token to only one intended sense, SALMA links a token to multiple senses and provides a score to each sense. A smart web-based annotation tool was developed to support scoring multiple senses against a given word. In addition to sense annotations, we also annotated the corpus using six types of named entities. The quality of our annotations was assessed using various metrics (Kappa, Linear Weighted Kappa, Quadratic Weighted Kappa, Mean Average Error, and Root Mean Square Error), which show very high inter-annotator agreement. To establish a Word Sense Disambiguation baseline using our SALMA corpus, we developed an end-to-end Word Sense Disambiguation system using Target Sense Verification. We used this system to evaluate three Target Sense Verification models available in the literature. Our best model achieved an accuracy with 84.2% using Modern and 78.7% using Ghani. The full corpus and the annotation tool are open-source and publicly available at https://sina.birzeit.edu/salma/.
</details>
<details>
<summary>摘要</summary>
SALMA，首个阿拉伯语意义注释 корпу斯，包含约34000个字符，所有字符都有意义注释。 corpora 使用两个不同的意义集 simultaneously (Modern 和 Ghani)。 SALMA 的创新在于如何将字符和意义相关联。而不是将字符与一个固定的意义相关联，SALMA 将字符与多个意义相关联，并为每个意义提供一个分数。为支持多个意义对一个词的分数，我们开发了一个智能的网络基于的注释工具。此外，我们还对 corpora 进行了六种命名实体的注释。我们对注释的质量进行了多种 metric 评估（Kappa、线性权重Kappa、quadratic Weighted Kappa、平均误差和根平方误差），它们显示了非常高的间对注释者一致性。为建立基于我们 SALMA  корпу的单词意义推断基线，我们开发了一个 Target Sense Verification 基于的全 End-to-end Word Sense Disambiguation 系统。我们使用这个系统来评估Literature 中提供的三种 Target Sense Verification 模型。我们的最佳模型在Modern 和 Ghani 中达到了84.2%和78.7%的准确率。整个corpus 和注释工具都是开源的，可以在 <https://sina.birzeit.edu/salma/> 获取。
</details></li>
</ul>
<hr>
<h2 id="LLMs-and-Finetuning-Benchmarking-cross-domain-performance-for-hate-speech-detection"><a href="#LLMs-and-Finetuning-Benchmarking-cross-domain-performance-for-hate-speech-detection" class="headerlink" title="LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection"></a>LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18964">http://arxiv.org/abs/2310.18964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Nasir, Aadish Sharma, Kokil Jaidka</li>
<li>for: 这 paper 比较了不同的预训练和精度调整的大语言模型（LLMs）在仇恨言语检测中的表现。</li>
<li>methods: 本研究发现了cross-domain 适用性和过拟合风险是LLMs的主要挑战。我们通过评估发现了需要更多的标签多样性来让模型更好地捕捉仇恨言语的细节。</li>
<li>results: 我们的研究结果表明，通过适度调整和更多的标签多样性，可以提高模型的泛化性和检测精度。我们认为未来的仇恨言语检测应该强调cross-domain泛化和合适的benchmarking实践。<details>
<summary>Abstract</summary>
This paper compares different pre-trained and fine-tuned large language models (LLMs) for hate speech detection. Our research underscores challenges in LLMs' cross-domain validity and overfitting risks. Through evaluations, we highlight the need for fine-tuned models that grasp the nuances of hate speech through greater label heterogeneity. We conclude with a vision for the future of hate speech detection, emphasizing cross-domain generalizability and appropriate benchmarking practices.
</details>
<details>
<summary>摘要</summary>
这篇论文比较了不同的预训练和微调大型自然语言模型（LLM）对仇视言语检测的性能。我们的研究强调了LLM在不同领域的交叉领域有效性和过拟合风险。通过评估，我们强调需要微调模型，以便更好地捕捉仇视言语的细节和多样性。我们 conclude with a future vision for hate speech detection，强调跨领域一致性和合适的标准化实践。Note that the word " LL" in the original text was translated as "LLM" in Simplified Chinese, as "LL" is not a commonly used term in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="S2F-NER-Exploring-Sequence-to-Forest-Generation-for-Complex-Entity-Recognition"><a href="#S2F-NER-Exploring-Sequence-to-Forest-Generation-for-Complex-Entity-Recognition" class="headerlink" title="S2F-NER: Exploring Sequence-to-Forest Generation for Complex Entity Recognition"></a>S2F-NER: Exploring Sequence-to-Forest Generation for Complex Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18944">http://arxiv.org/abs/2310.18944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongxiu Xu, Heyan Huang, Yue Hu</li>
<li>for: 这篇论文主要针对复杂的实体识别问题（Named Entity Recognition，NER），例如嵌入、重叠和不连续的实体。</li>
<li>methods: 我们提出了一种新的序列到森林生成模式（Sequence-to-Forest，S2F-NER），它可以直接在句子中提取实体，而不是采用传统的序列到序列（Sequence-to-Sequence，Seq2Seq）生成模式。</li>
<li>results: 我们的模型在三个不连续NER数据集和两个嵌入NER数据集上表现出色，特别是对于不连续实体识别。<details>
<summary>Abstract</summary>
Named Entity Recognition (NER) remains challenging due to the complex entities, like nested, overlapping, and discontinuous entities. Existing approaches, such as sequence-to-sequence (Seq2Seq) generation and span-based classification, have shown impressive performance on various NER subtasks, but they are difficult to scale to datasets with longer input text because of either exposure bias issue or inefficient computation. In this paper, we propose a novel Sequence-to-Forest generation paradigm, S2F-NER, which can directly extract entities in sentence via a Forest decoder that decode multiple entities in parallel rather than sequentially. Specifically, our model generate each path of each tree in forest autoregressively, where the maximum depth of each tree is three (which is the shortest feasible length for complex NER and is far smaller than the decoding length of Seq2Seq). Based on this novel paradigm, our model can elegantly mitigates the exposure bias problem and keep the simplicity of Seq2Seq. Experimental results show that our model significantly outperforms the baselines on three discontinuous NER datasets and on two nested NER datasets, especially for discontinuous entity recognition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Retrofitting-Light-weight-Language-Models-for-Emotions-using-Supervised-Contrastive-Learning"><a href="#Retrofitting-Light-weight-Language-Models-for-Emotions-using-Supervised-Contrastive-Learning" class="headerlink" title="Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning"></a>Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18930">http://arxiv.org/abs/2310.18930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sapan Shah, Sreedhar Reddy, Pushpak Bhattacharyya</li>
<li>for: 这篇论文旨在探讨如何将情感方面的知识嵌入预训语言模型（BERT和RoBERTa）中，以提高模型的情感识别能力。</li>
<li>methods: 这篇论文使用对照学习方法将预训网络重新训练，使得文本片段具有相似情感时，在表现空间中被更加靠近地编码，而具有不同情感内容时则被推离。同时，这篇论文还确保了预训网络中对语言知识的不偏独影响。</li>
<li>results: 这篇论文的结果显示，使用这种方法更新预训网络的BERT和RoBERTa模型，可以实现情感识别的改进。对于情感分析和讽刺检测任务，这些模型比预训网络原始版本（约1%的提升）和其他已知方法更好。此外，这些更新后的模型在少量学习设定下表现更好。<details>
<summary>Abstract</summary>
We present a novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates pre-trained network weights using contrastive learning so that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and the fragments with different emotion content are pushed apart. While doing so, it also ensures that the linguistic knowledge already present in PLMs is not inadvertently perturbed. The language models retrofitted by our method, i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, as evaluated through different clustering and retrieval metrics. For the downstream tasks on sentiment analysis and sarcasm detection, they perform better than their pre-trained counterparts (about 1% improvement in F1-score) and other existing approaches. Additionally, a more significant boost in performance is observed for the retrofitted models over pre-trained ones in few-shot learning setting.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的改进方法，用于启用语言模型（PLM）中的情感方面，如BERT和RoBERTa。我们的方法通过对预训练网络权重进行更新，使得表达同样情感的文本片段在表示空间中相近，而表达不同情感的片段则被推迟。同时，我们的方法 также确保了预训练语言模型中的语言知识不会偶然受到影响。我们修改后的语言模型，即BERTEmo和RoBERTaEmo，可以生成情感意识的文本表示，根据不同的聚类和检索指标进行评估。在情感分析和讽刺检测下投入下，它们与预训练模型（大约1%的提升）和其他现有方法相比，表现出较好的性能。此外，我们发现在少量学习 Setting中，修改后的模型比预训练模型表现更好，具有更大的提升。
</details></li>
</ul>
<hr>
<h2 id="Sentence-Bag-Graph-Formulation-for-Biomedical-Distant-Supervision-Relation-Extraction"><a href="#Sentence-Bag-Graph-Formulation-for-Biomedical-Distant-Supervision-Relation-Extraction" class="headerlink" title="Sentence Bag Graph Formulation for Biomedical Distant Supervision Relation Extraction"></a>Sentence Bag Graph Formulation for Biomedical Distant Supervision Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18912">http://arxiv.org/abs/2310.18912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Yang Liu, Xiaoyan Liu, Tianming Liang, Gaurav Sharma, Liang Xue, Maozu Guo</li>
<li>for: 提高生物医学数据中 distant supervision relation extraction 的精度和效果。</li>
<li>methods: 提出了一种基于图的框架，使用 message-passing 的信息汇集机制，解决了远级指导关系提取中的噪声标注问题，同时也能够有效地捕捉句子袋内sentence之间的依赖关系。</li>
<li>results: 在两个大规模生物医学关系集和 NYT 集上进行了广泛的实验，并证明了我们提出的方法可以在生物医学数据中 distant supervision relation extraction 中表现出色，同时也在普通文本挖掘领域中表现出优秀的relation extraction 能力。<details>
<summary>Abstract</summary>
We introduce a novel graph-based framework for alleviating key challenges in distantly-supervised relation extraction and demonstrate its effectiveness in the challenging and important domain of biomedical data. Specifically, we propose a graph view of sentence bags referring to an entity pair, which enables message-passing based aggregation of information related to the entity pair over the sentence bag. The proposed framework alleviates the common problem of noisy labeling in distantly supervised relation extraction and also effectively incorporates inter-dependencies between sentences within a bag. Extensive experiments on two large-scale biomedical relation datasets and the widely utilized NYT dataset demonstrate that our proposed framework significantly outperforms the state-of-the-art methods for biomedical distant supervision relation extraction while also providing excellent performance for relation extraction in the general text mining domain.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的图structure-based框架，用于解决远程supervised关系抽取中的一些主要挑战，并在生物医学数据中进行了实质性的证明。特别是，我们提出了一种将句子袋视为实体对的图视图，使得对于实体对的信息在句子袋中进行消息传递基于的聚合。该提议的框架可以解决远程supervised关系抽取中的常见问题，即标签杂乱，并有效地 incorporate句子之间的依赖关系。我们在两个大规模的生物医学关系数据集和常用的NYT数据集上进行了广泛的实验，得到了我们提议的框架在生物医学关系抽取中的显著超越州方法的性能，同时也在文本挖掘领域中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Pre-trained-Speech-Processing-Models-Contain-Human-Like-Biases-that-Propagate-to-Speech-Emotion-Recognition"><a href="#Pre-trained-Speech-Processing-Models-Contain-Human-Like-Biases-that-Propagate-to-Speech-Emotion-Recognition" class="headerlink" title="Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition"></a>Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18877">http://arxiv.org/abs/2310.18877</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/isaaconline/speat">https://github.com/isaaconline/speat</a></li>
<li>paper_authors: Isaac Slaughter, Craig Greenberg, Reva Schwartz, Aylin Caliskan</li>
<li>for: 这个研究旨在检测语音处理模型中的偏见，具体来说是检测预训练模型中的偏见。</li>
<li>methods: 这个研究使用了Speech Embedding Association Test（SpEAT）来检测预训练模型中的偏见。SpEAT是基于自然语言处理中的词嵌入协会测试，用于量化模型对不同概念的偏见，如种族、性别等。</li>
<li>results: 这个研究发现了14种预训练模型中的偏见，包括abled人群对disabled人群的正面偏见、欧洲裔美国人群对非洲裔美国人群的正面偏见、女性对♂的正面偏见、美国口音 speaker对非美国口音 speaker的正面偏见、年轻人群对老年人群的正面偏见。此外，研究还发现了这些偏见在下游任务Speech Emotion Recognition（SER）中的影响。在66个测试中（69%），由SpEAT测试发现的偏见与SER任务中的偏见相关。<details>
<summary>Abstract</summary>
Previous work has established that a person's demographics and speech style affect how well speech processing models perform for them. But where does this bias come from? In this work, we present the Speech Embedding Association Test (SpEAT), a method for detecting bias in one type of model used for many speech tasks: pre-trained models. The SpEAT is inspired by word embedding association tests in natural language processing, which quantify intrinsic bias in a model's representations of different concepts, such as race or valence (something's pleasantness or unpleasantness) and capture the extent to which a model trained on large-scale socio-cultural data has learned human-like biases. Using the SpEAT, we test for six types of bias in 16 English speech models (including 4 models also trained on multilingual data), which come from the wav2vec 2.0, HuBERT, WavLM, and Whisper model families. We find that 14 or more models reveal positive valence (pleasantness) associations with abled people over disabled people, with European-Americans over African-Americans, with females over males, with U.S. accented speakers over non-U.S. accented speakers, and with younger people over older people. Beyond establishing that pre-trained speech models contain these biases, we also show that they can have real world effects. We compare biases found in pre-trained models to biases in downstream models adapted to the task of Speech Emotion Recognition (SER) and find that in 66 of the 96 tests performed (69%), the group that is more associated with positive valence as indicated by the SpEAT also tends to be predicted as speaking with higher valence by the downstream model. Our work provides evidence that, like text and image-based models, pre-trained speech based-models frequently learn human-like biases. Our work also shows that bias found in pre-trained models can propagate to the downstream task of SER.
</details>
<details>
<summary>摘要</summary>
先前的研究已经证明人的民族和语言风格会影响语音处理模型对他们的性能。但是这种偏见来自哪里？在这项工作中，我们介绍了语音嵌入协会测试（SpEAT），用于检测语音处理模型中的偏见。SpEAT Draws inspiration from natural language processing中的嵌入协会测试，用于衡量不同概念的嵌入表示，如种族或语言风格，并捕捉模型从大规模社会文化数据中学习的人类化偏见。使用SpEAT，我们测试了16种英语语音模型（包括4种多语言模型），来自wav2vec 2.0、HuBERT、WavLM和Whisper模型家族。我们发现14个或更多的模型表现出了有利可能（愉悦）偏见，即abled人群比 disabled人群更有利可能，European-Americans比 African-Americans更有利可能，女性比男性更有利可能，U.S.口音说话者比非U.S.口音说话者更有利可能，以及年轻人比老年人更有利可能。我们不仅证明了语音处理模型中的这些偏见，还表明它们可能有实际的影响。我们比较了预训练模型中的偏见和下游任务speech emotion recognition（SER）模型中的偏见，发现在96次测试中（69%），与预训练模型中的偏见相关的组 Also tends to be predicted as speaking with higher valence by the downstream model。我们的工作证明了，如文本和图像基于模型一样，预训练语音基于模型经常学习人类化偏见。我们的工作还表明了预训练模型中的偏见可能会传播到下游任务中。
</details></li>
</ul>
<hr>
<h2 id="MUST-A-Multilingual-Student-Teacher-Learning-approach-for-low-resource-speech-recognition"><a href="#MUST-A-Multilingual-Student-Teacher-Learning-approach-for-low-resource-speech-recognition" class="headerlink" title="MUST: A Multilingual Student-Teacher Learning approach for low-resource speech recognition"></a>MUST: A Multilingual Student-Teacher Learning approach for low-resource speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18865">http://arxiv.org/abs/2310.18865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Umar Farooq, Rehan Ahmad, Thomas Hain</li>
<li>for: 本研究旨在解决语音识别系统训练中数据稀缺问题，通过学生教师学习（KD）方法。</li>
<li>methods: 本研究使用的方法包括提议一种多语言学生教师（MUST）学习方法，利用一个预训练的映射模型将教师语言的 posterior 映射到学生语言的 ASR 模型中。</li>
<li>results: 根据实验结果，使用 MUST 学习方法可以将Relative Character Error Rate（CER）降低到9.5%，相比基eline monolingual ASR 模型。<details>
<summary>Abstract</summary>
Student-teacher learning or knowledge distillation (KD) has been previously used to address data scarcity issue for training of speech recognition (ASR) systems. However, a limitation of KD training is that the student model classes must be a proper or improper subset of the teacher model classes. It prevents distillation from even acoustically similar languages if the character sets are not same. In this work, the aforementioned limitation is addressed by proposing a MUltilingual Student-Teacher (MUST) learning which exploits a posteriors mapping approach. A pre-trained mapping model is used to map posteriors from a teacher language to the student language ASR. These mapped posteriors are used as soft labels for KD learning. Various teacher ensemble schemes are experimented to train an ASR model for low-resource languages. A model trained with MUST learning reduces relative character error rate (CER) up to 9.5% in comparison with a baseline monolingual ASR.
</details>
<details>
<summary>摘要</summary>
学生教师学习或知识蒸馏（KD）已经曾用于解决训练语音识别（ASR）系统的数据稀缺问题。然而，KD 训练的一个限制是学生模型类型必须是教师模型类型的正确或错误子集。这会防止训练不同语言的扩展，即使字符集不同。在这种情况下，我们提出了一种多语言学生教师（MUST）学习方法，利用 posterior mapping 技术。我们使用一个预训练的映射模型将教师语言的 posterior 映射到学生语言 ASR 中。这些映射 posterior 用作 KD 学习的软标签。我们对各种教师集合方案进行了实验，以训练一个低资源语言的 ASR 模型。与基线单语言 ASR 模型相比，我们的 MUST 学习方法可以降低相对字符错误率（CER）的差异为9.5%。
</details></li>
</ul>
<hr>
<h2 id="Counterfactually-Probing-Language-Identity-in-Multilingual-Models"><a href="#Counterfactually-Probing-Language-Identity-in-Multilingual-Models" class="headerlink" title="Counterfactually Probing Language Identity in Multilingual Models"></a>Counterfactually Probing Language Identity in Multilingual Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18862">http://arxiv.org/abs/2310.18862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/venkatasg/multilingual-counterfactual-probing">https://github.com/venkatasg/multilingual-counterfactual-probing</a></li>
<li>paper_authors: Anirudh Srinivasan, Venkata S Govindarajan, Kyle Mahowald</li>
<li>for: 这 paper 探讨了语言模型中语言信息的组织方式，使用一种技术 called AlterRep 进行 counterfactual probing。</li>
<li>methods: 作者使用了一种 linear classifier 来解释 tokens 的语言标识 Task，并通过 counterfactual probing 方法来探讨模型的内部结构。</li>
<li>results: 研究发现，给定一个 Language X 模板，向 Language Y 方向推动 embedding 会系统性地增加 Language Y 词汇的概率，超过第三方控制语言。但是，这并不特别地推动模型转化为翻译相当的 Language Y 词汇。向 Language X 方向推动也有一定的效果，但是会有些程度下降。总之，这些结果表明大量多语言语言模型具有both语言特定和语言通用的结构，并且 counterfactual probing 可以成功应用于多语言模型。<details>
<summary>Abstract</summary>
Techniques in causal analysis of language models illuminate how linguistic information is organized in LLMs. We use one such technique, AlterRep, a method of counterfactual probing, to explore the internal structure of multilingual models (mBERT and XLM-R). We train a linear classifier on a binary language identity task, to classify tokens between Language X and Language Y. Applying a counterfactual probing procedure, we use the classifier weights to project the embeddings into the null space and push the resulting embeddings either in the direction of Language X or Language Y. Then we evaluate on a masked language modeling task. We find that, given a template in Language X, pushing towards Language Y systematically increases the probability of Language Y words, above and beyond a third-party control language. But it does not specifically push the model towards translation-equivalent words in Language Y. Pushing towards Language X (the same direction as the template) has a minimal effect, but somewhat degrades these models. Overall, we take these results as further evidence of the rich structure of massive multilingual language models, which include both a language-specific and language-general component. And we show that counterfactual probing can be fruitfully applied to multilingual models.
</details>
<details>
<summary>摘要</summary>
使用 causal 分析技术可以探索语言模型（mBERT 和 XLM-R）中的语言信息结构。我们使用一种方法——Counterfactual probing，以探索这些模型的内部结构。我们在一个 binary 语言标识任务上训练了一个线性分类器，以分类Token是来自哪种语言。通过对这些分类器权重进行Counterfactual probing操作，我们可以将表示Vector проек到null空间中，并将其推动向Language X 或 Language Y 方向。然后，我们在一个隐藏语言模型任务上进行评估。我们发现，当给定一个 Language X 模板时，推动向 Language Y 方向会系统地增加 Language Y 词汇的概率，而这与第三种控制语言相比，这种效果明显。但是，不会 Specifically push 模型向翻译相同的 Language Y 词汇方向。推动向 Language X （与模板相同的方向）的效果相对较小，但是会有一定程度的降低这些模型的性能。总之，我们认为这些结果是证明大型多语言语言模型具有了rich结构，包括语言特定和语言通用的组成部分。同时，我们示出了对多语言模型的 counterfactual probing 可以得到有用的结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.CL_2023_10_29/" data-id="cloimip7r00d3s48888dn5kr3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.LG_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T10:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.LG_2023_10_29/">cs.LG - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improved-Motor-Imagery-Classification-Using-Adaptive-Spatial-Filters-Based-on-Particle-Swarm-Optimization-Algorithm"><a href="#Improved-Motor-Imagery-Classification-Using-Adaptive-Spatial-Filters-Based-on-Particle-Swarm-Optimization-Algorithm" class="headerlink" title="Improved Motor Imagery Classification Using Adaptive Spatial Filters Based on Particle Swarm Optimization Algorithm"></a>Improved Motor Imagery Classification Using Adaptive Spatial Filters Based on Particle Swarm Optimization Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19202">http://arxiv.org/abs/2310.19202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong Xiong, Ying Wang, Tianyuan Song, Jinguo Huang, Guixia Kang</li>
<li>for: 这个论文主要针对的应用领域是 robot控制、roke rehabilitation 和 stroke 或脊梁损伤患者的助手。</li>
<li>methods: 该论文提出了一种基于 particle swarm optimization 算法 (PSO) 的适应空间筛选解决方案，用于提取 MI-EEG 信号中更加有效的空间特征，以提高分类性能。</li>
<li>results:  Comparative experiments 表明，该提案的方法在两个公共数据集（2a 和 2b）上实现了显著的平均识别率提高，达到 74.61% 和 81.19% 分别。相比基eline algorithm（FBCSP），该提案的算法提高了 11.44% 和 7.11% 在两个数据集上。<details>
<summary>Abstract</summary>
As a typical self-paced brain-computer interface (BCI) system, the motor imagery (MI) BCI has been widely applied in fields such as robot control, stroke rehabilitation, and assistance for patients with stroke or spinal cord injury. Many studies have focused on the traditional spatial filters obtained through the common spatial pattern (CSP) method. However, the CSP method can only obtain fixed spatial filters for specific input signals. Besides, CSP method only focuses on the variance difference of two types of electroencephalogram (EEG) signals, so the decoding ability of EEG signals is limited. To obtain more effective spatial filters for better extraction of spatial features that can improve classification to MI-EEG, this paper proposes an adaptive spatial filter solving method based on particle swarm optimization algorithm (PSO). A training and testing framework based on filter bank and spatial filters (FBCSP-ASP) is designed for MI EEG signal classification. Comparative experiments are conducted on two public datasets (2a and 2b) from BCI competition IV, which show the outstanding average recognition accuracy of FBCSP-ASP. The proposed method has achieved significant performance improvement on MI-BCI. The classification accuracy of the proposed method has reached 74.61% and 81.19% on datasets 2a and 2b, respectively. Compared with the baseline algorithm (FBCSP), the proposed algorithm improves 11.44% and 7.11% on two datasets respectively. Furthermore, the analysis based on mutual information, t-SNE and Shapley values further proves that ASP features have excellent decoding ability for MI-EEG signals, and explains the improvement of classification performance by the introduction of ASP features.
</details>
<details>
<summary>摘要</summary>
如常的自适应脑机器接口（BCI）系统中，运动想象（MI）BCI已经广泛应用在机器人控制、roke rehabilitation 和roke 或脊梁损伤患者的助手等领域。许多研究都集中在传统的空间筛选（CSP）方法上。然而，CSP 方法只能获得特定输入信号的固定空间筛选。此外，CSP 方法只关注两种电enzephalogram（EEG）信号之间的差异，因此EEG 信号的解码能力受限。为了获得更有效的空间筛选，提高MI-EEG 信号的特征提取，这篇文章提出了基于聚合粒子猎 optimization 算法（PSO）的自适应空间筛选解决方案。为MI EEG 信号类型的分类，设计了一个基于筛选银行和空间筛选（FBCSP-ASP）的训练和测试框架。对于BCI 竞赛 IV 公共数据集（2a 和 2b）进行了比较性实验，实验结果表明，提案方法在MI-BCI 中 achieved significant performance improvement。提案方法的识别率为74.61% 和 81.19% 在数据集 2a 和 2b 中，相比基准算法（FBCSP），提案算法提高了11.44% 和 7.11% 在两个数据集中。此外，基于mutual information、t-SNE 和 Shapley 值的分析进一步证明了ASP 特征对MI-EEG 信号的解码能力具有优秀性，并解释了提案方法的性能提升原因。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Motor-Imagery-Decoding-in-Brain-Computer-Interfaces-using-Riemann-Tangent-Space-Mapping-and-Cross-Frequency-Coupling"><a href="#Enhancing-Motor-Imagery-Decoding-in-Brain-Computer-Interfaces-using-Riemann-Tangent-Space-Mapping-and-Cross-Frequency-Coupling" class="headerlink" title="Enhancing Motor Imagery Decoding in Brain Computer Interfaces using Riemann Tangent Space Mapping and Cross Frequency Coupling"></a>Enhancing Motor Imagery Decoding in Brain Computer Interfaces using Riemann Tangent Space Mapping and Cross Frequency Coupling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19198">http://arxiv.org/abs/2310.19198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong Xiong, Li Su, Jinguo Huang, Guixia Kang<br>for: 这个论文的目的是提高motor imagery（MI）特征的编码和解码能力。methods: 这篇论文提出了一种基于Riemannian geometry和Cross-Frequency Coupling（CFC）的新方法，称为Riemann Tangent Space Mapping using Dichotomous Filter Bank with Convolutional Neural Network（DFBRTS），用于提高MI特征的表达质量和解码能力。DFBRTS使用了一个完整的二进制树结构的 dichotomous filter bank来精炼EEG信号，然后使用Riemann Tangent Space Mapping提取每个子带中的突出的EEG信号特征。最后，一个轻量级的卷积神经网络被用于进一步提取特征和分类，在彼此之间同时受到了cross-entropy和center loss的联合监督。results: 对于BCI竞赛IV 2a（BCIC-IV-2a）数据集和OpenBMI数据集进行了广泛的实验，DFBRTS在两个数据集上显示出了明显的优异性，在四个类和二个类的保留分类中分别达到了78.16%和71.58%的高精度分类率，与现有的参考值进行比较。<details>
<summary>Abstract</summary>
Objective: Motor Imagery (MI) serves as a crucial experimental paradigm within the realm of Brain Computer Interfaces (BCIs), aiming to decoding motor intentions from electroencephalogram (EEG) signals. Method: Drawing inspiration from Riemannian geometry and Cross-Frequency Coupling (CFC), this paper introduces a novel approach termed Riemann Tangent Space Mapping using Dichotomous Filter Bank with Convolutional Neural Network (DFBRTS) to enhance the representation quality and decoding capability pertaining to MI features. DFBRTS first initiates the process by meticulously filtering EEG signals through a Dichotomous Filter Bank, structured in the fashion of a complete binary tree. Subsequently, it employs Riemann Tangent Space Mapping to extract salient EEG signal features within each sub-band. Finally, a lightweight convolutional neural network is employed for further feature extraction and classification, operating under the joint supervision of cross-entropy and center loss. To validate the efficacy, extensive experiments were conducted using DFBRTS on two well-established benchmark datasets: the BCI competition IV 2a (BCIC-IV-2a) dataset and the OpenBMI dataset. The performance of DFBRTS was benchmarked against several state-of-the-art MI decoding methods, alongside other Riemannian geometry-based MI decoding approaches. Results: DFBRTS significantly outperforms other MI decoding algorithms on both datasets, achieving a remarkable classification accuracy of 78.16% for four-class and 71.58% for two-class hold-out classification, as compared to the existing benchmarks.
</details>
<details>
<summary>摘要</summary>
目的：使用电气生物 интерфей斯（BCI）中的动作幻像（MI）作为关键实验方法，从电气生物学信号（EEG）中提取动作意图。方法：基于里敦纬度 geometry和跨频相关（CFC），这篇论文提出了一种新的方法，即里敦 Tangent Space Mapping using Dichotomous Filter Bank with Convolutional Neural Network（DFBRTS），以提高MI特征的表达质量和解码能力。DFBRTS首先通过完整的 binary tree 结构的 dichotomous Filter Bank 精细筛选 EEG 信号，然后使用里敦 Tangent Space Mapping 提取每个子带中的优秀 EEG 信号特征。最后，一种轻量级的 convolutional neural network 进行进一步的特征提取和分类，在joint 超VI 和中心损失的协同监督下运行。为证明DFBRTS的效果，对DFBRTS进行了广泛的实验，并与其他里敦 geometry 基于MI解码方法进行比较。结果：DFBRTS在两个常用的benchmark数据集上（BCIC-IV-2a数据集和OpenBMI数据集）上显著地超过了其他MI解码方法，达到了78.16%的四类分类率和71.58%的二类分类率。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Normalization-in-Recurrent-Neural-Network-of-Grid-Cells"><a href="#Conformal-Normalization-in-Recurrent-Neural-Network-of-Grid-Cells" class="headerlink" title="Conformal Normalization in Recurrent Neural Network of Grid Cells"></a>Conformal Normalization in Recurrent Neural Network of Grid Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19192">http://arxiv.org/abs/2310.19192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dehong Xu, Ruiqi Gao, Wen-Hao Zhang, Xue-Xin Wei, Ying Nian Wu</li>
<li>for: 该研究探讨了grid cells在脑内的响应模式，以及这些模式如何影响agent的 Navigation。</li>
<li>methods: 研究使用了高维神经网络，并提出了一种简单而普遍的准确normalization方法，以便在agent移动时保持响应vector的正确尺寸。</li>
<li>results: 实验结果表明，通过使用准确normalization方法，grid cells可以形成六角形响应模式，并且这些模式与agent的实际位置在2D physical space有直接的关系。<details>
<summary>Abstract</summary>
Grid cells in the entorhinal cortex of the mammalian brain exhibit striking hexagon firing patterns in their response maps as the animal (e.g., a rat) navigates in a 2D open environment. The responses of the population of grid cells collectively form a vector in a high-dimensional neural activity space, and this vector represents the self-position of the agent in the 2D physical space. As the agent moves, the vector is transformed by a recurrent neural network that takes the velocity of the agent as input. In this paper, we propose a simple and general conformal normalization of the input velocity for the recurrent neural network, so that the local displacement of the position vector in the high-dimensional neural space is proportional to the local displacement of the agent in the 2D physical space, regardless of the direction of the input velocity. Our numerical experiments on the minimally simple linear and non-linear recurrent networks show that conformal normalization leads to the emergence of the hexagon grid patterns. Furthermore, we derive a new theoretical understanding that connects conformal normalization to the emergence of hexagon grid patterns in navigation tasks.
</details>
<details>
<summary>摘要</summary>
“ENTORHINAL CORTEX中的格子细胞在动物（例如鼠）在2D开放环境中探索时表现出惊人的六角发射模式。这些细胞的响应集体形成一个高维神经活动空间中的向量，该向量表示动物的自身位置在2D物理空间中。随着动物的移动，这个向量被一个循环神经网络转换，该神经网络的输入是动物的速度。在这篇论文中，我们提出了一种简单而普遍的几何正常化方法，以确保输入速度的local displacement在高维神经空间中与动物在2D物理空间中的local displacement成正比。我们的数值实验表明，几何正常化会导致格子网格模式的出现。此外，我们还derived一种新的理论理解，该理解连接几何正常化与导航任务中的格子网格模式的出现。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="The-Power-of-Explainability-in-Forecast-Informed-Deep-Learning-Models-for-Flood-Mitigation"><a href="#The-Power-of-Explainability-in-Forecast-Informed-Deep-Learning-Models-for-Flood-Mitigation" class="headerlink" title="The Power of Explainability in Forecast-Informed Deep Learning Models for Flood Mitigation"></a>The Power of Explainability in Forecast-Informed Deep Learning Models for Flood Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19166">http://arxiv.org/abs/2310.19166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimeng Shi, Vitalii Stebliankin, Giri Narasimhan</li>
<li>for: 这 paper 是为了提出一种基于深度学习架构的洪水管理方法，以优化洪水预 release 的决策。</li>
<li>methods: 这 paper 使用了 Forecast Informed Deep Learning Architecture (FIDLAR)， combinig 预测和深度学习来实现洪水管理。</li>
<li>results: 实验结果表明，FIDLAR 比现有的状态艺术有几个数量级的速度提高，并且可以提供更好的预 release 决策。这些速度提高使得 FIDLAR 可以用于实时洪水管理。此外，这 paper 还使用了工具来解释模型的决策，从而更好地理解洪水管理中环境因素的贡献。<details>
<summary>Abstract</summary>
Floods can cause horrific harm to life and property. However, they can be mitigated or even avoided by the effective use of hydraulic structures such as dams, gates, and pumps. By pre-releasing water via these structures in advance of extreme weather events, water levels are sufficiently lowered to prevent floods. In this work, we propose FIDLAR, a Forecast Informed Deep Learning Architecture, achieving flood management in watersheds with hydraulic structures in an optimal manner by balancing out flood mitigation and unnecessary wastage of water via pre-releases. We perform experiments with FIDLAR using data from the South Florida Water Management District, which manages a coastal area that is highly prone to frequent storms and floods. Results show that FIDLAR performs better than the current state-of-the-art with several orders of magnitude speedup and with provably better pre-release schedules. The dramatic speedups make it possible for FIDLAR to be used for real-time flood management. The main contribution of this paper is the effective use of tools for model explainability, allowing us to understand the contribution of the various environmental factors towards its decisions.
</details>
<details>
<summary>摘要</summary>
洪水可以带来惊人的破坏力和财产损失。然而，通过有效地使用 гидро利用结构，如坝、闸门和泵，可以减轻或缓解洪水的影响。在这种情况下，我们提出了 FIDLAR，一种基于预测的深度学习架构，通过在洪水事件前预先释放水，以达到Optimal flood management in watersheds with hydraulic structures by balancing flood mitigation and water wastage via pre-releases.我们在使用南佛瑞达水资源管理区的数据进行实验，这是一个常遇洪水的沿海地区。结果显示，FIDLAR比当前状态艺术有几个数量级的速度减速，并且可以证明更好的预release schedule。这些减速使得FIDLAR可以用于实时洪水管理。本文的主要贡献是通过工具来描述模型的解释，以便理解响应不同环境因素的决策的贡献。
</details></li>
</ul>
<hr>
<h2 id="RAIFLE-Reconstruction-Attacks-on-Interaction-based-Federated-Learning-with-Active-Data-Manipulation"><a href="#RAIFLE-Reconstruction-Attacks-on-Interaction-based-Federated-Learning-with-Active-Data-Manipulation" class="headerlink" title="RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning with Active Data Manipulation"></a>RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning with Active Data Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19163">http://arxiv.org/abs/2310.19163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dzungvpham/raifle">https://github.com/dzungvpham/raifle</a></li>
<li>paper_authors: Dzung Pham, Shreyas Kulkarni, Amir Houmansadr</li>
<li>for: 这个论文关注了 federated learning (FL) 中的用户隐私问题，具体来说是在用户互动域中的 recommender systems (RS) 和 online learning to rank (OLTR) 中。</li>
<li>methods: 这篇论文使用了一种名为 RAIFLE 的整合优化基于攻击框架，用于攻击 IFL 系统中的用户隐私。RAIFLE 使用了一种新的攻击技术名为 Active Data Manipulation (ADM)，通过在训练特征上操纵ITEMS来导致本地 FL 更新中的 adversarial 行为。</li>
<li>results: 论文表明 RAIFLE 可以在 IFL 系统中更有效地攻击用户隐私，并且可以干扰隐私防御技术，如安全汇聚和私人信息检索。基于这些发现，论文提出了一些Countermeasure 建议来 mitigate 这种攻击。<details>
<summary>Abstract</summary>
Federated learning (FL) has recently emerged as a privacy-preserving approach for machine learning in domains that rely on user interactions, particularly recommender systems (RS) and online learning to rank (OLTR). While there has been substantial research on the privacy of traditional FL, little attention has been paid to studying the privacy properties of these interaction-based FL (IFL) systems. In this work, we show that IFL can introduce unique challenges concerning user privacy, particularly when the central server has knowledge and control over the items that users interact with. Specifically, we demonstrate the threat of reconstructing user interactions by presenting RAIFLE, a general optimization-based reconstruction attack framework customized for IFL. RAIFLE employs Active Data Manipulation (ADM), a novel attack technique unique to IFL, where the server actively manipulates the training features of the items to induce adversarial behaviors in the local FL updates. We show that RAIFLE is more impactful than existing FL privacy attacks in the IFL context, and describe how it can undermine privacy defenses like secure aggregation and private information retrieval. Based on our findings, we propose and discuss countermeasure guidelines to mitigate our attack in the context of federated RS/OLTR specifically and IFL more broadly.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）已经被认为是一种保护用户隐私的机器学习方法，尤其是在用户互动域中，如推荐系统（RS）和在线学习排名（OLTR）等领域。 although there has been extensive research on the privacy of traditional FL, little attention has been paid to the privacy properties of these interaction-based FL（IFL）systems. in this work, we show that IFL can introduce unique challenges concerning user privacy, particularly when the central server has knowledge and control over the items that users interact with. specifically, we demonstrate the threat of reconstructing user interactions by presenting RAIFLE, a general optimization-based reconstruction attack framework customized for IFL. RAIFLE employs Active Data Manipulation（ADM）, a novel attack technique unique to IFL, where the server actively manipulates the training features of the items to induce adversarial behaviors in the local FL updates. we show that RAIFLE is more impactful than existing FL privacy attacks in the IFL context, and describe how it can undermine privacy defenses like secure aggregation and private information retrieval. based on our findings, we propose and discuss countermeasure guidelines to mitigate our attack in the context of federated RS/OLTR specifically and IFL more broadly.
</details></li>
</ul>
<hr>
<h2 id="Transfer-Learning-in-Transformer-Based-Demand-Forecasting-For-Home-Energy-Management-System"><a href="#Transfer-Learning-in-Transformer-Based-Demand-Forecasting-For-Home-Energy-Management-System" class="headerlink" title="Transfer Learning in Transformer-Based Demand Forecasting For Home Energy Management System"></a>Transfer Learning in Transformer-Based Demand Forecasting For Home Energy Management System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19159">http://arxiv.org/abs/2310.19159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gargya Gokhale, Jonas Van Gompel, Bert Claessens, Chris Develder</li>
<li>for: 这个研究旨在开发一个基于转移学习的家用电力负载预测模型，以提高家用电力负载预测的精度和效率。</li>
<li>methods: 研究人员使用了一种名为“时间融合 трансформа”的先进预测模型，并通过将这个全球模型调整到新的一个家用电力负载数据中，以提高预测的精度和效率。</li>
<li>results: 研究人员发现，使用转移学习设置可以比仅使用单一家用电力负载数据更好地预测家用电力负载，具体而言，可以降低预测误差率约15%，并且可以降低家用电力负载成本约2%。<details>
<summary>Abstract</summary>
Increasingly, homeowners opt for photovoltaic (PV) systems and/or battery storage to minimize their energy bills and maximize renewable energy usage. This has spurred the development of advanced control algorithms that maximally achieve those goals. However, a common challenge faced while developing such controllers is the unavailability of accurate forecasts of household power consumption, especially for shorter time resolutions (15 minutes) and in a data-efficient manner. In this paper, we analyze how transfer learning can help by exploiting data from multiple households to improve a single house's load forecasting. Specifically, we train an advanced forecasting model (a temporal fusion transformer) using data from multiple different households, and then finetune this global model on a new household with limited data (i.e. only a few days). The obtained models are used for forecasting power consumption of the household for the next 24 hours~(day-ahead) at a time resolution of 15 minutes, with the intention of using these forecasts in advanced controllers such as Model Predictive Control. We show the benefit of this transfer learning setup versus solely using the individual new household's data, both in terms of (i) forecasting accuracy ($\sim$15\% MAE reduction) and (ii) control performance ($\sim$2\% energy cost reduction), using real-world household data.
</details>
<details>
<summary>摘要</summary>
HOMEOWNERS 对光伏系统和/或电池储存系统的选择在增加，以最大化能源成本和可再生能源使用。这导致了高级控制算法的发展，以最大化这些目标。然而，开发这些控制器时常遇到缺乏精确的家用电力消耗预测，特别是在短时间尺度（15分钟）和高效率下。在这篇文章中，我们分析了如何使用传播学习来解决这个问题。我们使用多个不同的家庭的数据来训练进阶预测模型（时间融合变换器），然后在新的家庭中进行精确化训练（仅使用几天的数据）。所得到的模型用于预测新家庭的电力消耗预测，时间尺度为24小时（日前），每15分钟一次。我们显示了这个传播学习设置的优点，包括预测精度（约15% MAE减少）和控制性能（约2%能源成本减少），使用实际家庭数据进行评估。
</details></li>
</ul>
<hr>
<h2 id="Real-World-Implementation-of-Reinforcement-Learning-Based-Energy-Coordination-for-a-Cluster-of-Households"><a href="#Real-World-Implementation-of-Reinforcement-Learning-Based-Energy-Coordination-for-a-Cluster-of-Households" class="headerlink" title="Real-World Implementation of Reinforcement Learning Based Energy Coordination for a Cluster of Households"></a>Real-World Implementation of Reinforcement Learning Based Energy Coordination for a Cluster of Households</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19155">http://arxiv.org/abs/2310.19155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gargya Gokhale, Niels Tiben, Marie-Sophie Verwee, Manu Lahariya, Bert Claessens, Chris Develder<br>for: 这 paper 的目的是研究如何通过聚合控制多幢住宅建筑物来为现代电力网提供支持服务，包括备用服务。methods: 这 paper 使用了学习反馈控制（RL）技术来协调8幢住宅建筑物的电力消耗，不需要任何建筑模型或模拟器，因此实施和扩展非常方便。results: 这 paper 通过实验示出了RL基于排名系统选择哪些户型动用可变资产，并使用实时PI控制机制来控制选择的资产，实现了功能的电力跟踪和RL基于数据驱动的排名效果的可行性。<details>
<summary>Abstract</summary>
Given its substantial contribution of 40\% to global power consumption, the built environment has received increasing attention to serve as a source of flexibility to assist the modern power grid. In that respect, previous research mainly focused on energy management of individual buildings. In contrast, in this paper, we focus on aggregated control of a set of residential buildings, to provide grid supporting services, that eventually should include ancillary services. In particular, we present a real-life pilot study that studies the effectiveness of reinforcement-learning (RL) in coordinating the power consumption of 8 residential buildings to jointly track a target power signal. Our RL approach relies solely on observed data from individual households and does not require any explicit building models or simulators, making it practical to implement and easy to scale. We show the feasibility of our proposed RL-based coordination strategy in a real-world setting. In a 4-week case study, we demonstrate a hierarchical control system, relying on an RL-based ranking system to select which households to activate flex assets from, and a real-time PI control-based power dispatch mechanism to control the selected assets. Our results demonstrate satisfactory power tracking, and the effectiveness of the RL-based ranks which are learnt in a purely data-driven manner.
</details>
<details>
<summary>摘要</summary>
由于它的严重贡献了40%的全球电力消耗，建筑环境在现代电力网络中获得了越来越多的注意力，以满足需求。在这个意义上，之前的研究主要集中在建筑物之间的能源管理。相比之下，在这篇论文中，我们将关注一组住宅建筑物的总控制，以为电力网络提供支持服务，最终应包括辅助服务。具体来说，我们将展示一个实际的 Pilot 研究，研究使用强化学习（RL）来协调8个住宅建筑物的电力消耗，以同步跟踪目标电力信号。我们的RL方法不需要任何建筑物模型或模拟器，因此实施可行和扩展容易。我们在实际情况下展示了我们提议的RL-基于协调策略的可行性。在4个星期的案例研究中，我们实现了一个层次控制系统，通过RL-基于排名系统来选择需要活动的资产，并使用实时PI控制-基于的电力派发机制来控制选择的资产。我们的结果表明了满意的电力跟踪，以及RL-基于排名系统的学习效果，这些排名系统是通过实际数据驱动学习而学习的。
</details></li>
</ul>
<hr>
<h2 id="MAG-GNN-Reinforcement-Learning-Boosted-Graph-Neural-Network"><a href="#MAG-GNN-Reinforcement-Learning-Boosted-Graph-Neural-Network" class="headerlink" title="MAG-GNN: Reinforcement Learning Boosted Graph Neural Network"></a>MAG-GNN: Reinforcement Learning Boosted Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19142">http://arxiv.org/abs/2310.19142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, Muhan Zhang</li>
<li>for: 提高Graph Neural Networks（GNNs）的结构编码能力，以提高GNNs的表达能力。</li>
<li>methods: 使用搜索算法来选择一小subset of subgraphs，并使用强化学习（RL）Agent来更新subgraph set，以提高GNNs的表达能力。</li>
<li>results: 在多个 datasets 上进行了广泛的实验，显示了 MAG-GNN 可以与现有方法竞争，甚至超过一些subgraph GNNs，同时也可以减少subgraph GNNs 的运行时间。<details>
<summary>Abstract</summary>
While Graph Neural Networks (GNNs) recently became powerful tools in graph learning tasks, considerable efforts have been spent on improving GNNs' structural encoding ability. A particular line of work proposed subgraph GNNs that use subgraph information to improve GNNs' expressivity and achieved great success. However, such effectivity sacrifices the efficiency of GNNs by enumerating all possible subgraphs. In this paper, we analyze the necessity of complete subgraph enumeration and show that a model can achieve a comparable level of expressivity by considering a small subset of the subgraphs. We then formulate the identification of the optimal subset as a combinatorial optimization problem and propose Magnetic Graph Neural Network (MAG-GNN), a reinforcement learning (RL) boosted GNN, to solve the problem. Starting with a candidate subgraph set, MAG-GNN employs an RL agent to iteratively update the subgraphs to locate the most expressive set for prediction. This reduces the exponential complexity of subgraph enumeration to the constant complexity of a subgraph search algorithm while keeping good expressivity. We conduct extensive experiments on many datasets, showing that MAG-GNN achieves competitive performance to state-of-the-art methods and even outperforms many subgraph GNNs. We also demonstrate that MAG-GNN effectively reduces the running time of subgraph GNNs.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 在图学任务中最近成为了强大工具，但是大量的工作被投入到了GNNs的结构编码能力的提高中。一种特定的工作提出了子图GNNs，使用子图信息来提高GNNs的表达能力，并取得了很大的成功。然而，这种表达能力来源于完全对所有可能的子图进行枚举，这会导致GNNs的效率下降。在这篇论文中，我们分析了完全子图枚举的必要性，并证明了一个模型可以通过考虑一小部分的子图来达到相似的表达能力。然后，我们将这个问题转化为一个 combinatorial 优化问题，并提出了磁矢量图神经网络（MAG-GNN）来解决这个问题。MAG-GNN从候选子图集开始，使用了一个强化学习（RL）的代理人来逐步更新子图，以查找最有表达力的集合用于预测。这将枚举子图的枚举复杂度从对数复杂度降低到常数复杂度，保持好的表达能力。我们在许多数据集上进行了广泛的实验，显示MAG-GNN与当前状态的方法竞争，甚至超过了许多子图GNNs。我们还证明了MAG-GNN可以有效减少子图GNNs的运行时间。
</details></li>
</ul>
<hr>
<h2 id="Worst-case-Performance-of-Popular-Approximate-Nearest-Neighbor-Search-Implementations-Guarantees-and-Limitations"><a href="#Worst-case-Performance-of-Popular-Approximate-Nearest-Neighbor-Search-Implementations-Guarantees-and-Limitations" class="headerlink" title="Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations"></a>Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19126">http://arxiv.org/abs/2310.19126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Indyk, Haike Xu</li>
<li>for: 这些纸都是为了研究近似 neighboor搜索算法的最坏情况性能的。</li>
<li>methods: 这些算法包括HNSW、NSG和DiskANN。</li>
<li>results: 我们发现，对于DiskANN的”慢预处理”版本，它可以在数据集中有 bounded “内在”维度时支持常数准确度和多余logarithmic查询时间的近似最近邻搜索查询。对于其他数据结构variant studied，包括DiskANN的”快预处理”版本、HNSW和NSG，我们提出了一家实例集，其中查询过程可以 linear in instance size 的时间内返回”合理”的准确率。例如，对于DiskANN，我们显示了，在实例大小为 n 时，查询过程至少需要 0.1 n 步骤才能查找其中的5个最近邻。<details>
<summary>Abstract</summary>
Graph-based approaches to nearest neighbor search are popular and powerful tools for handling large datasets in practice, but they have limited theoretical guarantees. We study the worst-case performance of recent graph-based approximate nearest neighbor search algorithms, such as HNSW, NSG and DiskANN. For DiskANN, we show that its "slow preprocessing" version provably supports approximate nearest neighbor search query with constant approximation ratio and poly-logarithmic query time, on data sets with bounded "intrinsic" dimension. For the other data structure variants studied, including DiskANN with "fast preprocessing", HNSW and NSG, we present a family of instances on which the empirical query time required to achieve a "reasonable" accuracy is linear in instance size. For example, for DiskANN, we show that the query procedure can take at least $0.1 n$ steps on instances of size $n$ before it encounters any of the $5$ nearest neighbors of the query.
</details>
<details>
<summary>摘要</summary>
Graph-based方法是实际中处理大数据集的强大工具，但它们在理论上有限的保证。我们研究近期的图形基于近似最近邻搜索算法，如HNSW、NSG和DiskANN的最坏情况性能。对于DiskANN，我们表明其"慢预处理"版本可以在数据集中的"内在维度"是bounded时提供常数准确率和多项几何查询时间的近似最近邻搜索查询。对其他数据结构变体，包括DiskANN的"快预处理"版本、HNSW和NSG，我们提供了一家实际上的实例，其查询时间与实例大小线性相关。例如，对于DiskANN，我们表明其查询过程可以在实例大小为$n$时间内至少执行$0.1n$步骤才会遇到5个最近邻居。
</details></li>
</ul>
<hr>
<h2 id="Software-engineering-for-deep-learning-applications-usage-of-SWEng-and-MLops-tools-in-GitHub-repositories"><a href="#Software-engineering-for-deep-learning-applications-usage-of-SWEng-and-MLops-tools-in-GitHub-repositories" class="headerlink" title="Software engineering for deep learning applications: usage of SWEng and MLops tools in GitHub repositories"></a>Software engineering for deep learning applications: usage of SWEng and MLops tools in GitHub repositories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19124">http://arxiv.org/abs/2310.19124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evangelia Panourgia, Theodoros Plessas, Diomidis Spinellis</li>
<li>for: 这篇论文主要关注于深度学习（DL）软件开发中的软件工程（SE）实践，特别是DL软件开发中的工程挑战和资料驱动的非决定性模式。</li>
<li>methods: 本研究使用Python为主要编程语言，采用 précédente MSR 研究的工具使用方法，扫描GitHub上popular的应用DL项目，探索这些项目中的SE工具使用情况。</li>
<li>results: 研究发现，大约70%的GitHub库中包含至少一个SE工具，软件配置管理工具是最多使用的，而维护工具则较少使用。另外，MLOps工具的使用相对较少，只有9个工具在该样本中被使用。TensorBoard是唯一在对�る repository 中使用的MLOps工具。<details>
<summary>Abstract</summary>
The rising popularity of deep learning (DL) methods and techniques has invigorated interest in the topic of SE4DL, the application of software engineering (SE) practices on deep learning software. Despite the novel engineering challenges brought on by the data-driven and non-deterministic paradigm of DL software, little work has been invested into developing AI-targeted SE tools. On the other hand, tools tackling more general engineering issues in DL are actively used and referred to under the umbrella term of ``MLOps tools''. Furthermore, the available literature supports the utility of conventional SE tooling in DL software development. Building upon previous MSR research on tool usage in open-source software works, we identify conventional and MLOps tools adopted in popular applied DL projects that use Python as the main programming language. About 70% of the GitHub repositories mined contained at least one conventional SE tool. Software configuration management tools are the most adopted, while the opposite applies to maintenance tools. Substantially fewer MLOps tools were in use, with only 9 tools out of a sample of 80 used in at least one repository. The majority of them were open-source rather than proprietary. One of these tools, TensorBoard, was found to be adopted in about half of the repositories in our study. Consequently, the use of conventional SE tooling demonstrates its relevance to DL software. Further research is recommended on the adoption of MLOps tooling by open-source projects, focusing on the relevance of particular tool types, the development of required tools, as well as ways to promote the use of already available tools.
</details>
<details>
<summary>摘要</summary>
随着深度学习（DL）方法和技术的普及，关注SE4DL（深度学习软件工程）领域的应用而增加。然而，由于深度学习软件的数据驱动和不确定的理论带来的新的工程挑战，对于AI目标的SE工具仍然受到了少量投入。相反，关于更一般的机器学习（ML）工程问题，如MLOps工具，活跃地使用和引用。此外，现有的文献支持传统的SE工具在深度学习软件开发中的可用性。基于之前的微软研究人员在开源软件项目中工具使用情况，我们识别了传统和MLOps工具在流行的应用深度学习项目中的采用情况。我们发现，大约70%的GitHub存储库包含至少一个传统SE工具。软件配置管理工具是最广泛采用的，而维护工具则相对较少。与此同时，MLOps工具的采用远远少于传统SE工具，只有80个存储库中的9个被使用。大多数这些工具是开源的，而不是商业化的。 tensorBoard 是这些工具中的一个，在我们的研究中被采用的约半数。因此，传统SE工具在深度学习软件开发中的使用表明了它们的重要性。进一步的研究建议在开源项目中采用MLOps工具，特别是关注特定工具类型、开发需要的工具以及如何促进现有工具的使用。
</details></li>
</ul>
<hr>
<h2 id="Proving-Linear-Mode-Connectivity-of-Neural-Networks-via-Optimal-Transport"><a href="#Proving-Linear-Mode-Connectivity-of-Neural-Networks-via-Optimal-Transport" class="headerlink" title="Proving Linear Mode Connectivity of Neural Networks via Optimal Transport"></a>Proving Linear Mode Connectivity of Neural Networks via Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19103">http://arxiv.org/abs/2310.19103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/9aze/ot_lmc">https://github.com/9aze/ot_lmc</a></li>
<li>paper_authors: Damien Ferbach, Baptiste Goujaud, Gauthier Gidel, Aymeric Dieuleveut</li>
<li>for: 本研究探讨了高维非凸优化问题的能量景观，以解释现代深度神经网络架构的效果。</li>
<li>methods: 本文提出了一种理论框架，可以理解现有两次权重训练后得到的两个解的连续性。基于 Wasserstein 距离的整体速度，我们表明了两个够宽的两层神经网络，通过权重训练来连续地连接。</li>
<li>results: 我们提供了一种上下限bounds，可以量化每层神经网络的宽度，以便确保连续性。此外，我们还经验表明了积分权重分布的维度与连续性之间的相关性。<details>
<summary>Abstract</summary>
The energy landscape of high-dimensional non-convex optimization problems is crucial to understanding the effectiveness of modern deep neural network architectures. Recent works have experimentally shown that two different solutions found after two runs of a stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a framework theoretically explaining this empirical observation. Based on convergence rates in Wasserstein distance of empirical measures, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we express upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension of the support of the weight distribution of neurons, which dictates Wasserstein convergence rates is correlated with linear mode connectivity.
</details>
<details>
<summary>摘要</summary>
高维非对称优化问题的能量景观对现代深度神经网络架构的效果是关键。latest studies have shown that two different solutions found after two runs of stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a theoretical framework to explain this empirical observation. Based on the convergence rates of empirical measures in Wasserstein distance, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we provide upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension of the support of the weight distribution of neurons, which dictates Wasserstein convergence rates, is correlated with linear mode connectivity.
</details></li>
</ul>
<hr>
<h2 id="Atom-Low-bit-Quantization-for-Efficient-and-Accurate-LLM-Serving"><a href="#Atom-Low-bit-Quantization-for-Efficient-and-Accurate-LLM-Serving" class="headerlink" title="Atom: Low-bit Quantization for Efficient and Accurate LLM Serving"></a>Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19102">http://arxiv.org/abs/2310.19102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, Baris Kasikci</li>
<li>for: 提高 Large Language Models（LLMs）在内容生成、智能客服和情感分析等应用中的效率，以适应应用场景中的增长需求。</li>
<li>methods: 使用批处理技术批处理多个请求，以提高 GPU 资源的使用效率和throughput。</li>
<li>results: 在服务器上实现高 durchput 提高 ($7.73\times$ 比FP16和 $2.53\times$ 比INT8 归一化)，同时保持同样的响应时间目标，而不会增加精度损失。<details>
<summary>Abstract</summary>
The growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for LLM service providers. To efficiently use GPU resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further speed up batching, LLM quantization techniques reduce memory consumption and increase computing capacity. However, prevalent quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.   To maximize LLMs' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss. Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization. It attains high accuracy by applying a novel mixed-precision and fine-grained quantization process. We evaluate Atom on 4-bit weight-activation quantization setups in the serving context. Atom improves end-to-end throughput by up to $7.73\times$ compared to the FP16 and by $2.53\times$ compared to INT8 quantization, while maintaining the same latency target.
</details>
<details>
<summary>摘要</summary>
受大语言模型（LLM）应用的内容生成、智能客服和情感分析等领域的需求不断增长， LLM 服务提供商面临着巨大的挑战。为了高效利用 GPU 资源并提高通过put，批处多个请求已成为了流行的方法；而为了进一步加速批处， LLM 量化技术可以降低内存占用量和提高计算能力。然而，现有的量化方案（如 8 位 weight-activation 量化）无法完全利用现代 GPU 的能力，导致性能下降。为了最大化 LLM 的服务通过put，我们介绍 Atom，一种低位量化方法，可以 achieve high throughput improvements with negligible accuracy loss。Atom 使用低位操作和减少内存占用量，以提高服务通过put。它通过应用一种新的混合精度和细致的量化过程，保持高度的准确性。我们在 4 位 weight-activation 量化设置下测试 Atom。Atom 可以提高终端通过put的吞吐量，比FP16和 INT8 量化的吞吐量高出 $7.73\times$，而且保持同样的响应时间目标。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Towards-an-Expanded-Toolkit-for-ML-Supported-Decision-Making-in-the-Public-Sector"><a href="#Bridging-the-Gap-Towards-an-Expanded-Toolkit-for-ML-Supported-Decision-Making-in-the-Public-Sector" class="headerlink" title="Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector"></a>Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19091">http://arxiv.org/abs/2310.19091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Unai Fischer Abaigar, Christoph Kern, Noam Barda, Frauke Kreuter</li>
<li>For: This paper aims to bridge the gap between machine learning (ML) and public sector decision-making by addressing key technical challenges that arise when aligning intricate policy objectives with the precise formalization requirements of ML models.* Methods: The paper concentrates on pivotal points of the ML pipeline that connect the model to its operational environment, including the significance of representative training data and the importance of a model setup that facilitates effective decision-making. The paper also links these challenges with emerging methodological advancements, such as causal ML, domain adaptation, uncertainty quantification, and multi-objective optimization.* Results: The paper provides a comprehensive overview of the challenges that arise when using ML in the public sector, and highlights the importance of addressing these challenges in order to harmonize ML and public sector objectives. The paper also illustrates the path forward for addressing these challenges, including the use of emerging methodological advancements.<details>
<summary>Abstract</summary>
Machine Learning (ML) systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health. While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, they still face the challenge of aligning intricate and nuanced policy objectives with the precise formalization requirements necessitated by ML models. In this paper, we aim to bridge the gap between ML and public sector decision-making by presenting a comprehensive overview of key technical challenges where disjunctions between policy goals and ML models commonly arise. We concentrate on pivotal points of the ML pipeline that connect the model to its operational environment, delving into the significance of representative training data and highlighting the importance of a model setup that facilitates effective decision-making. Additionally, we link these challenges with emerging methodological advancements, encompassing causal ML, domain adaptation, uncertainty quantification, and multi-objective optimization, illustrating the path forward for harmonizing ML and public sector objectives.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>训练数据的选择和调整：ML 模型的精度和可靠性受到训练数据的影响，但是公共部门的决策过程中的训练数据可能不够完整或不够有代表性。2. 模型设置的构成和调整：为了使 ML 模型能够实际地支持公共部门的决策过程，需要适当地设置和调整模型的参数和架构。3. 适用于公共部门的 ML 技术发展：包括 causal ML、领域适应、uncertainty quantification 和多目标优化在内的新技术可以帮助解决 ML 和公共部门之间的匹配问题。本文通过聚焦 ML pipeline 中的关键点子，探讨 ML 模型如何与公共部门的决策过程进行匹配，并提出了一些实际的方法来解决这些挑战。这些方法包括：1. 使用更多的代表性丰富的训练数据来优化 ML 模型的性能。2. 适当地设置和调整 ML 模型的参数和架构，以便更好地支持公共部门的决策过程。3. 采用新的 ML 技术，例如 causal ML、领域适应、uncertainty quantification 和多目标优化，来解决 ML 和公共部门之间的匹配问题。</details></li>
</ol>
<hr>
<h2 id="Efficient-Cluster-Selection-for-Personalized-Federated-Learning-A-Multi-Armed-Bandit-Approach"><a href="#Efficient-Cluster-Selection-for-Personalized-Federated-Learning-A-Multi-Armed-Bandit-Approach" class="headerlink" title="Efficient Cluster Selection for Personalized Federated Learning: A Multi-Armed Bandit Approach"></a>Efficient Cluster Selection for Personalized Federated Learning: A Multi-Armed Bandit Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19069">http://arxiv.org/abs/2310.19069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhou Ni, Morteza Hashemi</li>
<li>for: 这篇论文目的是为了解决在人工智能学习中的联邦学习网络中的问题，特别是在变化很大的数据分布和设备能力下。</li>
<li>methods: 这篇论文使用了一种名为“动态Upper Confidence Bound”的算法，它是基于多臂枪（MAB）的方法，用于在联邦学习网络中聚合用户。这个算法可以将新用户的数据分布与最佳的聚合群相匹配。</li>
<li>results: 这篇论文的实验结果显示，在不同的数据分布和设备能力下，这个算法可以有效地处理变化很大的联邦学习enario。<details>
<summary>Abstract</summary>
Federated learning (FL) offers a decentralized training approach for machine learning models, prioritizing data privacy. However, the inherent heterogeneity in FL networks, arising from variations in data distribution, size, and device capabilities, poses challenges in user federation. Recognizing this, Personalized Federated Learning (PFL) emphasizes tailoring learning processes to individual data profiles. In this paper, we address the complexity of clustering users in PFL, especially in dynamic networks, by introducing a dynamic Upper Confidence Bound (dUCB) algorithm inspired by the multi-armed bandit (MAB) approach. The dUCB algorithm ensures that new users can effectively find the best cluster for their data distribution by balancing exploration and exploitation. The performance of our algorithm is evaluated in various cases, showing its effectiveness in handling dynamic federated learning scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sketching-Algorithms-for-Sparse-Dictionary-Learning-PTAS-and-Turnstile-Streaming"><a href="#Sketching-Algorithms-for-Sparse-Dictionary-Learning-PTAS-and-Turnstile-Streaming" class="headerlink" title="Sketching Algorithms for Sparse Dictionary Learning: PTAS and Turnstile Streaming"></a>Sketching Algorithms for Sparse Dictionary Learning: PTAS and Turnstile Streaming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19068">http://arxiv.org/abs/2310.19068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory Dexter, Petros Drineas, David P. Woodruff, Taisuke Yasuda</li>
<li>for: 这 paper 的目的是扩展 sketching 算法的应用范围，包括稀疏字储学习和 EUCLIDEAN $k$-means 归类问题。</li>
<li>methods: 这 paper 使用了新的技术来推广 sketching 算法的应用范围，包括一种新的 PTAS 方法和新的上界和下界。</li>
<li>results: 这 paper 得到了一些新的结果，包括一种新的 PTAS 方法和新的上界和下界，以及一些关于 dictionary learning 和 $k$-means 归类问题的研究。<details>
<summary>Abstract</summary>
Sketching algorithms have recently proven to be a powerful approach both for designing low-space streaming algorithms as well as fast polynomial time approximation schemes (PTAS). In this work, we develop new techniques to extend the applicability of sketching-based approaches to the sparse dictionary learning and the Euclidean $k$-means clustering problems. In particular, we initiate the study of the challenging setting where the dictionary/clustering assignment for each of the $n$ input points must be output, which has surprisingly received little attention in prior work. On the fast algorithms front, we obtain a new approach for designing PTAS's for the $k$-means clustering problem, which generalizes to the first PTAS for the sparse dictionary learning problem. On the streaming algorithms front, we obtain new upper bounds and lower bounds for dictionary learning and $k$-means clustering. In particular, given a design matrix $\mathbf A\in\mathbb R^{n\times d}$ in a turnstile stream, we show an $\tilde O(nr/\epsilon^2 + dk/\epsilon)$ space upper bound for $r$-sparse dictionary learning of size $k$, an $\tilde O(n/\epsilon^2 + dk/\epsilon)$ space upper bound for $k$-means clustering, as well as an $\tilde O(n)$ space upper bound for $k$-means clustering on random order row insertion streams with a natural "bounded sensitivity" assumption. On the lower bounds side, we obtain a general $\tilde\Omega(n/\epsilon + dk/\epsilon)$ lower bound for $k$-means clustering, as well as an $\tilde\Omega(n/\epsilon^2)$ lower bound for algorithms which can estimate the cost of a single fixed set of candidate centers.
</details>
<details>
<summary>摘要</summary>
algorithm 已经证明是一种强大的方法，不仅用于设计具有低空间流处理器的算法，也用于快速的多项时间算法（PTAS）。在这个工作中，我们开发了新的技术，以扩展画 sketching 方法的应用范围至简短字典学习和欧几何 $k$-means 聚类问题。具体来说，我们开始研究具有复杂的设定，其中每个输入点的字典/聚类分配必须被输出，这个问题在先前的工作中很少获得关注。在快速算法方面，我们取得了一新的方法，用于设计 PTAS 的 $k$-means 聚类问题，这个方法可扩展到简短字典学习问题的首次 PTAS。在流处理算法方面，我们取得了新的上界和下界，用于字典学习和 $k$-means 聚类问题。具体来说，在turnstile流中，我们显示了一个 $\tilde O(nr/\epsilon^2 + dk/\epsilon)$ 的空间上界，用于 $r$-简字典学习的大小为 $k$，以及一个 $\tilde O(n/\epsilon^2 + dk/\epsilon)$ 的空间上界，用于 $k$-means 聚类问题。此外，我们还取得了一个 $\tilde O(n)$ 的空间上界，用于 $k$-means 聚类在随机排序推入流中。在下界方面，我们取得了一个通用的 $\tilde\Omega(n/\epsilon + dk/\epsilon)$ 下界，用于 $k$-means 聚类问题，以及一个 $\tilde\Omega(n/\epsilon^2)$ 下界，用于可以估计单一集合中心的成本的算法。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-LLP-Methods-Challenges-and-Approaches"><a href="#Evaluating-LLP-Methods-Challenges-and-Approaches" class="headerlink" title="Evaluating LLP Methods: Challenges and Approaches"></a>Evaluating LLP Methods: Challenges and Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19065">http://arxiv.org/abs/2310.19065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gaabrielfranco/llp-variants-datasets-benchmarks">https://github.com/gaabrielfranco/llp-variants-datasets-benchmarks</a></li>
<li>paper_authors: Gabriel Franco, Giovanni Comarela, Mark Crovella</li>
<li>for: 本研究是为了解决Label Proportions（LLP）问题，这是一个机器学习问题，有很多实际应用。</li>
<li>methods: 本研究使用了生成 variant-specific 数据集，以捕捉不同的 dependence structure 和 bag 特性。此外，还使用了一种新的模型选择方法，以适应 LLQ 问题的特殊性。</li>
<li>results: 研究发现，选择最佳算法需要考虑不同的 LLP 变种和模型选择方法。通过对一些常见 LLQ 算法进行了广泛的比较， demonstrate 了需要我们提出的方法。<details>
<summary>Abstract</summary>
Learning from Label Proportions (LLP) is an established machine learning problem with numerous real-world applications. In this setting, data items are grouped into bags, and the goal is to learn individual item labels, knowing only the features of the data and the proportions of labels in each bag. Although LLP is a well-established problem, it has several unusual aspects that create challenges for benchmarking learning methods. Fundamental complications arise because of the existence of different LLP variants, i.e., dependence structures that can exist between items, labels, and bags. Accordingly, the first algorithmic challenge is the generation of variant-specific datasets capturing the diversity of dependence structures and bag characteristics. The second methodological challenge is model selection, i.e., hyperparameter tuning; due to the nature of LLP, model selection cannot easily use the standard machine learning paradigm. The final benchmarking challenge consists of properly evaluating LLP solution methods across various LLP variants. We note that there is very little consideration of these issues in prior work, and there are no general solutions for these challenges proposed to date. To address these challenges, we develop methods capable of generating LLP datasets meeting the requirements of different variants. We use these methods to generate a collection of datasets encompassing the spectrum of LLP problem characteristics, which can be used in future evaluation studies. Additionally, we develop guidelines for benchmarking LLP algorithms, including the model selection and evaluation steps. Finally, we illustrate the new methods and guidelines by performing an extensive benchmark of a set of well-known LLP algorithms. We show that choosing the best algorithm depends critically on the LLP variant and model selection method, demonstrating the need for our proposed approach.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we develop methods capable of generating LLP datasets meeting the requirements of different variants. We use these methods to generate a collection of datasets encompassing the spectrum of LLP problem characteristics, which can be used in future evaluation studies. Additionally, we develop guidelines for benchmarking LLP algorithms, including the model selection and evaluation steps. Finally, we illustrate the new methods and guidelines by performing an extensive benchmark of a set of well-known LLP algorithms. We show that choosing the best algorithm depends critically on the LLP variant and model selection method, demonstrating the need for our proposed approach.翻译结果：LLP（学习从标签分量）是一个已经有长期应用的机器学习问题，具有许多实际应用场景。在这个设定中，数据项目被分组为袋子，目标是从数据特征和标签分量中学习各自的项目标签。虽然LLP是一个已知的问题，但它有一些不寻常的特点，导致评估学习方法的挑战。主要的挑战包括：1. 生成 variant-specific 数据集，捕捉不同的依赖结构和袋子特征的多样性。2. 因为 LLP 的特点，选择最佳模型不能使用标准机器学习范文。3. 评估 LLP 解决方案的多样性，以确保它们在不同的 LLP 变体中表现良好。为了解决这些挑战，我们开发了生成 LLP 数据集的方法，以满足不同变体的需求。我们使用这些方法生成了一系列包含 LLP 问题特征谱的数据集，可以在未来的评估研究中使用。此外，我们还提供了评估 LLP 算法的指南，包括模型选择和评估步骤。最后，我们使用新方法和指南对一组知名 LLP 算法进行了广泛的比较。我们发现，选择最佳算法取决于 LLP 变体和模型选择方法，这说明了我们的提出的方法的需要。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-the-Learnability-of-Apple-Tasting"><a href="#Revisiting-the-Learnability-of-Apple-Tasting" class="headerlink" title="Revisiting the Learnability of Apple Tasting"></a>Revisiting the Learnability of Apple Tasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19064">http://arxiv.org/abs/2310.19064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinod Raman, Unique Subedi, Ananth Raman, Ambuj Tewari</li>
<li>for: 研究在apple tasting反馈下的在线分类问题。</li>
<li>methods: 使用 combinatorial perspective 研究在线学习可能性，并提出了一个新的参数Effective width，用于量化在可 realizable 设定下的最差预期错误数。</li>
<li>results: 在 realizable 设定下，showed that the expected number of mistakes for any learner under apple tasting feedback can only be $\Theta(1), \Theta(\sqrt{T})$, or $\Theta(T)$。<details>
<summary>Abstract</summary>
In online binary classification under \textit{apple tasting} feedback, the learner only observes the true label if it predicts "1". First studied by \cite{helmbold2000apple}, we revisit this classical partial-feedback setting and study online learnability from a combinatorial perspective. We show that the Littlestone dimension continues to prove a tight quantitative characterization of apple tasting in the agnostic setting, closing an open question posed by \cite{helmbold2000apple}. In addition, we give a new combinatorial parameter, called the Effective width, that tightly quantifies the minimax expected mistakes in the realizable setting. As a corollary, we use the Effective width to establish a \textit{trichotomy} of the minimax expected number of mistakes in the realizable setting. In particular, we show that in the realizable setting, the expected number of mistakes for any learner under apple tasting feedback can only be $\Theta(1), \Theta(\sqrt{T})$, or $\Theta(T)$.
</details>
<details>
<summary>摘要</summary>
在在线二分类学习中，学习者只会看到真实标签，如果预测结果为1。这个问题最早由Helmbold等人（2000）研究，我们现在从 combinatorial 角度重新研究这个古典的partial-feedback 设定，并证明 Littlestone 维度仍然是agnostic 设定中的一个紧张量量化 caracterization。此外，我们还提出了一个新的 combinatorial 参数，called Effective width，它紧密地量化了可行情况下的最差预期错误。为此，我们使用 Effective width 证明了可行情况下的最差预期错误数可以只是 $\Theta(1), \Theta(\sqrt{T})$ 或 $\Theta(T)$。
</details></li>
</ul>
<hr>
<h2 id="Feature-Aggregation-in-Joint-Sound-Classification-and-Localization-Neural-Networks"><a href="#Feature-Aggregation-in-Joint-Sound-Classification-and-Localization-Neural-Networks" class="headerlink" title="Feature Aggregation in Joint Sound Classification and Localization Neural Networks"></a>Feature Aggregation in Joint Sound Classification and Localization Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19063">http://arxiv.org/abs/2310.19063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brendan Healy, Patrick McNamee, Zahra Nili Ahmadabadi</li>
<li>for: 本研究探讨了深度学习技术在共同声音信号分类和地点化网络中的应用。现有状态的声音源地点化深度学习网络缺乏特征聚合在其架构中。特征聚合可以提高模型性能，因为它使得不同特征尺度上的信息可以被集成，从而提高特征Robustness和不变性。这特别重要在SSL网络中，因为它们必须在直接和间接声音信号之间进行区分。为解决这个漏洞，我们将计算机视觉网络中的特征聚合技术应用到声音检测网络中。</li>
<li>methods: 我们采用了计算机视觉网络中的特征聚合技术，包括Path Aggregation Network (PANet)、Weighted Bi-directional Feature Pyramid Network (BiFPN)和Scale Encoding Network (SEN)等。这些技术被integrated into a SSL control architecture，并被评估使用两种声音分类和两种方向射 regression 的指标。PANet和BiFPN是计算机视觉模型中已知的聚合器，而我们提议的SEN是更加压缩的聚合器。</li>
<li>results: 结果表明，包含特征聚合的模型在声音分类和地点化方面的性能都高于控制模型，即Sound Event Localization and Detection network (SELDnet)。特征聚合技术提高了声音检测神经网络的性能，特别是在方向射 regression 方面。<details>
<summary>Abstract</summary>
This study addresses the application of deep learning techniques in joint sound signal classification and localization networks. Current state-of-the-art sound source localization deep learning networks lack feature aggregation within their architecture. Feature aggregation enhances model performance by enabling the consolidation of information from different feature scales, thereby improving feature robustness and invariance. This is particularly important in SSL networks, which must differentiate direct and indirect acoustic signals. To address this gap, we adapt feature aggregation techniques from computer vision neural networks to signal detection neural networks. Additionally, we propose the Scale Encoding Network (SEN) for feature aggregation to encode features from various scales, compressing the network for more computationally efficient aggregation. To evaluate the efficacy of feature aggregation in SSL networks, we integrated the following computer vision feature aggregation sub-architectures into a SSL control architecture: Path Aggregation Network (PANet), Weighted Bi-directional Feature Pyramid Network (BiFPN), and SEN. These sub-architectures were evaluated using two metrics for signal classification and two metrics for direction-of-arrival regression. PANet and BiFPN are established aggregators in computer vision models, while the proposed SEN is a more compact aggregator. The results suggest that models incorporating feature aggregations outperformed the control model, the Sound Event Localization and Detection network (SELDnet), in both sound signal classification and localization. The feature aggregation techniques enhance the performance of sound detection neural networks, particularly in direction-of-arrival regression.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Escaping-Saddle-Points-in-Heterogeneous-Federated-Learning-via-Distributed-SGD-with-Communication-Compression"><a href="#Escaping-Saddle-Points-in-Heterogeneous-Federated-Learning-via-Distributed-SGD-with-Communication-Compression" class="headerlink" title="Escaping Saddle Points in Heterogeneous Federated Learning via Distributed SGD with Communication Compression"></a>Escaping Saddle Points in Heterogeneous Federated Learning via Distributed SGD with Communication Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19059">http://arxiv.org/abs/2310.19059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijin Chen, Zhize Li, Yuejie Chi</li>
<li>For: 提高 federated learning（FL）中communication efficiency和学习精度的问题。* Methods: 提出了一种新的error-feedback scheme，实现了在不同客户端数据异ogeneous的情况下，通过压缩信息进行分布式SGD算法的实现。* Results: 证明了Power-EF算法可以在不同客户端数据异ogeneous情况下，逃脱平均点，并且在第二阶段 convergence 中，展现出线性增长。<details>
<summary>Abstract</summary>
We consider the problem of finding second-order stationary points of heterogeneous federated learning (FL). Previous works in FL mostly focus on first-order convergence guarantees, which do not rule out the scenario of unstable saddle points. Meanwhile, it is a key bottleneck of FL to achieve communication efficiency without compensating the learning accuracy, especially when local data are highly heterogeneous across different clients. Given this, we propose a novel algorithm Power-EF that only communicates compressed information via a novel error-feedback scheme. To our knowledge, Power-EF is the first distributed and compressed SGD algorithm that provably escapes saddle points in heterogeneous FL without any data homogeneity assumptions. In particular, Power-EF improves to second-order stationary points after visiting first-order (possibly saddle) points, using additional gradient queries and communication rounds only of almost the same order required by first-order convergence, and the convergence rate exhibits a linear speedup in terms of the number of workers. Our theory improves/recovers previous results, while extending to much more tolerant settings on the local data. Numerical experiments are provided to complement the theory.
</details>
<details>
<summary>摘要</summary>
我们考虑到寻找非常复杂的联邦学习（FL）中的第二阶站点问题。前一些FL工作主要集中在第一阶均衡保证，这不能排除不稳定的阶均点的情况。另一方面，在FL中实现通信效率不损学习精度的挑战，尤其是当地方数据具有很高的不同客户端的多样性时。为了解决这个问题，我们提出了一个新的算法Power-EF，它仅在一个新的错误反馈方案下进行压缩通信。我们知道Power-EF是首个分布式压缩SGD算法，可以在不同客户端的数据多样性下，避免阶均点而实现第二阶站点，并且在额外的梯度询问和通信轮次上进行几乎相同的复杂度。我们的理论提高了/恢复了先前的结果，同时扩展到许多更允许的本地数据设置。实验数据来补充理论。
</details></li>
</ul>
<hr>
<h2 id="Object-centric-architectures-enable-efficient-causal-representation-learning"><a href="#Object-centric-architectures-enable-efficient-causal-representation-learning" class="headerlink" title="Object-centric architectures enable efficient causal representation learning"></a>Object-centric architectures enable efficient causal representation learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19054">http://arxiv.org/abs/2310.19054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Mansouri, Jason Hartford, Yan Zhang, Yoshua Bengio</li>
<li>for: 这篇论文旨在探讨如何在多个物体的观察数据上实现 causal representation learning，以实现对每个物体的属性的分离。</li>
<li>methods: 该论文使用了对象中心学习和 causal representation learning 的最新发展，通过修改 Slot Attention 架构，使用稀有的干扰来强制实现对每个物体的属性的分离。</li>
<li>results: 该论文在一系列简单的图像基于的分离实验中成功地分离了一组物体的属性，并且需要更少的干扰than comparable approach 。<details>
<summary>Abstract</summary>
Causal representation learning has showed a variety of settings in which we can disentangle latent variables with identifiability guarantees (up to some reasonable equivalence class). Common to all of these approaches is the assumption that (1) the latent variables are represented as $d$-dimensional vectors, and (2) that the observations are the output of some injective generative function of these latent variables. While these assumptions appear benign, we show that when the observations are of multiple objects, the generative function is no longer injective and disentanglement fails in practice. We can address this failure by combining recent developments in object-centric learning and causal representation learning. By modifying the Slot Attention architecture arXiv:2006.15055, we develop an object-centric architecture that leverages weak supervision from sparse perturbations to disentangle each object's properties. This approach is more data-efficient in the sense that it requires significantly fewer perturbations than a comparable approach that encodes to a Euclidean space and we show that this approach successfully disentangles the properties of a set of objects in a series of simple image-based disentanglement experiments.
</details>
<details>
<summary>摘要</summary>
causal representation learning 在多种设置中展示了可以分离干扰变量的可靠性保证 ( hasta certain extent 的等价类). 这些方法假设：1) 干扰变量是 $d$-维 вектор表示; 2) 观察是这些干扰变量的生成函数的输出。 although these assumptions seem innocuous, we show that when the observations are of multiple objects, the generative function is no longer injective and disentanglement fails in practice. 我们可以通过结合近期的对象中心学习和 causal representation learning 来解决这种失败。 我们修改了 arXiv:2006.15055 中的槽注意架构，以便在 sparse perturbations 的 weak supervision 下，为每个对象分离其特性。 这种方法比一种在 Euclidean space 中编码并且需要更少的扰动而言，我们展示了这种方法可以成功地分离一系列的图像基于的对象分离实验中的对象特性。
</details></li>
</ul>
<hr>
<h2 id="Datasets-and-Benchmarks-for-Nanophotonic-Structure-and-Parametric-Design-Simulations"><a href="#Datasets-and-Benchmarks-for-Nanophotonic-Structure-and-Parametric-Design-Simulations" class="headerlink" title="Datasets and Benchmarks for Nanophotonic Structure and Parametric Design Simulations"></a>Datasets and Benchmarks for Nanophotonic Structure and Parametric Design Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19053">http://arxiv.org/abs/2310.19053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jungtaekkim/nanophotonic-structures">https://github.com/jungtaekkim/nanophotonic-structures</a></li>
<li>paper_authors: Jungtaek Kim, Mingxuan Li, Oliver Hinder, Paul W. Leu</li>
<li>for: 这个论文主要针对的应用是设计和理解奈米光学结构，以实现太阳能电池、反射层、电磁干扰屏蔽、光滤波器和LED等多种应用。</li>
<li>methods: 这篇论文使用了电动力学模拟来模拟电磁场的时间变化和光学性质。同时，它还提出了一些参数结构设计问题的评价框架和标准。</li>
<li>results: 研究人员通过对不同Grid大小的电动力学模拟进行比较，发现可以通过灵活地选择评价精度来提高结构设计。此外，他们还提出了一些参数结构设计问题的解决方案。<details>
<summary>Abstract</summary>
Nanophotonic structures have versatile applications including solar cells, anti-reflective coatings, electromagnetic interference shielding, optical filters, and light emitting diodes. To design and understand these nanophotonic structures, electrodynamic simulations are essential. These simulations enable us to model electromagnetic fields over time and calculate optical properties. In this work, we introduce frameworks and benchmarks to evaluate nanophotonic structures in the context of parametric structure design problems. The benchmarks are instrumental in assessing the performance of optimization algorithms and identifying an optimal structure based on target optical properties. Moreover, we explore the impact of varying grid sizes in electrodynamic simulations, shedding light on how evaluation fidelity can be strategically leveraged in enhancing structure designs.
</details>
<details>
<summary>摘要</summary>
几何光子结构具有多方面应用，包括太阳能电池、反射层、电磁干扰隔绝、光滤波器和发光二极管。为设计和理解这些几何光子结构，电动力学模拟是必备的。这些模拟可以模拟电磁场过时的变化，并计算光学性能。在这个工作中，我们介绍了框架和参考标准，用于评估几何光子结构在参数结构设计问题中的性能。这些参考标准可以评估优化算法的性能，并帮助选择基于目标光学性能的最佳结构。此外，我们还探讨了在电动力学模拟中不同格子大小的影响，照明了如何积极地利用评估实价来提升结构设计。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Permutation-Tests-Applications-to-Kernel-Methods"><a href="#Differentially-Private-Permutation-Tests-Applications-to-Kernel-Methods" class="headerlink" title="Differentially Private Permutation Tests: Applications to Kernel Methods"></a>Differentially Private Permutation Tests: Applications to Kernel Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19043">http://arxiv.org/abs/2310.19043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antoninschrab/dpkernel-paper">https://github.com/antoninschrab/dpkernel-paper</a></li>
<li>paper_authors: Ilmun Kim, Antonin Schrab</li>
<li>for: 隐私保护的敏感数据分析</li>
<li>methods: 使用差异性保护的排序测试（differentially private permutation tests），extend classical non-private permutation tests to private settings，maintain both finite-sample validity and differential privacy</li>
<li>results: 提出了 differentially private kernel tests（dpMMD和dpHSIC），可以在不同的隐私环境下实现最佳的能力，实现了在Synthetic和实际场景下的竞争力比较Here’s the breakdown of each point:</li>
<li>for: The paper is written for the purpose of privacy-preserving data analysis, specifically in the context of hypothesis testing.</li>
<li>methods: The paper introduces differentially private permutation tests as a way to extend classical non-private permutation tests to private settings while maintaining both finite-sample validity and differential privacy.</li>
<li>results: The paper proposes two differentially private kernel tests (dpMMD and dpHSIC) that can achieve optimal power under different privacy regimes, and demonstrates their competitive power through empirical evaluations on synthetic and real-world data.<details>
<summary>Abstract</summary>
Recent years have witnessed growing concerns about the privacy of sensitive data. In response to these concerns, differential privacy has emerged as a rigorous framework for privacy protection, gaining widespread recognition in both academic and industrial circles. While substantial progress has been made in private data analysis, existing methods often suffer from impracticality or a significant loss of statistical efficiency. This paper aims to alleviate these concerns in the context of hypothesis testing by introducing differentially private permutation tests. The proposed framework extends classical non-private permutation tests to private settings, maintaining both finite-sample validity and differential privacy in a rigorous manner. The power of the proposed test depends on the choice of a test statistic, and we establish general conditions for consistency and non-asymptotic uniform power. To demonstrate the utility and practicality of our framework, we focus on reproducing kernel-based test statistics and introduce differentially private kernel tests for two-sample and independence testing: dpMMD and dpHSIC. The proposed kernel tests are straightforward to implement, applicable to various types of data, and attain minimax optimal power across different privacy regimes. Our empirical evaluations further highlight their competitive power under various synthetic and real-world scenarios, emphasizing their practical value. The code is publicly available to facilitate the implementation of our framework.
</details>
<details>
<summary>摘要</summary>
近年来，有越来越多的关注关于敏感数据的隐私问题。为回应这些问题，差分隐私在学术和工业圈中得到了广泛的认可，成为隐私保护的严格框架。虽然在私人数据分析方面已经做出了大量的进展，但现有方法经常受到实用性或统计效率的限制。这篇论文的目标是在假设测试中解决这些问题，通过引入差分隐私排序测试来保持rigorous的隐私和统计有效性。我们的框架将经典的非私人排序测试扩展到私人设置下，并保持了finite-sample的有效性和差分隐私。我们的测试能力取决于选择的测试统计量，我们确定了一般的一致性和非假设统计上的强大能力。为了证明我们的框架的实用性和实用性，我们将重点关注使用归一化测试统计量，并引入差分隐私kernel测试：dpMMD和dpHSIC。这些差分隐私kernel测试是易于实现，适用于各种数据类型，并在不同的隐私环境下具有最佳的可比性。我们的实验证明了它们在不同的 sintetic 和实际场景下具有竞争力，强调它们的实际价值。代码publicly available，以便实现我们的框架。
</details></li>
</ul>
<hr>
<h2 id="On-Linear-Separation-Capacity-of-Self-Supervised-Representation-Learning"><a href="#On-Linear-Separation-Capacity-of-Self-Supervised-Representation-Learning" class="headerlink" title="On Linear Separation Capacity of Self-Supervised Representation Learning"></a>On Linear Separation Capacity of Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19041">http://arxiv.org/abs/2310.19041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shulei Wang</li>
<li>for: 本研究旨在探讨数据增强学习在多材料模型下的表达学习，以及这种表达学习是如何提高线性分类器的表达能力的。</li>
<li>methods: 本研究使用了自助学习和数据增强学习方法，并对这些方法的表达学习效果进行了分析。</li>
<li>results: 研究发现，数据增强学习可以提高线性分类器的表达能力，并且可以在多材料模型下 Linearly separate manifolds。此外，研究还发现，自助学习可以在小样本大量数据下提高线性分类器的表达能力。<details>
<summary>Abstract</summary>
Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. Our theoretical analysis further underscores that the performance of downstream linear classifiers primarily hinges on the linear separability of data representations rather than the size of the labeled data set, reaffirming the viability of constructing efficient classifiers with limited labeled data amid an expansive unlabeled data set.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-Learning-for-the-identification-of-phase-transitions-in-interacting-agent-based-systems"><a href="#Machine-Learning-for-the-identification-of-phase-transitions-in-interacting-agent-based-systems" class="headerlink" title="Machine Learning for the identification of phase-transitions in interacting agent-based systems"></a>Machine Learning for the identification of phase-transitions in interacting agent-based systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19039">http://arxiv.org/abs/2310.19039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolaos Evangelou, Dimitrios G. Giovanis, George A. Kevrekidis, Grigorios A. Pavliotis, Ioannis G. Kevrekidis</li>
<li>for: 这篇论文的目的是提出一种数据驱动的框架，用于描述agent-based模型（ABM）中的相态转变。</li>
<li>methods: 该论文使用了Diffusion Maps算法来Identify一个简洁的数据驱动变量，并使用深度学习框架来获得一个参数化的坐标系，以便在这些坐标系中identify一个参数dependent的涨落函数。</li>
<li>results: 该论文通过使用这种数据驱动的方法，成功地construct了一个相态转变的 диаграм。<details>
<summary>Abstract</summary>
Deriving closed-form, analytical expressions for reduced-order models, and judiciously choosing the closures leading to them, has long been the strategy of choice for studying phase- and noise-induced transitions for agent-based models (ABMs). In this paper, we propose a data-driven framework that pinpoints phase transitions for an ABM in its mean-field limit, using a smaller number of variables than traditional closed-form models. To this end, we use the manifold learning algorithm Diffusion Maps to identify a parsimonious set of data-driven latent variables, and show that they are in one-to-one correspondence with the expected theoretical order parameter of the ABM. We then utilize a deep learning framework to obtain a conformal reparametrization of the data-driven coordinates that facilitates, in our example, the identification of a single parameter-dependent ODE in these coordinates. We identify this ODE through a residual neural network inspired by a numerical integration scheme (forward Euler). We then use the identified ODE -- enabled through an odd symmetry transformation -- to construct the bifurcation diagram exhibiting the phase transition.
</details>
<details>
<summary>摘要</summary>
使用闭式表达式和选择合适的闭式来研究基于代理模型（ABM）的相对阶段和噪声引起的转变，已经是长期的策略。在这篇文章中，我们提出了一个数据驱动的框架，用于在ABM的含义场限制下标出相对阶段的转变点。为此，我们使用扩散地图算法来确定一个简洁的数据驱动的秘密变量，并证明它们与ABM的预期的理论参量之间存在一一对应关系。然后，我们使用深度学习框架来获得一个符号映射，以便在这些坐标系中进行数据驱动的协调。通过这种方式，我们可以在这些坐标系中提取出一个参数依赖的径谱方程。我们使用这个径谱方程，通过一种奇偶变换，构建了相对阶段的分布图。
</details></li>
</ul>
<hr>
<h2 id="Does-Invariant-Graph-Learning-via-Environment-Augmentation-Learn-Invariance"><a href="#Does-Invariant-Graph-Learning-via-Environment-Augmentation-Learn-Invariance" class="headerlink" title="Does Invariant Graph Learning via Environment Augmentation Learn Invariance?"></a>Does Invariant Graph Learning via Environment Augmentation Learn Invariance?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19035">http://arxiv.org/abs/2310.19035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lfhase/gala">https://github.com/lfhase/gala</a></li>
<li>paper_authors: Yongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, James Cheng</li>
<li>For: 本 paper 的目的是学习图像上的不变性，以便在图像上进行对外部数据进行泛化。* Methods: 本 paper 使用环境扩充来提高图像的不变性学习，但是这些环境扩充的有用性从未被证明。因此，本 paper 提出了一些最小假设，包括变化 suficiency 和变化 consistency，以便可能地学习图像的不变性。* Results: 本 paper 提出了一个新的框架 Graph invAriant Learning Assistant (GALA)，该框架包括一个助手模型，该模型需要对图像环境变化或分布变化敏感。助手模型的proxy预测可以判断图像中的杂乱子图的变化。根据这些 proxy 预测，提取图像中最大不变性子图可以唯一地标识图像的不变性子图，并且在成功的 OOD 泛化下保证不变性。经过对多个 dataset 的广泛实验，包括 DrugOOD 等，确认了 GALA 的有效性。<details>
<summary>Abstract</summary>
Invariant graph representation learning aims to learn the invariance among data from different environments for out-of-distribution generalization on graphs. As the graph environment partitions are usually expensive to obtain, augmenting the environment information has become the de facto approach. However, the usefulness of the augmented environment information has never been verified. In this work, we find that it is fundamentally impossible to learn invariant graph representations via environment augmentation without additional assumptions. Therefore, we develop a set of minimal assumptions, including variation sufficiency and variation consistency, for feasible invariant graph learning. We then propose a new framework Graph invAriant Learning Assistant (GALA). GALA incorporates an assistant model that needs to be sensitive to graph environment changes or distribution shifts. The correctness of the proxy predictions by the assistant model hence can differentiate the variations in spurious subgraphs. We show that extracting the maximally invariant subgraph to the proxy predictions provably identifies the underlying invariant subgraph for successful OOD generalization under the established minimal assumptions. Extensive experiments on datasets including DrugOOD with various graph distribution shifts confirm the effectiveness of GALA.
</details>
<details>
<summary>摘要</summary>
《固定 graph 表示学习中的不变性学习目标是学习数据集中的不变性，以实现对不同环境的外部数据泛化。然而，通常获取 graph 环境分区是非常昂贵的，因此通常会使用环境扩充来解决这个问题。然而，这种环境扩充的有用性从来没有得到证明。在这种情况下，我们发现，通过环境扩充来学习不变的 graph 表示是不可能的，因此我们提出了一些最小化假设，包括变化充分和变化一致，以便实现可能的不变的 graph 学习。然后，我们提出了一个新的框架Graph invAriant Learning Assistant（GALA）。GALA 包含一个助手模型，该模型需要对 graph 环境变化或分布变化敏感。如果助手模型的代理预测正确，那么可以区分真正的变量和误差的变量。我们证明，从助手模型的代理预测中提取最大可变的子图可以识别下来的不变的子图，并且在我们提出的假设下，可以 garantuee 对外部数据的泛化。我们的实验结果表明，GALA 在具有不同 graph 分布变化的数据集上具有非常高的有效性。》
</details></li>
</ul>
<hr>
<h2 id="An-Improved-Relaxation-for-Oracle-Efficient-Adversarial-Contextual-Bandits"><a href="#An-Improved-Relaxation-for-Oracle-Efficient-Adversarial-Contextual-Bandits" class="headerlink" title="An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits"></a>An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19025">http://arxiv.org/abs/2310.19025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiarash Banihashem, MohammadTaghi Hajiaghayi, Suho Shin, Max Springer</li>
<li>for: 这个论文是为了解决 adversarial contextual bandits 问题的 oracle-efficient relaxation。</li>
<li>methods: 这个论文使用的方法是一个 online adversary 选择 cost sequence，contexts 是从 known distribution 随机地被引入。</li>
<li>results: 这个论文的 regret bound 是 $O(T^{\frac{2}{3}(K\log(|\Pi|))^{\frac{1}{3})$，比之前的最好 bound $O((TK)^{\frac{2}{3}(\log(|\Pi|))^{\frac{1}{3})$ 更好。此外，这个论文还是第一个能够与 Langford 和 Zhang 在 NeurIPS 2007 提出的原始 bound 匹配的 result。<details>
<summary>Abstract</summary>
We present an oracle-efficient relaxation for the adversarial contextual bandits problem, where the contexts are sequentially drawn i.i.d from a known distribution and the cost sequence is chosen by an online adversary. Our algorithm has a regret bound of $O(T^{\frac{2}{3}(K\log(|\Pi|))^{\frac{1}{3})$ and makes at most $O(K)$ calls per round to an offline optimization oracle, where $K$ denotes the number of actions, $T$ denotes the number of rounds and $\Pi$ denotes the set of policies. This is the first result to improve the prior best bound of $O((TK)^{\frac{2}{3}(\log(|\Pi|))^{\frac{1}{3})$ as obtained by Syrgkanis et al. at NeurIPS 2016, and the first to match the original bound of Langford and Zhang at NeurIPS 2007 which was obtained for the stochastic case.
</details>
<details>
<summary>摘要</summary>
我们提出了一个 oracle-efficient relaxation 的方法来解决对抗上下文带状奖励问题，其中上下文是以独立 Identically distributed（i.i.d）方式从一个已知分布中随机获取，而问题选择的成本序列则是由一个在线 adversary 选择。我们的算法具有一个 regret  bound of $O(T^{\frac{2}{3}}(K\log(|\Pi|))^{\frac{1}{3}})$，并在每个回合最多做 $O(K)$ 个调用于 offline 优化库的请求，其中 $K$ 表示行动的数量，$T$ 表示回合的数量，$\Pi$ 表示策略的集合。这是第一个超越先前最好的 bound of $O((TK)^{\frac{2}{3}}(\log(|\Pi|))^{\frac{1}{3}})$，它是 Syrgkanis et al. 在 NeurIPS 2016 上提出的，并且是第一个与 Langford 和 Zhang 在 NeurIPS 2007 上提出的原始 bound 匹配，这个 bound 是为 Stochastic 情况。
</details></li>
</ul>
<hr>
<h2 id="Optimization-Landscape-of-Policy-Gradient-Methods-for-Discrete-time-Static-Output-Feedback"><a href="#Optimization-Landscape-of-Policy-Gradient-Methods-for-Discrete-time-Static-Output-Feedback" class="headerlink" title="Optimization Landscape of Policy Gradient Methods for Discrete-time Static Output Feedback"></a>Optimization Landscape of Policy Gradient Methods for Discrete-time Static Output Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19022">http://arxiv.org/abs/2310.19022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingliang Duan, Jie Li, Xuyang Chen, Kai Zhao, Shengbo Eben Li, Lin Zhao</li>
<li>for: 这篇论文探讨了使用policy gradient方法实现线性时域不变（LTI）系统的优化控制问题。</li>
<li>methods: 该论文使用了三种policy gradient方法：混合策略梯度法、自然策略梯度法和Gauss-Newton法。</li>
<li>results: 论文提出了关于这三种方法的新发现，包括它们在 дискреase时间LTI系统中的收敛性和约瑟率。此外，论文还证明了vanilla policy gradient方法在初始化 nearby local minima时的线性收敛性。<details>
<summary>Abstract</summary>
In recent times, significant advancements have been made in delving into the optimization landscape of policy gradient methods for achieving optimal control in linear time-invariant (LTI) systems. Compared with state-feedback control, output-feedback control is more prevalent since the underlying state of the system may not be fully observed in many practical settings. This paper analyzes the optimization landscape inherent to policy gradient methods when applied to static output feedback (SOF) control in discrete-time LTI systems subject to quadratic cost. We begin by establishing crucial properties of the SOF cost, encompassing coercivity, L-smoothness, and M-Lipschitz continuous Hessian. Despite the absence of convexity, we leverage these properties to derive novel findings regarding convergence (and nearly dimension-free rate) to stationary points for three policy gradient methods, including the vanilla policy gradient method, the natural policy gradient method, and the Gauss-Newton method. Moreover, we provide proof that the vanilla policy gradient method exhibits linear convergence towards local minima when initialized near such minima. The paper concludes by presenting numerical examples that validate our theoretical findings. These results not only characterize the performance of gradient descent for optimizing the SOF problem but also provide insights into the effectiveness of general policy gradient methods within the realm of reinforcement learning.
</details>
<details>
<summary>摘要</summary>
近些时间，在政策梯度方法中探索优化景观的进展很大。相比状态反馈控制，输出反馈控制更为普遍，因为实际情况中系统的下面状态可能不完全 observable。这篇论文分析了在静态输出反馈（SOF）控制中政策梯度方法的优化景观。我们首先证明了SOF成本函数的重要性质，包括半征性、L-smoothness和M-Lipschitz连续偏导。尽管不具有凸性，我们利用这些性质来 derivate 新的发现，包括政策梯度方法的三种方法（包括混合政策梯度方法、自然政策梯度方法和Gauss-Newton方法）的收敛性（以及几乎维度独立的速率）。此外，我们提供了证明，在 initialization 近于 Local minima 时，混合政策梯度方法 exhibits 线性收敛到 Local minima。文章结束，通过数学实验证明我们的理论发现。这些结果不仅描述了随机梯度下引擎的 SOF 问题的优化，还为束缚学习中的政策梯度方法提供了信息。
</details></li>
</ul>
<hr>
<h2 id="Behavior-Alignment-via-Reward-Function-Optimization"><a href="#Behavior-Alignment-via-Reward-Function-Optimization" class="headerlink" title="Behavior Alignment via Reward Function Optimization"></a>Behavior Alignment via Reward Function Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19007">http://arxiv.org/abs/2310.19007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhawal Gupta, Yash Chandak, Scott M. Jordan, Philip S. Thomas, Bruno Castro da Silva</li>
<li>for: 本研究旨在设计导引问题解决的优化学习搜寻（RL）代理人，以实现特定行为的目标。</li>
<li>methods: 本研究使用了一个新的两级目标框架，将auxiliary reward函数与环境的主要优化函数整合，以学习行为调整优化函数。</li>
<li>results: 本研究的结果显示，使用本研究的方法可以对RL代理人的政策优化过程进行自动调整，以减少问题解决中的限制和偏误。此外，本研究还证明了其可以对不同的任务和环境进行适用，并且可以实现高性能的解决方案，即使auxiliary reward函数存在误差或偏误。<details>
<summary>Abstract</summary>
Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific behaviors is a complex task. This is challenging since it requires the identification of reward structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively modifying the reward structure to offer denser and more frequent feedback can lead to unintended outcomes and promote behaviors that are not aligned with the designer's intended goal. Although potential-based reward shaping is often suggested as a remedy, we systematically investigate settings where deploying it often significantly impairs performance. To address these issues, we introduce a new framework that uses a bi-level objective to learn \emph{behavior alignment reward functions}. These functions integrate auxiliary rewards reflecting a designer's heuristics and domain knowledge with the environment's primary rewards. Our approach automatically determines the most effective way to blend these types of feedback, thereby enhancing robustness against heuristic reward misspecification. Remarkably, it can also adapt an agent's policy optimization process to mitigate suboptimalities resulting from limitations and biases inherent in the underlying RL algorithms. We evaluate our method's efficacy on a diverse set of tasks, from small-scale experiments to high-dimensional control challenges. We investigate heuristic auxiliary rewards of varying quality -- some of which are beneficial and others detrimental to the learning process. Our results show that our framework offers a robust and principled way to integrate designer-specified heuristics. It not only addresses key shortcomings of existing approaches but also consistently leads to high-performing solutions, even when given misaligned or poorly-specified auxiliary reward functions.
</details>
<details>
<summary>摘要</summary>
设计奖励函数以有效引导学习控制（RL）代理人行为是一个复杂的任务。这是因为它需要识别不 sparse的奖励结构，以避免不恰当的奖励引导代理人行为。直接修改奖励结构以提供更密集和更频繁的反馈可能会导致不预期的结果，并且激励代理人不符合设计者的目标行为。虽然潜在基于奖励的奖励形成 often 被建议作为解决方案，但我们系统地调查这种方法在一些情况下可能会导致性能下降。为解决这些问题，我们提出一种新的框架，使用二级目标学习行为Alignment奖励函数。这些函数将auxiliary奖励与环境的主要奖励相结合，以便自动确定最有效的奖励杂合方式，从而提高对奖励misspecification的Robustness。此外，它还可以通过调整代理人的政策优化过程来抑制基于RL算法的限制和偏见所导致的优化不足。我们对这种方法的可行性进行了多种任务的测试，从小规模实验到高维控制挑战。我们研究了不同质量的辅助奖励，一些有利于学习过程，而另一些有害。我们的结果表明，我们的框架可以采取一种原则性的方式来整合设计者指定的euristic。它不仅解决了现有方法的主要缺陷，还一致地导致高性能的解决方案，即使auxiliary奖励函数给出了偏移或低质量的指示。
</details></li>
</ul>
<hr>
<h2 id="Kernel-based-Joint-Multiple-Graph-Learning-and-Clustering-of-Graph-Signals"><a href="#Kernel-based-Joint-Multiple-Graph-Learning-and-Clustering-of-Graph-Signals" class="headerlink" title="Kernel-based Joint Multiple Graph Learning and Clustering of Graph Signals"></a>Kernel-based Joint Multiple Graph Learning and Clustering of Graph Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19005">http://arxiv.org/abs/2310.19005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad H. Alizade, Aref Einizade</li>
<li>for: 这种paper是为了掌握图像处理中的图结构学习和划分问题。</li>
<li>methods: 这种方法使用了kernel-based算法，结合了节点特征信息，以jointly partition signals和学习图。</li>
<li>results: 实验结果表明，这种方法在比较 estado-of-the-art方法时表现出了更高的效果。<details>
<summary>Abstract</summary>
Within the context of Graph Signal Processing (GSP), Graph Learning (GL) is concerned with the inference of a graph's topology from nodal observations, i.e., graph signals. However, data is often in mixed form, relating to different underlying structures. This heterogeneity necessitates the joint clustering and learning of multiple graphs. In many real-life applications, there are available node-side covariates (i.e., kernels) that imperatively should be incorporated, which has not been addressed by the rare graph signal clustering approaches. To this end and inspired by the rich K-means framework, we propose a novel kernel-based algorithm to incorporate this node-side information as we jointly partition the signals and learn a graph for each cluster. Numerical experiments demonstrate its effectiveness over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
在图像处理（GSP）中，图学习（GL）关注图像的结构划分，即从节点观测获取图像。然而，数据往往是混合形式的，关系不同的基础结构。这种多元性需要同时划分多个图。在许多实际应用中，有可用的节点侧特征（即kernel），需要考虑其中的信息，这一点未在前期的图像信号划分方法中被考虑。为此，我们基于rich K-means框架，提出一种新的kernel-based算法，并在同时划分信号和学习图中jointly使用节点侧信息。数值实验表明其效果胜过现有的状态。
</details></li>
</ul>
<hr>
<h2 id="A-U-turn-on-Double-Descent-Rethinking-Parameter-Counting-in-Statistical-Learning"><a href="#A-U-turn-on-Double-Descent-Rethinking-Parameter-Counting-in-Statistical-Learning" class="headerlink" title="A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning"></a>A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18988">http://arxiv.org/abs/2310.18988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alicia Curth, Alan Jeffares, Mihaela van der Schaar</li>
<li>for: 本研究探讨了double descent现象在传统统计机器学习方法中的存在，并挑战了现有的U型曲线假设。</li>
<li>methods: 研究使用了非神经网络模型，包括线性回归、树和加法拟合。</li>
<li>results: 研究发现，当 Parameter 数量增加时，模型的测试错误率会经历两次下降，而不是传统的U型曲线预测。此外，通过视角改变，研究发现这种现象是由多个复杂性轴的交叠导致的。<details>
<summary>Abstract</summary>
Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a U-shaped curve reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count p grows past sample size n - a phenomenon dubbed double descent. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include linear regression, trees, and boosting. In this work, we take a closer look at evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to what is being plotted on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and only) when and where the transition between these underlying axes occurs, and that its location is thus not inherently tied to the interpolation threshold p=n. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as smoothers and propose a generalized measure for the effective number of parameters they use on unseen examples, using which we find that their apparent double descent curves indeed fold back into more traditional convex shapes - providing a resolution to tensions between double descent and statistical intuition.
</details>
<details>
<summary>摘要</summary>
传统统计智能认为，模型复杂度和预测误差之间存在一个很好地理解的关系，通常表现为一个U型曲线，反映模型在过拟合和under拟合两个 режиmes之间的转换。然而，受深度学习的成功影响，近期一些influential的工作表明，这种理论是通常不准确的，存在一个第二个下降的测试误差情况，被称为double descent。而且，这种现象不仅限于深度学习设置，还出现在非神经网络模型中，如线性回归、树和抛物模型。在这项工作中，我们更加仔细地研究了这些经典统计机器学习方法的证据，并挑战这些方法的double descent现象是否真的超出传统的U型复杂度-通用曲线的限制。我们发现，只要注意把plot的x轴上的图表绘制得到的是多少个复杂度轴，double descent现象就会变得更加明确。我们示示了第二个下降出现在这些下推轴之间的转换处，并且其位置不是因为 interpolate threshold p=n 决定的。然后，我们采用了一种类非Parametric统计视角，将这些方法看作是简单器，并提出了一种通用的效果参数计数器，用于测试这些方法在未seen例中的表现。我们发现，这些方法的apparent double descent曲线实际上是fold back到了传统的convex形状，解决了对double descent和统计直觉之间的矛盾。
</details></li>
</ul>
<hr>
<h2 id="TRIAGE-Characterizing-and-auditing-training-data-for-improved-regression"><a href="#TRIAGE-Characterizing-and-auditing-training-data-for-improved-regression" class="headerlink" title="TRIAGE: Characterizing and auditing training data for improved regression"></a>TRIAGE: Characterizing and auditing training data for improved regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18970">http://arxiv.org/abs/2310.18970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seedatnabeel/triage">https://github.com/seedatnabeel/triage</a></li>
<li>paper_authors: Nabeel Seedat, Jonathan Crabbé, Zhaozhi Qian, Mihaela van der Schaar</li>
<li>for: 本研究旨在提供一种适用于回归任务的数据Characterization方法，以提高机器学习算法的稳定性和性能。</li>
<li>methods: 本方法基于Conformal predictive distributions提供一个模型无关的评分方法，称为TRIAGE score。该分数用于分析个体样本的训练动态和 caracterizar样本为模型下未经估计、过估计或良好估计。</li>
<li>results: 研究人员通过应用TRIAGE分析了多个回归任务，并证明了TRIAGE的 charactization是一致的。此外，TRIAGE还可以用于选择数据集和获取特征。总的来说，TRIAGE highlights the value of data characterization in real-world regression applications.<details>
<summary>Abstract</summary>
Data quality is crucial for robust machine learning algorithms, with the recent interest in data-centric AI emphasizing the importance of training data characterization. However, current data characterization methods are largely focused on classification settings, with regression settings largely understudied. To address this, we introduce TRIAGE, a novel data characterization framework tailored to regression tasks and compatible with a broad class of regressors. TRIAGE utilizes conformal predictive distributions to provide a model-agnostic scoring method, the TRIAGE score. We operationalize the score to analyze individual samples' training dynamics and characterize samples as under-, over-, or well-estimated by the model. We show that TRIAGE's characterization is consistent and highlight its utility to improve performance via data sculpting/filtering, in multiple regression settings. Additionally, beyond sample level, we show TRIAGE enables new approaches to dataset selection and feature acquisition. Overall, TRIAGE highlights the value unlocked by data characterization in real-world regression applications
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>机器学习算法中数据质量的重要性已得到更多的关注，特别是在数据中心式AI时代，数据Characterization的重要性得到了更多的认可。然而，现有的数据Characterization方法主要集中在分类任务上， regression任务相对较少研究。为了解决这个问题，我们介绍了一种新的数据Characterization框架，即TRIAGE，该框架适用于多种回归器，并且可以提供一个模型无关的评分方法，即TRIAGE分数。我们将TRIAGE分数操作化，以分析个体样本的训练过程和模型评估。我们示出了TRIAGE的分类是一致的，并且它的用途可以提高数据雕刻/筛选的性能，在多种回归任务中。此外，TRIAGE还可以用于数据集选择和特征收集新的方法。总之，TRIAGE highlights the value of data characterization in real-world regression applications.
</details></li>
</ul>
<hr>
<h2 id="Playing-in-the-Dark-No-regret-Learning-with-Adversarial-Constraints"><a href="#Playing-in-the-Dark-No-regret-Learning-with-Adversarial-Constraints" class="headerlink" title="Playing in the Dark: No-regret Learning with Adversarial Constraints"></a>Playing in the Dark: No-regret Learning with Adversarial Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18955">http://arxiv.org/abs/2310.18955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Sinha, Rahul Vaze</li>
<li>For: 本文探讨了线性 convex 优化框架的扩展，包括额外的长期反对抗约束。特别是，在一线策略决定动作后，除了一个凸成本函数外，反对抗还会透露一组 $k$ 凸约束。成本和约束函数可以随时间变化，无法预测未来函数的信息。* Methods: 本文提出了一种元策略，同时实现了下线性累积约束和下线性 regret。这是通过将受限问题降低到标准 OCO 问题的 recursive 构建的一个黑盒减reduction。我们表明，可以通过解决 surrogate 问题使用任何适应 OCO 策略，满足标准数据依赖 regret bound。* Results: 本文提出了一种新的 Lyapunov 基于证明技术，揭示了 regret 与某些顺序不等式之间的连接。通过一种新的分解结果，我们得出了 regret 的优化性质。 finally, 本文应用于在线多任务学习和网络控制问题。<details>
<summary>Abstract</summary>
We study a generalization of the classic Online Convex Optimization (OCO) framework by considering additional long-term adversarial constraints. Specifically, after an online policy decides its action on a round, in addition to a convex cost function, the adversary also reveals a set of $k$ convex constraints. The cost and the constraint functions could change arbitrarily with time, and no information about the future functions is assumed to be available. In this paper, we propose a meta-policy that simultaneously achieves a sublinear cumulative constraint violation and a sublinear regret. This is achieved via a black box reduction of the constrained problem to the standard OCO problem for a recursively constructed sequence of surrogate cost functions. We show that optimal performance bounds can be achieved by solving the surrogate problem using any adaptive OCO policy enjoying a standard data-dependent regret bound. A new Lyapunov-based proof technique is presented that reveals a connection between regret and certain sequential inequalities through a novel decomposition result. We conclude the paper by highlighting applications to online multi-task learning and network control problems.
</details>
<details>
<summary>摘要</summary>
我们研究了online convex optimization（OCO）框架的一种普遍化，其中包括额外的长期反对派对约束。具体来说，在一个线上策略决定其行动后，除了一个凸成本函数外，反对派也会公布一组$k$个凸约束。成本函数和约束函数可以随时间变化，并不知道未来函数的信息。在这篇论文中，我们提议一个meta策略，可以同时实现一个凸累累约束和一个凸后悔。这是通过一种黑盒减少法将受约束问题转化为标准OCO问题的 recursively constructed sequence of surrogate cost functions。我们表明了一种新的 Lyapunov-based 证明技术，可以通过一种新的分解结果显示回归和某些顺序不等式之间的联系。最后，我们将报告应用于在线多任务学习和网络控制问题。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Bias-of-Gradient-Descent-for-Two-layer-ReLU-and-Leaky-ReLU-Networks-on-Nearly-orthogonal-Data"><a href="#Implicit-Bias-of-Gradient-Descent-for-Two-layer-ReLU-and-Leaky-ReLU-Networks-on-Nearly-orthogonal-Data" class="headerlink" title="Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data"></a>Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18935">http://arxiv.org/abs/2310.18935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Kou, Zixiang Chen, Quanquan Gu</li>
<li>for: 这个论文主要探讨了Gradient Descent的隐式偏好在训练非平滑神经网络时的影响。</li>
<li>methods: 本文使用了Gradient Descent来训练两层完全相连（漏斗 activtion function）神经网络，并研究了隐式偏好在不同activation function下的影响。</li>
<li>results: 本文发现在训练数据接近对称的情况下，隐式偏好会使Gradient Descent寻找一个稳定的rank值，并且这値值会在ReLU activation function下随着训练进程的推移而变化。此外，本文还发现隐式偏好会使Gradient Descent寻找一个神经网络，使得所有的训练数据点都具有相同的normalized margin。实验结果与理论结果匹配。<details>
<summary>Abstract</summary>
The implicit bias towards solutions with favorable properties is believed to be a key reason why neural networks trained by gradient-based optimization can generalize well. While the implicit bias of gradient flow has been widely studied for homogeneous neural networks (including ReLU and leaky ReLU networks), the implicit bias of gradient descent is currently only understood for smooth neural networks. Therefore, implicit bias in non-smooth neural networks trained by gradient descent remains an open question. In this paper, we aim to answer this question by studying the implicit bias of gradient descent for training two-layer fully connected (leaky) ReLU neural networks. We showed that when the training data are nearly-orthogonal, for leaky ReLU activation function, gradient descent will find a network with a stable rank that converges to $1$, whereas for ReLU activation function, gradient descent will find a neural network with a stable rank that is upper bounded by a constant. Additionally, we show that gradient descent will find a neural network such that all the training data points have the same normalized margin asymptotically. Experiments on both synthetic and real data backup our theoretical findings.
</details>
<details>
<summary>摘要</summary>
“偏好解释器的偏好”是一个关键因素，使得神经网络通过梯度下降优化得到良好的泛化能力。而“梯度流”中的偏好已经广泛研究过 homogeneous 神经网络（包括 ReLU 和泄漏 ReLU 网络），但是“梯度下降”中的偏好对非满意神经网络仍然是一个开Question。在这篇论文中，我们尝试答复这个问题，通过研究两层全连接（泄漏 ReLU）神经网络在训练过程中的偏好。我们发现，当训练数据几乎正交时，使用泄漏 ReLU 活动函数时，梯度下降会找到一个稳定的权重积分，其渐近值为 1；而使用 ReLU 活动函数时，梯度下降会找到一个神经网络，其稳定权重积分上限为一个常数。此外，我们还发现，梯度下降会找到一个神经网络，使得所有的训练数据点都具有相同的归一化margin。实验结果证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Remaining-Useful-Life-Prediction-of-Lithium-ion-Batteries-using-Spatio-temporal-Multimodal-Attention-Networks"><a href="#Remaining-Useful-Life-Prediction-of-Lithium-ion-Batteries-using-Spatio-temporal-Multimodal-Attention-Networks" class="headerlink" title="Remaining Useful Life Prediction of Lithium-ion Batteries using Spatio-temporal Multimodal Attention Networks"></a>Remaining Useful Life Prediction of Lithium-ion Batteries using Spatio-temporal Multimodal Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18924">http://arxiv.org/abs/2310.18924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Dhruvadityamittal/RUL_Prediction_of_LIB_using_Spatio_temporal_Multimodal_Attention_Networks">https://github.com/Dhruvadityamittal/RUL_Prediction_of_LIB_using_Spatio_temporal_Multimodal_Attention_Networks</a></li>
<li>paper_authors: Sungho Suh, Dhruv Aditya Mittal, Hymalai Bello, Bo Zhou, Mayank Shekhar Jha, Paul Lukowicz</li>
<li>For: The paper aims to predict the remaining useful life of Lithium-ion batteries in real-world scenarios, addressing the limitations of existing methods and improving the reliability and efficiency of battery operations.* Methods: The proposed method uses a two-stage remaining useful life prediction scheme based on a spatio-temporal multimodal attention network (ST-MAN), which captures complex spatio-temporal dependencies in the battery data and neglected features such as temperature, internal resistance, and material type.* Results: The proposed ST-MAN model outperforms existing CNN and LSTM-based methods, achieving state-of-the-art performance in predicting the remaining useful life of Li-ion batteries.<details>
<summary>Abstract</summary>
Lithium-ion batteries are widely used in various applications, including electric vehicles and renewable energy storage. The prediction of the remaining useful life (RUL) of batteries is crucial for ensuring reliable and efficient operation, as well as reducing maintenance costs. However, determining the life cycle of batteries in real-world scenarios is challenging, and existing methods have limitations in predicting the number of cycles iteratively. In addition, existing works often oversimplify the datasets, neglecting important features of the batteries such as temperature, internal resistance, and material type. To address these limitations, this paper proposes a two-stage remaining useful life prediction scheme for Lithium-ion batteries using a spatio-temporal multimodal attention network (ST-MAN). The proposed model is designed to iteratively predict the number of cycles required for the battery to reach the end of its useful life, based on available data. The proposed ST-MAN is to capture the complex spatio-temporal dependencies in the battery data, including the features that are often neglected in existing works. Experimental results demonstrate that the proposed ST-MAN model outperforms existing CNN and LSTM-based methods, achieving state-of-the-art performance in predicting the remaining useful life of Li-ion batteries. The proposed method has the potential to improve the reliability and efficiency of battery operations and is applicable in various industries, including automotive and renewable energy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Hyperbolic-Graph-Neural-Networks-at-Scale-A-Meta-Learning-Approach"><a href="#Hyperbolic-Graph-Neural-Networks-at-Scale-A-Meta-Learning-Approach" class="headerlink" title="Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach"></a>Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18918">http://arxiv.org/abs/2310.18918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nurendra Choudhary, Nikhil Rao, Chandan K. Reddy</li>
<li>for: 提高几何神经网络（HNNs）的泛化能力和可扩展性，以便在新任务上快速学习和掌握大型图数据集。</li>
<li>methods: 学习图节点和边的本地子图中的抽象特征，并将其转移到新的子图上进行几拟 shot 学习。引入一种新的方法——几何 GRAph Meta Learner（H-GRAM），可以在节点 classification 和链接预测任务中学习并转移抽象信息，以便更快地学习新的任务。</li>
<li>results: 在多个具有挑战性的几拟 shot Setting 中，H-GRAM 能够有效地学习和转移信息，并且在大型图数据集上可以扩展性地提高性能。与标准 HNNs 相比，我们的方法可以更好地扩展到大型图数据集和提高性能。<details>
<summary>Abstract</summary>
The progress in hyperbolic neural networks (HNNs) research is hindered by their absence of inductive bias mechanisms, which are essential for generalizing to new tasks and facilitating scalable learning over large datasets. In this paper, we aim to alleviate these issues by learning generalizable inductive biases from the nodes' local subgraph and transfer them for faster learning over new subgraphs with a disjoint set of nodes, edges, and labels in a few-shot setting. We introduce a novel method, Hyperbolic GRAph Meta Learner (H-GRAM), that, for the tasks of node classification and link prediction, learns transferable information from a set of support local subgraphs in the form of hyperbolic meta gradients and label hyperbolic protonets to enable faster learning over a query set of new tasks dealing with disjoint subgraphs. Furthermore, we show that an extension of our meta-learning framework also mitigates the scalability challenges seen in HNNs faced by existing approaches. Our comparative analysis shows that H-GRAM effectively learns and transfers information in multiple challenging few-shot settings compared to other state-of-the-art baselines. Additionally, we demonstrate that, unlike standard HNNs, our approach is able to scale over large graph datasets and improve performance over its Euclidean counterparts.
</details>
<details>
<summary>摘要</summary>
progress in гиперболических нейронных сетях (HNNs) 研究受到其缺乏抽象假设机制的限制，这些机制是必需的 для泛化到新任务和促进大量数据集上的学习。在这篇论文中，我们想要解决这些问题，通过从节点的本地子图中学习通用的抽象假设，并将其传递给新的子图上快速学习。我们提出了一种新的方法，即гиперболическиеGRAPH元学习器（H-GRAM），它在节点分类和链接预测任务上，通过从支持本地子图集中学习hyperbolic meta 梯度和标签hyerbolic气泡来启用快速学习新任务。此外，我们还证明了我们的meta学习框架的扩展可以解决现有方法所面临的扩展性问题。我们的比较分析表明，H-GRAM在多个具有挑战性的几个shot设定中能够有效地学习和传递信息，并且与标准HNNs相比，我们的方法可以在大规模图数据集上提高性能。
</details></li>
</ul>
<hr>
<h2 id="Estimating-the-Rate-Distortion-Function-by-Wasserstein-Gradient-Descent"><a href="#Estimating-the-Rate-Distortion-Function-by-Wasserstein-Gradient-Descent" class="headerlink" title="Estimating the Rate-Distortion Function by Wasserstein Gradient Descent"></a>Estimating the Rate-Distortion Function by Wasserstein Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18908">http://arxiv.org/abs/2310.18908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiboyang/wgd">https://github.com/yiboyang/wgd</a></li>
<li>paper_authors: Yibo Yang, Stephan Eckstein, Marcel Nutz, Stephan Mandt</li>
<li>for: 本研究的目的是提出一种基于最优运输的Rate-Distortion（R-D）函数估计方法，用于评估数据源的压缩性。</li>
<li>methods: 该方法使用 Wasserstein 梯度下降算法学习优化的抽象分布，不同于经典的 Blahut–Arimoto 算法，它预先固定了往复分布的支持。</li>
<li>results: 实验表明，该方法可以在低比特率源上获得相当或更紧的约束，而需要许多更少的调整和计算努力。此外，该方法还与最大极值估计有关，并引入了一种新的测试源。<details>
<summary>Abstract</summary>
In the theory of lossy compression, the rate-distortion (R-D) function $R(D)$ describes how much a data source can be compressed (in bit-rate) at any given level of fidelity (distortion). Obtaining $R(D)$ for a given data source establishes the fundamental performance limit for all compression algorithms. We propose a new method to estimate $R(D)$ from the perspective of optimal transport. Unlike the classic Blahut--Arimoto algorithm which fixes the support of the reproduction distribution in advance, our Wasserstein gradient descent algorithm learns the support of the optimal reproduction distribution by moving particles. We prove its local convergence and analyze the sample complexity of our R-D estimator based on a connection to entropic optimal transport. Experimentally, we obtain comparable or tighter bounds than state-of-the-art neural network methods on low-rate sources while requiring considerably less tuning and computation effort. We also highlight a connection to maximum-likelihood deconvolution and introduce a new class of sources that can be used as test cases with known solutions to the R-D problem.
</details>
<details>
<summary>摘要</summary>
理论上，吞吐率-损均衡（R-D）函数 $R(D)$ 描述了数据源可以通过压缩（bit-rate）来实现任何级别的准确性（损均）。确定 $R(D)$ 对于给定数据源是吞吐率压缩算法的基本性能上限。我们提出了一种基于最优运输的新方法来估计 $R(D)$。这种方法不同于经典的布拉哈特-阿里莫托算法，它在预先固定往复复制分布的支持上进行估计。我们的渐进梯度滚动算法会学习最优往复复制分布的支持，通过移动粒子来实现。我们证明了本方法的本地收敛性和样本复杂性，并与经典神经网络方法进行比较。实验结果表明，我们的方法可以在低比特率源上获得相对或更紧的约束，而且需要远少的调整和计算努力。我们还 highlight了与最大似然减杂的连接，并介绍了一种新的测试集，其中可以使用已知解决R-D问题的源。
</details></li>
</ul>
<hr>
<h2 id="Topological-or-Non-topological-A-Deep-Learning-Based-Prediction"><a href="#Topological-or-Non-topological-A-Deep-Learning-Based-Prediction" class="headerlink" title="Topological, or Non-topological? A Deep Learning Based Prediction"></a>Topological, or Non-topological? A Deep Learning Based Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18907">http://arxiv.org/abs/2310.18907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xercxis/P_zeta">https://github.com/xercxis/P_zeta</a></li>
<li>paper_authors: Ashiqur Rasul, Md Shafayat Hossain, Ankan Ghosh Dastider, Himaddri Roy, M. Zahid Hasan, Quazi D. M. Khosru</li>
<li>for: 预测和发现新材料的性能预测和材料设计</li>
<li>methods: 使用深度学习模型，结合 persistently homology 和图神经网络，实现高精度的材料分类</li>
<li>results: 实验结果显示，该模型的准确率为 91.4%，F1 分数为 88.5%，在分类非材料和材料中表现出色，超过其他状态对照模型<details>
<summary>Abstract</summary>
Prediction and discovery of new materials with desired properties are at the forefront of quantum science and technology research. A major bottleneck in this field is the computational resources and time complexity related to finding new materials from ab initio calculations. In this work, an effective and robust deep learning-based model is proposed by incorporating persistent homology and graph neural network which offers an accuracy of 91.4% and an F1 score of 88.5% in classifying topological vs. non-topological materials, outperforming the other state-of-the-art classifier models. The incorporation of the graph neural network encodes the underlying relation between the atoms into the model based on their own crystalline structures and thus proved to be an effective method to represent and process non-euclidean data like molecules with a relatively shallow network. The persistent homology pipeline in the suggested neural network is capable of integrating the atom-specific topological information into the deep learning model, increasing robustness, and gain in performance. It is believed that the presented work will be an efficacious tool for predicting the topological class and therefore enable the high-throughput search for novel materials in this field.
</details>
<details>
<summary>摘要</summary>
科学家们正在努力探索新材料的搜索和预测，以满足现代科学和技术的需求。然而，计算资源和计算复杂性问题成为了这一领域的主要瓶颈。在这篇文章中，我们提出了一种有效和可靠的深度学习模型，通过结合持续同态和图神经网络来减少计算资源的占用和提高预测的精度。这种模型在分类非普遍材料和普遍材料方面的准确率达91.4%，F1分数达88.5%，超过了其他现有的分类模型。在这种模型中，图神经网络允许通过晶体结构中的原子之间的关系来编码材料的结构，从而实现了对非欧几何数据的有效处理。持续同态管道在建议的神经网络中允许将原子特征的拓扑信息纳入深度学习模型中，从而提高了模型的稳定性和性能。总之，这种方法将成为预测材料的拓扑类别的有效工具，并促进高速搜索新材料的搜索。
</details></li>
</ul>
<hr>
<h2 id="Learning-Subgrid-Scale-Models-in-Discontinuous-Galerkin-Methods-with-Neural-Ordinary-Differential-Equations-for-Compressible-Navier–Stokes-Equations"><a href="#Learning-Subgrid-Scale-Models-in-Discontinuous-Galerkin-Methods-with-Neural-Ordinary-Differential-Equations-for-Compressible-Navier–Stokes-Equations" class="headerlink" title="Learning Subgrid-Scale Models in Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier–Stokes Equations"></a>Learning Subgrid-Scale Models in Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier–Stokes Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18897">http://arxiv.org/abs/2310.18897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shinhoo Kang, Emil M. Constantinescu</li>
<li>for: 该文章的目的是提出一种基于神经普通微分方程的新方法，用于在离散哈姆频率（DG）空间积分中学习低级别模型的影响。</li>
<li>methods: 该方法使用神经网络来学习低级别模型中缺失的涨落尺度，从而提高低级别DG近似的准确性和加速筛选高级别DG simulation的运算速度。</li>
<li>results: 作者通过多维泰勒-格林涡漩示例来证明该方法的性能，并证明该方法不仅可以重construct低级别模型的涨落尺度，还可以加速筛选高级别DG simulation的运算速度，提高了模型的准确性和效率。<details>
<summary>Abstract</summary>
The growing computing power over the years has enabled simulations to become more complex and accurate. However, high-fidelity simulations, while immensely valuable for scientific discovery and problem solving, come with significant computational demands. As a result, it is common to run a low-fidelity model with a subgrid-scale model to reduce the computational cost, but selecting the appropriate subgrid-scale models and tuning them are challenging. We propose a novel method for learning the subgrid-scale model effects when simulating partial differential equations using neural ordinary differential equations in the context of discontinuous Galerkin (DG) spatial discretization. Our approach learns the missing scales of the low-order DG solver at a continuous level and hence improves the accuracy of the low-order DG approximations as well as accelerates the filtered high-order DG simulations with a certain degree of precision. We demonstrate the performance of our approach through multidimensional Taylor--Green vortex examples at different Reynolds numbers and times, which cover laminar, transitional, and turbulent regimes. The proposed method not only reconstructs the subgrid-scale from the low-order (1st-order) approximation but also speeds up the filtered high-order DG (6th-order) simulation by two orders of magnitude.
</details>
<details>
<summary>摘要</summary>
随着计算能力的提高， simulations 已经能够更加复杂和准确。然而，高精度 simulations 的计算需求很大，因此通常采用低精度模型和缺失涂抹模型来降低计算成本。然而，选择合适的缺失涂抹模型并调整它们是困难的。我们提出了一种基于神经ordinary differential equations的新方法，用于在discontinuous Galerkin（DG）空间积分方法中学习subgrid-scale模型的效果。我们的方法可以在 kontinuierlichen Level学习低阶DG解的缺失尺度，从而提高低阶DG的准确性和加速筛选高阶DG（6th-order） simulations的过程。我们通过多维 Taylor--Green涡涌示例来证明我们的方法的性能，这些示例覆盖了laminar、transition和turbulent режиmes。我们的方法不仅可以从低阶（1st-order）解中重construct subgrid-scale，还可以加速筛选高阶DG simulations的过程，提高速度两个数量级。
</details></li>
</ul>
<hr>
<h2 id="D2NO-Efficient-Handling-of-Heterogeneous-Input-Function-Spaces-with-Distributed-Deep-Neural-Operators"><a href="#D2NO-Efficient-Handling-of-Heterogeneous-Input-Function-Spaces-with-Distributed-Deep-Neural-Operators" class="headerlink" title="D2NO: Efficient Handling of Heterogeneous Input Function Spaces with Distributed Deep Neural Operators"></a>D2NO: Efficient Handling of Heterogeneous Input Function Spaces with Distributed Deep Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18888">http://arxiv.org/abs/2310.18888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zecheng Zhang, Christian Moya, Lu Lu, Guang Lin, Hayden Schaeffer</li>
<li>for: 解决 Parametric partial differential equations、动力系统控制和反向问题中的hetERogeneous输入函数问题</li>
<li>methods: 使用Discretization-invariant neural operators和分布式方法处理多感器输入函数</li>
<li>results: 提出一种新的分布式方法，可以降低Gradient descent back-propagation步数，提高效率而不失精度，并 Validated by four numerical examples<details>
<summary>Abstract</summary>
Neural operators have been applied in various scientific fields, such as solving parametric partial differential equations, dynamical systems with control, and inverse problems. However, challenges arise when dealing with input functions that exhibit heterogeneous properties, requiring multiple sensors to handle functions with minimal regularity. To address this issue, discretization-invariant neural operators have been used, allowing the sampling of diverse input functions with different sensor locations. However, existing frameworks still require an equal number of sensors for all functions. In our study, we propose a novel distributed approach to further relax the discretization requirements and solve the heterogeneous dataset challenges. Our method involves partitioning the input function space and processing individual input functions using independent and separate neural networks. A centralized neural network is used to handle shared information across all output functions. This distributed methodology reduces the number of gradient descent back-propagation steps, improving efficiency while maintaining accuracy. We demonstrate that the corresponding neural network is a universal approximator of continuous nonlinear operators and present four numerical examples to validate its performance.
</details>
<details>
<summary>摘要</summary>
我们的方法包括将输入函数空间分区，并使用独立的 neural network 处理个别输入函数。中央 neural network 用于处理所有输出函数之间的共享信息。这种分布式方法可以降低梯度下降反propagation 步骤数量，提高效率而无损准确性。我们证明了相应的 neural network 是一个 universal approximator 的连续非线性算子，并在四个数字示例中验证了其性能。
</details></li>
</ul>
<hr>
<h2 id="A-foundational-neural-operator-that-continuously-learns-without-forgetting"><a href="#A-foundational-neural-operator-that-continuously-learns-without-forgetting" class="headerlink" title="A foundational neural operator that continuously learns without forgetting"></a>A foundational neural operator that continuously learns without forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18885">http://arxiv.org/abs/2310.18885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tapas Tripura, Souvik Chakraborty</li>
<li>for: 本研究旨在开发一种基础模型，用于科学计算中的物理问题。</li>
<li>methods: 该模型基于神经网络和波峰分解技术，并使用了闭合结构和记忆 Ensemble 技术来学习多种物理系统的解方程。</li>
<li>results: 该模型能够同时学习多种 Parametric PDE 的解方程，并能够快速适应新的 Parametric PDE。同时，该模型也能够保持Positive Transfer和避免 Catastrophic Forgetting。经过广泛的 benchmark 测试，该模型可以在预测阶段比task-specific基eline模型表现更好，并且具有较少的hyperparameter tuning。<details>
<summary>Abstract</summary>
Machine learning has witnessed substantial growth, leading to the development of advanced artificial intelligence models crafted to address a wide range of real-world challenges spanning various domains, such as computer vision, natural language processing, and scientific computing. Nevertheless, the creation of custom models for each new task remains a resource-intensive undertaking, demanding considerable computational time and memory resources. In this study, we introduce the concept of the Neural Combinatorial Wavelet Neural Operator (NCWNO) as a foundational model for scientific computing. This model is specifically designed to excel in learning from a diverse spectrum of physics and continuously adapt to the solution operators associated with parametric partial differential equations (PDEs). The NCWNO leverages a gated structure that employs local wavelet experts to acquire shared features across multiple physical systems, complemented by a memory-based ensembling approach among these local wavelet experts. This combination enables rapid adaptation to new challenges. The proposed foundational model offers two key advantages: (i) it can simultaneously learn solution operators for multiple parametric PDEs, and (ii) it can swiftly generalize to new parametric PDEs with minimal fine-tuning. The proposed NCWNO is the first foundational operator learning algorithm distinguished by its (i) robustness against catastrophic forgetting, (ii) the maintenance of positive transfer for new parametric PDEs, and (iii) the facilitation of knowledge transfer across dissimilar tasks. Through an extensive set of benchmark examples, we demonstrate that the NCWNO can outperform task-specific baseline operator learning frameworks with minimal hyperparameter tuning at the prediction stage. We also show that with minimal fine-tuning, the NCWNO performs accurate combinatorial learning of new parametric PDEs.
</details>
<details>
<summary>摘要</summary>
In this study, we introduce the Neural Combinatorial Wavelet Neural Operator (NCWNO) as a foundational model for scientific computing. This model is specifically designed to excel in learning from a diverse spectrum of physics and continuously adapt to the solution operators associated with parametric partial differential equations (PDEs). The NCWNO leverages a gated structure that employs local wavelet experts to acquire shared features across multiple physical systems, complemented by a memory-based ensembling approach among these local wavelet experts. This combination enables rapid adaptation to new challenges.The proposed foundational model offers two key advantages: (i) it can simultaneously learn solution operators for multiple parametric PDEs, and (ii) it can swiftly generalize to new parametric PDEs with minimal fine-tuning. Additionally, the NCWNO is distinguished by its:* Robustness against catastrophic forgetting* Maintenance of positive transfer for new parametric PDEs* Facilitation of knowledge transfer across dissimilar tasksThrough an extensive set of benchmark examples, we demonstrate that the NCWNO can outperform task-specific baseline operator learning frameworks with minimal hyperparameter tuning at the prediction stage. We also show that with minimal fine-tuning, the NCWNO can accurately combine learning of new parametric PDEs.
</details></li>
</ul>
<hr>
<h2 id="Simple-and-Asymmetric-Graph-Contrastive-Learning-without-Augmentations"><a href="#Simple-and-Asymmetric-Graph-Contrastive-Learning-without-Augmentations" class="headerlink" title="Simple and Asymmetric Graph Contrastive Learning without Augmentations"></a>Simple and Asymmetric Graph Contrastive Learning without Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18884">http://arxiv.org/abs/2310.18884</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tengxiao1/graphacl">https://github.com/tengxiao1/graphacl</a></li>
<li>paper_authors: Teng Xiao, Huaisheng Zhu, Zhengyu Chen, Suhang Wang</li>
<li>for: 本文研究了对异谱图进行对照学习，并提出了一种简单的算法GraphACL，可以 capture一步邻居信息和两步同类相似性。</li>
<li>methods: 本文使用了对照学习方法，并提出了一种偏 asymmetric 视角来处理异谱图。</li>
<li>results: 实验结果表明，GraphACL 可以在异谱图上 achieve 出色的表现，并且在 homophilic 和异谱图上都具有优异的泛化能力。<details>
<summary>Abstract</summary>
Graph Contrastive Learning (GCL) has shown superior performance in representation learning in graph-structured data. Despite their success, most existing GCL methods rely on prefabricated graph augmentation and homophily assumptions. Thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. In this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. We find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), is easy to implement and does not rely on graph augmentations and homophily assumptions. We provide theoretical and empirical evidence that GraphACL can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. Experimental results show that the simple GraphACL significantly outperforms state-of-the-art graph contrastive learning and self-supervised learning methods on homophilic and heterophilic graphs. The code of GraphACL is available at https://github.com/tengxiao1/GraphACL.
</details>
<details>
<summary>摘要</summary>
图像对比学习（GCL）在图结构数据中的表示学习表现出色。然而，大多数现有的GCL方法都基于先制制图像增强和同类连接假设。因此，它们在不同类型连接的图中失去泛化能力。在这篇论文中，我们研究了在同类连接和不同类型连接图中进行对比学习的问题。我们发现，只需考虑偏 asymmetric 的邻居节点视角，就可以获得了良好的表现。 resulting algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), 易于实现并不需要图像增强和同类连接假设。我们提供了理论和实验证据，表明 GraphACL 可以捕捉一次邻居信息和两次同类连接相似性，这些都是模型不同类型连接图的关键。实验结果表明，简单的 GraphACL 在同类连接和不同类型连接图中明显超越了当前最佳的图像对比学习和自然学习方法。GraphACL 的代码可以在 https://github.com/tengxiao1/GraphACL 上获取。
</details></li>
</ul>
<hr>
<h2 id="Correlation-Aware-Sparsified-Mean-Estimation-Using-Random-Projection"><a href="#Correlation-Aware-Sparsified-Mean-Estimation-Using-Random-Projection" class="headerlink" title="Correlation Aware Sparsified Mean Estimation Using Random Projection"></a>Correlation Aware Sparsified Mean Estimation Using Random Projection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18868">http://arxiv.org/abs/2310.18868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/11hifish/Rand-Proj-Spatial">https://github.com/11hifish/Rand-Proj-Spatial</a></li>
<li>paper_authors: Shuli Jiang, Pranay Sharma, Gauri Joshi</li>
<li>for: 这篇论文主要探讨了分布式vector mean估计问题，是分布式优化和联合学习（Federated Learning，FL）中通用的子routine。</li>
<li>methods: 这篇论文使用了 Rand-$k$ 簇范例化技术来减少分布式传输成本，每个客户端将 $k &lt; d$ 个坐标发送到服务器。然而，Rand-$k$ 无法考虑实际应用中客户端之间的相互联系。这篇论文提出了 Rand-$k$-Spatial 估计器，利用服务器端的客户端间联系信息来改善 Rand-$k$ 的性能。然而，Rand-$k$-Spatial 的性能仍然不足。这篇论文提出了 Rand-Proj-Spatial 估计器，具有更加灵活的嵌入构造和解oding程序，可以更好地利用客户端间的联系信息。</li>
<li>results: 这篇论文的实验结果显示，Rand-Proj-Spatial 比 Rand-$k$-Spatial 和其他更加复杂的簇范例化技术更高效。此外，这篇论文还提出了一种可以根据客户端间联系信息不同程度的弹性 Rand-Proj-Spatial 方法，并且在实验中证明其效果。<details>
<summary>Abstract</summary>
We study the problem of communication-efficient distributed vector mean estimation, a commonly used subroutine in distributed optimization and Federated Learning (FL). Rand-$k$ sparsification is a commonly used technique to reduce communication cost, where each client sends $k < d$ of its coordinates to the server. However, Rand-$k$ is agnostic to any correlations, that might exist between clients in practical scenarios. The recently proposed Rand-$k$-Spatial estimator leverages the cross-client correlation information at the server to improve Rand-$k$'s performance. Yet, the performance of Rand-$k$-Spatial is suboptimal. We propose the Rand-Proj-Spatial estimator with a more flexible encoding-decoding procedure, which generalizes the encoding of Rand-$k$ by projecting the client vectors to a random $k$-dimensional subspace. We utilize Subsampled Randomized Hadamard Transform (SRHT) as the projection matrix and show that Rand-Proj-Spatial with SRHT outperforms Rand-$k$-Spatial, using the correlation information more efficiently. Furthermore, we propose an approach to incorporate varying degrees of correlation and suggest a practical variant of Rand-Proj-Spatial when the correlation information is not available to the server. Experiments on real-world distributed optimization tasks showcase the superior performance of Rand-Proj-Spatial compared to Rand-$k$-Spatial and other more sophisticated sparsification techniques.
</details>
<details>
<summary>摘要</summary>
我们研究了一个分布式向量均值估计问题，这是分布式优化和联合学习（FL）中广泛使用的一种子 Routine。 Rand-$k$ 精炼是一种常用的减少通信成本的技术，每个客户端向服务器发送 $k < d$ 个坐标。然而，Rand-$k$ 无法考虑客户端之间的协方差信息，这可能导致性能下降。我们提出了 Rand-$k$-Spatial 估计器，使用服务器端的协方差信息来改进 Rand-$k$ 的性能。然而，Rand-$k$-Spatial 的性能仍然有限制。我们提出了 Rand-Proj-Spatial 估计器，它使用随机 $k$-维空间的投影来扩展 Rand-$k$ 的编码过程。我们使用 Subsampled Randomized Hadamard Transform (SRHT) 作为投影矩阵，并证明 Rand-Proj-Spatial 使用 SRHT 的投影可以更好地利用协方差信息。此外，我们提出了一种根据协方差信息不同程度的变化来修改 Rand-Proj-Spatial 的方法，并建议在服务器端不可获得协方差信息时使用实际 variant。我们在实际分布式优化任务上进行了实验，并证明 Rand-Proj-Spatial 的性能比 Rand-$k$-Spatial 和其他更复杂的精炼技术更高。
</details></li>
</ul>
<hr>
<h2 id="Peer-to-Peer-Deep-Learning-for-Beyond-5G-IoT"><a href="#Peer-to-Peer-Deep-Learning-for-Beyond-5G-IoT" class="headerlink" title="Peer-to-Peer Deep Learning for Beyond-5G IoT"></a>Peer-to-Peer Deep Learning for Beyond-5G IoT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18861">http://arxiv.org/abs/2310.18861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srinivasa Pranav, José M. F. Moura</li>
<li>for: 这个论文是为了解决智能城市等 beyond-5G  computing 环境中的规模问题，而不需要中央服务器或云端协调。</li>
<li>methods: 这个算法使用 max norm synchronization 来驱动训练，保留了设备上的深度模型训练，并使用本地设备之间的通信来实现分布式共识。每个设备会逐次交替进行两个阶段：1）设备上的学习，2）分布式合作，其中它们与附近的设备结合模型参数。</li>
<li>results: 这个算法可以让所有参与设备都 дости得到与 federated 和中央训练相同的测试性能，甚至在 100 个设备和宽松的单器散发加重的情况下。此外，这个算法还可以在不同的网络拓扑、罕见的通信和非Identical 数据分布情况下进行扩展。<details>
<summary>Abstract</summary>
We present P2PL, a practical multi-device peer-to-peer deep learning algorithm that, unlike the federated learning paradigm, does not require coordination from edge servers or the cloud. This makes P2PL well-suited for the sheer scale of beyond-5G computing environments like smart cities that otherwise create range, latency, bandwidth, and single point of failure issues for federated approaches.   P2PL introduces max norm synchronization to catalyze training, retains on-device deep model training to preserve privacy, and leverages local inter-device communication to implement distributed consensus. Each device iteratively alternates between two phases: 1) on-device learning and 2) distributed cooperation where they combine model parameters with nearby devices. We empirically show that all participating devices achieve the same test performance attained by federated and centralized training -- even with 100 devices and relaxed singly stochastic consensus weights. We extend these experimental results to settings with diverse network topologies, sparse and intermittent communication, and non-IID data distributions.
</details>
<details>
<summary>摘要</summary>
我们介绍P2PL，一种实用多设备 peer-to-peer深度学习算法，不同于联邦学习模式，不需要边缘服务器或云端协调。这使得P2PL在 beyond-5G 计算环境中，如智能城市，创造范围、延迟、带宽和单点故障问题，而 federated 方法不适用。P2PL 引入最大范数同步来促进训练，保留设备上深度模型训练，并利用本地设备间通信实现分布式共识。每个设备会逐次 alternate between two 阶段：1）设备上学习和 2）分布式合作，其中 combines 模型参数与附近设备。我们实验表明，参与训练的所有设备可以达到 federated 和中央训练所得到的测试性能，即使有 100 个设备和松弛单调共识加权。我们还将这些实验结果扩展到不同的网络拓扑、笔数和间歇性通信、非标一致数据分布的设置下。
</details></li>
</ul>
<hr>
<h2 id="Bayes-beats-Cross-Validation-Efficient-and-Accurate-Ridge-Regression-via-Expectation-Maximization"><a href="#Bayes-beats-Cross-Validation-Efficient-and-Accurate-Ridge-Regression-via-Expectation-Maximization" class="headerlink" title="Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization"></a>Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18860">http://arxiv.org/abs/2310.18860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu Yu Tew, Mario Boley, Daniel F. Schmidt</li>
<li>for: 本研究提出了一种新的方法，用于调整ridge regression中的正则化参数（λ），它比遗弃一个样本的跨Validation（LOOCV）更快速，同时可以提供与LOOCV risk最小化的拟合参数的同等或更高质量的估计。</li>
<li>methods: 本研究使用了一种 bayesian 的ridge regression形式ulation，通过一个迭代的期望最大化（EM）过程来学习jointly 估计 $\lambda$ 和拟合参数。</li>
<li>results: 研究表明，该方法可以在大 enough $n$ 的情况下，无需设定任何难以确定的 гипер参数，具有唯一最优解，并且在 $O(\min(n, p))$ 操作下实现单一迭代EM循环。此外，研究还发现，通过采用合适的预处理步骤，可以在 $O(n \min(n, p))$ 操作下评估单个 $\lambda$ 值，而不需要评估所有 $l$ 个 candidate $\lambda$ 值。<details>
<summary>Abstract</summary>
We present a novel method for tuning the regularization hyper-parameter, $\lambda$, of a ridge regression that is faster to compute than leave-one-out cross-validation (LOOCV) while yielding estimates of the regression parameters of equal, or particularly in the setting of sparse covariates, superior quality to those obtained by minimising the LOOCV risk. The LOOCV risk can suffer from multiple and bad local minima for finite $n$ and thus requires the specification of a set of candidate $\lambda$, which can fail to provide good solutions. In contrast, we show that the proposed method is guaranteed to find a unique optimal solution for large enough $n$, under relatively mild conditions, without requiring the specification of any difficult to determine hyper-parameters. This is based on a Bayesian formulation of ridge regression that we prove to have a unimodal posterior for large enough $n$, allowing for both the optimal $\lambda$ and the regression coefficients to be jointly learned within an iterative expectation maximization (EM) procedure. Importantly, we show that by utilizing an appropriate preprocessing step, a single iteration of the main EM loop can be implemented in $O(\min(n, p))$ operations, for input data with $n$ rows and $p$ columns. In contrast, evaluating a single value of $\lambda$ using fast LOOCV costs $O(n \min(n, p))$ operations when using the same preprocessing. This advantage amounts to an asymptotic improvement of a factor of $l$ for $l$ candidate values for $\lambda$ (in the regime $q, p \in O(\sqrt{n})$ where $q$ is the number of regression targets).
</details>
<details>
<summary>摘要</summary>
我团队提出了一种新的方法来调整ridge regression中的正则化超参数($\lambda$ )，这种方法比逐个留下一个（LOOCV）更快速计算，而且可以提供与LOOCVrisk相同或更高质量的回归参数估计。LOOCV risk可能会在finite $n$ 下存在多个和坏的地方极 minimum，因此可能需要指定一组 candidate $\lambda$，这可能会导致不良的解决方案。然而，我们证明了该方法在 suficiently large $n$ 下是唯一优化解决方案，不需要指定任何难以确定的超参数。这是基于ridge regression的 Bayesian 表述，我们证明了其 posterior 在 suficiently large $n$ 下是单模的，因此可以通过 iterative expectation maximization (EM) 过程来同时学习 optimal $\lambda$ 和回归系数。其中，我们还证明了可以通过适当的预处理步骤，在 $O(\min(n, p))$ 操作下完成一次主 EM 循环，其中 $n$ 是行数，$p$ 是列数。与此相比，通过快速 LOOCV 评估 $\lambda$ 的值需要 $O(n \min(n, p))$ 操作。这种优势在 $l$ 个 candidate $\lambda$ 值的情况下（在 $q, p \in O(\sqrt{n})$  regime 中）amounts to an asymptotic improvement factor of $l$。
</details></li>
</ul>
<hr>
<h2 id="SiDA-Sparsity-Inspired-Data-Aware-Serving-for-Efficient-and-Scalable-Large-Mixture-of-Experts-Models"><a href="#SiDA-Sparsity-Inspired-Data-Aware-Serving-for-Efficient-and-Scalable-Large-Mixture-of-Experts-Models" class="headerlink" title="SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models"></a>SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18859">http://arxiv.org/abs/2310.18859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixu Du, Shiyu Li, Yuhao Wu, Xiangyu Jiang, Jingwei Sun, Qilin Zheng, Yongkai Wu, Ang Li, Hai “Helen” Li, Yiran Chen</li>
<li>for: 这篇论文的目的是提出一种高效的大型 mixture-of-experts（MoE）模型测试方法，以减少 GPU 内存使用量并提高模型效率。</li>
<li>methods: 这篇论文使用了一种叫做 SiDA（Sparsity-inspired Data-Aware）的方法，它利用了系统的主内存和 GPU 内存，并利用了 MoE 模型中专家活化的内在绝对性来优化模型效率。</li>
<li>results: 根据论文的结果，SiDA 方法可以实现大幅提高 MoE 模型的测试速度，将对 GPU 内存的使用量减少到 1%，同时保持模型效率不变。具体来说，SiDA 方法可以实现 Up to 3.93X 的测试速度增加、Up to 75% 的延迟降低和 Up to 80% 的 GPU 内存储存量减少。<details>
<summary>Abstract</summary>
Mixture-of-Experts (MoE) has emerged as a favorable architecture in the era of large models due to its inherent advantage, i.e., enlarging model capacity without incurring notable computational overhead. Yet, the realization of such benefits often results in ineffective GPU memory utilization, as large portions of the model parameters remain dormant during inference. Moreover, the memory demands of large models consistently outpace the memory capacity of contemporary GPUs. Addressing this, we introduce SiDA (Sparsity-inspired Data-Aware), an efficient inference approach tailored for large MoE models. SiDA judiciously exploits both the system's main memory, which is now abundant and readily scalable, and GPU memory by capitalizing on the inherent sparsity on expert activation in MoE models. By adopting a data-aware perspective, SiDA achieves enhanced model efficiency with a neglectable performance drop. Specifically, SiDA attains a remarkable speedup in MoE inference with up to 3.93X throughput increasing, up to 75% latency reduction, and up to 80% GPU memory saving with down to 1% performance drop. This work paves the way for scalable and efficient deployment of large MoE models, even in memory-constrained systems.
</details>
<details>
<summary>摘要</summary>
大型模型时代，混合专家（MoE）架构已成为有利的选择，因为它可以无需增加显著的计算负担来扩大模型的容量。然而，实现这些利点经常导致大量模型参数在推理过程中处于休眠状态，同时大模型的内存需求常常超过当今的GPU内存容量。为解决这个问题，我们提出了SiDA（基于缺省性的数据意识），这是一种高效的推理方法，专门为大MoE模型设计。SiDA利用系统的主存，这是现在充足且可扩展的，同时还利用GPU内存，通过利用MoE模型中专家活动的自然缺省性来提高模型效率。通过采用数据意识的视角，SiDA实现了更高的模型效率，减少了推理延迟和GPU内存占用，同时保持了模型性能的稳定。具体来说，SiDA在MoE推理中可以达到3.93倍的吞吐量提高、75%的延迟减少和80%的GPU内存减少，同时保持模型性能下降不到1%。这项工作为大MoE模型的扩展和高效部署提供了可行的方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.LG_2023_10_29/" data-id="cloimipcu00r2s488a7n57054" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/eess.IV_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T09:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/eess.IV_2023_10_29/">eess.IV - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Subjective-Quality-Evaluation-of-Point-Clouds-Using-a-Head-Mounted-Display"><a href="#Subjective-Quality-Evaluation-of-Point-Clouds-Using-a-Head-Mounted-Display" class="headerlink" title="Subjective Quality Evaluation of Point Clouds Using a Head Mounted Display"></a>Subjective Quality Evaluation of Point Clouds Using a Head Mounted Display</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19179">http://arxiv.org/abs/2310.19179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joao Prazeres, Rafael Rodrigues, Manuela Pereira, Antonio M. G. Pinheiro</li>
<li>for: 这个论文报告了对静止点云编码器MPEG V-PCC、G-PCC、深度学习编码器RS-DLPCC以及受欢迎的Draco编码器的主观质量评估。</li>
<li>methods: 该论文使用了18名参与者通过头戴式显示器直接比较了3D表示的扭曲点云的视觉效果，并对所获得的主观评分（MOS）与之前两项研究中对同一内容的视觉效果进行了比较，包括潘森相关指数、斯宾塞排名相关指数、平均方差和外围异常指数。</li>
<li>results: 结果表明这三项研究之间存在高度相关性，并且对所有评估中的差异没有发现任何显著差异。<details>
<summary>Abstract</summary>
This paper reports on a subjective quality evaluation of static point clouds encoded with the MPEG codecs V-PCC and G-PCC, the deep learning-based codec RS-DLPCC, and the popular Draco codec. 18 subjects visualized 3D representations of distorted point clouds using a Head Mounted Display, which allowed for a direct comparison with their reference. The Mean Opinion Scores (MOS) obtained in this subjective evaluation were compared with the MOS from two previous studies, where the same content was visualized either on a 2D display or a 3D stereoscopic display, through the Pearson Correlation, Spearman Rank Order Correlation, Root Mean Square Error, and the Outlier Ratio. The results indicate that the three studies are highly correlated with one another. Moreover, a statistical analysis between all evaluations showed no significant differences between them.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)这篇论文报道了一项主观质量评估，涉及到静止点云编码器MPEG codecs V-PCC和G-PCC、深度学习基于的RS-DLPCC编码器以及受欢迎的Draco编码器。18名参与者通过头戴式显示器 visualized 3D表示distorted点云，可以直接与参考进行比较。获得的主观意见分（MOS）在这个主观评估中被与之前两个研究相比较，这两个研究分别使用2D显示器和3D立体显示器显示同一内容。通过皮尔逊相关度、Spearman排序相关度、平均方差误差和异常比率进行比较。结果表明这三个研究之间存在高度相关性，并且在所有评估中没有发现显著差异。
</details></li>
</ul>
<hr>
<h2 id="Transport-of-Intensity-Model-for-Single-Mask-X-ray-Differential-Phase-Contrast-Imaging"><a href="#Transport-of-Intensity-Model-for-Single-Mask-X-ray-Differential-Phase-Contrast-Imaging" class="headerlink" title="Transport-of-Intensity Model for Single-Mask X-ray Differential Phase Contrast Imaging"></a>Transport-of-Intensity Model for Single-Mask X-ray Differential Phase Contrast Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19087">http://arxiv.org/abs/2310.19087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingcheng Yuan, Mini Das</li>
<li>for: 该研究旨在提高软组织和肿瘤的可见度，使用X射线阶段差图像技术。</li>
<li>methods: 该研究提出了一种基于运输Intensity公式的单面phas imaging系统模型，以提供图像形成过程的直观理解。此外，该研究还展示了使用单一的干扰图像来 Retrieval attenuation和分别阶段差信息，不需要spectral信息或探测器&#x2F;Mask移动。</li>
<li>results: 该研究通过实验和Monte Carlo仿真示出了模型的有效性和提议的Retrieval方法。该模型超越了现有模型的限制，提供了直观的图像形成过程的视觉化，同时允许优化分别阶段差投影 geometries，进一步提高了实际应用的可行性。<details>
<summary>Abstract</summary>
X-ray phase contrast imaging has emerged as a promising technique for enhancing contrast and visibility of light-element materials, including soft tissues and tumors. In this paper, we propose a novel model for a single-mask phase imaging system based on the transport-of-intensity equation. Our model offers an intuitive understanding of signal and contrast formation in single-mask phase imaging systems. We also demonstrate efficient retrieval of attenuation and differential phase contrast with just one intensity image without requiring spectral information or mask/detector movement. The model validity as well as the proposed retrieval method is demonstrated via both experimental results on a system developed in-house as well as with Monte Carlo simulations. Our proposed model overcomes the limitations of existing models by providing an intuitive visualization of the image formation process. It also allows optimizing differential phase imaging geometries for practical applications, further enhancing broader applicability. Furthermore, the general methodology described herein offers insight on deriving transport-of-intensity models for novel X-ray imaging systems with periodic structures in the beam path.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/eess.IV_2023_10_29/" data-id="cloimipij016ws488cl7bgn4r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/eess.SP_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T08:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/eess.SP_2023_10_29/">eess.SP - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Optical-STAR-RIS-Aided-VLC-Systems-RSMA-Versus-NOMA"><a href="#Optical-STAR-RIS-Aided-VLC-Systems-RSMA-Versus-NOMA" class="headerlink" title="Optical STAR-RIS-Aided VLC Systems: RSMA Versus NOMA"></a>Optical STAR-RIS-Aided VLC Systems: RSMA Versus NOMA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19141">http://arxiv.org/abs/2310.19141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Maraqa, Sylvester Aboagye, Telex M. N. Ngatched<br>for: This paper aims to study the performance of optical simultaneous transmission and reflection reconfigurable intelligent surface (OSTAR-RIS) in a multi-user indoor visible light communication (VLC) system.methods: The proposed system uses a novel multi-user indoor VLC system assisted by OSTAR-RIS, which employs both power-domain non-orthogonal multiple access (NOMA) and rate splitting multiple access (RSMA) to improve the sum rate performance. The roll and yaw angles of the reflector elements, as well as the refractive index of the refractor elements in OSTAR-RIS, are jointly optimized using a sum rate maximization problem.results: The simulation results show that the proposed OSTAR-RIS RSMA-aided VLC system outperforms the OSTAR-RIS NOMA-based VLC system in terms of both the sum rate and the sum energy efficiency.<details>
<summary>Abstract</summary>
A critical concern within the realm of visible light communications (VLC) pertains to enhancing system data rate, particularly in scenarios where the direct line-of-sight (LoS) connection is obstructed by obstacles. The deployment of meta-surface-based simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) has emerged to combat challenging LoS blockage scenarios and to provide 360 coverage in radio-frequency wireless systems. Recently, the concept of optical simultaneous transmission and reflection reconfigurable intelligent surface (OSTAR-RIS) has been promoted for VLC systems. This work is dedicated to studying the performance of OSTAR-RIS in detail and unveiling the VLC system performance gain under such technology. Specifically, we propose a novel multi-user indoor VLC system that is assisted by OSTAR-RIS. To improve the sum rate performance of the proposed system, both power-domain non-orthogonal multiple access (NOMA) and rate splitting multiple access (RSMA) are investigated in this work. To realize this, a sum rate maximization problem that jointly optimizes the roll and yaw angles of the reflector elements as well as the refractive index of the refractor elements in OSTAR-RIS is formulated, solved, and evaluated. The maximization problem takes into account practical considerations, such as the presence of non-users (i.e., blockers) and the orientation of the recipient's device. The sine-cosine meta-heuristic algorithm is employed to get the optimal solution of the formulated non-convex optimization problem. Moreover, the study delves into the sum energy efficiency optimization of the proposed system. Simulation results indicate that the proposed OSTAR-RIS RSMA-aided VLC system outperforms the OSTAR-RIS NOMA-based VLC system in terms of both the sum rate and the sum energy efficiency.
</details>
<details>
<summary>摘要</summary>
Visible light communication (VLC) 的一个关键问题是提高系统数据率，特别是在直线视线 (LoS) 连接被障碍物阻挡时。Meta-surface-based simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) 的部署已经在这些场景中提供了一种解决方案，并提供了360度的覆盖。在这些系统中，optical simultaneous transmission and reflection reconfigurable intelligent surface (OSTAR-RIS) 的概念也在提出。本研究的目的是研究OSTAR-RIS的性能，探讨VLC系统在这种技术下的性能提升。我们提出了一种基于OSTAR-RIS的多用户indoor VLC系统，并使用了power-domain non-orthogonal multiple access (NOMA) 和 rate splitting multiple access (RSMA) 来提高系统的总率性能。为此，我们提出了一个总率最大化问题，该问题jointly优化了OSTAR-RIS中 reflector元素的扭积角和折射率，以及recipient的设备方向。该问题考虑了实际因素，例如阻挡物 (i.e., 堵塞) 和recipient的设备方向。我们使用了sine-cosine meta-heuristic algorithm来获得优化问题的解。此外，我们还研究了系统的总能效率优化。实验结果表明，我们的OSTAR-RIS RSMA-aided VLC系统在总率和总能效率方面都超过了OSTAR-RIS NOMA-based VLC系统。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/eess.SP_2023_10_29/" data-id="cloimipjv01acs4889cprbryu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/28/cs.CV_2023_10_28/" class="article-date">
  <time datetime="2023-10-28T13:00:00.000Z" itemprop="datePublished">2023-10-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/28/cs.CV_2023_10_28/">cs.CV - 2023-10-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Learning-based-Compressed-Domain-Multimedia-for-Man-and-Machine-A-Taxonomy-and-Application-to-Point-Cloud-Classification"><a href="#Deep-Learning-based-Compressed-Domain-Multimedia-for-Man-and-Machine-A-Taxonomy-and-Application-to-Point-Cloud-Classification" class="headerlink" title="Deep Learning-based Compressed Domain Multimedia for Man and Machine: A Taxonomy and Application to Point Cloud Classification"></a>Deep Learning-based Compressed Domain Multimedia for Man and Machine: A Taxonomy and Application to Point Cloud Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18849">http://arxiv.org/abs/2310.18849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelrahman Seleem, André F. R. Guarda, Nuno M. M. Rodrigues, Fernando Pereira</li>
<li>for: 本研究旨在提出一种基于深度学习的图像和视频处理技术，以提高计算机视觉任务的性能和减少计算复杂度。</li>
<li>methods: 该研究使用了深度学习来提取图像和视频数据中的特征，并使用了一种新的稳定性分析方法来评估不同的图像和视频处理算法。</li>
<li>results: 研究结果显示，使用了基于深度学习的图像和视频处理算法可以大幅提高计算机视觉任务的性能，同时减少计算复杂度。此外，研究还发现了一些新的图像和视频处理算法，可以在不同的应用场景中得到优秀的效果。<details>
<summary>Abstract</summary>
In the current golden age of multimedia, human visualization is no longer the single main target, with the final consumer often being a machine which performs some processing or computer vision tasks. In both cases, deep learning plays a undamental role in extracting features from the multimedia representation data, usually producing a compressed representation referred to as latent representation. The increasing development and adoption of deep learning-based solutions in a wide area of multimedia applications have opened an exciting new vision where a common compressed multimedia representation is used for both man and machine. The main benefits of this vision are two-fold: i) improved performance for the computer vision tasks, since the effects of coding artifacts are mitigated; and ii) reduced computational complexity, since prior decoding is not required. This paper proposes the first taxonomy for designing compressed domain computer vision solutions driven by the architecture and weights compatibility with an available spatio-temporal computer vision processor. The potential of the proposed taxonomy is demonstrated for the specific case of point cloud classification by designing novel compressed domain processors using the JPEG Pleno Point Cloud Coding standard under development and adaptations of the PointGrid classifier. Experimental results show that the designed compressed domain point cloud classification solutions can significantly outperform the spatial-temporal domain classification benchmarks when applied to the decompressed data, containing coding artifacts, and even surpass their performance when applied to the original uncompressed data.
</details>
<details>
<summary>摘要</summary>
在当今的金色 Multimedia 时代，人类视觉不再是唯一的主要目标，最终consumer  часто是一个机器，执行一些处理或计算机视觉任务。在这两种情况下，深度学习在抽取 Multimedia 表示数据中的特征方面发挥了关键作用，通常生成一个压缩表示 referred to as 封顶表示。随着深度学习基于解决方案在各种 Multimedia 应用领域的开发和采用，开启了一个新的视野，在这个视野中，一个通用的压缩 Multimedia 表示被用于人类和机器。这个视野的主要优点有两个方面：一是提高计算机视觉任务的性能，因为编码artifacts的影响被减少; 二是降低计算复杂性，因为不需要先 decode。这篇文章提出了首个设计压缩领域计算机视觉解决方案的taxonomy，该taxonomy基于可用的空间temporal计算机视觉处理器的建立和重量相容性。特点的实验结果表明，通过设计 novel 压缩领域处理器，使用 JPEG Pleno Point Cloud Coding 标准在开发中和PointGrid分类器的改进，可以在压缩领域实现显著的点云分类性能提升，并在应用到解码后的数据中 even surpass 其性能。
</details></li>
</ul>
<hr>
<h2 id="INCODE-Implicit-Neural-Conditioning-with-Prior-Knowledge-Embeddings"><a href="#INCODE-Implicit-Neural-Conditioning-with-Prior-Knowledge-Embeddings" class="headerlink" title="INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings"></a>INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18846">http://arxiv.org/abs/2310.18846</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmindflow/INCODE">https://github.com/xmindflow/INCODE</a></li>
<li>paper_authors: Amirhossein Kazerouni, Reza Azad, Alireza Hosseini, Dorit Merhof, Ulas Bagci</li>
<li>for: 提高信号表示的精度和灵活性，解决现有INR的细节捕捉和鲁棒性问题</li>
<li>methods: 利用深度先验知识调整抽象函数的参数，并通过任务特定预训练模型进行任务特定参数调整，以优化表示过程</li>
<li>results: 在多种信号表示任务上具有更高的精度、质量、灵活性和速度，并能够解决复杂的音频、图像、3D形状重建、NeRFs、反问题等任务，并且在各种难题上具有优于现有INR的表现<details>
<summary>Abstract</summary>
Implicit Neural Representations (INRs) have revolutionized signal representation by leveraging neural networks to provide continuous and smooth representations of complex data. However, existing INRs face limitations in capturing fine-grained details, handling noise, and adapting to diverse signal types. To address these challenges, we introduce INCODE, a novel approach that enhances the control of the sinusoidal-based activation function in INRs using deep prior knowledge. INCODE comprises a harmonizer network and a composer network, where the harmonizer network dynamically adjusts key parameters of the activation function. Through a task-specific pre-trained model, INCODE adapts the task-specific parameters to optimize the representation process. Our approach not only excels in representation, but also extends its prowess to tackle complex tasks such as audio, image, and 3D shape reconstructions, as well as intricate challenges such as neural radiance fields (NeRFs), and inverse problems, including denoising, super-resolution, inpainting, and CT reconstruction. Through comprehensive experiments, INCODE demonstrates its superiority in terms of robustness, accuracy, quality, and convergence rate, broadening the scope of signal representation. Please visit the project's website for details on the proposed method and access to the code.
</details>
<details>
<summary>摘要</summary>
归一神经表示（INR）已经革命化了信号表示方法，通过利用神经网络提供连续和平滑的数据表示方式。然而，现有INR受到细节capturing、鲁棒性和多样化信号类型的限制。为解决这些挑战，我们介绍了INCODE，一种新的方法，它使用深度先验知识来强化神经征函数的控制。INCODE包括一个和 composer网络，其中和网络动态调整 activation 函数的关键参数。通过一个任务特定的预训练模型，INCODE可以将任务特定的参数适应优化表示过程。我们的方法不仅在表示方面卓越，还能够扩展到复杂的任务，如音频、图像、三维形状重建、神经辐射场（NeRF）、反问题（包括噪声、超分解、缺失、CT重建）等。通过全面的实验，INCODE在稳定性、准确性、质量和收敛率方面表现出优异，扩大信号表示的范围。请参考项目网站了解提出的方法和获取代码。
</details></li>
</ul>
<hr>
<h2 id="Customizing-360-Degree-Panoramas-through-Text-to-Image-Diffusion-Models"><a href="#Customizing-360-Degree-Panoramas-through-Text-to-Image-Diffusion-Models" class="headerlink" title="Customizing 360-Degree Panoramas through Text-to-Image Diffusion Models"></a>Customizing 360-Degree Panoramas through Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18840">http://arxiv.org/abs/2310.18840</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/littlewhitesea/stitchdiffusion">https://github.com/littlewhitesea/stitchdiffusion</a></li>
<li>paper_authors: Hai Wang, Xiaoyu Xiang, Yuchen Fan, Jing-Hao Xue</li>
<li>for: 本研究旨在提出一种基于diffusion模型的个性化文本到图像（T2I）合成方法，用于自适应地生成360度全景图像。</li>
<li>methods: 我们首先为这项任务提前抽象了一个预训练的T2I扩散模型，然后使用LoRA进行精度调整。然而，这些调整并不能保证左右两侧图像的连续性，这是360度全景图像的重要特征。因此，我们提出了StitchDiffusion方法，包括在拼接块中进行预除噪音处理，以及应用全局剪辑来生成无缝360度全景图像。</li>
<li>results: 我们的自定义模型，加上我们提出的StitchDiffusion方法，可以生成高质量的360度全景图像。此外，我们的自定义模型在生成未在训练数据集中看到的场景时表现出了异常的泛化能力。<details>
<summary>Abstract</summary>
Personalized text-to-image (T2I) synthesis based on diffusion models has attracted significant attention in recent research. However, existing methods primarily concentrate on customizing subjects or styles, neglecting the exploration of global geometry. In this study, we propose an approach that focuses on the customization of 360-degree panoramas, which inherently possess global geometric properties, using a T2I diffusion model. To achieve this, we curate a paired image-text dataset specifically designed for the task and subsequently employ it to fine-tune a pre-trained T2I diffusion model with LoRA. Nevertheless, the fine-tuned model alone does not ensure the continuity between the leftmost and rightmost sides of the synthesized images, a crucial characteristic of 360-degree panoramas. To address this issue, we propose a method called StitchDiffusion. Specifically, we perform pre-denoising operations twice at each time step of the denoising process on the stitch block consisting of the leftmost and rightmost image regions. Furthermore, a global cropping is adopted to synthesize seamless 360-degree panoramas. Experimental results demonstrate the effectiveness of our customized model combined with the proposed StitchDiffusion in generating high-quality 360-degree panoramic images. Moreover, our customized model exhibits exceptional generalization ability in producing scenes unseen in the fine-tuning dataset. Code is available at https://github.com/littlewhitesea/StitchDiffusion.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>研究中的个性化文本到图像（T2I）合成技术受到了非常重视。然而，现有的方法主要集中在主题或风格方面，忽略了全球几何特性的探索。在这种研究中，我们提出了一种方法，该方法是通过T2I扩散模型进行个性化360度全景图像的合成。为此，我们制作了特定于任务的图像文本对集，然后使用它来练化一个预训练的T2I扩散模型。然而，练化后的模型本身不能保证左侧和右侧图像的连续性，这是360度全景图像的关键特征。为解决这个问题，我们提出了一种方法called StitchDiffusion。具体来说，我们在每个时间步中对固定块进行预处理操作两次，并采用全球裁剪来合成无缝360度全景图像。实验结果表明，我们的定制模型与StitchDiffusion结合可以生成高质量的360度全景图像。此外，我们的定制模型还表现出了非常好的泛化能力，可以生成未在练化数据集中出现的场景。代码可以在https://github.com/littlewhitesea/StitchDiffusion上找到。
</details></li>
</ul>
<hr>
<h2 id="UniCat-Crafting-a-Stronger-Fusion-Baseline-for-Multimodal-Re-Identification"><a href="#UniCat-Crafting-a-Stronger-Fusion-Baseline-for-Multimodal-Re-Identification" class="headerlink" title="UniCat: Crafting a Stronger Fusion Baseline for Multimodal Re-Identification"></a>UniCat: Crafting a Stronger Fusion Baseline for Multimodal Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18812">http://arxiv.org/abs/2310.18812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jennifer Crawford, Haoli Yin, Luke McDermott, Daniel Cummings</li>
<li>for: 这个论文目标是为了解决多模态重识别任务中的批量化问题，提高对多种数据流的重识别能力。</li>
<li>methods: 该论文使用了多种方法，包括单模态和多模态方法，以及各种拼接和融合策略。</li>
<li>results: 研究发现，使用单模态方法可以获得更好的表示，而不是使用多模态方法。此外，使用不同的拼接和融合策略也可以提高表示的质量。<details>
<summary>Abstract</summary>
Multimodal Re-Identification (ReID) is a popular retrieval task that aims to re-identify objects across diverse data streams, prompting many researchers to integrate multiple modalities into a unified representation. While such fusion promises a holistic view, our investigations shed light on potential pitfalls. We uncover that prevailing late-fusion techniques often produce suboptimal latent representations when compared to methods that train modalities in isolation. We argue that this effect is largely due to the inadvertent relaxation of the training objectives on individual modalities when using fusion, what others have termed modality laziness. We present a nuanced point-of-view that this relaxation can lead to certain modalities failing to fully harness available task-relevant information, and yet, offers a protective veil to noisy modalities, preventing them from overfitting to task-irrelevant data. Our findings also show that unimodal concatenation (UniCat) and other late-fusion ensembling of unimodal backbones, when paired with best-known training techniques, exceed the current state-of-the-art performance across several multimodal ReID benchmarks. By unveiling the double-edged sword of "modality laziness", we motivate future research in balancing local modality strengths with global representations.
</details>
<details>
<summary>摘要</summary>
多模态重识别（ReID）是一个广泛应用的检索任务，旨在透过多种数据流处理对象的重识别，引起了许多研究人员将多种模式融合到一个统一表示中。然而，我们的调查发现，使用合并技术时常会产生优化后的下降，相比于单独训练模式时的表示。我们认为这是由于将多个模式融合时，不小心放弃各个模式的训练目标，导致模式懒散（modality laziness）的问题。我们提出一种复杂的观点，认为这种放弃可以使某些模式在任务相关的信息上充分发挥作用，同时防止不相关的数据泛染。我们的发现还表明，将单模式 concatenation（UniCat）和其他融合技术与最佳训练技术结合，可以在多模态ReIDbenchmark上超越当前状态的表现。我们的研究揭示了“模式懒散”的双重剑，激励未来的研究人员在地方模式强大性和全球表示之间寻求平衡。
</details></li>
</ul>
<hr>
<h2 id="A-Review-on-the-Applications-of-Machine-Learning-for-Tinnitus-Diagnosis-Using-EEG-Signals"><a href="#A-Review-on-the-Applications-of-Machine-Learning-for-Tinnitus-Diagnosis-Using-EEG-Signals" class="headerlink" title="A Review on the Applications of Machine Learning for Tinnitus Diagnosis Using EEG Signals"></a>A Review on the Applications of Machine Learning for Tinnitus Diagnosis Using EEG Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18795">http://arxiv.org/abs/2310.18795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzaneh Ramezani, Hamidreza Bolhasani</li>
<li>for: 这个研究的目的是使用机器学习技术来识别或预测听力障碍患者，以便早期诊断和治疗。</li>
<li>methods: 这些研究使用了多种数据模式和机器学习技术来识别和分类听力障碍患者。</li>
<li>results: 这些研究的结果表明，使用EEG信号作为输入数据，可以准确地识别和预测听力障碍患者。但是，研究结果存在差异和矛盾，需要进一步的研究以更好地理解听力障碍的特征和预测方法。<details>
<summary>Abstract</summary>
Tinnitus is a prevalent hearing disorder that can be caused by various factors such as age, hearing loss, exposure to loud noises, ear infections or tumors, certain medications, head or neck injuries, and psychological conditions like anxiety and depression. While not every patient requires medical attention, about 20% of sufferers seek clinical intervention. Early diagnosis is crucial for effective treatment. New developments have been made in tinnitus detection to aid in early detection of this illness. Over the past few years, there has been a notable growth in the usage of electroencephalography (EEG) to study variations in oscillatory brain activity related to tinnitus. However, the results obtained from numerous studies vary greatly, leading to conflicting conclusions. Currently, clinicians rely solely on their expertise to identify individuals with tinnitus. Researchers in this field have incorporated various data modalities and machine-learning techniques to aid clinicians in identifying tinnitus characteristics and classifying people with tinnitus. The purpose of writing this article is to review articles that focus on using machine learning (ML) to identify or predict tinnitus patients using EEG signals as input data. We have evaluated 11 articles published between 2016 and 2023 using a systematic literature review (SLR) method. This article arranges perfect summaries of all the research reviewed and compares the significant aspects of each. Additionally, we performed statistical analyses to gain a deeper comprehension of the most recent research in this area. Almost all of the reviewed articles followed a five-step procedure to achieve the goal of tinnitus. Disclosure. Finally, we discuss the open affairs and challenges in this method of tinnitus recognition or prediction and suggest future directions for research.
</details>
<details>
<summary>摘要</summary>
听力障碍（tinnitus）是一种非常普遍的听力疾病，可以由年龄、听力损伤、高音响应、耳感染或肿瘤、某些药物、头或Neck伤等多种因素引起。虽然不是所有患者需要医疗干预，但约20%的患者会寻求临床 intervención。早期诊断非常重要，以便有效的治疗。在过去几年中，对听力障碍检测方法的新发展带来了一定的进步。通过使用电enzephalography（EEG）研究听力障碍相关的脑动力学特征，已经有了一定的进步。然而，这些研究的结果很多样化，导致了不一致的结论。目前，临床医生仅仅靠自己的专业知识来诊断听力障碍。研究人员在这一领域已经结合了不同的数据模式和机器学习技术，以帮助临床医生识别听力障碍特征并将患者分类。本文的目的是对使用机器学习（ML）识别或预测听力障碍患者的研究进行系统性的文献综述。我们对2016年至2023年间发表的11篇文章进行了系统性的文献综述，并对每篇文章进行了精确的摘要。此外，我们还进行了统计分析，以更深入地了解最近的研究发展。大多数复习的文章遵循了五步程序来实现听力障碍识别或预测的目标。最后，我们讨论了这一方法的开放问题和挑战，并建议未来的研究方向。
</details></li>
</ul>
<hr>
<h2 id="PrObeD-Proactive-Object-Detection-Wrapper"><a href="#PrObeD-Proactive-Object-Detection-Wrapper" class="headerlink" title="PrObeD: Proactive Object Detection Wrapper"></a>PrObeD: Proactive Object Detection Wrapper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18788">http://arxiv.org/abs/2310.18788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishal Asnani, Abhinav Kumar, Suya You, Xiaoming Liu</li>
<li>For: 提高$2D$物体检测的性能，使其能够更好地检测普通和掩蔽的图像中的物体。* Methods: 基于 wrapper 的扩展方法 PrObeD，包括一个编码器-解码器架构，通过生成图像依赖的信号（模板）来加密输入图像，并通过解码器从Encrypted images中提取这个模板。* Results: 对 MS-COCO、CAMO、COD$10$K 和 NC$4$K 数据集进行了实验，并在不同的检测器上显示了提高的检测性能。<details>
<summary>Abstract</summary>
Previous research in $2D$ object detection focuses on various tasks, including detecting objects in generic and camouflaged images. These works are regarded as passive works for object detection as they take the input image as is. However, convergence to global minima is not guaranteed to be optimal in neural networks; therefore, we argue that the trained weights in the object detector are not optimal. To rectify this problem, we propose a wrapper based on proactive schemes, PrObeD, which enhances the performance of these object detectors by learning a signal. PrObeD consists of an encoder-decoder architecture, where the encoder network generates an image-dependent signal termed templates to encrypt the input images, and the decoder recovers this template from the encrypted images. We propose that learning the optimum template results in an object detector with an improved detection performance. The template acts as a mask to the input images to highlight semantics useful for the object detector. Finetuning the object detector with these encrypted images enhances the detection performance for both generic and camouflaged. Our experiments on MS-COCO, CAMO, COD$10$K, and NC$4$K datasets show improvement over different detectors after applying PrObeD. Our models/codes are available at https://github.com/vishal3477/Proactive-Object-Detection.
</details>
<details>
<summary>摘要</summary>
PrObeD consists of an encoder-decoder architecture, where the encoder network generates an image-dependent signal called templates to encrypt the input images, and the decoder recovers this template from the encrypted images. We believe that learning the optimum template results in an object detector with improved detection performance. The template acts as a mask to the input images, highlighting semantics that are useful for the object detector. Finetuning the object detector with these encrypted images improves the detection performance for both generic and camouflaged objects.Our experiments on the MS-COCO, CAMO, COD$10$K, and NC$4$K datasets show that PrObeD improves the detection performance of different object detectors. Our models and codes are available at https://github.com/vishal3477/Proactive-Object-Detection.Simplified Chinese translation:前一些研究主要关注在二维对象检测中的不同任务，包括检测通用和涂抹图像中的对象。这些工作被视为通过对输入图像进行修改来实现对象检测的被动方法。然而，神经网络中的学习结果可能并不是最优的，因此我们认为这些学习结果可能并不是最优的。为了解决这个问题，我们提出了一种基于主动方法的包装器，称为PrObeD，它可以提高对象检测器的性能。PrObeD包括一个编码器-解码器架构，其中编码器网络生成一个图像具有依赖关系的信号，称为模板，并将这个模板用于对输入图像进行加密。解码器则可以从加密后的图像中提取出这个模板。我们认为，学习最优的模板可以提高对象检测器的检测性能。模板可以视为对输入图像进行修饰，使对象检测器更容易察见用于检测的 semantics。通过在这些加密图像上进行训练，可以提高对象检测器的检测性能，包括通用和涂抹图像中的对象。我们在 MS-COCO、CAMO、COD$10$K 和 NC$4$K 数据集上进行了实验，结果显示 PrObeD 可以提高不同的对象检测器的检测性能。我们的模型和代码可以在 https://github.com/vishal3477/Proactive-Object-Detection 上获取。
</details></li>
</ul>
<hr>
<h2 id="CityRefer-Geography-aware-3D-Visual-Grounding-Dataset-on-City-scale-Point-Cloud-Data"><a href="#CityRefer-Geography-aware-3D-Visual-Grounding-Dataset-on-City-scale-Point-Cloud-Data" class="headerlink" title="CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data"></a>CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18773">http://arxiv.org/abs/2310.18773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atr-dbi/cityrefer">https://github.com/atr-dbi/cityrefer</a></li>
<li>paper_authors: Taiki Miyanishi, Fumiya Kitamori, Shuhei Kurita, Jungdae Lee, Motoaki Kawanabe, Nakamasa Inoue</li>
<li>for: 城市级3D点云数据是用于表示细节和复杂的户外结构的有前途的方式，可以用于吸引人的应用，如自适应导航和无人机。</li>
<li>methods: 我们引入了CityRefer数据集，其包含35k个自然语言描述和5k个地标标签，以及与OpenStreetMap的同步。我们还开发了基线系统，可以学习编码语言描述、3D物体实例和城市的地标信息，以实现视Grounding。</li>
<li>results: 据我们知道，CityRefer数据集是当前最大的城市级视Grounding数据集，用于本地化特定3D对象。<details>
<summary>Abstract</summary>
City-scale 3D point cloud is a promising way to express detailed and complicated outdoor structures. It encompasses both the appearance and geometry features of segmented city components, including cars, streets, and buildings, that can be utilized for attractive applications such as user-interactive navigation of autonomous vehicles and drones. However, compared to the extensive text annotations available for images and indoor scenes, the scarcity of text annotations for outdoor scenes poses a significant challenge for achieving these applications. To tackle this problem, we introduce the CityRefer dataset for city-level visual grounding. The dataset consists of 35k natural language descriptions of 3D objects appearing in SensatUrban city scenes and 5k landmarks labels synchronizing with OpenStreetMap. To ensure the quality and accuracy of the dataset, all descriptions and labels in the CityRefer dataset are manually verified. We also have developed a baseline system that can learn encoded language descriptions, 3D object instances, and geographical information about the city's landmarks to perform visual grounding on the CityRefer dataset. To the best of our knowledge, the CityRefer dataset is the largest city-level visual grounding dataset for localizing specific 3D objects.
</details>
<details>
<summary>摘要</summary>
城市级3D点云是一种有前途的方式表达细节和复杂的户外结构。它包括城市组成部分的外观和几何特征，包括汽车、街道和建筑物，可以用于有吸引力的应用，如用户交互导航自动汽车和无人机。然而，相比于图像和室内场景的广泛文本注释，户外场景的文本注释的缺乏对市场具有显著的挑战，以实现这些应用。为解决这个问题，我们介绍了城市参照数据集（CityRefer），该数据集包含35000个自然语言描述3D объек在敏捷城市场景中出现的场景和5000个地标标签，与OpenStreetMap相匹配。为保证数据集的质量和准确性，所有的描述和标签在CityRefer数据集中都是手动验证的。我们还开发了一个基eline系统，可以学习编码的自然语言描述、3D объек实例和城市的地标信息，以在CityRefer数据集上进行视觉定位。据我们所知，CityRefer数据集是当前最大的城市级视觉定位数据集，用于特定3D对象的本地化。
</details></li>
</ul>
<hr>
<h2 id="Online-Multi-view-Anomaly-Detection-with-Disentangled-Product-of-Experts-Modeling"><a href="#Online-Multi-view-Anomaly-Detection-with-Disentangled-Product-of-Experts-Modeling" class="headerlink" title="Online Multi-view Anomaly Detection with Disentangled Product-of-Experts Modeling"></a>Online Multi-view Anomaly Detection with Disentangled Product-of-Experts Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18728">http://arxiv.org/abs/2310.18728</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cshaowang/dPoE">https://github.com/cshaowang/dPoE</a></li>
<li>paper_authors: Hao Wang, Zhi-Qi Cheng, Jingdong Sun, Xin Yang, Xiao Wu, Hongyang Chen, Yan Yang</li>
<li>for: 本研究的目的是提出一种能够处理多视图数据的异常检测方法，以解决现有方法中的一些缺陷，如只适用于两个视图或特定类型异常等。</li>
<li>methods: 本研究使用了多视图学习、分解表示学习和生成模型等方法，其中包括一个Product-of-Experts（PoE）层、一个Total Correction（TC）推定器和一个联合损失函数等。</li>
<li>results: 经过广泛的实验测试，提出的dPoE模型在六个真实世界数据集上表现出色，舒过基elines明显。<details>
<summary>Abstract</summary>
Multi-view or even multi-modal data is appealing yet challenging for real-world applications. Detecting anomalies in multi-view data is a prominent recent research topic. However, most of the existing methods 1) are only suitable for two views or type-specific anomalies, 2) suffer from the issue of fusion disentanglement, and 3) do not support online detection after model deployment. To address these challenges, our main ideas in this paper are three-fold: multi-view learning, disentangled representation learning, and generative model. To this end, we propose dPoE, a novel multi-view variational autoencoder model that involves (1) a Product-of-Experts (PoE) layer in tackling multi-view data, (2) a Total Correction (TC) discriminator in disentangling view-common and view-specific representations, and (3) a joint loss function in wrapping up all components. In addition, we devise theoretical information bounds to control both view-common and view-specific representations. Extensive experiments on six real-world datasets demonstrate that the proposed dPoE outperforms baselines markedly.
</details>
<details>
<summary>摘要</summary>
多视图或多模式数据吸引了现实应用的研究者，但是检测多视图数据中异常现象是一个挑战。现有的大多数方法1）只适用于两个视图或类型特定异常，2）受混合解决问题的影响，3）无法在模型部署后进行在线检测。为解决这些挑战，我们的主要想法是三重：多视图学习、分解表示学习和生成模型。为此，我们提出了dPoE，一种新的多视图变量自适应器模型，它包括（1）一个Product-of-Experts（PoE）层来处理多视图数据，（2）一个总正确（TC）推分器来分解视图共同和视图特定表示，以及（3）一个联合损失函数来包装所有组件。此外，我们设计了理论信息约束来控制视图共同和视图特定表示。广泛的实验证明了我们提出的dPoE明显超过基eline。
</details></li>
</ul>
<hr>
<h2 id="Audio-Visual-Instance-Segmentation"><a href="#Audio-Visual-Instance-Segmentation" class="headerlink" title="Audio-Visual Instance Segmentation"></a>Audio-Visual Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18709">http://arxiv.org/abs/2310.18709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruohao Guo, Yaru Chen, Yanyu Qi, Wenzhen Yue, Dantong Niu, Xianghua Ying</li>
<li>for: 这个论文目标是提出一种新的多模态任务，即音频视频实例分割（AVIS），目的是同时在可见视频中识别、分割和跟踪各种声音实例。</li>
<li>methods: 该论文使用了一种简单的基础模型，其中添加了一个音频分支和一个跨模态融合模块，以使用Mask2Former来找到所有声音实例。</li>
<li>results: 该论文使用两种脊梁进行评估，并得到了在AVISeg上的较好的性能。作者认为，AVIS将激励社区尝试更加全面的多模态理解。<details>
<summary>Abstract</summary>
In this paper, we propose a new multi-modal task, namely audio-visual instance segmentation (AVIS), in which the goal is to identify, segment, and track individual sounding object instances in audible videos, simultaneously. To our knowledge, it is the first time that instance segmentation has been extended into the audio-visual domain. To better facilitate this research, we construct the first audio-visual instance segmentation benchmark (AVISeg). Specifically, AVISeg consists of 1,258 videos with an average duration of 62.6 seconds from YouTube and public audio-visual datasets, where 117 videos have been annotated by using an interactive semi-automatic labeling tool based on the Segment Anything Model (SAM). In addition, we present a simple baseline model for the AVIS task. Our new model introduces an audio branch and a cross-modal fusion module to Mask2Former to locate all sounding objects. Finally, we evaluate the proposed method using two backbones on AVISeg. We believe that AVIS will inspire the community towards a more comprehensive multi-modal understanding.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个新的多模态任务，即听视频实例分割（AVIS），目的是同时在听sible的视频中识别、分割和跟踪具有声音的对象实例。我们认为这是多模态理解的一个新的突破口。为了更好地推进这项研究，我们建立了首个听视频实例分割benchmark（AVISeg）。具体来说，AVISeg包括YouTube和公共听视频数据集的1,258个视频，视频的平均时长为62.6秒，其中117个视频通过使用基于Segment Anything Model（SAM）的交互式半自动标注工具进行了标注。此外，我们提出了一个简单的基线模型 дляAVIS任务。我们的新模型在Mask2Former模型中添加了一个声音支持和一个跨模态融合模块，以便在听sible的视频中找到所有的声音对象。最后，我们使用两个背景网络测试了我们的提议方法。我们认为AVIS将鼓励社区更加全面地理解多模态。
</details></li>
</ul>
<hr>
<h2 id="Triplet-Attention-Transformer-for-Spatiotemporal-Predictive-Learning"><a href="#Triplet-Attention-Transformer-for-Spatiotemporal-Predictive-Learning" class="headerlink" title="Triplet Attention Transformer for Spatiotemporal Predictive Learning"></a>Triplet Attention Transformer for Spatiotemporal Predictive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18698">http://arxiv.org/abs/2310.18698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuesong Nie, Xi Chen, Haoyuan Jin, Zhihang Zhu, Yunfeng Yan, Donglian Qi</li>
<li>for: 预测未来序列 based on 历史序列，提高预测质量 while maintaining 计算效率</li>
<li>methods: 使用 triplet attention transformer，包括 Triplet Attention Module (TAM)，替代传统的 recurrent units， capture both inter-frame dynamics 和 intra-frame static features</li>
<li>results: 在多种场景下，包括移动对象轨迹预测、交通流预测、驾驶场景预测和人体动作捕捉，实现了 state-of-the-art 性能，超过了现有的 recurrent-based 和 recurrent-free 方法<details>
<summary>Abstract</summary>
Spatiotemporal predictive learning offers a self-supervised learning paradigm that enables models to learn both spatial and temporal patterns by predicting future sequences based on historical sequences. Mainstream methods are dominated by recurrent units, yet they are limited by their lack of parallelization and often underperform in real-world scenarios. To improve prediction quality while maintaining computational efficiency, we propose an innovative triplet attention transformer designed to capture both inter-frame dynamics and intra-frame static features. Specifically, the model incorporates the Triplet Attention Module (TAM), which replaces traditional recurrent units by exploring self-attention mechanisms in temporal, spatial, and channel dimensions. In this configuration: (i) temporal tokens contain abstract representations of inter-frame, facilitating the capture of inherent temporal dependencies; (ii) spatial and channel attention combine to refine the intra-frame representation by performing fine-grained interactions across spatial and channel dimensions. Alternating temporal, spatial, and channel-level attention allows our approach to learn more complex short- and long-range spatiotemporal dependencies. Extensive experiments demonstrate performance surpassing existing recurrent-based and recurrent-free methods, achieving state-of-the-art under multi-scenario examination including moving object trajectory prediction, traffic flow prediction, driving scene prediction, and human motion capture.
</details>
<details>
<summary>摘要</summary>
《空时空间预测学习》提供了一种自主学习 paradigma，允许模型通过预测未来序列基于历史序列来学习空间和时间模式。主流方法受限于缺乏并行化和实际场景下的表现不佳，我们提议一种创新的 triplet attention transformer，用于捕捉空间、时间和通道维度的自我注意力机制。在这种配置下：（i）时间ток包含了抽象的间隔frame的表示，以便捕捉自然的时间依赖关系；（ii）空间和通道注意力结合以进一步细化内帧表示，通过在空间和通道维度进行细化的交互来增强模型对于短距离和长距离空间时间关系的学习能力。 alternate temporal、空间和通道级别的注意力允许我们的方法学习更复杂的短距离和长距离空间时间关系。广泛的实验证明了我们的方法在多种场景下，包括人体动作跟踪、交通流量预测、驾驶场景预测和人体动作捕捉等，性能超过了现有的循环单元和循环自由方法，实现了状态当前的水平。
</details></li>
</ul>
<hr>
<h2 id="Foundational-Models-in-Medical-Imaging-A-Comprehensive-Survey-and-Future-Vision"><a href="#Foundational-Models-in-Medical-Imaging-A-Comprehensive-Survey-and-Future-Vision" class="headerlink" title="Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision"></a>Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18689">http://arxiv.org/abs/2310.18689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bobby Azad, Reza Azad, Sania Eskandari, Afshin Bozorgpour, Amirhossein Kazerouni, Islem Rekik, Dorit Merhof</li>
<li>For: This paper provides a comprehensive overview of foundation models in the domain of medical imaging, with a focus on their applications, opportunities, and future directions.* Methods: The paper classifies foundation models within the medical domain based on training strategies, imaging modalities, specific organs of interest, and algorithms integral to these models.* Results: The paper discusses the practical use cases of some selected approaches and addresses the challenges and research pathways associated with foundational models in medical imaging, including interpretability, data management, computational requirements, and contextual comprehension.<details>
<summary>Abstract</summary>
Foundation models, large-scale, pre-trained deep-learning models adapted to a wide range of downstream tasks have gained significant interest lately in various deep-learning problems undergoing a paradigm shift with the rise of these models. Trained on large-scale dataset to bridge the gap between different modalities, foundation models facilitate contextual reasoning, generalization, and prompt capabilities at test time. The predictions of these models can be adjusted for new tasks by augmenting the model input with task-specific hints called prompts without requiring extensive labeled data and retraining. Capitalizing on the advances in computer vision, medical imaging has also marked a growing interest in these models. To assist researchers in navigating this direction, this survey intends to provide a comprehensive overview of foundation models in the domain of medical imaging. Specifically, we initiate our exploration by providing an exposition of the fundamental concepts forming the basis of foundation models. Subsequently, we offer a methodical taxonomy of foundation models within the medical domain, proposing a classification system primarily structured around training strategies, while also incorporating additional facets such as application domains, imaging modalities, specific organs of interest, and the algorithms integral to these models. Furthermore, we emphasize the practical use case of some selected approaches and then discuss the opportunities, applications, and future directions of these large-scale pre-trained models, for analyzing medical images. In the same vein, we address the prevailing challenges and research pathways associated with foundational models in medical imaging. These encompass the areas of interpretability, data management, computational requirements, and the nuanced issue of contextual comprehension.
</details>
<details>
<summary>摘要</summary>
大量训练的深度学习模型（foundation models）在不同领域的深度学习问题中受到了非常大的关注。这些模型可以在各种模式之间进行Contextual reasoning，通过加入任务特定的提示（prompts）来调整模型的预测，无需大量的标注数据和重新训练。随着计算机视觉领域的进步，医学影像领域也开始关注这些模型。本文旨在为医学影像领域的研究人员提供一份全面的评论，以帮助他们在这个方向中探索。 Specifically, we begin by providing an overview of the fundamental concepts that underlie foundation models. We then offer a systematic taxonomy of foundation models within the medical domain, classifying them based on their training strategies, application domains, imaging modalities, specific organs of interest, and the algorithms used in these models. We also highlight the practical use cases of some selected approaches and discuss the opportunities, applications, and future directions of these large-scale pre-trained models for analyzing medical images. Furthermore, we address the challenges and research pathways associated with foundational models in medical imaging, including interpretability, data management, computational requirements, and the nuanced issue of contextual comprehension.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Object-Detection-in-Optical-Remote-Sensing-Imagery-via-Attention-based-Feature-Distillation"><a href="#Efficient-Object-Detection-in-Optical-Remote-Sensing-Imagery-via-Attention-based-Feature-Distillation" class="headerlink" title="Efficient Object Detection in Optical Remote Sensing Imagery via Attention-based Feature Distillation"></a>Efficient Object Detection in Optical Remote Sensing Imagery via Attention-based Feature Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18676">http://arxiv.org/abs/2310.18676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pourya Shamsolmoali, Jocelyn Chanussot, Huiyu Zhou, Yue Lu</li>
<li>for: 这篇论文主要针对的是实时观测中的有效物体检测方法，并且使用知识传播（KD）技术来实现轻量级模型，同时保持精度。</li>
<li>methods: 本文提出了一个新的知识传播方法，即注意力基本Distillation（AFD），这个方法可以将教师模型中的本地和全球资讯都传播到学生模型中，以提高学生模型的检测精度。此外，本文还引入了一个多例对劲机制，以分辨背景和前景元素，并将其传播到学生模型中。</li>
<li>results: 本文的实验结果显示，这个AFD方法可以在两个公共的航空图像benchmark上实现和其他状态顶对称模型相同的检测性能，同时具有轻量级的特点。<details>
<summary>Abstract</summary>
Efficient object detection methods have recently received great attention in remote sensing. Although deep convolutional networks often have excellent detection accuracy, their deployment on resource-limited edge devices is difficult. Knowledge distillation (KD) is a strategy for addressing this issue since it makes models lightweight while maintaining accuracy. However, existing KD methods for object detection have encountered two constraints. First, they discard potentially important background information and only distill nearby foreground regions. Second, they only rely on the global context, which limits the student detector's ability to acquire local information from the teacher detector. To address the aforementioned challenges, we propose Attention-based Feature Distillation (AFD), a new KD approach that distills both local and global information from the teacher detector. To enhance local distillation, we introduce a multi-instance attention mechanism that effectively distinguishes between background and foreground elements. This approach prompts the student detector to focus on the pertinent channels and pixels, as identified by the teacher detector. Local distillation lacks global information, thus attention global distillation is proposed to reconstruct the relationship between various pixels and pass it from teacher to student detector. The performance of AFD is evaluated on two public aerial image benchmarks, and the evaluation results demonstrate that AFD in object detection can attain the performance of other state-of-the-art models while being efficient.
</details>
<details>
<summary>摘要</summary>
Recently, efficient object detection methods have received significant attention in remote sensing. Although deep convolutional networks often have excellent detection accuracy, deploying them on resource-limited edge devices is challenging. Knowledge distillation (KD) is a strategy that can address this issue by making models lightweight while maintaining accuracy. However, existing KD methods for object detection have two limitations. First, they discard potentially important background information and only distill nearby foreground regions. Second, they only rely on global context, which limits the student detector's ability to acquire local information from the teacher detector.To address these challenges, we propose Attention-based Feature Distillation (AFD), a new KD approach that distills both local and global information from the teacher detector. To enhance local distillation, we introduce a multi-instance attention mechanism that effectively distinguishes between background and foreground elements. This approach prompts the student detector to focus on the pertinent channels and pixels, as identified by the teacher detector. Local distillation lacks global information, so we propose attention global distillation to reconstruct the relationship between various pixels and pass it from teacher to student detector.We evaluate the performance of AFD on two public aerial image benchmarks, and the results show that AFD can achieve the performance of other state-of-the-art models while being efficient.
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-for-Generalist-Geospatial-Artificial-Intelligence"><a href="#Foundation-Models-for-Generalist-Geospatial-Artificial-Intelligence" class="headerlink" title="Foundation Models for Generalist Geospatial Artificial Intelligence"></a>Foundation Models for Generalist Geospatial Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18660">http://arxiv.org/abs/2310.18660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes Jakubik, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, Daiki Kimura, Naomi Simumba, Linsong Chu, S. Karthik Mukkavilli, Devyani Lambhate, Kamal Das, Ranjini Bangalore, Dario Oliveira, Michal Muszynski, Kumar Ankur, Muthukumaran Ramasubramanian, Iksha Gurung, Sam Khallaghi, Hanxi, Li, Michael Cecil, Maryam Ahmadi, Fatemeh Kordi, Hamed Alemohammad, Manil Maskey, Raghu Ganti, Kommy Weldemariam, Rahul Ramachandran<br>for:* 这篇论文的目的是为了提出一个高度可调整且可重用的人工智能（AI）模型的开发，以便在地球科学和遥测中具有重要影响。methods:* 这篇论文使用了自我指导的方法来预训foundational models，然后使用小量标签数据进行精确化。results:* 这篇论文的研究显示了一个首次的框架可以有效地将foundational models预训和精确化，并在多个地球观测任务上表现出色，例如多 Spectral satellite imagery 的测试。<details>
<summary>Abstract</summary>
Significant progress in the development of highly adaptable and reusable Artificial Intelligence (AI) models is expected to have a significant impact on Earth science and remote sensing. Foundation models are pre-trained on large unlabeled datasets through self-supervision, and then fine-tuned for various downstream tasks with small labeled datasets. This paper introduces a first-of-a-kind framework for the efficient pre-training and fine-tuning of foundational models on extensive geospatial data. We have utilized this framework to create Prithvi, a transformer-based geospatial foundational model pre-trained on more than 1TB of multispectral satellite imagery from the Harmonized Landsat-Sentinel 2 (HLS) dataset. Our study demonstrates the efficacy of our framework in successfully fine-tuning Prithvi to a range of Earth observation tasks that have not been tackled by previous work on foundation models involving multi-temporal cloud gap imputation, flood mapping, wildfire scar segmentation, and multi-temporal crop segmentation. Our experiments show that the pre-trained model accelerates the fine-tuning process compared to leveraging randomly initialized weights. In addition, pre-trained Prithvi compares well against the state-of-the-art, e.g., outperforming a conditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%) in the structural similarity index. Finally, due to the limited availability of labeled data in the field of Earth observation, we gradually reduce the quantity of available labeled data for refining the model to evaluate data efficiency and demonstrate that data can be decreased significantly without affecting the model's accuracy. The pre-trained 100 million parameter model and corresponding fine-tuning workflows have been released publicly as open source contributions to the global Earth sciences community through Hugging Face.
</details>
<details>
<summary>摘要</summary>
“预计在人工智能（AI）模型的开发中，有 significante进步，这将对地球科学和远程感知产生重要影响。基础模型通过自我超vision，在大量无标签数据上自我预训练，然后使用小量标签数据进行精度调整。这篇论文介绍了一种新的框架，用于高效地预训练和精度调整基础模型，并在extensive geospatial数据上进行了实践。我们使用了这个框架，创造了一个基于转换器的地ospatial基础模型，名为Prithvi，并在 более чем 1TB的多spectral卫星图像上进行了预训练。我们的研究表明，我们的框架可以成功地将Prithvi fine-tune到多种地观测任务中，包括多temporal云阴掩模型、洪水地图、野火痕分割和多temporal作物分割。我们的实验显示，预训练后的模型可以加速 fine-tuning 过程，并且与随机初始化的权重相比，具有更高的准确率。此外，我们的Prithvi模型在多temporal云阴掩模型中与状态艺术模型进行比较，在结构相似指数中提高了5pp（或5.7%）。最后，由于地球观测领域内标注数据的有限性，我们逐渐减少可用标注数据的量来评估数据效率，并证明可以大幅减少数据量而无需影响模型的准确率。预训练10000万参数模型和相应的精度调整工作流已经公开发布在Hugging Face上，作为对全球地球科学社区的开源贡献。”
</details></li>
</ul>
<hr>
<h2 id="Med-DANet-V2-A-Flexible-Dynamic-Architecture-for-Efficient-Medical-Volumetric-Segmentation"><a href="#Med-DANet-V2-A-Flexible-Dynamic-Architecture-for-Efficient-Medical-Volumetric-Segmentation" class="headerlink" title="Med-DANet V2: A Flexible Dynamic Architecture for Efficient Medical Volumetric Segmentation"></a>Med-DANet V2: A Flexible Dynamic Architecture for Efficient Medical Volumetric Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18656">http://arxiv.org/abs/2310.18656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Shen, Yifu Zhang, Wenxuan Wang, Chen Chen, Jing Liu, Shanshan Song, Jiangyun Li</li>
<li>for: 这个论文的目的是提高医疗影像三维分类的计算效率。</li>
<li>methods: 这个方法使用了动态推论基于层次复杂度，并 dynamically选择适合不同层次的2D候选模型。</li>
<li>results: 该方法在BraTS 2019和2020的实验中实现了与前一代方法相似或更好的性能，并且具有许多少的模型复杂度。相比Med-DANet和TransBTS，我们的框架可以提高模型效率，并且具有相似的分类结果。<details>
<summary>Abstract</summary>
Recent works have shown that the computational efficiency of 3D medical image (e.g. CT and MRI) segmentation can be impressively improved by dynamic inference based on slice-wise complexity. As a pioneering work, a dynamic architecture network for medical volumetric segmentation (i.e. Med-DANet) has achieved a favorable accuracy and efficiency trade-off by dynamically selecting a suitable 2D candidate model from the pre-defined model bank for different slices. However, the issues of incomplete data analysis, high training costs, and the two-stage pipeline in Med-DANet require further improvement. To this end, this paper further explores a unified formulation of the dynamic inference framework from the perspective of both the data itself and the model structure. For each slice of the input volume, our proposed method dynamically selects an important foreground region for segmentation based on the policy generated by our Decision Network and Crop Position Network. Besides, we propose to insert a stage-wise quantization selector to the employed segmentation model (e.g. U-Net) for dynamic architecture adapting. Extensive experiments on BraTS 2019 and 2020 show that our method achieves comparable or better performance than previous state-of-the-art methods with much less model complexity. Compared with previous methods Med-DANet and TransBTS with dynamic and static architecture respectively, our framework improves the model efficiency by up to nearly 4.1 and 17.3 times with comparable segmentation results on BraTS 2019.
</details>
<details>
<summary>摘要</summary>
To address these issues, this paper proposes a unified formulation of the dynamic inference framework from both the data and model perspectives. For each slice of the input volume, our method dynamically selects an important foreground region for segmentation based on the policy generated by our Decision Network and Crop Position Network. Additionally, we propose inserting a stage-wise quantization selector into the employed segmentation model (such as U-Net) for dynamic architecture adapting.Experiments on the BraTS 2019 and 2020 datasets show that our method achieves performance comparable to or better than previous state-of-the-art methods with much less model complexity. Compared with previous methods Med-DANet and TransBTS with dynamic and static architecture, respectively, our framework improves model efficiency by up to nearly 4.1 and 17.3 times with comparable segmentation results on BraTS 2019.
</details></li>
</ul>
<hr>
<h2 id="Feature-Guided-Masked-Autoencoder-for-Self-supervised-Learning-in-Remote-Sensing"><a href="#Feature-Guided-Masked-Autoencoder-for-Self-supervised-Learning-in-Remote-Sensing" class="headerlink" title="Feature Guided Masked Autoencoder for Self-supervised Learning in Remote Sensing"></a>Feature Guided Masked Autoencoder for Self-supervised Learning in Remote Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18653">http://arxiv.org/abs/2310.18653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhu-xlab/fgmae">https://github.com/zhu-xlab/fgmae</a></li>
<li>paper_authors: Yi Wang, Hugo Hernández Hernández, Conrad M Albrecht, Xiao Xiang Zhu</li>
<li>for: 这篇论文旨在探讨自我监督学习帮助vised transformer在远程感知中进行预训。</li>
<li>methods: 本论文使用Masked AutoEncoder（MAE）作为预训模型，并将spectral和spatial remote sensing图像特征作为改进MAE重建目标。</li>
<li>results: 实验结果显示Feature Guided Masked Autoencoder（FG-MAE）可以提高多spectral图像和SAR图像的semantic理解，并且具有很好的扩展性。<details>
<summary>Abstract</summary>
Self-supervised learning guided by masked image modelling, such as Masked AutoEncoder (MAE), has attracted wide attention for pretraining vision transformers in remote sensing. However, MAE tends to excessively focus on pixel details, thereby limiting the model's capacity for semantic understanding, in particular for noisy SAR images. In this paper, we explore spectral and spatial remote sensing image features as improved MAE-reconstruction targets. We first conduct a study on reconstructing various image features, all performing comparably well or better than raw pixels. Based on such observations, we propose Feature Guided Masked Autoencoder (FG-MAE): reconstructing a combination of Histograms of Oriented Graidents (HOG) and Normalized Difference Indices (NDI) for multispectral images, and reconstructing HOG for SAR images. Experimental results on three downstream tasks illustrate the effectiveness of FG-MAE with a particular boost for SAR imagery. Furthermore, we demonstrate the well-inherited scalability of FG-MAE and release a first series of pretrained vision transformers for medium resolution SAR and multispectral images.
</details>
<details>
<summary>摘要</summary>
自领导学习，如遮盲自动编码（MAE），在远程感知领域内吸引了广泛的关注，用于预训练视Transformer。然而，MAE往往过分关注像素细节，因此限制模型对Semantic理解的能力，特别是对噪音SAR图像。在这篇论文中，我们探索谱spectral和空间Remote sensing图像特征作为改进MAE重建目标。我们首先进行了不同图像特征的重建研究，并发现所有特征都可以相对或更好地than raw pixels。基于这些观察，我们提出了特征引导遮盲自动编码（FG-MAE）：对多spectral图像重建 histogram of oriented gradients（HOG）和normalized difference indices（NDI）的组合，对SAR图像重建HOG。我们的实验结果表明FG-MAE在三个下游任务上表现出了效果，特别是对SAR图像。此外，我们还证明了FG-MAE具有良好的扩展性，并发布了首个媒体分辨率SAR和多spectral图像预训练的视Transformer。
</details></li>
</ul>
<hr>
<h2 id="Local-Global-Self-Supervised-Visual-Representation-Learning"><a href="#Local-Global-Self-Supervised-Visual-Representation-Learning" class="headerlink" title="Local-Global Self-Supervised Visual Representation Learning"></a>Local-Global Self-Supervised Visual Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18651">http://arxiv.org/abs/2310.18651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alijavidani/local_global_representation_learning">https://github.com/alijavidani/local_global_representation_learning</a></li>
<li>paper_authors: Ali Javidani, Mohammad Amin Sadeghi, Babak Nadjar Araabi</li>
<li>for: 本研究旨在探讨将patch-level特征学习纳入现有自动学习批处理方法中，以提高学习得到的视觉表示的质量。</li>
<li>methods: 我们提出了一种简单 yet effective的patch-matching算法，可以在扩展视图下找到图像中匹配的补丁。然后，我们使用基于Vision Transformer（ViT）的自动学习框架，将扩展视图和补丁进行自我超vised学习。这种方法可以同时生成图像级别和补丁级别的表示。</li>
<li>results: 我们在小、中、大规模数据集上预训练了我们的方法，并证明了我们的方法可以在图像分类和下游任务中超越当前状态艺技。<details>
<summary>Abstract</summary>
Self-supervised representation learning methods mainly focus on image-level instance discrimination. This study explores the potential benefits of incorporating patch-level discrimination into existing methods to enhance the quality of learned representations by simultaneously looking at local and global visual features. Towards this idea, we present a straightforward yet effective patch-matching algorithm that can find the corresponding patches across the augmented views of an image. The augmented views are subsequently fed into a self-supervised learning framework employing Vision Transformer (ViT) as its backbone. The result is the generation of both image-level and patch-level representations. Leveraging the proposed patch-matching algorithm, the model minimizes the representation distance between not only the CLS tokens but also the corresponding patches. As a result, the model gains a more comprehensive understanding of both the entirety of the image as well as its finer details. We pretrain the proposed method on small, medium, and large-scale datasets. It is shown that our approach could outperform state-of-the-art image-level representation learning methods on both image classification and downstream tasks. Keywords: Self-Supervised Learning; Visual Representations; Local-Global Representation Learning; Patch-Wise Representation Learning; Vision Transformer (ViT)
</details>
<details>
<summary>摘要</summary>
自我监督学习方法主要关注图像级别的实例识别。本研究探讨可以将图像级别的识别与patch级别的识别结合到现有方法中，以提高学习的表示质量。为了实现这一目标，我们提出了一种简单又有效的补丁匹配算法，可以在扩展视图中找到图像中的相对应补丁。这些扩展视图然后被 feed into一个自我监督学习框架，使用 Vision Transformer（ViT）作为 backing。通过这种方法，我们可以同时生成图像级别的表示和patch级别的表示。通过补丁匹配算法，模型可以将 CLS Token 的表示距离和相应的补丁之间的表示距离减小。因此，模型可以更好地理解图像的整体特征和细节特征。我们在小、中、大样本大小上进行预训练，并显示了我们的方法可以在图像级别表示学习中超越状态艺术的图像级别表示学习方法。关键词：自我监督学习;视觉表示;本地-全局表示学习;补丁级别表示学习; Vision Transformer（ViT）
</details></li>
</ul>
<hr>
<h2 id="Switching-Temporary-Teachers-for-Semi-Supervised-Semantic-Segmentation"><a href="#Switching-Temporary-Teachers-for-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="Switching Temporary Teachers for Semi-Supervised Semantic Segmentation"></a>Switching Temporary Teachers for Semi-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18640">http://arxiv.org/abs/2310.18640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naver-ai/dual-teacher">https://github.com/naver-ai/dual-teacher</a></li>
<li>paper_authors: Jaemin Na, Jung-Woo Ha, Hyung Jin Chang, Dongyoon Han, Wonjun Hwang</li>
<li>for: 这篇研究旨在提高 semi-supervised  semantic segmentation 的效果，并且解决 teacher 和学生模型之间的问题。</li>
<li>methods: 这篇研究使用了 dual temporary teacher 方法，将 teacher 和学生模型分为两个短期教师，以降低学生模型与教师模型之间的 Coupling 问题。</li>
<li>results: 这篇研究在 PASCAL VOC、Cityscapes 和 ADE20K 测试 benchmark 上达到了竞争性的表现，并且训练时间比 state-of-the-art 方法短得多。此外，这篇研究还证明了其方法是模型不敏感的，可以与 CNN 和 Transformer 等不同类型的模型搭配使用。<details>
<summary>Abstract</summary>
The teacher-student framework, prevalent in semi-supervised semantic segmentation, mainly employs the exponential moving average (EMA) to update a single teacher's weights based on the student's. However, EMA updates raise a problem in that the weights of the teacher and student are getting coupled, causing a potential performance bottleneck. Furthermore, this problem may become more severe when training with more complicated labels such as segmentation masks but with few annotated data. This paper introduces Dual Teacher, a simple yet effective approach that employs dual temporary teachers aiming to alleviate the coupling problem for the student. The temporary teachers work in shifts and are progressively improved, so consistently prevent the teacher and student from becoming excessively close. Specifically, the temporary teachers periodically take turns generating pseudo-labels to train a student model and maintain the distinct characteristics of the student model for each epoch. Consequently, Dual Teacher achieves competitive performance on the PASCAL VOC, Cityscapes, and ADE20K benchmarks with remarkably shorter training times than state-of-the-art methods. Moreover, we demonstrate that our approach is model-agnostic and compatible with both CNN- and Transformer-based models. Code is available at \url{https://github.com/naver-ai/dual-teacher}.
</details>
<details>
<summary>摘要</summary>
教师-学生框架，广泛存在 semi-supervised 语义分割中，主要采用积分移动平均（EMA）来更新单个教师的参数基于学生的。然而，EMA 更新可能会导致教师和学生的参数相互关联，从而引起性能瓶颈。此外，这种问题可能会在具有更复杂的标签，如分割mask，但具有少量标注数据的情况下变得更加严重。本文介绍了 Dual Teacher，一种简单 yet effective 的方法，它采用了双临时教师来解决学生模型与教师模型之间的关联问题。这两个临时教师会在交替的时间间隔内为学生模型生成 pseudo-标签，以便在每个轮次中维护学生模型的独特特征。因此， Dual Teacher 在 PASCAL VOC、Cityscapes 和 ADE20K 测试集上 achieve 竞争性性能，并且训练时间较短于当前state-of-the-art 方法。此外，我们还证明了我们的方法是模型无关的，可以与 CNN 和 Transformer 等模型结合使用。代码可以在 \url{https://github.com/naver-ai/dual-teacher} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Plastic-and-Stable-Exemplar-Free-Incremental-Learning-A-Dual-Learner-Framework-with-Cumulative-Parameter-Averaging"><a href="#Towards-Plastic-and-Stable-Exemplar-Free-Incremental-Learning-A-Dual-Learner-Framework-with-Cumulative-Parameter-Averaging" class="headerlink" title="Towards Plastic and Stable Exemplar-Free Incremental Learning: A Dual-Learner Framework with Cumulative Parameter Averaging"></a>Towards Plastic and Stable Exemplar-Free Incremental Learning: A Dual-Learner Framework with Cumulative Parameter Averaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18639">http://arxiv.org/abs/2310.18639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenju Sun, Qingyong Li, Wen Wang, Yangli-ao Geng</li>
<li>for: 这个研究是为了解决增量学习中的困难，特别是在例项自由情况下，当学习新任务时不能访问旧任务的样本。</li>
<li>methods: 这个方法使用了单任务学习（STL）和平均参数积存（CPA）技术，具有单任务学习和综合学习两种模式。</li>
<li>results: 实验结果显示，这个方法在CIFAR-100和Tiny-ImageNet上比过几个状态顶对的增量学习基eline表现出色，尤其在任务增量学习和类别增量学习情况下。<details>
<summary>Abstract</summary>
The dilemma between plasticity and stability presents a significant challenge in Incremental Learning (IL), especially in the exemplar-free scenario where accessing old-task samples is strictly prohibited during the learning of a new task. A straightforward solution to this issue is learning and storing an independent model for each task, known as Single Task Learning (STL). Despite the linear growth in model storage with the number of tasks in STL, we empirically discover that averaging these model parameters can potentially preserve knowledge across all tasks. Inspired by this observation, we propose a Dual-Learner framework with Cumulative Parameter Averaging (DLCPA). DLCPA employs a dual-learner design: a plastic learner focused on acquiring new-task knowledge and a stable learner responsible for accumulating all learned knowledge. The knowledge from the plastic learner is transferred to the stable learner via cumulative parameter averaging. Additionally, several task-specific classifiers work in cooperation with the stable learner to yield the final prediction. Specifically, when learning a new task, these modules are updated in a cyclic manner: i) the plastic learner is initially optimized using a self-supervised loss besides the supervised loss to enhance the feature extraction robustness; ii) the stable learner is then updated with respect to the plastic learner in a cumulative parameter averaging manner to maintain its task-wise generalization; iii) the task-specific classifier is accordingly optimized to align with the stable learner. Experimental results on CIFAR-100 and Tiny-ImageNet show that DLCPA outperforms several state-of-the-art exemplar-free baselines in both Task-IL and Class-IL settings.
</details>
<details>
<summary>摘要</summary>
increments 学习（IL）中的困境在选择between plasticity and stability 上是一个 significannot challenge, especially in the exemplar-free scenario where accessing old-task samples is strictly prohibited during the learning of a new task. A straightforward solution to this issue is learning and storing an independent model for each task, known as Single Task Learning (STL). Despite the linear growth in model storage with the number of tasks in STL, we empirically discover that averaging these model parameters can potentially preserve knowledge across all tasks. Inspired by this observation, we propose a Dual-Learner framework with Cumulative Parameter Averaging (DLCPA). DLCPA employs a dual-learner design: a plastic learner focused on acquiring new-task knowledge and a stable learner responsible for accumulating all learned knowledge. The knowledge from the plastic learner is transferred to the stable learner via cumulative parameter averaging. Additionally, several task-specific classifiers work in cooperation with the stable learner to yield the final prediction. Specifically, when learning a new task, these modules are updated in a cyclic manner: i) the plastic learner is initially optimized using a self-supervised loss besides the supervised loss to enhance the feature extraction robustness; ii) the stable learner is then updated with respect to the plastic learner in a cumulative parameter averaging manner to maintain its task-wise generalization; iii) the task-specific classifier is accordingly optimized to align with the stable learner. Experimental results on CIFAR-100 and Tiny-ImageNet show that DLCPA outperforms several state-of-the-art exemplar-free baselines in both Task-IL and Class-IL settings.
</details></li>
</ul>
<hr>
<h2 id="ODM3D-Alleviating-Foreground-Sparsity-for-Enhanced-Semi-Supervised-Monocular-3D-Object-Detection"><a href="#ODM3D-Alleviating-Foreground-Sparsity-for-Enhanced-Semi-Supervised-Monocular-3D-Object-Detection" class="headerlink" title="ODM3D: Alleviating Foreground Sparsity for Enhanced Semi-Supervised Monocular 3D Object Detection"></a>ODM3D: Alleviating Foreground Sparsity for Enhanced Semi-Supervised Monocular 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18620">http://arxiv.org/abs/2310.18620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijia Zhang, Dongnan Liu, Chao Ma, Weidong Cai</li>
<li>for: 提高单光图像3D物体检测（M3OD）的性能，使其能够更好地检测自动驾驶中的3D物体。</li>
<li>methods: 使用semi-supervised learning，将LiDAR频谱知识注入到单光图像检测器中，并通过提取前景稀缺性来进行更加有效的知识传递。</li>
<li>results: 在KITTI验证和测试环境中，其方法 ranked 1st，在BEV和3D检测纪录中都有显著的提升，舒过所有现有的单光方法，包括直接监督和半监督方法。<details>
<summary>Abstract</summary>
Monocular 3D object detection (M3OD) is a significant yet inherently challenging task in autonomous driving due to absence of implicit depth cues in a single RGB image. In this paper, we strive to boost currently underperforming monocular 3D object detectors by leveraging an abundance of unlabelled data via semi-supervised learning. Our proposed ODM3D framework entails cross-modal knowledge distillation at various levels to inject LiDAR-domain knowledge into a monocular detector during training. By identifying foreground sparsity as the main culprit behind existing methods' suboptimal training, we exploit the precise localisation information embedded in LiDAR points to enable more foreground-attentive and efficient distillation via the proposed BEV occupancy guidance mask, leading to notably improved knowledge transfer and M3OD performance. Besides, motivated by insights into why existing cross-modal GT-sampling techniques fail on our task at hand, we further design a novel cross-modal object-wise data augmentation strategy for effective RGB-LiDAR joint learning. Our method ranks 1st in both KITTI validation and test benchmarks, significantly surpassing all existing monocular methods, supervised or semi-supervised, on both BEV and 3D detection metrics.
</details>
<details>
<summary>摘要</summary>
《单目三维物体检测（M3OD）是自主驾驶中的一项重要 yet inherently 挑战性任务，因为单个 RGB 图像中缺乏隐式深度提示。在这篇文章中，我们努力提高目前的单目三维物体检测器，通过利用大量未标注数据进行 semi-supervised 学习。我们提出的 ODM3D 框架在不同层次进行交叉模态知识填充，以在训练中注入 LiDAR 频谱知识到单目检测器。通过发现前景稀畴是现有方法训练不佳的主要原因，我们利用 LiDAR 点的精确地址信息来实现更加前景注意和高效的填充，从而提高知识传递和 M3OD 性能。此外，鉴于现有交叉模态 GT 采样技术在我们任务上失效的原因，我们还设计了一种新的交叉模态对象增强数据采样策略，以便有效地在 RGB 和 LiDAR  JOINT 学习中进行对象增强。我们的方法在 KITTI 验证和测试benchmark上rank 1st，明显超过了所有现有的单目方法，包括直接监督和 semi-supervised 方法，在 BEV 和 3D 检测 метриках上。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalisation-via-Risk-Distribution-Matching"><a href="#Domain-Generalisation-via-Risk-Distribution-Matching" class="headerlink" title="Domain Generalisation via Risk Distribution Matching"></a>Domain Generalisation via Risk Distribution Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18598">http://arxiv.org/abs/2310.18598</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nktoan/risk-distribution-matching">https://github.com/nktoan/risk-distribution-matching</a></li>
<li>paper_authors: Toan Nguyen, Kien Do, Bao Duong, Thin Nguyen</li>
<li>for: 这篇论文旨在解决域对应（Domain Generalization，DG）中的问题，提出一个新的方法，利用风险分布来描述域，以 достиieving 域之对称。</li>
<li>methods: 这篇论文使用的方法是基于风险分布的，即使用最大mean距离（MMD）距离来测量风险分布之间的差异，并将其用于域之对称。</li>
<li>results: 实验结果显示，这篇论文提出的方法（Risk Distribution Matching，RDM）在标准的benchmark数据集上具有较高的域对称能力，并且比其他state-of-the-art DG方法更有效率。<details>
<summary>Abstract</summary>
We propose a novel approach for domain generalisation (DG) leveraging risk distributions to characterise domains, thereby achieving domain invariance. In our findings, risk distributions effectively highlight differences between training domains and reveal their inherent complexities. In testing, we may observe similar, or potentially intensifying in magnitude, divergences between risk distributions. Hence, we propose a compelling proposition: Minimising the divergences between risk distributions across training domains leads to robust invariance for DG. The key rationale behind this concept is that a model, trained on domain-invariant or stable features, may consistently produce similar risk distributions across various domains. Building upon this idea, we propose Risk Distribution Matching (RDM). Using the maximum mean discrepancy (MMD) distance, RDM aims to minimise the variance of risk distributions across training domains. However, when the number of domains increases, the direct optimisation of variance leads to linear growth in MMD computations, resulting in inefficiency. Instead, we propose an approximation that requires only one MMD computation, by aligning just two distributions: that of the worst-case domain and the aggregated distribution from all domains. Notably, this method empirically outperforms optimising distributional variance while being computationally more efficient. Unlike conventional DG matching algorithms, RDM stands out for its enhanced efficacy by concentrating on scalar risk distributions, sidestepping the pitfalls of high-dimensional challenges seen in feature or gradient matching. Our extensive experiments on standard benchmark datasets demonstrate that RDM shows superior generalisation capability over state-of-the-art DG methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的领域通用化（DG）方法，利用风险分布来特征化领域，从而实现领域不变性。我们发现，风险分布能够有效地披露训练领域之间的差异和内在复杂性。在测试中，我们可能会观察到类似或者可能加剧的差异between风险分布。因此，我们提出了一个有力的提议：将领域之间风险分布的差异降到最小化，以实现Robust Invariance for DG。这个概念的关键思想是，通过训练领域不变或稳定的特征，我们可以在不同领域上通过风险分布的匹配来实现模型的稳定性。基于这个想法，我们提出了风险分布匹配（RDM）。使用最大平均差（MMD）距离，RDM aimsto minimize the variance of risk distributions across training domains。然而，当领域数量增加时，直接优化差异会导致线性增长的MMD计算，从而变得不效率。因此，我们提出了一种简化方法，只需要一次MMD计算，通过对最坏领域的分布和所有领域的分布进行对应。与传统的DG匹配算法不同，RDM更加有效地做到了通过scalar风险分布来快速匹配，而不需要高维度的特征或梯度匹配。我们对标准 benchmark 数据集进行了广泛的实验，发现RDM在state-of-the-art DG方法中显示出了更好的总体化能力。
</details></li>
</ul>
<hr>
<h2 id="This-Looks-Like-Those-Illuminating-Prototypical-Concepts-Using-Multiple-Visualizations"><a href="#This-Looks-Like-Those-Illuminating-Prototypical-Concepts-Using-Multiple-Visualizations" class="headerlink" title="This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations"></a>This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18589">http://arxiv.org/abs/2310.18589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henrymachiyu/this-looks-like-those_protoconcepts">https://github.com/henrymachiyu/this-looks-like-those_protoconcepts</a></li>
<li>paper_authors: Chiyu Ma, Brandon Zhao, Chaofan Chen, Cynthia Rudin</li>
<li>for: 这个论文的目的是提出一种可解释的图像分类方法，结合深度学习和案例基础理解。</li>
<li>methods: 这个方法使用多个图像 patches 来学习概念，并使用这些概念进行可解释的图像分类。</li>
<li>results: 实验结果表明，这种方法可以应用于各种现有的prototype-based图像分类网络中，并在标准数据集上实现相同的准确率。<details>
<summary>Abstract</summary>
We present ProtoConcepts, a method for interpretable image classification combining deep learning and case-based reasoning using prototypical parts. Existing work in prototype-based image classification uses a ``this looks like that'' reasoning process, which dissects a test image by finding prototypical parts and combining evidence from these prototypes to make a final classification. However, all of the existing prototypical part-based image classifiers provide only one-to-one comparisons, where a single training image patch serves as a prototype to compare with a part of our test image. With these single-image comparisons, it can often be difficult to identify the underlying concept being compared (e.g., ``is it comparing the color or the shape?''). Our proposed method modifies the architecture of prototype-based networks to instead learn prototypical concepts which are visualized using multiple image patches. Having multiple visualizations of the same prototype allows us to more easily identify the concept captured by that prototype (e.g., ``the test image and the related training patches are all the same shade of blue''), and allows our model to create richer, more interpretable visual explanations. Our experiments show that our ``this looks like those'' reasoning process can be applied as a modification to a wide range of existing prototypical image classification networks while achieving comparable accuracy on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
我们提出了ProtoConcepts，一种可读性高的图像分类方法，结合深度学习和倡议式推理，使用 protoypical parts。现有的图像分类方法中，使用“这看起来像那”的思维过程，将试验图像分解为 protoypical parts，并从这些 protoypical parts 中获取证据，以进行最终的分类。然而，所有的单一图像比较方法都仅提供一对一的比较，即一个训练图像区块作为一个 prototype，与试验图像中的一部分进行比较。这种单一图像比较可能很难理解到被比较的基本概念（例如，“是对颜色或形状的比较？”）。我们的提案方法改变 prototype-based 网络的架构，以学习 protoypical concepts，这些概念可以通过多个图像区块进行可读性更高的visual化。有多个可读性更高的 visual explanation，我们的模型可以更好地识别基本概念，并创建更加可读性更高的图像解释。我们的实验显示，我们的“这看起来像那”的思维过程可以与现有的 prototype-based 图像分类网络结合，在benchmark dataset上实现相似的准确性。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Multi-Modality-Learning-for-Multi-Label-Skin-Lesion-Classification"><a href="#Self-Supervised-Multi-Modality-Learning-for-Multi-Label-Skin-Lesion-Classification" class="headerlink" title="Self-Supervised Multi-Modality Learning for Multi-Label Skin Lesion Classification"></a>Self-Supervised Multi-Modality Learning for Multi-Label Skin Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18583">http://arxiv.org/abs/2310.18583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dylan-h-wang/skin-sm3">https://github.com/dylan-h-wang/skin-sm3</a></li>
<li>paper_authors: Hao Wang, Euijoon Ahn, Lei Bi, Jinman Kim</li>
<li>for: 该研究旨在提高多Modal Skin Lesion 诊断精度，使用自助学习算法和多modal 特征。</li>
<li>methods: 该算法使用了对匹配的dermoscopic和临床图像进行最大化的相似性 Maximization，以及基于归一化的Clustering分析生成Surrogate pseudo-multi-labels。</li>
<li>results: 研究结果表明，该算法在 Seven-point Skin Lesion 数据集上表现更好于其他状态对照算法，并且能够准确地识别多种皮肤病变。<details>
<summary>Abstract</summary>
The clinical diagnosis of skin lesion involves the analysis of dermoscopic and clinical modalities. Dermoscopic images provide a detailed view of the surface structures whereas clinical images offer a complementary macroscopic information. The visual diagnosis of melanoma is also based on seven-point checklist which involves identifying different visual attributes. Recently, supervised learning approaches such as convolutional neural networks (CNNs) have shown great performances using both dermoscopic and clinical modalities (Multi-modality). The seven different visual attributes in the checklist are also used to further improve the the diagnosis. The performances of these approaches, however, are still reliant on the availability of large-scaled labeled data. The acquisition of annotated dataset is an expensive and time-consuming task, more so with annotating multi-attributes. To overcome this limitation, we propose a self-supervised learning (SSL) algorithm for multi-modality skin lesion classification. Our algorithm enables the multi-modality learning by maximizing the similarities between paired dermoscopic and clinical images from different views. In addition, we generate surrogate pseudo-multi-labels that represent seven attributes via clustering analysis. We also propose a label-relation-aware module to refine each pseudo-label embedding and capture the interrelationships between pseudo-multi-labels. We validated the effectiveness of our algorithm using well-benchmarked seven-point skin lesion dataset. Our results show that our algorithm achieved better performances than other state-of-the-art SSL counterparts.
</details>
<details>
<summary>摘要</summary>
诊断皮肤损伤的临床 диагностика involves 分析 dermoscopic 和临床特征。 dermoscopic 图像提供表面结构的详细视图，而临床图像提供较大规模的信息。诊断 меланомой 还是基于七点检查表，包括不同的视觉特征。最近，supervised learning 方法 such as convolutional neural networks (CNNs) 在多Modalities 上表现出色，使用dermoscopic 和临床特征。七个不同的视觉特征也用于进一步改进诊断。然而，这些方法的表现仍然受到大规模标注数据的可用性的限制。获取标注数据集是一项expensive 和时间consuming的任务，更是与标注多属性。为了突破这些限制，我们提出了一种自助学习（SSL）算法 для多Modalities 皮肤损伤分类。我们的算法使得多Modalities 学习，通过最大化不同视角dermoscopic 和临床图像之间的相似性。此外，我们生成了surrogate pseudo-multi-labels，通过分类分析代表七个属性。我们还提出了一种标签关系意识模块，用于修复每个 pseudo-label embedding 并捕捉 pseudo-multi-labels 之间的关系。我们 validated 我们的算法使用 well-benchmarked 七点皮肤损伤数据集。我们的结果显示，我们的算法在与其他状态时的SSL 对手中表现出色。
</details></li>
</ul>
<hr>
<h2 id="MultiScale-Spectral-Spatial-Convolutional-Transformer-for-Hyperspectral-Image-Classification"><a href="#MultiScale-Spectral-Spatial-Convolutional-Transformer-for-Hyperspectral-Image-Classification" class="headerlink" title="MultiScale Spectral-Spatial Convolutional Transformer for Hyperspectral Image Classification"></a>MultiScale Spectral-Spatial Convolutional Transformer for Hyperspectral Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18550">http://arxiv.org/abs/2310.18550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Gong, Xian Zhou, Wen Yao</li>
<li>For: The paper is written for hyperspectral image classification, and it proposes a new architecture called MultiscaleFormer that captures both spectral and spatial information.* Methods: The proposed method uses multiscale spatial patches as tokens to formulate the spatial Transformer, and generates multiscale spectral-spatial representation of each pixel. It also uses a modified spectral-spatial CAF module to fuse cross-layer spectral and spatial information.* Results: The proposed method outperforms other architectures for hyperspectral image classification on commonly used real-world datasets.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了干涉谱图像分类而写的，并提出了一种新的架构方案 called MultiscaleFormer，它能够捕捉谱图像的 spectral 和 spatial 信息。* Methods: 该方法使用多个级别的空间块作为 токен，以形成空间 transformer，并生成每个像素的多级 spectral-spatial 表示。它还使用一种修改后的 spectral-spatial CAF 模块来融合层次 spectral 和 spatial 信息。* Results: 该方法在常用的实际 dataset 上进行了实验，并与其他架构进行了比较，结果显示了该方法的优越性。<details>
<summary>Abstract</summary>
Due to the powerful ability in capturing the global information, Transformer has become an alternative architecture of CNNs for hyperspectral image classification. However, general Transformer mainly considers the global spectral information while ignores the multiscale spatial information of the hyperspectral image. In this paper, we propose a multiscale spectral-spatial convolutional Transformer (MultiscaleFormer) for hyperspectral image classification. First, the developed method utilizes multiscale spatial patches as tokens to formulate the spatial Transformer and generates multiscale spatial representation of each band in each pixel. Second, the spatial representation of all the bands in a given pixel are utilized as tokens to formulate the spectral Transformer and generate the multiscale spectral-spatial representation of each pixel. Besides, a modified spectral-spatial CAF module is constructed in the MultiFormer to fuse cross-layer spectral and spatial information. Therefore, the proposed MultiFormer can capture the multiscale spectral-spatial information and provide better performance than most of other architectures for hyperspectral image classification. Experiments are conducted over commonly used real-world datasets and the comparison results show the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
由于Transformer的强大能力 capture global information，因此成为了干扰器的替代架构for hyperspectral image classification。然而，通常的Transformer主要考虑全球 spectral information，而忽略了多尺度空间信息的干扰器图像。在本文中，我们提出了一种多尺度 spectral-spatial convolutional Transformer（MultiscaleFormer）for hyperspectral image classification。首先，我们开发的方法使用多尺度空间块作为токен，并生成每个像素的多尺度空间表示。其次，所有帧在每个像素中的 spectral representation被作为 tokens，并生成每个像素的多尺度 spectral-spatial表示。此外，我们修改了 spectral-spatial CAF模块，以融合层次 spectral和空间信息。因此，我们提出的 MultiFormer 可以捕捉多尺度 spectral-spatial信息，并提供更好的性能 than most other architectures for hyperspectral image classification。我们对常用的实际 dataset进行了实验，并 compare 结果表明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="MEDAVET-Traffic-Vehicle-Anomaly-Detection-Mechanism-based-on-spatial-and-temporal-structures-in-vehicle-traffic"><a href="#MEDAVET-Traffic-Vehicle-Anomaly-Detection-Mechanism-based-on-spatial-and-temporal-structures-in-vehicle-traffic" class="headerlink" title="MEDAVET: Traffic Vehicle Anomaly Detection Mechanism based on spatial and temporal structures in vehicle traffic"></a>MEDAVET: Traffic Vehicle Anomaly Detection Mechanism based on spatial and temporal structures in vehicle traffic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18548">http://arxiv.org/abs/2310.18548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana Rosalía Huamán Reyna, Alex Josué Flórez Farfán, Geraldo Pereira Rocha Filho, Sandra Sampaio, Robson de Grande, Luis Hideo, Vasconcelos Nakamura, Rodolfo Ipolito Meneguette</li>
<li>for: 这篇论文是为了模型交通异常检测而写的。</li>
<li>methods: 该论文使用计算机视觉技术进行车辆跟踪，并使用бипаolar图和Convex Hull算法定义运动区域。异常检测使用QuadTree和靠近 occluded 的数据结构。</li>
<li>results: 实验结果显示，该方法在 Track4 测试集上得到了85.7% 的 F1 分数和25.432 的平方差。<details>
<summary>Abstract</summary>
Currently, there are computer vision systems that help us with tasks that would be dull for humans, such as surveillance and vehicle tracking. An important part of this analysis is to identify traffic anomalies. An anomaly tells us that something unusual has happened, in this case on the highway. This paper aims to model vehicle tracking using computer vision to detect traffic anomalies on a highway. We develop the steps of detection, tracking, and analysis of traffic: the detection of vehicles from video of urban traffic, the tracking of vehicles using a bipartite graph and the Convex Hull algorithm to delimit moving areas. Finally for anomaly detection we use two data structures to detect the beginning and end of the anomaly. The first is the QuadTree that groups vehicles that are stopped for a long time on the road and the second that approaches vehicles that are occluded. Experimental results show that our method is acceptable on the Track4 test set, with an F1 score of 85.7% and a mean squared error of 25.432.
</details>
<details>
<summary>摘要</summary>
现在，计算机视觉系统可以帮助我们完成一些人类厌热的任务，如Surveillance和车辆跟踪。这个分析的一个重要组成部分是检测交通异常。异常告诉我们 чтоomething不寻常发生在公路上。这篇论文旨在通过计算机视觉来模型车辆跟踪，检测公路上的交通异常。我们开发了检测、跟踪和分析交通的步骤：从城市交通视频中检测车辆，使用二分图和Convex Hull算法来定义移动区域，并用两种数据结构来检测异常的开始和结束。实验结果表明，我们的方法在Track4测试集上得到了可接受的结果，F1分数为85.7%，平均方差为25.432。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/28/cs.CV_2023_10_28/" data-id="cloimip9t00jus488fphh8vmy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/4/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
