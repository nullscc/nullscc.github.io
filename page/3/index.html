
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/3/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_09_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/07/cs.SD_2023_09_07/" class="article-date">
  <time datetime="2023-09-07T15:00:00.000Z" itemprop="datePublished">2023-09-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/07/cs.SD_2023_09_07/">cs.SD - 2023-09-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multiple-Representation-Transfer-from-Large-Language-Models-to-End-to-End-ASR-Systems"><a href="#Multiple-Representation-Transfer-from-Large-Language-Models-to-End-to-End-ASR-Systems" class="headerlink" title="Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems"></a>Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04031">http://arxiv.org/abs/2309.04031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takuma Udagawa, Masayuki Suzuki, Gakuto Kurata, Masayasu Muraoka, George Saon</li>
<li>for: 实现语言模型知识传递到终端自动语音识别系统中</li>
<li>methods: 使用多种方法获取和传递多个语言模型表示</li>
<li>results: 显示将多个语言模型表示传递到转构器基本自动语音识别系统中可以取得更好的效果<details>
<summary>Abstract</summary>
Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single representation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的知识传递是一种可能性的技术，用于将语言知识结合到端到端自动语音识别系统（ASR）中。然而，现有的工作只将单一的表现传递到 LLM（例如预训BERT的最后一层），而文本表现的非唯一性可以从不同层、上下文和模型中获取。在这个工作中，我们探索了许多方法来获取和传递多个 LLM 的表现到探索式 ASR 系统。尽管概念简单，但我们显示了传递多个 LLM 的表现可以是有效的代替。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Audio-Captioning-via-Audibility-Guidance"><a href="#Zero-Shot-Audio-Captioning-via-Audibility-Guidance" class="headerlink" title="Zero-Shot Audio Captioning via Audibility Guidance"></a>Zero-Shot Audio Captioning via Audibility Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03884">http://arxiv.org/abs/2309.03884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tal Shaharabany, Ariel Shaulov, Lior Wolf</li>
<li>for: 这篇论文主要关注的是Audio Captioning зада题，它的目标是生成对应音频文本的描述。</li>
<li>methods: 这篇论文提出了三个希望目标（fluency, faithfulness, audibility），并使用了三个网络来实现这些目标：一个大语言模型GPT-2、一个多媒体匹配网络ImageBind、以及一个文本分类器。</li>
<li>results: 研究人员通过对AudioCap数据集进行测试，发现使用audibility导向 significantly enhances表现，比基线更好。<details>
<summary>Abstract</summary>
The task of audio captioning is similar in essence to tasks such as image and video captioning. However, it has received much less attention. We propose three desiderata for captioning audio -- (i) fluency of the generated text, (ii) faithfulness of the generated text to the input audio, and the somewhat related (iii) audibility, which is the quality of being able to be perceived based only on audio. Our method is a zero-shot method, i.e., we do not learn to perform captioning. Instead, captioning occurs as an inference process that involves three networks that correspond to the three desired qualities: (i) A Large Language Model, in our case, for reasons of convenience, GPT-2, (ii) A model that provides a matching score between an audio file and a text, for which we use a multimodal matching network called ImageBind, and (iii) A text classifier, trained using a dataset we collected automatically by instructing GPT-4 with prompts designed to direct the generation of both audible and inaudible sentences. We present our results on the AudioCap dataset, demonstrating that audibility guidance significantly enhances performance compared to the baseline, which lacks this objective.
</details>
<details>
<summary>摘要</summary>
audio captioning的任务类似于图像和视频captioning，但它受到了远 fewer attention。我们提出了三个愿景 для captioning audio：（i）流畅的生成文本，（ii）对输入音频的 faithfulness，以及一定程度相关的（iii）可见度，即基于 purely audio 可以Perceived。我们的方法是一种零极方法，即不需要学习 captioning。而是通过三个网络来完成推理过程：（i）一个大语言模型，我们使用 GPT-2 的框架，（ii）一个对 audio 文件和文本进行匹配的模型，我们使用一个多modal matching network called ImageBind，（iii）一个基于自动生成的文本分类器，我们使用一个自动生成的 dataset 来训练。我们在 AudioCap 数据集上展示了我们的结果，显示了带有可见度指导的性能明显高于基eline，lacking this objective。
</details></li>
</ul>
<hr>
<h2 id="Causal-Signal-Based-DCCRN-with-Overlapped-Frame-Prediction-for-Online-Speech-Enhancement"><a href="#Causal-Signal-Based-DCCRN-with-Overlapped-Frame-Prediction-for-Online-Speech-Enhancement" class="headerlink" title="Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement"></a>Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03684">http://arxiv.org/abs/2309.03684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julitta Bartolewska, Stanisław Kacprzak, Konrad Kowalczyk</li>
<li>for: 提高单道麦克风信号质量和 inteligibilty</li>
<li>methods: 使用信号基于的 causal DCCRN，减少预测覆盖范围和网络参数数量</li>
<li>results: 实验结果显示，提posed模型在不同的语音提升指标方面具有相似或更好的性能，同时降低缓存延迟和网络参数数量约30%<details>
<summary>Abstract</summary>
The aim of speech enhancement is to improve speech signal quality and intelligibility from a noisy microphone signal. In many applications, it is crucial to enable processing with small computational complexity and minimal requirements regarding access to future signal samples (look-ahead). This paper presents signal-based causal DCCRN that improves online single-channel speech enhancement by reducing the required look-ahead and the number of network parameters. The proposed modifications include complex filtering of the signal, application of overlapped-frame prediction, causal convolutions and deconvolutions, and modification of the loss function. Results of performed experiments indicate that the proposed model with overlapped signal prediction and additional adjustments, achieves similar or better performance than the original DCCRN in terms of various speech enhancement metrics, while it reduces the latency and network parameter number by around 30%.
</details>
<details>
<summary>摘要</summary>
“目的是提高语音信号质量和可识别度，从含噪 microphone 信号中提取出清晰的语音信号。在许多应用中，需要减少计算复杂性和未来信号样本的需求（look-ahead）。这篇论文提出了信号基于的 causal DCCRN，用于在线单通道语音增强，提高语音质量和可识别度，同时减少计算复杂性和网络参数数量。提出的修改包括信号复杂滤波、叠加框预测、 causal 卷积和卷积反卷积，以及损失函数修改。实验结果表明，提出的模型，与原始 DCCRN 相比，在多种语音增强指标上具有类似或更好的性能，同时降低了延迟和网络参数数量约30%。”
</details></li>
</ul>
<hr>
<h2 id="Spiking-Structured-State-Space-Model-for-Monaural-Speech-Enhancement"><a href="#Spiking-Structured-State-Space-Model-for-Monaural-Speech-Enhancement" class="headerlink" title="Spiking Structured State Space Model for Monaural Speech Enhancement"></a>Spiking Structured State Space Model for Monaural Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03641">http://arxiv.org/abs/2309.03641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Du, Xu Liu, Yansong Chua</li>
<li>for: 提高speech干扰除之效率和计算成本，使用深度学习方法。</li>
<li>methods: 使用Spiking Structured State Space Model (Spiking-S4)，结合Spiking Neural Networks (SNN) 的能量效率和Structured State Space Models (S4) 的长距离序列模型能力。</li>
<li>results: 对 DNS Challenge 和 VoiceBank+Demand  datasets 进行评估，发现Spiking-S4 与现有Artificial Neural Network (ANN) 方法相当，但具有更少的计算资源（参数和 Floating Point Operations (FLOPs)）。<details>
<summary>Abstract</summary>
Speech enhancement seeks to extract clean speech from noisy signals. Traditional deep learning methods face two challenges: efficiently using information in long speech sequences and high computational costs. To address these, we introduce the Spiking Structured State Space Model (Spiking-S4). This approach merges the energy efficiency of Spiking Neural Networks (SNN) with the long-range sequence modeling capabilities of Structured State Space Models (S4), offering a compelling solution. Evaluation on the DNS Challenge and VoiceBank+Demand Datasets confirms that Spiking-S4 rivals existing Artificial Neural Network (ANN) methods but with fewer computational resources, as evidenced by reduced parameters and Floating Point Operations (FLOPs).
</details>
<details>
<summary>摘要</summary>
speech enhancement aims to extract clean speech from noisy signals. traditional deep learning methods face two challenges: efficiently using information in long speech sequences and high computational costs. to address these, we introduce the spiking structured state space model (spiking-s4). this approach merges the energy efficiency of spiking neural networks (snn) with the long-range sequence modeling capabilities of structured state space models (s4), offering a compelling solution. evaluation on the dns challenge and voicebank+demand datasets confirms that spiking-s4 rivals existing artificial neural network (ann) methods but with fewer computational resources, as evidenced by reduced parameters and floating point operations (flops).
</details></li>
</ul>
<hr>
<h2 id="Topological-fingerprints-for-audio-identification"><a href="#Topological-fingerprints-for-audio-identification" class="headerlink" title="Topological fingerprints for audio identification"></a>Topological fingerprints for audio identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03516">http://arxiv.org/abs/2309.03516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wreise/top_audio_id">https://github.com/wreise/top_audio_id</a></li>
<li>paper_authors: Wojciech Reise, Ximena Fernández, Maria Dominguez, Heather A. Harrington, Mariano Beguerisse-Díaz</li>
<li>for: 这篇论文是为了开发一种Audio文件重复检测的方法，以便在不同的情况下准确地检测Audio文件的重复性。</li>
<li>methods: 该方法使用 persistente homology 技术，对 audio signal 的 local spectral decomposition 进行计算，并使用 filtered cubical complexes 来编码 audio content。</li>
<li>results: 实验结果表明，该方法可以准确地检测时间对齐的 audio 轨迹，并且在 topological distortions 的情况下表现出excel。<details>
<summary>Abstract</summary>
We present a topological audio fingerprinting approach for robustly identifying duplicate audio tracks. Our method applies persistent homology on local spectral decompositions of audio signals, using filtered cubical complexes computed from mel-spectrograms. By encoding the audio content in terms of local Betti curves, our topological audio fingerprints enable accurate detection of time-aligned audio matchings. Experimental results demonstrate the accuracy of our algorithm in the detection of tracks with the same audio content, even when subjected to various obfuscations. Our approach outperforms existing methods in scenarios involving topological distortions, such as time stretching and pitch shifting.
</details>
<details>
<summary>摘要</summary>
我们提出了一种拓扑音频指纹方法，用于坚定地识别 duplicates 的音频轨迹。我们的方法使用 persistente homology 在本地 spectral decompositions 中应用 filtered cubical complexes，从 mel-spectrograms 中计算出的 audio signals。通过将音频内容编码成本地 Betti 曲线，我们的拓扑音频指纹可以准确地检测时间对齐的音频匹配。实验结果表明我们的算法在包括拓扑扭曲在内的不同场景下具有高精度，比如时间延迟和调高。我们的方法也超过了现有的方法，在拓扑扭曲场景下表现更佳。
</details></li>
</ul>
<hr>
<h2 id="Simulating-room-transfer-functions-between-transducers-mounted-on-audio-devices-using-a-modified-image-source-method"><a href="#Simulating-room-transfer-functions-between-transducers-mounted-on-audio-devices-using-a-modified-image-source-method" class="headerlink" title="Simulating room transfer functions between transducers mounted on audio devices using a modified image source method"></a>Simulating room transfer functions between transducers mounted on audio devices using a modified image source method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03486">http://arxiv.org/abs/2309.03486</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audiolabs/DEISM">https://github.com/audiolabs/DEISM</a></li>
<li>paper_authors: Zeyu Xu, Adrian Herzog, Alexander Lodermeyer, Emanuël A. P. Habets, Albert G. Prinn</li>
<li>for: 本研究旨在扩展图像源方法（ISM），以包括听取器和发射器附件的声学折射效应。</li>
<li>methods: 本研究使用圆锥幂直径幂系数来扩展ISM，以包括发射器和听取器附件的声学折射效应。</li>
<li>results: 研究表明，提案方法的准确性与房间内设备的大小、形状、数量和位置有关。一种简化版的提案方法也被提出，可以大幅减少计算成本。<details>
<summary>Abstract</summary>
The image source method (ISM) is often used to simulate room acoustics due to its ease of use and computational efficiency. The standard ISM is limited to simulations of room impulse responses between point sources and omnidirectional receivers. In this work, the ISM is extended using spherical harmonic directivity coefficients to include acoustic diffraction effects due to source and receiver transducers mounted on physical devices, which are typically encountered in practical situations. The proposed method is verified using finite element simulations of various loudspeaker and microphone configurations in a rectangular room. It is shown that the accuracy of the proposed method is related to the sizes, shapes, number, and positions of the devices inside a room. A simplified version of the proposed method, which can significantly reduce computational effort, is also presented. The proposed method and its simplified version can simulate room transfer functions more accurately than currently available image source methods and can aid the development and evaluation of speech and acoustic signal processing algorithms, including speech enhancement, acoustic scene analysis, and acoustic parameter estimation.
</details>
<details>
<summary>摘要</summary>
<SYS><TRANSLATE>图像源方法（ISM）经常用于模拟房间听音，因为它的使用容易和计算效率高。标准的ISM仅能模拟房间冲击响应 между点源和全irectional接收器。在这种工作中，ISM通过使用圆锥幂直径系数来包括听音折射效应，由源和接收器适配器安装在物理设备上，这些设备通常在实际应用中遇到。提议的方法被证明通过finite element simulations of various loudspeaker和microphone配置在rectangular room中。结果表明，提议的方法的准确性与房间内设备的大小、形状、数量和位置有关。一个简化版的提议方法，可以减少计算努力，也被提出。提议的方法和其简化版可以更准确地模拟房间传递函数，并且可以帮助开发和评估speech和听音信号处理算法，包括speech增强、听音场分析和听音参数估计。</TRANSLATE></SYS>Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/07/cs.SD_2023_09_07/" data-id="clmjn91oe00bx0j88cfdyc00v" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/07/cs.LG_2023_09_07/" class="article-date">
  <time datetime="2023-09-07T10:00:00.000Z" itemprop="datePublished">2023-09-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/07/cs.LG_2023_09_07/">cs.LG - 2023-09-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Bayesian-Dynamic-DAG-Learning-Application-in-Discovering-Dynamic-Effective-Connectome-of-Brain"><a href="#Bayesian-Dynamic-DAG-Learning-Application-in-Discovering-Dynamic-Effective-Connectome-of-Brain" class="headerlink" title="Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain"></a>Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07080">http://arxiv.org/abs/2309.07080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdolmahdi Bagheri, Mohammad Pasande, Kevin Bello, Alireza Akhondi-Asl, Babak Nadjar Araabi</li>
<li>for: 提取 Dynamic Effective Connectome (DEC) 可以揭示脑中复杂的机制。</li>
<li>methods: 使用 Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization（BDyMA）方法，可以解决高维动态DAG发现和 fMRI 数据质量低的两大挑战。</li>
<li>results: 比较baseline和现有方法，BDyMA方法可以更准确地检测高维网络，并且可以 incorporate  prior knowledge 进行动态 causal发现，提高结果的准确性。<details>
<summary>Abstract</summary>
Understanding the complex mechanisms of the brain can be unraveled by extracting the Dynamic Effective Connectome (DEC). Recently, score-based Directed Acyclic Graph (DAG) discovery methods have shown significant improvements in extracting the causal structure and inferring effective connectivity. However, learning DEC through these methods still faces two main challenges: one with the fundamental impotence of high-dimensional dynamic DAG discovery methods and the other with the low quality of fMRI data. In this paper, we introduce Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization \textbf{(BDyMA)} method to address the challenges in discovering DEC. The presented dynamic causal model enables us to discover bidirected edges as well. Leveraging an unconstrained framework in the BDyMA method leads to more accurate results in detecting high-dimensional networks, achieving sparser outcomes, making it particularly suitable for extracting DEC. Additionally, the score function of the BDyMA method allows the incorporation of prior knowledge into the process of dynamic causal discovery which further enhances the accuracy of results. Comprehensive simulations on synthetic data and experiments on Human Connectome Project (HCP) data demonstrate that our method can handle both of the two main challenges, yielding more accurate and reliable DEC compared to state-of-the-art and baseline methods. Additionally, we investigate the trustworthiness of DTI data as prior knowledge for DEC discovery and show the improvements in DEC discovery when the DTI data is incorporated into the process.
</details>
<details>
<summary>摘要</summary>
理解大脑的复杂机制可以通过提取动态有效连ome（DEC）来解开。最近，基于分数的导向无环图（DAG）发现方法在提取 causal 结构和推导有效连接方面表现出了显著的改进。然而，通过这些方法学习 DEC 仍面临两个主要挑战：一是高维动态 DAG 发现方法的基础不足，二是 fMRI 数据质量低下。在这篇文章中，我们介绍了 Bayesian 动态 DAG 学习方法（BDyMA），用于解决这两个挑战。BDyMA 方法可以检测 bidirected 边，并且利用不受限制的框架，以获得更高精度的结果，特别适合提取 DEC。此外，BDyMA 方法的分数函数允许在动态 causal 发现过程中 incorporate 先前知识，进一步提高结果的准确性。我们在 synthetic 数据和 HCP 数据上进行了广泛的 simulate 和实验，结果表明，我们的方法可以解决两个主要挑战，并且与现有基准方法和基准方法相比，提取 DEC 的结果更加准确和可靠。此外，我们还 investigate DTI 数据的可靠性作为 DEC 发现的先前知识，并证明在 incorporate DTI 数据到发现过程中，可以提高 DEC 的准确性。
</details></li>
</ul>
<hr>
<h2 id="SRN-SZ-Deep-Leaning-Based-Scientific-Error-bounded-Lossy-Compression-with-Super-resolution-Neural-Networks"><a href="#SRN-SZ-Deep-Leaning-Based-Scientific-Error-bounded-Lossy-Compression-with-Super-resolution-Neural-Networks" class="headerlink" title="SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks"></a>SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04037">http://arxiv.org/abs/2309.04037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyang Liu, Sheng Di, Sian Jin, Kai Zhao, Xin Liang, Zizhong Chen, Franck Cappello</li>
<li>for: 科学数据压缩（error-bound lossy compression）技术的应用，以提高现代超级计算机系统中科学数据的管理效率。</li>
<li>methods: 利用深度学习网络（super-resolution neural networks）实现数据格子扩展（hierarchical data grid expansion）方法，并使用最新的超分辨率网络HAT进行压缩。</li>
<li>results: SRN-SZ在比较多种现有压缩器的实验中，可以达到75%的压缩率提升，同时保持与PSNR相同的错误约束。<details>
<summary>Abstract</summary>
The fast growth of computational power and scales of modern super-computing systems have raised great challenges for the management of exascale scientific data. To maintain the usability of scientific data, error-bound lossy compression is proposed and developed as an essential technique for the size reduction of scientific data with constrained data distortion. Among the diverse datasets generated by various scientific simulations, certain datasets cannot be effectively compressed by existing error-bounded lossy compressors with traditional techniques. The recent success of Artificial Intelligence has inspired several researchers to integrate neural networks into error-bounded lossy compressors. However, those works still suffer from limited compression ratios and/or extremely low efficiencies. To address those issues and improve the compression on the hard-to-compress datasets, in this paper, we propose SRN-SZ, which is a deep learning-based scientific error-bounded lossy compressor leveraging the hierarchical data grid expansion paradigm implemented by super-resolution neural networks. SRN-SZ applies the most advanced super-resolution network HAT for its compression, which is free of time-costing per-data training. In experiments compared with various state-of-the-art compressors, SRN-SZ achieves up to 75% compression ratio improvements under the same error bound and up to 80% compression ratio improvements under the same PSNR than the second-best compressor.
</details>
<details>
<summary>摘要</summary>
现代超级计算系统的快速增长和大规模数据管理问题，带来了科学数据的可用性带来挑战。为维护科学数据的可用性，Error-bound lossy compression被提出和开发为科学数据的大小减少技术。不同的科学仿真数据之间，一些数据无法使用现有的Error-bounded lossy compressor进行有效压缩。人工智能的最近成功，许多研究人员在Error-bounded lossy compressor中 интеGRATE neural networks。然而，这些工作仍然受到压缩率有限和/或非常低效的困扰。为解决这些问题并提高压缩硬件数据，在这篇论文中，我们提出了SRN-SZ，这是一种基于深度学习的科学Error-bounded lossy compressor，利用层次数据格式扩展 paradigm和超分辨率神经网络（HAT）进行压缩。SRN-SZ在压缩中使用了HAT，免除了每个数据需要时间成本的训练。在对比各种现状 compressor 的实验中，SRN-SZ在同等Error bound下实现了75%的压缩率提升，并在同等PSNR下实现了80%的压缩率提升。
</details></li>
</ul>
<hr>
<h2 id="Brief-technical-note-on-linearizing-recurrent-neural-networks-RNNs-before-vs-after-the-pointwise-nonlinearity"><a href="#Brief-technical-note-on-linearizing-recurrent-neural-networks-RNNs-before-vs-after-the-pointwise-nonlinearity" class="headerlink" title="Brief technical note on linearizing recurrent neural networks (RNNs) before vs after the pointwise nonlinearity"></a>Brief technical note on linearizing recurrent neural networks (RNNs) before vs after the pointwise nonlinearity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04030">http://arxiv.org/abs/2309.04030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marino Pagan, Adrian Valente, Srdjan Ostojic, Carlos D. Brody</li>
<li>for: 这个论文研究了环境依赖性的神经网络（RNN）动态系统的特性。</li>
<li>methods: 该论文使用了两种不同的线性化方法来研究RNN动态系统：一种是基于活动（输出单元后点wise非线性）的线性化，另一种是基于活动（输入单元前点wise非线性）的线性化。</li>
<li>results: 该论文发现了这两种线性化方法之间的关系，以及它们对动态系统的影响。它还显示了在活动动态系统中存在一些Context-dependent效果，而在活动动态系统中这些效果并不明显。<details>
<summary>Abstract</summary>
Linearization of the dynamics of recurrent neural networks (RNNs) is often used to study their properties. The same RNN dynamics can be written in terms of the ``activations" (the net inputs to each unit, before its pointwise nonlinearity) or in terms of the ``activities" (the output of each unit, after its pointwise nonlinearity); the two corresponding linearizations are different from each other. This brief and informal technical note describes the relationship between the two linearizations, between the left and right eigenvectors of their dynamics matrices, and shows that some context-dependent effects are readily apparent under linearization of activity dynamics but not linearization of activation dynamics.
</details>
<details>
<summary>摘要</summary>
linearization of recurrent neural networks (RNNs) 的动态是经常用来研究其性质。这些 RNN 动态可以表示为“活动”（每个单元的输出， после其点对点非线性）或“启动”（每个单元的输入， перед其点对点非线性）；这两种对应的 linearization 是不同的。这篇简短且不正式的技术笔记描述了这两种 linearization 之间的关系，以及它们动态矩阵的左和右各 eigenvector 之间的关系，并证明了在活动动力化下有些上下文依赖的效果是不太明显的。
</details></li>
</ul>
<hr>
<h2 id="TIDE-Textual-Identity-Detection-for-Evaluating-and-Augmenting-Classification-and-Language-Models"><a href="#TIDE-Textual-Identity-Detection-for-Evaluating-and-Augmenting-Classification-and-Language-Models" class="headerlink" title="TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models"></a>TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04027">http://arxiv.org/abs/2309.04027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/TIDAL">https://github.com/google-research-datasets/TIDAL</a></li>
<li>paper_authors: Emmanuel Klu, Sameer Sethi</li>
<li>for: 本研究旨在提高文本分类和生成模型的公平性，尤其是在文本数据集中检测和修正不公平的数据和模型。</li>
<li>methods: 本研究使用了一个新的标识词典（TIDAL），包含15,123个标识词和其相关的感受上下文，以提高文本中的标识上下文可用性和Machine Learning公平性技术的效果。</li>
<li>results: 研究发现，使用助手注释技术可以提高人工审核过程的可靠性和速度，而我们的数据集和方法在评估和修正期间更能发现不平等现象，并生成更公平的模型。这些方法为实际场景中扩大分类和生成模型公平性提供了一个实用的路线图。<details>
<summary>Abstract</summary>
Machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. Evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. When these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. In this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. We create a new, more comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ML fairness techniques. We evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. Results show our assistive annotation technique improves the reliability and velocity of human-in-the-loop processes. Our dataset and methods uncover more disparities during evaluation, and also produce more fair models during remediation. These approaches provide a practical path forward for scaling classifier and generative model fairness in real-world settings.
</details>
<details>
<summary>摘要</summary>
We create a new and comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and their associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of machine learning fairness techniques. We evaluate our approaches using human contributors, and conduct experiments focused on dataset and model debiasing. Our results show that our assistive annotation technique improves the reliability and velocity of human-in-the-loop processes. Our dataset and methods uncover more disparities during evaluation, and also produce more fair models during remediation. These approaches provide a practical path forward for scaling classifier and generative model fairness in real-world settings.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Transport-with-Tempered-Exponential-Measures"><a href="#Optimal-Transport-with-Tempered-Exponential-Measures" class="headerlink" title="Optimal Transport with Tempered Exponential Measures"></a>Optimal Transport with Tempered Exponential Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04015">http://arxiv.org/abs/2309.04015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Amid, Frank Nielsen, Richard Nock, Manfred K. Warmuth</li>
<li>for: 这个论文主要是为了解决优化运输问题中的两个主要子领域之间的冲突：一是不含规则的优化运输（”`a-la-Kantorovich”），它会导致计划非常稀疏，但算法性能不佳；另一个是带有Entropic规则的优化运输（”`a-la-Sinkhorn-Cuturi”），它可以获得近线性approximation算法，但计划会非常稠密。</li>
<li>methods: 这篇论文提出了一种渐进的扩展，即将带有渐进的凝聚排序（tempered exponential measures）应用于优化运输问题中。这种方法可以同时实现非常快的approximation算法和计划的稀疏性。</li>
<li>results: 论文的结果表明，这种渐进的扩展可以在优化运输问题中获得非常好的性能，具有非常快的approximation算法和计划的稀疏性。此外，它还可以自然地应用于不平衡优化运输问题中。<details>
<summary>Abstract</summary>
In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, ``\`a-la-Kantorovich'', which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, ``\`a-la-Sinkhorn-Cuturi'', which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that a generalization of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity which is under control up to sparsity patterns. In addition, it fits naturally in the unbalanced optimal transport problem setting as well.
</details>
<details>
<summary>摘要</summary>
在优化运输领域，有两个显著的子领域面对着：（i）无杂化优化运输，“\`a-la-Kantorovich”，它会导致非常稀疏的计划，但算法执行效率不佳；（ii）Entropic-regulated optimal transport，“\`a-la-Sinkhorn-Cuturi”，它可以得到近线性approximation算法，但计划会变得非常稠密。在这篇论文中，我们展示了一种扩展后者的方法，即使用温和的指数分布，可以达到非常便利的中间位置，具有非常快的approximation算法和控制在某些尺度上的稀疏性。此外，它自然地适应了不均衡优化运输问题的设置。
</details></li>
</ul>
<hr>
<h2 id="An-Element-wise-RSAV-Algorithm-for-Unconstrained-Optimization-Problems"><a href="#An-Element-wise-RSAV-Algorithm-for-Unconstrained-Optimization-Problems" class="headerlink" title="An Element-wise RSAV Algorithm for Unconstrained Optimization Problems"></a>An Element-wise RSAV Algorithm for Unconstrained Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04013">http://arxiv.org/abs/2309.04013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiheng Zhang, Jiahao Zhang, Jie Shen, Guang Lin</li>
<li>for: 这个论文是为了提出一种新的优化算法，以满足不条件能量散失定律并且具有改进的能量对齐性。</li>
<li>methods: 这个算法使用了元素刻relaxed scalar auxiliary variable（E-RSAV）技术，并且提供了对 convex 设置的严格证明，以及在单变量情况下的加速算法，以提高线性增长率。</li>
<li>results: 作者通过大量的数学实验 validate了他们的算法的稳定性和快速增长率。<details>
<summary>Abstract</summary>
We present a novel optimization algorithm, element-wise relaxed scalar auxiliary variable (E-RSAV), that satisfies an unconditional energy dissipation law and exhibits improved alignment between the modified and the original energy. Our algorithm features rigorous proofs of linear convergence in the convex setting. Furthermore, we present a simple accelerated algorithm that improves the linear convergence rate to super-linear in the univariate case. We also propose an adaptive version of E-RSAV with Steffensen step size. We validate the robustness and fast convergence of our algorithm through ample numerical experiments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的优化算法，元素缓和Scalar助变量（E-RSAV），它满足不受条件的能量泄漏定律，并且在修改后和原始能量之间存在改善的对齐。我们的算法具有在凸设定下的严格证明的线性收敛性。此外，我们还提出了在单变量情况下使用加速器，以提高收敛率的超线性收敛。此外，我们还提出了适应E-RSAV的Steffensen步长。我们通过大量的数学实验证明了我们的算法的稳定性和快速收敛性。Here's the word-for-word translation of the text into Simplified Chinese:我们提出了一种新的优化算法，元素缓和Scalar助变量（E-RSAV），它满足不受条件的能量泄漏定律，并且在修改后和原始能量之间存在改善的对齐。我们的算法具有在凸设定下的严格证明的线性收敛性。此外，我们还提出了在单变量情况下使用加速器，以提高收敛率的超线性收敛。此外，我们还提出了适应E-RSAV的Steffensen步长。我们通过大量的数学实验证明了我们的算法的稳定性和快速收敛性。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Transformer-for-Material-Segmentation"><a href="#Multimodal-Transformer-for-Material-Segmentation" class="headerlink" title="Multimodal Transformer for Material Segmentation"></a>Multimodal Transformer for Material Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04001">http://arxiv.org/abs/2309.04001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif</li>
<li>for: 这篇论文的目的是提出一种新的多modalities融合策略，以增强多modalities识别材料的性能。</li>
<li>methods: 这篇论文提出了一个名为Multi-Modal Segmentation Transformer（MMSFormer）的新模型，该模型包括一个 novel fusion strategy，用于融合不同的四种模式：RGB、Angel of Linear Polarization（AoLP）、Degree of Linear Polarization（DoLP）和Near-Infrared（NIR）。</li>
<li>results: 根据MCubeS数据集的评估，MMSFormer模型取得了52.05%的mIoU，比前一代的模型高出9.1%和10.4%在检测砾石和人类类别上。<details>
<summary>Abstract</summary>
Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for overall model performance. Furthermore, our ablation studies also highlight the capacity of different input modalities to improve performance in the identification of different types of materials. The code and pretrained models will be made available at https://github.com/csiplab/MMSFormer.
</details>
<details>
<summary>摘要</summary>
利用多Modalities的信息共同 optimize multimodal segmentation任务的性能。然而，将不同Modalities的信息有效融合仍然是一个挑战，因为每种Modalities具有不同的特点。在这篇论文中，我们提出了一种新的融合策略，可以有效地融合不同组合的四种Modalities：RGB、Angle of Linear Polarization（AoLP）、Degree of Linear Polarization（DoLP）和 Near-Infrared（NIR）。我们还提出了一种新的模型，即Multi-Modal Segmentation Transformer（MMSFormer），该模型包含了提出的融合策略，用于进行多模态材料分 segmentation。MMSFormer实现了52.05%的mIoU，超过当前的州度-OF-艺（MCubeS）数据集的现状。例如，我们的方法在检测粗砾 (+10.4%)和人 (+9.1%)类型时提供了显著的改进。归因分析表明，不同的模块在融合块中具有重要的作用，并且不同的输入Modalities可以提高不同类型材料的识别性能。代码和预训练模型将在https://github.com/csiplab/MMSFormer上公开。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Self-Supervised-Representations-to-Multi-Domain-Setups"><a href="#Adapting-Self-Supervised-Representations-to-Multi-Domain-Setups" class="headerlink" title="Adapting Self-Supervised Representations to Multi-Domain Setups"></a>Adapting Self-Supervised Representations to Multi-Domain Setups</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03999">http://arxiv.org/abs/2309.03999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Kalibhat, Sam Sharpe, Jeremy Goodsitt, Bayan Bruss, Soheil Feizi</li>
<li>for: 提高多 домен数据上自动学习模型的泛化能力</li>
<li>methods: 提出了一种通用、轻量级的领域分离模块（DDM），可以插入任何自我监督编码器，以实现多个多样化领域中的表示学习</li>
<li>results: 对多个多样化领域的数据进行预训练，DDM可以提高自我监督模型的线性探测精度（最多3.5%），并且在无法获得领域标签时，DDM可以使用稳定的聚类方法来找到pseudo领域，从而提高模型对未看过的领域的泛化能力（7.4%）。<details>
<summary>Abstract</summary>
Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on multi-domain benchmarks including PACS, DomainNet and WILDS. Models trained with DDM show significantly improved generalization (7.4%) to unseen domains compared to baselines. Therefore, DDM can efficiently adapt self-supervised encoders to provide high-quality, generalizable representations for diverse multi-domain data.
</details>
<details>
<summary>摘要</summary>
当前最先进的自动编程方法，在各自的领域上有效，但是对未见的领域有限的泛化能力。我们发现这些模型在混合领域上训练时表现差异，使其不适用于实际世界上的多种多样化环境。因此，我们提出一个通用、轻量级的领域分离模块（DDM），可以与任何自动编程Encoder结合使用，以有效地进行多个多样化领域的表示学习，无论有多少共同类。在预训练期间，DDM通过自我指导的方式，在表示空间中强制实施分离，将领域特有的部分与领域不变的部分分离开。当领域标签不可用时，DDM使用了一种鲁棒的聚类方法来找到pseudo领域。我们显示，在DDM的预训练下，可以在多个多样化领域的测试集上显示3.5%的改进，包括SimCLR、MoCo、BYOL、DINO、SimSiam和Barlow Twins等自动编程模型。此外，DDM训练后的模型在未见领域上的泛化能力提高了7.4%。因此，DDM可以高效地适应自动编程Encoder，以提供高质量、泛化的表示，用于多种多样化的数据。
</details></li>
</ul>
<hr>
<h2 id="Creating-a-Systematic-ESG-Environmental-Social-Governance-Scoring-System-Using-Social-Network-Analysis-and-Machine-Learning-for-More-Sustainable-Company-Practices"><a href="#Creating-a-Systematic-ESG-Environmental-Social-Governance-Scoring-System-Using-Social-Network-Analysis-and-Machine-Learning-for-More-Sustainable-Company-Practices" class="headerlink" title="Creating a Systematic ESG (Environmental Social Governance) Scoring System Using Social Network Analysis and Machine Learning for More Sustainable Company Practices"></a>Creating a Systematic ESG (Environmental Social Governance) Scoring System Using Social Network Analysis and Machine Learning for More Sustainable Company Practices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05607">http://arxiv.org/abs/2309.05607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aarav Patel, Peter Gloor</li>
<li>For: This paper aims to create a data-driven ESG evaluation system that provides more balanced and systemized scores by incorporating social sentiment.* Methods: The authors use Python web scrapers to collect data from Wikipedia, Twitter, LinkedIn, and Google News for S&amp;P 500 companies, and then clean and analyze the data using NLP algorithms to obtain sentiment scores for ESG subcategories. They train and calibrate machine-learning algorithms with S&amp;P Global ESG Ratings to test their predictive capabilities.* Results: The Random-Forest model shows encouraging results with a mean absolute error of 13.4% and a correlation of 26.1% (p-value 0.0372), indicating that measuring ESG social sentiment across sub-categories can help executives focus efforts on areas people care about most.Here’s the simplified Chinese text:* For: 这篇论文旨在创建一种基于数据的 ESG 评估系统，以提供更加均衡和系统化的评估结果，通过包含社交情感。* Methods: 作者使用 Python 网络抓取器收集 Wikipedia、Twitter、LinkedIn 和 Google News 上 S&amp;P 500 公司的数据，然后清洁和分析数据，使用 NLP 算法获得 ESG 子类别的情感分数。他们使用机器学习算法与 S&amp;P 全球 ESG 评级进行训练和校准。* Results: Random Forest 模型显示了鼓动的结果， mean absolute error 为 13.4%，相关度为 26.1% (p-value 0.0372)，表明 mesure ESG 社交情感 across sub-categories 可以帮助行政人员更加专注于人们关心的领域。<details>
<summary>Abstract</summary>
Environmental Social Governance (ESG) is a widely used metric that measures the sustainability of a company practices. Currently, ESG is determined using self-reported corporate filings, which allows companies to portray themselves in an artificially positive light. As a result, ESG evaluation is subjective and inconsistent across raters, giving executives mixed signals on what to improve. This project aims to create a data-driven ESG evaluation system that can provide better guidance and more systemized scores by incorporating social sentiment. Social sentiment allows for more balanced perspectives which directly highlight public opinion, helping companies create more focused and impactful initiatives. To build this, Python web scrapers were developed to collect data from Wikipedia, Twitter, LinkedIn, and Google News for the S&P 500 companies. Data was then cleaned and passed through NLP algorithms to obtain sentiment scores for ESG subcategories. Using these features, machine-learning algorithms were trained and calibrated to S&P Global ESG Ratings to test their predictive capabilities. The Random-Forest model was the strongest model with a mean absolute error of 13.4% and a correlation of 26.1% (p-value 0.0372), showing encouraging results. Overall, measuring ESG social sentiment across sub-categories can help executives focus efforts on areas people care about most. Furthermore, this data-driven methodology can provide ratings for companies without coverage, allowing more socially responsible firms to thrive.
</details>
<details>
<summary>摘要</summary>
环境社会治理（ESG）是一种广泛使用的指标，用于衡量公司的可持续发展实践。目前，ESG是通过自我报告的公司签据来确定的，这使得ESG评估变得主观和不一致，导致公司获得混乱的信息，从而难以决策。本项目的目标是创建一个数据驱动的ESG评估系统，为公司提供更好的指导和更系统化的评分。这个系统通过 incorporating社会情绪来提供更均衡的视角，直接反映公众意见，帮助公司制定更加焦点和有效的措施。为建立这个系统，我们使用Python网络抓取器收集了S&P 500公司的数据，并从Wikipedia、Twitter、LinkedIn和Google News中提取了相关信息。然后，我们清洁了数据并通过自然语言处理（NLP）算法获得了ESG下的情绪分数。使用这些特征，我们使用机器学习算法进行训练和校准，并与S&P全球ESG评级进行比较。Random Forest模型在这些模型中表现最佳，其中 mean absolute error 为13.4%，相关度为26.1%（p-value 0.0372），这表明了这种方法的潜在力量。总之，通过评估ESG社会情绪在不同的子类别中，可以帮助公司更好地focus其努力在人们关心的方面。此外，这种数据驱动的方法ологи也可以为没有评级的公司提供评估， allowing more socially responsible firms to thrive。
</details></li>
</ul>
<hr>
<h2 id="ConDA-Contrastive-Domain-Adaptation-for-AI-generated-Text-Detection"><a href="#ConDA-Contrastive-Domain-Adaptation-for-AI-generated-Text-Detection" class="headerlink" title="ConDA: Contrastive Domain Adaptation for AI-generated Text Detection"></a>ConDA: Contrastive Domain Adaptation for AI-generated Text Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03992">http://arxiv.org/abs/2309.03992</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amritabh/conda-gen-text-detection">https://github.com/amritabh/conda-gen-text-detection</a></li>
<li>paper_authors: Amrita Bhattacharjee, Tharindu Kumarage, Raha Moraffah, Huan Liu</li>
<li>for: 这个论文的目的是建立一种可以检测AI生成的新闻文本的检测器。</li>
<li>methods: 该论文使用了域 adaptation 技术和对比学习来学习域不变的表示，以便在无监督情况下进行检测。</li>
<li>results: 该论文的实验结果显示，使用该方法可以获得较好的检测性能，比最佳基eline的表现提高了31.7%，并且与完全监督的检测器的表现相差只有0.8%。<details>
<summary>Abstract</summary>
Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain invariant representations that are effective for the final unsupervised detection task. Our experiments demonstrate the effectiveness of our framework, resulting in average performance gains of 31.7% from the best performing baselines, and within 0.8% margin of a fully supervised detector. All our code and data is available at https://github.com/AmritaBh/ConDA-gen-text-detection.
</details>
<details>
<summary>摘要</summary>
We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain-invariant representations that are effective for the final unsupervised detection task. Our experiments demonstrate the effectiveness of our framework, resulting in average performance gains of 31.7% from the best-performing baselines and within 0.8% margin of a fully supervised detector. All our code and data are available at <https://github.com/AmritaBh/ConDA-gen-text-detection>.
</details></li>
</ul>
<hr>
<h2 id="Derivation-of-Coordinate-Descent-Algorithms-from-Optimal-Control-Theory"><a href="#Derivation-of-Coordinate-Descent-Algorithms-from-Optimal-Control-Theory" class="headerlink" title="Derivation of Coordinate Descent Algorithms from Optimal Control Theory"></a>Derivation of Coordinate Descent Algorithms from Optimal Control Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03990">http://arxiv.org/abs/2309.03990</a></li>
<li>repo_url: None</li>
<li>paper_authors: I. M. Ross</li>
<li>for: 这篇论文探讨了一种新的优化原理，即将多种优化算法集成到一个中心来源中，以便更好地解决优化问题。</li>
<li>methods: 该论文使用了一种最大原理和一组”控制” Lyapunov 函数来 derivate 基本的坐标下降算法。</li>
<li>results: 该论文显示了坐标下降算法的转化是与控制的泪函数的控制的耗散相关的。<details>
<summary>Abstract</summary>
Recently, it was posited that disparate optimization algorithms may be coalesced in terms of a central source emanating from optimal control theory. Here we further this proposition by showing how coordinate descent algorithms may be derived from this emerging new principle. In particular, we show that basic coordinate descent algorithms can be derived using a maximum principle and a collection of max functions as "control" Lyapunov functions. The convergence of the resulting coordinate descent algorithms is thus connected to the controlled dissipation of their corresponding Lyapunov functions. The operational metric for the search vector in all cases is given by the Hessian of the convex objective function.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:近些时候，有人提出了一种可能将不同优化算法集中到一个中心源上的提议，我们在这里进一步发展这个提议，证明了基本坐标下降算法可以从这种新的原理中得到。具体来说，我们证明了基本坐标下降算法可以通过最大原理和一组"控制" Lyapunov 函数来 derivation。这些 Lyapunov 函数的满足程度的控制则导致坐标下降算法的 converge。搜索向量在所有情况下的操作度量是对对像函数的梯度。
</details></li>
</ul>
<hr>
<h2 id="Noisy-Computing-of-the-mathsf-OR-and-mathsf-MAX-Functions"><a href="#Noisy-Computing-of-the-mathsf-OR-and-mathsf-MAX-Functions" class="headerlink" title="Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions"></a>Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03986">http://arxiv.org/abs/2309.03986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Banghua Zhu, Ziao Wang, Nadim Ghaddar, Jiantao Jiao, Lele Wang</li>
<li>for: 这篇论文是关于计算函数的论文， specifically 是关于使用不准确的查询来计算 $\mathsf{OR}$ 函数和 $\mathsf{MAX}$ 函数。</li>
<li>methods: 这篇论文使用了不准确的查询来计算 $\mathsf{OR}$ 函数和 $\mathsf{MAX}$ 函数。</li>
<li>results: 论文表明了一个预期的查询数量为 $(1 \pm o(1)) \frac{n\log \frac{1}{\delta}{D_{\mathsf{KL}(p | 1-p)}$ 是足够和必要的来计算这两个函数，并且这个结论与之前的研究相比，对 $p$ 的依赖关系有所紧张。<details>
<summary>Abstract</summary>
We consider the problem of computing a function of $n$ variables using noisy queries, where each query is incorrect with some fixed and known probability $p \in (0,1/2)$. Specifically, we consider the computation of the $\mathsf{OR}$ function of $n$ bits (where queries correspond to noisy readings of the bits) and the $\mathsf{MAX}$ function of $n$ real numbers (where queries correspond to noisy pairwise comparisons). We show that an expected number of queries of \[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}{D_{\mathsf{KL}(p \| 1-p)} \] is both sufficient and necessary to compute both functions with a vanishing error probability $\delta = o(1)$, where $D_{\mathsf{KL}(p \| 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. Compared to previous work, our results tighten the dependence on $p$ in both the upper and lower bounds for the two functions.
</details>
<details>
<summary>摘要</summary>
我们考虑一个函数computing的问题，其中每个查询都有一定的错误率$p \in (0,1/2)$。我们考虑了两个函数：一是$n$个数位的$\mathsf{OR}$函数，另一个是$n$个数字的$\mathsf{MAX}$函数。我们显示了，需要一个平均的查询数量为 $(1 \pm o(1)) \frac{n\log \frac{1}{\delta}{D_{\mathsf{KL}(p \| 1-p)}$，以确保函数的误差概率为 $\delta = o(1)$。相比之前的研究，我们的结果对$p$的依赖性提高了在上下限 both bounds for the two functions。
</details></li>
</ul>
<hr>
<h2 id="LanSER-Language-Model-Supported-Speech-Emotion-Recognition"><a href="#LanSER-Language-Model-Supported-Speech-Emotion-Recognition" class="headerlink" title="LanSER: Language-Model Supported Speech Emotion Recognition"></a>LanSER: Language-Model Supported Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03978">http://arxiv.org/abs/2309.03978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taesik Gong, Josh Belanich, Krishna Somandepalli, Arsha Nagrani, Brian Eoff, Brendan Jou</li>
<li>for: 本研究想要提高Speech Emotion Recognition（SER）模型的准确性和效率，使其能够在大量的语音数据上进行训练和测试。</li>
<li>methods: 本研究使用了弱型学习方法，将大型自然语言模型中的弱label推导到语音数据中，以减少人工标注的成本。在弱label推导中，使用文本推理方法选择了最高推理得分的情感label，并将其与自动语音识别得到的语音脚本进行结合。</li>
<li>results: 根据实验结果显示，使用弱型学习方法训练的SER模型在标准SER数据上表现出色，比其他基eline模型更高的准确性和效率。此外，使用弱label推导的表现显示，即使模型仅从文本中推导到情感label，也能够模型语音中的声音内容。<details>
<summary>Abstract</summary>
Speech emotion recognition (SER) models typically rely on costly human-labeled data for training, making scaling methods to large speech datasets and nuanced emotion taxonomies difficult. We present LanSER, a method that enables the use of unlabeled data by inferring weak emotion labels via pre-trained large language models through weakly-supervised learning. For inferring weak labels constrained to a taxonomy, we use a textual entailment approach that selects an emotion label with the highest entailment score for a speech transcript extracted via automatic speech recognition. Our experimental results show that models pre-trained on large datasets with this weak supervision outperform other baseline models on standard SER datasets when fine-tuned, and show improved label efficiency. Despite being pre-trained on labels derived only from text, we show that the resulting representations appear to model the prosodic content of speech.
</details>
<details>
<summary>摘要</summary>
对话情感识别（SER）模型通常需要昂贵的人工标注数据进行训练，使得扩大到大型对话数据和细化情感分类的方法困难。我们介绍了LanSER，一种方法可以使用无标注数据进行训练，通过弱型超级vised学习来推导弱情感标签。为了对约束taxonomy中的情感标签进行推导，我们使用文本包含关系方法，选择一个 speech 稿中的情感标签，通过自动语音识别来提取 speech 稿。我们的实验结果表明，在标注只基于文本的情况下，使用这种弱超级vised学习训练的模型可以在标准SER数据集上超越基eline模型，并显示改进的标签效率。尽管模型只基于文本标签进行训练，但我们发现模型 Apparently 捕捉到了语音中的 просодические内容。
</details></li>
</ul>
<hr>
<h2 id="DBsurf-A-Discrepancy-Based-Method-for-Discrete-Stochastic-Gradient-Estimation"><a href="#DBsurf-A-Discrepancy-Based-Method-for-Discrete-Stochastic-Gradient-Estimation" class="headerlink" title="DBsurf: A Discrepancy Based Method for Discrete Stochastic Gradient Estimation"></a>DBsurf: A Discrepancy Based Method for Discrete Stochastic Gradient Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03974">http://arxiv.org/abs/2309.03974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pau Mulet Arabi, Alec Flowers, Lukas Mauch, Fabien Cardinaux</li>
<li>for: 这 paper 是用来解决计算分布参数预期的 gradient 问题的。</li>
<li>methods: 这 paper 使用了 Reinforce 算法来解决 gradient 估计问题，并 introduce 了一种新的采样方法来减少实际分布和采样之间的差异。</li>
<li>results: 对比 exist 的估计器，DBsurf 在 least squares 问题中具有最低的偏度，并在不同的 dataset 和采样设置下训练 VAE 达到了最佳结果。  finally, 这 paper 使用 DBsurf 构建了一种简单而高效的 neural architecture search 算法，达到了状态革命的性能。<details>
<summary>Abstract</summary>
Computing gradients of an expectation with respect to the distributional parameters of a discrete distribution is a problem arising in many fields of science and engineering. Typically, this problem is tackled using Reinforce, which frames the problem of gradient estimation as a Monte Carlo simulation. Unfortunately, the Reinforce estimator is especially sensitive to discrepancies between the true probability distribution and the drawn samples, a common issue in low sampling regimes that results in inaccurate gradient estimates. In this paper, we introduce DBsurf, a reinforce-based estimator for discrete distributions that uses a novel sampling procedure to reduce the discrepancy between the samples and the actual distribution. To assess the performance of our estimator, we subject it to a diverse set of tasks. Among existing estimators, DBsurf attains the lowest variance in a least squares problem commonly used in the literature for benchmarking. Furthermore, DBsurf achieves the best results for training variational auto-encoders (VAE) across different datasets and sampling setups. Finally, we apply DBsurf to build a simple and efficient Neural Architecture Search (NAS) algorithm with state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
计算对分布参数的期望 gradient 是科学和工程多个领域的问题。通常，这个问题使用 Reinforce 来解决，它将问题定义为 Monte Carlo  simulations。然而，Reinforce 估计器在低抽样 régime 中尤其敏感于真实分布与抽样之间的差异，导致不准确的 gradient 估计。在这篇论文中，我们介绍 DBsurf，一种基于 Reinforce 的分布估计器，使用一种新的抽样过程来减少抽样与实际分布之间的差异。为评估我们的估计器的性能，我们对它进行了多种任务的测试。在现有的估计器中，DBsurf 的方差最低，而且在不同的数据集和抽样设置下，DBsurf 在训练 variational autoencoders (VAE) 中表现最佳。最后，我们使用 DBsurf 建立了一个简单而高效的 neural architecture search (NAS) 算法，并达到了状态空间的前iers Performance。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Concept-Embedding-Model-ACEM-No-train-time-concepts-No-issue"><a href="#Automatic-Concept-Embedding-Model-ACEM-No-train-time-concepts-No-issue" class="headerlink" title="Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!"></a>Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03970">http://arxiv.org/abs/2309.03970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Jain</li>
<li>for: 本研究旨在提高神经网络的可解释性和可读性，特别是在安全关键领域和提供社会权利的情况下。</li>
<li>methods: 本研究使用自动概念嵌入模型（ACEMs），它们可以自动学习概念标注，而不需要手动标注大量数据。</li>
<li>results: ACEMs可以超越概念嵌入模型（CEMs）的一个重要限制，即需要概念标注的全量训练数据。<details>
<summary>Abstract</summary>
Interpretability and explainability of neural networks is continuously increasing in importance, especially within safety-critical domains and to provide the social right to explanation. Concept based explanations align well with how humans reason, proving to be a good way to explain models. Concept Embedding Models (CEMs) are one such concept based explanation architectures. These have shown to overcome the trade-off between explainability and performance. However, they have a key limitation -- they require concept annotations for all their training data. For large datasets, this can be expensive and infeasible. Motivated by this, we propose Automatic Concept Embedding Models (ACEMs), which learn the concept annotations automatically.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT interpretability 和 explainability of neural networks 在安全关键领域和提供社会的权利Explain 中得到不断增加的重要性，特别是在大规模数据集中。基于概念的解释Architecture 对于 humans 的理解方式非常合理， proven 可以Effective 地解释模型。基于概念的嵌入模型（CEMs）是一种这样的解释Architecture ，它们能够超越性能和解释之间的交易。然而，它们具有一个关键的限制---它们需要概念注释 для所有的训练数据。对于大规模数据集，这可能是非常昂贵和不可能的。 Motivated  by 这一点，我们提出了自动概念嵌入模型（ACEMs），它们可以自动学习概念注释。TRANSLATE_TEXT_END
</details></li>
</ul>
<hr>
<h2 id="Improving-Resnet-9-Generalization-Trained-on-Small-Datasets"><a href="#Improving-Resnet-9-Generalization-Trained-on-Small-Datasets" class="headerlink" title="Improving Resnet-9 Generalization Trained on Small Datasets"></a>Improving Resnet-9 Generalization Trained on Small Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03965">http://arxiv.org/abs/2309.03965</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/omarawad2/HAET2021_Huawei">https://github.com/omarawad2/HAET2021_Huawei</a></li>
<li>paper_authors: Omar Mohamed Awad, Habib Hajimolahoseini, Michael Lim, Gurpreet Gosal, Walid Ahmed, Yang Liu, Gordon Deng</li>
<li>for: 这个论文是为了参加ICLR竞赛，目标是在 less than 10 minutes 内达到 CIFAR-10 图像分类任务最高准确率。</li>
<li>methods: 该方法包括应用多种技术来提高 ResNet-9 的通用性，包括：锐度感知优化、标签整合、梯度中心化、输入图像白平衡以及基于学习的训练。</li>
<li>results: 实验表明，通过在 less than 10 minutes 内训练只使用 CIFAR-10  dataset 的 10% subset，ResNet-9 可以达到 88% 的准确率。<details>
<summary>Abstract</summary>
This paper presents our proposed approach that won the first prize at the ICLR competition on Hardware Aware Efficient Training. The challenge is to achieve the highest possible accuracy in an image classification task in less than 10 minutes. The training is done on a small dataset of 5000 images picked randomly from CIFAR-10 dataset. The evaluation is performed by the competition organizers on a secret dataset with 1000 images of the same size. Our approach includes applying a series of technique for improving the generalization of ResNet-9 including: sharpness aware optimization, label smoothing, gradient centralization, input patch whitening as well as metalearning based training. Our experiments show that the ResNet-9 can achieve the accuracy of 88% while trained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets
</details>
<details>
<summary>摘要</summary>
本文介绍我们提出的方法，在ICLR竞赛中获得了首奖，挑战是在10分钟内达到最高精度在图像分类任务中。我们使用了一系列技术来提高ResNet-9的通用性，包括：锐度感知优化、标签平滑、梯度中心化、输入裁剪白净化以及基于学习的训练。我们的实验表明，只需在CIFAR-10 dataset中训练10%的子集，ResNet-9可以在10分钟内达到88%的精度。
</details></li>
</ul>
<hr>
<h2 id="REALM-Robust-Entropy-Adaptive-Loss-Minimization-for-Improved-Single-Sample-Test-Time-Adaptation"><a href="#REALM-Robust-Entropy-Adaptive-Loss-Minimization-for-Improved-Single-Sample-Test-Time-Adaptation" class="headerlink" title="REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation"></a>REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03964">http://arxiv.org/abs/2309.03964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skyler Seto, Barry-John Theobald, Federico Danieli, Navdeep Jaitly, Dan Busbridge</li>
<li>for:  Mitigate performance loss due to distribution shifts between train and test data, without access to the training data and without knowledge of the model training procedure.</li>
<li>methods:  Use a pre-trained model adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization, but with a new approach that improves robustness to noisy samples.</li>
<li>results:  Achieve better adaptation accuracy than previous approaches throughout the adaptation process on corruptions of CIFAR-10 and ImageNet-1K, demonstrating effectiveness.Here’s the full summary in Simplified Chinese:</li>
<li>for:  Mitigate performance loss due to distribution shifts between train and test data, without access to the training data and without knowledge of the model training procedure.</li>
<li>methods:  Use a pre-trained model adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization, but with a new approach that improves robustness to noisy samples.</li>
<li>results:  Achieve better adaptation accuracy than previous approaches throughout the adaptation process on corruptions of CIFAR-10 and ImageNet-1K, demonstrating effectiveness.<details>
<summary>Abstract</summary>
Fully-test-time adaptation (F-TTA) can mitigate performance loss due to distribution shifts between train and test data (1) without access to the training data, and (2) without knowledge of the model training procedure. In online F-TTA, a pre-trained model is adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization. However, models adapted with online using entropy minimization, are unstable especially in single sample settings, leading to degenerate solutions, and limiting the adoption of TTA inference strategies. Prior works identify noisy, or unreliable, samples as a cause of failure in online F-TTA. One solution is to ignore these samples, which can lead to bias in the update procedure, slow adaptation, and poor generalization. In this work, we present a general framework for improving robustness of F-TTA to these noisy samples, inspired by self-paced learning and robust loss functions. Our proposed approach, Robust Entropy Adaptive Loss Minimization (REALM), achieves better adaptation accuracy than previous approaches throughout the adaptation process on corruptions of CIFAR-10 and ImageNet-1K, demonstrating its effectiveness.
</details>
<details>
<summary>摘要</summary>
完全测试时适应（F-TTA）可以减少因为分布变化而导致的性能下降（1）无需访问训练数据，以及（2）无需知道模型训练过程。在线F-TTA中，一个预训练模型通过使用一条流式测试样本来适应，并使用自我指导目标，如Entropy降低，来进行适应。然而，使用在线Entropy降低来适应，尤其在单个样本设置下，容易导致不稳定，Resulting in degenerate solutions, and limiting the adoption of TTA inference strategies.先前的工作认为噪音或不可靠的样本是适应失败的原因。一种解决方案是忽略这些样本，可能会导致更新过程中的偏见，慢速适应，和差异化。在这个工作中，我们提出了一个普遍适用的框架，可以提高F-TTA的异常性，以解决这些噪音样本的问题，得到更好的适应精度。我们提出的方法，即Robust Entropy Adaptive Loss Minimization（REALM），在CIFAR-10和ImageNet-1K上进行了适应过程中的整体性能提高，证明其效果。
</details></li>
</ul>
<hr>
<h2 id="ImageBind-LLM-Multi-modality-Instruction-Tuning"><a href="#ImageBind-LLM-Multi-modality-Instruction-Tuning" class="headerlink" title="ImageBind-LLM: Multi-modality Instruction Tuning"></a>ImageBind-LLM: Multi-modality Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03905">http://arxiv.org/abs/2309.03905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/llama-adapter">https://github.com/opengvlab/llama-adapter</a></li>
<li>paper_authors: Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, Yu Qiao</li>
<li>for: 这个论文的目的是提出一种基于 ImageBind 的多Modalität 语言模型调教方法（LLMs），以便响应不同的感知Modalität，包括语音、3D 点云、视频和它们的Embedding 空间运算。</li>
<li>methods: 该方法使用一个学习可能的绑定网络将 ImageBind 的图像编码器的Embedding 空间与 LLaMA 的语言模型的Embedding 空间绑定在一起。然后，通过一种无注意力和初始化的阀值机制，将图像特征转换为word token的权重，以便在语言模型中进行多Modalität  instrucion 跟踪。</li>
<li>results: 在训练中，ImageBind-LLM 可以响应多Modalität 的 instrucion，并且在语言生成质量方面表现出色。在推理时，通过一种提posed的视觉缓存模型，对多Modalität 输入进行进一步的跨Modalität 嵌入增强。这种缓存模型可以从三百万个由 ImageBind 提取的图像特征中提取出来，从而有效地解决训练-推理Modalität 差距问题。<details>
<summary>Abstract</summary>
We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by a proposed visual cache model for further cross-modal embedding enhancement. The training-free cache model retrieves from three million image features extracted by ImageBind, which effectively mitigates the training-inference modality discrepancy. Notably, with our approach, ImageBind-LLM can respond to instructions of diverse modalities and demonstrate significant language generation quality. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.
</details>
<details>
<summary>摘要</summary>
我们介绍ImageBind-LLM，一种基于ImageBind的多modal性指令调整方法 для大语言模型（LLM）。现有工作主要关注语言和图像指令调整，与之不同，我们的ImageBind-LLM可以响应多modal性条件，包括音频、3D点云、视频和它们的嵌入空间加法。在训练过程中，我们采用学习 bind 网络将 ImageBind 的图像编码器的嵌入空间与 LLaMA 的嵌入空间进行对应。然后，通过 bind 网络将图像特征转换为 word tokens 的所有层，通过无注意力和初始化的阻止机制，逐渐注入视觉指令。帮助joint embedding of ImageBind，简单的图像文本训练可以让我们的模型在多modal性指令跟踪能力方面表现出色。在推理过程中，多modal输入被 feed 到相应的 ImageBind 编码器，并被一种提议的视觉缓存模型进行进一步跨Modal embedding 增强。这种无需训练的缓存模型通过从 ImageBind 提取的三百万个图像特征，有效地解决了训练-推理模态不一致问题。值得注意的是，通过我们的方法，ImageBind-LLM可以响应多modal性指令并达到显著的语言生成质量。代码可以在 https://github.com/OpenGVLab/LLaMA-Adapter 上下载。
</details></li>
</ul>
<hr>
<h2 id="DiffusionEngine-Diffusion-Model-is-Scalable-Data-Engine-for-Object-Detection"><a href="#DiffusionEngine-Diffusion-Model-is-Scalable-Data-Engine-for-Object-Detection" class="headerlink" title="DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection"></a>DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03893">http://arxiv.org/abs/2309.03893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manlin Zhang, Jie Wu, Yuxi Ren, Ming Li, Jie Qin, Xuefeng Xiao, Wei Liu, Rui Wang, Min Zheng, Andy J. Ma</li>
<li>for: 这篇论文的目的是推广深度学习中的数据引擎，以提高物体检测的效果。</li>
<li>methods: 这篇论文使用了Diffusion Model，一种可扩展的数据引擎，以生成高质量的物体检测训练对。它还提出了一种名为Detection-Adapter的新方法，可以使得Diffusion Model更好地适应物体检测任务。</li>
<li>results:  experiments show that 使用Diffusion Engine可以在多种场景下提高物体检测的效果，如不同的检测算法、自动预训练、数据稀缺、标签缺乏等。例如，在使用Diffusion Engine和DINO-based adapter进行数据扩大后，COCO上的mAP提高了3.1%，VOC上提高了7.6%，Clipart上提高了11.5%。<details>
<summary>Abstract</summary>
Data is the cornerstone of deep learning. This paper reveals that the recently developed Diffusion Model is a scalable data engine for object detection. Existing methods for scaling up detection-oriented data often require manual collection or generative models to obtain target images, followed by data augmentation and labeling to produce training pairs, which are costly, complex, or lacking diversity. To address these issues, we presentDiffusionEngine (DE), a data scaling-up engine that provides high-quality detection-oriented training pairs in a single stage. DE consists of a pre-trained diffusion model and an effective Detection-Adapter, contributing to generating scalable, diverse and generalizable detection data in a plug-and-play manner. Detection-Adapter is learned to align the implicit semantic and location knowledge in off-the-shelf diffusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two datasets, i.e., COCO-DE and VOC-DE, to scale up existing detection benchmarks for facilitating follow-up research. Extensive experiments demonstrate that data scaling-up via DE can achieve significant improvements in diverse scenarios, such as various detection algorithms, self-supervised pre-training, data-sparse, label-scarce, cross-domain, and semi-supervised learning. For example, when using DE with a DINO-based adapter to scale up data, mAP is improved by 3.1% on COCO, 7.6% on VOC, and 11.5% on Clipart.
</details>
<details>
<summary>摘要</summary>
“数据是深度学习的基础石头。本文揭示了最近发展的扩散模型，它是一种可扩展的数据引擎，用于对象检测。现有的方法通常需要手动收集或生成模型来获取目标图像，然后进行数据扩展和标注，这是成本高、复杂或缺乏多样性的。为解决这些问题，我们提出了扩散引擎（DE），它可以在单个阶段内提供高质量的检测 oriented 训练对。DE 由一个预训练的扩散模型和一个有效的检测适配器组成，可以在插入式的方式下生成可扩展、多样化和普适的检测数据。检测适配器通过将含义扩散模型中的隐式语义和位置知识与检测相关的信号相对而学习，以进一步提高矩形框预测。此外，我们也贡献了 COCO-DE 和 VOC-DE 两个数据集，以扩大现有的检测标准benchmark，便于后续研究。广泛的实验表明，通过 DE 的数据扩展可以在多种enario中实现显著的改善，包括不同的检测算法、自我主义预训练、数据稀缺、标签缺乏、跨频训练和半支持学习。例如，当使用 DE 与 DINO 的适配器扩展数据时，COCO 上的 mAP 提高了3.1%，VOC 上提高了7.6%，Clipart 上提高了11.5%。”
</details></li>
</ul>
<hr>
<h2 id="ArtiGrasp-Physically-Plausible-Synthesis-of-Bi-Manual-Dexterous-Grasping-and-Articulation"><a href="#ArtiGrasp-Physically-Plausible-Synthesis-of-Bi-Manual-Dexterous-Grasping-and-Articulation" class="headerlink" title="ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation"></a>ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03891">http://arxiv.org/abs/2309.03891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo, Jie Song, Otmar Hilliges</li>
<li>for: 本研究旨在提出一种新的方法，用于synthesize bi-manual手臂物体交互，包括抓取和定位。</li>
<li>methods: 本方法利用了强化学习和物理模拟来训练一个策略，该策略控制了全局和局部手姿。</li>
<li>results: 我们的方法在实验中证明可以高效地synthesize bi-manual手臂交互，并且可以在不同的物体和环境下进行抓取和定位。<details>
<summary>Abstract</summary>
We present ArtiGrasp, a novel method to synthesize bi-manual hand-object interactions that include grasping and articulation. This task is challenging due to the diversity of the global wrist motions and the precise finger control that are necessary to articulate objects. ArtiGrasp leverages reinforcement learning and physics simulations to train a policy that controls the global and local hand pose. Our framework unifies grasping and articulation within a single policy guided by a single hand pose reference. Moreover, to facilitate the training of the precise finger control required for articulation, we present a learning curriculum with increasing difficulty. It starts with single-hand manipulation of stationary objects and continues with multi-agent training including both hands and non-stationary objects. To evaluate our method, we introduce Dynamic Object Grasping and Articulation, a task that involves bringing an object into a target articulated pose. This task requires grasping, relocation, and articulation. We show our method's efficacy towards this task. We further demonstrate that our method can generate motions with noisy hand-object pose estimates from an off-the-shelf image-based regressor.
</details>
<details>
<summary>摘要</summary>
我们介绍ArtiGrasp，一种新的方法，用于生成双手手套对象的交互，包括抓取和扭转。由于全球肘部运动的多样性以及需要精准的手指控制来扭转物体，这个任务非常具有挑战性。ArtiGrasp 利用了强化学习和物理模拟来训练一个控制全球和局部手姿的策略。我们的框架将抓取和扭转合并在一个单一的策略中，即单一的手姿参考。此外，为了帮助精准的手指控制，我们提出了一种学习级联，从单手操作静止物体开始，然后与多个代理人进行训练，包括双手和不定形物体。为了评估我们的方法，我们引入了动态物体抓取和扭转任务，这个任务需要抓取、重新定位和扭转。我们示出了我们方法的效果。此外，我们还证明了我们的方法可以生成具有噪音手套对象pose估计的手套动作。
</details></li>
</ul>
<hr>
<h2 id="A-Function-Interpretation-Benchmark-for-Evaluating-Interpretability-Methods"><a href="#A-Function-Interpretation-Benchmark-for-Evaluating-Interpretability-Methods" class="headerlink" title="A Function Interpretation Benchmark for Evaluating Interpretability Methods"></a>A Function Interpretation Benchmark for Evaluating Interpretability Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03886">http://arxiv.org/abs/2309.03886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/multimodal-interpretability/find">https://github.com/multimodal-interpretability/find</a></li>
<li>paper_authors: Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, Antonio Torralba</li>
<li>for: 本研究目的是评估自动化解释方法的效果，以便在真实世界模型上应用。</li>
<li>methods: 本研究使用语言模型（LM）生成代码和语言描述函数行为。</li>
<li>results: 研究发现，使用黑盒访问函数的LM可以偶尔推测函数结构，但是这些描述通常只 capture global函数行为，缺乏地方腐败。这些结果表明，FIND 可以用于评估更复杂的解释方法的性能，以便在真实世界模型上应用。<details>
<summary>Abstract</summary>
Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate new and existing methods that use language models (LMs) to produce code-based and language descriptions of function behavior. We find that an off-the-shelf LM augmented with only black-box access to functions can sometimes infer their structure, acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, LM-based descriptions tend to capture global function behavior and miss local corruptions. These results show that FIND will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.
</details>
<details>
<summary>摘要</summary>
Labeling neural network submodules with human-readable descriptions 是有用的 для许多下游任务，例如：把这些描述 surface 出来，导引 intervención 和可能 même 解释模型的重要行为。到目前为止，大多数机制性描述已经是小型模型、窄领域和大量的人工劳动。将所有人类可读的子计算机入模型中将 almost certainly 需要自动生成和验证描述的工具。最近，使用学习模型内 Loop 的技术已经开始受到欢迎，但评估这些工具的方法却有限和不一致。本文介绍 FIND（功能 interpretability and Description），一个用于评估自动解释方法的benchmarksuite。 FIND包含类似于训练后神经网络中的组件，以及这些组件的描述。这些函数通过文本和数字领域进行过程构造，并包括噪音、组合、近似、和偏见等实际问题。我们评估新和现有的语言模型（LM）使用语言模型生成代码和语言描述函数行为的方法。我们发现，只有靠black-box访问函数的LM可以偶尔推理出函数结构，行为如一位科学家，提出 гипотезы、建议实验和根据新数据更新描述。然而，LM-based描述通常捕捉全局函数行为，而忽略地方腐化。这些结果表明，FIND将是用于评估更复杂的解释方法的有用工具。
</details></li>
</ul>
<hr>
<h2 id="DoLa-Decoding-by-Contrasting-Layers-Improves-Factuality-in-Large-Language-Models"><a href="#DoLa-Decoding-by-Contrasting-Layers-Improves-Factuality-in-Large-Language-Models" class="headerlink" title="DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models"></a>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03883">http://arxiv.org/abs/2309.03883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/voidism/dola">https://github.com/voidism/dola</a></li>
<li>paper_authors: Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, Pengcheng He</li>
<li>for: 降低大语言模型（LLM）中的幻觉，即在预训练后生成的内容与实际信息不匹配。</li>
<li>methods: 提出了一种简单的解码策略，不需要基于已经学习的知识进行条件或进一步调整。该策略通过对不同层的层数进行对比，从 vocabulary 空间中获取下一个token的分布，利用了 LLM 中的知识具有局部特征的事实。</li>
<li>results: 该 Decoding by Contrasting Layers（DoLa）策略能够更好地把握事实，降低 LLM 中的幻觉。DoLa 在多个选择任务和开放式生成任务中表现出色，例如提高了 LLaMA 家族模型在 TruthfulQA 中的表现，提高了真实率12-17%的绝对点数。这表明 DoLa 可以使 LLM 可靠地生成真实的事实。<details>
<summary>Abstract</summary>
Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.
</details>
<details>
<summary>摘要</summary>
尽管它们具有印象的能力，大语言模型（LLM）仍然容易出现幻觉，即生成与实际见到的预训练数据不符的内容。我们提出了一种简单的解码策略，可以降低 LLM 中的幻觉，不需要conditioning于检索到的外部知识 nor 额外 fine-tuning。我们的方法通过对 later layers 和 earlier layers 的投影到 vocabulary space 中进行对比，利用了 LLM 中的实际知识在特定 transformer layers 中的强制性，以提取 factual knowledge 并减少生成错误的内容。我们称之为 Decoding by Contrasting Layers（DoLa）方法。我们发现 DoLa 方法可以更好地抽出 factual knowledge，并减少 incorrect facts 的生成。DoLa 方法在多个选择任务和开放式生成任务中均有改进 TruthfulQA 模型的表现，例如提高 LLaMA 家族模型的表现 by 12-17% 绝对点数，这表明 DoLa 方法可以使 LLM 可靠地生成真实的事实。
</details></li>
</ul>
<hr>
<h2 id="Better-Practices-for-Domain-Adaptation"><a href="#Better-Practices-for-Domain-Adaptation" class="headerlink" title="Better Practices for Domain Adaptation"></a>Better Practices for Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03879">http://arxiv.org/abs/2309.03879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linus Ericsson, Da Li, Timothy M. Hospedales</li>
<li>for: 本文旨在探讨领域适应（DA）在实际应用中遇到的分布shift问题，以及如何通过不使用标签来适应模型。</li>
<li>methods: 本文使用了多种适应算法来解决领域适应问题，包括无监督领域适应（UDA）、源自由领域适应（SFDA）和测试时适应（TTA）。</li>
<li>results: 本文通过使用合适的验证分割和评估指标来评估各种适应算法，并发现了一些挑战。虽然实际性 дости可能更差than预期，但使用正确的验证分割和评估指标可以帮助提高领域适应的性能。<details>
<summary>Abstract</summary>
Distribution shifts are all too common in real-world applications of machine learning. Domain adaptation (DA) aims to address this by providing various frameworks for adapting models to the deployment data without using labels. However, the domain shift scenario raises a second more subtle challenge: the difficulty of performing hyperparameter optimisation (HPO) for these adaptation algorithms without access to a labelled validation set. The unclear validation protocol for DA has led to bad practices in the literature, such as performing HPO using the target test labels when, in real-world scenarios, they are not available. This has resulted in over-optimism about DA research progress compared to reality. In this paper, we analyse the state of DA when using good evaluation practice, by benchmarking a suite of candidate validation criteria and using them to assess popular adaptation algorithms. We show that there are challenges across all three branches of domain adaptation methodology including Unsupervised Domain Adaptation (UDA), Source-Free Domain Adaptation (SFDA), and Test Time Adaptation (TTA). While the results show that realistically achievable performance is often worse than expected, they also show that using proper validation splits is beneficial, as well as showing that some previously unexplored validation metrics provide the best options to date. Altogether, our improved practices covering data, training, validation and hyperparameter optimisation form a new rigorous pipeline to improve benchmarking, and hence research progress, within this important field going forward.
</details>
<details>
<summary>摘要</summary>
<TRANSLATION>机器学习应用中的分布Shift是非常常见的问题。领域适应（DA）目的是为了适应模型到部署数据，而不需要使用标签。然而，领域Shift场景还存在一个更为细微的挑战：无法在部署数据上进行标签的验证，导致HPO（超参数优化）的过程中使用目标测试标签，这在实际应用中是不可用的。这在DA研究中导致了一些坏习惯，如在目标测试标签上进行HPO，从而导致了对DA研究进展的过度估计。在这篇论文中，我们分析了使用良好评估方法时的DA情况，并对一些候选验证标准进行比较。我们发现，领域适应方法学习中存在三大挑战：无监督领域适应（UDA）、源自领域适应（SFDA）和测试时适应（TTA）。虽然结果表明实际可以达到的性能通常比预期更差，但是也表明使用正确的验证分割是有利的，同时也发现一些以前未探索的验证指标可以提供最佳选择。总的来说，我们提出了一种新的严格的管道，包括数据、训练、验证和超参数优化，以提高研究进度。</TRANSLATION>Note: Please note that the translation is in Simplified Chinese, and the sentence structure and vocabulary may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="OpinionGPT-Modelling-Explicit-Biases-in-Instruction-Tuned-LLMs"><a href="#OpinionGPT-Modelling-Explicit-Biases-in-Instruction-Tuned-LLMs" class="headerlink" title="OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs"></a>OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03876">http://arxiv.org/abs/2309.03876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Haller, Ansar Aynetdinov, Alan Akbik</li>
<li>For: The paper aims to make biases in instruction-tuning explicit and transparent by presenting a web demo called OpinionGPT, which allows users to investigate different biases in the answers provided by the model.* Methods: The authors trained an instruction-tuning model on a corpus of text representing 11 different biases (political, geographic, gender, age) and derived from members of these demographics.* Results: The authors showcased the web application OpinionGPT, which allows side-by-side comparison of answers provided by the model for different biases.<details>
<summary>Abstract</summary>
Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers. With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics. This paper presents OpinionGPT, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de).
</details>
<details>
<summary>摘要</summary>
现代大型自然语言模型（LLM）在生成适应自然语言指令的能力方面呈现出了惊人的表现。然而，一个开放的研究问题是训练模型内置的偏见。例如，如果用于训练LLM的数据主要由特定政治偏见的人写成，那么生成的答案可能会带有这种偏见。现有研究努力去除这些偏见，或者抑制可能带有偏见的答案。而我们则选择了一个不同的视角：而不是trying to suppress them，我们想要让它们变得显着和透明。为此，我们提出了一个名为OpinionGPT的网页示例，用户可以在这里提问和选择想要调查的偏见。示例将使用根据每种选择的偏见进行微调的模型回答问题，从而实现了侧对比。为了训练基础模型，我们identified 11种偏见（政治、地理、性别、年龄），并生成了一个基于这些人类论述的指令微调集。本文介绍了OpinionGPT，详细介绍了我们如何训练偏见意识的模型，并显示了网页应用程序（可以在https://opiniongpt.informatik.hu-berlin.de/）。
</details></li>
</ul>
<hr>
<h2 id="A-Tutorial-on-the-Non-Asymptotic-Theory-of-System-Identification"><a href="#A-Tutorial-on-the-Non-Asymptotic-Theory-of-System-Identification" class="headerlink" title="A Tutorial on the Non-Asymptotic Theory of System Identification"></a>A Tutorial on the Non-Asymptotic Theory of System Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03873">http://arxiv.org/abs/2309.03873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ingvar Ziemann, Anastasios Tsiamis, Bruce Lee, Yassir Jedra, Nikolai Matni, George J. Pappas</li>
<li>for: 本文主要适用于线性系统识别理论中最近发展的非假函数方法。</li>
<li>methods: 本文使用了覆盖技巧、汉森-维特不等式和自正常化Martingale方法，这些工具在系统识别问题中特别有用。</li>
<li>results: 本文使用这些工具给出了不等式基于最小二乘估计器的性能表现的流畅证明，并绘制了扩展到某些非线性识别问题的想法。<details>
<summary>Abstract</summary>
This tutorial serves as an introduction to recently developed non-asymptotic methods in the theory of -- mainly linear -- system identification. We emphasize tools we deem particularly useful for a range of problems in this domain, such as the covering technique, the Hanson-Wright Inequality and the method of self-normalized martingales. We then employ these tools to give streamlined proofs of the performance of various least-squares based estimators for identifying the parameters in autoregressive models. We conclude by sketching out how the ideas presented herein can be extended to certain nonlinear identification problems.
</details>
<details>
<summary>摘要</summary>
这个教程是非偏极方法理论的引入教程，主要探讨线性系统识别问题。我们强调在这个领域中 particualrly 有用的工具，如覆盖技巧、汉森-温特不等式和自适应马丁加LS。然后，我们使用这些工具来提供直观的证明，证明不同类型的自回归模型参数的最小二乘估计的性能。 finally，我们简要介绍了如何将这些想法扩展到某些非线性识别问题。Note: "非偏极方法" (non-asymptotic methods) in the original text is translated as "非偏极方法理论" (non-asymptotic method theory) in Simplified Chinese, as there is no direct translation for "non-asymptotic" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="CenTime-Event-Conditional-Modelling-of-Censoring-in-Survival-Analysis"><a href="#CenTime-Event-Conditional-Modelling-of-Censoring-in-Survival-Analysis" class="headerlink" title="CenTime: Event-Conditional Modelling of Censoring in Survival Analysis"></a>CenTime: Event-Conditional Modelling of Censoring in Survival Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03851">http://arxiv.org/abs/2309.03851</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahmedhshahin/CenTime">https://github.com/ahmedhshahin/CenTime</a></li>
<li>paper_authors: Ahmed H. Shahin, An Zhao, Alexander C. Whitehead, Daniel C. Alexander, Joseph Jacob, David Barber</li>
<li>for: 预测医疗事件时间（例如死亡或癌症复发）基于初始观察数据，以提高医疗预测的精度。</li>
<li>methods: 提出了一种新的生存分析方法（CenTime），通过直接估计事件时间来提高预测精度。该方法具有一种创新的事件conditional缺失机制，可以在缺失数据情况下表现良好。</li>
<li>results: 比较了CenTime与标准生存分析方法（如Cox准确风险模型和DeepHit），结果显示CenTime可以提供最佳性能，同时保持与其他方法相同的排名性能。<details>
<summary>Abstract</summary>
Survival analysis is a valuable tool for estimating the time until specific events, such as death or cancer recurrence, based on baseline observations. This is particularly useful in healthcare to prognostically predict clinically important events based on patient data. However, existing approaches often have limitations; some focus only on ranking patients by survivability, neglecting to estimate the actual event time, while others treat the problem as a classification task, ignoring the inherent time-ordered structure of the events. Furthermore, the effective utilization of censored samples - training data points where the exact event time is unknown - is essential for improving the predictive accuracy of the model. In this paper, we introduce CenTime, a novel approach to survival analysis that directly estimates the time to event. Our method features an innovative event-conditional censoring mechanism that performs robustly even when uncensored data is scarce. We demonstrate that our approach forms a consistent estimator for the event model parameters, even in the absence of uncensored data. Furthermore, CenTime is easily integrated with deep learning models with no restrictions on batch size or the number of uncensored samples. We compare our approach with standard survival analysis methods, including the Cox proportional-hazard model and DeepHit. Our results indicate that CenTime offers state-of-the-art performance in predicting time-to-death while maintaining comparable ranking performance. Our implementation is publicly available at https://github.com/ahmedhshahin/CenTime.
</details>
<details>
<summary>摘要</summary>
生存分析是一种有用的工具，可以根据基线观察数据来估计特定事件的时间，如死亡或癌症复发。这 particu-larly useful in healthcare，可以预测 based on patient data 的临床重要事件的发生时间。然而，现有的方法往往有限制，一些只是将患者按照生存可能性排名，而忽略了实际事件时间的估计；另一些对事件作为分类任务进行处理，忽略了事件的时间顺序结构。此外，利用 censored 样本的有效使用是关键提高预测模型的准确性。在本文中，我们介绍了 CenTime，一种新的生存分析方法，可以直接估计事件时间。我们的方法具有创新的事件 conditional 封锁机制，可以在缺失完整数据时表现稳定。我们展示了 CenTime 可以在缺失完整数据情况下形成一致的事件模型参数估计器，并且可以与深度学习模型无约束地集成。我们对标准生存分析方法，包括 Cox 相对危险模型和 DeepHit，进行比较。结果表明，CenTime 可以在预测时间到死亡的任务上提供状态机器的性能，同时保持与标准方法相比的比较良好的排名性。我们的实现可以在 https://github.com/ahmedhshahin/CenTime 上获取。
</details></li>
</ul>
<hr>
<h2 id="Mixtures-of-Gaussians-are-Privately-Learnable-with-a-Polynomial-Number-of-Samples"><a href="#Mixtures-of-Gaussians-are-Privately-Learnable-with-a-Polynomial-Number-of-Samples" class="headerlink" title="Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples"></a>Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03847">http://arxiv.org/abs/2309.03847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Afzali, Hassan Ashtiani, Christopher Liaw</li>
<li>for: 估计混合 Gaussian 函数数据，即使受到差分隐私（DP）的限制。</li>
<li>methods: 提出了一种新的框架，可能对其他任务有用。该框架基于两个必要条件：(1) 分布集（如 Gaussian）是可列册解码的，(2) 该分布集在总差 variation 距离方面具有 “地方小” 覆盖。</li>
<li>results: 得到了 $\tilde{O}(k^2 d^4 \log(1&#x2F;\delta) &#x2F; \alpha^2 \varepsilon)$ 样本的 suffice 条件，以便在 $\alpha$ 总差 variation 距离内估计混合 Gaussian 函数数据，同时满足 $(\varepsilon, \delta)$-DP。这是现有最佳 finite sample complexity 上限，不假设 GMM 的任何结构。<details>
<summary>Abstract</summary>
We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.   To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a "locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].
</details>
<details>
<summary>摘要</summary>
我们研究对于数据 privacy 的推估混合 Gaussian 的问题。我们的主要结果是，只需要 $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ 样本来推估 $k$ 个 Gaussian 的混合，并且保证 $\alpha$ 的总差异距离条件，这是第一个不假设 GMM 的结构的最佳时间复杂度上界。我们的解法基于一个新的框架，它可能对其他任务也有用。在高度概括的话，我们证明了如果一个分布类别（如 Gaussian）满足以下两个条件，则其混合的分布也可以私有地学习：1. 分布类别是可识别的（list decodable）。2. 分布类别在总差异距离下是 "局部小" 的覆盖（locally small cover）。我们的证明绕过了已知的一个障碍，即 Unlike Gaussian, GMM 不是 "局部小" 的覆盖。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Feature-Learning-under-Structured-Data"><a href="#Gradient-Based-Feature-Learning-under-Structured-Data" class="headerlink" title="Gradient-Based Feature Learning under Structured Data"></a>Gradient-Based Feature Learning under Structured Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03843">http://arxiv.org/abs/2309.03843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, Murat A. Erdogdu</li>
<li>for: 本文研究了单指数模型在不同结构的输入数据上的学习复杂性。</li>
<li>methods: 本文使用了梯度下降法和权重正规化来学习单指数模型。</li>
<li>results: 研究发现，在带有射频结构的输入数据上，通常使用的球形梯度动力学可能无法正确地回归真向量，而适当的权重正规化可以解决这个问题。此外，通过利用输入卷积环境和目标之间的对称性，本文可以获得改进的样本复杂性和超越下界的 rotationally invariant kernel 方法。<details>
<summary>Abstract</summary>
Recent works have demonstrated that the sample complexity of gradient-based learning of single index models, i.e. functions that depend on a 1-dimensional projection of the input data, is governed by their information exponent. However, these results are only concerned with isotropic data, while in practice the input often contains additional structure which can implicitly guide the algorithm. In this work, we investigate the effect of a spiked covariance structure and reveal several interesting phenomena. First, we show that in the anisotropic setting, the commonly used spherical gradient dynamics may fail to recover the true direction, even when the spike is perfectly aligned with the target direction. Next, we show that appropriate weight normalization that is reminiscent of batch normalization can alleviate this issue. Further, by exploiting the alignment between the (spiked) input covariance and the target, we obtain improved sample complexity compared to the isotropic case. In particular, under the spiked model with a suitably large spike, the sample complexity of gradient-based training can be made independent of the information exponent while also outperforming lower bounds for rotationally invariant kernel methods.
</details>
<details>
<summary>摘要</summary>
Recent research has shown that the sample complexity of gradient-based learning of single index models, i.e. functions that depend on a 1-dimensional projection of the input data, is governed by their information exponent. However, these results only apply to isotropic data, while in practice the input often contains additional structure that can implicitly guide the algorithm. In this study, we investigate the effect of a spiked covariance structure and reveal several interesting phenomena. First, we show that in the anisotropic setting, the commonly used spherical gradient dynamics may fail to recover the true direction, even when the spike is perfectly aligned with the target direction. Next, we show that appropriate weight normalization that is reminiscent of batch normalization can alleviate this issue. Furthermore, by exploiting the alignment between the (spiked) input covariance and the target, we obtain improved sample complexity compared to the isotropic case. In particular, under the spiked model with a suitably large spike, the sample complexity of gradient-based training can be made independent of the information exponent while also outperforming lower bounds for rotationally invariant kernel methods.
</details></li>
</ul>
<hr>
<h2 id="Early-warning-via-transitions-in-latent-stochastic-dynamical-systems"><a href="#Early-warning-via-transitions-in-latent-stochastic-dynamical-systems" class="headerlink" title="Early warning via transitions in latent stochastic dynamical systems"></a>Early warning via transitions in latent stochastic dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03842">http://arxiv.org/abs/2309.03842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingyu Feng, Ting Gao, Wang Xiao, Jinqiao Duan</li>
<li>for: 这篇论文的目的是为了提供一种可以检测复杂系统或高维观测数据的早期警示方法。</li>
<li>methods: 本研究使用一种新的方法：指定方向的不变动对映，来捕捉低维抽象空间中的发展动力学。</li>
<li>results: 通过应用这种方法，我们成功地获得了有效的对应对象，并获得了检测点transition的早期警示信号。<details>
<summary>Abstract</summary>
Early warnings for dynamical transitions in complex systems or high-dimensional observation data are essential in many real world applications, such as gene mutation, brain diseases, natural disasters, financial crises, and engineering reliability. To effectively extract early warning signals, we develop a novel approach: the directed anisotropic diffusion map that captures the latent evolutionary dynamics in low-dimensional manifold. Applying the methodology to authentic electroencephalogram (EEG) data, we successfully find the appropriate effective coordinates, and derive early warning signals capable of detecting the tipping point during the state transition. Our method bridges the latent dynamics with the original dataset. The framework is validated to be accurate and effective through numerical experiments, in terms of density and transition probability. It is shown that the second coordinate holds meaningful information for critical transition in various evaluation metrics.
</details>
<details>
<summary>摘要</summary>
早期预警 для动力学转移在复杂系统或高维观测数据中非常重要，例如基因变化、脑病、自然灾害、金融危机和工程可靠性。为了有效提取早期预警信号，我们开发了一种新方法：导向异otropic扩散地图，这种方法可以捕捉低维抽象空间中的潜在演化动力学。通过应用这种方法，我们成功地找到了适当的有效坐标，并提取了可以检测状态转移的早期预警信号。我们的方法可以将潜在动力学与原始数据集相连接。我们通过数值实验证明了我们的方法是准确和有效的，在density和转移概率方面。结果表明，第二坐标包含了关键的转移点信息。
</details></li>
</ul>
<hr>
<h2 id="Bootstrapping-Adaptive-Human-Machine-Interfaces-with-Offline-Reinforcement-Learning"><a href="#Bootstrapping-Adaptive-Human-Machine-Interfaces-with-Offline-Reinforcement-Learning" class="headerlink" title="Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning"></a>Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03839">http://arxiv.org/abs/2309.03839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jensen Gao, Siddharth Reddy, Glen Berseth, Anca D. Dragan, Sergey Levine<br>for: 这种论文旨在帮助用户完成带有噪音、高维度指令信号（例如从大脑计算机接口）的Sequential decision-making任务，例如机器人 теле操作。methods: 本文提出了一种基于人工智能的循环学习算法，用于让界面将原始指令信号映射到行为上，通过线上预训练和线下精度调整。为了解决噪音指令信号和罕见奖励的挑战，我们开发了一种新的方法，用于表示和推理用户在给定路径上的长期意图。results: 我们通过一个用户研究，让12名参与者通过眼睛控制的128个指令信号来完成一个模拟的导航任务，发现我们的方法可以更好地帮助用户完成目标导航，比基eline指令接口。我们还在模拟的Sawyer推push任务和Lunar Lander游戏中测试了我们的方法，并发现它在这些领域中也有所提高。我们还进行了大量的减少实验，以确认每个方法的重要性。<details>
<summary>Abstract</summary>
Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated navigation task by using their eye gaze to modulate a 128-dimensional command signal from their webcam. The results show that our method enables successful goal navigation more often than a baseline directional interface, by learning to denoise user commands signals and provide shared autonomy assistance. We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated user commands, and find that our method improves over baseline interfaces in these domains as well. Extensive ablation experiments with simulated user commands empirically motivate each component of our method.
</details>
<details>
<summary>摘要</summary>
可 adaptive 界面可以帮助用户完成Sequential 决策任务，如 робо控制，尤其是从 brain-computer interface 获得的噪音高维度指令信号。现有的人 loop 机器学习技术可以使这些系统进步，但它们通常受到个人用户数据收集的限制。在这篇论文中，我们提出了一种强化学习算法，以训练界面将原始指令信号映射到操作使用混合式的离线预训练和在线精度调整。为了解决指令信号噪音和奖励稀缺的挑战，我们开发了一种新的用户长期意图表示和推理方法。我们主要通过一个用户研究，在其中12名参与者通过眼球跟踪来修改来自 webcam 的 128 维度指令信号来评估我们的方法。结果表明，我们的方法可以更多地帮助用户完成目标 Navigation，而不是只是提供方向性的指导。我们进一步在 simulated Sawyer 推动任务和 eye gaze 控制下测试了我们的方法，以及 Lunar Lander 游戏中的 simulated 用户指令，并发现我们的方法在这些领域中也有所提高。我们还进行了大量的减少实验，以确认每个方法组件的重要性。
</details></li>
</ul>
<hr>
<h2 id="Cross-Task-Attention-Network-Improving-Multi-Task-Learning-for-Medical-Imaging-Applications"><a href="#Cross-Task-Attention-Network-Improving-Multi-Task-Learning-for-Medical-Imaging-Applications" class="headerlink" title="Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications"></a>Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03837">http://arxiv.org/abs/2309.03837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangwook Kim, Thomas G. Purdie, Chris McIntosh<br>for: This paper is written to improve the accuracy of medical imaging tasks using a novel attention-based multi-task learning (MTL) framework.methods: The proposed framework, called Cross-Task Attention Network (CTAN), utilizes cross-task attention mechanisms to incorporate information by interacting across tasks.results: Compared to standard single-task learning (STL) and two widely used MTL baselines (hard parameter sharing and multi-task attention network), CTAN demonstrated a 4.67% improvement in performance and outperformed both baselines across four medical imaging datasets.<details>
<summary>Abstract</summary>
Multi-task learning (MTL) is a powerful approach in deep learning that leverages the information from multiple tasks during training to improve model performance. In medical imaging, MTL has shown great potential to solve various tasks. However, existing MTL architectures in medical imaging are limited in sharing information across tasks, reducing the potential performance improvements of MTL. In this study, we introduce a novel attention-based MTL framework to better leverage inter-task interactions for various tasks from pixel-level to image-level predictions. Specifically, we propose a Cross-Task Attention Network (CTAN) which utilizes cross-task attention mechanisms to incorporate information by interacting across tasks. We validated CTAN on four medical imaging datasets that span different domains and tasks including: radiation treatment planning prediction using planning CT images of two different target cancers (Prostate, OpenKBP); pigmented skin lesion segmentation and diagnosis using dermatoscopic images (HAM10000); and COVID-19 diagnosis and severity prediction using chest CT scans (STOIC). Our study demonstrates the effectiveness of CTAN in improving the accuracy of medical imaging tasks. Compared to standard single-task learning (STL), CTAN demonstrated a 4.67% improvement in performance and outperformed both widely used MTL baselines: hard parameter sharing (HPS) with an average performance improvement of 3.22%; and multi-task attention network (MTAN) with a relative decrease of 5.38%. These findings highlight the significance of our proposed MTL framework in solving medical imaging tasks and its potential to improve their accuracy across domains.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）是深度学习中一种 poderoso 的方法，利用多个任务的信息在训练中来提高模型性能。在医疗影像领域，MTL 表现出了很大的潜力，可以解决多种任务。然而，现有的医疗影像MTL架构受限于任务之间的信息共享，从而减少了MTL 的性能提高 potential。在本研究中，我们提出了一种 novel attention-based MTL 框架，可以更好地利用多任务之间的交互来提高各种任务的性能。具体来说，我们提出了一种 Cross-Task Attention Network（CTAN），通过跨任务注意机制来捕捉多任务之间的信息交互。我们在四个医疗影像 Dataset 上验证了 CTAN，包括：基于规划 CT 图像的两种不同肿瘤抑制（肾癌和开放KBP）；粉刺皮肤抑制和诊断使用 dermatoscopic 图像（HAM10000）；以及COVID-19 诊断和严重程度预测使用胸部 CT 成像（STOIC）。我们的研究表明，CTAN 可以提高医疗影像任务的准确率。相比标准单任务学习（STL），CTAN 表现出4.67%的提高，并在两种广泛使用的 MTL 基线上出perform：硬件参数共享（HPS）的平均提高率为3.22%，以及多任务注意网络（MTAN）的相对下降5.38%。这些结果表明了我们提出的 MTL 框架在解决医疗影像任务方面的重要性和其在不同领域中的可能性。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Demonstration-via-Probabilistic-Diagrammatic-Teaching"><a href="#Learning-from-Demonstration-via-Probabilistic-Diagrammatic-Teaching" class="headerlink" title="Learning from Demonstration via Probabilistic Diagrammatic Teaching"></a>Learning from Demonstration via Probabilistic Diagrammatic Teaching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03835">http://arxiv.org/abs/2309.03835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Zhi, Tianyi Zhang, Matthew Johnson-Roberson</li>
<li>for: 这篇论文旨在探讨一种新的教学方法，即图文教学（Diagrammatic Teaching），该方法通过让用户在2D图像上绘制示例轨迹，然后通过抽象模型来生成新的运动轨迹。</li>
<li>methods: 该论文提出了一种名为ray-tracing probabilistic trajectory learning（RPTL）的框架，该框架可以从用户绘制的2D示例轨迹中提取时间变化的概率密度，然后通过抽象模型来预测新的运动轨迹。</li>
<li>results: 论文通过在模拟和真实机器人上进行实验，证明了图文教学和RPTL框架的有效性。<details>
<summary>Abstract</summary>
Learning for Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions in an intuitive manner. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, applies ray-tracing to find corresponding regions in 3D Cartesian space, and fits a probabilistic model of motion trajectories to these regions. New motion trajectories, which mimic those sketched by the user, can then be generated from the probabilistic model. We empirically validate our framework both in simulation and on real robots, which include a fixed-base manipulator and a quadruped-mounted manipulator.
</details>
<details>
<summary>摘要</summary>
学习示例（LfD）允许机器人学习新技能，通过模仿专家示范，让用户通过直观的方式表达指令。现代LfD的进步 часто靠近身体教学或远程操作作为用户指示示范的媒介。身体教学需要机器人的物理操作，而远程操作需要更多的硬件掌握。本文介绍一种代替LfD的新方法，即图解教学。图解教学的目标是通过让用户在2D场景图上绘制示范轨迹，并将其 sinthez为3D任务空间的生成模型。此外，我们还提出了基于投影图学习的投影概率轨迹学习（RPTL）框架。RPTL从2D图上提取时间变化的概率密度，使用投影图将其映射到3D坐标系中，并适应一个概率动作轨迹模型。通过这个模型，可以生成与用户绘制的示范轨迹类似的新动作轨迹。我们在实验中 validate了我们的框架，包括在模拟环境中和真实的机器人上进行了实验，其中包括一个固定基 manipulator和一个四脚支承 manipulate manipulator。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-Drift-in-Textual-Data-An-Unsupervised-Method-for-Detecting-and-Mitigating-Drift-in-Machine-Learning-Models"><a href="#Uncovering-Drift-in-Textual-Data-An-Unsupervised-Method-for-Detecting-and-Mitigating-Drift-in-Machine-Learning-Models" class="headerlink" title="Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models"></a>Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03831">http://arxiv.org/abs/2309.03831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Khaki, Akhouri Abhinav Aditya, Zohar Karnin, Lan Ma, Olivia Pan, Samarth Marudheri Chandrashekar</li>
<li>for: 本研究旨在提出一种无监督式抖散检测方法，以优化机器学习模型的性能和在线客户体验质量指标。</li>
<li>methods: 本方法采用了两步进行抖散检测。第一步是将生产数据编码为目标分布，模型训练数据为参照分布。第二步是使用核函数基本统计测试，通过最大均值差距（MMD）距离度量，比较参照和目标分布的差异，并估计抖散程度。</li>
<li>results: 研究表明，使用该方法可以快速和精准地检测抖散，并且可以特定生产数据带来高抖散的问题。重新训练使用这些标识出的高抖散样本的模型，可以提高在线客户体验质量指标。<details>
<summary>Abstract</summary>
Drift in machine learning refers to the phenomenon where the statistical properties of data or context, in which the model operates, change over time leading to a decrease in its performance. Therefore, maintaining a constant monitoring process for machine learning model performance is crucial in order to proactively prevent any potential performance regression. However, supervised drift detection methods require human annotation and consequently lead to a longer time to detect and mitigate the drift. In our proposed unsupervised drift detection method, we follow a two step process. Our first step involves encoding a sample of production data as the target distribution, and the model training data as the reference distribution. In the second step, we employ a kernel-based statistical test that utilizes the maximum mean discrepancy (MMD) distance metric to compare the reference and target distributions and estimate any potential drift. Our method also identifies the subset of production data that is the root cause of the drift. The models retrained using these identified high drift samples show improved performance on online customer experience quality metrics.
</details>
<details>
<summary>摘要</summary>
机器学习中的漂移指的是数据或上下文的统计性质随着时间的变化，导致模型表现下降。因此，为了保证机器学习模型的表现不断改进，需要进行持续监测。然而，supervised drift检测方法需要人工标注，因此可能会延迟检测和缓解漂移的时间。我们提出的无监测漂移检测方法采用两步进程。在第一步中，我们将生产数据编码为目标分布，而模型训练数据则编码为参考分布。在第二步中，我们使用kernel-based统计测试，利用最大均值差距（MMD）距离度量来比较参考和目标分布的差异，并估计任何可能的漂移。此外，我们还可以确定生产数据中具有高漂移样本的子集，并使用这些样本重新训练模型，以改进在线客户体验质量指标。
</details></li>
</ul>
<hr>
<h2 id="ArtHDR-Net-Perceptually-Realistic-and-Accurate-HDR-Content-Creation"><a href="#ArtHDR-Net-Perceptually-Realistic-and-Accurate-HDR-Content-Creation" class="headerlink" title="ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation"></a>ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03827">http://arxiv.org/abs/2309.03827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishav Bakul Barua, Ganesh Krishnasamy, KokSheik Wong, Kalin Stefanov, Abhinav Dhall</li>
<li>for: 本研究旨在填补现有方法不充分考虑人类视觉特点的缺陷，通过基于Convolutional Neural Network的Architecture来实现高动态范围内容创建。</li>
<li>methods: 本研究使用多张曝光低动态范围图像（LDR）特征作为输入，通过提出ArtHDR-Net架构来实现高动态范围内容创建。</li>
<li>results: 实验结果表明，ArtHDR-Net可以在HDR-VDP-2分数（即意见评分指数）中达到状态 искусственный知识领域的前iersperformancedegree，并且在PSNR和SSIM指标中达到竞争性的水平。<details>
<summary>Abstract</summary>
High Dynamic Range (HDR) content creation has become an important topic for modern media and entertainment sectors, gaming and Augmented/Virtual Reality industries. Many methods have been proposed to recreate the HDR counterparts of input Low Dynamic Range (LDR) images/videos given a single exposure or multi-exposure LDRs. The state-of-the-art methods focus primarily on the preservation of the reconstruction's structural similarity and the pixel-wise accuracy. However, these conventional approaches do not emphasize preserving the artistic intent of the images in terms of human visual perception, which is an essential element in media, entertainment and gaming. In this paper, we attempt to study and fill this gap. We propose an architecture called ArtHDR-Net based on a Convolutional Neural Network that uses multi-exposed LDR features as input. Experimental results show that ArtHDR-Net can achieve state-of-the-art performance in terms of the HDR-VDP-2 score (i.e., mean opinion score index) while reaching competitive performance in terms of PSNR and SSIM.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）内容创作已成为现代媒体和娱乐领域的重要话题，游戏和虚拟/增强现实领域。许多方法已经被提出来重创造LDR图像/视频的HDR对应图像，其中大多数方法主要关注保持重建图像的结构相似性和像素精度。然而，这些传统方法不强调保持图像的艺术目标，即人类视觉的感知，这是媒体、娱乐和游戏领域的重要元素。在这篇论文中，我们尝试了研究并填补这个空白。我们提出了一种 Architecture called ArtHDR-Net，基于卷积神经网络，使用多 exposure LDR特征作为输入。实验结果表明，ArtHDR-Net 可以 дости到现状体顶峰性表现（HDR-VDP-2 分数），并且在 PSNR 和 SSIM 方面达到竞争性的表现。
</details></li>
</ul>
<hr>
<h2 id="Prime-and-Modulate-Learning-Generation-of-forward-models-with-signed-back-propagation-and-environmental-cues"><a href="#Prime-and-Modulate-Learning-Generation-of-forward-models-with-signed-back-propagation-and-environmental-cues" class="headerlink" title="Prime and Modulate Learning: Generation of forward models with signed back-propagation and environmental cues"></a>Prime and Modulate Learning: Generation of forward models with signed back-propagation and environmental cues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03825">http://arxiv.org/abs/2309.03825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sama Daryanavard, Bernd Porr</li>
<li>for: 这篇论文是关于深度神经网络学习的一种新方法，它采用 Prime and Modulate 模式，通过利用错误信号的正负指示和环境反馈来优化学习。</li>
<li>methods: 这篇论文使用的方法是 Prime and Modulate 模式，它利用错误信号的正负指示和环境反馈来优化学习。</li>
<li>results: 论文中的实验结果显示，使用 Prime and Modulate 模式可以提高学习速度，比 conventinal back-propagation 更快。<details>
<summary>Abstract</summary>
Deep neural networks employing error back-propagation for learning can suffer from exploding and vanishing gradient problems. Numerous solutions have been proposed such as normalisation techniques or limiting activation functions to linear rectifying units. In this work we follow a different approach which is particularly applicable to closed-loop learning of forward models where back-propagation makes exclusive use of the sign of the error signal to prime the learning, whilst a global relevance signal modulates the rate of learning. This is inspired by the interaction between local plasticity and a global neuromodulation. For example, whilst driving on an empty road, one can allow for slow step-wise optimisation of actions, whereas, at a busy junction, an error must be corrected at once. Hence, the error is the priming signal and the intensity of the experience is a modulating factor in the weight change. The advantages of this Prime and Modulate paradigm is twofold: it is free from normalisation and it makes use of relevant cues from the environment to enrich the learning. We present a mathematical derivation of the learning rule in z-space and demonstrate the real-time performance with a robotic platform. The results show a significant improvement in the speed of convergence compared to that of the conventional back-propagation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Training-Acceleration-of-Low-Rank-Decomposed-Networks-using-Sequential-Freezing-and-Rank-Quantization"><a href="#Training-Acceleration-of-Low-Rank-Decomposed-Networks-using-Sequential-Freezing-and-Rank-Quantization" class="headerlink" title="Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization"></a>Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03824">http://arxiv.org/abs/2309.03824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Habib Hajimolahoseini, Walid Ahmed, Yang Liu</li>
<li>for: 压缩深度学习模型，提高训练和执行速度，不需要采用小rank值</li>
<li>methods: 提议两种加速低级别分解模型的方法：rank优化和顺序冻结分解层</li>
<li>results: 实验结果显示，这两种方法可以在训练和执行过程中提高模型的 Throughput 达到 60% 和 37%，保持模型精度接近原始模型。<details>
<summary>Abstract</summary>
Low Rank Decomposition (LRD) is a model compression technique applied to the weight tensors of deep learning models in order to reduce the number of trainable parameters and computational complexity. However, due to high number of new layers added to the architecture after applying LRD, it may not lead to a high training/inference acceleration if the decomposition ranks are not small enough. The issue is that using small ranks increases the risk of significant accuracy drop after decomposition. In this paper, we propose two techniques for accelerating low rank decomposed models without requiring to use small ranks for decomposition. These methods include rank optimization and sequential freezing of decomposed layers. We perform experiments on both convolutional and transformer-based models. Experiments show that these techniques can improve the model throughput up to 60% during training and 37% during inference when combined together while preserving the accuracy close to that of the original models
</details>
<details>
<summary>摘要</summary>
低阶减解（LRD）是一种深度学习模型压缩技术，用于减少训练参数和计算复杂性。然而，由于LRD应用后新增的层数较多，可能无法实现高度的训练/推理加速。问题在于，使用小rank增加风险性质下降精度。在本文中，我们提出了两种加速低阶减解模型的技术，不需要使用小rank进行减解。这两种方法包括阶数优化和顺序冻结减解层。我们在卷积和转换器基于模型上进行了实验，实验结果显示，这两种方法可以在训练和推理过程中提高模型 Throughput 达60%和37%，保持精度与原始模型几乎相同。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Risk-Minimization-for-Losses-without-Variance"><a href="#Empirical-Risk-Minimization-for-Losses-without-Variance" class="headerlink" title="Empirical Risk Minimization for Losses without Variance"></a>Empirical Risk Minimization for Losses without Variance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03818">http://arxiv.org/abs/2309.03818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanhua Fang, Ping Li, Gennady Samorodnitsky</li>
<li>for: 本文考虑了一个empirical risk minimization问题，在重 tailed设定下进行研究，数据只有$p$-th moment，$p \in (1,2)$。</li>
<li>methods: 作者使用了Catoni的方法（Catoni, 2012）来 robustly estimate risk values，并使用通用的Generic Chaining方法来提供补偿 bound。</li>
<li>results: 作者通过 theoretically investigating two types of optimization methods，robust gradient descent algorithm和empirical risk-based methods，并进行了广泛的数据研究，发现empirical risk via Catoni-style estimation的优化器实际上表现更好，这表明直接基于剪辑数据进行估计可能会得到不atisfactory的结果。<details>
<summary>Abstract</summary>
This paper considers an empirical risk minimization problem under heavy-tailed settings, where data does not have finite variance, but only has $p$-th moment with $p \in (1,2)$. Instead of using estimation procedure based on truncated observed data, we choose the optimizer by minimizing the risk value. Those risk values can be robustly estimated via using the remarkable Catoni's method (Catoni, 2012). Thanks to the structure of Catoni-type influence functions, we are able to establish excess risk upper bounds via using generalized generic chaining methods. Moreover, we take computational issues into consideration. We especially theoretically investigate two types of optimization methods, robust gradient descent algorithm and empirical risk-based methods. With an extensive numerical study, we find that the optimizer based on empirical risks via Catoni-style estimation indeed shows better performance than other baselines. It indicates that estimation directly based on truncated data may lead to unsatisfactory results.
</details>
<details>
<summary>摘要</summary>
这个论文考虑了具有重 tailed 设定的 empirical risk minimization 问题，数据并不具有有限 variance，仅仅具有 $p $-th moment，其中 $p \in (1,2) $。而不是使用 truncated 观察数据的估计过程，我们选择了使用 risk 值来选择优化器。这些 risk 值可以通过 Catoni 方法 (Catoni, 2012) 进行 robust 估计。由于 Catoni-type influence functions 的结构，我们可以通过 generalized generic chaining methods 来确定剩余风险Upper bound。此外，我们还考虑了计算问题。我们特别是使用 robust gradient descent algorithm 和 empirical risk-based methods 进行了理论研究。经过广泛的数值研究，我们发现了基于 empirical risks via Catoni-style estimation 的优化器实际上比其他基准更好的性能。这表明，直接基于 truncated data 进行估计可能会导致不满足的结果。
</details></li>
</ul>
<hr>
<h2 id="AnthroNet-Conditional-Generation-of-Humans-via-Anthropometrics"><a href="#AnthroNet-Conditional-Generation-of-Humans-via-Anthropometrics" class="headerlink" title="AnthroNet: Conditional Generation of Humans via Anthropometrics"></a>AnthroNet: Conditional Generation of Humans via Anthropometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03812">http://arxiv.org/abs/2309.03812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Unity-Technologies/AnthroNet">https://github.com/Unity-Technologies/AnthroNet</a></li>
<li>paper_authors: Francesco Picetti, Shrinath Deshpande, Jonathan Leban, Soroosh Shahtalebi, Jay Patel, Peifeng Jing, Chunpu Wang, Charles Metze III, Cameron Sun, Cera Laidlaw, James Warren, Kathy Huynh, River Page, Jonathan Hogins, Adam Crespi, Sujoy Ganguly, Salehe Erfanian Ebadi</li>
<li>for: 这个论文是用于开发一种可以生成各种人体形态和姿势的人体模型的。</li>
<li>methods: 这个模型使用了一种深度生成架构，通过直接模拟特定的人体特征来生成人体模型，并且可以在任意的姿势下生成人体。此外，这个模型还使用了一个非常多样化的动画库，以便增加模型训练的学习前提。</li>
<li>results: 这个模型通过使用100000个生成的姿势人体 mesh和其相应的人体测量数据进行训练，可以生成高度准确的人体模型，同时也可以对人体进行精准的测量。此外，这个模型还可以生成数百万个独特的人体形态和姿势，用于非商业学术研究用途。<details>
<summary>Abstract</summary>
We present a novel human body model formulated by an extensive set of anthropocentric measurements, which is capable of generating a wide range of human body shapes and poses. The proposed model enables direct modeling of specific human identities through a deep generative architecture, which can produce humans in any arbitrary pose. It is the first of its kind to have been trained end-to-end using only synthetically generated data, which not only provides highly accurate human mesh representations but also allows for precise anthropometry of the body. Moreover, using a highly diverse animation library, we articulated our synthetic humans' body and hands to maximize the diversity of the learnable priors for model training. Our model was trained on a dataset of $100k$ procedurally-generated posed human meshes and their corresponding anthropometric measurements. Our synthetic data generator can be used to generate millions of unique human identities and poses for non-commercial academic research purposes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的人体模型，基于广泛的人体中心的测量，可以生成多种人体形态和姿势。我们的模型可以直接模拟特定的人类标识，通过深度生成架构，可以在任意姿势中生成人类。这是首先使用只有生成的数据进行端到端训练的人体模型，不仅提供了高度准确的人体网格表示，还允许精确的人体人体测量。此外，我们使用了高度多样化的动画库，以 maximizelearnable prior的多样性，并将我们的人体和手部动作拟合到最大化多样性。我们的模型在100,000个生成的姿势人体网格和其相应的人体测量数据集上进行了端到端训练。我们的 sintetic数据生成器可以生成 millions of 独特的人类标识和姿势，供非商业学术研究使用。
</details></li>
</ul>
<hr>
<h2 id="Improved-theoretical-guarantee-for-rank-aggregation-via-spectral-method"><a href="#Improved-theoretical-guarantee-for-rank-aggregation-via-spectral-method" class="headerlink" title="Improved theoretical guarantee for rank aggregation via spectral method"></a>Improved theoretical guarantee for rank aggregation via spectral method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03808">http://arxiv.org/abs/2309.03808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziliang Samuel Zhong, Shuyang Ling</li>
<li>for:  Ranking items based on pairwise comparisons, with applications in sports, recommendation systems, and other web applications.</li>
<li>methods:  Spectral ranking algorithms based on unnormalized and normalized data matrices, with a focus on deriving entry-wise perturbation error bounds and an error bound on the maximum displacement for each item.</li>
<li>results:  Improved sample complexity and theoretical analysis of the eigenvectors and error bounds for the spectral ranking algorithms, with confirmed numerical experiments.<details>
<summary>Abstract</summary>
Given pairwise comparisons between multiple items, how to rank them so that the ranking matches the observations? This problem, known as rank aggregation, has found many applications in sports, recommendation systems, and other web applications. As it is generally NP-hard to find a global ranking that minimizes the mismatch (known as the Kemeny optimization), we focus on the Erd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, each pairwise comparison is a corrupted copy of the true score difference. We investigate spectral ranking algorithms that are based on unnormalized and normalized data matrices. The key is to understand their performance in recovering the underlying scores of each item from the observed data. This reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart. By using the leave-one-out technique, we provide a sharper $\ell_{\infty}$-norm perturbation bound of the eigenvectors and also derive an error bound on the maximum displacement for each item, with only $\Omega(n\log n)$ samples. Our theoretical analysis improves upon the state-of-the-art results in terms of sample complexity, and our numerical experiments confirm these theoretical findings.
</details>
<details>
<summary>摘要</summary>
In the ERO model, each pairwise comparison is a corrupted copy of the true score difference. We investigate spectral ranking algorithms based on unnormalized and normalized data matrices. The key is to understand their performance in recovering the underlying scores of each item from the observed data. This reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart.Using the leave-one-out technique, we provide a sharper $\ell_{\infty}$-norm perturbation bound of the eigenvectors and also derive an error bound on the maximum displacement for each item. Our theoretical analysis improves upon the state-of-the-art results in terms of sample complexity, and our numerical experiments confirm these theoretical findings.Here is the text in Simplified Chinese:给定多个对比评估中的多个项目，如何将它们排名，以便与观察数据匹配呢？这个问题，称为排名聚合，在体育、推荐系统和其他网络应用中有广泛的应用。然而，找到一个全局排名，以iminimize the mismatch（known as the Kemeny optimization）是NP-hard的，因此我们将关注 Erdős-Rényi outliers（ERO）模型。在这个模型中，每个对比评估是对真实分数差的损害版本。我们 investigate spectral ranking algorithms based on unnormalized and normalized data matrices。关键在于理解它们在 recuperate the underlying scores of each item from the observed data。这将reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart。使用留一个出 techniques，我们提供了一个更加精细的 $\ell_{\infty}$-norm perturbation bound of the eigenvectors，并也 derive an error bound on the maximum displacement for each item。我们的理论分析超过了现状最佳结果，并且我们的数值实验也证明了这些理论发现。
</details></li>
</ul>
<hr>
<h2 id="Pareto-Frontiers-in-Neural-Feature-Learning-Data-Compute-Width-and-Luck"><a href="#Pareto-Frontiers-in-Neural-Feature-Learning-Data-Compute-Width-and-Luck" class="headerlink" title="Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck"></a>Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03800">http://arxiv.org/abs/2309.03800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, Cyril Zhang</li>
<li>for: 这项研究探讨了深度学习中计算统计温馈的细则设计选择。</li>
<li>methods: 该文件开始考虑了离线稀疏偏好学习，这是一个supervised分类问题，可以得到Gradient-based Training的多层感知器的统计查询下限。这个下限可以被解释为多种资源贸易前ier：成功学习只能在一个具有足够财富（大型模型）、知识（大量数据）、耐心（多个训练轮）或幸运（多个随机猜测）的情况下进行。</li>
<li>results: 我们经过理论和实验研究发现，离线稀疏初始化和增加网络宽度可以在这种设定下提供显著的样本效率提升。在这种情况下，网络宽度扮演了平行搜索的角色：它增加了找到”彩礼卷”神经元的概率，这些神经元更有效地学习稀疏特征。最后，我们表明了离线稀疏parity任务可以作为实际需要轴对映射学习的问题的代理。我们在表格类型分类 benchmark 上使用了宽度大、稀疏初始化的 MLP 模型，这些网络有时会超越调整后的随机森林。<details>
<summary>Abstract</summary>
This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real problems requiring axis-aligned feature learning. We demonstrate improved sample efficiency on tabular classification benchmarks by using wide, sparsely-initialized MLP models; these networks sometimes outperform tuned random forests.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Conformal-Autoregressive-Generation-Beam-Search-with-Coverage-Guarantees"><a href="#Conformal-Autoregressive-Generation-Beam-Search-with-Coverage-Guarantees" class="headerlink" title="Conformal Autoregressive Generation: Beam Search with Coverage Guarantees"></a>Conformal Autoregressive Generation: Beam Search with Coverage Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03797">http://arxiv.org/abs/2309.03797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Deutschmann, Marvin Alberts, María Rodríguez Martínez</li>
<li>for: 这 paper 是为了提出两种基于某规遵循性预测（CP）的搜索算法扩展，以生成具有理论保证的序列集。</li>
<li>methods: 这两种方法都基于 CP 算法，第一种方法是使用动态化大小的子集搜索结果，而不同于传统 CP 方法，它们的最高保证度取决于后续调整度量。第二种方法则在解码过程中引入了对准集预测过程，生成一个可变宽度的搜索结果，这个过程可以根据先前设置的保证度来选择。</li>
<li>results: 作者们提供了每种方法的边缘保证 bound，并对其进行了empirical评估，包括自然语言处理和化学等领域的任务。<details>
<summary>Abstract</summary>
We introduce two new extensions to the beam search algorithm based on conformal predictions (CP) to produce sets of sequences with theoretical coverage guarantees. The first method is very simple and proposes dynamically-sized subsets of beam search results but, unlike typical CP procedures, has an upper bound on the achievable guarantee depending on a post-hoc calibration measure. Our second algorithm introduces the conformal set prediction procedure as part of the decoding process, producing a variable beam width which adapts to the current uncertainty. While more complex, this procedure can achieve coverage guarantees selected a priori. We provide marginal coverage bounds for each method, and evaluate them empirically on a selection of tasks drawing from natural language processing and chemistry.
</details>
<details>
<summary>摘要</summary>
我们介绍两种基于对准预测（CP）的粒子搜寻算法扩展，以生成具有理论覆盖保证的序列集。第一种方法非常简单，它在粒子搜寻结果中 dynamically 选择子集，但与通常的 CP 程序不同的是，它具有属于后置检测量的最大保证上限。我们的第二个算法则是在解oding过程中引入对准集预测程序，它可以随着目前的不确定程度而变化粒子幅频率。虽然更加复杂，但这个程序可以根据先前选择的保证 bounds 来生成覆盖保证。我们提供每个方法的margin coverage bounds，并对其进行实验评估，包括自然语言处理和化学领域的任务。
</details></li>
</ul>
<hr>
<h2 id="Adversarially-Robust-Deep-Learning-with-Optimal-Transport-Regularized-Divergences"><a href="#Adversarially-Robust-Deep-Learning-with-Optimal-Transport-Regularized-Divergences" class="headerlink" title="Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences"></a>Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03791">http://arxiv.org/abs/2309.03791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremiah Birrell, Mohammadreza Ebrahimi</li>
<li>for: 这 paper 的目的是提出一种基于最优运输regularized divergence的方法，以提高深度学习模型的抗击势能力。</li>
<li>methods: 这 paper 使用了一种基于信息差 divergence 和最优运输成本的新类优化方法，以提高 adversarial robustness。</li>
<li>results: 这 paper 的实验结果显示， compared to 先前的方法， $ARMOR_D$ 方法在 MNIST 数据集上的抗击势能力达到了 $98.29%$ 和 $98.18%$  против $FGSM$ 和 $PGD^{40}$ 攻击，减少了误差率超过 $19.7%$ 和 $37.2%$。在恶意软件检测中，$ARMOR_D$ 方法也提高了对 $rFGSM^{50}$ 攻击的抗击势能力，相比先前的最佳 adversarial training 方法，提高了 $37.0%$。同时，它还降低了 false negative 和 false positive 率。<details>
<summary>Abstract</summary>
We introduce the $ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness of deep learning models. These methods are based on a new class of optimal-transport-regularized divergences, constructed via an infimal convolution between an information divergence and an optimal-transport (OT) cost. We use these as tools to enhance adversarial robustness by maximizing the expected loss over a neighborhood of distributions, a technique known as distributionally robust optimization. Viewed as a tool for constructing adversarial samples, our method allows samples to be both transported, according to the OT cost, and re-weighted, according to the information divergence. We demonstrate the effectiveness of our method on malware detection and image recognition applications and find that, to our knowledge, it outperforms existing methods at enhancing the robustness against adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$ against $FGSM$ and $98.18\%$ against $PGD^{40}$ on the MNIST dataset, reducing the error rate by more than $19.7\%$ and $37.2\%$ respectively compared to prior methods. Similarly, in malware detection, a discrete (binary) data domain, $ARMOR_D$ improves the robustified accuracy under $rFGSM^{50}$ attack compared to the previous best-performing adversarial training methods by $37.0\%$ while lowering false negative and false positive rates by $51.1\%$ and $57.53\%$, respectively.
</details>
<details>
<summary>摘要</summary>
我们引入 $ARMOR_D$ 方法作为深度学习模型中强化敌方攻击性能的新方法。这些方法基于一种新的最佳运输规定化分散度，通过infimal扩展（infimal convolution）between信息分散和最佳运输（OT）成本。我们使用这些工具来强化敌方攻击性能，通过 Maximizing the expected loss over a neighborhood of distributions， known as distributionally robust optimization。 viewed as a tool for constructing adversarial samples，我们的方法可以将样本transported according to the OT cost，并且重新权重 according to the information divergence。我们在这篇文章中证明了 $ARMOR_D$ 的有效性，在这些应用中它可以与现有的方法相比，对于敌方攻击性能来说，提高Robustified accuracy。在 MNIST dataset 上，$ARMOR_D$ 可以实现 Robustified accuracy 的提高， Specifically, it yields a Robustified accuracy of $98.29\%$ against $FGSM$ and $98.18\%$ against $PGD^{40}$. 相比于先前的方法，这些结果表明 $ARMOR_D$ 可以降低错误率超过 $19.7\%$ 和 $37.2\%$。在恶意软件检测中，一个给定（二进制）数据领域的 $ARMOR_D$ 可以在 $rFGSM^{50}$ 攻击下，与先前最佳的对抗式训练方法相比，提高 Robustified accuracy 37.0%，同时降低错误率和假阳性率的值。
</details></li>
</ul>
<hr>
<h2 id="CPU-frequency-scheduling-of-real-time-applications-on-embedded-devices-with-temporal-encoding-based-deep-reinforcement-learning"><a href="#CPU-frequency-scheduling-of-real-time-applications-on-embedded-devices-with-temporal-encoding-based-deep-reinforcement-learning" class="headerlink" title="CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning"></a>CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03779">http://arxiv.org/abs/2309.03779</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/coladog/tinyagent">https://github.com/coladog/tinyagent</a></li>
<li>paper_authors: Ti Zhou, Man Lin</li>
<li>for: 这种研究旨在开发高效的电源管理方法，用于 periodic 任务在小型设备上进行执行。</li>
<li>methods: 研究者首先研究了小型设备中现有 Linux 内置的方法的局限性，并描述了三种困难以管理的工作负荷&#x2F;系统模式。然后，他们开发了基于强化学习的技术，使用时间编码， derivate 一个高效的 DVFS govemor。这个govemor只使用了一个性能计数器，与内置的 Linux 机制相同，并不需要明确的任务模型。</li>
<li>results: 研究者实现了一个基于 Nvidia Jetson Nano 板的原型系统，并在六个应用程序上进行了测试，包括两个自定义应用程序和四个标准应用程序。在不同的时间限制下，该方法可以快速 derivate 一个适应性能要求的 DVFS govemor，并在能源节约方面超越内置的 Linux 方法。在 Mibench 工作负荷上，与性能余地从 0.04 s 到 0.4 s 不同，提出的方法可以节省 3% - 11% 更多的能源。AudioReg 和 FaceReg 应用程序测试显示了 5% - 14% 的能源节约改进。研究者已经开源了其在内核中的量化神经网络引擎代码库，可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/coladog/tinyagent%E3%80%82">https://github.com/coladog/tinyagent。</a><details>
<summary>Abstract</summary>
Small devices are frequently used in IoT and smart-city applications to perform periodic dedicated tasks with soft deadlines. This work focuses on developing methods to derive efficient power-management methods for periodic tasks on small devices. We first study the limitations of the existing Linux built-in methods used in small devices. We illustrate three typical workload/system patterns that are challenging to manage with Linux's built-in solutions. We develop a reinforcement-learning-based technique with temporal encoding to derive an effective DVFS governor even with the presence of the three system patterns. The derived governor uses only one performance counter, the same as the built-in Linux mechanism, and does not require an explicit task model for the workload. We implemented a prototype system on the Nvidia Jetson Nano Board and experimented with it with six applications, including two self-designed and four benchmark applications. Under different deadline constraints, our approach can quickly derive a DVFS governor that can adapt to performance requirements and outperform the built-in Linux approach in energy saving. On Mibench workloads, with performance slack ranging from 0.04 s to 0.4 s, the proposed method can save 3% - 11% more energy compared to Ondemand. AudioReg and FaceReg applications tested have 5%- 14% energy-saving improvement. We have open-sourced the implementation of our in-kernel quantized neural network engine. The codebase can be found at: https://github.com/coladog/tinyagent.
</details>
<details>
<summary>摘要</summary>
小型设备常用于物联网和智能城市应用程序， periodic 执行固定任务， 具有软 Deadline。 这项工作关注于开发高效的电源管理方法，以便在小型设备上进行 periodic 任务。 我们首先研究现有的 Linux 内置方法，以及它们在小型设备中的局限性。 我们描述了三种常见的工作负荷/系统模式，这些模式使得 Linux 的内置解决方案不够有效。 我们开发了基于 reinforcement learning 的技术，使用 temporal 编码，以 derivation 高效的 DVFS GOVERNOR。 我们的方法使用同 Linux 内置机制一样的一个性能计数器，不需要明确的任务模型。 我们实现了一个原型系统，并在 Nvidia Jetson Nano Board 上进行了测试，使用了六个应用程序，包括两个自定义的和四个 referential 应用程序。 在不同的截止时间限制下，我们的方法可以快速 derivation 一个适应性能需求的 DVFS GOVERNOR，并且在能源节约方面超过 Linux 内置方法。 在 Mibench 工作负荷下，在性能拥有弹性范围内（0.04 s - 0.4 s），我们的方法可以节省 3% - 11% 更多的能源。 AudioReg 和 FaceReg 应用程序的能源节约提升为 5% - 14%。 我们已经开源了我们的内核 quantized neural network 引擎的实现代码，代码库可以在以下链接中找到：https://github.com/coladog/tinyagent。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Safety-Concerns-in-Automated-Driving-Perception"><a href="#Deep-Learning-Safety-Concerns-in-Automated-Driving-Perception" class="headerlink" title="Deep Learning Safety Concerns in Automated Driving Perception"></a>Deep Learning Safety Concerns in Automated Driving Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03774">http://arxiv.org/abs/2309.03774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephanie Abrecht, Alexander Hirsch, Shervin Raafatnia, Matthias Woehrle</li>
<li>for: 这篇论文的目的是为了提高自动驾驶系统中的深度学习 Component 的安全性，并且提出了一个系统性和完整的方法来处理安全性 Concerns。</li>
<li>methods: 这篇论文使用了一种称为“安全 Concerns”的概念，它是根据 existing standards 和 academic publications 的反馈，以提高自动驾驶系统中的安全性。</li>
<li>results: 这篇论文发现了一个更加精确和完整的方法来处理安全 Concerns，并且引入了一个新的分类方法来帮助标准化和跨功能团队共同处理这些 Concerns。<details>
<summary>Abstract</summary>
Recent advances in the field of deep learning and impressive performance of deep neural networks (DNNs) for perception have resulted in an increased demand for their use in automated driving (AD) systems. The safety of such systems is of utmost importance and thus requires to consider the unique properties of DNNs.   In order to achieve safety of AD systems with DNN-based perception components in a systematic and comprehensive approach, so-called safety concerns have been introduced as a suitable structuring element. On the one hand, the concept of safety concerns is -- by design -- well aligned to existing standards relevant for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has already inspired several academic publications and upcoming standards on AI safety such as ISO PAS 8800.   While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In particular, this paper introduces an additional categorization for a better understanding as well as enabling cross-functional teams to jointly address the concerns.
</details>
<details>
<summary>摘要</summary>
Recent advances in deep learning and the impressive performance of deep neural networks (DNNs) for perception have led to an increased demand for their use in automated driving (AD) systems. However, the safety of such systems is of utmost importance, and thus it is necessary to consider the unique properties of DNNs. To achieve safety in AD systems with DNN-based perception components, so-called safety concerns have been introduced as a suitable structuring element. This concept is well aligned with existing standards relevant to the safety of AD systems, such as ISO 21448 (SOTIF), and has inspired several academic publications and upcoming standards on AI safety, such as ISO PAS 8800. While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In particular, this paper introduces an additional categorization for a better understanding, as well as enabling cross-functional teams to jointly address the concerns.
</details></li>
</ul>
<hr>
<h2 id="Neural-lasso-a-unifying-approach-of-lasso-and-neural-networks"><a href="#Neural-lasso-a-unifying-approach-of-lasso-and-neural-networks" class="headerlink" title="Neural lasso: a unifying approach of lasso and neural networks"></a>Neural lasso: a unifying approach of lasso and neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03770">http://arxiv.org/abs/2309.03770</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Delgado, Ernesto Curbelo, Danae Carreras</li>
<li>for: 本文旨在演示如何将统计技术与机器学习技术结合，以获得两者之间的优点。</li>
<li>methods: 本文使用隐藏 нейрон网络来表示逻辑法，并发现了这两种方法在优化方面存在差异。特别是，机器学习方法通常在单步使用单个验证集进行优化，而统计方法则使用两步优化方法，基于交叉验证。统计方法的更复杂的优化结果在小训练集时得到更 precis的参数估计。</li>
<li>results: 经过实验，使用新的优化算法可以更好地识别重要变量，并且在synthetic和实际数据集上达到更高的性能。<details>
<summary>Abstract</summary>
In recent years, there is a growing interest in combining techniques attributed to the areas of Statistics and Machine Learning in order to obtain the benefits of both approaches. In this article, the statistical technique lasso for variable selection is represented through a neural network. It is observed that, although both the statistical approach and its neural version have the same objective function, they differ due to their optimization. In particular, the neural version is usually optimized in one-step using a single validation set, while the statistical counterpart uses a two-step optimization based on cross-validation. The more elaborated optimization of the statistical method results in more accurate parameter estimation, especially when the training set is small. For this reason, a modification of the standard approach for training neural networks, that mimics the statistical framework, is proposed. During the development of the above modification, a new optimization algorithm for identifying the significant variables emerged. Experimental results, using synthetic and real data sets, show that this new optimization algorithm achieves better performance than any of the three previous optimization approaches.
</details>
<details>
<summary>摘要</summary>
近年来，人们对将统计学和机器学习技术结合使用的兴趣日益增长。本文通过神经网络表示统计技术lasso的变量选择。研究发现，尽管这两种方法的目标函数都相同，但它们在优化方面有所不同。特别是，神经网络版本通常在一步使用单个验证集进行优化，而统计方法则使用两步优化基于交叉验证。统计方法的更复杂的优化导致更准确的参数估计，特别是当训练集较小时。为此，一种基于统计框架的修改方法 для训练神经网络被提出。在该修改方法的开发过程中，一种新的变量选择优化算法被发现。实验结果，使用 sintetic和实际数据集，显示这种新的优化算法在三种先前的优化方法中表现更好。
</details></li>
</ul>
<hr>
<h2 id="M-otion-mode-Based-Prediction-of-Ejection-Fraction-using-Echocardiograms"><a href="#M-otion-mode-Based-Prediction-of-Ejection-Fraction-using-Echocardiograms" class="headerlink" title="M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms"></a>M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03759">http://arxiv.org/abs/2309.03759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thomassutter/mmodeecho">https://github.com/thomassutter/mmodeecho</a></li>
<li>paper_authors: Ece Ozkan, Thomas M. Sutter, Yurong Hu, Sebastian Balzer, Julia E. Vogt</li>
<li>for: 早期检测心脏功能异常，通过常规检查是诊断心血管疾病的关键。</li>
<li>methods: 使用M模式电子镜图进行心脏功能评估，并利用自适应模型拟合扩展对数据进行学习。</li>
<li>results: 使用M模式电子镜图进行心脏功能评估，可以准确地评估心脏功能，并且在有限数据场景下可以减少人工干预。<details>
<summary>Abstract</summary>
Early detection of cardiac dysfunction through routine screening is vital for diagnosing cardiovascular diseases. An important metric of cardiac function is the left ventricular ejection fraction (EF), where lower EF is associated with cardiomyopathy. Echocardiography is a popular diagnostic tool in cardiology, with ultrasound being a low-cost, real-time, and non-ionizing technology. However, human assessment of echocardiograms for calculating EF is time-consuming and expertise-demanding, raising the need for an automated approach. In this work, we propose using the M(otion)-mode of echocardiograms for estimating the EF and classifying cardiomyopathy. We generate multiple artificial M-mode images from a single echocardiogram and combine them using off-the-shelf model architectures. Additionally, we extend contrastive learning (CL) to cardiac imaging to learn meaningful representations from exploiting structures in unlabeled data allowing the model to achieve high accuracy, even with limited annotations. Our experiments show that the supervised setting converges with only ten modes and is comparable to the baseline method while bypassing its cumbersome training process and being computationally much more efficient. Furthermore, CL using M-mode images is helpful for limited data scenarios, such as having labels for only 200 patients, which is common in medical applications.
</details>
<details>
<summary>摘要</summary>
早期发现心脏功能不全通过常规检查是诊断心血管疾病的关键。一个重要的心脏功能指标是左心室泵出率（EF），其下降与心肌疾病有关。电子心室graph是卡диологи中广泛使用的诊断工具，它是一种低成本、实时、非辐射技术。然而，人工评估电子心室гра征用于计算EF是时间consuming和专业需求很高，这有需要自动化方法。在这种工作中，我们提议使用M（运动）模式电子心室гра来估计EF和诊断心肌疾病。我们生成多个人工M模式图像从单个电子心室гра，并将它们组合使用现有模型建筑。此外，我们扩展了对比学习（CL）到心脏成像领域，以学习具有意义的表示从未标注的数据中获得有用信息，使模型可以在有限的标注下达到高精度。我们的实验表明，在监督设置下，只需要使用十个模式即可与基eline方法相当，并且可以快速地 converges。此外，使用M模式图像进行CL可以在医疗应用中有限的数据情况下提供帮助，例如只有200名患者的标注。
</details></li>
</ul>
<hr>
<h2 id="TSGBench-Time-Series-Generation-Benchmark"><a href="#TSGBench-Time-Series-Generation-Benchmark" class="headerlink" title="TSGBench: Time Series Generation Benchmark"></a>TSGBench: Time Series Generation Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03755">http://arxiv.org/abs/2309.03755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihao Ang, Qiang Huang, Yifan Bao, Anthony K. H. Tung, Zhiyong Huang<br>for:* 这 paper 是为了提供一个统一的评估方法来评估时间序列生成（TSG）方法的性能。methods:* 这 paper 使用了一个名为 \textsf{TSGBench} 的 TSG benchmark，它包括三个模块：一个公共数据集的收集、一个标准化的预处理管道，以及一个涵盖多种评估指标的评估集。results:* 该 paper 通过对十种先进 TSG 方法在十个不同领域的实验来评估 \textsf{TSGBench} 的可靠性和一致性。结果显示，\textsf{TSGBench} 能够准确地评估 TSG 方法的性能，并且能够提供细致的方法评估结果。<details>
<summary>Abstract</summary>
Synthetic Time Series Generation (TSG) is crucial in a range of applications, including data augmentation, anomaly detection, and privacy preservation. Although significant strides have been made in this field, existing methods exhibit three key limitations: (1) They often benchmark against similar model types, constraining a holistic view of performance capabilities. (2) The use of specialized synthetic and private datasets introduces biases and hampers generalizability. (3) Ambiguous evaluation measures, often tied to custom networks or downstream tasks, hinder consistent and fair comparison.   To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural TSG Benchmark, designed for a unified and comprehensive assessment of TSG methods. It comprises three modules: (1) a curated collection of publicly available, real-world datasets tailored for TSG, together with a standardized preprocessing pipeline; (2) a comprehensive evaluation measures suite including vanilla measures, new distance-based assessments, and visualization tools; (3) a pioneering generalization test rooted in Domain Adaptation (DA), compatible with all methods. We have conducted extensive experiments across ten real-world datasets from diverse domains, utilizing ten advanced TSG methods and twelve evaluation measures, all gauged through \textsf{TSGBench}. The results highlight its remarkable efficacy and consistency. More importantly, \textsf{TSGBench} delivers a statistical breakdown of method rankings, illuminating performance variations across different datasets and measures, and offering nuanced insights into the effectiveness of each method.
</details>
<details>
<summary>摘要</summary>
“人工时间系列生成（TSG）在许多应用中是重要的，包括数据增强、异常探测和隐私保护。尽管这个领域已经做出了重要的进步，但现有的方法具有三个关键的限制：（1）它们经常对相似的模型类型进行评估，从而阻碍了全面的性能可能性。（2）使用特殊的人工生成和私人数据集 introduces 偏见和降低了普遍化。（3）不确定的评估标准，常常与自定义网络或下游任务相关，导致了不一致和公平的比较。”“为了突破这些限制，我们引入了 \textsf{TSGBench}，TSG Benchmark 的首个版本，旨在提供一个统一和完整的 TSG 方法评估平台。它包括三个模组：（1）一个精心选择的公共可用数据集，配备标准化的预处理管线；（2）一个完整的评估标准组合，包括净量测试、新的距离基准和视觉工具；（3）一个开创性的适应领域（DA）基础，可以与所有方法相容。我们在十个真实世界数据集上进行了广泛的实验，使用了十个进步的 TSG 方法和十二个评估标准，全部通过 \textsf{TSGBench} 进行评估。结果显示了它的很好的效果和一致性。”“更重要的是，\textsf{TSGBench} 提供了一个统计分析的方法排名，透过不同的数据集和评估标准进行分析，将方法的性能差异显示出来，并提供了深入的探究每个方法的效能。”
</details></li>
</ul>
<hr>
<h2 id="Convergence-Analysis-of-Decentralized-ASGD"><a href="#Convergence-Analysis-of-Decentralized-ASGD" class="headerlink" title="Convergence Analysis of Decentralized ASGD"></a>Convergence Analysis of Decentralized ASGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03754">http://arxiv.org/abs/2309.03754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauro DL Tosi, Martin Theobald</li>
<li>for: 这种研究旨在提高 Stochastic Gradient Descent (SGD) 的训练效率，以便在多个设备上进行分布式训练。</li>
<li>methods: 该研究使用的方法包括 asynchronous SGD (ASGD) 和 decentralized asynchronous SGD (DASGD)，以及一种新的 convergencerate 分析方法。</li>
<li>results: 研究结果表明，DASGD 的 convergencerate 为 $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(QS_{avg}\epsilon^{-3&#x2F;2}) + \mathcal{O}(S_{avg}\epsilon^{-1})$，当 gradients 不受限制时， convergencerate 为 $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(\sqrt{\hat{S}<em>{avg}\hat{S}</em>{max}\epsilon^{-1})$。这些结果适用于不变、 homogeneous 和 L-smooth 目标函数。<details>
<summary>Abstract</summary>
Over the last decades, Stochastic Gradient Descent (SGD) has been intensively studied by the Machine Learning community. Despite its versatility and excellent performance, the optimization of large models via SGD still is a time-consuming task. To reduce training time, it is common to distribute the training process across multiple devices. Recently, it has been shown that the convergence of asynchronous SGD (ASGD) will always be faster than mini-batch SGD. However, despite these improvements in the theoretical bounds, most ASGD convergence-rate proofs still rely on a centralized parameter server, which is prone to become a bottleneck when scaling out the gradient computations across many distributed processes.   In this paper, we present a novel convergence-rate analysis for decentralized and asynchronous SGD (DASGD) which does not require partial synchronization among nodes nor restrictive network topologies. Specifically, we provide a bound of $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(QS_{avg}\epsilon^{-3/2}) + \mathcal{O}(S_{avg}\epsilon^{-1})$ for the convergence rate of DASGD, where $S_{avg}$ is the average staleness between models, $Q$ is a constant that bounds the norm of the gradients, and $\epsilon$ is a (small) error that is allowed within the bound. Furthermore, when gradients are not bounded, we prove the convergence rate of DASGD to be $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(\sqrt{\hat{S}_{avg}\hat{S}_{max}\epsilon^{-1})$, with $\hat{S}_{max}$ and $\hat{S}_{avg}$ representing a loose version of the average and maximum staleness, respectively. Our convergence proof holds for a fixed stepsize and any non-convex, homogeneous, and L-smooth objective function. We anticipate that our results will be of high relevance for the adoption of DASGD by a broad community of researchers and developers.
</details>
<details>
<summary>摘要</summary>
最近几十年，随机梯度下降（SGD）在机器学习社区中被广泛研究。尽管它具有优秀的灵活性和性能，但是通过SGD来优化大型模型仍然是一个时间consuming的任务。为了减少训练时间，常常将训练过程分布到多个设备上。近些年来，人们发现了异步SGD（ASGD）的速度会比小批量SGD更快。然而，大多数ASGD的速度分析仍然基于中央参数服务器，这会导致训练过程中的瓶颈。在这篇论文中，我们提出了一种新的异步SGD（DASGD）的速度分析，不需要在节点之间进行部分同步，也不需要固定的网络拓扑。我们提供了一个速度 bound的式子： $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(QS_{avg}\epsilon^{-3/2}) + \mathcal{O}(S_{avg}\epsilon^{-1})$, 其中 $S_{avg}$ 是模型之间的均值偏差， $Q$ 是约束 gradients 的常数，而 $\epsilon$ 是允许的误差。而当 gradients 不受限制时，我们证明 DASGD 的速度为 $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(\sqrt{\hat{S}_{avg}\hat{S}_{max}\epsilon^{-1})$, 其中 $\hat{S}_{max}$ 和 $\hat{S}_{avg}$ 分别表示模型之间的平均和最大偏差。我们的速度证明在任何非 convex、同质、L-smooth 目标函数上都适用，并且适用于固定步长。我们预计这些结果将对广泛的研究人员和开发人员产生很高的 relevance。
</details></li>
</ul>
<hr>
<h2 id="Medoid-Silhouette-clustering-with-automatic-cluster-number-selection"><a href="#Medoid-Silhouette-clustering-with-automatic-cluster-number-selection" class="headerlink" title="Medoid Silhouette clustering with automatic cluster number selection"></a>Medoid Silhouette clustering with automatic cluster number selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03751">http://arxiv.org/abs/2309.03751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars Lenssen, Erich Schubert</li>
<li>for: 这篇论文主要是关于聚类评估的一种方法和其效率分析。</li>
<li>methods: 该论文使用了一种基于emedoid的Silhouette方法，并提供了两种快速版本，其中一种可以保证与原始方法相同的结果，并且提供了一个$O(k^2)$的运行速度提升。</li>
<li>results: 在实际数据集上，使用这种方法可以获得10464倍的速度提升，并且提供了一种直接选择最佳聚类数量的方法。<details>
<summary>Abstract</summary>
The evaluation of clustering results is difficult, highly dependent on the evaluated data set and the perspective of the beholder. There are many different clustering quality measures, which try to provide a general measure to validate clustering results. A very popular measure is the Silhouette. We discuss the efficient medoid-based variant of the Silhouette, perform a theoretical analysis of its properties, provide two fast versions for the direct optimization, and discuss the use to choose the optimal number of clusters. We combine ideas from the original Silhouette with the well-known PAM algorithm and its latest improvements FasterPAM. One of the versions guarantees equal results to the original variant and provides a run speedup of $O(k^2)$. In experiments on real data with 30000 samples and $k$=100, we observed a 10464$\times$ speedup compared to the original PAMMEDSIL algorithm. Additionally, we provide a variant to choose the optimal number of clusters directly.
</details>
<details>
<summary>摘要</summary>
“评估嵌套结果的评估是很困难的，很多因素会影响，包括评估数据集和观察者的角度。有许多不同的嵌套质量指标，尝试提供一个通用的验证嵌套结果的方法。一个非常受欢迎的指标是Silhouette。我们讨论了内部中心基于Silhouette的快速版本，进行了理论分析其性能，提供了两个快速版本以直接优化，并讨论了选择最佳嵌套数量的方法。我们结合了原始Silhouette与Well-known PAM算法以及其最新改进FasterPAM。其中一个版本保证与原始版本相同的结果，并提供了$O(k^2)$的执行速度提升。在实际数据上，我们观察到30000样本和$k$=100的情况下，实现了10464倍的执行速度提升，相比原始PAMMEDSIL算法。此外，我们还提供了选择最佳嵌套数量的方法。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Pipeline-Based-Conversational-Agents-with-Large-Language-Models"><a href="#Enhancing-Pipeline-Based-Conversational-Agents-with-Large-Language-Models" class="headerlink" title="Enhancing Pipeline-Based Conversational Agents with Large Language Models"></a>Enhancing Pipeline-Based Conversational Agents with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03748">http://arxiv.org/abs/2309.03748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mina Foosherian, Hendrik Purwins, Purna Rathnayake, Touhidul Alam, Rui Teimao, Klaus-Dieter Thoben</li>
<li>for: 这篇论文探讨了基于大语言模型（LLM）的自然语言处理（NLP）系统在设计和开发阶段以及运行阶段中的应用可能性。</li>
<li>methods: 论文采用了基于GPT-4的LLM技术，在两个阶段中使用LLM来增强pipeline-based conversational agent的能力。在设计和开发阶段，LLM可以帮助生成训练数据、提取实体和同义词、本地化和人物设计等。在运行阶段，LLM可以帮助 Contextualization、意图类型分类、避免对话崩溃和处理离cope问题、自动修改语句、重新译答案、准确问题解释和关闭问题回答等。</li>
<li>results: 通过在私人银行领域使用GPT-4进行了一些非正式实验，证明了LLM在提高pipeline-based conversational agent的能力方面的潜力。但是，由于隐私问题和现有系统集成的需求，公司可能会拒绝完全取代pipeline-based agents with LLMs。因此， authors提出了一种混合方法，即将LLMs integrate into pipeline-based agents，以保留现有系统集成和隐私保护的优势，同时充分利用LLMs的能力来提高 conversational agents的能力。<details>
<summary>Abstract</summary>
The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example. Companies may be hesitant to replace their pipeline-based agents with LLMs entirely due to privacy concerns and the need for deep integration within their existing ecosystems. A hybrid approach in which LLMs' are integrated into the pipeline-based agents allows them to save time and costs of building and running agents by capitalizing on the capabilities of LLMs while retaining the integration and privacy safeguards of their existing systems.
</details>
<details>
<summary>摘要</summary>
最新的人工智能和深度学习技术突破了基于大语言模型（LLM）的代理人，如GPT-4。然而，许多商业对话代理人开发工具是管道式的，它们有限制能够保持人类化的对话。这篇论文研究了 LLM 能够在两个阶段增强管道式对话代理人：1）在设计和开发阶段，LLM 可以帮助生成训练数据、提取实体和同义词、本地化和人物设计。2）在运行阶段，LLM 可以帮助 contextualization、意图分类以避免对话崩溃和处理 OUT-OF-SCOPE 问题、自动修正语句、重新表述响应、制定歧义问题、总结和启用关闭问答功能。我们在私人银行领域使用 GPT-4 进行了一些不正式的实验，以示出上述情况。由于隐私问题和现有系统集成的需求，公司可能会尝试保留其管道式代理人并尝试将 LLM  integrate into其中，以节省建立和运行代理人的时间和成本，同时利用 LLM 的能力而无需折衣现有系统的隐私保护和集成。
</details></li>
</ul>
<hr>
<h2 id="Learning-continuous-valued-treatment-effects-through-representation-balancing"><a href="#Learning-continuous-valued-treatment-effects-through-representation-balancing" class="headerlink" title="Learning continuous-valued treatment effects through representation balancing"></a>Learning continuous-valued treatment effects through representation balancing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03731">http://arxiv.org/abs/2309.03731</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/christopher-br/cbrnet">https://github.com/christopher-br/cbrnet</a></li>
<li>paper_authors: Christopher Bockel-Rickermann, Toon Vanderschueren, Jeroen Berrevoets, Tim Verdonck, Wouter Verbeke</li>
<li>for: 估计对象的治疗效果，即“剂量响应”，在各种领域都具有重要性，从医疗到商业、经济和以上等。</li>
<li>methods: 我们提出了一种名为CBRNet的 causal机器学习方法，用于从观察数据中估计个体剂量响应。CBRNet采用了Neyman-Rubin potential outcome框架，并将权衡表示学习扩展到连续valued treatments中。我们的工作是首次在连续valued treatment Setting中应用表示均衡。</li>
<li>results: 我们的实验表明，CBRNet可以准确地估计受 Selection bias的治疗效果，并与其他现有的方法相比，具有竞争性的性能。<details>
<summary>Abstract</summary>
Estimating the effects of treatments with an associated dose on an instance's outcome, the "dose response", is relevant in a variety of domains, from healthcare to business, economics, and beyond. Such effects, also known as continuous-valued treatment effects, are typically estimated from observational data, which may be subject to dose selection bias. This means that the allocation of doses depends on pre-treatment covariates. Previous studies have shown that conventional machine learning approaches fail to learn accurate individual estimates of dose responses under the presence of dose selection bias. In this work, we propose CBRNet, a causal machine learning approach to estimate an individual dose response from observational data. CBRNet adopts the Neyman-Rubin potential outcome framework and extends the concept of balanced representation learning for overcoming selection bias to continuous-valued treatments. Our work is the first to apply representation balancing in a continuous-valued treatment setting. We evaluate our method on a newly proposed benchmark. Our experiments demonstrate CBRNet's ability to accurately learn treatment effects under selection bias and competitive performance with respect to other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose CBRNet, a causal machine learning approach that estimates individual dose responses from observational data. CBRNet is based on the Neyman-Rubin potential outcome framework and extends the concept of balanced representation learning to overcome selection bias for continuous-valued treatments. Our work is the first to apply representation balancing in a continuous-valued treatment setting.We evaluate CBRNet on a newly proposed benchmark and demonstrate its ability to accurately learn treatment effects under selection bias, as well as competitive performance compared to other state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="A-Causal-Perspective-on-Loan-Pricing-Investigating-the-Impacts-of-Selection-Bias-on-Identifying-Bid-Response-Functions"><a href="#A-Causal-Perspective-on-Loan-Pricing-Investigating-the-Impacts-of-Selection-Bias-on-Identifying-Bid-Response-Functions" class="headerlink" title="A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions"></a>A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03730">http://arxiv.org/abs/2309.03730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Bockel-Rickermann, Sam Verboven, Tim Verdonck, Wouter Verbeke</li>
<li>for:  This paper aims to address the problem of selection bias in personalized pricing policies, which is not well-studied in the existing pricing literature.</li>
<li>methods:  The authors use causal inference and experimental methods to simulate varying levels of selection bias on a semi-synthetic dataset of mortgage loan applications in Belgium. They compare the performance of conventional methods such as logistic regression and neural networks with state-of-the-art methods from causal machine learning.</li>
<li>results:  The authors show that conventional methods are adversely affected by selection bias, while state-of-the-art methods from causal machine learning are capable of overcoming this bias and providing more accurate estimates of individual bid-response functions.<details>
<summary>Abstract</summary>
In lending, where prices are specific to both customers and products, having a well-functioning personalized pricing policy in place is essential to effective business making. Typically, such a policy must be derived from observational data, which introduces several challenges. While the problem of ``endogeneity'' is prominently studied in the established pricing literature, the problem of selection bias (or, more precisely, bid selection bias) is not. We take a step towards understanding the effects of selection bias by posing pricing as a problem of causal inference. Specifically, we consider the reaction of a customer to price a treatment effect. In our experiments, we simulate varying levels of selection bias on a semi-synthetic dataset on mortgage loan applications in Belgium. We investigate the potential of parametric and nonparametric methods for the identification of individual bid-response functions. Our results illustrate how conventional methods such as logistic regression and neural networks suffer adversely from selection bias. In contrast, we implement state-of-the-art methods from causal machine learning and show their capability to overcome selection bias in pricing data.
</details>
<details>
<summary>摘要</summary>
在贷款业务中，因为价格对特定客户和产品都具有特定的价格，有效的个性化价格策略是非常重要。通常，这种策略需要基于观察数据来 derivation，这会引入一些挑战。而“内生性”这个问题在已有的价格理论中已经得到了广泛的研究，但是“拍卖选择偏见”这个问题却没有得到过 suficient 的关注。我们通过视为价格推断问题来解决这个问题，specifically, we consider the reaction of a customer to price as a treatment effect.在我们的实验中，我们使用semi-synthetic dataset on Belgium mortgage loan applications来 simulate varying levels of selection bias。我们 investigate the potential of parametric and nonparametric methods for the identification of individual bid-response functions.我们的结果表明， conventional methods such as logistic regression and neural networks will suffer from selection bias.然而，我们实现了当前的 causal machine learning 方法，并证明它们可以在价格数据中超越选择偏见。
</details></li>
</ul>
<hr>
<h2 id="A-Natural-Gas-Consumption-Forecasting-System-for-Continual-Learning-Scenarios-based-on-Hoeffding-Trees-with-Change-Point-Detection-Mechanism"><a href="#A-Natural-Gas-Consumption-Forecasting-System-for-Continual-Learning-Scenarios-based-on-Hoeffding-Trees-with-Change-Point-Detection-Mechanism" class="headerlink" title="A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism"></a>A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03720">http://arxiv.org/abs/2309.03720</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rasvob/hoeffding-trees-with-cpd-multistep-forecasing">https://github.com/rasvob/hoeffding-trees-with-cpd-multistep-forecasing</a></li>
<li>paper_authors: Radek Svoboda, Sebastian Basterrech, Jędrzej Kozal, Jan Platoš, Michał Woźniak</li>
<li>for: 预测天然气消耗，考虑季节性和趋势，是精细规划天然气供应和消耗的关键。在供应威胁时，这也是一个关键因素，确保社会能够充足地供应能源。</li>
<li>methods: 本文提出了一种新的多步式预测天然气消耗方法，integrating change point detection，用于数据流处理。该方法可以在实际应用中进行适应性学习。</li>
<li>results: 根据提出的方法，我们在天然气消耗预测中实现了更高的预测精度。此外，我们还发现使用变化点检测可以选择不同的模型集，并且使用预测错误反馈Loop可以更好地适应不同的预测场景。<details>
<summary>Abstract</summary>
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model collection for successive time frames. Thus, three model collection selection procedures (with and without an error feedback loop) are defined and evaluated for forecasting scenarios with various densities of detected change points. These models were compared with change point agnostic baseline approaches. Our experiments show that fewer change points result in a lower forecasting error regardless of the model collection selection procedure employed. Also, simpler model collection selection procedures omitting forecasting error feedback leads to more robust forecasting models suitable for continual learning tasks.
</details>
<details>
<summary>摘要</summary>
预测天然气消耗，考虑季节性和趋势，是至关重要的，以便规划其供应和消耗，同时优化其获取成本，主要由工业实体进行。然而，在供应威胁时，也是一个关键元素，确保提供这种原料，以满足个人消耗需求，保障社会能源安全。本文介绍了一种新的多步预测天然气消耗，并 integrate change point detection 的方法，以便在数据流处理中实现连续学习。我们使用了 Hoeffding 树预测器和 Pruned Exact Linear Time (PELT) 算法来检测变点。通过将不同的模型集选择与时间帧相关联，我们定义了三种模型集选择过程（包括和不包括错误反馈循环），并对各种变点密度的预测场景进行评估。这些模型与无变点背景下的基准方法进行比较。我们的实验表明， fewer change points 对预测错误都有负面影响，无论使用哪种模型集选择过程。此外，简单的模型集选择过程，即不包括预测错误反馈，可以生成更加稳定的预测模型，适用于连续学习任务。
</details></li>
</ul>
<hr>
<h2 id="A-State-Representation-for-Diminishing-Rewards"><a href="#A-State-Representation-for-Diminishing-Rewards" class="headerlink" title="A State Representation for Diminishing Rewards"></a>A State Representation for Diminishing Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03710">http://arxiv.org/abs/2309.03710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ted Moskovitz, Samo Hromadka, Ahmed Touati, Diana Borsa, Maneesh Sahani</li>
<li>for: 本研究旨在解决多任务学习中agent快速适应不同预期的资源分配问题。</li>
<li>methods: 本文提出了一种新的状态表示法，称为λ表示法（λR），该表示法可以快速评估策略，并且在自然世界中，任务之间的依赖关系更加复杂的情况下具有优势。</li>
<li>results: 本文证明了λ表示法的正式性质，并在机器学习和自然行为研究中展示了其normative优势和实用性。<details>
<summary>Abstract</summary>
A common setting in multitask reinforcement learning (RL) demands that an agent rapidly adapt to various stationary reward functions randomly sampled from a fixed distribution. In such situations, the successor representation (SR) is a popular framework which supports rapid policy evaluation by decoupling a policy's expected discounted, cumulative state occupancies from a specific reward function. However, in the natural world, sequential tasks are rarely independent, and instead reflect shifting priorities based on the availability and subjective perception of rewarding stimuli. Reflecting this disjunction, in this paper we study the phenomenon of diminishing marginal utility and introduce a novel state representation, the $\lambda$ representation ($\lambda$R) which, surprisingly, is required for policy evaluation in this setting and which generalizes the SR as well as several other state representations from the literature. We establish the $\lambda$R's formal properties and examine its normative advantages in the context of machine learning, as well as its usefulness for studying natural behaviors, particularly foraging.
</details>
<details>
<summary>摘要</summary>
一般情况下，多任务学习（RL）中的agent需要快速适应不同的静态奖励函数的Random sampling。在这种情况下，Successor representation（SR）是一种广泛使用的框架，它通过分离一个策略的预期折扣、累积状态占用与特定奖励函数解耦来支持快速策略评估。然而，在自然世界中，继任任务很少是独立的，而是受到奖励刺激的可用性和主观感受的变化。基于这种分歧，在这篇论文中，我们研究了减少的偏好效应和引入了一种新的状态表示方法，λ表示（λR），这种表示方法 surprisingly 需要 для策略评估，并且总结了λR的正式性质和在机器学习中的normative优点，以及在研究自然行为，特别是搜寻行为中的使用价值。
</details></li>
</ul>
<hr>
<h2 id="Chat-Failures-and-Troubles-Reasons-and-Solutions"><a href="#Chat-Failures-and-Troubles-Reasons-and-Solutions" class="headerlink" title="Chat Failures and Troubles: Reasons and Solutions"></a>Chat Failures and Troubles: Reasons and Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03708">http://arxiv.org/abs/2309.03708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manal Helal, Patrick Holthaus, Gabriella Lakatos, Farshid Amirabdollahian</li>
<li>for: 本研究探讨了人机交互（HRI）中常见的问题，导致聊天失败和问题。</li>
<li>methods: 本研究使用了适当的机器人和聊天模型，识别常见的问题导致失败，识别可能的解决方案，并规划了不断改进。</li>
<li>results: 研究建议使用封闭控制算法，使用训练过的人工智能（AI）预训练模型，提供词汇筛选、批处理新数据集重新训练、在数据流上学习和&#x2F;或使用奖励学习模型自动更新训练模型，以减少错误。<details>
<summary>Abstract</summary>
This paper examines some common problems in Human-Robot Interaction (HRI) causing failures and troubles in Chat. A given use case's design decisions start with the suitable robot, the suitable chatting model, identifying common problems that cause failures, identifying potential solutions, and planning continuous improvement. In conclusion, it is recommended to use a closed-loop control algorithm that guides the use of trained Artificial Intelligence (AI) pre-trained models and provides vocabulary filtering, re-train batched models on new datasets, learn online from data streams, and/or use reinforcement learning models to self-update the trained models and reduce errors.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了人机合作（HRI）在聊天中出现的一些常见问题，导致失败和困难。这些设计决策包括适用的机器人、适用的对话模型、识别常见问题的原因、识别可能的解决方案、并规划持续改进。在结论中，建议使用封闭控制算法，使用已经训练的人工智能（AI）预训练模型，提供词汇筛选、批处理新数据集、在数据流中学习、以及使用强化学习模型自动更新训练模型，以降低错误。
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Semi-Supervised-Approach-with-Triplet-Markov-Chains"><a href="#A-Probabilistic-Semi-Supervised-Approach-with-Triplet-Markov-Chains" class="headerlink" title="A Probabilistic Semi-Supervised Approach with Triplet Markov Chains"></a>A Probabilistic Semi-Supervised Approach with Triplet Markov Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03707">http://arxiv.org/abs/2309.03707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katherine Morales, Yohan Petetin</li>
<li>for: 这篇论文旨在提出一个基于条件随机统计的 semi-supervised 泛化模型，用于类别Sequential data。</li>
<li>methods: 本论文提出了一个基于条件随机统计的Variational Bayesian inference方法，用于在半指定情况下训练受条件随机统计模型。</li>
<li>results: 本论文的结果显示，使用该方法可以在半指定情况下训练出高品质的类别模型，并且可以应对不同类型的Sequential data。<details>
<summary>Abstract</summary>
Triplet Markov chains are general generative models for sequential data which take into account three kinds of random variables: (noisy) observations, their associated discrete labels and latent variables which aim at strengthening the distribution of the observations and their associated labels. However, in practice, we do not have at our disposal all the labels associated to the observations to estimate the parameters of such models. In this paper, we propose a general framework based on a variational Bayesian inference to train parameterized triplet Markov chain models in a semi-supervised context. The generality of our approach enables us to derive semi-supervised algorithms for a variety of generative models for sequential Bayesian classification.
</details>
<details>
<summary>摘要</summary>
三重马尔可夫链是一种泛化生成模型，用于处理序列数据，它考虑了三种随机变量：受损的观察值、其关联的整数标签和隐藏变量，这些隐藏变量的目的是强化观察值和其关联的标签的分布。然而，在实践中，我们通常不 possess所有标签与观察值的对应关系，以便 estimate triplet Markov chain 模型的参数。在这篇论文中，我们提出了一种基于变分 bayesian 推理的通用框架，用于在半supervised情况下训练参数化的 triplet Markov chain 模型。我们的方法的通用性允许我们 derivate 半supervised 算法 для多种序列 Bayesian 分类模型。
</details></li>
</ul>
<hr>
<h2 id="DiffDefense-Defending-against-Adversarial-Attacks-via-Diffusion-Models"><a href="#DiffDefense-Defending-against-Adversarial-Attacks-via-Diffusion-Models" class="headerlink" title="DiffDefense: Defending against Adversarial Attacks via Diffusion Models"></a>DiffDefense: Defending against Adversarial Attacks via Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03702">http://arxiv.org/abs/2309.03702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hondamunigeprasannasilva/diffdefence">https://github.com/hondamunigeprasannasilva/diffdefence</a></li>
<li>paper_authors: Hondamunige Prasanna Silva, Lorenzo Seidenari, Alberto Del Bimbo</li>
<li>for: 这个论文是为了防御机器学习分类器免受敏感攻击而设计的。</li>
<li>methods: 这个论文使用了扩散模型来保护机器学习分类器，而不需要对分类器本身进行任何修改。</li>
<li>results: 论文表明，这种方法可以提供对敏感攻击的鲁棒防御，同时保持清晰精度和快速响应，并且可以与现有的分类器集成使用。Here’s the English version of the paper’s abstract for reference:”This paper presents a novel reconstruction method that leverages Diffusion Models to protect machine learning classifiers against adversarial attacks, all without requiring any modifications to the classifiers themselves. The susceptibility of machine learning models to minor input perturbations renders them vulnerable to adversarial attacks. While diffusion-based methods are typically disregarded for adversarial defense due to their slow reverse process, this paper demonstrates that our proposed method offers robustness against adversarial threats while preserving clean accuracy, speed, and plug-and-play compatibility.”<details>
<summary>Abstract</summary>
This paper presents a novel reconstruction method that leverages Diffusion Models to protect machine learning classifiers against adversarial attacks, all without requiring any modifications to the classifiers themselves. The susceptibility of machine learning models to minor input perturbations renders them vulnerable to adversarial attacks. While diffusion-based methods are typically disregarded for adversarial defense due to their slow reverse process, this paper demonstrates that our proposed method offers robustness against adversarial threats while preserving clean accuracy, speed, and plug-and-play compatibility. Code at: https://github.com/HondamunigePrasannaSilva/DiffDefence.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Short-Term-Load-Forecasting-Using-A-Particle-Swarm-Optimized-Multi-Head-Attention-Augmented-CNN-LSTM-Network"><a href="#Short-Term-Load-Forecasting-Using-A-Particle-Swarm-Optimized-Multi-Head-Attention-Augmented-CNN-LSTM-Network" class="headerlink" title="Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network"></a>Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03694">http://arxiv.org/abs/2309.03694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paapa Kwesi Quansah</li>
<li>for: 短期负载预测是电力系统的重要任务，因为它的非线性和动态性导致预测的问题。</li>
<li>methods: 我们的方法使用Particle-Swarm Optimization算法自动探索和优化参数，Multi-Head Attention机制确定关键特征，并实现了 Computational Efficiency。</li>
<li>results: 我们的方法在使用真实的电力需求数据进行评估，其精度、稳定性和计算效率都较前一代方法有明显提升。特别是，我们的 Mean Absolute Percentage Error 为 1.9376，与现有的方法相比，表示了一个新的时代在短期负载预测方面。<details>
<summary>Abstract</summary>
Short-term load forecasting is of paramount importance in the efficient operation and planning of power systems, given its inherent non-linear and dynamic nature. Recent strides in deep learning have shown promise in addressing this challenge. However, these methods often grapple with hyperparameter sensitivity, opaqueness in interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that surmounts these obstacles. Our approach harnesses the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to discern the salient features crucial for accurate forecasting, and a streamlined framework for computational efficiency. Our method undergoes rigorous evaluation using a genuine electricity demand dataset. The results underscore its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error of 1.9376 marks a significant advancement over existing state-of-the-art approaches, heralding a new era in short-term load forecasting.
</details>
<details>
<summary>摘要</summary>
短期荷压预测对电力系统的有效运行和规划具有极高的重要性，因为它的自然非线性和动态特性。Recent advances in deep learning have shown promise in addressing this challenge. However, these methods often struggle with hyperparameter sensitivity, lack of interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that overcomes these challenges. Our approach leverages the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to identify the critical features for accurate forecasting, and a streamlined framework for computational efficiency. Our method is rigorously evaluated using a real electricity demand dataset. The results demonstrate its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error of 1.9376 represents a significant improvement over existing state-of-the-art approaches, marking a new era in short-term load forecasting.Here's the word-for-word translation of the text into Simplified Chinese:短期荷压预测对电力系统的有效运行和规划具有极高的重要性，因为它的自然非线性和动态特性。Recent advances in deep learning have shown promise in addressing this challenge. However, these methods often struggle with hyperparameter sensitivity, lack of interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that overcomes these challenges. Our approach leverages the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to identify the critical features for accurate forecasting, and a streamlined framework for computational efficiency. Our method is rigorously evaluated using a real electricity demand dataset. The results demonstrate its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error of 1.9376 represents a significant improvement over existing state-of-the-art approaches, marking a new era in short-term load forecasting.
</details></li>
</ul>
<hr>
<h2 id="A-computationally-lightweight-safe-learning-algorithm"><a href="#A-computationally-lightweight-safe-learning-algorithm" class="headerlink" title="A computationally lightweight safe learning algorithm"></a>A computationally lightweight safe learning algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03672">http://arxiv.org/abs/2309.03672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Baumann, Krzysztof Kowalczyk, Koen Tiels, Paweł Wachel</li>
<li>for: 提供一种安全学习算法，可以在训练过程中提供概率性安全保证，而不需要了解系统动态模型。</li>
<li>methods: 使用 Nadaraya-Watson 估计器，而不是 Gaussian processes，以实现概率性安全保证。 Nadaraya-Watson 估计器的扩展级别为对数级别，与数据点数量成正比。</li>
<li>results: 提供了一种 theoretically guaranteed 的安全学习算法，并通过数字七度自由机械手 manipulator 的数据进行了数值实验。<details>
<summary>Abstract</summary>
Safety is an essential asset when learning control policies for physical systems, as violating safety constraints during training can lead to expensive hardware damage. In response to this need, the field of safe learning has emerged with algorithms that can provide probabilistic safety guarantees without knowledge of the underlying system dynamics. Those algorithms often rely on Gaussian process inference. Unfortunately, Gaussian process inference scales cubically with the number of data points, limiting applicability to high-dimensional and embedded systems. In this paper, we propose a safe learning algorithm that provides probabilistic safety guarantees but leverages the Nadaraya-Watson estimator instead of Gaussian processes. For the Nadaraya-Watson estimator, we can reach logarithmic scaling with the number of data points. We provide theoretical guarantees for the estimates, embed them into a safe learning algorithm, and show numerical experiments on a simulated seven-degrees-of-freedom robot manipulator.
</details>
<details>
<summary>摘要</summary>
安全是学习控制策略的重要资产，因为训练时违反安全限制可能会导致设备损坏。为解决这个需求，安全学派在训练时提供了一些可靠的安全保证。这些算法通常基于 Gaussian process 推理，但是 Gaussian process 推理的扩展缩放率是 cubic 的，因此只适用于高维和嵌入式系统。在这篇论文中，我们提出了一种安全学习算法，可以提供可靠的安全保证，但是使用 Nadaraya-Watson 估计器而不是 Gaussian processes。Nadaraya-Watson 估计器的扩展缩放率是 logarithmic 的，我们提供了理论保证，将其包含到安全学习算法中，并在一个模拟的七度自由机械 manipulate 器上进行了数值实验。
</details></li>
</ul>
<hr>
<h2 id="Dataset-Generation-and-Bonobo-Classification-from-Weakly-Labelled-Videos"><a href="#Dataset-Generation-and-Bonobo-Classification-from-Weakly-Labelled-Videos" class="headerlink" title="Dataset Generation and Bonobo Classification from Weakly Labelled Videos"></a>Dataset Generation and Bonobo Classification from Weakly Labelled Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03671">http://arxiv.org/abs/2309.03671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre-Etienne Martin</li>
<li>for: 这个论文旨在提出一个使用常用机器学习方法实现的大猩猩检测和分类管道，以便在没有人工干预的情况下，使用触摸屏设备测试大猩猩。</li>
<li>methods: 这篇论文使用了一种新采集的大猩猩记录数据集，并使用了蛇须探测器来在视频中空间地检测到各个个体。 使用了手工特征和不同的分类算法以及深度学习方法，包括ResNet体系，进行大猩猩识别。</li>
<li>results: 研究表明，经过意义性的数据分离后，使用精心调整的ResNet模型可以达到75%的准确率。<details>
<summary>Abstract</summary>
This paper presents a bonobo detection and classification pipeline built from the commonly used machine learning methods. Such application is motivated by the need to test bonobos in their enclosure using touch screen devices without human assistance. This work introduces a newly acquired dataset based on bonobo recordings generated semi-automatically. The recordings are weakly labelled and fed to a macaque detector in order to spatially detect the individual present in the video. Handcrafted features coupled with different classification algorithms and deep-learning methods using a ResNet architecture are investigated for bonobo identification. Performance is compared in terms of classification accuracy on the splits of the database using different data separation methods. We demonstrate the importance of data preparation and how a wrong data separation can lead to false good results. Finally, after a meaningful separation of the data, the best classification performance is obtained using a fine-tuned ResNet model and reaches 75% of accuracy.
</details>
<details>
<summary>摘要</summary>
这份论文描述了一个使用常用机器学习方法建立的bonobo检测和分类管道。这种应用是由测试bonobos在围墙中使用触摸屏设备而不需要人类协助的需求所驱动。这个工作介绍了一个新收集的bonobo记录 dataset，该dataset是通过半自动方式生成的。记录是弱Label的，并且通过macaque检测器来在视频中空间检测个体。手工设计的特征与不同的分类算法和深度学习方法使用ResNet架构进行bonobo识别。performance是根据数据库的分辨率进行比较，并且我们示出了数据预处理的重要性，并如何一个错误的数据分离可能导致false好的结果。最后，我们使用精度调整的ResNet模型，达到了75%的准确率。
</details></li>
</ul>
<hr>
<h2 id="How-adversarial-attacks-can-disrupt-seemingly-stable-accurate-classifiers"><a href="#How-adversarial-attacks-can-disrupt-seemingly-stable-accurate-classifiers" class="headerlink" title="How adversarial attacks can disrupt seemingly stable accurate classifiers"></a>How adversarial attacks can disrupt seemingly stable accurate classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03665">http://arxiv.org/abs/2309.03665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver J. Sutton, Qinghua Zhou, Ivan Y. Tyukin, Alexander N. Gorban, Alexander Bastounis, Desmond J. Higham</li>
<li>for: 这篇论文主要针对了遭受敏感攻击的学习系统，以及这种攻击的特点和原因。</li>
<li>methods: 论文使用了一种简单的普适的框架，以解释实际系统中观察到的一些特点，如攻击性能强大且容易构造的攻击方法，以及对于高维输入数据的模型的敏感性。</li>
<li>results: 论文发现了一些重要的结论，包括：对于高维输入数据的模型，小的攻击可以轻松地破坏模型的准确性，而大的随机干扰无法触发模型的攻击性。此外，论文还发现了一种Counterintuitive的现象：即使模型在训练和测试数据上具有小的差异，也可能会隐藏攻击性的潜在问题。<details>
<summary>Abstract</summary>
Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability -- notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fails to trigger the adversarial instability of the network. A surprising takeaway is that even small margins separating a classifier's decision surface from training and testing data can hide adversarial susceptibility from being detected using randomly sampled perturbations. Counterintuitively, using additive noise during training or testing is therefore inefficient for eradicating or detecting adversarial examples, and more demanding adversarial training is required.
</details>
<details>
<summary>摘要</summary>
敌对攻击可以很快地改变一个正常的学习系统的输出，只需要对输入数据进行一些微小的修改。即使这些修改看起来无关紧要，也可以让学习系统失去精度。这种现象可能是因为分类器在处理高维输入数据时的一个基本特性。我们提出了一个简单的普适的框架，可以解释在实际系统中观察到的一些特性，例如易于构建的敌对攻击和对输入数据的随机干扰的抗性。我们证明了这些现象也直接出现在实际的神经网络中，即使大量的随机干扰也无法让神经网络失去精度。一个惊人的发现是，即使分类器的决策面和训练数据之间的差别很小，也可以隐藏敌对攻击的敏感性。因此，使用随机干扰来检测或消除敌对例子可能是不够fficient的，需要更加严格的敌对训练。
</details></li>
</ul>
<hr>
<h2 id="Alzheimer-Disease-Detection-from-Raman-Spectroscopy-of-the-Cerebrospinal-Fluid-via-Topological-Machine-Learning"><a href="#Alzheimer-Disease-Detection-from-Raman-Spectroscopy-of-the-Cerebrospinal-Fluid-via-Topological-Machine-Learning" class="headerlink" title="Alzheimer Disease Detection from Raman Spectroscopy of the Cerebrospinal Fluid via Topological Machine Learning"></a>Alzheimer Disease Detection from Raman Spectroscopy of the Cerebrospinal Fluid via Topological Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03664">http://arxiv.org/abs/2309.03664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Conti, Martina Banchelli, Valentina Bessi, Cristina Cecchi, Fabrizio Chiti, Sara Colantonio, Cristiano D’Andrea, Marella de Angelis, Davide Moroni, Benedetta Nacmias, Maria Antonietta Pascali, Sandro Sorbi, Paolo Matteini</li>
<li>for: 这个研究用于判断阿尔茨海默病（AD）的诊断。</li>
<li>methods: 这个研究使用了拉曼谱（RS）技术和机器学习（ML）方法来分类AD和正常Control组的液体抗体样本。</li>
<li>results: 研究发现，使用Raw Spectra和预处理后的Spectra可以有效地分类AD和Control组，并且使用 topological 分析提高了分类精度（&gt;87%）。<details>
<summary>Abstract</summary>
The cerebrospinal fluid (CSF) of 19 subjects who received a clinical diagnosis of Alzheimer's disease (AD) as well as of 5 pathological controls have been collected and analysed by Raman spectroscopy (RS). We investigated whether the raw and preprocessed Raman spectra could be used to distinguish AD from controls. First, we applied standard Machine Learning (ML) methods obtaining unsatisfactory results. Then, we applied ML to a set of topological descriptors extracted from raw spectra, achieving a very good classification accuracy (>87%). Although our results are preliminary, they indicate that RS and topological analysis together may provide an effective combination to confirm or disprove a clinical diagnosis of AD. The next steps will include enlarging the dataset of CSF samples to validate the proposed method better and, possibly, to understand if topological data analysis could support the characterization of AD subtypes.
</details>
<details>
<summary>摘要</summary>
CSF液（cerebrospinal fluid）19名被诊断为阿尔ц海默病（Alzheimer's disease，AD）的病人和5名病理控制人的样本已经被收集并分析了使用拉曼光谱（Raman spectroscopy，RS）。我们研究了使用Raw和预处理的拉曼谱来分别AD和控制人。首先，我们使用标准机器学习（Machine Learning，ML）方法，但得到的结果并不满足。然后，我们使用ML方法对原始谱的 topological descriptor进行分析，达到了非常好的分类精度（>87%）。虽然我们的结果是初步的，但它们表明RS和 topological数据分析可能是确认或反对临床诊断AD的有效组合。接下来的步骤将包括扩大CSF样本集，以更好地验证我们的方法，并可能理解 topological data analysis是否可以支持AD的分型。
</details></li>
</ul>
<hr>
<h2 id="Towards-Comparable-Knowledge-Distillation-in-Semantic-Image-Segmentation"><a href="#Towards-Comparable-Knowledge-Distillation-in-Semantic-Image-Segmentation" class="headerlink" title="Towards Comparable Knowledge Distillation in Semantic Image Segmentation"></a>Towards Comparable Knowledge Distillation in Semantic Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03659">http://arxiv.org/abs/2309.03659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Onno Niemann, Christopher Vox, Thorben Werner</li>
<li>for: 本研究旨在探讨知识塑造（KD）在Semantic Segmentation中的应用，以提高模型大小和推理速度。</li>
<li>methods: 本研究提出25种提出的塑化损失项，从14篇最新4年的论文中提取。然而，由于不同的训练配置，对 published results 进行比较是很困难的。为 illustration，使用相同的模型和数据集，Structural and Statistical Texture Distillation (SSTKD) 报告了学生mIoU提升4.54个和最终性能29.19，而 Adaptive Perspective Distillation (APD) 只提高学生性能2.06个百分点，但实现了最终性能39.25。这种极大的差异的原因通常是模型参数的不佳选择和引用点模型的下性能。</li>
<li>results: 我们发现，塑化过程中的hyperparameter tuning问题导致了塑化提升的消失。为了提高未来研究的比较性，我们建立了三个dataset和两个学生模型的坚实基线，并提供了广泛的hyperparameter tuning信息。我们发现，只有两种技术可以与我们的简单基线在ADE20K dataset上竞争。<details>
<summary>Abstract</summary>
Knowledge Distillation (KD) is one proposed solution to large model sizes and slow inference speed in semantic segmentation. In our research we identify 25 proposed distillation loss terms from 14 publications in the last 4 years. Unfortunately, a comparison of terms based on published results is often impossible, because of differences in training configurations. A good illustration of this problem is the comparison of two publications from 2022. Using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. The reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. In our work, we reveal problems of insufficient hyperparameter tuning by showing that distillation improvements of two widely accepted frameworks, SKD and IFVD, vanish when hyperparameters are optimized sufficiently. To improve comparability of future research in the field, we establish a solid baseline for three datasets and two student models and provide extensive information on hyperparameter tuning. We find that only two out of eight techniques can compete with our simple baseline on the ADE20K dataset.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）是一种提出的解决方案，以减少大型模型的存储空间和 semantic segmentation 的推理速度。在我们的研究中，我们发现了25种提出的distillation损失项，来自14篇过去4年发表的论文。 unfortunately，due to differences in training configurations, a comparison of these terms based on published results is often impossible. a good illustration of this problem is the comparison of two publications from 2022. using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. the reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. in our work, we reveal problems of insufficient hyperparameter tuning by showing that distillation improvements of two widely accepted frameworks, SKD and IFVD, vanish when hyperparameters are optimized sufficiently. to improve comparability of future research in the field, we establish a solid baseline for three datasets and two student models and provide extensive information on hyperparameter tuning. we find that only two out of eight techniques can compete with our simple baseline on the ADE20K dataset.
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-Automatic-Audiobook-Creation"><a href="#Large-Scale-Automatic-Audiobook-Creation" class="headerlink" title="Large-Scale Automatic Audiobook Creation"></a>Large-Scale Automatic Audiobook Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03926">http://arxiv.org/abs/2309.03926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brendan Walsh, Mark Hamilton, Greg Newby, Xi Wang, Serena Ruan, Sheng Zhao, Lei He, Shaofei Zhang, Eric Dettinger, William T. Freeman, Markus Weimer</li>
<li>for: 这个论文的目的是提供一种自动生成高质量的声音书，以提高文学作品的可访问性和读者参与度。</li>
<li>methods: 该论文使用了现代神经网络文本读取技术，将Project Gutenberg电子书库中的大量ebook内容自动转化为高质量声音书。具有自定义发音速度、式样和情感响应等功能。</li>
<li>results: 该论文通过自动生成大量的开源声音书，提供了一个交互式 demo，让用户快速创建自己的个性化声音书。总共提供了超过五千个开源声音书，可以在 \url{<a target="_blank" rel="noopener" href="https://aka.ms/audiobook%7D">https://aka.ms/audiobook}</a> 上听写。<details>
<summary>Abstract</summary>
An audiobook can dramatically improve a work of literature's accessibility and improve reader engagement. However, audiobooks can take hundreds of hours of human effort to create, edit, and publish. In this work, we present a system that can automatically generate high-quality audiobooks from online e-books. In particular, we leverage recent advances in neural text-to-speech to create and release thousands of human-quality, open-license audiobooks from the Project Gutenberg e-book collection. Our method can identify the proper subset of e-book content to read for a wide collection of diversely structured books and can operate on hundreds of books in parallel. Our system allows users to customize an audiobook's speaking speed and style, emotional intonation, and can even match a desired voice using a small amount of sample audio. This work contributed over five thousand open-license audiobooks and an interactive demo that allows users to quickly create their own customized audiobooks. To listen to the audiobook collection visit \url{https://aka.ms/audiobook}.
</details>
<details>
<summary>摘要</summary>
Audiobooks可以大幅提高文学作品的可accessibility和读者参与度。然而，制作Audiobooks需要数百个工作小时的人工努力。在这项工作中，我们介绍了一个系统，可以自动生成高质量的Audiobooks从在线电子书。具体来说，我们利用了最新的神经网络文本读取技术，创建并发布了数量之多的人类质量、开源Audiobooks从Project Gutenberg电子书集。我们的方法可以确定电子书内容的合适子集，并可以同时处理数百本书。我们的系统允许用户自定义Audiobook的说速和风格，以及情感倾斜。甚至可以根据一小段示例音频来匹配愿意的声音。这项工作投入了五千个开源Audiobooks以及一个交互式demo，允许用户快速创建自己的个性化Audiobooks。要听取Audiobook集，请访问 \url{https://aka.ms/audiobook}.
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Lipschitz-Stability-of-GNN-for-Fairness"><a href="#Characterizing-Lipschitz-Stability-of-GNN-for-Fairness" class="headerlink" title="Characterizing Lipschitz Stability of GNN for Fairness"></a>Characterizing Lipschitz Stability of GNN for Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03648">http://arxiv.org/abs/2309.03648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaning Jia, Chunhui Zhang, Jundong Li, Chuxu Zhang</li>
<li>for: 这篇论文旨在研究图神经网络（GNNs）中的稳定性和公平性问题。</li>
<li>methods: 本论文使用 lipschitz bound 技术来限制 GNN 模型输出的变化，并对 GNN 模型在不同偏见下的表现进行了分析。</li>
<li>results: 研究发现， lipschitz bound 可以有效地限制 GNN 模型输出的偏见变化，并且可以帮助 GNN 模型更好地平衡准确性和公平性。<details>
<summary>Abstract</summary>
The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant remains elusive for non-Euclidean neural networks like GNNs, especially within fairness contexts. To narrow this gap, we begin with the general GNNs operating on an attributed graph, and formulate a Lipschitz bound to limit the changes in the output regarding biases associated with the input. Additionally, we theoretically analyze how the Lipschitz constant of a GNN model could constrain the output perturbations induced by biases learned from data for fairness training. We experimentally validate the Lipschitz bound's effectiveness in limiting biases of the model output. Finally, from a training dynamics perspective, we demonstrate why the theoretical Lipschitz bound can effectively guide the GNN training to better trade-off between accuracy and fairness.
</details>
<details>
<summary>摘要</summary>
《利氏 bound》，一种由稳健统计学派导出的技术，可以限制输出对输入的变化，考虑到相关的无关不平等因素。它是一种有效和可证明的方法，可以在无需更多计算成本的情况下，检查机器学习模型的输出稳定性。最近，图 neural network（GNN），它们在非欧几何数据上运行，已经吸引了广泛的关注。然而，没有前期研究探讨过GNN利氏 bound，以推动模型输出稳定性的研究，特别是在非欧几何数据上，具有自然的偏见。由于常见的图数据在GNN训练中带有自然的偏见，这会对模型输出稳定性提出严重的挑战，以保持训练过程中的公正性。尽管利氏常数在控制欧几何神经网络稳定性方面已经被使用，但计算非欧几何神经网络利氏常数的精确值仍然是一个棘手的问题，特别是在公正性上下文中。为了bridging这个差距，我们开始使用通用的GNN在Attribute graph上运行，并形ulated一个利氏 bound来限制输出对输入偏见的变化。此外，我们还 theoretically analyzed how GNN模型的利氏常数可以控制因输入偏见而导致的模型输出偏见。我们通过实验验证了利氏 bound的有效性，限制模型输出偏见。最后，从训练动态角度来看，我们示出了理论上的利氏 bound可以有效地导向GNN训练，以更好地平衡准确性和公正性。
</details></li>
</ul>
<hr>
<h2 id="Insights-Into-the-Inner-Workings-of-Transformer-Models-for-Protein-Function-Prediction"><a href="#Insights-Into-the-Inner-Workings-of-Transformer-Models-for-Protein-Function-Prediction" class="headerlink" title="Insights Into the Inner Workings of Transformer Models for Protein Function Prediction"></a>Insights Into the Inner Workings of Transformer Models for Protein Function Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03631">http://arxiv.org/abs/2309.03631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/markuswenzel/xai-proteins">https://github.com/markuswenzel/xai-proteins</a></li>
<li>paper_authors: Markus Wenzel, Erik Grüner, Nils Strodthoff</li>
<li>for: 本研究探讨了使用可解释AI（XAI）来探讨神经网络在蛋白质功能预测中的内部工作机制，并将已有的XAI方法扩展以覆盖转换器模型中的隐藏表示。</li>
<li>methods: 本研究使用了已经广泛使用的XAI方法的整合梯度，以便检查转换器模型中的隐藏表示。</li>
<li>results: 该方法允许我们查看序列中特定的氨基酸哪些位置被转换器模型 particualr attention，并证明这些相关序列部分与生物和化学预期相匹配，包括在嵌入层和模型中的转换器头部。<details>
<summary>Abstract</summary>
Motivation: We explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too. Results: The approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins. Availability and Implementation: Source code can be accessed at https://github.com/markuswenzel/xai-proteins .
</details>
<details>
<summary>摘要</summary>
Motivation: 我们研究了如何使用可解释AI（XAI）来探索神经网络内部工作的方式，通过扩展广泛使用XAI方法的集成梯度的扩展，以便可以 inspectTransformer模型中的隐藏表示。结果：我们能够确定序列中的氨基酸，并证明这些相关的序列部分与生物和化学预期相匹配，包括嵌入层和模型中的转移头。可用性和实现：源代码可以在https://github.com/markuswenzel/xai-proteins上获取。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Self-Supervised-Learning-of-Speech-Representation-via-Invariance-and-Redundancy-Reduction"><a href="#Understanding-Self-Supervised-Learning-of-Speech-Representation-via-Invariance-and-Redundancy-Reduction" class="headerlink" title="Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction"></a>Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03619">http://arxiv.org/abs/2309.03619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuf Brima, Ulf Krumnack, Simone Pika, Gunther Heidemann</li>
<li>for:  investigate how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data</li>
<li>methods: propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks</li>
<li>results: MBT improves representation generalization over original BT, especially when fine-tuning with limited target data, highlighting the importance of designing objectives that encourage invariant and transferable representations.Here’s the same information in English:</li>
<li>for: investigate how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data</li>
<li>methods: propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks</li>
<li>results: MBT improves representation generalization over original BT, especially when fine-tuning with limited target data, highlighting the importance of designing objectives that encourage invariant and transferable representations.<details>
<summary>Abstract</summary>
The choice of the objective function is crucial in emerging high-quality representations from self-supervised learning. This paper investigates how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data. We propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks. Our results show MBT improves representation generalization over original BT, especially when fine-tuning with limited target data. This highlights the importance of designing objectives that encourage invariant and transferable representations. Our analysis provides insights into how the BT learning objective can be tailored to produce speech representations that excel when adapted to new downstream tasks. This study is an important step towards developing reusable self-supervised speech representations.
</details>
<details>
<summary>摘要</summary>
“选择目标函数的选择是自动化学习中非常重要的，以获得高质量的表现。这篇论文探索了不同的巴洛兄弟（BT）目标函数形ulations对下游任务性能的影响，特别是对于语音数据。我们提出了修改后的巴洛兄弟（MBT）目标函数，将潜在值 норmalized，以强制scale-invariance，并评估了语音识别、性别识别和关键字搜寻任务。我们的结果显示MBT改进了表现泛化，特别是当 fine-tuning  WITH limited target data。这表明设计目标函数可以导致不对称和可转移的表现。我们的分析提供了关于如何使用 BT 学习目标函数设计语音表现，以便适应新的下游任务。这篇研究是发展可重用自动化学习语音表现的重要一步。”
</details></li>
</ul>
<hr>
<h2 id="Filtration-Surfaces-for-Dynamic-Graph-Classification"><a href="#Filtration-Surfaces-for-Dynamic-Graph-Classification" class="headerlink" title="Filtration Surfaces for Dynamic Graph Classification"></a>Filtration Surfaces for Dynamic Graph Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03616">http://arxiv.org/abs/2309.03616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aidos-lab/filtration_surfaces">https://github.com/aidos-lab/filtration_surfaces</a></li>
<li>paper_authors: Franz Srambical, Bastian Rieck</li>
<li>for: 本文提出了一种新的方法来分类动态图，以解决现有方法的缺点，包括不可扩展性和缺少EdgeWeight信息。</li>
<li>methods: 本文提出了一种名为”滤波表面”的新方法，它是可扩展的和灵活的，并且可以处理变化的节点集和EdgeWeight信息。</li>
<li>results: 实验表明，filtration surfaces方法可以超过现有的基线，并且具有最低的总标准差。此外，本方法可以在不同的 EdgeWeight 情况下实现最佳性能。<details>
<summary>Abstract</summary>
Existing approaches for classifying dynamic graphs either lift graph kernels to the temporal domain, or use graph neural networks (GNNs). However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account. We propose filtration surfaces, a novel method that is scalable and flexible, to alleviate said restrictions. We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information. Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation.
</details>
<details>
<summary>摘要</summary>
现有的方法对动态图进行分类 Either lift graph kernels to the temporal domain, or use graph neural networks (GNNs). However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account. We propose filtration surfaces, a novel method that is scalable and flexible, to alleviate these restrictions. We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information. Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation.Here's the breakdown of the translation:* 现有的方法 (existing approaches) -> 现有的方法 (existing methods)* 对动态图进行分类 (classifying dynamic graphs) -> 对动态图进行分类 (classifying dynamic graphs)* Either lift graph kernels to the temporal domain, or use graph neural networks (GNNs) -> Either lift graph kernels to the temporal domain, or use graph neural networks (GNNs)* However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account -> However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account* We propose filtration surfaces, a novel method that is scalable and flexible -> We propose filtration surfaces, a novel method that is scalable and flexible* to alleviate these restrictions -> to alleviate these restrictions* We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information -> We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information* Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation -> Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation
</details></li>
</ul>
<hr>
<h2 id="Your-Battery-Is-a-Blast-Safeguarding-Against-Counterfeit-Batteries-with-Authentication"><a href="#Your-Battery-Is-a-Blast-Safeguarding-Against-Counterfeit-Batteries-with-Authentication" class="headerlink" title="Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with Authentication"></a>Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with Authentication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03607">http://arxiv.org/abs/2309.03607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mhackiori/eisthentication">https://github.com/mhackiori/eisthentication</a></li>
<li>paper_authors: Francesco Marchiori, Mauro Conti</li>
<li>for: 提高锂离子电池身份验证技术，保证设备使用合法电池，确保设备的运行状况和用户的安全。</li>
<li>methods: 提出了两种新的锂离子电池身份验证方法，包括DCAuth和EISthentication，通过机器学习模型对电池内部特征进行自动验证，不需要外部设备，并能抵抗多种常见伪造技术。</li>
<li>results: 对20个数据集进行了分析，提取了 meaningful 特征，并实现了高精度的锂离子电池身份验证（最高达0.99）和模型识别（最高达0.96），同时也能够保持相同的标识性表现。<details>
<summary>Abstract</summary>
Lithium-ion (Li-ion) batteries are the primary power source in various applications due to their high energy and power density. Their market was estimated to be up to 48 billion U.S. dollars in 2022. However, the widespread adoption of Li-ion batteries has resulted in counterfeit cell production, which can pose safety hazards to users. Counterfeit cells can cause explosions or fires, and their prevalence in the market makes it difficult for users to detect fake cells. Indeed, current battery authentication methods can be susceptible to advanced counterfeiting techniques and are often not adaptable to various cells and systems. In this paper, we improve the state of the art on battery authentication by proposing two novel methodologies, DCAuth and EISthentication, which leverage the internal characteristics of each cell through Machine Learning models. Our methods automatically authenticate lithium-ion battery models and architectures using data from their regular usage without the need for any external device. They are also resilient to the most common and critical counterfeit practices and can scale to several batteries and devices. To evaluate the effectiveness of our proposed methodologies, we analyze time-series data from a total of 20 datasets that we have processed to extract meaningful features for our analysis. Our methods achieve high accuracy in battery authentication for both architectures (up to 0.99) and models (up to 0.96). Moreover, our methods offer comparable identification performances. By using our proposed methodologies, manufacturers can ensure that devices only use legitimate batteries, guaranteeing the operational state of any system and safety measures for the users.
</details>
<details>
<summary>摘要</summary>
锂离子（Li-ion）电池是各种应用的主要电源，其 Market 在 2022 年估算为高达 48 亿美元。然而，广泛采用锂离子电池导致假电池生产，这可能会对用户造成安全隐患。假电池可能会引起爆炸或火灾，而其市场上的存在使用户检测假电池困难。实际上，当前的电池身份验证方法可能会受到先进的假造技术的影响，而且通常不适用于不同的电池和系统。在这篇论文中，我们改进了电池身份验证的状态艺，提出了两种新的方法，即 DCAuth 和 EISthentication，它们通过机器学习模型来利用每个电池的内部特征进行身份验证。我们的方法可以自动Authenticate 锂离子电池模型和体系，不需要任何外部设备，同时也能够抗性高于最常见和最critical的假电池实践。为了评估我们提出的方法的效果，我们分析了 20 个数据集中的时间序列数据，并将其处理以提取有用的特征。我们的方法在锂离子电池模型和体系上达到了高精度的身份验证效果（达到 0.99），同时也能够保持相似的标识性表现。通过使用我们的方法，制造商可以确保设备只使用合法电池，保证系统的操作状态和用户的安全措施。
</details></li>
</ul>
<hr>
<h2 id="Beyond-attention-deriving-biologically-interpretable-insights-from-weakly-supervised-multiple-instance-learning-models"><a href="#Beyond-attention-deriving-biologically-interpretable-insights-from-weakly-supervised-multiple-instance-learning-models" class="headerlink" title="Beyond attention: deriving biologically interpretable insights from weakly-supervised multiple-instance learning models"></a>Beyond attention: deriving biologically interpretable insights from weakly-supervised multiple-instance learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03925">http://arxiv.org/abs/2309.03925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Willem Bonnaffé, CRUK ICGC Prostate Group, Freddie Hamdy, Yang Hu, Ian Mills, Jens Rittscher, Clare Verrill, Dan J. Woodcock</li>
<li>for: 这项研究的目的是提高多例学习（MIL）中的解释性，以便更好地理解模型在数字生理学中的预测方式。</li>
<li>methods: 这项研究使用了一种post-training分析方法，包括生成精制encoder的 tile-level attention和预测分数，以生成预测权重地图（PAW），并通过与核体分割masks集成来提高解释性。</li>
<li>results: 研究发现，在诊断和评估患病程度时，高注意度区域并不一定与肿瘤区域重叠，这表明需要研究非肿瘤细胞，以便更好地评估肿瘤的发展。<details>
<summary>Abstract</summary>
Recent advances in attention-based multiple instance learning (MIL) have improved our insights into the tissue regions that models rely on to make predictions in digital pathology. However, the interpretability of these approaches is still limited. In particular, they do not report whether high-attention regions are positively or negatively associated with the class labels or how well these regions correspond to previously established clinical and biological knowledge. We address this by introducing a post-training methodology to analyse MIL models. Firstly, we introduce prediction-attention-weighted (PAW) maps by combining tile-level attention and prediction scores produced by a refined encoder, allowing us to quantify the predictive contribution of high-attention regions. Secondly, we introduce a biological feature instantiation technique by integrating PAW maps with nuclei segmentation masks. This further improves interpretability by providing biologically meaningful features related to the cellular organisation of the tissue and facilitates comparisons with known clinical features. We illustrate the utility of our approach by comparing PAW maps obtained for prostate cancer diagnosis (i.e. samples containing malignant tissue, 381/516 tissue samples) and prognosis (i.e. samples from patients with biochemical recurrence following surgery, 98/663 tissue samples) in a cohort of patients from the international cancer genome consortium (ICGC UK Prostate Group). Our approach reveals that regions that are predictive of adverse prognosis do not tend to co-locate with the tumour regions, indicating that non-cancer cells should also be studied when evaluating prognosis.
</details>
<details>
<summary>摘要</summary>
Recent advances in 注意力基于多个实例学习（MIL）have improved our understanding of the tissue regions that models rely on to make predictions in digital pathology. However, the interpretability of these approaches is still limited. In particular, they do not report whether high-attention regions are positively or negatively associated with the class labels or how well these regions correspond to previously established clinical and biological knowledge. We address this by introducing a post-training methodology to analyze MIL models. Firstly, we introduce prediction-attention-weighted (PAW) maps by combining tile-level attention and prediction scores produced by a refined encoder, allowing us to quantify the predictive contribution of high-attention regions. Secondly, we introduce a biological feature instantiation technique by integrating PAW maps with nuclei segmentation masks. This further improves interpretability by providing biologically meaningful features related to the cellular organization of the tissue and facilitates comparisons with known clinical features. We illustrate the utility of our approach by comparing PAW maps obtained for prostate cancer diagnosis (i.e. samples containing malignant tissue, 381/516 tissue samples) and prognosis (i.e. samples from patients with biochemical recurrence following surgery, 98/663 tissue samples) in a cohort of patients from the international cancer genome consortium (ICGC UK Prostate Group). Our approach reveals that regions that are predictive of adverse prognosis do not tend to co-locate with the tumour regions, indicating that non-cancer cells should also be studied when evaluating prognosis.
</details></li>
</ul>
<hr>
<h2 id="Interactive-Hyperparameter-Optimization-in-Multi-Objective-Problems-via-Preference-Learning"><a href="#Interactive-Hyperparameter-Optimization-in-Multi-Objective-Problems-via-Preference-Learning" class="headerlink" title="Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning"></a>Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03581">http://arxiv.org/abs/2309.03581</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/automl/interactive-mo-ml">https://github.com/automl/interactive-mo-ml</a></li>
<li>paper_authors: Joseph Giovanelli, Alexander Tornede, Tanja Tornede, Marius Lindauer</li>
<li>for: 这个论文的目的是提出一种人类中心的交互式HPO方法，以优化多目标机器学习（MO-ML）问题。</li>
<li>methods: 该方法使用了参与学习来提取用户的愿景，以便在HPO过程中选择合适的评价指标。</li>
<li>results: 在一个环境影响ml的实验研究中，该方法在基于用户预先选择的指标优化hp中表现出色，并且在高级用户知道选择哪个指标时也能够达到相似的表现。<details>
<summary>Abstract</summary>
Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learning to extract desiderata from users that guide the optimization. Instead of relying on the user guessing the most suitable indicator for their needs, our approach automatically learns an appropriate indicator. Concretely, we leverage pairwise comparisons of distinct Pareto fronts to learn such an appropriate quality indicator. Then, we optimize the hyperparameters of the underlying MO-ML algorithm towards this learned indicator using a state-of-the-art HPO approach. In an experimental study targeting the environmental impact of ML, we demonstrate that our approach leads to substantially better Pareto fronts compared to optimizing based on a wrong indicator pre-selected by the user, and performs comparable in the case of an advanced user knowing which indicator to pick.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种人类中心的交互式 HPO 方法，适应多目标 ML 的需求。而不是让用户自己猜测最适合他们需求的指标，我们的方法会自动学习一个合适的指标。具体来说，我们利用 Pareto 前纬度之间的对比来学习这个指标。然后，我们使用现有的 HPO 方法来优化对应的 MO-ML 算法的超参数，以达到这个学习的指标。在针对 Machine Learning 的环境影响的实验研究中，我们展示了我们的方法可以比使用错误的指标进行优化而获得substantially 更好的 Pareto 前纬度，并在用户知道选择哪个指标的情况下表现相对。
</details></li>
</ul>
<hr>
<h2 id="DTW-S-Shape-based-Comparison-of-Time-series-with-Ordered-Local-Trend"><a href="#DTW-S-Shape-based-Comparison-of-Time-series-with-Ordered-Local-Trend" class="headerlink" title="DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend"></a>DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03579">http://arxiv.org/abs/2309.03579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajitesh Srivastava</li>
<li>For: The paper is written for researchers in applied domains who work with time-series data and need to measure the similarity or distance between them, particularly in the context of epidemics.* Methods: The paper proposes a novel measure called DTW+S, which creates an interpretable matrix representation of time-series data and then applies Dynamic Time Warping to compute distances between the matrices.* Results: The paper demonstrates the utility of DTW+S in ensemble building and clustering of epidemic curves, and shows that it results in better classification compared to Dynamic Time Warping for a class of datasets, particularly when local trends rather than scale play a decisive role.<details>
<summary>Abstract</summary>
Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification and clustering. Existing measures may fail to capture similarities due to local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable "closeness-preserving" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in ensemble building and clustering of epidemic curves. We also demonstrate that our approach results in better classification compared to Dynamic Time Warping for a class of datasets, particularly when local trends rather than scale play a decisive role.
</details>
<details>
<summary>摘要</summary>
mesure distance ou similarity entre données de série temporelle est un aspect fondamental de nombreuses applications, notamment la classification et la clustering. Les mesures existantes peuvent ne pas capturer les similitudes en raison des tendances locales (formes) et peuvent même produire des résultats erronés. Notre objectif est de développer une mesure qui cherche des tendances similaires qui ont lieu à des moments similaires et qui est facile à interpréter pour les chercheurs dans les domaines appliqués. Cela est particulièremment utile pour les applications où les séries de temps ont une suite de courbes locales significatives qui sont ordonnées, comme les épidémies (une augmentation suivie d'une pointe suivie d'une baisse). Nous proposons une nouvelle mesure, DTW+S, qui crée une représentation "closeness-preserving" matrix de la série de temps, où chaque colonne représente les tendances locales, et puis applique l'alignement temporel dynamique pour calculer les distances entre ces matrices. Nous présentons une analyse théorique qui soutient le choix de cette représentation. Nous démontrons l'utilité de DTW+S dans la construction d'ensembles et la clustering de courbes d'épidémie. Nous également démontrons que notre approche produit des classements meilleurs que Dynamic Time Warping pour une classe de données, en particulier lorsque les tendances locales plutôt que la taille jouent un rôle décisif.
</details></li>
</ul>
<hr>
<h2 id="Sparse-Federated-Training-of-Object-Detection-in-the-Internet-of-Vehicles"><a href="#Sparse-Federated-Training-of-Object-Detection-in-the-Internet-of-Vehicles" class="headerlink" title="Sparse Federated Training of Object Detection in the Internet of Vehicles"></a>Sparse Federated Training of Object Detection in the Internet of Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03569">http://arxiv.org/abs/2309.03569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luping Rao, Chuan Ma, Ming Ding, Yuwen Qian, Lu Zhou, Zhe Liu</li>
<li>for: 提高智能交通系统（ITS）中的物体检测精度，以提供实时有效的交通管理服务。</li>
<li>methods: 基于联邦学习框架，在中央服务器上共享良好地地方模型，并在边缘设备上进行笼统训练。</li>
<li>results: 实验结果表明，提议的方案可以达到需要的物体检测率，同时减少了大量的通信成本。<details>
<summary>Abstract</summary>
As an essential component part of the Intelligent Transportation System (ITS), the Internet of Vehicles (IoV) plays a vital role in alleviating traffic issues. Object detection is one of the key technologies in the IoV, which has been widely used to provide traffic management services by analyzing timely and sensitive vehicle-related information. However, the current object detection methods are mostly based on centralized deep training, that is, the sensitive data obtained by edge devices need to be uploaded to the server, which raises privacy concerns. To mitigate such privacy leakage, we first propose a federated learning-based framework, where well-trained local models are shared in the central server. However, since edge devices usually have limited computing power, plus a strict requirement of low latency in IoVs, we further propose a sparse training process on edge devices, which can effectively lighten the model, and ensure its training efficiency on edge devices, thereby reducing communication overheads. In addition, due to the diverse computing capabilities and dynamic environment, different sparsity rates are applied to edge devices. To further guarantee the performance, we propose, FedWeg, an improved aggregation scheme based on FedAvg, which is designed by the inverse ratio of sparsity rates. Experiments on the real-life dataset using YOLO show that the proposed scheme can achieve the required object detection rate while saving considerable communication costs.
</details>
<details>
<summary>摘要</summary>
作为智能交通系统（ITS）的重要组成部分，互联网器（IoV）在解决交通问题中发挥了关键作用。对象检测是IoV中的关键技术，通过实时和敏感的车辆相关信息的分析来提供交通管理服务。然而，现有的对象检测方法都基于中央深度训练，即从边缘设备获取的敏感数据需要上传到服务器，这会导致隐私泄露。为了缓解这种隐私泄露，我们首先提出了基于联邦学习的框架，其中Well-trained的本地模型在中央服务器中分享。然而，边缘设备通常具有有限的计算能力， plus IoV中的延迟要求很低，我们进一步提议使用缺省训练过程在边缘设备上，可以有效减轻模型，并在边缘设备上提高训练效率，从而减少通信开销。此外，由于边缘设备的多样 computing 能力和动态环境，我们采用不同的缺省率应用于边缘设备。为了进一步保证性能，我们提出了FedWeg，一种基于FedAvg的改进聚合方案，其中借鉴缺省率进行反比分配。实验使用实际数据集和YOLO显示，提出的方案可以实现需要的对象检测率，同时减少了大量的通信成本。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Efficacy-of-Supervised-Learning-vs-Large-Language-Models-for-Identifying-Cognitive-Distortions-and-Suicidal-Risks-in-Chinese-Social-Media"><a href="#Evaluating-the-Efficacy-of-Supervised-Learning-vs-Large-Language-Models-for-Identifying-Cognitive-Distortions-and-Suicidal-Risks-in-Chinese-Social-Media" class="headerlink" title="Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media"></a>Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03564">http://arxiv.org/abs/2309.03564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/chatglm2-6b">https://github.com/thudm/chatglm2-6b</a></li>
<li>paper_authors: Hongzhi Qi, Qing Zhao, Changwei Song, Wei Zhai, Dan Luo, Shuo Liu, Yi Jing Yu, Fan Wang, Huijing Zou, Bing Xiang Yang, Jianqiang Li, Guanghui Fu</li>
<li>for: 本研究探讨了使用大语言模型在中国社交媒体平台上进行心理健康风险识别和认知扭曲识别。</li>
<li>methods: 本研究使用了三种不同的策略：零shot、少shot和精度调整，对三个大语言模型（GPT-3.5、GPT-4和RoBERTa）进行了评估。</li>
<li>results: 研究发现，使用大语言模型进行心理健康风险识别和认知扭曲识别时，存在明显的性能差距，主要归结于模型无法完全捕捉微分类。此外，GPT-4在多个场景中表现出色，而GPT-3.5在精度调整后表现出明显的改善。<details>
<summary>Abstract</summary>
Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tuning. Our findings revealed a discernible performance gap between the large language models and traditional supervised learning approaches, primarily attributed to the models' inability to fully grasp subtle categories. Notably, while GPT-4 outperforms its counterparts in multiple scenarios, GPT-3.5 shows significant enhancement in suicide risk classification after fine-tuning. To our knowledge, this investigation stands as the maiden attempt at gauging large language models on Chinese social media tasks. This study underscores the forward-looking and transformative implications of using large language models in the field of psychology. It lays the groundwork for future applications in psychological research and practice.
</details>
<details>
<summary>摘要</summary>
大型语言模型，特别是快速进步的GPT系列，在各个领域中受到广泛关注，其影响力渐渐增长。然而，在医学领域，特别是心理学，对这些语言模型的实际应用还很少。同时，社交媒体平台上的用户们正在不断表达自己的情感，一些情感经常转化为负面情感，甚至达到自杀倾向。在时间上采取有效的 intervención 和预防措施是非常重要的。为了办理这一点，我们的研究团队决定进行以下两项实验：识别自杀风险和认知扭曲的任务。我们使用了批处理学习作为基础，并对三种不同的策略进行了考验：零shot、少shot 和精度调整。我们的发现显示，大型语言模型与传统的批处理学习方法相比，存在明显的性能差距，主要归结于模型无法完全理解微妙的类别。值得注意的是，GPT-4在多种场景中表现出色，而GPT-3.5在精度调整后表现出显著的自杀风险分类提升。根据我们所知，这是大型语言模型在中文社交媒体任务上的首次尝试。这项研究标志着大型语言模型在心理学领域的前进，并为未来在心理研究和实践中的应用奠定基础。
</details></li>
</ul>
<hr>
<h2 id="Trinary-Decision-Trees-for-missing-value-handling"><a href="#Trinary-Decision-Trees-for-missing-value-handling" class="headerlink" title="Trinary Decision Trees for missing value handling"></a>Trinary Decision Trees for missing value handling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03561">http://arxiv.org/abs/2309.03561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henning Zakrisson</li>
<li>for: 提高决策树回归和分类器中处理缺失数据的性能</li>
<li>methods: 使用Trinary决策树算法，不假设缺失值包含回归或分类Response的信息</li>
<li>results: 在MCARScene中，Trinary树超过其他算法表现出色，尤其是在只缺失外挂数据时; 在IMScene中，Trinary树落后于其他算法; 混合模型TrinaryMIA树在所有缺失场景中表现稳定。<details>
<summary>Abstract</summary>
This paper introduces the Trinary decision tree, an algorithm designed to improve the handling of missing data in decision tree regressors and classifiers. Unlike other approaches, the Trinary decision tree does not assume that missing values contain any information about the response. Both theoretical calculations on estimator bias and numerical illustrations using real data sets are presented to compare its performance with established algorithms in different missing data scenarios (Missing Completely at Random (MCAR), and Informative Missingness (IM)). Notably, the Trinary tree outperforms its peers in MCAR settings, especially when data is only missing out-of-sample, while lacking behind in IM settings. A hybrid model, the TrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes (MIA) approach, shows robust performance in all types of missingness. Despite the potential drawback of slower training speed, the Trinary tree offers a promising and more accurate method of handling missing data in decision tree algorithms.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了三元决策树算法，用于改进决策树回归和分类器处理缺失数据的方法。与其他方法不同，三元决策树不 assumptions 缺失值包含响应的信息。在不同的缺失数据情况（完全随机缺失（MCAR）和有用缺失（IM））下，通过理论计算和实际数据示例，与现有算法进行比较。结果显示，三元树在MCAR情况下表现出色，特别是只缺失外样数据时，而在IM情况下则落后。一种混合模型，三元MIA树，将三元树和缺失在特征（MIA）方法结合，在所有类型的缺失情况下显示了稳定的表现。虽然训练速度可能 slower，但三元树提供了一种更准确和有 Promise 的缺失数据处理方法。
</details></li>
</ul>
<hr>
<h2 id="On-the-dynamics-of-multi-agent-nonlinear-filtering-and-learning"><a href="#On-the-dynamics-of-multi-agent-nonlinear-filtering-and-learning" class="headerlink" title="On the dynamics of multi agent nonlinear filtering and learning"></a>On the dynamics of multi agent nonlinear filtering and learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03557">http://arxiv.org/abs/2309.03557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayed Pouria Talebi, Danilo Mandic</li>
<li>for: 这篇论文旨在研究多智能体系统在分布式学习和联邦学习场景下的行为。</li>
<li>methods: 该文提出了一种通用的多智能体系统行为形式化方法，并给出了实现凝结学习行为的条件。</li>
<li>results: 文章采用这种方法在分布式和联邦学习场景下应用，并得到了一些有趣的结果。<details>
<summary>Abstract</summary>
Multiagent systems aim to accomplish highly complex learning tasks through decentralised consensus seeking dynamics and their use has garnered a great deal of attention in the signal processing and computational intelligence societies. This article examines the behaviour of multiagent networked systems with nonlinear filtering/learning dynamics. To this end, a general formulation for the actions of an agent in multiagent networked systems is presented and conditions for achieving a cohesive learning behaviour is given. Importantly, application of the so derived framework in distributed and federated learning scenarios are presented.
</details>
<details>
<summary>摘要</summary>
多智能系统目的是通过分散式协同决策动力学完成高度复杂的学习任务，这已经在信号处理和计算智能社区中吸引了很大的关注。这篇文章研究了多智能网络系统中非线性筛选/学习动力学的行为。为此，我们提供了多智能网络系统中代表者行为的一般形式化表述，并给出了实现协调学习行为的条件。特别是，我们在分布式和联邦学习场景中应用了所 derivated框架。
</details></li>
</ul>
<hr>
<h2 id="MVD-A-Novel-Methodology-and-Dataset-for-Acoustic-Vehicle-Type-Classification"><a href="#MVD-A-Novel-Methodology-and-Dataset-for-Acoustic-Vehicle-Type-Classification" class="headerlink" title="MVD:A Novel Methodology and Dataset for Acoustic Vehicle Type Classification"></a>MVD:A Novel Methodology and Dataset for Acoustic Vehicle Type Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03544">http://arxiv.org/abs/2309.03544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohd Ashhad, Omar Ahmed, Sooraj K. Ambat, Zeeshan Ali Haq, Mansaf Alam</li>
<li>for: 这 paper 是为了开发 acoustic traffic monitoring 和 vehicle-type classification 算法而设计的两个开源数据集 MVD 和 MVDA。</li>
<li>methods: 这 paper 使用了 cepstrum 和spectrum 基于本地和全球音频特征，以及多输入神经网络来准确分类 acoustic signals。</li>
<li>results: 实验结果表明，我们的方法可以超越先前的基线值，并在 MVD 和 MVDA 数据集上达到了 91.98% 和 96.66% 的准确率。<details>
<summary>Abstract</summary>
Rising urban populations have led to a surge in vehicle use and made traffic monitoring and management indispensable. Acoustic traffic monitoring (ATM) offers a cost-effective and efficient alternative to more computationally expensive methods of monitoring traffic such as those involving computer vision technologies. In this paper, we present MVD and MVDA: two open datasets for the development of acoustic traffic monitoring and vehicle-type classification algorithms, which contain audio recordings of moving vehicles. The dataset contain four classes- Trucks, Cars, Motorbikes, and a No-vehicle class. Additionally, we propose a novel and efficient way to accurately classify these acoustic signals using cepstrum and spectrum based local and global audio features, and a multi-input neural network. Experimental results show that our methodology improves upon the established baselines of previous works and achieves an accuracy of 91.98% and 96.66% on MVD and MVDA Datasets, respectively. Finally, the proposed model was deployed through an Android application to make it accessible for testing and demonstrate its efficacy.
</details>
<details>
<summary>摘要</summary>
城市人口增长导致交通量增加，使交通监测和管理成为不可或缺的。喷流交通监测（ATM）提供了一种Cost-effective和高效的交通监测方法，而不是基于计算机视觉技术的更复杂和昂贵的方法。在这篇论文中，我们提供了两个开放数据集，用于开发喷流交通监测和车辆类型分类算法：MVD和MVDA数据集。这两个数据集包含了四个类别：卡车、汽车、摩托车和无车类。此外，我们还提出了一种新的和高效的方法，使用cepstrum和spectrum基于本地和全球音频特征，以及多输入神经网络来准确地分类这些喷流信号。实验结果表明，我们的方法超越了先前的基eline，并达到了MVD数据集上的91.98%和MVDA数据集上的96.66%的准确率。最后，我们提出了一种通过Android应用程序进行部署的方法，以便在实验和示范其效果。
</details></li>
</ul>
<hr>
<h2 id="Subgraph-based-Tight-Frames-on-Graphs-with-Compact-Supports-and-Vanishing-Moments"><a href="#Subgraph-based-Tight-Frames-on-Graphs-with-Compact-Supports-and-Vanishing-Moments" class="headerlink" title="Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments"></a>Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03537">http://arxiv.org/abs/2309.03537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruigang Zheng, Xiaosheng Zhuang</li>
<li>for: 本研究提出了一种新的、通用的方法，用于在图上构建紧凑支持的帧，基于一系列的层次分 partitions。</li>
<li>methods: 我们的方法可以跟踪到来自层次分 partitions的抽象构造，并可以flexibly incorporate 子图 Laplacians 到我们的帧设计中。这使得我们可以调整 (子图) 消失 момент的帧lets，以及其他属性，如方向性，以便高效地表示图示Signals with path-like supports。</li>
<li>results: 我们的提议的图帧在非线性近似任务中表现出色。<details>
<summary>Abstract</summary>
In this work, we proposed a novel and general method to construct tight frames on graphs with compact supports based on a series of hierarchical partitions. Starting from our abstract construction that generalizes previous methods based on partition trees, we are able to flexibly incorporate subgraph Laplacians into our design of graph frames. Consequently, our general methods permit adjusting the (subgraph) vanishing moments of the framelets and extra properties, such as directionality, for efficiently representing graph signals with path-like supports. Several variants are explicitly defined and tested. Experimental results show our proposed graph frames perform superiorly in non-linear approximation tasks.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种新的和通用的方法，用于在图上构建紧凑支持的图帧。从我们的抽象构建起，我们能够通过层次分 partitions来扩展先前基于 partition trees 的方法。因此，我们的通用方法允许我们在图帧中调整（子图）衰减瞬间，以及其他特性，如方向性，以有效地表示图信号。我们还显式定义了多种变体并进行测试。实验结果表明，我们的提议的图帧在非线性近似任务中表现出色。Here's the translation in Traditional Chinese:在这个工作中，我们提出了一种新的和通用的方法，用于在图上建构紧凑支持的图架。从我们的抽象建构起，我们能够通过层次分 partitions 来扩展先前基于 partition trees 的方法。因此，我们的通用方法允许我们在图架中调整（子图）衰减瞬间，以及其他特性，如方向性，以有效地表示图信号。我们还明确定义了多种变体并进行测试。实验结果显示，我们的建议的图架在非线性近似任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Feature-Enhancer-Segmentation-Network-FES-Net-for-Vessel-Segmentation"><a href="#Feature-Enhancer-Segmentation-Network-FES-Net-for-Vessel-Segmentation" class="headerlink" title="Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation"></a>Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03535">http://arxiv.org/abs/2309.03535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq M. Khan, Muhammad Arsalan, Shahzaib Iqbal, Imran Razzak, Erik Meijering</li>
<li>for: 静脉血管精准分割，以跟踪和诊断视力损伤的进程。</li>
<li>methods: 提议一种新的特征增强分割网络（FES-Net），可以准确地分割每个像素，不需要额外的图像增强步骤。</li>
<li>results: FES-Net在四个公共可用的state-of-the-art datasets上表现出色，舒适性比其他竞争方法更高。<details>
<summary>Abstract</summary>
Diseases such as diabetic retinopathy and age-related macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for the tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-the-art datasets: DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the superior performance of FES-Net compared to other competitive approaches documented in the existing literature.
</details>
<details>
<summary>摘要</summary>
疾病如肥胖症和年龄相关的macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-the-art datasets: DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the superior performance of FES-Net compared to other competitive approaches documented in the existing literature.Here's the translation in Traditional Chinese:疾病如肥胖症和年龄相关的macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-the-art datasets: DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the superior performance of FES-Net compared to other competitive approaches documented in the existing literature.
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Negative-Learning-Approach-to-Partial-Domain-Adaptation-Using-Source-Prototypes"><a href="#A-Robust-Negative-Learning-Approach-to-Partial-Domain-Adaptation-Using-Source-Prototypes" class="headerlink" title="A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes"></a>A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03531">http://arxiv.org/abs/2309.03531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandipan Choudhuri, Suli Adeniye, Arunabha Sen</li>
<li>for: 提高partial domain adaptation的稳定性和泛化能力</li>
<li>methods:  ensemble learning + 多元标签反馈 + 域平衡 optimize intra-class compactness and inter-class separation</li>
<li>results: 比现有state-of-the-art PDA方法更高的稳定性和泛化能力<details>
<summary>Abstract</summary>
This work proposes a robust Partial Domain Adaptation (PDA) framework that mitigates the negative transfer problem by incorporating a robust target-supervision strategy. It leverages ensemble learning and includes diverse, complementary label feedback, alleviating the effect of incorrect feedback and promoting pseudo-label refinement. Rather than relying exclusively on first-order moments for distribution alignment, our approach offers explicit objectives to optimize intra-class compactness and inter-class separation with the inferred source prototypes and highly-confident target samples in a domain-invariant fashion. Notably, we ensure source data privacy by eliminating the need to access the source data during the adaptation phase through a priori inference of source prototypes. We conducted a series of comprehensive experiments, including an ablation analysis, covering a range of partial domain adaptation tasks. Comprehensive evaluations on benchmark datasets corroborate our framework's enhanced robustness and generalization, demonstrating its superiority over existing state-of-the-art PDA approaches.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Partial Domain Adaptation" (PDA) is translated as "半领域适应" (half-domain adaptation)* "negative transfer problem" is translated as "负向传递问题" (negative transfer problem)* "robust target-supervision strategy" is translated as "鲁棒的目标监督策略" (robust target supervision strategy)* "ensemble learning" is translated as "集成学习" (ensemble learning)* "diverse, complementary label feedback" is translated as "多样、补充的标签反馈" (diverse and complementary label feedback)* "first-order moments" is translated as "首领oment" (first-order moment)* "distribution alignment" is translated as "分布对齐" (distribution alignment)* "intra-class compactness" is translated as "类内准确性" (intra-class compactness)* "inter-class separation" is translated as "类间分化" (inter-class separation)* "source data privacy" is translated as "源数据隐私" (source data privacy)* "a priori inference" is translated as "先验知" (a priori inference)
</details></li>
</ul>
<hr>
<h2 id="Efficient-Single-Object-Detection-on-Image-Patches-with-Early-Exit-Enhanced-High-Precision-CNNs"><a href="#Efficient-Single-Object-Detection-on-Image-Patches-with-Early-Exit-Enhanced-High-Precision-CNNs" class="headerlink" title="Efficient Single Object Detection on Image Patches with Early Exit Enhanced High-Precision CNNs"></a>Efficient Single Object Detection on Image Patches with Early Exit Enhanced High-Precision CNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03530">http://arxiv.org/abs/2309.03530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arne Moos</li>
<li>for: 本文提出了一种用于移动机器人检测对象的新方法，主要是检测足球。由于对象在不同光照条件下会有动态变化和模糊图像，因此检测技术具有挑战性。</li>
<li>methods: 本文提出了一种特制 convolutional neural network（CNN） architecture，用于在计算限制的 роботиче平台上进行高精度对象检测。该CNN拟合器通过在图像块中高精度地分类单个对象并确定其准确的空间位置来实现对象检测。此外，文章还 интегрирова了 Early Exits 技术以降低计算成本。</li>
<li>results: 本文的实验结果表明，使用提议方法可以实现对象检测的精度为 100%，并且具有 recall 接近 87%，而且处理时间只需要约 170 $\mu$s per 假设。通过将提议方法与 Early Exit 技术结合使用，可以在平均上提高计算时间的优化率高达 28%。<details>
<summary>Abstract</summary>
This paper proposes a novel approach for detecting objects using mobile robots in the context of the RoboCup Standard Platform League, with a primary focus on detecting the ball. The challenge lies in detecting a dynamic object in varying lighting conditions and blurred images caused by fast movements. To address this challenge, the paper presents a convolutional neural network architecture designed specifically for computationally constrained robotic platforms. The proposed CNN is trained to achieve high precision classification of single objects in image patches and to determine their precise spatial positions. The paper further integrates Early Exits into the existing high-precision CNN architecture to reduce the computational cost of easily rejectable cases in the background class. The training process involves a composite loss function based on confidence and positional losses with dynamic weighting and data augmentation. The proposed approach achieves a precision of 100% on the validation dataset and a recall of almost 87%, while maintaining an execution time of around 170 $\mu$s per hypotheses. By combining the proposed approach with an Early Exit, a runtime optimization of more than 28%, on average, can be achieved compared to the original CNN. Overall, this paper provides an efficient solution for an enhanced detection of objects, especially the ball, in computationally constrained robotic platforms.
</details>
<details>
<summary>摘要</summary>
The training process involves a composite loss function based on confidence and positional losses with dynamic weighting and data augmentation. The proposed approach achieves a precision of 100% on the validation dataset and a recall of almost 87%, while maintaining an execution time of around 170 microseconds per hypotheses. By combining the proposed approach with an Early Exit, a runtime optimization of more than 28%, on average, can be achieved compared to the original CNN. Overall, this paper provides an efficient solution for enhanced object detection, especially the ball, in computationally constrained robotic platforms.
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-Continual-Federated-Clustering-via-Adaptive-Resonance-Theory"><a href="#Privacy-preserving-Continual-Federated-Clustering-via-Adaptive-Resonance-Theory" class="headerlink" title="Privacy-preserving Continual Federated Clustering via Adaptive Resonance Theory"></a>Privacy-preserving Continual Federated Clustering via Adaptive Resonance Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03487">http://arxiv.org/abs/2309.03487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Masuyama-lab/FCAC">https://github.com/Masuyama-lab/FCAC</a></li>
<li>paper_authors: Naoki Masuyama, Yusuke Nojima, Yuichiro Toda, Chu Kiong Loo, Hisao Ishibuchi, Naoyuki Kubota</li>
<li>for: 静态集成学习算法不能处理数据分布未知或持续变化的问题，这篇论文提出了一种隐私保护的连续联合分类算法来解决这个问题。</li>
<li>methods: 该算法使用了适应振荡理论基于的分类算法作为基准分类器，因此具有连续学习能力。</li>
<li>results: 实验结果表明，该算法在 sintetic 和实际数据上具有较高的分类性能，同时实现了数据隐私保护和连续学习能力。I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
With the increasing importance of data privacy protection, various privacy-preserving machine learning methods have been proposed. In the clustering domain, various algorithms with a federated learning framework (i.e., federated clustering) have been actively studied and showed high clustering performance while preserving data privacy. However, most of the base clusterers (i.e., clustering algorithms) used in existing federated clustering algorithms need to specify the number of clusters in advance. These algorithms, therefore, are unable to deal with data whose distributions are unknown or continually changing. To tackle this problem, this paper proposes a privacy-preserving continual federated clustering algorithm. In the proposed algorithm, an adaptive resonance theory-based clustering algorithm capable of continual learning is used as a base clusterer. Therefore, the proposed algorithm inherits the ability of continual learning. Experimental results with synthetic and real-world datasets show that the proposed algorithm has superior clustering performance to state-of-the-art federated clustering algorithms while realizing data privacy protection and continual learning ability. The source code is available at \url{https://github.com/Masuyama-lab/FCAC}.
</details>
<details>
<summary>摘要</summary>
随着数据隐私保护的重要性日益增加，各种隐私保护机器学习方法已经被提出。在聚类领域，使用联合学习框架（i.e., 联合聚类）的各种算法已经得到了广泛的研究和应用，并表现出高聚类性能而又保护数据隐私。然而，大多数基本聚类算法（i.e., 聚类算法）使用在现有的联合聚类算法中需要先 specify 聚类数量。这些算法因此无法处理数据的分布未知或持续变化。为解决这个问题，本文提出了一种隐私保护的 continual 联合聚类算法。在提出的算法中，使用基于适应振荡理论的聚类算法，该算法具有Continual Learning能力。实验结果表明，提出的算法与现有的联合聚类算法相比，具有更高的聚类性能，同时实现了数据隐私保护和Continual Learning能力。源代码可以在 \url{https://github.com/Masuyama-lab/FCAC} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Fast-FixMatch-Faster-Semi-Supervised-Learning-with-Curriculum-Batch-Size"><a href="#Fast-FixMatch-Faster-Semi-Supervised-Learning-with-Curriculum-Batch-Size" class="headerlink" title="Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size"></a>Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03469">http://arxiv.org/abs/2309.03469</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Chen, Chen Dun, Anastasios Kyrillidis</li>
<li>for: 这个论文的目的是提出一种名为CURRICULUM BATCH SIZE（CBS）的无标签批处理训练策略，以减少SSL训练计算量。</li>
<li>methods: 该论文使用了强化标签扩充、CURRICULUM Pseudo Labeling（CPL）和FixMatch算法来实现这个目标。</li>
<li>results: 该论文的实验结果表明，在CIFAR-10、CIFAR-100、SVHN和STL-10等 dataset上，使用CBS和强化标签扩充&#x2F;CPL可以减少训练计算量，同时保持与传统 FixMatch 算法相同的性能水平。此外，该论文还应用了这些策略在联合学习和流处理学习中，并取得了类似的结果。<details>
<summary>Abstract</summary>
Advances in Semi-Supervised Learning (SSL) have almost entirely closed the gap between SSL and Supervised Learning at a fraction of the number of labels. However, recent performance improvements have often come \textit{at the cost of significantly increased training computation}. To address this, we propose Curriculum Batch Size (CBS), \textit{an unlabeled batch size curriculum which exploits the natural training dynamics of deep neural networks.} A small unlabeled batch size is used in the beginning of training and is gradually increased to the end of training. A fixed curriculum is used regardless of dataset, model or number of epochs, and reduced training computations is demonstrated on all settings. We apply CBS, strong labeled augmentation, Curriculum Pseudo Labeling (CPL) \citep{FlexMatch} to FixMatch \citep{FixMatch} and term the new SSL algorithm Fast FixMatch. We perform an ablation study to show that strong labeled augmentation and/or CPL do not significantly reduce training computations, but, in synergy with CBS, they achieve optimal performance. Fast FixMatch also achieves substantially higher data utilization compared to previous state-of-the-art. Fast FixMatch achieves between $2.1\times$ - $3.4\times$ reduced training computations on CIFAR-10 with all but 40, 250 and 4000 labels removed, compared to vanilla FixMatch, while attaining the same cited state-of-the-art error rate \citep{FixMatch}. Similar results are achieved for CIFAR-100, SVHN and STL-10. Finally, Fast MixMatch achieves between $2.6\times$ - $3.3\times$ reduced training computations in federated SSL tasks and online/streaming learning SSL tasks, which further demonstrate the generializbility of Fast MixMatch to different scenarios and tasks.
</details>
<details>
<summary>摘要</summary>
SSL 技术的进步已经几乎完全减少了与超级vised learning（SSL）之间的差距，但是最近的性能改进往往来于提高训练计算量。为了解决这个问题，我们提出了批处理大小学习纲程（CBS），它利用深度神经网络的自然训练动力学来实现。在训练的早期，使用一个小的无标签批处理大小，逐渐增加到训练的末尾。不 matter what dataset, model or number of epochs, we use a fixed curriculum regardless of the dataset, model or number of epochs.我们发现，使用 CBS 可以减少训练计算量。在 FixMatch 基础上，我们采用了强化标签增强、CURRICULUM Pseudo Labeling（CPL）和 Curriculum Batch Size（CBS），并将其称为 Fast FixMatch。我们进行了一个ablation study，发现强化标签增强和/或 CPL 不会减少训练计算量，但是在 CBS 的帮助下，它们可以实现最佳性能。 Fast FixMatch 还实现了对 CIFAR-10 的大量数据使用，相比之前的状态态峰值，提高了数据利用率。Fast MixMatch 在 federated SSL 任务和在线/流动学习 SSL 任务中实现了 $2.6\times$ - $3.3\times$ 减少训练计算量，这些结果再次证明 Fast MixMatch 对不同场景和任务的通用性。
</details></li>
</ul>
<hr>
<h2 id="Cross-Image-Context-Matters-for-Bongard-Problems"><a href="#Cross-Image-Context-Matters-for-Bongard-Problems" class="headerlink" title="Cross-Image Context Matters for Bongard Problems"></a>Cross-Image Context Matters for Bongard Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03468">http://arxiv.org/abs/2309.03468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nraghuraman/bongard-context">https://github.com/nraghuraman/bongard-context</a></li>
<li>paper_authors: Nikhil Raghuraman, Adam W. Harley, Leonidas Guibas</li>
<li>for: 解决自然图像Bongard问题，提高现有方法的性能。</li>
<li>methods: 使用简单的方法将涉及多个支持图像的跨图像上下文integrated into the model,以提高模型的准确率。</li>
<li>results: 实验结果表明，新方法可以 дости得到新的状态数据集Bongard-LOGO（75.3%）和Bongard-HOI（72.45%）的优秀表现，并在原始Bongard问题集（60.84%）上达到了强表现。<details>
<summary>Abstract</summary>
Current machine learning methods struggle to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy (where chance is 50%). Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a variety of simple methods to take this cross-image context into account, and demonstrate substantial gains over prior methods, leading to new state-of-the-art performance on Bongard-LOGO (75.3%) and Bongard-HOI (72.45%) and strong performance on the original Bongard problem set (60.84%).
</details>
<details>
<summary>摘要</summary>
（Current machine learning methods are struggling to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy, where chance is 50%. Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a variety of simple methods to take this cross-image context into account, and demonstrate substantial gains over prior methods, leading to new state-of-the-art performance on Bongard-LOGO (75.3%) and Bongard-HOI (72.45%) and strong performance on the original Bongard problem set (60.84%).）
</details></li>
</ul>
<hr>
<h2 id="Multi-Modality-Guidance-Network-For-Missing-Modality-Inference"><a href="#Multi-Modality-Guidance-Network-For-Missing-Modality-Inference" class="headerlink" title="Multi-Modality Guidance Network For Missing Modality Inference"></a>Multi-Modality Guidance Network For Missing Modality Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03452">http://arxiv.org/abs/2309.03452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuokai Zhao, Harish Palani, Tianyi Liu, Lena Evans, Ruth Toner</li>
<li>for: 解决多模态模型在推理阶段缺失模式时的问题，提高模型在不同模式下的推理性能。</li>
<li>methods: 提出一种新的导航网络，通过在训练阶段优化多模态表示来促进单模态模型的训练，从而提高推理性能。</li>
<li>results: 实际实验表明，提出的框架可以在暴力检测任务中训练单模态模型，其性能明显超过传统训练方法，同时保持同样的推理成本。<details>
<summary>Abstract</summary>
Multimodal models have gained significant success in recent years. Standard multimodal approaches often assume unchanged modalities from training stage to inference stage. In practice, however, many scenarios fail to satisfy such assumptions with missing modalities during inference, leading to limitations on where multimodal models can be applied. While existing methods mitigate the problem through reconstructing the missing modalities, it increases unnecessary computational cost, which could be just as critical, especially for large, deployed systems. To solve the problem from both sides, we propose a novel guidance network that promotes knowledge sharing during training, taking advantage of the multimodal representations to train better single-modality models for inference. Real-life experiment in violence detection shows that our proposed framework trains single-modality models that significantly outperform its traditionally trained counterparts while maintaining the same inference cost.
</details>
<details>
<summary>摘要</summary>
Translate the given text into Simplified Chinese.</SYS>多模态模型在最近几年内取得了 significante 成功。标准的多模态方法frequently假设从训练阶段到推论阶段modalities的不变，但在实践中，许多情况下fail to satisfy这些假设， resulting in missing modalities during inference, leading to limitations on where multimodal models can be applied. While existing methods mitigate the problem through reconstructing the missing modalities, it increases unnecessary computational cost, which could be just as critical, especially for large, deployed systems. To solve the problem from both sides, we propose a novel guidance network that promotes knowledge sharing during training, taking advantage of the multimodal representations to train better single-modality models for inference. Real-life experiment in violence detection shows that our proposed framework trains single-modality models that significantly outperform its traditionally trained counterparts while maintaining the same inference cost.(Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. Traditional Chinese is also widely used, especially in Hong Kong, Taiwan, and other countries.)
</details></li>
</ul>
<hr>
<h2 id="Cross-domain-Sound-Recognition-for-Efficient-Underwater-Data-Analysis"><a href="#Cross-domain-Sound-Recognition-for-Efficient-Underwater-Data-Analysis" class="headerlink" title="Cross-domain Sound Recognition for Efficient Underwater Data Analysis"></a>Cross-domain Sound Recognition for Efficient Underwater Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03451">http://arxiv.org/abs/2309.03451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongsoo Park, Dong-Gyun Han, Hyoung Sul La, Sangmin Lee, Yoonchang Han, Eun-Jin Yang</li>
<li>for: 这 paper 是为了分析大量的海洋声学数据而提出的一种深度学习方法，利用一个基于广泛非海洋（天空）声音的模型。</li>
<li>methods: 我们提出了一种两重方法，首先使用 PCA 和 UMAP 可视化海洋数据，然后选择候选标签进行进一步训练。</li>
<li>results: 我们通过对selected海洋数据和非海洋数据进行训练，实现了一个准确率超过 84.3% 的模型，以识别海洋中的气枪声音。<details>
<summary>Abstract</summary>
This paper presents a novel deep learning approach for analyzing massive underwater acoustic data by leveraging a model trained on a broad spectrum of non-underwater (aerial) sounds. Recognizing the challenge in labeling vast amounts of underwater data, we propose a two-fold methodology to accelerate this labor-intensive procedure.   The first part of our approach involves PCA and UMAP visualization of the underwater data using the feature vectors of an aerial sound recognition model. This enables us to cluster the data in a two dimensional space and listen to points within these clusters to understand their defining characteristics. This innovative method simplifies the process of selecting candidate labels for further training.   In the second part, we train a neural network model using both the selected underwater data and the non-underwater dataset. We conducted a quantitative analysis to measure the precision, recall, and F1 score of our model for recognizing airgun sounds, a common type of underwater sound. The F1 score achieved by our model exceeded 84.3%, demonstrating the effectiveness of our approach in analyzing underwater acoustic data.   The methodology presented in this paper holds significant potential to reduce the amount of labor required in underwater data analysis and opens up new possibilities for further research in the field of cross-domain data analysis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>PCA and UMAP visualization of the underwater data using the feature vectors of an aerial sound recognition model. This enables clustering of the data in a two-dimensional space and listening to points within these clusters to understand their defining characteristics.2. Training a neural network model using both the selected underwater data and the non-underwater dataset. The model’s precision, recall, and F1 score for recognizing airgun sounds, a common type of underwater sound, were measured and found to exceed 84.3%.The proposed methodology has the potential to significantly reduce the amount of labor required in underwater data analysis and opens up new possibilities for cross-domain data analysis.</details></li>
</ol>
<hr>
<h2 id="XGen-7B-Technical-Report"><a href="#XGen-7B-Technical-Report" class="headerlink" title="XGen-7B Technical Report"></a>XGen-7B Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03450">http://arxiv.org/abs/2309.03450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, Caiming Xiong</li>
<li>for: 这篇论文目标是为了提高大语言模型（LLMs）的性能，并且开源其模型，以便研究人员可以在此基础上进行进一步的研究和应用。</li>
<li>methods: 这篇论文使用了一系列的7B参数模型，并在8K字节长度上进行了训练。此外，它还对公共领域的教程数据进行了精度调整，创造了专门用于 instruciton-tuned 模型（XGen-Inst）。</li>
<li>results: 论文的评估结果显示，XGen模型在标准测试 benchmark 上达到了相对或更好的结果，而且在长序模型化任务上表现更出色，特别是在8K字节长度上。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling tasks shows the benefits of our 8K-sequence models over 2K-sequence open-source LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Broadband-Ground-Motion-Synthesis-via-Generative-Adversarial-Neural-Operators-Development-and-Validation"><a href="#Broadband-Ground-Motion-Synthesis-via-Generative-Adversarial-Neural-Operators-Development-and-Validation" class="headerlink" title="Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation"></a>Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03447">http://arxiv.org/abs/2309.03447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yzshi5/gm-gano">https://github.com/yzshi5/gm-gano</a></li>
<li>paper_authors: Yaozhong Shi, Grigorios Lavrentiadis, Domniki Asimaki, Zachary E. Ross, Kamyar Azizzadenesheli</li>
<li>For: The paper is written for generating ground-motion synthesis using a Generative Adversarial Neural Operator (GANO) for earthquake engineering applications.* Methods: The paper uses Neural Operators, a resolution-invariant architecture that allows for training the model independently of the data sampling frequency, to generate three-component acceleration time histories conditioned on moment magnitude, rupture distance, time-average shear-wave velocity at the top $30m$, and tectonic environment or style of faulting.* Results: The paper shows that the proposed framework, called cGM-GANO, can recover the magnitude, distance, and $V_{S30}$ scaling of Fourier amplitude and pseudo-spectral accelerations, and produces consistent median scaling with conventional Ground Motion Models (GMMs) for the corresponding tectonic environments. However, the largest misfit is observed at short distances due to the scarcity of training data.<details>
<summary>Abstract</summary>
We present a data-driven model for ground-motion synthesis using a Generative Adversarial Neural Operator (GANO) that combines recent advancements in machine learning and open access strong motion data sets to generate three-component acceleration time histories conditioned on moment magnitude ($M$), rupture distance ($R_{rup}$), time-average shear-wave velocity at the top $30m$ ($V_{S30}$), and tectonic environment or style of faulting. We use Neural Operators, a resolution invariant architecture that guarantees that the model training is independent of the data sampling frequency. We first present the conditional ground-motion synthesis algorithm (referred to heretofore as cGM-GANO) and discuss its advantages compared to previous work. Next, we verify the cGM-GANO framework using simulated ground motions generated with the Southern California Earthquake Center (SCEC) Broadband Platform (BBP). We lastly train cGM-GANO on a KiK-net dataset from Japan, showing that the framework can recover the magnitude, distance, and $V_{S30}$ scaling of Fourier amplitude and pseudo-spectral accelerations. We evaluate cGM-GANO through residual analysis with the empirical dataset as well as by comparison with conventional Ground Motion Models (GMMs) for selected ground motion scenarios. Results show that cGM-GANO produces consistent median scaling with the GMMs for the corresponding tectonic environments. The largest misfit is observed at short distances due to the scarcity of training data. With the exception of short distances, the aleatory variability of the response spectral ordinates is also well captured, especially for subduction events due to the adequacy of training data. Applications of the presented framework include generation of risk-targeted ground motions for site-specific engineering applications.
</details>
<details>
<summary>摘要</summary>
我们提出了基于数据驱动的震动Synthesize模型，使用Generative Adversarial Neural Operator（GANO），结合了最新的机器学习技术和开放获取的强震数据集，以生成三Component加速度时间历史记录，受力级($M）、破裂距离($R_{rup}$)、顶部30米时间均辐射波速度($V_{S30}$)和地震环境或斜坡类型。我们使用Neural Operators，一种约束独立的建模架构，确保模型训练不受数据采样频率的影响。我们首先介绍了受控震动Synthesize算法（简称为cGM-GANO），并讨论了它与前一个作品的优点。然后，我们验证了cGM-GANO框架使用SCEC Broadband Platform（BBP）生成的模拟震动数据。最后，我们在日本KiK-net数据集上训练了cGM-GANO，并显示了其恢复了震动级、距离和$V_{S30}$的振荡尺度的能力。我们通过预测数据的差异分析和与传统地震模型（GMMs）的比较来评估cGM-GANO。结果表明，cGM-GANO在相应的地震环境中具有一致的中值振荡，但短距离处存在较大的差异。此外，cGM-GANO能够准确捕捉随机变量的响应特征ORD，特别是在沉入事件中，因为训练数据的充足。应用包括生成基于风险的地震动数据，用于特定工程应用。
</details></li>
</ul>
<hr>
<h2 id="Punctate-White-Matter-Lesion-Segmentation-in-Preterm-Infants-Powered-by-Counterfactually-Generative-Learning"><a href="#Punctate-White-Matter-Lesion-Segmentation-in-Preterm-Infants-Powered-by-Counterfactually-Generative-Learning" class="headerlink" title="Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning"></a>Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03440">http://arxiv.org/abs/2309.03440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehua Ren, Yongheng Sun, Miaomiao Wang, Yuying Feng, Xianjun Li, Chao Jin, Jian Yang, Chunfeng Lian, Fan Wang</li>
<li>for: 这个研究旨在提出一种可靠地自动分类潜在白质脑症（PWMLs）的方法，以便在诊断和治疗相关疗病的过程中优化诊断。</li>
<li>methods: 这个研究使用了对抗事实的思维和辅助任务的脑组织分类来学习细部位置信息和形态特征，以提高PWMLs的精确分类和定位。</li>
<li>results: 研究结果显示，这个方法可以在实际临床数据集上达到现有方法的州OF-THE-ART性能，并且提供了一个简单易用的深度学习框架（i.e., DeepPWML），可以轻松地应用于实际应用中。<details>
<summary>Abstract</summary>
Accurate segmentation of punctate white matter lesions (PWMLs) are fundamental for the timely diagnosis and treatment of related developmental disorders. Automated PWMLs segmentation from infant brain MR images is challenging, considering that the lesions are typically small and low-contrast, and the number of lesions may dramatically change across subjects. Existing learning-based methods directly apply general network architectures to this challenging task, which may fail to capture detailed positional information of PWMLs, potentially leading to severe under-segmentations. In this paper, we propose to leverage the idea of counterfactual reasoning coupled with the auxiliary task of brain tissue segmentation to learn fine-grained positional and morphological representations of PWMLs for accurate localization and segmentation. A simple and easy-to-implement deep-learning framework (i.e., DeepPWML) is accordingly designed. It combines the lesion counterfactual map with the tissue probability map to train a lightweight PWML segmentation network, demonstrating state-of-the-art performance on a real-clinical dataset of infant T1w MR images. The code is available at \href{https://github.com/ladderlab-xjtu/DeepPWML}{https://github.com/ladderlab-xjtu/DeepPWML}.
</details>
<details>
<summary>摘要</summary>
精准分割脑白 matter斑点病变 (PWMLs) 是诊断和治疗相关developmental disorders的基础。自动从 infant brain MR 图像中提取 PWMLs 的分割是一项挑战，因为这些斑点通常很小，对比度很低，并且每个主体中斑点的数量可能会很大。现有的学习基于方法直接将通用网络架构应用到这项任务上，可能会失去 PWMLs 的细致位坐信息，导致严重的下 segmentation。在这篇论文中，我们提出了利用对立想法 coupled with 脑组织 segmentation 作为 auxillary task，以学习 PWMLs 的细致位坐和形态表示。我们设计了一个简单易用的深度学习框架（i.e., DeepPWML），该框架结合 lesion counterfactual map 和 tissue probability map 来训练一个轻量级 PWML segmentation 网络，并在实际临床数据集上达到了 state-of-the-art 性能。代码可以在 \href{https://github.com/ladderlab-xjtu/DeepPWML}{https://github.com/ladderlab-xjtu/DeepPWML} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Tucker-Decomposition-Modeling-Commonality-and-Peculiarity-on-Tensor-Data"><a href="#Personalized-Tucker-Decomposition-Modeling-Commonality-and-Peculiarity-on-Tensor-Data" class="headerlink" title="Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data"></a>Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03439">http://arxiv.org/abs/2309.03439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuyun Hu, Naichen Shi, Raed Al Kontar, Hao Yan</li>
<li>for: address the limitations of traditional tensor decomposition methods in capturing heterogeneity across different datasets</li>
<li>methods: personalized Tucker decomposition (perTucker) that decomposes tensor data into shared global components and personalized local components, with a mode orthogonality assumption and a proximal gradient regularized block coordinate descent algorithm</li>
<li>results: effective in anomaly detection, client classification, and clustering through a simulation study and two case studies on solar flare detection and tonnage signal classification.Here is the summary in Traditional Chinese:</li>
<li>for: Addressing the limitations of traditional tensor decomposition methods in capturing heterogeneity across different datasets</li>
<li>methods: 个人化Tucker分解（perTucker），将tensor资料分解为共同全部 ком成分和个人化地方成分，并假设模式正交性和 proximal梯度调整Block coordinate descent algorithm</li>
<li>results: 实现异常检测、客户分类和集群，通过一个 simulations study和两个案例研究：太阳风检测和吨位信号分类。<details>
<summary>Abstract</summary>
We propose personalized Tucker decomposition (perTucker) to address the limitations of traditional tensor decomposition methods in capturing heterogeneity across different datasets. perTucker decomposes tensor data into shared global components and personalized local components. We introduce a mode orthogonality assumption and develop a proximal gradient regularized block coordinate descent algorithm that is guaranteed to converge to a stationary point. By learning unique and common representations across datasets, we demonstrate perTucker's effectiveness in anomaly detection, client classification, and clustering through a simulation study and two case studies on solar flare detection and tonnage signal classification.
</details>
<details>
<summary>摘要</summary>
我们提出个性化图ucker分解（perTucker）来解决传统张量分解方法不能捕捉不同数据集之间的多样性的限制。perTucker将张量数据分解成共享全局组件和个性化地方组件。我们提出了一种方向正交假设，并开发了一种距离正则化块坐标推移算法，这个算法能够保证收敛到站点点。通过学习不同数据集之间的共同和特有表示，我们示出perTucker在异常检测、客户分类和群集分类中的效果。通过一个 simulations 和两个实际案例（太阳闪光检测和吨位信号分类），我们证明了perTucker的有效性。
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Robust-Federated-Learning-with-Variance-Reduction-and-Differential-Privacy"><a href="#Byzantine-Robust-Federated-Learning-with-Variance-Reduction-and-Differential-Privacy" class="headerlink" title="Byzantine-Robust Federated Learning with Variance Reduction and Differential Privacy"></a>Byzantine-Robust Federated Learning with Variance Reduction and Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03437">http://arxiv.org/abs/2309.03437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikai Zhang, Rui Hu</li>
<li>for: 保护数据隐私和防止Byzantine攻击 during federated learning (FL) 训练过程，保持数据在客户端（即物联网设备）上，并且只在客户端上共享模型更新。</li>
<li>methods: 我们提出了一种新的 federated learning 方案，通过将采样和势量驱动的差异隐私 Mechanism 引入到客户端级别的隐私保证机制中，以防止Byzantine攻击。我们的安全设计不违反客户端级别的隐私保证机制，因此我们的方法可以保持同样的客户端级别隐私保证。</li>
<li>results: 我们在不同的IID和非IID数据集和不同的任务上进行了广泛的实验，并对不同的Byzantine攻击进行了比较。结果表明我们的框架可以提高系统的Robustness against Byzantine attacks，同时保持强的隐私保证。<details>
<summary>Abstract</summary>
Federated learning (FL) is designed to preserve data privacy during model training, where the data remains on the client side (i.e., IoT devices), and only model updates of clients are shared iteratively for collaborative learning. However, this process is vulnerable to privacy attacks and Byzantine attacks: the local model updates shared throughout the FL network will leak private information about the local training data, and they can also be maliciously crafted by Byzantine attackers to disturb the learning. In this paper, we propose a new FL scheme that guarantees rigorous privacy and simultaneously enhances system robustness against Byzantine attacks. Our approach introduces sparsification- and momentum-driven variance reduction into the client-level differential privacy (DP) mechanism, to defend against Byzantine attackers. The security design does not violate the privacy guarantee of the client-level DP mechanism; hence, our approach achieves the same client-level DP guarantee as the state-of-the-art. We conduct extensive experiments on both IID and non-IID datasets and different tasks and evaluate the performance of our approach against different Byzantine attacks by comparing it with state-of-the-art defense methods. The results of our experiments show the efficacy of our framework and demonstrate its ability to improve system robustness against Byzantine attacks while achieving a strong privacy guarantee.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Equal-Long-term-Benefit-Rate-Adapting-Static-Fairness-Notions-to-Sequential-Decision-Making"><a href="#Equal-Long-term-Benefit-Rate-Adapting-Static-Fairness-Notions-to-Sequential-Decision-Making" class="headerlink" title="Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making"></a>Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03426">http://arxiv.org/abs/2309.03426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuancheng-xu/elbert">https://github.com/yuancheng-xu/elbert</a></li>
<li>paper_authors: Yuancheng Xu, Chenghao Deng, Yanchao Sun, Ruijie Zheng, Xiyao Wang, Jieyu Zhao, Furong Huang</li>
<li>for: 这篇论文关注机器学习模型的决策对长期影响的公平性问题。</li>
<li>methods: 该论文使用Markov Decision Process（MDP）框架来解决长期公平性问题，并定义了长期偏见的概念。</li>
<li>results: 该论文提出了一种名为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，该概念考虑了不同时间步骤的重要性差异，并且可以使用标准政策优化方法来减少偏见。实验结果表明，ELBERT-PO方法可以有效地减少偏见并保持高效性。<details>
<summary>Abstract</summary>
Decisions made by machine learning models may have lasting impacts over time, making long-term fairness a crucial consideration. It has been shown that when ignoring the long-term effect, naively imposing fairness criterion in static settings can actually exacerbate bias over time. To explicitly address biases in sequential decision-making, recent works formulate long-term fairness notions in Markov Decision Process (MDP) framework. They define the long-term bias to be the sum of static bias over each time step. However, we demonstrate that naively summing up the step-wise bias can cause a false sense of fairness since it fails to consider the importance difference of different time steps during transition. In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. Moreover, we show that the policy gradient of Long-term Benefit Rate can be analytically reduced to standard policy gradient. This makes standard policy optimization methods applicable for reducing the bias, leading to our proposed bias mitigation method ELBERT-PO. Experiments on three sequential decision making environments show that ELBERT-PO significantly reduces bias and maintains high utility. Code is available at https://github.com/Yuancheng-Xu/ELBERT.
</details>
<details>
<summary>摘要</summary>
In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. We show that the policy gradient of Long-term Benefit Rate can be analytically reduced to standard policy gradient, making standard policy optimization methods applicable for reducing the bias. This leads to our proposed bias mitigation method ELBERT-PO.We evaluate ELBERT-PO on three sequential decision-making environments and show that it significantly reduces bias and maintains high utility. Our code is available at https://github.com/Yuancheng-Xu/ELBERT.Here's the Simplified Chinese translation:机器学习模型的决策可能会有长期影响，因此长期公平是一个非常重要的考虑因素。先前的工作已经证明了，在忽略长期影响的情况下，简单地强制执行公平准则可能会在时间流逝后恶化偏见。为了明确遗传偏见在序列决策中的问题， latest works 在 Markov Decision Process (MDP) 框架下形ulated long-term fairness notions。然而，这些不ions fail to consider the importance difference of different time steps during transitions, leading to a false sense of fairness。在这种情况下，我们引入了 Equal Long-term Benefit Rate (ELBERT)，一种考虑时间重要性的变化的长期公平原则。我们显示了 Long-term Benefit Rate 的政策梯度可以被分析式地减少到标准政策梯度，使得标准政策优化方法可以用来减少偏见。这导致我们的偏见减少方法 ELBERT-PO。我们在三个序列决策环境中评估 ELBERT-PO，并发现它可以减少偏见，同时保持高的用户。我们的代码可以在 https://github.com/Yuancheng-Xu/ELBERT 上获取。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-as-Optimizers"><a href="#Large-Language-Models-as-Optimizers" class="headerlink" title="Large Language Models as Optimizers"></a>Large Language Models as Optimizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03409">http://arxiv.org/abs/2309.03409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen</li>
<li>for: 用于解决各种实际应用中缺乏导数的优化问题。</li>
<li>methods: 利用大型自然语言模型（LLMs）作为优化器，通过自然语言描述优化任务来生成新的解决方案。</li>
<li>results: 比较human-设计的提示，使用OPRO优化的提示在GSM8K和Big-Bench Hard任务上的性能提高至多达8%和50%。<details>
<summary>Abstract</summary>
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
</details>
<details>
<summary>摘要</summary>
优化是 ubique 存在的。偏导函数基本算法在各种问题上表现出了强大的工具性，但在许多实际应用中，缺乏梯度带来了挑战。在这项工作中，我们提出了一种简单有效的方法，即优化 by PROmpting (OPRO)，它利用大型自然语言模型 (LLMs) 作为优化器，并将优化任务描述为自然语言中的提示。在每次优化步骤中，LLM 生成新的解决方案，这些解决方案基于之前生成的解决方案和其值，然后评估这些新的解决方案，并将其添加到下一个优化步骤中。我们首先在线性回归和旅行商问题上应用 OPRO，然后扩展到提示优化，即找到最佳提示以最大化任务准确率。通过各种 LLMS，我们示示了最佳提示由 OPRO 优化的比人设计提示高达 8% 的 GSM8K 和高达 50% 的 Big-Bench Hard 任务。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/07/cs.LG_2023_09_07/" data-id="clmjn91my00850j88fncq1d7h" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/07/eess.IV_2023_09_07/" class="article-date">
  <time datetime="2023-09-07T09:00:00.000Z" itemprop="datePublished">2023-09-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/07/eess.IV_2023_09_07/">eess.IV - 2023-09-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Eval-A-Benchmark-for-Cross-Dataset-Evaluation-of-Abdominal-Multi-Organ-Segmentation"><a href="#A-Eval-A-Benchmark-for-Cross-Dataset-Evaluation-of-Abdominal-Multi-Organ-Segmentation" class="headerlink" title="A-Eval: A Benchmark for Cross-Dataset Evaluation of Abdominal Multi-Organ Segmentation"></a>A-Eval: A Benchmark for Cross-Dataset Evaluation of Abdominal Multi-Organ Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03906">http://arxiv.org/abs/2309.03906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uni-medical/a-eval">https://github.com/uni-medical/a-eval</a></li>
<li>paper_authors: Ziyan Huang, Zhongying Deng, Jin Ye, Haoyu Wang, Yanzhou Su, Tianbin Li, Hui Sun, Junlong Cheng, Jianpin Chen, Junjun He, Yun Gu, Shaoting Zhang, Lixu Gu, Yu Qiao</li>
<li>for: 本研究的目的是提供一个跨 dataset 的评估指标（A-Eval），用于评估多个腹部多器官分类模型的扩展性。</li>
<li>methods: 本研究使用了四个大规模公共数据集：FLARE22、AMOS、WORD 和 TotalSegmentator，各自提供了广泛的腹部多器官分类标签。实验使用了这些数据集的训练集和验证集，以建立一个多样化的 benchmark。</li>
<li>results: 研究发现，模型可以通过使用不同的数据集和不同的训练策略来提高扩展性。另外，模型的大小也影响了跨 dataset 的扩展性。通过这些分析，研究强调了训练数据集的多样化和模型的设计在提高模型扩展性方面的重要性。<details>
<summary>Abstract</summary>
Although deep learning have revolutionized abdominal multi-organ segmentation, models often struggle with generalization due to training on small, specific datasets. With the recent emergence of large-scale datasets, some important questions arise: \textbf{Can models trained on these datasets generalize well on different ones? If yes/no, how to further improve their generalizability?} To address these questions, we introduce A-Eval, a benchmark for the cross-dataset Evaluation ('Eval') of Abdominal ('A') multi-organ segmentation. We employ training sets from four large-scale public datasets: FLARE22, AMOS, WORD, and TotalSegmentator, each providing extensive labels for abdominal multi-organ segmentation. For evaluation, we incorporate the validation sets from these datasets along with the training set from the BTCV dataset, forming a robust benchmark comprising five distinct datasets. We evaluate the generalizability of various models using the A-Eval benchmark, with a focus on diverse data usage scenarios: training on individual datasets independently, utilizing unlabeled data via pseudo-labeling, mixing different modalities, and joint training across all available datasets. Additionally, we explore the impact of model sizes on cross-dataset generalizability. Through these analyses, we underline the importance of effective data usage in enhancing models' generalization capabilities, offering valuable insights for assembling large-scale datasets and improving training strategies. The code and pre-trained models are available at \href{https://github.com/uni-medical/A-Eval}{https://github.com/uni-medical/A-Eval}.
</details>
<details>
<summary>摘要</summary>
although deep learning have revolutionized 腹部多器官分割，模型经常受到泛化问题的影响，即在不同的数据集上进行训练后，模型是否能够具有良好的泛化能力？如果可以，那么如何进一步提高其泛化能力？为了回答这些问题，我们提出了A-Eval，一个跨数据集评估（Eval）的权威指标集，用于评估腹部多器官分割模型的泛化能力。我们在四个大规模公共数据集上进行了训练：FLARE22、AMOS、WORD和TotalSegmentator，每个数据集都提供了详细的腹部多器官分割标签。为了评估，我们将这些数据集的验证集和BTCV数据集的训练集结合起来，组成一个robust的指标集，包括五个不同的数据集。我们使用这个指标集来评估不同模型的泛化能力，强调多样数据使用方案的影响，包括单独训练每个数据集、使用无标签数据 Pseudo-labeling、混合不同模式和共同训练所有可用数据集。此外，我们还研究模型大小对跨数据集泛化能力的影响。通过这些分析，我们强调了有效地使用数据的重要性，并提供了值得关注的数据准备和训练策略。代码和预训练模型可以在 <https://github.com/uni-medical/A-Eval> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Continuous-Exposure-Value-Representations-for-Single-Image-HDR-Reconstruction"><a href="#Learning-Continuous-Exposure-Value-Representations-for-Single-Image-HDR-Reconstruction" class="headerlink" title="Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction"></a>Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03900">http://arxiv.org/abs/2309.03900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Su-Kai Chen, Hung-Lin Yen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Wen-Hsiao Peng, Yen-Yu Lin</li>
<li>for: 这个论文的目的是为了提高单张图像的HDR重建。</li>
<li>methods: 这个论文使用了深度学习来生成LDR堆栈，并使用了一个隐式函数来生成LDR图像的不同曝光值（EV）。</li>
<li>results: 该方法可以生成包含多个不同EV的连续的LDR堆栈，从而提高HDR重建质量。对比 existed方法，我们的CEVR模型表现更优。<details>
<summary>Abstract</summary>
Deep learning is commonly used to reconstruct HDR images from LDR images. LDR stack-based methods are used for single-image HDR reconstruction, generating an HDR image from a deep learning-generated LDR stack. However, current methods generate the stack with predetermined exposure values (EVs), which may limit the quality of HDR reconstruction. To address this, we propose the continuous exposure value representation (CEVR), which uses an implicit function to generate LDR images with arbitrary EVs, including those unseen during training. Our approach generates a continuous stack with more images containing diverse EVs, significantly improving HDR reconstruction. We use a cycle training strategy to supervise the model in generating continuous EV LDR images without corresponding ground truths. Our CEVR model outperforms existing methods, as demonstrated by experimental results.
</details>
<details>
<summary>摘要</summary>
深度学习通常用于从LDR图像中重建HDR图像。现有的LDR堆栈基本方法用于单个图像HDR重建，通过深度学习生成的LDR堆栈来生成HDR图像。然而，现有方法通常使用预先确定的曝光值（EV）来生成堆栈，这可能会限制HDR重建质量。为解决这个问题，我们提出了连续曝光值表示（CEVR），它使用隐式函数来生成具有任意EV的LDR图像，包括训练过程中未经见到的EV。我们的方法生成了更多包含多样EV的LDR图像，Significantly Improving HDR重建。我们使用循环训练策略来监督模型在生成连续EV LDR图像时，无需对应的真实参考图像。我们的CEVR模型在实验结果中胜过现有方法。
</details></li>
</ul>
<hr>
<h2 id="T2IW-Joint-Text-to-Image-Watermark-Generation"><a href="#T2IW-Joint-Text-to-Image-Watermark-Generation" class="headerlink" title="T2IW: Joint Text to Image &amp; Watermark Generation"></a>T2IW: Joint Text to Image &amp; Watermark Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03815">http://arxiv.org/abs/2309.03815</a></li>
<li>repo_url: None</li>
<li>paper_authors: An-An Liu, Guokai Zhang, Yuting Su, Ning Xu, Yongdong Zhang, Lanjun Wang</li>
<li>for: 本研究旨在提出一种新的文本条件下的图像生成模型，以提高图像生成的真实性。</li>
<li>methods: 本研究使用了一种新的文本和水印（T2IW）任务，以确保图像质量受到最小影响，同时保证水印信息的可读性。</li>
<li>results: 实验结果表明，本方法可以在不同的后处理攻击下保持水印信息的透明度和可靠性，同时图像质量受到最小影响。<details>
<summary>Abstract</summary>
Recent developments in text-conditioned image generative models have revolutionized the production of realistic results. Unfortunately, this has also led to an increase in privacy violations and the spread of false information, which requires the need for traceability, privacy protection, and other security measures. However, existing text-to-image paradigms lack the technical capabilities to link traceable messages with image generation. In this study, we introduce a novel task for the joint generation of text to image and watermark (T2IW). This T2IW scheme ensures minimal damage to image quality when generating a compound image by forcing the semantic feature and the watermark signal to be compatible in pixels. Additionally, by utilizing principles from Shannon information theory and non-cooperative game theory, we are able to separate the revealed image and the revealed watermark from the compound image. Furthermore, we strengthen the watermark robustness of our approach by subjecting the compound image to various post-processing attacks, with minimal pixel distortion observed in the revealed watermark. Extensive experiments have demonstrated remarkable achievements in image quality, watermark invisibility, and watermark robustness, supported by our proposed set of evaluation metrics.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:近期的文本受限画像生成模型的发展，已经对生成真实的结果产生了革命性的变革。然而，这也导致了隐私侵犯和虚假信息的扩散，需要跟踪性、隐私保护和其他安全措施。然而，现有的文本到图像的观念没有技术能力将可追溯的消息与图像生成连接起来。在本研究中，我们提出了一种新的文本到图像和水印（T2IW）任务，以保证在生成复杂图像时，semantic feature和水印信号在像素级别上具有可 compatibles性。此外，我们利用了信息理论和非合作游戏理论，将复杂图像中的 revelaed image 和 revelaed watermark 分离开来。此外，我们通过对复杂图像进行多种后处理攻击，使得 revelaed watermark 中的像素扰动很小。广泛的实验结果表明，我们的方法在图像质量、水印隐身和水印Robustness 等方面具有杰出的成果，支持我们提出的评价指标集。
</details></li>
</ul>
<hr>
<h2 id="Label-efficient-Contrastive-Learning-based-model-for-nuclei-detection-and-classification-in-3D-Cardiovascular-Immunofluorescent-Images"><a href="#Label-efficient-Contrastive-Learning-based-model-for-nuclei-detection-and-classification-in-3D-Cardiovascular-Immunofluorescent-Images" class="headerlink" title="Label-efficient Contrastive Learning-based model for nuclei detection and classification in 3D Cardiovascular Immunofluorescent Images"></a>Label-efficient Contrastive Learning-based model for nuclei detection and classification in 3D Cardiovascular Immunofluorescent Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03744">http://arxiv.org/abs/2309.03744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazanin Moradinasab, Rebecca A. Deaton, Laura S. Shankman, Gary K. Owens, Donald E. Brown<br>for:这个论文旨在提出一个 Label-efficient Contrastive learning-based (LECL) 模型，用于检测和类别各种类型的胞况在 3D 免疫染色图像中。methods:我们提出了一个 Extended Maximum Intensity Projection (EMIP) 方法来解决将多个层级萤幕转换为 2D 图像所带来的问题，以及一个 Supervised Contrastive Learning (SCL) 方法来进行弱监督学习设定。results:我们在心血管 dataset 上进行了实验，结果显示我们的提案的框架具有效果和效率地检测和类别各种类型的胞况在 3D 免疫染色图像中。<details>
<summary>Abstract</summary>
Recently, deep learning-based methods achieved promising performance in nuclei detection and classification applications. However, training deep learning-based methods requires a large amount of pixel-wise annotated data, which is time-consuming and labor-intensive, especially in 3D images. An alternative approach is to adapt weak-annotation methods, such as labeling each nucleus with a point, but this method does not extend from 2D histopathology images (for which it was originally developed) to 3D immunofluorescent images. The reason is that 3D images contain multiple channels (z-axis) for nuclei and different markers separately, which makes training using point annotations difficult. To address this challenge, we propose the Label-efficient Contrastive learning-based (LECL) model to detect and classify various types of nuclei in 3D immunofluorescent images. Previous methods use Maximum Intensity Projection (MIP) to convert immunofluorescent images with multiple slices to 2D images, which can cause signals from different z-stacks to falsely appear associated with each other. To overcome this, we devised an Extended Maximum Intensity Projection (EMIP) approach that addresses issues using MIP. Furthermore, we performed a Supervised Contrastive Learning (SCL) approach for weakly supervised settings. We conducted experiments on cardiovascular datasets and found that our proposed framework is effective and efficient in detecting and classifying various types of nuclei in 3D immunofluorescent images.
</details>
<details>
<summary>摘要</summary>
Previous methods use Maximum Intensity Projection (MIP) to convert immunofluorescent images with multiple slices to 2D images, which can cause signals from different z-stacks to falsely appear associated with each other. To overcome this, we devised an Extended Maximum Intensity Projection (EMIP) approach that addresses issues using MIP. Furthermore, we performed a Supervised Contrastive Learning (SCL) approach for weakly supervised settings. We conducted experiments on cardiovascular datasets and found that our proposed framework is effective and efficient in detecting and classifying various types of nuclei in 3D immunofluorescent images.Translation note:* "nuclei" is translated as "核体" (hépán) in Simplified Chinese.* "deep learning-based methods" is translated as "深度学习方法" (shēngrán xuéxí fāngchéng) in Simplified Chinese.* "pixel-wise annotated data" is translated as "像素级标注数据" (xiàngxī jí biāo xiǎngxī) in Simplified Chinese.* "2D histopathology images" is translated as "2D histopathology图像" (2D histopathology túxiàng) in Simplified Chinese.* "3D immunofluorescent images" is translated as "3D免疫染色图像" (3D mǐngyì zhèngsè túxiàng) in Simplified Chinese.* "z-axis" is translated as "z轴" (z jía) in Simplified Chinese.* "Maximum Intensity Projection" is translated as "最大强度投影" (zuìdà qiángdàng tóuè) in Simplified Chinese.* "Extended Maximum Intensity Projection" is translated as "扩展最大强度投影" (kuòxiān zuìdà qiángdàng tóuè) in Simplified Chinese.* "Supervised Contrastive Learning" is translated as "有监督的对比学习" (yǒu jiāndū de duìbǐ xuéxí) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="MS-UNet-v2-Adaptive-Denoising-Method-and-Training-Strategy-for-Medical-Image-Segmentation-with-Small-Training-Data"><a href="#MS-UNet-v2-Adaptive-Denoising-Method-and-Training-Strategy-for-Medical-Image-Segmentation-with-Small-Training-Data" class="headerlink" title="MS-UNet-v2: Adaptive Denoising Method and Training Strategy for Medical Image Segmentation with Small Training Data"></a>MS-UNet-v2: Adaptive Denoising Method and Training Strategy for Medical Image Segmentation with Small Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03686">http://arxiv.org/abs/2309.03686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyuan Chen, Yufei Han, Pin Xu, Yanyi Li, Kuan Li, Jianping Yin</li>
<li>for: 这个研究是为了提高医疗影像分类 tasks 的表现，并且解决单层 U-Net 构造不够强大的问题，以及当疗病数据量小时的问题。</li>
<li>methods: 我们提出了一个名为 MS-UNet 的新型 U-Net 模型，它使用了多层嵌入式 decoder 结构，具有更好的Semantic feature mapping 能力，从而帮助网络学习更多详细的特征。此外，我们还提出了一个新的边缘损失和一个可插入的精细化 Denoising 模组，这些模组可以对 MS-UNet 进行改进，并且可以个别应用于其他模型中。</li>
<li>results: 实验结果显示，MS-UNet 可以对医疗影像分类 tasks 进行更好的表现，特别是当疗病数据量小时，并且可以对网络进行更高效的特征学习。此外，我们提出的边缘损失和 Denoising 模组可以对 MS-UNet 进行明显改进。<details>
<summary>Abstract</summary>
Models based on U-like structures have improved the performance of medical image segmentation. However, the single-layer decoder structure of U-Net is too "thin" to exploit enough information, resulting in large semantic differences between the encoder and decoder parts. Things get worse if the number of training sets of data is not sufficiently large, which is common in medical image processing tasks where annotated data are more difficult to obtain than other tasks. Based on this observation, we propose a novel U-Net model named MS-UNet for the medical image segmentation task in this study. Instead of the single-layer U-Net decoder structure used in Swin-UNet and TransUnet, we specifically design a multi-scale nested decoder based on the Swin Transformer for U-Net. The proposed multi-scale nested decoder structure allows the feature mapping between the decoder and encoder to be semantically closer, thus enabling the network to learn more detailed features. In addition, we propose a novel edge loss and a plug-and-play fine-tuning Denoising module, which not only effectively improves the segmentation performance of MS-UNet, but could also be applied to other models individually. Experimental results show that MS-UNet could effectively improve the network performance with more efficient feature learning capability and exhibit more advanced performance, especially in the extreme case with a small amount of training data, and the proposed Edge loss and Denoising module could significantly enhance the segmentation performance of MS-UNet.
</details>
<details>
<summary>摘要</summary>
模型基于U字结构已经提高医学影像分割的性能。然而，单层decoder结构的U字网是“薄”到足够利用信息，导致encoder和decoder部分之间的semantic diferencia较大，这会在医学影像处理任务中，where annotated data更加困难获得，使得问题更加严重。为了解决这个问题，我们在本研究中提出了一种名为MS-UNet的新的U字网模型。而不是Swim Transformer和TransUnet中使用的单层U字网decoder结构，我们专门设计了一种基于Swin Transformer的多层嵌套decoder结构。这种多层嵌套decoder结构使得feature mapping междуdecoder和encoder更加接近，因此使得网络能够学习更多的细节特征。此外，我们还提出了一种新的边缘损失和可重复使用的精度调整Denosing模块，这些模块不仅可以有效地提高MS-UNet的分割性能，还可以应用于其他模型。实验结果表明，MS-UNet可以有效地提高网络性能，并且在小量训练数据情况下表现更加出色，而提出的边缘损失和Denosing模块也可以显著提高MS-UNet的分割性能。
</details></li>
</ul>
<hr>
<h2 id="Anatomy-informed-Data-Augmentation-for-Enhanced-Prostate-Cancer-Detection"><a href="#Anatomy-informed-Data-Augmentation-for-Enhanced-Prostate-Cancer-Detection" class="headerlink" title="Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection"></a>Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03652">http://arxiv.org/abs/2309.03652</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mic-dkfz/anatomy_informed_da">https://github.com/mic-dkfz/anatomy_informed_da</a></li>
<li>paper_authors: Balint Kovacs, Nils Netzer, Michael Baumgartner, Carolin Eith, Dimitrios Bounias, Clara Meinzer, Paul F. Jaeger, Kevin S. Zhang, Ralf Floca, Adrian Schrader, Fabian Isensee, Regula Gnirs, Magdalena Goertz, Viktoria Schuetz, Albrecht Stenzinger, Markus Hohenfellner, Heinz-Peter Schlemmer, Ivo Wolf, David Bonekamp, Klaus H. Maier-Hein</li>
<li>for: 这篇论文主要针对医疗影像分析中的肿瘤检测，特别是肺癌检测。</li>
<li>methods: 这篇论文提出了一种新的生物学参考变数，它利用邻近器官的信息来模拟典型的生物Physiological deformations of the prostate, and generates unique lesion shapes without altering their label.</li>
<li>results: 这篇论文透过实验证明了这种增强器的有效性，并且显示了它可以轻松地整合到常用的增强框架中。<details>
<summary>Abstract</summary>
Data augmentation (DA) is a key factor in medical image analysis, such as in prostate cancer (PCa) detection on magnetic resonance images. State-of-the-art computer-aided diagnosis systems still rely on simplistic spatial transformations to preserve the pathological label post transformation. However, such augmentations do not substantially increase the organ as well as tumor shape variability in the training set, limiting the model's ability to generalize to unseen cases with more diverse localized soft-tissue deformations. We propose a new anatomy-informed transformation that leverages information from adjacent organs to simulate typical physiological deformations of the prostate and generates unique lesion shapes without altering their label. Due to its lightweight computational requirements, it can be easily integrated into common DA frameworks. We demonstrate the effectiveness of our augmentation on a dataset of 774 biopsy-confirmed examinations, by evaluating a state-of-the-art method for PCa detection with different augmentation settings.
</details>
<details>
<summary>摘要</summary>
增强数据 (DA) 是医学图像分析中关键因素，例如肠癌检测在核磁共振图像中。现有的计算机辅助诊断系统仍然仅使用简单的空间变换来保持疾病标签后变换。然而，这些扩展不会显著增加器官以及肿瘤形态多样性在训练集中，限制模型对未经见的案例中更多的地方软组织弯曲的适应能力。我们提议一种新的生物学信息指导的变换，利用邻近器官信息来模拟Typical的生理弯曲，生成唯一的癌症形态，无需改变其标签。由于其轻量级计算需求，它可以轻松地integrated into common DA frameworks。我们在774个采样中证明了我们的扩展的效果，通过评估一种state-of-the-art方法 для PCa检测不同的扩展设置。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-3D-Object-Localization-from-Single-Calibrated-Images-A-Study-of-Basketballs"><a href="#Context-Aware-3D-Object-Localization-from-Single-Calibrated-Images-A-Study-of-Basketballs" class="headerlink" title="Context-Aware 3D Object Localization from Single Calibrated Images: A Study of Basketballs"></a>Context-Aware 3D Object Localization from Single Calibrated Images: A Study of Basketballs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03640">http://arxiv.org/abs/2309.03640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabriel-vanzandycke/deepsport">https://github.com/gabriel-vanzandycke/deepsport</a></li>
<li>paper_authors: Marcello Davide Caio, Gabriel Van Zandycke, Christophe De Vleeschouwer</li>
<li>for: 这个论文的目的是提出一种基于单个滤波图像的三维篮球定位方法。</li>
<li>methods: 该方法使用图像自身和物体位置作为输入，通过计算物体 projection onto the ground plane within the image 来预测物体的高度在图像空间中。然后，通过利用知道的投影矩阵，重建物体的3D坐标。</li>
<li>results: 对于DeepSport dataset的实验，该方法表现出了明显的准确性提升，比前方工作更加有效。这些结果开启了更好的篮球跟踪和理解的可能性，推动计算机视觉在多个领域的进步。<details>
<summary>Abstract</summary>
Accurately localizing objects in three dimensions (3D) is crucial for various computer vision applications, such as robotics, autonomous driving, and augmented reality. This task finds another important application in sports analytics and, in this work, we present a novel method for 3D basketball localization from a single calibrated image. Our approach predicts the object's height in pixels in image space by estimating its projection onto the ground plane within the image, leveraging the image itself and the object's location as inputs. The 3D coordinates of the ball are then reconstructed by exploiting the known projection matrix. Extensive experiments on the public DeepSport dataset, which provides ground truth annotations for 3D ball location alongside camera calibration information for each image, demonstrate the effectiveness of our method, offering substantial accuracy improvements compared to recent work. Our work opens up new possibilities for enhanced ball tracking and understanding, advancing computer vision in diverse domains. The source code of this work is made publicly available at \url{https://github.com/gabriel-vanzandycke/deepsport}.
</details>
<details>
<summary>摘要</summary>
“三维空间中的物件精确位置化（3D）是许多计算机视觉应用中的关键，如 робо械、自动驾驶和增强现实。在这个工作中，我们提出了一种新的方法，用于从单一测量过的图像中精确地推断篮球的3D位置。我们的方法利用图像中的物件位置和图像自身作为输入，预测物件的高度在图像空间中的像素数据，并利用知道的投影矩阵从图像中重建3D坐标。我们在公共的DeepSport dataset上进行了广泛的实验，该dataset提供了这些图像的摄取条件和摄像机协调信息，以及3D篮球位置的真实标注。我们的方法与最近的工作相比，具有了优秀的准确性。我们的工作开启了新的可能性，将计算机视觉应用于多元领域进行进一步发展。我们的源代码可以在 \url{https://github.com/gabriel-vanzandycke/deepsport} 上获取。”
</details></li>
</ul>
<hr>
<h2 id="Spatial-encoding-of-BOLD-fMRI-time-series-for-categorizing-static-images-across-visual-datasets-A-pilot-study-on-human-vision"><a href="#Spatial-encoding-of-BOLD-fMRI-time-series-for-categorizing-static-images-across-visual-datasets-A-pilot-study-on-human-vision" class="headerlink" title="Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision"></a>Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03590">http://arxiv.org/abs/2309.03590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vamshi K. Kancharala, Debanjali Bhattacharya, Neelam Sinha</li>
<li>for: 这项研究使用功能磁共振成像(fMRI)技术来研究大脑的功能，具体来说是通过检测血液含氧量变化来探测大脑的活动。</li>
<li>methods: 这项研究使用了BOLD5000数据集，这是一个公共可用的fMRI扫描数据集，包含了5254个多种类别的图像，来研究不同类别的图像如何影响大脑的活动。为了更好地理解视觉，研究人员使用了古拉姆angular field（GAF）和马可夫过渡场（MTF）来获得2D BOLDTS，然后使用了分类NN来进行分类。</li>
<li>results: 研究人员发现，并行的CNN模型在多类分类任务中表现出色，比其他网络模型提高了7%的性能。这个结果表明了并行的CNN模型在研究大脑如何处理不同类型的图像时的优势。<details>
<summary>Abstract</summary>
Functional MRI (fMRI) is widely used to examine brain functionality by detecting alteration in oxygenated blood flow that arises with brain activity. In this study, complexity specific image categorization across different visual datasets is performed using fMRI time series (TS) to understand differences in neuronal activities related to vision. Publicly available BOLD5000 dataset is used for this purpose, containing fMRI scans while viewing 5254 images of diverse categories, drawn from three standard computer vision datasets: COCO, ImageNet and SUN. To understand vision, it is important to study how brain functions while looking at different images. To achieve this, spatial encoding of fMRI BOLD TS has been performed that uses classical Gramian Angular Field (GAF) and Markov Transition Field (MTF) to obtain 2D BOLD TS, representing images of COCO, Imagenet and SUN. For classification, individual GAF and MTF features are fed into regular CNN. Subsequently, parallel CNN model is employed that uses combined 2D features for classifying images across COCO, Imagenet and SUN. The result of 2D CNN models is also compared with 1D LSTM and Bi-LSTM that utilizes raw fMRI BOLD signal for classification. It is seen that parallel CNN model outperforms other network models with an improvement of 7% for multi-class classification. Clinical relevance- The obtained result of this analysis establishes a baseline in studying how differently human brain functions while looking at images of diverse complexities.
</details>
<details>
<summary>摘要</summary>
Functional MRI (fMRI) 广泛用于检测脑功能的变化，特别是通过检测脑活动时血液氧化程度的变化。在这个研究中，使用 fMRI 时间序列（TS）来分类不同的视觉数据集，以了解视觉相关的神经活动之间的差异。使用公共可用的 BOLD5000 数据集，包含 fMRI 扫描视看 5254 个多种类别的图像，这些图像来自三个标准计算机视觉数据集：COCO、ImageNet 和 SUN。为了理解视觉，需要研究脑如何在不同的图像上工作。为此，使用类别 Gramian Angular Field (GAF) 和 Markov Transition Field (MTF) 来获得 2D BOLD TS，表示 COCO、ImageNet 和 SUN 三个数据集中的图像。然后，使用单独的 GAF 和 MTF 特征进行分类，并使用组合的 2D 特征来分类图像。结果显示，并行 CNN 模型在多类分类中表现出色，提高了7%的性能。临床 relevance - 这个分析结果建立了研究人类脑如何在不同复杂度的图像上工作的基线。
</details></li>
</ul>
<hr>
<h2 id="Secure-Control-of-Networked-Inverted-Pendulum-Visual-Servo-System-with-Adverse-Effects-of-Image-Computation-Extended-Version"><a href="#Secure-Control-of-Networked-Inverted-Pendulum-Visual-Servo-System-with-Adverse-Effects-of-Image-Computation-Extended-Version" class="headerlink" title="Secure Control of Networked Inverted Pendulum Visual Servo System with Adverse Effects of Image Computation (Extended Version)"></a>Secure Control of Networked Inverted Pendulum Visual Servo System with Adverse Effects of Image Computation (Extended Version)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03556">http://arxiv.org/abs/2309.03556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dajun Du, Changda Zhang, Qianjiang Lu, Minrui Fei, Huiyu Zhou</li>
<li>for: 本研究探讨了在通信网络上传输视频信息时可能会遇到的图像攻击问题，以及这些攻击对系统性能的影响。</li>
<li>methods: 本研究提出了一种基于快速缩放选择性图像加密（F2SIE）算法的新的网络倾斜镜视 servo系统（NIPVSS），该系统不仅可以保证实时性，还可以提高安全性。</li>
<li>results: 实验结果表明，提出的新的NIPVSS方法可以有效地防止图像攻击，并且可以保证系统的稳定性。<details>
<summary>Abstract</summary>
When visual image information is transmitted via communication networks, it easily suffers from image attacks, leading to system performance degradation or even crash. This paper investigates secure control of networked inverted pendulum visual servo system (NIPVSS) with adverse effects of image computation. Firstly, the image security limitation of the traditional NIPVSS is revealed, where its stability will be destroyed by eavesdropping-based image attacks. Then, a new NIPVSS with the fast scaled-selective image encryption (F2SIE) algorithm is proposed, which not only meets the real-time requirement by reducing the computational complexity, but also improve the security by reducing the probability of valuable information being compromised by eavesdropping-based image attacks. Secondly, adverse effects of the F2SIE algorithm and image attacks are analysed, which will produce extra computational delay and errors. Then, a closed-loop uncertain time-delay model of the new NIPVSS is established, and a robust controller is designed to guarantee system asymptotic stability. Finally, experimental results of the new NIPVSS demonstrate the feasibility and effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
当视觉图像信息通过通信网络传输时，容易受到图像攻击，导致系统性能下降或even crash。本文研究了安全控制的网络 inverted pendulum visual servo系统（NIPVSS），并对图像计算所造成的不良影响进行分析。首先，传统NIPVSS的图像安全限制被揭示，其稳定性将被侵犯者发送的图像攻击所 destrucción。然后，一种新的NIPVSS，使用快速缩放选择性图像加密算法（F2SIE），不仅可以实现实时要求，还可以提高安全性，减少侵犯者通过图像攻击获得有价值信息的概率。其次，F2SIE算法和图像攻击的副作用被分析，它们会生成额外的计算延迟和错误。然后，一个closed-loop不确定时延模型的新NIPVSS被建立，并设计了一个Robust控制器，以保证系统的极限稳定性。最后，新NIPVSS的实验结果证明了提案的方法的可行性和有效性。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Deep-Learning-based-Melanoma-Classification-using-Immunohistochemistry-and-Routine-Histology-A-Three-Center-Study"><a href="#Evaluating-Deep-Learning-based-Melanoma-Classification-using-Immunohistochemistry-and-Routine-Histology-A-Three-Center-Study" class="headerlink" title="Evaluating Deep Learning-based Melanoma Classification using Immunohistochemistry and Routine Histology: A Three Center Study"></a>Evaluating Deep Learning-based Melanoma Classification using Immunohistochemistry and Routine Histology: A Three Center Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03494">http://arxiv.org/abs/2309.03494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Wies, Lucas Schneider, Sarah Haggenmueller, Tabea-Clara Bucher, Sarah Hobelsberger, Markus V. Heppt, Gerardo Ferrara, Eva I. Krieghoff-Henning, Titus J. Brinker</li>
<li>for: 用于提高诊断皮肤癌的准确性</li>
<li>methods: 使用 Deep Learning (DL) 支持系统自动检查组织结构和细胞组成</li>
<li>results: DL 基于 MelanA 的助手系统与标准 H&amp;E 染色 Slides 的性能相同，可能通过多种染色类型的协同分析提高病理医生的诊断效果。<details>
<summary>Abstract</summary>
Pathologists routinely use immunohistochemical (IHC)-stained tissue slides against MelanA in addition to hematoxylin and eosin (H&E)-stained slides to improve their accuracy in diagnosing melanomas. The use of diagnostic Deep Learning (DL)-based support systems for automated examination of tissue morphology and cellular composition has been well studied in standard H&E-stained tissue slides. In contrast, there are few studies that analyze IHC slides using DL. Therefore, we investigated the separate and joint performance of ResNets trained on MelanA and corresponding H&E-stained slides. The MelanA classifier achieved an area under receiver operating characteristics curve (AUROC) of 0.82 and 0.74 on out of distribution (OOD)-datasets, similar to the H&E-based benchmark classification of 0.81 and 0.75, respectively. A combined classifier using MelanA and H&E achieved AUROCs of 0.85 and 0.81 on the OOD datasets. DL MelanA-based assistance systems show the same performance as the benchmark H&E classification and may be improved by multi stain classification to assist pathologists in their clinical routine.
</details>
<details>
<summary>摘要</summary>
PATHOLOGISTS 常用免疫 histochemical（IHC）染色的组织标本与 MelanA 标本进行诊断，以提高诊断皮肤癌的精度。使用基于 Deep Learning（DL）的诊断支持系统来自动检查组织结构和细胞成分已经得到了广泛的研究，但是对 IHC 标本的分析却有少量的研究。因此，我们调查了使用 ResNet 在 MelanA 和相应的 H&E 标本上训练的表现。MelanA 分类器在 OOD 数据集上的地区下Receiver Operating Characteristics Curve（AUROC）为 0.82 和 0.74，与 H&E 基准分类的 0.81 和 0.75 相似。具有 MelanA 和 H&E 的共同分类器在 OOD 数据集上的 AUROC 为 0.85 和 0.81。DL MelanA 基础的辅助系统显示和 H&E 基准分类的相同表现，并且可能会通过多标本分类来帮助病理学家在临床 Routine 中。
</details></li>
</ul>
<hr>
<h2 id="SAM3D-Segment-Anything-Model-in-Volumetric-Medical-Images"><a href="#SAM3D-Segment-Anything-Model-in-Volumetric-Medical-Images" class="headerlink" title="SAM3D: Segment Anything Model in Volumetric Medical Images"></a>SAM3D: Segment Anything Model in Volumetric Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03493">http://arxiv.org/abs/2309.03493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nhat-Tan Bui, Dinh-Hieu Hoang, Minh-Triet Tran, Ngan Le</li>
<li>for: 本研究旨在提出一种基于深度学习的自动图像分割方法，用于医疗图像分析中的图像分割任务。</li>
<li>methods: 该方法基于Segment Anything Model（SAM），并使用SAM encoder提取输入图像的有用表示。与其他现有的SAM-基于三维图像分割方法不同，该方法不需要将输入图像分割成一系列的2D slice，而是直接处理整个3D图像。</li>
<li>results: 经过广泛的实验，该方法在多个医疗图像Dataset上达到了与其他现有方法相当的竞争性result，而且在参数量方面更是有效率。<details>
<summary>Abstract</summary>
Image segmentation is a critical task in medical image analysis, providing valuable information that helps to make an accurate diagnosis. In recent years, deep learning-based automatic image segmentation methods have achieved outstanding results in medical images. In this paper, inspired by the Segment Anything Model (SAM), a foundation model that has received much attention for its impressive accuracy and powerful generalization ability in 2D still image segmentation, we propose a SAM3D that targets at 3D volumetric medical images and utilizes the pre-trained features from the SAM encoder to capture meaningful representations of input images. Different from other existing SAM-based volumetric segmentation methods that perform the segmentation by dividing the volume into a set of 2D slices, our model takes the whole 3D volume image as input and processes it simply and effectively that avoids training a significant number of parameters. Extensive experiments are conducted on multiple medical image datasets to demonstrate that our network attains competitive results compared with other state-of-the-art methods in 3D medical segmentation tasks while being significantly efficient in terms of parameters.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是医疗图像分析中的关键任务，它提供了诊断的有价值信息。在最近的几年中，基于深度学习的自动图像分割方法在医疗图像中取得了出色的结果。在这篇论文中，我们提议了一个基于Segment Anything Model（SAM）的3D医疗图像分割模型（SAM3D），该模型利用SAMEncoder预训练的特征来捕捉输入图像的有意义表示。与其他现有的SAM基于volumetric segmentation方法不同，我们的模型不需要将体volume分割成一系列的2D slice，而是直接处理整个3D图像，从而避免了训练大量参数。我们在多个医疗图像数据集上进行了广泛的实验，以示我们的网络与其他状态之前的方法在3D医疗图像分 segmentation任务中具有竞争力，同时在参数上具有显著的效率优势。
</details></li>
</ul>
<hr>
<h2 id="TSI-Net-A-Timing-Sequence-Image-Segmentation-Network-for-Intracranial-Artery-Segmentation-in-Digital-Subtraction-Angiography"><a href="#TSI-Net-A-Timing-Sequence-Image-Segmentation-Network-for-Intracranial-Artery-Segmentation-in-Digital-Subtraction-Angiography" class="headerlink" title="TSI-Net: A Timing Sequence Image Segmentation Network for Intracranial Artery Segmentation in Digital Subtraction Angiography"></a>TSI-Net: A Timing Sequence Image Segmentation Network for Intracranial Artery Segmentation in Digital Subtraction Angiography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03477">http://arxiv.org/abs/2309.03477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lemeng Wang, Wentao Liu, Weijin Xu, Haoyuan Li, Huihua Yang, Feng Gao<br>for:The paper is written for automating the segmentation of intracranial arteries in digital subtraction angiography (DSA) sequences, which is an important step in the diagnosis and treatment of cerebrovascular diseases.methods:The proposed method, called TSI-Net, uses a bi-directional ConvGRU module (BCM) in the encoder to input variable-length DSA sequences and segment them into 2D images, while also incorporating a sensitive detail branch (SDB) to supervise fine vessels.results:The method achieved a Sen evaluation metric of 0.797 on the DSA sequence dataset DIAS, which is a 3% improvement compared to other state-of-the-art methods.<details>
<summary>Abstract</summary>
Cerebrovascular disease is one of the major diseases facing the world today. Automatic segmentation of intracranial artery (IA) in digital subtraction angiography (DSA) sequences is an important step in the diagnosis of vascular related diseases and in guiding neurointerventional procedures. While, a single image can only show part of the IA within the contrast medium according to the imaging principle of DSA technology. Therefore, 2D DSA segmentation methods are unable to capture the complete IA information and treatment of cerebrovascular diseases. We propose A timing sequence image segmentation network with U-shape, called TSI-Net, which incorporates a bi-directional ConvGRU module (BCM) in the encoder. The network incorporates a bi-directional ConvGRU module (BCM) in the encoder, which can input variable-length DSA sequences, retain past and future information, segment them into 2D images. In addition, we introduce a sensitive detail branch (SDB) at the end for supervising fine vessels. Experimented on the DSA sequence dataset DIAS, the method performs significantly better than state-of-the-art networks in recent years. In particular, it achieves a Sen evaluation metric of 0.797, which is a 3% improvement compared to other methods.
</details>
<details>
<summary>摘要</summary>
脑血管疾病是当今世界面临的一大健康问题。自动将脑动脉（IA）分割成数字抵消成像（DSA）序列中的一个重要步骤，对于脑血管相关疾病的诊断和神经内部进行操作是非常重要。然而，单个图像只能显示脑动脉中的一部分，根据DSA技术的假设，因此2D DSA分割方法无法捕捉完整的IA信息。我们提出了一种名为TSI-Net的时序序列图像分割网络，该网络包含一个双向ConvGRU模块（BCM）在编码器中。该网络可以输入变长的DSA序列，同时保留过去和未来信息，将其分割成2D图像。此外，我们还引入了敏感细节分支（SDB），用于监督细血管。在DIAS数据集上进行实验，该方法与过去几年最佳方法相比，表现出了显著的改善，具体来说，它的Sen评价指标达0.797，比其他方法提高3%。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Quality-Assessment-of-360-circ-Images-Based-on-Generative-Scanpath-Representation"><a href="#Perceptual-Quality-Assessment-of-360-circ-Images-Based-on-Generative-Scanpath-Representation" class="headerlink" title="Perceptual Quality Assessment of 360$^\circ$ Images Based on Generative Scanpath Representation"></a>Perceptual Quality Assessment of 360$^\circ$ Images Based on Generative Scanpath Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03472">http://arxiv.org/abs/2309.03472</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiangjiesui/gsr">https://github.com/xiangjiesui/gsr</a></li>
<li>paper_authors: Xiangjie Sui, Hanwei Zhu, Xuelin Liu, Yuming Fang, Shiqi Wang, Zhou Wang</li>
<li>for: 这篇论文的目的是提出一种基于生成扫描路径表示（GSR）的高效的全息图像质量评估方法，以解决现有的全息图像质量评估模型忽略了用户观看行为的问题。</li>
<li>methods: 该方法使用了生成扫描路径的技术，通过定义观看条件（包括开始观看点和探索时间），生成了一系列的扫描路径，并将这些扫描路径转换为全息图像的唯一的GSR。然后，通过学习GSR的质量地图，实现高效的全息图像质量评估。</li>
<li>results: 实验结果表明，提出的方法可以快速地和高度一致地评估全息图像的质量，特别是在用户观看条件下存在局部扭曲的情况下。code将会在<a target="_blank" rel="noopener" href="https://github.com/xiangjieSui/GSR%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/xiangjieSui/GSR上发布。</a><details>
<summary>Abstract</summary>
Despite substantial efforts dedicated to the design of heuristic models for omnidirectional (i.e., 360$^\circ$) image quality assessment (OIQA), a conspicuous gap remains due to the lack of consideration for the diversity of viewing behaviors that leads to the varying perceptual quality of 360$^\circ$ images. Two critical aspects underline this oversight: the neglect of viewing conditions that significantly sway user gaze patterns and the overreliance on a single viewport sequence from the 360$^\circ$ image for quality inference. To address these issues, we introduce a unique generative scanpath representation (GSR) for effective quality inference of 360$^\circ$ images, which aggregates varied perceptual experiences of multi-hypothesis users under a predefined viewing condition. More specifically, given a viewing condition characterized by the starting point of viewing and exploration time, a set of scanpaths consisting of dynamic visual fixations can be produced using an apt scanpath generator. Following this vein, we use the scanpaths to convert the 360$^\circ$ image into the unique GSR, which provides a global overview of gazed-focused contents derived from scanpaths. As such, the quality inference of the 360$^\circ$ image is swiftly transformed to that of GSR. We then propose an efficient OIQA computational framework by learning the quality maps of GSR. Comprehensive experimental results validate that the predictions of the proposed framework are highly consistent with human perception in the spatiotemporal domain, especially in the challenging context of locally distorted 360$^\circ$ images under varied viewing conditions. The code will be released at https://github.com/xiangjieSui/GSR
</details>
<details>
<summary>摘要</summary>
尽管对权重图像质量评估（OIQA）的设计做出了大量努力，但是存在一个显著的漏洞，即因为忽略了观看行为多样性，导致360度图像的质量强度不均匀。两个关键因素把握这一点：忽略了观看条件的影响，以及对360度图像的质量做出判断只是基于单个视窗序列。为了解决这些问题，我们介绍了一种新的生成扫描路径表示（GSR），用于有效地评估360度图像的质量，该表示方法通过综合考虑多种假设用户的视觉经验来捕捉多样的观看行为。更具体地说，给定一个观看条件，包括开始观看和探索时间，我们可以使用适合的扫描路径生成器生成一系列的扫描路径，这些扫描路径包含动态的视觉固定点。然后，我们使用这些扫描路径将360度图像转换成唯一的GSR，该GSR提供了一个全面的观看关注点，即观看关注点的总和。因此，我们可以快速地将360度图像的质量评估转换成GSR的质量评估。我们then提出了一种高效的OIQA计算框架，通过学习GSR的质量地图来实现。实验结果表明，我们的提议的框架预测结果与人类视觉在空间时间域的吻合程度非常高，特别是在360度图像下的局部扭曲视图下的多种观看条件下。代码将在https://github.com/xiangjieSui/GSR上发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/07/eess.IV_2023_09_07/" data-id="clmjn91qx00hp0j88dsn93ali" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/06/cs.SD_2023_09_06/" class="article-date">
  <time datetime="2023-09-06T15:00:00.000Z" itemprop="datePublished">2023-09-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/06/cs.SD_2023_09_06/">cs.SD - 2023-09-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="RoDia-A-New-Dataset-for-Romanian-Dialect-Identification-from-Speech"><a href="#RoDia-A-New-Dataset-for-Romanian-Dialect-Identification-from-Speech" class="headerlink" title="RoDia: A New Dataset for Romanian Dialect Identification from Speech"></a>RoDia: A New Dataset for Romanian Dialect Identification from Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03378">http://arxiv.org/abs/2309.03378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Codrut Rotaru, Nicolae-Catalin Ristea, Radu Tudor Ionescu</li>
<li>For: The paper is written for the task of Romanian dialect identification from speech, with the goal of providing a valuable resource for future research.* Methods: The paper introduces a new dataset called RoDia, which includes a varied compilation of speech samples from five distinct regions of Romania, as well as a set of competitive models to be used as baselines.* Results: The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging.Here are the three key points in Simplified Chinese text:* For: 这篇论文是为了罗马尼亚语言方言识别从语音中进行的，以提供未来研究的价值资源。* Methods: 论文介绍了一个新的数据集 called RoDia，该数据集包括来自罗马尼亚五个不同地区的语音样本，以及一些竞争性的模型。* Results: 最高分模型的macro F1分数为59.83%，微 F1分数为62.08%，表示任务是具有挑战性。<details>
<summary>Abstract</summary>
Dialect identification is a critical task in speech processing and language technology, enhancing various applications such as speech recognition, speaker verification, and many others. While most research studies have been dedicated to dialect identification in widely spoken languages, limited attention has been given to dialect identification in low-resource languages, such as Romanian. To address this research gap, we introduce RoDia, the first dataset for Romanian dialect identification from speech. The RoDia dataset includes a varied compilation of speech samples from five distinct regions of Romania, covering both urban and rural environments, totaling 2 hours of manually annotated speech data. Along with our dataset, we introduce a set of competitive models to be used as baselines for future research. The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging. We thus believe that RoDia is a valuable resource that will stimulate research aiming to address the challenges of Romanian dialect identification. We publicly release our dataset and code at https://github.com/codrut2/RoDia.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>语言识别是语音处理和语言技术中的关键任务，提高了许多应用程序，如语音识别、speaker verification 等。然而，大多数研究都集中在广泛使用的语言上进行了研究，对低资源语言，如罗马尼亚语，的研究几乎没有任何关注。为了解决这个研究差距，我们介绍了 RoDia，罗马尼亚语言识别从speech的第一个数据集。RoDia数据集包括来自罗马尼亚五个不同区域的speech样本，涵盖了城市和农村环境，总计2小时的手动标注的语音数据。同时，我们也引入了一些竞争力强的模型，用于未来研究的基线。最高分模型的macro F1分数为59.83%，微 F1分数为62.08%，这表明这个任务非常困难。因此，我们认为RoDia是一个有价值的资源，将激发研究者解决罗马尼亚语言识别的挑战。我们将公开发布我们的数据集和代码在https://github.com/codrut2/RoDia。
</details></li>
</ul>
<hr>
<h2 id="Highly-Controllable-Diffusion-based-Any-to-Any-Voice-Conversion-Model-with-Frame-level-Prosody-Feature"><a href="#Highly-Controllable-Diffusion-based-Any-to-Any-Voice-Conversion-Model-with-Frame-level-Prosody-Feature" class="headerlink" title="Highly Controllable Diffusion-based Any-to-Any Voice Conversion Model with Frame-level Prosody Feature"></a>Highly Controllable Diffusion-based Any-to-Any Voice Conversion Model with Frame-level Prosody Feature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03364">http://arxiv.org/abs/2309.03364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyungguen Byun, Sunkuk Moon, Erik Visser</li>
<li>for: 这个论文的目的是提出一种可控性高的语音修饰系统，可以同时实现任意到任意语音转换（VC）和语速修饰。</li>
<li>methods: 该模型使用了一个帧级 просодиFeature来有效地传递气声和能量特征。具体来说，投射和内容嵌入被 feed到一个扩散基于decoder中，生成一个转换后的语音mel-spectrogram。为了调整说话速度，系统还包括一个自动学习模型后处理步骤，以提高可控性。</li>
<li>results: 该模型与State-of-the-art方法相比，可以覆盖各种基本频率（F0）、能量和速度修饰的范围，同时保持转换后的语音质量。<details>
<summary>Abstract</summary>
We propose a highly controllable voice manipulation system that can perform any-to-any voice conversion (VC) and prosody modulation simultaneously. State-of-the-art VC systems can transfer sentence-level characteristics such as speaker, emotion, and speaking style. However, manipulating the frame-level prosody, such as pitch, energy and speaking rate, still remains challenging. Our proposed model utilizes a frame-level prosody feature to effectively transfer such properties. Specifically, pitch and energy trajectories are integrated in a prosody conditioning module and then fed alongside speaker and contents embeddings to a diffusion-based decoder generating a converted speech mel-spectrogram. To adjust the speaking rate, our system includes a self-supervised model based post-processing step which allows improved controllability. The proposed model showed comparable speech quality and improved intelligibility compared to a SOTA approach. It can cover a varying range of fundamental frequency (F0), energy and speed modulation while maintaining converted speech quality.
</details>
<details>
<summary>摘要</summary>
我们提出了一个高度可控的语音修饰系统，可同时进行任何到任何语音转换（VC）和情感调整。现状顶尖的VC系统可以传递句子水平特征，如说话者、情感和说话风格。然而，控制帧级的语音特征，如抽象高度和能量谱，仍然是挑战。我们的提议的模型利用帧级语音特征来有效地传递这些属性。 Specifically, 抽象高度和能量谱在一个语音条件模块中被 интеGRATED，然后与说话者和内容嵌入式一起被 diffusion-based decoder 生成一个转换后的语音mel-spectrogram。为了调整说话速度，我们的系统包括一个基于自我超vised模型的后期处理步骤，以提高可控性。我们的模型在比较Speech quality和智能性方面与顶尖方法相当，可以覆盖各种基本频率（F0）、能量和速度调整的范围，而不 sacrificing converted speech quality。
</details></li>
</ul>
<hr>
<h2 id="Parameter-Efficient-Audio-Captioning-With-Faithful-Guidance-Using-Audio-text-Shared-Latent-Representation"><a href="#Parameter-Efficient-Audio-Captioning-With-Faithful-Guidance-Using-Audio-text-Shared-Latent-Representation" class="headerlink" title="Parameter Efficient Audio Captioning With Faithful Guidance Using Audio-text Shared Latent Representation"></a>Parameter Efficient Audio Captioning With Faithful Guidance Using Audio-text Shared Latent Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03340">http://arxiv.org/abs/2309.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arvind Krishna Sridhar, Yinyi Guo, Erik Visser, Rehana Mahfuz</li>
<li>for: 本研究旨在 Addressing overparameterization and hallucination issues in pretrained transformer architectures for automated audio captioning tasks.</li>
<li>methods: 我们提出了一种数据增强技术和一种 Parameter-efficient inference time faithful decoding algorithm to improve the performance and efficiency of audio captioning models.</li>
<li>results: 我们的方法可以在 benchmark datasets 上达到与更大的模型相同的性能，并且可以减少模型的复杂性和内存占用。<details>
<summary>Abstract</summary>
There has been significant research on developing pretrained transformer architectures for multimodal-to-text generation tasks. Albeit performance improvements, such models are frequently overparameterized, hence suffer from hallucination and large memory footprint making them challenging to deploy on edge devices. In this paper, we address both these issues for the application of automated audio captioning. First, we propose a data augmentation technique for generating hallucinated audio captions and show that similarity based on an audio-text shared latent space is suitable for detecting hallucination. Then, we propose a parameter efficient inference time faithful decoding algorithm that enables smaller audio captioning models with performance equivalent to larger models trained with more data. During the beam decoding step, the smaller model utilizes an audio-text shared latent representation to semantically align the generated text with corresponding input audio. Faithful guidance is introduced into the beam probability by incorporating the cosine similarity between latent representation projections of greedy rolled out intermediate beams and audio clip. We show the efficacy of our algorithm on benchmark datasets and evaluate the proposed scheme against baselines using conventional audio captioning and semantic similarity metrics while illustrating tradeoffs between performance and complexity.
</details>
<details>
<summary>摘要</summary>
有很多研究关于开发预训练转换器架构来实现多Modal-to-文本生成任务。虽然性能提高，但这些模型往往过参数，因此容易出现幻觉和大内存占用问题，使其在边缘设备上部署困难。在这篇论文中，我们解决了这些问题，并应用于自动化音频captioning。我们首先提出了一种数据增强技术，用于生成幻觉音频caption，并证明了基于音频-文本共同特征空间的相似性是适用于检测幻觉的。然后，我们提出了一种具有效率的执行时实际 faithful decoding算法，可以使得音频captioning模型更小，性能与更多数据训练的大型模型相当。在探索步骤中，小型模型使用音频-文本共同特征来semantic地将生成的文本与输入音频进行对应。我们引入了对于扩展步骤的 faithful guidance，使用 audio clip的cosine similarity来衡量latent representation projections的greedy rolled out intermediate beams的相似性。我们在标准 benchmark数据集上证明了我们的算法的效果，并对基于传统音频captioning和semantic相似度度量的基eline进行评估，同时 illustrate tradeoffs between performance和复杂性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Geometrical-Acoustic-Simulations-of-Spatial-Room-Impulse-Responses-for-Improved-Sound-Event-Detection-and-Localization"><a href="#Leveraging-Geometrical-Acoustic-Simulations-of-Spatial-Room-Impulse-Responses-for-Improved-Sound-Event-Detection-and-Localization" class="headerlink" title="Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization"></a>Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03337">http://arxiv.org/abs/2309.03337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Ick, Brian McFee</li>
<li>for: 实现 зву标注数据的增加，以满足深度神经网络模型的需求。</li>
<li>methods: 使用几何学Acoustic simulation generate novel SRIR dataset，以训练SELD模型。</li>
<li>results: 透过实验资料增强现有的benchmark，并提高SELD模型的性能。<details>
<summary>Abstract</summary>
As deeper and more complex models are developed for the task of sound event localization and detection (SELD), the demand for annotated spatial audio data continues to increase. Annotating field recordings with 360$^{\circ}$ video takes many hours from trained annotators, while recording events within motion-tracked laboratories are bounded by cost and expertise. Because of this, localization models rely on a relatively limited amount of spatial audio data in the form of spatial room impulse response (SRIR) datasets, which limits the progress of increasingly deep neural network based approaches. In this work, we demonstrate that simulated geometrical acoustics can provide an appealing solution to this problem. We use simulated geometrical acoustics to generate a novel SRIR dataset that can train a SELD model to provide similar performance to that of a real SRIR dataset. Furthermore, we demonstrate using simulated data to augment existing datasets, improving on benchmarks set by state of the art SELD models. We explore the potential and limitations of geometric acoustic simulation for localization and event detection. We also propose further studies to verify the limitations of this method, as well as further methods to generate synthetic data for SELD tasks without the need to record more data.
</details>
<details>
<summary>摘要</summary>
In this study, we propose using simulated geometrical acoustics as a solution to this problem. By generating a novel SRIR dataset using simulated geometrical acoustics, we demonstrate that our approach can provide similar performance to that of a real SRIR dataset in training a SELD model. Additionally, we show that using simulated data to augment existing datasets can improve the performance of state-of-the-art SELD models.We explore the potential and limitations of geometric acoustic simulation for localization and event detection, and propose further studies to verify the limitations of this method. We also suggest additional methods for generating synthetic data for SELD tasks without the need for additional recording.Translation notes:* "field recordings" 被翻译为 "野外录音"* "motion-tracked laboratories" 被翻译为 "运动跟踪实验室"* "SRIR datasets" 被翻译为 "声学室应答数据集"* "simulated geometrical acoustics" 被翻译为 "计算机生成的几何声学"* "SELD models" 被翻译为 "声事件定位和检测模型"
</details></li>
</ul>
<hr>
<h2 id="Presenting-the-SWTC-A-Symbolic-Corpus-of-Themes-from-John-Williams’-Star-Wars-Episodes-I-IX"><a href="#Presenting-the-SWTC-A-Symbolic-Corpus-of-Themes-from-John-Williams’-Star-Wars-Episodes-I-IX" class="headerlink" title="Presenting the SWTC: A Symbolic Corpus of Themes from John Williams’ Star Wars Episodes I-IX"></a>Presenting the SWTC: A Symbolic Corpus of Themes from John Williams’ Star Wars Episodes I-IX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03298">http://arxiv.org/abs/2309.03298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claire Arthur, Frank Lehman, John McNamara</li>
<li>for: 这份报告描述了一个新的符号化乐曲资料库，名为星球战争主题集（SWTC），该库包含了John Williams创作的完整星球战争三部曲（集I-IX）中的64个不同、重复和符号意义强的乐Theme和动机。</li>
<li>methods: 该报告介绍了一种新的humdrum标准 для非功能和声编码，基于Harte（2005、2010）的研究。该报告还描述了译写和编码过程。</li>
<li>results: 该报告提供了一些简要的摘要统计数据，并表明SWTC虽然规模较小，但代表了20世纪一位最具影响力的作曲家之一的作品，以及电影和多媒体音乐材料的未exploredsubset。<details>
<summary>Abstract</summary>
This paper presents a new symbolic corpus of musical themes from the complete Star Wars trilogies (Episodes I-IX) by John Williams. The corpus files are made available in multiple formats (.krn, .sib, and .musicxml) and include melodic, harmonic, and formal information. The Star Wars Thematic Corpus (SWTC) contains a total of 64 distinctive, recurring, and symbolically meaningful themes and motifs, commonly referred to as leitmotifs. Through this corpus we also introduce a new humdrum standard for non-functional harmony encodings, **harte, based on Harte (2005, 2010). This report details the motivation, describes the transcription and encoding processes, and provides some brief summary statistics. While relatively small in scale, the SWTC represents a unified collection from one of the most prolific and influential composers of the 20th century, and the under-studied subset of film and multimedia musical material in general. We hope the SWTC will provide insights into John Williams' compositional style, as well as prove useful in comparisons against other thematic corpora from film and beyond.
</details>
<details>
<summary>摘要</summary>
Note: Some minor changes were made to the original text to improve readability and clarity, but the overall meaning and content remain the same.
</details></li>
</ul>
<hr>
<h2 id="Real-time-auralization-for-performers-on-virtual-stages"><a href="#Real-time-auralization-for-performers-on-virtual-stages" class="headerlink" title="Real-time auralization for performers on virtual stages"></a>Real-time auralization for performers on virtual stages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03149">http://arxiv.org/abs/2309.03149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernesto Accolti, Lukas Aspöck, Manuj Yadav, Michael Vorländer</li>
<li>for: 这篇论文描述了一个互动系统，用于实验室中的舞台声学实验。</li>
<li>methods: 该系统使用了一组矩阵来补做听到自己乐器的声音的问题，以及考虑了听到其他乐器的声音的问题。</li>
<li>results: 试验表明，该系统可以准确地模拟声音，并且可以跟踪实际的听众声音。<details>
<summary>Abstract</summary>
This article presents an interactive system for stage acoustics experimentation including considerations for hearing one's own and others' instruments. The quality of real-time auralization systems for psychophysical experiments on music performance depends on the system's calibration and latency, among other factors (e.g. visuals, simulation methods, haptics, etc). The presented system focuses on the acoustic considerations for laboratory implementations. The calibration is implemented as a set of filters accounting for the microphone-instrument distances and the directivity factors, as well as the transducers' frequency responses. Moreover, sources of errors are characterized using both state-of-the-art information and derivations from the mathematical definition of the calibration filter. In order to compensate for hardware latency without cropping parts of the simulated impulse responses, the virtual direct sound of musicians hearing themselves is skipped from the simulation and addressed by letting the actual direct sound reach the listener through open headphones. The required latency compensation of the interactive part (i.e. hearing others) meets the minimum distance requirement between musicians, which is 2 m for the implemented system. Finally, a proof of concept is provided that includes objective and subjective experiments, which give support to the feasibility of the proposed setup.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Efficient-Temporary-Deepfake-Location-Approach-Based-Embeddings-for-Partially-Spoofed-Audio-Detection"><a href="#An-Efficient-Temporary-Deepfake-Location-Approach-Based-Embeddings-for-Partially-Spoofed-Audio-Detection" class="headerlink" title="An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection"></a>An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03036">http://arxiv.org/abs/2309.03036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankun Xie, Haonan Cheng, Yutian Wang, Long Ye</li>
<li>for: 本研究旨在提出一种精细化的半假声音检测方法，以准确地检测声音中的真实性。</li>
<li>methods: 本方法包括两个新的部分：嵌入相似模块和时间卷积操作。嵌入相似模块用于生成一个可分离真实和假声音的 embedding 空间，而时间卷积操作则用于计算邻域帧之间的相似性，并动态选择有用的邻域进行卷积。</li>
<li>results: 对于 ASVspoof2019 Partial Spoof 数据集，我们的方法表现出了对基线模型的超越性，并在跨数据集场景中也达到了优秀的表现。代码已经在线发布。<details>
<summary>Abstract</summary>
Partially spoofed audio detection is a challenging task, lying in the need to accurately locate the authenticity of audio at the frame level. To address this issue, we propose a fine-grained partially spoofed audio detection method, namely Temporal Deepfake Location (TDL), which can effectively capture information of both features and locations. Specifically, our approach involves two novel parts: embedding similarity module and temporal convolution operation. To enhance the identification between the real and fake features, the embedding similarity module is designed to generate an embedding space that can separate the real frames from fake frames. To effectively concentrate on the position information, temporal convolution operation is proposed to calculate the frame-specific similarities among neighboring frames, and dynamically select informative neighbors to convolution. Extensive experiments show that our method outperform baseline models in ASVspoof2019 Partial Spoof dataset and demonstrate superior performance even in the crossdataset scenario. The code is released online.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传入文本转化为简化中文。<</SYS>>假 Audio 检测是一项复杂的任务，因为它需要准确地确定听录中的真实性，具体来说是在帧级别上。为解决这个问题，我们提议一种细化的假 Audio 检测方法，即 Temporal Deepfake Location（TDL），它可以准确地捕捉听录中的特征和位置信息。具体来说，我们的方法包括两个新的部分：嵌入相似模块和时间核算操作。为了增强真实和假听录之间的识别，嵌入相似模块是为了生成一个可以分离真听录和假听录的嵌入空间。而时间核算操作则是为了计算每帧的相似性，并选择相邻帧中的有用 neighboor 进行 convolution。通过这两个部分，我们的方法可以更好地捕捉听录中的信息，并且在跨数据集场景下表现出优于基eline模型。我们的代码已经在线发布。
</details></li>
</ul>
<hr>
<h2 id="Indoor-Localization-Using-Radio-Vision-and-Audio-Sensors-Real-Life-Data-Validation-and-Discussion"><a href="#Indoor-Localization-Using-Radio-Vision-and-Audio-Sensors-Real-Life-Data-Validation-and-Discussion" class="headerlink" title="Indoor Localization Using Radio, Vision and Audio Sensors: Real-Life Data Validation and Discussion"></a>Indoor Localization Using Radio, Vision and Audio Sensors: Real-Life Data Validation and Discussion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02961">http://arxiv.org/abs/2309.02961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilayda Yaman, Guoda Tian, Erik Tegler, Patrik Persson, Nikhil Challa, Fredrik Tufvesson, Ove Edfors, Kalle Astrom, Steffen Malkowsky, Liang Liu</li>
<li>for: 本研究探讨了使用 радио、视觉和听音感知器进行indoor定位方法，并在同一环境中进行评估。</li>
<li>methods: 本研究使用了现有算法来评估 радио基本的机器学习算法、视觉基本的ORB-SLAM3算法和听音基本的SFS2算法。</li>
<li>results: 研究发现，使用不同感知器可以实现高精度和可靠的indoor定位，但是每种感知器都有其优缺点，需要根据具体情况进行选择和权衡。<details>
<summary>Abstract</summary>
This paper investigates indoor localization methods using radio, vision, and audio sensors, respectively, in the same environment. The evaluation is based on state-of-the-art algorithms and uses a real-life dataset. More specifically, we evaluate a machine learning algorithm for radio-based localization with massive MIMO technology, an ORB-SLAM3 algorithm for vision-based localization with an RGB-D camera, and an SFS2 algorithm for audio-based localization with microphone arrays. Aspects including localization accuracy, reliability, calibration requirements, and potential system complexity are discussed to analyze the advantages and limitations of using different sensors for indoor localization tasks. The results can serve as a guideline and basis for further development of robust and high-precision multi-sensory localization systems, e.g., through sensor fusion and context and environment-aware adaptation.
</details>
<details>
<summary>摘要</summary>
Note:* " Simplified Chinese" is also known as "简化字" or "简化字" in Chinese.* " radio" is translated as "无线电" (wú xiàn diàn)* " vision" is translated as "视觉" (zhì jìng)* " audio" is translated as "声音" (shēng yīn)* " sensor" is translated as "感测器" (gǎn cè qì)* " localization" is translated as "位置确定" (wèi zhì jiè dìng)* " accuracy" is translated as "准确性" (zhèng qiú xìng)* " reliability" is translated as "可靠性" (kě jiào xìng)* " calibration" is translated as "准备" (zhèng bèi)* " system complexity" is translated as "系统复杂度" (xiàng zhì zhòng le)Please note that the translation is based on the standard Simplified Chinese language and may have some variations depending on the region or context.
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-Blind-Spots-in-Spoken-Language-Processing"><a href="#Addressing-the-Blind-Spots-in-Spoken-Language-Processing" class="headerlink" title="Addressing the Blind Spots in Spoken Language Processing"></a>Addressing the Blind Spots in Spoken Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06572">http://arxiv.org/abs/2309.06572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Moryossef</li>
<li>for: 本研究探讨了人类交流中的关键 yet often overlooked 非语言表达，包括协同语言动作和 facial expressions，以及它们对自然语言处理（NLP）的影响。</li>
<li>methods: 我们提出了一种基于 sign language processing 的 universal automatic gesture segmentation and transcription models，用于将非语言表达转换为文本形式。这种方法旨在bridge spoken language understanding 中的盲点，提高 NLP 模型的范围和可用性。</li>
<li>results: 我们通过示例推动了强调仅仅 rely solely on text-based models 的局限性。我们提出了一种可计算 эффектив和灵活的方法，可以轻松地与现有的 NLP 管道集成。我们 conclude by calling upon the research community to contribute to the development of universal transcription methods and to validate their effectiveness in capturing the complexities of real-world, multi-modal interactions.<details>
<summary>Abstract</summary>
This paper explores the critical but often overlooked role of non-verbal cues, including co-speech gestures and facial expressions, in human communication and their implications for Natural Language Processing (NLP). We argue that understanding human communication requires a more holistic approach that goes beyond textual or spoken words to include non-verbal elements. Borrowing from advances in sign language processing, we propose the development of universal automatic gesture segmentation and transcription models to transcribe these non-verbal cues into textual form. Such a methodology aims to bridge the blind spots in spoken language understanding, enhancing the scope and applicability of NLP models. Through motivating examples, we demonstrate the limitations of relying solely on text-based models. We propose a computationally efficient and flexible approach for incorporating non-verbal cues, which can seamlessly integrate with existing NLP pipelines. We conclude by calling upon the research community to contribute to the development of universal transcription methods and to validate their effectiveness in capturing the complexities of real-world, multi-modal interactions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Self-Supervised-Disentanglement-of-Harmonic-and-Rhythmic-Features-in-Music-Audio-Signals"><a href="#Self-Supervised-Disentanglement-of-Harmonic-and-Rhythmic-Features-in-Music-Audio-Signals" class="headerlink" title="Self-Supervised Disentanglement of Harmonic and Rhythmic Features in Music Audio Signals"></a>Self-Supervised Disentanglement of Harmonic and Rhythmic Features in Music Audio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02796">http://arxiv.org/abs/2309.02796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WuYiming6526/HARD-DAFx2023">https://github.com/WuYiming6526/HARD-DAFx2023</a></li>
<li>paper_authors: Yiming Wu</li>
<li>for: 本研究旨在推断音乐Audio生成过程中隐藏的多个有用的尺度表示，以实现可控的数据生成。</li>
<li>methods: 本文提出了一种基于深度神经网络的自监学习方法，用于推断音乐Audio生成过程中的谱写和和声表示。在训练阶段，Variational Autoencoder被训练来重建输入的mel-spectrogram，并在每次前向计算中应用了一个向量旋转操作，以假设特征向量的维度与抽象音间的幂距离相关。因此，在训练后的Variational Autoencoder中，旋转的尺度特征表示了音频中的抽象音谱，而不旋转的尺度特征表示了不变的rhythmic内容。</li>
<li>results: 在predictor-based拟合度量上测试了学习的特征，并证明了该方法的有效性。此外，本文还应用了该方法到自动生成音乐混音的问题中，并达到了良好的结果。<details>
<summary>Abstract</summary>
The aim of latent variable disentanglement is to infer the multiple informative latent representations that lie behind a data generation process and is a key factor in controllable data generation. In this paper, we propose a deep neural network-based self-supervised learning method to infer the disentangled rhythmic and harmonic representations behind music audio generation. We train a variational autoencoder that generates an audio mel-spectrogram from two latent features representing the rhythmic and harmonic content. In the training phase, the variational autoencoder is trained to reconstruct the input mel-spectrogram given its pitch-shifted version. At each forward computation in the training phase, a vector rotation operation is applied to one of the latent features, assuming that the dimensions of the feature vectors are related to pitch intervals. Therefore, in the trained variational autoencoder, the rotated latent feature represents the pitch-related information of the mel-spectrogram, and the unrotated latent feature represents the pitch-invariant information, i.e., the rhythmic content. The proposed method was evaluated using a predictor-based disentanglement metric on the learned features. Furthermore, we demonstrate its application to the automatic generation of music remixes.
</details>
<details>
<summary>摘要</summary>
目标是抽取数据生成过程中隐藏的多个有用特征表示，这是可控数据生成的关键因素。在这篇论文中，我们提出了一种基于深度神经网络的自supervised learning方法，用于抽取音乐Audio生成过程中的拍和和和声特征。我们训练了一个变量自动编码器，该编码器从两个隐藏特征中生成了一个Audio mel-spectrogram，其中一个隐藏特征表示拍和内容，另一个隐藏特征表示和声内容。在训练阶段，变量自动编码器被训练以重建输入 mel-spectrogram，并在每次前向计算中应用了一个向量旋转操作，假设输入特征向量的维度与抽象Interval相关。因此，在训练过程中，旋转的隐藏特征表示输入 mel-spectrogram 的抽象信息，而不旋转的隐藏特征表示拍和内容。我们使用predictor-based拟合度量来评估学习的特征。此外，我们还 demonstarted its应用于自动生成音乐remix。
</details></li>
</ul>
<hr>
<h2 id="GRASS-Unified-Generation-Model-for-Speech-to-Semantic-Tasks"><a href="#GRASS-Unified-Generation-Model-for-Speech-to-Semantic-Tasks" class="headerlink" title="GRASS: Unified Generation Model for Speech-to-Semantic Tasks"></a>GRASS: Unified Generation Model for Speech-to-Semantic Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02780">http://arxiv.org/abs/2309.02780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aobo Xia, Shuyu Lei, Yushu Yang, Xiang Guo, Hua Chai</li>
<li>for: 本文探讨了对speech-to-semantic任务的指令细化技术，提出了一个综合的端到端（E2E）框架，通过给定任务相关的提示，对语音数据生成目标文本。</li>
<li>methods: 我们在大量和多样的数据进行预训练，使用文本到语音（TTS）系统生成 instruciton-speech 对。</li>
<li>results: 我们的提posed模型在许多标准准点上达到了状态元（SOTA）的 результа们，包括speech命名实体识别、speech情感分类、speech问答等。此外，我们的模型在零shot和几shotenario中也达到了竞争性的性能。<details>
<summary>Abstract</summary>
This paper explores the instruction fine-tuning technique for speech-to-semantic tasks by introducing a unified end-to-end (E2E) framework that generates target text conditioned on a task-related prompt for audio data. We pre-train the model using large and diverse data, where instruction-speech pairs are constructed via a text-to-speech (TTS) system. Extensive experiments demonstrate that our proposed model achieves state-of-the-art (SOTA) results on many benchmarks covering speech named entity recognition, speech sentiment analysis, speech question answering, and more, after fine-tuning. Furthermore, the proposed model achieves competitive performance in zero-shot and few-shot scenarios. To facilitate future work on instruction fine-tuning for speech-to-semantic tasks, we release our instruction dataset and code.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了对话 semantics 任务的指令细化技术，通过提出一种端到端 (E2E) 框架，使得目标文本根据任务相关的提示生成 Conditioned 于听力数据。我们在大量和多样的数据上进行预训练，使用文本到声音 (TTS) 系统来构建指令演讲对。广泛的实验表明，我们提出的模型在许多标准底下取得了状态的最佳 (SOTA) 结果，包括对话名 entity 识别、对话情感分类、对话问答等，并且在零 shot 和几 shot 情况下表现竞争力。为便于未来对指令细化 для speech-to-semantic 任务的研究，我们发布了我们的指令集和代码。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-Measurement-of-Multiple-Acoustic-Attributes-Using-Structured-Periodic-Test-Signals-Including-Music-and-Other-Sound-Materials"><a href="#Simultaneous-Measurement-of-Multiple-Acoustic-Attributes-Using-Structured-Periodic-Test-Signals-Including-Music-and-Other-Sound-Materials" class="headerlink" title="Simultaneous Measurement of Multiple Acoustic Attributes Using Structured Periodic Test Signals Including Music and Other Sound Materials"></a>Simultaneous Measurement of Multiple Acoustic Attributes Using Structured Periodic Test Signals Including Music and Other Sound Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02767">http://arxiv.org/abs/2309.02767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hideki Kawahara, Kohei Yatabe, Ken-Ichi Sakakibara, Mitsunori Mizumachi, Tatsuya Kitamura</li>
<li>for: 这项研究旨在 simultaneously 测量音频性能中的线性时间不变（LTI）回响、信号依赖时间不变（SDTI）组件、随机和时间变化（RTV）组件。</li>
<li>methods: 这个框架使用结构化 periodic test signals 来测量音频性能，并可以使用音乐作品和其他声音材料作为测试信号，通过添加微量杂音来”保护”它们。</li>
<li>results: 实现了交互式、实时测量工具，并将其开源。此外，这个框架还用于对抽取器的性能进行对比性评价。<details>
<summary>Abstract</summary>
We introduce a general framework for measuring acoustic properties such as liner time-invariant (LTI) response, signal-dependent time-invariant (SDTI) component, and random and time-varying (RTV) component simultaneously using structured periodic test signals. The framework also enables music pieces and other sound materials as test signals by "safeguarding" them by adding slight deterministic "noise." Measurement using swept-sin, MLS (Maxim Length Sequence), and their variants are special cases of the proposed framework. We implemented interactive and real-time measuring tools based on this framework and made them open-source. Furthermore, we applied this framework to assess pitch extractors objectively.
</details>
<details>
<summary>摘要</summary>
我们提出了一种通用的措施来同时测量音响性特性，包括时间不变（LTI）响应、时间不变信号（SDTI）组件以及随机时变（RTV）组件。这个框架还允许使用音乐作品和其他声音材料作为测试信号，并通过添加微量权值“噪声”来“保护”它们。使用滚动磁盘、MLS（最长长序）和其他变体的测量方法是这个框架的特殊情况。我们实现了基于这个框架的互动式和实时测量工具，并将其开源。此外，我们使用了这个框架对抽取器进行对比性评估。
</details></li>
</ul>
<hr>
<h2 id="MuLanTTS-The-Microsoft-Speech-Synthesis-System-for-Blizzard-Challenge-2023"><a href="#MuLanTTS-The-Microsoft-Speech-Synthesis-System-for-Blizzard-Challenge-2023" class="headerlink" title="MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023"></a>MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02743">http://arxiv.org/abs/2309.02743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihang Xu, Shaofei Zhang, Xi Wang, Jiajun Zhang, Wenning Wei, Lei He, Sheng Zhao</li>
<li>For: 本研究开发了一个名为MuLanTTS的微软终端神经文本读取系统，应援Blizzard Challenge 2023。* Methods: 本研究基于DelightfulTTS，采用了上下文和情感编码器，以扩展 sentences 的长形属性和对话表达性。* Results: MuLanTTS 在两个任务中获得了4.3和4.5的平均评价分数，与自然语音相似，并保持了好的相似性。<details>
<summary>Abstract</summary>
In this paper, we present MuLanTTS, the Microsoft end-to-end neural text-to-speech (TTS) system designed for the Blizzard Challenge 2023. About 50 hours of audiobook corpus for French TTS as hub task and another 2 hours of speaker adaptation as spoke task are released to build synthesized voices for different test purposes including sentences, paragraphs, homographs, lists, etc. Building upon DelightfulTTS, we adopt contextual and emotion encoders to adapt the audiobook data to enrich beyond sentences for long-form prosody and dialogue expressiveness. Regarding the recording quality, we also apply denoise algorithms and long audio processing for both corpora. For the hub task, only the 50-hour single speaker data is used for building the TTS system, while for the spoke task, a multi-speaker source model is used for target speaker fine tuning. MuLanTTS achieves mean scores of quality assessment 4.3 and 4.5 in the respective tasks, statistically comparable with natural speech while keeping good similarity according to similarity assessment. The excellent and similarity in this year's new and dense statistical evaluation show the effectiveness of our proposed system in both tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了MuLanTTS系统，这是微软公司的终端神经文本读取（TTS）系统，旨在参加2023年的Blizzard挑战。我们发布了50小时的法语TTSHub任务和2小时的说话人适应任务的音频资料，用于建立不同测试目的的合成声音，包括句子、段落、同义词、列表等。基于DelightfulTTS，我们采用了上下文和情感编码器，以适应音频资料，提高长形味道和对话表达性。在录音质量方面，我们还应用了降噪算法和长 audio处理。在主任务中，我们只使用单个说话人的50小时数据来建立TTS系统，而在 spoke任务中，我们使用多个说话人的源模型进行目标说话人细化。MuLanTTS在两个任务中得到了4.3和4.5的平均评价分数，与自然语音相似，同时保持了好的相似性。这年的新和紧密的统计评价结果表明了我们提posed系统在两个任务中的效果。
</details></li>
</ul>
<hr>
<h2 id="Stylebook-Content-Dependent-Speaking-Style-Modeling-for-Any-to-Any-Voice-Conversion-using-Only-Speech-Data"><a href="#Stylebook-Content-Dependent-Speaking-Style-Modeling-for-Any-to-Any-Voice-Conversion-using-Only-Speech-Data" class="headerlink" title="Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data"></a>Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02730">http://arxiv.org/abs/2309.02730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyungseob Lim, Kyungguen Byun, Sunkuk Moon, Erik Visser</li>
<li>for: 本研究旨在提出一种可以准确传递target speaker的语言风格信息的any-to-anyvoice转换方法。</li>
<li>methods: 本方法利用一种自动学习模型（SSL）来采集target speaker的发言风格信息，并将其转化为一组名为“stylebook”的嵌入。然后，这个stylebook与源语音的phonetic content进行attend，以确定最终的target style。最后，通过一种扩散模型来生成转换后的语音mel-spectrogram。</li>
<li>results: 实验结果显示，combined with our proposed method and a diffusion-based generative model, any-to-any voice conversion tasks can achieve better speaker similarity than baseline models, while the increase in computational complexity with longer utterances is suppressed.<details>
<summary>Abstract</summary>
While many recent any-to-any voice conversion models succeed in transferring some target speech's style information to the converted speech, they still lack the ability to faithfully reproduce the speaking style of the target speaker. In this work, we propose a novel method to extract rich style information from target utterances and to efficiently transfer it to source speech content without requiring text transcriptions or speaker labeling. Our proposed approach introduces an attention mechanism utilizing a self-supervised learning (SSL) model to collect the speaking styles of a target speaker each corresponding to the different phonetic content. The styles are represented with a set of embeddings called stylebook. In the next step, the stylebook is attended with the source speech's phonetic content to determine the final target style for each source content. Finally, content information extracted from the source speech and content-dependent target style embeddings are fed into a diffusion-based decoder to generate the converted speech mel-spectrogram. Experiment results show that our proposed method combined with a diffusion-based generative model can achieve better speaker similarity in any-to-any voice conversion tasks when compared to baseline models, while the increase in computational complexity with longer utterances is suppressed.
</details>
<details>
<summary>摘要</summary>
许多最近的任意到任意语音转换模型成功地将目标语音的风格信息传递到转换后的语音中，但他们仍然缺乏将目标说话者的说话风格忠实地复制到转换后的语音中的能力。在这项工作中，我们提出了一种新的方法，可以从目标语音中提取丰富的风格信息，并将其效率地传递到源语音内容中，不需要文本转录或说话者标注。我们的提议的方法使用一种自动学习（SSL）模型来收集目标说话者每个不同的phonetic content的说话风格。这些风格被表示为一组叫做风格书的嵌入。在下一步，风格书与源语音的phonetic content进行了attend操作，以确定每个源内容的最终目标风格。最后，从源语音中提取的内容信息和 Content-dependent target style embeddings被 fed into一个扩散-based decoder，以生成转换后的语音mel-spectrogram。实验结果表明，我们的提议方法与扩散-based生成模型结合使用可以在任意到任意语音转换任务中实现更高的说话者相似性，而且与长语音的计算复杂度增长相对减少。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/06/cs.SD_2023_09_06/" data-id="clmjn91od00bv0j88cejp349u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_09_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/06/eess.AS_2023_09_06/" class="article-date">
  <time datetime="2023-09-06T14:00:00.000Z" itemprop="datePublished">2023-09-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/06/eess.AS_2023_09_06/">eess.AS - 2023-09-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Role-of-Communication-and-Reference-Songs-in-the-Mixing-Process-Insights-from-Professional-Mix-Engineers"><a href="#The-Role-of-Communication-and-Reference-Songs-in-the-Mixing-Process-Insights-from-Professional-Mix-Engineers" class="headerlink" title="The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers"></a>The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03404">http://arxiv.org/abs/2309.03404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumya Sai Vanka, Maryam Safi, Jean-Baptiste Rolland, György Fazekas</li>
<li>for: 本研究旨在理解专业混音工程师与客户之间的交流和反馈过程，以便更好地支持创作过程和实现混音的目标。</li>
<li>methods: 本研究采用了两个阶段的探索性研究方法，包括 semi-structured 采访和在线问卷调查。</li>
<li>results: 研究发现，混音工程师与客户之间的合作、理解和意图是混音过程中非常重要的因素，这些因素可以帮助开发更好地支持这些实践的智能多轨混音系统。<details>
<summary>Abstract</summary>
Effective music mixing requires technical and creative finesse, but clear communication with the client is crucial. The mixing engineer must grasp the client's expectations, and preferences, and collaborate to achieve the desired sound. The tacit agreement for the desired sound of the mix is often established using guides like reference songs and demo mixes exchanged between the artist and the engineer and sometimes verbalised using semantic terms. This paper presents the findings of a two-phased exploratory study aimed at understanding how professional mixing engineers interact with clients and use their feedback to guide the mixing process. For phase one, semi-structured interviews were conducted with five mixing engineers with the aim of gathering insights about their communication strategies, creative processes, and decision-making criteria. Based on the inferences from these interviews, an online questionnaire was designed and administered to a larger group of 22 mixing engineers during the second phase. The results of this study shed light on the importance of collaboration, empathy, and intention in the mixing process, and can inform the development of smart multi-track mixing systems that better support these practices. By highlighting the significance of these findings, this paper contributes to the growing body of research on the collaborative nature of music production and provides actionable recommendations for the design and implementation of innovative mixing tools.
</details>
<details>
<summary>摘要</summary>
Effective music mixing requires both technical and creative skills, but clear communication with the client is crucial. The mixing engineer must understand the client's expectations and preferences and collaborate to achieve the desired sound. The tacit agreement for the desired sound of the mix is often established using guides such as reference songs and demo mixes exchanged between the artist and the engineer, and sometimes verbally using semantic terms. This paper presents the findings of a two-phased exploratory study aimed at understanding how professional mixing engineers interact with clients and use their feedback to guide the mixing process. For phase one, semi-structured interviews were conducted with five mixing engineers to gather insights into their communication strategies, creative processes, and decision-making criteria. Based on these findings, an online questionnaire was designed and administered to a larger group of 22 mixing engineers during the second phase. The results of this study highlight the importance of collaboration, empathy, and intention in the mixing process and can inform the development of smart multi-track mixing systems that better support these practices. By highlighting the significance of these findings, this paper contributes to the growing body of research on the collaborative nature of music production and provides actionable recommendations for the design and implementation of innovative mixing tools.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-ASR-Pretrained-Conformers-for-Speaker-Verification-through-Transfer-Learning-and-Knowledge-Distillation"><a href="#Leveraging-ASR-Pretrained-Conformers-for-Speaker-Verification-through-Transfer-Learning-and-Knowledge-Distillation" class="headerlink" title="Leveraging ASR Pretrained Conformers for Speaker Verification through Transfer Learning and Knowledge Distillation"></a>Leveraging ASR Pretrained Conformers for Speaker Verification through Transfer Learning and Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03019">http://arxiv.org/abs/2309.03019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danwei Cai, Ming Li</li>
<li>for: 本研究使用 ASR 预训练 Conformers 进行 speaker verification，利用它们对语音信号的模型化能力。</li>
<li>methods: 我们提出了三种策略：(1) 使用转移学习初始化说话人嵌入网络，提高泛化和降低过拟合。 (2) 使用知识塑化训练更加灵活的说话人验证模型，并将 frame-level ASR 损失作为辅助任务。 (3) 使用轻量级的说话人适配器实现高效的特征转换，无需改变原始 ASR Conformer，以便并行进行 ASR 和说话人验证。</li>
<li>results: 在 VoxCeleb 上进行实验，我们发现：(1) 转移学习可以提供 0.48% EER 的性能。 (2) 知识塑化可以提供 0.43% EER 的性能。 (3) 使用说话人适配器可以实现 0.57% EER 的性能，只需添加 4.92M 参数到一个 130.94M 参数的模型上。 在整体来说，我们的方法可以有效地将 ASR 能力传递到 speaker verification 任务中。<details>
<summary>Abstract</summary>
This paper explores the use of ASR-pretrained Conformers for speaker verification, leveraging their strengths in modeling speech signals. We introduce three strategies: (1) Transfer learning to initialize the speaker embedding network, improving generalization and reducing overfitting. (2) Knowledge distillation to train a more flexible speaker verification model, incorporating frame-level ASR loss as an auxiliary task. (3) A lightweight speaker adaptor for efficient feature conversion without altering the original ASR Conformer, allowing parallel ASR and speaker verification. Experiments on VoxCeleb show significant improvements: transfer learning yields a 0.48% EER, knowledge distillation results in a 0.43% EER, and the speaker adaptor approach, with just an added 4.92M parameters to a 130.94M-parameter model, achieves a 0.57% EER. Overall, our methods effectively transfer ASR capabilities to speaker verification tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>使用转移学习初始化说话人嵌入网络，以提高通用性和降低过拟合。2. 使用知识塑化训练更flexible的说话人验证模型，并将帧级ASR损失作为辅助任务。3. 使用轻量级说话人适配器，以实现高效的特征转换而不需要修改原ASR Conformer，允许并行进行ASR和说话人验证。实验结果表明，我们的方法可以有效地将ASR能力传递到说话人验证任务中。* 使用转移学习初始化说话人嵌入网络，EER下降至0.48%。* 使用知识塑化训练更flexible的说话人验证模型，EER下降至0.43%。* 使用轻量级说话人适配器，EER下降至0.57%。总之，我们的方法可以有效地将ASR能力传递到说话人验证任务中，并且可以提高说话人验证模型的通用性和灵活性。</details></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/06/eess.AS_2023_09_06/" data-id="clmjn91pi00e30j883uzk98l3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/06/cs.LG_2023_09_06/" class="article-date">
  <time datetime="2023-09-06T10:00:00.000Z" itemprop="datePublished">2023-09-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/06/cs.LG_2023_09_06/">cs.LG - 2023-09-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Community-Based-Hierarchical-Positive-Unlabeled-PU-Model-Fusion-for-Chronic-Disease-Prediction"><a href="#Community-Based-Hierarchical-Positive-Unlabeled-PU-Model-Fusion-for-Chronic-Disease-Prediction" class="headerlink" title="Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction"></a>Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03386">http://arxiv.org/abs/2309.03386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangwu001/putree">https://github.com/yangwu001/putree</a></li>
<li>paper_authors: Yang Wu, Xurui Li, Xuhong Zhang, Yangyang Kang, Changlong Sun, Xiaozhong Liu</li>
<li>for: 这个研究旨在解决 chronic disease 预测问题，使用 positive-unlabeled 学习方法。</li>
<li>methods: 我们提出了一个 novel Positive-Unlabeled Learning Tree（PUtree）算法，考虑了不同的社区（例如年龄或收入），以提高疾病预测的准确性。我们还提出了一个 hierarchical PU 模型建立方法，以及一个 adversarial PU 风险估计方法，以捕捉阶层 PU 关系。</li>
<li>results: 我们在两个 bencmark 和一个新的 diabetes 预测数据集上证明了 PUtree 的超越性表现，以及其变型版本的表现。<details>
<summary>Abstract</summary>
Positive-Unlabeled (PU) Learning is a challenge presented by binary classification problems where there is an abundance of unlabeled data along with a small number of positive data instances, which can be used to address chronic disease screening problem. State-of-the-art PU learning methods have resulted in the development of various risk estimators, yet they neglect the differences among distinct populations. To address this issue, we present a novel Positive-Unlabeled Learning Tree (PUtree) algorithm. PUtree is designed to take into account communities such as different age or income brackets, in tasks of chronic disease prediction. We propose a novel approach for binary decision-making, which hierarchically builds community-based PU models and then aggregates their deliverables. Our method can explicate each PU model on the tree for the optimized non-leaf PU node splitting. Furthermore, a mask-recovery data augmentation strategy enables sufficient training of the model in individual communities. Additionally, the proposed approach includes an adversarial PU risk estimator to capture hierarchical PU-relationships, and a model fusion network that integrates data from each tree path, resulting in robust binary classification results. We demonstrate the superior performance of PUtree as well as its variants on two benchmarks and a new diabetes-prediction dataset.
</details>
<details>
<summary>摘要</summary>
Positive-Unlabeled（PU）学习是 binary 分类问题中的一个挑战，其中有庞大量的无标签数据以及一小量的正样本，可以用来解决慢性病creening问题。当前的PU学习方法已经导致了不同的风险估计器的开发，但它们忽视了不同人口群体之间的差异。为了解决这个问题，我们提出了一种新的Positive-Unlabeled学习树（PUtree）算法。PUtree 设计了考虑不同年龄或收入水平的社区，在慢性病预测任务中进行分类。我们提出了一种新的二元决策方法，即层次建立社区基于PU模型的 Hierarchical PU 模型，并将其拼接成 robust 的二元分类结果。此外，我们还提出了一种防御PU关系的风险估计器，以及一种数据 fusion 网络，以确保模型在每个树路径上的训练。我们在两个标准benchmark和一个新的 диабе特病预测 dataset 上展示了 PUtree 的superior 性能和其变体。
</details></li>
</ul>
<hr>
<h2 id="ViewMix-Augmentation-for-Robust-Representation-in-Self-Supervised-Learning"><a href="#ViewMix-Augmentation-for-Robust-Representation-in-Self-Supervised-Learning" class="headerlink" title="ViewMix: Augmentation for Robust Representation in Self-Supervised Learning"></a>ViewMix: Augmentation for Robust Representation in Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03360">http://arxiv.org/abs/2309.03360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjon Das, Xin Zhong</li>
<li>for: 本研究旨在提高自动学习方法的表征学习能力。</li>
<li>methods: 本研究使用的是基于共同嵌入架构的自动学习方法，并采用了ViewMix增强策略来提高表征学习能力。</li>
<li>results: 研究表明，采用ViewMix增强策略可以提高自动学习方法的表征学习能力和 robustness。<details>
<summary>Abstract</summary>
Joint Embedding Architecture-based self-supervised learning methods have attributed the composition of data augmentations as a crucial factor for their strong representation learning capabilities. While regional dropout strategies have proven to guide models to focus on lesser indicative parts of the objects in supervised methods, it hasn't been adopted by self-supervised methods for generating positive pairs. This is because the regional dropout methods are not suitable for the input sampling process of the self-supervised methodology. Whereas dropping informative pixels from the positive pairs can result in inefficient training, replacing patches of a specific object with a different one can steer the model from maximizing the agreement between different positive pairs. Moreover, joint embedding representation learning methods have not made robustness their primary training outcome. To this end, we propose the ViewMix augmentation policy, specially designed for self-supervised learning, upon generating different views of the same image, patches are cut and pasted from one view to another. By leveraging the different views created by this augmentation strategy, multiple joint embedding-based self-supervised methodologies obtained better localization capability and consistently outperformed their corresponding baseline methods. It is also demonstrated that incorporating ViewMix augmentation policy promotes robustness of the representations in the state-of-the-art methods. Furthermore, our experimentation and analysis of compute times suggest that ViewMix augmentation doesn't introduce any additional overhead compared to other counterparts.
</details>
<details>
<summary>摘要</summary>
joint embedding architecture-based self-supervised learning方法中，数据增强的组合被认为是关键因素，使得模型具有强的表示学习能力。而区域排除策略已经在指导模型专注于对象中的较少指标部分，但它们没有被自动学习方法采用，因为这些方法不适合自动学习方法的输入采样过程。因为排除有用信息的像素会导致训练不efficient，而将对象中的特定部分替换为不同的像素会使模型尝试最大化不同的正方向对的协调。此外，joint embedding表示学习方法没有做到坚持robustness作为主要训练目标。因此，我们提出了ViewMix增强策略，特地设计为自动学习。在生成不同视图的同时，patches从一个视图中被剪辑并粘贴到另一个视图中。通过利用不同的视图，generated by这种增强策略，我们实现了多种joint embedding-based自动学习方法的更好的局部化能力，并且一直高于对应的基eline方法。此外，我们的实验和分析表明，ViewMix增强策略不会增加计算时间的额外负担。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-linear-interpolators-The-role-of-ensembling"><a href="#Ensemble-linear-interpolators-The-role-of-ensembling" class="headerlink" title="Ensemble linear interpolators: The role of ensembling"></a>Ensemble linear interpolators: The role of ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03354">http://arxiv.org/abs/2309.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingqi Wu, Qiang Sun</li>
<li>for: 本文研究如何 ensemble 稳定化 interpolator，以提高 interpolator 的泛化性能。</li>
<li>methods: 本文使用 bagged linear interpolator 和 multiplier-bootstrap-based bagged least square estimator，以及类 bootstrap 和 Bernoulli bootstrap。</li>
<li>results: 研究发现，bagging 可以有效地减少 interpolator 的偏差，从而实现 bounded 的 out-of-sample prediction risk。此外， authors 还发现 bagging  acted as an implicit regularization，并与其 explicitly regularized counterpart 相当。<details>
<summary>Abstract</summary>
Interpolators are unstable. For example, the mininum $\ell_2$ norm least square interpolator exhibits unbounded test errors when dealing with noisy data. In this paper, we study how ensemble stabilizes and thus improves the generalization performance, measured by the out-of-sample prediction risk, of an individual interpolator. We focus on bagged linear interpolators, as bagging is a popular randomization-based ensemble method that can be implemented in parallel. We introduce the multiplier-bootstrap-based bagged least square estimator, which can then be formulated as an average of the sketched least square estimators. The proposed multiplier bootstrap encompasses the classical bootstrap with replacement as a special case, along with a more intriguing variant which we call the Bernoulli bootstrap.   Focusing on the proportional regime where the sample size scales proportionally with the feature dimensionality, we investigate the out-of-sample prediction risks of the sketched and bagged least square estimators in both underparametrized and overparameterized regimes. Our results reveal the statistical roles of sketching and bagging. In particular, sketching modifies the aspect ratio and shifts the interpolation threshold of the minimum $\ell_2$ norm estimator. However, the risk of the sketched estimator continues to be unbounded around the interpolation threshold due to excessive variance. In stark contrast, bagging effectively mitigates this variance, leading to a bounded limiting out-of-sample prediction risk. To further understand this stability improvement property, we establish that bagging acts as a form of implicit regularization, substantiated by the equivalence of the bagged estimator with its explicitly regularized counterpart. We also discuss several extensions.
</details>
<details>
<summary>摘要</summary>
interpolators 不稳定。例如，最小二乘法最小值 interpolator 对噪音数据处理时会表现出无限大的测试错误。在这篇论文中，我们研究如何 ensemble 使 interpolator 更加稳定，从而提高个体 interpolator 的泛化性能， measured by out-of-sample prediction risk。我们关注 bagged linear interpolators，因为 bagging 是一种流行的随机化基于ensemble方法，可以并行实现。我们介绍了 multiplier-bootstrap-based bagged least square estimator，可以表示为一系列sketched least square estimators的平均值。我们在 proportionate 模式下（即样本大小与特征维度成正比） investigate out-of-sample prediction risks of sketched and bagged least square estimators in both underparametrized and overparameterized regimes。我们的结果表明 sketching 和 bagging 的统计作用。特别是，sketching 改变了 minimum $\ell_2$ norm estimator 的方向比例和插值阈值，但是风险仍然是无限大，尤其是在插值阈值附近。与此相反，bagging 能够有效减少风险，导致 bounded 的外样预测风险。为了更好地理解这种稳定性改善性质，我们证明了 bagging 是一种含义隐藏的正则化，并且通过显式正则化的等价性证明了这一点。我们还讨论了一些扩展。
</details></li>
</ul>
<hr>
<h2 id="Source-Camera-Identification-and-Detection-in-Digital-Videos-through-Blind-Forensics"><a href="#Source-Camera-Identification-and-Detection-in-Digital-Videos-through-Blind-Forensics" class="headerlink" title="Source Camera Identification and Detection in Digital Videos through Blind Forensics"></a>Source Camera Identification and Detection in Digital Videos through Blind Forensics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03353">http://arxiv.org/abs/2309.03353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Venkata Udaya Sameer, Shilpa Mukhopadhyay, Ruchira Naskar, Ishaan Dali</li>
<li>for: 本研究旨在透过机器学习特征提取、特征选择和后续的来源分类，确定影片来源的身份，以解决数位影片的来源识别问题。</li>
<li>methods: 本研究使用机器学习特征提取、特征选择和来源分类的方法来解决数位影片的来源识别问题。</li>
<li>results: 我们的实验结果显示，与传统指纹基本法相比，机器学习特征提取和来源分类方法更高效地解决数位影片的来源识别问题。<details>
<summary>Abstract</summary>
Source camera identification in digital videos is the problem of associating an unknown digital video with its source device, within a closed set of possible devices. The existing techniques in source detection of digital videos try to find a fingerprint of the actual source in the video in form of PRNU (Photo Response Non--Uniformity), and match it against the SPN (Sensor Pattern Noise) of each possible device. The highest correlation indicates the correct source. We investigate the problem of identifying a video source through a feature based approach using machine learning. In this paper, we present a blind forensic technique of video source authentication and identification, based on feature extraction, feature selection and subsequent source classification. The main aim is to determine whether a claimed source for a video is actually its original source. If not, we identify its original source. Our experimental results prove the efficiency of the proposed method compared to traditional fingerprint based technique.
</details>
<details>
<summary>摘要</summary>
源 Camera 识别在数字视频中是一个问题，即将未知的数字视频与其源设备相关联，在一个封闭的可能性集中。现有的数字视频来源检测技术会找到视频中的实际来源指纹（PRNU），并将其与每个可能的设备的感知模式噪声（SPN）进行匹配。最高匹配指示正确的源。我们研究了通过机器学习特征基本方法来实现视频源认证和识别。在这篇论文中，我们提出了一种盲目的视频源认证和识别技术，基于特征提取、特征选择和后续源分类。主要目标是判断一个视频的来源是否实际上是其原始来源。如果不是，我们就可以确定其原始来源。我们的实验结果表明，我们提出的方法比传统的指纹基本技术更高效。
</details></li>
</ul>
<hr>
<h2 id="Using-Neural-Networks-for-Fast-SAR-Roughness-Estimation-of-High-Resolution-Images"><a href="#Using-Neural-Networks-for-Fast-SAR-Roughness-Estimation-of-High-Resolution-Images" class="headerlink" title="Using Neural Networks for Fast SAR Roughness Estimation of High Resolution Images"></a>Using Neural Networks for Fast SAR Roughness Estimation of High Resolution Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03351">http://arxiv.org/abs/2309.03351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeovafarias/sar-roughness-estimation-neural-nets">https://github.com/jeovafarias/sar-roughness-estimation-neural-nets</a></li>
<li>paper_authors: Li Fan, Jeova Farias Sales Rocha Neto</li>
<li>for: 这个论文的目的是提出一种基于神经网络的高分辨率Synthetic Aperture Radar（SAR）图像处理方法，以便快速和可靠地估计SAR图像中的粗糙度参数。</li>
<li>methods: 这个方法使用神经网络学习模型来预测G_I^0分布下的参数，然后使用这些参数来估计SAR图像中的粗糙度。</li>
<li>results: 研究发现，使用这种神经网络基于的估计方法可以快速、准确地估计SAR图像中的粗糙度，并且可以在实时中进行像素级粗糙度估计，而且可以使用一个简单的网络来实现这一点。<details>
<summary>Abstract</summary>
The analysis of Synthetic Aperture Radar (SAR) imagery is an important step in remote sensing applications, and it is a challenging problem due to its inherent speckle noise. One typical solution is to model the data using the $G_I^0$ distribution and extract its roughness information, which in turn can be used in posterior imaging tasks, such as segmentation, classification and interpretation. This leads to the need of quick and reliable estimation of the roughness parameter from SAR data, especially with high resolution images. Unfortunately, traditional parameter estimation procedures are slow and prone to estimation failures. In this work, we proposed a neural network-based estimation framework that first learns how to predict underlying parameters of $G_I^0$ samples and then can be used to estimate the roughness of unseen data. We show that this approach leads to an estimator that is quicker, yields less estimation error and is less prone to failures than the traditional estimation procedures for this problem, even when we use a simple network. More importantly, we show that this same methodology can be generalized to handle image inputs and, even if trained on purely synthetic data for a few seconds, is able to perform real time pixel-wise roughness estimation for high resolution real SAR imagery.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "The analysis of Synthetic Aperture Radar (SAR) imagery is an important step in remote sensing applications, and it is a challenging problem due to its inherent speckle noise. One typical solution is to model the data using the $G_I^0$ distribution and extract its roughness information, which in turn can be used in posterior imaging tasks, such as segmentation, classification and interpretation. This leads to the need of quick and reliable estimation of the roughness parameter from SAR data, especially with high resolution images. Unfortunately, traditional parameter estimation procedures are slow and prone to estimation failures. In this work, we proposed a neural network-based estimation framework that first learns how to predict underlying parameters of $G_I^0$ samples and then can be used to estimate the roughness of unseen data. We show that this approach leads to an estimator that is quicker, yields less estimation error and is less prone to failures than the traditional estimation procedures for this problem, even when we use a simple network. More importantly, we show that this same methodology can be generalized to handle image inputs and, even if trained on purely synthetic data for a few seconds, is able to perform real time pixel-wise roughness estimation for high resolution real SAR imagery."中文翻译：<<SYS>>Synthetic Aperture Radar（SAR）成像分析是远程感知应用中的一个重要步骤，但它受到自然的折射噪声的限制。一种常见的解决方案是使用 $G_I^0$ 分布来模型数据，并从其中提取粗糙信息，以便在后续图像处理任务中使用，如分 segmentation、分类和解释等。这导致了高分辨率图像中很快获得粗糙参数的估计的需求。然而，传统的估计方法过于慢且容易出现估计错误。在这种情况下，我们提出了一种基于神经网络的估计框架，它首先学习了 $G_I^0$ 样本下的下面参数，然后可以用来估计未经见过的数据中的粗糙。我们表明，这种方法比传统的估计方法快速、估计误差小于、失败率较低。此外，我们还证明了这种方法可以普遍应用于图像输入，即使只有几秒钟的synthetic数据训练，也可以在实时ixel-wise粗糙估计中心高分辨率实际SAR成像数据。
</details></li>
</ul>
<hr>
<h2 id="REBOOT-Reuse-Data-for-Bootstrapping-Efficient-Real-World-Dexterous-Manipulation"><a href="#REBOOT-Reuse-Data-for-Bootstrapping-Efficient-Real-World-Dexterous-Manipulation" class="headerlink" title="REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation"></a>REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03322">http://arxiv.org/abs/2309.03322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Hu, Aaron Rovinsky, Jianlan Luo, Vikash Kumar, Abhishek Gupta, Sergey Levine</li>
<li>for: 学习灵活的抓取技能，以提高机器人手臂在实际世界中的操作能力。</li>
<li>methods: 利用近期的可效RL和重播缓存启动技术，将不同任务或物品的数据作为新任务的启动点，大幅提高学习效率。</li>
<li>results: 在实际世界中使用四 fingers 机器人手臂快速学习复杂的抓取技能，并完成实际训练周期，无需人工重置和奖励工程。<details>
<summary>Abstract</summary>
Dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. Reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a starting point for training new tasks, significantly improving learning efficiency. Additionally, our system completes the real-world training cycle by incorporating learned resets via an imitation-based pickup policy as well as learned reward functions, eliminating the need for manual resets and reward engineering. We demonstrate the benefits of reusing past data as replay buffer initialization for new tasks, for instance, the fast acquisition of intricate manipulation skills in the real world on a four-fingered robotic hand. (Videos: https://sites.google.com/view/reboot-dexterous)
</details>
<details>
<summary>摘要</summary>
dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a starting point for training new tasks, significantly improving learning efficiency. Additionally, our system completes the real-world training cycle by incorporating learned resets via an imitation-based pickup policy as well as learned reward functions, eliminating the need for manual resets and reward engineering. We demonstrate the benefits of reusing past data as replay buffer initialization for new tasks, for instance, the fast acquisition of intricate manipulation skills in the real world on a four-fingered robotic hand. (Videos: https://sites.google.com/view/reboot-dexterous)Here's the translation in Traditional Chinese:dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a starting point for training new tasks, significantly improving learning efficiency. Additionally, our system completes the real-world training cycle by incorporating learned resets via an imitation-based pickup policy as well as learned reward functions, eliminating the need for manual resets and reward engineering. We demonstrate the benefits of reusing past data as replay buffer initialization for new tasks, for instance, the fast acquisition of intricate manipulation skills in the real world on a four-fingered robotic hand. (Videos: https://sites.google.com/view/reboot-dexterous)
</details></li>
</ul>
<hr>
<h2 id="Fitness-Approximation-through-Machine-Learning"><a href="#Fitness-Approximation-through-Machine-Learning" class="headerlink" title="Fitness Approximation through Machine Learning"></a>Fitness Approximation through Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03318">http://arxiv.org/abs/2309.03318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/itaitzruia4/approxml">https://github.com/itaitzruia4/approxml</a></li>
<li>paper_authors: Itai Tzruia, Tomer Halperin, Moshe Sipper, Achiya Elyasaf</li>
<li>for: 这个论文主要目标是提出一种基于机器学习模型的遗传算法中的健康估计方法，以优化遗传算法的运行效率。</li>
<li>methods: 该方法使用机器学习模型来保持一个遗传算法中个体的健康估计，并在进行遗传算法的演化运行中不断更新该模型。</li>
<li>results: 实验结果表明，使用该方法可以显著提高遗传算法的运行效率，并且fitness分数与实际遗传算法的fitness分数相似或者只有轻微差异。<details>
<summary>Abstract</summary>
We present a novel approach to performing fitness approximation in genetic algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary agents in Gymnasium (game) simulators -- where fitness computation is costly. Maintaining a dataset of sampled individuals along with their actual fitness scores, we continually update throughout an evolutionary run a fitness-approximation ML model. We compare different methods for: 1) switching between actual and approximate fitness, 2) sampling the population, and 3) weighting the samples. Experimental findings demonstrate significant improvement in evolutionary runtimes, with fitness scores that are either identical or slightly lower than that of the fully run GA -- depending on the ratio of approximate-to-actual-fitness computation. Our approach is generic and can be easily applied to many different domains.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用机器学习（ML）模型来实现遗传算法（GA）中的健康近似计算，专注于在游戏模拟器（Gymnasium）中的EVOLUTIONARY AGENTS，其中健康计算成本高。我们在EVOLUTIONARY RUN期间不断更新一个包含采样个体以及其实际健康分数的数据集。我们比较了不同的方法来：1）在actual和approximate fitness之间切换，2）采样人口，和3）Weighting the samples。实验结果表明，我们的方法可以显著减少EVOLUTIONARY RUN时间，并且fitness分数与完全运行GA的fitness分数相似或者略低，具体取决于approximate-to-actual fitness computation的比例。我们的方法可以应用到多个领域。
</details></li>
</ul>
<hr>
<h2 id="Robotic-Table-Tennis-A-Case-Study-into-a-High-Speed-Learning-System"><a href="#Robotic-Table-Tennis-A-Case-Study-into-a-High-Speed-Learning-System" class="headerlink" title="Robotic Table Tennis: A Case Study into a High Speed Learning System"></a>Robotic Table Tennis: A Case Study into a High Speed Learning System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03315">http://arxiv.org/abs/2309.03315</a></li>
<li>repo_url: None</li>
<li>paper_authors: David B. D’Ambrosio, Jonathan Abelian, Saminda Abeyruwan, Michael Ahn, Alex Bewley, Justin Boyd, Krzysztof Choromanski, Omar Cortes, Erwin Coumans, Tianli Ding, Wenbo Gao, Laura Graesser, Atil Iscen, Navdeep Jaitly, Deepali Jain, Juhana Kangaspunta, Satoshi Kataoka, Gus Kouretas, Yuheng Kuang, Nevena Lazic, Corey Lynch, Reza Mahjourian, Sherry Q. Moore, Thinh Nguyen, Ken Oslund, Barney J Reed, Krista Reymann, Pannag R. Sanketi, Anish Shankar, Pierre Sermanet, Vikas Sindhwani, Avi Singh, Vincent Vanhoucke, Grace Vesom, Peng Xu</li>
<li>for: 这个论文是为了描述一个现实世界中的机器人学习系统，该系统在之前的研究中已经能够与人类进行了数百场网球比赛，并且能够准确地返回球到 DESIRED 目标。</li>
<li>methods: 这个系统使用了一个高度优化的感知子系统，高速低延迟的机器人控制器，一种可以避免实际世界中的损害并用于零拟合转移的 simulations  paradigm，以及自动化的实际世界环境重置，以便在物理机器人上进行自主的训练和评估。</li>
<li>results: 论文的结果表明，通过使用这些技术，该系统能够在实际世界中进行自主的训练和评估，并且可以在不同的环境下进行适应和复杂的任务。<details>
<summary>Abstract</summary>
We present a deep-dive into a real-world robotic learning system that, in previous work, was shown to be capable of hundreds of table tennis rallies with a human and has the ability to precisely return the ball to desired targets. This system puts together a highly optimized perception subsystem, a high-speed low-latency robot controller, a simulation paradigm that can prevent damage in the real world and also train policies for zero-shot transfer, and automated real world environment resets that enable autonomous training and evaluation on physical robots. We complement a complete system description, including numerous design decisions that are typically not widely disseminated, with a collection of studies that clarify the importance of mitigating various sources of latency, accounting for training and deployment distribution shifts, robustness of the perception system, sensitivity to policy hyper-parameters, and choice of action space. A video demonstrating the components of the system and details of experimental results can be found at https://youtu.be/uFcnWjB42I0.
</details>
<details>
<summary>摘要</summary>
我们对一个实际应用的机器人学习系统进行深入探讨，之前的研究已经证明该系统可以与人类进行百场乒乓球比赛，并且具有准确返回球的目标能力。该系统结合了高度优化的感知子系统、高速低延迟的机器人控制器、可预防物理世界中的损害并训练政策的 simulate  парадигма，以及自动化的实际环境重置，允许无人控制和评估实际机器人。我们附加了完整的系统描述，包括许多不常公布的设计决策，以及一系列研究，以解释mitigating 多种延迟的重要性、训练和部署分布shift 的考虑、感知系统的Robustness、策略参数的敏感性和行为空间的选择。视频展示了系统的组件和实验结果的细节，可以在 https://youtu.be/uFcnWjB42I0 找到。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Learning-of-Intrusion-Responses-through-Recursive-Decomposition"><a href="#Scalable-Learning-of-Intrusion-Responses-through-Recursive-Decomposition" class="headerlink" title="Scalable Learning of Intrusion Responses through Recursive Decomposition"></a>Scalable Learning of Intrusion Responses through Recursive Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03292">http://arxiv.org/abs/2309.03292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kim Hammar, Rolf Stadler</li>
<li>for: 本研究旨在Automated Intrusion Response（AIR）技术的研究和IT基础设施中的攻击者和 защитник之间的互动作为一个偏见的随机游戏。</li>
<li>methods: 我们采用了一种基于强化学习和自我玩家的方法，其中攻击和防御策略在自我玩家中互相演化，从而到达一个平衡点。</li>
<li>results: 我们的实验结果显示，使用我们提出的DFSP算法可以在一个真实的基础设施配置下实现高效地学习和实现平衡策略。在虚拟环境中，我们发现了学习的策略在真实的攻击和应急回应中表现良好，并且DFSP算法在比较实际的基础设施配置下显示出了显著的优势。<details>
<summary>Abstract</summary>
We study automated intrusion response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed stochastic game. To solve the game we follow an approach where attack and defense strategies co-evolve through reinforcement learning and self-play toward an equilibrium. Solutions proposed in previous work prove the feasibility of this approach for small infrastructures but do not scale to realistic scenarios due to the exponential growth in computational complexity with the infrastructure size. We address this problem by introducing a method that recursively decomposes the game into subgames which can be solved in parallel. Applying optimal stopping theory we show that the best response strategies in these subgames exhibit threshold structures, which allows us to compute them efficiently. To solve the decomposed game we introduce an algorithm called Decompositional Fictitious Self-Play (DFSP), which learns Nash equilibria through stochastic approximation. We evaluate the learned strategies in an emulation environment where real intrusions and response actions can be executed. The results show that the learned strategies approximate an equilibrium and that DFSP significantly outperforms a state-of-the-art algorithm for a realistic infrastructure configuration.
</details>
<details>
<summary>摘要</summary>
我们研究自动化入侵应急应对措施，将攻击者和防御者的互动视为部分可见随机游戏。为解决这个游戏，我们采用一种方法，其中攻击和防御策略通过自适应学习和自我玩家相互演化而往往达到均衡。在过去的研究中，我们证明了这种方法对小规模基础设施是可行的，但是在实际场景中，由基础设施规模带来的计算复杂性呈指数增长，使得这种方法无法承受。为解决这个问题，我们提出了一种方法，即 recursively decomposing the game into subgames，可以并行解决。通过优化停止理论，我们显示了攻击和防御策略在这些子游戏中具有阈值结构，从而可以高效地计算它们。为解决分解后的游戏，我们提出了一种算法called Decompositional Fictitious Self-Play (DFSP)，它通过随机方法学习达到均衡。我们在一个模拟环境中测试了学习的策略，并发现它们接近均衡，并且DFSP在一个现实主义基础设施配置下显著超过了当前状态艺术 Algorithm的性能。
</details></li>
</ul>
<hr>
<h2 id="R2D2-Deep-neural-network-series-for-near-real-time-high-dynamic-range-imaging-in-radio-astronomy"><a href="#R2D2-Deep-neural-network-series-for-near-real-time-high-dynamic-range-imaging-in-radio-astronomy" class="headerlink" title="R2D2: Deep neural network series for near real-time high-dynamic range imaging in radio astronomy"></a>R2D2: Deep neural network series for near real-time high-dynamic range imaging in radio astronomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03291">http://arxiv.org/abs/2309.03291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aghabiglou A, Chu C S, Jackson A, Dabbech A, Wiaux Y</li>
<li>for: 这个论文是用于描述一种基于深度学习和数据一致更新的高分辨率高动态范围成像技术，用于天文学中的电磁波探测。</li>
<li>methods: 这种技术使用了混合深度学习神经网络（DNN）和数据一致更新来实现高分辨率高动态范围成像。它的重建是建立为一系列的差异图像，每个差异图像是DNN的输出。</li>
<li>results: 论文通过对有敏感观测数据的测试和比较，发现这种技术可以提供高精度成像，并且比CLEAN和其他两种 Algorithms（AIRI和uSARA）更快速和有更低的计算成本。<details>
<summary>Abstract</summary>
We present a novel AI approach for high-resolution high-dynamic range synthesis imaging by radio interferometry (RI) in astronomy. R2D2, standing for "{R}esidual-to-{R}esidual {D}NN series for high-{D}ynamic range imaging", is a model-based data-driven approach relying on hybrid deep neural networks (DNNs) and data-consistency updates. Its reconstruction is built as a series of residual images estimated as the outputs of DNNs, each taking the residual dirty image of the previous iteration as an input. The approach can be interpreted as a learned version of a matching pursuit approach, whereby model components are iteratively identified from residual dirty images, and of which CLEAN is a well-known example. We propose two variants of the R2D2 model, built upon two distinctive DNN architectures: a standard U-Net, and a novel unrolled architecture. We demonstrate their use for monochromatic intensity imaging on highly-sensitive observations of the radio galaxy Cygnus~A at S band, from the Very Large Array (VLA). R2D2 is validated against CLEAN and the recent RI algorithms AIRI and uSARA, which respectively inject a learned implicit regularization and an advanced handcrafted sparsity-based regularization into the RI data. With only few terms in its series, the R2D2 model is able to deliver high-precision imaging, significantly superior to CLEAN and matching the precision of AIRI and uSARA. In terms of computational efficiency, R2D2 runs at a fraction of the cost of AIRI and uSARA, and is also faster than CLEAN, opening the door to real-time precision imaging in RI.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的人工智能方法，用于高分辨率高动态范围成像探测（RI）在天文学中。我们称之为“R2D2”，即“差异-差异神经网络序列 для高动态范围成像”。这是一种基于混合深度神经网络（DNN）和数据一致更新的模型驱动方法。它的重建是建立为一系列的差异图像，每个图像是DNN的输出。这些图像可以看作是对差异废弃图像的学习版本，其中CLEAN是一个常见的例子。我们提出了两种R2D2模型，基于两种不同的DNN架构：标准的U-Net和一种新的卷积架构。我们在高敏感的S带观测数据上用这些模型进行灰度成像，并与CLEAN和最近的RI算法AIRI和uSARA进行比较。 results show that R2D2可以提供高精度的成像，与AIRI和uSARA的精度相当，且在计算效率方面更高，只需几个序列。这使得实时高精度成像在RI中成为可能。
</details></li>
</ul>
<hr>
<h2 id="Let-Quantum-Neural-Networks-Choose-Their-Own-Frequencies"><a href="#Let-Quantum-Neural-Networks-Choose-Their-Own-Frequencies" class="headerlink" title="Let Quantum Neural Networks Choose Their Own Frequencies"></a>Let Quantum Neural Networks Choose Their Own Frequencies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03279">http://arxiv.org/abs/2309.03279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Jaderberg, Antonio A. Gentile, Youssef Achari Berrada, Elvira Shishenina, Vincent E. Elfving</li>
<li>for: 这篇论文旨在探讨Parameterized quantum circuits作为机器学习模型，以及其中的代表性 Fourier 系列。</li>
<li>methods: 论文提出了一种通过添加可调参数到数据编码器中，来实现可调频率（TF）量子模型。</li>
<li>results: 数字实验表明，TF 模型可以学习拥有恰当性和弹性频谱特性的生成器，并且在解决 Navier-Stokes 方程中显示出了改善的准确率。<details>
<summary>Abstract</summary>
Parameterized quantum circuits as machine learning models are typically well described by their representation as a partial Fourier series of the input features, with frequencies uniquely determined by the feature map's generator Hamiltonians. Ordinarily, these data-encoding generators are chosen in advance, fixing the space of functions that can be represented. In this work we consider a generalization of quantum models to include a set of trainable parameters in the generator, leading to a trainable frequency (TF) quantum model. We numerically demonstrate how TF models can learn generators with desirable properties for solving the task at hand, including non-regularly spaced frequencies in their spectra and flexible spectral richness. Finally, we showcase the real-world effectiveness of our approach, demonstrating an improved accuracy in solving the Navier-Stokes equations using a TF model with only a single parameter added to each encoding operation. Since TF models encompass conventional fixed frequency models, they may offer a sensible default choice for variational quantum machine learning.
</details>
<details>
<summary>摘要</summary>
parameterized quantum circuits as machine learning models 通常可以用幂 fourier 系列来描述输入特征，频率唯一由特征映射生成器的哈密顿ians决定。通常，这些数据编码生成器会在先前选择，确定可以表示的函数空间。在这种工作中，我们考虑了一种扩展量子模型，即含有可训练参数的生成器，导致可训练频率（TF）量子模型。我们数值展示了TF模型可以学习生成器具有感兴趣的性质，包括非Regularly spaced 频谱和灵活的 спектral richness。最后，我们展示了我们的方法的实际效果，通过在 Navier-Stokes 方程中使用TF模型，只有每个编码操作中添加一个参数，提高解决问题的准确性。由于TF模型包括 fixede frequency 模型，它们可能会成为变量量子机器学习的合适默认选择。
</details></li>
</ul>
<hr>
<h2 id="Matcha-TTS-A-fast-TTS-architecture-with-conditional-flow-matching"><a href="#Matcha-TTS-A-fast-TTS-architecture-with-conditional-flow-matching" class="headerlink" title="Matcha-TTS: A fast TTS architecture with conditional flow matching"></a>Matcha-TTS: A fast TTS architecture with conditional flow matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03199">http://arxiv.org/abs/2309.03199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivam Mehta, Ruibo Tu, Jonas Beskow, Éva Székely, Gustav Eje Henter</li>
<li>for: 这个论文是为了提出一种新的编码器-解码器架构，用于快速的语音合成模型训练。</li>
<li>methods: 这个方法使用最优运输 conditional flow匹配（OT-CFM）来训练一个基于ODE的解码器，能够在 fewer synthesis steps 中实现高质量输出。</li>
<li>results: 与强制搜索基线模型相比，这个系统具有最小的内存占用量，在长语音上具有与最快模型相当的速度，并在听力测试中获得最高意见分。Please note that the abstract is in English, so the Chinese translation may not be perfect.<details>
<summary>Abstract</summary>
We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest models on long utterances, and attains the highest mean opinion score in a listening test. Please see https://shivammehta25.github.io/Matcha-TTS/ for audio examples, code, and pre-trained models.
</details>
<details>
<summary>摘要</summary>
我们介绍Matcha-TTS，一种新的编码器-解码器架构，用于快速的语音合成模型训练，使用最佳交通流匹配（OT-CFM）。这种方法使得decoder可以在 fewer synthesis steps 中达到高质量输出，比使用 scored matching 训练的模型更快。我们在设计中也特别注重了每个合成步骤的运行速度。Matcha-TTS 是一种 probabilistic 、非自适应的语音合成系统，可以从零开始学习，不需要外部对齐。相比强大的预训练基线模型，Matcha-TTS 系统具有最小的内存占用量，可以在长句子上匹配最快的模型，并在听力测试中获得最高的意见分。请参考 https://shivammehta25.github.io/Matcha-TTS/ 获取音频示例、代码和预训练模型。
</details></li>
</ul>
<hr>
<h2 id="Blink-Link-Local-Differential-Privacy-in-Graph-Neural-Networks-via-Bayesian-Estimation"><a href="#Blink-Link-Local-Differential-Privacy-in-Graph-Neural-Networks-via-Bayesian-Estimation" class="headerlink" title="Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation"></a>Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03190">http://arxiv.org/abs/2309.03190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhxchd/blink_gnn">https://github.com/zhxchd/blink_gnn</a></li>
<li>paper_authors: Xiaochen Zhu, Vincent Y. F. Tan, Xiaokui Xiao</li>
<li>for: 增强图像推理任务中节点嵌入的能力，但训练可能会涉及隐私问题。</li>
<li>methods: 使用分布式节点的链接地方ifferential privacy，协作不可信服务器训练Graph Neural Networks（GNNs），无需披露图像中任务的链接存在。</li>
<li>results: 我们的方法可以更好地减少隐私影响GNNs的准确性，并提供了不同隐私预算下的两种变体，能够在不同的隐私环境下表现更好。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have gained an increasing amount of popularity due to their superior capability in learning node embeddings for various graph inference tasks, but training them can raise privacy concerns. To address this, we propose using link local differential privacy over decentralized nodes, enabling collaboration with an untrusted server to train GNNs without revealing the existence of any link. Our approach spends the privacy budget separately on links and degrees of the graph for the server to better denoise the graph topology using Bayesian estimation, alleviating the negative impact of LDP on the accuracy of the trained GNNs. We bound the mean absolute error of the inferred link probabilities against the ground truth graph topology. We then propose two variants of our LDP mechanism complementing each other in different privacy settings, one of which estimates fewer links under lower privacy budgets to avoid false positive link estimates when the uncertainty is high, while the other utilizes more information and performs better given relatively higher privacy budgets. Furthermore, we propose a hybrid variant that combines both strategies and is able to perform better across different privacy budgets. Extensive experiments show that our approach outperforms existing methods in terms of accuracy under varying privacy budgets.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 在不同的推理任务中学习节点嵌入的能力得到了越来越多的推广，但是训练它们可能会引起隐私问题。为了解决这个问题，我们提出了使用分布式节点的链接本地敏感度保护（LDP），允许不可信服务器在训练 GNNs 时不 revel 链接的存在。我们的方法将隐私预算分配给链接和图度的两个部分，使服务器通过浮点估计来更好地净化图像 topology，从而减轻LDP对训练 GNNs 的影响。我们 bounds 推断链接概率的平均绝对误差与真实图像 topology 之间的差异。此外，我们还提出了两种不同的隐私设置下的LDP机制，其中一种在低隐私预算下更少地估计链接，以避免高度不确定性时的假阳性链接估计；另一种则更加利用信息，在相对较高的隐私预算下表现更好。 finally，我们提出了这两种机制的混合变体，可以在不同的隐私预算下表现更好。我们的实验表明，我们的方法在不同的隐私预算下可以比既有方法更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="SLiMe-Segment-Like-Me"><a href="#SLiMe-Segment-Like-Me" class="headerlink" title="SLiMe: Segment Like Me"></a>SLiMe: Segment Like Me</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03179">http://arxiv.org/abs/2309.03179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aliasghar Khani, Saeid Asgari Taghanaki, Aditya Sanghi, Ali Mahdavi Amiri, Ghassan Hamarneh</li>
<li>for: 这个研究用于提出一个单一示例的图像分割方法，使用大量的感知语言模型，例如稳定扩散（SD），以达到高精度的图像分割。</li>
<li>methods: 这个方法使用SD的优化器，将图像和其分割标识对应到彼此的数值表示，然后将这些数值表示优化为每个分割区域的图像特征。</li>
<li>results: 实验结果显示，SLiMe可以在单一示例下进行图像分割，并且在几个shot的情况下进行改进，得到更高的精度和可靠性。此外，SLiMe比其他一阶和几阶分割方法表现更好。<details>
<summary>Abstract</summary>
Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segment any real-world image during inference with the granularity of the segmented region in the training image, using just one example. Moreover, leveraging additional training data when available, i.e. few-shot, improves the performance of SLiMe. We carried out a knowledge-rich set of experiments examining various design factors and showed that SLiMe outperforms other existing one-shot and few-shot segmentation methods.
</details>
<details>
<summary>摘要</summary>
significanthavebeenmadeusingslargelocation-language models,likeStableDiffusion(SD),fordownstreampurposes,includingimageediting,imagecorrespondence,and3Dshapegeneration.Inspiredbytheseadvances,weexploreleveragingtheseextensivevision-language modelsforsegmentingimagesatanydesiredgranularityusingasfewasoneannotatedsamplebyproposingSLiMe.SLiMeframesthisproblemasanoptimizationtask.Specifically,givenasingletrainingimageanditssegmentationmask,wefirstextractattentionmaps,includingournovel"weightedaccumulatedself-attentionmap"fromtheSDprior.Then,usingtheextractedattentionmaps,thetextembeddingsofStableDiffusionareoptimizedsuchthat,eachofthem,learnaboutasinglesegmentedregionfromthetrainingimage.Theselearnedembeddingsthenhighlightthesegmentedregionintheattentionmaps,whichinturncanthenbederivedfromthe segmentationmap.ThisenablesSLiMe to segmentanyreal-worldimage duringinferencewiththegranularityofthesegmentedregioninthe trainingimage,usingjustoneexample.Moreover,leveragingadditionaltrainingdatawhenavailable,i.e.few-shot,improves theperformanceofSLiMe.We carriedoutaknowledgerichsetofexperiments examiningvariousdesigndetailsandshowedthatSLiMeoutperformsother existingoneshotandfew-shotsegmentationmethods.
</details></li>
</ul>
<hr>
<h2 id="Temporal-Inductive-Path-Neural-Network-for-Temporal-Knowledge-Graph-Reasoning"><a href="#Temporal-Inductive-Path-Neural-Network-for-Temporal-Knowledge-Graph-Reasoning" class="headerlink" title="Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning"></a>Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03251">http://arxiv.org/abs/2309.03251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Dong, Pengyang Wang, Meng Xiao, Zhiyuan Ning, Pengfei Wang, Yuanchun Zhou</li>
<li>for: 本文旨在提出一种基于历史信息的时间知识图（TKG）推理方法，以便预测未来事件。</li>
<li>methods: 本文提出了一种基于历史信息的时间induction neural network（TiPNN）模型，该模型通过一个统一的历史temporal图来捕捉和嵌入历史信息，然后通过定义的查询意识 temporal paths来模型历史路径信息相关于查询。</li>
<li>results: 经验表明，提出的模型不仅实现了显著性能提升，还能够处理 inductive 设置，并且可以通过历史temporal图提供推理证据。<details>
<summary>Abstract</summary>
Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to comprehensively capture and encapsulate information from history. Subsequently, we utilize the defined query-aware temporal paths to model historical path information related to queries on history temporal graph for the reasoning. Extensive experiments illustrate that the proposed model not only attains significant performance enhancements but also handles inductive settings, while additionally facilitating the provision of reasoning evidence through history temporal graphs.
</details>
<details>
<summary>摘要</summary>
Temporal Knowledge Graph (TKG) 是传统知识图 (KG) 的扩展，它包含时间维度。这些知识图中的推理是一个关键的任务，旨在预测未来事实基于过去发生的事件。主要挑战在于发掘历史子图中的结构依赖和时间对称。现有的方法通常透过实体模型来建模 TKG，但在实际情况中，有着数量繁多的实体，且时间进程中新增的实体可能会增加。这使得实体依赖的方法对于处理大量实体和时间进程中新增的实体而具有挑战。因此，我们提出了时间导引路径神经网 (TiPNN)，它在实体独立的角度下模型历史信息。具体来说，TiPNN 使用了一个统一的图，即历史时间图，以全面捕捉和储存历史信息。然后，我们利用定义的查询相关时间路径来模型历史时间图中相关查询的路径信息。实验结果显示，我们的方法不仅实现了重要的性能提升，而且可以处理导入设定，同时还可以提供推理证据。
</details></li>
</ul>
<hr>
<h2 id="3D-Object-Positioning-Using-Differentiable-Multimodal-Learning"><a href="#3D-Object-Positioning-Using-Differentiable-Multimodal-Learning" class="headerlink" title="3D Object Positioning Using Differentiable Multimodal Learning"></a>3D Object Positioning Using Differentiable Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03177">http://arxiv.org/abs/2309.03177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Zanyk-McLean, Krishna Kumar, Paul Navratil</li>
<li>for: 该论文旨在优化计算机图形场景中对观察者或参照物的对象位置。</li>
<li>methods: 该论文使用了模拟的激光探测数据和可微分渲染来优化对象位置，并使用了两种感知模式（图像像素损失和激光探测）来加速 converges。</li>
<li>results: 该论文显示了将两种感知模式融合在一起可以更快地 converges 对象位置，这种方法可能对自动驾驶汽车的训练和Scene Understanding有很好的应用。<details>
<summary>Abstract</summary>
This article describes a multi-modal method using simulated Lidar data via ray tracing and image pixel loss with differentiable rendering to optimize an object's position with respect to an observer or some referential objects in a computer graphics scene. Object position optimization is completed using gradient descent with the loss function being influenced by both modalities. Typical object placement optimization is done using image pixel loss with differentiable rendering only, this work shows the use of a second modality (Lidar) leads to faster convergence. This method of fusing sensor input presents a potential usefulness for autonomous vehicles, as these methods can be used to establish the locations of multiple actors in a scene. This article also presents a method for the simulation of multiple types of data to be used in the training of autonomous vehicles.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇文章描述了一种多Modal方法，使用模拟的激光数据和图像像素损失，通过可微分渲染来优化对象在计算机图形场景中的位置，与观察者或参考物体的关系。这种方法使用梯度下降，损失函数受到两种模式的影响。通常的对象放置优化只使用图像像素损失和可微分渲染，这种工作表明使用第二种模式（激光）会更快 converges。这种整合感知输入的方法有可能为自动驾驶车辆提供用于场景中多个演员的位置确定的潜在用途，此外，这篇文章还描述了用于训练自动驾驶车辆的多种数据类型的模拟方法。
</details></li>
</ul>
<hr>
<h2 id="GPT-InvestAR-Enhancing-Stock-Investment-Strategies-through-Annual-Report-Analysis-with-Large-Language-Models"><a href="#GPT-InvestAR-Enhancing-Stock-Investment-Strategies-through-Annual-Report-Analysis-with-Large-Language-Models" class="headerlink" title="GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models"></a>GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03079">http://arxiv.org/abs/2309.03079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UditGupta10/GPT-InvestAR">https://github.com/UditGupta10/GPT-InvestAR</a></li>
<li>paper_authors: Udit Gupta</li>
<li>For: The paper aims to simplify the process of assessing annual reports of all firms by leveraging the capabilities of large language models (LLMs).* Methods: The paper uses LLMs to generate insights from annual reports and combines these insights with historical stock price data to train a machine learning model.* Results: The walkforward test results show promising outperformance compared to S&amp;P500 returns, indicating the effectiveness of the proposed framework.Here is the same information in Simplified Chinese:* For: 这篇论文目标是使用大自然语言模型（LLM）来简化所有公司年度报告的评估过程。* Methods: 论文使用LLM生成年度报告的 Insights，并将这些 Insights 与历史股票价格数据结合，用机器学习模型进行训练。* Results: 跨越测试结果表明，提档模型可以与S&amp;P500股票价格相比，表明提档模型的有效性。<details>
<summary>Abstract</summary>
Annual Reports of publicly listed companies contain vital information about their financial health which can help assess the potential impact on Stock price of the firm. These reports are comprehensive in nature, going up to, and sometimes exceeding, 100 pages. Analysing these reports is cumbersome even for a single firm, let alone the whole universe of firms that exist. Over the years, financial experts have become proficient in extracting valuable information from these documents relatively quickly. However, this requires years of practice and experience. This paper aims to simplify the process of assessing Annual Reports of all the firms by leveraging the capabilities of Large Language Models (LLMs). The insights generated by the LLM are compiled in a Quant styled dataset and augmented by historical stock price data. A Machine Learning model is then trained with LLM outputs as features. The walkforward test results show promising outperformance wrt S&P500 returns. This paper intends to provide a framework for future work in this direction. To facilitate this, the code has been released as open source.
</details>
<details>
<summary>摘要</summary>
<<SYS> translate="no"<</SYS>公开上市公司的年度报告含有关键信息，可以帮助评估公司股价的可能影响。这些报告是全面的，可以达到或超过100页。分析这些报告是繁琐的，即使只是对单个公司进行分析，更是对整个公司宇宙进行分析。过去数年，金融专家已经熟悉地从这些文档中提取有价值信息。然而，这需要多年的做实践和经验。这篇论文想要简化所有公司的年度报告评估过程，通过利用大语言模型（LLM）的能力。生成的各种数据被编译成Quant样式数据集，并与历史股票价格数据相结合。然后，使用机器学习模型训练LMM输出作为特征。walkforward测试结果表明，这种方法可以显著超越S&P500回报。这篇论文的目标是提供未来研究的框架。为此，我们已经公开发布了代码。注：下面的文本是使用Simplified Chinese表示的。如果您想要使用Traditional Chinese，请将<<SYS> translate="no"<</SYS> replaced with <<SYS> translate="yes"<</SYS>>>
</details></li>
</ul>
<hr>
<h2 id="Impression-Informed-Multi-Behavior-Recommender-System-A-Hierarchical-Graph-Attention-Approach"><a href="#Impression-Informed-Multi-Behavior-Recommender-System-A-Hierarchical-Graph-Attention-Approach" class="headerlink" title="Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach"></a>Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03169">http://arxiv.org/abs/2309.03169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dong Li, Divya Bhargavi, Vidya Sagar Ravipati</li>
<li>For: This paper aims to address the limitations of traditional recommender systems that rely solely on implicit feedback, such as item purchases, by incorporating multi-behavior interactions and hierarchical graph attention to improve recommendation accuracy.* Methods: The proposed Hierarchical Multi-behavior Graph Attention Network (HMGN) utilizes attention mechanisms to capture information from both inter and intra-behaviors, and a multi-task Hierarchical Bayesian Personalized Ranking (HBPR) for optimization. The approach also incorporates a specialized multi-behavior sub-graph sampling technique and can handle scalability, knowledge metadata, and time-series data.* Results: The proposed HMGN model demonstrates a notable performance boost of up to 64% in NDCG@100 metrics compared to conventional graph neural network methods, indicating its effectiveness in improving recommendation accuracy by leveraging multi-behavior interactions.<details>
<summary>Abstract</summary>
While recommender systems have significantly benefited from implicit feedback, they have often missed the nuances of multi-behavior interactions between users and items. Historically, these systems either amalgamated all behaviors, such as \textit{impression} (formerly \textit{view}), \textit{add-to-cart}, and \textit{buy}, under a singular 'interaction' label, or prioritized only the target behavior, often the \textit{buy} action, discarding valuable auxiliary signals. Although recent advancements tried addressing this simplification, they primarily gravitated towards optimizing the target behavior alone, battling with data scarcity. Additionally, they tended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these gaps, we introduce the \textbf{H}ierarchical \textbf{M}ulti-behavior \textbf{G}raph Attention \textbf{N}etwork (HMGN). This pioneering framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-task Hierarchical Bayesian Personalized Ranking (HBPR) for optimization. Recognizing the need for scalability, our approach integrates a specialized multi-behavior sub-graph sampling technique. Moreover, the adaptability of HMGN allows for the seamless inclusion of knowledge metadata and time-series data. Empirical results attest to our model's prowess, registering a notable performance boost of up to 64\% in NDCG@100 metrics over conventional graph neural network methods.
</details>
<details>
<summary>摘要</summary>
While recommender systems have greatly benefited from implicit feedback, they have often overlooked the subtleties of multi-behavior interactions between users and items. Historically, these systems either lumped all behaviors, such as  impression (formerly view), add-to-cart, and buy, under a single 'interaction' label, or prioritized only the target behavior, often the buy action, discarding valuable auxiliary signals. Although recent advancements tried addressing this oversimplification, they primarily focused on optimizing the target behavior alone, struggling with data scarcity. Additionally, they tended to ignore the intrinsic hierarchy of behaviors. To bridge these gaps, we introduce the  hierarchical multi-behavior graph attention network (HMGN). This groundbreaking framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-task Hierarchical Bayesian Personalized Ranking (HBPR) for optimization. Recognizing the need for scalability, our approach integrates a specialized multi-behavior sub-graph sampling technique. Moreover, the adaptability of HMGN allows for the seamless inclusion of knowledge metadata and time-series data. Empirical results demonstrate our model's excellence, achieving a notable performance boost of up to 64% in NDCG@100 metrics over conventional graph neural network methods.
</details></li>
</ul>
<hr>
<h2 id="Split-Boost-Neural-Networks"><a href="#Split-Boost-Neural-Networks" class="headerlink" title="Split-Boost Neural Networks"></a>Split-Boost Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03167">http://arxiv.org/abs/2309.03167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">https://github.com/Aastha2104/Parkinson-Disease-Prediction</a></li>
<li>paper_authors: Raffaele Giuseppe Cestari, Gabriele Maroni, Loris Cannelli, Dario Piga, Simone Formentin</li>
<li>for: 该论文旨在提出一种新的训练策略，以提高Feed-Forward网络的性能，并自动包含正则化行为。</li>
<li>methods: 该论文提出了一种Split-Boost训练策略，通过分解网络 INTO smaller sub-networks，并在每个sub-network中进行boosting，以提高网络的性能和正则化。</li>
<li>results: 在一个真实世界（匿名）数据集上进行了 benchmark 医疗保险设计问题的测试，结果显示，使用Split-Boost训练策略可以提高网络的性能，并且自动包含正则化行为，从而降低了总体的hyperparameter数量和训练时间。<details>
<summary>Abstract</summary>
The calibration and training of a neural network is a complex and time-consuming procedure that requires significant computational resources to achieve satisfactory results. Key obstacles are a large number of hyperparameters to select and the onset of overfitting in the face of a small amount of data. In this framework, we propose an innovative training strategy for feed-forward architectures - called split-boost - that improves performance and automatically includes a regularizing behaviour without modeling it explicitly. Such a novel approach ultimately allows us to avoid explicitly modeling the regularization term, decreasing the total number of hyperparameters and speeding up the tuning phase. The proposed strategy is tested on a real-world (anonymized) dataset within a benchmark medical insurance design problem.
</details>
<details>
<summary>摘要</summary>
neural network 的准确和训练是一个复杂和时间消耗的过程，需要大量计算资源以获得满意的结果。关键障碍是大量的超参数选择和数据少量增强问题。在这个框架下，我们提出了一种新的训练策略 для批处理体系（split-boost），可以提高性能并自动包含正则化行为无需显式表示。这种新的方法最终允许我们避免显式表示正则化项，减少总体超参数数量和优化阶段速度。我们在一个匿名的实际数据集上测试了我们的策略，并在一个匿名的医疗保险设计问题中进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Recharge-UAV-Coverage-Path-Planning-through-Deep-Reinforcement-Learning"><a href="#Learning-to-Recharge-UAV-Coverage-Path-Planning-through-Deep-Reinforcement-Learning" class="headerlink" title="Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning"></a>Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03157">http://arxiv.org/abs/2309.03157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/theilem/uavSim">https://github.com/theilem/uavSim</a></li>
<li>paper_authors: Mirco Theile, Harald Bayerlein, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli</li>
<li>for: 这篇论文解决了功能有限的无人机（UAV）在覆盖区域时的能量有限的问题，通过使用深度学习（DRL）和地图观察来优化覆盖路径。</li>
<li>methods: 该论文提出了一种基于 proximal policy optimization（PPO）的DRL方法，使用动作掩模和折扣因子安排来优化覆盖 trajectory，并通过历史位置处理来处理出现的突然变化。</li>
<li>results: 该论文的方法在不同的目标区域和地图上表现出优于基线方法，并能够普适化到不同的地图上。<details>
<summary>Abstract</summary>
Coverage path planning (CPP) is a critical problem in robotics, where the goal is to find an efficient path that covers every point in an area of interest. This work addresses the power-constrained CPP problem with recharge for battery-limited unmanned aerial vehicles (UAVs). In this problem, a notable challenge emerges from integrating recharge journeys into the overall coverage strategy, highlighting the intricate task of making strategic, long-term decisions. We propose a novel proximal policy optimization (PPO)-based deep reinforcement learning (DRL) approach with map-based observations, utilizing action masking and discount factor scheduling to optimize coverage trajectories over the entire mission horizon. We further provide the agent with a position history to handle emergent state loops caused by the recharge capability. Our approach outperforms a baseline heuristic, generalizes to different target zones and maps, with limited generalization to unseen maps. We offer valuable insights into DRL algorithm design for long-horizon problems and provide a publicly available software framework for the CPP problem.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Driven-Neural-Polar-Codes-for-Unknown-Channels-With-and-Without-Memory"><a href="#Data-Driven-Neural-Polar-Codes-for-Unknown-Channels-With-and-Without-Memory" class="headerlink" title="Data-Driven Neural Polar Codes for Unknown Channels With and Without Memory"></a>Data-Driven Neural Polar Codes for Unknown Channels With and Without Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03148">http://arxiv.org/abs/2309.03148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziv Aharoni, Bashar Huleihel, Henry D. Pfister, Haim H. Permuter</li>
<li>for: 该论文的目的是提出一种基于数据驱动的极码设计方法，用于处理有和无内存的通道。</li>
<li>methods: 该方法利用成功级联（SC）解码器的结构，采用神经网络（NN）取代原始SC解码器的核心元素，包括检查节点、位节点和软决策。此外，还采用一个附加的NN来嵌入通道输出到SC解码器的输入空间。</li>
<li>results: 该方法可以提供理论保证和计算复杂度不随通道内存大小增长。在实验中，该方法在无内存通道和具有内存通道上表现出优于最佳极码解码器（SC和SCT解码器）。此外，该方法还适用于SC和SCT解码器不适用的情况。<details>
<summary>Abstract</summary>
In this work, a novel data-driven methodology for designing polar codes for channels with and without memory is proposed. The methodology is suitable for the case where the channel is given as a "black-box" and the designer has access to the channel for generating observations of its inputs and outputs, but does not have access to the explicit channel model. The proposed method leverages the structure of the successive cancellation (SC) decoder to devise a neural SC (NSC) decoder. The NSC decoder uses neural networks (NNs) to replace the core elements of the original SC decoder, the check-node, the bit-node and the soft decision. Along with the NSC, we devise additional NN that embeds the channel outputs into the input space of the SC decoder. The proposed method is supported by theoretical guarantees that include the consistency of the NSC. Also, the NSC has computational complexity that does not grow with the channel memory size. This sets its main advantage over successive cancellation trellis (SCT) decoder for finite state channels (FSCs) that has complexity of $O(|\mathcal{S}|^3 N\log N)$, where $|\mathcal{S}|$ denotes the number of channel states. We demonstrate the performance of the proposed algorithms on memoryless channels and on channels with memory. The empirical results are compared with the optimal polar decoder, given by the SC and SCT decoders. We further show that our algorithms are applicable for the case where there SC and SCT decoders are not applicable.
</details>
<details>
<summary>摘要</summary>
“这个研究中提出了一种基于数据的方法，用于设计具有和无记忆频道的� polar 码。这种方法适用于频道给出了“黑盒”，设计师可以通过频道生成输入和输出的观察，但无法取得频道的具体模型。我们的方法利用成功的继续取消（SC）解oder的结构，创建了一个内置 neural network（NSC）解oder。NSC解oder使用神经网络（NN）取代原始 SC 解oder的核心元素，包括检查点、位元点和软决定。此外，我们还创建了一个附加的 NN，将频道输出嵌入到 SC 解oder 的输入空间中。我们的方法具有理论保证，包括 NSC 的一致性。此外，NSC 的计算复杂度不随频道内存大小增长。这使得 NSC 在finite state channels（FSCs）中的计算复杂度与 SCT 解oder 不同，SCT 解oder 的计算复杂度为 $O(|\mathcal{S}|^3 N\log N)$，where $|\mathcal{S}|$ 表示频道状态的数量。我们在无记忆频道和记忆频道上进行实验，比较了我们的方法与最佳 polar 解oder（SC 和 SCT 解oder）的实验结果。我们还证明了我们的方法可以应用于 SC 和 SCT 解oder 不适用的情况。”
</details></li>
</ul>
<hr>
<h2 id="The-Best-Arm-Evades-Near-optimal-Multi-pass-Streaming-Lower-Bounds-for-Pure-Exploration-in-Multi-armed-Bandits"><a href="#The-Best-Arm-Evades-Near-optimal-Multi-pass-Streaming-Lower-Bounds-for-Pure-Exploration-in-Multi-armed-Bandits" class="headerlink" title="The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits"></a>The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03145">http://arxiv.org/abs/2309.03145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sepehr Assadi, Chen Wang</li>
<li>for: 这个论文是用来解决多臂枪（Multi-armed Bandit，MAB）中的纯探索问题，即如何在有限资源下选择最佳臂来获得最大的回报。</li>
<li>methods: 这个论文使用了流式算法，并且使用了优化的抽样复杂度来解决这个问题。具体来说，这个论文使用了$O\left(\frac{n}{\Delta^2}\right)$的抽样复杂度，其中$n$是臂的数量，$\Delta$是最佳臂和第二最佳臂之间的差距。</li>
<li>results: 这个论文获得了一个near-optimal的抽象贸易优化，即使用流式算法可以在$O\left(\frac{\log(1&#x2F;\Delta)}{\log\log(1&#x2F;\Delta)}\right)$个过程中获得$O\left(\frac{n}{\Delta^2}\right)$的抽样复杂度。这个结果与 Jin et al. 的 $O(\log(\frac{1}{\Delta}))$-pass algorithm相同（即使用$O(1)$的内存和$O\left(\frac{n}{\Delta^2}\right)$的抽样复杂度），并解决了 Assadi 和 Wang 提出的一个开问题。<details>
<summary>Abstract</summary>
We give a near-optimal sample-pass trade-off for pure exploration in multi-armed bandits (MABs) via multi-pass streaming algorithms: any streaming algorithm with sublinear memory that uses the optimal sample complexity of $O(\frac{n}{\Delta^2})$ requires $\Omega(\frac{\log{(1/\Delta)}{\log\log{(1/\Delta)})$ passes. Here, $n$ is the number of arms and $\Delta$ is the reward gap between the best and the second-best arms. Our result matches the $O(\log(\frac{1}{\Delta}))$-pass algorithm of Jin et al. [ICML'21] (up to lower order terms) that only uses $O(1)$ memory and answers an open question posed by Assadi and Wang [STOC'20].
</details>
<details>
<summary>摘要</summary>
我们提供了一种近似优化的样本传递贸易，用于纯exploration在多把枪（MAB）中，通过多路流动算法：任何具有减 Linear 内存的流动算法都需要 $\Omega(\frac{\log{(1/\Delta)}{\log\log{(1/\Delta)})$  passes。在这里， $n$ 是把枪的数量， $\Delta$ 是最佳和第二最佳把枪之间的奖励差。我们的结果与 Jin et al. 的 $O(\log(\frac{1}{\Delta}))$-pass算法（up to lower order terms）相匹配，该算法只需 $O(1)$ 内存并解决了Assadi 和 Wang 提出的问题（STOC'20）。
</details></li>
</ul>
<hr>
<h2 id="Using-Multiple-Vector-Channels-Improves-E-n-Equivariant-Graph-Neural-Networks"><a href="#Using-Multiple-Vector-Channels-Improves-E-n-Equivariant-Graph-Neural-Networks" class="headerlink" title="Using Multiple Vector Channels Improves E(n)-Equivariant Graph Neural Networks"></a>Using Multiple Vector Channels Improves E(n)-Equivariant Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03139">http://arxiv.org/abs/2309.03139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Levy, Sékou-Oumar Kaba, Carmelo Gonzales, Santiago Miret, Siamak Ravanbakhsh</li>
<li>for: 用于物理科学领域的机器学习研究</li>
<li>methods: 使用多通道E(n)-对称图 neural network</li>
<li>results: 在多种物理系统 benchmark 任务上表现更好，但 RUNTIME 和参数数量几乎不变<details>
<summary>Abstract</summary>
We present a natural extension to E(n)-equivariant graph neural networks that uses multiple equivariant vectors per node. We formulate the extension and show that it improves performance across different physical systems benchmark tasks, with minimal differences in runtime or number of parameters. The proposed multichannel EGNN outperforms the standard singlechannel EGNN on N-body charged particle dynamics, molecular property predictions, and predicting the trajectories of solar system bodies. Given the additional benefits and minimal additional cost of multi-channel EGNN, we suggest that this extension may be of practical use to researchers working in machine learning for the physical sciences
</details>
<details>
<summary>摘要</summary>
我团队提出了一种自然扩展，用于E(n)-对称图 neural network，使用每个节点多个对称向量。我们阐述了这种扩展，并证明其在不同物理系统 benchmark 任务中提高性能，差异非常小，运行时间和参数数量也几乎不变。我们提出的多通道EGNN 超过了标准单通道EGNN 在N-体电荷 particel动力学、分子性质预测和太阳系天体轨道预测等任务中的性能。考虑到这种扩展的附加优点和运行时间和参数数量的几乎不变，我们建议这种扩展可能对物理科学领域的研究人员有实际意义。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Manufacturing-Defects-in-PCBs-via-Data-Centric-Machine-Learning-on-Solder-Paste-Inspection-Features"><a href="#Detecting-Manufacturing-Defects-in-PCBs-via-Data-Centric-Machine-Learning-on-Solder-Paste-Inspection-Features" class="headerlink" title="Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Solder Paste Inspection Features"></a>Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Solder Paste Inspection Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03113">http://arxiv.org/abs/2309.03113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jubilee Prasad-Rao, Roohollah Heidary, Jesse Williams</li>
<li>For: 印刷电路板（PCB）生产中的缺陷检测，使用粘接粉检测（SPI）和自动光学检测（AOI）机器可以提高操作效率，大幅减少人工干预。* Methods: 使用SPI提取的特征来训练机器学习（ML）模型，在PCB生产过程中检测缺陷。使用6万个封装的特征，对2万个组件进行训练，共计15387个PCB。在数据预处理步骤中进行迭代，使用基础的极大均值抛物质（XGBoost）ML模型。* Results: 使用不同级别的训练实例，包括封装级、组件级和PCB级，使ML模型能够捕捉到pin级、组件级和PCB级之间的交互效果。结果表明，将不同级别的训练实例组合起来可以提高缺陷检测性能。<details>
<summary>Abstract</summary>
Automated detection of defects in Printed Circuit Board (PCB) manufacturing using Solder Paste Inspection (SPI) and Automated Optical Inspection (AOI) machines can help improve operational efficiency and significantly reduce the need for manual intervention. In this paper, using SPI-extracted features of 6 million pins, we demonstrate a data-centric approach to train Machine Learning (ML) models to detect PCB defects at three stages of PCB manufacturing. The 6 million PCB pins correspond to 2 million components that belong to 15,387 PCBs. Using a base extreme gradient boosting (XGBoost) ML model, we iterate on the data pre-processing step to improve detection performance. Combining pin-level SPI features using component and PCB IDs, we developed training instances also at the component and PCB level. This allows the ML model to capture any inter-pin, inter-component, or spatial effects that may not be apparent at the pin level. Models are trained at the pin, component, and PCB levels, and the detection results from the different models are combined to identify defective components.
</details>
<details>
<summary>摘要</summary>
自动检测印刷电路板（PCB）制造过程中的缺陷使用沉积镀 paste inspection（SPI）和自动光学检查（AOI）机器可以提高运行效率，并大幅减少人工干预。在这篇论文中，使用SPI提取的600万个封装件特征来训练机器学习（ML）模型，以检测PCB制造过程中的缺陷。这600万个PCB封装件相应于2000万个组件，这些组件属于15387个PCB。使用基础的极限梯度提升（XGBoost）ML模型，我们在数据预处理步骤中进行迭代优化。将封装件级SPI特征与组件和PCB ID相结合，我们开发了训练实例，同时在组件和PCB级别进行训练。这样允许ML模型捕捉到封装件级、组件级和PCB级之间的相互作用，从而提高检测性能。训练得到的不同模型的检测结果进行合并，以识别缺陷的组件。
</details></li>
</ul>
<hr>
<h2 id="Graph-Theory-Applications-in-Advanced-Geospatial-Research"><a href="#Graph-Theory-Applications-in-Advanced-Geospatial-Research" class="headerlink" title="Graph Theory Applications in Advanced Geospatial Research"></a>Graph Theory Applications in Advanced Geospatial Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03249">http://arxiv.org/abs/2309.03249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surajit Ghosh, Archita Mallick, Anuva Chowdhury, Kounik De Sarkar</li>
<li>for: 本研究探讨了图论算法在地理科学中的应用，包括网络分析、空间连接性、地理信息系统等多种空间问题的解决方案。</li>
<li>methods: 本文介绍了图论算法在地理科学中的应用，包括度量空间关系、分析空间结构、地理信息系统等方面的实现。</li>
<li>results: 本研究提供了各种实际应用的案例研究，描述了图论算法在地理科学中的实际 significanc，以及未来研究的可能性和挑战。<details>
<summary>Abstract</summary>
Geospatial sciences include a wide range of applications, from environmental monitoring transportation to infrastructure planning, as well as location-based analysis and services. Graph theory algorithms in mathematics have emerged as indispensable tools in these domains due to their capability to model and analyse spatial relationships efficiently. This technical report explores the applications of graph theory algorithms in geospatial sciences, highlighting their role in network analysis, spatial connectivity, geographic information systems, and various other spatial problem-solving scenarios. It provides a comprehensive idea about the key concepts and algorithms of graph theory that assist the modelling processes. The report provides insights into the practical significance of graph theory in addressing real-world geospatial challenges and opportunities. It lists the extensive research, innovative technologies and methodologies implemented in this field.
</details>
<details>
<summary>摘要</summary>
地理科学包括各种应用，从环境监测到交通规划，以及基础设施规划，同时还包括位置基于分析和服务。数学中的图论算法在这些领域中已成为不可或缺的工具，因为它可以有效地模拟和分析空间关系。本技术报告探讨了图论算法在地理科学中的应用，包括网络分析、空间连接性、地理信息系统等多种空间问题解决方案。报告还提供了关键概念和算法的全面了解，以及在实际世界中图论的实际应用和挑战。报告还列举了这一领域的广泛研究、创新技术和方法。
</details></li>
</ul>
<hr>
<h2 id="ContrastWSD-Enhancing-Metaphor-Detection-with-Word-Sense-Disambiguation-Following-the-Metaphor-Identification-Procedure"><a href="#ContrastWSD-Enhancing-Metaphor-Detection-with-Word-Sense-Disambiguation-Following-the-Metaphor-Identification-Procedure" class="headerlink" title="ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure"></a>ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03103">http://arxiv.org/abs/2309.03103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad Elzohbi, Richard Zhao</li>
<li>for: 本研究旨在提高 метафора检测精度，提出了一种基于 RoBERTa 的 Metaphor Detection 模型，称为 ContrastWSD。</li>
<li>methods: 该模型结合 Metaphor Identification Procedure (MIP) 和 Word Sense Disambiguation (WSD)，通过对词语上下文含义和基本含义进行对比，判断 sentence 中是否存在 метафо拉用法。</li>
<li>results: 在多个 benchmark 数据集上进行测试，与强基线相比，ContrastWSD 表现出色， indicating the effectiveness in advancing metaphor detection。<details>
<summary>Abstract</summary>
This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文提出了ContrastWSD模型，基于RoBERTa语言模型，结合了Metaphor Identification Procedure（MIP）和Word Sense Disambiguation（WSD）两者，以提取和对比语言中单词的上下文含义和基本含义，以判断单词是否在句子中使用了象征性。通过利用WSD模型中的词义，我们的模型提高了象征检测的过程，并超越了仅仅基于上下文嵌入或者将基本定义和其他外部知识相结合的方法。我们在各种标准数据集上评估了我们的方法，并与强基线相比较，表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="ORL-AUDITOR-Dataset-Auditing-in-Offline-Deep-Reinforcement-Learning"><a href="#ORL-AUDITOR-Dataset-Auditing-in-Offline-Deep-Reinforcement-Learning" class="headerlink" title="ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning"></a>ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03081">http://arxiv.org/abs/2309.03081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/link-zju/orl-auditor">https://github.com/link-zju/orl-auditor</a></li>
<li>paper_authors: Linkang Du, Min Chen, Mingyang Sun, Shouling Ji, Peng Cheng, Jiming Chen, Zhikun Zhang<br>for:* 这篇论文的目的是为了提供一个新的数据实验验证机制，以保护机器学习模型在安全重要领域中的数据库。methods:* 这篇论文使用了聚合奖励作为唯一识别符，以验证训练在特定数据库上的机器学习模型。results:* 这篇论文的实验结果显示，使用 ORL-AUDITOR 可以实现高准确率（95%）和低伪阳性率（2.88%）的数据实验验证，并且可以实现实际的实验设置和资料集验证。<details>
<summary>Abstract</summary>
Data is a critical asset in AI, as high-quality datasets can significantly improve the performance of machine learning models. In safety-critical domains such as autonomous vehicles, offline deep reinforcement learning (offline DRL) is frequently used to train models on pre-collected datasets, as opposed to training these models by interacting with the real-world environment as the online DRL. To support the development of these models, many institutions make datasets publicly available with opensource licenses, but these datasets are at risk of potential misuse or infringement. Injecting watermarks to the dataset may protect the intellectual property of the data, but it cannot handle datasets that have already been published and is infeasible to be altered afterward. Other existing solutions, such as dataset inference and membership inference, do not work well in the offline DRL scenario due to the diverse model behavior characteristics and offline setting constraints. In this paper, we advocate a new paradigm by leveraging the fact that cumulative rewards can act as a unique identifier that distinguishes DRL models trained on a specific dataset. To this end, we propose ORL-AUDITOR, which is the first trajectory-level dataset auditing mechanism for offline RL scenarios. Our experiments on multiple offline DRL models and tasks reveal the efficacy of ORL-AUDITOR, with auditing accuracy over 95% and false positive rates less than 2.88%. We also provide valuable insights into the practical implementation of ORL-AUDITOR by studying various parameter settings. Furthermore, we demonstrate the auditing capability of ORL-AUDITOR on open-source datasets from Google and DeepMind, highlighting its effectiveness in auditing published datasets. ORL-AUDITOR is open-sourced at https://github.com/link-zju/ORL-Auditor.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>数据是人工智能中关键资产，高质量数据集可以显著提高机器学习模型的性能。在安全关键领域如自动驾驶车辆，线上深度优化学习（线上DRL）常常用于训练模型，而不是在真实环境中进行交互式训练。为支持这些模型的开发，许多机构将数据集公开发布，但这些数据集受到潜在的违用或侵犯的威胁。把水印注入到数据集可以保护数据的知识产权，但它无法处理已经发布的数据集，也无法在后续更改。现有的解决方案，如数据集推理和会员推理，在线上DRL场景中不太有效，因为模型的行为特点和环境约束。在这篇论文中，我们主张一种新的思路，利用总奖励可以作为特定数据集训练DRL模型的唯一标识符。为此，我们提出了ORL-AUDITOR，它是第一个路径级数据集审核机制。我们的实验表明，ORL-AUDITOR的审核精度高于95%，false positive率低于2.88%。我们还提供了实现ORL-AUDITOR的有价值参数研究，以及对open-source数据集的审核能力。ORL-AUDITOR已经开源在https://github.com/link-zju/ORL-Auditor。
</details></li>
</ul>
<hr>
<h2 id="Parameterizing-pressure-temperature-profiles-of-exoplanet-atmospheres-with-neural-networks"><a href="#Parameterizing-pressure-temperature-profiles-of-exoplanet-atmospheres-with-neural-networks" class="headerlink" title="Parameterizing pressure-temperature profiles of exoplanet atmospheres with neural networks"></a>Parameterizing pressure-temperature profiles of exoplanet atmospheres with neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03075">http://arxiv.org/abs/2309.03075</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timothygebhard/ml4ptp">https://github.com/timothygebhard/ml4ptp</a></li>
<li>paper_authors: Timothy D. Gebhard, Daniel Angerhausen, Björn S. Konrad, Eleonora Alei, Sascha P. Quanz, Bernhard Schölkopf</li>
<li>for: 这篇论文的目的是提出一个新的、数据驱动的压力-温度（PT）�profile parameterization scheme，以便改善行星大气层的探测和理解。</li>
<li>methods: 这篇论文使用一个基于神经网络的隐藏Variable模型，以学习一个分布 над PT profile的函数，并使用一个对应的解oder网络将 $P$ 转换为 $T$.</li>
<li>results: 在对两个公开可用的自我一致PT profile数据集进行训练和评估后，发现这个方法可以在比较少的 Parameters下，实现与基于多个 Parameters的基eline方法相同或更好的适摄性。在一个基于现有文献的AR中，使用这个方法（仅使用两个参数）可以实现一个更紧密、更准确的 posterior distribution over PT profile，并且还可以将AR的运算速度提高到三倍以上。<details>
<summary>Abstract</summary>
Atmospheric retrievals (AR) of exoplanets typically rely on a combination of a Bayesian inference technique and a forward simulator to estimate atmospheric properties from an observed spectrum. A key component in simulating spectra is the pressure-temperature (PT) profile, which describes the thermal structure of the atmosphere. Current AR pipelines commonly use ad hoc fitting functions here that limit the retrieved PT profiles to simple approximations, but still use a relatively large number of parameters. In this work, we introduce a conceptually new, data-driven parameterization scheme for physically consistent PT profiles that does not require explicit assumptions about the functional form of the PT profiles and uses fewer parameters than existing methods. Our approach consists of a latent variable model (based on a neural network) that learns a distribution over functions (PT profiles). Each profile is represented by a low-dimensional vector that can be used to condition a decoder network that maps $P$ to $T$. When training and evaluating our method on two publicly available datasets of self-consistent PT profiles, we find that our method achieves, on average, better fit quality than existing baseline methods, despite using fewer parameters. In an AR based on existing literature, our model (using two parameters) produces a tighter, more accurate posterior for the PT profile than the five-parameter polynomial baseline, while also speeding up the retrieval by more than a factor of three. By providing parametric access to physically consistent PT profiles, and by reducing the number of parameters required to describe a PT profile (thereby reducing computational cost or freeing resources for additional parameters of interest), our method can help improve AR and thus our understanding of exoplanet atmospheres and their habitability.
</details>
<details>
<summary>摘要</summary>
通常情况下，外星 planet 的大气 retrieval 都是通过一种 bayesian inference 技术和一个前向模拟器来估算大气属性从观测 спектrum 中。在模拟 спектrum 时，一个关键的组件是压力-温度（PT）规则，它描述了大气的热结构。现有的 AR 管道通常使用一些假设来限制获取的 PT profile 的形式，但仍然使用较多的参数。在这个工作中，我们提出了一种新的、数据驱动的参数化方法，不需要明确PT profile的函数形式假设，并且使用 fewer 参数 than existing methods。我们的方法包括一个 latent variable model（基于神经网络），它学习一个分布 sobre 函数（PT profile）。每个 profile 被表示为一个低维度的向量，可以用来 condition 一个 decoder network 将 $P$ 映射到 $T$。在训练和评估我们的方法时，我们发现在两个公共可用的数据集上，我们的方法可以在平均上实现更好的适应质量，尽管使用 fewer 参数。在基于现有文献的 AR 中，我们的模型（使用两个参数）可以生成一个更紧凑、更准确的 posterior  для PT profile，而且也可以提高计算速度，比基eline 方法多于三倍。通过提供 физиikal consistent PT profile 的参数化访问，并且降低计算 cost 或释放更多的参数来描述 PT profile，我们的方法可以帮助改进 AR，从而提高外星 planet 大气的理解和可居住性。
</details></li>
</ul>
<hr>
<h2 id="Character-Queries-A-Transformer-based-Approach-to-On-Line-Handwritten-Character-Segmentation"><a href="#Character-Queries-A-Transformer-based-Approach-to-On-Line-Handwritten-Character-Segmentation" class="headerlink" title="Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation"></a>Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03072">http://arxiv.org/abs/2309.03072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jungomi/character-queries">https://github.com/jungomi/character-queries</a></li>
<li>paper_authors: Michael Jungo, Beat Wolf, Andrii Maksai, Claudiu Musat, Andreas Fischer</li>
<li>for: 这个论文的目的是提出一种基于Transformer架构的在线手写文本分割方法，用于帮助手写识别模型进一步改进分割精度。</li>
<li>methods: 这种方法使用了一种基于$k$-means算法的 clustering 分配方法，将样本点与文本中的字符进行匹配，并在Transformer搅拌块中使用学习的字符查询来形成每个分支。</li>
<li>results: 在两个常用的在线手写数据集IAM-OnDB和HANDS-VNOnDB上， authors 创建了分割ground truth，并对多种方法进行评估，得出了总体最佳的结果。<details>
<summary>Abstract</summary>
On-line handwritten character segmentation is often associated with handwriting recognition and even though recognition models include mechanisms to locate relevant positions during the recognition process, it is typically insufficient to produce a precise segmentation. Decoupling the segmentation from the recognition unlocks the potential to further utilize the result of the recognition. We specifically focus on the scenario where the transcription is known beforehand, in which case the character segmentation becomes an assignment problem between sampling points of the stylus trajectory and characters in the text. Inspired by the $k$-means clustering algorithm, we view it from the perspective of cluster assignment and present a Transformer-based architecture where each cluster is formed based on a learned character query in the Transformer decoder block. In order to assess the quality of our approach, we create character segmentation ground truths for two popular on-line handwriting datasets, IAM-OnDB and HANDS-VNOnDB, and evaluate multiple methods on them, demonstrating that our approach achieves the overall best results.
</details>
<details>
<summary>摘要</summary>
在线手写字符分 segmentation 常常与手写识别相关，尽管识别模型包含了定位相关的机制，但通常不够准确地分 segmentation。解耦分 segmentation 和识别可以解放更多的潜在用途。我们专注于知道过程中的文本束缚，在这种情况下，字符分 segmentation 变成了将样本点掌握轨迹与字符在文本中的匹配问题。受 $k$-means 聚合算法启发，我们从cluster分配的角度出发，并在Transformer嵌入oder块中学习每个cluster的字符查询。为评估我们的方法质量，我们创建了两个常见在线手写数据集的字符分 segmentation真实值，并评估了多种方法，并示出我们的方法在总体上获得了最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Active-Subspaces-for-Effective-and-Scalable-Uncertainty-Quantification-in-Deep-Neural-Networks"><a href="#Learning-Active-Subspaces-for-Effective-and-Scalable-Uncertainty-Quantification-in-Deep-Neural-Networks" class="headerlink" title="Learning Active Subspaces for Effective and Scalable Uncertainty Quantification in Deep Neural Networks"></a>Learning Active Subspaces for Effective and Scalable Uncertainty Quantification in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03061">http://arxiv.org/abs/2309.03061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanket Jantre, Nathan M. Urban, Xiaoning Qian, Byung-Jun Yoon</li>
<li>for: 提供了Well-calibrated predictions with quantified uncertainty and robustness for neural networks.</li>
<li>methods: 使用constructing a low-dimensional subspace of the neural network parameters to reduce computational complexity and enable effective and scalable Bayesian inference via either Monte Carlo (MC) sampling methods or variational inference.</li>
<li>results: 提供了可靠的预测和稳定的uncertainty estimates for various regression tasks.<details>
<summary>Abstract</summary>
Bayesian inference for neural networks, or Bayesian deep learning, has the potential to provide well-calibrated predictions with quantified uncertainty and robustness. However, the main hurdle for Bayesian deep learning is its computational complexity due to the high dimensionality of the parameter space. In this work, we propose a novel scheme that addresses this limitation by constructing a low-dimensional subspace of the neural network parameters-referred to as an active subspace-by identifying the parameter directions that have the most significant influence on the output of the neural network. We demonstrate that the significantly reduced active subspace enables effective and scalable Bayesian inference via either Monte Carlo (MC) sampling methods, otherwise computationally intractable, or variational inference. Empirically, our approach provides reliable predictions with robust uncertainty estimates for various regression tasks.
</details>
<details>
<summary>摘要</summary>
bayesian 推论 для神经网络，或 bayesian 深度学习，具有提供准确的预测和质量量化的不确定性的潜力。然而，bayesian 深度学习的主要障碍是其参数空间的维度太高，导致计算复杂度过高。在这种工作中，我们提出了一种新的方案，即在神经网络参数空间中构建一个低维的子空间（称为活跃子空间），并将神经网络输出的影响因素约束在这个子空间中。我们证明了这个减少后的活跃子空间可以实现有效和可扩展的bayesian推论，通过MC抽样方法或变量推断。我们的方法在各种回归任务上提供了可靠的预测和强度的不确定性估计。
</details></li>
</ul>
<hr>
<h2 id="CoLA-Exploiting-Compositional-Structure-for-Automatic-and-Efficient-Numerical-Linear-Algebra"><a href="#CoLA-Exploiting-Compositional-Structure-for-Automatic-and-Efficient-Numerical-Linear-Algebra" class="headerlink" title="CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra"></a>CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03060">http://arxiv.org/abs/2309.03060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wilson-labs/cola">https://github.com/wilson-labs/cola</a></li>
<li>paper_authors: Andres Potapczynski, Marc Finzi, Geoff Pleiss, Andrew Gordon Wilson</li>
<li>for:  Linear algebra problems in machine learning, such as eigendecompositions, solving linear systems, computing matrix exponentials, and trace estimation.</li>
<li>methods:  Proposes a simple and general framework called CoLA (Compositional Linear Algebra) that combines a linear operator abstraction with compositional dispatch rules to construct memory and runtime efficient numerical algorithms.</li>
<li>results:  Accelerates many algebraic operations and makes it easy to prototype matrix structures and algorithms, with applications in partial differential equations, Gaussian processes, equivariant model construction, and unsupervised learning.<details>
<summary>Abstract</summary>
Many areas of machine learning and science involve large linear algebra problems, such as eigendecompositions, solving linear systems, computing matrix exponentials, and trace estimation. The matrices involved often have Kronecker, convolutional, block diagonal, sum, or product structure. In this paper, we propose a simple but general framework for large-scale linear algebra problems in machine learning, named CoLA (Compositional Linear Algebra). By combining a linear operator abstraction with compositional dispatch rules, CoLA automatically constructs memory and runtime efficient numerical algorithms. Moreover, CoLA provides memory efficient automatic differentiation, low precision computation, and GPU acceleration in both JAX and PyTorch, while also accommodating new objects, operations, and rules in downstream packages via multiple dispatch. CoLA can accelerate many algebraic operations, while making it easy to prototype matrix structures and algorithms, providing an appealing drop-in tool for virtually any computational effort that requires linear algebra. We showcase its efficacy across a broad range of applications, including partial differential equations, Gaussian processes, equivariant model construction, and unsupervised learning.
</details>
<details>
<summary>摘要</summary>
很多机器学习和科学领域都需要大规模的线性代数问题，如特征值分解、解linear系统、计算矩阵幂和跟踪估计。这些矩阵通常具有克罗内cker、卷积、块对角、和乘法结构。在这篇论文中，我们提出了一个简单 yet 通用的大规模线性代数问题解决方案，名为CoLA（Compositional Linear Algebra）。通过将线性运算抽象与compositional发送规则相结合，CoLA自动构建了高效的内存和运行时数值算法。此外，CoLA还提供了内存高效的自动微分、低精度计算和GPU加速，并在JAX和PyTorch中支持多个发送规则。CoLA可以加速许多线性运算，同时使得创建矩阵结构和算法变得容易，提供了许多应用领域的Drop-in工具。我们在各种应用中展示了CoLA的效果，包括偏微分方程、Gaussian процес序、对称型模型构建和无监督学习。
</details></li>
</ul>
<hr>
<h2 id="Automated-CVE-Analysis-for-Threat-Prioritization-and-Impact-Prediction"><a href="#Automated-CVE-Analysis-for-Threat-Prioritization-and-Impact-Prediction" class="headerlink" title="Automated CVE Analysis for Threat Prioritization and Impact Prediction"></a>Automated CVE Analysis for Threat Prioritization and Impact Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03040">http://arxiv.org/abs/2309.03040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Aghaei, Ehab Al-Shaer, Waseem Shadid, Xi Niu</li>
<li>For: The paper aims to address the challenge of accurately estimating the attack severity of publicly disclosed cybersecurity vulnerabilities and identifying potential countermeasures.* Methods: The paper proposes a novel predictive model and tool called CVEDrill, which uses machine learning techniques to estimate the CVSS vector and classify CVEs into the appropriate CWE hierarchy classes.* Results: The paper claims that CVEDrill can accurately estimate the CVSS vector and outperform state-of-the-art tools like ChaptGPT in terms of accuracy and timeliness. The paper also demonstrates the effectiveness of CVEDrill in identifying potential countermeasures for a large number of CVEs.<details>
<summary>Abstract</summary>
The Common Vulnerabilities and Exposures (CVE) are pivotal information for proactive cybersecurity measures, including service patching, security hardening, and more. However, CVEs typically offer low-level, product-oriented descriptions of publicly disclosed cybersecurity vulnerabilities, often lacking the essential attack semantic information required for comprehensive weakness characterization and threat impact estimation. This critical insight is essential for CVE prioritization and the identification of potential countermeasures, particularly when dealing with a large number of CVEs. Current industry practices involve manual evaluation of CVEs to assess their attack severities using the Common Vulnerability Scoring System (CVSS) and mapping them to Common Weakness Enumeration (CWE) for potential mitigation identification. Unfortunately, this manual analysis presents a major bottleneck in the vulnerability analysis process, leading to slowdowns in proactive cybersecurity efforts and the potential for inaccuracies due to human errors. In this research, we introduce our novel predictive model and tool (called CVEDrill) which revolutionizes CVE analysis and threat prioritization. CVEDrill accurately estimates the CVSS vector for precise threat mitigation and priority ranking and seamlessly automates the classification of CVEs into the appropriate CWE hierarchy classes. By harnessing CVEDrill, organizations can now implement cybersecurity countermeasure mitigation with unparalleled accuracy and timeliness, surpassing in this domain the capabilities of state-of-the-art tools like ChaptGPT.
</details>
<details>
<summary>摘要</summary>
共享漏洞和曝露（CVE）是重要的cyber安全措施的基础信息，包括服务修补、安全强化等。然而，CVEs通常只提供低层级、产品特定的漏洞描述，缺乏关键的攻击semantic信息，这使得全面的漏洞特点和威胁影响的评估受到限制。这种缺失的信息是CVEDrill的出现所需的，CVEDrill是一种新的预测模型和工具，可以准确地估算CVSS向量，并准确地将CVE分类到适当的CWE层次结构中。通过CVEDrill，组织可以现在实现cyber安全防范措施的准确性和时效性，超越现有的工具 like ChaptGPT。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Polycystic-Kidney-Disease-Utilizing-Neural-Networks-for-Accurate-and-Early-Detection-through-Gene-Expression-Analysis"><a href="#Deep-Learning-for-Polycystic-Kidney-Disease-Utilizing-Neural-Networks-for-Accurate-and-Early-Detection-through-Gene-Expression-Analysis" class="headerlink" title="Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis"></a>Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03033">http://arxiv.org/abs/2309.03033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kapil Panda, Anirudh Mazumder</li>
<li>for: 早期诊断肾脏瘤病（PKD），以避免疾病的发展并实现有效的管理。</li>
<li>methods: 使用深度学习方法分析患者的基因表达，以实现准确和可靠的疾病检测。</li>
<li>results: 研究提出了一种可以准确预测患有PKD的患者的神经网络模型。Translation:</li>
<li>for: Early detection of polycystic kidney disease (PKD) to avoid the progression of the disease and achieve effective management.</li>
<li>methods: Using deep learning methods to analyze patient gene expressions to achieve accurate and robust predictions of possible PKD.</li>
<li>results: The study proposed a deep neural network model that can accurately predict PKD in patients based on their gene expressions.<details>
<summary>Abstract</summary>
With Polycystic Kidney Disease (PKD) potentially leading to fatal complications in patients due to the formation of cysts in the kidneys, early detection of PKD is crucial for effective management of the condition. However, the various patient-specific factors that play a role in the diagnosis make it an intricate puzzle for clinicians to solve. Therefore, in this study, we aim to utilize a deep learning-based approach for early disease detection. The devised neural network can achieve accurate and robust predictions for possible PKD in patients by analyzing patient gene expressions.
</details>
<details>
<summary>摘要</summary>
您的文本如下：肾脏癌病（PKD）可能会导致患者严重的合并症状，因为肾脏中形成肿瘤。因此，早期检测PKD非常重要，以便有效管理这种疾病。然而，各种患者特定的因素会影响诊断，使得临床医生面临着一个复杂的谜题。因此，在这项研究中，我们采用了深度学习基本的方法，以便在患者基因表达中准确预测PKD的可能性。
</details></li>
</ul>
<hr>
<h2 id="Universal-Preprocessing-Operators-for-Embedding-Knowledge-Graphs-with-Literals"><a href="#Universal-Preprocessing-Operators-for-Embedding-Knowledge-Graphs-with-Literals" class="headerlink" title="Universal Preprocessing Operators for Embedding Knowledge Graphs with Literals"></a>Universal Preprocessing Operators for Embedding Knowledge Graphs with Literals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03023">http://arxiv.org/abs/2309.03023</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/patryk.preisner/mkga">https://gitlab.com/patryk.preisner/mkga</a></li>
<li>paper_authors: Patryk Preisner, Heiko Paulheim</li>
<li>for: 该论文主要针对知识图（KG）中的实体进行压缩表示，以便更好地进行知识检索和推论。</li>
<li>methods: 该论文提出了一组通用预处理算子，可以将KG中的文字、数值、时间和图像信息转换为numerical embedding，以便与任何嵌入方法结合使用。</li>
<li>results: 在kgbench数据集上，使用三种不同的嵌入方法，对 transformed KGs 进行嵌入后，得到了promising的结果。<details>
<summary>Abstract</summary>
Knowledge graph embeddings are dense numerical representations of entities in a knowledge graph (KG). While the majority of approaches concentrate only on relational information, i.e., relations between entities, fewer approaches exist which also take information about literal values (e.g., textual descriptions or numerical information) into account. Those which exist are typically tailored towards a particular modality of literal and a particular embedding method. In this paper, we propose a set of universal preprocessing operators which can be used to transform KGs with literals for numerical, temporal, textual, and image information, so that the transformed KGs can be embedded with any method. The results on the kgbench dataset with three different embedding methods show promising results.
</details>
<details>
<summary>摘要</summary>
知识图embeddings是知识图中实体的 dense数字表示。大多数方法只关注关系信息，即实体之间的关系，而 fewer方法会考虑实体的文字描述或数字信息。这些方法通常针对特定的modalität和嵌入方法进行定制。在这篇论文中，我们提议一组通用预处理操作，可以将知识图中的文字、时间、数字和图像信息转换为可以使用任何嵌入方法的格式。对kgbench数据集的实验结果表明，这些预处理操作可以取得承诺的结果。
</details></li>
</ul>
<hr>
<h2 id="Amortised-Inference-in-Bayesian-Neural-Networks"><a href="#Amortised-Inference-in-Bayesian-Neural-Networks" class="headerlink" title="Amortised Inference in Bayesian Neural Networks"></a>Amortised Inference in Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03018">http://arxiv.org/abs/2309.03018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sheev13/bnn_amort_inf">https://github.com/sheev13/bnn_amort_inf</a></li>
<li>paper_authors: Tommy Rochussen</li>
<li>for: 本研究目的是提出一种更加数据效率的概率元学习方法，以便在具有有限数据量的情况下进行预测。</li>
<li>methods: 我们提出了一种基于权重权值的概率元学习方法，即APOVI-BNN，它通过单个前向传播来实现权重权值的权值整合。</li>
<li>results: 我们的模型在有限数据量情况下表现出色，在一个一维回归问题和一个图像完成问题中，我们的模型在其他概率元学习模型中表现出色。<details>
<summary>Abstract</summary>
Meta-learning is a framework in which machine learning models train over a set of datasets in order to produce predictions on new datasets at test time. Probabilistic meta-learning has received an abundance of attention from the research community in recent years, but a problem shared by many existing probabilistic meta-models is that they require a very large number of datasets in order to produce high-quality predictions with well-calibrated uncertainty estimates. In many applications, however, such quantities of data are simply not available.   In this dissertation we present a significantly more data-efficient approach to probabilistic meta-learning through per-datapoint amortisation of inference in Bayesian neural networks, introducing the Amortised Pseudo-Observation Variational Inference Bayesian Neural Network (APOVI-BNN). First, we show that the approximate posteriors obtained under our amortised scheme are of similar or better quality to those obtained through traditional variational inference, despite the fact that the amortised inference is performed in a single forward pass. We then discuss how the APOVI-BNN may be viewed as a new member of the neural process family, motivating the use of neural process training objectives for potentially better predictive performance on complex problems as a result. Finally, we assess the predictive performance of the APOVI-BNN against other probabilistic meta-models in both a one-dimensional regression problem and in a significantly more complex image completion setting. In both cases, when the amount of training data is limited, our model is the best in its class.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>机器学习框架中的元学习是通过多个数据集进行训练，以生成新数据集上的预测。在过去几年中，对于probabilistic meta-learning而言，研究者们对其进行了大量的关注，但是现有的许多probabilistic meta-模型具有一个共同的问题，即它们需要很多数据集来生成高质量的预测和准确度估计。在许多应用中，这些数据集的量可能不够多。在这个论文中，我们提出了一种更加数据效率的元学习方法，通过权重参数化的概率meta-学习，引入了 Pseudo-Observation Variational Inference Bayesian Neural Network (APOVI-BNN)。我们首先表明，我们的approximate posterior obtained under our amortized scheme和traditional variational inference scheme的质量相似或更高，即使在单个前进推进行权重参数化的概率meta-学习。然后，我们讨论了APOVI-BNN如何被视为一种新的神经过程成员，并且motivate使用神经过程训练目标可能更好地适应复杂问题。最后，我们评估了APOVI-BNN的预测性能与其他probabilistic meta-模型在一个一维回归问题和一个更加复杂的图像完成问题中。在这两个问题中，当训练数据有限时，我们的模型成为其类型中的最佳。
</details></li>
</ul>
<hr>
<h2 id="SymED-Adaptive-and-Online-Symbolic-Representation-of-Data-on-the-Edge"><a href="#SymED-Adaptive-and-Online-Symbolic-Representation-of-Data-on-the-Edge" class="headerlink" title="SymED: Adaptive and Online Symbolic Representation of Data on the Edge"></a>SymED: Adaptive and Online Symbolic Representation of Data on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03014">http://arxiv.org/abs/2309.03014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Hofstätter, Shashikant Ilager, Ivan Lujic, Ivona Brandic</li>
<li>for: 该论文旨在处理互联网物联网（IoT）生成的数据，使用Edge computing模式将数据处理靠近数据源。</li>
<li>methods: 该论文提出了一种Symbolic Representation（SR）算法，用于将原始数据转换为符号，以便在Edge设备上进行数据分析（例如异常检测和趋势预测）。SR算法是一种可靠的数据压缩技术，但现有的SR算法都是中央化的设计，无法处理实时数据。该论文提出了一种在线、适应、分布式的Symbolic Edge Data表示方法（SymED）。</li>
<li>results: 该论文的实验结果表明，SymED可以（i）将原始数据压缩到9.5%的平均压缩率；(ii)在DTW空间保持低的重建误差（13.25）；(iii)同时提供在线适应性，以便处理实时流动的IoT数据，typical latency为42ms每个符号。<details>
<summary>Abstract</summary>
The edge computing paradigm helps handle the Internet of Things (IoT) generated data in proximity to its source. Challenges occur in transferring, storing, and processing this rapidly growing amount of data on resource-constrained edge devices. Symbolic Representation (SR) algorithms are promising solutions to reduce the data size by converting actual raw data into symbols. Also, they allow data analytics (e.g., anomaly detection and trend prediction) directly on symbols, benefiting large classes of edge applications. However, existing SR algorithms are centralized in design and work offline with batch data, which is infeasible for real-time cases. We propose SymED - Symbolic Edge Data representation method, i.e., an online, adaptive, and distributed approach for symbolic representation of data on edge. SymED is based on the Adaptive Brownian Bridge-based Aggregation (ABBA), where we assume low-powered IoT devices do initial data compression (senders) and the more robust edge devices do the symbolic conversion (receivers). We evaluate SymED by measuring compression performance, reconstruction accuracy through Dynamic Time Warping (DTW) distance, and computational latency. The results show that SymED is able to (i) reduce the raw data with an average compression rate of 9.5%; (ii) keep a low reconstruction error of 13.25 in the DTW space; (iii) simultaneously provide real-time adaptability for online streaming IoT data at typical latencies of 42ms per symbol, reducing the overall network traffic.
</details>
<details>
<summary>摘要</summary>
Edge compute 模式可以处理互联网对象（IoT）生成的数据在其原始位置附近。但是，将这样快速增长的数据转移、存储和处理在资源有限的边缘设备上存在挑战。symbolic representation（SR）算法是一种有 promise 的解决方案，可以将实际的Raw data 转换为符号，从而降低数据大小。此外，SR 还允许在符号上进行数据分析（例如，异常检测和趋势预测），对大多数边缘应用程序产生了好处。然而，现有的 SR 算法都是中央化的设计，并且在批处理数据的情况下进行了线上工作，这对实时情况不太可靠。我们提出了SymED - 符号Edge数据表示方法，即在线、适应性、分布式的Symbolic Representation方法。SymED基于Adaptive Brownian Bridge-based Aggregation（ABBA），假设低功率IoT设备进行初步数据压缩（发送器），而更 robust的边缘设备进行符号转换（接收器）。我们通过测量压缩性能、重建精度通过动态时间戳（DTW）距离和计算延迟来评估SymED。结果显示，SymED能够：(i) 将原始数据压缩到平均压缩率为9.5%；(ii) 在DTW空间保持低于13.25的重建错误；(iii) 同时提供在线适应性，对常见42ms每个符号的实时串流IoT数据进行实时适应。
</details></li>
</ul>
<hr>
<h2 id="Theoretical-Explanation-of-Activation-Sparsity-through-Flat-Minima-and-Adversarial-Robustness"><a href="#Theoretical-Explanation-of-Activation-Sparsity-through-Flat-Minima-and-Adversarial-Robustness" class="headerlink" title="Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness"></a>Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03004">http://arxiv.org/abs/2309.03004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Peng, Lei Qi, Yinghuan Shi, Yang Gao<br>for:这篇论文旨在解释 activation sparsity 的出现，以及如何采用 gradient sparsity 来解释其 Emergence。methods:这篇论文使用了 theoretical explanation 和 random matrix theory (RMT) 来分析 activation sparsity 的出现。results:这篇论文提出了两个 plug-and-play module 和一个 radical modification，以提高训练和推理的效率。 Validational experiments 表明这些 modify 可以提高 sparsity，并且可以降低训练和推理的成本。<details>
<summary>Abstract</summary>
A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources of flatness when arguing sparsities' necessity, we discover the phenomenon of spectral concentration, i.e., the ratio between the largest and the smallest non-zero singular values of weight matrices is small. We utilize random matrix theory (RMT) as a powerful theoretical tool to analyze stochastic gradient noises and discuss the emergence of spectral concentration. With these insights, we propose two plug-and-play modules for both training from scratch and sparsity finetuning, as well as one radical modification that only applies to from-scratch training. Another under-testing module for both sparsity and flatness is also immediate from our theories. Validational experiments are conducted to verify our explanation. Experiments for productivity demonstrate modifications' improvement in sparsity, indicating further theoretical cost reduction in both training and inference.
</details>
<details>
<summary>摘要</summary>
近期观察到多层感知（MLP）层中的活动稀畴可以带来很大的计算成本减少的机会。尽管一些研究归因于训练动力学，但理论上解释活动稀畴的出现仍然受到限制，只有在浅网络、小训练步骤和修改训练中才能解释。为了填补这些差距，我们提出了梯度稀畴的概念，作为活动稀畴的来源，并提出了基于这个概念的理论解释，它解释了梯度稀畴和活动稀畴是对抗攻击的必要步骤，这是因为隐藏特征和参数的平均值的折衔。这种理论适用于标准地训练的层 нор化纯MLP，并可以扩展到转换器或其他架构。为了消除其他源的平均值，我们发现了特征集中性现象，即权重矩阵的最大和最小非零特征值之间的比率很小。我们利用随机矩阵理论（RMT）作为一种强大的理论工具，分析随机梯度噪声，并讨论emergence of spectral concentration。基于这些发现，我们提出了两个插件和完善模块，以及一个 радикаль modification，只适用于从头开始训练。另外，我们还提出了一个尚未测试的模块，可以用于 both sparsity和平均性。验证性实验表明，我们的解释是正确的。产品性实验表明，这些修改可以提高减少训练和推理的计算成本。
</details></li>
</ul>
<hr>
<h2 id="Natural-and-Robust-Walking-using-Reinforcement-Learning-without-Demonstrations-in-High-Dimensional-Musculoskeletal-Models"><a href="#Natural-and-Robust-Walking-using-Reinforcement-Learning-without-Demonstrations-in-High-Dimensional-Musculoskeletal-Models" class="headerlink" title="Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models"></a>Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02976">http://arxiv.org/abs/2309.02976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Schumacher, Thomas Geijtenbeek, Vittorio Caggiano, Vikash Kumar, Syn Schmitt, Georg Martius, Daniel F. B. Haeufle</li>
<li>for: The paper aims to develop a reinforcement learning (RL) method for natural walking in complex natural environments, with the goal of achieving human-like walking without relying on extensive expert data sets.</li>
<li>methods: The paper uses a reinforcement learning approach to train a controller that can handle the multi-objective control problem of stability, robustness, and energy efficiency in bipedal walking. The controller is trained using a simulation environment and optimizes energy minimization as the objective function.</li>
<li>results: The paper achieves human-like walking with bipedal biomechanical models in 3D using reinforcement learning, without relying on extensive expert data sets. The resulting controllers are robust against perturbations and can adapt to new behaviors, demonstrating the potential of RL for studying human walking in complex natural environments.<details>
<summary>Abstract</summary>
Humans excel at robust bipedal walking in complex natural environments. In each step, they adequately tune the interaction of biomechanical muscle dynamics and neuronal signals to be robust against uncertainties in ground conditions. However, it is still not fully understood how the nervous system resolves the musculoskeletal redundancy to solve the multi-objective control problem considering stability, robustness, and energy efficiency. In computer simulations, energy minimization has been shown to be a successful optimization target, reproducing natural walking with trajectory optimization or reflex-based control methods. However, these methods focus on particular motions at a time and the resulting controllers are limited when compensating for perturbations. In robotics, reinforcement learning~(RL) methods recently achieved highly stable (and efficient) locomotion on quadruped systems, but the generation of human-like walking with bipedal biomechanical models has required extensive use of expert data sets. This strong reliance on demonstrations often results in brittle policies and limits the application to new behaviors, especially considering the potential variety of movements for high-dimensional musculoskeletal models in 3D. Achieving natural locomotion with RL without sacrificing its incredible robustness might pave the way for a novel approach to studying human walking in complex natural environments. Videos: https://sites.google.com/view/naturalwalkingrl
</details>
<details>
<summary>摘要</summary>
人类在复杂的自然环境中能够表现出稳定的双脚行走。每一步，他们能够有效地调整生物机械动力学和神经信号的交互，以使得行走具有对地面条件不确定性的鲁棒性。然而，nervous system如何解决musculoskeletal redundancy以解决多目标控制问题，包括稳定性、鲁棒性和能效性，仍未完全了解。在计算机 simulations中，能量最小化已经被证明是一个成功的优化目标，通过轨迹优化或刺激控制方法来复制自然的行走。然而，这些方法通常只关注特定的动作，而 resulting controllers 有限制性，无法赔偿干扰。在机器人学中，使用奖励学习（RL）方法已经实现了高稳定性（而且高效）的四足系统行走，但生成人类类似的行走需要大量的专家数据集。这种强依赖于示例的方式通常会导致 brittle policies 和限制应用于新的行为，特别是考虑到高维musculoskeletal模型在3D中的可能性的多样性。实现自然的行走通过RL而不 sacrificing its incredible robustness 可能会开启一种新的方法来研究人类在复杂自然环境中的行走。Video: <https://sites.google.com/view/naturalwalkingrl>
</details></li>
</ul>
<hr>
<h2 id="On-the-Impact-of-Feeding-Cost-Risk-in-Aquaculture-Valuation-and-Decision-Making"><a href="#On-the-Impact-of-Feeding-Cost-Risk-in-Aquaculture-Valuation-and-Decision-Making" class="headerlink" title="On the Impact of Feeding Cost Risk in Aquaculture Valuation and Decision Making"></a>On the Impact of Feeding Cost Risk in Aquaculture Valuation and Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02970">http://arxiv.org/abs/2309.02970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kevinkamm/aquaculturestochasticfeeding">https://github.com/kevinkamm/aquaculturestochasticfeeding</a></li>
<li>paper_authors: Christian Oliver Ewald, Kevin Kamm</li>
<li>for: 研究了动物商品中的随机 alimentation 成本对影响，特别是关注于养殖业。</li>
<li>methods: 使用了 soybean futures 来推断鲑鱼饲料的随机行为，假设饲料采用 Schwartz-2-factor 模型。 compare了使用决策规则，包括随机或决定性 alimentation 成本，以及不包括 alimentation 成本风险。</li>
<li>results: 发现在一些情况下，考虑到随机 alimentation 成本可以导致显著改善，而在其他情况下，决定性 alimentation 成本可以作为备用。新采用的规则都显示出了更好的性能，而计算成本几乎为零。此外，我们还使用了深度神经网络来推断决策边界，从而改进了以往的回归方法和拟合方法。<details>
<summary>Abstract</summary>
We study the effect of stochastic feeding costs on animal-based commodities with particular focus on aquaculture. More specifically, we use soybean futures to infer on the stochastic behaviour of salmon feed, which we assume to follow a Schwartz-2-factor model. We compare the decision of harvesting salmon using a decision rule assuming either deterministic or stochastic feeding costs, i.e. including feeding cost risk. We identify cases, where accounting for stochastic feeding costs leads to significant improvements as well as cases where deterministic feeding costs are a good enough proxy. Nevertheless, in all of these cases, the newly derived rules show superior performance, while the additional computational costs are negligible. From a methodological point of view, we demonstrate how to use Deep-Neural-Networks to infer on the decision boundary that determines harvesting or continuation, improving on more classical regression-based and curve-fitting methods. To achieve this we use a deep classifier, which not only improves on previous results but also scales well for higher dimensional problems, and in addition mitigates effects due to model uncertainty, which we identify in this article. effects due to model uncertainty, which we identify in this article.
</details>
<details>
<summary>摘要</summary>
我们研究生物动物商品的影响，特别是鱼养。我们使用сояbean futures来推测鲑鱼饲料的随机行为，假设采用斯威尔-2-因子模型。我们比较使用决策规则，假设饲料成本是确定的或随机的，即包括饲料成本风险。我们发现在某些情况下，考虑随机饲料成本会导致显著改善，而在其他情况下，确定饲料成本是可以作为备用的。然而，在所有情况下，我们新 derive的规则具有更高的性能，而计算成本也很低。从方法学的角度来看，我们示例了如何使用深度神经网络来推测决策边界，这有所提高过传统的回归分析和适应方法。为了实现这一点，我们使用深度分类器，不仅提高了前一代结果，还可以适应更高维度的问题，同时减轻模型不确定性的影响。
</details></li>
</ul>
<hr>
<h2 id="CR-VAE-Contrastive-Regularization-on-Variational-Autoencoders-for-Preventing-Posterior-Collapse"><a href="#CR-VAE-Contrastive-Regularization-on-Variational-Autoencoders-for-Preventing-Posterior-Collapse" class="headerlink" title="CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse"></a>CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02968">http://arxiv.org/abs/2309.02968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fotios Lygerakis, Elmar Rueckert</li>
<li>for: 解决Variational Autoencoder（VAE）的后降现象，即 latent representation 与输入 Independence。</li>
<li>methods: 提出了一种新的解决方案——强制对应regularization for Variational Autoencoders（CR-VAE），通过增加一个对应的对比目标函数，以最大化输入和其latent representation之间的信息流。</li>
<li>results: 在多个视觉数据集上测试，CR-VAE 比前一个方法更好地避免后降现象，并且表现更好。<details>
<summary>Abstract</summary>
The Variational Autoencoder (VAE) is known to suffer from the phenomenon of \textit{posterior collapse}, where the latent representations generated by the model become independent of the inputs. This leads to degenerated representations of the input, which is attributed to the limitations of the VAE's objective function. In this work, we propose a novel solution to this issue, the Contrastive Regularization for Variational Autoencoders (CR-VAE). The core of our approach is to augment the original VAE with a contrastive objective that maximizes the mutual information between the representations of similar visual inputs. This strategy ensures that the information flow between the input and its latent representation is maximized, effectively avoiding posterior collapse. We evaluate our method on a series of visual datasets and demonstrate, that CR-VAE outperforms state-of-the-art approaches in preventing posterior collapse.
</details>
<details>
<summary>摘要</summary>
“Variational Autoencoder（VAE）经常会面临“ posterior collapse”现象，即生成的伪阶梯表现与输入无关。这导致输入的表现受损，归因于VAE的目标函数所带来的限制。在这个研究中，我们提出一个新的解决方案，即对VAE进行对比调整（CR-VAE）。我们的方法是通过增加原始VAE的对比目标，以 Maximize the mutual information between the representations of similar visual inputs。这策略可以确保输入和其伪阶梯表现之间的信息流汇流，彻底避免 posterior collapse。我们在一系列的视觉数据集上评估了我们的方法，并证明了CR-VAE可以更好地避免 posterior collapse。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="M3D-NCA-Robust-3D-Segmentation-with-Built-in-Quality-Control"><a href="#M3D-NCA-Robust-3D-Segmentation-with-Built-in-Quality-Control" class="headerlink" title="M3D-NCA: Robust 3D Segmentation with Built-in Quality Control"></a>M3D-NCA: Robust 3D Segmentation with Built-in Quality Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02954">http://arxiv.org/abs/2309.02954</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Kalkhof, Anirban Mukhopadhyay</li>
<li>for: 这篇论文的目的是提出一种具有效率和可靠性的医疗影像分类方法，以应对资源受限的环境和对影像领域的变化。</li>
<li>methods: 本文使用的方法是基于神经细胞自动机（NCA）的分类方法，并通过n-level patchification来实现3D医疗影像的分类。此外，本文还提出了一个基于M3D-NCA的质量指标，可自动检测NCAs中的错误。</li>
<li>results: 相比于UNet模型，M3D-NCA在脑径和膀胱分类中表现出2%的 Dies积分误差，并且可以在Raspberry Pi 4 Model B（2GB RAM）上运行。这表明M3D-NCA可能是一个有效和可靠的医疗影像分类方法，尤其在资源受限的环境中。<details>
<summary>Abstract</summary>
Medical image segmentation relies heavily on large-scale deep learning models, such as UNet-based architectures. However, the real-world utility of such models is limited by their high computational requirements, which makes them impractical for resource-constrained environments such as primary care facilities and conflict zones. Furthermore, shifts in the imaging domain can render these models ineffective and even compromise patient safety if such errors go undetected. To address these challenges, we propose M3D-NCA, a novel methodology that leverages Neural Cellular Automata (NCA) segmentation for 3D medical images using n-level patchification. Moreover, we exploit the variance in M3D-NCA to develop a novel quality metric which can automatically detect errors in the segmentation process of NCAs. M3D-NCA outperforms the two magnitudes larger UNet models in hippocampus and prostate segmentation by 2% Dice and can be run on a Raspberry Pi 4 Model B (2GB RAM). This highlights the potential of M3D-NCA as an effective and efficient alternative for medical image segmentation in resource-constrained environments.
</details>
<details>
<summary>摘要</summary>
医疗图像分割依赖于大规模深度学习模型，如UNet基于的建筑。然而，实际应用中这些模型的计算需求很高，使其在资源有限的环境中，如初级医疗机构和武装冲突区域，成为不切实际的。此外，图像领域的变化可以让这些模型失效，甚至威胁 patient safety 如果这些错误未经检测。为解决这些挑战，我们提出了 M3D-NCA，一种新的方法，利用神经细胞自动机（NCA）分割三维医疗图像，使用 n-level 补丁化。此外，我们利用 M3D-NCA 的变异来开发一种新的质量指标，可以自动检测 NCAs 分割过程中的错误。M3D-NCA 在 hippocampus 和肾脏分割中比两个 UNet 模型高出 2% Dice，并且可以在 Raspberry Pi 4 Model B（2GB RAM）上运行。这说明 M3D-NCA 可以作为医疗图像分割的有效和高效的替代方案。
</details></li>
</ul>
<hr>
<h2 id="EvoCLINICAL-Evolving-Cyber-Cyber-Digital-Twin-with-Active-Transfer-Learning-for-Automated-Cancer-Registry-System"><a href="#EvoCLINICAL-Evolving-Cyber-Cyber-Digital-Twin-with-Active-Transfer-Learning-for-Automated-Cancer-Registry-System" class="headerlink" title="EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System"></a>EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03246">http://arxiv.org/abs/2309.03246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simula-complex/evoclinical">https://github.com/simula-complex/evoclinical</a></li>
<li>paper_authors: Chengjie Lu, Qinghua Xu, Tao Yue, Shaukat Ali, Thomas Schwitalla, Jan F. Nygård</li>
<li>for: The paper is written for the Cancer Registry of Norway (CRN) and its stakeholders, providing a solution for the correct operation of the automated cancer registry system (GURI) and the synchronization of the cyber-cyber digital twin (CCDT) with the evolving GURI.</li>
<li>methods: The paper proposes EvoCLINICAL, a method that fine-tunes the pretrained CCDT with a dataset labelled by querying a new GURI version, using a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset.</li>
<li>results: The paper demonstrates the effectiveness of EvoCLINICAL through evaluation on three evolution processes, showing that the precision, recall, and F1 score are all greater than 91%. Additionally, the paper shows that employing active learning in EvoCLINICAL consistently improves its performance.Here is the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文是为抗癌 региistry of Norway（CRN）和其各方面利益者提供的解决方案，确保自动化抗癌注册系统（GURI）的正确运行，以及 cyber-cyber数字生物体（CCDT）与不断发展的 GURI 同步。</li>
<li>methods: 论文提出了 EvoCLINICAL，一种方法，通过使用遗传算法选择优化的肿瘤消息子集，将预先训练的 CCDT 与新版 GURI 标注的数据进行微调。</li>
<li>results: 论文通过对三次进化过程进行评估，显示 EvoCLINICAL 的效果是非常高，其精度、回归率和 F1 分数都大于 91%。此外，论文还表明，在 EvoCLINICAL 中使用活动学习可以逐次提高其表现。<details>
<summary>Abstract</summary>
The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, and hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for the previous version of GURI as the pretrained model and fine-tunes it with the dataset labelled by querying a new GURI version. EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset and query GURI with it. We evaluate EvoCLINICAL on three evolution processes. The precision, recall, and F1 score are all greater than 91%, demonstrating the effectiveness of EvoCLINICAL. Furthermore, we replace the active learning part of EvoCLINICAL with random selection to study the contribution of transfer learning to the overall performance of EvoCLINICAL. Results show that employing active learning in EvoCLINICAL increases its performances consistently.
</details>
<details>
<summary>摘要</summary>
挪威癌症注册系统（CRN）收集癌症患者信息，通过收到医疗机构（如医学实验室和医院）发送的癌症消息。这些消息被自动化癌症注册系统：GURI validate。GURI的正常运行是关键，因为它为癌症研究提供关键的癌症相关统计，并且是基础设施。为了促进GURI的研发和分析，我们提出了基于数字响应的癌症数字双（CCDT）的构建。然而，GURI不断演化，因为新的医学诊断和治疗、技术进步等。因此，CCDT也需要不断更新，以同步 avec GURI。这种同步困难在于，需要新版本的GURI标注的大量数据来更新CCDT。为解决这个问题，我们提出了EvoCLINICAL，它将以前版本的CCDT作为预训练模型，并使用新版本GURI标注的数据进行微调。EvoCLINICAL使用遗传算法选择候选 dataset中的最佳子集，并将其提交给GURI进行查询。我们对EvoCLINICAL进行了三次演化测试，结果显示，精度、准确率和F1分数都高于91%，这表明EvoCLINICAL的效果。此外，我们将EvoCLINICAL的活动学习部分替换为随机选择，以研究转移学习对EvoCLINICAL的总表现的贡献。结果表明，在EvoCLINICAL中使用活动学习可以持续提高其表现。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-quantum-classical-fusion-neural-network-to-improve-protein-ligand-binding-affinity-predictions-for-drug-discovery"><a href="#A-hybrid-quantum-classical-fusion-neural-network-to-improve-protein-ligand-binding-affinity-predictions-for-drug-discovery" class="headerlink" title="A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery"></a>A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03919">http://arxiv.org/abs/2309.03919</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Banerjee, S. He Yuxun, S. Konakanchi, L. Ogunfowora, S. Roy, S. Selvaras, L. Domingo, M. Chehimi, M. Djukic, C. Johnson</li>
<li>for: 预测药物和蛋白质之间的绑定Affinity，特别是在疾病发展中直接影响蛋白质的蛋白质。</li>
<li>methods: 提出了一种新的Hybrid量子-классический深度学习模型，结合3D和空间图像卷积神经网络，并在优化的量子架构中进行了 synergistic Integration。</li>
<li>results: 对比于现有的类型模型，提出的模型在预测绑定Affinity方面提高了6%的准确率，同时对比于前期类型方法，其 convergence性能显著更稳定。<details>
<summary>Abstract</summary>
The field of drug discovery hinges on the accurate prediction of binding affinity between prospective drug molecules and target proteins, especially when such proteins directly influence disease progression. However, estimating binding affinity demands significant financial and computational resources. While state-of-the-art methodologies employ classical machine learning (ML) techniques, emerging hybrid quantum machine learning (QML) models have shown promise for enhanced performance, owing to their inherent parallelism and capacity to manage exponential increases in data dimensionality. Despite these advances, existing models encounter issues related to convergence stability and prediction accuracy. This paper introduces a novel hybrid quantum-classical deep learning model tailored for binding affinity prediction in drug discovery. Specifically, the proposed model synergistically integrates 3D and spatial graph convolutional neural networks within an optimized quantum architecture. Simulation results demonstrate a 6% improvement in prediction accuracy relative to existing classical models, as well as a significantly more stable convergence performance compared to previous classical approaches.
</details>
<details>
<summary>摘要</summary>
领域的药物发现核心在于准确预测药物分子和目标蛋白之间的绑定亲和力，特别是当这些蛋白直接影响疾病进程时。然而，估计绑定亲和力需要较大的金融和计算资源。现有的方法使用了经典的机器学习（ML）技术，而新兴的量子机器学习（QML）模型则表现出了改善表现，具有内置的并行性和数据维度的加法性。然而，现有的模型受到了稳定性和预测精度的限制。这篇论文提出了一种新的量子-古典深度学习模型，用于预测药物绑定亲和力。特别是，该模型 synergistically  integrate了3D和空间图 convolutional neural networks within an optimized quantum architecture。实验结果表明，该模型相比现有的古典模型，提高了预测精度的6%，同时也比前一些古典方法更加稳定地跑出结果。
</details></li>
</ul>
<hr>
<h2 id="Estimating-irregular-water-demands-with-physics-informed-machine-learning-to-inform-leakage-detection"><a href="#Estimating-irregular-water-demands-with-physics-informed-machine-learning-to-inform-leakage-detection" class="headerlink" title="Estimating irregular water demands with physics-informed machine learning to inform leakage detection"></a>Estimating irregular water demands with physics-informed machine learning to inform leakage detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02935">http://arxiv.org/abs/2309.02935</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swn-group-at-tu-berlin/lila-pinn">https://github.com/swn-group-at-tu-berlin/lila-pinn</a></li>
<li>paper_authors: Ivo Daniel, Andrea Cominola</li>
<li>for:  This paper aims to develop a physics-informed machine learning algorithm for timely identifying and accurately localizing leakages in drinking water distribution networks.</li>
<li>methods:  The algorithm utilizes pressure data and a fully connected neural network to estimate unknown irregular water demands, leveraging the Bernoulli equation and effectively linearizing the leakage detection problem.</li>
<li>results:  The algorithm was tested on data from the L-Town benchmark network and showed good capability for estimating most irregular demands, with R2 larger than 0.8. Identification results for leakages under the presence of irregular demands could be improved by a factor of 5.3 for abrupt leaks and a factor of 3.0 for incipient leaks when compared to results disregarding irregular demands.<details>
<summary>Abstract</summary>
Leakages in drinking water distribution networks pose significant challenges to water utilities, leading to infrastructure failure, operational disruptions, environmental hazards, property damage, and economic losses. The timely identification and accurate localisation of such leakages is paramount for utilities to mitigate these unwanted effects. However, implementation of algorithms for leakage detection is limited in practice by requirements of either hydraulic models or large amounts of training data. Physics-informed machine learning can utilise hydraulic information thereby circumventing both limitations. In this work, we present a physics-informed machine learning algorithm that analyses pressure data and therefrom estimates unknown irregular water demands via a fully connected neural network, ultimately leveraging the Bernoulli equation and effectively linearising the leakage detection problem. Our algorithm is tested on data from the L-Town benchmark network, and results indicate a good capability for estimating most irregular demands, with R2 larger than 0.8. Identification results for leakages under the presence of irregular demands could be improved by a factor of 5.3 for abrupt leaks and a factor of 3.0 for incipient leaks when compared the results disregarding irregular demands.
</details>
<details>
<summary>摘要</summary>
饮水供应网络中的泄漏问题对水公司带来了重大挑战，导致基础设施失效、运营干扰、环境危害、财务损失等。快速识别和准确定位泄漏是Utility公司控制这些不良影响的关键。然而，实施泄漏检测算法在实践中受到了水力模型的要求和大量的训练数据的限制。 физи学 Informed machine learning可以利用水力信息，这样就可以 circumvent这些限制。在这种工作中，我们提出了一种基于物理学习的泄漏检测算法，通过分析压力数据，以全连接神经网络的方式来估算不可知的异常水需求，最终利用白劳利方程，有效地线性化泄漏检测问题。我们的算法在L-Town标准网络数据集上进行测试，结果显示，我们的算法可以准确地估算大多数异常水需求，R2值大于0.8。在存在异常水需求的情况下，泄漏的标识结果可以提高了5.3倍（偏泄漏）和3.0倍（incipient leakage）。
</details></li>
</ul>
<hr>
<h2 id="GroupEnc-encoder-with-group-loss-for-global-structure-preservation"><a href="#GroupEnc-encoder-with-group-loss-for-global-structure-preservation" class="headerlink" title="GroupEnc: encoder with group loss for global structure preservation"></a>GroupEnc: encoder with group loss for global structure preservation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02917">http://arxiv.org/abs/2309.02917</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Novak, Sofie Van Gassen, Yvan Saeys</li>
<li>for: 本研究旨在提出一种基于Variational Autoencoder（VAE）和SQuadMDS算法的深度学习模型，用于降维高维数据并 preserve its结构。</li>
<li>methods: 该模型使用了一种叫做’group loss’函数，以避免VAE模型中的全局结构扭曲问题，同时保持模型参数化和架构灵活。</li>
<li>results: 通过使用公共可用的生物单细胞脱氧核酸数据集进行验证，本研究发现该模型可以更好地 preserve 数据的结构，并且在RNX曲线上表现较好。<details>
<summary>Abstract</summary>
Recent advances in dimensionality reduction have achieved more accurate lower-dimensional embeddings of high-dimensional data. In addition to visualisation purposes, these embeddings can be used for downstream processing, including batch effect normalisation, clustering, community detection or trajectory inference. We use the notion of structure preservation at both local and global levels to create a deep learning model, based on a variational autoencoder (VAE) and the stochastic quartet loss from the SQuadMDS algorithm. Our encoder model, called GroupEnc, uses a 'group loss' function to create embeddings with less global structure distortion than VAEs do, while keeping the model parametric and the architecture flexible. We validate our approach using publicly available biological single-cell transcriptomic datasets, employing RNX curves for evaluation.
</details>
<details>
<summary>摘要</summary>
近期的维度减少技术已经实现了更高精度的lower-dimensional嵌入。除了可视化目的外，这些嵌入还可以用于下游处理，包括批处理normalization、聚类、社区探测或轨迹推断。我们基于variational autoencoder（VAE）和stochastic quartet loss（SQuadMDS）算法提出了一种深度学习模型，称之为GroupEnc。我们的编码器模型使用了'group loss'函数来创建具有较少全局结构扭曲的嵌入，而保持模型参数化和架构灵活。我们使用公共可用的生物单细胞肽转录数据集进行验证，并使用RNX曲线进行评估。
</details></li>
</ul>
<hr>
<h2 id="Persona-aware-Generative-Model-for-Code-mixed-Language"><a href="#Persona-aware-Generative-Model-for-Code-mixed-Language" class="headerlink" title="Persona-aware Generative Model for Code-mixed Language"></a>Persona-aware Generative Model for Code-mixed Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02915">http://arxiv.org/abs/2309.02915</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/victor7246/paradox">https://github.com/victor7246/paradox</a></li>
<li>paper_authors: Ayan Sengupta, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: 本研究旨在开发一种基于人物的代码混淆生成模型，以生成更加真实的混淆文本。</li>
<li>methods: 该模型基于Transformer编码器-解码器，通过编码每句话根据用户人物来生成混淆文本，并提出了一个对齐模块以 garantizar生成的序列匹配实际混淆文本的格式。</li>
<li>results: 对比非人物基于模型，PARADOX在四个新指标中平均表现出1.6个BLEU分、47%更好的混淆性和32%更好的语义凝集性。<details>
<summary>Abstract</summary>
Code-mixing and script-mixing are prevalent across online social networks and multilingual societies. However, a user's preference toward code-mixing depends on the socioeconomic status, demographics of the user, and the local context, which existing generative models mostly ignore while generating code-mixed texts. In this work, we make a pioneering attempt to develop a persona-aware generative model to generate texts resembling real-life code-mixed texts of individuals. We propose a Persona-aware Generative Model for Code-mixed Generation, PARADOX, a novel Transformer-based encoder-decoder model that encodes an utterance conditioned on a user's persona and generates code-mixed texts without monolingual reference data. We propose an alignment module that re-calibrates the generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed texts that are semantically more meaningful and linguistically more valid. To evaluate the personification capabilities of PARADOX, we propose four new metrics -- CM BLEU, CM Rouge-1, CM Rouge-L and CM KS. On average, PARADOX achieves 1.6 points better CM BLEU, 47% better perplexity and 32% better semantic coherence than the non-persona-based counterparts.
</details>
<details>
<summary>摘要</summary>
《Code-mixing和script-mixing在在线社交媒体和多语言社会中很普遍。然而，用户对代码混合的偏好取决于用户的社oeconomicStatus、用户的人口结构和当地的文化环境，现有的生成模型大多忽略这些因素。在这项工作中，我们提出了一种人物意识感知的代码混合生成模型，名为PARADOX。PARADOX是一种基于Transformer的编码器-解码器模型，对于一句话来说，通过用户的人物来Conditional Encoding，生成代码混合文本，不需要单语言参考数据。我们还提出了一个对齐模块，用于重新调整生成的序列，使其更加接近实际的代码混合文本。PARADOX生成的代码混合文本具有更高的semantic coherence和linguistic validity。为评估PARADOX的人格化能力，我们提出了四种新的评价指标：CM BLEU、CM Rouge-1、CM Rouge-L和CM KS。在 average，PARADOX在这些指标中表现了1.6个BLEU点、47%的perplexity和32%的semantic coherence提高。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the Traditional Chinese version of the text, and some characters and phrases may be different in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Ensemble-DNN-for-Age-of-Information-Minimization-in-UAV-assisted-Networks"><a href="#Ensemble-DNN-for-Age-of-Information-Minimization-in-UAV-assisted-Networks" class="headerlink" title="Ensemble DNN for Age-of-Information Minimization in UAV-assisted Networks"></a>Ensemble DNN for Age-of-Information Minimization in UAV-assisted Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02913">http://arxiv.org/abs/2309.02913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mouhamed Naby Ndiaye, El Houcine Bergou, Hajar El Hammouti</li>
<li>For: 本研究旨在解决无人机支持网络中的年龄信息问题（Age-of-Information，AoI）。我们的目标是最小化各设备之间的预期AoI。* Methods: 我们首先 derive了AoI的闭式表达式，然后将问题转化为非束缚最小化问题，并使用Ensemble Deep Neural Network（EDNN）方法解决。具体来说，我们使用了Lagrangian函数来隐式地培养DNNs。* Results: 我们的实验表明，提议的EDNN方法可以减少预期AoI，实现了$29.5%$的减少。<details>
<summary>Abstract</summary>
This paper addresses the problem of Age-of-Information (AoI) in UAV-assisted networks. Our objective is to minimize the expected AoI across devices by optimizing UAVs' stopping locations and device selection probabilities. To tackle this problem, we first derive a closed-form expression of the expected AoI that involves the probabilities of selection of devices. Then, we formulate the problem as a non-convex minimization subject to quality of service constraints. Since the problem is challenging to solve, we propose an Ensemble Deep Neural Network (EDNN) based approach which takes advantage of the dual formulation of the studied problem. Specifically, the Deep Neural Networks (DNNs) in the ensemble are trained in an unsupervised manner using the Lagrangian function of the studied problem. Our experiments show that the proposed EDNN method outperforms traditional DNNs in reducing the expected AoI, achieving a remarkable reduction of $29.5\%$.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Multimodal-Learning-Framework-for-Comprehensive-3D-Mineral-Prospectivity-Modeling-with-Jointly-Learned-Structure-Fluid-Relationships"><a href="#A-Multimodal-Learning-Framework-for-Comprehensive-3D-Mineral-Prospectivity-Modeling-with-Jointly-Learned-Structure-Fluid-Relationships" class="headerlink" title="A Multimodal Learning Framework for Comprehensive 3D Mineral Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships"></a>A Multimodal Learning Framework for Comprehensive 3D Mineral Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02911">http://arxiv.org/abs/2309.02911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Zheng, Hao Deng, Ruisheng Wang, Jingjie Wu</li>
<li>for: 这个研究旨在开发一个新的多modal融合模型，用于三维矿物潜力地图（3D MPM），实现了深度网络架构的多modal融合。</li>
<li>methods: 这个模型使用了卷积神经网（CNN）和多层感知神经网（MLP），并使用了对映分析（CCA）来调整和融合多modal的特征。</li>
<li>results: 实验结果显示，这个模型在分辨矿物含有区域和预测矿物潜力方面表现出色，比其他模型更好，并且通过ablation研究确定了联合特征使用和CCA包含的好处。<details>
<summary>Abstract</summary>
This study presents a novel multimodal fusion model for three-dimensional mineral prospectivity mapping (3D MPM), effectively integrating structural and fluid information through a deep network architecture. Leveraging Convolutional Neural Networks (CNN) and Multilayer Perceptrons (MLP), the model employs canonical correlation analysis (CCA) to align and fuse multimodal features. Rigorous evaluation on the Jiaojia gold deposit dataset demonstrates the model's superior performance in distinguishing ore-bearing instances and predicting mineral prospectivity, outperforming other models in result analyses. Ablation studies further reveal the benefits of joint feature utilization and CCA incorporation. This research not only advances mineral prospectivity modeling but also highlights the pivotal role of data integration and feature alignment for enhanced exploration decision-making.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "three-dimensional mineral prospectivity mapping" (3D MPM) is translated as "三维矿产可能性映射" (3D MPM)* "Convolutional Neural Networks" (CNN) is translated as "卷积神经网络" (CNN)* "Multilayer Perceptrons" (MLP) is translated as "多层感知器" (MLP)* "canonical correlation analysis" (CCA) is translated as "同异分析" (CCA)* "ore-bearing instances" is translated as "矿石样本" (ore-bearing instances)* "mineral prospectivity" is translated as "矿产可能性" (mineral prospectivity)* "ablation studies" is translated as "缺除研究" (ablation studies)
</details></li>
</ul>
<hr>
<h2 id="DECODE-Data-driven-Energy-Consumption-Prediction-leveraging-Historical-Data-and-Environmental-Factors-in-Buildings"><a href="#DECODE-Data-driven-Energy-Consumption-Prediction-leveraging-Historical-Data-and-Environmental-Factors-in-Buildings" class="headerlink" title="DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings"></a>DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02908">http://arxiv.org/abs/2309.02908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Mishra, Haroon R. Lone, Aayush Mishra</li>
<li>for: 预测建筑物的能源消耗，以提高效率的能源管理。</li>
<li>methods: 使用历史能源数据、占用模式和天气情况来预测建筑物的能源消耗，并使用Long Short-Term Memory（LSTM）模型进行预测。</li>
<li>results: 比较多种预测方法，LSTM模型在短、中、长期预测方面具有较高的准确率和最佳的 mean absolute error（MAE）值，并且可以使用有限的数据进行高效的预测。<details>
<summary>Abstract</summary>
Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consumption forecasts even when trained on a limited dataset. We address concerns about overfitting (variance) and underfitting (bias) through rigorous training and evaluation on real-world data. In summary, our research contributes to energy prediction by offering a robust LSTM model that outperforms alternative methods and operates with remarkable efficiency, generalizability, and reliability.
</details>
<details>
<summary>摘要</summary>
Compared to linear regression, decision trees, and random forest, the proposed LSTM model demonstrates exceptional prediction accuracy, with an R2 score of 0.97 and a mean absolute error (MAE) of 0.007. The model is also efficient in achieving accurate energy consumption forecasts even when trained on a limited dataset. To address concerns about overfitting and underfitting, the model is trained and evaluated on real-world data.In summary, this research contributes to energy prediction by offering a robust LSTM model that outperforms alternative methods and operates with remarkable efficiency, generalizability, and reliability.
</details></li>
</ul>
<hr>
<h2 id="Testing-properties-of-distributions-in-the-streaming-model"><a href="#Testing-properties-of-distributions-in-the-streaming-model" class="headerlink" title="Testing properties of distributions in the streaming model"></a>Testing properties of distributions in the streaming model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03245">http://arxiv.org/abs/2309.03245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Sampriti Roy, Yadu Vasudev</li>
<li>for: 测试分布的标准访问模型和条件访问模型中的内存限制下的分布性测试</li>
<li>methods: 使用优化的样本数量和内存占用来测试分布性，并提供了一个可靠的分布表示法</li>
<li>results: 可以efficiently学习一个峰值分布，并且可以将其扩展到更大的可分解分布集合上<details>
<summary>Abstract</summary>
We study distribution testing in the standard access model and the conditional access model when the memory available to the testing algorithm is bounded. In both scenarios, the samples appear in an online fashion and the goal is to test the properties of distribution using an optimal number of samples subject to a memory constraint on how many samples can be stored at a given time. First, we provide a trade-off between the sample complexity and the space complexity for testing identity when the samples are drawn according to the conditional access oracle. We then show that we can learn a succinct representation of a monotone distribution efficiently with a memory constraint on the number of samples that are stored that is almost optimal. We also show that the algorithm for monotone distributions can be extended to a larger class of decomposable distributions.
</details>
<details>
<summary>摘要</summary>
我们研究分布测试在标准访问模型和条件访问模型中，当内存可用于测试算法是有限的情况下。在两个场景下，样本会出现在在线的方式下，目标是使用最优的样本数量来测试分布的性质，受到内存限制如何保存样本。首先，我们提供了样本复杂性和空间复杂性之间的贸易OFF，当样本按照条件访问 oracle 采样时。然后，我们示出了可以高效地学习均衡分布的简短表示，并且内存限制是几乎最优的。最后，我们展示了该算法可以扩展到更大的分布类型。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Framework-for-Discovering-Discrete-Symmetries"><a href="#A-Unified-Framework-for-Discovering-Discrete-Symmetries" class="headerlink" title="A Unified Framework for Discovering Discrete Symmetries"></a>A Unified Framework for Discovering Discrete Symmetries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02898">http://arxiv.org/abs/2309.02898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Karjol, Rohan Kashyap, Aditya Gopalan, Prathosh A. P</li>
<li>for: 学习一个对Symmetry的函数</li>
<li>methods: 使用多臂炮算法和梯度下降来有效地优化线性和张量函数，以实现Symmetry的探索</li>
<li>results: 在图像数字和多项式回归任务上，提出了一种有效的方法，可以快速和有效地探索Symmetry<details>
<summary>Abstract</summary>
We consider the problem of learning a function respecting a symmetry from among a class of symmetries. We develop a unified framework that enables symmetry discovery across a broad range of subgroups including locally symmetric, dihedral and cyclic subgroups. At the core of the framework is a novel architecture composed of linear and tensor-valued functions that expresses functions invariant to these subgroups in a principled manner. The structure of the architecture enables us to leverage multi-armed bandit algorithms and gradient descent to efficiently optimize over the linear and the tensor-valued functions, respectively, and to infer the symmetry that is ultimately learnt. We also discuss the necessity of the tensor-valued functions in the architecture. Experiments on image-digit sum and polynomial regression tasks demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
我们考虑了学习一个函数尊重一种对称性的问题，从一类对称性中选择函数。我们开发了一个统一的框架，可以在各种子群中找到对称的发现，包括地方对称、二元对称和循环对称子群。框架的核心是一种新的建筑，包括线性和张量函数，这些函数可以在对称性原理下表示对称的函数。这种结构使我们可以利用多臂投机算法和梯度下降来有效地优化线性函数和张量函数，并从中推断出 ultimately 学习的对称性。我们还讨论了张量函数的必要性。在图像数字和多项式回归任务中进行了实验，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Non-Clashing-Teaching-Maps-for-Balls-in-Graphs"><a href="#Non-Clashing-Teaching-Maps-for-Balls-in-Graphs" class="headerlink" title="Non-Clashing Teaching Maps for Balls in Graphs"></a>Non-Clashing Teaching Maps for Balls in Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02876">http://arxiv.org/abs/2309.02876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jérémie Chalopin, Victor Chepoi, Fionn Mc Inerney, Sébastien Ratel</li>
<li>for: This paper is focused on studying non-clashing teaching and its applications in machine learning, particularly in the context of concept learning.</li>
<li>methods: The authors use techniques from graph theory and combinatorics to derive non-clashing teaching maps (NCTMs) and non-clashing teaching dimension (NCTD) for the concept class $\mathcal{B}(G)$ consisting of all balls of a graph $G$. They also use the theory of NP-completeness to prove lower bounds on the size of NCTMs and NCTD.</li>
<li>results: The authors show that the decision problem {\sc B-NCTD$^+$} for NCTD$^+$ is NP-complete in split, co-bipartite, and bipartite graphs, and they provide matching upper bounds for trees, interval graphs, cycles, and trees of cycles. They also design an approximate NCTM$^+$ for Gromov-hyperbolic graphs of size 2.<details>
<summary>Abstract</summary>
Recently, Kirkpatrick et al. [ALT 2019] and Fallat et al. [JMLR 2023] introduced non-clashing teaching and showed it to be the most efficient machine teaching model satisfying the benchmark for collusion-avoidance set by Goldman and Mathias. A teaching map $T$ for a concept class $\cal{C}$ assigns a (teaching) set $T(C)$ of examples to each concept $C \in \cal{C}$. A teaching map is non-clashing if no pair of concepts are consistent with the union of their teaching sets. The size of a non-clashing teaching map (NCTM) $T$ is the maximum size of a $T(C)$, $C \in \cal{C}$. The non-clashing teaching dimension NCTD$(\cal{C})$ of $\cal{C}$ is the minimum size of an NCTM for $\cal{C}$. NCTM$^+$ and NCTD$^+(\cal{C})$ are defined analogously, except the teacher may only use positive examples.   We study NCTMs and NCTM$^+$s for the concept class $\mathcal{B}(G)$ consisting of all balls of a graph $G$. We show that the associated decision problem {\sc B-NCTD$^+$} for NCTD$^+$ is NP-complete in split, co-bipartite, and bipartite graphs. Surprisingly, we even prove that, unless the ETH fails, {\sc B-NCTD$^+$} does not admit an algorithm running in time $2^{2^{o(vc)}\cdot n^{O(1)}$, nor a kernelization algorithm outputting a kernel with $2^{o(vc)}$ vertices, where vc is the vertex cover number of $G$. These are extremely rare results: it is only the second (fourth, resp.) problem in NP to admit a double-exponential lower bound parameterized by vc (treewidth, resp.), and only one of very few problems to admit an ETH-based conditional lower bound on the number of vertices in a kernel. We complement these lower bounds with matching upper bounds. For trees, interval graphs, cycles, and trees of cycles, we derive NCTM$^+$s or NCTMs for $\mathcal{B}(G)$ of size proportional to its VC-dimension. For Gromov-hyperbolic graphs, we design an approximate NCTM$^+$ for $\mathcal{B}(G)$ of size 2.
</details>
<details>
<summary>摘要</summary>
最近， Kirkpatrick 等人（ALT 2019）和 Fallat 等人（JMLR 2023）提出了不冲突教学模型，并证明它是最有效的机器教学模型，满足 Golman 和 Mathias 的合法性标准。一个教学地图 $T$ 对于一个概念集合 $\cal{C}$ 将每个概念 $C \in \cal{C}$  assigns一个（教学）集 $T(C)$ 的示例。一个教学地图是不冲突的，如果没有任何两个概念的教学集合的union是一致的。教学地图的大小（NCTM）是最大的 $T(C)$ 的大小，其中 $C \in \cal{C}$。概念集合 $\cal{C}$ 的不冲突教学维度（NCTD）是最小的 NCTM 的大小。NCTM 和 NCTD 的定义类似，只是教师可以仅使用正例示例。我们研究了 NCTM 和 NCTM 加上的概念集合 $\mathcal{B}(G)$，其中 $G$ 是一个图。我们证明了关联的决策问题（B-NCTD +) 是 NP 完备的，并且在 split、co-bipartite 和 bipartite 图中是 NP 完备的。这些结果非常罕见：只有第二个（第四个，resp.）的问题在 NP 中具有 double-exponential 下界参数化的 vc（vertex cover number of G），并且只有一些问题具有 ETH 基于的 conditional lower bound 参数化的 vertices 的数量。我们补充了这些下界参数化的下界。对于树、间隔图、循环图和树的循环图，我们得到了 NCTM 加上或 NCTM 的大小与其 VC 维度相当。对于 Gromov-hyperbolic 图，我们设计了一个 Approximate NCTM 加上的大小为 2。
</details></li>
</ul>
<hr>
<h2 id="Learning-Hybrid-Dynamics-Models-With-Simulator-Informed-Latent-States"><a href="#Learning-Hybrid-Dynamics-Models-With-Simulator-Informed-Latent-States" class="headerlink" title="Learning Hybrid Dynamics Models With Simulator-Informed Latent States"></a>Learning Hybrid Dynamics Models With Simulator-Informed Latent States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02873">http://arxiv.org/abs/2309.02873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Ensinger, Sebastian Ziesche, Sebastian Trimpe</li>
<li>for: 这 paper 的目的是提出一种新的 Hybrid 模型，用于将学习模型和物理模型相结合，以提高预测结果的物理意义和精度。</li>
<li>methods: 这 paper 使用了一种基于观察器的方法，将学习模型的latent state informing via black-box simulator，以避免预测结果的积累错误。</li>
<li>results: 这 paper 的实验结果表明，这种 Hybrid 模型可以更好地控制预测结果，并且可以避免因学习模型的缺失而导致的预测错误。<details>
<summary>Abstract</summary>
Dynamics model learning deals with the task of inferring unknown dynamics from measurement data and predicting the future behavior of the system. A typical approach to address this problem is to train recurrent models. However, predictions with these models are often not physically meaningful. Further, they suffer from deteriorated behavior over time due to accumulating errors. Often, simulators building on first principles are available being physically meaningful by design. However, modeling simplifications typically cause inaccuracies in these models. Consequently, hybrid modeling is an emerging trend that aims to combine the best of both worlds. In this paper, we propose a new approach to hybrid modeling, where we inform the latent states of a learned model via a black-box simulator. This allows to control the predictions via the simulator preventing them from accumulating errors. This is especially challenging since, in contrast to previous approaches, access to the simulator's latent states is not available. We tackle the task by leveraging observers, a well-known concept from control theory, inferring unknown latent states from observations and dynamics over time. In our learning-based setting, we jointly learn the dynamics and an observer that infers the latent states via the simulator. Thus, the simulator constantly corrects the latent states, compensating for modeling mismatch caused by learning. To maintain flexibility, we train an RNN-based residuum for the latent states that cannot be informed by the simulator.
</details>
<details>
<summary>摘要</summary>
随机动力模型学习的任务是从测量数据中推断未知动力学和未来系统的行为预测。一般来说，用训练回归模型来解决这个问题。然而，这些模型的预测通常不是物理意义上的。另外，它们因时间的积累而受到衰减的影响。而Physics-based模拟器通常是物理意义上的，但模型简化通常会导致不准确。因此，Hybrid模型是一种趋势，它旨在结合这两种世界。在这篇论文中，我们提出了一种新的Hybrid模型方法，我们通过黑盒模拟器来控制learned模型的预测。这是因为，与前一种方法不同，我们不可以直接访问黑盒模拟器的latent状态。我们利用observer，一种控制理论中的概念，在观测和动力学上时间的推断未知latent状态。在我们的学习基于的设置中，我们同时学习动力学和一个observer，它可以通过黑盒模拟器来推断latent状态。因此，黑盒模拟器不断地更正latent状态，以补做模型匹配问题。为保持灵活性，我们在learned模型中训练一个RNN基的剩余，用于latent状态的推断。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Momentum-Knowledge-Distillation-in-Online-Continual-Learning"><a href="#Rethinking-Momentum-Knowledge-Distillation-in-Online-Continual-Learning" class="headerlink" title="Rethinking Momentum Knowledge Distillation in Online Continual Learning"></a>Rethinking Momentum Knowledge Distillation in Online Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02870">http://arxiv.org/abs/2309.02870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Michel, Maorong Wang, Ling Xiao, Toshihiko Yamasaki</li>
<li>for: 这篇论文主要针对在连续数据流中训练神经网络，并且数据只能看到一次。</li>
<li>methods: 论文提出了一种直接又有效的方法，即将动力知识传播（MKD）应用于多个标志性OCL方法中，以提高现有方法的准确率。</li>
<li>results: 论文表明，通过应用MKD，可以提高现有state-of-the-art的准确率超过10%点，并且解释了MKD在OCL训练中的内幕和影响。<details>
<summary>Abstract</summary>
Online Continual Learning (OCL) addresses the problem of training neural networks on a continuous data stream where multiple classification tasks emerge in sequence. In contrast to offline Continual Learning, data can be seen only once in OCL. In this context, replay-based strategies have achieved impressive results and most state-of-the-art approaches are heavily depending on them. While Knowledge Distillation (KD) has been extensively used in offline Continual Learning, it remains under-exploited in OCL, despite its potential. In this paper, we theoretically analyze the challenges in applying KD to OCL. We introduce a direct yet effective methodology for applying Momentum Knowledge Distillation (MKD) to many flagship OCL methods and demonstrate its capabilities to enhance existing approaches. In addition to improving existing state-of-the-arts accuracy by more than $10\%$ points on ImageNet100, we shed light on MKD internal mechanics and impacts during training in OCL. We argue that similar to replay, MKD should be considered a central component of OCL.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:在线连续学习（OCL） addresses the problem of training神经网络在连续数据流中进行多个分类任务的顺序出现。与离线连续学习不同，在OCL中数据只能看一次。在这个上下文中，回放基本策略取得了非常出色的结果，而现状的大部分方法都倚靠它们。而知识传授（KD）在离线连续学习中广泛使用，但在OCL中它尚未得到充分利用，尽管它在这个领域潜在的潜力很大。在这篇论文中，我们理论分析了在OCL中应用KD的挑战。我们介绍了一种直接又有效的方法，将摩托力知识传授（MKD）应用到许多标准OCL方法中，并证明它能够改进现有的状态的表现。除了在ImageNet100上提高现有状态的准确率 более10个百分点外，我们还照明了MKD在训练中的内部机制和影响。我们认为，与回放一样，MKD应该被视为OCL的中央组成部分。
</details></li>
</ul>
<hr>
<h2 id="On-Reducing-Undesirable-Behavior-in-Deep-Reinforcement-Learning-Models"><a href="#On-Reducing-Undesirable-Behavior-in-Deep-Reinforcement-Learning-Models" class="headerlink" title="On Reducing Undesirable Behavior in Deep Reinforcement Learning Models"></a>On Reducing Undesirable Behavior in Deep Reinforcement Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02869">http://arxiv.org/abs/2309.02869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ophir M. Carmel, Guy Katz</li>
<li>for: 提高深度强化学习（DRL）系统的可靠性和可理解性，降低DRL系统的不良行为。</li>
<li>methods: 提出了一种基于EXTRACTING decision tree classifiers的 Framework，将其 integrate into DRL 训练 loop，以便评价系统的错误行为。</li>
<li>results: 实现了一种可以 straightforwardly extend 现有框架，仅增加训练时间的负面影响，可以significantly reduce the frequency of undesirable behavior，并且在一些情况下，even improve performance。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) has proven extremely useful in a large variety of application domains. However, even successful DRL-based software can exhibit highly undesirable behavior. This is due to DRL training being based on maximizing a reward function, which typically captures general trends but cannot precisely capture, or rule out, certain behaviors of the system. In this paper, we propose a novel framework aimed at drastically reducing the undesirable behavior of DRL-based software, while maintaining its excellent performance. In addition, our framework can assist in providing engineers with a comprehensible characterization of such undesirable behavior. Under the hood, our approach is based on extracting decision tree classifiers from erroneous state-action pairs, and then integrating these trees into the DRL training loop, penalizing the system whenever it performs an error. We provide a proof-of-concept implementation of our approach, and use it to evaluate the technique on three significant case studies. We find that our approach can extend existing frameworks in a straightforward manner, and incurs only a slight overhead in training time. Further, it incurs only a very slight hit to performance, or even in some cases - improves it, while significantly reducing the frequency of undesirable behavior.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Event-Sequence-Modeling-with-Contrastive-Relational-Inference"><a href="#Enhancing-Event-Sequence-Modeling-with-Contrastive-Relational-Inference" class="headerlink" title="Enhancing Event Sequence Modeling with Contrastive Relational Inference"></a>Enhancing Event Sequence Modeling with Contrastive Relational Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02868">http://arxiv.org/abs/2309.02868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Wang, Zhixuan Chu, Tao Zhou, Caigao Jiang, Hongyan Hao, Minjie Zhu, Xindong Cai, Qing Cui, Longfei Li, James Y Zhang, Siqiao Xue, Jun Zhou</li>
<li>for: 这个论文主要用于模型连续时间事件序列，尤其是捕捉事件之间的交互关系，以进行事件序列预测任务。</li>
<li>methods: 该论文提出了一种新的方法，即基于神经 relacional inference（NRI）的 Hawkes 过程，可以同时学习事件序列的动态模式和事件间关系。该方法使用了强迫推理的方法，通过搜索prototype路径来拟合关系约束。</li>
<li>results: 实验结果表明，该模型能够有效地捕捉事件序列中事件间的交互关系，并且在三个实际数据集上表现出色。<details>
<summary>Abstract</summary>
Neural temporal point processes(TPPs) have shown promise for modeling continuous-time event sequences. However, capturing the interactions between events is challenging yet critical for performing inference tasks like forecasting on event sequence data. Existing TPP models have focused on parameterizing the conditional distribution of future events but struggle to model event interactions. In this paper, we propose a novel approach that leverages Neural Relational Inference (NRI) to learn a relation graph that infers interactions while simultaneously learning the dynamics patterns from observational data. Our approach, the Contrastive Relational Inference-based Hawkes Process (CRIHP), reasons about event interactions under a variational inference framework. It utilizes intensity-based learning to search for prototype paths to contrast relationship constraints. Extensive experiments on three real-world datasets demonstrate the effectiveness of our model in capturing event interactions for event sequence modeling tasks.
</details>
<details>
<summary>摘要</summary>
neural temporal point processes (TPPs) 有推荐力模型 continuous-time 事件序列。然而，捕捉事件之间的互动是挑战性强且重要的，以便在事件序列数据上进行预测任务。现有的 TPP 模型主要关注未来事件的 conditional 分布，但它们忽略了事件之间的互动。在这篇论文中，我们提出了一种新的方法，利用 Neural Relational Inference (NRI) 来学习一个关系图，从事件序列数据中推断事件之间的互动关系，同时也学习事件序列的动态模式。我们的方法，即 Contrastive Relational Inference-based Hawkes Process (CRIHP)，在 variational inference 框架下进行事件互动的推理。它利用了强度学习来搜索prototype path，以规定关系约束。我们在三个实际 datasets 上进行了广泛的实验， demonstrates 我们的模型能够准确地捕捉事件之间的互动，以便进行事件序列模型任务。
</details></li>
</ul>
<hr>
<h2 id="A-recommender-for-the-management-of-chronic-pain-in-patients-undergoing-spinal-cord-stimulation"><a href="#A-recommender-for-the-management-of-chronic-pain-in-patients-undergoing-spinal-cord-stimulation" class="headerlink" title="A recommender for the management of chronic pain in patients undergoing spinal cord stimulation"></a>A recommender for the management of chronic pain in patients undergoing spinal cord stimulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03918">http://arxiv.org/abs/2309.03918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tigran Tchrakian, Mykhaylo Zayats, Alessandra Pascale, Dat Huynh, Pritish Parida, Carla Agurto Rios, Sergiy Zhuk, Jeffrey L. Rogers, ENVISION Studies Physician Author Group, Boston Scientific Research Scientists Consortium</li>
<li>For: 这篇研究的目的是为了提供一个推荐系统，以帮助抗痛症病人在进行脊梗刺激治疗时，选择最佳的刺激参数。* Methods: 这篇研究使用了一种名为多臂环境推荐（CMAB）的方法，将推荐疗法传递给病人，以改善他们的病情。这个系统使用了一个数位健康生态系统，让病人可以在自己的家中使用，并且与病人监控系统相互作用，以关闭病人的疗法循环。* Results: 这篇研究发现，使用这个推荐系统可以帮助抗痛症病人获得更好的治疗效果。在一群脊梗刺激植入器的患者中，使用这个系统可以 statistically significant 地提高生活质量指标和患者状态（PS）的改善率。在 moderate PS 的患者中（N&#x3D;7），100% 的患者表现了 statistically significant 的改善，并且 5&#x2F;7 的患者有 improved PS dwell time。<details>
<summary>Abstract</summary>
Spinal cord stimulation (SCS) is a therapeutic approach used for the management of chronic pain. It involves the delivery of electrical impulses to the spinal cord via an implanted device, which when given suitable stimulus parameters can mask or block pain signals. Selection of optimal stimulation parameters usually happens in the clinic under the care of a provider whereas at-home SCS optimization is managed by the patient. In this paper, we propose a recommender system for the management of pain in chronic pain patients undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB) approach to develop a system that recommends SCS settings to patients with the aim of improving their condition. These recommendations, sent directly to patients though a digital health ecosystem, combined with a patient monitoring system closes the therapeutic loop around a chronic pain patient over their entire patient journey. We evaluated the system in a cohort of SCS-implanted ENVISION study subjects (Clinicaltrials.gov ID: NCT03240588) using a combination of quality of life metrics and Patient States (PS), a novel measure of holistic outcomes. SCS recommendations provided statistically significant improvement in clinical outcomes (pain and/or QoL) in 85\% of all subjects (N=21). Among subjects in moderate PS (N=7) prior to receiving recommendations, 100\% showed statistically significant improvements and 5/7 had improved PS dwell time. This analysis suggests SCS patients may benefit from SCS recommendations, resulting in additional clinical improvement on top of benefits already received from SCS therapy.
</details>
<details>
<summary>摘要</summary>
脊梗刺激疗法（SCS）是一种治疗方法用于管理慢性疼痛。它通过在脊梗中植入设备，发送电rical impulses，可以阻塞或掩盖疼痛信号。在临床中选择最佳刺激参数通常由提供者进行，而在家用SCS优化则由患者自行管理。本文提出一种基于多重抓拍机（CMAB）的推荐系统，用于chronic pain患者在receiving SCS治疗时的疼痛管理。这些推荐，通过数字医疗生态系统直接发送给患者，与患者监测系统结合，实现了chronic pain患者的整体疾病管理过程。我们在ENVISION研究（Clinicaltrials.gov ID：NCT03240588）中使用了质量生活指标和患者状态（PS），一种新的总体结果度量，对这些推荐进行评估。SCS推荐给85%的所有 subjects（N=21）带来了 statistically significant的临床改善（疼痛和/或质量生活）。在moderate PS（N=7）前receiving推荐的 subjects中，100%显示了 statistically significant的改善，并有5/7 subjects的 PS dwell time进行了改善。这一分析表明SCS患者可能会从SCS推荐中受益，从而得到额外的临床改善。
</details></li>
</ul>
<hr>
<h2 id="Generalised-Mutual-Information-a-Framework-for-Discriminative-Clustering"><a href="#Generalised-Mutual-Information-a-Framework-for-Discriminative-Clustering" class="headerlink" title="Generalised Mutual Information: a Framework for Discriminative Clustering"></a>Generalised Mutual Information: a Framework for Discriminative Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02858">http://arxiv.org/abs/2309.02858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Ohl, Pierre-Alexandre Mattei, Charles Bouveyron, Warith Harchaoui, Mickaël Leclercq, Arnaud Droit, Frédéric Precioso</li>
<li>for: 本研究旨在探讨深度归一化中使用的互信息（MI）是否适用于归一化训练 neural network 的 clustering 目标，并提出一种基于 distance 或 kernel 的扩展方法来改进 MI。</li>
<li>methods: 本研究使用了一种基于 MI 的归一化训练方法，并对其进行了改进，包括在训练过程中添加正则化项以提高归一化质量。</li>
<li>results: 研究发现，使用 GEMINI 可以在不需要正则化的情况下实现更好的归一化质量，并且可以自动选择合适的cluster数量，这些特性在深度归一化 clustering 中尚未得到了充分研究。<details>
<summary>Abstract</summary>
In the last decade, recent successes in deep clustering majorly involved the Mutual Information (MI) as an unsupervised objective for training neural networks with increasing regularisations. While the quality of the regularisations have been largely discussed for improvements, little attention has been dedicated to the relevance of MI as a clustering objective. In this paper, we first highlight how the maximisation of MI does not lead to satisfying clusters. We identified the Kullback-Leibler divergence as the main reason of this behaviour. Hence, we generalise the mutual information by changing its core distance, introducing the Generalised Mutual Information (GEMINI): a set of metrics for unsupervised neural network training. Unlike MI, some GEMINIs do not require regularisations when training as they are geometry-aware thanks to distances or kernels in the data space. Finally, we highlight that GEMINIs can automatically select a relevant number of clusters, a property that has been little studied in deep discriminative clustering context where the number of clusters is a priori unknown.
</details>
<details>
<summary>摘要</summary>
最近一个 décennie，深度划分大多数使用了互信息（MI）作为无监督目标函数来训练神经网络，并逐渐增加了正则化。然而，对于改进质量的正则化，相对较少的注意力被吸引到互信息作为划分目标的可重要性。在这篇论文中，我们首先指出了maximizing MI不能导致满意的划分。我们认为，库拉布-莱布尔异常（KL divergence）是主要的原因。因此，我们总结了互信息，并引入了通用的互信息（GEMINI）：一组无监督神经网络训练中的度量集合。与MI不同，一些GEMINIs不需要训练时添加正则化，因为它们在数据空间中具有geometry-awareness，可以通过距离或核函数来保证。最后，我们指出了GEMINIs可以自动选择相关的几个划分，这是在深度探测划分上，划分数量是先验不知的情况下，很少被研究的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Review-of-Common-Log-Data-Sets-Used-for-Evaluation-of-Sequence-based-Anomaly-Detection-Techniques"><a href="#A-Critical-Review-of-Common-Log-Data-Sets-Used-for-Evaluation-of-Sequence-based-Anomaly-Detection-Techniques" class="headerlink" title="A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques"></a>A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02854">http://arxiv.org/abs/2309.02854</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ait-aecid/anomaly-detection-log-datasets">https://github.com/ait-aecid/anomaly-detection-log-datasets</a></li>
<li>paper_authors: Max Landauer, Florian Skopik, Markus Wurzenberger</li>
<li>for: 本研究旨在分析六种公开available的日志数据集，以探讨异常现象的表现和简单的检测技术。</li>
<li>methods: 本研究使用了深度学习技术来检测日志数据中的异常现象，并分析了这些异常的表现。</li>
<li>results: 研究发现，大多数异常不直接与顺序表现相关，并且高度的检测技术不必要以获得高检测率。<details>
<summary>Abstract</summary>
Log data store event execution patterns that correspond to underlying workflows of systems or applications. While most logs are informative, log data also include artifacts that indicate failures or incidents. Accordingly, log data are often used to evaluate anomaly detection techniques that aim to automatically disclose unexpected or otherwise relevant system behavior patterns. Recently, detection approaches leveraging deep learning have increasingly focused on anomalies that manifest as changes of sequential patterns within otherwise normal event traces. Several publicly available data sets, such as HDFS, BGL, Thunderbird, OpenStack, and Hadoop, have since become standards for evaluating these anomaly detection techniques, however, the appropriateness of these data sets has not been closely investigated in the past. In this paper we therefore analyze six publicly available log data sets with focus on the manifestations of anomalies and simple techniques for their detection. Our findings suggest that most anomalies are not directly related to sequential manifestations and that advanced detection techniques are not required to achieve high detection rates on these data sets.
</details>
<details>
<summary>摘要</summary>
log数据存储事件执行模式，与系统或应用下的工作流相对应。大多数日志数据都很有用，但日志数据还包含了 artefacts， indicating failures or incidents.因此，日志数据经常用于评估异常检测技术，以揭示不可预期的系统行为模式。现在，使用深度学习的检测方法在增长。这些方法通常focus on sequential pattern changes within otherwise normal event traces，而publicly available data sets such as HDFS, BGL, Thunderbird, OpenStack, and Hadoop have become standards for evaluating these techniques. However, the appropriateness of these data sets has not been closely investigated in the past. In this paper, we therefore analyze six publicly available log data sets with a focus on the manifestations of anomalies and simple techniques for their detection. Our findings suggest that most anomalies are not directly related to sequential manifestations and that advanced detection techniques are not required to achieve high detection rates on these data sets.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distillation-Layer-that-Lets-the-Student-Decide"><a href="#Knowledge-Distillation-Layer-that-Lets-the-Student-Decide" class="headerlink" title="Knowledge Distillation Layer that Lets the Student Decide"></a>Knowledge Distillation Layer that Lets the Student Decide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02843">http://arxiv.org/abs/2309.02843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adagorgun/letkd-framework">https://github.com/adagorgun/letkd-framework</a></li>
<li>paper_authors: Ada Gorgun, Yeti Z. Gurbuz, A. Aydin Alatan</li>
<li>for: The paper is written for improving the performance of knowledge distillation (KD) in deep neural networks, specifically in the intermediate layers.</li>
<li>methods: The paper proposes a learnable KD layer for the student model, which embeds the teacher’s knowledge in the feature transform and improves KD with two distinct abilities: i) learning how to leverage the teacher’s knowledge, and ii) feeding forward the transferred knowledge deeper.</li>
<li>results: The paper demonstrates the effectiveness of the proposed approach on three popular classification benchmarks through rigorous experimentation, showing that the student model enjoys the teacher’s knowledge during inference as well as training.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提高深度神经网络中知识储存（KD）的性能，特别是在中间层。</li>
<li>methods: 论文提出了一种可学习的KD层，将教师模型的知识嵌入学生模型的特征转换中，并通过两种能力提高KD：i) 学习如何利用教师模型的知识，ii) 将知识传递 deeper。</li>
<li>results: 论文通过严谨的实验证明了该方法的有效性，在三个Popular classification benchmark上显示学生模型在推理过程中也能够获得教师模型的知识。<details>
<summary>Abstract</summary>
Typical technique in knowledge distillation (KD) is regularizing the learning of a limited capacity model (student) by pushing its responses to match a powerful model's (teacher). Albeit useful especially in the penultimate layer and beyond, its action on student's feature transform is rather implicit, limiting its practice in the intermediate layers. To explicitly embed the teacher's knowledge in feature transform, we propose a learnable KD layer for the student which improves KD with two distinct abilities: i) learning how to leverage the teacher's knowledge, enabling to discard nuisance information, and ii) feeding forward the transferred knowledge deeper. Thus, the student enjoys the teacher's knowledge during the inference besides training. Formally, we repurpose 1x1-BN-ReLU-1x1 convolution block to assign a semantic vector to each local region according to the template (supervised by the teacher) that the corresponding region of the student matches. To facilitate template learning in the intermediate layers, we propose a novel form of supervision based on the teacher's decisions. Through rigorous experimentation, we demonstrate the effectiveness of our approach on 3 popular classification benchmarks. Code is available at: https://github.com/adagorgun/letKD-framework
</details>
<details>
<summary>摘要</summary>
To implement this approach, we repurpose a 1x1-BN-ReLU-1x1 convolution block to assign a semantic vector to each local region of the student, based on the template (supervised by the teacher) that the corresponding region of the student matches. To facilitate template learning in the intermediate layers, we propose a novel form of supervision based on the teacher's decisions.We have conducted rigorous experiments on three popular classification benchmarks, and the results demonstrate the effectiveness of our approach. The code for our method is available at: https://github.com/adagorgun/letKD-framework.
</details></li>
</ul>
<hr>
<h2 id="Random-postprocessing-for-combinatorial-Bayesian-optimization"><a href="#Random-postprocessing-for-combinatorial-Bayesian-optimization" class="headerlink" title="Random postprocessing for combinatorial Bayesian optimization"></a>Random postprocessing for combinatorial Bayesian optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02842">http://arxiv.org/abs/2309.02842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keisuke Morita, Yoshihiko Nishikawa, Masayuki Ohzeki</li>
<li>for: 这个论文主要是关于 Bayesian 优化的Sequential 方法，包括 Bayesian 优化技术的各种应用。</li>
<li>methods: 这个论文使用的方法主要是 Bayesian 优化技术，以及一种减少重复采样的后处理方法。</li>
<li>results: 研究发现，使用后处理方法可以减少 Bayesian 优化的步骤数量，特别是当使用最大 posterior 估计时。这种简单 yet 通用的策略可以解决高维问题中 Bayesian 优化的慢速收敛问题。<details>
<summary>Abstract</summary>
Model-based sequential approaches to discrete "black-box" optimization, including Bayesian optimization techniques, often access the same points multiple times for a given objective function in interest, resulting in many steps to find the global optimum. Here, we numerically study the effect of a postprocessing method on Bayesian optimization that strictly prohibits duplicated samples in the dataset. We find the postprocessing method significantly reduces the number of sequential steps to find the global optimum, especially when the acquisition function is of maximum a posterior estimation. Our results provide a simple but general strategy to solve the slow convergence of Bayesian optimization for high-dimensional problems.
</details>
<details>
<summary>摘要</summary>
模型基于的顺序方法，包括 bayesian 优化技术，通常对于 Interest 函数多次访问同一个点，从而导致寻找全局最优点需要很多步骤。在这里，我们 numerically 研究了在 bayesian 优化中使用禁止重复样本的后处理方法的影响。我们发现该后处理方法可以大幅减少在找到全局最优点的sequential步骤数量，特别是当使用最大 posterior 估计的获取函数时。我们的结果提供了一种简单 yet 通用的策略来解决高维问题中 bayesian 优化的慢 converges。
</details></li>
</ul>
<hr>
<h2 id="EGIC-Enhanced-Low-Bit-Rate-Generative-Image-Compression-Guided-by-Semantic-Segmentation"><a href="#EGIC-Enhanced-Low-Bit-Rate-Generative-Image-Compression-Guided-by-Semantic-Segmentation" class="headerlink" title="EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation"></a>EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03244">http://arxiv.org/abs/2309.03244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikolai10/egic">https://github.com/nikolai10/egic</a></li>
<li>paper_authors: Nikolai Körber, Eduard Kromer, Andreas Siebert, Sascha Hauke, Daniel Mueller-Gritschneder</li>
<li>for: 这篇论文是为了提出一种新的生成图像压缩方法，以便高效地从单个模型中横跨干扰度-感知曲线。</li>
<li>methods: 该方法使用了一种隐式编码的变体图像 interpolating，预测MSE优化和GAN优化解码器输出之间的差异。在接收方side，用户可以控制GAN基于的重建中 residual的影响。</li>
<li>results: EGIC超过了多种感知 oriented和干扰度 oriented的基elines，包括HiFiC、MRIC和DIRAC，并且在干扰度端与VTM-20.0接近。EGIC简单实现、轻量级（例如0.18x HiFiC的模型参数），并且具有优秀的 interpolating 特性，使其成为实际应用中的优秀候选人。<details>
<summary>Abstract</summary>
We introduce EGIC, a novel generative image compression method that allows traversing the distortion-perception curve efficiently from a single model. Specifically, we propose an implicitly encoded variant of image interpolation that predicts the residual between a MSE-optimized and GAN-optimized decoder output. On the receiver side, the user can then control the impact of the residual on the GAN-based reconstruction. Together with improved GAN-based building blocks, EGIC outperforms a wide-variety of perception-oriented and distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and provides excellent interpolation characteristics, which makes it a promising candidate for practical applications targeting the low bit range.
</details>
<details>
<summary>摘要</summary>
我们介绍EGIC，一种新的生成式图像压缩方法，可以快速地从单一模型中进行扭曲视觉曲线的积极控制。具体来说，我们提出了一种隐式编码的图像 interpolate 方法，可以预测MSE优化和 GAN 优化对 Decoder 输出的差异。在接收端，用户可以控制这个差异对 GAN 基础的重建影响。与改进的 GAN 基础块相结合，EGIC 在视觉方面超过了广泛的感知方向和扭曲方向的基准，包括 HiFiC、MRIC 和 DIRAC，同时与 VTM-20.0 的扭曲端相差几乎没有差异。EGIC 简单实现、轻量级（例如，0.18x 模型参数相比 HiFiC），且具有优秀的 interpolate 特性，这使得它在实际应用中成为一个有前途的候选人。
</details></li>
</ul>
<hr>
<h2 id="BigVSAN-Enhancing-GAN-based-Neural-Vocoders-with-Slicing-Adversarial-Network"><a href="#BigVSAN-Enhancing-GAN-based-Neural-Vocoders-with-Slicing-Adversarial-Network" class="headerlink" title="BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network"></a>BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02836">http://arxiv.org/abs/2309.02836</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sony/bigvsan_eval">https://github.com/sony/bigvsan_eval</a></li>
<li>paper_authors: Takashi Shibuya, Yuhta Takida, Yuki Mitsufuji</li>
<li>for: 这 paper 的目的是研究 Generative Adversarial Network (GAN) 基于的 vocoder 是否可以通过改进 GAN 训练框架来提高音频 synthesis 的质量。</li>
<li>methods: 这 paper 使用了 Slicing Adversarial Network (SAN) 训练框架，通过修改 least-squares GAN 的损失函数来满足 SAN 的要求，以提高 GAN-based vocoder 的性能。</li>
<li>results: 经过实验，这 paper 发现 SAN 可以提高 GAN-based vocoder 的表现，包括 BigVGAN，并且需要小量修改。 codes 可以在 <a target="_blank" rel="noopener" href="https://github.com/sony/bigvsan">https://github.com/sony/bigvsan</a> 中找到。<details>
<summary>Abstract</summary>
Generative adversarial network (GAN)-based vocoders have been intensively studied because they can synthesize high-fidelity audio waveforms faster than real-time. However, it has been reported that most GANs fail to obtain the optimal projection for discriminating between real and fake data in the feature space. In the literature, it has been demonstrated that slicing adversarial network (SAN), an improved GAN training framework that can find the optimal projection, is effective in the image generation task. In this paper, we investigate the effectiveness of SAN in the vocoding task. For this purpose, we propose a scheme to modify least-squares GAN, which most GAN-based vocoders adopt, so that their loss functions satisfy the requirements of SAN. Through our experiments, we demonstrate that SAN can improve the performance of GAN-based vocoders, including BigVGAN, with small modifications. Our code is available at https://github.com/sony/bigvsan.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)生成对抗网络（GAN）基本的语音合成器已经得到了广泛的研究，因为它们可以在实时之前生成高质量的音频波形。然而，有研究表明，大多数GAN无法在特征空间找到优化的投影，以便将真实和假数据分开。在文献中，已经证明了剖面对抗网络（SAN）是一种改进的GAN训练框架，可以在特征空间找到优化的投影。在这篇论文中，我们 investigate SAN在语音合成任务中的效果。为了实现这一目标，我们提议修改最小二乘GAN的损失函数，使其满足SAN的要求。通过我们的实验，我们证明了SAN可以提高GAN基本的语音合成器，包括BigVGAN， WITH小 modification。我们的代码可以在https://github.com/sony/bigvsan上获取。
</details></li>
</ul>
<hr>
<h2 id="Roulette-A-Semantic-Privacy-Preserving-Device-Edge-Collaborative-Inference-Framework-for-Deep-Learning-Classification-Tasks"><a href="#Roulette-A-Semantic-Privacy-Preserving-Device-Edge-Collaborative-Inference-Framework-for-Deep-Learning-Classification-Tasks" class="headerlink" title="Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks"></a>Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02820">http://arxiv.org/abs/2309.02820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyi Li, Guocheng Liao, Lin Chen, Xu Chen</li>
<li>For: This paper proposes a framework for task-oriented semantic privacy-preserving collaborative inference for deep learning classifiers, with a focus on protecting the ground truth of the data as private information.* Methods: The proposed framework, called Roulette, uses a novel paradigm of split learning where the back-end DNN is frozen and the front-end DNN is retrained to be both a feature extractor and an encryptor. Additionally, the paper provides a differential privacy guarantee and analyzes the hardness of ground truth inference attacks.* Results: The paper conducts extensive performance evaluations using realistic datasets and demonstrates that Roulette can effectively defend against various attacks and achieve good model accuracy, improving inference accuracy by 21% averaged over benchmarks in a situation with severe non-i.i.d. data distribution.<details>
<summary>Abstract</summary>
Deep learning classifiers are crucial in the age of artificial intelligence. The device-edge-based collaborative inference has been widely adopted as an efficient framework for promoting its applications in IoT and 5G/6G networks. However, it suffers from accuracy degradation under non-i.i.d. data distribution and privacy disclosure. For accuracy degradation, direct use of transfer learning and split learning is high cost and privacy issues remain. For privacy disclosure, cryptography-based approaches lead to a huge overhead. Other lightweight methods assume that the ground truth is non-sensitive and can be exposed. But for many applications, the ground truth is the user's crucial privacy-sensitive information. In this paper, we propose a framework of Roulette, which is a task-oriented semantic privacy-preserving collaborative inference framework for deep learning classifiers. More than input data, we treat the ground truth of the data as private information. We develop a novel paradigm of split learning where the back-end DNN is frozen and the front-end DNN is retrained to be both a feature extractor and an encryptor. Moreover, we provide a differential privacy guarantee and analyze the hardness of ground truth inference attacks. To validate the proposed Roulette, we conduct extensive performance evaluations using realistic datasets, which demonstrate that Roulette can effectively defend against various attacks and meanwhile achieve good model accuracy. In a situation where the non-i.i.d. is very severe, Roulette improves the inference accuracy by 21\% averaged over benchmarks, while making the accuracy of discrimination attacks almost equivalent to random guessing.
</details>
<details>
<summary>摘要</summary>
深度学习分类器在人工智能时代具有核心作用。设备边缘基于的合作推理已广泛采用为提高其应用在互联网器 Things（IoT）和5G/6G网络中。然而，它受到异步数据分布的准确性下降和隐私泄露问题。Direct使用传输学习和分裂学习可能会带来高成本和隐私问题。为隐私泄露， криптография基本方法会带来巨大的负担。其他轻量级方法假设了地面 truth 是非敏感信息，可以暴露。但在许多应用程序中，地面 truth 是用户的关键隐私信息。在本文中，我们提出了一个名为 Roulette 的任务 oriented semantic privacy-preserving 合作推理框架，其中我们将地面 truth 视为私人信息。我们开发了一种新的分裂学习方法，其中后端 DNN 冻结，前端 DNN 重新训练为特征提取器和加密器。此外，我们提供了一种差分隐私保证，并分析了地面 truth 推理攻击的困难程度。为验证提出的 Roulette，我们进行了广泛的性能评估，使用实际数据集，其结果表明，Roulette 可以有效防止多种攻击，同时保持良好的模型准确率。在异步数据分布情况下，Roulette 可以提高推理准确率21%，而地面 truth 推理攻击的准确率接近随机猜测。
</details></li>
</ul>
<hr>
<h2 id="Combining-Thermodynamics-based-Model-of-the-Centrifugal-Compressors-and-Active-Machine-Learning-for-Enhanced-Industrial-Design-Optimization"><a href="#Combining-Thermodynamics-based-Model-of-the-Centrifugal-Compressors-and-Active-Machine-Learning-for-Enhanced-Industrial-Design-Optimization" class="headerlink" title="Combining Thermodynamics-based Model of the Centrifugal Compressors and Active Machine Learning for Enhanced Industrial Design Optimization"></a>Combining Thermodynamics-based Model of the Centrifugal Compressors and Active Machine Learning for Enhanced Industrial Design Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02818">http://arxiv.org/abs/2309.02818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shadi Ghiasi, Guido Pazzi, Concettina Del Grosso, Giovanni De Magistris, Giacomo Veneri<br>for: 这个论文主要是为了提高中心旋转压缩机的设计过程中的优化过程，使用活动学习（Active Learning）和 Gaussian Process 基于 surrogate 模型来减少计算成本。methods: 这个论文使用了一个内部的 thermodynamics-based 压缩机模型和 Gaussian Process 基于 surrogate 模型，在一个可deployable的 Active Learning  Setting 中结合了这两个模型，并通过了一些实验和扩展来提高 surrogate 模型的性能。results: 这个论文的实验结果表明，使用 ActiveCompDesign 框架可以significantly 提高 surrogate 模型的性能，并且在生产环境中可以减少压缩机的设计优化计算时间，相比于仅仅使用内部 thermodynamics-based 模型，提高了46%。<details>
<summary>Abstract</summary>
The design process of centrifugal compressors requires applying an optimization process which is computationally expensive due to complex analytical equations underlying the compressor's dynamical equations. Although the regression surrogate models could drastically reduce the computational cost of such a process, the major challenge is the scarcity of data for training the surrogate model. Aiming to strategically exploit the labeled samples, we propose the Active-CompDesign framework in which we combine a thermodynamics-based compressor model (i.e., our internal software for compressor design) and Gaussian Process-based surrogate model within a deployable Active Learning (AL) setting. We first conduct experiments in an offline setting and further, extend it to an online AL framework where a real-time interaction with the thermodynamics-based compressor's model allows the deployment in production. ActiveCompDesign shows a significant performance improvement in surrogate modeling by leveraging on uncertainty-based query function of samples within the AL framework with respect to the random selection of data points. Moreover, our framework in production has reduced the total computational time of compressor's design optimization to around 46% faster than relying on the internal thermodynamics-based simulator, achieving the same performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送文本到简化中文。<</SYS>>设计中心rifugal压缩机需要应用优化过程，这个过程 computationally expensive 因为压缩机的动力学方程在下面的复杂的分析方程中。虽然回归准确模型可以减少计算成本，但是主要挑战是缺乏训练数据。为了策略性利用标注样本，我们提出了 Active-CompDesign 框架，其 combining 内部的 thermodynamics-based 压缩机模型和 Gaussian Process 基于的准确模型在一个可deployable Active Learning （AL）设置下。我们首先在 offline 设置中进行实验，然后将其扩展到 online AL 框架，在生产环境中实现实时交互。ActiveCompDesign 在 surrogate 模型中表现出了显著的性能提高，通过在 AL 框架中使用样本uncertainty 基于的问题函数来选择样本。此外，我们的框架在生产环境中减少了压缩机的设计优化总计算时间约为 46%，并且实现了同等性能。
</details></li>
</ul>
<hr>
<h2 id="Automated-Bioinformatics-Analysis-via-AutoBA"><a href="#Automated-Bioinformatics-Analysis-via-AutoBA" class="headerlink" title="Automated Bioinformatics Analysis via AutoBA"></a>Automated Bioinformatics Analysis via AutoBA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03242">http://arxiv.org/abs/2309.03242</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joshuachou2018/autoba">https://github.com/joshuachou2018/autoba</a></li>
<li>paper_authors: Juexiao Zhou, Bin Zhang, Xiuying Chen, Haoyang Li, Xiaopeng Xu, Siyuan Chen, Xin Gao</li>
<li>for: 这份研究是为了提供一个自动化的 bioinformatics 分析工具，供处理各种 omics 数据。</li>
<li>methods: 这个工具基于大型自然语言模型，可以自动生成各种 bioinformatics 任务的详细步骤计划。用户只需提供少量的输入，便可以完成复杂的数据分析。</li>
<li>results: 这个工具在对不同类型的 omics 数据进行分析时，有着高度的自动化和灵活性。它可以根据输入数据的变化，自动设计分析过程。相比 online bioinformatics 服务，这个工具可以地方式进行分析，保持数据隐私。<details>
<summary>Abstract</summary>
With the fast-growing and evolving omics data, the demand for streamlined and adaptable tools to handle the analysis continues to grow. In response to this need, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AI agent based on a large language model designed explicitly for conventional omics data analysis. AutoBA simplifies the analytical process by requiring minimal user input while delivering detailed step-by-step plans for various bioinformatics tasks. Through rigorous validation by expert bioinformaticians, AutoBA's robustness and adaptability are affirmed across a diverse range of omics analysis cases, including whole genome sequencing (WGS), RNA sequencing (RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA's unique capacity to self-design analysis processes based on input data variations further underscores its versatility. Compared with online bioinformatic services, AutoBA deploys the analysis locally, preserving data privacy. Moreover, different from the predefined pipeline, AutoBA has adaptability in sync with emerging bioinformatics tools. Overall, AutoBA represents a convenient tool, offering robustness and adaptability for complex omics data analysis.
</details>
<details>
<summary>摘要</summary>
随着快速增长和发展的Omics数据，对于流lined和适应性好的分析工具的需求不断增长。为回应这个需求，我们介绍了自动生物信息分析（AutoBA），这是基于大型自然语言模型的自主AI代理，专门为传统的Omics数据分析设计。AutoBA简化了分析过程，需 minimal用户输入，并提供了详细的步骤计划 для多种生物信息分析任务。经过专家生物信息学家的严格验证，AutoBA在多种Omics分析场景中 Display robustness and adaptability，包括全 genomic sequencing（WGS）、RNA sequencing（RNA-seq）、单个细胞RNA-seq、ChIP-seq和 spatial transcriptomics。AutoBA的独特的自动设计分析过程基于输入数据的变化，进一步强调其 versatility。与在线生物信息服务相比，AutoBA在本地部署分析，保护数据隐私。此外，与预定的管道不同，AutoBA具有适应新生物信息工具的能力。总之，AutoBA表示一种便捷的工具，提供了 robustness和适应性 для复杂的Omics数据分析。
</details></li>
</ul>
<hr>
<h2 id="Introducing-Thermodynamics-Informed-Symbolic-Regression-–-A-Tool-for-Thermodynamic-Equations-of-State-Development"><a href="#Introducing-Thermodynamics-Informed-Symbolic-Regression-–-A-Tool-for-Thermodynamic-Equations-of-State-Development" class="headerlink" title="Introducing Thermodynamics-Informed Symbolic Regression – A Tool for Thermodynamic Equations of State Development"></a>Introducing Thermodynamics-Informed Symbolic Regression – A Tool for Thermodynamic Equations of State Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02805">http://arxiv.org/abs/2309.02805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scoop-group/tisr">https://github.com/scoop-group/tisr</a></li>
<li>paper_authors: Viktor Martinek, Ophelia Frotscher, Markus Richter, Roland Herzog</li>
<li>for: 这篇论文旨在描述一种新的符号回归工具，用于加速热动力学方程状态（EOS）的发展过程。</li>
<li>methods: 这种符号回归工具基于现有的符号回归工具，并添加了一些扩展来处理热动力学数据的散度和不同的剩余处理选项。</li>
<li>results: 这篇论文报道了这种符号回归工具的现状和进展，并讨论了未来的发展方向。<details>
<summary>Abstract</summary>
Thermodynamic equations of state (EOS) are essential for many industries as well as in academia. Even leaving aside the expensive and extensive measurement campaigns required for the data acquisition, the development of EOS is an intensely time-consuming process, which does often still heavily rely on expert knowledge and iterative fine-tuning. To improve upon and accelerate the EOS development process, we introduce thermodynamics-informed symbolic regression (TiSR), a symbolic regression (SR) tool aimed at thermodynamic EOS modeling. TiSR is already a capable SR tool, which was used in the research of https://doi.org/10.1007/s10765-023-03197-z. It aims to combine an SR base with the extensions required to work with often strongly scattered experimental data, different residual pre- and post-processing options, and additional features required to consider thermodynamic EOS development. Although TiSR is not ready for end users yet, this paper is intended to report on its current state, showcase the progress, and discuss (distant and not so distant) future directions. TiSR is available at https://github.com/scoop-group/TiSR and can be cited as https://doi.org/10.5281/zenodo.8317547.
</details>
<details>
<summary>摘要</summary>
thermodynamic equation of state (EOS) 是多个行业以及学术界的重要工具。即使不考虑数据收集的昂贵和广泛的测量活动，EOS的开发还是一个非常时间consuming的过程，它frequently仍然受到专家知识和迭代精细调整的限制。为了改进和加速EOS开发过程，我们介绍了thermodynamic-informed symbolic regression (TiSR)，一种符号 regression (SR) 工具，旨在用于 termodynamic EOS 模型化。TiSR 已经是一种可靠的 SR 工具，在 https://doi.org/10.1007/s10765-023-03197-z 中的研究中使用。它旨在将 SR 基础结合 thermodynamic EOS 开发所需的扩展，以及不同的剩下预处理和后处理选项，以及考虑 termodynamic EOS 开发中的其他特性。虽然 TiSR 还没有满足用户的需求，但这篇文章的目的是报告当前状况，展示进步，并讨论未来的方向。TiSR 可以在 https://github.com/scoop-group/TiSR 上获取，并可以引用为 https://doi.org/10.5281/zenodo.8317547。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Encoding-and-Decoding-of-Information-for-Split-Learning-in-Mobile-Edge-Computing-Leveraging-Information-Bottleneck-Theory"><a href="#Dynamic-Encoding-and-Decoding-of-Information-for-Split-Learning-in-Mobile-Edge-Computing-Leveraging-Information-Bottleneck-Theory" class="headerlink" title="Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory"></a>Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02787">http://arxiv.org/abs/2309.02787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Alhussein, Moshi Wei, Arashmid Akhavain</li>
<li>For: 本文提出了一种基于数据处理不等和信息瓶颈理论的分布式学习框架，用于在移动端计算中实现隐私保护的分布式学习。* Methods: 本文提出了一种基于encoder-decoder神经网络架构的新训练机制，可以在不同的实时网络条件和应用需求下进行调整，以提高预测性能。* Results: 作者通过应用该训练机制于一个基于 millimeter-wave 的吞吐量预测问题，实现了提高预测性能的目的。此外，文章还提供了一些关于循环神经网络的新视角和挑战。<details>
<summary>Abstract</summary>
Split learning is a privacy-preserving distributed learning paradigm in which an ML model (e.g., a neural network) is split into two parts (i.e., an encoder and a decoder). The encoder shares so-called latent representation, rather than raw data, for model training. In mobile-edge computing, network functions (such as traffic forecasting) can be trained via split learning where an encoder resides in a user equipment (UE) and a decoder resides in the edge network. Based on the data processing inequality and the information bottleneck (IB) theory, we present a new framework and training mechanism to enable a dynamic balancing of the transmission resource consumption with the informativeness of the shared latent representations, which directly impacts the predictive performance. The proposed training mechanism offers an encoder-decoder neural network architecture featuring multiple modes of complexity-relevance tradeoffs, enabling tunable performance. The adaptability can accommodate varying real-time network conditions and application requirements, potentially reducing operational expenditure and enhancing network agility. As a proof of concept, we apply the training mechanism to a millimeter-wave (mmWave)-enabled throughput prediction problem. We also offer new insights and highlight some challenges related to recurrent neural networks from the perspective of the IB theory. Interestingly, we find a compression phenomenon across the temporal domain of the sequential model, in addition to the compression phase that occurs with the number of training epochs.
</details>
<details>
<summary>摘要</summary>
分布式学习是一种隐私保护的分布式学习模式，在其中，一个机器学习模型（例如神经网络）被分解成两个部分（即编码器和解码器）。编码器将所谓的潜在表示分享给模型训练，而不是直接使用原始数据。在移动边缘计算中，网络功能（如交通预测）可以通过分布式学习进行训练，其中编码器位于用户设备（UE）中，而解码器位于边缘网络中。基于数据处理不等式和信息瓶颈（IB）理论，我们提出了一新的框架和训练机制，以实现在传输资源消耗和潜在表示的信息含量之间进行动态均衡，直接影响预测性能。我们的训练机制提供了一种具有多种复杂度-相关性质的编码器-解码器神经网络架构，可以实现调整性能。这种适应性可以满足不同的实时网络条件和应用要求，可能减少操作成本和提高网络灵活性。作为证明，我们将训练机制应用于使用 millimeter-wave（mmWave）技术实现的吞吐量预测问题。我们还提供了一些新的意见和挑战，从信息瓶颈理论的角度出发，关于循环神经网络的问题。有趣的是，我们发现在时间频谱中的压缩现象，以及与训练环节数的压缩相关的压缩阶段。
</details></li>
</ul>
<hr>
<h2 id="CVE-driven-Attack-Technique-Prediction-with-Semantic-Information-Extraction-and-a-Domain-specific-Language-Model"><a href="#CVE-driven-Attack-Technique-Prediction-with-Semantic-Information-Extraction-and-a-Domain-specific-Language-Model" class="headerlink" title="CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model"></a>CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02785">http://arxiv.org/abs/2309.02785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Aghaei, Ehab Al-Shaer</li>
<li>for: 本研究目的是bridging Common Vulnerabilities and Exposures (CVEs) 和 ATT&amp;CK 攻击技术之间的知识空白，以便更好地预测和防范网络攻击。</li>
<li>methods: 本研究使用了Semantic Role Labeling (SRL) 技术来从不结构化的网络安全报告中提取威胁动作，然后将其与MITRE的攻击功能类 correlate。这种自动化的相关性可以帮助创建标注数据，以便分类新的威胁动作到攻击技能类和 TTPs。</li>
<li>results: 实验结果表明，TTPpredictor 的准确率达到了 Approximately 98%，F1-score 在95% 到 98% 之间。与现状的语言模型工具如 ChatGPT 相比，TTPpredictor 表现更加出色。总的来说，本研究提供了一种可靠的解决方案，可以将 CVEs 与可能的攻击技术相连接，从而提高网络安全专业人员的预测和防范能力。<details>
<summary>Abstract</summary>
This paper addresses a critical challenge in cybersecurity: the gap between vulnerability information represented by Common Vulnerabilities and Exposures (CVEs) and the resulting cyberattack actions. CVEs provide insights into vulnerabilities, but often lack details on potential threat actions (tactics, techniques, and procedures, or TTPs) within the ATT&CK framework. This gap hinders accurate CVE categorization and proactive countermeasure initiation. The paper introduces the TTPpredictor tool, which uses innovative techniques to analyze CVE descriptions and infer plausible TTP attacks resulting from CVE exploitation. TTPpredictor overcomes challenges posed by limited labeled data and semantic disparities between CVE and TTP descriptions. It initially extracts threat actions from unstructured cyber threat reports using Semantic Role Labeling (SRL) techniques. These actions, along with their contextual attributes, are correlated with MITRE's attack functionality classes. This automated correlation facilitates the creation of labeled data, essential for categorizing novel threat actions into threat functionality classes and TTPs. The paper presents an empirical assessment, demonstrating TTPpredictor's effectiveness with accuracy rates of approximately 98% and F1-scores ranging from 95% to 98% in precise CVE classification to ATT&CK techniques. TTPpredictor outperforms state-of-the-art language model tools like ChatGPT. Overall, this paper offers a robust solution for linking CVEs to potential attack techniques, enhancing cybersecurity practitioners' ability to proactively identify and mitigate threats.
</details>
<details>
<summary>摘要</summary>
TTPpredictor overcomes challenges posed by limited labeled data and semantic disparities between CVE and TTP descriptions. It extracts threat actions from unstructured cyber threat reports using Semantic Role Labeling (SRL) techniques, and correlates these actions with MITRE's attack functionality classes. This automated correlation facilitates the creation of labeled data, essential for categorizing novel threat actions into threat functionality classes and TTPs.The paper presents an empirical assessment, demonstrating TTPpredictor's effectiveness with accuracy rates of approximately 98% and F1-scores ranging from 95% to 98% in precise CVE classification to ATT&CK techniques. TTPpredictor outperforms state-of-the-art language model tools like ChatGPT. This paper offers a robust solution for linking CVEs to potential attack techniques, enhancing cybersecurity practitioners' ability to proactively identify and mitigate threats.
</details></li>
</ul>
<hr>
<h2 id="Norm-Tweaking-High-performance-Low-bit-Quantization-of-Large-Language-Models"><a href="#Norm-Tweaking-High-performance-Low-bit-Quantization-of-Large-Language-Models" class="headerlink" title="Norm Tweaking: High-performance Low-bit Quantization of Large Language Models"></a>Norm Tweaking: High-performance Low-bit Quantization of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02784">http://arxiv.org/abs/2309.02784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Li, Qingyuan Li, Bo Zhang, Xiangxiang Chu</li>
<li>for: 这个论文目的是提出一种名为“norm tweaking”的技术，用于提高大型自然语言模型（LLM）的压缩精度，并且不需要牺牲准确性。</li>
<li>methods: 本文使用了一种名为“channel-wise distance constraint”的约化方法，将数据生成和调整过的数据用于更新normalization层的加重。</li>
<li>results: 本文的方法可以实现高精度的压缩，并且在多个测试数据集上表现出色，比较现有的PTQ方法更好。甚至在GLM-130B和OPT-66B上，本文的方法可以达到浮点数压缩的同等精度水平。<details>
<summary>Abstract</summary>
As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our method demonstrates significant improvements in both weight-only quantization and joint quantization of weights and activations, surpassing existing PTQ methods. On GLM-130B and OPT-66B, our method even achieves the same level of accuracy at 2-bit quantization as their float ones. Our simple and effective approach makes it more practical for real-world applications.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:随着大型自然语言模型（LLMs）的大小不断增长，模型压缩而不失 precisión的挑战已成为其部署的关键问题。虽然一些量化方法，如GPTQ，已经在实现了可接受的4位加量量化方面做出了进展，但在更低的位数量化中往往会导致性能下降。在这篇论文中，我们介绍了一种技术called norm tweaking，可以作为当前PTQ方法的插件，以实现高精度而且cost-efficient。我们的方法是在观察到Rectifying the quantized activation distribution to match its float counterpart可以快速地恢复LLMs的精度的基础上提出的。为了实现这一点，我们 metros carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization。我们在多个开源的LLMs上进行了广泛的实验，我们的方法在weight-only quantization和joint quantization of weights and activations中都有显著的改进，超越了现有的PTQ方法。在GLM-130B和OPT-66B上，我们的方法甚至可以在2位量化下达到与浮点版本的同等精度水平。我们的简单而有效的方法使其更加实用于实际应用。
</details></li>
</ul>
<hr>
<h2 id="Improving-diagnosis-and-prognosis-of-lung-cancer-using-vision-transformers-A-scoping-review"><a href="#Improving-diagnosis-and-prognosis-of-lung-cancer-using-vision-transformers-A-scoping-review" class="headerlink" title="Improving diagnosis and prognosis of lung cancer using vision transformers: A scoping review"></a>Improving diagnosis and prognosis of lung cancer using vision transformers: A scoping review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02783">http://arxiv.org/abs/2309.02783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazrat Ali, Farida Mohsen, Zubair Shah<br>for:This scoping review aims to identify recent developments in vision transformer-based AI methods for lung cancer imaging applications.methods:The review includes 34 studies published from 2020 to 2022 that use vision transformer-based AI methods for lung cancer diagnosis and prognosis. These methods include the combination of vision transformers with convolutional neural networks or UNet models.results:The studies have shown that vision transformer-based models are increasingly popular for developing AI methods for lung cancer applications, but their computational complexity and clinical relevance are important factors to be considered for future research. The review provides valuable insights for researchers in the field of AI and healthcare to advance the state-of-the-art in lung cancer diagnosis and prognosis.Here is the simplified Chinese text for the three information points:for:这份scoping review旨在Identify最近发展在呼吸器transformer基于AI方法的肺癌成像应用领域。methods:这份综述包括2020年至2022年发表的34篇研究，这些研究使用呼吸器transformer基于AI方法进行肺癌诊断和预后预测。这些方法包括呼吸器transformer与卷积神经网络或UNet模型的组合。results:这些研究表明，呼吸器transformer基于模型在肺癌应用领域的流行度在增加，但计算复杂性和临床 relevance 是未来研究的关键因素。这份综述为医学AI和健康领域的研究人员提供了有价值的洞察，以进一步提高肺癌诊断和预后预测的state-of-the-art。<details>
<summary>Abstract</summary>
Vision transformer-based methods are advancing the field of medical artificial intelligence and cancer imaging, including lung cancer applications. Recently, many researchers have developed vision transformer-based AI methods for lung cancer diagnosis and prognosis. This scoping review aims to identify the recent developments on vision transformer-based AI methods for lung cancer imaging applications. It provides key insights into how vision transformers complemented the performance of AI and deep learning methods for lung cancer. Furthermore, the review also identifies the datasets that contributed to advancing the field. Of the 314 retrieved studies, this review included 34 studies published from 2020 to 2022. The most commonly addressed task in these studies was the classification of lung cancer types, such as lung squamous cell carcinoma versus lung adenocarcinoma, and identifying benign versus malignant pulmonary nodules. Other applications included survival prediction of lung cancer patients and segmentation of lungs. The studies lacked clear strategies for clinical transformation. SWIN transformer was a popular choice of the researchers; however, many other architectures were also reported where vision transformer was combined with convolutional neural networks or UNet model. It can be concluded that vision transformer-based models are increasingly in popularity for developing AI methods for lung cancer applications. However, their computational complexity and clinical relevance are important factors to be considered for future research work. This review provides valuable insights for researchers in the field of AI and healthcare to advance the state-of-the-art in lung cancer diagnosis and prognosis. We provide an interactive dashboard on lung-cancer.onrender.com/.
</details>
<details>
<summary>摘要</summary>
医学人工智能和肺癌影像领域中，视Transformer基本方法在不断发展。近年来，许多研究人员开发了基于视Transformer的AI方法用于肺癌诊断和预后预测。本篇综述的目的是找到最近发展的视Transformer基本方法在肺癌影像应用中的进展。它提供了关于如何使用视Transformer complement AI和深度学习方法的关键洞察。此外，文章还标出了该领域的主要数据集。总共检索到314篇文献，本综述包含2020年至2022年发表的34篇文献。研究中最常 addressed task是分类肺癌类型，如肺平滑细胞癌和肺管细胞癌，以及识别正常与恶性肺脏瘤。其他应用包括肺癌患者存活预测和肺分 segmentation。研究缺乏明确的临床转化策略。SWIN transformer是研究人员的受欢迎选择，但其他架构也被报道，例如将视Transformer与卷积神经网络或UNet模型结合使用。可以确定，基于视Transformer的模型在肺癌应用领域的 популярность不断增长。然而，计算复杂度和临床 relevance 是未来研究的重要因素。本综述为医学人工智能和健康领域的研究人员提供了有价值的洞察，以推动肺癌诊断和预后预测的状态艺。我们提供了肺癌诊断和预后预测的互动dashboard，可以在lung-cancer.onrender.com/上查看。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effects-of-Heterogeneous-Errors-on-Multi-fidelity-Bayesian-Optimization"><a href="#On-the-Effects-of-Heterogeneous-Errors-on-Multi-fidelity-Bayesian-Optimization" class="headerlink" title="On the Effects of Heterogeneous Errors on Multi-fidelity Bayesian Optimization"></a>On the Effects of Heterogeneous Errors on Multi-fidelity Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02771">http://arxiv.org/abs/2309.02771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Zanjani Foumani, Amin Yousefpour, Mehdi Shishehbor, Ramin Bostanabad</li>
<li>for: 这个研究的目的是提出一种多元信息（Multi-fidelity）优化方法，以缓解在实际应用中高精度数据（High-fidelity）搜集所带来的成本问题。</li>
<li>methods: 这个方法使用了多元信息优化（Multi-fidelity）技术，并将低精度数据（Low-fidelity）与高精度数据（High-fidelity）融合以提高优化效率。</li>
<li>results: 这个研究显示了该方法可以将低精度数据与高精度数据融合，以提高优化效率，并且可以减少高精度数据的搜集成本。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) is a sequential optimization strategy that is increasingly employed in a wide range of areas including materials design. In real world applications, acquiring high-fidelity (HF) data through physical experiments or HF simulations is the major cost component of BO. To alleviate this bottleneck, multi-fidelity (MF) methods are used to forgo the sole reliance on the expensive HF data and reduce the sampling costs by querying inexpensive low-fidelity (LF) sources whose data are correlated with HF samples. However, existing multi-fidelity BO (MFBO) methods operate under the following two assumptions that rarely hold in practical applications: (1) LF sources provide data that are well correlated with the HF data on a global scale, and (2) a single random process can model the noise in the fused data. These assumptions dramatically reduce the performance of MFBO when LF sources are only locally correlated with the HF source or when the noise variance varies across the data sources. In this paper, we dispense with these incorrect assumptions by proposing an MF emulation method that (1) learns a noise model for each data source, and (2) enables MFBO to leverage highly biased LF sources which are only locally correlated with the HF source. We illustrate the performance of our method through analytical examples and engineering problems on materials design.
</details>
<details>
<summary>摘要</summary>
泛化优化（BO）是一种顺序优化策略，在许多领域都在广泛应用，包括材料设计。在实际应用中，通过物理实验或高精度计算获得高精度数据是BO的主要成本 componenet。为了缓解这个瓶颈，使用多质量（MF）方法，可以减少抽样成本，通过寻求便宜的低精度（LF）来源的数据，这些数据与高精度数据相关。然而，现有的MFBO方法在实际应用中存在两个不当的假设：（1）LF来源提供的数据与高精度数据在全局范围内很好地相关，和（2）数据源中的噪声可以用单个随机过程模型。这两个假设在实际应用中非常少有效，这会使MFBO表现下降。在这篇论文中，我们抛弃这两个错误假设，提出了一种MF优化方法，它可以（1）学习每个数据源的噪声模型，和（2）使MFBO可以利用高偏见的LF来源。我们通过分析例子和工程设计问题来证明我们的方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Unifying-over-smoothing-and-over-squashing-in-graph-neural-networks-A-physics-informed-approach-and-beyond"><a href="#Unifying-over-smoothing-and-over-squashing-in-graph-neural-networks-A-physics-informed-approach-and-beyond" class="headerlink" title="Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond"></a>Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02769">http://arxiv.org/abs/2309.02769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqi Shao, Dai Shi, Andi Han, Yi Guo, Qibin Zhao, Junbin Gao</li>
<li>For: The paper aims to address critical computational challenges in graph neural networks (GNNs), such as over-smoothing, over-squashing, and limited expressive power, by introducing a novel approach inspired by the time-reversal principle.* Methods: The proposed method, called Multi-Scaled Heat Kernel based GNN (MHKG), leverages the time-reversal of the graph heat equation to enhance the sharpness of graph node features. The method further generalizes to a more flexible model called G-MHKG, which allows for more control over over-smoothing, over-squashing, and expressive power.* Results: The proposed models are shown to outperform several GNN baseline models in performance across various graph datasets characterized by both homophily and heterophily. Additionally, the paper demonstrates the ability of the proposed models to handle both over-smoothing and over-squashing under mild conditions, and uncovers a trade-off between these two issues.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have emerged as one of the leading approaches for machine learning on graph-structured data. Despite their great success, critical computational challenges such as over-smoothing, over-squashing, and limited expressive power continue to impact the performance of GNNs. In this study, inspired from the time-reversal principle commonly utilized in classical and quantum physics, we reverse the time direction of the graph heat equation. The resulted reversing process yields a class of high pass filtering functions that enhance the sharpness of graph node features. Leveraging this concept, we introduce the Multi-Scaled Heat Kernel based GNN (MHKG) by amalgamating diverse filtering functions' effects on node features. To explore more flexible filtering conditions, we further generalize MHKG into a model termed G-MHKG and thoroughly show the roles of each element in controlling over-smoothing, over-squashing and expressive power. Notably, we illustrate that all aforementioned issues can be characterized and analyzed via the properties of the filtering functions, and uncover a trade-off between over-smoothing and over-squashing: enhancing node feature sharpness will make model suffer more from over-squashing, and vice versa. Furthermore, we manipulate the time again to show how G-MHKG can handle both two issues under mild conditions. Our conclusive experiments highlight the effectiveness of proposed models. It surpasses several GNN baseline models in performance across graph datasets characterized by both homophily and heterophily.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 已经成为机器学习图数据的一种主要方法。 despite their great success,  However,  there are still several computational challenges that affect the performance of GNNs, such as over-smoothing, over-squashing, and limited expressive power. In this study, we draw inspiration from the time-reversal principle commonly used in classical and quantum physics, and reverse the time direction of the graph heat equation. This process yields a class of high-pass filtering functions that enhance the sharpness of graph node features. Based on this concept, we introduce the Multi-Scaled Heat Kernel based GNN (MHKG) by combining the effects of diverse filtering functions on node features. To explore more flexible filtering conditions, we further generalize MHKG into a model called G-MHKG and thoroughly demonstrate the roles of each element in controlling over-smoothing, over-squashing, and expressive power.We find that all of these issues can be characterized and analyzed through the properties of the filtering functions, and discover a trade-off between over-smoothing and over-squashing: enhancing node feature sharpness will make the model more susceptible to over-squashing, and vice versa. Furthermore, we manipulate the time to show how G-MHKG can handle both of these issues under mild conditions. Our experimental results demonstrate the effectiveness of our proposed models, surpassing several GNN baseline models in performance across graph datasets characterized by both homophily and heterophily.
</details></li>
</ul>
<hr>
<h2 id="Towards-Unsupervised-Graph-Completion-Learning-on-Graphs-with-Features-and-Structure-Missing"><a href="#Towards-Unsupervised-Graph-Completion-Learning-on-Graphs-with-Features-and-Structure-Missing" class="headerlink" title="Towards Unsupervised Graph Completion Learning on Graphs with Features and Structure Missing"></a>Towards Unsupervised Graph Completion Learning on Graphs with Features and Structure Missing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02762">http://arxiv.org/abs/2309.02762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sichao Fu, Qinmu Peng, Yang He, Baokun Du, Xinge You</li>
<li>for: 提高graph neural network（GNN）在图数据中的表现，特别是在图数据中存在特定的特征和结构缺失时。</li>
<li>methods: 提出了一种更通用的graph completion learning（GCL）框架，通过自我监督学习提高存在特定特征和结构缺失的GNN变种在图数据中的表现。</li>
<li>results: 通过对八个数据集、三种GNN变种和五种缺失率进行广泛的实验，证明了我们提出的方法的效iveness。<details>
<summary>Abstract</summary>
In recent years, graph neural networks (GNN) have achieved significant developments in a variety of graph analytical tasks. Nevertheless, GNN's superior performance will suffer from serious damage when the collected node features or structure relationships are partially missing owning to numerous unpredictable factors. Recently emerged graph completion learning (GCL) has received increasing attention, which aims to reconstruct the missing node features or structure relationships under the guidance of a specifically supervised task. Although these proposed GCL methods have made great success, they still exist the following problems: the reliance on labels, the bias of the reconstructed node features and structure relationships. Besides, the generalization ability of the existing GCL still faces a huge challenge when both collected node features and structure relationships are partially missing at the same time. To solve the above issues, we propose a more general GCL framework with the aid of self-supervised learning for improving the task performance of the existing GNN variants on graphs with features and structure missing, termed unsupervised GCL (UGCL). Specifically, to avoid the mismatch between missing node features and structure during the message-passing process of GNN, we separate the feature reconstruction and structure reconstruction and design its personalized model in turn. Then, a dual contrastive loss on the structure level and feature level is introduced to maximize the mutual information of node representations from feature reconstructing and structure reconstructing paths for providing more supervision signals. Finally, the reconstructed node features and structure can be applied to the downstream node classification task. Extensive experiments on eight datasets, three GNN variants and five missing rates demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
近年来，图 neural network (GNN) 在多种图分析任务上取得了显著的进步。然而，GNN 的性能会受到部分缺失的节点特征或结构关系的不可预测因素产生严重损害。随后出现的图完成学习 (GCL) 得到了更多的关注，它目的是在指定的超级任务下重建缺失的节点特征或结构关系。虽然这些提出的 GCL 方法已经取得了很大的成功，但它们还存在以下问题：依赖于标签、节点特征和结构关系的偏见。此外，现有的 GCL 总的适应能力仍然面临着同时缺失节点特征和结构关系的挑战。为解决上述问题，我们提出了一个更加通用的 GCL 框架，通过自动学习提高现有 GNN 变体在图中缺失特征和结构时的任务性能，称为无监督 GCL (UGCL)。具体来说，为了避免在 GNN 的消息传递过程中缺失节点特征和结构的匹配问题，我们将特征重建和结构重建分别设计为独立的个性化模型，然后引入结构和特征两级对比损失来最大化节点表示之间的互信息。最后，重建的节点特征和结构可以应用于下游节点分类任务。我们在八个数据集、三种 GNN 变体和五种缺失率进行了广泛的实验，结果表明了我们提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="GPT-Can-Solve-Mathematical-Problems-Without-a-Calculator"><a href="#GPT-Can-Solve-Mathematical-Problems-Without-a-Calculator" class="headerlink" title="GPT Can Solve Mathematical Problems Without a Calculator"></a>GPT Can Solve Mathematical Problems Without a Calculator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03241">http://arxiv.org/abs/2309.03241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/mathglm">https://github.com/thudm/mathglm</a></li>
<li>paper_authors: Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, Jie Tang</li>
<li>for: 挑战大语言模型无法准确进行多位数 arithmetic 操作的假设</li>
<li>methods: 使用充足的训练数据，一个200亿参数的语言模型可以准确地进行多位数 arithmetic 操作，无需数据泄露</li>
<li>results: 我们的 MathGLM 在一个5,000个样本的中文数学问题测试集上达到了和GPT-4相似的性能，而GPT-4 的多位数 multiplication 准确率只有4.3%<details>
<summary>Abstract</summary>
Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set. Our code and data are public at https://github.com/THUDM/MathGLM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SWAP-Exploiting-Second-Ranked-Logits-for-Adversarial-Attacks-on-Time-Series"><a href="#SWAP-Exploiting-Second-Ranked-Logits-for-Adversarial-Attacks-on-Time-Series" class="headerlink" title="SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series"></a>SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02752">http://arxiv.org/abs/2309.02752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang George Dong, Liangwei Nathan Zheng, Weitong Chen, Wei Emma Zhang, Lin Yue</li>
<li>for: 这篇研究旨在提高时间序列分类模型的攻击性能。</li>
<li>methods: 提出了一种新的攻击方法SWAP，通过对第二名的条件预测值进行强化，以提高攻击成功率。</li>
<li>results: 实验结果显示SWAP可以实现超过50%的攻击成功率，较现有方法提高18%。<details>
<summary>Abstract</summary>
Time series classification (TSC) has emerged as a critical task in various domains, and deep neural models have shown superior performance in TSC tasks. However, these models are vulnerable to adversarial attacks, where subtle perturbations can significantly impact the prediction results. Existing adversarial methods often suffer from over-parameterization or random logit perturbation, hindering their effectiveness. Additionally, increasing the attack success rate (ASR) typically involves generating more noise, making the attack more easily detectable. To address these limitations, we propose SWAP, a novel attacking method for TSC models. SWAP focuses on enhancing the confidence of the second-ranked logits while minimizing the manipulation of other logits. This is achieved by minimizing the Kullback-Leibler divergence between the target logit distribution and the predictive logit distribution. Experimental results demonstrate that SWAP achieves state-of-the-art performance, with an ASR exceeding 50% and an 18% increase compared to existing methods.
</details>
<details>
<summary>摘要</summary>
时间序列分类（TSC）任务在不同领域中变得越来越重要，深度神经网络在TSC任务中表现出色。然而，这些模型受到恶意攻击的威胁，其中微的杂乱可能导致预测结果受到重大影响。现有的恶意方法经常受到过参数化或随机Logit扰动的限制，这限制了其效iveness。尽管提高攻击成功率（ASR）通常需要生成更多的噪音，但这会使攻击更易被发现。为解决这些局限性，我们提出了SWAP，一种新的攻击方法 дляTSC模型。SWAP通过提高第二个排名的Logit的信任程度，同时最小化其他Logit的扰动。这是通过控制target Logit分布和预测Logit分布之间的Kullback-Leibler差异来实现的。实验结果表明，SWAP可以达到状态的损失表现，ASR超过50%，与现有方法相比增加18%。
</details></li>
</ul>
<hr>
<h2 id="Safe-Neural-Control-for-Non-Affine-Control-Systems-with-Differentiable-Control-Barrier-Functions"><a href="#Safe-Neural-Control-for-Non-Affine-Control-Systems-with-Differentiable-Control-Barrier-Functions" class="headerlink" title="Safe Neural Control for Non-Affine Control Systems with Differentiable Control Barrier Functions"></a>Safe Neural Control for Non-Affine Control Systems with Differentiable Control Barrier Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04492">http://arxiv.org/abs/2309.04492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Xiao, Ross Allen, Daniela Rus</li>
<li>for: 本文解决了非线性控制系统的安全控制问题。</li>
<li>methods: 该方法使用控制边界函数(CBF)来优化 quadratic costs，并通过 incorporating higher-order CBFs into neural ordinary differential equation-based learning models 来保证安全性。</li>
<li>results: 该方法可以学习复杂且优化的控制策略，并能够 addressed the conservativeness of CBFs such that the system state will not stay unnecessarily far away from safe set boundaries。furthermore, the imitation learning model is capable of learning complex and optimal control policies that are usually intractable online。<details>
<summary>Abstract</summary>
This paper addresses the problem of safety-critical control for non-affine control systems. It has been shown that optimizing quadratic costs subject to state and control constraints can be sub-optimally reduced to a sequence of quadratic programs (QPs) by using Control Barrier Functions (CBFs). Our recently proposed High Order CBFs (HOCBFs) can accommodate constraints of arbitrary relative degree. The main challenges in this approach are that it requires affine control dynamics and the solution of the CBF-based QP is sub-optimal since it is solved point-wise. To address these challenges, we incorporate higher-order CBFs into neural ordinary differential equation-based learning models as differentiable CBFs to guarantee safety for non-affine control systems. The differentiable CBFs are trainable in terms of their parameters, and thus, they can address the conservativeness of CBFs such that the system state will not stay unnecessarily far away from safe set boundaries. Moreover, the imitation learning model is capable of learning complex and optimal control policies that are usually intractable online. We illustrate the effectiveness of the proposed framework on LiDAR-based autonomous driving and compare it with existing methods.
</details>
<details>
<summary>摘要</summary>
To address these challenges, the authors incorporate higher-order CBFs into neural ordinary differential equation-based learning models as differentiable CBFs to guarantee safety for non-affine control systems. The differentiable CBFs are trainable in terms of their parameters, allowing them to address the conservativeness of CBFs and ensure that the system state does not stay unnecessarily far away from safe set boundaries. Additionally, the imitation learning model is capable of learning complex and optimal control policies that are usually intractable online.The proposed framework is evaluated on LiDAR-based autonomous driving and compared with existing methods, demonstrating its effectiveness. The key contributions of the paper include the development of a safe and efficient control method for non-affine control systems using higher-order CBFs and neural networks, and the demonstration of its effectiveness in a real-world application.
</details></li>
</ul>
<hr>
<h2 id="Offensive-Hebrew-Corpus-and-Detection-using-BERT"><a href="#Offensive-Hebrew-Corpus-and-Detection-using-BERT" class="headerlink" title="Offensive Hebrew Corpus and Detection using BERT"></a>Offensive Hebrew Corpus and Detection using BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02724">http://arxiv.org/abs/2309.02724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sinalab/offensivehebrew">https://github.com/sinalab/offensivehebrew</a></li>
<li>paper_authors: Nagham Hamad, Mustafa Jarrar, Mohammad Khalilia, Nadim Nashif<br>for: 本研究 targets  Hebrew offensive language detection, which has been understudied in low-resource languages.methods: 本研究使用了 Twitter 上的 15,881 个微博，并采用了 Five-class 分类法（不夹着、仇恨、暴力、色情、无偏见）。 Each tweet was labeled by Arabic-Hebrew bilingual speakers, and the annotation process was challenging due to the need for familiarity with Israeli culture, politics, and practices.results: 研究人员使用了自己的数据集和另一个已发表的数据集进行 fine-tuning HeBERT 和 AlephBERT 模型。结果显示，将 HeBERT 模型在我们的数据集上进行 fine-tuning，并测试在 D_OLaH 上得到了 2% 的提升。而使用我们的数据集和 D_OLaH 进行 fine-tuning，然后测试在我们的数据集上得到了 69% 的准确率。这表明我们的数据集具有通用性。<details>
<summary>Abstract</summary>
Offensive language detection has been well studied in many languages, but it is lagging behind in low-resource languages, such as Hebrew. In this paper, we present a new offensive language corpus in Hebrew. A total of 15,881 tweets were retrieved from Twitter. Each was labeled with one or more of five classes (abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew bilingual speakers. The annotation process was challenging as each annotator is expected to be familiar with the Israeli culture, politics, and practices to understand the context of each tweet. We fine-tuned two Hebrew BERT models, HeBERT and AlephBERT, using our proposed dataset and another published dataset. We observed that our data boosts HeBERT performance by 2% when combined with D_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69% accuracy, while fine-tuning on D_OLaH and testing on our data yields 57% accuracy, which may be an indication to the generalizability our data offers. Our dataset and fine-tuned models are available on GitHub and Huggingface.
</details>
<details>
<summary>摘要</summary>
偏Language检测在多种语言中已经得到了广泛的研究，但在low-resource语言中，如希伯来语，却落后于其他语言。在这篇论文中，我们提供了一个新的希伯来语偏Language corpus。从Twitter上检索了15,881封微博，并将每封微博分为五类（凶恶、仇恨、暴力、色情或无偏Language），由阿拉伯希伯来双语 speaker进行标注。标注过程是具有挑战性的，因为每个标注员需要熟悉以色列文化、政治和实践，以理解每封微博的上下文。我们使用我们提议的数据集和另一个已发表的数据集进行练化两个希伯来BERT模型（HeBERT和AlephBERT）。我们发现，将HeBERT练化于我们的数据集后，与D_OLaH进行组合，可以提高HeBERT的性能2%。对AlephBERT进行练化并在D_OLaH上测试，可以达到69%的准确率，而对HeBERT进行练化并在我们的数据集上测试，可以达到57%的准确率，这可能是我们数据的总体化能力的表现。我们的数据集和练化后的模型都可以在GitHub和Huggingface上获取。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-frontiers-of-deep-learning-innovations-shaping-diverse-domains"><a href="#Unveiling-the-frontiers-of-deep-learning-innovations-shaping-diverse-domains" class="headerlink" title="Unveiling the frontiers of deep learning: innovations shaping diverse domains"></a>Unveiling the frontiers of deep learning: innovations shaping diverse domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02712">http://arxiv.org/abs/2309.02712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shams Forruque Ahmed, Md. Sakib Bin Alam, Maliha Kabir, Shaila Afrin, Sabiha Jannat Rafa, Aanushka Mehjabin, Amir H. Gandomi</li>
<li>for: 本研究旨在探讨深度学习在各个领域的应用和挑战。</li>
<li>methods: 本研究使用了深度学习来处理大量数据，并利用了LSTM和GRU等闭合架构来解决数据挑战。</li>
<li>results: 研究发现，深度学习可以在各个领域中提供高度的预测和分析精度，但需要大量数据来支持。<details>
<summary>Abstract</summary>
Deep learning (DL) enables the development of computer models that are capable of learning, visualizing, optimizing, refining, and predicting data. In recent years, DL has been applied in a range of fields, including audio-visual data processing, agriculture, transportation prediction, natural language, biomedicine, disaster management, bioinformatics, drug design, genomics, face recognition, and ecology. To explore the current state of deep learning, it is necessary to investigate the latest developments and applications of deep learning in these disciplines. However, the literature is lacking in exploring the applications of deep learning in all potential sectors. This paper thus extensively investigates the potential applications of deep learning across all major fields of study as well as the associated benefits and challenges. As evidenced in the literature, DL exhibits accuracy in prediction and analysis, makes it a powerful computational tool, and has the ability to articulate itself and optimize, making it effective in processing data with no prior training. Given its independence from training data, deep learning necessitates massive amounts of data for effective analysis and processing, much like data volume. To handle the challenge of compiling huge amounts of medical, scientific, healthcare, and environmental data for use in deep learning, gated architectures like LSTMs and GRUs can be utilized. For multimodal learning, shared neurons in the neural network for all activities and specialized neurons for particular tasks are necessary.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）允许开发计算机模型，能够学习、视觉、优化、纠正和预测数据。在过去几年，DL在各种领域应用，包括音频视频数据处理、农业、交通预测、自然语言、生物医学、灾害管理、生物信息学、药物设计、 genomics、人脸识别和生态学。为了探索当前深度学习的状态，需要调查最新的发展和应用深度学习在这些领域。然而，文献缺乏探索深度学习在所有领域的应用。这篇论文因此进行了广泛的调查，探讨了深度学习在所有主要领域的应用，以及相关的利点和挑战。据文献显示，DL在预测和分析中准确，使其成为强大的计算工具，同时具有自适应和优化能力，使其在没有先进trainings的情况下处理数据非常有效。由于DL的独立性，它需要巨量数据进行有效的分析和处理，类似于大量数据。为了处理医疗、科学、健康和环境数据的巨量化，可以使用门控架构如LSTM和GRU。为实现多模态学习，共享神经网络中的所有活动的特征神经和特定任务的专业神经是必要的。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Imperfect-Symmetry-a-Novel-Symmetry-Learning-Actor-Critic-Extension"><a href="#Addressing-Imperfect-Symmetry-a-Novel-Symmetry-Learning-Actor-Critic-Extension" class="headerlink" title="Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension"></a>Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02711">http://arxiv.org/abs/2309.02711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-abr/Adaptive-Symmetry-Learning">https://github.com/m-abr/Adaptive-Symmetry-Learning</a></li>
<li>paper_authors: Miguel Abreu, Luis Paulo Reis, Nuno Lau</li>
<li>for: 本研究的目的是capturing human brain的symmetry能力through reinforcement learning，以便在具有不完全或不准确的对称描述时，能够自适应地学习和执行任务。</li>
<li>methods: 本研究使用了一种名为Adaptive Symmetry Learning（ASL）的模型减少actor-critic扩展，该模型可以在学习过程中适应不完全或不准确的对称描述，并且包括对称适应组件和模块化损失函数，以保证所有状态具有共同的对称关系。</li>
<li>results: 对于一个四脚蚂蚁模型，ASL可以在多个方向的移动任务中从大的偏差中恢复，并且可以在不同的隐藏对称状态下扩展知识。相比之下，相对方法在大多数情况下只能达到相似或更差的性能，因此ASL是一种值得采用的方法，可以充分利用模型对称性，同时补做固有的偏差。<details>
<summary>Abstract</summary>
Symmetry, a fundamental concept to understand our environment, often oversimplifies reality from a mathematical perspective. Humans are a prime example, deviating from perfect symmetry in terms of appearance and cognitive biases (e.g. having a dominant hand). Nevertheless, our brain can easily overcome these imperfections and efficiently adapt to symmetrical tasks. The driving motivation behind this work lies in capturing this ability through reinforcement learning. To this end, we introduce Adaptive Symmetry Learning (ASL) $\unicode{x2013}$ a model-minimization actor-critic extension that addresses incomplete or inexact symmetry descriptions by adapting itself during the learning process. ASL consists of a symmetry fitting component and a modular loss function that enforces a common symmetric relation across all states while adapting to the learned policy. The performance of ASL is compared to existing symmetry-enhanced methods in a case study involving a four-legged ant model for multidirectional locomotion tasks. The results demonstrate that ASL is capable of recovering from large perturbations and generalizing knowledge to hidden symmetric states. It achieves comparable or better performance than alternative methods in most scenarios, making it a valuable approach for leveraging model symmetry while compensating for inherent perturbations.
</details>
<details>
<summary>摘要</summary>
“对照”是一个基本概念，帮助我们更好地理解我们的环境。然而，从数学角度来看，它往往过度简化现实。人类是一个好例子，他们在外表和认知上存在偏差（如有主导手）。然而，我们的大脑可以快速超越这些不完整性，并快速适应 симметrical 任务。这种能力是我们 capture 的目的，我们提出了 Adaptive Symmetry Learning（ASL）。ASL 是一种基于 actor-critic 扩展的模型减少法，可以在学习过程中适应不完整或不准确的对照描述。ASL 包括对照适应组件和一个模块化损失函数，使得在学习策略时对所有状态都 enforces 一致的对照关系。我们在一个四脚蚂蚁模型中进行了一个案例研究，以评估 ASL 在多向移动任务上的性能。结果表明，ASL 可以快速复原大幅偏差，并将知识扩展到隐藏的对照状态。它在大多数情况下与其他方法相当或更好的性能，使得它成为一种值得使用的方法，用于利用模型对照性而弥补内在偏差。
</details></li>
</ul>
<hr>
<h2 id="Improved-Outlier-Robust-Seeding-for-k-means"><a href="#Improved-Outlier-Robust-Seeding-for-k-means" class="headerlink" title="Improved Outlier Robust Seeding for k-means"></a>Improved Outlier Robust Seeding for k-means</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02710">http://arxiv.org/abs/2309.02710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Deshpande, Rameshwar Pratap</li>
<li>For: The paper proposes a simple and efficient algorithm for robust $k$-means clustering, which is robust to outliers and has a provable $O(1)$ approximation guarantee.* Methods: The algorithm uses a modified $D^2$ sampling distribution to make it robust to outliers, and it runs in $O(ndk)$ time.* Results: The algorithm outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and has a provable $O(1)$ approximation guarantee. The algorithm is also modified to output exactly $k$ clusters while keeping its running time linear in $n$ and $d$. The proposed algorithm outperforms previous methods, including $k$-means++, uniform random seeding, greedy sampling for $k$ means, and robust $k$-means++, on standard real-world and synthetic data sets.<details>
<summary>Abstract</summary>
The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\log k)$ approximation guarantee \cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \textit{w.r.t.} $k$-means solution on inliers, does not hold.   Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee.   Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, while keeping its running time linear in $n$ and $d$. This is an improvement over previous results for robust $k$-means based on LP relaxation and rounding \cite{Charikar}, \cite{KrishnaswamyLS18} and \textit{robust $k$-means++} \cite{DeshpandeKP20}. Our empirical results show the advantage of our algorithm over $k$-means++~\cite{AV2007}, uniform random seeding, greedy sampling for $k$ means~\cite{tkmeanspp}, and robust $k$-means++~\cite{DeshpandeKP20}, on standard real-world and synthetic data sets used in previous work. Our proposal is easily amenable to scalable, faster, parallel implementations of $k$-means++ \cite{Bahmani,BachemL017} and is of independent interest for coreset constructions in the presence of outliers \cite{feldman2007ptas,langberg2010universal,feldman2011unified}.
</details>
<details>
<summary>摘要</summary>
“$k$-means是一个受欢迎的聚类目标，但它本身是非坚固的和敏感于噪音。$k$-means++的内置seed或初始化使用$D^{2}$抽样，并且有一个可证的$O(\log k)$近似保证 \cite{AV2007}。但在噪音或噪音的存在下，$D^{2}$抽样更可能从距离噪音的外部选择中心，而不是内部的填充集，因此其近似保证与$k$-means解不同。 assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers。”“我们的算法在$O(ndk)$时间内运行，输出$O(k)$个聚类，抛弃一点点多于最佳噪音数量，并且有一个可证的$O(1)$近似保证。我们的算法也可以修改以输出精确$k$个聚类，而不是$O(k)$个聚类，保持时间线性增长。这是与前一 Results for 坚固 $k$-means based on LP relaxation and rounding \cite{Charikar}, \cite{KrishnaswamyLS18} and robust $k$-means++ \cite{DeshpandeKP20} 的优化。我们的实验结果显示我们的算法在标准的实验数据上的优势，比$k$-means++ \cite{AV2007}、单调随机seed \cite{tkmeanspp}和坚固 $k$-means++ \cite{DeshpandeKP20}。”“我们的提议是容易实现批量、更快的$k$-means++实现 \cite{Bahmani,BachemL017}，并且独立 interessant для核心sets constructions在噪音存在下 \cite{feldman2007ptas,langberg2010universal,feldman2011unified}。”
</details></li>
</ul>
<hr>
<h2 id="Certifying-LLM-Safety-against-Adversarial-Prompting"><a href="#Certifying-LLM-Safety-against-Adversarial-Prompting" class="headerlink" title="Certifying LLM Safety against Adversarial Prompting"></a>Certifying LLM Safety against Adversarial Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02705">http://arxiv.org/abs/2309.02705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, Hima Lakkaraju</li>
<li>for: 防止语言模型生成有害内容</li>
<li>methods: 使用擦除和检查机制，对输入提问进行安全检查，并使用开源语言模型Llama 2作为安全筛选器</li>
<li>results: 对于恶意提问，实现了强制保证安全性的证明，同时对安全提问保持良好的性能<details>
<summary>Abstract</summary>
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial sequence at the end of the prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Empirical results demonstrate that our technique obtains strong certified safety guarantees on harmful prompts while maintaining good performance on safe prompts. For example, against adversarial suffixes of length 20, it certifiably detects 93% of the harmful prompts and labels 94% of the safe prompts as safe using the open source language model Llama 2 as the safety filter.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在公共使用时采用了保险措施，以确保其输出不会 causese harm。一个Alignment的语言模型应该拒绝用户的请求生成危险内容。然而，这些安全措施可能会被恶意设计的提示Sequence circumvent the model's safety guards and cause it to produce harmful content。在这项工作中，我们引入了“erase-and-check”，第一个防御性提示的框架，具有可靠的安全保证。我们将 individually erase tokens and inspect the resulting subsequences using a safety filter。如果任何 subsequences或输入提示被安全筛选器认为危险，我们就会将输入提示标记为危险。这保证了任何 adversarial modification of a harmful prompt up to a certain size will also be labeled harmful。我们防御了三种攻击模式：i） adversarial suffix，ii） adversarial insertion，和 iii） adversarial infusion。我们的技术在危险提示上获得了强的可靠安全保证，而不会影响安全提示的性能。例如，对于 adversarial suffixes of length 20，我们可靠地检测了93%的危险提示，并将94%的安全提示标记为安全使用开源语言模型 Llama 2 作为安全筛选器。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-EDFs-Bi-equivariant-Denoising-Generative-Modeling-on-SE-3-for-Visual-Robotic-Manipulation"><a href="#Diffusion-EDFs-Bi-equivariant-Denoising-Generative-Modeling-on-SE-3-for-Visual-Robotic-Manipulation" class="headerlink" title="Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation"></a>Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02685">http://arxiv.org/abs/2309.02685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tomato1mule/diffusion_edf">https://github.com/tomato1mule/diffusion_edf</a></li>
<li>paper_authors: Hyunwoo Ryu, Jiwoo Kim, Junwoo Chang, Hyun Seok Ahn, Joohwan Seo, Taehan Kim, Yubin Kim, Jongeun Choi, Roberto Horowitz</li>
<li>for: 本研究旨在提高机器人学习的数据效率、通用性和稳定性，并应用扩展的Diffusion-EDFs方法。</li>
<li>methods: 该方法 integrates SE(3)-equivariance into diffusion generative modeling，以提高数据效率和通用性。</li>
<li>results: 研究表明，该方法只需要5-10个任务示例就能够实现有效的端到端训练，并且在扩展性方面表现出色。<details>
<summary>Abstract</summary>
Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spatio-Temporal-Contrastive-Self-Supervised-Learning-for-POI-level-Crowd-Flow-Inference"><a href="#Spatio-Temporal-Contrastive-Self-Supervised-Learning-for-POI-level-Crowd-Flow-Inference" class="headerlink" title="Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference"></a>Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03239">http://arxiv.org/abs/2309.03239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songyu Ke, Ting Li, Li Song, Yanping Sun, Qintian Sun, Junbo Zhang, Yu Zheng</li>
<li>for: 这个研究旨在提高实时人流监控和城市规划等领域中的准确性，因为现有的城市感知技术所提供的数据质量不够高，无法实时监控每个Points of Interest（POI）的人流。</li>
<li>methods: 这个研究使用了自我supervised attributed graph representation learning技术，将人流监控问题转换为一个自我supervised learning任务，并提出了一个新的对应推导框架（CSST）。</li>
<li>results: 研究表明，使用CSST预训练了大量的杂凑数据，并与高精度的人流数据进行了调整，可以实现更高的精度和可靠性在人流监控中。<details>
<summary>Abstract</summary>
Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) The scarcity and rarity of labeled data, 2) The intricate spatio-temporal dependencies among POIs, and 3) The myriad correlations between precise crowd flow and GPS reports.   To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel Contrastive Self-learning framework for Spatio-Temporal data (CSST). Our approach initiates with the construction of a spatial adjacency graph founded on the POIs and their respective distances. We then employ a contrastive learning technique to exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped prediction approach to anticipate the representation of the target subgraph from similar instances. Following the pre-training phase, the model is fine-tuned with accurate crowd flow data. Our experiments, conducted on two real-world datasets, demonstrate that the CSST pre-trained on extensive noisy data consistently outperforms models trained from scratch.
</details>
<details>
<summary>摘要</summary>
为了有效管理交通和城市规划，精准获取人流量 Points of Interest (POIs) 非常重要。然而，由于城市感知技术的限制，大多数数据质量不够高，无法精准监测 POIs 上的人流量。这使得从低质量数据中推断精准人流量变得非常困难。这些挑战的主要原因包括：1）数据的罕见性和罕见性，2） POIs 之间的复杂空间时间关系，3）精准人流量和 GPS 报告之间的多重相关性。为解决这些挑战，我们将人流量推断问题转化为一种自动归类 attributed graph representation learning 任务，并提出了一种新的 Contrastive Self-learning 框架 для Spatio-Temporal 数据 (CSST)。我们的方法从 POIs 的空间邻接图开始，然后使用对比学习技术来利用大量的无标注 Spatio-Temporal 数据。我们采用了交换预测方法，以预测目标子图的表示。在预训练阶段后，我们将模型精度地 fine-tune 到准确的人流量数据上。我们在两个真实世界数据集上进行了实验，结果表明，CSST 在庞大的噪音数据上进行预训练后， invariably 在精度上超过从零开始训练的模型。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Design-Choices-and-Their-Impact-on-Emotion-Recognition-Model-Development-and-Evaluation"><a href="#Implicit-Design-Choices-and-Their-Impact-on-Emotion-Recognition-Model-Development-and-Evaluation" class="headerlink" title="Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation"></a>Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03238">http://arxiv.org/abs/2309.03238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mimansa Jaiswal</li>
<li>for: 这项研究的目的是提高情绪识别的准确性和可靠性，并处理数据采集和标注中的主观性问题。</li>
<li>methods: 本研究使用多模态数据集，包括视频、audio和文本数据，并使用数据拓展技术和对注释方法进行分析，以解决数据采集和标注中的问题。此外，研究还使用了对抗网络来隔离主观性变量，并提出了一种优化的社会学评价指标来测试模型。</li>
<li>results: 本研究实现了提高情绪识别的准确性和可靠性，并解决了数据采集和标注中的主观性问题。研究还发现，使用对抗网络可以隔离主观性变量，并且提出了一种优化的社会学评价指标，可以用于Cost-effective的情绪识别模型测试。<details>
<summary>Abstract</summary>
Emotion recognition is a complex task due to the inherent subjectivity in both the perception and production of emotions. The subjectivity of emotions poses significant challenges in developing accurate and robust computational models. This thesis examines critical facets of emotion recognition, beginning with the collection of diverse datasets that account for psychological factors in emotion production.   To handle the challenge of non-representative training data, this work collects the Multimodal Stressed Emotion dataset, which introduces controlled stressors during data collection to better represent real-world influences on emotion production. To address issues with label subjectivity, this research comprehensively analyzes how data augmentation techniques and annotation schemes impact emotion perception and annotator labels. It further handles natural confounding variables and variations by employing adversarial networks to isolate key factors like stress from learned emotion representations during model training. For tackling concerns about leakage of sensitive demographic variables, this work leverages adversarial learning to strip sensitive demographic information from multimodal encodings. Additionally, it proposes optimized sociological evaluation metrics aligned with cost-effective, real-world needs for model testing.   This research advances robust, practical emotion recognition through multifaceted studies of challenges in datasets, labels, modeling, demographic and membership variable encoding in representations, and evaluation. The groundwork has been laid for cost-effective, generalizable emotion recognition models that are less likely to encode sensitive demographic information.
</details>
<details>
<summary>摘要</summary>
《情感认知问题具有复杂性，主要由情感生产和感知的主观性所致。这种主观性对计算机模型的开发带来了很大挑战。本论文研究了情感认知的关键方面，从数据采集开始。为了解决训练数据不准确表示实际情况的问题，本研究采集了多模态压力情感数据集，并在数据采集过程中引入了控制的压力因素，以更好地表示实际情况下的情感生产。为了解决标签主观性的问题，本研究全面分析了数据扩展技术和注释方案的影响，并对情感识别器的训练方法进行了优化。此外，本研究还处理了自然的干扰变量和变化，使用了对抗网络来隔离学习过程中的主要因素，如压力，从多模态编码中隔离了敏感人群信息。此外，本研究还提出了优化的社会学评价指标，以更好地适应实际的成本效益需求。通过这些研究，本论文提出了一种可靠、实用的情感认知模型，可以减少敏感人群信息的泄露。
</details></li>
</ul>
<hr>
<h2 id="RLSynC-Offline-Online-Reinforcement-Learning-for-Synthon-Completion"><a href="#RLSynC-Offline-Online-Reinforcement-Learning-for-Synthon-Completion" class="headerlink" title="RLSynC: Offline-Online Reinforcement Learning for Synthon Completion"></a>RLSynC: Offline-Online Reinforcement Learning for Synthon Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02671">http://arxiv.org/abs/2309.02671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frazier N. Baker, Ziqi Chen, Xia Ning</li>
<li>for: 本研究旨在提高 semi-template-based retrosynthesis 方法的实用性和可读性，通过对完整的 synthons 进行分析和评估，从而帮助化学家快速和有效地规划合成过程。</li>
<li>methods: 本研究使用了一种基于强化学习的 offline-online 方法 RLSynC，将一个代理人分配到每个 synthon，这些代理人通过一步步进行的动作搜索，以完成 synthons。RLSynC 会从 both offline 训练集和线上互动中学习策略，并使用一个前向合成模型来评估预测的反应中的可能性，以帮助搜索动作。</li>
<li>results: 实验结果显示，RLSynC 可以与现有的 retrosynthesis 方法相比，提高 synthon 完成率高达 14.9%，并且在 retrosynthesis 方面提高了 14.0%。这些结果显示 RLSynC 具有优秀的实用性和可读性，并且可以帮助化学家更快速地规划合成过程。<details>
<summary>Abstract</summary>
Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare RLSynC with the state-of-the-art retrosynthesis methods. Our experimental results demonstrate that RLSynC can outperform these methods with improvement as high as 14.9% on synthon completion, and 14.0% on retrosynthesis, highlighting its potential in synthesis planning.
</details>
<details>
<summary>摘要</summary>
逆synthesis是确定所需产品的前一系列化学反应的过程。半模板基于的逆synthesis方法，首先预测产品中的反应中心，然后完成结果的synthons回到原料。这些方法提供了必要的可读性和实际应用性，以便在合成规划中提供指导。我们开发了一种新的线上-线下强化学习方法RLSynC，用于在半模板基于的方法中进行synthon完成。RLSynC分配了一个代理人到每个synthon中，这些代理人逐步进行动作，以同步进行完成。RLSynC从线上训练集和在线互动中学习策略，这allow RLSynC可以探索新的反应空间。RLSynC使用前向合成模型来评估预测的原料是否可以合成产品，并因此引导动作搜索。我们与当前领域的状态势比较RLSynC，我们的实验结果表明，RLSynC可以与现有的逆synthesis方法相比，在synthon完成和逆synthesis方面提高14.9%和14.0%。这highlights RLSynC在合成规划中的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Marketing-Budget-Allocation-with-Offline-Constrained-Deep-Reinforcement-Learning"><a href="#Marketing-Budget-Allocation-with-Offline-Constrained-Deep-Reinforcement-Learning" class="headerlink" title="Marketing Budget Allocation with Offline Constrained Deep Reinforcement Learning"></a>Marketing Budget Allocation with Offline Constrained Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02669">http://arxiv.org/abs/2309.02669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchi Cai, Jiyan Jiang, Wenpeng Zhang, Shiji Zhou, Xierui Song, Li Yu, Lihong Gu, Xiaodong Zeng, Jinjie Gu, Guannan Zhang</li>
<li>for: 本研究旨在解决在线广告运营中利用先前收集的线上数据的预算分配问题。</li>
<li>methods: 我们提出了一种基于游戏理论的半策略权值学习方法，可以减少存储多少策略的需求，从而实现减少策略数量，使其在实际应用中更加实用和有利。</li>
<li>results: 我们的实验表明，提出的方法可以在一个大规模的广告运营中，与多种基eline方法相比，具有更高的效果和稳定性。此外，我们还证明了该方法可以确定优化预算分配策略，而前一代值基学习方法无法实现这一点。<details>
<summary>Abstract</summary>
We study the budget allocation problem in online marketing campaigns that utilize previously collected offline data. We first discuss the long-term effect of optimizing marketing budget allocation decisions in the offline setting. To overcome the challenge, we propose a novel game-theoretic offline value-based reinforcement learning method using mixed policies. The proposed method reduces the need to store infinitely many policies in previous methods to only constantly many policies, which achieves nearly optimal policy efficiency, making it practical and favorable for industrial usage. We further show that this method is guaranteed to converge to the optimal policy, which cannot be achieved by previous value-based reinforcement learning methods for marketing budget allocation. Our experiments on a large-scale marketing campaign with tens-of-millions users and more than one billion budget verify the theoretical results and show that the proposed method outperforms various baseline methods. The proposed method has been successfully deployed to serve all the traffic of this marketing campaign.
</details>
<details>
<summary>摘要</summary>
我们研究线上广告营运中的预算分配问题，利用先前收集的线上数据。我们首先讨论了长期影响优化广告预算分配决策的效果。为了解决这个挑战，我们提出了一种新的游戏理论基础的偏变策略混合方法。这个方法可以将传统上需要储存无限多个策略的问题降低到仅储存常量多个策略，实现了近乎最佳策略效率，使其在实际应用中成为可能和有利的。我们进一步显示了这个方法可以对广告预算分配问题进行传递均衡，而这是先前值基推广学中无法实现的。我们的实验显示，这个方法可以在大规模的广告营运中实现理论上的最佳性和实际上的优化性，并且已经成功部署到处理整个广告营运的应用中。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-Over-Images-Vertical-Decompositions-and-Pre-Trained-Backbones-Are-Difficult-to-Beat"><a href="#Federated-Learning-Over-Images-Vertical-Decompositions-and-Pre-Trained-Backbones-Are-Difficult-to-Beat" class="headerlink" title="Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat"></a>Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03237">http://arxiv.org/abs/2309.03237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erdong Hu, Yuxin Tang, Anastasios Kyrillidis, Chris Jermaine</li>
<li>for: 研究了许多 Federated Learning 中的算法，并对各种图像分类任务进行了测试。</li>
<li>methods: 考虑了许多前不得以考虑的问题，如数据集中图像多样性的影响、使用预训练特征提取“backbone”、评估学习器表现等。</li>
<li>results: 在多种设置下，发现Vertically decomposing a neural network 最佳化结果，并超越了标准的重新结合方法。<details>
<summary>Abstract</summary>
We carefully evaluate a number of algorithms for learning in a federated environment, and test their utility for a variety of image classification tasks. We consider many issues that have not been adequately considered before: whether learning over data sets that do not have diverse sets of images affects the results; whether to use a pre-trained feature extraction "backbone"; how to evaluate learner performance (we argue that classification accuracy is not enough), among others. Overall, across a wide variety of settings, we find that vertically decomposing a neural network seems to give the best results, and outperforms more standard reconciliation-used methods.
</details>
<details>
<summary>摘要</summary>
我们仔细评估了许多 federated learning 环境下的算法，并测试其用于多种图像分类任务。我们考虑了许多未得到足够考虑的问题：不同数据集之间的图像多样性对结果的影响；使用预训练的特征提取“脊梁”；评估学习器性能的方法（我们认为准确率不够）等。总之，在各种设置下，我们发现将神经网络垂直分解得到最佳结果，并超越了标准的重新调节方法。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-as-Kernel-Approximation"><a href="#Contrastive-Learning-as-Kernel-Approximation" class="headerlink" title="Contrastive Learning as Kernel Approximation"></a>Contrastive Learning as Kernel Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02651">http://arxiv.org/abs/2309.02651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantinos Christopher Tsiolis</li>
<li>for:  This paper is written for those interested in contrastive learning and its applications in feature learning.</li>
<li>methods: The paper uses contrastive learning methods to train low-dimensional vector representations of high-dimensional inputs on large unlabelled datasets. These methods include contrastive loss functions, such as those that implicitly approximate a positive semidefinite (PSD) kernel.</li>
<li>results: The paper provides an overview of the current theoretical understanding of contrastive learning, specifically in relation to the minimizers of contrastive loss functions and their relationship to prior methods for learning features from unlabelled data. It highlights popular contrastive loss functions whose minimizers implicitly approximate a PSD kernel.<details>
<summary>Abstract</summary>
In standard supervised machine learning, it is necessary to provide a label for every input in the data. While raw data in many application domains is easily obtainable on the Internet, manual labelling of this data is prohibitively expensive. To circumvent this issue, contrastive learning methods produce low-dimensional vector representations (also called features) of high-dimensional inputs on large unlabelled datasets. This is done by training with a contrastive loss function, which enforces that similar inputs have high inner product and dissimilar inputs have low inner product in the feature space. Rather than annotating each input individually, it suffices to define a means of sampling pairs of similar and dissimilar inputs. Contrastive features can then be fed as inputs to supervised learning systems on much smaller labelled datasets to obtain high accuracy on end tasks of interest.   The goal of this thesis is to provide an overview of the current theoretical understanding of contrastive learning, specifically as it pertains to the minimizers of contrastive loss functions and their relationship to prior methods for learning features from unlabelled data. We highlight popular contrastive loss functions whose minimizers implicitly approximate a positive semidefinite (PSD) kernel. The latter is a well-studied object in functional analysis and learning theory that formalizes a notion of similarity between elements of a space. PSD kernels provide an implicit definition of features through the theory of reproducing kernel Hilbert spaces.
</details>
<details>
<summary>摘要</summary>
通常的超级vised机器学习中需要为每个输入提供标签。然而，在许多应用领域中，Raw Data可以轻松地从互联网上获取，但是手动标签这些数据是非常昂贵的。为了解决这个问题，对比学习方法生成了低维度的向量表示（也称为特征），使得高维度的输入可以在大量未标记数据上进行学习。这是通过训练一个对比损失函数来实现的，该函数要求相似的输入在特征空间中具有高内积，而不相似的输入具有低内积。相比于每个输入都需要手动标签，只需定义一种采样相似和不相似的输入对来。对比特征可以被feed到标记数据集上的超级vised学习系统中，以实现高精度的终点任务。本论文的目标是提供对现有的对比学习理论的概括，具体来说是关于对比损失函数的最小化器和其与之前未标记数据学习的方法之间的关系。我们强调了流行的对比损失函数，其最小化器隐式地 Approximate一个正semidefinite（PSD）kernel。后者是函数分析和学习理论中已有研究的一种概念，它 formalizes a notion of similarity between elements of a space。PSD kernel Provides an implicit definition of features through the theory of reproducing kernel Hilbert spaces。
</details></li>
</ul>
<hr>
<h2 id="TFBEST-Dual-Aspect-Transformer-with-Learnable-Positional-Encoding-for-Failure-Prediction"><a href="#TFBEST-Dual-Aspect-Transformer-with-Learnable-Positional-Encoding-for-Failure-Prediction" class="headerlink" title="TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction"></a>TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02641">http://arxiv.org/abs/2309.02641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Mohapatra, Saptarshi Sengupta<br>for:* 预测硬盘失效，帮助数据中心避免数据损失和信任问题。methods:* 使用Self-Monitoring, Analysis and Reporting Technology（S.M.A.R.T）logs进行预测，并提出一种基于Transformer架构的新方法——Temporal-fusion Bi-encoder Self-attention Transformer（TFBEST）。results:* 在使用Seagate硬盘数据进行测试，TFBEST方法与当前状态艺术方法进行比较，显示TFBEST方法在10年的Backblaze数据中显著超越了state-of-the-art RUL预测方法。<details>
<summary>Abstract</summary>
Hard Disk Drive (HDD) failures in datacenters are costly - from catastrophic data loss to a question of goodwill, stakeholders want to avoid it like the plague. An important tool in proactively monitoring against HDD failure is timely estimation of the Remaining Useful Life (RUL). To this end, the Self-Monitoring, Analysis and Reporting Technology employed within HDDs (S.M.A.R.T.) provide critical logs for long-term maintenance of the security and dependability of these essential data storage devices. Data-driven predictive models in the past have used these S.M.A.R.T. logs and CNN/RNN based architectures heavily. However, they have suffered significantly in providing a confidence interval around the predicted RUL values as well as in processing very long sequences of logs. In addition, some of these approaches, such as those based on LSTMs, are inherently slow to train and have tedious feature engineering overheads. To overcome these challenges, in this work we propose a novel transformer architecture - a Temporal-fusion Bi-encoder Self-attention Transformer (TFBEST) for predicting failures in hard-drives. It is an encoder-decoder based deep learning technique that enhances the context gained from understanding health statistics sequences and predicts a sequence of the number of days remaining before a disk potentially fails. In this paper, we also provide a novel confidence margin statistic that can help manufacturers replace a hard-drive within a time frame. Experiments on Seagate HDD data show that our method significantly outperforms the state-of-the-art RUL prediction methods during testing over the exhaustive 10-year data from Backblaze (2013-present). Although validated on HDD failure prediction, the TFBEST architecture is well-suited for other prognostics applications and may be adapted for allied regression problems.
</details>
<details>
<summary>摘要</summary>
硬盘驱动器（HDD）在数据中心失效的情况非常昂贵，从悬峰数据丢失到口碑问题，各方都想避免这种情况。为了早期发现HDD失效，有一种重要的工具是计算硬盘的剩余有用生命（RUL）的准确预测。为此，硬盘内部的自我监控、分析和报告技术（S.M.A.R.T）提供了关键的日志记录，以长期保持硬盘存储设备的安全和可靠性。在过去，数据驱动的预测模型使用了这些S.M.A.R.T.日志和卷积神经网络（CNN/RNN）结构，但它们受到了预测RUL值的置信度范围和处理非常长的日志序列的限制。此外，一些这些方法，如基于LSTM的方法，在训练时间过长和特征工程杂乱的问题。为了解决这些挑战，我们在这种工作中提出了一种新的变换器架构——时间融合双指纹自注意Transformer（TFBEST），用于预测硬盘失效。这是一种基于encoder-decoder的深度学习技术，它可以从健康统计序列中获得更多的上下文，并预测硬盘失效的天数剩余。在这篇论文中，我们还提出了一种新的置信度范围统计，可以帮助制造商在一定时间内替换硬盘。在对Seagate HDD数据进行测试时，我们发现我们的方法在对比当前状态艺术RUL预测方法时表现出色。虽然我们的方法已经验证在硬盘失效预测中，但TFBEST架构适用于其他预测应用程序，可能适用于相关的回归问题。
</details></li>
</ul>
<hr>
<h2 id="Epi-Curriculum-Episodic-Curriculum-Learning-for-Low-Resource-Domain-Adaptation-in-Neural-Machine-Translation"><a href="#Epi-Curriculum-Episodic-Curriculum-Learning-for-Low-Resource-Domain-Adaptation-in-Neural-Machine-Translation" class="headerlink" title="Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation"></a>Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02640">http://arxiv.org/abs/2309.02640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyu Chen, Di Zhuang, Mingchen Li, J. Morris Chang</li>
<li>for: 提高新领域少量数据下NMT模型的表现</li>
<li>methods: 提出了一种新的 episodic 训练框架，以及一种 denoised curriculum learning 技术</li>
<li>results: 实验表明，Epi-Curriculum 可以提高模型的鲁棒性和适应性，并且可以在seen和unseen领域中提高模型的表现<details>
<summary>Abstract</summary>
Neural Machine Translation (NMT) models have become successful, but their performance remains poor when translating on new domains with a limited number of data. In this paper, we present a novel approach Epi-Curriculum to address low-resource domain adaptation (DA), which contains a new episodic training framework along with denoised curriculum learning. Our episodic training framework enhances the model's robustness to domain shift by episodically exposing the encoder/decoder to an inexperienced decoder/encoder. The denoised curriculum learning filters the noised data and further improves the model's adaptability by gradually guiding the learning process from easy to more difficult tasks. Experiments on English-German and English-Romanian translation show that: (i) Epi-Curriculum improves both model's robustness and adaptability in seen and unseen domains; (ii) Our episodic training framework enhances the encoder and decoder's robustness to domain shift.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multiclass-Alignment-of-Confidence-and-Certainty-for-Network-Calibration"><a href="#Multiclass-Alignment-of-Confidence-and-Certainty-for-Network-Calibration" class="headerlink" title="Multiclass Alignment of Confidence and Certainty for Network Calibration"></a>Multiclass Alignment of Confidence and Certainty for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02636">http://arxiv.org/abs/2309.02636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinith Kugathasan, Muhammad Haris Khan</li>
<li>for: 提高模型预测结果的可靠性和准确性，特别是在安全关键应用场景。</li>
<li>methods: 基于模型参数全部使用的训练时间calibration方法，以及一种简单的插件式auxiliary loss函数multi-class alignment of predictive mean confidence and predictive certainty (MACC)。</li>
<li>results: 在10个复杂的数据集上进行了广泛的实验，得到了state-of-the-art的calibration性能，包括在预测内部和预测外部的预测。我们将代码和模型公开发布。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have made great strides in pushing the state-of-the-art in several challenging domains. Recent studies reveal that they are prone to making overconfident predictions. This greatly reduces the overall trust in model predictions, especially in safety-critical applications. Early work in improving model calibration employs post-processing techniques which rely on limited parameters and require a hold-out set. Some recent train-time calibration methods, which involve all model parameters, can outperform the postprocessing methods. To this end, we propose a new train-time calibration method, which features a simple, plug-and-play auxiliary loss known as multi-class alignment of predictive mean confidence and predictive certainty (MACC). It is based on the observation that a model miscalibration is directly related to its predictive certainty, so a higher gap between the mean confidence and certainty amounts to a poor calibration both for in-distribution and out-of-distribution predictions. Armed with this insight, our proposed loss explicitly encourages a confident (or underconfident) model to also provide a low (or high) spread in the presoftmax distribution. Extensive experiments on ten challenging datasets, covering in-domain, out-domain, non-visual recognition and medical image classification scenarios, show that our method achieves state-of-the-art calibration performance for both in-domain and out-domain predictions. Our code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在许多复杂领域取得了大幅进步，但最新的研究表明它们有很大的自信问题。这会大幅降低模型预测结果的总体信任度，特别是在安全关键应用中。早期的模型校准方法使用了post处理技术，这些技术具有有限的参数和需要保留集。一些最近的运行时校准方法，它们包含所有模型参数，可以超越post处理方法。为此，我们提出了一种新的运行时校准方法，它具有一个简单、插件化的多类准确率对照（MACC）损失函数。这种损失函数基于模型误差与预测确定性之间的直接关系，因此，一个高于平均信任度的模型也应该提供一个低于平均信任度的 spreadoption 分布。利用这一见解，我们的提议的损失函数显式地鼓励一个自信（或不自信）的模型也提供一个低（或高）的 spreadoption 分布。我们在十个复杂的数据集上进行了广泛的实验，包括预测领域、非预测领域、非视觉识别和医学图像分类场景，结果显示，我们的方法在预测领域和非预测领域的校准性表现为状态机器人的最佳。我们的代码和模型将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-from-Hierarchical-Weak-Preference-Feedback"><a href="#Deep-Reinforcement-Learning-from-Hierarchical-Weak-Preference-Feedback" class="headerlink" title="Deep Reinforcement Learning from Hierarchical Weak Preference Feedback"></a>Deep Reinforcement Learning from Hierarchical Weak Preference Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02632">http://arxiv.org/abs/2309.02632</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abukharin3/heron">https://github.com/abukharin3/heron</a></li>
<li>paper_authors: Alexander Bukharin, Yixiao Li, Pengcheng He, Weizhu Chen, Tuo Zhao</li>
<li>for: 本研究的目的是开发一种新的奖励学习框架，以便在困难任务上学习奖励函数，并且减少人类干预的成本。</li>
<li>methods: 本研究使用了 preference-based 奖励模型，通过比较执行轨迹的决策树来训练奖励函数，然后使用这个奖励函数进行策略学习。</li>
<li>results: 研究发现，使用 HERON 框架可以训练高性能的代理人，同时提供更好的样本效率和鲁棒性。<details>
<summary>Abstract</summary>
Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; simply by ranking the importance of the reward factors. More specifically, we propose a new RL framework -- HERON, which compares trajectories using a hierarchical decision tree induced by the given ranking. These comparisons are used to train a preference-based reward model, which is then used for policy learning. We find that our framework can not only train high performing agents on a variety of difficult tasks, but also provide additional benefits such as improved sample efficiency and robustness. Our code is available at https://github.com/abukharin3/HERON.
</details>
<details>
<summary>摘要</summary>
奖励设计是实用渐进学习（RL）的基本 yet 挑战性方面。 для简单任务，研究人员通常手工设计奖励函数，例如使用一些奖励因素的线性组合。 however, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; simply by ranking the importance of the reward factors. More specifically, we propose a new RL framework -- HERON, which compares trajectories using a hierarchical decision tree induced by the given ranking. These comparisons are used to train a preference-based reward model, which is then used for policy learning. We find that our framework can not only train high performing agents on a variety of difficult tasks, but also provide additional benefits such as improved sample efficiency and robustness. Our code is available at https://github.com/abukharin3/HERON.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/06/cs.LG_2023_09_06/" data-id="clmjn91mx00830j885uqu8ftz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/06/eess.IV_2023_09_06/" class="article-date">
  <time datetime="2023-09-06T09:00:00.000Z" itemprop="datePublished">2023-09-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/06/eess.IV_2023_09_06/">eess.IV - 2023-09-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Kidney-abnormality-segmentation-in-thorax-abdomen-CT-scans"><a href="#Kidney-abnormality-segmentation-in-thorax-abdomen-CT-scans" class="headerlink" title="Kidney abnormality segmentation in thorax-abdomen CT scans"></a>Kidney abnormality segmentation in thorax-abdomen CT scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03383">http://arxiv.org/abs/2309.03383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Efrain Humpire Mamani, Nikolas Lessmann, Ernst Th. Scholten, Mathias Prokop, Colin Jacobs, Bram van Ginneken</li>
<li>for:  This paper aims to support clinicians in identifying and quantifying renal abnormalities such as cysts, lesions, masses, metastases, and primary tumors using a deep learning approach for segmenting kidney parenchyma and kidney abnormalities.</li>
<li>methods:  The paper introduces an end-to-end segmentation method that was trained on 215 contrast-enhanced thoracic-abdominal CT scans, with half of these scans containing one or more abnormalities. The method incorporates four additional components: an end-to-end multi-resolution approach, a set of task-specific data augmentations, a modified loss function using top-$k$, and spatial dropout.</li>
<li>results:  The paper reports that the best-performing model attained Dice scores of 0.965 and 0.947 for segmenting kidney parenchyma in two test sets, outperforming an independent human observer. In segmenting kidney abnormalities within the 30 test scans containing them, the top-performing method achieved a Dice score of 0.585, while an independent second human observer reached a score of 0.664.<details>
<summary>Abstract</summary>
In this study, we introduce a deep learning approach for segmenting kidney parenchyma and kidney abnormalities to support clinicians in identifying and quantifying renal abnormalities such as cysts, lesions, masses, metastases, and primary tumors. Our end-to-end segmentation method was trained on 215 contrast-enhanced thoracic-abdominal CT scans, with half of these scans containing one or more abnormalities.   We began by implementing our own version of the original 3D U-Net network and incorporated four additional components: an end-to-end multi-resolution approach, a set of task-specific data augmentations, a modified loss function using top-$k$, and spatial dropout. Furthermore, we devised a tailored post-processing strategy. Ablation studies demonstrated that each of the four modifications enhanced kidney abnormality segmentation performance, while three out of four improved kidney parenchyma segmentation. Subsequently, we trained the nnUNet framework on our dataset. By ensembling the optimized 3D U-Net and the nnUNet with our specialized post-processing, we achieved marginally superior results.   Our best-performing model attained Dice scores of 0.965 and 0.947 for segmenting kidney parenchyma in two test sets (20 scans without abnormalities and 30 with abnormalities), outperforming an independent human observer who scored 0.944 and 0.925, respectively. In segmenting kidney abnormalities within the 30 test scans containing them, the top-performing method achieved a Dice score of 0.585, while an independent second human observer reached a score of 0.664, suggesting potential for further improvement in computerized methods.   All training data is available to the research community under a CC-BY 4.0 license on https://doi.org/10.5281/zenodo.8014289
</details>
<details>
<summary>摘要</summary>
在本研究中，我们介绍了一种深度学习方法用于分割肾脏和肾脏畸形，以支持临床医生在识别和评估肾脏畸形，如肿瘤、斑块、肿瘤、转移和原发性肿瘤。我们的端到端分割方法在215个对比增强的胸腹Computed Tomography（CT）扫描图像上进行训练，其中半数图像包含一个或多个畸形。我们开始实现我们自己的版本的原始3D U-Net网络，并添加了四个附加组件：端到端多分辨率方法、任务特定的数据增强、修改后-$k$损失函数和空间抽取。此外，我们设计了特制的后处理策略。减少学习中的每一个修改都提高了肾脏畸形分割性能，而三个中有四个提高了肾脏分割性能。然后，我们将nnUNet框架在我们的数据集上训练。通过将优化的3D U-Net和nnUNet框架 ensemble，我们实现了有些微的提高。我们最佳的模型在两个测试集（20个无畸形图像和30个含畸形图像）中，对肾脏分割达到了Dice分数为0.965和0.947，超过了一名独立的人类观察员的分数（0.944和0.925）。在30个测试图像中含有畸形的情况下，我们的最佳方法达到了Dice分数为0.585，而第二名独立的人类观察员达到了分数为0.664，表明计算机方法还有更多的提高空间。所有的训练数据都是通过CC-BY 4.0license在https://doi.org/10.5281/zenodo.8014289上公开发布。
</details></li>
</ul>
<hr>
<h2 id="Compact-Representation-of-n-th-order-TGV"><a href="#Compact-Representation-of-n-th-order-TGV" class="headerlink" title="Compact Representation of n-th order TGV"></a>Compact Representation of n-th order TGV</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03359">http://arxiv.org/abs/2309.03359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manu Ghulyani, Muthuvel Arigovindan</li>
<li>for: 本研究旨在探讨高顺度 Television (TV) 变形的扩展，并提供一个简单实现的 nth 顺度全总变形 (TGV) 的方法。</li>
<li>methods: 本研究使用了两种简单的表示方法来解决 TGV 变形的问题，并且这些表示方法可以实现在现有的数位图像处理系统上。</li>
<li>results: 本研究获得了一个简单的 TGV 变形方法，并且该方法可以在高顺度下实现 piece-wise 多项函数的表示。此外，本研究还获得了一个可以实现在现有的数位图像处理系统上的 TGV 变形算法。<details>
<summary>Abstract</summary>
Although regularization methods based on derivatives are favored for their robustness and computational simplicity, research exploring higher-order derivatives remains limited. This scarcity can possibly be attributed to the appearance of oscillations in reconstructions when directly generalizing TV-1 to higher orders (3 or more). Addressing this, Bredies et. al introduced a notable approach for generalizing total variation, known as Total Generalized Variation (TGV). This technique introduces a regularization that generates estimates embodying piece-wise polynomial behavior of varying degrees across distinct regions of an image.Importantly, to our current understanding, no sufficiently general algorithm exists for solving TGV regularization for orders beyond 2. This is likely because of two problems: firstly, the problem is complex as TGV regularization is defined as a minimization problem with non-trivial constraints, and secondly, TGV is represented in terms of tensor-fields which is difficult to implement. In this work we tackle the first challenge by giving two simple and implementable representations of n th order TGV
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Real-Time-Dynamic-Data-Driven-Deformable-Registration-for-Image-Guided-Neurosurgery-Computational-Aspects"><a href="#Real-Time-Dynamic-Data-Driven-Deformable-Registration-for-Image-Guided-Neurosurgery-Computational-Aspects" class="headerlink" title="Real-Time Dynamic Data Driven Deformable Registration for Image-Guided Neurosurgery: Computational Aspects"></a>Real-Time Dynamic Data Driven Deformable Registration for Image-Guided Neurosurgery: Computational Aspects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03336">http://arxiv.org/abs/2309.03336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikos Chrisochoides, Andrey Fedorov, Yixun Liu, Andriy Kot, Panos Foteinos, Fotis Drakopoulos, Christos Tsolakis, Emmanuel Billias, Olivier Clatz, Nicholas Ayache, Alex Golby, Peter Black, Ron Kikinis</li>
<li>for: 这篇论文是为了描述一种基于数学模型的脑成像注射技术，以帮助 neurosurgeon 在手术中准确定位脑肿和关键脑结构。</li>
<li>methods: 这种技术使用了动态数据驱动非固定准确注射方法（D4NRR），该方法可以在手术过程中动态调整先前的成像数据，以 compte pour intra-operative brain shift。</li>
<li>results: 这篇论文总结了这种技术的计算方面的不同变体，以及其在过去15年的进展和未来的发展方向。<details>
<summary>Abstract</summary>
Current neurosurgical procedures utilize medical images of various modalities to enable the precise location of tumors and critical brain structures to plan accurate brain tumor resection. The difficulty of using preoperative images during the surgery is caused by the intra-operative deformation of the brain tissue (brain shift), which introduces discrepancies concerning the preoperative configuration. Intra-operative imaging allows tracking such deformations but cannot fully substitute for the quality of the pre-operative data. Dynamic Data Driven Deformable Non-Rigid Registration (D4NRR) is a complex and time-consuming image processing operation that allows the dynamic adjustment of the pre-operative image data to account for intra-operative brain shift during the surgery. This paper summarizes the computational aspects of a specific adaptive numerical approximation method and its variations for registering brain MRIs. It outlines its evolution over the last 15 years and identifies new directions for the computational aspects of the technique.
</details>
<details>
<summary>摘要</summary>
现有的神经外科手段使用不同模式的医疗图像来准确定位肿瘤和critical brain structures，以便精准地进行肿瘤除脑手术。然而，使用过程中的脑组织变形（brain shift）会导致医疗图像与先前的配置存在差异，从而使得使用过程中的医疗图像变得更加困难。运行时的图像捕捉可以跟踪这些变形，但无法完全取代先前的数据质量。动态数据驱动非rigid Registration（D4NRR）是一种复杂且时间消耗的图像处理操作，它允许在手术过程中动态调整先前的MRI数据，以 compte pour intra-operative brain shift。这篇文章概述了D4NRR的计算方面的特点和其变化在过去15年中，并提出了新的计算方法的发展方向。
</details></li>
</ul>
<hr>
<h2 id="SADIR-Shape-Aware-Diffusion-Models-for-3D-Image-Reconstruction"><a href="#SADIR-Shape-Aware-Diffusion-Models-for-3D-Image-Reconstruction" class="headerlink" title="SADIR: Shape-Aware Diffusion Models for 3D Image Reconstruction"></a>SADIR: Shape-Aware Diffusion Models for 3D Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03335">http://arxiv.org/abs/2309.03335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nivetha Jayakumar, Tonmoy Hossain, Miaomiao Zhang</li>
<li>for: 三维图像重建从有限数量的二维图像中是计算机视觉和图像分析领域的长期挑战。现有的深度学习基本方法已经实现了在这个领域的出色表现，但是现有的深度网络通常无法有效地利用图像中对象的形状结构。因此，重建的对象的topology可能不会很好地保留，导致图像中的缺陷，如缺陷、洞孔或不一致的连接。在这篇论文中，我们提出了一种基于扩散模型的形态认知网络，名为SADIR，以解决这些问题。</li>
<li>methods: 我们的SADIR模型在不同于先前的方法，它不仅仅基于图像INTENSITY的空间相关性来进行三维重建，而是利用在训练数据中学习的形态约束来引导重建过程。为此，我们开发了一个共同学习网络，该网络同时学习一个变换模型中的平均形状。每个重建的图像都被视为变换模型中的一个偏shifted变换。</li>
<li>results: 我们在大脑和心脏Magnetic Resonance Imaging (MRI)图像上验证了我们的SADIR模型。实验结果表明，我们的方法在减少重建错误和更好地保留图像中对象的形状结构方面都有更高的表现。<details>
<summary>Abstract</summary>
3D image reconstruction from a limited number of 2D images has been a long-standing challenge in computer vision and image analysis. While deep learning-based approaches have achieved impressive performance in this area, existing deep networks often fail to effectively utilize the shape structures of objects presented in images. As a result, the topology of reconstructed objects may not be well preserved, leading to the presence of artifacts such as discontinuities, holes, or mismatched connections between different parts. In this paper, we propose a shape-aware network based on diffusion models for 3D image reconstruction, named SADIR, to address these issues. In contrast to previous methods that primarily rely on spatial correlations of image intensities for 3D reconstruction, our model leverages shape priors learned from the training data to guide the reconstruction process. To achieve this, we develop a joint learning network that simultaneously learns a mean shape under deformation models. Each reconstructed image is then considered as a deformed variant of the mean shape. We validate our model, SADIR, on both brain and cardiac magnetic resonance images (MRIs). Experimental results show that our method outperforms the baselines with lower reconstruction error and better preservation of the shape structure of objects within the images.
</details>
<details>
<summary>摘要</summary>
三维图像重建从有限数量的二维图像是计算机视觉和图像分析领域的长期挑战。而深度学习基于的方法已经在这个领域 achieved impressive performance, but existing deep networks often fail to effectively utilize the shape structures of objects presented in images. As a result, the topology of reconstructed objects may not be well preserved, leading to the presence of artifacts such as discontinuities, holes, or mismatched connections between different parts. In this paper, we propose a shape-aware network based on diffusion models for 3D image reconstruction, named SADIR, to address these issues. In contrast to previous methods that primarily rely on spatial correlations of image intensities for 3D reconstruction, our model leverages shape priors learned from the training data to guide the reconstruction process. To achieve this, we develop a joint learning network that simultaneously learns a mean shape under deformation models. Each reconstructed image is then considered as a deformed variant of the mean shape. We validate our model, SADIR, on both brain and cardiac magnetic resonance images (MRIs). Experimental results show that our method outperforms the baselines with lower reconstruction error and better preservation of the shape structure of objects within the images.
</details></li>
</ul>
<hr>
<h2 id="Expert-Uncertainty-and-Severity-Aware-Chest-X-Ray-Classification-by-Multi-Relationship-Graph-Learning"><a href="#Expert-Uncertainty-and-Severity-Aware-Chest-X-Ray-Classification-by-Multi-Relationship-Graph-Learning" class="headerlink" title="Expert Uncertainty and Severity Aware Chest X-Ray Classification by Multi-Relationship Graph Learning"></a>Expert Uncertainty and Severity Aware Chest X-Ray Classification by Multi-Relationship Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03331">http://arxiv.org/abs/2309.03331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengliang Zhang, Xinyue Hu, Lin Gu, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu</li>
<li>For: 该论文旨在提高胸部X射线检测中的疾病标签检测精度，通过考虑疾病严重程度和不确定性进行精度更高的疾病标签检测。* Methods: 该论文采用了规则基本方法与临床专家讨论的关键词来重新提取疾病标签，并使用多关系图学习方法和专家不确定性感知损失函数进一步提高检测结果的可读性。* Results: 该论文的实验结果表明，考虑疾病严重程度和不确定性的模型可以超越之前的状态艺术方法的性能。<details>
<summary>Abstract</summary>
Patients undergoing chest X-rays (CXR) often endure multiple lung diseases. When evaluating a patient's condition, due to the complex pathologies, subtle texture changes of different lung lesions in images, and patient condition differences, radiologists may make uncertain even when they have experienced long-term clinical training and professional guidance, which makes much noise in extracting disease labels based on CXR reports. In this paper, we re-extract disease labels from CXR reports to make them more realistic by considering disease severity and uncertainty in classification. Our contributions are as follows: 1. We re-extracted the disease labels with severity and uncertainty by a rule-based approach with keywords discussed with clinical experts. 2. To further improve the explainability of chest X-ray diagnosis, we designed a multi-relationship graph learning method with an expert uncertainty-aware loss function. 3. Our multi-relationship graph learning method can also interpret the disease classification results. Our experimental results show that models considering disease severity and uncertainty outperform previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
病人在胸部X射线检查（CXR）时经常患有多种肺病。在评估病人情况时，由于肺脏病变的复杂性、不同肺脏病变图像中的微小Texture变化以及病人状况的不同， radiologist可能会对病理标签进行不确定的识别，即使他们已经经历了长期的临床培训和专业指导。这会带来很多噪音在基于CXR报告中提取病理标签。在这篇论文中，我们重新提取了病理标签，以使其更加真实性。我们的贡献包括：1. 我们使用规则基于方法和与临床专家讨论的关键词来重新提取病理标签，并考虑疾病严重性和不确定性。2. 为了进一步提高胸部X射线诊断的可解释性，我们设计了一种多关系图学学习方法，并使用专家不确定性感知损失函数。3. 我们的多关系图学学习方法还可以解释肺病分类结果。我们的实验结果表明，考虑疾病严重性和不确定性的模型可以超越前一代方法的性能。
</details></li>
</ul>
<hr>
<h2 id="CoNeS-Conditional-neural-fields-with-shift-modulation-for-multi-sequence-MRI-translation"><a href="#CoNeS-Conditional-neural-fields-with-shift-modulation-for-multi-sequence-MRI-translation" class="headerlink" title="CoNeS: Conditional neural fields with shift modulation for multi-sequence MRI translation"></a>CoNeS: Conditional neural fields with shift modulation for multi-sequence MRI translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03320">http://arxiv.org/abs/2309.03320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyjdswx/cones">https://github.com/cyjdswx/cones</a></li>
<li>paper_authors: Yunjie Chen, Marius Staring, Olaf M. Neve, Stephan R. Romeijn, Erik F. Hensen, Berit M. Verbist, Jelmer M. Wolterink, Qian Tao</li>
<li>for:  This paper is written for the purpose of proposing a new method for multi-sequence magnetic resonance imaging (MRI) translation, which can overcome the limitations of conventional convolutional neural network (CNN) models and provide high-quality synthesized images for clinical downstream tasks.</li>
<li>methods:  The proposed method, called Conditional Neural fields with Shift modulation (CoNeS), uses a multi-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixel mapping, and leverages shift modulation with a learned latent code to condition the target images on the source image.</li>
<li>results:  The proposed method was tested on two datasets, BraTS 2018 and an in-house clinical dataset of vestibular schwannoma patients, and was found to outperform state-of-the-art methods for multi-sequence MRI translation both visually and quantitatively. Additionally, spectral analysis showed that CoNeS was able to overcome the spectral bias issue common in conventional CNN models.<details>
<summary>Abstract</summary>
Multi-sequence magnetic resonance imaging (MRI) has found wide applications in both modern clinical studies and deep learning research. However, in clinical practice, it frequently occurs that one or more of the MRI sequences are missing due to different image acquisition protocols or contrast agent contraindications of patients, limiting the utilization of deep learning models trained on multi-sequence data. One promising approach is to leverage generative models to synthesize the missing sequences, which can serve as a surrogate acquisition. State-of-the-art methods tackling this problem are based on convolutional neural networks (CNN) which usually suffer from spectral biases, resulting in poor reconstruction of high-frequency fine details. In this paper, we propose Conditional Neural fields with Shift modulation (CoNeS), a model that takes voxel coordinates as input and learns a representation of the target images for multi-sequence MRI translation. The proposed model uses a multi-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixel mapping. Hence, each target image is represented as a neural field that is conditioned on the source image via shift modulation with a learned latent code. Experiments on BraTS 2018 and an in-house clinical dataset of vestibular schwannoma patients showed that the proposed method outperformed state-of-the-art methods for multi-sequence MRI translation both visually and quantitatively. Moreover, we conducted spectral analysis, showing that CoNeS was able to overcome the spectral bias issue common in conventional CNN models. To further evaluate the usage of synthesized images in clinical downstream tasks, we tested a segmentation network using the synthesized images at inference.
</details>
<details>
<summary>摘要</summary>
多序列磁共振成像（MRI）在现代临床研究和深度学习领域都有广泛的应用。然而，在临床实践中，有时一些MRI序列会缺失，这限制了由深度学习模型在多序列数据上训练的使用。一种有前途的方法是使用生成模型生成缺失的序列，这可以作为代用性的获得。现状的方法通常是使用卷积神经网络（CNN），它们通常会受到频率偏好，导致高频精细 Details的重建不佳。在这篇论文中，我们提出了基于 Conditional Neural fields with Shift modulation（CoNeS）模型，它将 voxel 坐标作为输入，学习target images的多序列 MRI 翻译。我们的模型使用多层感知神经网络（MLP） instead of CNN 作为像素到像素映射的解码器。因此，每个target image都被表示为一个 conditioned on the source image的神经场，通过 shift modulation with a learned latent code 来进行映射。我们在 BraTS 2018 和一个内部的临床数据集中进行了实验，结果显示，我们的方法超过了当前state-of-the-art方法在多序列 MRI 翻译中 both visually and quantitatively。此外，我们进行了spectral analysis，显示 CoNeS 能够超越 convential CNN 模型中的频率偏好问题。为了进一步评估生成图像在临床下渠道任务中的使用，我们在推理阶段使用生成图像进行 segmentation 网络的测试。
</details></li>
</ul>
<hr>
<h2 id="3D-Transformer-based-on-deformable-patch-location-for-differential-diagnosis-between-Alzheimer’s-disease-and-Frontotemporal-dementia"><a href="#3D-Transformer-based-on-deformable-patch-location-for-differential-diagnosis-between-Alzheimer’s-disease-and-Frontotemporal-dementia" class="headerlink" title="3D Transformer based on deformable patch location for differential diagnosis between Alzheimer’s disease and Frontotemporal dementia"></a>3D Transformer based on deformable patch location for differential diagnosis between Alzheimer’s disease and Frontotemporal dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03183">http://arxiv.org/abs/2309.03183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy-Dung Nguyen, Michaël Clément, Boris Mansencal, Pierrick Coupé</li>
<li>for: 该研究旨在提高阿尔ц海默病和前rontemporal dementia的多类差异诊断。</li>
<li>methods: 该研究使用了一种基于transformer的3D扩展模型，并提出了一种有效的数据增强技术来解决数据稀缺问题。</li>
<li>results: 实验结果表明，提出的方法可以与现有的状态艺术方法竞争，并且可以显示出每种疾病的诊断关键区域。Here’s the summary in English for reference:</li>
<li>for: The study aims to improve the multi-class differential diagnosis of Alzheimer’s disease and Frontotemporal dementia.</li>
<li>methods: The study uses a novel 3D transformer-based architecture with a deformable patch location module, and proposes an efficient combination of data augmentation techniques to overcome data scarcity.</li>
<li>results: Experimental results show that the proposed approach is competitive with state-of-the-art methods and can visualize the most relevant brain regions used for diagnosis.<details>
<summary>Abstract</summary>
Alzheimer's disease and Frontotemporal dementia are common types of neurodegenerative disorders that present overlapping clinical symptoms, making their differential diagnosis very challenging. Numerous efforts have been done for the diagnosis of each disease but the problem of multi-class differential diagnosis has not been actively explored. In recent years, transformer-based models have demonstrated remarkable success in various computer vision tasks. However, their use in disease diagnostic is uncommon due to the limited amount of 3D medical data given the large size of such models. In this paper, we present a novel 3D transformer-based architecture using a deformable patch location module to improve the differential diagnosis of Alzheimer's disease and Frontotemporal dementia. Moreover, to overcome the problem of data scarcity, we propose an efficient combination of various data augmentation techniques, adapted for training transformer-based models on 3D structural magnetic resonance imaging data. Finally, we propose to combine our transformer-based model with a traditional machine learning model using brain structure volumes to better exploit the available data. Our experiments demonstrate the effectiveness of the proposed approach, showing competitive results compared to state-of-the-art methods. Moreover, the deformable patch locations can be visualized, revealing the most relevant brain regions used to establish the diagnosis of each disease.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病和前rontemporal dementia 是常见的神经退化疾病，它们在临床表现上存在 overlap，这使得它们的分类诊断非常困难。各种努力已经在每种疾病的诊断方面做出了很多，但多类分类诊断问题尚未得到了active exploration。在最近的几年中，transformer-based模型在计算机视觉任务中表现出了惊人的成功。然而，它们在疾病诊断中的使用不寻常，主要是因为3D医疗数据的有限性，这些模型的大小。在本文中，我们提出了一种基于transformer的3D扩展模型，使用可变的质心位置模块来提高阿尔茨海默病和前rontemporal dementia的分类诊断。此外，为了解决数据缺乏问题，我们提议了一种有效的数据扩展技术组合，适用于在3D结构磁共振成像数据上训练transformer-based模型。最后，我们提议将我们的transformer-based模型与传统的机器学习模型相结合，以更好地利用可用的数据。我们的实验结果表明，我们的方法具有竞争力，与当前的方法相比，并且可以visualize可变的质心位置，表明在诊断每种疾病时，哪些脑区最 relevante。
</details></li>
</ul>
<hr>
<h2 id="The-Secrets-of-Non-Blind-Poisson-Deconvolution"><a href="#The-Secrets-of-Non-Blind-Poisson-Deconvolution" class="headerlink" title="The Secrets of Non-Blind Poisson Deconvolution"></a>The Secrets of Non-Blind Poisson Deconvolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03105">http://arxiv.org/abs/2309.03105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhiram Gnanasambandam, Yash Sanghvi, Stanley H. Chan</li>
<li>for: 非干扰式图像推整，适用于对于实际摄像机读取器所获得的图像进行推整，以提高图像质量。</li>
<li>methods: 本文使用了系统性分析来检查目前Literature中的波兰non-blind推整算法，包括传统和深度学习方法。我们组装了五个”秘密”，以帮助设计算法。</li>
<li>results: 我们建立了一个证明型方法， combinining the five secrets，并发现这个新方法可以与一些最新的方法相比，而且超过一些较老的方法。<details>
<summary>Abstract</summary>
Non-blind image deconvolution has been studied for several decades but most of the existing work focuses on blur instead of noise. In photon-limited conditions, however, the excessive amount of shot noise makes traditional deconvolution algorithms fail. In searching for reasons why these methods fail, we present a systematic analysis of the Poisson non-blind deconvolution algorithms reported in the literature, covering both classical and deep learning methods. We compile a list of five "secrets" highlighting the do's and don'ts when designing algorithms. Based on this analysis, we build a proof-of-concept method by combining the five secrets. We find that the new method performs on par with some of the latest methods while outperforming some older ones.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:非干擦图像去推算已经被研究数十年，但大多数现有工作都集中在模糊而不是噪声。在光子限制条件下，然而，过度的射频噪声使传统的推算算法失败。在寻找这些方法失败的原因时，我们提供了系统性的分析，涵盖了古典和深度学习方法。我们编辑了五个“秘密”，描述了设计算法时的做法和不做法。基于这种分析，我们构建了一个证明方法，并发现其与一些最新的方法性能相似，而对一些较老的方法性能更高。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-level-rain-image-generative-model-based-on-GAN"><a href="#Hierarchical-level-rain-image-generative-model-based-on-GAN" class="headerlink" title="Hierarchical-level rain image generative model based on GAN"></a>Hierarchical-level rain image generative model based on GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02964">http://arxiv.org/abs/2309.02964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyuan Liu, Tong Jia, Xingyu Xing, Jianfeng Wu, Junyi Chen</li>
<li>for:  This paper aims to improve the performance of visual perception algorithms in autonomous vehicles under various weather conditions, specifically rain, by generating realistic rain images using a hierarchical-level generative model called RCCycleGAN.</li>
<li>methods:  The RCCycleGAN model is based on a generative adversarial network (GAN) and uses a hierarchical structure to generate images of light, medium, and heavy rain. The model is trained using natural rain images and optimized with a specific training strategy to alleviate the problem of mode collapse.</li>
<li>results:  Compared to two baseline models, CycleGAN and DerainCycleGAN, RCCycleGAN achieves improved peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) on a test dataset. The ablation experiments also demonstrate the effectiveness of the model tuning.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目的是提高自动驾驶车辆在不同天气条件下视觉识别算法的性能，特别是在雨天下，通过使用层次结构的生成模型RCCycleGAN来生成真实的雨图像。</li>
<li>methods: RCCycleGAN模型基于生成对抗网络（GAN），使用层次结构生成雨图像的不同强度，并通过使用自然雨图像进行训练和优化，以解决模式混合问题。</li>
<li>results: 相比两个基eline模型CycleGAN和DerainCycleGAN，RCCycleGAN在测试数据集上的峰值信号噪声比（PSNR）和结构相似度（SSIM）都得到了提高，并通过ablation实验证明模型调整的效果。<details>
<summary>Abstract</summary>
Autonomous vehicles are exposed to various weather during operation, which is likely to trigger the performance limitations of the perception system, leading to the safety of the intended functionality (SOTIF) problems. To efficiently generate data for testing the performance of visual perception algorithms under various weather conditions, a hierarchical-level rain image generative model, rain conditional CycleGAN (RCCycleGAN), is constructed. RCCycleGAN is based on the generative adversarial network (GAN) and can generate images of light, medium, and heavy rain. Different rain intensities are introduced as labels in conditional GAN (CGAN). Meanwhile, the model structure is optimized and the training strategy is adjusted to alleviate the problem of mode collapse. In addition, natural rain images of different intensities are collected and processed for model training and validation. Compared with the two baseline models, CycleGAN and DerainCycleGAN, the peak signal-to-noise ratio (PSNR) of RCCycleGAN on the test dataset is improved by 2.58 dB and 0.74 dB, and the structural similarity (SSIM) is improved by 18% and 8%, respectively. The ablation experiments are also carried out to validate the effectiveness of the model tuning.
</details>
<details>
<summary>摘要</summary>
自适应汽车在运行过程中曝露于不同的天气条件，这可能会导致感知系统的性能限制，从而影响安全功能（SOTIF）的问题。为了有效地生成测试感知算法在不同天气条件下的性能数据，我们构建了一个层次结构的雨图生成模型——雨征 conditional CycleGAN（RCCycleGAN）。RCCycleGAN基于生成对抗网络（GAN），可以生成不同雨强度的雨图。在CGAN中，不同雨强度被用作标签。此外，模型结构被优化，并调整了训练策略，以解决模式混合问题。此外，我们还收集了不同雨强度的自然雨图，用于模型训练和验证。相比基eline模型CycleGAN和DerainCycleGAN，RCCycleGAN在测试集上的峰值信号噪比（PSNR）提高了2.58 dB和0.74 dB，同时 Structural Similarity（SSIM）提高了18%和8%。我们还进行了减少实验，以验证模型调整的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Non-Invasive-Interpretable-NAFLD-Diagnostic-Method-Combining-TCM-Tongue-Features"><a href="#A-Non-Invasive-Interpretable-NAFLD-Diagnostic-Method-Combining-TCM-Tongue-Features" class="headerlink" title="A Non-Invasive Interpretable NAFLD Diagnostic Method Combining TCM Tongue Features"></a>A Non-Invasive Interpretable NAFLD Diagnostic Method Combining TCM Tongue Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02959">http://arxiv.org/abs/2309.02959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cshan-github/selectornet">https://github.com/cshan-github/selectornet</a></li>
<li>paper_authors: Shan Cao, Qunsheng Ruan, Qingfeng Wu</li>
<li>for: 这个研究是为了提出一种非侵入性和可解释的非酒精肝病诊断方法，用户只需提供年龄、性别、身高、体重、腰围和腊围等几个数据，并且使用语言特征来进行诊断。</li>
<li>methods: 这个方法使用了一个名为SelectorNet的选择网络，该网络结合了注意力机制和特征选择机制，可以自动学习选择重要的特征。</li>
<li>results: 实验结果显示，提案的方法可以使用非侵入性数据 achieves an accuracy of 77.22%，并且提供了吸引人的解释矩阵。<details>
<summary>Abstract</summary>
Non-alcoholic fatty liver disease (NAFLD) is a clinicopathological syndrome characterized by hepatic steatosis resulting from the exclusion of alcohol and other identifiable liver-damaging factors. It has emerged as a leading cause of chronic liver disease worldwide. Currently, the conventional methods for NAFLD detection are expensive and not suitable for users to perform daily diagnostics. To address this issue, this study proposes a non-invasive and interpretable NAFLD diagnostic method, the required user-provided indicators are only Gender, Age, Height, Weight, Waist Circumference, Hip Circumference, and tongue image. This method involves merging patients' physiological indicators with tongue features, which are then input into a fusion network named SelectorNet. SelectorNet combines attention mechanisms with feature selection mechanisms, enabling it to autonomously learn the ability to select important features. The experimental results show that the proposed method achieves an accuracy of 77.22\% using only non-invasive data, and it also provides compelling interpretability matrices. This study contributes to the early diagnosis of NAFLD and the intelligent advancement of TCM tongue diagnosis. The project in this paper is available at: https://github.com/cshan-github/SelectorNet.
</details>
<details>
<summary>摘要</summary>
非酒精肝病 (NAFLD) 是一种临床生物学特征的疾病，表现为肝脂肪肿 formation，不含酒精和其他可识别的肝脏损害因素。它在全球范围内成为慢性肝病的主要原因。目前，NAFLD 的检测方法是成本高且不适合用户每天进行检测。为解决这个问题，本研究提出了一种非侵入性的 NAFLD 检测方法，需要用户提供的指标只有性别、年龄、身高、体重、腰围和股围，以及舌头图像。本方法将患者的生理指标与舌头特征合并，然后将其输入到选择器网络（SelectorNet）中。SelectorNet 结合了注意力机制和特征选择机制，可以自动学习选择重要的特征。实验结果表明，提议的方法在只使用非侵入数据时实现了77.22%的准确率，并且还提供了吸引人的解释矩阵。本研究对 NAFLD 的早期诊断和中医舌头诊断的智能进步做出了贡献。项目在以下链接可以获取：https://github.com/cshan-github/SelectorNet。
</details></li>
</ul>
<hr>
<h2 id="Bandwidth-efficient-Inference-for-Neural-Image-Compression"><a href="#Bandwidth-efficient-Inference-for-Neural-Image-Compression" class="headerlink" title="Bandwidth-efficient Inference for Neural Image Compression"></a>Bandwidth-efficient Inference for Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02855">http://arxiv.org/abs/2309.02855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanzhi Yin, Tongda Xu, Yongsheng Liang, Yuanyuan Wang, Yanghao Li, Yan Wang, Jingjing Liu</li>
<li>for: 这篇论文主要针对移动和边缘设备上实现神经网络推断时的对话带宽和电力限制问题。</li>
<li>methods: 本论文提出了一个终端对应、可微分的对话带宽有效神经推断方法，包括使用对应压缩、量化和Entropy coding。具体来说，使用了对应压缩、量化和Entropy coding的变数压缩、量化和Entropy coding的变数压缩、量化和Entropy coding的变数压缩、量化和Entropy coding的变数压缩。</li>
<li>results: 本论文的实验结果显示，可以透过优化现有的模型量化方法，实现19倍的对话带宽减少，同时节省6.21倍的能源。<details>
<summary>Abstract</summary>
With neural networks growing deeper and feature maps growing larger, limited communication bandwidth with external memory (or DRAM) and power constraints become a bottleneck in implementing network inference on mobile and edge devices. In this paper, we propose an end-to-end differentiable bandwidth efficient neural inference method with the activation compressed by neural data compression method. Specifically, we propose a transform-quantization-entropy coding pipeline for activation compression with symmetric exponential Golomb coding and a data-dependent Gaussian entropy model for arithmetic coding. Optimized with existing model quantization methods, low-level task of image compression can achieve up to 19x bandwidth reduction with 6.21x energy saving.
</details>
<details>
<summary>摘要</summary>
随着神经网络的深度和特征地图的大小增加，在移动和边缘设备上进行网络推理时限制了外部内存（或DRAM）的通信带宽和能源限制变成瓶颈。在这篇论文中，我们提出了一种终端到终端可微 differentiable带宽高效的神经推理方法，其中活动被压缩使用神经数据压缩方法。具体来说，我们提出了一个转换-量化-Entropy编码管道，用于活动压缩，并使用对称的加权几何压缩和数据依赖的 Gaussian Entropy 模型进行算术编码。通过与现有模型归一化方法结合优化，可以实现到 19 倍的带宽减少和 6.21 倍的能源浪费 reduction。
</details></li>
</ul>
<hr>
<h2 id="A-flexible-and-accurate-total-variation-and-cascaded-denoisers-based-image-reconstruction-algorithm-for-hyperspectrally-compressed-ultrafast-photography"><a href="#A-flexible-and-accurate-total-variation-and-cascaded-denoisers-based-image-reconstruction-algorithm-for-hyperspectrally-compressed-ultrafast-photography" class="headerlink" title="A flexible and accurate total variation and cascaded denoisers-based image reconstruction algorithm for hyperspectrally compressed ultrafast photography"></a>A flexible and accurate total variation and cascaded denoisers-based image reconstruction algorithm for hyperspectrally compressed ultrafast photography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02835">http://arxiv.org/abs/2309.02835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Guo, Jiali Yao, Dalong Qi, Pengpeng Ding, Chengzhi Jin, Ning Xu, Zhiling Zhang, Yunhua Yao, Lianzhong Deng, Zhiyong Wang, Zhenrong Sun, Shian Zhang</li>
<li>for: 该研究旨在开发一种高级推理图像重建算法，以提高高速激光图像捕捉器（HCUP）中的图像重建质量。</li>
<li>methods: 该算法基于全量变量（TV）和堆式降噪器（CD），通过在多进程替换方法中的迭代执行，以保持图像平滑性而利用深度降噪网络获取更多约束，解决了HCUP中的通用稀热表示问题。</li>
<li>results: 实验和仿真结果表明，提出的TV-CD算法可以有效提高HCUP中图像重建精度和质量，并推动HCUP在捕捉高维复杂物理、化学和生物ultrafast光学场景中的实际应用。<details>
<summary>Abstract</summary>
Hyperspectrally compressed ultrafast photography (HCUP) based on compressed sensing and the time- and spectrum-to-space mappings can simultaneously realize the temporal and spectral imaging of non-repeatable or difficult-to-repeat transient events passively in a single exposure. It possesses an incredibly high frame rate of tens of trillions of frames per second and a sequence depth of several hundred, and plays a revolutionary role in single-shot ultrafast optical imaging. However, due to the ultra-high data compression ratio induced by the extremely large sequence depth as well as the limited fidelities of traditional reconstruction algorithms over the reconstruction process, HCUP suffers from a poor image reconstruction quality and fails to capture fine structures in complex transient scenes. To overcome these restrictions, we propose a flexible image reconstruction algorithm based on the total variation (TV) and cascaded denoisers (CD) for HCUP, named the TV-CD algorithm. It applies the TV denoising model cascaded with several advanced deep learning-based denoising models in the iterative plug-and-play alternating direction method of multipliers framework, which can preserve the image smoothness while utilizing the deep denoising networks to obtain more priori, and thus solving the common sparsity representation problem in local similarity and motion compensation. Both simulation and experimental results show that the proposed TV-CD algorithm can effectively improve the image reconstruction accuracy and quality of HCUP, and further promote the practical applications of HCUP in capturing high-dimensional complex physical, chemical and biological ultrafast optical scenes.
</details>
<details>
<summary>摘要</summary>
高级色彩压缩超快摄影（HCUP）基于压缩感知和时间-спектр空间映射，可同时实现非重复或困难重复的脉冲事件的时间和频谱成像，在单张拍摄下完成。它具有惊人的帧率上亿亿帧/秒和序列深度上百个，扮演了单板超快光学成像革命性角色。然而，由于超高压缩比引起的极大序列深度以及传统重建算法的限制，HCUP受到低图像重建质量和复杂场景中细节损失的限制。为了突破这些限制，我们提出了基于总变量（TV）和堆叠滤波器（CD）的灵活图像重建算法，称为TV-CD算法。它在融合了多个高级深度学习基础模型的迭代方向幂法框架中，应用TV滤波器先后堆叠多个高级深度学习基础模型，保持图像的平滑性，同时利用深度滤波器获取更多的前提，解决了常见的简并表示问题。实验和 simulate 结果表明，提议的 TV-CD 算法可以有效提高 HCUP 图像重建质量和精度，并且推动 HCUP 在高维复物理、化学和生物ultrafast光学场景中的应用。
</details></li>
</ul>
<hr>
<h2 id="Improving-Image-Classification-of-Knee-Radiographs-An-Automated-Image-Labeling-Approach"><a href="#Improving-Image-Classification-of-Knee-Radiographs-An-Automated-Image-Labeling-Approach" class="headerlink" title="Improving Image Classification of Knee Radiographs: An Automated Image Labeling Approach"></a>Improving Image Classification of Knee Radiographs: An Automated Image Labeling Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02681">http://arxiv.org/abs/2309.02681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jikai Zhang, Carlos Santos, Christine Park, Maciej Mazurowski, Roy Colglazier<br>for: 这个研究的目的是为了提高骨科X线成像诊断的图像分类模型，以便更好地识别正常骨科X线图像和异常骨科X线图像或过去的骨 JOINT 代替手术。methods: 这个研究使用了自动标签方法，以将大量未标注的骨科X线图像标签为正常或异常。这个自动标签方法首先在小型标注数据集上训练，然后将其扩展到更大的未标注数据集上。results: 这个研究发现，使用自动标签方法可以将骨科X线图像分类模型的性能提高，特别是在正常骨科X线图像和异常骨科X线图像之间的区别。在7,382名病人的数据集上训练和637名病人的验证数据集上验证，这个图像分类模型的性能在不同的类别中都有所提高。<details>
<summary>Abstract</summary>
Large numbers of radiographic images are available in knee radiology practices which could be used for training of deep learning models for diagnosis of knee abnormalities. However, those images do not typically contain readily available labels due to limitations of human annotations. The purpose of our study was to develop an automated labeling approach that improves the image classification model to distinguish normal knee images from those with abnormalities or prior arthroplasty. The automated labeler was trained on a small set of labeled data to automatically label a much larger set of unlabeled data, further improving the image classification performance for knee radiographic diagnosis. We developed our approach using 7,382 patients and validated it on a separate set of 637 patients. The final image classification model, trained using both manually labeled and pseudo-labeled data, had the higher weighted average AUC (WAUC: 0.903) value and higher AUC-ROC values among all classes (normal AUC-ROC: 0.894; abnormal AUC-ROC: 0.896, arthroplasty AUC-ROC: 0.990) compared to the baseline model (WAUC=0.857; normal AUC-ROC: 0.842; abnormal AUC-ROC: 0.848, arthroplasty AUC-ROC: 0.987), trained using only manually labeled data. DeLong tests show that the improvement is significant on normal (p-value<0.002) and abnormal (p-value<0.001) images. Our findings demonstrated that the proposed automated labeling approach significantly improves the performance of image classification for radiographic knee diagnosis, allowing for facilitating patient care and curation of large knee datasets.
</details>
<details>
<summary>摘要</summary>
大量的骨灰像可以在膝关节 radiology 实践中使用，以便用于深度学习模型的膝关节疾病诊断。然而，这些图像通常没有Ready available的标签，因为人类标注的限制。我们的研究旨在开发一种自动标注方法，以提高膝关节图像分类模型的精度。我们使用了7,382名患者和637名患者的分 sepate set来验证我们的方法。我们的方法使得图像分类模型可以更好地分辨正常膝关节图像和疾病图像或者过去arthroplasty。我们使用了 manually labeled和pseudo-labeled数据来训练我们的图像分类模型，其中weighted average AUC（WAUC）值为0.903，而且在所有类型的AUC-ROC值中都高于基线模型（WAUC=0.857）。DeLong测试表明，在正常（p-value<0.002）和疾病（p-value<0.001）图像上，我们的提高是 statistically significant。我们的发现表明，我们提posed的自动标注方法可以显著提高膝关节图像分类的性能，使得patient care和大量膝关节数据的护理成为可能。
</details></li>
</ul>
<hr>
<h2 id="Progressive-Attention-Guidance-for-Whole-Slide-Vulvovaginal-Candidiasis-Screening"><a href="#Progressive-Attention-Guidance-for-Whole-Slide-Vulvovaginal-Candidiasis-Screening" class="headerlink" title="Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening"></a>Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02670">http://arxiv.org/abs/2309.02670</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cjdbehumble/miccai2023-vvc-screening">https://github.com/cjdbehumble/miccai2023-vvc-screening</a></li>
<li>paper_authors: Jiangdong Cai, Honglin Xiong, Maosong Cao, Luyan Liu, Lichi Zhang, Qian Wang</li>
<li>for: 这项研究旨在开发一种自动检测和识别绒毛菌（VVC）感染的整片扫描图像（WSI）分类方法，以帮助缓解绒毛菌感染的巨大危机和防控措施。</li>
<li>methods: 这种方法首先使用一个预训练的检测模型作为先前的指导，然后设计了跳过自我注意模块来细化绒毛菌的注意力。最后，使用一种对比学习方法来缓解由扫描图像的风格差引起的过拟合，并降低注意力的扩散到假阳性区域。</li>
<li>results: 实验结果表明，我们的框架可以达到状态级表现，并且在绒毛菌感染检测中具有较高的准确率和特异性。代码和示例数据可以在<a target="_blank" rel="noopener" href="https://github.com/cjdbehumble/MICCAI2023-VVC-Screening%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/cjdbehumble/MICCAI2023-VVC-Screening上下载。</a><details>
<summary>Abstract</summary>
Vulvovaginal candidiasis (VVC) is the most prevalent human candidal infection, estimated to afflict approximately 75% of all women at least once in their lifetime. It will lead to several symptoms including pruritus, vaginal soreness, and so on. Automatic whole slide image (WSI) classification is highly demanded, for the huge burden of disease control and prevention. However, the WSI-based computer-aided VCC screening method is still vacant due to the scarce labeled data and unique properties of candida. Candida in WSI is challenging to be captured by conventional classification models due to its distinctive elongated shape, the small proportion of their spatial distribution, and the style gap from WSIs. To make the model focus on the candida easier, we propose an attention-guided method, which can obtain a robust diagnosis classification model. Specifically, we first use a pre-trained detection model as prior instruction to initialize the classification model. Then we design a Skip Self-Attention module to refine the attention onto the fined-grained features of candida. Finally, we use a contrastive learning method to alleviate the overfitting caused by the style gap of WSIs and suppress the attention to false positive regions. Our experimental results demonstrate that our framework achieves state-of-the-art performance. Code and example data are available at https://github.com/cjdbehumble/MICCAI2023-VVC-Screening.
</details>
<details>
<summary>摘要</summary>
“ vulvovaginal candidiasis (VVC) 是人类最常见的发炎菌感染，估计会影响约75%的所有女性至少一次生活中。 它会导致多种症状，包括痒痛、阴道疼痛等。 automatic whole slide image (WSI) 分类是很受需求的，因为疾病控制和预防的负担非常大。 但是， WSI 基于的计算机支持 VCC 检测方法仍然无人，因为罕有 Labelled 数据和发炎菌的特有性。 发炎菌在 WSI 中具有特殊的长条形和小比例的问题，以及 WSIs 的样本距离。 为了让模型更容易捕捉发炎菌，我们提出了注意力导向的方法。 我们首先使用预训的检测模型作为专案初始化的参考模型。 然后，我们设计了 Skip Self-Attention 模块，以精确地调整注意力到发炎菌的细节特征。 最后，我们使用了对抗学习方法，以解决由 WSIs 的样本距离导致的过滤过问题。 我们的实验结果显示，我们的框架可以实现州际表现。 Code 和示例数据可以在 https://github.com/cjdbehumble/MICCAI2023-VVC-Screening 上获取。”
</details></li>
</ul>
<hr>
<h2 id="Review-of-photoacoustic-imaging-plus-X"><a href="#Review-of-photoacoustic-imaging-plus-X" class="headerlink" title="Review of photoacoustic imaging plus X"></a>Review of photoacoustic imaging plus X</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02638">http://arxiv.org/abs/2309.02638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daohuai Jiang, Luyao Zhu, Shangqing Tong, Yuting Shen, Feng Gao, Fei Gao</li>
<li>for: 本文提供了一种新的医疗技术综述，即 photoacoustic imaging（PAI） Plus X，其中X表示其他多种先进技术，包括但不限于PAI Plus treatment、PAI Plus 新电路设计、PAI Plus 精准位别系统、PAI Plus 快速扫描系统、PAI Plus 新式 ultrasound 探测器、PAI Plus 高级 Laser 源、PAI Plus 深度学习、PAI Plus 其他成像模式。</li>
<li>methods: 本文详细介绍了 PAI Plus X 技术的当前状况、技术优势和应用前景，报道了近三年来的研究进展。</li>
<li>results: 本文综述了 PAI Plus X 技术的挑战和未来发展前景。<details>
<summary>Abstract</summary>
Photoacoustic imaging (PAI) is a novel modality in biomedical imaging technology that combines the rich optical contrast with the deep penetration of ultrasound. To date, PAI technology has found applications in various biomedical fields. In this review, we present an overview of the emerging research frontiers on PAI plus other advanced technologies, named as PAI plus X, which includes but not limited to PAI plus treatment, PAI plus new circuits design, PAI plus accurate positioning system, PAI plus fast scanning systems, PAI plus novel ultrasound sensors, PAI plus advanced laser sources, PAI plus deep learning, and PAI plus other imaging modalities. We will discuss each technology's current state, technical advantages, and prospects for application, reported mostly in recent three years. Lastly, we discuss and summarize the challenges and potential future work in PAI plus X area.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/06/eess.IV_2023_09_06/" data-id="clmjn91qw00hn0j881td2buuf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.SD_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T15:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/cs.SD_2023_09_05/">cs.SD - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Music-Source-Separation-with-Band-Split-RoPE-Transformer"><a href="#Music-Source-Separation-with-Band-Split-RoPE-Transformer" class="headerlink" title="Music Source Separation with Band-Split RoPE Transformer"></a>Music Source Separation with Band-Split RoPE Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02612">http://arxiv.org/abs/2309.02612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Tsung Lu, Ju-Chiang Wang, Qiuqiang Kong, Yun-Ning Hung</li>
<li>for: 这个研究的目的是提出一种频域方法来实现音乐源分离（MSS），以分离音乐录音为多个音乐元素，如 vocals、bass、鼓等。</li>
<li>methods: 该方法基于一种带划分模块，将输入复杂spectrogram projected into subband-level表示，然后用一个堆栈的层次Transformer来模型内部band和间隔band序列 для多个band掩码估计。</li>
<li>results: 在MUSDB18HQ和500首额外歌曲上训练BS-RoFormer系统后，在Sound Demixing Challenge（SDX23）的MSS轨道上 ranked the first place。使用一个较小的BS-RoFormer版本在MUSDB18HQ上进行训练，可以达到state-of-the-art result，无需额外的训练数据，平均SDR为9.80 dB。<details>
<summary>Abstract</summary>
Music source separation (MSS) aims to separate a music recording into multiple musically distinct stems, such as vocals, bass, drums, and more. Recently, deep learning approaches such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been used, but the improvement is still limited. In this paper, we propose a novel frequency-domain approach based on a Band-Split RoPE Transformer (called BS-RoFormer). BS-RoFormer relies on a band-split module to project the input complex spectrogram into subband-level representations, and then arranges a stack of hierarchical Transformers to model the inner-band as well as inter-band sequences for multi-band mask estimation. To facilitate training the model for MSS, we propose to use the Rotary Position Embedding (RoPE). The BS-RoFormer system trained on MUSDB18HQ and 500 extra songs ranked the first place in the MSS track of Sound Demixing Challenge (SDX23). Benchmarking a smaller version of BS-RoFormer on MUSDB18HQ, we achieve state-of-the-art result without extra training data, with 9.80 dB of average SDR.
</details>
<details>
<summary>摘要</summary>
音乐源分离（MSS）目标是将音乐录音分解成多个音乐上的独立元素，如 vocals、bass、鼓等。近年来，深度学习方法如卷积神经网络（CNN）和循环神经网络（RNN）已经被应用，但是改进的空间还是有限。在这篇论文中，我们提出了一种新的频域方法，基于Band-Split RoPE Transformer（BS-RoFormer）。BS-RoFormer使用带分模块将输入复杂spectrogramProjected into subband-level表示，然后排列一个堆栈的层次Transformer来模型内部band以及交叉band的序列 для多个混合预测。为了训练MSS模型，我们提出了Rotary Position Embedding（RoPE）。BS-RoFormer系统在MUSDB18HQ和500首额外歌曲上训练后，在Sound Demixing Challenge（SDX23）中的MSS轨道上排名第一。对MUSDB18HQ进行训练BS-RoFormer系统的一个较小版本，我们实现了无需额外训练数据的状态级Result，即9.80 dB的平均SDR。
</details></li>
</ul>
<hr>
<h2 id="BWSNet-Automatic-Perceptual-Assessment-of-Audio-Signals"><a href="#BWSNet-Automatic-Perceptual-Assessment-of-Audio-Signals" class="headerlink" title="BWSNet: Automatic Perceptual Assessment of Audio Signals"></a>BWSNet: Automatic Perceptual Assessment of Audio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02592">http://arxiv.org/abs/2309.02592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clément Le Moine Veillon, Victor Rosi, Pablo Arias Sarah, Léane Salais, Nicolas Obin</li>
<li>for: 这篇论文是用于提出一种基于Best-Worst scaling（BWS）实验获取人类评价的模型。</li>
<li>methods: 这种模型使用了一组成本函数和约束，将声音样本映射到表示 изучаем特性的嵌入空间中。</li>
<li>results: 对两个BWS实验数据集进行测试后，结果表明模型的嵌入空间结构与人类评价具有一致性。<details>
<summary>Abstract</summary>
This paper introduces BWSNet, a model that can be trained from raw human judgements obtained through a Best-Worst scaling (BWS) experiment. It maps sound samples into an embedded space that represents the perception of a studied attribute. To this end, we propose a set of cost functions and constraints, interpreting trial-wise ordinal relations as distance comparisons in a metric learning task. We tested our proposal on data from two BWS studies investigating the perception of speech social attitudes and timbral qualities. For both datasets, our results show that the structure of the latent space is faithful to human judgements.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文介绍了BWSNet模型，该模型可以通过简单的人类评价获得的Best-Worst排序（BWS）实验来训练。它将声音样本映射到一个表示识别的特性的嵌入空间中。为此，我们提议了一组成本函数和约束，将试验性评价转化为度量学习任务中的距离比较。我们在两个BWS研究中测试了我们的建议，分别研究了社会态度和材质质量的声音识别。对于两个数据集，我们的结果显示，模型中的嵌入空间结构与人类评价具有一定的相似性。
</details></li>
</ul>
<hr>
<h2 id="Symbolic-Music-Representations-for-Classification-Tasks-A-Systematic-Evaluation"><a href="#Symbolic-Music-Representations-for-Classification-Tasks-A-Systematic-Evaluation" class="headerlink" title="Symbolic Music Representations for Classification Tasks: A Systematic Evaluation"></a>Symbolic Music Representations for Classification Tasks: A Systematic Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02567">http://arxiv.org/abs/2309.02567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Zhang, Emmanouil Karystinaios, Simon Dixon, Gerhard Widmer, Carlos Eduardo Cancino-Chacón</li>
<li>for: 本研究目的是对深度学习方法在音乐信息检索（MIR）领域的应用进行系统性的检查和评估。</li>
<li>methods: 本研究使用了矩阵表示（piano roll）、序列表示和图表示等不同的符号音乐表示方法，并与符号谱和表演进行比较。同时，我们还提出了一种新的图表示方法，用于表示符号性能。</li>
<li>results: 我们的系统性评估表明，图表示方法在全球分类任务中表现出色，并且训练时间较短。同时，矩阵表示方法在某些任务中表现较好，而序列表示方法则在其他任务中表现较差。<details>
<summary>Abstract</summary>
Music Information Retrieval (MIR) has seen a recent surge in deep learning-based approaches, which often involve encoding symbolic music (i.e., music represented in terms of discrete note events) in an image-like or language like fashion. However, symbolic music is neither an image nor a sentence, and research in the symbolic domain lacks a comprehensive overview of the different available representations. In this paper, we investigate matrix (piano roll), sequence, and graph representations and their corresponding neural architectures, in combination with symbolic scores and performances on three piece-level classification tasks. We also introduce a novel graph representation for symbolic performances and explore the capability of graph representations in global classification tasks. Our systematic evaluation shows advantages and limitations of each input representation. Our results suggest that the graph representation, as the newest and least explored among the three approaches, exhibits promising performance, while being more light-weight in training.
</details>
<details>
<summary>摘要</summary>
音乐信息检索（MIR）最近几年受到深度学习方法的普遍应用，这些方法常常将 симвоlic music（即用字符表示的音乐）转化为图像或语言的形式。然而，symbolic music并不是图像也不是句子，学术研究在这个域的不同表示方式缺乏全面的概述。本文 investigate matrix（钢琴 Roll）、序列和图表表示方法和其相应的神经网络架构，并与symbolic scores和表演中的三个 Piece-level 分类任务进行系统性的评估。我们还介绍了一种新的图表表示方法 для符号性表演，并对全球分类任务中的图表表示的可能性进行了探索。我们的系统性评估显示每种输入表示方法的优势和局限性。结果表明，图表表示方法，作为最新和最少研究的一种方法，具有潜在的表现优势，同时具有轻量级的训练需求。
</details></li>
</ul>
<hr>
<h2 id="Employing-Real-Training-Data-for-Deep-Noise-Suppression"><a href="#Employing-Real-Training-Data-for-Deep-Noise-Suppression" class="headerlink" title="Employing Real Training Data for Deep Noise Suppression"></a>Employing Real Training Data for Deep Noise Suppression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02432">http://arxiv.org/abs/2309.02432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Xu, Marvin Sach, Jan Pirklbauer, Tim Fingscheidt</li>
<li>for: 提高深度噪音减除（DNS）模型的训练，使其能够更好地适应实际应用环境中的噪音。</li>
<li>methods: 使用实际训练数据，而不是传统的参照基于损失函数。在这种情况下，我们提出了一种基于非侵入式深度神经网络（DNN）的方法，named PESQ-DNN，来估算噪音下的语音质量评价（PESQ）分数。</li>
<li>results: 与参照方法相比，使用实际训练数据和PESQ-DNN的方法在DNS训练中表现出色，在synthetic test data上的PESQ分数提高0.32分，在实际测试数据上也超过了基线值0.05 DNSMOS分。<details>
<summary>Abstract</summary>
Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.
</details>
<details>
<summary>摘要</summary>
多个深度噪音消除（DNS）模型通常通过参考基于的损失来训练，但在实际应用中，加法麦克风模型可能不够。因此，使用实际训练数据来训练DNS模型可以减少训练/推理匹配问题。使用实际数据进行DNS训练需要使用生成方法或无参考损失。在这种工作中，我们提议使用一个端到端非侵入性的深度神经网络（DNN），名为PESQ-DNN，来估算受损数据的语音质量评分（PESQ）分数。PESQ-DNN提供了一种无参考的语音质量损失，可以使用实际数据进行DNS训练，最大化PESQ分数。此外，我们使用一种每个粒子 alternate 训练协议，首先更新DNS模型使用实际数据，然后PESQ-DNN使用生成数据进行更新。使用PESQ-DNN进行训练的DNS模型超越了所有参考方法，使用 толькоSynthetic 训练数据。在Synthetic 测试数据上，我们的提议方法与Interspeech 2021 DNS Challenge 基准值差异为0.32 PESQ分数。在实际测试数据上，我们的方法也超越了基准值，差异为0.05 DNSMOS分数。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Adaptation-with-Pre-trained-Speech-Encoders-for-Continuous-Emotion-Recognition"><a href="#Personalized-Adaptation-with-Pre-trained-Speech-Encoders-for-Continuous-Emotion-Recognition" class="headerlink" title="Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition"></a>Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02418">http://arxiv.org/abs/2309.02418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh Tran, Yufeng Yin, Mohammad Soleymani</li>
<li>for: 实现无监督化人性化情感识别</li>
<li>methods: 使用自类监督学习方法，首先预训一个具有可读取 speaker embedding 的 encoder，然后提出一种无监督方法来补偿标签分布差异</li>
<li>results: 实验结果显示，我们的方法可以与强己基eline比较，并实现情感估计的州监督性能Here’s a brief summary of the paper in English:The paper aims to achieve unsupervised personalized emotion recognition. To do this, the authors first pre-train an encoder with learnable speaker embeddings in a self-supervised manner to learn robust speech representations conditioned on speakers. Then, they propose an unsupervised method to compensate for label distribution shifts by finding similar speakers and leveraging their label distributions from the training set. Experimental results on the MSP-Podcast corpus show that their method consistently outperforms strong personalization baselines and achieves state-of-the-art performance for valence estimation.<details>
<summary>Abstract</summary>
There are individual differences in expressive behaviors driven by cultural norms and personality. This between-person variation can result in reduced emotion recognition performance. Therefore, personalization is an important step in improving the generalization and robustness of speech emotion recognition. In this paper, to achieve unsupervised personalized emotion recognition, we first pre-train an encoder with learnable speaker embeddings in a self-supervised manner to learn robust speech representations conditioned on speakers. Second, we propose an unsupervised method to compensate for the label distribution shifts by finding similar speakers and leveraging their label distributions from the training set. Extensive experimental results on the MSP-Podcast corpus indicate that our method consistently outperforms strong personalization baselines and achieves state-of-the-art performance for valence estimation.
</details>
<details>
<summary>摘要</summary>
人们之间存在文化规范和个性的表达行为差异，这些差异可能导致情绪识别性能下降。因此，个性化是提高情绪识别系统通用性和鲁棒性的重要步骤。在这篇论文中，我们首先在不监督的情况下预训一个编码器，使其学习不同说话者的Robust语音表示形式。其次，我们提出了一种不监督的方法，通过找到类似的说话者并利用它们在训练集中的标签分布来补偿标签分布的变化。我们对MSP-Podcast集合进行了广泛的实验，结果表明，我们的方法可以一直超过强个性化基eline和实现情绪识别的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Generating-Realistic-Images-from-In-the-wild-Sounds"><a href="#Generating-Realistic-Images-from-In-the-wild-Sounds" class="headerlink" title="Generating Realistic Images from In-the-wild Sounds"></a>Generating Realistic Images from In-the-wild Sounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02405">http://arxiv.org/abs/2309.02405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/etilelab/Generating-Realistic-Images-from-In-the-wild-Sounds">https://github.com/etilelab/Generating-Realistic-Images-from-In-the-wild-Sounds</a></li>
<li>paper_authors: Taegyeong Lee, Jeonghun Kang, Hyeonyu Kim, Taehwan Kim</li>
<li>for: 这篇论文的目的是生成自然环境中的声音图像。</li>
<li>methods: 这篇论文使用了音频描述、音频注意力和句子注意力来表示声音的丰富特征，并使用了CLIP分数和AudioCLIP来直接优化声音。</li>
<li>results: 实验表明，这种方法可以生成高质量的声音图像，并在野外音频Dataset上超过基eline的 both quantitative和qualitative评估。Here’s the full translation in Traditional Chinese:</li>
<li>for: 这篇论文的目的是生成自然环境中的声音图像。</li>
<li>methods: 这篇论文使用了音频描述、音频注意力和句子注意力来表示声音的丰富特征，并使用了CLIP分数和AudioCLIP来直接优化声音。</li>
<li>results: 实验表明，这种方法可以生成高质量的声音图像，并在野外音频Dataset上超过基eline的 both quantitative和qualitative评估。<details>
<summary>Abstract</summary>
Representing wild sounds as images is an important but challenging task due to the lack of paired datasets between sound and images and the significant differences in the characteristics of these two modalities. Previous studies have focused on generating images from sound in limited categories or music. In this paper, we propose a novel approach to generate images from in-the-wild sounds. First, we convert sound into text using audio captioning. Second, we propose audio attention and sentence attention to represent the rich characteristics of sound and visualize the sound. Lastly, we propose a direct sound optimization with CLIPscore and AudioCLIP and generate images with a diffusion-based model. In experiments, it shows that our model is able to generate high quality images from wild sounds and outperforms baselines in both quantitative and qualitative evaluations on wild audio datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本为简化中文。</SYS>>寻求将野性声音转换为图像是一项重要但具有挑战性的任务，主要是因为声音和图像之间没有匹配数据集和这两种模式之间存在显著的不同。先前的研究主要集中在限定类别或音乐中生成图像。在这篇论文中，我们提出了一种将野性声音转换为图像的新方法。首先，我们使用音频描述将声音转换为文本。然后，我们提出了音频注意力和句子注意力来表达声音的丰富特征和视觉化声音。最后，我们提出了直接声音优化CLIPscore和AudioCLIP，并使用扩散模型生成图像。在实验中，我们发现我们的模型能够从野性声音中生成高质量图像，并在野性音频数据集上超过基eline在量和质量评价中。
</details></li>
</ul>
<hr>
<h2 id="Voice-Morphing-Two-Identities-in-One-Voice"><a href="#Voice-Morphing-Two-Identities-in-One-Voice" class="headerlink" title="Voice Morphing: Two Identities in One Voice"></a>Voice Morphing: Two Identities in One Voice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02404">http://arxiv.org/abs/2309.02404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Sushanta K. Pani, Anurag Chowdhury, Morgan Sandler, Arun Ross</li>
<li>for: 本研究旨在探讨语音识别系统中的 morph 攻击，并提出了一种基于语音特征的 morph 攻击方法，即 Voice Identity Morphing (VIM)。</li>
<li>methods: 本研究使用了 ECAPA-TDNN 和 x-vector 两种常见的 speaker recognition 系统，并通过对 Librispeech 数据集进行实验来评估这两种系统对 VIM 攻击的抵抗力。</li>
<li>results: 实验结果显示，ECAPA-TDNN 和 x-vector 两种系统都具有较高的抵抗力，但是 VIM 攻击还是能够在这两种系统中获得较高的成功率（MMPMR 超过 80%），而且 false match rate 仅为 1%。<details>
<summary>Abstract</summary>
In a biometric system, each biometric sample or template is typically associated with a single identity. However, recent research has demonstrated the possibility of generating "morph" biometric samples that can successfully match more than a single identity. Morph attacks are now recognized as a potential security threat to biometric systems. However, most morph attacks have been studied on biometric modalities operating in the image domain, such as face, fingerprint, and iris. In this preliminary work, we introduce Voice Identity Morphing (VIM) - a voice-based morph attack that can synthesize speech samples that impersonate the voice characteristics of a pair of individuals. Our experiments evaluate the vulnerabilities of two popular speaker recognition systems, ECAPA-TDNN and x-vector, to VIM, with a success rate (MMPMR) of over 80% at a false match rate of 1% on the Librispeech dataset.
</details>
<details>
<summary>摘要</summary>
在生物特征识别系统中，每个生物样本或模板通常与单一身份相关联。然而，最近的研究表明可以生成“变形”生物样本，可以成功匹配多个身份。这种“变形攻击”现在被认为是生物特征识别系统的安全性威胁。然而，大多数变形攻击都在生物特征Modalities的图像领域进行研究，如脸、指纹和肉眼。在这项初步工作中，我们介绍了语音身份变换（VIM） - 一种基于语音的变形攻击，可以合成speech样本，模拟两个人的语音特征。我们的实验评估了两种流行的说话识别系统，ECAPA-TDNN和x-vector，对VIM的抵触性，在Librispeech数据集上达到了80%的成功率，false match率为1%。
</details></li>
</ul>
<hr>
<h2 id="The-Batik-plays-Mozart-Corpus-Linking-Performance-to-Score-to-Musicological-Annotations"><a href="#The-Batik-plays-Mozart-Corpus-Linking-Performance-to-Score-to-Musicological-Annotations" class="headerlink" title="The Batik-plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations"></a>The Batik-plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02399">http://arxiv.org/abs/2309.02399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patricia Hu, Gerhard Widmer</li>
<li>for: 这个论文旨在创建一个高质量、高精度的钢琴演奏数据集，结合专业钢琴演奏家罗兰多的莫扎特钢琴奏鸣录音和专家标注的Sheet Music。</li>
<li>methods: 这个论文使用了精度准确的注释对照方法，将演奏录音和Sheet Music进行了精度匹配，并将音乐学研究注释（和声、满声、段落）与演奏录音进行了连接。</li>
<li>results: 这个研究创建了一个高质量、高精度的钢琴演奏数据集，可以用于研究表达性演奏和其与结构特征之间的关系。两项探索性实验表明，这个数据集可以用于分析表达性演奏的多种方面。<details>
<summary>Abstract</summary>
We present the Batik-plays-Mozart Corpus, a piano performance dataset combining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored B\"osendorfer grand piano, and are available both as MIDI files and audio recordings. They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can further be connected to the musicological annotations (harmony, cadences, phrases) on these scores that were recently published by Hentschel et al. (2021).   The result is a high-quality, high-precision corpus mapping scores and musical structure annotations to precise note-level professional performance information. As the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects. In the paper, we outline the curation process of the alignment and conduct two exploratory experiments to demonstrate its usefulness in analyzing expressive performance.
</details>
<details>
<summary>摘要</summary>
我们现在提出了《巴提克扮演莫扎特资料集》，这是一个结合专业莫扎特钢琴室内乐演奏和专家标注的谱面级演奏数据集。这些演奏来自维也纳钢琴家罗兰·巴提克在计算机监测的博Senderfer大钢琴上进行的录音，并可以作为MIDI文件和音频录音形式获得。它们已经精准地对应了当今标准版莫扎特谱面（新莫扎特版），以便进一步与musicology注释（和声、结束、段落）相连接。结果是一个高质量、高精度的谱面和音乐结构注释映射到专业演奏信息的高级资料集。这是第一个类型的资料集，可以作为研究表达性演奏和结构方面的多种方面的 valuable resource。在论文中，我们介绍了对Alignment的筛选过程和进行了两项探索性实验，以示其在分析表达性演奏方面的用于。
</details></li>
</ul>
<hr>
<h2 id="PESTO-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective"><a href="#PESTO-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective" class="headerlink" title="PESTO: Pitch Estimation with Self-supervised Transposition-equivariant Objective"></a>PESTO: Pitch Estimation with Self-supervised Transposition-equivariant Objective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02265">http://arxiv.org/abs/2309.02265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alain Riou, Stefan Lattner, Gaëtan Hadjeres, Geoffroy Peeters</li>
<li>for: 本研究使用自主学习（SSL）方法解决抗频率估计问题。</li>
<li>methods: 我们使用具有满足抗频率转换协变性的小型 ($&lt;$ 30k参数) SIAMESE神经网络，该网络使用两个不同的抗频率版本的同一个音频（通过Constant-Q Transform表示）作为输入。我们提议一种新的类型-基于的转换平衡对象来避免encoder-only设置中的模型塌缩。此外，我们设计了网络的架构，使其具有转换保持性，通过引入学习的Toeplitz矩阵。</li>
<li>results: 我们对唱歌声和乐器抗频率估计两个任务进行评估，并显示我们的模型能够在任务和数据集之间进行泛化，同时具有轻量级和实时应用compatibility。具体来说，我们的结果超过了自主学习基eline，并将自主学习和指导学习之间的性能差降到最小。<details>
<summary>Abstract</summary>
In this paper, we address the problem of pitch estimation using Self Supervised Learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being trained only on a small unlabeled dataset. We use a lightweight ($<$ 30k parameters) Siamese neural network that takes as inputs two different pitch-shifted versions of the same audio represented by its Constant-Q Transform. To prevent the model from collapsing in an encoder-only setting, we propose a novel class-based transposition-equivariant objective which captures pitch information. Furthermore, we design the architecture of our network to be transposition-preserving by introducing learnable Toeplitz matrices.   We evaluate our model for the two tasks of singing voice and musical instrument pitch estimation and show that our model is able to generalize across tasks and datasets while being lightweight, hence remaining compatible with low-resource devices and suitable for real-time applications. In particular, our results surpass self-supervised baselines and narrow the performance gap between self-supervised and supervised methods for pitch estimation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 Addresses the problem of pitch estimation using Self Supervised Learning (SSL). 我们使用的 SSL 模式是声调转换对应性，这使得我们的模型可以在听到的一小量无标签数据上训练后，准确地进行声调估计于单声音音频。我们使用一个轻量级 ($<$ 30k 参数) 的 Siamese 神经网络，它接受两个不同声调版本的同一个音频，用 Constant-Q Transform 表示。为了避免encoder-only设置中的模型崩溃，我们提出了一种新的类型-based transposition-equivariant 目标，捕捉声调信息。此外，我们设计了我们的网络架构，使其保持声调转换对应性，通过引入学习 Toeplitz 矩阵。  我们评估我们的模型在两个任务中： singing voice 和 musical instrument 声调估计，并示出我们的模型可以适应任务和数据集，同时具有轻量级和实时应用 compatibles。具体来说，我们的结果超过了自我监督基elines，并将自我监督和监督方法之间的性能差退到最小。
</details></li>
</ul>
<hr>
<h2 id="FSD-An-Initial-Chinese-Dataset-for-Fake-Song-Detection"><a href="#FSD-An-Initial-Chinese-Dataset-for-Fake-Song-Detection" class="headerlink" title="FSD: An Initial Chinese Dataset for Fake Song Detection"></a>FSD: An Initial Chinese Dataset for Fake Song Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02232">http://arxiv.org/abs/2309.02232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xieyuankun/fsd-dataset">https://github.com/xieyuankun/fsd-dataset</a></li>
<li>paper_authors: Yuankun Xie, Jingjing Zhou, Xiaolin Lu, Zhenghao Jiang, Yuxin Yang, Haonan Cheng, Long Ye</li>
<li>for: 本研究旨在探讨歌曲深刻护照技术的应用和挑战。</li>
<li>methods: 我们首先构建了一个中文假歌曲检测（FSD）数据集，以便研究歌曲深刻护照领域的特点和挑战。然后，我们使用FSD数据集训练Audio DeepFake Detection（ADD）模型，并对其进行了两种enario的评估：一种是使用原始歌曲，另一种是使用分离的声音轨迹。</li>
<li>results: 我们的实验结果表明，使用歌曲训练的ADD模型与使用语音训练的ADD模型相比，在FSD测试集上的平均相同错误率下降了38.58%。<details>
<summary>Abstract</summary>
Singing voice synthesis and singing voice conversion have significantly advanced, revolutionizing musical experiences. However, the rise of "Deepfake Songs" generated by these technologies raises concerns about authenticity. Unlike Audio DeepFake Detection (ADD), the field of song deepfake detection lacks specialized datasets or methods for song authenticity verification. In this paper, we initially construct a Chinese Fake Song Detection (FSD) dataset to investigate the field of song deepfake detection. The fake songs in the FSD dataset are generated by five state-of-the-art singing voice synthesis and singing voice conversion methods. Our initial experiments on FSD revealed the ineffectiveness of existing speech-trained ADD models for the task of song deepFake detection. Thus, we employ the FSD dataset for the training of ADD models. We subsequently evaluate these models under two scenarios: one with the original songs and another with separated vocal tracks. Experiment results show that song-trained ADD models exhibit a 38.58% reduction in average equal error rate compared to speech-trained ADD models on the FSD test set.
</details>
<details>
<summary>摘要</summary>
嗓音合成和嗓音转化技术已经有了很大的进步，对音乐经验产生了革命性的变革。然而，“深伪歌曲”（Deepfake Song）的出现使得真实性的问题引起了关注。与音频深伪检测（ADD）不同的是，歌曲真实性检测领域没有专门的数据集或方法进行真实性验证。在这篇论文中，我们首先构建了中文伪歌曲检测（FSD）数据集，以调查歌曲深伪检测领域的情况。 fake songs在FSD数据集中是由五种当前最佳的嗓音合成和嗓音转化方法生成的。我们对FSD数据集进行了初步的实验，发现现有的音频深伪检测模型对歌曲深伪检测task不具有效果。因此，我们使用FSD数据集来训练ADD模型。我们随后对这些模型进行了两种情况的评估：一种是使用原始的歌曲，另一种是使用分离的嗓音轨道。实验结果表明，使用歌曲训练的ADD模型在FSD测试集上的平均错误率比使用音频训练的ADD模型下降38.58%。
</details></li>
</ul>
<hr>
<h2 id="Bring-the-Noise-Introducing-Noise-Robustness-to-Pretrained-Automatic-Speech-Recognition"><a href="#Bring-the-Noise-Introducing-Noise-Robustness-to-Pretrained-Automatic-Speech-Recognition" class="headerlink" title="Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition"></a>Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02145">http://arxiv.org/abs/2309.02145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Eickhoff, Matthias Möller, Theresa Pekarek Rosin, Johannes Twiefel, Stefan Wermter</li>
<li>for: 提高 Automatic Speech Recognition (ASR) 系统在听风讲语音中的表现，特别是在听风讲语音中受到干扰的情况下。</li>
<li>methods: 提出了一种新的方法，可以将 End-to-End (E2E) 系统中的干扰除能力提取出来，并将其用于任何 encoder-decoder 架构。这种方法包括提取 Conformer ASR 模型中的隐藏动作，并将其传递给一个解码器来预测干扰除后的spectrogram。</li>
<li>results: 研究表明，使用 Cleancoder 预处理器可以有效地除去听风讲语音中的干扰，并且可以提高 downstream 模型在听风讲语音中的总 Word Error Rate (WER)。此外，研究还表明，使用 Cleancoder 预处理器可以训练更小的 Conformer ASR 模型从 scratch。<details>
<summary>Abstract</summary>
In recent research, in the domain of speech processing, large End-to-End (E2E) systems for Automatic Speech Recognition (ASR) have reported state-of-the-art performance on various benchmarks. These systems intrinsically learn how to handle and remove noise conditions from speech. Previous research has shown, that it is possible to extract the denoising capabilities of these models into a preprocessor network, which can be used as a frontend for downstream ASR models. However, the proposed methods were limited to specific fully convolutional architectures. In this work, we propose a novel method to extract the denoising capabilities, that can be applied to any encoder-decoder architecture. We propose the Cleancoder preprocessor architecture that extracts hidden activations from the Conformer ASR model and feeds them to a decoder to predict denoised spectrograms. We train our pre-processor on the Noisy Speech Database (NSD) to reconstruct denoised spectrograms from noisy inputs. Then, we evaluate our model as a frontend to a pretrained Conformer ASR model as well as a frontend to train smaller Conformer ASR models from scratch. We show that the Cleancoder is able to filter noise from speech and that it improves the total Word Error Rate (WER) of the downstream model in noisy conditions for both applications.
</details>
<details>
<summary>摘要</summary>
In this study, we propose a novel method to extract the denoising capabilities that can be applied to any encoder-decoder architecture. We introduce the Cleancoder preprocessor architecture, which extracts hidden activations from the Conformer ASR model and feeds them to a decoder to predict denoised spectrograms. We train our preprocessor on the Noisy Speech Database (NSD) to reconstruct denoised spectrograms from noisy inputs.We evaluate our model as a frontend to a pretrained Conformer ASR model as well as a frontend to train smaller Conformer ASR models from scratch. Our results show that the Cleancoder is able to filter noise from speech and improve the total Word Error Rate (WER) of the downstream model in noisy conditions for both applications.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Methods-for-Ground-Truth-Free-Foreign-Accent-Conversion"><a href="#Evaluating-Methods-for-Ground-Truth-Free-Foreign-Accent-Conversion" class="headerlink" title="Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion"></a>Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02133">http://arxiv.org/abs/2309.02133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/unilight/seq2seq-vc">https://github.com/unilight/seq2seq-vc</a></li>
<li>paper_authors: Wen-Chin Huang, Tomoki Toda</li>
<li>for: 本研究旨在评估三种 latest proposed methods for ground-truth-free foreign accent conversion (FAC), 以便控制发音和 speaker identity。</li>
<li>methods: 这三种方法均基于 sequence-to-sequence (seq2seq) 和 non-parallel voice conversion (VC) 模型，以提取发音和控制发音人物的特征。</li>
<li>results: 我们的实验评估结果显示，这三种方法在不同评估车辆中并没有一个显著的优势，与之前的研究结论相 contradistinction。 我们还对 seq2seq 模型的训练输入和输出进行了解释，并分析了非平行 VC 模型的设计选择，并发现 intelligibility 指标与主观的发音程度之间无法建立直接的相关性。<details>
<summary>Abstract</summary>
Foreign accent conversion (FAC) is a special application of voice conversion (VC) which aims to convert the accented speech of a non-native speaker to a native-sounding speech with the same speaker identity. FAC is difficult since the native speech from the desired non-native speaker to be used as the training target is impossible to collect. In this work, we evaluate three recently proposed methods for ground-truth-free FAC, where all of them aim to harness the power of sequence-to-sequence (seq2seq) and non-parallel VC models to properly convert the accent and control the speaker identity. Our experimental evaluation results show that no single method was significantly better than the others in all evaluation axes, which is in contrast to conclusions drawn in previous studies. We also explain the effectiveness of these methods with the training input and output of the seq2seq model and examine the design choice of the non-parallel VC model, and show that intelligibility measures such as word error rates do not correlate well with subjective accentedness. Finally, our implementation is open-sourced to promote reproducible research and help future researchers improve upon the compared systems.
</details>
<details>
<summary>摘要</summary>
外语口音转换（FAC）是一种特殊的voice转换（VC）应用，旨在将非本地语言speaker的带有口音的语音转换成本地语言speaker的Native-sounding语音，同时保持speaker identity。FAC是具有挑战性，因为获得desired non-native speaker的Native speech为训练目标是不可能的。在这项工作中，我们评估了三种最近提出的ground-truth-free FAC方法，它们都是基于sequence-to-sequence（seq2seq）和非平行VC模型来实现口音转换和控制speaker identity。我们的实验评估结果表明，无一个方法在所有评估轴上显著更好，与之前的研究不同。我们还解释了这些方法的效iveness，包括seq2seq模型的训练输入和输出，以及非平行VC模型的设计选择，并证明了对于智能性指标 word error rates不相关于主观的口音度。最后，我们开源了我们的实现，以便促进可重复性的研究，并帮助未来的研究人员改进相关的系统。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.SD_2023_09_05/" data-id="clmjn91oc00bt0j88dnwfdk04" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.LG_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T10:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/cs.LG_2023_09_05/">cs.LG - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Superclustering-by-finding-statistically-significant-separable-groups-of-optimal-gaussian-clusters"><a href="#Superclustering-by-finding-statistically-significant-separable-groups-of-optimal-gaussian-clusters" class="headerlink" title="Superclustering by finding statistically significant separable groups of optimal gaussian clusters"></a>Superclustering by finding statistically significant separable groups of optimal gaussian clusters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02623">http://arxiv.org/abs/2309.02623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berng/GMSDB">https://github.com/berng/GMSDB</a></li>
<li>paper_authors: Oleg I. Berngardt</li>
<li>for: 这个论文是为了提出一种基于BIC criterion和统计可分离性的超集群算法。</li>
<li>methods: 该算法包括三个阶段：首先，将数据集表示为一个mixture of Gaussian Distributions - 分布；其次，使用Mahalanobis distance和cluster size来估计分布之间的距离和分布大小；最后，使用DBSCAN方法将分布组合成超集群，并通过最大值引入矩阵质量指标来选择最佳超集群数量。</li>
<li>results: 该算法在测试数据集上达到了良好的结果，并且能够预测新数据中的正确超集群。然而，该算法具有低速度和随机性的缺点，需要较大的数据集进行分 clustering，这是许多统计方法的共同特点。<details>
<summary>Abstract</summary>
The paper presents the algorithm for clustering a dataset by grouping the optimal, from the point of view of the BIC criterion, number of Gaussian clusters into the optimal, from the point of view of their statistical separability, superclusters.   The algorithm consists of three stages: representation of the dataset as a mixture of Gaussian distributions - clusters, which number is determined based on the minimum of the BIC criterion; using the Mahalanobis distance, to estimate the distances between the clusters and cluster sizes; combining the resulting clusters into superclusters using the DBSCAN method by finding its hyperparameter (maximum distance) providing maximum value of introduced matrix quality criterion at maximum number of superclusters. The matrix quality criterion corresponds to the proportion of statistically significant separated superclusters among all found superclusters.   The algorithm has only one hyperparameter - statistical significance level, and automatically detects optimal number and shape of superclusters based of statistical hypothesis testing approach. The algorithm demonstrates a good results on test datasets in noise and noiseless situations. An essential advantage of the algorithm is its ability to predict correct supercluster for new data based on already trained clusterer and perform soft (fuzzy) clustering. The disadvantages of the algorithm are: its low speed and stochastic nature of the final clustering. It requires a sufficiently large dataset for clustering, which is typical for many statistical methods.
</details>
<details>
<summary>摘要</summary>
文章提出了一种算法，用于将数据集分为优化的超集群。该算法包括三个阶段：1. 将数据集表示为一种mixture of Gaussian distributions - 集群，集群数量由BIC criterion的最小值确定；2. 使用Mahalanobis distance来估计集群之间的距离和集群大小；3. 使用DBSCAN方法将集群组合成超集群，并通过最大值引入的矩阵质量标准来确定最大值。该算法具有一个超参数 - 统计 significancLevel，可以自动找到最佳数量和形状的超集群，基于统计假设检测方法。它在测试数据集上显示了良好的效果，包括噪声和噪声free的情况。该算法的一个优点是它可以预测新数据中的正确超集群，并实现软（柔） clustering。However, the algorithm has some disadvantages, such as low speed and stochastic nature of the final clustering. It requires a large enough dataset for clustering, which is common for many statistical methods.Here's the translation in Traditional Chinese:文章提出了一个算法，用于将数据集分为优化的超集群。该算法包括三个阶段：1. 将数据集表示为一种mixture of Gaussian distributions - 集群，集群数量由BIC criterion的最小值确定；2. 使用Mahalanobis distance来估计集群之间的距离和集群大小；3. 使用DBSCAN方法将集群组合成超集群，并通过最大值引入的矩阵质量标准来确定最大值。该算法具有一个超参数 - 统计 significancLevel，可以自动找到最佳数量和形状的超集群，基于统计假设检测方法。它在测试数据集上显示了良好的效果，包括噪声和噪声free的情况。该算法的一个优点是它可以预测新数据中的正确超集群，并实现软（柔） clustering。然而，该算法有一些缺点，例如低速和统计性的最终分 clustering。它需要一个够大的数据集来类别，这是许多统计方法的常见需求。
</details></li>
</ul>
<hr>
<h2 id="Compressing-Vision-Transformers-for-Low-Resource-Visual-Learning"><a href="#Compressing-Vision-Transformers-for-Low-Resource-Visual-Learning" class="headerlink" title="Compressing Vision Transformers for Low-Resource Visual Learning"></a>Compressing Vision Transformers for Low-Resource Visual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02617">http://arxiv.org/abs/2309.02617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chensy7/efficient-vit">https://github.com/chensy7/efficient-vit</a></li>
<li>paper_authors: Eric Youn, Sai Mitheran J, Sanjana Prabhu, Siyuan Chen</li>
<li>For: The paper aims to bring vision transformers to resource-constrained devices like unmanned aerial vehicles (UAVs) for applications such as surveillance and environmental monitoring.* Methods: The paper utilizes popular model compression techniques like distillation, pruning, and quantization to reduce the size of the vision transformer model and improve its deployment on mobile and edge scenarios.* Results: The paper targets to achieve rapid inference of a vision transformer on an NVIDIA Jetson Nano (4GB) with minimal accuracy loss, enabling the deployment of ViTs on resource-constrained devices.Here’s the Chinese version of the three key points:* 为： paper 目标是将视transformer 部署到有限资源设备中，如无人飞机（UAV），用于应用程序如监视和环境监测等。* 方法： paper 利用流行的模型压缩技术，如精神投射、剪辑和量化，来减少视transformer 模型的大小，提高其在移动和边缘场景中的部署。* 结果： paper 目标是在 NVIDIA Jetson Nano（4GB）上实现视transformer 快速推理，保持最小的准确性损失，以实现resource-constrained devices 上的 ViT 部署。<details>
<summary>Abstract</summary>
Vision transformer (ViT) and its variants have swept through visual learning leaderboards and offer state-of-the-art accuracy in tasks such as image classification, object detection, and semantic segmentation by attending to different parts of the visual input and capturing long-range spatial dependencies. However, these models are large and computation-heavy. For instance, the recently proposed ViT-B model has 86M parameters making it impractical for deployment on resource-constrained devices. As a result, their deployment on mobile and edge scenarios is limited. In our work, we aim to take a step toward bringing vision transformers to the edge by utilizing popular model compression techniques such as distillation, pruning, and quantization.   Our chosen application environment is an unmanned aerial vehicle (UAV) that is battery-powered and memory-constrained, carrying a single-board computer on the scale of an NVIDIA Jetson Nano with 4GB of RAM. On the other hand, the UAV requires high accuracy close to that of state-of-the-art ViTs to ensure safe object avoidance in autonomous navigation, or correct localization of humans in search-and-rescue. Inference latency should also be minimized given the application requirements. Hence, our target is to enable rapid inference of a vision transformer on an NVIDIA Jetson Nano (4GB) with minimal accuracy loss. This allows us to deploy ViTs on resource-constrained devices, opening up new possibilities in surveillance, environmental monitoring, etc. Our implementation is made available at https://github.com/chensy7/efficient-vit.
</details>
<details>
<summary>摘要</summary>
《vision transformer（ViT）和其变种在视觉学领域中占据了领先地位，并提供了最佳精度在图像分类、物体检测和semantic segmentation任务中，通过不同部分的视觉输入注意和捕捉长距离 espacial dependent。然而，这些模型很大和计算沉重。例如，最近提出的ViT-B模型有86M参数，使其在资源有限设备上不可 praktical。因此，它们在移动和边缘场景中的部署受限。在我们的工作中，我们决心将视觉 transformer 带到边缘，通过使用流行的模型压缩技术 such as distillation、pruning和quantization。our chosen application environment is an unmanned aerial vehicle (UAV) that is battery-powered and memory-constrained, carrying a single-board computer on the scale of an NVIDIA Jetson Nano with 4GB of RAM。在这种应用环境中，UAV需要高精度，接近现状的state-of-the-art ViTs，以确保自主导航中的物体避免和人员Localization的正确。同时，推理延迟应该被最小化，因为应用要求。因此，我们的目标是在NVIDIA Jetson Nano (4GB)上快速推理一个vision transformer，并保持最小的精度损失。这使得我们能够在资源有限设备上部署ViTs，开 up new possibilities in surveillance, environmental monitoring, etc。我们的实现可以在https://github.com/chensy7/efficient-vit中找到。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-aided-Joint-Training-free-Secure-Semantic-Communications-via-Multi-modal-Prompts"><a href="#Generative-AI-aided-Joint-Training-free-Secure-Semantic-Communications-via-Multi-modal-Prompts" class="headerlink" title="Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts"></a>Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02616">http://arxiv.org/abs/2309.02616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyang Du, Guangyuan Liu, Dusit Niyato, Jiayi Zhang, Jiawen Kang, Zehui Xiong, Bo Ai, Dong In Kim</li>
<li>for: 降低网络资源消耗，实现通信目标，而不需要 JOINT 训练 semantic encoder 和 decoder。</li>
<li>methods: 利用 Generative Artificial Intelligence (GAI) 模型，实现准确的内容解码。并 introducing covert communications aided by a friendly jammer，以确保成功的传输和安全通信。</li>
<li>results: 提出一种 GAI-aided SemCom 系统，可以准确地重建源消息，并提供了一种安全的传输方式。<details>
<summary>Abstract</summary>
Semantic communication (SemCom) holds promise for reducing network resource consumption while achieving the communications goal. However, the computational overheads in jointly training semantic encoders and decoders-and the subsequent deployment in network devices-are overlooked. Recent advances in Generative artificial intelligence (GAI) offer a potential solution. The robust learning abilities of GAI models indicate that semantic decoders can reconstruct source messages using a limited amount of semantic information, e.g., prompts, without joint training with the semantic encoder. A notable challenge, however, is the instability introduced by GAI's diverse generation ability. This instability, evident in outputs like text-generated images, limits the direct application of GAI in scenarios demanding accurate message recovery, such as face image transmission. To solve the above problems, this paper proposes a GAI-aided SemCom system with multi-model prompts for accurate content decoding. Moreover, in response to security concerns, we introduce the application of covert communications aided by a friendly jammer. The system jointly optimizes the diffusion step, jamming, and transmitting power with the aid of the generative diffusion models, enabling successful and secure transmission of the source messages.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Algorithms-for-Fusion-of-Physics-Based-Wildfire-Spread-Models-with-Satellite-Data-for-Initializing-Wildfire-Forecasts"><a href="#Generative-Algorithms-for-Fusion-of-Physics-Based-Wildfire-Spread-Models-with-Satellite-Data-for-Initializing-Wildfire-Forecasts" class="headerlink" title="Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts"></a>Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02615">http://arxiv.org/abs/2309.02615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bryan Shaddy, Deep Ray, Angel Farguell, Valentina Calaza, Jan Mandel, James Haley, Kyle Hilburn, Derek V. Mallia, Adam Kochanski, Assad Oberai</li>
<li>for: 这项研究的目的是开发一种基于卫星测量数据的高级野火行为模型，以便更好地预测野火的 spreadof.</li>
<li>methods: 这项研究使用了一种named conditional Wasserstein Generative Adversarial Network (cWGAN)，通过训练WRF-SFIRE模型，来INFER野火爆发时间从卫星活动火灾数据中。</li>
<li>results: 研究测试了四起加利福尼亚州的野火，并与高分辨率空中红外测量数据进行比较。结果显示，cWGAN的预测结果具有高度准确性， average Sorensen’s coefficient为0.81， average ignition time error为32分钟。<details>
<summary>Abstract</summary>
Increases in wildfire activity and the resulting impacts have prompted the development of high-resolution wildfire behavior models for forecasting fire spread. Recent progress in using satellites to detect fire locations further provides the opportunity to use measurements to improve fire spread forecasts from numerical models through data assimilation. This work develops a method for inferring the history of a wildfire from satellite measurements, providing the necessary information to initialize coupled atmosphere-wildfire models from a measured wildfire state in a physics-informed approach. The fire arrival time, which is the time the fire reaches a given spatial location, acts as a succinct representation of the history of a wildfire. In this work, a conditional Wasserstein Generative Adversarial Network (cWGAN), trained with WRF-SFIRE simulations, is used to infer the fire arrival time from satellite active fire data. The cWGAN is used to produce samples of likely fire arrival times from the conditional distribution of arrival times given satellite active fire detections. Samples produced by the cWGAN are further used to assess the uncertainty of predictions. The cWGAN is tested on four California wildfires occurring between 2020 and 2022, and predictions for fire extent are compared against high resolution airborne infrared measurements. Further, the predicted ignition times are compared with reported ignition times. An average Sorensen's coefficient of 0.81 for the fire perimeters and an average ignition time error of 32 minutes suggest that the method is highly accurate.
</details>
<details>
<summary>摘要</summary>
due to the increasing frequency and impact of wildfires, there is a need for high-resolution models of wildfire behavior to predict the spread of fires. recent advances in satellite technology allow for the detection of fire locations, which can be used to improve fire spread forecasts through data assimilation. this study develops a method for inferring the history of a wildfire from satellite measurements, providing the necessary information to initialize coupled atmosphere-wildfire models in a physics-informed approach. the fire arrival time, which is the time it takes for the fire to reach a specific location, is used as a concise representation of the history of a wildfire. in this study, a conditional Wasserstein generative adversarial network (cWGAN) is used to infer the fire arrival time from satellite active fire data. the cWGAN is trained with WRF-SFIRE simulations and produces samples of likely fire arrival times from the conditional distribution of arrival times given satellite active fire detections. the samples produced by the cWGAN are used to assess the uncertainty of predictions. the cWGAN is tested on four california wildfires that occurred between 2020 and 2022, and the predictions for fire extent are compared against high-resolution airborne infrared measurements. the predicted ignition times are also compared with reported ignition times. the results show an average sorensen's coefficient of 0.81 for the fire perimeters and an average ignition time error of 32 minutes, indicating that the method is highly accurate.
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds"><a href="#Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds" class="headerlink" title="Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds"></a>Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02614">http://arxiv.org/abs/2309.02614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Blaxzter/Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds">https://github.com/Blaxzter/Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds</a></li>
<li>paper_authors: Frederic Abraham, Matthew Stephenson</li>
<li>for: 这个论文是为了探讨使用生成对抗网络（GANs）来生成适应性的游戏逻辑游戏《愤怒的鸟》稳定结构的适用性。</li>
<li>methods: 这篇论文使用了GANs生成稳定结构，包括一个细致的编码&#x2F;解码过程来将游戏场景描述转换为适合网格表示的格子表示，以及使用当今最佳GAN架构和训练方法来生成新的结构设计。</li>
<li>results: 研究结果表明，GANs可以成功地应用于生成复杂且稳定的游戏结构。<details>
<summary>Abstract</summary>
This paper investigates the suitability of using Generative Adversarial Networks (GANs) to generate stable structures for the physics-based puzzle game Angry Birds. While previous applications of GANs for level generation have been mostly limited to tile-based representations, this paper explores their suitability for creating stable structures made from multiple smaller blocks. This includes a detailed encoding/decoding process for converting between Angry Birds level descriptions and a suitable grid-based representation, as well as utilizing state-of-the-art GAN architectures and training methods to produce new structure designs. Our results show that GANs can be successfully applied to generate a varied range of complex and stable Angry Birds structures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="T-SaS-Toward-Shift-aware-Dynamic-Adaptation-for-Streaming-Data"><a href="#T-SaS-Toward-Shift-aware-Dynamic-Adaptation-for-Streaming-Data" class="headerlink" title="T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data"></a>T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02610">http://arxiv.org/abs/2309.02610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijieying Ren, Tianxiang Zhao, Wei Qin, Kunpeng Liu</li>
<li>for: 本研究旨在解决流动数据中的突然分布shift问题，无需提前知道分布boundary。</li>
<li>methods: 提出了一种 Bayesian 框架（T-SaS），通过一个整数分布模型来捕捉流动数据中的突然shift。然后，通过动态网络选择条件来适应不同分布。</li>
<li>results: 对于流动数据中的分布shift问题，本研究的方法能够准确地检测分布boundary，并且在下游预测或分类任务中效果更高。<details>
<summary>Abstract</summary>
In many real-world scenarios, distribution shifts exist in the streaming data across time steps. Many complex sequential data can be effectively divided into distinct regimes that exhibit persistent dynamics. Discovering the shifted behaviors and the evolving patterns underlying the streaming data are important to understand the dynamic system. Existing methods typically train one robust model to work for the evolving data of distinct distributions or sequentially adapt the model utilizing explicitly given regime boundaries. However, there are two challenges: (1) shifts in data streams could happen drastically and abruptly without precursors. Boundaries of distribution shifts are usually unavailable, and (2) training a shared model for all domains could fail to capture varying patterns. This paper aims to solve the problem of sequential data modeling in the presence of sudden distribution shifts that occur without any precursors. Specifically, we design a Bayesian framework, dubbed as T-SaS, with a discrete distribution-modeling variable to capture abrupt shifts of data. Then, we design a model that enable adaptation with dynamic network selection conditioned on that discrete variable. The proposed method learns specific model parameters for each distribution by learning which neurons should be activated in the full network. A dynamic masking strategy is adopted here to support inter-distribution transfer through the overlapping of a set of sparse networks. Extensive experiments show that our proposed method is superior in both accurately detecting shift boundaries to get segments of varying distributions and effectively adapting to downstream forecast or classification tasks.
</details>
<details>
<summary>摘要</summary>
在许多实际场景中，流动数据中的分布shift存在时间步骤之间。许多复杂的顺序数据可以有效地分解为不同的领域，这些领域具有持续的动态。了解流动数据中的shift和下沉的 Patterns是理解动态系统的关键。现有方法通常是在流动数据中训练一个Robust模型，以便在不同分布下进行适应。然而，存在两个挑战：（1）数据流中的shift可能会发生急剧和突然，没有预ursor;（2）训练所有领域的共享模型可能无法捕捉不同的模式。这篇论文的目标是解决流动数据中的顺序数据模型化问题，具体来说是在不同分布下快速适应。我们提出了一种 Bayesian框架，名为T-SaS，其中包含一个用于捕捉数据shift的离散分布模型。然后，我们设计了一种基于动态网络选择的模型，可以在不同分布下适应。我们的方法可以学习每个分布的特定参数，并通过在全网络中活化特定神经元来实现这一点。我们采用了一种动态遮盾策略，以支持间隔分布转移。我们的实验表明，我们的方法在准确地检测分布shift并在下游预测或分类任务中适应效果更高。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Variational-Inference-for-Online-Supervised-Learning"><a href="#Distributed-Variational-Inference-for-Online-Supervised-Learning" class="headerlink" title="Distributed Variational Inference for Online Supervised Learning"></a>Distributed Variational Inference for Online Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02606">http://arxiv.org/abs/2309.02606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pptx/distributed-mapping">https://github.com/pptx/distributed-mapping</a></li>
<li>paper_authors: Parth Paritosh, Nikolay Atanasov, Sonia Martinez</li>
<li>for: 这 paper 的目的是开发智能感知网络中的有效解决方案，以提供下一代位置、跟踪和地图服务。</li>
<li>methods: 该 paper 使用分布式概率推理算法，应用于连续变量、不可解 posterior 和大规模实时数据。在中央设置下，变量推理是基本技术，用于approximate Bayesian estimation，其中一个难以解决的 posterior density 被 aproximated 为参数化density。</li>
<li>results: 该 paper 提出了一种分布式逻辑低幂（DELBO），用于在感知网络中进行分布式变量推理。DELBO 是一个 weighted sum of observation likelihood 和偏好函数 divergence，其中 gap 是 due to consensus 和模型误差。该 algorithm 可以解决 binary classification 和回归问题，并处理流动数据。在 Gaussian variational densities 中，我们设计了一种在线分布式算法，用于最大化 DELBO，并对非线性 likelihood 进行特化。最后，我们 derivated 一个 diagonally 的版本，用于在高维模型中进行在线分布式推理。这 paper 的结果表明，DGVI 可以高效地解决 $1$-rank correction 问题，并在多机器人概率地图中应用。<details>
<summary>Abstract</summary>
Developing efficient solutions for inference problems in intelligent sensor networks is crucial for the next generation of location, tracking, and mapping services. This paper develops a scalable distributed probabilistic inference algorithm that applies to continuous variables, intractable posteriors and large-scale real-time data in sensor networks. In a centralized setting, variational inference is a fundamental technique for performing approximate Bayesian estimation, in which an intractable posterior density is approximated with a parametric density. Our key contribution lies in the derivation of a separable lower bound on the centralized estimation objective, which enables distributed variational inference with one-hop communication in a sensor network. Our distributed evidence lower bound (DELBO) consists of a weighted sum of observation likelihood and divergence to prior densities, and its gap to the measurement evidence is due to consensus and modeling errors. To solve binary classification and regression problems while handling streaming data, we design an online distributed algorithm that maximizes DELBO, and specialize it to Gaussian variational densities with non-linear likelihoods. The resulting distributed Gaussian variational inference (DGVI) efficiently inverts a $1$-rank correction to the covariance matrix. Finally, we derive a diagonalized version for online distributed inference in high-dimensional models, and apply it to multi-robot probabilistic mapping using indoor LiDAR data.
</details>
<details>
<summary>摘要</summary>
开发高效的推理解决方案对于智能感知网络中的位置、跟踪和地图服务是关键。本文提出了一种可扩展的分布式概率推理算法，可应用于连续变量、不可解 posterior 和大规模实时数据。在中央设置下，变量推理是概率推理的基本技术，用于进行approximate Bayesian estimation，其中一个难以求解 posterior density 被approximated 为 Parametric density。我们的关键贡献在于 derive 一个可分离的下界于中央估计目标，这使得分布式变量推理可以在感知网络中使用一 hop 通信。我们称之为分布式证据下界（DELBO），它包括观察可能性和偏好分布之间的差异，这种差异是由consensus和模型误差引起的。为处理流动数据，我们设计了一种在线分布式算法，该算法可以最大化 DELBO，并特化为 Gaussian variational densities  WITH non-linear likelihoods。这些非线性 likelihoods 可以处理流动数据。最后，我们 derivate 一个 diagonale 版本，用于在高维模型中进行在线分布式推理。我们应用了这种方法于多机器人概率地图使用indoor LiDAR数据。
</details></li>
</ul>
<hr>
<h2 id="Screening-of-Pneumonia-and-Urinary-Tract-Infection-at-Triage-using-TriNet"><a href="#Screening-of-Pneumonia-and-Urinary-Tract-Infection-at-Triage-using-TriNet" class="headerlink" title="Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet"></a>Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02604">http://arxiv.org/abs/2309.02604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Z. Lu</li>
<li>For: 提高急诊室效率和健康质量， Addressing the issue of overloaded traditional clinical workflows and inaccurate diagnoses in emergency departments.* Methods: 使用机器学习模型自动化排查和诊断确认，TriNet模型在医疗排查数据上培育，可以高度准确地检测患有肺炎和恶性肾炎等常见疾病。* Results:  TriNet模型的正确预测率高于当前临床标准，表明机器学习医疗指导可以提供高特异性的免费非侵入式检测，提高急诊室效率而不增加风险。<details>
<summary>Abstract</summary>
Due to the steady rise in population demographics and longevity, emergency department visits are increasing across North America. As more patients visit the emergency department, traditional clinical workflows become overloaded and inefficient, leading to prolonged wait-times and reduced healthcare quality. One of such workflows is the triage medical directive, impeded by limited human workload, inaccurate diagnoses and invasive over-testing. To address this issue, we propose TriNet: a machine learning model for medical directives that automates first-line screening at triage for conditions requiring downstream testing for diagnosis confirmation. To verify screening potential, TriNet was trained on hospital triage data and achieved high positive predictive values in detecting pneumonia (0.86) and urinary tract infection (0.93). These models outperform current clinical benchmarks, indicating that machine-learning medical directives can offer cost-free, non-invasive screening with high specificity for common conditions, reducing the risk of over-testing while increasing emergency department efficiency.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因为人口征性和寿命的不断增长，北美洲的急诊室访问量在增长。随着更多的患者来到急诊室，传统的临床工作流程变得过载和不效率，导致排队时间增长和健康保健质量下降。其中一个工作流程是护理医疗指南，受到人工负荷、不准确诊断和侵入性过测的限制。为解决这个问题，我们提出了TriNet：一种基于机器学习的医疗指南，用于在急诊室triage阶段自动进行第一线检测，以确认需要下游测试的疾病。为验证检测潜力，TriNet在医院triage数据上进行训练，在肺炎（0.86）和摄肾炎（0.93）方面达到了高正确预测值。这些模型超越了现有的临床标准，表明机器学习医疗指南可以免费、不侵入性地进行检测，提高急诊室效率，降低风险的过测。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Pretraining-Improves-Performance-and-Inference-Efficiency-in-Multiple-Lung-Ultrasound-Interpretation-Tasks"><a href="#Self-Supervised-Pretraining-Improves-Performance-and-Inference-Efficiency-in-Multiple-Lung-Ultrasound-Interpretation-Tasks" class="headerlink" title="Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks"></a>Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02596">http://arxiv.org/abs/2309.02596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blake VanBerlo, Brian Li, Jesse Hoey, Alexander Wong</li>
<li>for: 这个研究旨在检查自我超参数化预训练是否可以生成适用于多个分类任务的脑网络特征提取器，以及这种预训练方法在肺超声分类任务中的性能。</li>
<li>methods: 这个研究使用了自我超参数化预训练方法，并在三个肺超声分类任务上进行了细化。</li>
<li>results: 研究结果表明，使用自我超参数化预训练方法可以提高肺超声分类任务的平均横距下 Receiver Operating Characteristic  Curve（AUC）的值，并且可以降低推理时间。<details>
<summary>Abstract</summary>
In this study, we investigated whether self-supervised pretraining could produce a neural network feature extractor applicable to multiple classification tasks in B-mode lung ultrasound analysis. When fine-tuning on three lung ultrasound tasks, pretrained models resulted in an improvement of the average across-task area under the receiver operating curve (AUC) by 0.032 and 0.061 on local and external test sets respectively. Compact nonlinear classifiers trained on features outputted by a single pretrained model did not improve performance across all tasks; however, they did reduce inference time by 49% compared to serial execution of separate fine-tuned models. When training using 1% of the available labels, pretrained models consistently outperformed fully supervised models, with a maximum observed test AUC increase of 0.396 for the task of view classification. Overall, the results indicate that self-supervised pretraining is useful for producing initial weights for lung ultrasound classifiers.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了自我超vision学习是否可以生成适用于多个分类任务的神经网络特征提取器。当 fine-tuning 三个肺超声任务时，预训练模型导致了平均 across-task 接收操作曲线下的面积下降值（AUC）的提高，分别为 0.032 和 0.061 在本地和外部测试集上。 compact nonlinear 分类器在一个预训练模型输出的特征上训练后没有提高所有任务的性能，但它们可以将推理时间减少 49%，比对 separte fine-tuned 模型的串行执行更快。当使用 1% 可用标签进行训练时，预训练模型一直 OUTperform 完全supervised 模型，最大观察到的测试 AUC 提高为 0.396  для视图分类任务。总的来说，结果表明自我超vision学习是生成肺超声分类器初始 веса的有用方法。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Autoregressive-Multi-Modal-Models-Pretraining-and-Instruction-Tuning"><a href="#Scaling-Autoregressive-Multi-Modal-Models-Pretraining-and-Instruction-Tuning" class="headerlink" title="Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning"></a>Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02591">http://arxiv.org/abs/2309.02591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/CM3Leon">https://github.com/kyegomez/CM3Leon</a></li>
<li>paper_authors: Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, Armen Aghajanyan</li>
<li>for: 这个论文的目的是提出一种基于Token-based decoder-only多模态语言模型，可以生成和填充文本和图像。</li>
<li>methods: 这个模型使用了CM3多模态架构，并在更多的指令样本上进行了扩大和调参。它还包括一个大规模的检索补做预训练阶段，以及一个第二个多任务监督练练（SFT）阶段。</li>
<li>results: 这个模型可以实现高质量的文本到图像生成和图像到文本生成，并且可以在语言导向图像修改和图像控制生成等任务中示出无 precedent的可控性。在相应的实验中，CM3Leon在文本到图像生成任务中实现了比较方法的5倍减少的训练计算量（零shot MS-COCO FID为4.88）。<details>
<summary>Abstract</summary>
We present CM3Leon (pronounced "Chameleon"), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon uses the CM3 multi-modal architecture but additionally shows the extreme benefits of scaling up and tuning on more diverse instruction-style data. It is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate unprecedented levels of controllability in tasks ranging from language-guided image editing to image-controlled generation and segmentation.
</details>
<details>
<summary>摘要</summary>
我们介绍CM3Leon（发音为“毫毫”），一种具有生成和填充功能的多Modal语言模型，可以生成和填充文本和图像。CM3Leon使用CM3多Modal架构，同时还能够在更多的指令样本数据上扩大和调整，从而实现极高的性能提升。它是首个基于文本Only语言模型的多Modal模型，通过大规模的检索增强预训练阶段和第二个多任务监督精度调整（SFT）阶段进行训练。它同时是一个通用的模型，可以进行文本到图像和图像到文本的生成，使我们能够介绍自包含的对比编码方法，以生成高质量的输出。广泛的实验表明，这种方法对多Modal模型非常有效。CM3Leon在文本到图像生成任务中实现了状态机器的表现（零 shot MS-COCO FID为4.88），并且在语言导向图像修改、图像控制生成和分割等任务中也能够实现无前例的可控性。
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-for-Sequential-Volumetric-Design-Tasks"><a href="#Representation-Learning-for-Sequential-Volumetric-Design-Tasks" class="headerlink" title="Representation Learning for Sequential Volumetric Design Tasks"></a>Representation Learning for Sequential Volumetric Design Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02583">http://arxiv.org/abs/2309.02583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Ferdous Alam, Yi Wang, Linh Tran, Chin-Yi Cheng, Jieliang Luo</li>
<li>for: 这篇论文是为了提出一种基于变换器模型的自动化建筑设计方法，以便利用学习的设计知识来评估和生成建筑设计。</li>
<li>methods: 该论文使用变换器模型来编码设计知识，并从一群专家或高性能的设计序列中提取有用的表示。然后，它使用这些表示来评估设计和生成sequential设计。</li>
<li>results: 论文通过使用变换器模型来评估设计的预测性和生成sequential设计，并在一个大量的sequentialvolumetric设计数据集上进行了实验。结果显示，论文的方法可以准确地评估设计的预测性和生成sequential设计，并且可以自动完成volumetric设计序列的生成。<details>
<summary>Abstract</summary>
Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the preference model by estimating the density of the learned representations whereas we train an autoregressive transformer model for sequential design generation. We demonstrate our ideas by leveraging a novel dataset of thousands of sequential volumetric designs. Our preference model can compare two arbitrarily given design sequences and is almost 90% accurate in evaluation against random design sequences. Our autoregressive model is also capable of autocompleting a volumetric design sequence from a partial design sequence.
</details>
<details>
<summary>摘要</summary>
三维设计，也称为质量设计，是职业建筑设计的第一步，具有顺序的性质。由于三维设计过程复杂，下面的设计过程含有价值信息。许多努力已经被дела以生成合理的三维设计解决方案，但生成的设计解决方案质量不稳定，评估设计解决方案需要 Either a comprehensive set of metrics or expensive human expertise。而前一些方法仅学习了最终的设计而不是顺序的设计任务，我们提议将专家或高性能的设计序列知识编码到 transformer-based 模型中，然后提取有用的表示。后来，我们将学习的表示用于重要的下游应用程序，如设计偏好评估和过程设计生成。我们开发了偏好模型，通过估计学习的表示密度来评估两个任意给定的设计序列。我们还训练了一个自然语言模型，用于生成顺序的设计序列。我们利用了一个 novel dataset of thousands of sequential volumetric designs，并证明了我们的想法。我们的偏好模型可以比较两个任意给定的设计序列，准确率接近 90%。我们的自然语言模型也可以自动完成一个三维设计序列的生成，从一个受限的设计序列开始。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Intractable-Epileptogenic-Brain-Networks-with-Deep-Learning-Algorithms-A-Novel-and-Comprehensive-Framework-for-Scalable-Seizure-Prediction-with-Unimodal-Neuroimaging-Data-in-Pediatric-Patients"><a href="#Unveiling-Intractable-Epileptogenic-Brain-Networks-with-Deep-Learning-Algorithms-A-Novel-and-Comprehensive-Framework-for-Scalable-Seizure-Prediction-with-Unimodal-Neuroimaging-Data-in-Pediatric-Patients" class="headerlink" title="Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients"></a>Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02580">http://arxiv.org/abs/2309.02580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bliss Singhal, Fnu Pooja</li>
<li>for: 预测儿童癫痫症发作，提高医疗工作者对癫痫症管理的反应性。</li>
<li>methods: 使用机器学习算法对单模态神经成像数据进行评估，包括电enzephalogram信号。</li>
<li>results: 研究发现，深度学习算法在预测癫痫症发作中表现更好于逻辑回归和k最近邻居算法，特别是Recurrent Neural Network（RNN）和Convolutional Neural Network（CNN）。<details>
<summary>Abstract</summary>
Epilepsy is a prevalent neurological disorder affecting 50 million individuals worldwide and 1.2 million Americans. There exist millions of pediatric patients with intractable epilepsy, a condition in which seizures fail to come under control. The occurrence of seizures can result in physical injury, disorientation, unconsciousness, and additional symptoms that could impede children's ability to participate in everyday tasks. Predicting seizures can help parents and healthcare providers take precautions, prevent risky situations, and mentally prepare children to minimize anxiety and nervousness associated with the uncertainty of a seizure. This research proposes a novel and comprehensive framework to predict seizures in pediatric patients by evaluating machine learning algorithms on unimodal neuroimaging data consisting of electroencephalogram signals. The bandpass filtering and independent component analysis proved to be effective in reducing the noise and artifacts from the dataset. Various machine learning algorithms' performance is evaluated on important metrics such as accuracy, precision, specificity, sensitivity, F1 score and MCC. The results show that the deep learning algorithms are more successful in predicting seizures than logistic Regression, and k nearest neighbors. The recurrent neural network (RNN) gave the highest precision and F1 Score, long short-term memory (LSTM) outperformed RNN in accuracy and convolutional neural network (CNN) resulted in the highest Specificity. This research has significant implications for healthcare providers in proactively managing seizure occurrence in pediatric patients, potentially transforming clinical practices, and improving pediatric care.
</details>
<details>
<summary>摘要</summary>
“偏头痛是一种流行的神经系统疾病，全球病例约5000万人，美国病例约120万人。有数百万名儿童患有难治性偏头痛，这些病例中的病人可能会受到肢体伤害、混乱、失去知觉和其他 симптом，这些病情可能会干扰儿童日常生活。预测偏头痛可以帮助家长和医疗保健专业人员预防危险情况，准确预测可以帮助儿童减少焦虑和不安，这有助于改善儿童健康。本研究提出了一个新的和完整的预测偏头痛框架，通过评估机器学习算法在单一神经内部成像数据上的表现。实验结果显示，深度学习算法在预测偏头痛方面表现更好，比逻辑回传和k最近邻居。RNN和LSTM也获得了较高的精度和F1分数，CNN则获得了最高的特异性。这些研究结果具有重要的实践意义，可以帮助医疗保健专业人员更好地管理儿童偏头痛的发生，将有助于改善儿童健康，并可能改变临床实践。”
</details></li>
</ul>
<hr>
<h2 id="Anatomy-Driven-Pathology-Detection-on-Chest-X-rays"><a href="#Anatomy-Driven-Pathology-Detection-on-Chest-X-rays" class="headerlink" title="Anatomy-Driven Pathology Detection on Chest X-rays"></a>Anatomy-Driven Pathology Detection on Chest X-rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02578">http://arxiv.org/abs/2309.02578</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/philip-mueller/adpd">https://github.com/philip-mueller/adpd</a></li>
<li>paper_authors: Philip Müller, Felix Meissen, Johannes Brandt, Georgios Kaissis, Daniel Rueckert</li>
<li>For: 这篇论文的目的是提出一种自动检测医疗扫描图像中的疾病，并提供高度解释性的支持，以帮助医生做出了解的决策。* Methods: 这篇论文使用了弱地监督的物体检测方法，从图像水平的标签中学习疾病的（粗略）本地化。* Results: 这篇论文的结果显示，使用了解析预报的方法可以超过弱地监督的方法和仅有限的训练样本，并且与完全监督的检测方法竞争。<details>
<summary>Abstract</summary>
Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supervised detection with limited training samples, and our MIL approach is competitive with both baseline approaches, therefore demonstrating the potential of our approach.
</details>
<details>
<summary>摘要</summary>
医学影像检测和定位技术可以自动解读医学影像，如胸部X射线图像，并提供高水平的解释性，以支持 radiologist 做出 informed 决策。但是，标注疾病 bounding box 是一项时间consuming 的任务，因此大型公共数据集 для此目的罕见。现有方法通常使用弱型 supervised object detection 学习疾病的 (粗略) 本地化，但是由于缺乏 bounding box 监督，性能有限。我们因此提议 anatomy-driven pathology detection (ADPD)，使用容易标注的 bounding box 来代表疾病。我们研究了两种训练方法：supervised training 使用 anatomy-level pathology labels 和 multiple instance learning (MIL) 使用 image-level pathology labels。我们的结果显示，我们的 anatomy-level 训练方法高于弱型方法和有限训练样本的完全supervised detection，而我们的 MIL 方法与两个基eline方法竞争，因此证明了我们的方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="Emphysema-Subtyping-on-Thoracic-Computed-Tomography-Scans-using-Deep-Neural-Networks"><a href="#Emphysema-Subtyping-on-Thoracic-Computed-Tomography-Scans-using-Deep-Neural-Networks" class="headerlink" title="Emphysema Subtyping on Thoracic Computed Tomography Scans using Deep Neural Networks"></a>Emphysema Subtyping on Thoracic Computed Tomography Scans using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02576">http://arxiv.org/abs/2309.02576</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diagnijmegen/bodyct-dram-emph-subtype">https://github.com/diagnijmegen/bodyct-dram-emph-subtype</a></li>
<li>paper_authors: Weiyi Xie, Colin Jacobs, Jean-Paul Charbonnier, Dirk Jan Slebos, Bram van Ginneken</li>
<li>for:  automatizarea clasificării și evaluării severității emfizemului, pentru gestionarea eficientă a BPCO și studiului heterogeneității bolii.</li>
<li>methods:  utilizarea unui algoritm de învățare profundă pentru a simula sistemul de scor visual al Societății Fleischner pentru clasificarea emfizemului și evaluarea severității.</li>
<li>results:  o precizie a predicțiilor de 52%, față de o metodă publicată anterior cu o precizie de 45%. În plus, o bună înțelegere între scorurile predicate de metoda noastră și scorurile visuale.<details>
<summary>Abstract</summary>
Accurate identification of emphysema subtypes and severity is crucial for effective management of COPD and the study of disease heterogeneity. Manual analysis of emphysema subtypes and severity is laborious and subjective. To address this challenge, we present a deep learning-based approach for automating the Fleischner Society's visual score system for emphysema subtyping and severity analysis. We trained and evaluated our algorithm using 9650 subjects from the COPDGene study. Our algorithm achieved the predictive accuracy at 52\%, outperforming a previously published method's accuracy of 45\%. In addition, the agreement between the predicted scores of our method and the visual scores was good, where the previous method obtained only moderate agreement. Our approach employs a regression training strategy to generate categorical labels while simultaneously producing high-resolution localized activation maps for visualizing the network predictions. By leveraging these dense activation maps, our method possesses the capability to compute the percentage of emphysema involvement per lung in addition to categorical severity scores. Furthermore, the proposed method extends its predictive capabilities beyond centrilobular emphysema to include paraseptal emphysema subtypes.
</details>
<details>
<summary>摘要</summary>
正确识别 chronic obstructive pulmonary disease (COPD) 中的肺脂肪病变Subtype和严重程度是管理 COPD 和病理多样性研究的关键。现有的手动分析方法具有劳动密集和主观性问题。为了解决这个挑战，我们提出了一个基于深度学习的方法，可以自动应用 Fleischner Society 的视觉分数系统来分类肺脂肪病变和严重程度的分析。我们在 COPDGene 研究中训练和评估了我们的算法，并取得了52%的预测精度，比之前已出版的方法的45%的精度高。此外，我们的方法可以生成高分辨率的本地启动图表，可以让用户视觉化网络预测结果。此外，我们的方法还可以在分割肺脂肪病变的中心lobular emphysema 之外，还可以分类para septal emphysema 的亚型。
</details></li>
</ul>
<hr>
<h2 id="Causal-Structure-Recovery-of-Linear-Dynamical-Systems-An-FFT-based-Approach"><a href="#Causal-Structure-Recovery-of-Linear-Dynamical-Systems-An-FFT-based-Approach" class="headerlink" title="Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach"></a>Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02571">http://arxiv.org/abs/2309.02571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mishfad Shaikh Veedu, James Melbourne, Murti V. Salapaka</li>
<li>for: 这种研究探讨了如何从时间序列数据中学习 causal effect，尤其是在存在时间依赖关系时。</li>
<li>methods: 该研究使用了频域逻辑（FD）来表示时间序列数据，并使用了多ivariate Wiener projection来重建系统的 causal 结构。</li>
<li>results: 研究发现，使用 FD 可以有效地实现 causal inference，并且可以使用 do-calculus 机制来实现类型Single-door（with cycles）等经典条件。此外，研究还发现，对于具有交互关系的系统，使用 multivariate Wiener projections 可以实现高效的图重建，并且可以避免时间域方法的缺点。<details>
<summary>Abstract</summary>
Learning causal effects from data is a fundamental and well-studied problem across science, especially when the cause-effect relationship is static in nature. However, causal effect is less explored when there are dynamical dependencies, i.e., when dependencies exist between entities across time. Identifying dynamic causal effects from time-series observations is computationally expensive when compared to the static scenario. We demonstrate that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. Since FFT accumulates all the time dependencies on every frequency, causal inference can be performed efficiently by considering the state variables as random variables at any given frequency. We additionally show that, for systems with interactions that are LTI, do-calculus machinery can be realized in the FD resulting in versions of the classical single-door (with cycles), front and backdoor criteria. We demonstrate, for a large class of problems, graph reconstruction using multivariate Wiener projections results in a significant computational advantage with $O(n)$ complexity over reconstruction algorithms such as the PC algorithm which has $O(n^q)$ complexity, where $q$ is the maximum neighborhood size. This advantage accrues due to some remarkable properties of the phase response of the frequency-dependent Wiener coefficients which is not present in any time-domain approach.
</details>
<details>
<summary>摘要</summary>
学习 causal effect from data 是科学中的基本和很受欢迎的问题，特别是当 causal relationship 是静态的时候。然而， causal effect 在动态依赖关系时 menos explored。从时间序列观察中Identifying dynamic causal effects 是计算expensive的，与静态场景相比。我们 demonstarte that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. 因为 FFT 汇聚所有时间依赖项，因此 causal inference 可以efficiently perform by considering the state variables as random variables at any given frequency. We additionally show that, for systems with interactions that are LTI, do-calculus machinery can be realized in the FD resulting in versions of the classical single-door (with cycles), front and backdoor criteria. We demonstrate, for a large class of problems, graph reconstruction using multivariate Wiener projections results in a significant computational advantage with $O(n)$ complexity over reconstruction algorithms such as the PC algorithm which has $O(n^q)$ complexity, where $q$ is the maximum neighborhood size. This advantage accrues due to some remarkable properties of the phase response of the frequency-dependent Wiener coefficients which is not present in any time-domain approach.
</details></li>
</ul>
<hr>
<h2 id="Sparse-Partitioning-Around-Medoids"><a href="#Sparse-Partitioning-Around-Medoids" class="headerlink" title="Sparse Partitioning Around Medoids"></a>Sparse Partitioning Around Medoids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02557">http://arxiv.org/abs/2309.02557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars Lenssen, Erich Schubert</li>
<li>for: 这篇论文主要关注的是用Partitioning Around Medoids（PAM）分 clustering技术，并且处理非对称的情况，以及大量的问题。</li>
<li>methods: 这篇论文使用的方法包括PAM、快速PAM和组合来提高问题的解决方案。</li>
<li>results: 这篇论文的结果显示，这种方法可以在实际应用中提供更好的解决方案，并且可以适应更大的问题。<details>
<summary>Abstract</summary>
Partitioning Around Medoids (PAM, k-Medoids) is a popular clustering technique to use with arbitrary distance functions or similarities, where each cluster is represented by its most central object, called the medoid or the discrete median. In operations research, this family of problems is also known as facility location problem (FLP). FastPAM recently introduced a speedup for large k to make it applicable for larger problems, but the method still has a runtime quadratic in N. In this chapter, we discuss a sparse and asymmetric variant of this problem, to be used for example on graph data such as road networks. By exploiting sparsity, we can avoid the quadratic runtime and memory requirements, and make this method scalable to even larger problems, as long as we are able to build a small enough graph of sufficient connectivity to perform local optimization. Furthermore, we consider asymmetric cases, where the set of medoids is not identical to the set of points to be covered (or in the interpretation of facility location, where the possible facility locations are not identical to the consumer locations). Because of sparsity, it may be impossible to cover all points with just k medoids for too small k, which would render the problem unsolvable, and this breaks common heuristics for finding a good starting condition. We, hence, consider determining k as a part of the optimization problem and propose to first construct a greedy initial solution with a larger k, then to optimize the problem by alternating between PAM-style "swap" operations where the result is improved by replacing medoids with better alternatives and "remove" operations to reduce the number of k until neither allows further improving the result quality. We demonstrate the usefulness of this method on a problem from electrical engineering, with the input graph derived from cartographic data.
</details>
<details>
<summary>摘要</summary>
分割附近中点（PAM，k-中点）是一种流行的聚类技术，可以用于任意距离函数或相似性，每个群由其中心对象代表，称为中点或离散中间值。在运维研究中，这家问题也称为设施位置问题（FLP）。Recently，FastPAM引入了一种加速方法，以便在更大的问题上应用，但该方法仍然有线性增长在N上。在这章中，我们讨论了一种稀疏和非对称的变体，用于应用于图数据，如公路网络。通过利用稀疏性，我们可以避免 quadratic runtime和内存需求，并使这种方法可扩展到更大的问题，只要我们能够构建一个具有足够连接度的小 graph。此外，我们考虑非对称情况，其中中点集不同于要覆盖的点集（或在设施位置问题中，可能的设施位置不同于消费者位置）。由于稀疏性，可能无法使用 too small k 覆盖所有点，这会使问题无解，并使常见的启动策略无效。我们因此考虑 determining k 为优化问题的一部分，并提议先构建一个大于 k 的欢迎初始解，然后通过 PAM 风格的 "swap" 操作和 "remove" 操作来优化问题，直到 neither 允许再提高结果质量。我们在电力工程中的一个问题上 demonstrate 了这种方法的有用性，其输入图由地图数据 derivation。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-for-Efficiently-Fine-tuning-Vision-Transformer-with-Encrypted-Images"><a href="#Domain-Adaptation-for-Efficiently-Fine-tuning-Vision-Transformer-with-Encrypted-Images" class="headerlink" title="Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images"></a>Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02556">http://arxiv.org/abs/2309.02556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teru Nagamori, Sayaka Shiota, Hitoshi Kiya</li>
<li>for: 这个论文应用于保持模型性能的维护和优化，特别是在使用trasformed data时。</li>
<li>methods: 本论文提出了一种基于vision transformer（ViT）的域 adapted fine-tuning方法，不会对模型的性能造成下降。</li>
<li>results: 在实验中，提出的方法能够防止模型的性能下降，甚至在使用加密图像的CIFAR-10和CIFAR-100 datasets上。<details>
<summary>Abstract</summary>
In recent years, deep neural networks (DNNs) trained with transformed data have been applied to various applications such as privacy-preserving learning, access control, and adversarial defenses. However, the use of transformed data decreases the performance of models. Accordingly, in this paper, we propose a novel method for fine-tuning models with transformed images under the use of the vision transformer (ViT). The proposed domain adaptation method does not cause the accuracy degradation of models, and it is carried out on the basis of the embedding structure of ViT. In experiments, we confirmed that the proposed method prevents accuracy degradation even when using encrypted images with the CIFAR-10 and CIFAR-100 datasets.
</details>
<details>
<summary>摘要</summary>
近年来，深度神经网络（DNNs）在使用修改后的数据进行训练后应用于了各种应用程序，如隐私保护学习、访问控制和敌意防御。然而，使用修改后的数据会导致模型的性能下降。因此，在这篇论文中，我们提出了一种基于视Transformer（ViT）的新方法，用于精细调整模型使用修改图像。我们的领域适应方法不会导致模型的精度下降，并且基于ViT的嵌入结构进行实现。在实验中，我们证明了该方法可以在使用加密图像的CIFAR-10和CIFAR-100数据集上预防精度下降。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-the-Impact-of-Self-Supervised-Pretraining-for-Diagnostic-Tasks-with-Radiological-Images"><a href="#A-Survey-of-the-Impact-of-Self-Supervised-Pretraining-for-Diagnostic-Tasks-with-Radiological-Images" class="headerlink" title="A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images"></a>A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02555">http://arxiv.org/abs/2309.02555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blake VanBerlo, Jesse Hoey, Alexander Wong</li>
<li>for: 本研究旨在summarizing recent research on using self-supervised pretraining for X-ray, computed tomography, magnetic resonance, and ultrasound imaging, 以便比较自主监督学习和完全监督学习在诊断任务中的表现。</li>
<li>methods: 本研究使用了自主监督学习方法，并对其与完全监督学习进行比较，以探讨自主监督学习在诊断任务中的表现。</li>
<li>results: 研究发现，自主监督学习通常在下游任务中表现更好于完全监督学习，特别是当无标例数量远大于标记数量时。此外，研究还提出了一些建议和实践方法，如结合临床知识和理论上正确的自主监督学习方法，评估在公共数据集上，扩大ultrasound领域的证据，并Characterizing自主监督学习的泛化性。<details>
<summary>Abstract</summary>
Self-supervised pretraining has been observed to be effective at improving feature representations for transfer learning, leveraging large amounts of unlabelled data. This review summarizes recent research into its usage in X-ray, computed tomography, magnetic resonance, and ultrasound imaging, concentrating on studies that compare self-supervised pretraining to fully supervised learning for diagnostic tasks such as classification and segmentation. The most pertinent finding is that self-supervised pretraining generally improves downstream task performance compared to full supervision, most prominently when unlabelled examples greatly outnumber labelled examples. Based on the aggregate evidence, recommendations are provided for practitioners considering using self-supervised learning. Motivated by limitations identified in current research, directions and practices for future study are suggested, such as integrating clinical knowledge with theoretically justified self-supervised learning methods, evaluating on public datasets, growing the modest body of evidence for ultrasound, and characterizing the impact of self-supervised pretraining on generalization.
</details>
<details>
<summary>摘要</summary>
自我超vision学习已被观察到可以提高特征表示，以便在转移学习中提高性能。这篇评论总结了最近关于这一点的研究，涉及到X射线、计算机Tomography、磁共振和ultrasound成像中的自我超vision学习，并对这些研究进行比较，以了解在诊断任务中（如分类和 segmentation）中的表现。最重要的发现是，自我超vision学习通常比全supervision学习提高下游任务性能，特别是当无标例大量出现于标例例子之上。根据总体证据，我们提供了对实践者使用自我超vision学习的建议，以及根据现有研究的局限性，未来研究的方向和实践。
</details></li>
</ul>
<hr>
<h2 id="Data-Aggregation-for-Hierarchical-Clustering"><a href="#Data-Aggregation-for-Hierarchical-Clustering" class="headerlink" title="Data Aggregation for Hierarchical Clustering"></a>Data Aggregation for Hierarchical Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02552">http://arxiv.org/abs/2309.02552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elki-project/elki">https://github.com/elki-project/elki</a></li>
<li>paper_authors: Erich Schubert, Andreas Lang</li>
<li>for: 使用 Hierarchical Agglomerative Clustering (HAC) 进行数据分 clustering，但是由于数据量大，需要使用有效的数据处理方法。</li>
<li>methods: 使用 BETULA 数据集成算法，一种稳定的数据分集方法，可以在具有限制的资源的系统上进行实时处理。</li>
<li>results: 使用 BETULA 数据集成算法可以在具有限制的资源的系统上进行 Hierarchical Agglomerative Clustering (HAC)，并且只需要小量的数据处理loss，从而实现对大规模数据集的探索分析。<details>
<summary>Abstract</summary>
Hierarchical Agglomerative Clustering (HAC) is likely the earliest and most flexible clustering method, because it can be used with many distances, similarities, and various linkage strategies. It is often used when the number of clusters the data set forms is unknown and some sort of hierarchy in the data is plausible. Most algorithms for HAC operate on a full distance matrix, and therefore require quadratic memory. The standard algorithm also has cubic runtime to produce a full hierarchy. Both memory and runtime are especially problematic in the context of embedded or otherwise very resource-constrained systems. In this section, we present how data aggregation with BETULA, a numerically stable version of the well known BIRCH data aggregation algorithm, can be used to make HAC viable on systems with constrained resources with only small losses on clustering quality, and hence allow exploratory data analysis of very large data sets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Continual-Improvement-of-Threshold-Based-Novelty-Detection"><a href="#Continual-Improvement-of-Threshold-Based-Novelty-Detection" class="headerlink" title="Continual Improvement of Threshold-Based Novelty Detection"></a>Continual Improvement of Threshold-Based Novelty Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02551">http://arxiv.org/abs/2309.02551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abe Ejilemele, Jorge Mendez-Mendez</li>
<li>for: 解决 neural network 在 dynamical 和 open-world 环境中探测未知类的问题</li>
<li>methods: 使用 linear search 和 leave-one-out cross-validation 自动选择阈值</li>
<li>results: 在 MNIST、Fashion MNIST 和 CIFAR-10 上提高总准确率<details>
<summary>Abstract</summary>
When evaluated in dynamic, open-world situations, neural networks struggle to detect unseen classes. This issue complicates the deployment of continual learners in realistic environments where agents are not explicitly informed when novel categories are encountered. A common family of techniques for detecting novelty relies on thresholds of similarity between observed data points and the data used for training. However, these methods often require manually specifying (ahead of time) the value of these thresholds, and are therefore incapable of adapting to the nature of the data. We propose a new method for automatically selecting these thresholds utilizing a linear search and leave-one-out cross-validation on the ID classes. We demonstrate that this novel method for selecting thresholds results in improved total accuracy on MNIST, Fashion MNIST, and CIFAR-10.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:在动态开放世界中，神经网络对未经见过的类型探测强度不足。这使得持续学习者在真实环境中部署不太可能。一种常见的新类探测技术是基于训练数据点和观察数据点之间的相似性阈值。然而，这些方法通常需要手动指定（在前期）阈值的值，因此无法适应数据的特点。我们提出了一种新的方法，利用线性搜索和留下一个类的批处理来自动选择阈值。我们示示了这种新方法可以在MNIST、Fashion MNIST和CIFAR-10上提高总准确率。
</details></li>
</ul>
<hr>
<h2 id="Structural-Concept-Learning-via-Graph-Attention-for-Multi-Level-Rearrangement-Planning"><a href="#Structural-Concept-Learning-via-Graph-Attention-for-Multi-Level-Rearrangement-Planning" class="headerlink" title="Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning"></a>Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02547">http://arxiv.org/abs/2309.02547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manav Kulshrestha, Ahmed H. Qureshi</li>
<li>for: 这种论文旨在提出一种深度学习方法，用于多层物体重新排序规划，以便让机器人在复杂和自由环境中与物体进行互动。</li>
<li>methods: 这种方法使用图注意力网络，以解决多层物体重新排序规划中的结构依赖关系。</li>
<li>results: 这种方法可以在不同的场景中，包括真实世界和虚拟环境中，对多层物体进行重新排序规划，并且可以具有更高的灵活性和效率。<details>
<summary>Abstract</summary>
Robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. Existing work focuses primarily on single-level rearrangement planning and, even if multiple levels exist, dependency relations among substructures are geometrically simpler, like tower stacking. We propose Structural Concept Learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. It is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. We compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding to achieve better performance, flexibility, and efficiency. The dataset, supplementary details, videos, and code implementation are available at: https://manavkulshrestha.github.io/scl
</details>
<details>
<summary>摘要</summary>
robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. existing work focuses primarily on single-level rearrangement planning, and even if multiple levels exist, the dependency relations among substructures are geometrically simpler, like tower stacking. we propose structural concept learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. it is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. we compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding to achieve better performance, flexibility, and efficiency. the dataset, supplementary details, videos, and code implementation are available at: https://manavkulshrestha.github.io/sclHere's the word-for-word translation of the text into Simplified Chinese:瑜珈机器人操作任务，如物体重新排序，对机器人与复杂且随机环境进行交互起着关键作用。现有工作主要集中在单级重新排序规划上，即使多级存在，也是更加简单的圆柱堆叠关系。我们提出了结构概念学习（SCL），一种基于深度学习的多级物体重新排序规划方法，通过图注意力网络来解决场景理解。SCL在自生成的 simulate 数据集上进行训练，可以处理未看过的场景，具有更多的对象和更高的结构复杂度，独立抽象出各自的子结构，以便在多个机器人上并发执行任务，并能够通过实际世界进行推广。我们与一系列经典和模型基于的基准进行比较，表明我们的方法能够充分利用场景理解，实现更高的性能、灵活性和效率。数据集、补充细节、视频和代码实现可以在：https://manavkulshrestha.github.io/scl 获取。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Bandsplit-Neural-Network-for-Cinematic-Audio-Source-Separation"><a href="#A-Generalized-Bandsplit-Neural-Network-for-Cinematic-Audio-Source-Separation" class="headerlink" title="A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation"></a>A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02539">http://arxiv.org/abs/2309.02539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karn N. Watcharasupat, Chih-Wei Wu, Yiwei Ding, Iroro Orife, Aaron J. Hipple, Phillip A. Williams, Scott Kramer, Alexander Lerch, William Wolcott</li>
<li>for: 这个论文的目的是提出一种可扩展的频谱分解模型，用于从音频混合中提取对话、音乐和特效的三个分配。</li>
<li>methods: 这个模型使用了 psycho-acoustic 频率缩放，并使用了基于信号噪声比和1-norm的损失函数。它还利用了共同编码器的信息共享特性，以降低训练和推理时的计算复杂度，提高分离性能，并允许在推理时进行轻松的拓展。</li>
<li>results: 该模型在 Divide and Remaster 数据集上达到了最佳性能，对话分配的性能超过了理想的噪声比面积。<details>
<summary>Abstract</summary>
Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with performance above the ideal ratio mask for the dialogue stem.
</details>
<details>
<summary>摘要</summary>
电影音频源分离是一个相对较新的子任务，旨在从它们的混合中提取对话束、音乐束和特效束。在这项工作中，我们开发了一种通用于任何完整或过complete的频谱分解的模型。我们使用了听觉动机驱动的频谱缩放，以便更可靠地提取特征。我们还提出了基于信号噪声比和1- нор的损失函数，以及在训练和推理过程中共享信息的公共编码器设计，以降低计算复杂性，提高分离性能，并在推理时提供灵活性。我们的最佳模型在Divide and Remaster数据集上达到了对话束性能的最佳状态，超过理想的噪声比面积。
</details></li>
</ul>
<hr>
<h2 id="Experience-and-Prediction-A-Metric-of-Hardness-for-a-Novel-Litmus-Test"><a href="#Experience-and-Prediction-A-Metric-of-Hardness-for-a-Novel-Litmus-Test" class="headerlink" title="Experience and Prediction: A Metric of Hardness for a Novel Litmus Test"></a>Experience and Prediction: A Metric of Hardness for a Novel Litmus Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02534">http://arxiv.org/abs/2309.02534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicos Isaak, Loizos Michael<br>for: 这篇论文的目的是提出一种基于机器学习的系统，用于评估Winograd schema的难度水平，并在大规模实验中证明该系统的可靠性和准确性。methods: 该系统采用两种不同的方法，即随机森林和深度学习（LSTM），以评估Winograd schema的难度水平。results: 该研究通过大规模实验表明，人类对Winograd schema的表现异常差异，并且该系统能够快速和准确地评估Winograd schema的难度水平。<details>
<summary>Abstract</summary>
In the last decade, the Winograd Schema Challenge (WSC) has become a central aspect of the research community as a novel litmus test. Consequently, the WSC has spurred research interest because it can be seen as the means to understand human behavior. In this regard, the development of new techniques has made possible the usage of Winograd schemas in various fields, such as the design of novel forms of CAPTCHAs.   Work from the literature that established a baseline for human adult performance on the WSC has shown that not all schemas are the same, meaning that they could potentially be categorized according to their perceived hardness for humans. In this regard, this \textit{hardness-metric} could be used in future challenges or in the WSC CAPTCHA service to differentiate between Winograd schemas.   Recent work of ours has shown that this could be achieved via the design of an automated system that is able to output the hardness-indexes of Winograd schemas, albeit with limitations regarding the number of schemas it could be applied on. This paper adds to previous research by presenting a new system that is based on Machine Learning (ML), able to output the hardness of any Winograd schema faster and more accurately than any other previously used method. Our developed system, which works within two different approaches, namely the random forest and deep learning (LSTM-based), is ready to be used as an extension of any other system that aims to differentiate between Winograd schemas, according to their perceived hardness for humans. At the same time, along with our developed system we extend previous work by presenting the results of a large-scale experiment that shows how human performance varies across Winograd schemas.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie，Winograd Schema Challenge（WSC）已成为研究社区中的一个重要测试方法。因此，WSC 已经激发了大量研究兴趣，因为它可以用来理解人类行为。在这种情况下，开发新技术使得Winograd schema可以在不同领域中应用，如设计新型 CAPTCHAs。根据文献中的基准数据，人类成人在 WSC 中的性能不同，这意味着不同的 Winograd schema 可能会有不同的抵抗度。因此，我们可以使用这个“抵抗度指标”来分类不同的 Winograd schema。我们的先前研究已经证明了可以通过设计自动化系统来输出 Winograd schema 的抵抗度指标，但是这种方法只能处理有限数量的 Winograd schema。在这篇论文中，我们提出了一种基于机器学习（ML）的新系统，可以快速和准确地输出 Winograd schema 的抵抗度指标。我们的系统采用了两种不同的方法，即随机森林和深度学习（LSTM），可以作为任何其他系统的扩展，以便在分类不同 Winograd schema 时使用。同时，我们也将我们开发的系统与先前的研究相结合，并发表了一项大规模的实验结果，以显示人类在不同 Winograd schema 中的性能如何变化。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-on-the-Probability-Simplex"><a href="#Diffusion-on-the-Probability-Simplex" class="headerlink" title="Diffusion on the Probability Simplex"></a>Diffusion on the Probability Simplex</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02530">http://arxiv.org/abs/2309.02530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Griffin Floto, Thorsteinn Jonsson, Mihai Nica, Scott Sanner, Eric Zhengyu Zhu</li>
<li>for: 本文提出了一种基于扩散模型的生成模型，以填充缺失数据。</li>
<li>methods: 该方法使用概率 simplicial 进行扩散处理，并使用 softmax 函数和 Ornstein-Unlenbeck 过程来实现。</li>
<li>results: 该方法可以生成连续的数据分布，并且可以自然地扩展到包括 bounded image generation 等应用。<details>
<summary>Abstract</summary>
Diffusion models learn to reverse the progressive noising of a data distribution to create a generative model. However, the desired continuous nature of the noising process can be at odds with discrete data. To deal with this tension between continuous and discrete objects, we propose a method of performing diffusion on the probability simplex. Using the probability simplex naturally creates an interpretation where points correspond to categorical probability distributions. Our method uses the softmax function applied to an Ornstein-Unlenbeck Process, a well-known stochastic differential equation. We find that our methodology also naturally extends to include diffusion on the unit cube which has applications for bounded image generation.
</details>
<details>
<summary>摘要</summary>
Diffusion模型学习将推进数据分布的进程逆转，创建生成模型。然而，数据的连续性和精度之间存在矛盾，这种矛盾可能会影响模型的性能。为了解决这种矛盾，我们提出了在概率 simpliciter 上进行Diffusion的方法。使用概率 simpliciter 自然地创建了点对应的分类概率分布的解释。我们的方法使用Ornstein-Unlenbeck过程和softmax函数。我们发现我们的方法也自然地扩展到包括Diffusion在单位立方体上，这有应用于 bounded image generation。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Adversarial-Training-Does-Not-Increase-Recourse-Costs"><a href="#Adaptive-Adversarial-Training-Does-Not-Increase-Recourse-Costs" class="headerlink" title="Adaptive Adversarial Training Does Not Increase Recourse Costs"></a>Adaptive Adversarial Training Does Not Increase Recourse Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02528">http://arxiv.org/abs/2309.02528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ian Hardy, Jayanth Yetukuri, Yang Liu</li>
<li>for: 本研究旨在探讨对 adversarial 训练的 adaptive 方法对 algorithmic recourse 成本的影响。</li>
<li>methods: 本研究使用了 adversarial 训练和 adaptive 训练方法，以 investigate 模型在不同攻击半径下的Robustness和 recourse 成本之间的关系。</li>
<li>results: 研究发现，使用 adaptive adversarial training 可以减少模型对攻击的脆弱性，但这并不会增加 algorithmic recourse 成本。这种方法可能为在 recoursability 重要的领域提供可Affordable的Robustness。<details>
<summary>Abstract</summary>
Recent work has connected adversarial attack methods and algorithmic recourse methods: both seek minimal changes to an input instance which alter a model's classification decision. It has been shown that traditional adversarial training, which seeks to minimize a classifier's susceptibility to malicious perturbations, increases the cost of generated recourse; with larger adversarial training radii correlating with higher recourse costs. From the perspective of algorithmic recourse, however, the appropriate adversarial training radius has always been unknown. Another recent line of work has motivated adversarial training with adaptive training radii to address the issue of instance-wise variable adversarial vulnerability, showing success in domains with unknown attack radii. This work studies the effects of adaptive adversarial training on algorithmic recourse costs. We establish that the improvements in model robustness induced by adaptive adversarial training show little effect on algorithmic recourse costs, providing a potential avenue for affordable robustness in domains where recoursability is critical.
</details>
<details>
<summary>摘要</summary>
最近的工作已经将敌意攻击方法和算法救济方法联系起来：两者都寻找输入实例中最小的变化，使模型的分类决策发生变化。已经证明，传统的敌意训练，即减少攻击者所用的恶意扰动，会增加生成的救济成本，与攻击训练半径的大小正相关。从算法救济的角度来看， however，合适的攻击训练半径一直未知。另一些最近的工作已经提出了适应式敌意训练，以Addressing the issue of instance-wise variable adversarial vulnerability，并在不同领域中获得了成功。这项工作研究了适应式敌意训练对算法救济成本的影响。我们证明了，通过适应式敌意训练提高模型的鲁棒性，对算法救济成本没有显著影响，这提供了可能的便宜鲁棒性途径，特别在知道攻击训练半径的情况下。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-CPU-and-GPU-Profiling-for-Deep-Learning-Models"><a href="#Comparative-Analysis-of-CPU-and-GPU-Profiling-for-Deep-Learning-Models" class="headerlink" title="Comparative Analysis of CPU and GPU Profiling for Deep Learning Models"></a>Comparative Analysis of CPU and GPU Profiling for Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02521">http://arxiv.org/abs/2309.02521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipesh Gyawali</li>
<li>for: 这个论文旨在研究用于训练深度神经网络时 CPU 和 GPU 的资源分配和消耗。</li>
<li>methods: 该论文使用 Pytorch 框架来实现深度神经网络的训练，并对 CPU 和 GPU 的运行时间和内存使用进行分析。</li>
<li>results: 研究显示，对于深度神经网络，GPU 的运行时间比 CPU 更低，但对于更简单的网络，GPU 的提升不太明显。<details>
<summary>Abstract</summary>
Deep Learning(DL) and Machine Learning(ML) applications are rapidly increasing in recent days. Massive amounts of data are being generated over the internet which can derive meaningful results by the use of ML and DL algorithms. Hardware resources and open-source libraries have made it easy to implement these algorithms. Tensorflow and Pytorch are one of the leading frameworks for implementing ML projects. By using those frameworks, we can trace the operations executed on both GPU and CPU to analyze the resource allocations and consumption. This paper presents the time and memory allocation of CPU and GPU while training deep neural networks using Pytorch. This paper analysis shows that GPU has a lower running time as compared to CPU for deep neural networks. For a simpler network, there are not many significant improvements in GPU over the CPU.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）和机器学习（ML）应用在最近几年内逐渐增加。互联网上生成的巨量数据可以通过ML和DL算法提取有意义的结果。硬件资源和开源库的出现使得实现这些算法变得更加容易。TensorFlow和PyTorch是实现ML项目的主要框架之一。通过使用这些框架，我们可以跟踪CPU和GPU上执行的操作，并分析资源分配和消耗。本文介绍了在使用PyTorch训练深度神经网络时，CPU和GPU的时间和内存分配。本文分析显示，在深度神经网络训练中，GPU的运行时间比CPU更低。对于简单的网络，GPU不比CPU具有显著提高。
</details></li>
</ul>
<hr>
<h2 id="Towards-User-Guided-Actionable-Recourse"><a href="#Towards-User-Guided-Actionable-Recourse" class="headerlink" title="Towards User Guided Actionable Recourse"></a>Towards User Guided Actionable Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02517">http://arxiv.org/abs/2309.02517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayanth Yetukuri, Ian Hardy, Yang Liu</li>
<li>for: This paper focuses on developing a tool for providing actionable recourse to users who are negatively impacted by machine learning models in critical fields such as healthcare, banking, and criminal justice.</li>
<li>methods: The paper proposes a gradient-based approach to capture user preferences via soft constraints in three simple forms: scoring continuous features, bounding feature values, and ranking categorical features.</li>
<li>results: The proposed approach, called User Preferred Actionable Recourse (UP-AR), is evaluated through extensive experiments to verify its effectiveness.<details>
<summary>Abstract</summary>
Machine Learning's proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user's actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experiments to verify the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-RL-via-Disentangled-Environment-and-Agent-Representations"><a href="#Efficient-RL-via-Disentangled-Environment-and-Agent-Representations" class="headerlink" title="Efficient RL via Disentangled Environment and Agent Representations"></a>Efficient RL via Disentangled Environment and Agent Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02435">http://arxiv.org/abs/2309.02435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Gmelin, Shikhar Bahl, Russell Mendonca, Deepak Pathak</li>
<li>for: 提高模型自适应RL算法的表示能力，通过 Agent 对环境的视觉知识，如形状或面具，进行权重调整。</li>
<li>methods: 使用简单的 auxillary 损失函数将 Agent 对环境的视觉知识 incorporated 到 RL 目标中，提高模型的表示能力。</li>
<li>results: 在 18 个不同的难度较高的视觉 simulate 环境中，模型表现出色，超过了现有的模型自由方法。Note: “模型自适应RL算法” refers to the fact that the model is able to adapt to the environment, and “视觉知识” refers to the knowledge of the agent’s visual perception of the environment.<details>
<summary>Abstract</summary>
Agents that are aware of the separation between themselves and their environments can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, such as its shape or mask, which is often inexpensive to obtain. This is incorporated into the RL objective using a simple auxiliary loss. We show that our method, Structured Environment-Agent Representations, outperforms state-of-the-art model-free approaches over 18 different challenging visual simulation environments spanning 5 different robots. Website at https://sear-rl.github.io/
</details>
<details>
<summary>摘要</summary>
Agent 可以掌握自己和环境的分离，可以利用这种理解来形成有效的视觉输入表示。我们提出一种学习这种结构化表示的方法，使用视觉知识，如机器人的形状或面具，这些知识通常很便宜获得。我们将这种方法 integrate 到 RL 目标中使用了一个简单的辅助损失。我们显示了我们的方法，结构化环境-代理表示（SEAR），在18个复杂的视觉 simulate 环境中，使用5种不同的机器人，超过了当前无模型的方法。更多信息可以在https://sear-rl.github.io/ 上查看。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Building-a-Winning-Team-Selecting-Source-Model-Ensembles-using-a-Submodular-Transferability-Estimation-Approach"><a href="#Building-a-Winning-Team-Selecting-Source-Model-Ensembles-using-a-Submodular-Transferability-Estimation-Approach" class="headerlink" title="Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach"></a>Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02429">http://arxiv.org/abs/2309.02429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vimal K B, Saketh Bachu, Tanmay Garg, Niveditha Lakshmi Narasimhan, Raghavan Konuru, Vineeth N Balasubramanian</li>
<li>For: 该研究旨在估计公共可用预训练模型的迁移性，以便在转移学习任务中选择合适的模型。* Methods: 该研究提出了一种基于最优 tranSport 的 suBmOdular tRaNsferability 度量（OSBORN），用于评估多个源模型集的迁移性。OSBORN 考虑了图像频谱差异、任务差异和模型集凝聚性，以提供可靠的迁移性估计。* Results: 该研究在图像分类和semantic segmentation任务上对 OSBORN 进行了性能评估，并与当前状态的metric MS-LEEP 和 E-LEEP 进行了比较。 results 表明，OSBORN 在这些任务中表现出色，并且在模型集迁移中提供了更可靠的估计。<details>
<summary>Abstract</summary>
Estimating the transferability of publicly available pretrained models to a target task has assumed an important place for transfer learning tasks in recent years. Existing efforts propose metrics that allow a user to choose one model from a pool of pre-trained models without having to fine-tune each model individually and identify one explicitly. With the growth in the number of available pre-trained models and the popularity of model ensembles, it also becomes essential to study the transferability of multiple-source models for a given target task. The few existing efforts study transferability in such multi-source ensemble settings using just the outputs of the classification layer and neglect possible domain or task mismatch. Moreover, they overlook the most important factor while selecting the source models, viz., the cohesiveness factor between them, which can impact the performance and confidence in the prediction of the ensemble. To address these gaps, we propose a novel Optimal tranSport-based suBmOdular tRaNsferability metric (OSBORN) to estimate the transferability of an ensemble of models to a downstream task. OSBORN collectively accounts for image domain difference, task difference, and cohesiveness of models in the ensemble to provide reliable estimates of transferability. We gauge the performance of OSBORN on both image classification and semantic segmentation tasks. Our setup includes 28 source datasets, 11 target datasets, 5 model architectures, and 2 pre-training methods. We benchmark our method against current state-of-the-art metrics MS-LEEP and E-LEEP, and outperform them consistently using the proposed approach.
</details>
<details>
<summary>摘要</summary>
公共可用预训练模型的评估对于目标任务中的转移学习任务已经占据了重要的地位。现有的努力提出了可以让用户从一 pool 中选择一个预训练模型，而无需 individually 精度 each model 和确定一个特定的模型。随着可用的预训练模型的数量和模型集的流行，也变得重要对多源模型的转移性进行研究。现有的努力研究了多源模型中的转移性，但是忽视了可能存在的领域或任务差异，以及选择源模型时最重要的因素——模型集成性因素，这可能会影响预测结果和置信度。为解决这些缺陷，我们提出了一种新的 Optimal tranSport-based suBmOdular tRaNsferability metric (OSBORN)，用于评估 Ensemble 模型下某个下游任务的转移性。OSBORN 共同考虑图像领域差异、任务差异和模型集成性，以提供可靠的转移性估计。我们在图像分类和 semantic segmentation 任务上测试了我们的方法，我们的设置包括 28 个源数据集、11 个目标数据集、5 个模型架构和 2 种预训练方法。我们对现有的 metric MS-LEEP 和 E-LEEP 进行比较，并在我们的方法中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Deep-Learning-Models-through-Tensorization-A-Comprehensive-Survey-and-Framework"><a href="#Enhancing-Deep-Learning-Models-through-Tensorization-A-Comprehensive-Survey-and-Framework" class="headerlink" title="Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework"></a>Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02428">http://arxiv.org/abs/2309.02428</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mhelal/TensorsPyBook">https://github.com/mhelal/TensorsPyBook</a></li>
<li>paper_authors: Manal Helal</li>
<li>for: 本文旨在介绍tensorization，一种将多维数据转换为简单的二维矩阵的方法，以便在线性代数学习算法中使用。</li>
<li>methods: 本文详细介绍了tensorization的过程，包括多维数据源、多方分析方法和其integration with Deep Neural Networks模型。</li>
<li>results: 本文的实验结果表明，使用多维数据和多方分析方法可以更好地捕捉数据之间的复杂关系，同时减少模型参数数量和加速处理速度。<details>
<summary>Abstract</summary>
The burgeoning growth of public domain data and the increasing complexity of deep learning model architectures have underscored the need for more efficient data representation and analysis techniques. This paper is motivated by the work of Helal (2023) and aims to present a comprehensive overview of tensorization. This transformative approach bridges the gap between the inherently multidimensional nature of data and the simplified 2-dimensional matrices commonly used in linear algebra-based machine learning algorithms. This paper explores the steps involved in tensorization, multidimensional data sources, various multiway analysis methods employed, and the benefits of these approaches. A small example of Blind Source Separation (BSS) is presented comparing 2-dimensional algorithms and a multiway algorithm in Python. Results indicate that multiway analysis is more expressive. Contrary to the intuition of the dimensionality curse, utilising multidimensional datasets in their native form and applying multiway analysis methods grounded in multilinear algebra reveal a profound capacity to capture intricate interrelationships among various dimensions while, surprisingly, reducing the number of model parameters and accelerating processing. A survey of the multi-away analysis methods and integration with various Deep Neural Networks models is presented using case studies in different domains.
</details>
<details>
<summary>摘要</summary>
“公共领域数据的急剧增长和深度学习模型的复杂化，使得更有效的数据表示和分析技术成为了必要。这篇论文受Helal（2023）的研究启发，旨在提供tensorization的全面概述。这种变革性的方法 bridge了数据的自然多维性和常用的两维矩阵在线性 алгебра学习算法之间的 gap。本文探讨tensorization的步骤、多维数据源、多向分析方法和其优点。在Python中，用二维算法和多向算法进行盲源分离（BSS）的小例子被提供，结果表明多向分析更加表达力。相反于维度恩恶的直觉，使用原始形式的多维数据和应用多向分析方法，根据多线性代数的基础，能够捕捉多维维度之间的复杂关系，同时减少模型参数量和加速处理。本文还提供了多向分析方法的报告和与不同领域的深度神经网络模型集成的案例研究。”
</details></li>
</ul>
<hr>
<h2 id="Cognitive-Architectures-for-Language-Agents"><a href="#Cognitive-Architectures-for-Language-Agents" class="headerlink" title="Cognitive Architectures for Language Agents"></a>Cognitive Architectures for Language Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02427">http://arxiv.org/abs/2309.02427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysymyth/awesome-language-agents">https://github.com/ysymyth/awesome-language-agents</a></li>
<li>paper_authors: Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths</li>
<li>for: 本文旨在提出一种新的语言智能代理模型，具有更强的语言理解和决策能力。</li>
<li>methods: 本文使用了大量自然语言处理技术和符号智能技术，以及最新的语言模型，为语言代理模型提供了更多的可能性和可能性。</li>
<li>results: 本文提出了一种名为“CoALA”的概念框架，用于系统化和总结现有的语言代理模型，并提出了未来语言代理模型的发展方向。<details>
<summary>Abstract</summary>
Recent efforts have incorporated large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning. However, these efforts have largely been piecemeal, lacking a systematic framework for constructing a fully-fledged language agent. To address this challenge, we draw on the rich history of agent design in symbolic artificial intelligence to develop a blueprint for a new wave of cognitive language agents. We first show that LLMs have many of the same properties as production systems, and recent efforts to improve their grounding or reasoning mirror the development of cognitive architectures built around production systems. We then propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework. Finally, we use the CoALA framework to highlight gaps and propose actionable directions toward more capable language agents in the future.
</details>
<details>
<summary>摘要</summary>
最近努力已经将大型语言模型（LLM）与外部资源（例如互联网）或内部控制流（例如提示链）结合使用，以完成需要固定或理解的任务。然而，这些努力一般都是偶发的，缺乏一个系统化的框架来建立完善的语言代理。为解决这个挑战，我们 drawing on the rich history of agent design in symbolic artificial intelligence，开发了一个蓝图来建立一波新的认知语言代理。我们首先表明LLM具有许多与生产系统相似的性质，而最近努力提高LLM的固定或理解与生产系统的发展相似。然后，我们提出了语言代理认知框架（CoALA），一种概念框架，用于系统化多种LLM基于的理解、固定、学习和决策方法，并将其视为语言代理在框架中的实例。最后，我们使用CoALA框架， highlighting 缺陷并提出了更加强大的语言代理未来的可行方向。
</details></li>
</ul>
<hr>
<h2 id="Monotone-Tree-Based-GAMI-Models-by-Adapting-XGBoost"><a href="#Monotone-Tree-Based-GAMI-Models-by-Adapting-XGBoost" class="headerlink" title="Monotone Tree-Based GAMI Models by Adapting XGBoost"></a>Monotone Tree-Based GAMI Models by Adapting XGBoost</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02426">http://arxiv.org/abs/2309.02426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Hu, Soroush Aramideh, Jie Chen, Vijayan N. Nair</li>
<li>for: 本研究使用机器学习建模来适应低阶函数ANOVA模型，包括主效应和次阶交互效应。这些GAMI（GAM + 交互）模型可以直接解释为函数主效应和交互效应的抽象图形。但是，现有的GAMI模型不能考虑幂射性要求。</li>
<li>methods: 本研究提出了一种基于搜索树的幂射性GAMI模型，称为幂射GAMI-Tree。该模型使用XGBoost算法进行适应，并使用筛选技术选择重要的交互效应。最后，使用XGBoost算法进行适应，并使用解析和纯化技术来获得幂射GAMI模型。</li>
<li>results: 使用 simulate datasets 进行测试，monotone GAMI-Tree 和 EBM 都可以使用割据函数来实现幂射性。但是，与主效应不同，交互效应通常不幂射。<details>
<summary>Abstract</summary>
Recent papers have used machine learning architecture to fit low-order functional ANOVA models with main effects and second-order interactions. These GAMI (GAM + Interaction) models are directly interpretable as the functional main effects and interactions can be easily plotted and visualized. Unfortunately, it is not easy to incorporate the monotonicity requirement into the existing GAMI models based on boosted trees, such as EBM (Lou et al. 2013) and GAMI-Lin-T (Hu et al. 2022). This paper considers models of the form $f(x)=\sum_{j,k}f_{j,k}(x_j, x_k)$ and develops monotone tree-based GAMI models, called monotone GAMI-Tree, by adapting the XGBoost algorithm. It is straightforward to fit a monotone model to $f(x)$ using the options in XGBoost. However, the fitted model is still a black box. We take a different approach: i) use a filtering technique to determine the important interactions, ii) fit a monotone XGBoost algorithm with the selected interactions, and finally iii) parse and purify the results to get a monotone GAMI model. Simulated datasets are used to demonstrate the behaviors of mono-GAMI-Tree and EBM, both of which use piecewise constant fits. Note that the monotonicity requirement is for the full model. Under certain situations, the main effects will also be monotone. But, as seen in the examples, the interactions will not be monotone.
</details>
<details>
<summary>摘要</summary>
近期研究使用机器学习建筑来适应低阶函数ANOVA模型，包括主效和二阶交互。这些GAMI（GAM + 交互）模型直接可以解释为函数主效和交互，可以轻松地图示和可见化。然而，不能容易地将幂质要求 incorporate 到现有的GAMI模型中，如EBM（Lou et al. 2013）和GAMI-Lin-T（Hu et al. 2022）。这篇论文考虑模型形式为 $f(x)=\sum_{j,k}f_{j,k}(x_j, x_k)$，并开发了幂质树基于XGBoost算法的幂质GAMI模型，称为幂质GAMI-Tree。使用XGBoost算法直接适应幂质模型是 straightforward。然而，适应模型仍然是黑盒模型。我们采取了不同的方法：一、使用筛选技术确定重要的交互项，二、使用选择的交互项适应幂质XGBoost算法，并 finally iii）解析和纯化结果，以获得幂质GAMI模型。模拟数据集用于示示幂质GAMI-Tree和EBM两者使用块状常量适应的行为。注意，幂质性要求是全模型的。在某些情况下，主效也可能是幂质的，但交互项通常不是幂质的。
</details></li>
</ul>
<hr>
<h2 id="On-the-Minimax-Regret-in-Online-Ranking-with-Top-k-Feedback"><a href="#On-the-Minimax-Regret-in-Online-Ranking-with-Top-k-Feedback" class="headerlink" title="On the Minimax Regret in Online Ranking with Top-k Feedback"></a>On the Minimax Regret in Online Ranking with Top-k Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02425">http://arxiv.org/abs/2309.02425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Zhang, Ambuj Tewari</li>
<li>for: 这篇论文关注在线排名算法的实时反馈问题，具体来说是在排名结果中提供反馈，并且只有top-$k$项被反馈。</li>
<li>methods: 这篇论文使用了partial monitoring技术来分析在线排名算法。</li>
<li>results: 论文提供了对top-$k$反馈模型下的全面 caracterization，并给出了一种高效的算法来实现最小最大偏差率。<details>
<summary>Abstract</summary>
In online ranking, a learning algorithm sequentially ranks a set of items and receives feedback on its ranking in the form of relevance scores. Since obtaining relevance scores typically involves human annotation, it is of great interest to consider a partial feedback setting where feedback is restricted to the top-$k$ items in the rankings. Chaudhuri and Tewari [2017] developed a framework to analyze online ranking algorithms with top $k$ feedback. A key element in their work was the use of techniques from partial monitoring. In this paper, we further investigate online ranking with top $k$ feedback and solve some open problems posed by Chaudhuri and Tewari [2017]. We provide a full characterization of minimax regret rates with the top $k$ feedback model for all $k$ and for the following ranking performance measures: Pairwise Loss, Discounted Cumulative Gain, and Precision@n. In addition, we give an efficient algorithm that achieves the minimax regret rate for Precision@n.
</details>
<details>
<summary>摘要</summary>
在在线排名中，一种学习算法顺序排序一组项目，并接收反馈的形式为相关性分数。由于获取相关性分数通常需要人工标注，因此对于partial feedback Setting而言，很有吸引力。查户和特ва里（2017）提出了在线排名算法的框架，并使用partial monitoring技术。在这篇论文中，我们进一步调查在线排名的top-$k$反馈模型，并解决了查户和特ва里（2017）提出的一些问题。我们为所有的$k$和以下排名性能指标提供了完整的 caracterization of minimax regret rate：Pairwise Loss、Discounted Cumulative Gain和Precision@n。此外，我们还提供了可效的算法，可以实现Precision@n的 minimax regret rate。
</details></li>
</ul>
<hr>
<h2 id="Maximum-Mean-Discrepancy-Meets-Neural-Networks-The-Radon-Kolmogorov-Smirnov-Test"><a href="#Maximum-Mean-Discrepancy-Meets-Neural-Networks-The-Radon-Kolmogorov-Smirnov-Test" class="headerlink" title="Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test"></a>Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02422">http://arxiv.org/abs/2309.02422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghoon Paik, Michael Celentano, Alden Green, Ryan J. Tibshirani</li>
<li>for: 这个论文是关于最大均值差（MMD）测试的一个报告，MMD 是一种基于 maximizing the mean difference over samples from one distribution $P$ versus another $Q$ 的非 Parametric 两个样本测试。</li>
<li>methods: 这个论文使用了 Radon  bounded variation（RBV）空间中的一个函数 $f$ 来定义 MMD，并且使用了 neural network 来优化这个测试。</li>
<li>results: 论文提出了一种基于 RBV 空间的 MMD 测试，称为 Radon-Kolmogorov-Smirnov（RKS）测试，并证明了这个测试有 asymptotically full power 和 asymptotic null distribution。 Additionally, the paper compares the RKS test with the more traditional kernel MMD test and discusses its strengths and weaknesses through extensive experiments.<details>
<summary>Abstract</summary>
Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. This allows us to leverage the power of modern deep learning toolkits to (approximately) optimize the criterion that underlies the RKS test. We prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out extensive experiments to elucidate the strengths and weakenesses of the RKS test versus the more traditional kernel MMD test.
</details>
<details>
<summary>摘要</summary>
“最大均值差（MMD）是一类非参数性两个样本测试，基于将一个分布 $P$ 与另一个分布 $Q$ 的样本进行比较，并对样本进行数据变换 $f$ 生成的最大均值差。我们 Drawing inspiration from recent work connecting functions of Radon bounded variation (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the Radon-Kolmogorov-Smirnov (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test - the function $f$ achieving the maximum mean difference - is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. This allows us to leverage the power of modern deep learning toolkits to (approximately) optimize the criterion that underlies the RKS test. We prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out extensive experiments to elucidate the strengths and weaknesses of the RKS test versus the more traditional kernel MMD test.”Note: Simplified Chinese is a standardized form of Chinese that is used in mainland China and is different from Traditional Chinese, which is used in Hong Kong, Taiwan, and other regions.
</details></li>
</ul>
<hr>
<h2 id="Computing-SHAP-Efficiently-Using-Model-Structure-Information"><a href="#Computing-SHAP-Efficiently-Using-Model-Structure-Information" class="headerlink" title="Computing SHAP Efficiently Using Model Structure Information"></a>Computing SHAP Efficiently Using Model Structure Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02417">http://arxiv.org/abs/2309.02417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Hu, Ke Wang</li>
<li>for: 这个论文的目的是提出一种可以快速计算 SHAP 值的方法，以解决 SHAP 计算的时间复杂度问题。</li>
<li>methods: 这篇论文提出了三种方法来计算 SHAP 值，包括基于函数分解的方法、基于模型结构信息的方法和一种迭代方法。</li>
<li>results: 这些方法可以快速计算 SHAP 值，并且可以适用于不同的模型结构信息。对比 Castor &amp; Gomez (2008) 的采样方法，这些方法显示更高的准确性和效率。<details>
<summary>Abstract</summary>
SHAP (SHapley Additive exPlanations) has become a popular method to attribute the prediction of a machine learning model on an input to its features. One main challenge of SHAP is the computation time. An exact computation of Shapley values requires exponential time complexity. Therefore, many approximation methods are proposed in the literature. In this paper, we propose methods that can compute SHAP exactly in polynomial time or even faster for SHAP definitions that satisfy our additivity and dummy assumptions (eg, kernal SHAP and baseline SHAP). We develop different strategies for models with different levels of model structure information: known functional decomposition, known order of model (defined as highest order of interaction in the model), or unknown order. For the first case, we demonstrate an additive property and a way to compute SHAP from the lower-order functional components. For the second case, we derive formulas that can compute SHAP in polynomial time. Both methods yield exact SHAP results. Finally, if even the order of model is unknown, we propose an iterative way to approximate Shapley values. The three methods we propose are computationally efficient when the order of model is not high which is typically the case in practice. We compare with sampling approach proposed in Castor & Gomez (2008) using simulation studies to demonstrate the efficacy of our proposed methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="First-and-zeroth-order-implementations-of-the-regularized-Newton-method-with-lazy-approximated-Hessians"><a href="#First-and-zeroth-order-implementations-of-the-regularized-Newton-method-with-lazy-approximated-Hessians" class="headerlink" title="First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians"></a>First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02412">http://arxiv.org/abs/2309.02412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Doikov, Geovani Nunes Grapiglia</li>
<li>for: 这个论文旨在解决通用非凸优化问题，提出了一种基于Cubically regularized Newton method的first-order（Hessian-free）和zero-order（derivative-free）实现方法。</li>
<li>methods: 这些方法使用了finite difference approximations of the derivatives，并使用了特殊的适应搜索过程，以适应不同的迭代数。此外，它们还使用了lazys Hessian update，可以重用上一次计算的Hessianapproximation矩阵。</li>
<li>results: 作者证明了这些方法的全局复杂度上限为$\mathcal{O}( n^{1&#x2F;2} \epsilon^{-3&#x2F;2})$函数和梯度评估数，对于Hessian-free方法，以及$\mathcal{O}( n^{3&#x2F;2} \epsilon^{-3&#x2F;2} )$函数评估数，对于derivative-free方法。这些复杂度上限在先前知道的joint dependence on $n$和$\epsilon$上有所改善。<details>
<summary>Abstract</summary>
In this work, we develop first-order (Hessian-free) and zero-order (derivative-free) implementations of the Cubically regularized Newton method for solving general non-convex optimization problems. For that, we employ finite difference approximations of the derivatives. We use a special adaptive search procedure in our algorithms, which simultaneously fits both the regularization constant and the parameters of the finite difference approximations. It makes our schemes free from the need to know the actual Lipschitz constants. Additionally, we equip our algorithms with the lazy Hessian update that reuse a previously computed Hessian approximation matrix for several iterations. Specifically, we prove the global complexity bound of $\mathcal{O}( n^{1/2} \epsilon^{-3/2})$ function and gradient evaluations for our new Hessian-free method, and a bound of $\mathcal{O}( n^{3/2} \epsilon^{-3/2} )$ function evaluations for the derivative-free method, where $n$ is the dimension of the problem and $\epsilon$ is the desired accuracy for the gradient norm. These complexity bounds significantly improve the previously known ones in terms of the joint dependence on $n$ and $\epsilon$, for the first-order and zeroth-order non-convex optimization.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们开发了第一顺（无积分梯度法）和零顺（无导数法）实现方法，用于解决通用非凸优化问题。我们使用finite difference方法来估算derivatives。我们使用特殊的适应搜索过程，以同时适应正则化常数和finite difference估算的参数。这使得我们的算法不需要知道实际的Lipschitz常数。此外，我们将我们的算法备备了懒散的Hessian更新，可以重用多个迭代中计算的Hessian近似矩阵。我们证明了新的Hessian-free方法的全球复杂度上限为 $\mathcal{O}(n^{1/2} \epsilon^{-3/2})$ 函数和导数评估数，其中 $n$ 是问题的维度，$\epsilon$ 是求导数评估的精度。这些复杂度上限在之前已知的joint $n$ 和 $\epsilon$ 的依赖关系方面具有显著改善。
</details></li>
</ul>
<hr>
<h2 id="Delta-LoRA-Fine-Tuning-High-Rank-Parameters-with-the-Delta-of-Low-Rank-Matrices"><a href="#Delta-LoRA-Fine-Tuning-High-Rank-Parameters-with-the-Delta-of-Low-Rank-Matrices" class="headerlink" title="Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices"></a>Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02411">http://arxiv.org/abs/2309.02411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, Lei Zhang</li>
<li>for: 本研究提出了Delta-LoRA，一种能够高效地 Parametric efficient fine-tune大语言模型（LLMs）的新方法。</li>
<li>methods: Delta-LoRA不仅更新了低级矩阵 $\bA$ 和 $\bB$，还将学习传播到预训练 веса $\bW$ 上，通过两低级矩阵的乘积 delta（$\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$）来进行更新。这种策略有效地解决了递归更新低级矩阵的限制，使得 delta-LoRA 可以学习出适合下游任务的表示。</li>
<li>results: 对比LoRA和其他低级适应方法，Delta-LoRA在多种任务上表现出色，并且与LoRA相比，Delta-LoRA具有相同的内存需求和计算成本。<details>
<summary>Abstract</summary>
In this paper, we present Delta-LoRA, which is a novel parameter-efficient approach to fine-tune large language models (LLMs). In contrast to LoRA and other low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates the low-rank matrices $\bA$ and $\bB$, but also propagate the learning to the pre-trained weights $\bW$ via updates utilizing the delta of the product of two low-rank matrices ($\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$). Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks. Moreover, as the update of $\bW$ does not need to compute the gradients of $\bW$ and store their momentums, Delta-LoRA shares comparable memory requirements and computational costs with LoRA. Extensive experiments show that Delta-LoRA significantly outperforms existing low-rank adaptation methods. We further support these results with comprehensive analyses that underscore the effectiveness of Delta-LoRA.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Delta-LoRA，这是一种新的精简型 parameter-efficient 方法，用于精细调整大型语言模型（LLM）。与LoRA和其他低级别适应方法，如AdaLoRA，相比，Delta-LoRA不仅更新了低级别矩阵 $\bA$ 和 $\bB$，还通过利用低级别矩阵 $\bA^{(t+1)}$ 和 $\bB^{(t+1)}$ 的乘积的 delta 进行更新，从而有效地解决了适应过程中低级别矩阵的增量更新是不够的问题。此外，因为更新 $\bW$ 不需要计算 $\bW$ 的梯度和存储它们的动量，Delta-LoRA 与 LoRA 的内存需求和计算成本相似。实验结果表明，Delta-LoRA 明显超过了现有的低级别适应方法。我们还提供了详细的分析，以证明 Delta-LoRA 的效果。
</details></li>
</ul>
<hr>
<h2 id="In-Ear-Voice-Towards-Milli-Watt-Audio-Enhancement-With-Bone-Conduction-Microphones-for-In-Ear-Sensing-Platforms"><a href="#In-Ear-Voice-Towards-Milli-Watt-Audio-Enhancement-With-Bone-Conduction-Microphones-for-In-Ear-Sensing-Platforms" class="headerlink" title="In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms"></a>In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02393">http://arxiv.org/abs/2309.02393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Schilk, Niccolò Polvani, Andrea Ronco, Milos Cernak, Michele Magno</li>
<li>for: 减少耳机中的噪音和扭曲声音，提高耳机中的语音识别率。</li>
<li>methods: 使用新型MEMS骨尖振荡 Microphone，实现个性化语音活动检测和更高精度的声音提升应用。基于骨尖振荡数据和循环神经网络的低功耗个性化语音检测算法，与传统 Microphone 输入的方法进行比较。</li>
<li>results: 使用骨尖振荡 Microphone 实现的个性化语音检测算法，可以在 12.8ms 内准确地检测到语音，准确率为 95%。使用 Ambiq Apollo 4 Blue SoC 实现的最终实现可以在 2.64mW 的平均功耗下达到 43h 的电池寿命，没有循环征逐。<details>
<summary>Abstract</summary>
The recent ubiquitous adoption of remote conferencing has been accompanied by omnipresent frustration with distorted or otherwise unclear voice communication. Audio enhancement can compensate for low-quality input signals from, for example, small true wireless earbuds, by applying noise suppression techniques. Such processing relies on voice activity detection (VAD) with low latency and the added capability of discriminating the wearer's voice from others - a task of significant computational complexity. The tight energy budget of devices as small as modern earphones, however, requires any system attempting to tackle this problem to do so with minimal power and processing overhead, while not relying on speaker-specific voice samples and training due to usability concerns.   This paper presents the design and implementation of a custom research platform for low-power wireless earbuds based on novel, commercial, MEMS bone-conduction microphones. Such microphones can record the wearer's speech with much greater isolation, enabling personalized voice activity detection and further audio enhancement applications. Furthermore, the paper accurately evaluates a proposed low-power personalized speech detection algorithm based on bone conduction data and a recurrent neural network running on the implemented research platform. This algorithm is compared to an approach based on traditional microphone input. The performance of the bone conduction system, achieving detection of speech within 12.8ms at an accuracy of 95\% is evaluated. Different SoC choices are contrasted, with the final implementation based on the cutting-edge Ambiq Apollo 4 Blue SoC achieving 2.64mW average power consumption at 14uJ per inference, reaching 43h of battery life on a miniature 32mAh li-ion cell and without duty cycling.
</details>
<details>
<summary>摘要</summary>
最近广泛的远程会议普及，伴随着各种不清晰或扭曲的声音通话的感知不满。声音增强可以补做小型真 wireless earbuds 的低质量输入信号，通过应用隐藏噪声技术。这种处理需要快速响应，并能够分辨使用者的声音和其他声音。这是计算机科学的复杂任务。由于现代耳机的能量预算很低，任何系统都不能依赖于使用者的声音样本和训练。这篇论文描述了一个自定义研究平台，基于新型 MEMS 骨尘扬icrophones。这些 microphones 可以记录使用者的说话，并提供个性化声音活动检测和其他声音增强应用。此外，论文还评估了一种低功耗个性化speech检测算法，基于骨尘数据和回归神经网络。这种算法与传统 Mikrophone 输入的方法进行比较。骨尘系统的性能，包括检测speech within 12.8ms 的精度为 95%，被评估。不同的 SoC 选择被比较，最终实现基于cutting-edge Ambiq Apollo 4 Blue SoC 的实现，具有2.64mW 的平均电功耗和14uJ 的推理消耗，可以在32mAh 的锂离子电池上支持43h 的电池寿命，无需循环停止。
</details></li>
</ul>
<hr>
<h2 id="Explaining-grokking-through-circuit-efficiency"><a href="#Explaining-grokking-through-circuit-efficiency" class="headerlink" title="Explaining grokking through circuit efficiency"></a>Explaining grokking through circuit efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02390">http://arxiv.org/abs/2309.02390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, Ramana Kumar</li>
<li>for: 本研究旨在解释神经网络通用化的一个意外的谜题：吸收（grokking）。</li>
<li>methods: 本文提出了一种解释吸收的理论，即任务存在一个通用解和一个记忆解，其中通用解需要更长的学习时间但更高效，生成更大的logits具有相同的参数范数。</li>
<li>results: 本文提出了四个新预测，并对吸收进行了证明。其中最引人注目的是两种新的行为：半吸收和半通用。半吸收指的是网络从完美到低测试准确率的回卷，而半通用指的是网络在部分测试数据上显示延迟的通用。<details>
<summary>Abstract</summary>
One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.
</details>
<details>
<summary>摘要</summary>
一种非常有趣的神经网络泛化问题是"grokking"：一个网络在完美训练后却表现出低泛化性能。我们提出，grokking发生在任务允许一种泛化解决方案和一种记忆解决方案，其中泛化解决方案需要更长时间学习，但生成更大的logits，同样的参数范数。我们假设记忆ircuits在更大的训练集上变得更不效率，而泛化ircuits不变。这表明存在一个关键的训练集大小，在这个大小上，记忆和泛化是相同效率的。我们提出并证明了四个新预测，提供了重要的证据支持我们的解释。最引人注目的是我们发现了两种新和意外的行为：ungrokking，在完美训练后网络测试准确率下降，以及半grokking，在部分测试集上网络显示延迟的泛化。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-and-Transferable-Design-for-Robust-LEGO-Manipulation"><a href="#A-Lightweight-and-Transferable-Design-for-Robust-LEGO-Manipulation" class="headerlink" title="A Lightweight and Transferable Design for Robust LEGO Manipulation"></a>A Lightweight and Transferable Design for Robust LEGO Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02354">http://arxiv.org/abs/2309.02354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixuan Liu, Yifan Sun, Changliu Liu</li>
<li>for: 这个论文旨在解决机器人LEGO搬运问题，即使用硬件-软件共设以实现安全高效的LEGO搬运。</li>
<li>methods: 该论文使用了进化策略来安全优化机器人运动，并设计了一个终端工具（EOAT），以降低问题维度并使大型工业机器人更容易搬运LEGO brick。</li>
<li>results: 实验表明，EOAT可靠地进行LEGO brick搬运，而学习框架可以efficiently和安全地提高搬运性能，达到100%成功率。该共设在多个机器人（FANUC LR-mate 200id&#x2F;7L和Yaskawa GP4）上进行了多机器人演示，显示其可以准确地搬运不同的LEGO拼接品。<details>
<summary>Abstract</summary>
LEGO is a well-known platform for prototyping pixelized objects. However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement. This paper investigates safe and efficient robotic LEGO manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks. In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation. Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100\% success rate. The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the proposed solution enables sustainable robotic LEGO prototyping, in which the robot can repeatedly assemble and disassemble different prototypes.
</details>
<details>
<summary>摘要</summary>
LEGO 是一个很好的平台 для板图物体的原型设计。然而，机器人LEGO 板图处理（即LEGO 积木的操作）具有紧密的连接和精度要求，这使得机器人LEGO 板图处理变得困难。这篇论文探讨了安全和高效的机器人LEGO 板图处理方法。特别是，这篇论文通过硬件软件共设计来降低处理复杂性。一个端部工具（EOAT）被设计出来，可以减少问题维度，使大型工业机器人更容易地操作LEGO 积木。此外，这篇论文使用进化策略来安全地优化机器人的运动路径，以实现100%的成功率。实验表明，EOAT可靠地操作LEGO 积木，并且学习框架可以安全地提高操作性能。此外，我们在多个机器人（即FANUC LR-mate 200id/7L和Yaskawa GP4）上部署了这种共设计，以示其通用性和传输性。最后，我们显示了我们的解决方案可以实现可持续的机器人LEGO 板图处理，在这种情况下，机器人可以不断地组装和解 assemble不同的原型。
</details></li>
</ul>
<hr>
<h2 id="Exact-Inference-for-Continuous-Time-Gaussian-Process-Dynamics"><a href="#Exact-Inference-for-Continuous-Time-Gaussian-Process-Dynamics" class="headerlink" title="Exact Inference for Continuous-Time Gaussian Process Dynamics"></a>Exact Inference for Continuous-Time Gaussian Process Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02351">http://arxiv.org/abs/2309.02351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Ensinger, Nicholas Tagliapietra, Sebastian Ziesche, Sebastian Trimpe</li>
<li>for: 本文旨在学习连续时间动力系统。</li>
<li>methods: 本文使用高斯过程（GP）动力模型学习方法，并利用多步和泰勒 интегратор来实现直接推断。</li>
<li>results: 本文提出了一种flexible推断方案，可以准确表示连续时间动力系统。经验和理论分析表明，该方法可以准确表示连续时间动力系统。<details>
<summary>Abstract</summary>
Physical systems can often be described via a continuous-time dynamical system. In practice, the true system is often unknown and has to be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved. Thus, we aim for a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. Many higher-order integrators require dynamics evaluations at intermediate time steps making exact GP inference intractable. In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. In order to make direct inference tractable, we propose to leverage multistep and Taylor integrators. We demonstrate how to derive flexible inference schemes for these types of integrators. Further, we derive tailored sampling schemes that allow to draw consistent dynamics functions from the learned posterior. This is crucial to sample consistent predictions from the dynamics model. We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.
</details>
<details>
<summary>摘要</summary>
Physical systems can often be described using a continuous-time dynamical system. In practice, the true system is often unknown and must be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties must be conserved. Therefore, we aim to learn a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. Many higher-order integrators require dynamics evaluations at intermediate time steps, making exact GP inference intractable. In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. To make direct inference tractable, we propose to leverage multistep and Taylor integrators. We derive how to derive flexible inference schemes for these types of integrators. Furthermore, we derive tailored sampling schemes that allow drawing consistent dynamics functions from the learned posterior. This is crucial to sample consistent predictions from the dynamics model. We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.Translated by Google Translate with some modifications to improve readability.
</details></li>
</ul>
<hr>
<h2 id="PolyLUT-Learning-Piecewise-Polynomials-for-Ultra-Low-Latency-FPGA-LUT-based-Inference"><a href="#PolyLUT-Learning-Piecewise-Polynomials-for-Ultra-Low-Latency-FPGA-LUT-based-Inference" class="headerlink" title="PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference"></a>PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02334">http://arxiv.org/abs/2309.02334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Andronic, George A. Constantinides</li>
<li>for: 这个论文主要用于描述一种基于多变量多项式的神经网络训练方法，以实现在场程序可编程阵列（FPGAs）上进行深度学习推理的快速实现。</li>
<li>methods: 该方法使用多变量多项式作为神经网络的基本构建块，利用FPGA的柔logic隐藏多项式评估所带来的额外开销。</li>
<li>results: 该方法可以在三个任务中实现同等准确性，但使用许多 fewer layers of soft logic，从而实现了明显的响应时间和面积改进。<details>
<summary>Abstract</summary>
Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. We demonstrate the effectiveness of this approach in three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and handwritten digit recognition using the MNIST dataset.
</details>
<details>
<summary>摘要</summary>
“Field-programmable gate arrays (FPGAs) 广泛应用于深度学习推理。标准深度神经网络推理包括交叠的线性映射和非线性活动函数的计算。现有的工作是将线性映射和非线性活动函数硬编码到 FPGA  lookup tables (LUTs) 中。我们的工作是基于线性映射和非线性活动函数可以在 FPGA 中实现更多的功能的想法。在这篇论文中，我们提出了一种新的方法，用多变量波动函数作为深度神经网络的基本构建块进行训练。我们的方法利用了FPGA 的软逻辑的灵活性，将波动函数评估隐藏在 LUTs 中，无损耗。我们表明，使用波动构建块可以与使用线性函数相比，减少许多层次软逻辑的数量，导致了显著的延迟和面积改善。我们在三个任务中证明了这种方法的有效性：网络侵入检测、CERN Large Hadron Collider 中的截割识别和 MNIST 数据集上的手写数字识别。”
</details></li>
</ul>
<hr>
<h2 id="Resilient-VAE-Unsupervised-Anomaly-Detection-at-the-SLAC-Linac-Coherent-Light-Source"><a href="#Resilient-VAE-Unsupervised-Anomaly-Detection-at-the-SLAC-Linac-Coherent-Light-Source" class="headerlink" title="Resilient VAE: Unsupervised Anomaly Detection at the SLAC Linac Coherent Light Source"></a>Resilient VAE: Unsupervised Anomaly Detection at the SLAC Linac Coherent Light Source</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02333">http://arxiv.org/abs/2309.02333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Humble, William Colocho, Finn O’Shea, Daniel Ratner, Eric Darve</li>
<li>for: 这个论文旨在运用深度学习进行异常检测，并且不需要正常训练集（即没有异常数据）或完整的标签训练集。</li>
<li>methods: 这篇论文提出了一种名为Resilient Variational Autoencoder（ResVAE）的深度生成模型，它能够在对于异常数据进行训练时，抵制异常数据的影响，并提供特征层级的异常属性。</li>
<li>results: 在SLAC Linac Coherent Light Source（LCLS）的加速器状态测量系统中应用ResVAE方法，可以实现各种不同类型的异常检测。<details>
<summary>Abstract</summary>
Significant advances in utilizing deep learning for anomaly detection have been made in recent years. However, these methods largely assume the existence of a normal training set (i.e., uncontaminated by anomalies) or even a completely labeled training set. In many complex engineering systems, such as particle accelerators, labels are sparse and expensive; in order to perform anomaly detection in these cases, we must drop these assumptions and utilize a completely unsupervised method. This paper introduces the Resilient Variational Autoencoder (ResVAE), a deep generative model specifically designed for anomaly detection. ResVAE exhibits resilience to anomalies present in the training data and provides feature-level anomaly attribution. During the training process, ResVAE learns the anomaly probability for each sample as well as each individual feature, utilizing these probabilities to effectively disregard anomalous examples in the training data. We apply our proposed method to detect anomalies in the accelerator status at the SLAC Linac Coherent Light Source (LCLS). By utilizing shot-to-shot data from the beam position monitoring system, we demonstrate the exceptional capability of ResVAE in identifying various types of anomalies that are visible in the accelerator.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习在异常检测方面作出了 significante进步。然而，这些方法假设存在一个正常的训练集（即不受异常影响）或者完全标注的训练集。在许多复杂的工程系统中，如粒子加速器，标签是稀缺的并且昂贵的；因此，在这些情况下，我们必须放弃这些假设，并使用一种完全无监督的方法进行异常检测。这篇文章介绍了一种名为 Resilient Variational Autoencoder（ResVAE）的深度生成模型，特别是设计用于异常检测。ResVAE具有对异常示例的抗性，并提供了每个样本和每个特征的异常投入量。在训练过程中，ResVAE learns the anomaly probability for each sample as well as each individual feature, utilizing these probabilities to effectively disregard anomalous examples in the training data. We apply our proposed method to detect anomalies in the accelerator status at the SLAC Linac Coherent Light Source (LCLS) by utilizing shot-to-shot data from the beam position monitoring system. We demonstrate the exceptional capability of ResVAE in identifying various types of anomalies that are visible in the accelerator.
</details></li>
</ul>
<hr>
<h2 id="Information-Processing-by-Neuron-Populations-in-the-Central-Nervous-System-Mathematical-Structure-of-Data-and-Operations"><a href="#Information-Processing-by-Neuron-Populations-in-the-Central-Nervous-System-Mathematical-Structure-of-Data-and-Operations" class="headerlink" title="Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations"></a>Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02332">http://arxiv.org/abs/2309.02332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin N. P. Nilsson</li>
<li>for: 这个论文的目的是探索脑中神经元群体的准确编码和运算方法。</li>
<li>methods: 该论文使用了一种现代化的神经元模型，并利用了这个模型的准确性来描述神经元群体的运算方法。</li>
<li>results: 该论文发现，神经元群体可以被视为一种低级编程语言中的操作符，并且可以通过不同的连接来实现多种操作，如特殊化、泛化、新鲜性检测、维度减少、逆模型、预测和关联记忆等。<details>
<summary>Abstract</summary>
In the intricate architecture of the mammalian central nervous system, neurons form populations. Axonal bundles communicate between these clusters using spike trains as their medium. However, these neuron populations' precise encoding and operations have yet to be discovered. In our analysis, the starting point is a state-of-the-art mechanistic model of a generic neuron endowed with plasticity. From this simple framework emerges a profound mathematical construct: The representation and manipulation of information can be precisely characterized by an algebra of finite convex cones. Furthermore, these neuron populations are not merely passive transmitters. They act as operators within this algebraic structure, mirroring the functionality of a low-level programming language. When these populations interconnect, they embody succinct yet potent algebraic expressions. These networks allow them to implement many operations, such as specialization, generalization, novelty detection, dimensionality reduction, inverse modeling, prediction, and associative memory. In broader terms, this work illuminates the potential of matrix embeddings in advancing our understanding in fields like cognitive science and AI. These embeddings enhance the capacity for concept processing and hierarchical description over their vector counterparts.
</details>
<details>
<summary>摘要</summary>
在哺乳动物中心神经系统的复杂建筑中，神经细胞形成群体。轴突集合通过快速列表传输信息。然而，这些神经细胞群体的精确编码和操作仍未得到了发现。在我们的分析中，起始点是一种现代机制模型，这种模型拥有пластично性。从这个简单的框架中，出现了一种深刻的数学构造：神经细胞群体可以精确地表示和操纵信息，这可以通过一种Finite convex cone的代数来表示。此外，这些神经细胞群体不仅是被动的传输器。它们作为代数中的操作符，模仿低级编程语言的功能。当这些群体相互连接时，它们实现了简洁却强大的代数表达。这些网络允许它们实现许多操作，如特殊化、通用化、发现新的特征、维度减少、反向模型、预测和相关记忆。在更广泛的意义上，这项工作推照了矩阵嵌入在认知科学和人工智能领域的潜在作用。这些嵌入可以提高概念处理和层次描述的能力，而不是 vector 的对应。
</details></li>
</ul>
<hr>
<h2 id="SeisCLIP-A-seismology-foundation-model-pre-trained-by-multi-modal-data-for-multi-purpose-seismic-feature-extraction"><a href="#SeisCLIP-A-seismology-foundation-model-pre-trained-by-multi-modal-data-for-multi-purpose-seismic-feature-extraction" class="headerlink" title="SeisCLIP: A seismology foundation model pre-trained by multi-modal data for multi-purpose seismic feature extraction"></a>SeisCLIP: A seismology foundation model pre-trained by multi-modal data for multi-purpose seismic feature extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02320">http://arxiv.org/abs/2309.02320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sixu0/SeisCLIP">https://github.com/sixu0/SeisCLIP</a></li>
<li>paper_authors: Xu Si, Xinming Wu, Hanlin Sheng, Jun Zhu, Zefeng Li</li>
<li>for: 这篇论文是为了提出一个基础模型，以应对各种地震学任务中的欠缺标签资料和地区对应性问题。</li>
<li>methods: 这篇论文使用了对比学习方法，将多Modal资料集合进行联合预训。具体来说，它使用了一个对应TransformerEncoder来提取时间频率地震谱中的重要特征，以及一个MLPEncoder来联合事件的相位和来源信息。</li>
<li>results: 这篇论文的结果显示，SeisCLIP的性能在不同区域的事件分类、地点定位和聚集机制分析任务中，都大于基准方法的性能。特别是，SeisCLIP可以在不同区域的资料上进行 Transfer Learning，并且可以对应不同的地震谱和事件类型。<details>
<summary>Abstract</summary>
Training specific deep learning models for particular tasks is common across various domains within seismology. However, this approach encounters two limitations: inadequate labeled data for certain tasks and limited generalization across regions. To address these challenges, we develop SeisCLIP, a seismology foundation model trained through contrastive learning from multi-modal data. It consists of a transformer encoder for extracting crucial features from time-frequency seismic spectrum and an MLP encoder for integrating the phase and source information of the same event. These encoders are jointly pre-trained on a vast dataset and the spectrum encoder is subsequently fine-tuned on smaller datasets for various downstream tasks. Notably, SeisCLIP's performance surpasses that of baseline methods in event classification, localization, and focal mechanism analysis tasks, employing distinct datasets from different regions. In conclusion, SeisCLIP holds significant potential as a foundational model in the field of seismology, paving the way for innovative directions in foundation-model-based seismology research.
</details>
<details>
<summary>摘要</summary>
通常在地震学中特定任务上训练特定的深度学习模型是常见的。然而，这种方法有两个限制：不够的标注数据 для某些任务和地区之间的限制。为解决这些挑战，我们开发了SeisCLIP，一个基于对比学习的地震学基础模型。它包括一个转换器编码器，用于提取时频地震谱中的关键特征，以及一个多层感知编码器，用于将相关的阶段和来源信息相结合。这两个编码器在大量数据集上共同培养，并且spectrum编码器在小型数据集上进行精细调整，以便在不同地区的下游任务中进行最佳化。值得注意的是，SeisCLIP的性能超过了基eline方法在事件分类、地点归属和焦点机制分析等任务中的性能，使用不同地区的数据集。总之，SeisCLIP具有广泛的应用前景，可能成为地震学研究中的基础模型，开拓出新的方向。
</details></li>
</ul>
<hr>
<h2 id="A-study-on-the-impact-of-pre-trained-model-on-Just-In-Time-defect-prediction"><a href="#A-study-on-the-impact-of-pre-trained-model-on-Just-In-Time-defect-prediction" class="headerlink" title="A study on the impact of pre-trained model on Just-In-Time defect prediction"></a>A study on the impact of pre-trained model on Just-In-Time defect prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02317">http://arxiv.org/abs/2309.02317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Guo, Xiaopeng Gao, Zhenyu Zhang, W. K. Chan, Bo Jiang</li>
<li>for: 本研究主要针对Just-In-Time（JIT）缺陷预测任务中，前期研究者主要关注单个预训模型的性能，而忽略了不同预训模型之间的关系。</li>
<li>methods: 本研究建立了六个模型：RoBERTaJIT、CodeBERTJIT、BARTJIT、PLBARTJIT、GPT2JIT和CodeGPTJIT，每个模型都使用了不同的预训模型作为 backing。我们系统地探讨这些模型之间的差异和连接。</li>
<li>results: 我们发现，使用 Commit 代码和 Commit 消息作为输入，每个模型都显示出了改进。此外，我们还发现，当预训模型的类型相似时，需要消耗的训练资源几乎相同。此外，我们还发现，在零shot和几shot情况下，不同的预训模型在缺陷检测中表现出色。这些结果为JIT缺陷预测任务中使用预训模型优化提供新的视角，并 highlight了需要更多注意的因素。<details>
<summary>Abstract</summary>
Previous researchers conducting Just-In-Time (JIT) defect prediction tasks have primarily focused on the performance of individual pre-trained models, without exploring the relationship between different pre-trained models as backbones. In this study, we build six models: RoBERTaJIT, CodeBERTJIT, BARTJIT, PLBARTJIT, GPT2JIT, and CodeGPTJIT, each with a distinct pre-trained model as its backbone. We systematically explore the differences and connections between these models. Specifically, we investigate the performance of the models when using Commit code and Commit message as inputs, as well as the relationship between training efficiency and model distribution among these six models. Additionally, we conduct an ablation experiment to explore the sensitivity of each model to inputs. Furthermore, we investigate how the models perform in zero-shot and few-shot scenarios. Our findings indicate that each model based on different backbones shows improvements, and when the backbone's pre-training model is similar, the training resources that need to be consumed are much more closer. We also observe that Commit code plays a significant role in defect detection, and different pre-trained models demonstrate better defect detection ability with a balanced dataset under few-shot scenarios. These results provide new insights for optimizing JIT defect prediction tasks using pre-trained models and highlight the factors that require more attention when constructing such models. Additionally, CodeGPTJIT and GPT2JIT achieved better performance than DeepJIT and CC2Vec on the two datasets respectively under 2000 training samples. These findings emphasize the effectiveness of transformer-based pre-trained models in JIT defect prediction tasks, especially in scenarios with limited training data.
</details>
<details>
<summary>摘要</summary>
previous researchers 在 Just-In-Time（JIT）缺陷预测任务中主要关注各种预训练模型的性能，而未探讨不同预训练模型之间的关系。在这个研究中，我们建立了六个模型：RoBERTaJIT、CodeBERTJIT、BARTJIT、PLBARTJIT、GPT2JIT和CodeGPTJIT，每个模型都有不同的预训练模型作为其背景。我们系统地探讨这些模型之间的差异和联系。特别是，我们研究使用 Commit 代码和 Commit 消息作为输入时模型的性能，以及这些模型在不同训练资源的情况下的关系。此外，我们进行了一项排除实验，以探讨每个模型对输入的敏感性。此外，我们还研究了这些模型在零处和几处场景下的性能。我们的发现表明，每个模型基于不同的背景都有改进，而当背景模型的预训练类似时，需要消耗的训练资源很接近。我们还发现， Commit 代码在缺陷检测中发挥了重要作用，不同的预训练模型在均衡数据集下的几处场景下表现出不同的缺陷检测能力。这些发现为优化 JIT 缺陷预测任务中使用预训练模型提供了新的视角，并高亮需要更多注意的因素。此外， CodeGPTJIT 和 GPT2JIT 在两个数据集上分别以 2000 个训练样本的情况下表现出了更好的性能，这些发现强调了基于 transformer 预训练模型的 JIT 缺陷预测任务的效iveness，特别是有限的训练数据 scenarios。
</details></li>
</ul>
<hr>
<h2 id="Graph-Self-Contrast-Representation-Learning"><a href="#Graph-Self-Contrast-Representation-Learning" class="headerlink" title="Graph Self-Contrast Representation Learning"></a>Graph Self-Contrast Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02304">http://arxiv.org/abs/2309.02304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GRAND-Lab/MERIT">https://github.com/GRAND-Lab/MERIT</a></li>
<li>paper_authors: Minjie Chen, Yao Cheng, Ye Wang, Xiang Li, Ming Gao</li>
<li>for: 提高图像表示学习的效果和效率。</li>
<li>methods: 提出一种新的图像自我对比框架GraphSC，只使用一个正例和一个负例，使用 triplet loss 作为目标函数。</li>
<li>results: 通过对 19 种现有方法进行比较，证明 GraphSC 在无监督和传输学习 Setting 中表现出色。<details>
<summary>Abstract</summary>
Graph contrastive learning (GCL) has recently emerged as a promising approach for graph representation learning. Some existing methods adopt the 1-vs-K scheme to construct one positive and K negative samples for each graph, but it is difficult to set K. For those methods that do not use negative samples, it is often necessary to add additional strategies to avoid model collapse, which could only alleviate the problem to some extent. All these drawbacks will undoubtedly have an adverse impact on the generalizability and efficiency of the model. In this paper, to address these issues, we propose a novel graph self-contrast framework GraphSC, which only uses one positive and one negative sample, and chooses triplet loss as the objective. Specifically, self-contrast has two implications. First, GraphSC generates both positive and negative views of a graph sample from the graph itself via graph augmentation functions of various intensities, and use them for self-contrast. Second, GraphSC uses Hilbert-Schmidt Independence Criterion (HSIC) to factorize the representations into multiple factors and proposes a masked self-contrast mechanism to better separate positive and negative samples. Further, Since the triplet loss only optimizes the relative distance between the anchor and its positive/negative samples, it is difficult to ensure the absolute distance between the anchor and positive sample. Therefore, we explicitly reduced the absolute distance between the anchor and positive sample to accelerate convergence. Finally, we conduct extensive experiments to evaluate the performance of GraphSC against 19 other state-of-the-art methods in both unsupervised and transfer learning settings.
</details>
<details>
<summary>摘要</summary>
graph contrastive learning (GCL) 近期 emerged as a promising approach for graph representation learning. some existing methods adopt the 1-vs-K scheme to construct one positive and K negative samples for each graph, but it is difficult to set K. for those methods that do not use negative samples, it is often necessary to add additional strategies to avoid model collapse, which could only alleviate the problem to some extent. all these drawbacks will undoubtedly have an adverse impact on the generalizability and efficiency of the model. in this paper, to address these issues, we propose a novel graph self-contrast framework GraphSC, which only uses one positive and one negative sample, and chooses triplet loss as the objective. specifically, self-contrast has two implications. first, GraphSC generates both positive and negative views of a graph sample from the graph itself via graph augmentation functions of various intensities, and use them for self-contrast. second, GraphSC uses Hilbert-Schmidt Independence Criterion (HSIC) to factorize the representations into multiple factors and proposes a masked self-contrast mechanism to better separate positive and negative samples. further, since the triplet loss only optimizes the relative distance between the anchor and its positive/negative samples, it is difficult to ensure the absolute distance between the anchor and positive sample. therefore, we explicitly reduced the absolute distance between the anchor and positive sample to accelerate convergence. finally, we conduct extensive experiments to evaluate the performance of GraphSC against 19 other state-of-the-art methods in both unsupervised and transfer learning settings.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Semantic-Communication-with-Deep-Generative-Models-–-An-ICASSP-Special-Session-Overview"><a href="#Enhancing-Semantic-Communication-with-Deep-Generative-Models-–-An-ICASSP-Special-Session-Overview" class="headerlink" title="Enhancing Semantic Communication with Deep Generative Models – An ICASSP Special Session Overview"></a>Enhancing Semantic Communication with Deep Generative Models – An ICASSP Special Session Overview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02478">http://arxiv.org/abs/2309.02478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Grassucci, Yuki Mitsufuji, Ping Zhang, Danilo Comminiello</li>
<li>for: 本研究探讨了 semantic communication 在Future AI-driven communication systems 中的潜在作用，以及如何使用 deep generative models  Addressing  semantic communication challenges.</li>
<li>methods: 本研究使用了 deep generative models 来解决 semantic communication 中的 complex data 提取和处理 semantic information 的挑战，并在实际应用中提供了 novel research pathways  для future semantic communication frameworks.</li>
<li>results: 本研究显示了 deep generative models 可以帮助 enhance semantic communication frameworks 在 dealing with real-world complex data 和 channel corruptions 上，并提供了 novel research pathways  для future semantic communication frameworks.<details>
<summary>Abstract</summary>
Semantic communication is poised to play a pivotal role in shaping the landscape of future AI-driven communication systems. Its challenge of extracting semantic information from the original complex content and regenerating semantically consistent data at the receiver, possibly being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper discloses the semantic communication challenges from the machine learning perspective and unveils how deep generative models will significantly enhance semantic communication frameworks in dealing with real-world complex data, extracting and exploiting semantic information, and being robust to channel corruptions. Alongside establishing this emerging field, this paper charts novel research pathways for the next generative semantic communication frameworks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTSemantic communication is poised to play a pivotal role in shaping the landscape of future AI-driven communication systems. Its challenge of extracting semantic information from the original complex content and regenerating semantically consistent data at the receiver, possibly being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper discloses the semantic communication challenges from the machine learning perspective and unveils how deep generative models will significantly enhance semantic communication frameworks in dealing with real-world complex data, extracting and exploiting semantic information, and being robust to channel corruptions. Alongside establishing this emerging field, this paper charts novel research pathways for the next generative semantic communication frameworks.TRANSLATE_TEXTSemantic communication is ready to play a crucial role in shaping the future of AI-driven communication systems. The challenge of extracting semantic information from complex content and generating consistent data at the receiver, while being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper reveals the challenges of semantic communication from a machine learning perspective and shows how deep generative models will significantly improve semantic communication frameworks in dealing with real-world complex data, extracting and utilizing semantic information, and being robust to channel corruptions. The paper also explores new research paths for the next generation of semantic communication frameworks.
</details></li>
</ul>
<hr>
<h2 id="Inferring-effective-couplings-with-Restricted-Boltzmann-Machines"><a href="#Inferring-effective-couplings-with-Restricted-Boltzmann-Machines" class="headerlink" title="Inferring effective couplings with Restricted Boltzmann Machines"></a>Inferring effective couplings with Restricted Boltzmann Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02292">http://arxiv.org/abs/2309.02292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alfonso-navas/inferring_effective_couplings_with_RBMs">https://github.com/alfonso-navas/inferring_effective_couplings_with_RBMs</a></li>
<li>paper_authors: Aurélien Decelle, Cyril Furtlehner, Alfonso De Jesus Navas Gómez, Beatriz Seoane</li>
<li>for: 该研究旨在提出一种简单的解决方案，以便更好地理解生成模型的物理含义。</li>
<li>methods: 该研究使用了一种直接映射Restricted Boltzmann Machine的能量函数和有效的Isling振荡 Hamiltonian，包括高阶交互between spins。</li>
<li>results: 研究结果表明，该方法能够有效地学习正确的交互网络，并可以应用于模型复杂数据集。同时，研究还评估了不同的训练方法的影响于模型质量。<details>
<summary>Abstract</summary>
Generative models offer a direct way to model complex data. Among them, energy-based models provide us with a neural network model that aims to accurately reproduce all statistical correlations observed in the data at the level of the Boltzmann weight of the model. However, one challenge is to understand the physical interpretation of such models. In this study, we propose a simple solution by implementing a direct mapping between the energy function of the Restricted Boltzmann Machine and an effective Ising spin Hamiltonian that includes high-order interactions between spins. This mapping includes interactions of all possible orders, going beyond the conventional pairwise interactions typically considered in the inverse Ising approach, and allowing the description of complex datasets. Earlier work attempted to achieve this goal, but the proposed mappings did not do properly treat the complexity of the problem or did not contain direct prescriptions for practical application. To validate our method, we perform several controlled numerical experiments where the training samples are equilibrium samples of predefined models containing local external fields, two-body and three-body interactions in various low-dimensional topologies. The results demonstrate the effectiveness of our proposed approach in learning the correct interaction network and pave the way for its application in modeling interesting datasets. We also evaluate the quality of the inferred model based on different training methods.
</details>
<details>
<summary>摘要</summary>
<?xml:namespace prefix = "o" ns = "urn:schemas-microsoft-com:office:office" />生成模型提供了一种直接模型复杂数据的方式。其中，能量基模型为我们提供了一种基于神经网络的模型，该模型目标是在数据中识别所有统计相关性，并在模型级别达到 Boltzmann 权重。然而，一个挑战是理解这些模型的物理含义。在这项研究中，我们提出了一种简单的解决方案，即在能量函数上进行直接映射，以实现一个包含高阶交互的有效牛顿矩阵 Hamiltonian。这种映射包括所有可能的阶数交互，超出了传统的对应 inverse Ising 方法中考虑的对称交互，并允许描述复杂的数据集。过去的工作已经尝试了这种目标，但是提出的映射没有正确地处理问题的复杂性或者没有直接的实践指导。为验证我们的方法，我们在各种控制的数字实验中进行了许多数据训练，其中训练样本是已知模型中的平衡样本，包括局部外场、二体和三体交互在不同的低维度拓扑上。结果表明我们的提posed方法可以有效地学习正确的交互网络，并为模型化有趣的数据集铺平道路。我们还评估了不同的训练方法的影响。
</details></li>
</ul>
<hr>
<h2 id="Haystack-A-Panoptic-Scene-Graph-Dataset-to-Evaluate-Rare-Predicate-Classes"><a href="#Haystack-A-Panoptic-Scene-Graph-Dataset-to-Evaluate-Rare-Predicate-Classes" class="headerlink" title="Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes"></a>Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02286">http://arxiv.org/abs/2309.02286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Lorenz, Florian Barthel, Daniel Kienzle, Rainer Lienhart</li>
<li>For: 这个论文是为了构建一个新的Scene Graph Dataset和一组测试 metrics，以便测试Scene Graph生成模型在罕见 predicate class 上的预测性能。* Methods: 作者提出了一个模型协助的注意力Annotation管道，以高效地找到图像中的罕见 predicate class。这个管道使用了一些新的技术，如模型预测和人工纠正，以提高注意力Annotation的准确性和效率。* Results: 作者通过对 Haystack Dataset 进行测试，发现这个 dataset 可以帮助提高Scene Graph生成模型在罕见 predicate class 上的预测性能。此外，Haystack  Dataset 还包含了 Explicit Negative Annotations，这些 Annotations 可以帮助改进现有的Scene Graph生成模型。<details>
<summary>Abstract</summary>
Current scene graph datasets suffer from strong long-tail distributions of their predicate classes. Due to a very low number of some predicate classes in the test sets, no reliable metrics can be retrieved for the rarest classes. We construct a new panoptic scene graph dataset and a set of metrics that are designed as a benchmark for the predictive performance especially on rare predicate classes. To construct the new dataset, we propose a model-assisted annotation pipeline that efficiently finds rare predicate classes that are hidden in a large set of images like needles in a haystack.   Contrary to prior scene graph datasets, Haystack contains explicit negative annotations, i.e. annotations that a given relation does not have a certain predicate class. Negative annotations are helpful especially in the field of scene graph generation and open up a whole new set of possibilities to improve current scene graph generation models.   Haystack is 100% compatible with existing panoptic scene graph datasets and can easily be integrated with existing evaluation pipelines. Our dataset and code can be found here: https://lorjul.github.io/haystack/. It includes annotation files and simple to use scripts and utilities, to help with integrating our dataset in existing work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PromptTTS-2-Describing-and-Generating-Voices-with-Text-Prompt"><a href="#PromptTTS-2-Describing-and-Generating-Voices-with-Text-Prompt" class="headerlink" title="PromptTTS 2: Describing and Generating Voices with Text Prompt"></a>PromptTTS 2: Describing and Generating Voices with Text Prompt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02285">http://arxiv.org/abs/2309.02285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian</li>
<li>for: 这个论文是用于解决传统文本至语音（TTS）方法中的问题，即使用文本提示（描述）而不是语音提示（参考语音）来提供语音变化的方法。</li>
<li>methods: 这篇论文提出了一种新的TTS方法，即PromptTTS 2，它使用了变化网络来提供不同语音变化的信息，以及一个提取语音特征的模型来生成高质量的文本提示。</li>
<li>results: 实验结果表明，Compared to先前的工作，PromptTTS 2可以更好地根据文本提示生成语音，并且支持采样多种语音变化，从而给用户提供更多的选择。此外，生成提示管道可以生成高质量的提示，从而减少大量标注成本。<details>
<summary>Abstract</summary>
Speech conveys more information than just text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompt for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variation network predicts the representation extracted from the reference speech (which contains full information about voice) based on the text prompt representation. For the prompt generation pipeline, it generates text prompts for speech with a speech understanding model to recognize voice attributes (e.g., gender, speed) from speech and a large language model to formulate text prompt based on the recognition results. Experiments on a large-scale (44K hours) speech dataset demonstrate that compared to the previous works, PromptTTS 2 generates voices more consistent with text prompts and supports the sampling of diverse voice variability, thereby offering users more choices on voice generation. Additionally, the prompt generation pipeline produces high-quality prompts, eliminating the large labeling cost. The demo page of PromptTTS 2 is available online\footnote{https://speechresearch.github.io/prompttts2}.
</details>
<details>
<summary>摘要</summary>
文本可以传递更多信息than just text, because the same word can be spoken in different voices to convey diverse information. 与传统的文本到语音（TTS）方法相比，使用文本提示（描述）更 user-friendly，因为语音提示可能困难或者不存在。 TTS 方法基于文本提示面临两个挑战：1）一个多对一问题，因为文本提示中不能完全捕捉语音变化的所有细节; 2）限制性的文本提示数据集，需要供应商和大量的数据标注成本来写文本提示 для语音。在这种工作中，我们介绍PromptTTS 2，用以解决这两个挑战。PromptTTS 2 使用变化网络提供不同语音变化的信息，并使用提取大量语言模型（LLM）来组成高质量文本提示。具体来说，变化网络预测基于参考语音（含有全部语音信息）的表示，基于文本提示表示来预测。 для提取大量语言模型，它生成语音提示，并使用语音理解模型来识别语音特征（例如，性别、速度）。实验表明，相比之前的工作，PromptTTS 2 可以更好地根据文本提示生成语音，并支持采样多种语音变化，从而为用户提供更多的选择。此外，提取大量语言模型可以消除大量标注成本。PromptTTS 2 的示例页面可以在线浏览（https://speechresearch.github.io/prompttts2）。
</details></li>
</ul>
<hr>
<h2 id="s-ID-Causal-Effect-Identification-in-a-Sub-Population"><a href="#s-ID-Causal-Effect-Identification-in-a-Sub-Population" class="headerlink" title="s-ID: Causal Effect Identification in a Sub-Population"></a>s-ID: Causal Effect Identification in a Sub-Population</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02281">http://arxiv.org/abs/2309.02281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Mohammad Abouei, Ehsan Mokhtarian, Negar Kiyavash</li>
<li>for: 本研究旨在解决在特定子population中 causal inference问题，即从观察数据中推断特定子population中 causal effect。</li>
<li>methods: 本研究使用了 necessary and sufficient conditions和一种sound and complete algorithm来解决这个问题。</li>
<li>results: 本研究提供了一个可靠的方法来在特定子population中推断 causal effect，并且可以应用于现有的causal inference方法中。<details>
<summary>Abstract</summary>
Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.
</details>
<details>
<summary>摘要</summary>
causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.Here's the translation in Traditional Chinese:causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Residual-based-Methods-on-Fault-Detection"><a href="#A-Comparison-of-Residual-based-Methods-on-Fault-Detection" class="headerlink" title="A Comparison of Residual-based Methods on Fault Detection"></a>A Comparison of Residual-based Methods on Fault Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02274">http://arxiv.org/abs/2309.02274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi-Ching Hsu, Gaetan Frusque, Olga Fink</li>
<li>for: 这篇论文主要针对复杂工业系统中的故障检测进行了一个初步的研究，即获得系统的健康状况的理解。</li>
<li>methods: 这篇论文使用了两种基于差异的方法：自适应神经网络和输入输出模型。</li>
<li>results: 研究发现，两种方法都能够在20个循环后检测到故障，并且具有低的假阳性率。而输入输出模型提供了更好的解释能力，即可能出现的故障类型和可能受损的组件。<details>
<summary>Abstract</summary>
An important initial step in fault detection for complex industrial systems is gaining an understanding of their health condition. Subsequently, continuous monitoring of this health condition becomes crucial to observe its evolution, track changes over time, and isolate faults. As faults are typically rare occurrences, it is essential to perform this monitoring in an unsupervised manner. Various approaches have been proposed not only to detect faults in an unsupervised manner but also to distinguish between different potential fault types. In this study, we perform a comprehensive comparison between two residual-based approaches: autoencoders, and the input-output models that establish a mapping between operating conditions and sensor readings. We explore the sensor-wise residuals and aggregated residuals for the entire system in both methods. The performance evaluation focuses on three tasks: health indicator construction, fault detection, and health indicator interpretation. To perform the comparison, we utilize the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model, specifically a subset of the turbofan engine dataset containing three different fault types. All models are trained exclusively on healthy data. Fault detection is achieved by applying a threshold that is determined based on the healthy condition. The detection results reveal that both models are capable of detecting faults with an average delay of around 20 cycles and maintain a low false positive rate. While the fault detection performance is similar for both models, the input-output model provides better interpretability regarding potential fault types and the possible faulty components.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>复杂的工业系统的故障检测的初步是了解它们的健康状况。因此，对这些健康状况的连续监测成为了非常重要的，以观察其演化，跟踪时间变化，并孤立故障。由于故障通常是罕见的，因此需要在无监督的情况下进行监测。多种方法已经被提议，不仅可以检测故障，还可以将不同的可能的故障类型分类出来。在这种研究中，我们进行了对两种剩余基于方法的比较：自适应神经网络和输入输出模型，它们可以将操作条件和传感器读数之间建立映射。我们分析整个系统的残差和传感器级别的残差，并对三个任务进行评估：健康指标的建构、故障检测和健康指标的解释。为了进行比较，我们使用了商用模块式飞航发动机模型（C-MAPSS）的动力学模型，具体是一个涉及到三种故障类型的发动机数据集。所有模型都是在健康状况下训练的。在检测故障的过程中，我们采用了一个基于健康状况的阈值，以确定是否存在故障。结果显示，两种模型都可以在约20个循环后检测故障，并保持低的假阳性率。虽然两种模型的故障检测性能相似，但输入输出模型提供了更好的可解释性，即可能的故障类型和可能的异常组件。
</details></li>
</ul>
<hr>
<h2 id="Graph-Based-Automatic-Feature-Selection-for-Multi-Class-Classification-via-Mean-Simplified-Silhouette"><a href="#Graph-Based-Automatic-Feature-Selection-for-Multi-Class-Classification-via-Mean-Simplified-Silhouette" class="headerlink" title="Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette"></a>Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02272">http://arxiv.org/abs/2309.02272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlevinwork/GB-AFS">https://github.com/davidlevinwork/GB-AFS</a></li>
<li>paper_authors: David Levin, Gonen Singer</li>
<li>For: 本研究旨在提出一种基于图的筛选方法，用于自动选择多类分类任务中的特征。* Methods: 该方法使用Jeffries-Matusita距离和t-分布随机邻域抽象（t-SNE）生成一个低维度空间，反映每个特征在每对类之间的分化能力。使用我们提出的新的简单化的抽象指数（MSS）来选择最小的特征数量。* Results: 实验结果表明，提案的GB-AFS方法在公共数据集上表现出优于其他筛选基于技术和自动特征选择方法。此外，GB-AFS方法可以维持使用所有特征时的准确率，只使用7%-30%的特征，从而实现了时间需要进行分类的减少，从15%下降到70%。<details>
<summary>Abstract</summary>
This paper introduces a novel graph-based filter method for automatic feature selection (abbreviated as GB-AFS) for multi-class classification tasks. The method determines the minimum combination of features required to sustain prediction performance while maintaining complementary discriminating abilities between different classes. It does not require any user-defined parameters such as the number of features to select. The methodology employs the Jeffries-Matusita (JM) distance in conjunction with t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space reflecting how effectively each feature can differentiate between each pair of classes. The minimum number of features is selected using our newly developed Mean Simplified Silhouette (abbreviated as MSS) index, designed to evaluate the clustering results for the feature selection task. Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches. Moreover, the proposed algorithm maintained the accuracy achieved when utilizing all features, while using only $7\%$ to $30\%$ of the features. Consequently, this resulted in a reduction of the time needed for classifications, from $15\%$ to $70\%$.
</details>
<details>
<summary>摘要</summary>
The methodology employs the Jeffries-Matusita distance and t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space reflecting how effectively each feature can differentiate between each pair of classes. The minimum number of features is selected using the newly developed Mean Simplified Silhouette (MSS) index, designed to evaluate the clustering results for the feature selection task.Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches. The proposed algorithm achieved the same accuracy as using all features, while using only $7\%$ to $30\%$ of the features, resulting in a reduction of the time needed for classifications from $15\%$ to $70\%$.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Sample-Selection-Through-Uncertainty-Estimation-and-Its-Application-in-Deep-Learning"><a href="#Optimal-Sample-Selection-Through-Uncertainty-Estimation-and-Its-Application-in-Deep-Learning" class="headerlink" title="Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning"></a>Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02476">http://arxiv.org/abs/2309.02476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Chen Liu, Chenlu Ye, Qing Lian, Yuan Yao, Tong Zhang</li>
<li>for: 这个研究旨在提出一个可靠地选择丛集和活跃学习的方法，以减少深度学习中大量标签的成本。</li>
<li>methods: 我们提出了一个理论上的最佳解决方案，即COPS（uncertainty based Optimal Sub-sampling）方法，可以最小化深度学习模型在抽样后的预期损失。</li>
<li>results: 我们通过实验证明了COPS方法的有效性，与基准方法相比，它在深度学习任务中表现出色。<details>
<summary>Abstract</summary>
Modern deep learning heavily relies on large labeled datasets, which often comse with high costs in terms of both manual labeling and computational resources. To mitigate these challenges, researchers have explored the use of informative subset selection techniques, including coreset selection and active learning. Specifically, coreset selection involves sampling data with both input ($\bx$) and output ($\by$), active learning focuses solely on the input data ($\bx$).   In this study, we present a theoretically optimal solution for addressing both coreset selection and active learning within the context of linear softmax regression. Our proposed method, COPS (unCertainty based OPtimal Sub-sampling), is designed to minimize the expected loss of a model trained on subsampled data. Unlike existing approaches that rely on explicit calculations of the inverse covariance matrix, which are not easily applicable to deep learning scenarios, COPS leverages the model's logits to estimate the sampling ratio. This sampling ratio is closely associated with model uncertainty and can be effectively applied to deep learning tasks. Furthermore, we address the challenge of model sensitivity to misspecification by incorporating a down-weighting approach for low-density samples, drawing inspiration from previous works.   To assess the effectiveness of our proposed method, we conducted extensive empirical experiments using deep neural networks on benchmark datasets. The results consistently showcase the superior performance of COPS compared to baseline methods, reaffirming its efficacy.
</details>
<details>
<summary>摘要</summary>
现代深度学习强调大量标注数据，往往带来高的人工标注和计算成本。为了解决这些挑战，研究人员探索了有用的子集选择技术，包括核心集选择和活动学习。特别是，核心集选择是采样数据中的输入($\bx$)和输出($\by$)。在这项研究中，我们提出了在线性软max回归方面的理论优化解决方案，即COPS（不确定性基于最优子集选择）。该方法旨在将模型训练过的子集数据的预期损失最小化。不同于现有方法，我们不使用直接计算 inverse covariance matrix的方法，而是利用模型的归一化值来估算抽样比率。这个抽样比率与模型不确定性密切相关，可以有效应用于深度学习任务。此外，我们还解决了模型偏置低概率样本的挑战，通过引入下降因子方法， drawing inspiration from previous works。为评估我们提出的方法的效果，我们在深度神经网络上进行了广泛的实验。结果一致地显示，COPS比基准方法有更好的表现，证明了其效果。
</details></li>
</ul>
<hr>
<h2 id="MA-VAE-Multi-head-Attention-based-Variational-Autoencoder-Approach-for-Anomaly-Detection-in-Multivariate-Time-series-Applied-to-Automotive-Endurance-Powertrain-Testing"><a href="#MA-VAE-Multi-head-Attention-based-Variational-Autoencoder-Approach-for-Anomaly-Detection-in-Multivariate-Time-series-Applied-to-Automotive-Endurance-Powertrain-Testing" class="headerlink" title="MA-VAE: Multi-head Attention-based Variational Autoencoder Approach for Anomaly Detection in Multivariate Time-series Applied to Automotive Endurance Powertrain Testing"></a>MA-VAE: Multi-head Attention-based Variational Autoencoder Approach for Anomaly Detection in Multivariate Time-series Applied to Automotive Endurance Powertrain Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02253">http://arxiv.org/abs/2309.02253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcs-crr/ma-vae">https://github.com/lcs-crr/ma-vae</a></li>
<li>paper_authors: Lucas Correia, Jan-Christoph Goos, Philipp Klein, Thomas Bäck, Anna V. Kononova</li>
<li>for: Automatic anomaly detection in automotive testing, to improve the efficiency of manual evaluation and detect more anomalies.</li>
<li>methods: Variational autoencoder with multi-head attention (MA-VAE) trained on unlabelled data, to avoid false positives and detect the majority of anomalies.</li>
<li>results: The approach achieves a detection rate of 67% and is 9% wrong when flagging an anomaly, with the potential to perform well with only a fraction of the training and validation subset.Here’s the text in Traditional Chinese characters:</li>
<li>for: 自动异常检测在汽车测试中，以提高人工评估的效率和检测更多的异常。</li>
<li>methods: 使用多头注意力的积欠 autoencoder（MA-VAE），通过训练无标的数据，避免false positives和检测异常的大多数。</li>
<li>results: 方法可以在实际工业数据集上取得67%的检测率，并且只有9%的时间错判异常，同时也具有只使用部分训练和验证subset的潜力。<details>
<summary>Abstract</summary>
A clear need for automatic anomaly detection applied to automotive testing has emerged as more and more attention is paid to the data recorded and manual evaluation by humans reaches its capacity. Such real-world data is massive, diverse, multivariate and temporal in nature, therefore requiring modelling of the testee behaviour. We propose a variational autoencoder with multi-head attention (MA-VAE), which, when trained on unlabelled data, not only provides very few false positives but also manages to detect the majority of the anomalies presented. In addition to that, the approach offers a novel way to avoid the bypass phenomenon, an undesirable behaviour investigated in literature. Lastly, the approach also introduces a new method to remap individual windows to a continuous time series. The results are presented in the context of a real-world industrial data set and several experiments are undertaken to further investigate certain aspects of the proposed model. When configured properly, it is 9% of the time wrong when an anomaly is flagged and discovers 67% of the anomalies present. Also, MA-VAE has the potential to perform well with only a fraction of the training and validation subset, however, to extract it, a more sophisticated threshold estimation method is required.
</details>
<details>
<summary>摘要</summary>
有一个明确的需求是自动异常检测应用于汽车测试，因为更多的注意力被 Pays attention to the recorded data and manual evaluation by humans reaches its capacity. 这种真实世界数据是庞大、多样、多变和时间性的，因此需要测试者行为的模型。 我们提议一种多头注意力自适应变换器（MA-VAE），当训练在无标签数据时，不仅几乎没有假阳性结果，而且能够检测大多数异常情况。 此外，该方法还解决了 литературе中所讨论的绕过现象，并 introduce a new method to remap individual windows to a continuous time series。 结果在一个实际工业数据集上展示，并进行了一些实验来深入调查某些方面的提案模型。 当配置正确时，MA-VAE 错误的时间为 9%，并发现 67% 的异常情况。 此外，MA-VAE 还有可能在只有一部分训练和验证集上表现良好，但是要从中提取它，需要更复杂的阈值估算方法。
</details></li>
</ul>
<hr>
<h2 id="RoBoSS-A-Robust-Bounded-Sparse-and-Smooth-Loss-Function-for-Supervised-Learning"><a href="#RoBoSS-A-Robust-Bounded-Sparse-and-Smooth-Loss-Function-for-Supervised-Learning" class="headerlink" title="RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning"></a>RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02250">http://arxiv.org/abs/2309.02250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mtanveer1/RoBoSS">https://github.com/mtanveer1/RoBoSS</a></li>
<li>paper_authors: Mushir Akhtar, M. Tanveer, Mohd. Arshad</li>
<li>For: The paper proposes a novel loss function called RoBoSS for supervised learning, which addresses the limitations of traditional loss functions in handling noisy and high-dimensional data, improving model interpretability, and reducing training time.* Methods: The paper introduces a new robust algorithm called $\mathcal{L}<em>{rbss}$-SVM that incorporates the RoBoSS loss function within the framework of support vector machine (SVM). The paper also presents a theoretical analysis of the RoBoSS loss function, including its classification-calibrated property and generalization ability.* Results: The paper evaluates the proposed $\mathcal{L}</em>{rbss}$-SVM model on 88 real-world UCI and KEEL datasets from diverse domains, and demonstrates its superiority in terms of remarkable generalization performance and efficiency in training time, especially in the biomedical realm.<details>
<summary>Abstract</summary>
In the domain of machine learning algorithms, the significance of the loss function is paramount, especially in supervised learning tasks. It serves as a fundamental pillar that profoundly influences the behavior and efficacy of supervised learning algorithms. Traditional loss functions, while widely used, often struggle to handle noisy and high-dimensional data, impede model interpretability, and lead to slow convergence during training. In this paper, we address the aforementioned constraints by proposing a novel robust, bounded, sparse, and smooth (RoBoSS) loss function for supervised learning. Further, we incorporate the RoBoSS loss function within the framework of support vector machine (SVM) and introduce a new robust algorithm named $\mathcal{L}_{rbss}$-SVM. For the theoretical analysis, the classification-calibrated property and generalization ability are also presented. These investigations are crucial for gaining deeper insights into the performance of the RoBoSS loss function in the classification tasks and its potential to generalize well to unseen data. To empirically demonstrate the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM, we evaluate it on $88$ real-world UCI and KEEL datasets from diverse domains. Additionally, to exemplify the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM within the biomedical realm, we evaluated it on two medical datasets: the electroencephalogram (EEG) signal dataset and the breast cancer (BreaKHis) dataset. The numerical results substantiate the superiority of the proposed $\mathcal{L}_{rbss}$-SVM model, both in terms of its remarkable generalization performance and its efficiency in training time.
</details>
<details>
<summary>摘要</summary>
在机器学习算法领域，损失函数的重要性特别在超级vised学习任务中。它作为基本柱石，对超级vised学习算法的行为和效果产生深远的影响。传统的损失函数，常见的使用，但容易处理噪音和高维数据、降低模型解释性和训练过程中的 converge 速度。本文提出了一种新的 robust、bounded、sparse和smooth（RoBoSS）损失函数，用于超级vised学习。此外，我们将RoBoSS损失函数 integrate 到支持向量机（SVM）框架中，并提出一种新的Robust算法称为 $\mathcal{L}_{rbss}$-SVM。对于理论分析，我们还提供了分类准备性和泛化能力的 investigate。这些调查对于理解RoBoSS损失函数在分类任务中的表现和其泛化能力具有深入的意义。为了证明提出的 $\mathcal{L}_{rbss}$-SVM的效果，我们对88个来自不同领域的UC Irvine和KEEL数据集进行了empirical评估。此外，为了强调 $\mathcal{L}_{rbss}$-SVM在医学领域的效果，我们对EEG信号数据集和BreaKHis数据集进行了评估。数据显示，提出的 $\mathcal{L}_{rbss}$-SVM模型在泛化性和训练时间效率方面具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Encoding-Seasonal-Climate-Predictions-for-Demand-Forecasting-with-Modular-Neural-Network"><a href="#Encoding-Seasonal-Climate-Predictions-for-Demand-Forecasting-with-Modular-Neural-Network" class="headerlink" title="Encoding Seasonal Climate Predictions for Demand Forecasting with Modular Neural Network"></a>Encoding Seasonal Climate Predictions for Demand Forecasting with Modular Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02248">http://arxiv.org/abs/2309.02248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Smit Marvaniya, Jitendra Singh, Nicolas Galichet, Fred Ochieng Otieno, Geeth De Mel, Kommy Weldemariam</li>
<li>For: 提高供应链功能的时间序列预测精度，使用短期天气特征作为外生输入。* Methods: 提出了一种新的模型框架，能够效率地编码季节气候预测，并通过模块化神经网络架构学习干扰的秘密表示。* Results: 对多个实际数据集进行了广泛的实验，结果显示，使用该模型框架可以降低约13%~17%的预测错误，相比现有的需求预测方法。<details>
<summary>Abstract</summary>
Current time-series forecasting problems use short-term weather attributes as exogenous inputs. However, in specific time-series forecasting solutions (e.g., demand prediction in the supply chain), seasonal climate predictions are crucial to improve its resilience. Representing mid to long-term seasonal climate forecasts is challenging as seasonal climate predictions are uncertain, and encoding spatio-temporal relationship of climate forecasts with demand is complex.   We propose a novel modeling framework that efficiently encodes seasonal climate predictions to provide robust and reliable time-series forecasting for supply chain functions. The encoding framework enables effective learning of latent representations -- be it uncertain seasonal climate prediction or other time-series data (e.g., buyer patterns) -- via a modular neural network architecture. Our extensive experiments indicate that learning such representations to model seasonal climate forecast results in an error reduction of approximately 13\% to 17\% across multiple real-world data sets compared to existing demand forecasting methods.
</details>
<details>
<summary>摘要</summary>
当前的时间序列预测问题使用短期天气特征作为外生输入。然而，在特定的时间序列预测解决方案（例如，供应链中的需求预测）中，季节气候预测是关键的，可以提高其抗难度。表示中期到长期季节气候预测的问题是复杂的，因为季节气候预测具有uncertainty，并且在空间时间上关系气候预测和需求的编码是复杂的。我们提出了一种新的模型框架，可以有效地编码季节气候预测，以提供可靠和可靠的时间序列预测。这种编码框架允许模型学习强大的含义表示，无论是uncertain季节气候预测还是其他时间序列数据（例如，买家模式）。我们的广泛实验表明，通过模型季节气候预测的编码，可以在多个实际数据集中降低错误率约13%到17%，相比之前的需求预测方法。
</details></li>
</ul>
<hr>
<h2 id="Self-Similarity-Based-and-Novelty-based-loss-for-music-structure-analysis"><a href="#Self-Similarity-Based-and-Novelty-based-loss-for-music-structure-analysis" class="headerlink" title="Self-Similarity-Based and Novelty-based loss for music structure analysis"></a>Self-Similarity-Based and Novelty-based loss for music structure analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02243">http://arxiv.org/abs/2309.02243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffroy Peeters</li>
<li>for: 音乐结构分析（MSA）的目标是确定音乐轨道中的乐段，并可能根据它们的相似性进行标签。本文提出一种监督方法来解决音乐边界检测问题。</li>
<li>methods: 我们同时学习特征和卷积核，并将它们结合使用。我们并同时优化SSM-损失和新鲜度损失。此外，我们还证明了通过自我注意力进行相对特征学习是MSA任务中有利的。</li>
<li>results: 我们对RWC-Pop和SALAMI中的多个子集进行比较，并证明了我们的方法的表现更佳。<details>
<summary>Abstract</summary>
Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity. In this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels. For this we jointly optimize -- a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and -- a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss. We also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA. Finally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI.
</details>
<details>
<summary>摘要</summary>
音乐结构分析（MSA）是目标在音乐轨道中标识 Musical Segments 并可能根据它们的相似性进行标签。在这篇论文中，我们提出了一种监督方法来实现音乐边界检测任务。我们同时学习特征和卷积核，并将它们结合使用。为此，我们同时优化 -- 基于学习的自相似矩阵（SSM）获得的特征loss，以及 -- 基于学习的卷积核应用于估计的SSM中的新鲜度loss。我们还证明了通过自我注意力实现相对特征学习对 MSA 任务有利。最后，我们比较了我们的方法与之前提出的方法在标准 RWC-Pop 和 SALAMI 中的表现。
</details></li>
</ul>
<hr>
<h2 id="Sample-Size-in-Natural-Language-Processing-within-Healthcare-Research"><a href="#Sample-Size-in-Natural-Language-Processing-within-Healthcare-Research" class="headerlink" title="Sample Size in Natural Language Processing within Healthcare Research"></a>Sample Size in Natural Language Processing within Healthcare Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02237">http://arxiv.org/abs/2309.02237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaya Chaturvedi, Diana Shamsutdinova, Felix Zimmer, Sumithra Velupillai, Daniel Stahl, Robert Stewart, Angus Roberts<br>for: This paper aims to provide recommendations on sample sizes for text classification tasks in the healthcare domain.methods: The authors use models trained on the MIMIC-III database of critical care records to classify documents as having or not having Unspecified Essential Hypertension or diabetes mellitus without mention of complication. They perform simulations using various classifiers on different sample sizes and class proportions.results: The study finds that a sample size larger than 1000 is sufficient to provide decent performance metrics, and that smaller sample sizes result in better results with K-nearest neighbours classifiers, while larger sample sizes provide better results with support vector machines and BERT models. The simulations provide guidelines for selecting appropriate sample sizes and class proportions, and can be modified for sample size estimates calculations with other datasets.Here is the text in Simplified Chinese:for: 这篇论文目的是为健康领域文本分类任务提供样本大小建议。methods: 作者使用基于MIMIC-III数据库的重症护理记录来分类文档是否有未特定高血压病例。他们使用不同的分类器进行了不同样本大小和类别比例的 simulations。results: 研究发现，样本大于1000是可以提供不错的性能指标的，而小样本更适合K nearest neighbors分类器，大样本更适合支持向量机和BERT模型。这些 simulations 提供了适合样本大小和类别比例的建议，可以用于其他数据集的样本大小估算。<details>
<summary>Abstract</summary>
Sample size calculation is an essential step in most data-based disciplines. Large enough samples ensure representativeness of the population and determine the precision of estimates. This is true for most quantitative studies, including those that employ machine learning methods, such as natural language processing, where free-text is used to generate predictions and classify instances of text. Within the healthcare domain, the lack of sufficient corpora of previously collected data can be a limiting factor when determining sample sizes for new studies. This paper tries to address the issue by making recommendations on sample sizes for text classification tasks in the healthcare domain.   Models trained on the MIMIC-III database of critical care records from Beth Israel Deaconess Medical Center were used to classify documents as having or not having Unspecified Essential Hypertension, the most common diagnosis code in the database. Simulations were performed using various classifiers on different sample sizes and class proportions. This was repeated for a comparatively less common diagnosis code within the database of diabetes mellitus without mention of complication.   Smaller sample sizes resulted in better results when using a K-nearest neighbours classifier, whereas larger sample sizes provided better results with support vector machines and BERT models. Overall, a sample size larger than 1000 was sufficient to provide decent performance metrics.   The simulations conducted within this study provide guidelines that can be used as recommendations for selecting appropriate sample sizes and class proportions, and for predicting expected performance, when building classifiers for textual healthcare data. The methodology used here can be modified for sample size estimates calculations with other datasets.
</details>
<details>
<summary>摘要</summary>
样本大小计算是数据基础学科中的一个重要步骤。大 enough的样本可以保证样本表示性和估计精度的准确性。这是对多个量学研究进行的，包括使用机器学习方法，如自然语言处理，其中自由文本用于生成预测和类型实例。在医疗领域，缺乏足够的先前收集的数据库可以是新研究确定样本大小的限制因素。本文试图解决这个问题，通过对文本分类任务在医疗领域中的样本大小进行建议。使用了基于MIMIC-III数据库的 kritical care记录，从Beth Israel Deaconess医疗中心中的文本进行分类，并将文档分为有无不特定重要高血压的诊断代码。使用不同的类ifier和样本大小进行了多个 simulations。这些 simulations 表明，使用 K-nearest neighbours 类ifier时，小样本大小更好的结果，而使用支持向量机和BERT模型时，更大的样本大小提供了更好的结果。总的来说，样本大于1000是足够的，以提供不错的性能指标。这些 simulations 提供了适用于选择合适的样本大小和类型占比，以及预测预期性能的指南，在建立文本医疗数据中的分类器时。这种方法可以在其他数据集上进行修改，以便为样本大小估算计算提供方法。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Model-based-Reinforcement-Learning-with-Large-State-Spaces"><a href="#Distributionally-Robust-Model-based-Reinforcement-Learning-with-Large-State-Spaces" class="headerlink" title="Distributionally Robust Model-based Reinforcement Learning with Large State Spaces"></a>Distributionally Robust Model-based Reinforcement Learning with Large State Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02236">http://arxiv.org/abs/2309.02236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shyam Sundhar Ramesh, Pier Giuseppe Sessa, Yifan Hu, Andreas Krause, Ilija Bogunovic</li>
<li>for: 解决权衡学习中的复杂动态系统、高维状态空间和实际世界动态与训练环境不同问题。</li>
<li>methods: 使用分布性Robust Markov决策过程，使用 Gaussian Processes 和最大差异减少算法，fficiently 学习多输出 Nominal 过程动力学。</li>
<li>results: 提出了一种独立数据集获取成本高、不同不同的不确定性集合下的 statistically 可靠的样本复杂度。实验结果表明方法具有分布性不稳定性和高效性。<details>
<summary>Abstract</summary>
Three major challenges in reinforcement learning are the complex dynamical systems with large state spaces, the costly data acquisition processes, and the deviation of real-world dynamics from the training environment deployment. To overcome these issues, we study distributionally robust Markov decision processes with continuous state spaces under the widely used Kullback-Leibler, chi-square, and total variation uncertainty sets. We propose a model-based approach that utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn multi-output nominal transition dynamics, leveraging access to a generative model (i.e., simulator). We further demonstrate the statistical sample complexity of the proposed method for different uncertainty sets. These complexity bounds are independent of the number of states and extend beyond linear dynamics, ensuring the effectiveness of our approach in identifying near-optimal distributionally-robust policies. The proposed method can be further combined with other model-free distributionally robust reinforcement learning methods to obtain a near-optimal robust policy. Experimental results demonstrate the robustness of our algorithm to distributional shifts and its superior performance in terms of the number of samples needed.
</details>
<details>
<summary>摘要</summary>
三大挑战在强化学习中是复杂的动力系统和大状态空间，以及实际世界中的动力学与训练环境部署不符。为了解决这些问题，我们研究了分布式Robust Markov决策过程，在Kullback-Leibler、χ²和总变量不确定集中进行研究。我们提出了基于模型的方法，利用 Gaussian Processes 和最大变量减少算法，高效地学习多输出номinal传输动力学，利用 simulator 的访问权限。我们还证明了提posed方法的统计样本复杂性，对不同的不确定集进行证明。这些复杂性下限独立于状态数量，超越了线性动力学，保证了我们的方法在确定分布下采取近似优化的策略。我们的方法可以与其他model-free分布式Robust reinforcement learning方法结合，获得近似优化的强化策略。实验结果表明，我们的算法对分布shift具有鲁棒性，并且在样本数量方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Improving-equilibrium-propagation-without-weight-symmetry-through-Jacobian-homeostasis"><a href="#Improving-equilibrium-propagation-without-weight-symmetry-through-Jacobian-homeostasis" class="headerlink" title="Improving equilibrium propagation without weight symmetry through Jacobian homeostasis"></a>Improving equilibrium propagation without weight symmetry through Jacobian homeostasis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02214">http://arxiv.org/abs/2309.02214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/laborieux-axel/generalized-holo-ep">https://github.com/laborieux-axel/generalized-holo-ep</a></li>
<li>paper_authors: Axel Laborieux, Friedemann Zenke</li>
<li>for: 这个论文旨在探讨 equilibrio propagation（EP）算法在生物或分析 нейROMorphic substrate上计算神经网络的梯度时的问题。</li>
<li>methods: 这篇论文使用了 generalized EP 算法，可以不需要权重对称，并分离了两种偏误的来源。</li>
<li>results: 研究发现，对于复杂非对称神经网络，finite nudge不会引起问题，但权重不对称会导致低任务性能，因为 EP 的 neuronal error vectors 与 BP 的不同。为解决这个问题，我们提出了一个新的homeostatic objective，可以直接惩罚神经网络的 Jacobian 函数的功能不对称。这种目标可以帮助神经网络更好地解决复杂任务，如 ImageNet 32x32。<details>
<summary>Abstract</summary>
Equilibrium propagation (EP) is a compelling alternative to the backpropagation of error algorithm (BP) for computing gradients of neural networks on biological or analog neuromorphic substrates. Still, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e., nudges, to estimate unbiased gradients efficiently. Both requirements are challenging to implement in physical systems. Yet, whether and how weight asymmetry affects its applicability is unknown because, in practice, it may be masked by biases introduced through the finite nudge. To address this question, we study generalized EP, which can be formulated without weight symmetry, and analytically isolate the two sources of bias. For complex-differentiable non-symmetric networks, we show that the finite nudge does not pose a problem, as exact derivatives can still be estimated via a Cauchy integral. In contrast, weight asymmetry introduces bias resulting in low task performance due to poor alignment of EP's neuronal error vectors compared to BP. To mitigate this issue, we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network's fixed point. This homeostatic objective dramatically improves the network's ability to solve complex tasks such as ImageNet 32x32. Our results lay the theoretical groundwork for studying and mitigating the adverse effects of imperfections of physical networks on learning algorithms that rely on the substrate's relaxation dynamics.
</details>
<details>
<summary>摘要</summary>
“均衡传播”（EP）是一种可观的替代方案，用于计算神经网络中的梯度。它需要权重对称和无限小均衡干扰（即推干），以计算不偏的梯度。但是，实际上实现这两个需求是很困难的。特别是，权重的不对称性可能会对EP的适用范围造成影响，但是这个问题仍未得到解释。为了解决这个问题，我们研究了一种不对称的EP，并分析了两种偏差的来源。在复杂的不对称神经网络中，我们发现了一个与推干无关的问题，即权重的不对称性会导致梯度的误差。为了解决这个问题，我们提出了一个新的自适应目标，它直接评估神经网络的Jacobian中的功能不对称性。我们的结果显示，这个自适应目标可以对神经网络的任务性能提高，特别是在ImageNet 32x32类型的复杂任务中。我们的研究提供了实际的理论基础，用于研究和缓解物理网络上学习算法的偏差问题。”
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Machine-Learning-with-Multi-source-Data"><a href="#Distributionally-Robust-Machine-Learning-with-Multi-source-Data" class="headerlink" title="Distributionally Robust Machine Learning with Multi-source Data"></a>Distributionally Robust Machine Learning with Multi-source Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02211">http://arxiv.org/abs/2309.02211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyu Wang, Peter Bühlmann, Zijian Guo</li>
<li>for: 本研究旨在提高预测性能，解决经典机器学习方法在目标分布与源Population之间的差异问题。</li>
<li>methods: 本文提出了一种基于多个源Population的分布ally robust预测模型，通过最大化对目标分布的劳动价值来优化预测性能。</li>
<li>results: 相比经典采样风险最小化，提案的Robust预测模型可以提高预测性能对于具有分布差异的目标Population。我们还证明了该模型是源Population的conditional outcome模型的加权平均。此外，我们还提出了一种偏向Corrector来改善基本机器学习算法的涨进率。<details>
<summary>Abstract</summary>
Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the convergence rate. Our proposal can be seen as a distributionally robust federated learning approach that is computationally efficient and easy to implement using arbitrary machine learning base algorithms, satisfies some privacy constraints, and has a nice interpretation of different sources' importance for predicting a given target covariate distribution. We demonstrate the performance of our proposed group distributionally robust method on simulated and real data with random forests and neural networks as base-learning algorithms.
</details>
<details>
<summary>摘要</summary>
The proposed model is a weighted average of the source populations' conditional outcome models, and we develop a novel bias-corrected estimator to estimate the optimal aggregation weight. This approach can be seen as a distributionally robust federated learning method that is computationally efficient and easy to implement using arbitrary machine learning base algorithms, satisfies some privacy constraints, and has a nice interpretation of different sources' importance for predicting a given target covariate distribution.We demonstrate the performance of our proposed group distributionally robust method on simulated and real data with random forests and neural networks as base-learning algorithms.
</details></li>
</ul>
<hr>
<h2 id="Language-Models-for-Novelty-Detection-in-System-Call-Traces"><a href="#Language-Models-for-Novelty-Detection-in-System-Call-Traces" class="headerlink" title="Language Models for Novelty Detection in System Call Traces"></a>Language Models for Novelty Detection in System Call Traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02206">http://arxiv.org/abs/2309.02206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Fournier, Daniel Aloise, Leandro R. Costa</li>
<li>for: 本研究旨在检测计算机系统中的新型行为，以便更好地了解系统的行为和性能。</li>
<li>methods: 本研究使用语言模型来检测系统调用序列中的新型行为，并评估了三种不同的神经网络架构：LSTM、Transformer 和 Longformer。</li>
<li>results: 研究发现，使用这些神经网络架构可以达到高于 95% 的 F-score 和 AuROC 值，并且需要 minimal 的专家手工设计。代码和训练模型已经公开发布在 GitHub，而数据集则可以在 Zenodo 上下载。<details>
<summary>Abstract</summary>
Due to the complexity of modern computer systems, novel and unexpected behaviors frequently occur. Such deviations are either normal occurrences, such as software updates and new user activities, or abnormalities, such as misconfigurations, latency issues, intrusions, and software bugs. Regardless, novel behaviors are of great interest to developers, and there is a genuine need for efficient and effective methods to detect them. Nowadays, researchers consider system calls to be the most fine-grained and accurate source of information to investigate the behavior of computer systems. Accordingly, this paper introduces a novelty detection methodology that relies on a probability distribution over sequences of system calls, which can be seen as a language model. Language models estimate the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition, they would be unlikely under the model. Following the success of neural networks for language models, three architectures are evaluated in this work: the widespread LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer. However, large neural networks typically require an enormous amount of data to be trained effectively, and to the best of our knowledge, no massive modern datasets of kernel traces are publicly available. This paper addresses this limitation by introducing a new open-source dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors. The proposed methodology requires minimal expert hand-crafting and achieves an F-score and AuROC greater than 95% on most novelties while being data- and task-agnostic. The source code and trained models are publicly available on GitHub while the datasets are available on Zenodo.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于现代计算机系统的复杂性，常见和不常见的行为经常发生。这些异常可能是正常的，如软件更新和新用户活动，或者异常的，如配置问题、延迟问题、入侵和软件错误。无论如何，新的行为对开发者来说非常有趣，并且有一定的实际需求来检测它们。现在，研究人员通常认为系统调用是计算机系统行为的最细grained和准确的源泉。因此，这篇论文提出了一种基于系统调用的新特性检测方法。这种方法基于系统调用的概率分布，可以被视为语言模型。语言模型可以估计序列的可能性，因此新的行为往往会对模型进行异常。随着语义网络在语言模型方面的成功，这篇论文在这三种体系中评估了LSTM、Transformer和Longformer。然而，大型神经网络通常需要很多数据来进行有效地训练，而我们所知道的现代kernel traces数据集没有公开available。这篇论文解决了这个限制，通过介绍一个新的开源数据集，包含超过200万个网络请求，与7种不同的行为相关。提议的方法需要 minimal expert hand-crafting，并在大多数新的行为上达到了F-score和AuROC的95%以上。source code和训练模型都公开可用于GitHub，而数据集则可以在Zenodo上找到。
</details></li>
</ul>
<hr>
<h2 id="On-the-Complexity-of-Differentially-Private-Best-Arm-Identification-with-Fixed-Confidence"><a href="#On-the-Complexity-of-Differentially-Private-Best-Arm-Identification-with-Fixed-Confidence" class="headerlink" title="On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence"></a>On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02202">http://arxiv.org/abs/2309.02202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achraf Azize, Marc Jourdan, Aymen Al Marjani, Debabrota Basu</li>
<li>for: 这个论文的目的是研究在Fixed Confidence下的Best Arm Identification（BAI）问题，具体来说是在数据敏感应用中实现数据隐私的同时能够有效地确定最佳臂。</li>
<li>methods: 本论文使用了Global Differential Privacy（DP）的概念来保证数据隐私，并提出了一种名为AdaP-TT的变体，这种方法在运行arm-dependent adaptive episodes中添加了Laplace噪声来保证好的隐私利用协议。</li>
<li>results: 本论文提出了一个lower bound的计算复杂度下界，这个下界随着隐私预算($\epsilon$)的变化而改变，并且在高隐私 régime（小$\epsilon）下，这个下界受到一种新的信息理论量——总特征时间的影响。此外，本论文还提出了一种名为AdaP-TT的实验分析， validate our theoretical results.<details>
<summary>Abstract</summary>
Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies to name a few. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence under $\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive a lower bound on the sample complexity of any $\delta$-correct BAI algorithm satisfying $\epsilon$-global DP. Our lower bound suggests the existence of two privacy regimes depending on the privacy budget $\epsilon$. In the high-privacy regime (small $\epsilon$), the hardness depends on a coupled effect of privacy and a novel information-theoretic quantity, called the Total Variation Characteristic Time. In the low-privacy regime (large $\epsilon$), the sample complexity lower bound reduces to the classical non-private lower bound. Second, we propose AdaP-TT, an $\epsilon$-global DP variant of the Top Two algorithm. AdaP-TT runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. We derive an asymptotic upper bound on the sample complexity of AdaP-TT that matches with the lower bound up to multiplicative constants in the high-privacy regime. Finally, we provide an experimental analysis of AdaP-TT that validates our theoretical results.
</details>
<details>
<summary>摘要</summary>
Best Arm Identification (BAI) 问题在数据敏感应用中逐渐获得应用，如设计适应性临床试验、调整超参数以及进行用户研究等。由于这些应用所 invoke 的数据隐私问题，我们研究了在 fixed confidence 下的 BAI 问题，以确保数据隐私。首先，为了衡量隐私成本，我们 deriv 了 Any $\delta$-correct BAI 算法满足 $\epsilon$-global Differential Privacy (DP) 的下界。我们的下界表明，存在两种隐私 régime，即高隐私 régime（小 $\epsilon$）和低隐私 régime（大 $\epsilon$）。在高隐私 régime中，难度受到隐私和一种新的信息理论量，called Total Variation Characteristic Time 的coupled effect。在低隐私 régime中，下界降到 classical non-private 的下界。其次，我们提出了 AdaP-TT，一种 $\epsilon$-global DP 版本的 Top Two 算法。AdaP-TT 在 arm-dependent 的适应性集中运行，并添加 Laplace 噪声以保证好的隐私-实用办法。我们 deriv 了 AdaP-TT 的 asymptotic 上界，其与下界几乎相同，只有 multiplicative constants 的差异在高隐私 régime。最后，我们进行了 AdaP-TT 的实验分析，并证明了我们的理论结果。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Function-space-Representation-of-Neural-Networks"><a href="#Sparse-Function-space-Representation-of-Neural-Networks" class="headerlink" title="Sparse Function-space Representation of Neural Networks"></a>Sparse Function-space Representation of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02195">http://arxiv.org/abs/2309.02195</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Aidan Scannell, Riccardo Mereu, Paul Chang, Ella Tamir, Joni Pajarinen, Arno Solin</li>
<li>for: 提高深度神经网络（NNs）的不确定性估计和数据更新能力。</li>
<li>methods: 将NNs转换为函数空间，使用双参数化来提供笛卡尔表示， capture数据集中的信息，并提供一种可靠的方法来Quantifying uncertainty in supervised learning tasks。</li>
<li>results: 在UC Irvine（UCI）数据集上进行证明，提出的方法可以快速地 incorporate new data without retraining，保持预测性能。<details>
<summary>Abstract</summary>
Deep neural networks (NNs) are known to lack uncertainty estimates and struggle to incorporate new data. We present a method that mitigates these issues by converting NNs from weight space to function space, via a dual parameterization. Importantly, the dual parameterization enables us to formulate a sparse representation that captures information from the entire data set. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We provide proof-of-concept demonstrations with the proposed approach for quantifying uncertainty in supervised learning on UCI benchmark tasks.
</details>
<details>
<summary>摘要</summary>
深度神经网络（NN）因缺乏不确定性估计和新数据 интеграción能力而著称为不稳定。我们提出了一种方法，通过将 NN 从权重空间转换到函数空间，通过双参数化来解决这些问题。这种双参数化允许我们构建一种稀疏表示，捕捉整个数据集中的信息。这种方法可以有效地捕捉不确定性，并在不需要重新训练的情况下将新数据 integrate 到模型中。我们对超vised learning在UCIbenchmark任务上进行了证明性示范。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Federated-Deep-Reinforcement-Learning-based-Trajectory-Optimization-for-Multi-UAV-Assisted-Edge-Computing"><a href="#Personalized-Federated-Deep-Reinforcement-Learning-based-Trajectory-Optimization-for-Multi-UAV-Assisted-Edge-Computing" class="headerlink" title="Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing"></a>Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02193">http://arxiv.org/abs/2309.02193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengrong Song, Chuan Ma, Ming Ding, Howard H. Yang, Yuwen Qian, Xiangwei Zhou</li>
<li>for: 提高多架空车 trajectory optimization 和服务质量</li>
<li>methods: 融合 federated learning 和 deep reinforcement learning 技术</li>
<li>results: 提高训练效率和服务质量，比其他 DRL 方法更好<details>
<summary>Abstract</summary>
In the era of 5G mobile communication, there has been a significant surge in research focused on unmanned aerial vehicles (UAVs) and mobile edge computing technology. UAVs can serve as intelligent servers in edge computing environments, optimizing their flight trajectories to maximize communication system throughput. Deep reinforcement learning (DRL)-based trajectory optimization algorithms may suffer from poor training performance due to intricate terrain features and inadequate training data. To overcome this limitation, some studies have proposed leveraging federated learning (FL) to mitigate the data isolation problem and expedite convergence. Nevertheless, the efficacy of global FL models can be negatively impacted by the high heterogeneity of local data, which could potentially impede the training process and even compromise the performance of local agents. This work proposes a novel solution to address these challenges, namely personalized federated deep reinforcement learning (PF-DRL), for multi-UAV trajectory optimization. PF-DRL aims to develop individualized models for each agent to address the data scarcity issue and mitigate the negative impact of data heterogeneity. Simulation results demonstrate that the proposed algorithm achieves superior training performance with faster convergence rates, and improves service quality compared to other DRL-based approaches.
</details>
<details>
<summary>摘要</summary>
在5G移动通信时代，有一场很大的研究集中于无人飞行器（UAV）和边缘计算技术。UAV可以作为边缘计算环境中的智能服务器，优化其飞行轨迹以最大化通信系统吞吐量。基于深度反馈学习（DRL）的轨迹优化算法可能由于复杂的地形特征和不充分的训练数据而表现差。为了解决这些问题，一些研究提议使用联邦学习（FL）来缓解数据隔离问题并加速对称。然而，全球FL模型的效果可能受到本地数据的高多样性的影响，这可能会阻碍训练过程并可能对本地代理器的性能产生负面影响。这项工作提出了一种解决这些挑战的新方法，即个性化联邦深度反馈学习（PF-DRL），用于多个UAV轨迹优化。PF-DRL的目标是为每个代理器开发个性化模型，以解决数据缺乏问题并减少数据多样性的负面影响。实验结果表明，提议的算法可以在训练性能和快速对称上达到更高的水平，并提高服务质量。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-BERT-Language-Models-for-Multi-Lingual-ESG-Issue-Identification"><a href="#Leveraging-BERT-Language-Models-for-Multi-Lingual-ESG-Issue-Identification" class="headerlink" title="Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification"></a>Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02189">http://arxiv.org/abs/2309.02189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elvys Linhares Pontes, Mohamed Benjannet, Lam Kim Ming</li>
<li>for: 这个研究旨在探讨如何使用BERT语言模型精准地分类新闻文档中的ESG问题标签。</li>
<li>methods: 这个研究使用了多种BERT语言模型，包括RoBERTa和SVM等方法，以实现高精度的新闻文档分类。</li>
<li>results: 研究发现，RoBERTa分类器在英文测试集上获得了第二名，而在法语测试集上与其他方法并列第五名。此外，SVM基于的二分类模型在中文语言上表现出色，在测试集上获得了第二名。<details>
<summary>Abstract</summary>
Environmental, Social, and Governance (ESG) has been used as a metric to measure the negative impacts and enhance positive outcomes of companies in areas such as the environment, society, and governance. Recently, investors have increasingly recognized the significance of ESG criteria in their investment choices, leading businesses to integrate ESG principles into their operations and strategies. The Multi-Lingual ESG Issue Identification (ML-ESG) shared task encompasses the classification of news documents into 35 distinct ESG issue labels. In this study, we explored multiple strategies harnessing BERT language models to achieve accurate classification of news documents across these labels. Our analysis revealed that the RoBERTa classifier emerged as one of the most successful approaches, securing the second-place position for the English test dataset, and sharing the fifth-place position for the French test dataset. Furthermore, our SVM-based binary model tailored for the Chinese language exhibited exceptional performance, earning the second-place rank on the test dataset.
</details>
<details>
<summary>摘要</summary>
环境、社会和管理（ESG）已经被用作公司负面影响和改善效果的度量。最近，投资者对ESG标准的重要性日益认识，迫企业将ESG原则纳入其运营和战略中。本研究使用BERT语言模型来实现新闻文档的精确分类，并对英文和法文测试集进行了分析。我们发现，RoBERTa分类器在英文测试集上表现出色，排名第二，而在法文测试集上与其他模型分列第五。此外，我们为中文语言开发的SVM二分类模型也表现出优异，在测试集上排名第二。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Imitation-Learning-Algorithms-Recent-Developments-and-Challenges"><a href="#A-Survey-of-Imitation-Learning-Algorithms-Recent-Developments-and-Challenges" class="headerlink" title="A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges"></a>A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02473">http://arxiv.org/abs/2309.02473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maryam Zare, Parham M. Kebria, Abbas Khosravi, Saeid Nahavandi</li>
<li>for: This paper provides an introduction to imitation learning (IL) and an overview of its underlying assumptions and approaches, as well as a comprehensive guide to the growing field of IL in robotics and AI.</li>
<li>methods: The paper discusses recent advances and emerging areas of research in IL, including the use of demonstrations, the challenges associated with IL, and potential directions for future research.</li>
<li>results: The paper provides a detailed description of the current state of the field of IL, including recent advances and emerging areas of research, and offers a comprehensive guide to the field.<details>
<summary>Abstract</summary>
In recent years, the development of robotics and artificial intelligence (AI) systems has been nothing short of remarkable. As these systems continue to evolve, they are being utilized in increasingly complex and unstructured environments, such as autonomous driving, aerial robotics, and natural language processing. As a consequence, programming their behaviors manually or defining their behavior through reward functions (as done in reinforcement learning (RL)) has become exceedingly difficult. This is because such environments require a high degree of flexibility and adaptability, making it challenging to specify an optimal set of rules or reward signals that can account for all possible situations. In such environments, learning from an expert's behavior through imitation is often more appealing. This is where imitation learning (IL) comes into play - a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations.   This paper aims to provide an introduction to IL and an overview of its underlying assumptions and approaches. It also offers a detailed description of recent advances and emerging areas of research in the field. Additionally, the paper discusses how researchers have addressed common challenges associated with IL and provides potential directions for future research. Overall, the goal of the paper is to provide a comprehensive guide to the growing field of IL in robotics and AI.
</details>
<details>
<summary>摘要</summary>
IL is a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations. This paper aims to provide an introduction to IL and an overview of its underlying assumptions and approaches. It also offers a detailed description of recent advances and emerging areas of research in the field. Additionally, the paper discusses how researchers have addressed common challenges associated with IL and provides potential directions for future research.IL has gained increasing attention in recent years due to its ability to learn complex behaviors in a more efficient and effective manner than traditional RL methods. By learning from an expert's behavior, IL can overcome the challenges of specifying an optimal set of rules or reward signals in complex environments. Furthermore, IL can be applied to a wide range of tasks, such as robotic manipulation, human-robot interaction, and autonomous driving.The paper is organized as follows: In section 2, we provide an overview of the underlying assumptions and approaches of IL. In section 3, we discuss recent advances and emerging areas of research in the field. In section 4, we address common challenges associated with IL and provide potential directions for future research. Finally, in section 5, we conclude with a discussion of the potential of IL in robotics and AI.Overall, the goal of this paper is to provide a comprehensive guide to the growing field of IL in robotics and AI, and to highlight the potential of this approach for learning complex behaviors in a more efficient and effective manner.
</details></li>
</ul>
<hr>
<h2 id="Bias-Propagation-in-Federated-Learning"><a href="#Bias-Propagation-in-Federated-Learning" class="headerlink" title="Bias Propagation in Federated Learning"></a>Bias Propagation in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02160">http://arxiv.org/abs/2309.02160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/privacytrustlab/bias_in_FL">https://github.com/privacytrustlab/bias_in_FL</a></li>
<li>paper_authors: Hongyan Chang, Reza Shokri</li>
<li>for: 这篇论文旨在探讨联合学习中的群体公平问题，具体来说是研究如果一些党在联合学习中存在偏见，会如何影响整个网络中的模型。</li>
<li>methods: 作者使用了实际世界数据集来分析和解释联合学习中偏见的传播。他们分析发现，偏见党在训练过程中逐渐增加了对敏感特征的依赖度。</li>
<li>results: 研究发现，在联合学习中，偏见会导致模型受到敏感特征的影响，这种影响比中央训练模型使用整个数据集时更高。这表明，偏见在联合学习中存在，并且需要进行群体公平的审核和设计更加Robust的学习算法。<details>
<summary>Abstract</summary>
We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.
</details>
<details>
<summary>摘要</summary>
我们显示了参与联邦学习可能会对群体公平性造成损害。事实上，一些党征某些受排挤群体（根据敏感特征如性别或种族）的偏见可以透过网络传播到所有网络中的党。我们分析并解释了联邦学习中偏见传播的现象。我们发现，偏见党不知不觉地将偏见传递到小数的模型参数中，并在训练过程中不断增加受排挤群体的依赖。需要注意的是，在联邦学习中体验到的偏见高于各党在集中训练中训练的模型的情况。这表明偏见是由于算法所致。我们的工作呼吁审核联邦学习中的群体公平性，并设计不受偏见传播的学习算法。
</details></li>
</ul>
<hr>
<h2 id="Model-based-Offline-Policy-Optimization-with-Adversarial-Network"><a href="#Model-based-Offline-Policy-Optimization-with-Adversarial-Network" class="headerlink" title="Model-based Offline Policy Optimization with Adversarial Network"></a>Model-based Offline Policy Optimization with Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02157">http://arxiv.org/abs/2309.02157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junming-yang/moan">https://github.com/junming-yang/moan</a></li>
<li>paper_authors: Junming Yang, Xingguo Chen, Shengyuan Wang, Bolei Zhang</li>
<li>for: 这个论文主要针对的是OFFLINE reinforcement learning中的策略优化问题。</li>
<li>methods: 该论文提出了一种基于模型的OFFLINE reinforcement learning方法，使用了对抗学习来建立一个更加通用的转移模型，并通过对抗学习来自动提供模型不确定性的评估。</li>
<li>results:  compare to现有的基于模型的OFFLINE reinforcement learning方法，该方法在广泛的测试集上表现出色，可以更好地优化策略，同时可以更准确地评估模型的不确定性。<details>
<summary>Abstract</summary>
Model-based offline reinforcement learning (RL), which builds a supervised transition model with logging dataset to avoid costly interactions with the online environment, has been a promising approach for offline policy optimization. As the discrepancy between the logging data and online environment may result in a distributional shift problem, many prior works have studied how to build robust transition models conservatively and estimate the model uncertainty accurately. However, the over-conservatism can limit the exploration of the agent, and the uncertainty estimates may be unreliable. In this work, we propose a novel Model-based Offline policy optimization framework with Adversarial Network (MOAN). The key idea is to use adversarial learning to build a transition model with better generalization, where an adversary is introduced to distinguish between in-distribution and out-of-distribution samples. Moreover, the adversary can naturally provide a quantification of the model's uncertainty with theoretical guarantees. Extensive experiments showed that our approach outperforms existing state-of-the-art baselines on widely studied offline RL benchmarks. It can also generate diverse in-distribution samples, and quantify the uncertainty more accurately.
</details>
<details>
<summary>摘要</summary>
模型基于的线上强化学习（RL），建立了一个监督式转移模型，使用日志数据集来避免在线环境中的昂贵交互，已经是无线环境中的减少策略优化的有力的方法。然而，logging数据和线上环境之间的差异可能会导致分布转移问题，许多先前的工作都在如何建立保守的转移模型和准确地估计模型的不确定性方面进行了研究。然而，过于保守的建模可能会限制代理人的探索，并且不确定性估计可能是不可靠的。在这种情况下，我们提出了一种新的模型基于的线上策略优化框架（MOAN）。关键思想是通过对抗学习建立一个更好的泛化模型，其中一个对手用于分辨在数据集中的含义和 OUT-OF-DISTRIBUTION 样本。此外，对手还可以提供有理性保证的模型不确定性量化。我们的方法在广泛的 Offline RL 基准数据集上进行了广泛的实验，并显示了与现有的基准值相比，我们的方法能够更高效地优化策略，同时也能够更准确地量化不确定性。
</details></li>
</ul>
<hr>
<h2 id="Making-Large-Language-Models-Better-Reasoners-with-Alignment"><a href="#Making-Large-Language-Models-Better-Reasoners-with-Alignment" class="headerlink" title="Making Large Language Models Better Reasoners with Alignment"></a>Making Large Language Models Better Reasoners with Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02144">http://arxiv.org/abs/2309.02144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui</li>
<li>for: The paper aims to enhance the reasoning capabilities of large language models (LLMs) by addressing the “Assessment Misalignment” problem, which occurs when fine-tuned LLMs assign higher scores to subpar chain of thought (COT) reasoning processes.</li>
<li>methods: The paper introduces an “Alignment Fine-Tuning” (AFT) paradigm that involves fine-tuning LLMs with COT training data, generating multiple COT responses for each question, and calibrating the scores of positive and negative responses with a novel constraint alignment loss.</li>
<li>results: Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT in enhancing the reasoning capabilities of LLMs. The constraint alignment loss is found to be crucial for the performance of recent ranking-based alignment methods, such as DPO, RRHF, and PRO.Here’s the Chinese version of the three key information points:</li>
<li>for: 论文目的是提高大型自然语言模型（LLM）的理解能力，并解决“评价不一致”问题， LLM 通过精心调整可以提高其理解能力。</li>
<li>methods: 论文引入了“对Alignment”（AFT）方法，包括在 COT 训练数据上精心调整 LLM，生成每个问题多个 COT 答案，并对 LLM 的答案进行评分。</li>
<li>results: 实验结果表明， AFT 可以有效地提高 LLM 的理解能力，并且发现 Constraint 对 recient ranking-based alignment方法的性能是关键的。<details>
<summary>Abstract</summary>
Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.
</details>
<details>
<summary>摘要</summary>
理智是一种认知过程，通过证据来达成有sound的结论。理智能力是人工通用智能代理人的关键 Component。latest studies show that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an Assessment Misalignment problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an Alignment Fine-Tuning (AFT) paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Rapid-and-Efficient-Deep-Convolutional-Network-for-Chest-X-Ray-Tuberculosis-Detection"><a href="#A-Lightweight-Rapid-and-Efficient-Deep-Convolutional-Network-for-Chest-X-Ray-Tuberculosis-Detection" class="headerlink" title="A Lightweight, Rapid and Efficient Deep Convolutional Network for Chest X-Ray Tuberculosis Detection"></a>A Lightweight, Rapid and Efficient Deep Convolutional Network for Chest X-Ray Tuberculosis Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02140">http://arxiv.org/abs/2309.02140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dani-capellan/LightTBNet">https://github.com/dani-capellan/LightTBNet</a></li>
<li>paper_authors: Daniel Capellán-Martín, Juan J. Gómez-Valverde, David Bermejo-Peláez, María J. Ledesma-Carbayo<br>for: 这个论文旨在提高肺部X射线图像的诊断精度，特别是用于检测肺结核病。methods: 该论文提出了一种新的轻量级、快速和计算效率低的深度学习模型，称为LightTBNet，用于从肺部X射线图像中检测肺结核病。results: 在使用2个公共数据集的800张前额CXR图像上，该模型实现了0.906、0.907和0.961的准确率、F1分数和ROC曲线的AUC值，并在独立测试集上达到了极高的性能。<details>
<summary>Abstract</summary>
Tuberculosis (TB) is still recognized as one of the leading causes of death worldwide. Recent advances in deep learning (DL) have shown to enhance radiologists' ability to interpret chest X-ray (CXR) images accurately and with fewer errors, leading to a better diagnosis of this disease. However, little work has been done to develop models capable of diagnosing TB that offer good performance while being efficient, fast and computationally inexpensive. In this work, we propose LightTBNet, a novel lightweight, fast and efficient deep convolutional network specially customized to detect TB from CXR images. Using a total of 800 frontal CXR images from two publicly available datasets, our solution yielded an accuracy, F1 and area under the ROC curve (AUC) of 0.906, 0.907 and 0.961, respectively, on an independent test subset. The proposed model demonstrates outstanding performance while delivering a rapid prediction, with minimal computational and memory requirements, making it highly suitable for deployment in handheld devices that can be used in low-resource areas with high TB prevalence. Code publicly available at https://github.com/dani-capellan/LightTBNet.
</details>
<details>
<summary>摘要</summary>
抑阻疾病（TB）仍被认为是全球主要的死亡原因之一。最近的深度学习（DL）突破有助于诊断医生更准确地解读胸部X射线图像（CXR），从而提高TB的诊断精度。然而，对于开发能够诊断TB的模型来说，还很少有工作。在这项工作中，我们提出了LightTBNet，一种特制的轻量级、快速和计算成本低的深度卷积网络，用于从CXR图像中检测TB。使用总计800个前面CXR图像，我们的解决方案在独立测试集上得到了0.906、0.907和0.961的准确率、F1分数和ROC曲线的抑阻率，并且模型具有快速预测和较低的计算和存储需求，因此非常适合在低TB患率地区使用手持式设备进行部署。代码公开可用于https://github.com/dani-capellan/LightTBNet。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Simplicial-Attention-Neural-Networks"><a href="#Generalized-Simplicial-Attention-Neural-Networks" class="headerlink" title="Generalized Simplicial Attention Neural Networks"></a>Generalized Simplicial Attention Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02138">http://arxiv.org/abs/2309.02138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luciatesta97/generalized-simplicial-attention-neural-networks">https://github.com/luciatesta97/generalized-simplicial-attention-neural-networks</a></li>
<li>paper_authors: Claudio Battiloro, Lucia Testa, Lorenzo Giusti, Stefania Sardellitti, Paolo Di Lorenzo, Sergio Barbarossa</li>
<li>for: 这个论文旨在介绍一种新的神经网络模型，即通用 simplicial attention neural network (GSAN)，用于处理定义在 simplicial 复合体上的数据。</li>
<li>methods: 这种模型使用 masked self-attention 层来处理数据Components，基于 тополоógical signal processing 原理，设计了一系列自注意机制，可以处理不同 simplicial 顺序的数据组成部分，例如节点、边、triangle 和更高阶的 simplicial 结构。</li>
<li>results: 该模型可以在多个 inductive 和 transductive 任务中表现出色，包括 trajectory prediction、missing data imputation、graph classification 和 simplex prediction。此外，模型还 theoretically establishes  permutation equivariant 和 simplicial-aware 性。<details>
<summary>Abstract</summary>
The aim of this work is to introduce Generalized Simplicial Attention Neural Networks (GSANs), i.e., novel neural architectures designed to process data defined on simplicial complexes using masked self-attentional layers. Hinging on topological signal processing principles, we devise a series of self-attention schemes capable of processing data components defined at different simplicial orders, such as nodes, edges, triangles, and beyond. These schemes learn how to weight the neighborhoods of the given topological domain in a task-oriented fashion, leveraging the interplay among simplices of different orders through the Dirac operator and its Dirac decomposition. We also theoretically establish that GSANs are permutation equivariant and simplicial-aware. Finally, we illustrate how our approach compares favorably with other methods when applied to several (inductive and transductive) tasks such as trajectory prediction, missing data imputation, graph classification, and simplex prediction.
</details>
<details>
<summary>摘要</summary>
本工作的目标是介绍通用 simplicial 注意网络（GSAN），即基于 topological signal processing 原理的新型神经网络，用于处理定义在 simplicial 复合体上的数据。我们提出了一系列自我注意机制，可以处理不同 simplicial 顺序的数据组件，如节点、边、triangle 等，并通过DIRAC 算子和其 decompositions 学习权重邻域。我们还证明了 GSAN 具有 permutation 变换对称和 simplicial 意识。最后，我们比较了我们的方法与其他方法在 inductive 和 transductive 任务上的性能，包括 trajectory prediction、missing data imputation、graph classification 和 simplex prediction。Here's the translation in Traditional Chinese:本研究的目的是介绍通用 simplicial 注意网络（GSAN），即基于 topological signal processing 原理的新型神经网络，用于处理定义在 simplicial 复合体上的数据。我们提出了一系列自我注意机制，可以处理不同 simplicial 顺序的数据组件，如节点、边、triangle 等，并通过DIRAC 算子和其 decompositions 学习权重邻域。我们还证明了 GSAN 具有 permutation 变换对称和 simplicial 意识。最后，我们比较了我们的方法与其他方法在 inductive 和 transductive 任务上的性能，包括 trajectory prediction、missing data imputation、graph classification 和 simplex prediction。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Asymmetric-Momentum-Make-SGD-Greatest-Again"><a href="#A-Simple-Asymmetric-Momentum-Make-SGD-Greatest-Again" class="headerlink" title="A Simple Asymmetric Momentum Make SGD Greatest Again"></a>A Simple Asymmetric Momentum Make SGD Greatest Again</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02130">http://arxiv.org/abs/2309.02130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gongyue Zhang, Dinghuang Zhang, Shuwen Zhao, Donghan Liu, Carrie M. Toptan, Honghai Liu</li>
<li>for: 提高SGD算法的稳定性和精度，帮助避免极值点问题</li>
<li>methods: 使用损控制的非Symmetric摩擦（LCAM）技术，利用Weight conjugation和Traction effect来解释其工作机制</li>
<li>results: 比传统的SGD WITH momentum更高的性能，减少了计算复杂度，并在WRN28-10网络上实现了80.78%的峰值测试精度，比原始WRN paper和CAS更高，且用于 Nearly half convergence time.<details>
<summary>Abstract</summary>
We propose the simplest SGD enhanced method ever, Loss-Controlled Asymmetric Momentum(LCAM), aimed directly at the Saddle Point problem. Compared to the traditional SGD with Momentum, there's no increase in computational demand, yet it outperforms all current optimizers. We use the concepts of weight conjugation and traction effect to explain this phenomenon. We designed experiments to rapidly reduce the learning rate at specified epochs to trap parameters more easily at saddle points. We selected WRN28-10 as the test network and chose cifar10 and cifar100 as test datasets, an identical group to the original paper of WRN and Cosine Annealing Scheduling(CAS). We compared the ability to bypass saddle points of Asymmetric Momentum with different priorities. Finally, using WRN28-10 on Cifar100, we achieved a peak average test accuracy of 80.78\% around 120 epoch. For comparison, the original WRN paper reported 80.75\%, while CAS was at 80.42\%, all at 200 epoch. This means that while potentially increasing accuracy, we use nearly half convergence time. Our demonstration code is available at\\ https://github.com/hakumaicc/Asymmetric-Momentum-LCAM
</details>
<details>
<summary>摘要</summary>
我们提出了最简单的SGD增强方法──损失控制对称动量（LCAM），直接解决点积问题。与传统的SGD增强方法相比，我们的方法不增加计算负载，却能超越现有的增强器。我们利用几何 conjugation 和拖动效应来解释这一现象。我们设计了实验，以迅速降低学习率在指定的epoch中，以更容易地将参数固定在点积点。我们使用 WRN28-10 网络和 Cifar10 和 Cifar100 作为测试集，与原始 WRN 和 Cosine Annealing Scheduling（CAS）的测试集一样。我们比较不同优先顺位的对称动量对点积点的穿透性。最后，使用 WRN28-10 在 Cifar100 上 дости得了约 120 epoch 的峰值平均测试精度为 80.78%。与原始 WRN 报告的 80.75% 和 CAS 的 80.42% 相比，我们的方法可能增加精度，但使用的是近乎半倍的训练时间。我们的示例代码可以在 https://github.com/hakumaicc/Asymmetric-Momentum-LCAM 上找到。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Spatial-temporal-Data-for-Sleep-Stage-Classification-via-Hypergraph-Learning"><a href="#Exploiting-Spatial-temporal-Data-for-Sleep-Stage-Classification-via-Hypergraph-Learning" class="headerlink" title="Exploiting Spatial-temporal Data for Sleep Stage Classification via Hypergraph Learning"></a>Exploiting Spatial-temporal Data for Sleep Stage Classification via Hypergraph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02124">http://arxiv.org/abs/2309.02124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuze Liu, Ziming Zhao, Tiehua Zhang, Kang Wang, Xin Chen, Xiaowei Huang, Jun Yin, Zhishu Shen</li>
<li>for: 静脉sleepstage分类是诊断疾病的关键，现有模型主要使用卷积神经网络(CNN)和图 convolutional neural network(GNN)来模型几何数据和非几何数据，但是它们无法同时考虑多Modal数据的异质和交互，以及空间-时间相关性，从而限制了分类性能的提高。</li>
<li>methods: 我们提出了一种动态学习框架STHL，该框架使用幂图来编码空间-时间数据，幂图可以构建多Modal&#x2F;多种类数据而不是使用简单的对两个主体的对比。STHL创建空间和时间幂边分别来建立节点关系，然后进行类型特定幂图学习过程来编码特征到嵌入空间。</li>
<li>results: 我们的提出的STHL在sleep stage分类任务中表现出色，超过了当前状态的模型。<details>
<summary>Abstract</summary>
Sleep stage classification is crucial for detecting patients' health conditions. Existing models, which mainly use Convolutional Neural Networks (CNN) for modelling Euclidean data and Graph Convolution Networks (GNN) for modelling non-Euclidean data, are unable to consider the heterogeneity and interactivity of multimodal data as well as the spatial-temporal correlation simultaneously, which hinders a further improvement of classification performance. In this paper, we propose a dynamic learning framework STHL, which introduces hypergraph to encode spatial-temporal data for sleep stage classification. Hypergraphs can construct multi-modal/multi-type data instead of using simple pairwise between two subjects. STHL creates spatial and temporal hyperedges separately to build node correlations, then it conducts type-specific hypergraph learning process to encode the attributes into the embedding space. Extensive experiments show that our proposed STHL outperforms the state-of-the-art models in sleep stage classification tasks.
</details>
<details>
<summary>摘要</summary>
“睡眠阶段分类是诊断病人健康状况的关键。现有的模型主要使用卷积神经网络（CNN）来模型几何数据，以及图 convolutional neural network（GNN）来模型非几何数据，但这些模型无法同时考虑多modal数据的异质性和交互性以及空间-时间相关性，这限制了分类性能的进一步提高。本文提出了一种动态学习框架 STHL，它通过引入卷积 Graph 来编码空间-时间数据进行睡眠阶段分类。卷积 Graph 可以构造多modal/多种数据而不是使用简单的对两个主体之间的对应。STHL 首先在空间和时间上分别建立 hyperedge，然后进行类型特定的卷积 Graph 学习过程来编码特征到嵌入空间。广泛的实验表明，我们提出的 STHL 在睡眠阶段分类任务中表现出了比前状态艺术模型更高的性能。”Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Label-Information-for-Multimodal-Emotion-Recognition"><a href="#Leveraging-Label-Information-for-Multimodal-Emotion-Recognition" class="headerlink" title="Leveraging Label Information for Multimodal Emotion Recognition"></a>Leveraging Label Information for Multimodal Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02106">http://arxiv.org/abs/2309.02106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Digimonseeker/LE-MER">https://github.com/Digimonseeker/LE-MER</a></li>
<li>paper_authors: Peiying Wang, Sunlu Zeng, Junqing Chen, Lu Fan, Meng Chen, Youzheng Wu, Xiaodong He</li>
<li>for: 本研究旨在提高多Modal Emotion Recognition（MER）的准确率，通过结合语音和文本信息。</li>
<li>methods: 我们提出了一种新的方法，利用标签信息来提高MER。首先，我们获取语音和文本模态的代表标签嵌入，然后通过标签-token和标签-帧交互学习每个语音的标签感知表示。最后，我们设计了一种标签引导的混合模块，将标签意识的语音和文本表示进行情绪分类。</li>
<li>results: 我们在公共的IEMOCAP dataset上进行了广泛的实验，结果表明，我们的提议方法在基础模型的比较下，超越了现有的基线，达到了新的状态域性能。<details>
<summary>Abstract</summary>
Multimodal emotion recognition (MER) aims to detect the emotional status of a given expression by combining the speech and text information. Intuitively, label information should be capable of helping the model locate the salient tokens/frames relevant to the specific emotion, which finally facilitates the MER task. Inspired by this, we propose a novel approach for MER by leveraging label information. Specifically, we first obtain the representative label embeddings for both text and speech modalities, then learn the label-enhanced text/speech representations for each utterance via label-token and label-frame interactions. Finally, we devise a novel label-guided attentive fusion module to fuse the label-aware text and speech representations for emotion classification. Extensive experiments were conducted on the public IEMOCAP dataset, and experimental results demonstrate that our proposed approach outperforms existing baselines and achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
多modal情感识别（MER）目的是检测给定表达中的情感状态，通过结合语音和文本信息。Intuitively，标签信息应该能够帮助模型 locate 关键的框架和字符串，从而实现 MER 任务。 inspirited by this，我们提出了一种 novel approach for MER，利用标签信息。specifically，我们首先获得了表达和语音模式的表示性标签嵌入，然后通过标签-Token和标签-框架交互学习label-aware的文本和语音表示。finally，我们设计了一种标签导向的束合模块，将标签 aware的文本和语音表示进行情感分类。我们在公共的IEMOCAP数据集上进行了广泛的实验，实验结果表明，我们提出的方法在exist的基准点上出perform，并实现了新的状态anner-of-the-art performance。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Superquadric-Recomposition-of-3D-Objects-from-Multiple-Views"><a href="#Iterative-Superquadric-Recomposition-of-3D-Objects-from-Multiple-Views" class="headerlink" title="Iterative Superquadric Recomposition of 3D Objects from Multiple Views"></a>Iterative Superquadric Recomposition of 3D Objects from Multiple Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02102">http://arxiv.org/abs/2309.02102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/explainableml/isco">https://github.com/explainableml/isco</a></li>
<li>paper_authors: Stephan Alaniz, Massimiliano Mancini, Zeynep Akata</li>
<li>for: 这个论文旨在提出一种框架，即ISCO，以便从2D视图直接使用3D超quadrics来重建对象，无需训练3D超视觉模型。</li>
<li>methods: 该框架使用3D超quadrics作为semantic part，通过比较rendered 3D视图和2D图像缩影来优化超quadrics参数，进行 iterative 添加新的超quadrics，从粗略区域开始，然后是细节。</li>
<li>results: 实验表明，相比最近的单个实例超quadrics重建方法，ISCO提供了更加准确的3D重建结果，即使是从野生图像中。代码可以在<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/ISCO">https://github.com/ExplainableML/ISCO</a> 上获取。<details>
<summary>Abstract</summary>
Humans are good at recomposing novel objects, i.e. they can identify commonalities between unknown objects from general structure to finer detail, an ability difficult to replicate by machines. We propose a framework, ISCO, to recompose an object using 3D superquadrics as semantic parts directly from 2D views without training a model that uses 3D supervision. To achieve this, we optimize the superquadric parameters that compose a specific instance of the object, comparing its rendered 3D view and 2D image silhouette. Our ISCO framework iteratively adds new superquadrics wherever the reconstruction error is high, abstracting first coarse regions and then finer details of the target object. With this simple coarse-to-fine inductive bias, ISCO provides consistent superquadrics for related object parts, despite not having any semantic supervision. Since ISCO does not train any neural network, it is also inherently robust to out-of-distribution objects. Experiments show that, compared to recent single instance superquadrics reconstruction approaches, ISCO provides consistently more accurate 3D reconstructions, even from images in the wild. Code available at https://github.com/ExplainableML/ISCO .
</details>
<details>
<summary>摘要</summary>
人类善于重新组合新的物体，即可以从通用结构到细节上认出未知物体的共同点，这是机器很难复制的能力。我们提出了一个框架，即ISCO，可以使用2D视图直接从3D超quadrics中提取 semantic parts，无需训练一个使用3D超vision的模型。为了实现这一点，我们优化了超quadric参数，以组成特定物体的实例，并比较其渲染后的3D视图和2D图像轮廓。我们的ISCO框架会逐次添加新的超quadrics，以降低重建错误，从抽象到细节的方式进行卷积。通过这种简单的卷积偏好，ISCO提供了相关的对象部分的一致的超quadrics，即使没有任何 semantic 超视。由于ISCO不需要训练任何神经网络，因此也是自然地对 OUT-OF-distribution 对象有效。实验表明，相比最近的单个实例超quadrics重建方法，ISCO可以提供更加准确的3D重建结果，甚至从野外图像中。代码可以在 https://github.com/ExplainableML/ISCO 上找到。
</details></li>
</ul>
<hr>
<h2 id="TensorBank-Tensor-Lakehouse-for-Foundation-Model-Training"><a href="#TensorBank-Tensor-Lakehouse-for-Foundation-Model-Training" class="headerlink" title="TensorBank:Tensor Lakehouse for Foundation Model Training"></a>TensorBank:Tensor Lakehouse for Foundation Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02094">http://arxiv.org/abs/2309.02094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romeo Kienzler, Benedikt Blumenstiel, Zoltan Arnold Nagy, S. Karthik Mukkavilli, Johannes Schmude, Marcus Freitag, Michael Behrendt, Daniel Salles Civitarese, Naomi Simumba, Daiki Kimura, Hendrik Hamann</li>
<li>for: 这个论文旨在提出一种基于云存储和GPU内存的高维数据存储和流处理技术，用于训练基础模型。</li>
<li>methods: 这个论文使用了TensorBank，一种可以在云存储中流动地读取和写入高维数据的tensor lakehouse，以及Hierarchical Statistical Indices (HSI)等技术来加速查询。</li>
<li>results: 这个论文的实验结果表明，使用TensorBank和HSI可以在高维数据流处理中实现高速查询和数据转换，并且可以满足不同领域的需求，如计算机视觉、生物物理学、神经科学等。<details>
<summary>Abstract</summary>
Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making heavy use of open-source technology. Although, hardened for production use using geospatial-temporal data, this architecture generalizes to other use case like computer vision, computational neuroscience, biological sequence analysis and more.
</details>
<details>
<summary>摘要</summary>
保存和流处高维数据为基础模型训练成为了现代基础模型的关键要求。在这篇论文中，我们介绍了TensorBank，一个 Petabyte 级 tensor 湖屋，可以将 Cloud Object Store（COS）中的tensor流到 GPU 内存的缓存器，并基于复杂的关系查询使用 Hierarchical Statistical Indices（HSI）进行加速。我们的架构允许直接在块级别上地址 tensor，使用 HTTP 范围读取。将数据转换为 PyTorch 变换后，我们提供了一个通用 PyTorch 数据集类型，并将其与相应的数据工厂相关联，以便将关系查询和请求的转换作为实例。通过使用 HSI，我们可以跳过无关块，因为它们的统计信息在不同层次结构级别中具有不同的内容。这是一种基于开源技术的意见架构，并且通过使用 geospatial-temporal 数据进行硬化，可以普遍应用于其他应用场景，如计算机视觉、计算神经科学、生物Sequencing 分析等。
</details></li>
</ul>
<hr>
<h2 id="Natural-Example-Based-Explainability-a-Survey"><a href="#Natural-Example-Based-Explainability-a-Survey" class="headerlink" title="Natural Example-Based Explainability: a Survey"></a>Natural Example-Based Explainability: a Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03234">http://arxiv.org/abs/2309.03234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danderfer/Comp_Sci_Sem_2">https://github.com/danderfer/Comp_Sci_Sem_2</a></li>
<li>paper_authors: Antonin Poché, Lucas Hervier, Mohamed-Chafik Bakkay</li>
<li>for: 这篇论文旨在提供natural例子基于XAI的现状报告，描述了每种方法的优缺点，并比较它们的语义定义、认知影响和加值。</li>
<li>methods: 这篇论文涵盖了自然例子基于XAI的多种方法，包括相似例子、counterfactual和semi-factual例子、重要实例、概念范围和prototype等方法。</li>
<li>results: 论文总结了每种方法的优缺点，并比较了它们的语义定义、认知影响和加值，以便促进和促进未来的自然例子基于XAI工作。<details>
<summary>Abstract</summary>
Explainable Artificial Intelligence (XAI) has become increasingly significant for improving the interpretability and trustworthiness of machine learning models. While saliency maps have stolen the show for the last few years in the XAI field, their ability to reflect models' internal processes has been questioned. Although less in the spotlight, example-based XAI methods have continued to improve. It encompasses methods that use examples as explanations for a machine learning model's predictions. This aligns with the psychological mechanisms of human reasoning and makes example-based explanations natural and intuitive for users to understand. Indeed, humans learn and reason by forming mental representations of concepts based on examples.   This paper provides an overview of the state-of-the-art in natural example-based XAI, describing the pros and cons of each approach. A "natural" example simply means that it is directly drawn from the training data without involving any generative process. The exclusion of methods that require generating examples is justified by the need for plausibility which is in some regards required to gain a user's trust. Consequently, this paper will explore the following family of methods: similar examples, counterfactual and semi-factual, influential instances, prototypes, and concepts. In particular, it will compare their semantic definition, their cognitive impact, and added values. We hope it will encourage and facilitate future work on natural example-based XAI.
</details>
<details>
<summary>摘要</summary>
《可解释人工智能（XAI）在提高机器学习模型的可解释性和可信worthiness方面变得越来越重要。尽管焦点图在XAI领域中备受推崇，但它们能够反映模型内部过程的能力受到质疑。虽然不太受注意，但示例基于的XAI方法在不断改进。它包括使用示例来解释机器学习模型的预测的方法。这与人类的认知机理相符，使得示例基于的解释自然和直观，让用户更好地理解。实际上，人类学习和理解都是通过形成示例基于的认知来实现的。本文提供了当前自然示例基于XAI的状态艺术概述，描述了每种方法的优缺点。一个“自然”示例指的是直接从训练数据中提取的示例，不包括任何生成过程。这种排除方法是因为需要可靠性，以获得用户的信任。因此，本文将探讨以下家族方法：相似示例、 counterfactual和半实例、原型、概念。尤其是 comparing their semantic definition, cognitive impact, and added value. 我们希望这篇文章能够鼓励和促进未来的自然示例基于XAI的研究。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Approach-to-Unsupervised-Out-of-Distribution-Detection-with-Variational-Autoencoders"><a href="#An-Efficient-Approach-to-Unsupervised-Out-of-Distribution-Detection-with-Variational-Autoencoders" class="headerlink" title="An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders"></a>An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02084">http://arxiv.org/abs/2309.02084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjlab-ammi/vae4ood">https://github.com/zjlab-ammi/vae4ood</a></li>
<li>paper_authors: Zezhen Zeng, Bin Liu</li>
<li>for: 这 paper 关注 deep generative models (DGMs) 的Unsupervised out-of-distribution (OOD) detection。特别是，我们关注 vanilla Variational Autoencoders (VAE) 使用标准正态分布 для隐藏变量。这些模型具有更小的模型大小，使得更快的训练和推断，适用于有限资源应用程序比较复杂的 DGMs。</li>
<li>methods: 我们提出了一个新的 OOD 分数，叫做 Error Reduction (ER)，专门为 vanilla VAE 设计。ER 利用图像输入的损失函数重建图像，并考虑图像的科隆摩擦复杂性。</li>
<li>results: 我们在多个 dataset 上进行了实验，与基线方法相比，我们的方法显示出了更高的超越性。我们的代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/ZJLAB-AMMI/VAE4OOD%E3%80%82">https://github.com/ZJLAB-AMMI/VAE4OOD。</a><details>
<summary>Abstract</summary>
This paper is concerned with deep generative models (DGMs) for unsupervised out-of-distribution (OOD) detection. In particular, we focus on vanilla Variational Autoencoders (VAE) that use a standard normal prior distribution for the latent variables. These models have a smaller model size, enabling faster training and inference, making them well-suited for resource-limited applications compared to more complex DGMs. We propose a novel OOD score called Error Reduction (ER) specifically designed for vanilla VAE. ER incorporate the idea of reconstructing image inputs from their lossy counterparts and takes into account the Kolmogorov complexity of the images. Experimental results on diverse datasets demonstrate the superiority of our approach over baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/VAE4OOD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BeeTLe-A-Framework-for-Linear-B-Cell-Epitope-Prediction-and-Classification"><a href="#BeeTLe-A-Framework-for-Linear-B-Cell-Epitope-Prediction-and-Classification" class="headerlink" title="BeeTLe: A Framework for Linear B-Cell Epitope Prediction and Classification"></a>BeeTLe: A Framework for Linear B-Cell Epitope Prediction and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02071">http://arxiv.org/abs/2309.02071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanx749/bcell">https://github.com/yuanx749/bcell</a></li>
<li>paper_authors: Xiao Yuan</li>
<li>for: 这 paper 是为了提出一种新的深度学习基于多任务框架，用于线性 B 细胞 Epitope 预测和抗体类型特异 Epitope 分类。</li>
<li>methods: 该 paper 使用了序列基的神经网络模型，包括回卷层和 Transformer 层，以及一种基于 eigen 分解的 amino acid 编码方法，以帮助模型学习 Epitope 的表示。</li>
<li>results: 实验结果表明，提出的方法有效地预测了 B 细胞 Epitope，并与其他竞争方法相比，表现出色。<details>
<summary>Abstract</summary>
The process of identifying and characterizing B-cell epitopes, which are the portions of antigens recognized by antibodies, is important for our understanding of the immune system, and for many applications including vaccine development, therapeutics, and diagnostics. Computational epitope prediction is challenging yet rewarding as it significantly reduces the time and cost of laboratory work. Most of the existing tools do not have satisfactory performance and only discriminate epitopes from non-epitopes. This paper presents a new deep learning-based multi-task framework for linear B-cell epitope prediction as well as antibody type-specific epitope classification. Specifically, a sequenced-based neural network model using recurrent layers and Transformer blocks is developed. We propose an amino acid encoding method based on eigen decomposition to help the model learn the representations of epitopes. We introduce modifications to standard cross-entropy loss functions by extending a logit adjustment technique to cope with the class imbalance. Experimental results on data curated from the largest public epitope database demonstrate the validity of the proposed methods and the superior performance compared to competing ones.
</details>
<details>
<summary>摘要</summary>
“识别和Characterizing B细胞结构，即抗体认知的部分，是免疫系统理解和许多应用，如疫苗开发、治疗和诊断。 computation epitope prediction 是一个挑战性的任务，但是可以快速地减少实验室工作的时间和成本。 现有的工具中，只有一些具有不 satisfactory 的表现，并只能区分 epitope 和 non-epitope。 本文提出了一个新的深度学习基于多任务框架 Linear B细胞结构预测，以及抗体类型特定的 epitope 分类。 具体来说，我们开发了一个序列化的神经网络模型，使用回归层和 transformer 层。 我们提出了一个使用 eigen decomposition 来编码氨基酸的方法，帮助模型学习 epitope 的表现。 我们还对标准的十字熵损失函数进行修改，以处理类别偏好。 实验结果显示，提出的方法是有效的，并且与竞争方法相比，表现更好。”Note: "Simplified Chinese" is used here to refer to the written form of Chinese that uses simpler characters and grammar, as opposed to "Traditional Chinese" which is the more complex and traditional form of written Chinese.
</details></li>
</ul>
<hr>
<h2 id="Efficiency-is-Not-Enough-A-Critical-Perspective-of-Environmentally-Sustainable-AI"><a href="#Efficiency-is-Not-Enough-A-Critical-Perspective-of-Environmentally-Sustainable-AI" class="headerlink" title="Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI"></a>Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02065">http://arxiv.org/abs/2309.02065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Wright, Christian Igel, Gabrielle Samuel, Raghavendra Selvan</li>
<li>for: 本研究旨在探讨机器学习（ML）技术的环境可持续性问题，尤其是计算机浪费和能源消耗的问题。</li>
<li>methods: 本文使用了多种技术和非技术方法来探讨ML技术的环境可持续性问题，包括计算机浪费和能源消耗的分析、环境影响分析等。</li>
<li>results: 本文表明，提高ML技术的效率并不能够完全解决其环境可持续性问题。作者提出了一种系统思维的方法来解决这些问题，即考虑ML技术与其他变量之间的互动关系，以提高ML技术的环境可持续性。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) is currently spearheaded by machine learning (ML) methods such as deep learning (DL) which have accelerated progress on many tasks thought to be out of reach of AI. These ML methods can often be compute hungry, energy intensive, and result in significant carbon emissions, a known driver of anthropogenic climate change. Additionally, the platforms on which ML systems run are associated with environmental impacts including and beyond carbon emissions. The solution lionized by both industry and the ML community to improve the environmental sustainability of ML is to increase the efficiency with which ML systems operate in terms of both compute and energy consumption. In this perspective, we argue that efficiency alone is not enough to make ML as a technology environmentally sustainable. We do so by presenting three high level discrepancies between the effect of efficiency on the environmental sustainability of ML when considering the many variables which it interacts with. In doing so, we comprehensively demonstrate, at multiple levels of granularity both technical and non-technical reasons, why efficiency is not enough to fully remedy the environmental impacts of ML. Based on this, we present and argue for systems thinking as a viable path towards improving the environmental sustainability of ML holistically.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>计算和能源占用的差异：虽然提高 ML 系统的效率可以减少计算和能源占用，但是这些减少的量可能并不足以抵消 ML 系统的总碳排放。2. 数据和模型的差异：ML 系统的数据和模型可能会带来额外的环境影响，例如数据收集和处理的能源占用、模型的训练和测试所需的计算资源等。3. 生产和供应链的差异：ML 系统的生产和供应链可能会带来额外的环境影响，例如硬件生产和运输的能源占用、物资和设备的生产和供应等。因此，我们认为，只是提高 ML 系统的效率不足以全面解决 ML 对环境的影响。我们提出了以系统思维为基础的可持续性解决方案，以确保 ML 技术的发展和应用不会对环境产生负面影响。</details></li>
</ol>
<hr>
<h2 id="MvFS-Multi-view-Feature-Selection-for-Recommender-System"><a href="#MvFS-Multi-view-Feature-Selection-for-Recommender-System" class="headerlink" title="MvFS: Multi-view Feature Selection for Recommender System"></a>MvFS: Multi-view Feature Selection for Recommender System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02064">http://arxiv.org/abs/2309.02064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youngjune Lee, Yeongjong Jeong, Keunchan Park, SeongKu Kang</li>
<li>for: 提高推荐系统中功能选择的精度和效果，适应不同数据范围内的特征场景。</li>
<li>methods: 基于多视图网络，每个子网络学习不同特征模式下的特征重要性评估方法，以避免主导特征的偏袋问题，提高功能选择的多样性和效果。</li>
<li>results: 对实际数据进行实验，与现状的基准方法相比，提出了更高的精度和效果。<details>
<summary>Abstract</summary>
Feature selection, which is a technique to select key features in recommender systems, has received increasing research attention. Recently, Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations in that its selection process could be easily biased to major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective importance score modeling strategy which is applied independently to each field without incurring dependency among features. Experimental results on real-world datasets demonstrate the effectiveness of MvFS compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Feature selection，即在推荐系统中选择关键特征的技术，在研究中得到了越来越多的注意。最近，Adaptive Feature Selection（AdaFS）表现出了很好的表现，它可以适应选择每个数据实例中的特征，因为特征的重要性可以在数据中差异很大。然而，AdaFS方法仍然存在一些限制，即它的选择过程可能会受到主要特征的偏袋偏见。为解决这些问题，我们提议了 Multi-view Feature Selection（MvFS），它可以更有效地选择每个实例中的信息特征。最重要的是，MvFS使用了多视图网络，每个子网络都学习了不同特征模式下数据中的特征重要性。这样，MvFS可以减少偏袋偏见问题，并且促进了更加平衡的特征选择过程。另外，MvFS采用了一种独立应用于每个字段的重要性分数模型化策略，不会产生特征之间的依赖关系。实验结果表明，MvFS比 estado-of-the-art 基准方法更有效。
</details></li>
</ul>
<hr>
<h2 id="No-Regret-Caching-with-Noisy-Request-Estimates"><a href="#No-Regret-Caching-with-Noisy-Request-Estimates" class="headerlink" title="No-Regret Caching with Noisy Request Estimates"></a>No-Regret Caching with Noisy Request Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02055">http://arxiv.org/abs/2309.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Younes Ben Mazziane, Francescomaria Faticanti, Giovanni Neglia, Sara Alouf</li>
<li>for: 本研究旨在设计缓存策略，以满足在高负载和&#x2F;或内存缓存环境下的请求序列预测不准确情况。</li>
<li>methods: 本研究提出了噪声跟踪领导者（NFPL）算法，这是经典跟踪领导者（FPL）算法的一种变种，当请求估计噪声时，它可以提供下线 regret 的保证。</li>
<li>results: 研究人员通过比较传统缓存策略和提出的方案，并在实验中验证了提出的方法在做出缓存决策时的性能。<details>
<summary>Abstract</summary>
Online learning algorithms have been successfully used to design caching policies with regret guarantees. Existing algorithms assume that the cache knows the exact request sequence, but this may not be feasible in high load and/or memory-constrained scenarios, where the cache may have access only to sampled requests or to approximate requests' counters. In this paper, we propose the Noisy-Follow-the-Perturbed-Leader (NFPL) algorithm, a variant of the classic Follow-the-Perturbed-Leader (FPL) when request estimates are noisy, and we show that the proposed solution has sublinear regret under specific conditions on the requests estimator. The experimental evaluation compares the proposed solution against classic caching policies and validates the proposed approach under both synthetic and real request traces.
</details>
<details>
<summary>摘要</summary>
在线学习算法已经成功地设计了储存策略，并提供了 regret 保证。现有的算法假设缓存知道精确的请求序列，但在高负载和/或内存受限的情况下，这可能无法实现。在这篇论文中，我们提出了受扰引 perturbed 领袖（NFPL）算法，这是 класи的 Follow-the-Perturbed-Leader（FPL）在请求估计不精确时的变形，并证明了我们的解决方案具有下线性 regret 的特性。实验评估比较了我们的解决方案与传统的储存策略，并在实验中验证了我们的方法在实际请求追踪中的可行性。
</details></li>
</ul>
<hr>
<h2 id="Model-agnostic-network-inference-enhancement-from-noisy-measurements-via-curriculum-learning"><a href="#Model-agnostic-network-inference-enhancement-from-noisy-measurements-via-curriculum-learning" class="headerlink" title="Model-agnostic network inference enhancement from noisy measurements via curriculum learning"></a>Model-agnostic network inference enhancement from noisy measurements via curriculum learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02050">http://arxiv.org/abs/2309.02050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Wu, Yuanyuan Li, Jing Liu</li>
<li>For: The paper aims to enhance the performance of network inference models in the presence of noise.* Methods: The proposed framework leverages curriculum learning to mitigate the impact of noisy samples on network inference models. It is model-agnostic and can be integrated into various model-based and model-free network inference methods.* Results: The proposed framework demonstrates substantial performance augmentation under varied noise types, particularly when clean samples are abundant. It outperforms existing methods in various synthetic and real-world networks with diverse nonlinear dynamic processes.Here is the information in Simplified Chinese text:* For: 本文旨在增强网络推理模型在噪声中的性能。* Methods: 提议的框架利用课程学习来减轻噪声样本对网络推理模型的影响。它是模型无关的，可以与多种模型基于和模型自由的网络推理方法集成。* Results: 提议的框架在不同的噪声类型下示出了显著的性能提升，特别在有多个清晰样本的情况下。它比现有方法在多种 sintetic 和实际网络中表现出色，包括不同的非线性动力过程。<details>
<summary>Abstract</summary>
Noise is a pervasive element within real-world measurement data, significantly undermining the performance of network inference models. However, the quest for a comprehensive enhancement framework capable of bolstering noise resistance across a diverse array of network inference models has remained elusive. Here, we present an elegant and efficient framework tailored to amplify the capabilities of network inference models in the presence of noise. Leveraging curriculum learning, we mitigate the deleterious impact of noisy samples on network inference models. Our proposed framework is model-agnostic, seamlessly integrable into a plethora of model-based and model-free network inference methods. Notably, we utilize one model-based and three model-free network inference methods as the foundation. Extensive experimentation across various synthetic and real-world networks, encapsulating diverse nonlinear dynamic processes, showcases substantial performance augmentation under varied noise types, particularly thriving in scenarios enriched with clean samples. This framework's adeptness in fortifying both model-free and model-based network inference methodologies paves the avenue towards a comprehensive and unified enhancement framework, encompassing the entire spectrum of network inference models. Available Code: https://github.com/xiaoyuans/MANIE.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>噪声是现实世界测量数据中的一种普遍存在的元素，对网络推理模型的性能产生重要的损害。然而，找到一个全面提升框架，能够在多种网络推理模型中增强噪声抗性，一直是艰难的探索。在这里，我们提出了一种简洁而高效的框架，用于增强网络推理模型在噪声中的性能。通过课程学习，我们将噪声样本的负面影响降至最低。我们的提案的框架是模型无关的，可以顺利地与多种模型基于和模型自由的网络推理方法集成。特别是，我们使用了一个模型基于的和三个模型自由的网络推理方法作为基础。广泛的实验表明，我们的框架在不同的噪声类型下表现出了显著的性能提升，特别是在含有干净样本的场景下卓越。这种框架的强大性在增强模型基于和模型自由的网络推理方法中表现出了一个普遍的和完整的提升框架，开启了涵盖整个网络推理模型谱系的全面提升框架的新可能性。可以在 GitHub 上获取代码：https://github.com/xiaoyuans/MANIE。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Self-supervised-Learning-via-Scoring-Rules-Minimization"><a href="#Probabilistic-Self-supervised-Learning-via-Scoring-Rules-Minimization" class="headerlink" title="Probabilistic Self-supervised Learning via Scoring Rules Minimization"></a>Probabilistic Self-supervised Learning via Scoring Rules Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02048">http://arxiv.org/abs/2309.02048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Vahidi, Simon Schoßer, Lisa Wimmer, Yawei Li, Bernd Bischl, Eyke Hüllermeier, Mina Rezaei<br>for:本文提出了一种新的probabilistic自监学习方法（ProSMIN），用于提高表示质量并避免表示塌积。methods:方法使用了两个神经网络：在线网络和目标网络，它们之间协作学习不同视图下的样本表示的多样性分布。通过知识传播，两个网络学习对方的表示。results:本文通过多种下游任务的实验表明，ProSMIN可以获得superior的准确率和准确性。在ImageNet-O和ImageNet-C大规模 dataset上，ProSMIN超过了自我监督基线，表明其可扩展性和实际应用性。<details>
<summary>Abstract</summary>
In this paper, we propose a novel probabilistic self-supervised learning via Scoring Rule Minimization (ProSMIN), which leverages the power of probabilistic models to enhance representation quality and mitigate collapsing representations. Our proposed approach involves two neural networks; the online network and the target network, which collaborate and learn the diverse distribution of representations from each other through knowledge distillation. By presenting the input samples in two augmented formats, the online network is trained to predict the target network representation of the same sample under a different augmented view. The two networks are trained via our new loss function based on proper scoring rules. We provide a theoretical justification for ProSMIN's convergence, demonstrating the strict propriety of its modified scoring rule. This insight validates the method's optimization process and contributes to its robustness and effectiveness in improving representation quality. We evaluate our probabilistic model on various downstream tasks, such as in-distribution generalization, out-of-distribution detection, dataset corruption, low-shot learning, and transfer learning. Our method achieves superior accuracy and calibration, surpassing the self-supervised baseline in a wide range of experiments on large-scale datasets like ImageNet-O and ImageNet-C, ProSMIN demonstrates its scalability and real-world applicability.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的概率自助学习方法，即 Scoring Rule Minimization（ProSMIN），该方法利用概率模型来提高表示质量并避免表示塌雷。我们的提议方法包括两个神经网络：在线网络和目标网络，这两个网络共同学习输入样本的多样化分布。我们通过知识传承来让在线网络预测目标网络对同一个样本的不同扩展视图的表示。我们使用我们新的损失函数基于适当的分数规则来训练这两个网络。我们提供了对ProSMIN的优化过程的理论证明，证明其修改后的分数规则的稳定性。这种理论支持ProSMIN的优化过程，并且对其效果和可靠性做出了贡献。我们在各种下游任务上评估了我们的概率模型，包括在样本集中的概率泛化、样本集外的概率检测、数据集损害、低shot学习和传输学习。我们的方法在各种实验中都达到了更高的准确率和准确性，比自助学习基eline在广泛的实验中表现出更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Enhance-Multi-domain-Sentiment-Analysis-of-Review-Texts-through-Prompting-Strategies"><a href="#Enhance-Multi-domain-Sentiment-Analysis-of-Review-Texts-through-Prompting-Strategies" class="headerlink" title="Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies"></a>Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02045">http://arxiv.org/abs/2309.02045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yajing Wang, Zongwei Luo</li>
<li>for: 这篇论文旨在提高大型自然语言模型（LLMs）在特定任务中的表现， Specifically, the paper aims to enhance the performance of large language models (LLMs) in sentiment analysis tasks using prompting strategies.</li>
<li>methods: 该论文使用了两种新的提问策略，namely RolePlaying (RP) prompting和Chain-of-thought (CoT) prompting，并提出了RP-CoT提问策略。 These methods include two novel prompting strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting, as well as the RP-CoT prompting strategy.</li>
<li>results: 实验结果表明，采用提出的提问策略可以明显提高 sentiment analysis 精度。 The results demonstrate that the adoption of the proposed prompting strategies leads to a significant enhancement in sentiment analysis accuracy. Additionally, the CoT prompting strategy exhibits a notable impact on implicit sentiment analysis, with the RP-CoT prompting strategy delivering the most superior performance among all strategies.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate that the adoption of the proposed prompting strategies leads to a increasing enhancement in sentiment analysis accuracy. Further, the CoT prompting strategy exhibits a notable impact on implicit sentiment analysis, with the RP-CoT prompting strategy delivering the most superior performance among all strategies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diffusion-Generative-Inverse-Design"><a href="#Diffusion-Generative-Inverse-Design" class="headerlink" title="Diffusion Generative Inverse Design"></a>Diffusion Generative Inverse Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02040">http://arxiv.org/abs/2309.02040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marin Vlastelica, Tatiana López-Guevara, Kelsey Allen, Peter Battaglia, Arnaud Doucet, Kimberley Stachenfeld</li>
<li>for: 这种 inverse design 问题的目的是优化输入参数，以实现目标结果。</li>
<li>methods: 这种方法使用 graph neural networks (GNNs) 来估算 simulator 动态，并使用 gradient-或 sampling-based optimization 进行优化。</li>
<li>results: 这种方法可以高效地解决 inverse design 问题，并且可以减少对 simulator 的调用次数。<details>
<summary>Abstract</summary>
Inverse design refers to the problem of optimizing the input of an objective function in order to enact a target outcome. For many real-world engineering problems, the objective function takes the form of a simulator that predicts how the system state will evolve over time, and the design challenge is to optimize the initial conditions that lead to a target outcome. Recent developments in learned simulation have shown that graph neural networks (GNNs) can be used for accurate, efficient, differentiable estimation of simulator dynamics, and support high-quality design optimization with gradient- or sampling-based optimization procedures. However, optimizing designs from scratch requires many expensive model queries, and these procedures exhibit basic failures on either non-convex or high-dimensional problems.In this work, we show how denoising diffusion models (DDMs) can be used to solve inverse design problems efficiently and propose a particle sampling algorithm for further improving their efficiency. We perform experiments on a number of fluid dynamics design challenges, and find that our approach substantially reduces the number of calls to the simulator compared to standard techniques.
</details>
<details>
<summary>摘要</summary>
<<SYS>> inverse 设计指的是优化输入对象函数以实现目标结果。许多实际工程问题中，目标函数通常是一个预测系统状态在时间演变的模拟器，而设计挑战是确定初始条件以实现目标结果。现在的学习模拟技术发展已经显示了 Graph Neural Networks (GNNs) 可以用于精度、效率、可导的模拟器动力学Estimation，支持高质量的设计优化过程。然而，从头来设计需要许多昂贵的模拟器调用，这些过程在非凸或高维问题上存在基本错误。在这种情况下，我们表示denoising diffusion models (DDMs) 可以高效地解决 inverse 设计问题，并提出一种粒子抽样算法以进一步提高效率。我们在一些 fluid dynamics 设计挑战中进行了实验，发现我们的方法可以减少对模拟器的调用次数相比标准技术。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Data-Juicer-A-One-Stop-Data-Processing-System-for-Large-Language-Models"><a href="#Data-Juicer-A-One-Stop-Data-Processing-System-for-Large-Language-Models" class="headerlink" title="Data-Juicer: A One-Stop Data Processing System for Large Language Models"></a>Data-Juicer: A One-Stop Data Processing System for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02033">http://arxiv.org/abs/2309.02033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, Jingren Zhou</li>
<li>for: 提高大语言模型（LLM）数据处理的效率和可扩展性，以推动LLM应用研究。</li>
<li>methods: 提供一个易用、可扩展的数据处理系统，包括50多种可编程Operator和可插入工具，以满足不同LLM数据处理需求。</li>
<li>results: 对多种预训练和后调整用 caso，通过自动评估和视觉化评估，实现了较大的LLAMA性能提升（最高7.45%），并且在分布式计算环境下实现了大幅提高的处理效率和可扩展性。<details>
<summary>Abstract</summary>
The immense evolution in Large Language Models (LLMs) has underscored the importance of massive, diverse, and high-quality data. Despite this, existing open-source tools for LLM data processing remain limited and mostly tailored to specific datasets, with an emphasis on the reproducibility of released data over adaptability and usability, inhibiting potential applications. In response, we propose a one-stop, powerful yet flexible and user-friendly LLM data processing system named Data-Juicer. Our system offers over 50 built-in versatile operators and pluggable tools, which synergize modularity, composability, and extensibility dedicated to diverse LLM data processing needs. By incorporating visualized and automatic evaluation capabilities, Data-Juicer enables a timely feedback loop to accelerate data processing and gain data insights. To enhance usability, Data-Juicer provides out-of-the-box components for users with various backgrounds, and fruitful data recipes for LLM pre-training and post-tuning usages. Further, we employ multi-facet system optimization and seamlessly integrate Data-Juicer with both LLM and distributed computing ecosystems, to enable efficient and scalable data processing. Empirical validation of the generated data recipes reveals considerable improvements in LLaMA performance for various pre-training and post-tuning cases, demonstrating up to 7.45% relative improvement of averaged score across 16 LLM benchmarks and 16.25% higher win rate using pair-wise GPT-4 evaluation. The system's efficiency and scalability are also validated, supported by up to 88.7% reduction in single-machine processing time, 77.1% and 73.1% less memory and CPU usage respectively, and 7.91x processing acceleration when utilizing distributed computing ecosystems. Our system, data recipes, and multiple tutorial demos are released, calling for broader research centered on LLM data.
</details>
<details>
<summary>摘要</summary>
大量的语言模型（LLM）演化带来了数据处理的重要性。然而，现有的开源工具仍然有限，主要是针对特定数据集，强调数据重现性而不是应用和用户性，这限制了其应用前景。为此，我们提出了一个一站式、强大且灵活的 LLM 数据处理系统 named Data-Juicer。我们的系统提供了50多种可选的强大操作和可插入工具，这些工具结合了模块性、可组合性和扩展性，以适应不同的 LLM 数据处理需求。通过添加可视化和自动评估功能，Data-Juicer 可以帮助用户快速获得数据处理和数据 Insight。为了提高可用性，Data-Juicer 提供了为用户们提供了不同背景的各种组件，以及 LLMA 预训练和后处理的数据荚。此外，我们使用多方面的优化和与 LLMA 和分布式计算环境集成，以实现高效和可扩展的数据处理。我们的实验表明，生成的数据荚可以提高 LLMA 的性能，在16个 LLMA benchmark 和16个 GPT-4 评价中得到了7.45%的相对提高，并且在对照竞赛中获得了16.25%更高的胜率。系统的效率和扩展性也得到了验证，包括单机处理时间减少88.7%，内存和CPU使用量减少77.1%和73.1%，并且在使用分布式计算环境时实现了7.91倍的处理加速。我们的系统、数据荚和多个教程示例都已经发布，呼吁更广泛的 LLM 数据研究。
</details></li>
</ul>
<hr>
<h2 id="Non-Parametric-Representation-Learning-with-Kernels"><a href="#Non-Parametric-Representation-Learning-with-Kernels" class="headerlink" title="Non-Parametric Representation Learning with Kernels"></a>Non-Parametric Representation Learning with Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02028">http://arxiv.org/abs/2309.02028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Pascal Esser, Maximilian Fleissner, Debarghya Ghoshdastidar</li>
<li>for: 本研究旨在探讨无监督学习和自监督学习中的表示学习方法，尤其是使用内核函数来实现表示学习。</li>
<li>methods: 本研究使用了内核自适应学习模型，包括对冲积分别学习模型和自适应编码器模型。</li>
<li>results: 研究人员通过新的表示函数定理和总体抽象误差下界来分析和评估内核表示学习方法的性能。在小数据 regime中和对比神经网络模型的情况下，内核表示学习方法表现良好。<details>
<summary>Abstract</summary>
Unsupervised and self-supervised representation learning has become popular in recent years for learning useful features from unlabelled data. Representation learning has been mostly developed in the neural network literature, and other models for representation learning are surprisingly unexplored. In this work, we introduce and analyze several kernel-based representation learning approaches: Firstly, we define two kernel Self-Supervised Learning (SSL) models using contrastive loss functions and secondly, a Kernel Autoencoder (AE) model based on the idea of embedding and reconstructing data. We argue that the classical representer theorems for supervised kernel machines are not always applicable for (self-supervised) representation learning, and present new representer theorems, which show that the representations learned by our kernel models can be expressed in terms of kernel matrices. We further derive generalisation error bounds for representation learning with kernel SSL and AE, and empirically evaluate the performance of these methods in both small data regimes as well as in comparison with neural network based models.
</details>
<details>
<summary>摘要</summary>
“无监督和自监督表征学学习在最近几年内得到了广泛的推广，它们可以从无标签数据中学习有用的特征。表征学学习主要发展在神经网络文献中，而其他模型的表征学学习则尚未得到充分的探索。在这项工作中，我们引入并分析了一些基于核函数的表征学学习方法：首先，我们定义了两种核Self-Supervised Learning（SSL）模型，使用对比损失函数来学习特征；其次，我们基于数据嵌入和重建的想法，提出了一种核自编码器（AE）模型。我们 argue that classical representer theorems for supervised kernel machines are not always applicable for (self-supervised) representation learning, and present new representer theorems, which show that the representations learned by our kernel models can be expressed in terms of kernel matrices。我们还derive generalization error bounds for representation learning with kernel SSL and AE, and empirically evaluate the performance of these methods in both small data regimes as well as in comparison with神经网络基于模型。”Note that Simplified Chinese is a written language that uses shorter words and simpler grammar than Traditional Chinese. The translation is written in Simplified Chinese, but the original text is in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Granger-Causal-Inference-in-Multivariate-Hawkes-Processes-by-Minimum-Message-Length"><a href="#Granger-Causal-Inference-in-Multivariate-Hawkes-Processes-by-Minimum-Message-Length" class="headerlink" title="Granger Causal Inference in Multivariate Hawkes Processes by Minimum Message Length"></a>Granger Causal Inference in Multivariate Hawkes Processes by Minimum Message Length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02027">http://arxiv.org/abs/2309.02027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katerina Hlavackova-Schindler, Anna Melnykova, Irene Tubikanec</li>
<li>for: 本研究使用多变量骨PK进行现实生活现象的模型，如地震、股票市场交易、神经元活动、病毒传播等。</li>
<li>methods: 本文使用抽象衰减函数和估计连接图，以及基于最小消息长度（MML）原理的优化 criterion 和模型选择算法，以估计骨PK中组件之间的Granger causal关系。</li>
<li>results: 比较多种类方法，包括lasso类似 penalty 方法，MML基于方法在短时间尺度下减少过拟合现象，并 achieved highest F1 scores in specific sparse graph settings。此外，通过应用于G7 sovereign bond数据，得到了一致的 causal connections，与专业知识一致。<details>
<summary>Abstract</summary>
Multivariate Hawkes processes (MHPs) are versatile probabilistic tools used to model various real-life phenomena: earthquakes, operations on stock markets, neuronal activity, virus propagation and many others. In this paper, we focus on MHPs with exponential decay kernels and estimate connectivity graphs, which represent the Granger causal relations between their components. We approach this inference problem by proposing an optimization criterion and model selection algorithm based on the minimum message length (MML) principle. MML compares Granger causal models using the Occam's razor principle in the following way: even when models have a comparable goodness-of-fit to the observed data, the one generating the most concise explanation of the data is preferred. While most of the state-of-art methods using lasso-type penalization tend to overfitting in scenarios with short time horizons, the proposed MML-based method achieves high F1 scores in these settings. We conduct a numerical study comparing the proposed algorithm to other related classical and state-of-art methods, where we achieve the highest F1 scores in specific sparse graph settings. We illustrate the proposed method also on G7 sovereign bond data and obtain causal connections, which are in agreement with the expert knowledge available in the literature.
</details>
<details>
<summary>摘要</summary>
多变量骨灰过程（MHP）是一种通用的概率工具，用于模型各种现实生活中的现象：地震、股票市场交易、神经活动、病毒传播等。在这篇论文中，我们关注MHP中的指数衰减kernel，并估计连接图，该图表示MHP中的预测关系。我们通过提出一个优化目标函数和基于最小消息长度（MML）原理的模型选择算法来实现这一目标。MML比较了不同预测模型，并根据奥卡姆的剃刀原理选择最简洁的解释。而大多数当前的方法使用拉asso-类型的约束减少倾向于过拟合，而我们提出的MML基本方法在短时间尺度下具有高F1分数。我们进行了一个数学研究，与其他相关的古典和当前方法进行比较，我们在特定的稀疏图设置中 achievement最高的F1分数。我们还应用了该方法于G7国家债券数据，并获得了一致的 causal 连接，与文献中的专家知识相符。
</details></li>
</ul>
<hr>
<h2 id="RDGSL-Dynamic-Graph-Representation-Learning-with-Structure-Learning"><a href="#RDGSL-Dynamic-Graph-Representation-Learning-with-Structure-Learning" class="headerlink" title="RDGSL: Dynamic Graph Representation Learning with Structure Learning"></a>RDGSL: Dynamic Graph Representation Learning with Structure Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02025">http://arxiv.org/abs/2309.02025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Zhang, Yun Xiong, Yao Zhang, Yiheng Sun, Xi Chen, Yizhu Jiao, Yangyong Zhu</li>
<li>for: 本文主要研究如何在连续时间动态图中学习表示，以提高下游任务的效果。</li>
<li>methods: 本文提出了一种名为RDGSL的表示学习方法，同时也提出了一种名为动态图结构学习的新超级视图信号，以便帮助RDGSL更好地抗击动态图中的噪音。</li>
<li>results: 本文的实验结果表明，RDGSL可以在连续时间动态图中提供更好的表示，并且可以减少噪音的影响，从而提高下游任务的效果。特别是，RDGSL可以在动态图中提供5.1%绝对的AUC提升，比第二个基eline的表现更好。<details>
<summary>Abstract</summary>
Temporal Graph Networks (TGNs) have shown remarkable performance in learning representation for continuous-time dynamic graphs. However, real-world dynamic graphs typically contain diverse and intricate noise. Noise can significantly degrade the quality of representation generation, impeding the effectiveness of TGNs in downstream tasks. Though structure learning is widely applied to mitigate noise in static graphs, its adaptation to dynamic graph settings poses two significant challenges. i) Noise dynamics. Existing structure learning methods are ill-equipped to address the temporal aspect of noise, hampering their effectiveness in such dynamic and ever-changing noise patterns. ii) More severe noise. Noise may be introduced along with multiple interactions between two nodes, leading to the re-pollution of these nodes and consequently causing more severe noise compared to static graphs. In this paper, we present RDGSL, a representation learning method in continuous-time dynamic graphs. Meanwhile, we propose dynamic graph structure learning, a novel supervisory signal that empowers RDGSL with the ability to effectively combat noise in dynamic graphs. To address the noise dynamics issue, we introduce the Dynamic Graph Filter, where we innovatively propose a dynamic noise function that dynamically captures both current and historical noise, enabling us to assess the temporal aspect of noise and generate a denoised graph. We further propose the Temporal Embedding Learner to tackle the challenge of more severe noise, which utilizes an attention mechanism to selectively turn a blind eye to noisy edges and hence focus on normal edges, enhancing the expressiveness for representation generation that remains resilient to noise. Our method demonstrates robustness towards downstream tasks, resulting in up to 5.1% absolute AUC improvement in evolving classification versus the second-best baseline.
</details>
<details>
<summary>摘要</summary>
天时グラフネットワーク（TGN）の表现学习は、连続的时间のグラフで优れた性能を示しています。しかし、実世界の动的グラフでは、多様で复雑なノイズが存在します。このノイズは、表现生成の质を低下させ、TGNの下流タスクでの效果を妨げます。避けるために、构造学习が広く使用されますが、この动的グラフの设定では、2つの大きな挑戦を提起します。i) 时间的なノイズ。既存の构造学习メソッドは、时间的な侧面を考虑していません、これらのノイズを效果的に処理することができません。ii) より强いノイズ。ノイズは、2つのノード间の相互作用によって导入され、それらのノードを污染することで、より强いノイズを生成します。この研究では、RDGSL（时间グラフ表现学习）を提案しています。また、动的グラフ构造学习（DGSL）という新しいスーパーバイザー信号を提案しています。この信号を使用して、RDGSLは、动的グラフの中で效果的にノイズを処理することができます。ノイズの动的な侧面に対処するために、我々は、Dynamic Graph Filter（DGF）を导入しています。DGFは、现在と过去のノイズを考虑して、时间的なノイズを捉えることができます。さらに、Temporal Embedding Learner（TEL）を提案しています。TELは、注意メカニズムを使用して、ノイズのあるエッジに対して、注意を払わないことができます。これにより、正常なエッジに対する表现の生成が増强されます。RDGSLの方法は、下流タスクでのRobustnessを示しています。これにより、evolving classificationのAUCで5.1%の绝対値改善を示しています。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Early-Exiting-Predictive-Coding-Neural-Networks"><a href="#Dynamic-Early-Exiting-Predictive-Coding-Neural-Networks" class="headerlink" title="Dynamic Early Exiting Predictive Coding Neural Networks"></a>Dynamic Early Exiting Predictive Coding Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02022">http://arxiv.org/abs/2309.02022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alaa Zniber, Ouassim Karrakchou, Mounir Ghogho</li>
<li>for: 这篇论文主要关注于实时应用中的物联网感应器（IoT），如智能装备和健康监控等，它们生成大量数据，并且使用深度学习（DL）模型来进行智能处理。</li>
<li>methods: 这篇论文提出了一个使用预测代码理论和动态早期终止的浅层对称网络，以减少网络的参数和计算复杂度，并且与VGG-16模型在CIFAR-10影像分类任务中实现相似的准确性。</li>
<li>results: 这篇论文的结果显示，使用提案的浅层对称网络可以在CIFAR-10影像分类任务中实现相似的准确性，并且比VGG-16模型使用 fewer 参数和较低的计算复杂度。<details>
<summary>Abstract</summary>
Internet of Things (IoT) sensors are nowadays heavily utilized in various real-world applications ranging from wearables to smart buildings passing by agrotechnology and health monitoring. With the huge amounts of data generated by these tiny devices, Deep Learning (DL) models have been extensively used to enhance them with intelligent processing. However, with the urge for smaller and more accurate devices, DL models became too heavy to deploy. It is thus necessary to incorporate the hardware's limited resources in the design process. Therefore, inspired by the human brain known for its efficiency and low power consumption, we propose a shallow bidirectional network based on predictive coding theory and dynamic early exiting for halting further computations when a performance threshold is surpassed. We achieve comparable accuracy to VGG-16 in image classification on CIFAR-10 with fewer parameters and less computational complexity.
</details>
<details>
<summary>摘要</summary>
互联网物件（IoT）感应器目前在不同的实际应用中广泛使用，从戴式设备到智能建筑，途经农科技和健康监控。这些小型设备生成的巨量数据，深度学习（DL）模型已经广泛地应用以提高他们的智能处理。然而，随着设备的小型化和精确度的提高，DL模型已经成为不可deploy的。因此，为了考虑硬件的限制，我们需要在设计过程中考虑硬件的限制。因此，受人脑的效率和低功耗所 inspirited，我们提出了一个浅层的双向网络，基于预测编码理论和动态早退出，可以在性能门槛超过时间点扩展运算。我们在CIFAR-10上进行图像分类 tasks，与VGG-16模型相比，我们的模型具有较少的参数和较低的计算复杂度，并且具有相似的准确性。
</details></li>
</ul>
<hr>
<h2 id="PROMISE-Preconditioned-Stochastic-Optimization-Methods-by-Incorporating-Scalable-Curvature-Estimates"><a href="#PROMISE-Preconditioned-Stochastic-Optimization-Methods-by-Incorporating-Scalable-Curvature-Estimates" class="headerlink" title="PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates"></a>PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02014">http://arxiv.org/abs/2309.02014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary Frangella, Pratik Rathore, Shipu Zhao, Madeleine Udell</li>
<li>for: 解决大规模机器学习中的凸优化问题</li>
<li>methods: 使用笔记法实现随机梯度算法，包括SVRG、SAGA和Katyusha等方法，每种方法都有强的理论分析和有效的默认超参数值</li>
<li>results: 比较 tradicional stochastic gradient methods的性能，通过default超参数值，提出的方法在51个ridge和logistic regression问题上表现较好或与通过手动调整超参数值的方法匹配，并在理论上引入了quadratic regularity来确定线性减速的速度，该速度通常比condition number更紧Binding the convergence rate, and explains the fast global linear convergence of the proposed methods.<details>
<summary>Abstract</summary>
This paper introduces PROMISE ($\textbf{Pr}$econditioned Stochastic $\textbf{O}$ptimization $\textbf{M}$ethods by $\textbf{I}$ncorporating $\textbf{S}$calable Curvature $\textbf{E}$stimates), a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems arising in machine learning. PROMISE includes preconditioned versions of SVRG, SAGA, and Katyusha; each algorithm comes with a strong theoretical analysis and effective default hyperparameter values. In contrast, traditional stochastic gradient methods require careful hyperparameter tuning to succeed, and degrade in the presence of ill-conditioning, a ubiquitous phenomenon in machine learning. Empirically, we verify the superiority of the proposed algorithms by showing that, using default hyperparameter values, they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems assembled from benchmark machine learning repositories. On the theoretical side, this paper introduces the notion of quadratic regularity in order to establish linear convergence of all proposed methods even when the preconditioner is updated infrequently. The speed of linear convergence is determined by the quadratic regularity ratio, which often provides a tighter bound on the convergence rate compared to the condition number, both in theory and in practice, and explains the fast global linear convergence of the proposed methods.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了PROMISE（预conditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates），一个基于笔记的预conditioned随机梯度算法集，用于解决大规模凸优化问题。PROMISE包括预conditioned SVRG、SAGA和Katyusha算法，每个算法都有强大的理论分析和有效的默认超参数值。与传统的随机梯度方法相比，PROMISE的算法不需要精心调整超参数，并且在缺conditioning现象存在时表现更好。在实际中，我们通过显示使用默认超参数值时，PROMISE的算法可以超越或与优化的随机梯度优化器相当的性能。从理论角度来看，这篇论文引入了quadratic regularity的概念，以确定预conditioner更新不频繁时的线性准确率。预conditioner更新频繁时的准确率速度是quadratic regularity比率，这经常提供了更紧的 convergence rate bound，并且在理论和实践中都能够解释PROMISE算法的快速全局线性准确率。
</details></li>
</ul>
<hr>
<h2 id="iLoRE-Dynamic-Graph-Representation-with-Instant-Long-term-Modeling-and-Re-occurrence-Preservation"><a href="#iLoRE-Dynamic-Graph-Representation-with-Instant-Long-term-Modeling-and-Re-occurrence-Preservation" class="headerlink" title="iLoRE: Dynamic Graph Representation with Instant Long-term Modeling and Re-occurrence Preservation"></a>iLoRE: Dynamic Graph Representation with Instant Long-term Modeling and Re-occurrence Preservation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02012">http://arxiv.org/abs/2309.02012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Zhang, Yun Xiong, Yao Zhang, Xixi Wu, Yiheng Sun, Jiawei Zhang</li>
<li>for: 这个研究旨在提出一个具有快速更新和长期模型的动态图模型方法，以应对实际应用中的金融风险管理和骗案检测等问题。</li>
<li>methods: 这个方法使用了具有适应短期更新和长期模型的Adaptive Short-term Updater和Long-term Updater两部分，以解决现有方法中的三大限制：随机更新、不足的长期模型和缺乏重复性模型。具体来说，这个方法使用了Identity Attention机制，将Transformer-based updater与RNN-dominated设计相组合，以提高实现效果。</li>
<li>results: 这个研究的实验结果显示，iLoRE方法能够具有更高的效果和快速性，在实际应用中的金融风险管理和骗案检测等领域中表现出色。<details>
<summary>Abstract</summary>
Continuous-time dynamic graph modeling is a crucial task for many real-world applications, such as financial risk management and fraud detection. Though existing dynamic graph modeling methods have achieved satisfactory results, they still suffer from three key limitations, hindering their scalability and further applicability. i) Indiscriminate updating. For incoming edges, existing methods would indiscriminately deal with them, which may lead to more time consumption and unexpected noisy information. ii) Ineffective node-wise long-term modeling. They heavily rely on recurrent neural networks (RNNs) as a backbone, which has been demonstrated to be incapable of fully capturing node-wise long-term dependencies in event sequences. iii) Neglect of re-occurrence patterns. Dynamic graphs involve the repeated occurrence of neighbors that indicates their importance, which is disappointedly neglected by existing methods. In this paper, we present iLoRE, a novel dynamic graph modeling method with instant node-wise Long-term modeling and Re-occurrence preservation. To overcome the indiscriminate updating issue, we introduce the Adaptive Short-term Updater module that will automatically discard the useless or noisy edges, ensuring iLoRE's effectiveness and instant ability. We further propose the Long-term Updater to realize more effective node-wise long-term modeling, where we innovatively propose the Identity Attention mechanism to empower a Transformer-based updater, bypassing the limited effectiveness of typical RNN-dominated designs. Finally, the crucial re-occurrence patterns are also encoded into a graph module for informative representation learning, which will further improve the expressiveness of our method. Our experimental results on real-world datasets demonstrate the effectiveness of our iLoRE for dynamic graph modeling.
</details>
<details>
<summary>摘要</summary>
continuous-time dynamic graph modeling是现实世界中许多应用场景中的关键任务，如金融风险管理和欺诈检测。虽然现有的动态图模型方法已经达到了满意的结果，但它们仍然受到三个关键限制，阻碍其扩展性和更多的应用。i) 无分别更新。对于进来的边，现有的方法都会无分别地处理它们，这可能会导致更多的时间消耗和意外的噪音信息。ii) 不够的节点长期模型。它们都是基于循环神经网络（RNN）的后备，已经证明不能够全面捕捉节点长期关系在事件序列中。iii) 忽略重复模式。动态图中的重复 neighbbors 表明其重要性，这一点抑制地被现有的方法忽略。在这篇论文中，我们提出了 iLoRE，一种基于实时节点长期模型和重复保持的动态图模型方法。为了解决无分别更新问题，我们引入了自适应短期更新模块，可以自动抛弃无用或噪音的边，使 iLoRE 具有更高的效iveness 和实时能力。此外，我们还提出了长期更新模块，以实现更有效的节点长期模型，我们创新地提出了标识注意力机制，使用Transformer-based更新器，绕过传统的 RNN-主导的设计的局限性。最后，我们还编码了重复模式到图模块，以便更好地学习表达能力。我们的实验结果表明，iLoRE 可以有效地进行动态图模型。
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-Dynamics-of-Self-Supervised-Models"><a href="#Representation-Learning-Dynamics-of-Self-Supervised-Models" class="headerlink" title="Representation Learning Dynamics of Self-Supervised Models"></a>Representation Learning Dynamics of Self-Supervised Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02011">http://arxiv.org/abs/2309.02011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Esser, Satyaki Mukherjee, Debarghya Ghoshdastidar</li>
<li>for: 这 paper 旨在研究自助学习（SSL）模型中的学习动态，具体来说是 SSL 模型中的表示得到的学习过程。</li>
<li>methods: 这 paper 使用了多元回归的扩展来研究 SSL 模型的学习动态，并在这个过程中引入了正交约束来保证模型的学习。</li>
<li>results: 这 paper 发现，使用Gradient Descent在Grassmannian manifold上训练 SSL 模型时，模型会学习简单的标准差表示，并且会导致维度减少问题。此外， paper 还发现，在无穷宽扩展下，SSL 模型与超级vised模型的 neural tangent kernel 不同。<details>
<summary>Abstract</summary>
Self-Supervised Learning (SSL) is an important paradigm for learning representations from unlabelled data, and SSL with neural networks has been highly successful in practice. However current theoretical analysis of SSL is mostly restricted to generalisation error bounds. In contrast, learning dynamics often provide a precise characterisation of the behaviour of neural networks based models but, so far, are mainly known in supervised settings. In this paper, we study the learning dynamics of SSL models, specifically representations obtained by minimising contrastive and non-contrastive losses. We show that a naive extension of the dymanics of multivariate regression to SSL leads to learning trivial scalar representations that demonstrates dimension collapse in SSL. Consequently, we formulate SSL objectives with orthogonality constraints on the weights, and derive the exact (network width independent) learning dynamics of the SSL models trained using gradient descent on the Grassmannian manifold. We also argue that the infinite width approximation of SSL models significantly deviate from the neural tangent kernel approximations of supervised models. We numerically illustrate the validity of our theoretical findings, and discuss how the presented results provide a framework for further theoretical analysis of contrastive and non-contrastive SSL.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Establishing-a-real-time-traffic-alarm-in-the-city-of-Valencia-with-Deep-Learning"><a href="#Establishing-a-real-time-traffic-alarm-in-the-city-of-Valencia-with-Deep-Learning" class="headerlink" title="Establishing a real-time traffic alarm in the city of Valencia with Deep Learning"></a>Establishing a real-time traffic alarm in the city of Valencia with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02010">http://arxiv.org/abs/2309.02010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Folgado, Veronica Sanz, Johannes Hirn, Edgar Lorenzo-Saez, Javier Urchueguia</li>
<li>For: 这项研究的目的是分析城市Valencia的交通凝固和污染之间的关系，以及开发一种能够预测下一30分钟内交通异常高峰的警示系统。* Methods: 该研究使用了交通数据更新每10分钟，以及Long Short-Term Memory（LSTM）神经网络来预测下一30分钟内交通异常高峰。* Results: 研究结果表明，交通凝固对某些污染物（特别是$\text{NO}_\text{x}$）水平有显著影响。<details>
<summary>Abstract</summary>
Urban traffic emissions represent a significant concern due to their detrimental impacts on both public health and the environment. Consequently, decision-makers have flagged their reduction as a crucial goal. In this study, we first analyze the correlation between traffic flux and pollution in the city of Valencia, Spain. Our results demonstrate that traffic has a significant impact on the levels of certain pollutants (especially $\text{NO}_\text{x}$). Secondly, we develop an alarm system to predict if a street is likely to experience unusually high traffic in the next 30 minutes, using an independent three-tier level for each street. To make the predictions, we use traffic data updated every 10 minutes and Long Short-Term Memory (LSTM) neural networks. We trained the LSTM using traffic data from 2018, and tested it using traffic data from 2019.
</details>
<details>
<summary>摘要</summary>
城市交通排放对公共健康和环境产生了重要的影响，因此决策者们认为减少它们是一项重要的目标。在本研究中，我们首先分析了城市劳 Valle 的交通流量和污染物之间的相关性。我们的结果表明，交通具有对某些污染物（尤其是 $\text{NO}_\text{x}$）的显著影响。其次，我们开发了一个警示系统，可以预测下一30分钟内某条街道是否会出现不寻常高的交通流量，使用独立的三级水平 для每条街道。为了进行预测，我们使用了每10分钟更新的交通数据和Long Short-Term Memory（LSTM）神经网络。我们使用2018年的交通数据进行训练，并在2019年的交通数据上进行测试。
</details></li>
</ul>
<hr>
<h2 id="Aggregating-Correlated-Estimations-with-Almost-no-Training"><a href="#Aggregating-Correlated-Estimations-with-Almost-no-Training" class="headerlink" title="Aggregating Correlated Estimations with (Almost) no Training"></a>Aggregating Correlated Estimations with (Almost) no Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02005">http://arxiv.org/abs/2309.02005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theo Delemazure, François Durand, Fabien Mathieu</li>
<li>for: 解决各种各样的决策问题，因为许多问题无法得到确切的解决方案。</li>
<li>methods: 使用多种估计算法分配不同选项的分数，并考虑估计错误之间的相关性。</li>
<li>results: 比较不同的聚合规则在不同的数据集上的性能，并结论使用最大似然聚合规则在知道估计错误之间的相关性时表现最好，否则推荐使用嵌入式投票法（Embedded Voting，EV）。<details>
<summary>Abstract</summary>
Many decision problems cannot be solved exactly and use several estimation algorithms that assign scores to the different available options. The estimation errors can have various correlations, from low (e.g. between two very different approaches) to high (e.g. when using a given algorithm with different hyperparameters). Most aggregation rules would suffer from this diversity of correlations. In this article, we propose different aggregation rules that take correlations into account, and we compare them to naive rules in various experiments based on synthetic data. Our results show that when sufficient information is known about the correlations between errors, a maximum likelihood aggregation should be preferred. Otherwise, typically with limited training data, we recommend a method that we call Embedded Voting (EV).
</details>
<details>
<summary>摘要</summary>
许多决策问题无法准确解决，通常使用多种估计算法计算不同选项的得分。估计误差之间可能存在各种相关性，从低（例如两种完全不同的方法）到高（例如使用同一算法不同的超参数）。大多数聚合规则会受到这种多样性的影响。在这篇文章中，我们提出了考虑相关性的不同聚合规则，并与简单规则进行了多个实验，基于 синтетиче数据。我们的结果表明，当知道估计误差之间的相关性信息足够时，最大极化聚合应该被首选。否则，通常在有限训练数据的情况下，我们建议一种我们称为嵌入式投票（EV）方法。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-domain-shift-when-using-additional-data-for-the-MICCAI-KiTS23-Challenge"><a href="#Analyzing-domain-shift-when-using-additional-data-for-the-MICCAI-KiTS23-Challenge" class="headerlink" title="Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge"></a>Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02001">http://arxiv.org/abs/2309.02001</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Stoica, Mihaela Breaban, Vlad Barbu</li>
<li>for: 提高医疗影像3D分割模型的性能，使其更好地适应不同来源的数据。</li>
<li>methods: 研究如何在训练过程中减少频率域转移，使新收集的数据更好地与原始训练数据结合使用。</li>
<li>results: 比较 histogram matching 和标准化两种方法，发现 histogram matching 能够更好地减少频率域转移，提高模型的性能。<details>
<summary>Abstract</summary>
Using additional training data is known to improve the results, especially for medical image 3D segmentation where there is a lack of training material and the model needs to generalize well from few available data. However, the new data could have been acquired using other instruments and preprocessed such its distribution is significantly different from the original training data. Therefore, we study techniques which ameliorate domain shift during training so that the additional data becomes better usable for preprocessing and training together with the original data. Our results show that transforming the additional data using histogram matching has better results than using simple normalization.
</details>
<details>
<summary>摘要</summary>
使用更多训练数据可以提高结果，特别是医疗影像三维分割，因为这个领域缺乏训练材料，模型需要将少量可用的数据泛化好。然而，新的数据可能是使用不同的仪器获取的，其分布与原始训练数据有所不同。因此，我们研究如何在训练过程中缓解领域偏移，使得附加数据更好地可用于预处理和训练。我们的结果表明，使用 histogram matching 将附加数据转换可以获得更好的结果，比使用简单的 норmalization。
</details></li>
</ul>
<hr>
<h2 id="sasdim-self-adaptive-noise-scaling-diffusion-model-for-spatial-time-series-imputation"><a href="#sasdim-self-adaptive-noise-scaling-diffusion-model-for-spatial-time-series-imputation" class="headerlink" title="sasdim: self-adaptive noise scaling diffusion model for spatial time series imputation"></a>sasdim: self-adaptive noise scaling diffusion model for spatial time series imputation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01988">http://arxiv.org/abs/2309.01988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shunyang Zhang, Senzhang Wang, Xianzhen Tan, Ruochen Liu, Jian Zhang, Jianxin Wang</li>
<li>for: 填充空缺的空间时间序列数据，以提高智能交通和空气质量监测等实际应用中的性能。</li>
<li>methods: 提出了一种自适应噪声扩大演Diffusion Model named SaSDim，包括新的损失函数和透视模块，以更好地捕捉空间时间序列数据的动态相互关系。</li>
<li>results: 对三个实际数据集进行了广泛的实验，证明SaSDim可以与当前状态艺术基准相比，更高效地完成空间时间序列填充任务。<details>
<summary>Abstract</summary>
Spatial time series imputation is critically important to many real applications such as intelligent transportation and air quality monitoring. Although recent transformer and diffusion model based approaches have achieved significant performance gains compared with conventional statistic based methods, spatial time series imputation still remains as a challenging issue due to the complex spatio-temporal dependencies and the noise uncertainty of the spatial time series data. Especially, recent diffusion process based models may introduce random noise to the imputations, and thus cause negative impact on the model performance. To this end, we propose a self-adaptive noise scaling diffusion model named SaSDim to more effectively perform spatial time series imputation. Specially, we propose a new loss function that can scale the noise to the similar intensity, and propose the across spatial-temporal global convolution module to more effectively capture the dynamic spatial-temporal dependencies. Extensive experiments conducted on three real world datasets verify the effectiveness of SaSDim by comparison with current state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
空间时间序列填充是许多实际应用中的关键问题，如智能交通和空气质量监测。虽然最近的变换器和扩散模型基于方法已经实现了对传统统计学基于方法的显著性能提升，但是空间时间序列填充仍然是一个挑战性的问题，因为空间时间序列数据中存在复杂的空间时间依赖关系和噪声不确定性。尤其是最近的扩散过程基于模型可能会引入随机噪声到填充中，从而对模型性能产生负面影响。为此，我们提议一种自适应噪声扩大扩散模型 named SaSDim，以更有效地进行空间时间序列填充。特别是，我们提出了一个新的损失函数，可以尝试将噪声缩放到相似的强度，并提出了跨空间时间全球 convolution 模块，以更好地捕捉空间时间依赖关系的动态变化。广泛的实验在三个实际数据集上验证了 SaSDim 的有效性，与当前状态艺术基eline 进行比较。
</details></li>
</ul>
<hr>
<h2 id="Retail-store-customer-behavior-analysis-system-Design-and-Implementation"><a href="#Retail-store-customer-behavior-analysis-system-Design-and-Implementation" class="headerlink" title="Retail store customer behavior analysis system: Design and Implementation"></a>Retail store customer behavior analysis system: Design and Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03232">http://arxiv.org/abs/2309.03232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Dinh Nguyen, Keisuke Hihara, Tung Cao Hoang, Yumeka Utada, Akihiko Torii, Naoki Izumi, Nguyen Thanh Thuy, Long Quoc Tran<br>for: 本研究旨在提高顾客满意度，通过为顾客添加个性化价值。methods: 本研究使用了深度学习技术，包括深度神经网络，对顾客行为进行分析。results: 研究结果表明，使用深度学习技术可以更好地识别顾客行为，并提供更多的数据可视化。<details>
<summary>Abstract</summary>
Understanding customer behavior in retail stores plays a crucial role in improving customer satisfaction by adding personalized value to services. Behavior analysis reveals both general and detailed patterns in the interaction of customers with a store items and other people, providing store managers with insight into customer preferences. Several solutions aim to utilize this data by recognizing specific behaviors through statistical visualization. However, current approaches are limited to the analysis of small customer behavior sets, utilizing conventional methods to detect behaviors. They do not use deep learning techniques such as deep neural networks, which are powerful methods in the field of computer vision. Furthermore, these methods provide limited figures when visualizing the behavioral data acquired by the system. In this study, we propose a framework that includes three primary parts: mathematical modeling of customer behaviors, behavior analysis using an efficient deep learning based system, and individual and group behavior visualization. Each module and the entire system were validated using data from actual situations in a retail store.
</details>
<details>
<summary>摘要</summary>
理解顾客在商场中的行为对于提高顾客满意度非常重要，可以添加个性化的价值至服务。行为分析发现顾客与店内物品和其他人之间的交互，提供店长们顾客偏好的信息。现有的解决方案仅仅是通过传统方法来检测行为，不使用深度学习技术如深度神经网络，这些技术在计算机视觉领域是非常强大的。此外，这些方法只能提供有限的行为数据可视化图表。在本研究中，我们提出了一个框架，包括三个主要部分：顾客行为数学模型、深度学习基于系统Behavior分析和个人和组行为可视化。每个模块和整个系统都被使用实际情况的数据验证。
</details></li>
</ul>
<hr>
<h2 id="An-LSTM-Based-Predictive-Monitoring-Method-for-Data-with-Time-varying-Variability"><a href="#An-LSTM-Based-Predictive-Monitoring-Method-for-Data-with-Time-varying-Variability" class="headerlink" title="An LSTM-Based Predictive Monitoring Method for Data with Time-varying Variability"></a>An LSTM-Based Predictive Monitoring Method for Data with Time-varying Variability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01978">http://arxiv.org/abs/2309.01978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Qiu, Yu Lin, Inez Zwetsloot</li>
<li>for: 本研究旨在透过时间测量变化的资料进行偏差检测，并提出一个基于LSTM预测 интервал的控制图。</li>
<li>methods: 本研究使用了循环神经网络和其变体，并提出了一个基于LSTM预测 интервал的控制图。</li>
<li>results: 运算研究表明，提出的方法在处理时间变化的资料上具有更高的准确率和更好的性能，并且在实际应用中运行时间较短。<details>
<summary>Abstract</summary>
The recurrent neural network and its variants have shown great success in processing sequences in recent years. However, this deep neural network has not aroused much attention in anomaly detection through predictively process monitoring. Furthermore, the traditional statistic models work on assumptions and hypothesis tests, while neural network (NN) models do not need that many assumptions. This flexibility enables NN models to work efficiently on data with time-varying variability, a common inherent aspect of data in practice. This paper explores the ability of the recurrent neural network structure to monitor processes and proposes a control chart based on long short-term memory (LSTM) prediction intervals for data with time-varying variability. The simulation studies provide empirical evidence that the proposed model outperforms other NN-based predictive monitoring methods for mean shift detection. The proposed method is also applied to time series sensor data, which confirms that the proposed method is an effective technique for detecting abnormalities.
</details>
<details>
<summary>摘要</summary>
Recurrent Neural Networks (RNNs) 和其变体在过去几年内表现出了惊人的成功，但它们在异常检测中尚未受到太多关注。此外，传统的统计模型基于假设和 гипотезы测试，而神经网络（NN）模型则不需要这么多假设。这种灵活性使得NN模型能够高效地处理具有时间变化的变化的数据，这是实际数据的常见特性。本文探讨了RNN结构是否能够监控过程，并提出了基于长期记忆（LSTM）预测 интерval的控制图表方法。实验研究表明，提议的模型在mean shift检测方面表现出excel，并且应用于时间序列感知器数据，证明了该方法是异常检测的有效手段。
</details></li>
</ul>
<hr>
<h2 id="Linear-Regression-using-Heterogeneous-Data-Batches"><a href="#Linear-Regression-using-Heterogeneous-Data-Batches" class="headerlink" title="Linear Regression using Heterogeneous Data Batches"></a>Linear Regression using Heterogeneous Data Batches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01973">http://arxiv.org/abs/2309.01973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Jain, Rajat Sen, Weihao Kong, Abhimanyu Das, Alon Orlitsky</li>
<li>for: 本研究的目的是解决多源数据的难题，即每个源数据集都有一个未知的分支，每个分支都有一个未知的输入分布和输入-输出关系。</li>
<li>methods: 本研究使用了一种新的梯度下降算法，该算法可以扩展到不同的分支下，允许输入分布不同、未知和重 tailed ，同时可以在有限的批次数量下 recuperate 所有分支。</li>
<li>results: 本研究得到了较好的结果，可以在有限的批次数量下 recuperate 所有分支，并且可以避免分支分离要求。此外，研究还发现了一些优化的技巧，可以更好地处理各种不同的输入分布。<details>
<summary>Abstract</summary>
In many learning applications, data are collected from multiple sources, each providing a \emph{batch} of samples that by itself is insufficient to learn its input-output relationship. A common approach assumes that the sources fall in one of several unknown subgroups, each with an unknown input distribution and input-output relationship. We consider one of this setup's most fundamental and important manifestations where the output is a noisy linear combination of the inputs, and there are $k$ subgroups, each with its own regression vector. Prior work~\cite{kong2020meta} showed that with abundant small-batches, the regression vectors can be learned with only few, $\tilde\Omega( k^{3/2})$, batches of medium-size with $\tilde\Omega(\sqrt k)$ samples each. However, the paper requires that the input distribution for all $k$ subgroups be isotropic Gaussian, and states that removing this assumption is an ``interesting and challenging problem". We propose a novel gradient-based algorithm that improves on the existing results in several ways. It extends the applicability of the algorithm by: (1) allowing the subgroups' underlying input distributions to be different, unknown, and heavy-tailed; (2) recovering all subgroups followed by a significant proportion of batches even for infinite $k$; (3) removing the separation requirement between the regression vectors; (4) reducing the number of batches and allowing smaller batch sizes.
</details>
<details>
<summary>摘要</summary>
在许多学习应用中，数据来源从多个来源提供批处理的样本，每个来源的样本本身不够学习其输入输出关系。一种常见的方法假设来源分为多个未知 subgroup，每个 subgroup 有未知的输入分布和输入输出关系。我们考虑这个设置的一个最基本和重要的表现，输出是噪音加权的输入组合，有 $k$ subgroup，每个 subgroup 有自己的 regression vector。先前的研究（\ref{kong2020meta})  показа了，只要有充足的小批处理，可以通过只需要几个 $\tilde\Omega(k^{3/2})$ 中等大小的批处理，每个批处理有 $\tilde\Omega(\sqrt k)$ 个样本，学习 regression vector。然而，这个文章假设输入分布对所有 $k$ subgroup 都是均匀 Gaussian，并且称这是一个“有趣和挑战的问题”。我们提出了一种新的梯度基本算法，在以下方面改进了现有结果：1. 允许 subgroup 的下面输入分布不同、未知、重 tailed;2. 能够恢复所有 subgroup，即使 $k$ 为无穷大;3.  removing separation requirement between regression vectors;4. 减少批处理数量，允许小批处理大小更小。
</details></li>
</ul>
<hr>
<h2 id="AdaPlus-Integrating-Nesterov-Momentum-and-Precise-Stepsize-Adjustment-on-AdamW-Basis"><a href="#AdaPlus-Integrating-Nesterov-Momentum-and-Precise-Stepsize-Adjustment-on-AdamW-Basis" class="headerlink" title="AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis"></a>AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01966">http://arxiv.org/abs/2309.01966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Guan</li>
<li>for: 这篇论文是为了提出一种高效的优化器，即AdaPlus，该优化器基于AdamW基础上加入了内斯特鲁普振荡和精确步长调整。</li>
<li>methods: 该论文使用的方法包括Nesterov振荡和精确步长调整，并在AdamW基础上 интегрирова。</li>
<li>results: 实验结果表明，AdaPlus在图像分类任务上表现最佳，与SGD振荡相当，并在语言模型任务和GAN训练中表现出最高稳定性。<details>
<summary>Abstract</summary>
This paper proposes an efficient optimizer called AdaPlus which integrates Nesterov momentum and precise stepsize adjustment on AdamW basis. AdaPlus combines the advantages of AdamW, Nadam, and AdaBelief and, in particular, does not introduce any extra hyper-parameters. We perform extensive experimental evaluations on three machine learning tasks to validate the effectiveness of AdaPlus. The experiment results validate that AdaPlus (i) is the best adaptive method which performs most comparable with (even slightly better than) SGD with momentum on image classification tasks and (ii) outperforms other state-of-the-art optimizers on language modeling tasks and illustrates the highest stability when training GANs. The experiment code of AdaPlus is available at: https://github.com/guanleics/AdaPlus.
</details>
<details>
<summary>摘要</summary>
这份论文提出了一种高效的优化器called AdaPlus，它在AdamW基础上集成了内斯特鲁夫势和精确步长调整。AdaPlus将AdamW、Nadam和AdaBelief的优点结合在一起，并没有添加任何额外 гипер Parameters。我们在三个机器学习任务上进行了广泛的实验评估，以验证AdaPlus的效果。实验结果表明，AdaPlus（一）在图像分类任务上与SGD势动（even slightly better than）最佳的adaptive方法，（二）在语言模型任务上超越了其他当前最佳优化器，并且在训练GANs时示出了最高的稳定性。AdaPlus的实验代码可以在https://github.com/guanleics/AdaPlus上获取。
</details></li>
</ul>
<hr>
<h2 id="RADIO-Reference-Agnostic-Dubbing-Video-Synthesis"><a href="#RADIO-Reference-Agnostic-Dubbing-Video-Synthesis" class="headerlink" title="RADIO: Reference-Agnostic Dubbing Video Synthesis"></a>RADIO: Reference-Agnostic Dubbing Video Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01950">http://arxiv.org/abs/2309.01950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyeun Lee, Chaewon Kim, Sangjoon Yu, Jaejun Yoo, Gyeong-Moon Park</li>
<li>for: 提高语音驱动的讲头生成的精度和同步率。</li>
<li>methods: 引入RADIO框架，通过修改decoder层使用latent空间中的音频和参考特征进行调节，并在decoder中添加ViT块以强调高精度 Details。</li>
<li>results: 实验结果显示RADIO可以保持高同步率而不失去精度，尤其在参考帧与实际帆布偏差较大的情况下，方法超越了现有方法，反映其稳定性。<details>
<summary>Abstract</summary>
One of the most challenging problems in audio-driven talking head generation is achieving high-fidelity detail while ensuring precise synchronization. Given only a single reference image, extracting meaningful identity attributes becomes even more challenging, often causing the network to mirror the facial and lip structures too closely. To address these issues, we introduce RADIO, a framework engineered to yield high-quality dubbed videos regardless of the pose or expression in reference images. The key is to modulate the decoder layers using latent space composed of audio and reference features. Additionally, we incorporate ViT blocks into the decoder to emphasize high-fidelity details, especially in the lip region. Our experimental results demonstrate that RADIO displays high synchronization without the loss of fidelity. Especially in harsh scenarios where the reference frame deviates significantly from the ground truth, our method outperforms state-of-the-art methods, highlighting its robustness. Pre-trained model and codes will be made public after the review.
</details>
<details>
<summary>摘要</summary>
一个非常挑战的问题在audio驱动的讲头生成中是实现高精度的详细同步。只有一个参考图像，提取有意义的人脸特征变得更加挑战，常常导致网络夹紧脸和嘴的结构。为解决这些问题，我们介绍了RADIO框架，可以生成高质量的配音视频，不 regard of pose或表情参考图像中的姿势或表情。关键在于在 latent space 中模拟音频和参考特征。此外，我们在decoder中添加了ViT块，以强调高精度的细节，特别是嘴部regions。我们的实验结果表明，RADIO 能够实现高同步无损精度。尤其在参考图像与真实图像偏差较大的情况下，我们的方法超过了当前的状态艺技术， highlighting its robustness。预训练模型和代码将在审核后公开。
</details></li>
</ul>
<hr>
<h2 id="TODM-Train-Once-Deploy-Many-Efficient-Supernet-Based-RNN-T-Compression-For-On-device-ASR-Models"><a href="#TODM-Train-Once-Deploy-Many-Efficient-Supernet-Based-RNN-T-Compression-For-On-device-ASR-Models" class="headerlink" title="TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models"></a>TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01947">http://arxiv.org/abs/2309.01947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Shangguan, Haichuan Yang, Danni Li, Chunyang Wu, Yassir Fathullah, Dilin Wang, Ayushi Dalmia, Raghuraman Krishnamoorthi, Ozlem Kalinli, Junteng Jia, Jay Mahadeokar, Xin Lei, Mike Seltzer, Vikas Chandra</li>
<li>for: 这个论文是为了提高机器学习模型在专门的硬件上的表现，并且可以将模型训练到多种硬件上。</li>
<li>methods: 这个论文使用了一种新的方法 called TODM（Train Once Deploy Many），它可以将多个硬件友好的模型训练成一个Supernet，并且运用了三种技术来提高结果：适应性 Dropout、实时Alpha-分布知识传授和Scale Adam优化器。</li>
<li>results: 这个论文的结果显示，使用TODM Supernet可以与手动调整的模型相比，在LibriSpeech中的字误率（WER）上提高到3%以上，而且可以实现将训练多个模型的成本降到小数字。<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) models need to be optimized for specific hardware before they can be deployed on devices. This can be done by tuning the model's hyperparameters or exploring variations in its architecture. Re-training and re-validating models after making these changes can be a resource-intensive task. This paper presents TODM (Train Once Deploy Many), a new approach to efficiently train many sizes of hardware-friendly on-device ASR models with comparable GPU-hours to that of a single training job. TODM leverages insights from prior work on Supernet, where Recurrent Neural Network Transducer (RNN-T) models share weights within a Supernet. It reduces layer sizes and widths of the Supernet to obtain subnetworks, making them smaller models suitable for all hardware types. We introduce a novel combination of three techniques to improve the outcomes of the TODM Supernet: adaptive dropouts, an in-place Alpha-divergence knowledge distillation, and the use of ScaledAdam optimizer. We validate our approach by comparing Supernet-trained versus individually tuned Multi-Head State Space Model (MH-SSM) RNN-T using LibriSpeech. Results demonstrate that our TODM Supernet either matches or surpasses the performance of manually tuned models by up to a relative of 3% better in word error rate (WER), while efficiently keeping the cost of training many models at a small constant.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）模型需要对特定硬件进行优化才能在设备上部署。这可以通过调整模型的超参数或探索模型的架构来实现。然而，重新训练和重新验证模型 после进行这些变化可能会占用资源。这篇论文介绍了TODM（Train Once Deploy Many），一种新的方法，可以高效地在多种硬件上训练适合硬件的各种大小的语音识别模型，而无需浪费大量的GPU时间。TODM利用了先前的Supernet工作的经验，其中Recurrent Neural Network Transducer（RNN-T）模型在Supernet中共享权重。它采用了减少Supernet中层数和宽度，以获得适合所有硬件类型的子网络。我们提出了一种新的组合技术，包括适应性Dropout、在位Alpha-分配知识继承和Scale Adam优化器。我们验证了我们的方法，将Supernet训练的模型与手动调整的Multi-Head State Space Model（MH-SSM）RNN-T模型进行比较，使用LibriSpeech数据集。结果表明，我们的TODM Supernet Either Matches或超过了手动调整模型的性能，在单词错误率（WER）方面提高了Relative 3%，而且高效地保持了训练多个模型的成本在小于一个常数。
</details></li>
</ul>
<hr>
<h2 id="OHQ-On-chip-Hardware-aware-Quantization"><a href="#OHQ-On-chip-Hardware-aware-Quantization" class="headerlink" title="OHQ: On-chip Hardware-aware Quantization"></a>OHQ: On-chip Hardware-aware Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01945">http://arxiv.org/abs/2309.01945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Huang, Haotong Qin, Yangdong Liu, Jingzhuo Liang, Yifu Ding, Ying Li, Xianglong Liu</li>
<li>For: This paper focuses on developing an On-chip Hardware-aware Quantization (OHQ) framework for deploying advanced deep models on resource-constrained hardware.* Methods: The proposed OHQ framework uses Mask-guided Quantization Estimation (MQE) technique to efficiently estimate the accuracy metrics of operators under the constraints of on-chip-level computing power, and synthesizes network and hardware insights through linear programming to obtain optimized bit-width configurations.* Results: The proposed OHQ framework achieves accelerated inference after quantization for various architectures and compression ratios, with 70% and 73% accuracy for ResNet-18 and MobileNetV3, respectively, and improves latency by 15~30% compared to INT8 on deployment.Here is the simplified Chinese text:* For: 这篇论文关注开发一种在有限资源硬件上部署高级深度模型的 On-chip Hardware-aware Quantization (OHQ) 框架。* Methods: OHQ 框架使用 Mask-guided Quantization Estimation (MQE) 技术来高效地估算运算数据的精度指标，并通过线性 программирова来整合网络和硬件视角来获得优化的比特宽配置。* Results: OHQ 框架实现了对不同架构和压缩比进行加速的推理，实现了 ResNet-18 和 MobileNetV3 的 70% 和 73% 的精度，并提高了 INT8 的执行时间的 latency 比例。<details>
<summary>Abstract</summary>
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique to efficiently estimate the accuracy metrics of operators under the constraints of on-chip-level computing power.By synthesizing network and hardware insights through linear programming, we obtain optimized bit-width configurations. Notably, the quantization process occurs on-chip entirely without any additional computing devices and data access. We demonstrate accelerated inference after quantization for various architectures and compression ratios, achieving 70% and 73% accuracy for ResNet-18 and MobileNetV3, respectively. OHQ improves latency by 15~30% compared to INT8 on deployment.
</details>
<details>
<summary>摘要</summary>
量化技术在部署高级深度模型时 emerges as one of the most promising approaches, 以实现资源受限的硬件上的高效和高精度模型部署。混合精度量化利用多 bit-width 架构，可以激活模型的准确和效率潜力。然而，现有的混合精度量化受搜索空间的约束，导致 immense computational overhead。因此，量化过程通常需要分离的高性能设备，而不是本地进行，这也导致了差距 между 考虑的硬件指标和实际部署。在这篇论文中，我们提出了一个 On-chip Hardware-aware Quantization (OHQ) 框架，可以在本地进行硬件具有资源限制的混合精度量化，不需要访问在线设备。首先，我们构建了 On-chip Quantization Awareness (OQA) 管道，使得量化操作的实际效率指标可以在硬件上被识别。其次，我们提出了 Mask-guided Quantization Estimation (MQE) 技术，可以高效地估计量化操作下的准确指标，在硬件上进行约束。通过线性 программирова，我们将网络和硬件元素相互协同，以获得优化的 bit-width 配置。值得注意的是，量化过程完全发生在本地，不需要任何额外的计算设备和数据访问。我们在不同的架构和压缩比例下进行加速的推理，实现了 ResNet-18 和 MobileNetV3 的 70% 和 73% 的准确率。相比 INT8，OHQ 提高了执行时间，为 15%~30%。
</details></li>
</ul>
<hr>
<h2 id="Quantum-AI-empowered-Intelligent-Surveillance-Advancing-Public-Safety-Through-Innovative-Contraband-Detection"><a href="#Quantum-AI-empowered-Intelligent-Surveillance-Advancing-Public-Safety-Through-Innovative-Contraband-Detection" class="headerlink" title="Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection"></a>Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03231">http://arxiv.org/abs/2309.03231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Atif Ali Shah, Nasir Algeelani, Najeeb Al-Sammarraie</li>
<li>for: 提高智能监控系统的准确率和实时性，以满足 densely populated 环境中的监控需求。</li>
<li>methods:  integrate RentinaNet 模型和 Quantum CNN，实现量子人工智能在监控领域的应用。</li>
<li>results: Quantum-RetinaNet 模型能够具有高准确率和实时性，打破传统 CNN 模型的速度和准确率之间的负担，为智能监控系统提供了一个新的选择。<details>
<summary>Abstract</summary>
Surveillance systems have emerged as crucial elements in upholding peace and security in the modern world. Their ubiquity aids in monitoring suspicious activities effectively. However, in densely populated environments, continuous active monitoring becomes impractical, necessitating the development of intelligent surveillance systems. AI integration in the surveillance domain was a big revolution, however, speed issues have prevented its widespread implementation in the field. It has been observed that quantum artificial intelligence has led to a great breakthrough. Quantum artificial intelligence-based surveillance systems have shown to be more accurate as well as capable of performing well in real-time scenarios, which had never been seen before. In this research, a RentinaNet model is integrated with Quantum CNN and termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN, Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative integration positions it as a game-changer, addressing the challenges of active monitoring in densely populated scenarios. As demand for efficient surveillance solutions continues to grow, Quantum-RetinaNet offers a compelling alternative to existing CNN models, upholding accuracy standards without sacrificing real-time performance. The unique attributes of Quantum-RetinaNet have far-reaching implications for the future of intelligent surveillance. With its enhanced processing speed, it is poised to revolutionize the field, catering to the pressing need for rapid yet precise monitoring. As Quantum-RetinaNet becomes the new standard, it ensures public safety and security while pushing the boundaries of AI in surveillance.
</details>
<details>
<summary>摘要</summary>
现代世界中维护和平安全的重要元素之一是监测系统。它们在监测可疑活动方面具有重要作用。然而，在高密度环境中，不断的活动监测成为不切实际，因此需要开发智能监测系统。在监测领域中，人工智能（AI）的应用是一个大革命，但速度问题使得其在实际应用中尚未普及。尽管如此，量子人工智能（Quantum AI）的出现带来了很大的突破。量子人工智能基于的监测系统表现出了更高的准确率和在实时场景中的出色表现，这从未被见过。本研究将量子逻辑网络（Quantum-RetinaNet）与量子卷积神经网络（QCNN）结合，并且通过利用量子计算机（QCNN）的量子特性，量子逻辑网络能够平衡准确率和速度。这种创新的结合使得它成为了一个游戏规则，解决了高密度环境中不断监测的挑战。随着需求高效监测解决方案的增长，量子逻辑网络成为了一个有力的替代方案，保持准确标准而不 sacrificing real-time性。量子逻辑网络的独特特点在未来对智能监测的发展有着深远的影响，它的提高处理速度使得它成为了监测领域的新标准，为公众安全和安全提供保障，同时推动人工智能在监测领域的发展。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Brain-Transformer-with-Multi-level-Attention-for-Functional-Brain-Network-Analysis"><a href="#Dynamic-Brain-Transformer-with-Multi-level-Attention-for-Functional-Brain-Network-Analysis" class="headerlink" title="Dynamic Brain Transformer with Multi-level Attention for Functional Brain Network Analysis"></a>Dynamic Brain Transformer with Multi-level Attention for Functional Brain Network Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01941">http://arxiv.org/abs/2309.01941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Kan, Antonio Aodong Chen Gu, Hejie Cui, Ying Guo, Carl Yang</li>
<li>for: 这篇论文的目的是提出一个新的方法来分析大脑功能，以更好地理解大脑组织和神经网络的运作。</li>
<li>methods: 这篇论文使用了一种新的方法，即动态大脑网络分析法（Dynamic Brain Transformer，DART），它结合了静止大脑网络和动态大脑网络，以提高大脑功能分析的精度和可靠性。</li>
<li>results: 这篇论文的结果显示，DRAT方法可以对于血液测量信号的噪音问题进行更好的处理，并且可以提供更多的实用信息，例如哪些神经网络或动态网络在最终预测中做出了贡献。<details>
<summary>Abstract</summary>
Recent neuroimaging studies have highlighted the importance of network-centric brain analysis, particularly with functional magnetic resonance imaging. The emergence of Deep Neural Networks has fostered a substantial interest in predicting clinical outcomes and categorizing individuals based on brain networks. However, the conventional approach involving static brain network analysis offers limited potential in capturing the dynamism of brain function. Although recent studies have attempted to harness dynamic brain networks, their high dimensionality and complexity present substantial challenges. This paper proposes a novel methodology, Dynamic bRAin Transformer (DART), which combines static and dynamic brain networks for more effective and nuanced brain function analysis. Our model uses the static brain network as a baseline, integrating dynamic brain networks to enhance performance against traditional methods. We innovatively employ attention mechanisms, enhancing model explainability and exploiting the dynamic brain network's temporal variations. The proposed approach offers a robust solution to the low signal-to-noise ratio of blood-oxygen-level-dependent signals, a recurring issue in direct DNN modeling. It also provides valuable insights into which brain circuits or dynamic networks contribute more to final predictions. As such, DRAT shows a promising direction in neuroimaging studies, contributing to the comprehensive understanding of brain organization and the role of neural circuits.
</details>
<details>
<summary>摘要</summary>
近期神经成像研究强调了脑网络中心式分析的重要性，尤其是使用功能核磁共振成像。深度神经网络的出现促使了预测临床结果和根据脑网络分类个体的研究。然而，传统方法的静态脑网络分析具有有限的潜在，不能够捕捉脑功能的动态性。虽然最近的研究尝试了利用动态脑网络，但高维度和复杂性带来了巨大的挑战。本文提出了一种新的方法ology，名为动态脑传变（DART），它结合静态脑网络和动态脑网络，以更有效和细腻地分析脑功能。我们的模型使用静态脑网络作为基线，并将动态脑网络与静态脑网络结合，以提高性能。我们创新地使用注意力机制，提高模型解释性，并利用动态脑网络的时间变化。我们的提案方法可以解决血氧含量信号的低信号噪声问题，这是直接DNN模型中常见的问题。此外，我们的方法还提供了精准的哪些脑回路或动态网络在最终预测中做出了更大的贡献。因此，DART表示了神经成像研究中一个有前途的方向，为脑组织的全面理解和神经细胞网络的作用做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Developing-A-Fair-Individualized-Polysocial-Risk-Score-iPsRS-for-Identifying-Increased-Social-Risk-of-Hospitalizations-in-Patients-with-Type-2-Diabetes-T2D"><a href="#Developing-A-Fair-Individualized-Polysocial-Risk-Score-iPsRS-for-Identifying-Increased-Social-Risk-of-Hospitalizations-in-Patients-with-Type-2-Diabetes-T2D" class="headerlink" title="Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)"></a>Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02467">http://arxiv.org/abs/2309.02467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Huang, Jingchuan Guo, William T Donahoo, Zhengkang Fan, Ying Lu, Wei-Han Chen, Huilin Tang, Lori Bilello, Elizabeth A Shenkman, Jiang Bian<br>for: The paper aims to develop an EHR-based machine learning analytical pipeline to identify the unmet social needs associated with hospitalization risk in patients with type 2 diabetes (T2D).methods: The paper uses electronic health records (EHR) data from the University of Florida Health Integrated Data Repository, along with contextual and individual-level social determinants of health (SDoH) data, to develop an individualized polysocial risk score (iPsRS) to identify high social risk associated with hospitalizations in T2D patients. The paper also employs explainable AI (XAI) techniques and fairness assessment and optimization.results: The paper finds that the iPsRS achieved a C statistic of 0.72 in predicting 1-year hospitalization after fairness optimization across racial-ethnic groups. The iPsRS showed excellent utility for capturing individuals at high hospitalization risk, with the actual 1-year hospitalization rate in the top 5% of iPsRS being ~13 times as high as the bottom decile.Here is the information in Simplified Chinese text:for: 这个论文的目的是开发一个基于电子健康纪录 (EHR) 的机器学习分析管道，以识别患有类型 2  диабеetes (T2D) 患者的社会需求不足，导致的医院风险。methods: 这个论文使用大学OFlorida 健康集成数据存储库中的 EHR 数据，并与社会 Determinants of health (SDoH) 数据相结合，开发了一个个性化多社会风险分数 (iPsRS)，以识别 T2D 患者的高社会风险。论文还使用可解释 AI (XAI) 技术和公平评估和优化。results: 论文发现，iPsRS 在各个种族-民族组中进行公平优化后，对 1 年医院风险预测达到 C 统计值为 0.72。iPsRS 显示了极佳的实用性，可以准确地捕捉高医院风险的个人，Actual 1 年医院风险率在 top 5% 的 iPsRS 是 bottom decile 的 ~13 倍。<details>
<summary>Abstract</summary>
Background: Racial and ethnic minority groups and individuals facing social disadvantages, which often stem from their social determinants of health (SDoH), bear a disproportionate burden of type 2 diabetes (T2D) and its complications. It is therefore crucial to implement effective social risk management strategies at the point of care. Objective: To develop an EHR-based machine learning (ML) analytical pipeline to identify the unmet social needs associated with hospitalization risk in patients with T2D. Methods: We identified 10,192 T2D patients from the EHR data (from 2012 to 2022) from the University of Florida Health Integrated Data Repository, including contextual SDoH (e.g., neighborhood deprivation) and individual-level SDoH (e.g., housing stability). We developed an electronic health records (EHR)-based machine learning (ML) analytic pipeline, namely individualized polysocial risk score (iPsRS), to identify high social risk associated with hospitalizations in T2D patients, along with explainable AI (XAI) techniques and fairness assessment and optimization. Results: Our iPsRS achieved a C statistic of 0.72 in predicting 1-year hospitalization after fairness optimization across racial-ethnic groups. The iPsRS showed excellent utility for capturing individuals at high hospitalization risk; the actual 1-year hospitalization rate in the top 5% of iPsRS was ~13 times as high as the bottom decile. Conclusion: Our ML pipeline iPsRS can fairly and accurately screen for patients who have increased social risk leading to hospitalization in T2D patients.
</details>
<details>
<summary>摘要</summary>
背景：种族和民族少数群体和受社会不平等困扰的个人 often 患有型二糖尿病（T2D）和其合并症状。因此，实施有效的社会风险管理策略在点患者护理中是非常重要。目标：开发基于电子健康纪录（EHR）的机器学习（ML）分析管道，以识别患有T2D patients的医疗机器人化风险。方法：我们从2012年至2022年的University of Florida Health Integrated Data Repository中提取了10,192名T2D患者的EHR数据，包括上下文ual SDoH（例如，社区贫困）和个人级SDoH（例如，住房稳定）。我们开发了基于EHR的ML分析管道，即个性化多社会风险分数（iPsRS），以识别患有T2D patients中高社会风险与入院风险的个人，同时使用可解释AI（XAI）技术和公平评估和优化。结果：我们的iPsRS在各种种族-民族组中进行公平优化后，C Statistics 值为0.72，可以准确预测患有T2D patients的1年入院风险。iPsRS表现出了捕捉高入院风险个人的出色Utility，实际1年入院率在top 5% iPsRS的个人为bottom decile的13倍。结论：我们的ML管道iPsRS可以公平、准确地检测患有T2D patients中高社会风险导致入院风险的个人。
</details></li>
</ul>
<hr>
<h2 id="Provably-safe-systems-the-only-path-to-controllable-AGI"><a href="#Provably-safe-systems-the-only-path-to-controllable-AGI" class="headerlink" title="Provably safe systems: the only path to controllable AGI"></a>Provably safe systems: the only path to controllable AGI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01933">http://arxiv.org/abs/2309.01933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Tegmark, Steve Omohundro</li>
<li>for: 这篇论文写于如何使人类安全地树成功的人工通用智能（AGI）。</li>
<li>methods: 该论文提出了使用高级AI进行正式验证和机械解释来建立AGI满足人类指定的要求。</li>
<li>results: 该论文认为这将很快成为技术可能的，并且只有这一路径可以保证安全控制AGI。<details>
<summary>Abstract</summary>
We describe a path to humanity safely thriving with powerful Artificial General Intelligences (AGIs) by building them to provably satisfy human-specified requirements. We argue that this will soon be technically feasible using advanced AI for formal verification and mechanistic interpretability. We further argue that it is the only path which guarantees safe controlled AGI. We end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.
</details>
<details>
<summary>摘要</summary>
我们描述了一条路径，使人类安全快乐地与高能力人工通用智能机器人（AGI）共处，通过建立AGI以满足人类指定的要求来实现。我们认为，技术上很快就会实现这一点，使用先进的AI进行正式验证和机械解释。我们还认为，这是 garantuee 安全控制AGI的唯一路径。我们列出了一些挑战问题，解决这些问题会为这一 pozitivo 结果做出贡献，并邀请读者参与这项工作。Note: "guarantee" is translated as " garantuee" in Simplified Chinese, which is a common way to write "guarantee" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Regret-Analysis-of-Policy-Gradient-Algorithm-for-Infinite-Horizon-Average-Reward-Markov-Decision-Processes"><a href="#Regret-Analysis-of-Policy-Gradient-Algorithm-for-Infinite-Horizon-Average-Reward-Markov-Decision-Processes" class="headerlink" title="Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes"></a>Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01922">http://arxiv.org/abs/2309.01922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal</li>
<li>for: 这个论文关注的是无穷远 horizon average reward Markov Decision Process（MDP）。与现有的研究不同，我们的方法不假设MDP结构为线性的。</li>
<li>methods: 我们提出了基于政策梯度的算法，并证明其全球协调性。</li>
<li>results: 我们证明了该算法的$\tilde{\mathcal{O}({T}^{3&#x2F;4})$ regret。这是首次在average reward场景中对通用参数化政策梯度算法的 regret-bound计算。<details>
<summary>Abstract</summary>
In this paper, we consider an infinite horizon average reward Markov Decision Process (MDP). Distinguishing itself from existing works within this context, our approach harnesses the power of the general policy gradient-based algorithm, liberating it from the constraints of assuming a linear MDP structure. We propose a policy gradient-based algorithm and show its global convergence property. We then prove that the proposed algorithm has $\tilde{\mathcal{O}({T}^{3/4})$ regret. Remarkably, this paper marks a pioneering effort by presenting the first exploration into regret-bound computation for the general parameterized policy gradient algorithm in the context of average reward scenarios.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了一个无穷horizon平均奖励Markov决策过程（MDP）。与现有的研究不同，我们的方法利用了通用的策略梯度基本算法， liberating it from the constraints of assuming a linear MDP structure。我们提出了一种策略梯度基本算法，并证明其全球归一化性。然后，我们证明了我们的算法有$\tilde{\mathcal{O}({T}^{3/4})$的念骨。值得注意的是，这篇论文标志着我们在平均奖励场景中的首次探索regret bound computation的尝试。
</details></li>
</ul>
<hr>
<h2 id="RoboAgent-Generalization-and-Efficiency-in-Robot-Manipulation-via-Semantic-Augmentations-and-Action-Chunking"><a href="#RoboAgent-Generalization-and-Efficiency-in-Robot-Manipulation-via-Semantic-Augmentations-and-Action-Chunking" class="headerlink" title="RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking"></a>RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01918">http://arxiv.org/abs/2309.01918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, Vikash Kumar</li>
<li>For:  train a single robot to perform diverse manipulation skills in various settings* Methods:  semantic augmentations and action representations to extract performant policies with small datasets, and reliable task conditioning and expressive policy architecture to exhibit diverse skills in novel situations* Results:  trained a single agent capable of 12 unique skills with 7500 demonstrations, and demonstrated its generalization over 38 tasks in diverse kitchen scenes, outperforming prior methods by over 40% in unseen situations while being more sample efficient and amenable to capability improvements and extensions through fine-tuning.<details>
<summary>Abstract</summary>
The grand aim of having a single robot that can manipulate arbitrary objects in diverse settings is at odds with the paucity of robotics datasets. Acquiring and growing such datasets is strenuous due to manual efforts, operational costs, and safety challenges. A path toward such an universal agent would require a structured framework capable of wide generalization but trained within a reasonable data budget. In this paper, we develop an efficient system (RoboAgent) for training universal agents capable of multi-task manipulation skills using (a) semantic augmentations that can rapidly multiply existing datasets and (b) action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting. In addition, reliable task conditioning and an expressive policy architecture enable our agent to exhibit a diverse repertoire of skills in novel situations specified using language commands. Using merely 7500 demonstrations, we are able to train a single agent capable of 12 unique skills, and demonstrate its generalization over 38 tasks spread across common daily activities in diverse kitchen scenes. On average, RoboAgent outperforms prior methods by over 40% in unseen situations while being more sample efficient and being amenable to capability improvements and extensions through fine-tuning. Videos at https://robopen.github.io/
</details>
<details>
<summary>摘要</summary>
文章标题：一种高效的机器人多任务 manipulate 技能训练系统（RoboAgent）摘要：在具有单一机器人可 manipulate 任意物品的宏目标面前，机器人数据集的缺乏是一大问题。获取和扩大这些数据集需要大量的人工劳动、运维成本和安全挑战。为实现这样的通用代理人，我们需要一种结构化的框架，可以广泛适应但是在有限的数据预算内训练。本文提出了一种高效的机器人训练系统（RoboAgent），可以通过（a）semantic 扩充和（b）动作表示来快速增加现有数据集，并提取高性能策略。此外，我们还提出了可靠的任务条件和表达丰富的策略架构，使我们的代理人能够在新的语言命令下展现多样化的技能。使用仅 7500 示例，我们能够训练一个可以完成 12 种任务的单一机器人，并在多个日常活动的厨房场景中展现其普适性。在未看过的情况下，RoboAgent 比 Prior 方法高效了更多的 40%，同时更 sample 有效和可以通过细化进行改进和扩展。视频见 [[1](https://robopen.github.io/)]。Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Physics-Informed-Reinforcement-Learning-Review-and-Open-Problems"><a href="#A-Survey-on-Physics-Informed-Reinforcement-Learning-Review-and-Open-Problems" class="headerlink" title="A Survey on Physics Informed Reinforcement Learning: Review and Open Problems"></a>A Survey on Physics Informed Reinforcement Learning: Review and Open Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01909">http://arxiv.org/abs/2309.01909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chayan Banerjee, Kien Nguyen, Clinton Fookes, Maziar Raissi</li>
<li>for: 本研究旨在探讨physics-informed reinforcement learning（PIRL）的应用 potential，包括如何在reinforcement learning框架中integrate physical laws和约束，以提高算法的physical plausibility和实用性。</li>
<li>methods: 本研究使用了一种新的分类方法，通过对existings works的分析和比较，derive crucial insights并提出了一种新的taxonomy，用于 классификацияPIRLapproaches。这种分类方法将physics-informed reinforcement learning pipeline分成了三个主要部分：observational bias, inductive bias,和learning bias。</li>
<li>results: 本研究发现PIRLapproaches可以增强reinforcement learning algorithm的physical plausibility和精度，同时提高数据效率和实用性。此外，研究还发现了一些未解决的问题和挑战，例如如何在不同的physics domain中apply PIRLapproaches，以及如何调整PIRLapproaches的parameters和hyperparameters。<details>
<summary>Abstract</summary>
The inclusion of physical information in machine learning frameworks has revolutionized many application areas. This involves enhancing the learning process by incorporating physical constraints and adhering to physical laws. In this work we explore their utility for reinforcement learning applications. We present a thorough review of the literature on incorporating physics information, as known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning (PIRL). We introduce a novel taxonomy with the reinforcement learning pipeline as the backbone to classify existing works, compare and contrast them, and derive crucial insights. Existing works are analyzed with regard to the representation/ form of the governing physics modeled for integration, their specific contribution to the typical reinforcement learning architecture, and their connection to the underlying reinforcement learning pipeline stages. We also identify core learning architectures and physics incorporation biases (i.e., observational, inductive and learning) of existing PIRL approaches and use them to further categorize the works for better understanding and adaptation. By providing a comprehensive perspective on the implementation of the physics-informed capability, the taxonomy presents a cohesive approach to PIRL. It identifies the areas where this approach has been applied, as well as the gaps and opportunities that exist. Additionally, the taxonomy sheds light on unresolved issues and challenges, which can guide future research. This nascent field holds great potential for enhancing reinforcement learning algorithms by increasing their physical plausibility, precision, data efficiency, and applicability in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
“机器学习框架中包含物理信息的应用已经革命化了许多应用领域。这包括将物理限制和物理法则纳入机器学习过程中，以提高学习效能。在这个研究中，我们将Physics-informed reinforcement learning（PIRL）的 utility 在应用中探讨。我们提出了一个新的分类方案，并将现有的工作分为三大类：代表物理模型的形式、对应的实际应用和与基本的机器学习架构相关的Connection。我们还识别了现有PIRL方法的核心学习架构和物理包含偏见，并将它们用来进一步分类工作，以更好地理解和适应。这个统一的方法提供了一个广泛的见解，探讨PIRL在实际应用中的实现方式，包括这个领域的应用、缺乏和未来研究的机遇。这个新兴的领域具有增加物理可能性、精度、数据效率和实际应用中的应用潜力。”
</details></li>
</ul>
<hr>
<h2 id="Inferring-Actual-Treatment-Pathways-from-Patient-Records"><a href="#Inferring-Actual-Treatment-Pathways-from-Patient-Records" class="headerlink" title="Inferring Actual Treatment Pathways from Patient Records"></a>Inferring Actual Treatment Pathways from Patient Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01897">http://arxiv.org/abs/2309.01897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Wilkins-Caruana, Madhushi Bandara, Katarzyna Musial, Daniel Catchpoole, Paul J. Kennedy</li>
<li>for: 这个研究旨在从管理健康记录 (AHR) 中INFER实际进行的治疗步骤，以提高患者结果。</li>
<li>methods: 该研究使用了一种名为 Defrag 的方法，该方法可以学习健康事件序列的semantic和temporal含义，从而可靠地INFER治疗步骤。 Defrag 使用了人工神经网络 (NN)，这是由一种新的自我超vised学习目标实现的。</li>
<li>results: 研究表明，Defrag 可以有效地INFER breast cancer、lung cancer 和 melanoma 等癌症的治疗路径，并且在 synthetic data  экспериментах中，Defrag 显著超过了一些基准方法。<details>
<summary>Abstract</summary>
Treatment pathways are step-by-step plans outlining the recommended medical care for specific diseases; they get revised when different treatments are found to improve patient outcomes. Examining health records is an important part of this revision process, but inferring patients' actual treatments from health data is challenging due to complex event-coding schemes and the absence of pathway-related annotations. This study aims to infer the actual treatment steps for a particular patient group from administrative health records (AHR) - a common form of tabular healthcare data - and address several technique- and methodology-based gaps in treatment pathway-inference research. We introduce Defrag, a method for examining AHRs to infer the real-world treatment steps for a particular patient group. Defrag learns the semantic and temporal meaning of healthcare event sequences, allowing it to reliably infer treatment steps from complex healthcare data. To our knowledge, Defrag is the first pathway-inference method to utilise a neural network (NN), an approach made possible by a novel, self-supervised learning objective. We also developed a testing and validation framework for pathway inference, which we use to characterise and evaluate Defrag's pathway inference ability and compare against baselines. We demonstrate Defrag's effectiveness by identifying best-practice pathway fragments for breast cancer, lung cancer, and melanoma in public healthcare records. Additionally, we use synthetic data experiments to demonstrate the characteristics of the Defrag method, and to compare Defrag to several baselines where it significantly outperforms non-NN-based methods. Defrag significantly outperforms several existing pathway-inference methods and offers an innovative and effective approach for inferring treatment pathways from AHRs. Open-source code is provided to encourage further research in this area.
</details>
<details>
<summary>摘要</summary>
医疗路径是一系列步骤计划，用于确定特定疾病的推荐医疗方案。这些医疗路径会随着新的治疗方法的发现而进行修订。检查医疗记录是修订过程中的重要一步，但从医疗数据中推导病人实际接受的治疗步骤是具有挑战性的，因为医疗记录中的事件编码系统复杂，而且缺乏路径相关的注释。本研究的目标是从医疗记录中推导特定病人群的实际治疗步骤，并解决了一些技术和方法基础上的差距。我们介绍了一种名为Defrag的方法，可以从医疗记录中推导病人实际接受的治疗步骤。Defrag可以学习医疗事件序列的 semantic 和时间含义，以便可靠地从复杂的医疗数据中推导治疗步骤。我们知道，Defrag 是首个使用神经网络（NN）的医疗路径推导方法，我们还开发了一个用于评估和验证医疗路径推导能力的测试和验证框架。我们在公共医疗记录中identified breast cancer, lung cancer和melanoma的best-practice路径片段。此外，我们使用 sintetic 数据实验来描述Defrag 方法的特点，并与多种基eline进行比较，其中Defrag 显著超过非 NN 基eline。Defrag 提供了一种创新的和有效的医疗路径推导方法，可以从医疗记录中推导病人实际接受的治疗步骤。我们提供了开源代码，以便更多人进行进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="Extended-Symmetry-Preserving-Attention-Networks-for-LHC-Analysis"><a href="#Extended-Symmetry-Preserving-Attention-Networks-for-LHC-Analysis" class="headerlink" title="Extended Symmetry Preserving Attention Networks for LHC Analysis"></a>Extended Symmetry Preserving Attention Networks for LHC Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01886">http://arxiv.org/abs/2309.01886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael James Fenton, Alexander Shmakov, Hideki Okawa, Yuji Li, Ko-Yang Hsiao, Shih-Chieh Hsu, Daniel Whiteson, Pierre Baldi</li>
<li>for: 这个论文的目的是扩展基于通用注意机制的扩展性网络（SPANet），以便在 Large Hadron Collider 上处理涉及多个输入流的潜在重粒子征。</li>
<li>methods: 这篇论文使用了一种基于通用注意机制的扩展性网络（SPANet），以便处理多个输入流，包括电子和全局事件特征。</li>
<li>results: 研究发现，使用扩展的 SPANet 可以在 semi-leptonic 探测中提高探测能力，并且在 top quark pair 生成协同 Higgs  boson 中也能够获得显著提高。<details>
<summary>Abstract</summary>
Reconstructing unstable heavy particles requires sophisticated techniques to sift through the large number of possible permutations for assignment of detector objects to partons. An approach based on a generalized attention mechanism, symmetry preserving attention networks (SPANet), has been previously applied to top quark pair decays at the Large Hadron Collider, which produce six hadronic jets. Here we extend the SPANet architecture to consider multiple input streams, such as leptons, as well as global event features, such as the missing transverse momentum. In addition, we provide regression and classification outputs to supplement the parton assignment. We explore the performance of the extended capability of SPANet in the context of semi-leptonic decays of top quark pairs as well as top quark pairs produced in association with a Higgs boson. We find significant improvements in the power of three representative studies: search for ttH, measurement of the top quark mass and a search for a heavy Z' decaying to top quark pairs. We present ablation studies to provide insight on what the network has learned in each case.
</details>
<details>
<summary>摘要</summary>
重构不稳定的重子需要使用复杂的技术来对探测器对部分子的分配进行搜索。一种基于普通注意机制的扩展版本（SPANet）在大夸子机器中已经应用于top顺子异常衰变，生成六个坍塌探测器。在这里，我们扩展了SPANet架构，考虑多个输入流，如电子，以及全事件特征，如缺失横向动量。此外，我们还提供了回归和分类输出，以补充部分赋值。我们在semi-leptonic decay of top quark pairs和top quark pairs with Higgs boson production中探索了扩展的SPANet能力的表现。我们发现在三个代表性研究中有显著的改善：搜索ttH、测量top顺子质量和搜索重Z'衰变为top顺子对。我们进行了剥离研究，以了解网络在每个情况中学习的内容。
</details></li>
</ul>
<hr>
<h2 id="QuantEase-Optimization-based-Quantization-for-Language-Models-–-An-Efficient-and-Intuitive-Algorithm"><a href="#QuantEase-Optimization-based-Quantization-for-Language-Models-–-An-Efficient-and-Intuitive-Algorithm" class="headerlink" title="QuantEase: Optimization-based Quantization for Language Models – An Efficient and Intuitive Algorithm"></a>QuantEase: Optimization-based Quantization for Language Models – An Efficient and Intuitive Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01885">http://arxiv.org/abs/2309.01885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya Keerthi, Rahul Mazumder</li>
<li>for: This paper focuses on the Post-Training Quantization (PTQ) of Large Language Models (LLMs) to achieve efficient deployment.</li>
<li>methods: The paper introduces QuantEase, a layer-wise quantization framework that uses Coordinate Descent (CD) techniques to solve the discrete-structured non-convex optimization problem. The CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems, with straightforward updates that rely solely on matrix and vector operations.</li>
<li>results: The proposed approach achieves state-of-the-art performance in terms of perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements up to 15% over methods such as GPTQ. The outlier-aware variant of the approach retains significant weights (outliers) with complete precision, allowing for near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy.<details>
<summary>Abstract</summary>
With the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs. Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization. The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques. These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems. Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition. We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision. Our proposal attains state-of-the-art performance in terms of perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements up to 15% over methods such as GPTQ. Particularly noteworthy is our outlier-aware algorithm's capability to achieve near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform quantization or grouping techniques, improving upon methods such as SpQR by up to two times in terms of perplexity.
</details>
<details>
<summary>摘要</summary>
随着大语言模型（LLM）的流行，压缩技术的实现已成为一项突出的研究方向。这项研究探讨了 LLM 的 POST-TRAINING QUANTIZATION（PTQ）技术。基于最新的进展，我们提出了 QuantEase，一种层wise 压缩框架，其中每层都进行独立压缩。问题定义为一个离散结构非几何优化问题，因此我们开发了基于 Coordinate Descent（CD）技术的算法。这些 CD 基于的算法可以提供高质量的解决方案，并且具有简单的更新规则，只需要基于矩阵和向量操作，不需要矩阵反转或分解。此外，我们还提出了一种具有对重要权重（异常值）的考虑的变体，可以保留完整的精度。我们的提议在实验中实现了state-of-the-art的表现，包括折衔率和零shot精度，与方法such as GPTQ 相比，有相对提升达 15%。特别值得一提的是我们的异常值考虑变体可以在 LLM 中实现近似或小于 3 位压缩，而无需非几何压缩或分组技术，与方法such as SpQR 相比，可以提高表现的两倍。
</details></li>
</ul>
<hr>
<h2 id="Task-Generalization-with-Stability-Guarantees-via-Elastic-Dynamical-System-Motion-Policies"><a href="#Task-Generalization-with-Stability-Guarantees-via-Elastic-Dynamical-System-Motion-Policies" class="headerlink" title="Task Generalization with Stability Guarantees via Elastic Dynamical System Motion Policies"></a>Task Generalization with Stability Guarantees via Elastic Dynamical System Motion Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01884">http://arxiv.org/abs/2309.01884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Li, Nadia Figueroa</li>
<li>for: 学习从示例（Learning from Demonstration）的动力系统（Dynamical System），以稳定性和整合保证从少量轨迹学习激烈动作策略。</li>
<li>methods: 提议使用Gaussian Mixture Model（GMM）基于线性参数变化（LPV）动力系统模型，并将任务参数直接嵌入到GMM中，使其能够� Generic 到新任务实例。</li>
<li>results: 在许多模拟和实际 робоット实验中，Elastic-DS表现出柔顺性和灵活性，并保持了控制理论上的保证。详细视频可以在<a target="_blank" rel="noopener" href="https://sites.google.com/view/elastic-ds">https://sites.google.com/view/elastic-ds</a> 找到。<details>
<summary>Abstract</summary>
Dynamical System (DS) based Learning from Demonstration (LfD) allows learning of reactive motion policies with stability and convergence guarantees from a few trajectories. Yet, current DS learning techniques lack the flexibility to generalize to new task instances as they ignore explicit task parameters that inherently change the underlying trajectories. In this work, we propose Elastic-DS, a novel DS learning, and generalization approach that embeds task parameters into the Gaussian Mixture Model (GMM) based Linear Parameter Varying (LPV) DS formulation. Central to our approach is the Elastic-GMM, a GMM constrained to SE(3) task-relevant frames. Given a new task instance/context, the Elastic-GMM is transformed with Laplacian Editing and used to re-estimate the LPV-DS policy. Elastic-DS is compositional in nature and can be used to construct flexible multi-step tasks. We showcase its strength on a myriad of simulated and real-robot experiments while preserving desirable control-theoretic guarantees. Supplementary videos can be found at https://sites.google.com/view/elastic-ds
</details>
<details>
<summary>摘要</summary>
dynamical system (DS) 基于学习从示例 (LfD) 可以从一些轨迹学习响应性动作策略，并提供稳定性和收敛保证。然而，现有的 DS 学习技术缺乏扩展到新任务实例的灵活性，因为它们忽略了明确的任务参数，这些参数直接影响下面轨迹。在这项工作中，我们提出了 Elastic-DS，一种新的 DS 学习和泛化方法，它嵌入了任务参数到 Gaussian Mixture Model (GMM) 基于 Linear Parameter Varying (LPV) DS 形式ulation中。Elastic-DS 的核心思想是使用 Laplacian Editing 将 GMM 约束到 SE(3) 任务相关帧。给定一个新任务实例/上下文，Elastic-GMM 会被转换并用于重新估算 LPV-DS 策略。Elastic-DS 是可组合的，可以用来构建灵活的多步任务。我们在许多模拟和实际 робо臂实验中证明了它的强大，同时保留了控制理论上的可靠保证。补充视频可以在 https://sites.google.com/view/elastic-ds 找到。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Domain-Diffusion-Models-for-Image-Synthesis"><a href="#Gradient-Domain-Diffusion-Models-for-Image-Synthesis" class="headerlink" title="Gradient Domain Diffusion Models for Image Synthesis"></a>Gradient Domain Diffusion Models for Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01875">http://arxiv.org/abs/2309.01875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Gong</li>
<li>for: 这个论文应用于生成图像和视频Synthesis中的Diffusion模型，以提高它们的效率。</li>
<li>methods: 这个论文提出了在梯度领域进行Diffusion过程，以便更快地读取数据。这是因为梯度领域是原始图像领域的数学等价物，因此每个Diffusion步骤在图像领域有唯一的梯度领域表示。此外，梯度领域比图像领域更为简单，因此Diffusion模型在这个领域中更快地读取数据。</li>
<li>results: 数值实验证明， gradient domainDiffusion模型比原始Diffusion模型更有效率。这个方法可以应用于各种图像处理、计算机视觉和机器学习任务。<details>
<summary>Abstract</summary>
Diffusion models are getting popular in generative image and video synthesis. However, due to the diffusion process, they require a large number of steps to converge. To tackle this issue, in this paper, we propose to perform the diffusion process in the gradient domain, where the convergence becomes faster. There are two reasons. First, thanks to the Poisson equation, the gradient domain is mathematically equivalent to the original image domain. Therefore, each diffusion step in the image domain has a unique corresponding gradient domain representation. Second, the gradient domain is much sparser than the image domain. As a result, gradient domain diffusion models converge faster. Several numerical experiments confirm that the gradient domain diffusion models are more efficient than the original diffusion models. The proposed method can be applied in a wide range of applications such as image processing, computer vision and machine learning tasks.
</details>
<details>
<summary>摘要</summary>
Diffusion模型在生成图像和视频合成中越来越受欢迎。然而，由于扩散过程，它们需要较多的步骤才能够融合。为了解决这个问题，在这篇论文中，我们提议在梯度领域中进行扩散过程。有两个原因。一是，根据波恩Equation，梯度领域与原始图像领域是数学等价的。因此，每一步 diffusion 在图像领域中都有唯一的梯度领域表示。二是，梯度领域比图像领域更加稀疏。因此，梯度领域扩散模型更快 converges。一些数字实验证明，梯度领域扩散模型比原始扩散模型更高效。提议的方法可以应用于各种图像处理、计算机视觉和机器学习任务中。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Query-Based-Attack-against-ML-Based-Android-Malware-Detection-under-Zero-Knowledge-Setting"><a href="#Efficient-Query-Based-Attack-against-ML-Based-Android-Malware-Detection-under-Zero-Knowledge-Setting" class="headerlink" title="Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting"></a>Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01866">http://arxiv.org/abs/2309.01866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping He, Yifan Xia, Xuhong Zhang, Shouling Ji</li>
<li>for: 本研究旨在提高Android骗器检测方法的安全性，因为黑客可以利用骗器来攻击 Android 操作系统。</li>
<li>methods: 本研究使用机器学习基于的 Android 骗器检测方法，但是这些方法受到逆向工程的威胁。</li>
<li>results: 本研究提出了一种名为 AdvDroidZero 的高效的查询型攻击框架，可以在零知识情况下攻击机器学习基于的 Android 骗器检测方法，并且对多种主流机器学习基于的骗器检测方法和实际应用解决方案进行了广泛的评估。<details>
<summary>Abstract</summary>
The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.
</details>
<details>
<summary>摘要</summary>
Android 操作系统的广泛采用使得恶意 Android 应用程序成为了攻击者的吸引点。基于机器学习（ML）的 Android 黑客检测方法（AMD）是解决这个问题的关键，但它们受到了对抗示例的攻击的担忧。现有的对 ML-based AMD 方法的攻击方法表现出色，但它们假设了可能不是实际场景中的假设，例如特征空间、模型参数和训练数据的知识要求。为解决这个限制，我们介绍了 AdvDroidZero，一种基于查询的攻击框架，在零知识设定下运行。我们进行了广泛的评估，显示 AdvDroidZero 对主流 ML-based AMD 方法和实际应用中的抗病毒解决方案都有高效性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.LG_2023_09_05/" data-id="clmjn91mw007z0j882evl4e0e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/eess.IV_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T09:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/eess.IV_2023_09_05/">eess.IV - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-Improved-Upper-Bound-on-the-Rate-Distortion-Function-of-Images"><a href="#An-Improved-Upper-Bound-on-the-Rate-Distortion-Function-of-Images" class="headerlink" title="An Improved Upper Bound on the Rate-Distortion Function of Images"></a>An Improved Upper Bound on the Rate-Distortion Function of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02574">http://arxiv.org/abs/2309.02574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duanzhiihao/lossy-vae">https://github.com/duanzhiihao/lossy-vae</a></li>
<li>paper_authors: Zhihao Duan, Jack Ma, Jiangpeng He, Fengqing Zhu</li>
<li>for: 这 paper 是为了提出一种基于 Variational Autoencoders (VAEs) 的图像lossy compression方法。</li>
<li>methods: 这 paper 使用了一种新的 VAE 模型架构，应用了变量比率压缩技术，并提出了一种新的 \ourfunction{} 来稳定训练。</li>
<li>results: 这 paper 的实验结果表明，使用这种方法可以实现至少 30% BD-rate 减少，相比 VVC codec 的内部预测模式。这表明还有很大的潜在空间来提高图像lossy compression。<details>
<summary>Abstract</summary>
Recent work has shown that Variational Autoencoders (VAEs) can be used to upper-bound the information rate-distortion (R-D) function of images, i.e., the fundamental limit of lossy image compression. In this paper, we report an improved upper bound on the R-D function of images implemented by (1) introducing a new VAE model architecture, (2) applying variable-rate compression techniques, and (3) proposing a novel \ourfunction{} to stabilize training. We demonstrate that at least 30\% BD-rate reduction w.r.t. the intra prediction mode in VVC codec is achievable, suggesting that there is still great potential for improving lossy image compression. Code is made publicly available at https://github.com/duanzhiihao/lossy-vae.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，变分自动编码器（VAEs）可以用来上界图像的信息率-损失（R-D）函数，即图像压缩的基本上限。在这篇论文中，我们报道了一种改进的上界方法，包括（1）介绍新的 VAE 模型建立，（2）应用Variable-rate压缩技术，以及（3）提出一种新的 \ourfunction{} 来稳定训练。我们示示了至少30%的BD-率减少相对于VVC编码器的内部预测模式，这表明还有很大的潜在可能性 для改进图像压缩。代码可以在 GitHub 上获取，https://github.com/duanzhiihao/lossy-vae。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-Kidney-Layer-Segmentation-on-Whole-Slide-Imaging-using-Convolutional-Neural-Networks-and-Transformers"><a href="#Evaluation-Kidney-Layer-Segmentation-on-Whole-Slide-Imaging-using-Convolutional-Neural-Networks-and-Transformers" class="headerlink" title="Evaluation Kidney Layer Segmentation on Whole Slide Imaging using Convolutional Neural Networks and Transformers"></a>Evaluation Kidney Layer Segmentation on Whole Slide Imaging using Convolutional Neural Networks and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02563">http://arxiv.org/abs/2309.02563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhao Liu, Chenyang Qi, Shunxing Bao, Quan Liu, Ruining Deng, Yu Wang, Shilin Zhao, Haichun Yang, Yuankai Huo</li>
<li>for:  automatized image analysis in renal pathology</li>
<li>methods:  deep learning-based approach, including Swin-Unet, Medical-Transformer, TransUNet, U-Net, PSPNet, and DeepLabv3+</li>
<li>results:  Transformer models generally outperform CNN-based models, with a decent Mean Intersection over Union (mIoU) index and promising results for quantitative evaluation of renal cortical structures.Here’s the full Chinese text:</li>
<li>for:  automatized图像分析在肾脏病学中</li>
<li>methods: 使用深度学习方法，包括Swin-Unet、医疗转换器、TransUNet、U-Net、PSPNet和DeepLabv3+</li>
<li>results:  transformer模型通常比CNN模型表现更好，并具有可靠的 Mean Intersection over Union (mIoU) 指标和对肾脏 cortical结构的量化评估。<details>
<summary>Abstract</summary>
The segmentation of kidney layer structures, including cortex, outer stripe, inner stripe, and inner medulla within human kidney whole slide images (WSI) plays an essential role in automated image analysis in renal pathology. However, the current manual segmentation process proves labor-intensive and infeasible for handling the extensive digital pathology images encountered at a large scale. In response, the realm of digital renal pathology has seen the emergence of deep learning-based methodologies. However, very few, if any, deep learning based approaches have been applied to kidney layer structure segmentation. Addressing this gap, this paper assesses the feasibility of performing deep learning based approaches on kidney layer structure segmetnation. This study employs the representative convolutional neural network (CNN) and Transformer segmentation approaches, including Swin-Unet, Medical-Transformer, TransUNet, U-Net, PSPNet, and DeepLabv3+. We quantitatively evaluated six prevalent deep learning models on renal cortex layer segmentation using mice kidney WSIs. The empirical results stemming from our approach exhibit compelling advancements, as evidenced by a decent Mean Intersection over Union (mIoU) index. The results demonstrate that Transformer models generally outperform CNN-based models. By enabling a quantitative evaluation of renal cortical structures, deep learning approaches are promising to empower these medical professionals to make more informed kidney layer segmentation.
</details>
<details>
<summary>摘要</summary>
人类肾脏层结构分割，包括肾脏外带、内带和内 médulla 在人类肾脏整幅图像（WSI）中扮演着重要的作用，对于自动化图像分析在肾脏 Pathology 中起着关键性的作用。然而，现有的手动分割过程具有劳动密集和不可能处理大规模的数字 PATHOLOGY 图像的缺陷。为此，肾脏数字 PATHOLOGY 领域内部，出现了深度学习基本的方法。然而，深度学习基本的方法在肾脏层结构分割方面几乎没有被应用。为了解决这个差距，本研究评估了深度学习基本的方法在肾脏层结构分割方面的可能性。本研究采用了代表性的卷积神经网络（CNN）和转移神经网络（Transformer）分割方法，包括 Swin-Unet、医学转移神经网络（Medical-Transformer）、TransUNet、U-Net、PSPNet 和 DeepLabv3+。我们对六种常见的深度学习模型在mouse肾脏层分割中进行了数据量的评估。研究结果表明，Transformer 模型通常在肾脏外带层分割中表现更好，而 CNN 模型则表现较差。通过启用肾脏 cortical 结构的量化评估，深度学习方法在肾脏 PATHOLOGY 中表现了潜在的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-High-Performance-Learned-Image-Compression-With-Improved-Checkerboard-Context-Model-Deformable-Residual-Module-and-Knowledge-Distillation"><a href="#Fast-and-High-Performance-Learned-Image-Compression-With-Improved-Checkerboard-Context-Model-Deformable-Residual-Module-and-Knowledge-Distillation" class="headerlink" title="Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation"></a>Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02529">http://arxiv.org/abs/2309.02529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haisheng Fu, Feng Liang, Jie Liang, Yongqiang Wang, Guohe Zhang, Jingning Han</li>
<li>for: 这篇论文是为了提高深度学习基于图像压缩的速度和性能而研究的。</li>
<li>methods: 这篇论文使用了四种技术来平衡复杂性和性能的负担：1）引入了可变卷积模块，以便更好地除去输入图像中的纹理重复; 2）设计了检查ер布Context模型，可以在平行解码中保持性能，而无需遵循顺序的上下文适应模型; 3）开发了一种改进的三步知识传递和训练方案，以实现不同的复杂性和性能负担之间的平衡。</li>
<li>results: 对于比较的state-of-the-art学习图像编码方案，我们的方法可以在编码和解码过程中提高速度，并且在PSNR和MS-SSIM指标下测试时，在Kodak和Tecnick-40 datasets上超过了一些领先的学习方案。<details>
<summary>Abstract</summary>
Deep learning-based image compression has made great progresses recently. However, many leading schemes use serial context-adaptive entropy model to improve the rate-distortion (R-D) performance, which is very slow. In addition, the complexities of the encoding and decoding networks are quite high and not suitable for many practical applications. In this paper, we introduce four techniques to balance the trade-off between the complexity and performance. We are the first to introduce deformable convolutional module in compression framework, which can remove more redundancies in the input image, thereby enhancing compression performance. Second, we design a checkerboard context model with two separate distribution parameter estimation networks and different probability models, which enables parallel decoding without sacrificing the performance compared to the sequential context-adaptive model. Third, we develop an improved three-step knowledge distillation and training scheme to achieve different trade-offs between the complexity and the performance of the decoder network, which transfers both the final and intermediate results of the teacher network to the student network to help its training. Fourth, we introduce $L_{1}$ regularization to make the numerical values of the latent representation more sparse. Then we only encode non-zero channels in the encoding and decoding process, which can greatly reduce the encoding and decoding time. Experiments show that compared to the state-of-the-art learned image coding scheme, our method can be about 20 times faster in encoding and 70-90 times faster in decoding, and our R-D performance is also $2.3 \%$ higher. Our method outperforms the traditional approach in H.266/VVC-intra (4:4:4) and some leading learned schemes in terms of PSNR and MS-SSIM metrics when testing on Kodak and Tecnick-40 datasets.
</details>
<details>
<summary>摘要</summary>
深度学习基于图像压缩Recently, there have been great advances in deep learning-based image compression. However, many leading schemes use serial context-adaptive entropy models, which are very slow. In addition, the complexities of the encoding and decoding networks are quite high and not suitable for many practical applications. In this paper, we propose four techniques to balance the trade-off between complexity and performance. First, we introduce a deformable convolutional module in the compression framework, which can remove more redundancies in the input image, thereby enhancing compression performance. Second, we design a checkerboard context model with two separate distribution parameter estimation networks and different probability models, which enables parallel decoding without sacrificing performance compared to the sequential context-adaptive model. Third, we develop an improved three-step knowledge distillation and training scheme to achieve different trade-offs between complexity and performance of the decoder network. Fourth, we introduce $L_{1}$ regularization to make the numerical values of the latent representation more sparse. Then we only encode non-zero channels in the encoding and decoding process, which can greatly reduce the encoding and decoding time. Experiments show that compared to the state-of-the-art learned image coding scheme, our method can be about 20 times faster in encoding and 70-90 times faster in decoding, and our R-D performance is also $2.3 \%$ higher. Our method outperforms the traditional approach in H.266/VVC-intra (4:4:4) and some leading learned schemes in terms of PSNR and MS-SSIM metrics when testing on Kodak and Tecnick-40 datasets.
</details></li>
</ul>
<hr>
<h2 id="An-automated-high-resolution-phenotypic-assay-for-adult-Brugia-malayi-and-microfilaria"><a href="#An-automated-high-resolution-phenotypic-assay-for-adult-Brugia-malayi-and-microfilaria" class="headerlink" title="An automated, high-resolution phenotypic assay for adult Brugia malayi and microfilaria"></a>An automated, high-resolution phenotypic assay for adult Brugia malayi and microfilaria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03235">http://arxiv.org/abs/2309.03235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Upender Kalwa, Yunsoo Park, Michael J. Kimber, Santosh Pandey</li>
<li>For: The paper is written to test potential drug candidates for the treatment of Lymphatic filariasis (LF) caused by Brugia malayi, a thread-like parasitic worm.* Methods: The paper describes a multi-parameter phenotypic assay based on tracking the motility of adult B. malayi and mf in vitro, using various parameters such as centroid velocity, path curvature, angular velocity, eccentricity, extent, and Euler Number.* Results: The paper reports on the use of this assay to test the effectiveness of three anthelmintic drugs against adult B. malayi and mf, and provides a visual representation of pose estimates and behavioral attributes in both space and time scales.Here is the information in Simplified Chinese text:* 为: 本文用于测试抑菌药物对brugia malayi引起的淋巴感染的治疗方案。* 方法: 本文描述了一种多参数现象学试验，用于跟踪成人brugia malayi和mf的运动能力，包括中心速度、轨迹弯曲、角速度、弯曲率、轴心距离和欧拉数。* 结果: 本文报告了三种抑菌药物对成人brugia malayi和mf的效果，并提供了在空间和时间尺度上的Visual化pose估计和行为特征。<details>
<summary>Abstract</summary>
Brugia malayi are thread-like parasitic worms and one of the etiological agents of Lymphatic filariasis (LF). Existing anthelmintic drugs to treat LF are effective in reducing the larval microfilaria (mf) counts in human bloodstream but are less effective on adult parasites. To test potential drug candidates, we report a multi-parameter phenotypic assay based on tracking the motility of adult B. malayi and mf in vitro. For adult B. malayi, motility is characterized by the centroid velocity, path curvature, angular velocity, eccentricity, extent, and Euler Number. These parameters are evaluated in experiments with three anthelmintic drugs. For B. malayi mf, motility is extracted from the evolving body skeleton to yield positional data and bending angles at 74 key point. We achieved high-fidelity tracking of complex worm postures (self-occlusions, omega turns, body bending, and reversals) while providing a visual representation of pose estimates and behavioral attributes in both space and time scales.
</details>
<details>
<summary>摘要</summary>
布鲁迪亚马LAY是线形寄生虫，是淋巴疟疾（LF）的etiological agent之一。现有的安特藻药可以降低人体血液中的 larval microfilaria（mf）数量，但对成熟寄生虫 menos effective。为测试潜在的药物候选人，我们报道了一种多parameter fenotipic assay，基于成人布鲁迪亚马和mf的运动追踪。对于成人布鲁迪亚马，运动特征包括中心速度、轨迹弯曲、angular velocity、eccentricity、extent和Euler数。这些参数在三种安特藻药实验中被评估。对于布鲁迪亚马mf，运动是从发展的身体骨架中提取的，以获取位势数据和折弯角度。我们实现了高精度的追踪复杂的虫姿（自我 occlusions、omega turns、身体弯曲和反转），同时提供了视觉表示pose estimates和行为特征的空间和时间尺度。
</details></li>
</ul>
<hr>
<h2 id="Generating-Infinite-Resolution-Texture-using-GANs-with-Patch-by-Patch-Paradigm"><a href="#Generating-Infinite-Resolution-Texture-using-GANs-with-Patch-by-Patch-Paradigm" class="headerlink" title="Generating Infinite-Resolution Texture using GANs with Patch-by-Patch Paradigm"></a>Generating Infinite-Resolution Texture using GANs with Patch-by-Patch Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02340">http://arxiv.org/abs/2309.02340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4netzero/infinite_texture_gans">https://github.com/ai4netzero/infinite_texture_gans</a></li>
<li>paper_authors: Alhasan Abdellatif, Ahmed H. Elsheikh</li>
<li>for:  Generating texture images of infinite resolution using GANs with a patch-by-patch paradigm.</li>
<li>methods: Local padding in the generator and spatial stochastic modulation to ensure consistency and diversity of generated textures.</li>
<li>results: Superior scalability compared to existing approaches while maintaining visual coherence of generated textures.Here’s the simplified Chinese version:</li>
<li>for: 使用GANs基于 patch-by-patch 方法生成无限分辨率的текстура图像.</li>
<li>methods: 使用本地补充和空间随机 modify 保证生成图像具有一致性和多样性.</li>
<li>results: 与现有方法相比，实现了更高的可扩展性，同时保持生成图像的视觉一致性.<details>
<summary>Abstract</summary>
In this paper, we introduce a novel approach for generating texture images of infinite resolutions using Generative Adversarial Networks (GANs) based on a patch-by-patch paradigm. Existing texture synthesis techniques often rely on generating a large-scale texture using a one-forward pass to the generating model, this limits the scalability and flexibility of the generated images. In contrast, the proposed approach trains GANs models on a single texture image to generate relatively small patches that are locally correlated and can be seamlessly concatenated to form a larger image while using a constant GPU memory footprint. Our method learns the local texture structure and is able to generate arbitrary-size textures, while also maintaining coherence and diversity. The proposed method relies on local padding in the generator to ensure consistency between patches and utilizes spatial stochastic modulation to allow for local variations and diversity within the large-scale image. Experimental results demonstrate superior scalability compared to existing approaches while maintaining visual coherence of generated textures.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的方法，使用生成对抗网络（GANs）来生成无穷大分辨率的текxture图像。现有的 texture生成技术 oftentimes rely on generating a large-scale texture using a one-forward pass to the generating model, which limits the scalability and flexibility of the generated images. 在我们的方法中，我们将 GANs 模型训练在单个 texture 图像上，以生成相对较小的 patches，这些 patches 在 мест均匀的方式相互链接，可以形成更大的图像，同时使用常量的 GPU 内存占用。我们的方法学习了地方xture结构，能够生成任意大小的 texture，同时保持了视觉准确性和多样性。我们的方法使用地方padding在生成器中确保 patches 之间的一致性，并使用空间杂化调制器来允许局部变化和多样性在大规模图像中。实验结果表明，我们的方法可以与现有方法相比，在可扩展性和可靠性方面具有更高的可扩展性，同时保持生成的 texture 的视觉准确性。
</details></li>
</ul>
<hr>
<h2 id="DEEPBEAS3D-Deep-Learning-and-B-Spline-Explicit-Active-Surfaces"><a href="#DEEPBEAS3D-Deep-Learning-and-B-Spline-Explicit-Active-Surfaces" class="headerlink" title="DEEPBEAS3D: Deep Learning and B-Spline Explicit Active Surfaces"></a>DEEPBEAS3D: Deep Learning and B-Spline Explicit Active Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02335">http://arxiv.org/abs/2309.02335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helena Williams, João Pedrosa, Muhammad Asad, Laura Cattani, Tom Vercauteren, Jan Deprest, Jan D’hooge</li>
<li>for: 本研究旨在提供一种基于深度学习的自动分割方法，以便在临床应用中提高分割精度和可靠性。</li>
<li>methods: 本研究使用了一种基于Convolutional Neural Network (CNN)的3D分割框架，其中包括一种B-spline explicit active surface (BEAS)，以确保分割结果具有3D空间的缓和性和解剖可能性，同时允许用户精确地编辑3D表面。</li>
<li>results: 实验结果表明，提议的框架可以为用户提供明确的表面控制，相比4D View VOCAL（GE Healthcare，Zipf，奥地利），NASA-TLX指数下降30%，用户时间减少70%（p&lt;0.00001）。<details>
<summary>Abstract</summary>
Deep learning-based automatic segmentation methods have become state-of-the-art. However, they are often not robust enough for direct clinical application, as domain shifts between training and testing data affect their performance. Failure in automatic segmentation can cause sub-optimal results that require correction. To address these problems, we propose a novel 3D extension of an interactive segmentation framework that represents a segmentation from a convolutional neural network (CNN) as a B-spline explicit active surface (BEAS). BEAS ensures segmentations are smooth in 3D space, increasing anatomical plausibility, while allowing the user to precisely edit the 3D surface. We apply this framework to the task of 3D segmentation of the anal sphincter complex (AS) from transperineal ultrasound (TPUS) images, and compare it to the clinical tool used in the pelvic floor disorder clinic (4D View VOCAL, GE Healthcare; Zipf, Austria). Experimental results show that: 1) the proposed framework gives the user explicit control of the surface contour; 2) the perceived workload calculated via the NASA-TLX index was reduced by 30% compared to VOCAL; and 3) it required 7 0% (170 seconds) less user time than VOCAL (p< 0.00001)
</details>
<details>
<summary>摘要</summary>
深度学习自动分割方法已成为状态函数。然而，它们通常不够可靠，因为领域转换影响其性能。自动分割失败可能会导致不优化的结果需要更正。为解决这些问题，我们提出了一种三维扩展的交互式分割框架，它将分割由 convolutional neural network (CNN) 表示为 B-spline 显式活动表面 (BEAS)。BEAS 确保分割是三维空间上的平滑，提高了生物学可能性，同时允许用户精确地编辑三维表面。我们将这种框架应用于transperineal ultrasound (TPUS) 图像中的肛门缺陷复合体 (AS) 的三维分割任务，并与 pelvic floor disorder clinic (4D View VOCAL, GE Healthcare; Zipf, Austria) 使用的临床工具进行比较。实验结果表明：1. 我们的框架给用户显式地控制表面几何;2. 根据 NASA-TLX 指数，用户工作负担降低了30%，与 VOCAL 相比;3. 用户时间需要7% (170秒)  menos than VOCAL (P<0.00001)。
</details></li>
</ul>
<hr>
<h2 id="TiAVox-Time-aware-Attenuation-Voxels-for-Sparse-view-4D-DSA-Reconstruction"><a href="#TiAVox-Time-aware-Attenuation-Voxels-for-Sparse-view-4D-DSA-Reconstruction" class="headerlink" title="TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA Reconstruction"></a>TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02318">http://arxiv.org/abs/2309.02318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghong Zhou, Huangxuan Zhao, Jiemin Fang, Dongqiao Xiang, Lei Chen, Lingxia Wu, Feihong Wu, Wenyu Liu, Chuansheng Zheng, Xinggang Wang</li>
<li>for: 这篇论文主要适用于四维扫描 Angelography (4D DSA) 的重建，以提高诊断多种医疗疾病的效率，如arteriovenous malformations (AVM) 和 arteriovenous fistulas (AVF)。</li>
<li>methods: 这篇论文提出了一种名为 Time-aware Attenuation Voxel (TiAVox) 的方法，用于精炼iew 4D DSA 重建，以降低高射针 radiation 问题。TiAVox 使用了4D attenuation voxel 网格，反映了空间和时间维度的吸收性质。它通过最小化精炼图像和缺失2D DSA 图像之间的差异来优化。</li>
<li>results: 在临床和 sintetic 数据集上验证了 TiAVox 方法，获得了31.23 Peak Signal-to-Noise Ratio (PSNR) 的新视图合成效果，只使用了30个视图。传统的 Feldkamp-Davis-Kress 方法需要133个视图。此外，只使用了10个视图，TiAVox 还可以获得34.32 PSNR 的新视图合成效果和41.40 PSNR 的3D重建效果。同时，对 TiAVox 的关键组件进行了ablation 研究，以证明其关键性。<details>
<summary>Abstract</summary>
Four-dimensional Digital Subtraction Angiography (4D DSA) plays a critical role in the diagnosis of many medical diseases, such as Arteriovenous Malformations (AVM) and Arteriovenous Fistulas (AVF). Despite its significant application value, the reconstruction of 4D DSA demands numerous views to effectively model the intricate vessels and radiocontrast flow, thereby implying a significant radiation dose. To address this high radiation issue, we propose a Time-aware Attenuation Voxel (TiAVox) approach for sparse-view 4D DSA reconstruction, which paves the way for high-quality 4D imaging. Additionally, 2D and 3D DSA imaging results can be generated from the reconstructed 4D DSA images. TiAVox introduces 4D attenuation voxel grids, which reflect attenuation properties from both spatial and temporal dimensions. It is optimized by minimizing discrepancies between the rendered images and sparse 2D DSA images. Without any neural network involved, TiAVox enjoys specific physical interpretability. The parameters of each learnable voxel represent the attenuation coefficients. We validated the TiAVox approach on both clinical and simulated datasets, achieving a 31.23 Peak Signal-to-Noise Ratio (PSNR) for novel view synthesis using only 30 views on the clinically sourced dataset, whereas traditional Feldkamp-Davis-Kress methods required 133 views. Similarly, with merely 10 views from the synthetic dataset, TiAVox yielded a PSNR of 34.32 for novel view synthesis and 41.40 for 3D reconstruction. We also executed ablation studies to corroborate the essential components of TiAVox. The code will be publically available.
</details>
<details>
<summary>摘要</summary>
四维数字减除成像（4D DSA）在诊断各种医疾方面发挥关键作用，如血管肿瘤（AVM）和血管融合（AVF）。然而，4D DSA重建需要大量视图，以模拟血管复杂的凝固和流动，从而导致高射线暴露。为解决这一问题，我们提出了基于时间的抑减粒子（TiAVox）方法，以实现精度的4D DSA重建，并可以生成2D和3D DSA成像结果。TiAVox引入4D抑减粒子网格，表示空间和时间维度的抑减特性。它通过最小化与精心拍摄的2D DSA图像之间的差异来优化。不含任何神经网络，TiAVox具有特殊的物理解释性。每个学习粒子的参数表示抑减系数。我们在临床和模拟数据集上验证了TiAVox方法，在使用仅30个视图时，对新视图synthesis达到了31.23峰信号响应比（PSNR），而传统的Feldkamp-Davis-Kress方法需要133个视图。同样，只使用10个视图从 sintetic dataset，TiAVox可以达到PSNR为34.32和3D重建为41.40。我们还进行了ablation研究，以证明TiAVox的关键组件。代码将公开。
</details></li>
</ul>
<hr>
<h2 id="Advanced-Underwater-Image-Restoration-in-Complex-Illumination-Conditions"><a href="#Advanced-Underwater-Image-Restoration-in-Complex-Illumination-Conditions" class="headerlink" title="Advanced Underwater Image Restoration in Complex Illumination Conditions"></a>Advanced Underwater Image Restoration in Complex Illumination Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02217">http://arxiv.org/abs/2309.02217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Song, Mengkun She, Kevin Köser</li>
<li>for: 本研究旨在解决深水下摄影中的图像修复问题，具体来说是在200米深度以下的潜水环境中，因为自然光scarce，需要使用人工照明。传统的修复方法不适用于这种情况，因为照明源与摄像头相关，导致场景的变化。</li>
<li>methods: 本研究使用了新的约 Lambertian 表面的约束，通过对摄像头视图几何体中的对象或海底的变化，来估算照明场景。通过每个 voxel 存储一个信号因子和一个反射值，可以非常高效地修复摄像机-照明平台上的图像。</li>
<li>results: 实验结果表明，本方法可以准确地修复图像中的对象真实反射率，同时减少照明和媒体效果的影响。此外，本方法可以轻松扩展到其他场景，如在空中摄影或类似情况下。<details>
<summary>Abstract</summary>
Underwater image restoration has been a challenging problem for decades since the advent of underwater photography. Most solutions focus on shallow water scenarios, where the scene is uniformly illuminated by the sunlight. However, the vast majority of uncharted underwater terrain is located beyond 200 meters depth where natural light is scarce and artificial illumination is needed. In such cases, light sources co-moving with the camera, dynamically change the scene appearance, which make shallow water restoration methods inadequate. In particular for multi-light source systems (composed of dozens of LEDs nowadays), calibrating each light is time-consuming, error-prone and tedious, and we observe that only the integrated illumination within the viewing volume of the camera is critical, rather than the individual light sources. The key idea of this paper is therefore to exploit the appearance changes of objects or the seafloor, when traversing the viewing frustum of the camera. Through new constraints assuming Lambertian surfaces, corresponding image pixels constrain the light field in front of the camera, and for each voxel a signal factor and a backscatter value are stored in a volumetric grid that can be used for very efficient image restoration of camera-light platforms, which facilitates consistently texturing large 3D models and maps that would otherwise be dominated by lighting and medium artifacts. To validate the effectiveness of our approach, we conducted extensive experiments on simulated and real-world datasets. The results of these experiments demonstrate the robustness of our approach in restoring the true albedo of objects, while mitigating the influence of lighting and medium effects. Furthermore, we demonstrate our approach can be readily extended to other scenarios, including in-air imaging with artificial illumination or other similar cases.
</details>
<details>
<summary>摘要</summary>
水下图像修复问题已经是数十年来的挑战，自光学摄影的出现以来。大多数解决方案都专注于浅水enario，其中场景由太阳光均勋照。然而，真正的未探索的水下地形大多集中在200米深度以下，那里的自然光scarce，需要人工照明。在这种情况下，相机上方的灯光 sources co-moving with the camera, dynamically change the scene appearance, 使得浅水修复方法无法满足需求。特别是，现在LEDs组成的多光源系统中，每个灯光的准确性、繁杂性和耗时性都是问题。本文的关键想法是利用相机视图卷积体中对象或海底的变化来恢复图像。通过新的约束，每个像素对应的灯光场在前Camera的位置做出约束，并对每个 voxel 存储一个信号因子和一个反射值，可以用于非常高效地修复相机灯光平台的图像，从而实现了一个大型3D模型和地图的均衡塑造。为验证我们的方法的有效性，我们进行了大量的实验，其中包括模拟数据集和实际数据集。实验结果表明，我们的方法可以准确地恢复物体的真实反射率，同时减轻灯光和媒体效果的影响。此外，我们还证明了我们的方法可以轻松扩展到其他场景，包括在空中拍摄的人工照明情况或类似情况。
</details></li>
</ul>
<hr>
<h2 id="High-resolution-3D-Maps-of-Left-Atrial-Displacements-using-an-Unsupervised-Image-Registration-Neural-Network"><a href="#High-resolution-3D-Maps-of-Left-Atrial-Displacements-using-an-Unsupervised-Image-Registration-Neural-Network" class="headerlink" title="High-resolution 3D Maps of Left Atrial Displacements using an Unsupervised Image Registration Neural Network"></a>High-resolution 3D Maps of Left Atrial Displacements using an Unsupervised Image Registration Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02179">http://arxiv.org/abs/2309.02179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoforos Galazis, Anil Anthony Bharath, Marta Varela</li>
<li>for: 预测和诊断心血管疾病的诊断和预后。</li>
<li>methods: 使用高分辨率动力磁共振成像（Cine MRI）获得3D左心室运动和扭formation的全面图像，并提出一种自动化左心室运动特征 Extraction 工具。</li>
<li>results: 实验显示该工具可以准确跟踪左心室墙面在心脏周期中的运动， Hausdorff 距离平均值为2.51±1.3mm， Dice 分数平均值为0.96±0.02。<details>
<summary>Abstract</summary>
Functional analysis of the left atrium (LA) plays an increasingly important role in the prognosis and diagnosis of cardiovascular diseases. Echocardiography-based measurements of LA dimensions and strains are useful biomarkers, but they provide an incomplete picture of atrial deformations. High-resolution dynamic magnetic resonance images (Cine MRI) offer the opportunity to examine LA motion and deformation in 3D, at higher spatial resolution and with full LA coverage. However, there are no dedicated tools to automatically characterise LA motion in 3D. Thus, we propose a tool that automatically segments the LA and extracts the displacement fields across the cardiac cycle. The pipeline is able to accurately track the LA wall across the cardiac cycle with an average Hausdorff distance of $2.51 \pm 1.3~mm$ and Dice score of $0.96 \pm 0.02$.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对左心室（LA）的功能分析在心血管疾病诊断和 прогностицировании中日益重要。基于echo射频测量的LA大小和弹性 Parameters 可以作为生物标志物，但它们只提供了 incomplete 的atrial deformation 图像。高解度动力磁共振成像（Cine MRI）可以为LA运动和变形提供更高的空间分辨率和全面的LA覆盖率。然而，没有专门的工具来自动 caracterize LA的运动。因此，我们提议一种工具，可以自动将LA分割成多个部分，并提取cardiac cycle 中的变形场。管道可以准确地跟踪LA墙的运动，average Hausdorff distance 为2.51±1.3毫米，Dice score 为0.96±0.02。
</details></li>
</ul>
<hr>
<h2 id="INCEPTNET-Precise-And-Early-Disease-Detection-Application-For-Medical-Images-Analyses"><a href="#INCEPTNET-Precise-And-Early-Disease-Detection-Application-For-Medical-Images-Analyses" class="headerlink" title="INCEPTNET: Precise And Early Disease Detection Application For Medical Images Analyses"></a>INCEPTNET: Precise And Early Disease Detection Application For Medical Images Analyses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02147">http://arxiv.org/abs/2309.02147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AMiiR-S/Inceptnet_cancer_recognition">https://github.com/AMiiR-S/Inceptnet_cancer_recognition</a></li>
<li>paper_authors: Amirhossein Sajedi, Mohammad Javad Fadaeieslam</li>
<li>For: The paper is written for early disease detection and segmentation of medical images, with the goal of enhancing precision and performance.* Methods: The proposed method, called InceptNet, is a deep neural network that uses an Inception module to capture variations in scaled regions of interest, and improves the network’s ability to approximate an optimal local sparse structure.* Results: The proposed method outperformed previous works on four benchmark datasets, with improvements ranging from 0.0021 to 0.0544 in accuracy. Additionally, the method was found to be effective in detecting small scale structures in medical images.<details>
<summary>Abstract</summary>
In view of the recent paradigm shift in deep AI based image processing methods, medical image processing has advanced considerably. In this study, we propose a novel deep neural network (DNN), entitled InceptNet, in the scope of medical image processing, for early disease detection and segmentation of medical images in order to enhance precision and performance. We also investigate the interaction of users with the InceptNet application to present a comprehensive application including the background processes, and foreground interactions with users. Fast InceptNet is shaped by the prominent Unet architecture, and it seizes the power of an Inception module to be fast and cost effective while aiming to approximate an optimal local sparse structure. Adding Inception modules with various parallel kernel sizes can improve the network's ability to capture the variations in the scaled regions of interest. To experiment, the model is tested on four benchmark datasets, including retina blood vessel segmentation, lung nodule segmentation, skin lesion segmentation, and breast cancer cell detection. The improvement was more significant on images with small scale structures. The proposed method improved the accuracy from 0.9531, 0.8900, 0.9872, and 0.9881 to 0.9555, 0.9510, 0.9945, and 0.9945 on the mentioned datasets, respectively, which show outperforming of the proposed method over the previous works. Furthermore, by exploring the procedure from start to end, individuals who have utilized a trial edition of InceptNet, in the form of a complete application, are presented with thirteen multiple choice questions in order to assess the proposed method. The outcomes are evaluated through the means of Human Computer Interaction.
</details>
<details>
<summary>摘要</summary>
因为深度AI技术的最近 парадигShift，医疗图像处理方法得到了显著提高。在这项研究中，我们提出了一种新的深度神经网络（DNN），名为InceptNet，用于医疗图像处理中的疾病早期检测和图像分割，以提高精度和性能。我们还 investigate用户与InceptNet应用程序之间的交互，以提供一个全面的应用程序，包括背景进程和前景交互。快速InceptNet基于显著的Unet架构，并具有一个快速和成本效果的Inception模块，以便在不同的缩放级别上适应地捕捉疾病的变化。通过添加不同的平行核心大小的Inception模块，可以提高网络的能力来捕捉缩放区域的变化。为了实验，我们测试了这个模型在四个标准数据集上，包括血管segmentation、肺肿segmentation、皮肤病变segmentation和乳腺癌细胞检测。结果显示，提出的方法在图像中的小规模结构上的改进更为显著。我们的方法提高了对四个数据集的准确率，从0.9531、0.8900、0.9872和0.9881分别提高到0.9555、0.9510、0.9945和0.9945。此外，通过探索从开始到结束的过程，我们发现了一些人们在使用尝试版InceptNet时的问题，并通过多个选择题询问他们以评估提出的方法。结果被评估通过人计算机交互方式。
</details></li>
</ul>
<hr>
<h2 id="RawHDR-High-Dynamic-Range-Image-Reconstruction-from-a-Single-Raw-Image"><a href="#RawHDR-High-Dynamic-Range-Image-Reconstruction-from-a-Single-Raw-Image" class="headerlink" title="RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image"></a>RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02020">http://arxiv.org/abs/2309.02020</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackzou233/rawhdr">https://github.com/jackzou233/rawhdr</a></li>
<li>paper_authors: Yunhao Zou, Chenggang Yan, Ying Fu</li>
<li>for: 提高高动态范围图像的生成精度</li>
<li>methods: 利用 Raw 感知器数据、分别掌握较为容易和较为困难的区域、两种导航方法（对几个不具备信息的通道使用具备更多信息的通道作为导航）</li>
<li>results: 提高 Raw 图像到高动态范围图像的映射精度，并验证了该方法的优越性以及 newly 收集的 Raw&#x2F;HDR 对应集的有用性。<details>
<summary>Abstract</summary>
High dynamic range (HDR) images capture much more intensity levels than standard ones. Current methods predominantly generate HDR images from 8-bit low dynamic range (LDR) sRGB images that have been degraded by the camera processing pipeline. However, it becomes a formidable task to retrieve extremely high dynamic range scenes from such limited bit-depth data. Unlike existing methods, the core idea of this work is to incorporate more informative Raw sensor data to generate HDR images, aiming to recover scene information in hard regions (the darkest and brightest areas of an HDR scene). To this end, we propose a model tailor-made for Raw images, harnessing the unique features of Raw data to facilitate the Raw-to-HDR mapping. Specifically, we learn exposure masks to separate the hard and easy regions of a high dynamic scene. Then, we introduce two important guidances, dual intensity guidance, which guides less informative channels with more informative ones, and global spatial guidance, which extrapolates scene specifics over an extended spatial domain. To verify our Raw-to-HDR approach, we collect a large Raw/HDR paired dataset for both training and testing. Our empirical evaluations validate the superiority of the proposed Raw-to-HDR reconstruction model, as well as our newly captured dataset in the experiments.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）图像可以捕捉更多的Intensity级别than标准图像。当前方法主要从8比特低动态范围（LDR）sRGB图像中生成HDR图像，这些图像在摄像头处理管道中受到了压缩。然而，从有限比特数据中恢复极高动态范围场景是一项具有挑战性的任务。与现有方法不同，本文的核心想法是利用更有信息的Raw感知器数据来生成HDR图像，以便恢复场景信息在极高动态范围中的hard区域（极dark和极 bright区域）。为此，我们提出了特有的Raw图像模型，利用Raw数据的独特特点来促进Raw-to-HDR映射。具体来说，我们学习出处理硬区域的曝光面积，然后引入两种重要的导航：对不具有充足信息的通道来说，使用更具有信息的通道进行导航（双极Intensity导航），并在扩展的空间领域内对场景特征进行推导（全局空间导航）。为验证我们的Raw-to-HDR重建模型，我们收集了大量Raw/HDR对应的数据集，用于训练和测试。我们的实验结果证明了我们提出的Raw-to-HDR重建模型的优越性，以及我们新收集的数据集在实验中的有用性。
</details></li>
</ul>
<hr>
<h2 id="Logarithmic-Mathematical-Morphology-theory-and-applications"><a href="#Logarithmic-Mathematical-Morphology-theory-and-applications" class="headerlink" title="Logarithmic Mathematical Morphology: theory and applications"></a>Logarithmic Mathematical Morphology: theory and applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02007">http://arxiv.org/abs/2309.02007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Noyel</li>
<li>for:  Addressing the issue of lighting variations in Mathematical Morphology, a new framework named Logarithmic Mathematical Morphology (LMM) is defined.</li>
<li>methods:  The LMM framework uses an additive law that varies the amplitude of the structuring function according to the image amplitude, and models lighting variations with a physical cause.</li>
<li>results:  In images with uniform lighting variations, the LMM operators perform better than usual morphological operators. In eye-fundus images with non-uniform lighting variations, the LMM method for vessel segmentation shows better robustness to lighting variations compared to three state-of-the-art approaches.<details>
<summary>Abstract</summary>
Classically, in Mathematical Morphology, an image (i.e., a grey-level function) is analysed by another image which is named the structuring element or the structuring function. This structuring function is moved over the image domain and summed to the image. However, in an image presenting lighting variations, the analysis by a structuring function should require that its amplitude varies according to the image intensity. Such a property is not verified in Mathematical Morphology for grey level functions, when the structuring function is summed to the image with the usual additive law. In order to address this issue, a new framework is defined with an additive law for which the amplitude of the structuring function varies according to the image amplitude. This additive law is chosen within the Logarithmic Image Processing framework and models the lighting variations with a physical cause such as a change of light intensity or a change of camera exposure-time. The new framework is named Logarithmic Mathematical Morphology (LMM) and allows the definition of operators which are robust to such lighting variations. In images with uniform lighting variations, those new LMM operators perform better than usual morphological operators. In eye-fundus images with non-uniform lighting variations, a LMM method for vessel segmentation is compared to three state-of-the-art approaches. Results show that the LMM approach has a better robustness to such variations than the three others.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Empowering-Low-Light-Image-Enhancer-through-Customized-Learnable-Priors"><a href="#Empowering-Low-Light-Image-Enhancer-through-Customized-Learnable-Priors" class="headerlink" title="Empowering Low-Light Image Enhancer through Customized Learnable Priors"></a>Empowering Low-Light Image Enhancer through Customized Learnable Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01958">http://arxiv.org/abs/2309.01958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zheng980629/cue">https://github.com/zheng980629/cue</a></li>
<li>paper_authors: Naishan Zheng, Man Zhou, Yanmeng Dong, Xiangyu Rui, Jie Huang, Chongyi Li, Feng Zhao</li>
<li>for: 提高低光照图像的品质，增强图像的亮度和降低噪音。</li>
<li>methods: 使用自定义可学习的假设来改善深度 unfolding 架构的透明度，包括通过结构流和优化流两种方法来自定义 Masked Autoencoder（MAE）的特征表示能力。</li>
<li>results: 在多个低光照图像提升数据集上，提出的方法比现有方法更高效，并且可以更好地解释和 интерпретирова模型的输出。<details>
<summary>Abstract</summary>
Deep neural networks have achieved remarkable progress in enhancing low-light images by improving their brightness and eliminating noise. However, most existing methods construct end-to-end mapping networks heuristically, neglecting the intrinsic prior of image enhancement task and lacking transparency and interpretability. Although some unfolding solutions have been proposed to relieve these issues, they rely on proximal operator networks that deliver ambiguous and implicit priors. In this work, we propose a paradigm for low-light image enhancement that explores the potential of customized learnable priors to improve the transparency of the deep unfolding paradigm. Motivated by the powerful feature representation capability of Masked Autoencoder (MAE), we customize MAE-based illumination and noise priors and redevelop them from two perspectives: 1) \textbf{structure flow}: we train the MAE from a normal-light image to its illumination properties and then embed it into the proximal operator design of the unfolding architecture; and m2) \textbf{optimization flow}: we train MAE from a normal-light image to its gradient representation and then employ it as a regularization term to constrain noise in the model output. These designs improve the interpretability and representation capability of the model.Extensive experiments on multiple low-light image enhancement datasets demonstrate the superiority of our proposed paradigm over state-of-the-art methods. Code is available at https://github.com/zheng980629/CUE.
</details>
<details>
<summary>摘要</summary>
深度神经网络已经取得了优化低光照图像的显著进步，提高图像的亮度和消除噪声。然而，大多数现有方法是通过静默地构建终端到终端的映射网络，忽略了图像提升任务的内在先验知识和透明度和可解释性。虽然一些解开解决方案已经被提出，但它们基于距离运算网络，elivery ambiguous和隐藏先验知识。在这个工作中，我们提出了一种低光照图像提升的方法，探索了可定制学习先验知识的潜力，以改善深 unfolding 架构的透明度。我们的方法受到Masked Autoencoder (MAE) 的强大特征表示能力的激发，我们自定义 MAE 基于照明和噪声的特征，并将其重新设计为两种视角：1. 结构流：我们从正常光照图像开始，通过 MAE 进行训练，然后将其 embedding 到 proximal 操作符的设计中。2. 优化流：我们从正常光照图像开始，通过 MAE 进行训练，然后将其作为Regularization term来限制模型输出中的噪声。这些设计提高了模型的透明度和表示能力。我们对多个低光照图像提升数据进行了广泛的实验，结果表明我们的提出的方法在当前的方法之上表现出优异性。代码可以在 <https://github.com/zheng980629/CUE> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Duration-adaptive-Video-Highlight-Pre-caching-for-Vehicular-Communication-Network"><a href="#Duration-adaptive-Video-Highlight-Pre-caching-for-Vehicular-Communication-Network" class="headerlink" title="Duration-adaptive Video Highlight Pre-caching for Vehicular Communication Network"></a>Duration-adaptive Video Highlight Pre-caching for Vehicular Communication Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01944">http://arxiv.org/abs/2309.01944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Xu, Deshi Li, Kaitao Meng, Mingliu Liu, Shuya Zhu</li>
<li>for: 提高video播放质量和服务性能在 Vehicular Communication Networks (VCNs) 中</li>
<li>methods: 基于高光 entropy 模型、幂 transform 和 Iterative 高光方向 Trimming 算法</li>
<li>results: 比对assauming 方案具有显著提高高光 entropy 和 jitter 性能<details>
<summary>Abstract</summary>
Video traffic in vehicular communication networks (VCNs) faces exponential growth. However, different segments of most videos reveal various attractiveness for viewers, and the pre-caching decision is greatly affected by the dynamic service duration that edge nodes can provide services for mobile vehicles driving along a road. In this paper, we propose an efficient video highlight pre-caching scheme in the vehicular communication network, adapting to the service duration. Specifically, a highlight entropy model is devised with the consideration of the segments' popularity and continuity between segments within a period of time, based on which, an optimization problem of video highlight pre-caching is formulated. As this problem is non-convex and lacks a closed-form expression of the objective function, we decouple multiple variables by deriving candidate highlight segmentations of videos through wavelet transform, which can significantly reduce the complexity of highlight pre-caching. Then the problem is solved iteratively by a highlight-direction trimming algorithm, which is proven to be locally optimal. Simulation results based on real-world video datasets demonstrate significant improvement in highlight entropy and jitter compared to benchmark schemes.
</details>
<details>
<summary>摘要</summary>
视频流量在交通通信网络（VCN）中呈指数增长趋势。然而，不同的视频段落吸引了不同的观众，并且边节点可以在路面上驱动移动的车辆提供动态服务时间，这大大影响了预缓存决策。在这篇论文中，我们提出了一种高效的视频精彩预缓存方案，适应服务时间。具体来说，我们提出了一种高光积分模型，考虑视频段落的吸引力和时间内 segment之间的连续性，并基于此模型，我们形ulated一个预缓存问题。由于这个问题是非对称的和无法表达目标函数的closed-form，我们使用波лет变换 derivation candidate高光段落，这可以很大减少了预缓存的复杂性。然后，我们通过一种高光方向裁剪算法来解决这个问题，该算法是当地最优的。实验结果基于实际的视频数据集表明，我们的方案可以很大提高高光积分和频率抖动相比 benchmark 方案。
</details></li>
</ul>
<hr>
<h2 id="BigFUSE-Global-Context-Aware-Image-Fusion-in-Dual-View-Light-Sheet-Fluorescence-Microscopy-with-Image-Formation-Prior"><a href="#BigFUSE-Global-Context-Aware-Image-Fusion-in-Dual-View-Light-Sheet-Fluorescence-Microscopy-with-Image-Formation-Prior" class="headerlink" title="BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior"></a>BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01865">http://arxiv.org/abs/2309.01865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Liu, Gesine Muller, Nassir Navab, Carsten Marr, Jan Huisken, Tingying Peng</li>
<li>for: 提高LSFM中样品的高分辨率成像，抵消光散射引起的影像质量下降。</li>
<li>methods: 使用双视图成像技术，通过对样品从不同方向进行扫描，以实现高分辨率成像。并采用全局上下文意识的图像融合方法，基于本地图像质量来确定focus-defocus边界，并考虑光散射对ocus measures的影响。</li>
<li>results: 实验结果表明，BigFUSE可以自动排除结构化 artifacts，高效地融合 dual-view LSFM 中的信息，并且能够稳定地确定ocus-defocus边界，提高LSFM中样品的高分辨率成像质量。<details>
<summary>Abstract</summary>
Light-sheet fluorescence microscopy (LSFM), a planar illumination technique that enables high-resolution imaging of samples, experiences defocused image quality caused by light scattering when photons propagate through thick tissues. To circumvent this issue, dualview imaging is helpful. It allows various sections of the specimen to be scanned ideally by viewing the sample from opposing orientations. Recent image fusion approaches can then be applied to determine in-focus pixels by comparing image qualities of two views locally and thus yield spatially inconsistent focus measures due to their limited field-of-view. Here, we propose BigFUSE, a global context-aware image fuser that stabilizes image fusion in LSFM by considering the global impact of photon propagation in the specimen while determining focus-defocus based on local image qualities. Inspired by the image formation prior in dual-view LSFM, image fusion is considered as estimating a focus-defocus boundary using Bayes Theorem, where (i) the effect of light scattering onto focus measures is included within Likelihood; and (ii) the spatial consistency regarding focus-defocus is imposed in Prior. The expectation-maximum algorithm is then adopted to estimate the focus-defocus boundary. Competitive experimental results show that BigFUSE is the first dual-view LSFM fuser that is able to exclude structured artifacts when fusing information, highlighting its abilities of automatic image fusion.
</details>
<details>
<summary>摘要</summary>
光束照明微scopy (LSFM)，一种平面照明技术，可以实现高分辨率的样品图像。然而，当光子在厚重的样品中传播时，会产生光散射，导致图像失真。为解决这问题，双视图减少是有帮助的。它可以在不同的视角下扫描样品，从而获得不同部分的样品图像。然而，当应用最新的图像融合方法时，由于它们的视野有限，会导致空间不一致的Focus推估。为此，我们提出了BigFUSE，一种全球上下文意识的图像融合器。它可以在LSFM中稳定图像融合，并且考虑样品中光子的全球影响，以确定基于本地图像质量的Focus推估。我们受 dual-view LSFM 图像形成优先顺序的启发，图像融合被视为使用 bayes 定理来定义Focus推估边界，其中（i）光散射对Focus推估的影响被包含在likelihood中，（ii）在Prior中强制实施Focus推估的空间一致性。然后，采用了expectation-maximum算法来估算Focus推估边界。竞争性实验结果表明，BigFUSE 是首个可以排除结构残余的 dual-view LSFM 融合器，highlighting its ability of自动图像融合。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/eess.IV_2023_09_05/" data-id="clmjn91qv00hl0j886gm03gvj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/4/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
