
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/33/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/cs.LG_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T10:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/cs.LG_2023_09_18/">cs.LG - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Causal-Theories-and-Structural-Data-Representations-for-Improving-Out-of-Distribution-Classification"><a href="#Causal-Theories-and-Structural-Data-Representations-for-Improving-Out-of-Distribution-Classification" class="headerlink" title="Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification"></a>Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10211">http://arxiv.org/abs/2309.10211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donald Martin, Jr., David Kinney</li>
<li>for: 用于提高机器学习系统的稳定性和安全性，通过使用人类生成的 causal 知识来减少机器学习开发人员的论证不确定性。</li>
<li>methods: 使用人类中心的 causal 理论和动力学 литературы中的工具，将数据表示为epidemic系统的数据生成过程中的不变结构 causal 特征。</li>
<li>results: 通过使用这种数据表示方法，在训练神经网络时，可以提高数据外的泛化性表现，比如Naive Approach的数据表示方法。<details>
<summary>Abstract</summary>
We consider how human-centered causal theories and tools from the dynamical systems literature can be deployed to guide the representation of data when training neural networks for complex classification tasks. Specifically, we use simulated data to show that training a neural network with a data representation that makes explicit the invariant structural causal features of the data generating process of an epidemic system improves out-of-distribution (OOD) generalization performance on a classification task as compared to a more naive approach to data representation. We take these results to demonstrate that using human-generated causal knowledge to reduce the epistemic uncertainty of ML developers can lead to more well-specified ML pipelines. This, in turn, points to the utility of a dynamical systems approach to the broader effort aimed at improving the robustness and safety of machine learning systems via improved ML system development practices.
</details>
<details>
<summary>摘要</summary>
我们考虑了人类中心 causal 理论和动力系统文献中的工具如何用于导引训练 нейрон网络时的数据表现。 Specifically, 我们使用模拟数据显示了训练一个 нейрон网络使用明确表达数据生成过程中的抗变化结构特征，可以提高数据外的测验性能（OOD）的数据表现。 We take these results to demonstrate that使用人类生成的 causal 知识可以减少机器学习开发人员的 epistemic 不确定性，可以导致更好地规划的 ML 管线。 This, in turn, points to the utility of a dynamical systems approach to the broader effort aimed at improving the robustness and safety of machine learning systems via improved ML system development practices.
</details></li>
</ul>
<hr>
<h2 id="The-Kernel-Density-Integral-Transformation"><a href="#The-Kernel-Density-Integral-Transformation" class="headerlink" title="The Kernel Density Integral Transformation"></a>The Kernel Density Integral Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10194">http://arxiv.org/abs/2309.10194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/calvinmccarter/kditransform">https://github.com/calvinmccarter/kditransform</a></li>
<li>paper_authors: Calvin McCarter</li>
<li>for: 本研究旨在提出基于机器学习和统计方法处理表格数据时的特征预处理策略。</li>
<li>methods: 本文提议使用核密度积分变换作为特征预处理步骤，该方法包含了Linear Min-Max Scaling和量iles transformation两种领先的特征预处理方法，而无需hyperparameter调整。</li>
<li>results: 本研究示出，无需调整hyperparameter，核密度积分变换可以作为Linear Min-Max Scaling和量iles transformation的简单替换方法，并且可以具有这两种方法的稳定性和性能。此外，通过调整单个连续hyperparameter，我们经常可以超越这两种方法的性能。最后，本文还示出了核密度变换在统计数据分析中的益处，特别是在相关分析和单variate clustering中。<details>
<summary>Abstract</summary>
Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
</details>
<details>
<summary>摘要</summary>
<LC>zh</LC></SYS>功能预处理仍然在机器学习和统计方法应用到表格数据时扮演关键角色。在这篇论文中，我们提议使用核密度积分变换作为特征预处理步骤。我们的方法包含两种领先的特征预处理方法为限制 случа：线性最小最大缩放和量程变换。我们示示，无需hyperparameter调整，核密度积分变换可以作为简单的替换方法，具有对每个方法的强度。 Alternatively，通过调整单一连续的超参数，我们经常超越这两个方法。最后，我们示示了核密度变换可以营利地应用于统计数据分析，尤其是在相关分析和单variate归一化中。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Deep-Koopman-Model-for-Quality-Propagation-Analysis-in-Multistage-Manufacturing-Systems"><a href="#Stochastic-Deep-Koopman-Model-for-Quality-Propagation-Analysis-in-Multistage-Manufacturing-Systems" class="headerlink" title="Stochastic Deep Koopman Model for Quality Propagation Analysis in Multistage Manufacturing Systems"></a>Stochastic Deep Koopman Model for Quality Propagation Analysis in Multistage Manufacturing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10193">http://arxiv.org/abs/2309.10193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyi Chen, Harshal Maske, Huanyi Shui, Devesh Upadhyay, Michael Hopka, Joseph Cohen, Xingjian Lai, Xun Huan, Jun Ni</li>
<li>for: 这篇研究的目的是为了模型多阶制造系统（MMS）的复杂行为，并使用深度学习方法来实现这个目的。</li>
<li>methods: 这篇研究使用了Stochastic Deep Koopman（SDK）框架，将 kritical quality information 通过Variational Autoencoders（VAEs）提取出来，并使用Koopman operator来传递这些资讯。</li>
<li>results: 根据比较研究，SDK 模型在预测 MMS 中每个阶段的产品质量方面的准确性比其他常用的数据驱动模型高。此外，SDK 的特殊的扩散性和可追溯性使得可以实现制程中品质的追溯和根本原因分析。<details>
<summary>Abstract</summary>
The modeling of multistage manufacturing systems (MMSs) has attracted increased attention from both academia and industry. Recent advancements in deep learning methods provide an opportunity to accomplish this task with reduced cost and expertise. This study introduces a stochastic deep Koopman (SDK) framework to model the complex behavior of MMSs. Specifically, we present a novel application of Koopman operators to propagate critical quality information extracted by variational autoencoders. Through this framework, we can effectively capture the general nonlinear evolution of product quality using a transferred linear representation, thus enhancing the interpretability of the data-driven model. To evaluate the performance of the SDK framework, we carried out a comparative study on an open-source dataset. The main findings of this paper are as follows. Our results indicate that SDK surpasses other popular data-driven models in accuracy when predicting stagewise product quality within the MMS. Furthermore, the unique linear propagation property in the stochastic latent space of SDK enables traceability for quality evolution throughout the process, thereby facilitating the design of root cause analysis schemes. Notably, the proposed framework requires minimal knowledge of the underlying physics of production lines. It serves as a virtual metrology tool that can be applied to various MMSs, contributing to the ultimate goal of Zero Defect Manufacturing.
</details>
<details>
<summary>摘要</summary>
多stage制造系统（MMS）的模型化吸引了学术和实践领域的越来越多的关注。现代深度学习方法的提出，为了实现这项任务，成本和专业知识的减少提供了机会。本研究提出了一种随机深度库曼（SDK）框架，用于模型MMS的复杂行为。特别是，我们提出了一种使用Variational Autoencoders提取的重要质量信息的 Koopman 算子应用。通过这种框架，我们可以有效地捕捉产品质量的总非线性演化，使用传输的线性表示，从而提高数据驱动模型的解释性。为评估SDK框架的性能，我们进行了一项比较研究，用于一个开源数据集。研究结果显示，SDK在MMS中预测Stagewise产品质量方面的准确率高于其他流行的数据驱动模型。此外，SDK在随机潜在空间中的特有线性传播性能，可以跟踪产品质量的演化，从而实现质量演化的跟踪和根本分析方案的设计。值得一提的是，提出的方案不需要对制造线的物理基础知识。它可以作为虚拟测量工具，应用于不同的MMS，为无瑕制造做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Autoencoder-based-Anomaly-Detection-System-for-Online-Data-Quality-Monitoring-of-the-CMS-Electromagnetic-Calorimeter"><a href="#Autoencoder-based-Anomaly-Detection-System-for-Online-Data-Quality-Monitoring-of-the-CMS-Electromagnetic-Calorimeter" class="headerlink" title="Autoencoder-based Anomaly Detection System for Online Data Quality Monitoring of the CMS Electromagnetic Calorimeter"></a>Autoencoder-based Anomaly Detection System for Online Data Quality Monitoring of the CMS Electromagnetic Calorimeter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10157">http://arxiv.org/abs/2309.10157</a></li>
<li>repo_url: None</li>
<li>paper_authors: The CMS ECAL Collaboration</li>
<li>for: 该论文是关于CMS实验室中高能物理数据质量监测的研究。</li>
<li>methods: 该研究使用了一种基于自适应神经网络的异常检测系统，通过使用时间依赖性和空间变化来提高异常检测性能。</li>
<li>results: 该系统能够效率地检测异常，并保持非常低的假阳性率。研究 Validates 该系统的性能通过2018和2022 LHC冲撞数据中的异常检测结果，并在CMS在线数据质量监测工作流中首次部署该系统的结果。<details>
<summary>Abstract</summary>
The CMS detector is a general-purpose apparatus that detects high-energy collisions produced at the LHC. Online Data Quality Monitoring of the CMS electromagnetic calorimeter is a vital operational tool that allows detector experts to quickly identify, localize, and diagnose a broad range of detector issues that could affect the quality of physics data. A real-time autoencoder-based anomaly detection system using semi-supervised machine learning is presented enabling the detection of anomalies in the CMS electromagnetic calorimeter data. A novel method is introduced which maximizes the anomaly detection performance by exploiting the time-dependent evolution of anomalies as well as spatial variations in the detector response. The autoencoder-based system is able to efficiently detect anomalies, while maintaining a very low false discovery rate. The performance of the system is validated with anomalies found in 2018 and 2022 LHC collision data. Additionally, the first results from deploying the autoencoder-based system in the CMS online Data Quality Monitoring workflow during the beginning of Run 3 of the LHC are presented, showing its ability to detect issues missed by the existing system.
</details>
<details>
<summary>摘要</summary>
“CMS探测器是一个通用的实验设备，用于探测在LHC中产生的高能撞击。CMS电磁calorimeter在线时质量监控是实验专家快速识别、定位和诊断各种实验器问题的重要操作工具。这个实时自适应器基于机器学习系统可以实时检测CMS电磁calorimeter数据中的问题。我们引入了一种新的方法，利用时间递增的问题演进以及探测器响应的空间变化，以最大化问题检测性。这个自适应器基于系统能够快速检测问题，同时保持很低的伪阳性率。我们 validate了这个系统的性能，使用2018和2022年LHC撞击数据中的问题。此外，我们还将这个自适应器基于系统在CMS线上质量监控工作流程中的首次应用结果给出，证明它能够检测已有系统所忽略的问题。”
</details></li>
</ul>
<hr>
<h2 id="Realistic-Website-Fingerprinting-By-Augmenting-Network-Trace"><a href="#Realistic-Website-Fingerprinting-By-Augmenting-Network-Trace" class="headerlink" title="Realistic Website Fingerprinting By Augmenting Network Trace"></a>Realistic Website Fingerprinting By Augmenting Network Trace</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10147">http://arxiv.org/abs/2309.10147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spin-umass/realistic-website-fingerprinting-by-augmenting-network-traces">https://github.com/spin-umass/realistic-website-fingerprinting-by-augmenting-network-traces</a></li>
<li>paper_authors: Alireza Bahramali, Ardavan Bozorgi, Amir Houmansadr</li>
<li>for: 本研究旨在提高网络识别攻击的实际性，并且挑战了现有的WF攻击方法的假设。</li>
<li>methods: 本研究使用了网络追加技术（NetAugment），这种技术可以帮助WF攻击者在未知的网络条件下进行识别。具体来说，我们使用了半监督和自监督学习技术来实现NetAugment。</li>
<li>results: 我们的实验结果表明，使用了网络追加技术进行WF攻击可以提高攻击的准确率。例如，在一个关闭世界的场景下，我们的自监督WF攻击（NetCLR）在评估 traces 是未知的情况下达到了80%的准确率，而现有的Triplet Fingerprinting方法只达到了64.4%的准确率。<details>
<summary>Abstract</summary>
Website Fingerprinting (WF) is considered a major threat to the anonymity of Tor users (and other anonymity systems). While state-of-the-art WF techniques have claimed high attack accuracies, e.g., by leveraging Deep Neural Networks (DNN), several recent works have questioned the practicality of such WF attacks in the real world due to the assumptions made in the design and evaluation of these attacks. In this work, we argue that such impracticality issues are mainly due to the attacker's inability in collecting training data in comprehensive network conditions, e.g., a WF classifier may be trained only on samples collected on specific high-bandwidth network links but deployed on connections with different network conditions. We show that augmenting network traces can enhance the performance of WF classifiers in unobserved network conditions. Specifically, we introduce NetAugment, an augmentation technique tailored to the specifications of Tor traces. We instantiate NetAugment through semi-supervised and self-supervised learning techniques. Our extensive open-world and close-world experiments demonstrate that under practical evaluation settings, our WF attacks provide superior performances compared to the state-of-the-art; this is due to their use of augmented network traces for training, which allows them to learn the features of target traffic in unobserved settings. For instance, with a 5-shot learning in a closed-world scenario, our self-supervised WF attack (named NetCLR) reaches up to 80% accuracy when the traces for evaluation are collected in a setting unobserved by the WF adversary. This is compared to an accuracy of 64.4% achieved by the state-of-the-art Triplet Fingerprinting [35]. We believe that the promising results of our work can encourage the use of network trace augmentation in other types of network traffic analysis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Geometric-Framework-for-Neural-Feature-Learning"><a href="#A-Geometric-Framework-for-Neural-Feature-Learning" class="headerlink" title="A Geometric Framework for Neural Feature Learning"></a>A Geometric Framework for Neural Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10140">http://arxiv.org/abs/2309.10140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiangxiangxu/nfe">https://github.com/xiangxiangxu/nfe</a></li>
<li>paper_authors: Xiangxiang Xu, Lizhong Zheng</li>
<li>for: 这种Framework用于学习系统设计，基于神经元特征提取器，利用特征空间的几何结构。</li>
<li>methods: 该 Framework使用特征几何来解决学习问题，包括最佳特征应对、数据样本学习和多变量学习等。</li>
<li>results: 该 Framework可以应用于现有网络架构和优化器，并可以解释经典方法的连接。<details>
<summary>Abstract</summary>
We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的系统设计框架，基于神经特征提取器来利用特征空间的几何结构。首先，我们介绍了特征几何，这将统一统计依赖和特征在同一功能空间中的几何结构。通过应用特征几何，我们将每个学习问题解释为在依赖组件指定的学习设置中解决最佳特征近似问题。我们提议一种嵌套技术，用于从数据样本中学习最佳特征，可以应用于现有的网络架构和优化器。为了证明嵌套技术的应用，我们进一步讨论了多变量学习问题，包括受条件推理和多模态学习，并介绍了最佳特征和其与经典方法的连接。
</details></li>
</ul>
<hr>
<h2 id="Deep-smoothness-WENO-scheme-for-two-dimensional-hyperbolic-conservation-laws-A-deep-learning-approach-for-learning-smoothness-indicators"><a href="#Deep-smoothness-WENO-scheme-for-two-dimensional-hyperbolic-conservation-laws-A-deep-learning-approach-for-learning-smoothness-indicators" class="headerlink" title="Deep smoothness WENO scheme for two-dimensional hyperbolic conservation laws: A deep learning approach for learning smoothness indicators"></a>Deep smoothness WENO scheme for two-dimensional hyperbolic conservation laws: A deep learning approach for learning smoothness indicators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10117">http://arxiv.org/abs/2309.10117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatiana Kossaczká, Ameya D. Jagtap, Matthias Ehrhardt</li>
<li>for: 提高两个维度的欧拉方程的精度，特别是在尖型冲击和薄层扩散区域</li>
<li>methods: 通过对Weighted Essentially Non-Oscillatory（WENO）方法中的稳定指标进行深度学习修改，以提高数值解的准确性</li>
<li>results: 在多个文献中的测试问题上，新方法比传统的第五级WENO方法更高精度，特别是在尖型冲击和薄层扩散区域存在过度散射或超过冲击的情况下。<details>
<summary>Abstract</summary>
In this paper, we introduce an improved version of the fifth-order weighted essentially non-oscillatory (WENO) shock-capturing scheme by incorporating deep learning techniques. The established WENO algorithm is improved by training a compact neural network to adjust the smoothness indicators within the WENO scheme. This modification enhances the accuracy of the numerical results, particularly near abrupt shocks. Unlike previous deep learning-based methods, no additional post-processing steps are necessary for maintaining consistency. We demonstrate the superiority of our new approach using several examples from the literature for the two-dimensional Euler equations of gas dynamics. Through intensive study of these test problems, which involve various shocks and rarefaction waves, the new technique is shown to outperform traditional fifth-order WENO schemes, especially in cases where the numerical solutions exhibit excessive diffusion or overshoot around shocks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种基于深度学习技术改进的第五阶重量核非抽象冲击捕捉算法（WENO）。我们在WENO算法中添加了一个紧凑型神经网络，以调整WENO算法中的平滑指标。这种修改可以提高计算结果的准确性，特别是在强冲击的情况下。与过去的深度学习基于方法不同，我们的新方法不需要额外的后处理步骤，以保持一致性。我们通过对文献中的几个测试问题进行广泛的研究，包括各种冲击和薄层振荡，证明了我们的新方法在冲击强度较大的情况下表现更好，特别是在计算结果中出现过度散射或过度强化的情况下。
</details></li>
</ul>
<hr>
<h2 id="A-Semi-Supervised-Approach-for-Power-System-Event-Identification"><a href="#A-Semi-Supervised-Approach-for-Power-System-Event-Identification" class="headerlink" title="A Semi-Supervised Approach for Power System Event Identification"></a>A Semi-Supervised Approach for Power System Event Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10095">http://arxiv.org/abs/2309.10095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nima Taghipourbazargani, Lalitha Sankar, Oliver Kosut</li>
<li>for: 提高电力系统可靠性、安全性和稳定性，使用数据科学技术进行数据驱动事件识别。</li>
<li>methods: 使用 semi-supervised 学习技术，利用标注和无标注样本进行事件识别。</li>
<li>results: 对四类事件的识别性能显著提高，与只使用少量标注样本相比， graph-based LS 方法表现最佳。<details>
<summary>Abstract</summary>
Event identification is increasingly recognized as crucial for enhancing the reliability, security, and stability of the electric power system. With the growing deployment of Phasor Measurement Units (PMUs) and advancements in data science, there are promising opportunities to explore data-driven event identification via machine learning classification techniques. However, obtaining accurately-labeled eventful PMU data samples remains challenging due to its labor-intensive nature and uncertainty about the event type (class) in real-time. Thus, it is natural to use semi-supervised learning techniques, which make use of both labeled and unlabeled samples. %We propose a novel semi-supervised framework to assess the effectiveness of incorporating unlabeled eventful samples to enhance existing event identification methodologies. We evaluate three categories of classical semi-supervised approaches: (i) self-training, (ii) transductive support vector machines (TSVM), and (iii) graph-based label spreading (LS) method. Our approach characterizes events using physically interpretable features extracted from modal analysis of synthetic eventful PMU data. In particular, we focus on the identification of four event classes whose identification is crucial for grid operations. We have developed and publicly shared a comprehensive Event Identification package which consists of three aspects: data generation, feature extraction, and event identification with limited labels using semi-supervised methodologies. Using this package, we generate and evaluate eventful PMU data for the South Carolina synthetic network. Our evaluation consistently demonstrates that graph-based LS outperforms the other two semi-supervised methods that we consider, and can noticeably improve event identification performance relative to the setting with only a small number of labeled samples.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>电力系统中的事件识别日益被认为是提高系统可靠性、安全性和稳定性的关键。随着phasor Measurement Units（PMUs）的广泛部署和数据科学技术的进步，有希望通过机器学习分类技术来探索数据驱动的事件识别。然而，在实时获得正确标注的事件ful PMU数据样本上存在劳动 INTENSIVE和未知事件类型的问题。因此，使用半supervised学习技术，这些技术使用标注和未标注样本。 %We propose a novel semi-supervised framework to assess the effectiveness of incorporating unlabeled eventful samples to enhance existing event identification methodologies. We evaluate three categories of classical semi-supervised approaches: (i) self-training, (ii) transductive support vector machines (TSVM), and (iii) graph-based label spreading (LS) method. Our approach characterizes events using physically interpretable features extracted from modal analysis of synthetic eventful PMU data. In particular, we focus on the identification of four event classes whose identification is crucial for grid operations. We have developed and publicly shared a comprehensive Event Identification package which consists of three aspects: data generation, feature extraction, and event identification with limited labels using semi-supervised methodologies. Using this package, we generate and evaluate eventful PMU data for the South Carolina synthetic network. Our evaluation consistently demonstrates that graph-based LS outperforms the other two semi-supervised methods that we consider, and can noticeably improve event identification performance relative to the setting with only a small number of labeled samples.
</details></li>
</ul>
<hr>
<h2 id="Invariant-Probabilistic-Prediction"><a href="#Invariant-Probabilistic-Prediction" class="headerlink" title="Invariant Probabilistic Prediction"></a>Invariant Probabilistic Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10083">http://arxiv.org/abs/2309.10083</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexanderhenzi/ipp">https://github.com/alexanderhenzi/ipp</a></li>
<li>paper_authors: Alexander Henzi, Xinwei Shen, Michael Law, Peter Bühlmann</li>
<li>for: 本研究旨在探讨在数据分布变化下，使用统计方法实现robust性和不变性。</li>
<li>methods: 该文使用了一种 causality-inspired 框架，研究了probabilistic predictions 的不变性和robust性，并提出了一种可以在不同数据分布下实现不变性的方法。</li>
<li>results: 研究发现，在一般情况下，arbitrary distribution shifts 不会导致 invariant和robust probabilistic predictions，与点预测相比，probabilistic predictions 在不同数据分布下的性能更强。文章还提出了一种方法来实现不变性，并进行了对实验数据的验证。<details>
<summary>Abstract</summary>
In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant probabilistic predictions, called IPP, and study the consistency of the underlying parameters. Finally, we demonstrate the empirical performance of our proposed procedure on simulated as well as on single-cell data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近年来，有增长的兴趣在统计方法中具有对分布变化的鲁棒性。大多数相关的研究集中在点预测中，使用平方误差损失。然而，本文将注意力转移到 probabilistic 预测，它们意味着对输出变量的不确定性进行全面评估。在 causality 框架下，我们调查 probabilistic 预测的一致性和鲁棒性，使用合适的 scoring rule。我们发现，在一般情况下，不同分布变换不会导致鲁棒和一致的 probabilistic 预测，与点预测的情况不同。我们介绍如何选择评估指标和限制分布变换的类型，以便在 Gaussian 不同梯度线性模型中实现可识别性和一致性。为了实现这一目标，我们提出了一种名为 IPP 的方法，并研究其下面的参数一致性。最后，我们通过 simulations 和单元细胞数据进行了实验性评估。
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Perspective-on-Non-Stationary-Kernels-for-Deeper-Gaussian-Processes"><a href="#A-Unifying-Perspective-on-Non-Stationary-Kernels-for-Deeper-Gaussian-Processes" class="headerlink" title="A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes"></a>A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10068">http://arxiv.org/abs/2309.10068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus M. Noack, Hengrui Luo, Mark D. Risser</li>
<li>for: 本文旨在帮助机器学习实践者更好地理解非站立性的概率过程（Gaussian Process）中的非站立性形式，并提出一种新的kernel函数，以提高非站立性的预测性和不确定性评估。</li>
<li>methods: 本文使用了多种常见的非站立性kernels，并且对它们的性质进行了仔细的研究和比较，以挖掘它们的优点和缺点。</li>
<li>results: 本文通过使用不同的数据集和kernels进行了丰富的实践和比较，并发现了一些非站立性kernels的优点和缺点。基于这些发现，本文提出了一种新的kernel函数，以提高非站立性预测的准确性和不确定性评估。<details>
<summary>Abstract</summary>
The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat\'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more complicated functional form and the associated effort and expertise needed to define and tune them optimally. In this perspective, we want to help ML practitioners make sense of some of the most common forms of non-stationarity for Gaussian processes. We show a variety of kernels in action using representative datasets, carefully study their properties, and compare their performances. Based on our findings, we propose a new kernel that combines some of the identified advantages of existing kernels.
</details>
<details>
<summary>摘要</summary>
Gaussian process (GP) 是一种广泛使用的统计技术，用于数据不确定性评估和函数近似。在过去二十年中，GP 被机器学习领域采纳，因为它在数据稀缺的情况下表现出色，并且自然地提供了稳健的不确定性估计。然而，GP 的性能往往取决于核函数的细腻定制，这经常导致实践者在使用标准设置和商业化软件工具时感到不满。核函数是 GP 中最重要的构建块，它扮演了 covariance 算子的角色。在大多数应用研究中，使用 Stationary 核函数，但这些核函数的预测性能不佳，而且不符合实际情况。非站ARY 核函数可以提高性能，但它们的函数形式更复杂，需要更多的定制和优化。在这篇视点中，我们想帮助机器学习实践者理解 GP 中一些最常见的非站ARY 性。我们使用代表性的数据集，详细研究核函数的性质，并比较它们的性能。根据我们的发现，我们提出了一种新的核函数，它结合了一些已知核函数的优点。
</details></li>
</ul>
<hr>
<h2 id="Dual-Student-Networks-for-Data-Free-Model-Stealing"><a href="#Dual-Student-Networks-for-Data-Free-Model-Stealing" class="headerlink" title="Dual Student Networks for Data-Free Model Stealing"></a>Dual Student Networks for Data-Free Model Stealing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10058">http://arxiv.org/abs/2309.10058</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Beetham, Navid Kardan, Ajmal Mian, Mubarak Shah</li>
<li>for: 提高数据预processing中的模型骚乱攻击 robustness</li>
<li>methods: 提出了一种基于两个学生模型的 dual student 方法，通过培养两个学生模型来提供生成器模型生成样本的依据，并通过对两个学生模型的分歧来鼓励生成器模型生成更多的样本空间</li>
<li>results: 实验结果表明，对于数据预processing中的模型骚乱攻击，我们的方法可以提供更高的鲁棒性和更好的攻击效果，同时也可以减少查询量和训练计算成本Here is the simplified Chinese text:</li>
<li>for: 提高数据预处理中模型骚乱攻击robustness</li>
<li>methods: 基于两个学生模型的 dual student方法，通过培养两个学生模型来提供生成器模型生成样本的依据，并通过对两个学生模型的分歧来鼓励生成器模型生成更多的样本空间</li>
<li>results: 实验结果表明，对于数据预处理中模型骚乱攻击，我们的方法可以提供更高的鲁棒性和更好的攻击效果，同时也可以减少查询量和训练计算成本<details>
<summary>Abstract</summary>
Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.
</details>
<details>
<summary>摘要</summary>
existed 无数据模型偷窃方法使用一个生成器生成样本，以训练一个学生模型与目标模型输出匹配。为此，两个主要挑战是无法估计目标模型参数的梯度，以及生成具有很好的覆盖度的训练样本。我们提议一种双学生方法，其中两个学生在相互对应的情况下受训。如果两个学生对某个样本表示不同意，那么至少有一个学生将该样本错误地分类为目标模型。这种启发性偏好探索更多的输入空间。另一方面，我们利用学生模型的梯度来间接估计目标模型的梯度。我们表明，这种新的训练目标函数对生成器网络来说等价于优化一个下界的生成器损失。我们示出，我们的新优化框架提供更准确的目标模型梯度估计和 benchmark 分类数据集上的更高的准确率。此外，我们的方法可以更好地平衡提高查询效率和训练计算成本。最后，我们证明我们的方法在基于转移型敌对攻击的传输基于模型偷窃方法中服为更好的代理模型。
</details></li>
</ul>
<hr>
<h2 id="Actively-Learning-Reinforcement-Learning-A-Stochastic-Optimal-Control-Approach"><a href="#Actively-Learning-Reinforcement-Learning-A-Stochastic-Optimal-Control-Approach" class="headerlink" title="Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach"></a>Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10831">http://arxiv.org/abs/2309.10831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad S. Ramadan, Mahmoud A. Hayajnh, Michael T. Tolley, Kyriakos G. Vamvoudakis</li>
<li>for: 这个论文是为了解决两个问题：（一）控制实验室&#x2F;模拟和现实世界之间的模型不确定性导致的强化学习的脆弱性，以及（二）决策控制的计算成本过高。</li>
<li>methods: 该论文使用强化学习解决了随机动态计划方程的问题，从而获得了一个安全的强化学习控制器，可以自动探索和利用不确定性，并且可以在实时中学习。</li>
<li>results: 例如在模拟示例中，该控制器能够实时学习并适应不同的模型不确定性，并且能够保证控制器的安全性。<details>
<summary>Abstract</summary>
In this paper we provide framework to cope with two problems: (i) the fragility of reinforcement learning due to modeling uncertainties because of the mismatch between controlled laboratory/simulation and real-world conditions and (ii) the prohibitive computational cost of stochastic optimal control. We approach both problems by using reinforcement learning to solve the stochastic dynamic programming equation. The resulting reinforcement learning controller is safe with respect to several types of constraints constraints and it can actively learn about the modeling uncertainties. Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning. A simulation example demonstrates the efficacy of the proposed approach.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一个框架来解决两个问题：（i）控制学习中的模型不确定性导致实验室/模拟环境和实际环境之间的匹配问题，以及（ii） Stochastic Optimal Control 的计算成本过高。我们通过使用 reinforcement learning 解决随机动态程序方程，以获得一个安全的控制器，该控制器可以自动地进行探索和利用，并且可以活动地学习模型不确定性。一个示例在实验中证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Modular-Spatial-Clustering-Algorithm-with-Noise-Specification"><a href="#A-Modular-Spatial-Clustering-Algorithm-with-Noise-Specification" class="headerlink" title="A Modular Spatial Clustering Algorithm with Noise Specification"></a>A Modular Spatial Clustering Algorithm with Noise Specification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10047">http://arxiv.org/abs/2309.10047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akhil K, Srikanth H R</li>
<li>for: 提高 clustering 算法的精度和快速性，以便更好地处理数据挖掘、机器学习和模式识别等领域中的数据分类问题。</li>
<li>methods: 基于细菌园的生长模型，通过控制细菌的生长和消耗来实现理想的分组准则。模块化设计，可以根据具体任务和数据分布创建特定版本的算法。还提供了适当减少噪声的功能。</li>
<li>results: 提出了一种新的分组算法，即细菌园算法（Bacteria-Farm），它可以平衡性能和参数优化的时间。与其他分组算法相比，该算法具有更好的准确率和鲁棒性。<details>
<summary>Abstract</summary>
Clustering techniques have been the key drivers of data mining, machine learning and pattern recognition for decades. One of the most popular clustering algorithms is DBSCAN due to its high accuracy and noise tolerance. Many superior algorithms such as DBSCAN have input parameters that are hard to estimate. Therefore, finding those parameters is a time consuming process. In this paper, we propose a novel clustering algorithm Bacteria-Farm, which balances the performance and ease of finding the optimal parameters for clustering. Bacteria- Farm algorithm is inspired by the growth of bacteria in closed experimental farms - their ability to consume food and grow - which closely represents the ideal cluster growth desired in clustering algorithms. In addition, the algorithm features a modular design to allow the creation of versions of the algorithm for specific tasks / distributions of data. In contrast with other clustering algorithms, our algorithm also has a provision to specify the amount of noise to be excluded during clustering.
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese: clustering 技术已经是数据挖掘、机器学习和模式识别领域的关键驱动者，数十年来。DBSCAN 算法是最受欢迎的一种，因为它具有高度准确性和噪声忍容性。然而，许多更高级的算法，如 DBSCAN，具有难以估算的输入参数。因此，查找这些参数是一项时间consuming 的过程。在这篇论文中，我们提出了一种新的 clustering 算法，叫做 Bacteria-Farm，它可以平衡性能和找到最佳参数的易用性。Bacteria-Farm 算法是通过closed experimental farms 中细菌的生长和增长来 inspirited 的，这种生长模式与 clustering 算法中理想的群集生长非常相似。此外，算法还具有可重新配置的模块化设计，以便为特定任务/数据分布创建版本。与其他 clustering 算法不同，我们的算法还具有排除噪声的功能。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Token-Coordinate-Descent-Method-for-Semi-Decentralized-Vertical-Federated-Learning"><a href="#A-Multi-Token-Coordinate-Descent-Method-for-Semi-Decentralized-Vertical-Federated-Learning" class="headerlink" title="A Multi-Token Coordinate Descent Method for Semi-Decentralized Vertical Federated Learning"></a>A Multi-Token Coordinate Descent Method for Semi-Decentralized Vertical Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09977">http://arxiv.org/abs/2309.09977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Valdeira, Yuejie Chi, Cláudia Soares, João Xavier</li>
<li>For: 采用 Multi-Token Coordinate Descent (MTCD) 算法进行 semi-decentralized  вертикального联合学习，提高通信效率。* Methods: 利用客户端-服务器和客户端-客户端通信，每个客户端持有小subset的特征进行并行 Markov chain (block) coordinate descent 算法。* Results: 实现了 $\mathcal{O}(1&#x2F;T)$ 的收敛速率 для非对称目标函数，并且可以控制并行通信的数量。<details>
<summary>Abstract</summary>
Communication efficiency is a major challenge in federated learning (FL). In client-server schemes, the server constitutes a bottleneck, and while decentralized setups spread communications, they do not necessarily reduce them due to slower convergence. We propose Multi-Token Coordinate Descent (MTCD), a communication-efficient algorithm for semi-decentralized vertical federated learning, exploiting both client-server and client-client communications when each client holds a small subset of features. Our multi-token method can be seen as a parallel Markov chain (block) coordinate descent algorithm and it subsumes the client-server and decentralized setups as special cases. We obtain a convergence rate of $\mathcal{O}(1/T)$ for nonconvex objectives when tokens roam over disjoint subsets of clients and for convex objectives when they roam over possibly overlapping subsets. Numerical results show that MTCD improves the state-of-the-art communication efficiency and allows for a tunable amount of parallel communications.
</details>
<details>
<summary>摘要</summary>
通信效率是联邦学习（FL）的主要挑战。在客户端服务器方案中，服务器成为瓶颈，而分散式设计可以分散通信，但不一定可以减少它们，因为它们的减少速度较慢。我们提出了多token坐标降低（MTCD）算法，用于半分散式垂直联邦学习，利用每个客户端持有的小subset特征来实现高效的通信。我们的多token方法可以看作是并行Markov链（块）坐标降低算法，它包含客户端服务器和分散式设计的特殊情况。我们得到了非对称目标函数的 $\mathcal{O}(1/T)$ 收敛率，并且数学实验表明，MTCD可以提高当前最佳的通信效率，并允许调整的并行通信数量。
</details></li>
</ul>
<hr>
<h2 id="Des-q-a-quantum-algorithm-to-construct-and-efficiently-retrain-decision-trees-for-regression-and-binary-classification"><a href="#Des-q-a-quantum-algorithm-to-construct-and-efficiently-retrain-decision-trees-for-regression-and-binary-classification" class="headerlink" title="Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification"></a>Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09976">http://arxiv.org/abs/2309.09976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niraj Kumar, Romina Yalovetzky, Changhao Li, Pierre Minssen, Marco Pistoia</li>
<li>For:  This paper proposes a novel quantum algorithm for constructing and retraining decision trees in regression and binary classification tasks, with the goal of significantly reducing the time required for tree retraining.* Methods: The proposed algorithm, named Des-q, uses a quantum-accessible memory to efficiently estimate feature weights and perform k-piecewise linear tree splits at each internal node. It also employs a quantum-supervised clustering method based on the q-means algorithm to determine the k suitable anchor points for these splits.* Results: The simulated version of the Des-q algorithm is benchmarked against the state-of-the-art classical decision tree for regression and binary classification on multiple data sets with numerical features, and is shown to exhibit similar performance while significantly speeding up the periodic tree retraining.<details>
<summary>Abstract</summary>
Decision trees are widely used in machine learning due to their simplicity in construction and interpretability. However, as data sizes grow, traditional methods for constructing and retraining decision trees become increasingly slow, scaling polynomially with the number of training examples. In this work, we introduce a novel quantum algorithm, named Des-q, for constructing and retraining decision trees in regression and binary classification tasks. Assuming the data stream produces small increments of new training examples, we demonstrate that our Des-q algorithm significantly reduces the time required for tree retraining, achieving a poly-logarithmic time complexity in the number of training examples, even accounting for the time needed to load the new examples into quantum-accessible memory. Our approach involves building a decision tree algorithm to perform k-piecewise linear tree splits at each internal node. These splits simultaneously generate multiple hyperplanes, dividing the feature space into k distinct regions. To determine the k suitable anchor points for these splits, we develop an efficient quantum-supervised clustering method, building upon the q-means algorithm of Kerenidis et al. Des-q first efficiently estimates each feature weight using a novel quantum technique to estimate the Pearson correlation. Subsequently, we employ weighted distance estimation to cluster the training examples in k disjoint regions and then proceed to expand the tree using the same procedure. We benchmark the performance of the simulated version of our algorithm against the state-of-the-art classical decision tree for regression and binary classification on multiple data sets with numerical features. Further, we showcase that the proposed algorithm exhibits similar performance to the state-of-the-art decision tree while significantly speeding up the periodic tree retraining.
</details>
<details>
<summary>摘要</summary>
决策树在机器学习中广泛使用，因其建构和解释性很好。但是，随着数据集的增大，传统的决策树建构和重新训练方法会变得越来越慢，时间复杂度随着训练示例数量的增长而呈极函数关系。在这种情况下，我们提出了一种新的量子算法，名为Des-q，用于在回归和二分类任务中建构和重新训练决策树。假设数据流量产生小量的新训练示例，我们示出了Des-q算法可以在训练示例数量的极函数时间复杂度下，对决策树进行重新训练，而不是在训练示例数量的几乎方差时间复杂度下。我们的方法是在每个内部节点上使用k个 piecewise 线性树split，这些拆分同时生成多个抽象。为确定k个适当的吊革点，我们开发了一种高效的量子监督学习方法，基于kerenidis等人提出的q-means算法。Des-q首先高效地估计每个特征的权重，使用一种新的量子技术来估计pearson相关性。然后，我们使用质量距离估计来归类训练示例，并将其分为k个不同的区域。最后，我们使用同样的过程来扩展树。我们使用模拟版的算法对比州时的分类树的表现，并证明Des-q算法在多个数据集上的numerical特征上 exhibits similar performance，同时具有明显的时间复杂度优化。此外，我们还显示了Des-q算法在 periodic tree retraining 中的性能，并证明它在训练示例数量的增长中保持稳定性。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Study-of-Mix-based-Data-Augmentation-Methods-in-Physiological-Time-Series-Data"><a href="#Empirical-Study-of-Mix-based-Data-Augmentation-Methods-in-Physiological-Time-Series-Data" class="headerlink" title="Empirical Study of Mix-based Data Augmentation Methods in Physiological Time Series Data"></a>Empirical Study of Mix-based Data Augmentation Methods in Physiological Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09970">http://arxiv.org/abs/2309.09970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp-well-org/mix-augmentation-for-physiological-time-series-classification">https://github.com/comp-well-org/mix-augmentation-for-physiological-time-series-classification</a></li>
<li>paper_authors: Peikun Guo, Huiyuan Yang, Akane Sano</li>
<li>for: 这个论文主要是为了探讨在生理时间序分类任务中使用mixup等混合基于数据增强技术的可能性和效果。</li>
<li>methods: 这个论文使用了多种mix-based数据增强技术，包括mixup、cutmix和替换混合，对六个生理时间序数据集进行了系统性的评估，以确定这些技术在不同的感知数据和分类任务中的表现。</li>
<li>results: 研究结果表明，三种mix-based数据增强技术可以在六个生理时间序数据集上提高表现，而且这些改进不需要专家知识或广泛的参数调整。<details>
<summary>Abstract</summary>
Data augmentation is a common practice to help generalization in the procedure of deep model training. In the context of physiological time series classification, previous research has primarily focused on label-invariant data augmentation methods. However, another class of augmentation techniques (\textit{i.e., Mixup}) that emerged in the computer vision field has yet to be fully explored in the time series domain. In this study, we systematically review the mix-based augmentations, including mixup, cutmix, and manifold mixup, on six physiological datasets, evaluating their performance across different sensory data and classification tasks. Our results demonstrate that the three mix-based augmentations can consistently improve the performance on the six datasets. More importantly, the improvement does not rely on expert knowledge or extensive parameter tuning. Lastly, we provide an overview of the unique properties of the mix-based augmentation methods and highlight the potential benefits of using the mix-based augmentation in physiological time series data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language="zh-CN"<</SYS>>数据扩充是一种常见的方法来帮助深度模型训练过程中的泛化。在生理时间序列分类领域，先前的研究主要集中在标签不变的数据扩充方法上。然而，另一类 augmentation 技术（即 Mixup）在计算机视觉领域出现后，尚未在时间序列领域得到完全探索。在这种研究中，我们系统地评估了基于混合的扩充方法，包括 mixup、cutmix 和 manifold mixup，在六个生理时间序列 dataset 上，并评估了不同的感知数据和分类任务中的性能。我们的结果表明，三种混合基于的扩充方法可以一致地提高六个 dataset 的性能。此外，这些改进不需要专家知识或广泛的参数调整。最后，我们介绍了混合基于的扩充方法的独特性质，并强调了在生理时间序列数据中使用混合基于的扩充方法的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Prompt-a-Robot-to-Walk-with-Large-Language-Models"><a href="#Prompt-a-Robot-to-Walk-with-Large-Language-Models" class="headerlink" title="Prompt a Robot to Walk with Large Language Models"></a>Prompt a Robot to Walk with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09969">http://arxiv.org/abs/2309.09969</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HybridRobotics/prompt2walk">https://github.com/HybridRobotics/prompt2walk</a></li>
<li>paper_authors: Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath</li>
<li>for: 这个研究旨在使用几何提示来将大型自然语言模型（LLM）应用于机器人控制中。</li>
<li>methods: 这个研究使用了几何提示收集自物理环境，并使用了LLM进行循环预测控制命令。</li>
<li>results: 实验结果显示，这个方法可以有效地将机器人诱导到行走。这证明了LLM可以作为机器人动作控制中的低层反馈控制器。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Large language models (LLMs) pre-trained on vast internet-scale data have showcased remarkable capabilities across diverse domains. Recently, there has been escalating interest in deploying LLMs for robotics, aiming to harness the power of foundation models in real-world settings. However, this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions. To address these issues, we introduce a novel paradigm in which we use few-shot prompts collected from the physical environment, enabling the LLM to autoregressively generate low-level control commands for robots without task-specific fine-tuning. Experiments across various robots and environments validate that our method can effectively prompt a robot to walk. We thus illustrate how LLMs can proficiently function as low-level feedback controllers for dynamic motion control even in high-dimensional robotic systems. The project website and source code can be found at: https://prompt2walk.github.io/ .
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在互联网规模数据上预训练后表现出了各种各样的能力。近期，有越来越多的人们对于使用 LLM 在实际场景中进行部署表示了极大的兴趣。然而，这种方法面临着 significativetranslation challenges，特别是在将模型 anchored 到物理世界中和生成动态机器人运动。为了解决这些问题，我们提出了一种新的思路，即通过几个shot的提示从物理环境中收集，使 LLM 可以自动生成机器人的低级控制命令，无需特定任务的微调。经过对各种机器人和环境的实验，我们发现我们的方法可以有效地使机器人行走。我们因此证明了 LLM 可以在高维机器人系统中作为低级反馈控制器，进行动态运动控制。项目官网和代码可以在以下链接中找到：https://prompt2walk.github.io/。
</details></li>
</ul>
<hr>
<h2 id="Generating-and-Imputing-Tabular-Data-via-Diffusion-and-Flow-based-Gradient-Boosted-Trees"><a href="#Generating-and-Imputing-Tabular-Data-via-Diffusion-and-Flow-based-Gradient-Boosted-Trees" class="headerlink" title="Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees"></a>Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09968">http://arxiv.org/abs/2309.09968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexia Jolicoeur-Martineau, Kilian Fatras, Tal Kachman</li>
<li>for: 生成和填充混合类型（连续和分类）表格数据</li>
<li>methods: 使用分布式扩散和条件流匹配生成混合类型表格数据，而不是使用前一些工作中的神经网络函数近似器</li>
<li>results: 在不同的数据集上，我们的方法可以生成高度真实的人工数据，并且可以生成多种可能的数据填充结果，经验显示我们的方法通常超越深度学习生成方法，可以在CPU上并行训练而无需GPU，我们将代码发布到PyPI和CRAN上。<details>
<summary>Abstract</summary>
Tabular data is hard to acquire and is subject to missing values. This paper proposes a novel approach to generate and impute mixed-type (continuous and categorical) tabular data using score-based diffusion and conditional flow matching. Contrary to previous work that relies on neural networks as function approximators, we instead utilize XGBoost, a popular Gradient-Boosted Tree (GBT) method. In addition to being elegant, we empirically show on various datasets that our method i) generates highly realistic synthetic data when the training dataset is either clean or tainted by missing data and ii) generates diverse plausible data imputations. Our method often outperforms deep-learning generation methods and can trained in parallel using CPUs without the need for a GPU. To make it easily accessible, we release our code through a Python library on PyPI and an R package on CRAN.
</details>
<details>
<summary>摘要</summary>
<font size="4">表格数据具有困难和缺失值。这篇论文提出了一种新的方法，使用分数基diffusion和条件流匹配生成和填充混合类型（连续和分类）表格数据。与之前的工作不同，我们不使用神经网络作为函数估计器，而是使用XGBoost，一种受欢迎的梯度增强树（GBT）方法。我们的方法不仅简洁高效，而且在不同的数据集上经验表明，我们的方法可以生成高度真实的人工数据，并且可以生成多种可能的数据填充。我们的方法经常超过深度学习生成方法，并且可以在CPU上并行训练，不需要GPU。为便于使用，我们在Python库中发布了代码，并在CRAN上发布了R包。</font>Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Adversarial-Robustness-with-Expected-Viable-Performance"><a href="#Evaluating-Adversarial-Robustness-with-Expected-Viable-Performance" class="headerlink" title="Evaluating Adversarial Robustness with Expected Viable Performance"></a>Evaluating Adversarial Robustness with Expected Viable Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09928">http://arxiv.org/abs/2309.09928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan McCoppin, Colin Dawson, Sean M. Kennedy, Leslie M. Blaha</li>
<li>for: 评估预测模型的可靠性，特别关注对抗性扰动的影响。</li>
<li>methods: 使用可靠性指标，例如分类精度，来评估模型在不同抗性扰动下的性能。</li>
<li>results: 提出一种基于期望值的预测模型可靠性评估方法，以便更好地评估模型对抗性扰动的抗性性能。<details>
<summary>Abstract</summary>
We introduce a metric for evaluating the robustness of a classifier, with particular attention to adversarial perturbations, in terms of expected functionality with respect to possible adversarial perturbations. A classifier is assumed to be non-functional (that is, has a functionality of zero) with respect to a perturbation bound if a conventional measure of performance, such as classification accuracy, is less than a minimally viable threshold when the classifier is tested on examples from that perturbation bound. Defining robustness in terms of an expected value is motivated by a domain general approach to robustness quantification.
</details>
<details>
<summary>摘要</summary>
我们提出了一种度量分类器的Robustness，特别关注对抗攻击的影响，以期望功能性对可能的攻击 perturbations 的平均值。我们假设分类器对某个 perturbation bound 的测试例而言，如果使用 convential 度量指标，例如分类率，则分类器的性能低于最低可接受水平，则分类器对该 bound 的测试例是不可用的（即其功能性为零）。定义Robustness 以期望值的方式受到Domain 通用的Robustness 评估方法的 inspirations。
</details></li>
</ul>
<hr>
<h2 id="Graph-topological-property-recovery-with-heat-and-wave-dynamics-based-features-on-graphs"><a href="#Graph-topological-property-recovery-with-heat-and-wave-dynamics-based-features-on-graphs" class="headerlink" title="Graph topological property recovery with heat and wave dynamics-based features on graphs"></a>Graph topological property recovery with heat and wave dynamics-based features on graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09924">http://arxiv.org/abs/2309.09924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhananjay Bhaskar, Yanlei Zhang, Charles Xu, Xingzhi Sun, Oluwadamilola Fasina, Guy Wolf, Maximilian Nickel, Michael Perlmutter, Smita Krishnaswamy</li>
<li>for: 这个论文是为了提出一种基于解决常微分方程的图形网络（GDeNet），用于获取不同下游任务的连续节点和图级表示。</li>
<li>methods: 论文使用了解析解决方法，连接热和波方程的动力学特性和图形的 спектраль性质以及连续时间游走在图形上的行为。</li>
<li>results: 实验表明，这些动力学特性能够捕捉图形的几何和拓扑特征，并且在真实世界数据集上表现出优于其他方法。<details>
<summary>Abstract</summary>
In this paper, we propose Graph Differential Equation Network (GDeNet), an approach that harnesses the expressive power of solutions to PDEs on a graph to obtain continuous node- and graph-level representations for various downstream tasks. We derive theoretical results connecting the dynamics of heat and wave equations to the spectral properties of the graph and to the behavior of continuous-time random walks on graphs. We demonstrate experimentally that these dynamics are able to capture salient aspects of graph geometry and topology by recovering generating parameters of random graphs, Ricci curvature, and persistent homology. Furthermore, we demonstrate the superior performance of GDeNet on real-world datasets including citation graphs, drug-like molecules, and proteins.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了图Diffusion Equation Network（GDeNet），它利用图上解析方程的表达能力来获得不同下游任务的连续节点和图级表示。我们 derive了对热和波方程的动态性和图的spectral properties以及漫步过程的连续性有关的理论结果。我们通过实验表明，这些动态可以捕捉图的几何和topology特征，例如生成参数、Ricci curvature和持续同构。此外，我们在真实世界数据集上证明了GDeNet的优越性。
</details></li>
</ul>
<hr>
<h2 id="Distilling-HuBERT-with-LSTMs-via-Decoupled-Knowledge-Distillation"><a href="#Distilling-HuBERT-with-LSTMs-via-Decoupled-Knowledge-Distillation" class="headerlink" title="Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation"></a>Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09920">http://arxiv.org/abs/2309.09920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danilo de Oliveira, Timo Gerkmann</li>
<li>for: 压缩 HuBERT 模型的知识，以提高自动语音识别器的性能和储存需求。</li>
<li>methods: 使用知识传递和分离知识传递的方法，将 HuBERT 的 Transformer 层转换为 LSTM 型的压缩模型，以减少参数数量并提高自动语音识别器的性能。</li>
<li>results: 与 DistilHuBERT 相比， proposed 方法可以实现更好的自动语音识别器性能，并且对于储存需求产生更大的改善。<details>
<summary>Abstract</summary>
Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.
</details>
<details>
<summary>摘要</summary>
很多研究力量是用于压缩自动学习模型的知识，这些模型具有强大的能力，但却占用大量内存。在这项工作中，我们显示了知识储存方法（以及其最近提出的扩展方法，即分离知识储存）可以应用于压缩HuBERT。与其他方法不同的是，我们可以在压缩模型网络结构中有更多的自由。因此，我们提议将HuBERT的转换层压缩成一个使用LSTM的压缩模型，以降低参数数量，并且同时在自动语音识别中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Learning-Nonparametric-High-Dimensional-Generative-Models-The-Empirical-Beta-Copula-Autoencoder"><a href="#Learning-Nonparametric-High-Dimensional-Generative-Models-The-Empirical-Beta-Copula-Autoencoder" class="headerlink" title="Learning Nonparametric High-Dimensional Generative Models: The Empirical-Beta-Copula Autoencoder"></a>Learning Nonparametric High-Dimensional Generative Models: The Empirical-Beta-Copula Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09916">http://arxiv.org/abs/2309.09916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Coblenz, Oliver Grothe, Fabian Kächele<br>for:* 这个论文的目的是把自动编码器转化为生成模型，并且寻找简单有效的方法来实现这一点。methods:* 论文使用了多种方法来模型自动编码器的幂数空间，包括核密度估计、泊松分布、正规流等。results:* 研究发现，使用泊松分布模型自动编码器的幂数空间可以很好地生成新的数据样本，并且可以控制生成的数据样本具有特定的特征。此外，论文还提供了一种新的copula模型，即Empirical Beta Copula Autoencoder，可以更好地实现生成模型的目的。<details>
<summary>Abstract</summary>
By sampling from the latent space of an autoencoder and decoding the latent space samples to the original data space, any autoencoder can simply be turned into a generative model. For this to work, it is necessary to model the autoencoder's latent space with a distribution from which samples can be obtained. Several simple possibilities (kernel density estimates, Gaussian distribution) and more sophisticated ones (Gaussian mixture models, copula models, normalization flows) can be thought of and have been tried recently. This study aims to discuss, assess, and compare various techniques that can be used to capture the latent space so that an autoencoder can become a generative model while striving for simplicity. Among them, a new copula-based method, the Empirical Beta Copula Autoencoder, is considered. Furthermore, we provide insights into further aspects of these methods, such as targeted sampling or synthesizing new data with specific features.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate english text into simplified chineseBy sampling from the latent space of an autoencoder and decoding the latent space samples to the original data space, any autoencoder can simply be turned into a generative model. For this to work, it is necessary to model the autoencoder's latent space with a distribution from which samples can be obtained. Several simple possibilities (kernel density estimates, Gaussian distribution) and more sophisticated ones (Gaussian mixture models, copula models, normalization flows) can be thought of and have been tried recently. This study aims to discuss, assess, and compare various techniques that can be used to capture the latent space so that an autoencoder can become a generative model while striving for simplicity. Among them, a new copula-based method, the Empirical Beta Copula Autoencoder, is considered. Furthermore, we provide insights into further aspects of these methods, such as targeted sampling or synthesizing new data with specific features. traducción al chino simplificado通过从自编码器的幂 space 中采样并将幂 space 采样转换为原始数据空间，任何自编码器都可以简单地变成生成模型。为了实现这一点，需要模型自编码器的幂空间 distribution 中的样本。一些简单的可能性（kernel density estimates，Gaussian distribution）和更复杂的一些（Gaussian mixture models，copula models，normalization flows）都有被考虑和尝试过。本研究旨在讨论、评估和比较这些方法，以实现将自编码器转化为生成模型，同时寻求简单性。其中，一种新的 copula-based 方法，即 Empirical Beta Copula Autoencoder，被考虑。此外，我们还提供了这些方法的进一步含义，例如针对的采样或生成特定特征的新数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Generate-Lumped-Hydrological-Models"><a href="#Learning-to-Generate-Lumped-Hydrological-Models" class="headerlink" title="Learning to Generate Lumped Hydrological Models"></a>Learning to Generate Lumped Hydrological Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09904">http://arxiv.org/abs/2309.09904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Yang, Ting Fong May Chui</li>
<li>for: 这个研究旨在开发一种数据驱动的方法，用于表征水文功能在流域中的低维度表示，并使用这种表示来重建特定的水文功能。</li>
<li>methods: 这个研究使用深度学习方法来学习流域的水文功能，并直接从气候冲击和流域流量数据中学习出latent variable的值。</li>
<li>results: 研究发现，使用这种方法可以从全球超过3,000个流域的数据中学习出高质量的生成模型，并将这些生成模型应用到700个不同的流域中，得到了比或更好的估计结果。<details>
<summary>Abstract</summary>
In a lumped hydrological model structure, the hydrological function of a catchment is characterized by only a few parameters. Given a set of parameter values, a numerical function useful for hydrological prediction is generated. Thus, this study assumes that the hydrological function of a catchment can be sufficiently well characterized by a small number of latent variables. By specifying the variable values, a numerical function resembling the hydrological function of a real-world catchment can be generated using a generative model. In this study, a deep learning method is used to learn both the generative model and the latent variable values of different catchments directly from their climate forcing and runoff data, without using catchment attributes. The generative models can be used similarly to a lumped model structure, i.e., by estimating the optimal parameter or latent variable values using a generic model calibration algorithm, an optimal numerical model can be derived. In this study, generative models using eight latent variables were learned from data from over 3,000 catchments worldwide, and the learned generative models were applied to model over 700 different catchments using a generic calibration algorithm. The quality of the resulting optimal models was generally comparable to or better than that obtained using 36 different types of lump model structures or using non-generative deep learning methods. In summary, this study presents a data-driven approach for representing the hydrological function of a catchment in low-dimensional space and a method for reconstructing specific hydrological functions from the representations.
</details>
<details>
<summary>摘要</summary>
在汇集型水文模型结构中，湍水功能的某catchment被定义为只有一些参数。给定一组参数值，可以生成一个数值函数用于水文预测。因此，本研究假设catchment的水文功能可以通过一小数量的隐变量足够准确地表示。通过 specifying变量值，可以使用生成模型生成一个数值函数类似于实际世界catchment的水文函数。在本研究中，使用深度学习方法来学习catchment的生成模型和隐变量值，不使用catchment特征。生成模型可以与汇集型模型结构相同地使用，即通过优化参数或隐变量值使用一个通用模型调整算法来获得最佳数值模型。在本研究中，使用八个隐变量学习了来自全球3,000多个catchment的数据，并将学习的生成模型应用到700多个不同的catchment上使用通用调整算法。得到的优化模型质量通常与或更高于使用36种不同的汇集模型结构或非生成型深度学习方法所获得的质量。总之，本研究提出了一种数据驱动的方法，用于表示catchment的水文功能在低维空间中，以及一种方法，用于从表示中重建特定的水文功能。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-the-Joint-Control-of-Traffic-Light-Signaling-and-Vehicle-Speed-Advice"><a href="#Deep-Reinforcement-Learning-for-the-Joint-Control-of-Traffic-Light-Signaling-and-Vehicle-Speed-Advice" class="headerlink" title="Deep Reinforcement Learning for the Joint Control of Traffic Light Signaling and Vehicle Speed Advice"></a>Deep Reinforcement Learning for the Joint Control of Traffic Light Signaling and Vehicle Speed Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09881">http://arxiv.org/abs/2309.09881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes V. S. Busch, Robert Voelckner, Peter Sossalla, Christian L. Vielhaus, Roberto Calandra, Frank H. P. Fitzek</li>
<li>for: 提高城市堵塞的效率和环保性</li>
<li>methods: 使用深度强化学习控制交通信号灯和车辆行驶速度</li>
<li>results: 在八个 из十一个测试场景中，联合控制方法可以降低车辆旅行延迟，并且观察到车辆速度建议策略可以平滑车辆附近交通信号灯的速度变化。<details>
<summary>Abstract</summary>
Traffic congestion in dense urban centers presents an economical and environmental burden. In recent years, the availability of vehicle-to-anything communication allows for the transmission of detailed vehicle states to the infrastructure that can be used for intelligent traffic light control. The other way around, the infrastructure can provide vehicles with advice on driving behavior, such as appropriate velocities, which can improve the efficacy of the traffic system. Several research works applied deep reinforcement learning to either traffic light control or vehicle speed advice. In this work, we propose a first attempt to jointly learn the control of both. We show this to improve the efficacy of traffic systems. In our experiments, the joint control approach reduces average vehicle trip delays, w.r.t. controlling only traffic lights, in eight out of eleven benchmark scenarios. Analyzing the qualitative behavior of the vehicle speed advice policy, we observe that this is achieved by smoothing out the velocity profile of vehicles nearby a traffic light. Learning joint control of traffic signaling and speed advice in the real world could help to reduce congestion and mitigate the economical and environmental repercussions of today's traffic systems.
</details>
<details>
<summary>摘要</summary>
压力交通在紧张城市中带来经济和环境沉重负担。最近几年，车辆到任何通信技术的可用性允许车辆状态的详细传输到基础设施，以便智能交通灯控制。基础设施也可以为车辆提供适当的行驶方式建议，如 velocities，以改善交通系统的效率。一些研究工作使用深度强化学习控制交通灯或车辆速度。在这种工作中，我们提出了第一次同时学习交通灯控制和车辆速度建议的方法。我们示出这可以提高交通系统的效率。在我们的实验中，同时控制方法比只控制交通灯时间减少了平均车辆旅行延迟，在 eleven 个标准场景中出现了八个情况。分析车辆速度建议政策的Qualitative行为，我们发现这是通过车辆附近交通灯的速度profile的平滑来实现的。在实际世界中学习同时控制交通信号和车辆速度的可能会帮助减少拥堵和今天的交通系统的经济和环境后果。
</details></li>
</ul>
<hr>
<h2 id="Error-Reduction-from-Stacked-Regressions"><a href="#Error-Reduction-from-Stacked-Regressions" class="headerlink" title="Error Reduction from Stacked Regressions"></a>Error Reduction from Stacked Regressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09880">http://arxiv.org/abs/2309.09880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Xin Chen, Jason M. Klusowski, Yan Shuo Tan</li>
<li>for: 提高预测精度，使用核算法组合多个回归分析器</li>
<li>methods: 使用非负约束最小二乘法学习组合权重，其中每个回归分析器都是线性最小二乘法项</li>
<li>results: 在嵌入多维空间中，使用核算法组合可以减小人口风险，并且比最佳单个回归分析器更好Here’s a breakdown of each point:* “for”: The paper is written to improve the accuracy of predictions by combining multiple regression analyzers using a nuclear algorithm.* “methods”: The paper uses a non-negative constraint least-squares method to learn the combination weights of the constituent estimators, and the optimization problem can be reformulated as isotonic regression.* “results”: The resulting stacked estimator has a strictly smaller population risk than the best single estimator among them, and it can be implemented with the same order of computation as the best single estimator.<details>
<summary>Abstract</summary>
Stacking regressions is an ensemble technique that forms linear combinations of different regression estimators to enhance predictive accuracy. The conventional approach uses cross-validation data to generate predictions from the constituent estimators, and least-squares with nonnegativity constraints to learn the combination weights. In this paper, we learn these weights analogously by minimizing an estimate of the population risk subject to a nonnegativity constraint. When the constituent estimators are linear least-squares projections onto nested subspaces separated by at least three dimensions, we show that thanks to a shrinkage effect, the resulting stacked estimator has strictly smaller population risk than best single estimator among them. Here ``best'' refers to a model that minimizes a selection criterion such as AIC or BIC. In other words, in this setting, the best single estimator is inadmissible. Because the optimization problem can be reformulated as isotonic regression, the stacked estimator requires the same order of computation as the best single estimator, making it an attractive alternative in terms of both performance and implementation.
</details>
<details>
<summary>摘要</summary>
核 stacking 是一种ensemble技术，通过将不同的回归估计器组合起来，提高预测精度。传统方法使用 Cross-validation 数据生成组合估计器的预测，并使用 least-squares 方法学习组合权重。在这篇论文中，我们通过最小化人口风险的估计器来学习这些权重，并且对这些权重进行非负性约束。当组合估计器是线性最小二乘投影 onto 嵌入在至少三维空间中的子空间时，我们显示了一种减小效果，即核 stacked 估计器的人口风险比最佳单个估计器（按照 AIC 或 BIC 选择 criterion）更小。这里的“最佳”指的是一个模型，可以最小化一个选择 criterion。在这种设定下，最佳单个估计器是不可接受的。因为优化问题可以 reformulated 为iso-tonic regression，核 stacked 估计器需要与最佳单个估计器相同的计算顺序，因此它在性能和实现方面都是一个吸引人的选择。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-with-Fourier-Transform-and-Soft-Thresholding"><a href="#Domain-Generalization-with-Fourier-Transform-and-Soft-Thresholding" class="headerlink" title="Domain Generalization with Fourier Transform and Soft Thresholding"></a>Domain Generalization with Fourier Transform and Soft Thresholding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09866">http://arxiv.org/abs/2309.09866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyi Pan, Bin Wang, Zheyuan Zhan, Xin Zhu, Debesh Jha, Ahmet Enis Cetin, Concetto Spampinato, Ulas Bagci</li>
<li>for: 用于提高脑网络模型对不同来源图像的泛化性能</li>
<li>methods: 使用傅리曼变换基于的频谱预处理策略，并 introduce soft-thresholding函数来消除频谱中的背景干扰</li>
<li>results: 通过实验 validate our approach的效果，与传统和现有方法相比，具有较好的 segmentation  metric 和更好的泛化性能<details>
<summary>Abstract</summary>
Domain generalization aims to train models on multiple source domains so that they can generalize well to unseen target domains. Among many domain generalization methods, Fourier-transform-based domain generalization methods have gained popularity primarily because they exploit the power of Fourier transformation to capture essential patterns and regularities in the data, making the model more robust to domain shifts. The mainstream Fourier-transform-based domain generalization swaps the Fourier amplitude spectrum while preserving the phase spectrum between the source and the target images. However, it neglects background interference in the amplitude spectrum. To overcome this limitation, we introduce a soft-thresholding function in the Fourier domain. We apply this newly designed algorithm to retinal fundus image segmentation, which is important for diagnosing ocular diseases but the neural network's performance can degrade across different sources due to domain shifts. The proposed technique basically enhances fundus image augmentation by eliminating small values in the Fourier domain and providing better generalization. The innovative nature of the soft thresholding fused with Fourier-transform-based domain generalization improves neural network models' performance by reducing the target images' background interference significantly. Experiments on public data validate our approach's effectiveness over conventional and state-of-the-art methods with superior segmentation metrics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Prognosis-of-Multivariate-Battery-State-of-Performance-and-Health-via-Transformers"><a href="#Prognosis-of-Multivariate-Battery-State-of-Performance-and-Health-via-Transformers" class="headerlink" title="Prognosis of Multivariate Battery State of Performance and Health via Transformers"></a>Prognosis of Multivariate Battery State of Performance and Health via Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10014">http://arxiv.org/abs/2309.10014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah H. Paulson, Joseph J. Kubal, Susan J. Babinec</li>
<li>for: 本研究的目的是提供一种深度学习模型，用于预测锂离子电池性能和使用寿命。</li>
<li>methods: 该研究使用了深度变换网络模型，利用两个循环测试数据集，表征了六种锂离子电池化学组成（LFP、NMC111、NMC532、NMC622、HE5050和5Vspinel）、不同的电解液&#x2F;镍电极组合和充电&#x2F;充电方式。</li>
<li>results: 该研究的结果表明，使用深度学习模型可以高度准确地预测锂离子电池的性能和使用寿命，其中LFP快速充电数据集的预测结束时间误差为19循环，表明深度学习对锂离子电池健康状况的预测具有扎实的批处能力。<details>
<summary>Abstract</summary>
Batteries are an essential component in a deeply decarbonized future. Understanding battery performance and "useful life" as a function of design and use is of paramount importance to accelerating adoption. Historically, battery state of health (SOH) was summarized by a single parameter, the fraction of a battery's capacity relative to its initial state. A more useful approach, however, is a comprehensive characterization of its state and complexities, using an interrelated set of descriptors including capacity, energy, ionic and electronic impedances, open circuit voltages, and microstructure metrics. Indeed, predicting across an extensive suite of properties as a function of battery use is a "holy grail" of battery science; it can provide unprecedented insights toward the design of better batteries with reduced experimental effort, and de-risking energy storage investments that are necessary to meet CO2 reduction targets. In this work, we present a first step in that direction via deep transformer networks for the prediction of 28 battery state of health descriptors using two cycling datasets representing six lithium-ion cathode chemistries (LFP, NMC111, NMC532, NMC622, HE5050, and 5Vspinel), multiple electrolyte/anode compositions, and different charge-discharge scenarios. The accuracy of these predictions versus battery life (with an unprecedented mean absolute error of 19 cycles in predicting end of life for an LFP fast-charging dataset) illustrates the promise of deep learning towards providing deeper understanding and control of battery health.
</details>
<details>
<summary>摘要</summary>
锂离子电池是深度减碳未来的重要组件。理解锂离子电池性能和使用寿命的关系是加速采用的关键。历史上，锂离子电池状况（SOH）通常是用一个参数表示，即锂离子电池容量相对初始状态的比率。然而，一个更有用的方法是对锂离子电池状况进行全面描述，使用一组相关的参数，包括容量、能量、锂离子和电子阻抗、开路电压和微结构指标。实际上，预测锂离子电池的广泛性能特征是 battery science 的“圣杯”，可以提供前所未有的洞察，并帮助设计更好的锂离子电池，降低实验努力，并为温室气体减排目标做出更多的投资。在这项工作中，我们提出了一种首先采用深度变换网络来预测28个锂离子电池状况指标，使用两个循环数据集，表示六种锂离子陶瓷电池化学式（LFP、NMC111、NMC532、NMC622、HE5050和5Vspinel）、多种电解质/陶瓷组合、以及不同的充电-充电方案。预测的准确性（例如，LFP快充电数据集中预测结束生命的mean absolute error为19次）表明深度学习对锂离子电池健康提供了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Deep-Kernel-Machines"><a href="#Convolutional-Deep-Kernel-Machines" class="headerlink" title="Convolutional Deep Kernel Machines"></a>Convolutional Deep Kernel Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09814">http://arxiv.org/abs/2309.09814</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luisgarzac/Data-Science-Course---Udemy-frogames-Juan-Gabriel-Gomila">https://github.com/luisgarzac/Data-Science-Course---Udemy-frogames-Juan-Gabriel-Gomila</a></li>
<li>paper_authors: Edward Milsom, Ben Anson, Laurence Aitchison</li>
<li>for: 这篇论文主要是为了探讨深度kernel机器（DKM）的应用和发展。</li>
<li>methods: 本论文使用了深度kernel机器（DKM），其不同于传统的 neural network 和 deep kernel learning，因为它们都使用特征作为基本组件。此外，论文还提出了一种有效的间领点拟合方案。</li>
<li>results: 根据实验结果，使用了不同的 normalization 和 likelihood 的模型 variants，可以达到约 99% 的测试准确率在 MNIST 上，92% 在 CIFAR-10 上，71% 在 CIFAR-100 上，而且只需要训练约 28 个 GPU 小时，相比于全功能 NNGP &#x2F; NTK &#x2F; Myrtle kernels，速度提高了1-2个数量级。<details>
<summary>Abstract</summary>
Deep kernel machines (DKMs) are a recently introduced kernel method with the flexibility of other deep models including deep NNs and deep Gaussian processes. DKMs work purely with kernels, never with features, and are therefore different from other methods ranging from NNs to deep kernel learning and even deep Gaussian processes, which all use features as a fundamental component. Here, we introduce convolutional DKMs, along with an efficient inter-domain inducing point approximation scheme. Further, we develop and experimentally assess a number of model variants, including 9 different types of normalisation designed for the convolutional DKMs, two likelihoods, and two different types of top-layer. The resulting models achieve around 99% test accuracy on MNIST, 92% on CIFAR-10 and 71% on CIFAR-100, despite training in only around 28 GPU hours, 1-2 orders of magnitude faster than full NNGP / NTK / Myrtle kernels, whilst achieving comparable performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Optimal-Contracts-How-to-Exploit-Small-Action-Spaces"><a href="#Learning-Optimal-Contracts-How-to-Exploit-Small-Action-Spaces" class="headerlink" title="Learning Optimal Contracts: How to Exploit Small Action Spaces"></a>Learning Optimal Contracts: How to Exploit Small Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09801">http://arxiv.org/abs/2309.09801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Bacchiocchi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti</li>
<li>for: 解决主动자和代理人之间的主动-代理人问题，即主动자通过一系列的合同来劝动代理人进行成本高、不可见的行动，以实现有利的结果。</li>
<li>methods: 我们使用多轮合同的扩展版本，在没有主动자对代理人的信息的情况下，通过观察每轮的结果来学习最优的合同。我们采用一种算法，可以在小的动作空间下获得高probability的最优合同，并且可以在多轮合同中实现$\tilde{\mathcal{O}(T^{4&#x2F;5})$的 regret bound。</li>
<li>results: 我们解决了Zhu等人（2022）提出的开放问题，并且可以在相关的在线学习 Setting中提供$\tilde{\mathcal{O}(T^{4&#x2F;5})$的 regret bound，这比之前的 regret bound大大提高。<details>
<summary>Abstract</summary>
We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme -- called contract -- in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent's action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al.[2022]. Moreover, it can also be employed to provide a $\tilde{\mathcal{O}(T^{4/5})$ regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility, thus considerably improving previously-known regret bounds.
</details>
<details>
<summary>摘要</summary>
我们研究主体-代理人问题，在这个问题中，主体会提出一个结果相依的支付计划，以使代理人采取费用高、不可见的行动，导致更加有利的结果。我们考虑了经典单回版本的问题的扩展，在这个问题中，主体和代理人在多轮交互中进行互动，主体没有关于代理人的信息，只能通过每轮结果来学习最佳合同。我们关注在代理人行动空间尺度小的情况下。我们设计了一个可以在小于Outcome空间大小的情况下获得近似最佳合同的算法，并且可以提供$\tilde{\mathcal{O}(T^{4/5})$的 regret bound，这比前所未见的 regret bound要更好。此外，这个算法还可以在相关的在线学习设定下应用，以最大化主体的总用用，从而提高之前已知的 regret bound。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Initial-State-Buffer-for-Reinforcement-Learning"><a href="#Contrastive-Initial-State-Buffer-for-Reinforcement-Learning" class="headerlink" title="Contrastive Initial State Buffer for Reinforcement Learning"></a>Contrastive Initial State Buffer for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09752">http://arxiv.org/abs/2309.09752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nico Messikommer, Yunlong Song, Davide Scaramuzza</li>
<li>for: 提高强化学习的效率，使用有限样本学习</li>
<li>methods: 引入一种矛盾起始状态缓存，选择过去经验中的状态，用于初始化机器人在环境中，引导其走向更有信息的状态</li>
<li>results: 在两个复杂的机器人任务中，实验结果显示，我们的初始状态缓存可以比基线方案高效，同时也加速了训练的收敛Here’s the summary in English for reference:</li>
<li>for: Improving the efficiency of reinforcement learning, using limited samples to learn</li>
<li>methods: Introducing a Contrastive Initial State Buffer that strategically selects states from past experiences to initialize the agent in the environment, guiding it towards more informative states</li>
<li>results: Experimental results on two complex robotic tasks show that our initial state buffer achieves higher task performance than the baseline while also speeding up training convergence.<details>
<summary>Abstract</summary>
In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training convergence.
</details>
<details>
<summary>摘要</summary>
在强化学习中，探索和利用之间的贸易带来了复杂的挑战，以实现从有限样本中获得高效的学习。然而， latest works often overlook the potential of reusing past experiences for data collection.我们在独立于基础RL算法的情况下，引入了一个对比起始缓存，该缓存从过去经验中选择状态，并将其用于初始化机器人在环境中，以引导它到更有用的状态。我们在两个复杂的机器人任务上进行了验证：（i）一只四脚 robot 在困难的地形上行走，以及（ii）一架quadcopter飞机在赛道上飞行。实验结果表明，我们的起始缓存可以比基线提高任务性能，同时也可以加速训练的收敛。
</details></li>
</ul>
<hr>
<h2 id="Towards-Better-Modeling-with-Missing-Data-A-Contrastive-Learning-based-Visual-Analytics-Perspective"><a href="#Towards-Better-Modeling-with-Missing-Data-A-Contrastive-Learning-based-Visual-Analytics-Perspective" class="headerlink" title="Towards Better Modeling with Missing Data: A Contrastive Learning-based Visual Analytics Perspective"></a>Towards Better Modeling with Missing Data: A Contrastive Learning-based Visual Analytics Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09744">http://arxiv.org/abs/2309.09744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laixin Xie, Yang Ouyang, Longfei Chen, Ziming Wu, Quan Li</li>
<li>for:  addresses the challenges of missing data in machine learning (ML) modeling</li>
<li>methods:  uses Contrastive Learning (CL) framework to model observed data with missing values, without requiring any imputation</li>
<li>results:  demonstrates high predictive accuracy and model interpretability through quantitative experiments, expert interviews, and a qualitative user study.Here’s the full summary in Simplified Chinese:</li>
<li>for: Addresses the challenges of missing data in machine learning (ML) modeling</li>
<li>methods: 使用异常学习（CL）框架，模拟带有缺失数据的观察数据，不需要任何替换</li>
<li>results: 通过量化实验、专家采访和用户研究，证明高预测精度和模型可读性。<details>
<summary>Abstract</summary>
Missing data can pose a challenge for machine learning (ML) modeling. To address this, current approaches are categorized into feature imputation and label prediction and are primarily focused on handling missing data to enhance ML performance. These approaches rely on the observed data to estimate the missing values and therefore encounter three main shortcomings in imputation, including the need for different imputation methods for various missing data mechanisms, heavy dependence on the assumption of data distribution, and potential introduction of bias. This study proposes a Contrastive Learning (CL) framework to model observed data with missing values, where the ML model learns the similarity between an incomplete sample and its complete counterpart and the dissimilarity between other samples. Our proposed approach demonstrates the advantages of CL without requiring any imputation. To enhance interpretability, we introduce CIVis, a visual analytics system that incorporates interpretable techniques to visualize the learning process and diagnose the model status. Users can leverage their domain knowledge through interactive sampling to identify negative and positive pairs in CL. The output of CIVis is an optimized model that takes specified features and predicts downstream tasks. We provide two usage scenarios in regression and classification tasks and conduct quantitative experiments, expert interviews, and a qualitative user study to demonstrate the effectiveness of our approach. In short, this study offers a valuable contribution to addressing the challenges associated with ML modeling in the presence of missing data by providing a practical solution that achieves high predictive accuracy and model interpretability.
</details>
<details>
<summary>摘要</summary>
“缺失数据可能会对机器学习（ML）模型带来挑战。为了解决这个问题，现有的方法可以分为两种：特征填充和标签预测，它们主要是对缺失数据进行处理，以提高ML表现。这些方法将从观察到的数据中估算缺失的值，因此会遇到三个主要缺陷：需要不同的填充方法依照不同的缺失调制解调器制，依赖数据分布的假设，以及可能引入偏见。本研究提出了对缺失数据的对照学习（CL）框架，其中ML模型学习缺失数据中的相似性和不相似性。我们的提案方法不需要任何填充，并且可以提高可读性。为了增强可读性，我们导入了CIVis，一个可读性系统，它结合了可读技术来显示学习过程和诊断模型状态。用户可以透过互动采样来运用专业知识来选择负面和正面对照，而CIVis的输出则是一个已优化的模型，可以根据指定的特征进行下游任务预测。我们在回归和分类任务中提供了两个使用案例，并进行了量化实验、专家访谈和 качеitative用户研究，以证明我们的方法的有效性。简而言之，这个研究为ML模型在缺失数据下的挑战提供了实用的解决方案，可以实现高预测精度和模型可读性。”
</details></li>
</ul>
<hr>
<h2 id="The-NFLikelihood-an-unsupervised-DNNLikelihood-from-Normalizing-Flows"><a href="#The-NFLikelihood-an-unsupervised-DNNLikelihood-from-Normalizing-Flows" class="headerlink" title="The NFLikelihood: an unsupervised DNNLikelihood from Normalizing Flows"></a>The NFLikelihood: an unsupervised DNNLikelihood from Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09743">http://arxiv.org/abs/2309.09743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Humberto Reyes-Gonzalez, Riccardo Torre</li>
<li>for: 这个论文是为了探讨一种无监督的方法，基于正常化流程，来学习高维度的likelihood函数，具体来说是在高能物理分析中。</li>
<li>methods: 这个论文使用了自适应流程，基于 affine 和 rational quadratic spline 函数，来学习高维度的likelihood函数。</li>
<li>results: 论文通过实际例子示出，这种方法可以学习复杂的高维度likelihood函数，并且可以应用于高能物理分析中的几个实际问题。<details>
<summary>Abstract</summary>
We propose the NFLikelihood, an unsupervised version, based on Normalizing Flows, of the DNNLikelihood proposed in Ref.[1]. We show, through realistic examples, how Autoregressive Flows, based on affine and rational quadratic spline bijectors, are able to learn complicated high-dimensional Likelihoods arising in High Energy Physics (HEP) analyses. We focus on a toy LHC analysis example already considered in the literature and on two Effective Field Theory fits of flavor and electroweak observables, whose samples have been obtained throught the HEPFit code. We discuss advantages and disadvantages of the unsupervised approach with respect to the supervised one and discuss possible interplays of the two.
</details>
<details>
<summary>摘要</summary>
我们提出了NFLikelihood，一种无监督版本，基于归一化流，与Ref.[1]中提出的DNNLikelihood相似。我们通过实际的示例显示，使用自适应流，基于线性和quadratic spline bijectors，可以学习高维的Likelihood函数，出现在高能物理分析中。我们将focus on一个LHC分析示例，已经出现在文献中，以及两个Effective Field Theory的观测量 fits，其样本通过HEPFit代码获得。我们讨论无监督方法与监督方法之间的优劣点，以及两者之间的可能的互动。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-and-Data-Augmentation-in-Traffic-Classification-Using-a-Flowpic-Input-Representation"><a href="#Contrastive-Learning-and-Data-Augmentation-in-Traffic-Classification-Using-a-Flowpic-Input-Representation" class="headerlink" title="Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation"></a>Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09733">http://arxiv.org/abs/2309.09733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Finamore, Chao Wang, Jonatan Krolikowski, Jose M. Navarro, Fuxing Chen, Dario Rossi</li>
<li>for: 本研究是一篇关于交通分类（TC）的论文，采用了最新的深度学习（DL）方法。</li>
<li>methods: 本研究使用了少量学习、自我超vision via对比学习和数据增强等方法，以学习从少量样本中，并将学习结果转移到不同的数据集上。</li>
<li>results: 研究发现，使用这些DL方法，只需要使用100个输入样本，可以达到非常高的准确率，使用“流图”（i.e., 每个流量的2D histogram）作为输入表示。本研究还重现了原论文中的一些关键结果，并在三个额外的公共数据集上进行了数据增强的研究。<details>
<summary>Abstract</summary>
Over the last years we witnessed a renewed interest towards Traffic Classification (TC) captivated by the rise of Deep Learning (DL). Yet, the vast majority of TC literature lacks code artifacts, performance assessments across datasets and reference comparisons against Machine Learning (ML) methods. Among those works, a recent study from IMC'22 [17] is worth of attention since it adopts recent DL methodologies (namely, few-shot learning, self-supervision via contrastive learning and data augmentation) appealing for networking as they enable to learn from a few samples and transfer across datasets. The main result of [17] on the UCDAVIS19, ISCX-VPN and ISCX-Tor datasets is that, with such DL methodologies, 100 input samples are enough to achieve very high accuracy using an input representation called "flowpic" (i.e., a per-flow 2d histograms of the packets size evolution over time). In this paper (i) we reproduce [17] on the same datasets and (ii) we replicate its most salient aspect (the importance of data augmentation) on three additional public datasets, MIRAGE-19, MIRAGE-22 and UTMOBILENET21. While we confirm most of the original results, we also found a 20% accuracy drop on some of the investigated scenarios due to a data shift in the original dataset that we uncovered. Additionally, our study validates that the data augmentation strategies studied in [17] perform well on other datasets too. In the spirit of reproducibility and replicability we make all artifacts (code and data) available at [10].
</details>
<details>
<summary>摘要</summary>
过去几年，流行推理（TC）再次吸引了深度学习（DL）的关注。然而，大多数TC文献缺乏代码艺术ifacts，数据集之间的性能评估和对机器学习（ML）方法的参照比较。其中一项研究，IMC'22[17]，在网络领域引起了关注，因为它采用了当今DL技术（即少量学习、自我超视观察和数据扩展），这些技术可以通过几个样本学习并在数据集之间传递。这个研究的主要结果是，使用这些DL技术，只需要100个输入样本就可以达到非常高的准确率，使用名为"流图"（即每个流量2D histogram的包大小演化过时）的输入表示。在本文中，我们首先复制[17]中的主要方面（数据扩展的重要性）在三个公共数据集上进行了重复实验：MIRAGE-19、MIRAGE-22和UTMOBILENET21。我们证明了大部分原始结果的确认，但也发现了一些情况下的20%准确率下降，这是由原始数据集中的数据变换所致。此外，我们的研究还证明了在其他数据集上，[17]中研究的数据扩展策略也表现良好。为了保持可重复性和复制性，我们在[10]上公开了所有文件（代码和数据）。
</details></li>
</ul>
<hr>
<h2 id="Neural-Collapse-for-Unconstrained-Feature-Model-under-Cross-entropy-Loss-with-Imbalanced-Data"><a href="#Neural-Collapse-for-Unconstrained-Feature-Model-under-Cross-entropy-Loss-with-Imbalanced-Data" class="headerlink" title="Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data"></a>Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09725">http://arxiv.org/abs/2309.09725</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanlihongc/neural-collapse">https://github.com/wanlihongc/neural-collapse</a></li>
<li>paper_authors: Wanli Hong, Shuyang Ling</li>
<li>for: 这paper研究了不等式特征模型下的神经网络坍缩现象（Neural Collapse，NC）在不均衡数据上的扩展。</li>
<li>methods: 该paper使用了无约束特征模型（Unconstrained Feature Model，UFM）来解释NC现象。</li>
<li>results: 研究发现，在不均衡数据上，NC现象仍然存在，但是feature vectors内的坍缩现象不再是等角的，而是受样本大小的影响。此外，研究还发现了一个锐度的阈值，当阈值超过这个阈值时，小类坍缩（feature vectors of minority groups collapse to one single vector）会发生。最后，研究发现，随着样本大小的增加，数据不均衡的影响会逐渐减弱。<details>
<summary>Abstract</summary>
Recent years have witnessed the huge success of deep neural networks (DNNs) in various tasks of computer vision and text processing. Interestingly, these DNNs with massive number of parameters share similar structural properties on their feature representation and last-layer classifier at terminal phase of training (TPT). Specifically, if the training data are balanced (each class shares the same number of samples), it is observed that the feature vectors of samples from the same class converge to their corresponding in-class mean features and their pairwise angles are the same. This fascinating phenomenon is known as Neural Collapse (N C), first termed by Papyan, Han, and Donoho in 2019. Many recent works manage to theoretically explain this phenomenon by adopting so-called unconstrained feature model (UFM). In this paper, we study the extension of N C phenomenon to the imbalanced data under cross-entropy loss function in the context of unconstrained feature model. Our contribution is multi-fold compared with the state-of-the-art results: (a) we show that the feature vectors exhibit collapse phenomenon, i.e., the features within the same class collapse to the same mean vector; (b) the mean feature vectors no longer form an equiangular tight frame. Instead, their pairwise angles depend on the sample size; (c) we also precisely characterize the sharp threshold on which the minority collapse (the feature vectors of the minority groups collapse to one single vector) will take place; (d) finally, we argue that the effect of the imbalance in datasize diminishes as the sample size grows. Our results provide a complete picture of the N C under the cross-entropy loss for the imbalanced data. Numerical experiments confirm our theoretical analysis.
</details>
<details>
<summary>摘要</summary>
近年来，深度神经网络（DNN）在计算机视觉和自然语言处理等领域取得了巨大成功。意外的是，这些DNN具有庞大参数的结构性质在特定阶段训练（TPT）中的特征表示和最后一层分类器之间存在类似性。具体来说，如果训练数据均衡（每个类具有相同的样本数），则观察到样本从同一个类划分的特征向量相互吸引，其对角度保持相同。这种精彩的现象被称为神经塌缩（NC），由Papyan、Han和Donoho在2019年提出。许多最近的工作尝试理解这种现象，通过采用不受限制的特征模型（UFM）。在这篇论文中，我们研究了NC现象在不均衡数据下，使用交叉熵损失函数的情况。我们的贡献包括以下几点：（a）特征向量展现塌缩现象，即同一个类划分的特征向量塌缩到同一个均值向量；（b）均值特征向量不再形成等角紧凑框架，而是对样本大小具有相互关系的对角度；（c）我们也准确地描述了小于一定的阈值，下面的少数塌缩（特征向量少数组划分到一个向量）会发生的具体时间点；（d）最后，我们认为数据大小差异的影响随着样本大小的增长而减少。我们的结果为NC现象在交叉熵损失下的不均衡数据提供了完整的图像。数据实验证实了我们的理论分析。
</details></li>
</ul>
<hr>
<h2 id="FedLALR-Client-Specific-Adaptive-Learning-Rates-Achieve-Linear-Speedup-for-Non-IID-Data"><a href="#FedLALR-Client-Specific-Adaptive-Learning-Rates-Achieve-Linear-Speedup-for-Non-IID-Data" class="headerlink" title="FedLALR: Client-Specific Adaptive Learning Rates Achieve Linear Speedup for Non-IID Data"></a>FedLALR: Client-Specific Adaptive Learning Rates Achieve Linear Speedup for Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09719">http://arxiv.org/abs/2309.09719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Sun, Li Shen, Shixiang Chen, Jingwei Sun, Jing Li, Guangzhong Sun, Dacheng Tao</li>
<li>for: This paper focuses on improving the efficiency of federated learning, especially for training large-scale deep neural networks with heterogeneous data.</li>
<li>methods: The proposed method, FedLALR, adjusts the learning rate for each client based on local historical gradient squares and synchronized learning rates, which enables the method to converge and achieve linear speedup with respect to the number of clients.</li>
<li>results: The theoretical analysis and experimental results show that FedLALR outperforms several communication-efficient federated optimization methods in terms of convergence speed and scalability, and achieves promising results on CV and NLP tasks.<details>
<summary>Abstract</summary>
Federated learning is an emerging distributed machine learning method, enables a large number of clients to train a model without exchanging their local data. The time cost of communication is an essential bottleneck in federated learning, especially for training large-scale deep neural networks. Some communication-efficient federated learning methods, such as FedAvg and FedAdam, share the same learning rate across different clients. But they are not efficient when data is heterogeneous. To maximize the performance of optimization methods, the main challenge is how to adjust the learning rate without hurting the convergence. In this paper, we propose a heterogeneous local variant of AMSGrad, named FedLALR, in which each client adjusts its learning rate based on local historical gradient squares and synchronized learning rates. Theoretical analysis shows that our client-specified auto-tuned learning rate scheduling can converge and achieve linear speedup with respect to the number of clients, which enables promising scalability in federated optimization. We also empirically compare our method with several communication-efficient federated optimization methods. Extensive experimental results on Computer Vision (CV) tasks and Natural Language Processing (NLP) task show the efficacy of our proposed FedLALR method and also coincides with our theoretical findings.
</details>
<details>
<summary>摘要</summary>
归一学习是一种新般的分布式机器学习方法，允许大量客户端共同训练模型，无需交换本地数据。在归一学习中，通信时间成本是一个重要瓶颈，��pecially when training large-scale deep neural networks. Some communication-efficient federated learning methods, such as FedAvg and FedAdam, share the same learning rate across different clients. However, they are not efficient when data is heterogeneous. To maximize the performance of optimization methods, the main challenge is how to adjust the learning rate without hurting the convergence.在这篇论文中，我们提出了一种归一学习中的本地自适应学习率调整方法，称为FedLALR。每个客户端根据本地历史梯度平方和同步学习率进行自适应学习率调整。我们的客户端自定义自适应学习率调整策略可以使模型快速收敛，并且可以在客户端数量增加时实现线性的速度增长。我们还对几种通信效率高的联邦优化方法进行了比较实验。我们的实验结果表明，FedLALR方法可以在CV任务和NLP任务上达到良好的效果，并且与我们的理论预测相符。
</details></li>
</ul>
<hr>
<h2 id="Multi-Dictionary-Tensor-Decomposition"><a href="#Multi-Dictionary-Tensor-Decomposition" class="headerlink" title="Multi-Dictionary Tensor Decomposition"></a>Multi-Dictionary Tensor Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09717">http://arxiv.org/abs/2309.09717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxwell McNeil, Petko Bogdanov</li>
<li>for: 多方向数据的分析，如社交媒体、医疗、时空域等领域的数据分析</li>
<li>methods: 使用多字做 tensor decomposition 方法，利用各种数据驱动的假设来分解 tensor</li>
<li>results: 提出了一种多字做 tensor decomposition 框架（MDTD），可以利用外部structural信息来获得稀疏编码的 tensor 因子，并且可以处理大型稀疏tensor。实验表明，相比于字做法，MDTD 可以学习更简洁的模型，并且可以提高数据重建质量、缺失值填充质量和 tensor 维度的估计。同时，MDTD 的运行时间并不受影响，可以快速处理大型数据。<details>
<summary>Abstract</summary>
Tensor decomposition methods are popular tools for analysis of multi-way datasets from social media, healthcare, spatio-temporal domains, and others. Widely adopted models such as Tucker and canonical polyadic decomposition (CPD) follow a data-driven philosophy: they decompose a tensor into factors that approximate the observed data well. In some cases side information is available about the tensor modes. For example, in a temporal user-item purchases tensor a user influence graph, an item similarity graph, and knowledge about seasonality or trends in the temporal mode may be available. Such side information may enable more succinct and interpretable tensor decomposition models and improved quality in downstream tasks.   We propose a framework for Multi-Dictionary Tensor Decomposition (MDTD) which takes advantage of prior structural information about tensor modes in the form of coding dictionaries to obtain sparsely encoded tensor factors. We derive a general optimization algorithm for MDTD that handles both complete input and input with missing values. Our framework handles large sparse tensors typical to many real-world application domains. We demonstrate MDTD's utility via experiments with both synthetic and real-world datasets. It learns more concise models than dictionary-free counterparts and improves (i) reconstruction quality ($60\%$ fewer non-zero coefficients coupled with smaller error); (ii) missing values imputation quality (two-fold MSE reduction with up to orders of magnitude time savings) and (iii) the estimation of the tensor rank. MDTD's quality improvements do not come with a running time premium: it can decompose $19GB$ datasets in less than a minute. It can also impute missing values in sparse billion-entry tensors more accurately and scalably than state-of-the-art competitors.
</details>
<details>
<summary>摘要</summary>
tensor 分解方法是社交媒体、医疗、时空域等多维数据分析的流行工具。广泛采用的模型，如图克 decomposition 和 canonical polyadic decomposition (CPD) 采用数据驱动 philosophy：它们将 tensor  decomposed into factors that approximate observed data well。在某些情况下，tensor 模式上有侧信息可用，例如，在 temporal 用户 item 购买 tensor 中，用户影响图、item similarity graph 和 temporal 模式中的季节性或趋势信息可能可用。这些侧信息可能使 tensor 分解模型更简洁可读，改进下游任务质量。我们提出了一个 Multi-Dictionary Tensor Decomposition (MDTD) 框架，利用 tensor 模式上的编码字典来获得精简编码 tensor 因子。我们 deriv 一种通用优化算法 для MDTD，可以处理完整输入和 missing 值输入。我们的框架可以处理大量的巨大稀盐 tensor，通常出现在实际应用中。我们通过实验表明，MDTD 可以学习更简洁的模型，并提高（i）重建质量（60%  fewer non-zero coefficients 和 smaller error），（ii）缺失值插值质量（two-fold MSE reduction with up to orders of magnitude time savings）和（iii） tensor 级别的估计。MDTD 的质量改进不会带来运行时间开销：它可以在一分钟内分解 19GB 的数据。它还可以更准确和可扩展地插值缺失的 billion-entry  tensor than state-of-the-art 竞争对手。
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Data-driven-Methods-for-Adaptive-Forecasting-of-COVID-19-Cases"><a href="#A-Study-of-Data-driven-Methods-for-Adaptive-Forecasting-of-COVID-19-Cases" class="headerlink" title="A Study of Data-driven Methods for Adaptive Forecasting of COVID-19 Cases"></a>A Study of Data-driven Methods for Adaptive Forecasting of COVID-19 Cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09698">http://arxiv.org/abs/2309.09698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charithea Stylianides, Kleanthis Malialis, Panayiotis Kolios</li>
<li>for: 本研究旨在investigate数据驱动（学习、统计）方法，以适应COVID-19病毒传播的非站点性条件。</li>
<li>methods: 本研究使用数据驱动学习和统计方法，以incrementally更新模型，适应不断变化的病毒传播条件。</li>
<li>results: 实验结果表明，该方法在不同的病毒浪涌期内，可以提供高准确率的预测结果，并在疫情爆发时进行有效的预测。<details>
<summary>Abstract</summary>
Severe acute respiratory disease SARS-CoV-2 has had a found impact on public health systems and healthcare emergency response especially with respect to making decisions on the most effective measures to be taken at any given time. As demonstrated throughout the last three years with COVID-19, the prediction of the number of positive cases can be an effective way to facilitate decision-making. However, the limited availability of data and the highly dynamic and uncertain nature of the virus transmissibility makes this task very challenging. Aiming at investigating these challenges and in order to address this problem, this work studies data-driven (learning, statistical) methods for incrementally training models to adapt to these nonstationary conditions. An extensive empirical study is conducted to examine various characteristics, such as, performance analysis on a per virus wave basis, feature extraction, "lookback" window size, memory size, all for next-, 7-, and 14-day forecasting tasks. We demonstrate that the incremental learning framework can successfully address the aforementioned challenges and perform well during outbreaks, providing accurate predictions.
</details>
<details>
<summary>摘要</summary>
严重急性呼吸疾病SARS-CoV-2对公共卫生系统和医疗紧急应急响应有着深远的影响，尤其是在决定最有效的措施时采取决策。在过去三年的COVID-19疫情中，预测病例数量是一项有效的决策支持。然而，数据有限性和病毒传播性的高度动态和不确定性使得这项工作具有挑战性。本研究旨在调查这些挑战，并通过数据驱动（学习、统计）方法来适应这些非站点条件。我们进行了广泛的实践研究，包括精心分析不同的特征，如批处理大小、缓存大小、memory大小等，以及下一天、7天、14天预测任务的性能分析。我们示出了增量学习框架可以成功地解决上述挑战，并在爆发期间提供 precisions 的预测。
</details></li>
</ul>
<hr>
<h2 id="VULNERLIZER-Cross-analysis-Between-Vulnerabilities-and-Software-Libraries"><a href="#VULNERLIZER-Cross-analysis-Between-Vulnerabilities-and-Software-Libraries" class="headerlink" title="VULNERLIZER: Cross-analysis Between Vulnerabilities and Software Libraries"></a>VULNERLIZER: Cross-analysis Between Vulnerabilities and Software Libraries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09649">http://arxiv.org/abs/2309.09649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irdin Pekaric, Michael Felderer, Philipp Steinmüller</li>
<li>For: 本研究旨在提供一种新的漏洞检测方法，用于针对软件项目中的漏洞进行检测。* Methods: 本方法使用CVE和软件库数据，结合归一化算法生成漏洞和库之间的链接。此外，还进行模型训练，以更新分配的权重。* Results: 研究结果显示，使用VULNERLIZER方法可以准确预测未来可能出现漏洞的软件库，并达到预测精度75%或更高。<details>
<summary>Abstract</summary>
The identification of vulnerabilities is a continuous challenge in software projects. This is due to the evolution of methods that attackers employ as well as the constant updates to the software, which reveal additional issues. As a result, new and innovative approaches for the identification of vulnerable software are needed. In this paper, we present VULNERLIZER, which is a novel framework for cross-analysis between vulnerabilities and software libraries. It uses CVE and software library data together with clustering algorithms to generate links between vulnerabilities and libraries. In addition, the training of the model is conducted in order to reevaluate the generated associations. This is achieved by updating the assigned weights. Finally, the approach is then evaluated by making the predictions using the CVE data from the test set. The results show that the VULNERLIZER has a great potential in being able to predict future vulnerable libraries based on an initial input CVE entry or a software library. The trained model reaches a prediction accuracy of 75% or higher.
</details>
<details>
<summary>摘要</summary>
“找到漏洞是软件项目中不断的挑战。这是因为攻击者的方法不断发展以及软件不断更新，导致新的漏洞披露。为此，我们提出了一种新的漏洞识别框架——漏洞LIZER。它使用CVE和软件库数据，结合聚类算法生成漏洞和库之间的关联。此外，我们还进行了模型训练，以重新评估生成的关联。这是通过更新分配的权重来实现的。最后，我们对测试集中的CVE数据进行预测，结果显示，漏洞LIZER可以准确预测基于输入CVE记录或软件库的未来漏洞。训练模型的准确率达75%或更高。”
</details></li>
</ul>
<hr>
<h2 id="A-Discussion-on-Generalization-in-Next-Activity-Prediction"><a href="#A-Discussion-on-Generalization-in-Next-Activity-Prediction" class="headerlink" title="A Discussion on Generalization in Next-Activity Prediction"></a>A Discussion on Generalization in Next-Activity Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09618">http://arxiv.org/abs/2309.09618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luka Abb, Peter Pfeiffer, Peter Fettke, Jana-Rebecca Rehse</li>
<li>for: 本研究旨在评估深度学习技术在下一个活动预测中的效果，并提出了不同的预测场景，以促进未来研究的发展。</li>
<li>methods: 本研究使用了深度学习技术进行下一个活动预测，并评估了其预测性能使用公共可用事件日志。</li>
<li>results: 研究发现现有的评估方法带来很大的示例泄露问题，导致使用深度学习技术的预测方法并不如预期中效果好。<details>
<summary>Abstract</summary>
Next activity prediction aims to forecast the future behavior of running process instances. Recent publications in this field predominantly employ deep learning techniques and evaluate their prediction performance using publicly available event logs. This paper presents empirical evidence that calls into question the effectiveness of these current evaluation approaches. We show that there is an enormous amount of example leakage in all of the commonly used event logs, so that rather trivial prediction approaches perform almost as well as ones that leverage deep learning. We further argue that designing robust evaluations requires a more profound conceptual engagement with the topic of next-activity prediction, and specifically with the notion of generalization to new data. To this end, we present various prediction scenarios that necessitate different types of generalization to guide future research.
</details>
<details>
<summary>摘要</summary>
下一个活动预测目标是预测运行进程实例的未来行为。现有文献主要采用深度学习技术进行预测性能评估，使用公共可用事件日志进行评估。本文提供了实验证据，质疑现有评价方法的效果。我们发现所有常用的事件日志具有很大的示例泄露，使得基本的预测方法几乎与深度学习相当。我们还认为设计Robust评估需要更深刻的概念理解，特别是一致到新数据的总结。为此，我们提出了不同类型的预测场景，以引导未来研究。
</details></li>
</ul>
<hr>
<h2 id="Latent-assimilation-with-implicit-neural-representations-for-unknown-dynamics"><a href="#Latent-assimilation-with-implicit-neural-representations-for-unknown-dynamics" class="headerlink" title="Latent assimilation with implicit neural representations for unknown dynamics"></a>Latent assimilation with implicit neural representations for unknown dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09574">http://arxiv.org/abs/2309.09574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoyuan Li, Bin Dong, Pingwen Zhang</li>
<li>for: 这种研究是为了解决数据吸收中的高计算成本和数据维度问题。</li>
<li>methods: 该研究使用了新的抽象框架，即秘密吸收与卷积神经网络（LAINR），其中引入了圆形秘密神经表示（SINR）和数据驱动的神经网络 uncertainty 估计器。</li>
<li>results: 实验结果表明，与基于AutoEncoder的方法相比，LAINR在吸收过程中具有更高的精度和效率。<details>
<summary>Abstract</summary>
Data assimilation is crucial in a wide range of applications, but it often faces challenges such as high computational costs due to data dimensionality and incomplete understanding of underlying mechanisms. To address these challenges, this study presents a novel assimilation framework, termed Latent Assimilation with Implicit Neural Representations (LAINR). By introducing Spherical Implicit Neural Representations (SINR) along with a data-driven uncertainty estimator of the trained neural networks, LAINR enhances efficiency in assimilation process. Experimental results indicate that LAINR holds certain advantage over existing methods based on AutoEncoders, both in terms of accuracy and efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>数据融合在各种应用中是关键，但它经常遇到高计算成本的数据维度和下面机制的不完全理解。为解决这些挑战，本研究提出了一种新的融合框架，称为潜在融合（LAINR）。通过引入圆形潜在神经表示（SINR）以及基于训练神经网络的数据驱动 uncertainty 估计器，LAINR 提高了融合过程的效率。实验结果表明，LAINR 在比AutoEncoders 基于的方法上具有更高的准确性和效率。
</details></li>
</ul>
<hr>
<h2 id="New-Bounds-on-the-Accuracy-of-Majority-Voting-for-Multi-Class-Classification"><a href="#New-Bounds-on-the-Accuracy-of-Majority-Voting-for-Multi-Class-Classification" class="headerlink" title="New Bounds on the Accuracy of Majority Voting for Multi-Class Classification"></a>New Bounds on the Accuracy of Majority Voting for Multi-Class Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09564">http://arxiv.org/abs/2309.09564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Aeeneh, Nikola Zlatanov, Jiangshan Yu</li>
<li>for: 这个论文主要研究了多类别分类问题中的多数投票函数（MVF）的精度。</li>
<li>methods: 本论文使用了独立且非Identically分布的选民模型，并 derivated了MVF在多类别分类问题中的新上界。</li>
<li>results: 研究发现，在满足 certain conditions 的情况下，MVF在多类别分类问题中的误差率会指数减少到零，而在不满足这些条件的情况下，误差率会指数增长。此外，研究还发现了对真实分类算法的精度，其在best-case情况下可以达到小误差率，但在worst-case情况下可能高于MVF的误差率。<details>
<summary>Abstract</summary>
Majority voting is a simple mathematical function that returns the value that appears most often in a set. As a popular decision fusion technique, the majority voting function (MVF) finds applications in resolving conflicts, where a number of independent voters report their opinions on a classification problem. Despite its importance and its various applications in ensemble learning, data crowd-sourcing, remote sensing, and data oracles for blockchains, the accuracy of the MVF for the general multi-class classification problem has remained unknown. In this paper, we derive a new upper bound on the accuracy of the MVF for the multi-class classification problem. More specifically, we show that under certain conditions, the error rate of the MVF exponentially decays toward zero as the number of independent voters increases. Conversely, the error rate of the MVF exponentially grows with the number of independent voters if these conditions are not met.   We first explore the problem for independent and identically distributed voters where we assume that every voter follows the same conditional probability distribution of voting for different classes, given the true classification of the data point. Next, we extend our results for the case where the voters are independent but non-identically distributed. Using the derived results, we then provide a discussion on the accuracy of the truth discovery algorithms. We show that in the best-case scenarios, truth discovery algorithms operate as an amplified MVF and thereby achieve a small error rate only when the MVF achieves a small error rate, and vice versa, achieve a large error rate when the MVF also achieves a large error rate. In the worst-case scenario, the truth discovery algorithms may achieve a higher error rate than the MVF. Finally, we confirm our theoretical results using numerical simulations.
</details>
<details>
<summary>摘要</summary>
多数投票是一种简单的数学函数，返回集合中出现最多的值。作为一种受欢迎的决策融合技术，多数投票函数（MVF）在解决冲突、 ensemble learning、数据投票、远程感知和数据链等领域都有应用。 despite its importance and various applications, the accuracy of MVF for the general multi-class classification problem remains unknown. In this paper, we derive a new upper bound on the accuracy of MVF for the multi-class classification problem. Specifically, we show that under certain conditions, the error rate of MVF exponentially decays toward zero as the number of independent voters increases. Conversely, the error rate of MVF exponentially grows with the number of independent voters if these conditions are not met. We first explore the problem for independent and identically distributed voters, assuming that every voter follows the same conditional probability distribution of voting for different classes given the true classification of the data point. Next, we extend our results to the case where the voters are independent but non-identically distributed. Using the derived results, we then provide a discussion on the accuracy of truth discovery algorithms. We show that in the best-case scenarios, truth discovery algorithms operate as an amplified MVF and thereby achieve a small error rate only when the MVF achieves a small error rate, and vice versa, achieve a large error rate when the MVF also achieves a large error rate. In the worst-case scenario, truth discovery algorithms may achieve a higher error rate than MVF. Finally, we confirm our theoretical results using numerical simulations.
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Whisper-to-Enhance-Multi-Branched-Speech-Intelligibility-Prediction-Model-for-Hearing-Aids"><a href="#Utilizing-Whisper-to-Enhance-Multi-Branched-Speech-Intelligibility-Prediction-Model-for-Hearing-Aids" class="headerlink" title="Utilizing Whisper to Enhance Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids"></a>Utilizing Whisper to Enhance Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09548">http://arxiv.org/abs/2309.09548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryandhimas E. Zezario, Fei Chen, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao</li>
<li>for: 这个研究的目的是提高听力器扩展设备中的自动评估speech intelligibility的精度。</li>
<li>methods: 这个研究使用了一种改进后的多支分支扩展speech intelligibility预测模型，称为MBI-Net+和MBI-Net++。MBI-Net+使用Whisper嵌入来扩展语音特征，而MBI-Net++则使用了一个辅助任务来预测帧级和语录级的对象语音理解指标HASPI的分数。</li>
<li>results: 实验结果表明，MBI-Net++和MBI-Net+都比MBI-Net在多种指标上表现更好，而MBI-Net++还是MBI-Net+的更好。<details>
<summary>Abstract</summary>
Automated assessment of speech intelligibility in hearing aid (HA) devices is of great importance. Our previous work introduced a non-intrusive multi-branched speech intelligibility prediction model called MBI-Net, which achieved top performance in the Clarity Prediction Challenge 2022. Based on the promising results of the MBI-Net model, we aim to further enhance its performance by leveraging Whisper embeddings to enrich acoustic features. In this study, we propose two improved models, namely MBI-Net+ and MBI-Net++. MBI-Net+ maintains the same model architecture as MBI-Net, but replaces self-supervised learning (SSL) speech embeddings with Whisper embeddings to deploy cross-domain features. On the other hand, MBI-Net++ further employs a more elaborate design, incorporating an auxiliary task to predict frame-level and utterance-level scores of the objective speech intelligibility metric HASPI (Hearing Aid Speech Perception Index) and multi-task learning. Experimental results confirm that both MBI-Net++ and MBI-Net+ achieve better prediction performance than MBI-Net in terms of multiple metrics, and MBI-Net++ is better than MBI-Net+.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:自动评估听力器（HA）设备的听力 intelligibility 非常重要。我们之前的工作推出了一种非侵入式多支路听力 intelligibility 预测模型，称为 MBI-Net，在 Clarity Prediction Challenge 2022 中获得了优秀的成绩。基于 MBI-Net 模型的承诺 результа，我们希望进一步提高其性能，通过使用 Whisper 嵌入来增强语音特征。在这项研究中，我们提出了两种改进的模型，即 MBI-Net+ 和 MBI-Net++。MBI-Net+ 维持了同 MBI-Net 模型的同样结构，但是将 SSL 语音嵌入替换为 Whisper 嵌入，以便在不同频谱上进行跨频域特征的部署。而 MBI-Net++ 则进一步采用了更加复杂的设计，包括在对象听力指数 HASPI (Hearing Aid Speech Perception Index) 的帧级和语音级分数预测任务中使用多任务学习。实验结果表明，MBI-Net++ 和 MBI-Net+ 都在多个指标上超过 MBI-Net，而 MBI-Net++ 则是 MBI-Net+ 的更好。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Wasserstein-GANs-for-State-Preparation-at-Unseen-Points-of-a-Phase-Diagram"><a href="#Quantum-Wasserstein-GANs-for-State-Preparation-at-Unseen-Points-of-a-Phase-Diagram" class="headerlink" title="Quantum Wasserstein GANs for State Preparation at Unseen Points of a Phase Diagram"></a>Quantum Wasserstein GANs for State Preparation at Unseen Points of a Phase Diagram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09543">http://arxiv.org/abs/2309.09543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wiktor Jurasz, Christian B. Mendl</li>
<li>for: 本研究旨在扩展生成模型，特别是生成对抗网络（GANs）到量子域，并解决当前方法的局限性。</li>
<li>methods: 我们提出了一种新的混合类型-量子方法，基于量子沃氏赋形GANs，可以学习输入集中的测量期望函数，并生成未经见过的新状态。</li>
<li>results: 我们的方法可以生成新的状态，其测量期望函数遵循同一个下面函数，这些状态没有出现在输入集中。<details>
<summary>Abstract</summary>
Generative models and in particular Generative Adversarial Networks (GANs) have become very popular and powerful data generation tool. In recent years, major progress has been made in extending this concept into the quantum realm. However, most of the current methods focus on generating classes of states that were supplied in the input set and seen at the training time. In this work, we propose a new hybrid classical-quantum method based on quantum Wasserstein GANs that overcomes this limitation. It allows to learn the function governing the measurement expectations of the supplied states and generate new states, that were not a part of the input set, but which expectations follow the same underlying function.
</details>
<details>
<summary>摘要</summary>
生成模型，特别是生成对抗网络（GANs），在过去几年变得非常流行和强大，用于数据生成。然而，大多数当前方法仅能生成训练时提供的类别的状态。在这种情况下，我们提议一种新的混合类 quantum 方法，基于量子沃尔帕特 GANs，可以超越这一限制。它可以学习输入状态的测量预期函数，并生成没有在输入集中出现过的新状态，但是预期函数follows the same underlying function。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-SUPERB-Towards-A-Dynamic-Collaborative-and-Comprehensive-Instruction-Tuning-Benchmark-for-Speech"><a href="#Dynamic-SUPERB-Towards-A-Dynamic-Collaborative-and-Comprehensive-Instruction-Tuning-Benchmark-for-Speech" class="headerlink" title="Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech"></a>Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09510">http://arxiv.org/abs/2309.09510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dynamic-superb/dynamic-superb">https://github.com/dynamic-superb/dynamic-superb</a></li>
<li>paper_authors: Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee</li>
<li>for: This paper aims to present a benchmark for building universal speech models that can perform multiple tasks in a zero-shot fashion using instruction tuning.</li>
<li>methods: The paper proposes a benchmark called Dynamic-SUPERB, which combines 33 tasks and 22 datasets to provide comprehensive coverage of diverse speech tasks and harness instruction tuning. The paper also proposes several approaches to establish benchmark baselines, including the use of speech models, text language models, and the multimodal encoder.</li>
<li>results: The evaluation results show that while the baselines perform reasonably on seen tasks, they struggle with unseen ones. The paper also conducts an ablation study to assess the robustness and seek improvements in the performance.<details>
<summary>Abstract</summary>
Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.
</details>
<details>
<summary>摘要</summary>
To ensure comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute to the benchmark, facilitating its dynamic growth. Dynamic-SUPERB currently features 55 evaluation instances by combining 33 tasks and 22 datasets, providing a broad spectrum of dimensions for evaluation. We also propose several approaches to establish benchmark baselines, including the use of speech models, text language models, and the multimodal encoder.Evaluation results show that while these baselines perform reasonably well on seen tasks, they struggle with unseen ones. To improve performance, we conducted an ablation study to assess the robustness and seek improvements. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.Here's the translation in Simplified Chinese:文本语言模型已经展现出很强的零 shot 能力，可以通过提供良好的指令来泛化到未看过任务。然而，现有的语音处理研究主要集中在限定或特定的任务上，而lack of standardized benchmarks 使得不同方法之间的比较不公平。为此，我们提出了Dynamic-SUPERB，一个用于建立通用语音模型的 benchmark，可以通过指令调整来完成多个任务的零 shot 泛化。为确保语音任务的全面覆盖和利用指令调整，我们邀请社区参与合作，以便不断扩展 benchmark。Dynamic-SUPERB 目前已经包含了55个评估实例，通过组合 33 个任务和 22 个数据集，提供了广泛的维度评估。我们还提出了一些建立 benchmark 基准的方法，包括使用语音模型、文本语言模型和多模式Encoder。评估结果显示，虽然这些基准在看到任务上表现良好，但在未看到任务上表现不佳。为了提高性能，我们进行了一些剖析研究，以评估Robustness和寻找改进。我们将所有材料公开发布，并邀请研究人员一起合作项目，共同推动领域技术的发展。
</details></li>
</ul>
<hr>
<h2 id="Outlier-Insensitive-Kalman-Filtering-Theory-and-Applications"><a href="#Outlier-Insensitive-Kalman-Filtering-Theory-and-Applications" class="headerlink" title="Outlier-Insensitive Kalman Filtering: Theory and Applications"></a>Outlier-Insensitive Kalman Filtering: Theory and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09505">http://arxiv.org/abs/2309.09505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shunit Truzman, Guy Revach, Nir Shlezinger, Itzik Klein</li>
<li>for: 这篇论文是关于如何从含有噪音观测的动力系统中进行状态估计，以提高适用范围和精度。</li>
<li>methods: 本文提出了一个具有自适应能力的实时状态估计方法，可以快速处理含有噪音观测的动力系统，并且不需要调整参数。</li>
<li>results: 实验和场景评估表明，本文的方法能够对含有噪音观测的动力系统进行精确的状态估计，并且比于其他方法更具有抗错误性。<details>
<summary>Abstract</summary>
State estimation of dynamical systems from noisy observations is a fundamental task in many applications. It is commonly addressed using the linear Kalman filter (KF), whose performance can significantly degrade in the presence of outliers in the observations, due to the sensitivity of its convex quadratic objective function. To mitigate such behavior, outlier detection algorithms can be applied. In this work, we propose a parameter-free algorithm which mitigates the harmful effect of outliers while requiring only a short iterative process of the standard update step of the KF. To that end, we model each potential outlier as a normal process with unknown variance and apply online estimation through either expectation maximization or alternating maximization algorithms. Simulations and field experiment evaluations demonstrate competitive performance of our method, showcasing its robustness to outliers in filtering scenarios compared to alternative algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translates as:状态估计动力系统从噪声观测中是许多应用中的基本任务。通常使用线性卡尔曼筛（KF）来解决这个问题，但是在观测中出现异常值时，KF的性能会受到严重损害，因为它的对称二阶函数会变得敏感。为了解决这个问题，我们可以使用异常检测算法。在这种情况下，我们模型每个可能的异常为正常过程的不确定噪声，并通过在线估计来进行对其进行更新。这种方法不需要任何参数，仅需要短暂的迭代过程。在实验和场景评估中，我们的方法能够与其他算法相比赢得竞争优势，表明其对异常观测的抵抗力在筛码场景中较高。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Approaches-to-Predict-and-Detect-Early-Onset-of-Digital-Dermatitis-in-Dairy-Cows-using-Sensor-Data"><a href="#Machine-Learning-Approaches-to-Predict-and-Detect-Early-Onset-of-Digital-Dermatitis-in-Dairy-Cows-using-Sensor-Data" class="headerlink" title="Machine Learning Approaches to Predict and Detect Early-Onset of Digital Dermatitis in Dairy Cows using Sensor Data"></a>Machine Learning Approaches to Predict and Detect Early-Onset of Digital Dermatitis in Dairy Cows using Sensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10010">http://arxiv.org/abs/2309.10010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jennifer Magana, Dinu Gavojdian, Yakir Menachem, Teddy Lazebnik, Anna Zamansky, Amber Adams-Progar</li>
<li>for: 本研究旨在采用机器学习算法基于传感器行为数据早期发现和预测牛皮病（DD）。</li>
<li>methods: 本研究使用了机器学习模型，基于牛皮病症状出现的日期和时间，使用传感器数据进行预测和检测。</li>
<li>results: 研究发现，使用行为传感器数据预测和检测牛皮病的机器学习模型可达79%的准确率，预测牛皮病2天前的模型准确率为64%。这些机器学习模型可以帮助开发基于行为传感器数据的实时自动牛皮病监测和诊断工具，用于检测牛皮病的症状变化。<details>
<summary>Abstract</summary>
The aim of this study was to employ machine learning algorithms based on sensor behavior data for (1) early-onset detection of digital dermatitis (DD); and (2) DD prediction in dairy cows. With the ultimate goal to set-up early warning tools for DD prediction, which would than allow a better monitoring and management of DD under commercial settings, resulting in a decrease of DD prevalence and severity, while improving animal welfare. A machine learning model that is capable of predicting and detecting digital dermatitis in cows housed under free-stall conditions based on behavior sensor data has been purposed and tested in this exploratory study. The model for DD detection on day 0 of the appearance of the clinical signs has reached an accuracy of 79%, while the model for prediction of DD 2 days prior to the appearance of the first clinical signs has reached an accuracy of 64%. The proposed machine learning models could help to develop a real-time automated tool for monitoring and diagnostic of DD in lactating dairy cows, based on behavior sensor data under conventional dairy environments. Results showed that alterations in behavioral patterns at individual levels can be used as inputs in an early warning system for herd management in order to detect variances in health of individual cows.
</details>
<details>
<summary>摘要</summary>
The study proposed a machine learning model that can predict and detect DD in cows housed under free-stall conditions based on behavior sensor data. The model achieved an accuracy of 79% in detecting DD on day 0 of the appearance of clinical signs, and an accuracy of 64% in predicting DD 2 days prior to the first clinical signs.The results of the study showed that alterations in behavioral patterns at the individual level can be used as inputs in an early warning system for herd management to detect variances in the health of individual cows. The proposed machine learning models have the potential to develop a real-time automated tool for monitoring and diagnosis of DD in lactating dairy cows based on behavior sensor data under conventional dairy environments.
</details></li>
</ul>
<hr>
<h2 id="Face-Driven-Zero-Shot-Voice-Conversion-with-Memory-based-Face-Voice-Alignment"><a href="#Face-Driven-Zero-Shot-Voice-Conversion-with-Memory-based-Face-Voice-Alignment" class="headerlink" title="Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment"></a>Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09470">http://arxiv.org/abs/2309.09470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng-Yan Sheng, Yang Ai, Yan-Nian Chen, Zhen-Hua Ling</li>
<li>for: 这篇论文目标是提出一种基于 face 图像的零 shot 语音转换任务（zero-shot FaceVC），即将源 speaker 的语音特征转换到目标 speaker 的语音特征上，只使用目标 speaker 的单个 face 图像。</li>
<li>methods: 我们提议一种基于 memory-based face-voice 对应模块的 zero-shot FaceVC 方法，通过槽来对这两种模式进行对应，以 capture 语音特征从 face 图像中。我们还提出了一种混合超级visit策略，以解决voice conversion任务在训练和推断阶段之间的一贯性问题。</li>
<li>results: 通过广泛的实验，我们证明了我们提出的方法在 zero-shot FaceVC 任务中的优越性。我们还设计了系统的主观和客观评价指标，以全面评估homogeneity、多样性和一致性 controlled by face images。<details>
<summary>Abstract</summary>
This paper presents a novel task, zero-shot voice conversion based on face images (zero-shot FaceVC), which aims at converting the voice characteristics of an utterance from any source speaker to a newly coming target speaker, solely relying on a single face image of the target speaker. To address this task, we propose a face-voice memory-based zero-shot FaceVC method. This method leverages a memory-based face-voice alignment module, in which slots act as the bridge to align these two modalities, allowing for the capture of voice characteristics from face images. A mixed supervision strategy is also introduced to mitigate the long-standing issue of the inconsistency between training and inference phases for voice conversion tasks. To obtain speaker-independent content-related representations, we transfer the knowledge from a pretrained zero-shot voice conversion model to our zero-shot FaceVC model. Considering the differences between FaceVC and traditional voice conversion tasks, systematic subjective and objective metrics are designed to thoroughly evaluate the homogeneity, diversity and consistency of voice characteristics controlled by face images. Through extensive experiments, we demonstrate the superiority of our proposed method on the zero-shot FaceVC task. Samples are presented on our demo website.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文介绍了一个新的任务：基于面像的零shot语音转换（zero-shot FaceVC），该任务的目标是将任何来源说话人的语音特征转换为新来的目标说话人，只使用单个面像。为解决这个任务，作者们提出了一种面voice记忆基于的零shot FaceVC方法。这种方法利用了一种面voice记忆对应模块，将这两种模式相互对应，以捕捉面像中的语音特征。此外，作者们还提出了一种混合超级visión策略，以解决长期存在的语音转换任务的训练和推断阶段不一致问题。为了获得无关 speaker的内容相关表示，作者们将预训练的零shot语音转换模型的知识传递到了他们的零shot FaceVC模型。鉴于FaceVC和传统的语音转换任务之间的差异，作者们设计了系统的主观和客观评价指标，以全面评估零shot FaceVC模型的性能。通过广泛的实验，作者们证明了他们的提出的方法在零shot FaceVC任务中的优越性。详细的样例可以在他们的 Demo 网站上找到。
</details></li>
</ul>
<hr>
<h2 id="Active-anomaly-detection-based-on-deep-one-class-classification"><a href="#Active-anomaly-detection-based-on-deep-one-class-classification" class="headerlink" title="Active anomaly detection based on deep one-class classification"></a>Active anomaly detection based on deep one-class classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09465">http://arxiv.org/abs/2309.09465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkkim-home/AAD">https://github.com/mkkim-home/AAD</a></li>
<li>paper_authors: Minkyung Kim, Junsik Kim, Jongmin Yu, Jun Kyun Choi</li>
<li>for: 这 paper 是为了提高深度异常检测模型的训练效果而使用活动学习工具。</li>
<li>methods: 这 paper 使用了一种基于 adaptive boundary 的查询策略，以及一种 combining  noise contrastive estimation 和一类分类模型的 semi-supervised learning 方法。</li>
<li>results: 这 paper 在 seven 个异常检测数据集上分别采用了这两种方法，并分析了它们的效果。<details>
<summary>Abstract</summary>
Active learning has been utilized as an efficient tool in building anomaly detection models by leveraging expert feedback. In an active learning framework, a model queries samples to be labeled by experts and re-trains the model with the labeled data samples. It unburdens in obtaining annotated datasets while improving anomaly detection performance. However, most of the existing studies focus on helping experts identify as many abnormal data samples as possible, which is a sub-optimal approach for one-class classification-based deep anomaly detection. In this paper, we tackle two essential problems of active learning for Deep SVDD: query strategy and semi-supervised learning method. First, rather than solely identifying anomalies, our query strategy selects uncertain samples according to an adaptive boundary. Second, we apply noise contrastive estimation in training a one-class classification model to incorporate both labeled normal and abnormal data effectively. We analyze that the proposed query strategy and semi-supervised loss individually improve an active learning process of anomaly detection and further improve when combined together on seven anomaly detection datasets.
</details>
<details>
<summary>摘要</summary>
active learning 已经被用作一种高效的工具来建立异常检测模型，通过借助专家反馈来优化模型。在一个 active learning 框架中，一个模型会问选择要被标注的样本，然后使用这些标注的数据样本来重新训练模型。这不仅可以减少获取标注的数据量，还可以提高异常检测性能。然而，大多数现有的研究都是帮助专家标注最多的异常数据样本，这是一种优化的一类分类基于深度异常检测的方法。在这篇论文中，我们解决了 active learning 中的两个重要问题：查询策略和半supervised学习方法。首先，我们的查询策略选择的是一个适应边缘的不确定样本。其次，我们在训练一个一类分类模型时应用了噪声对照估算，以便同时使用标注的正常和异常数据来有效地捕捉一类分类模型。我们分析表明，我们提出的查询策略和半supervised损失函数分别提高了活动学习过程中的异常检测性能，并且当他们结合在一起时，可以更好地提高异常检测性能。我们在七个异常检测数据集上进行了分析。
</details></li>
</ul>
<hr>
<h2 id="CaT-Balanced-Continual-Graph-Learning-with-Graph-Condensation"><a href="#CaT-Balanced-Continual-Graph-Learning-with-Graph-Condensation" class="headerlink" title="CaT: Balanced Continual Graph Learning with Graph Condensation"></a>CaT: Balanced Continual Graph Learning with Graph Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09455">http://arxiv.org/abs/2309.09455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/superallen13/CaT-CGL">https://github.com/superallen13/CaT-CGL</a></li>
<li>paper_authors: Yilun Liu, Ruihong Qiu, Zi Huang</li>
<li>for: 本研究旨在解决 continual graph learning (CGL) 中的快速卡斯特罗菲特问题，提高模型的稳定性和性能。</li>
<li>methods: 该研究提出了一种 Condense and Train (CaT) 框架，包括对新来 graph 的压缩和存储，以及在内存中直接更新模型。</li>
<li>results: 实验结果表明，CaT 框架可以有效解决快速卡斯特罗菲特问题，并提高 CGL 的效果和效率。<details>
<summary>Abstract</summary>
Continual graph learning (CGL) is purposed to continuously update a graph model with graph data being fed in a streaming manner. Since the model easily forgets previously learned knowledge when training with new-coming data, the catastrophic forgetting problem has been the major focus in CGL. Recent replay-based methods intend to solve this problem by updating the model using both (1) the entire new-coming data and (2) a sampling-based memory bank that stores replayed graphs to approximate the distribution of historical data. After updating the model, a new replayed graph sampled from the incoming graph will be added to the existing memory bank. Despite these methods are intuitive and effective for the CGL, two issues are identified in this paper. Firstly, most sampling-based methods struggle to fully capture the historical distribution when the storage budget is tight. Secondly, a significant data imbalance exists in terms of the scales of the complex new-coming graph data and the lightweight memory bank, resulting in unbalanced training. To solve these issues, a Condense and Train (CaT) framework is proposed in this paper. Prior to each model update, the new-coming graph is condensed to a small yet informative synthesised replayed graph, which is then stored in a Condensed Graph Memory with historical replay graphs. In the continual learning phase, a Training in Memory scheme is used to update the model directly with the Condensed Graph Memory rather than the whole new-coming graph, which alleviates the data imbalance problem. Extensive experiments conducted on four benchmark datasets successfully demonstrate superior performances of the proposed CaT framework in terms of effectiveness and efficiency. The code has been released on https://github.com/superallen13/CaT-CGL.
</details>
<details>
<summary>摘要</summary>
kontinuous graf lerning (CGL) 是为了不断更新一个 graf 模型，使其能够处理流动式的 graf 数据。由于模型容易忘记先前学习的知识，因此 catastrophic forgetting 问题成为了 CGL 的主要关注点。latest replay-based methods 尝试解决这个问题，通过将模型更新使用整个新来的数据和一个储存 replayed graphs 的 memory bank，以便 aproximate 历史数据的分布。在更新模型后，将新的 replayed graph 从进来的 graf 中抽出来，并添加到现有的 memory bank 中。although 这些方法是直觉和有效的，这篇文章中提出了两个问题。首先，大多数抽样方法在存储预算仅仅允许的情况下，很难全面捕捉历史分布。其次，进来的新数据和储存在 memory bank 中的数据 scale 不对称，导致训练不平衡。为解决这些问题，这篇文章提出了一个 Condense and Train (CaT) 框架。在每次模型更新之前，新来的 graf 会被压缩成一个小而有用的 synthesized replayed graph，并将其储存在 Condensed Graph Memory 中。在持续学习阶段，使用 Train in Memory 方法将模型直接更新使用 Condensed Graph Memory，而不是整个新来的 graf。实际实验在四个 benchmark 数据集上显示，CaT 框架在效率和效果上具有优越的表现。code 已经在 https://github.com/superallen13/CaT-CGL 发布。
</details></li>
</ul>
<hr>
<h2 id="Asymptotically-Efficient-Online-Learning-for-Censored-Regression-Models-Under-Non-I-I-D-Data"><a href="#Asymptotically-Efficient-Online-Learning-for-Censored-Regression-Models-Under-Non-I-I-D-Data" class="headerlink" title="Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data"></a>Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09454">http://arxiv.org/abs/2309.09454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lantian Zhang, Lei Guo</li>
<li>for:  investigate the asymptotically efficient online learning problem for stochastic censored regression models.</li>
<li>methods: propose a two-step online algorithm, which achieves algorithm convergence and improves estimation performance.</li>
<li>results: show that the algorithm is strongly consistent and asymptotically normal, and the covariances of the estimates can achieve the Cramer-Rao bound asymptotically.<details>
<summary>Abstract</summary>
The asymptotically efficient online learning problem is investigated for stochastic censored regression models, which arise from various fields of learning and statistics but up to now still lacks comprehensive theoretical studies on the efficiency of the learning algorithms. For this, we propose a two-step online algorithm, where the first step focuses on achieving algorithm convergence, and the second step is dedicated to improving the estimation performance. Under a general excitation condition on the data, we show that our algorithm is strongly consistent and asymptotically normal by employing the stochastic Lyapunov function method and limit theories for martingales. Moreover, we show that the covariances of the estimates can achieve the Cramer-Rao (C-R) bound asymptotically, indicating that the performance of the proposed algorithm is the best possible that one can expect in general. Unlike most of the existing works, our results are obtained without resorting to the traditionally used but stringent conditions such as independent and identically distributed (i.i.d) assumption on the data, and thus our results do not exclude applications to stochastic dynamical systems with feedback. A numerical example is also provided to illustrate the superiority of the proposed online algorithm over the existing related ones in the literature.
</details>
<details>
<summary>摘要</summary>
“ Stochastic censored regression 模型中的 asymptotically 高效在线学习问题被研究。这种模型在学习和统计领域中有很多应用，但是现在还没有完整的理论研究。为解决这个问题，我们提出了一个两步在线算法，其中第一步是使算法快速收敛，第二步是提高估计性能。在数据中的普通激动情况下，我们使用杰尼尼抽象函数方法和 martingale 限论来证明我们的算法是强连续和强conv的。此外，我们还证明了估计的协方差可以在极限下达到 Cramer-Rao (C-R)  bound，这表示我们的算法在总体上具有最佳的性能。不同于大多数现有的研究，我们的结果不需要 resorting to i.i.d 假设，因此我们的结果不排除应用于Stochastic dynamical systems with feedback。一个数字Example 也是提供以 Illustrate 我们的在线算法在 literatura 中的优越性。”Note: Please keep in mind that the translation is done by a machine and may not be perfect, especially for idiomatic expressions or cultural references.
</details></li>
</ul>
<hr>
<h2 id="On-the-Use-of-the-Kantorovich-Rubinstein-Distance-for-Dimensionality-Reduction"><a href="#On-the-Use-of-the-Kantorovich-Rubinstein-Distance-for-Dimensionality-Reduction" class="headerlink" title="On the Use of the Kantorovich-Rubinstein Distance for Dimensionality Reduction"></a>On the Use of the Kantorovich-Rubinstein Distance for Dimensionality Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09442">http://arxiv.org/abs/2309.09442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaël Giordano</li>
<li>for: 这个论文的目的是研究使用康托罗维奇-鲁宾逊距离来建立分类问题中的样本复杂度描述器。</li>
<li>methods: 这篇论文使用了康托罗维奇-鲁宾逊距离来量化样本之间的geometry和topology信息，并将每个类别的点关联到一个措施中。</li>
<li>results: 论文表明，如果康托罗维奇-鲁宾逊距离 между这些措施较大，则存在一个1-Lipschitz分类器，可以良好地分类这些点。<details>
<summary>Abstract</summary>
The goal of this thesis is to study the use of the Kantorovich-Rubinstein distance as to build a descriptor of sample complexity in classification problems. The idea is to use the fact that the Kantorovich-Rubinstein distance is a metric in the space of measures that also takes into account the geometry and topology of the underlying metric space. We associate to each class of points a measure and thus study the geometrical information that we can obtain from the Kantorovich-Rubinstein distance between those measures. We show that a large Kantorovich-Rubinstein distance between those measures allows to conclude that there exists a 1-Lipschitz classifier that classifies well the classes of points. We also discuss the limitation of the Kantorovich-Rubinstein distance as a descriptor.
</details>
<details>
<summary>摘要</summary>
本论文的目标是研究使用庞特罗维奇-鲁比涅斯坦距离来建立分类问题中的样本复杂性描述器。我们使用庞特罗维奇-鲁比涅斯坦距离是度量空间概率的度量，同时也考虑度量空间的几何和 topology。我们对每个类别点分配一个概率，并研究庞特罗维奇-鲁比涅斯坦距离这些概率之间的几何信息。我们显示，当庞特罗维奇-鲁比涅斯坦距离大于某个阈值时，存在1-Lipschitz分类器可以良好地分类点类。我们还讨论了庞特罗维奇-鲁比涅斯坦距离作为描述器的限制。
</details></li>
</ul>
<hr>
<h2 id="DeepHEN-quantitative-prediction-essential-lncRNA-genes-and-rethinking-essentialities-of-lncRNA-genes"><a href="#DeepHEN-quantitative-prediction-essential-lncRNA-genes-and-rethinking-essentialities-of-lncRNA-genes" class="headerlink" title="DeepHEN: quantitative prediction essential lncRNA genes and rethinking essentialities of lncRNA genes"></a>DeepHEN: quantitative prediction essential lncRNA genes and rethinking essentialities of lncRNA genes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10008">http://arxiv.org/abs/2309.10008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanlin Zhang, Wenzheng Cheng</li>
<li>for: 本研究旨在解释非编码蛋白质基因的必需性。</li>
<li>methods: 该研究使用深度学习和图神经网络来预测非编码蛋白质基因的必需性。</li>
<li>results: 该模型能够预测非编码蛋白质基因的必需性，并能够区分序列特征和网络空间特征对必需性的影响。此外，该模型还能够解决其他方法因为必需性蛋白质基因数量低而导致的过拟合问题。<details>
<summary>Abstract</summary>
Gene essentiality refers to the degree to which a gene is necessary for the survival and reproductive efficacy of a living organism. Although the essentiality of non-coding genes has been documented, there are still aspects of non-coding genes' essentiality that are unknown to us. For example, We do not know the contribution of sequence features and network spatial features to essentiality. As a consequence, in this work, we propose DeepHEN that could answer the above question. By buidling a new lncRNA-proteion-protein network and utilizing both representation learning and graph neural network, we successfully build our DeepHEN models that could predict the essentiality of lncRNA genes. Compared to other methods for predicting the essentiality of lncRNA genes, our DeepHEN model not only tells whether sequence features or network spatial features have a greater influence on essentiality but also addresses the overfitting issue of those methods caused by the low number of essential lncRNA genes, as evidenced by the results of enrichment analysis.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:基因必需性指的是生物体的存活和繁殖能力中基因的必需程度。虽然非编码基因的必需性已经记录，但还有一些非编码基因的必需性还未知。例如，我们不知道序列特征和网络空间特征对必需性的贡献。因此，在这项工作中，我们提出了深度感知（DeepHEN），可以回答上述问题。我们通过构建新的lncRNA-蛋白质-蛋白质网络和使用表示学习和图神经网络，成功建立了我们的深度感知模型，可以预测lncRNA基因的必需性。与其他预测lncRNA基因必需性的方法相比，我们的深度感知模型不仅可以评估序列特征和网络空间特征对必需性的影响，还可以解决这些方法因为低数量必需lncRNA基因而导致的过拟合问题，根据结果分析中的恰合分析结果。
</details></li>
</ul>
<hr>
<h2 id="An-Iterative-Method-for-Unsupervised-Robust-Anomaly-Detection-Under-Data-Contamination"><a href="#An-Iterative-Method-for-Unsupervised-Robust-Anomaly-Detection-Under-Data-Contamination" class="headerlink" title="An Iterative Method for Unsupervised Robust Anomaly Detection Under Data Contamination"></a>An Iterative Method for Unsupervised Robust Anomaly Detection Under Data Contamination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09436">http://arxiv.org/abs/2309.09436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minkyung Kim, Jongmin Yu, Junsik Kim, Tae-Hyun Oh, Jun Kyun Choi<br>for:这个论文的目的是提高深入型异常检测模型的Robustness，使其能够更好地适应实际数据分布中的异常tail。methods:该论文提出了一种学习框架，通过iteratively更新样本级别的正常性权重，以提高深入型异常检测模型的学习效果。该框架是模型无关和参数适应的，可以应用于现有的异常检测方法。results:在五个异常检测benchmark dataset和两个图像 dataset上，该框架能够提高异常检测模型的Robustness，并且在不同的杂杂度水平下表现出优于现有的异常检测方法。<details>
<summary>Abstract</summary>
Most deep anomaly detection models are based on learning normality from datasets due to the difficulty of defining abnormality by its diverse and inconsistent nature. Therefore, it has been a common practice to learn normality under the assumption that anomalous data are absent in a training dataset, which we call normality assumption. However, in practice, the normality assumption is often violated due to the nature of real data distributions that includes anomalous tails, i.e., a contaminated dataset. Thereby, the gap between the assumption and actual training data affects detrimentally in learning of an anomaly detection model. In this work, we propose a learning framework to reduce this gap and achieve better normality representation. Our key idea is to identify sample-wise normality and utilize it as an importance weight, which is updated iteratively during the training. Our framework is designed to be model-agnostic and hyperparameter insensitive so that it applies to a wide range of existing methods without careful parameter tuning. We apply our framework to three different representative approaches of deep anomaly detection that are classified into one-class classification-, probabilistic model-, and reconstruction-based approaches. In addition, we address the importance of a termination condition for iterative methods and propose a termination criterion inspired by the anomaly detection objective. We validate that our framework improves the robustness of the anomaly detection models under different levels of contamination ratios on five anomaly detection benchmark datasets and two image datasets. On various contaminated datasets, our framework improves the performance of three representative anomaly detection methods, measured by area under the ROC curve.
</details>
<details>
<summary>摘要</summary>
大多数深度异常检测模型基于学习正常性从数据集中学习，由于异常性的多样性和不一致性，因此通常采用学习正常性假设。然而，在实际应用中，正常性假设经常被违反，因为真实数据分布包含异常尾部，即杂杂数据集。这会导致学习异常检测模型的时候， gap between假设和实际训练数据产生负面影响。在这种情况下，我们提出了一种学习框架，以减少这个 gap 并达到更好的正常性表示。我们的关键思想是在样本级别上确定正常性，并将其作为重要性Weight使用，这个Weight在训练过程中进行迭代更新。我们的框架采用模型无关和 гипер参数敏感的设计，可以应用于各种现有方法无需精心参数调整。我们在三种不同的深度异常检测方法上应用了我们的框架，分别是一类分类-, 概率模型-和重建基于的方法。此外，我们还考虑了异常检测目标中的终止条件，并提出了基于异常检测目标的终止 criterion。我们验证了我们的框架可以在不同的杂杂比例下提高异常检测模型的Robustness，并在五个异常检测benchmark datasets和两个图像 datasets上进行验证。在各种杂杂数据集上，我们的框架可以提高三种代表性异常检测方法的性能， measured by area under the ROC curve。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Time-Varying-Online-Stochastic-Optimization-under-Polyak-Lojasiewicz-Condition-with-Application-in-Conditional-Value-at-Risk-Statistical-Learning"><a href="#Distributionally-Time-Varying-Online-Stochastic-Optimization-under-Polyak-Lojasiewicz-Condition-with-Application-in-Conditional-Value-at-Risk-Statistical-Learning" class="headerlink" title="Distributionally Time-Varying Online Stochastic Optimization under Polyak-Łojasiewicz Condition with Application in Conditional Value-at-Risk Statistical Learning"></a>Distributionally Time-Varying Online Stochastic Optimization under Polyak-Łojasiewicz Condition with Application in Conditional Value-at-Risk Statistical Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09411">http://arxiv.org/abs/2309.09411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuen-Man Pun, Farhad Farokhi, Iman Shames</li>
<li>for: 这个论文研究了一系列随机优化问题，通过在线优化的视角来探讨。</li>
<li>methods: 论文使用了在线随机梯度 DESCENT 和在线随机 proximal 梯度 DESCENT，并且为这些方法提供了动态 regret bound。</li>
<li>results: 论文证明了在线随机梯度 DESCENT 和在线随机 proximal 梯度 DESCENT 的动态 regret bound，并且应用到了 Conditional Value-at-Risk (CVaR) 学习问题。<details>
<summary>Abstract</summary>
In this work, we consider a sequence of stochastic optimization problems following a time-varying distribution via the lens of online optimization. Assuming that the loss function satisfies the Polyak-{\L}ojasiewicz condition, we apply online stochastic gradient descent and establish its dynamic regret bound that is composed of cumulative distribution drifts and cumulative gradient biases caused by stochasticity. The distribution metric we adopt here is Wasserstein distance, which is well-defined without the absolute continuity assumption or with a time-varying support set. We also establish a regret bound of online stochastic proximal gradient descent when the objective function is regularized. Moreover, we show that the above framework can be applied to the Conditional Value-at-Risk (CVaR) learning problem. Particularly, we improve an existing proof on the discovery of the PL condition of the CVaR problem, resulting in a regret bound of online stochastic gradient descent.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们考虑一个时间变化分布下的随机优化问题序列，通过在线优化的镜头来分析。我们假设损失函数满足波Яakov-{\L}ojasiewicz条件，我们应用在线随机梯度下降并确定其动态 regret bound，该 regret bound包括累积分布漂移和累积梯度偏差由随机性引起的。我们采用的分布度量是沃氏距离，这是不含绝对连续性假设或时间变化支持集的。此外，我们还证明在线随机距离梯度下降可以应用于 conditional Value-at-Risk（CVaR）学习问题。特别是，我们改进了现有的PL条件发现证明，从而获得在线随机梯度下降的 regret bound。
</details></li>
</ul>
<hr>
<h2 id="Guided-Online-Distillation-Promoting-Safe-Reinforcement-Learning-by-Offline-Demonstration"><a href="#Guided-Online-Distillation-Promoting-Safe-Reinforcement-Learning-by-Offline-Demonstration" class="headerlink" title="Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration"></a>Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09408">http://arxiv.org/abs/2309.09408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinning Li, Xinyi Liu, Banghua Zhu, Jiantao Jiao, Masayoshi Tomizuka, Chen Tang, Wei Zhan</li>
<li>for: 提高安全性和效率的强化学习（Reinforcement Learning）策略，使得RL agents可以在具有成本限制的情况下获得高奖励。</li>
<li>methods: 使用大容量模型（如决策变换器DT）来学习离线数据中的专家策略，并通过指导在线安全RL训练来策略减小。</li>
<li>results: GOLD框架可以成功减小离线DT策略，并在实际执行时在安全性和效率两个方面表现出色，在多种安全关键的scenario中解决决策问题。<details>
<summary>Abstract</summary>
Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in real-world scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), an offline-to-online safe RL framework. GOLD distills an offline DT policy into a lightweight policy network through guided online safe RL training, which outperforms both the offline DT policy and online safe RL algorithms. Experiments in both benchmark safe RL tasks and real-world driving tasks based on the Waymo Open Motion Dataset (WOMD) demonstrate that GOLD can successfully distill lightweight policies and solve decision-making problems in challenging safety-critical scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in real-world scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), an offline-to-online safe RL framework. GOLD distills an offline DT policy into a lightweight policy network through guided online safe RL training, which outperforms both the offline DT policy and online safe RL algorithms. Experiments in both benchmark safe RL tasks and real-world driving tasks based on the Waymo Open Motion Dataset (WOMD) demonstrate that GOLD can successfully distill lightweight policies and solve decision-making problems in challenging safety-critical scenarios."中文翻译：<<SYS>>safe reinforcement learning（RL）的目标是找到一个政策，以高的奖励来满足成本限制。当学习从头开始时，safe RL 代理人往往太保守，这会阻碍探索和限制整体性能。在许多现实任务中，例如自动驾驶，大规模专家示范数据可以用于指导在线探索。我们认为从 offline 数据中提取专家策略来指导在线探索是一个有前途的解决方案，以减少保守性问题。大容量模型，如决策变换器（DT），已经在 offline 政策学习中证明自己的能力。然而，实际场景中收集的数据 редarily 包含危险情况（例如，相撞），这使得政策学习危险概念的缺乏。此外，这些大型政策网络不能在实际任务中实时进行计算，例如自动驾驶。为此，我们提出了 Guided Online Distillation（GOLD），一个 offline-to-online safe RL 框架。GOLD 通过在线安全 RL 培训来精炼 offline DT 政策，并且超过了 offline DT 政策和在线安全 RL 算法。在标准 safe RL 任务和基于 Waymo Open Motion Dataset（WOMD）的实际驾驶任务中，GOLD 可以成功精炼轻量级政策并解决安全关键问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/cs.LG_2023_09_18/" data-id="clogxf3ok00op5xra2gnhgpla" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/eess.IV_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T09:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/eess.IV_2023_09_18/">eess.IV - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mixed-Graph-Signal-Analysis-of-Joint-Image-Denoising-Interpolation"><a href="#Mixed-Graph-Signal-Analysis-of-Joint-Image-Denoising-Interpolation" class="headerlink" title="Mixed Graph Signal Analysis of Joint Image Denoising &#x2F; Interpolation"></a>Mixed Graph Signal Analysis of Joint Image Denoising &#x2F; Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10114">http://arxiv.org/abs/2309.10114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niruhan Viswarupan, Gene Cheung, Fengbo Lan, Michael Brown</li>
<li>for: 这个论文主要是研究如何jointly optimize denoising and interpolation of images from a mixed graph filtering perspective.</li>
<li>methods: 作者使用了linear denoiser和linear interpolator，并研究了在哪些情况下这两个操作应该独立执行，或者合并并且优化。</li>
<li>results: 实验表明，作者的 JOINT denoising &#x2F; interpolation方法在比较不同的情况下都能够显著超过独立的方法。<details>
<summary>Abstract</summary>
A noise-corrupted image often requires interpolation. Given a linear denoiser and a linear interpolator, when should the operations be independently executed in separate steps, and when should they be combined and jointly optimized? We study joint denoising / interpolation of images from a mixed graph filtering perspective: we model denoising using an undirected graph, and interpolation using a directed graph. We first prove that, under mild conditions, a linear denoiser is a solution graph filter to a maximum a posteriori (MAP) problem regularized using an undirected graph smoothness prior, while a linear interpolator is a solution to a MAP problem regularized using a directed graph smoothness prior. Next, we study two variants of the joint interpolation / denoising problem: a graph-based denoiser followed by an interpolator has an optimal separable solution, while an interpolator followed by a denoiser has an optimal non-separable solution. Experiments show that our joint denoising / interpolation method outperformed separate approaches noticeably.
</details>
<details>
<summary>摘要</summary>
受噪图像经常需要 interpolate。给定一个线性去噪器和一个线性 interpolator，当should these operations be independently executed in separate steps，和when should they be combined and jointly optimized？我们研究图像 joint denoising / interpolation from a mixed graph filtering perspective：我们模型denoising using an undirected graph，并 interpolating using a directed graph。我们首先证明，在某些条件下，一个线性去噪器是一个解 graph filter to a maximum a posteriori (MAP) problem regularized using an undirected graph smoothness prior，而一个线性 interpolator是一个解 to a MAP problem regularized using a directed graph smoothness prior。接着，我们研究了两种 joint interpolation / denoising problem variant：一个图像-based denoiser followed by an interpolator has an optimal separable solution，而一个 interpolator followed by a denoiser has an optimal non-separable solution。实验表明，我们的 joint denoising / interpolation method noticeably outperformed separate approaches。
</details></li>
</ul>
<hr>
<h2 id="MAD-Meta-Adversarial-Defense-Benchmark"><a href="#MAD-Meta-Adversarial-Defense-Benchmark" class="headerlink" title="MAD: Meta Adversarial Defense Benchmark"></a>MAD: Meta Adversarial Defense Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09776">http://arxiv.org/abs/2309.09776</a></li>
<li>repo_url: None</li>
<li>paper_authors: X. Peng, D. Zhou, G. Sun, J. Shi, L. Wu</li>
<li>For:	+ The paper aims to address the limitations of existing adversarial training (AT) methods, such as high computational cost, low generalization ability, and the dilemma between the original model and the defense model.* Methods:	+ The paper proposes a novel benchmark called meta adversarial defense (MAD), which consists of two MAD datasets and a MAD evaluation protocol.	+ The paper introduces a meta-learning based adversarial training (Meta-AT) algorithm as the baseline, which features high robustness to unseen adversarial attacks through few-shot learning.* Results:	+ The paper shows that the Meta-AT algorithm outperforms state-of-the-art methods in terms of robustness to adversarial attacks.	+ The paper also demonstrates that the model after Meta-AT maintains a relatively high clean-samples classification accuracy (CCA).Here is the simplified Chinese text for the three key information points:* For:	+ 本文目标是解决现有的针对性训练（AT）方法存在的三大限制，包括高计算成本、低泛化能力和模型与防御模型之间的矛盾。* Methods:	+ 本文提出了一个新的静态防御（MAD） benchmark，包括两个 MAD 数据集和一个 MAD 评估协议。	+ 本文引入了一种基于 meta-学 的针对性训练（Meta-AT）算法作为基准，该算法通过几个 adversarial 攻击实现了高度的鲁棒性。* Results:	+ 本文表明 Meta-AT 算法在针对性攻击方面的性能明显超过了现有的方法。	+ 本文还表明模型 после Meta-AT 保持了相对较高的清样分类精度（CCA）。<details>
<summary>Abstract</summary>
Adversarial training (AT) is a prominent technique employed by deep learning models to defend against adversarial attacks, and to some extent, enhance model robustness. However, there are three main drawbacks of the existing AT-based defense methods: expensive computational cost, low generalization ability, and the dilemma between the original model and the defense model. To this end, we propose a novel benchmark called meta adversarial defense (MAD). The MAD benchmark consists of two MAD datasets, along with a MAD evaluation protocol. The two large-scale MAD datasets were generated through experiments using 30 kinds of attacks on MNIST and CIFAR-10 datasets. In addition, we introduce a meta-learning based adversarial training (Meta-AT) algorithm as the baseline, which features high robustness to unseen adversarial attacks through few-shot learning. Experimental results demonstrate the effectiveness of our Meta-AT algorithm compared to the state-of-the-art methods. Furthermore, the model after Meta-AT maintains a relatively high clean-samples classification accuracy (CCA). It is worth noting that Meta-AT addresses all three aforementioned limitations, leading to substantial improvements. This benchmark ultimately achieved breakthroughs in investigating the transferability of adversarial defense methods to new attacks and the ability to learn from a limited number of adversarial examples. Our codes and attacked datasets address will be available at https://github.com/PXX1110/Meta_AT.
</details>
<details>
<summary>摘要</summary>
<SYS> translate_text=" Adversarial 训练（AT）是深度学习模型防御 против敌意攻击的一种常见技术，同时可以提高模型的鲁棒性。然而，现有的 AT 防御方法存在三个主要缺点：计算成本高、适应能力低、模型与防御模型之间的 contradicton。为此，我们提出了一个新的benchmark，即元敌意防御（MAD）。MAD  benchmark 包括两个 MAD 数据集，以及一个 MAD 评估协议。这两个大规模 MAD 数据集通过对 MNIST 和 CIFAR-10 数据集进行实验而生成。此外，我们引入了基于元学习的 adversarial 训练（Meta-AT）算法，它具有高度鲁棒性，能够通过少量学习对不同攻击来鲁棒化。实验结果表明，我们的 Meta-AT 算法与现状卷积方法相比，有较高的鲁棒性。此外，模型 después Meta-AT 保持了较高的清样分类精度（CCA）。值得注意的是，Meta-AT 解决了所有三种限制，带来了显著的改进。这个benchmark最终实现了对敌意防御方法的传播性和从少量敌意示例中学习的能力进行研究。我们的代码和攻击数据集将在 <https://github.com/PXX1110/Meta_AT> 上发布。</SYS>Here's the translation:这是一个文章，描述了一个新的benchmark，即元敌意防御（MAD），用于测试深度学习模型的防御能力。现有的防御方法有三个主要缺点：计算成本高、适应能力低、模型与防御模型之间的矛盾。为此，我们提出了一个新的Meta-AT算法，具有高度鲁棒性，能够通过少量学习对不同攻击来鲁棒化。实验结果显示，我们的 Meta-AT 算法与现状卷积方法相比，有较高的鲁棒性。此外，模型 después Meta-AT 保持了较高的清样分类精度（CCA）。值得注意的是，Meta-AT 解决了所有三种限制，带来了显著的改进。这个benchmark最终实现了对敌意防御方法的传播性和从少量敌意示例中学习的能力进行研究。我们的代码和攻击数据集将在 <https://github.com/PXX1110/Meta_AT> 上发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/eess.IV_2023_09_18/" data-id="clogxf3sr014j5xraaeff0i8t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/eess.SP_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T08:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/eess.SP_2023_09_18/">eess.SP - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ROAR-Fed-RIS-Assisted-Over-the-Air-Adaptive-Resource-Allocation-for-Federated-Learning"><a href="#ROAR-Fed-RIS-Assisted-Over-the-Air-Adaptive-Resource-Allocation-for-Federated-Learning" class="headerlink" title="ROAR-Fed: RIS-Assisted Over-the-Air Adaptive Resource Allocation for Federated Learning"></a>ROAR-Fed: RIS-Assisted Over-the-Air Adaptive Resource Allocation for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09883">http://arxiv.org/abs/2309.09883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayu Mao, Aylin Yener</li>
<li>for: 这 paper 旨在探讨基于无线通信的 Federated Learning (FL) 方法，并利用 Reconfigurable Intelligent Surface (RIS) 提高学习性能。</li>
<li>methods: 该 paper 使用 Cross-layer 视角，对客户端数据集分布和个体资源进行协调优化，并采用 RIS 帮助实现更好的学习环境。</li>
<li>results: 数值结果表明，在非 i.i.d. 数据和不完全 CSI 下，ROAR-Fed 算法可以 convergent，并且在不同客户端资源和数据分布情况下显示出优异性。<details>
<summary>Abstract</summary>
Over-the-air federated learning (OTA-FL) integrates communication and model aggregation by exploiting the innate superposition property of wireless channels. The approach renders bandwidth efficient learning, but requires care in handling the wireless physical layer impairments. In this paper, federated edge learning is considered for a network that is heterogeneous with respect to client (edge node) data set distributions and individual client resources, under a general non-convex learning objective. We augment the wireless OTA-FL system with a Reconfigurable Intelligent Surface (RIS) to enable a propagation environment with improved learning performance in a realistic time varying physical layer. Our approach is a cross-layer perspective that jointly optimizes communication, computation and learning resources, in this general heterogeneous setting. We adapt the local computation steps and transmission power of the clients in conjunction with the RIS phase shifts. The resulting joint communication and learning algorithm, RIS-assisted Over-the-air Adaptive Resource Allocation for Federated learning (ROAR-Fed) is shown to be convergent in this general setting. Numerical results demonstrate the effectiveness of ROAR-Fed under heterogeneous (non i.i.d.) data and imperfect CSI, indicating the advantage of RIS assisted learning in this general set up.
</details>
<details>
<summary>摘要</summary>
随空 Federated Learning (OTA-FL) 利用无线通信频率层的自然superposition 特性来实现带宽效率的学习。然而，需要在无线物理层缺陷处理方面予以关注。在本文中，我们考虑了Edge federated learning 在具有不同客户（边缘节点）数据集分布和个体客户资源的网络上。我们在一般非对称学习目标下对无线OTA-FL系统进行扩展，并添加了智能表面（RIS）以实现改进的物理层媒体。我们采用了跨层视角，同时优化通信、计算和学习资源。在这种一般不同设置下，我们适应性地调整客户端的计算步骤和传输功率，并与RIS相关的相位Shift。得到的联合通信和学习算法，即RIS协助的Over-the-air Adaptive Resource Allocation for Federated learning（ROAR-Fed），在这种一般设置下被证明是收敛的。数据示ROAR-Fed在不同（非i.i.d.）数据和不完美CSI下的效果，表明RIS协助学习在这种一般设置下具有优势。
</details></li>
</ul>
<hr>
<h2 id="RIS-Assisted-Energy-Harvesting-Gains-for-Bistatic-Backscatter-Networks-Performance-Analysis-and-RIS-Phase-Optimization"><a href="#RIS-Assisted-Energy-Harvesting-Gains-for-Bistatic-Backscatter-Networks-Performance-Analysis-and-RIS-Phase-Optimization" class="headerlink" title="RIS-Assisted Energy Harvesting Gains for Bistatic Backscatter Networks: Performance Analysis and RIS Phase Optimization"></a>RIS-Assisted Energy Harvesting Gains for Bistatic Backscatter Networks: Performance Analysis and RIS Phase Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09859">http://arxiv.org/abs/2309.09859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diluka Galappaththige, Fatemeh Rezaei, Chintha Tellambura, Sanjeewa Herath<br>for:  This paper aims to improve the energy autonomy of inexpensive tags in IoT networks by deploying a reconfigurable intelligent surface (RIS) to enhance RF power and overcome energy insecurities.methods:  The paper explores the use of RIS to improve the energy harvesting (EH) capabilities of tags, and analyzes the impact of linear and non-linear EH models on single-tag and multi-tag scenarios. The paper also derives key metrics such as received power, harvested power, achievable rate, outage probability, bit error rate, and diversity order.results:  The paper shows significant improvements in activation distance, received power, and achievable rate compared to benchmarks without RIS or with random RIS-phase design. For instance, the optimal design with a \num{200}-element RIS increases the activation distance by \qty{270}{\percent} and \qty{55}{\percent} compared to the benchmarks. The paper demonstrates that RIS deployment improves the energy autonomy of tags while maintaining the basic tag design intact.<details>
<summary>Abstract</summary>
Inexpensive tags powered by energy harvesting (EH) can realize green (energy-efficient) Internet of Things (IoT) networks. However, tags are vulnerable to energy insecurities, resulting in poor communication ranges, activation distances, and data rates. To overcome these challenges, we explore the use of a reconfigurable intelligent surface (RIS) for EH-based IoT networks. The RIS is deployed to enhance RF power at the tag, improving EH capabilities. We consider linear and non-linear EH models and analyze single-tag and multi-tag scenarios. For single-tag networks, the tag's maximum received power and the reader's signal-to-noise ratio with the optimized RIS phase-shifts are derived. Key metrics, such as received power, harvested power, achievable rate, outage probability, bit error rate, and diversity order, are also evaluated. The impact of RIS phase shift quantization errors is also studied. For the multi-tag case, an algorithm to compute the optimal RIS phase-shifts is developed. Numerical results and simulations demonstrate significant improvements compared to the benchmarks of no-RIS case and random RIS-phase design. For instance, our optimal design with a \num{200}-element RIS increases the activation distance by \qty{270}{\percent} and \qty{55}{\percent} compared to those benchmarks. In summary, RIS deployment improves the energy autonomy of tags while maintaining the basic tag design intact.
</details>
<details>
<summary>摘要</summary>
低成本的标签（标签）由能量收集（EH）能够实现绿色（能量高效）互联网络（IoT）。然而，标签受到能源不安全性的挑战，导致通信范围、活动距离和数据速率受到影响。为了解决这些挑战，我们研究使用可重新配置的智能表面（RIS）来增强标签上的RF能量，提高EH能力。我们考虑了线性和非线性EH模型，对单标签和多标签场景进行分析。在单标签网络中，我们计算了标签上最大接收功率和读取器与优化RIS相位偏移后的信号强度比。我们还评估了关键指标，如接收功率、收集功率、可达速率、失业概率、比特错误率和多样度。我们还研究了RIS相位偏移量的量化误差的影响。在多标签场景中，我们开发了计算优化RIS相位偏移的算法。numericalResults和 simulations表明，我们的优化设计可以与无RIS场景和随机RIS相位设计相比，提高标签的活动距离和数据速率。例如，我们的优化设计使用200个RIS元素可以提高标签的活动距离270%和55%。总之，RIS部署可以提高标签的能源自主性，而不需要修改标签的基本设计。
</details></li>
</ul>
<hr>
<h2 id="A-Read-Margin-Enhancement-Circuit-with-Dynamic-Bias-Optimization-for-MRAM"><a href="#A-Read-Margin-Enhancement-Circuit-with-Dynamic-Bias-Optimization-for-MRAM" class="headerlink" title="A Read Margin Enhancement Circuit with Dynamic Bias Optimization for MRAM"></a>A Read Margin Enhancement Circuit with Dynamic Bias Optimization for MRAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09797">http://arxiv.org/abs/2309.09797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renhe Chen, Albert Lee, Zirui Wang, Di Wu, Xufeng Kou</li>
<li>for: 提高Magnetic Random Access Memory（MRAM）的读出效率</li>
<li>methods: 使用动态偏好优化（DBO）电路实现实时跟踪优化读偏好电压，以适应过程Voltage-Temperature（PVT）变化</li>
<li>results: 对28奈米1Mb MRAM宽 macro进行了模拟研究，结果显示DBO电路能够在PVT变化下保持优化读偏好电压的跟踪精度高于90%，并且能够降低读出错误率，提高MRAM性能和可靠性。<details>
<summary>Abstract</summary>
This brief introduces a read bias circuit to improve readout yield of magnetic random access memories (MRAMs). A dynamic bias optimization (DBO) circuit is proposed to enable the real-time tracking of the optimal read voltage across processvoltage-temperature (PVT) variations within an MRAM array. It optimizes read performance by adjusting the read bias voltage dynamically for maximum sensing margin. Simulation results on a 28-nm 1Mb MRAM macro show that the tracking accuracy of the proposed DBO circuit remains above 90% even when the optimal sensing voltage varies up to 50%. Such dynamic tracking strategy further results in up to two orders of magnitude reduction in the bit error rate with respect to different variations, highlighting its effectiveness in enhancing MRAM performance and reliability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Asymptotic-Performance-of-the-GSVD-Based-MIMO-NOMA-Communications-with-Rician-Fading"><a href="#Asymptotic-Performance-of-the-GSVD-Based-MIMO-NOMA-Communications-with-Rician-Fading" class="headerlink" title="Asymptotic Performance of the GSVD-Based MIMO-NOMA Communications with Rician Fading"></a>Asymptotic Performance of the GSVD-Based MIMO-NOMA Communications with Rician Fading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09711">http://arxiv.org/abs/2309.09711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenguang Rao, Zhiguo Ding, Kanapathippillai Cumanan, Xuchu Dai</li>
<li>for: 这篇论文研究了MIMO-NOMA系统中使用GSVD作为前处理算法的性能。</li>
<li>methods: 该论文使用了operator-valued free probability theory中的线性化技巧和决定性方法来分析通道矩阵的普通特征。</li>
<li>results: 该论文提出了一种迭代过程来计算通道矩阵的Cauchy转换，并通过这种方法得到了通信系统的均值数据速率。此外，当通道模型为Rayleigh折射时，也 deriv了closed-form表达式。实验结果 validate了分析结果。<details>
<summary>Abstract</summary>
In recent years, the multiple-input multiple-output (MIMO) non-orthogonal multiple-access (NOMA) systems have attracted a significant interest in the relevant research communities. As a potential precoding scheme, the generalized singular value decomposition (GSVD) can be adopted in MIMO-NOMA systems and has been proved to have high spectral efficiency. In this paper, the performance of the GSVD-based MIMO-NOMA communications with Rician fading is studied. In particular, the distribution characteristics of generalized singular values (GSVs) of channel matrices are analyzed. Two novel mathematical tools, the linearization trick and the deterministic equivalent method, which are based on operator-valued free probability theory, are exploited to derive the Cauchy transform of GSVs. An iterative process is proposed to obtain the numerical values of the Cauchy transform of GSVs, which can be exploited to derive the average data rates of the communication system. In addition, the special case when the channel is modeled as Rayleigh fading, i.e., the line-of-sight propagation is trivial, is analyzed. In this case, the closed-form expressions of average rates are derived from the proposed iterative process. Simulation results are provided to validate the derived analytical results.
</details>
<details>
<summary>摘要</summary>
在最近几年，多输入多输出（MIMO）不对称多访问（NOMA）系统已经吸引了研究领域的广泛关注。作为MIMO-NOMA系统的可能的预编码方案，泛化协方差矩阵（GSVD）可以应用于MIMO-NOMA系统，并且已经证明具有高spectral efficiency。本文研究了GSVD基于MIMO-NOMA通信系统中的 ricain fading 的性能。特别是分析了通道矩阵的泛化协方差值（GSV）的分布特征。基于 оператор值自由概率论的两种新的数学工具——线性化技巧和deterministic equivalent方法——被利用来 derivethe Cauchy transform of GSVs。一种迭代过程被提出来获取GSVs的Cauchy transform的数值，可以用来计算通信系统的平均数据率。此外，当通道模型为Rayleigh fading，即线性视程是rivial的特殊情况也被分析。在这种情况下，通过提posed迭代过程，得到了Closed-form表达式的平均率。Simulation results are provided to validate the derived analytical results.
</details></li>
</ul>
<hr>
<h2 id="Uplink-Power-Control-for-Distributed-Massive-MIMO-with-1-Bit-ADCs"><a href="#Uplink-Power-Control-for-Distributed-Massive-MIMO-with-1-Bit-ADCs" class="headerlink" title="Uplink Power Control for Distributed Massive MIMO with 1-Bit ADCs"></a>Uplink Power Control for Distributed Massive MIMO with 1-Bit ADCs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09665">http://arxiv.org/abs/2309.09665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bikshapathi Gouda, Italo Atzeni, Antti Tölli</li>
<li>For: 这篇论文研究了分布式巨量多输入多输出系统中基站（BS）装备1比特数字转换器（ADC）的上行功率控制问题。* Methods: 该论文使用了1比特ADC的信号干扰分析和优化策略，并提出了三种算法来优化UE传输功率和BS中的扰干。* Results: 研究发现，在多BS情况下，通过合适地调整BS中的扰干，可以使SNDR在输出 JOIN combiner中变为单modal函数，从而使UE可以有效地被多个BS服务。此外，在多UE情况下，该论文提出了最优化UE传输功率和扰干的算法，以达到最小功率和最大最小SINDR的目标。<details>
<summary>Abstract</summary>
We consider the problem of uplink power control for distributed massive multiple-input multiple-output systems where the base stations (BSs) are equipped with 1-bit analog-to-digital converters (ADCs). The scenario with a single-user equipment (UE) is first considered to provide insights into the signal-tonoise-and-distortion ratio (SNDR). With a single BS, the SNDR is a unimodal function of the UE transmit power. With multiple BSs, the SNDR at the output of the joint combiner can be made unimodal by adding properly tuned dithering at each BS. As a result, the UE can be effectively served by multiple BSs with 1-bit ADCs. Considering the signal-to-interference-plus-noise-anddistortion ratio (SINDR) in the multi-UE scenario, we aim at optimizing the UE transmit powers and the dithering at each BS based on the min-power and max-min-SINDR criteria. To this end, we propose three algorithms with different convergence and complexity properties. Numerical results show that, if the desired SINDR can only be achieved via joint combining across multiple BSs with properly tuned dithering, the optimal UE transmit power is imposed by the distance to the farthest serving BS (unlike in the unquantized case). In this context, dithering plays a crucial role in enhancing the SINDR, especially for UEs with significant path loss disparity among the serving BSs.
</details>
<details>
<summary>摘要</summary>
我们考虑了分布式巨量多输入多输出系统中的上行电力控制问题，其中基站（BS）具有1比特分析类比器（ADC）。我们首先考虑了单用户设备（UE）的情况，以获得对SNDR的深入理解。单一BS情况下，SNDR是单一函数UE传输电力。多个BS情况下，通过对每个BS添加适当的测量偏差，则SNDR在多BS联合器出口可以变成单一函数。因此，UE可以从多个BS中有效地获得服务，并且使用1比特ADC。对于多UE情况下的SINDR，我们寻找了对UE传输电力和BS中的测量偏差进行优化，以达到最小电力和最大最小SINDR的目标。为此，我们提出了三种不同的算法，每个算法具有不同的参数和复杂度。实验结果显示，如果只能通过多BS联合器实现所需的SINDR，则UE传输电力将被最远的服务BS的距离所限制（不同于不量化情况下）。在这种情况下，测量偏差在增强SINDR方面扮演至关重要的角色，特别是UE对服务BS的距离差异较大。
</details></li>
</ul>
<hr>
<h2 id="Turbo-Coded-OFDM-OQAM-Using-Hilbert-Transform"><a href="#Turbo-Coded-OFDM-OQAM-Using-Hilbert-Transform" class="headerlink" title="Turbo Coded OFDM-OQAM Using Hilbert Transform"></a>Turbo Coded OFDM-OQAM Using Hilbert Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09620">http://arxiv.org/abs/2309.09620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kasturi Vasudevan, Surendra Kota, Lov Kumar, Himanshu Bhusan Mishra</li>
<li>for: 本文探讨了使用哈尔杯变换生成 orthogonal frequency division multiplexing（OFDM）与偏角幂强调幂（OQAM）的方法，并证明了这种方法与单边幂调幂（SSB）相等。</li>
<li>methods: 本文使用了哈尔杯变换来生成OFDM-OQAM，并使用了复数valued transmit filter来实现。 receiver使用 matched filter 来回填消息。</li>
<li>results: 在 discrete time 中 simulated 系统，使用 turbo coding 可以实现 bit-error-rate（BER）为 $10^{-5}$， average signal-to-noise ratio（SNR）为每比特接近 0 db。<details>
<summary>Abstract</summary>
Orthogonal frequency division multiplexing (OFDM) with offset quadrature amplitude modulation (OQAM) has been widely discussed in the literature and is considered a popular waveform for 5th generation (5G) wireless telecommunications and beyond. In this work, we show that OFDM-OQAM can be generated using the Hilbert transform and is equivalent to single sideband modulation (SSB), that has roots in analog telecommunications. The transmit filter for OFDM-OQAM is complex valued whose real part is given by the pulse corresponding to the root raised cosine spectrum and the imaginary part is the Hilbert transform of the real part. The real-valued digital information (message) are passed through the transmit filter and frequency division multiplexed on orthogonal subcarriers. The message bandwidth corresponding to each subcarrier is assumed to be narrow enough so that the channel can be considered ideal. Therefore, at the receiver, a matched filter can used to recover the message. Turbo coding is used to achieve bit-error-rate (BER) as low as $10^{-5}$ at an average signal-to-noise ratio (SNR) per bit close to 0 db. The system has been simulated in discrete time.
</details>
<details>
<summary>摘要</summary>
Orthogonal frequency division multiplexing (OFDM) with offset quadrature amplitude modulation (OQAM) 已经在文献中广泛讨论，被视为5代移动通信和更进一步的各种应用的流行的波形。在这个工作中，我们展示了OFDM-OQAM可以使用希尔伯特变换生成，与单边干扰模式（SSB）相当，这种模式在分析通信中有深根。发送器的滤波器为复数值，其实部分是根提高cosinuspectrum的脉冲，幂部分是希尔伯特变换实部。接收器使用匹配滤波器来回填消息。在接收器中，使用隐藏编码来实现 bit-error-rate（BER）在10^-5以下，均值信号响应（SNR）每比特接近0 db。系统在离散时间下进行了模拟。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Unscented-Kalman-Filter-under-Minimum-Error-Entropy-with-Fiducial-Points-for-Non-Gaussian-Systems"><a href="#Adaptive-Unscented-Kalman-Filter-under-Minimum-Error-Entropy-with-Fiducial-Points-for-Non-Gaussian-Systems" class="headerlink" title="Adaptive Unscented Kalman Filter under Minimum Error Entropy with Fiducial Points for Non-Gaussian Systems"></a>Adaptive Unscented Kalman Filter under Minimum Error Entropy with Fiducial Points for Non-Gaussian Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09577">http://arxiv.org/abs/2309.09577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyu Tian, Haiquan Zhao</li>
<li>for: 提高非泛解决方案中的噪声和异常测量数据处理能力</li>
<li>methods: 基于最小错误 entropy with fiducial points (MEEF) 提出一种新的 UKF 算法，并通过添加 correntropy 进一步增强噪声和异常测量数据的抑制能力</li>
<li>results: 通过实验示例，表明提出的算法在面临复杂非泛解决方案中的噪声和异常测量数据处理中具有良好的稳定性和估计精度<details>
<summary>Abstract</summary>
The minimum error entropy (MEE) has been extensively used in unscented Kalman filter (UKF) to handle impulsive noises or abnormal measurement data in non-Gaussian systems. However, the MEE-UKF has poor numerical stability due to the inverse operation of singular matrix. In this paper, a novel UKF based on minimum error entropy with fiducial points (MEEF) is proposed \textcolor{black}{to improve the problem of non-positive definite key matrix. By adding the correntropy to the error entropy, the proposed algorithm further enhances the ability of suppressing impulse noise and outliers. At the same time, considering the uncertainty of noise distribution, the modified Sage-Husa estimator of noise statistics is introduced to adaptively update the noise covariance matrix. In addition, the convergence analysis of the proposed algorithm provides a guidance for the selection of kernel width. The robustness and estimation accuracy of the proposed algorithm are manifested by the state tracking examples under complex non-Gaussian noises.
</details>
<details>
<summary>摘要</summary>
“非标准几何系统中的访问频率（MEE）已经广泛使用在非标准 Kalman统计（UKF）中，以处理快速变化的数据或异常测量变量。但MEE-UKF具有对对称矩降的逆操作，导致严重的数值稳定问题。本文提出了一种基于MEE的新型UKF（MEEF），以解决这个问题。通过将correnticropy添加到错误熵中，提高了对于快速变化和偏差测量的抑制能力。同时，透过考虑随机变量的不确定性，引入修改的Sage-Husa估计器来适应更新随机矩降矩。此外，提出了修改了kernel宽度的选择指南，以确保算法的稳定性和准确性。实际应用中，提出的算法在复杂的非标准几何系统中进行了稳定追踪。”
</details></li>
</ul>
<hr>
<h2 id="AI-Native-Transceiver-Design-for-Near-Field-Ultra-Massive-MIMO-Principles-and-Techniques"><a href="#AI-Native-Transceiver-Design-for-Near-Field-Ultra-Massive-MIMO-Principles-and-Techniques" class="headerlink" title="AI-Native Transceiver Design for Near-Field Ultra-Massive MIMO: Principles and Techniques"></a>AI-Native Transceiver Design for Near-Field Ultra-Massive MIMO: Principles and Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09575">http://arxiv.org/abs/2309.09575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Yu, Yifan Ma, Hengtao He, Shenghui Song, Jun Zhang, Khaled B. Letaief</li>
<li>for: 这篇论文旨在探讨近场多输入多输出（UM-MIMO）技术的algorithmic设计，以提高无线网络的spectral和能量效率。</li>
<li>methods: 论文提出了两种基于人工智能（AI）的框架，用于设计近场UM-MIMO传输器。这两种框架分别是迭代式和非迭代式框架。</li>
<li>results: 文章通过使用这两种框架，实现了近场UM-MIMO传输器的near-field beam focusing和频率域预测等功能，并且在多个关键性能指标上达到了显著的优势。<details>
<summary>Abstract</summary>
Ultra-massive multiple-input multiple-output (UM-MIMO) is a cutting-edge technology that promises to revolutionize wireless networks by providing an unprecedentedly high spectral and energy efficiency. The enlarged array aperture of UM-MIMO facilitates the accessibility of the near-field region, thereby offering a novel degree of freedom for communications and sensing. Nevertheless, the transceiver design for such systems is challenging because of the enormous system scale, the complicated channel characteristics, and the uncertainties in propagation environments. Therefore, it is critical to study scalable, low-complexity, and robust algorithms that can efficiently characterize and leverage the properties of the near-field channel. In this article, we will advocate two general frameworks from an artificial intelligence (AI)-native perspective, which are tailored for the algorithmic design of near-field UM-MIMO transceivers. Specifically, the frameworks for both iterative and non-iterative algorithms are discussed. Near-field beam focusing and channel estimation are presented as two tutorial-style examples to demonstrate the significant advantages of the proposed AI-native frameworks in terms of various key performance indicators.
</details>
<details>
<summary>摘要</summary>
ultra-massive多输入多输出（UM-MIMO）是一种最前线的技术，可以带来无 precedent的 spectral和能量效率。 UM-MIMO 的扩大数组天线可以访问近场区域，从而提供一种新的通信和感知的自由度。然而，UM-MIMO 的接收机设计具有巨大的系统规模，复杂的通道特性和传播环境的不确定性，因此需要研究可扩展、低复杂度和Robust的算法，以有效地描述和利用近场通道的特性。在这篇文章中，我们将提出两个总体框架，从人工智能（AI）原生的角度出发，用于near-field UM-MIMO 接收机的算法设计。具体来说，我们将讨论iterative和非iterative算法的框架。 near-field  beam focusing和通道估计被用作两个教程风格的示例，以 Illustrate了我们提出的 AI-native 框架在不同的关键性能指标方面的显著优势。
</details></li>
</ul>
<hr>
<h2 id="A-Covariance-Adaptive-Student’s-t-Based-Kalman-Filter"><a href="#A-Covariance-Adaptive-Student’s-t-Based-Kalman-Filter" class="headerlink" title="A Covariance Adaptive Student’s t Based Kalman Filter"></a>A Covariance Adaptive Student’s t Based Kalman Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09565">http://arxiv.org/abs/2309.09565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benyang Gong, Jiacheng He, Gang Wang, Bei Peng</li>
<li>for: 本文为了提高Student’s t基于 kalman filter（TKF）的精度和稳定性，采用 Gaussian mixture model（GMM）来更正TKF中对测量噪声的covariance矩阵，从而超过了TKF的调整限制。</li>
<li>methods: 本文使用GMM来生成测量噪声的covariance矩阵，并将其用于TKF中更正测量噪声的confidence水平。</li>
<li>results: 本文的实验结果表明，使用GMM来更正TKF中测量噪声的covariance矩阵，可以提高TKF的精度和稳定性，并超过了传统的TKF。<details>
<summary>Abstract</summary>
In the classical Kalman filter(KF), the estimated state is a linear combination of the one-step predicted state and measurement state, their confidence level change when the prediction mean square error matrix and covariance matrix of measurement noise vary. The existing student's t based Kalman filter(TKF) works similarly to the way KF works, they both work well with impulse noise, but when it comes to Gaussian noise, TKF encounters an adjustment limit of the confidence level, this can lead to inaccuracies in such situations. This brief optimizes TKF by using the Gaussian mixture model(GMM), which generates a reasonable covariance matrix from the measurement noise to replace the one used in the existing algorithm and breaks the adjustment limit of the confidence level. At the end of the brief, the performance of the covariance adaptive student's t based Kalman filter(TGKF) is verified.
</details>
<details>
<summary>摘要</summary>
经 classical Kalman filter（KF），估计状态是一个线性组合于一步预测状态和测量状态，其信度水平随预测均值方差矩阵和测量噪声均值矩阵的变化而变化。现有的学生t基于Kalman filter（TKF）与KF相似，它们都能够处理冲击噪声，但在 Gaussian 噪声时，TKF会遇到信度水平调整限制，这可能导致准确性下降。本文优化TKF使用 Gaussian mixture model（GMM）生成测量噪声的合理协方差矩阵，并让信度水平不再受限制。文章结尾，covariance adaptive student's t based Kalman filter（TGKF）的性能得到了验证。
</details></li>
</ul>
<hr>
<h2 id="Multi-user-beamforming-in-RIS-aided-communications-and-experimental-validations"><a href="#Multi-user-beamforming-in-RIS-aided-communications-and-experimental-validations" class="headerlink" title="Multi-user beamforming in RIS-aided communications and experimental validations"></a>Multi-user beamforming in RIS-aided communications and experimental validations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09460">http://arxiv.org/abs/2309.09460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhibo Zhou, Haifan Yin, Li Tan, Ruikun Zhang, Kai Wang, Yingzhuang Liu</li>
<li>for: 这篇论文目的是提出一种基于智能表面的多用户通信系统，以提高 Spectral Efficiency。</li>
<li>methods: 论文使用了频率积分和优化矩阵来实现通信系统中的多用户耦合。</li>
<li>results: 实验结果表明，在使用智能表面的情况下，系统的 Spectral Efficiency 提高了 13.48bps&#x2F;Hz，对应的接收功率提高了 26.6dB和17.5dB。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) is a promising technology for future wireless communications due to its capability of optimizing the propagation environments. Nevertheless, in literature, there are few prototypes serving multiple users. In this paper, we propose a whole flow of channel estimation and beamforming design for RIS, and set up an RIS-aided multi-user system for experimental validations. Specifically, we combine a channel sparsification step with generalized approximate message passing (GAMP) algorithm, and propose to generate the measurement matrix as Rademacher distribution to obtain the channel state information (CSI). To generate the reflection coefficients with the aim of maximizing the spectral efficiency, we propose a quadratic transform-based low-rank multi-user beamforming (QTLM) algorithm. Our proposed algorithms exploit the sparsity and low-rank properties of the channel, which has the advantages of light calculation and fast convergence. Based on the universal software radio peripheral devices, we built a complete testbed working at 5.8GHz and implemented all the proposed algorithms to verify the possibility of RIS assisting multi-user systems. Experimental results show that the system has obtained an average spectral efficiency increase of 13.48bps/Hz, with respective received power gains of 26.6dB and 17.5dB for two users, compared with the case when RIS is powered-off.
</details>
<details>
<summary>摘要</summary>
《可重配置智能表面（RIS）是未来无线通信技术的前景之一，因其可以优化传播环境。然而，在文献中，有很少的多用户体系。在这篇论文中，我们提出了渠道估计和扬射设计的整体流程，并设置了基于RIS的多用户系统 для实验验证。具体来说，我们将渠道简化步骤与通用简化消息传递算法（GAMP）相结合，并提议使用拉德美纳分布生成测量矩阵来获取渠道状态信息（CSI）。为了通过扬射矩阵来最大化спектル效率，我们提议使用quadratic transform-based low-rank multi-user beamforming（QTLM）算法。我们的提议算法利用渠道的稀疏和低级特性，具有轻量级计算和快速收敛的优点。基于通用软件无线通信外围设备，我们建立了工作在5.8GHz频率上的完整测试床，并实现了所有的提议算法，以验证RIS助け多用户系统的可能性。实验结果显示，系统在RIS启用下获得了平均spectral efficiency提高13.48bps/Hz，与RIS灭活时的相比，分别提高了接收功率26.6dB和17.5dB。
</details></li>
</ul>
<hr>
<h2 id="Energy-efficient-Integrated-Sensing-and-Communication-System-and-DNLFM-Waveform"><a href="#Energy-efficient-Integrated-Sensing-and-Communication-System-and-DNLFM-Waveform" class="headerlink" title="Energy-efficient Integrated Sensing and Communication System and DNLFM Waveform"></a>Energy-efficient Integrated Sensing and Communication System and DNLFM Waveform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09415">http://arxiv.org/abs/2309.09415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihua Ma, Zhifeng Yuan, Shuqiang Xia, Chen Bai, Zhongbin Wang, Yuxin Wang</li>
<li>for: 本研究旨在提高Integrated sensing and communication (ISAC)系统的能效性。</li>
<li>methods: 本文提出了针对ISAC系统的专门探测信号，并在探测和通信信号之间进行时频匹配窗口。此外，本文还提出了使用Discrete non-linear frequency modulation (DNLFM)来实现时间频率匹配和自适应窗口重量。</li>
<li>results:  simulations 结果表明，提出的方法可以提高ISAC系统的能效性。<details>
<summary>Abstract</summary>
Integrated sensing and communication (ISAC) is a key enabler of 6G. Unlike communication radio links, the sensing signal requires to experience round trips from many scatters. Therefore, sensing is more power-sensitive and faces a severer multi-target interference. In this paper, the ISAC system employs dedicated sensing signals, which can be reused as the communication reference signal. This paper proposes to add time-frequency matched windows at both the transmitting and receiving sides, which avoids mismatch loss and increases energy efficiency. Discrete non-linear frequency modulation (DNLFM) is further proposed to achieve both time-domain constant modulus and frequency-domain arbitrary windowing weights. DNLFM uses very few Newton iterations and a simple geometrically-equivalent method to generate, which greatly reduces the complex numerical integral in the conventional method. Moreover, the spatial-domain matched window is proposed to achieve low sidelobes. The simulation results show that the proposed methods gain a higher energy efficiency than conventional methods.
</details>
<details>
<summary>摘要</summary>
integration of sensing and communication (ISAC) is a key enabler of 6G. Unlike communication radio links, the sensing signal requires to experience round trips from many scatters. Therefore, sensing is more power-sensitive and faces a severer multi-target interference. In this paper, the ISAC system employs dedicated sensing signals, which can be reused as the communication reference signal. This paper proposes to add time-frequency matched windows at both the transmitting and receiving sides, which avoids mismatch loss and increases energy efficiency. Discrete non-linear frequency modulation (DNLFM) is further proposed to achieve both time-domain constant modulus and frequency-domain arbitrary windowing weights. DNLFM uses very few Newton iterations and a simple geometrically-equivalent method to generate, which greatly reduces the complex numerical integral in the conventional method. Moreover, the spatial-domain matched window is proposed to achieve low sidelobes. The simulation results show that the proposed methods gain a higher energy efficiency than conventional methods.Here's the text in Traditional Chinese if you prefer:integration of sensing and communication (ISAC) is a key enabler of 6G. Unlike communication radio links, the sensing signal requires to experience round trips from many scatters. Therefore, sensing is more power-sensitive and faces a severer multi-target interference. In this paper, the ISAC system employs dedicated sensing signals, which can be reused as the communication reference signal. This paper proposes to add time-frequency matched windows at both the transmitting and receiving sides, which avoids mismatch loss and increases energy efficiency. Discrete non-linear frequency modulation (DNLFM) is further proposed to achieve both time-domain constant modulus and frequency-domain arbitrary windowing weights. DNLFM uses very few Newton iterations and a simple geometrically-equivalent method to generate, which greatly reduces the complex numerical integral in the conventional method. Moreover, the spatial-domain matched window is proposed to achieve low sidelobes. The simulation results show that the proposed methods gain a higher energy efficiency than conventional methods.
</details></li>
</ul>
<hr>
<h2 id="Improving-Axial-Resolution-of-Optical-Resolution-Photoacoustic-Microscopy-with-Advanced-Frequency-Domain-Eigenspace-Based-Minimum-Variance-Beamforming-Method"><a href="#Improving-Axial-Resolution-of-Optical-Resolution-Photoacoustic-Microscopy-with-Advanced-Frequency-Domain-Eigenspace-Based-Minimum-Variance-Beamforming-Method" class="headerlink" title="Improving Axial Resolution of Optical Resolution Photoacoustic Microscopy with Advanced Frequency Domain Eigenspace Based Minimum Variance Beamforming Method"></a>Improving Axial Resolution of Optical Resolution Photoacoustic Microscopy with Advanced Frequency Domain Eigenspace Based Minimum Variance Beamforming Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09409">http://arxiv.org/abs/2309.09409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Hsiang Yu, Meng-Lin Li</li>
<li>for: 高解析力照明探针（OR-PAM）可以利用光学 фокусинг和声学检测，实现微scopic optical absorption imaging。但是，它的光学 lateral resolution 高，而声学轴resolution 低，导致不好的3D可视化，因此通常只会present 2D maximum amplitude projection images。</li>
<li>methods: 以 axial resolution 为问题，我们提出了一种基于Frequency-domain eigenspace-based minimum variance（F-EIBMV）的扫描技术来减少axial sidelobe interference和噪声。这种方法可以同时提高OR-PAM的axial resolution和对比度。</li>
<li>results: 对于25MHz OR-PAM系统，我们发现了axial point spread function的半峰宽度从69.3μm降低到16.89μm，表明了axial resolution的显著提高。<details>
<summary>Abstract</summary>
Optical resolution photoacoustic microscopy (OR-PAM) leverages optical focusing and acoustic detection for microscopic optical absorption imaging. Intrinsically it owns high optical lateral resolution and poor acoustic axial resolution. Such anisometric resolution hinders good 3-D visualization; thus 2-D maximum amplitude projection images are commonly presented in the literature. Since its axial resolution is limited by the bandwidth of acoustic detectors, ultrahigh frequency, and wideband detectors with Wiener deconvolution have been proposed to address this issue. Nonetheless, they also introduce other issues such as severe high-frequency attenuation and limited imaging depth. In this work, we view axial resolution improvement as an axial signal reconstruction problem, and the axial resolution degradation is caused by axial sidelobe interference. We propose an advanced frequency-domain eigenspace-based minimum variance (F-EIBMV) beamforming technique to suppress axial sidelobe interference and noises. This method can simultaneously enhance the axial resolution and contrast of OR-PAM. For a 25-MHz OR-PAM system, the full-width at half-maximum of an axial point spread function decreased significantly from 69.3 $\mu$m to 16.89 $\mu$m, indicating a significant improvement in axial resolution.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/eess.SP_2023_09_18/" data-id="clogxf3tl017s5xra7hfv7b36" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.SD_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T15:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.SD_2023_09_17/">cs.SD - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sound-Source-Distance-Estimation-in-Diverse-and-Dynamic-Acoustic-Conditions"><a href="#Sound-Source-Distance-Estimation-in-Diverse-and-Dynamic-Acoustic-Conditions" class="headerlink" title="Sound Source Distance Estimation in Diverse and Dynamic Acoustic Conditions"></a>Sound Source Distance Estimation in Diverse and Dynamic Acoustic Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09288">http://arxiv.org/abs/2309.09288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saksham Singh Kushwaha, Iran R. Roman, Magdalena Fuentes, Juan Pablo Bello</li>
<li>for: 本研究旨在实现在真实世界中移动声源的方向到达（DOA）和距离投机器录音器的判别。</li>
<li>methods: 本研究使用数据驱动的方法，利用大量开源数据集，包括多个环境下的附加麦克风记录，进行优化。</li>
<li>results: 我们提出了一种CRNN模型，可以在多个数据集上表现出色地估计移动声源的距离，超过了一个最近发表的方法。我们还进行了模型性能的函数分析，发现在声源真实距离不同和不同训练损失下，模型的性能存在最佳化点。本研究是首次通过深度学习实现了声源距离估计在多种听音条件下。<details>
<summary>Abstract</summary>
Localizing a moving sound source in the real world involves determining its direction-of-arrival (DOA) and distance relative to a microphone. Advancements in DOA estimation have been facilitated by data-driven methods optimized with large open-source datasets with microphone array recordings in diverse environments. In contrast, estimating a sound source's distance remains understudied. Existing approaches assume recordings by non-coincident microphones to use methods that are susceptible to differences in room reverberation. We present a CRNN able to estimate the distance of moving sound sources across multiple datasets featuring diverse rooms, outperforming a recently-published approach. We also characterize our model's performance as a function of sound source distance and different training losses. This analysis reveals optimal training using a loss that weighs model errors as an inverse function of the sound source true distance. Our study is the first to demonstrate that sound source distance estimation can be performed across diverse acoustic conditions using deep learning.
</details>
<details>
<summary>摘要</summary>
本文描述了一种基于深度学习的Sound Source Distance Estimation（SSDE）模型，可以在多种不同的室内环境中Estimate the distance of moving sound sources。我们使用了大量的开源数据集，并且使用了一种叫做Convolutional Recurrent Neural Network（CRNN）的模型，可以在多个不同的 datasets 中Outperform 已有的方法。我们还进行了一些性能分析，包括模型的训练损失函数的选择，以及模型在不同的室内环境下的性能。我们的研究表明，可以使用深度学习来Estimate the distance of moving sound sources across diverse acoustic conditions。这是一个新的领域，尚未得到过足够的研究。我们的模型可以在多个不同的室内环境下提供高精度的距离估计，并且可以在不同的训练损失函数下进行优化。在本研究中，我们使用了一些不同的训练损失函数，包括损失函数的weighted sum，以及一种叫做“inverse distance”的损失函数。我们的研究表明，使用“inverse distance”损失函数可以提高模型的性能，并且可以在不同的室内环境下提供更高的精度。总之，我们的研究表明，可以使用深度学习来Estimate the distance of moving sound sources across diverse acoustic conditions。我们的模型可以在多个不同的室内环境下提供高精度的距离估计，并且可以在不同的训练损失函数下进行优化。这对于各种应用场景，如智能家居、智能城市等，都具有重要的意义。
</details></li>
</ul>
<hr>
<h2 id="PromptVC-Flexible-Stylistic-Voice-Conversion-in-Latent-Space-Driven-by-Natural-Language-Prompts"><a href="#PromptVC-Flexible-Stylistic-Voice-Conversion-in-Latent-Space-Driven-by-Natural-Language-Prompts" class="headerlink" title="PromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts"></a>PromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09262">http://arxiv.org/abs/2309.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jixun Yao, Yuguang Yang, Yi Lei, Ziqian Ning, Yanni Hu, Yu Pan, Jingjing Yin, Hongbin Zhou, Heng Lu, Lei Xie</li>
<li>for: This paper aims to improve the style voice conversion process by using natural language prompts to generate a style vector and adapting the duration of discrete tokens.</li>
<li>methods: The proposed approach, called PromptVC, employs a latent diffusion model to sample the style vector from noise, with the process being conditioned on natural language prompts. The system also uses HuBERT to extract discrete tokens and replace them with the K-Means center embedding to minimize residual style information.</li>
<li>results: The subjective and objective evaluation results demonstrate the effectiveness of the proposed system, with improved style expressiveness and adaptability to different styles.<details>
<summary>Abstract</summary>
Style voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.
</details>
<details>
<summary>摘要</summary>
《 Style Voice Conversion 》 aims to transform the style of source speech to a desired style based on real-world application demands. However, current methods rely on pre-defined labels or reference speech to control the conversion process, which limits style diversity and lacks intuitive and interpretable style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and the latent diffusion model is trained to sample the style vector from noise, conditioned on natural language prompts. To enhance style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to minimize residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, allowing for adaptive duration adjustment based on different styles. Subjective and objective evaluation results demonstrate the effectiveness of our proposed system.
</details></li>
</ul>
<hr>
<h2 id="Zero-and-Few-shot-Sound-Event-Localization-and-Detection"><a href="#Zero-and-Few-shot-Sound-Event-Localization-and-Detection" class="headerlink" title="Zero- and Few-shot Sound Event Localization and Detection"></a>Zero- and Few-shot Sound Event Localization and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09223">http://arxiv.org/abs/2309.09223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuki Shimada, Kengo Uchida, Yuichiro Koyama, Takashi Shibuya, Shusuke Takahashi, Yuki Mitsufuji, Tatsuya Kawahara</li>
<li>for: 这个论文是为了解决静音定位和检测（SELD）系统中的零或几个批处理任务。</li>
<li>methods: 这个论文使用的方法包括逻辑学习网络（NN）和对准抽取语音样本（CLAP）。</li>
<li>results: 该论文的结果表明，使用 embed-ACCDOA 模型可以在零或几个批处理任务中提高静音定位和检测的性能，并且和完整的训练数据进行比较。<details>
<summary>Abstract</summary>
Sound event localization and detection (SELD) systems estimate direction-of-arrival (DOA) and temporal activation for sets of target classes. Neural network (NN)-based SELD systems have performed well in various sets of target classes, but they only output the DOA and temporal activation of preset classes that are trained before inference. To customize target classes after training, we tackle zero- and few-shot SELD tasks, in which we set new classes with a text sample or a few audio samples. While zero-shot sound classification tasks are achievable by embedding from contrastive language-audio pretraining (CLAP), zero-shot SELD tasks require assigning an activity and a DOA to each embedding, especially in overlapping cases. To tackle the assignment problem in overlapping cases, we propose an embed-ACCDOA model, which is trained to output track-wise CLAP embedding and corresponding activity-coupled Cartesian direction-of-arrival (ACCDOA). In our experimental evaluations on zero- and few-shot SELD tasks, the embed-ACCDOA model showed a better location-dependent scores than a straightforward combination of the CLAP audio encoder and a DOA estimation model. Moreover, the proposed combination of the embed-ACCDOA model and CLAP audio encoder with zero- or few-shot samples performed comparably to an official baseline system trained with complete train data in an evaluation dataset.
</details>
<details>
<summary>摘要</summary>
声音事件 lokalisierung和检测（SELD）系统估算irection-of-arrival（DOA）和时间活动 для集合Target classes。基于神经网络（NN）的SELD系统在不同的Target classes中表现良好，但它们只会在预测前训练的类型上输出DOA和时间活动。为了自定义目标类型 después de training，我们面临着零和几个shot SELD任务，在其中我们可以通过提供文本样本或几个音频样本来设置新的类型。零shot声音分类任务可以通过语音-语言预training（CLAP）的嵌入来实现，但零shot SELD任务需要将每个嵌入分配到活动和DOA，特别是在重叠的情况下。为了解决重叠情况中的分配问题，我们提出了一种嵌入-ACCDOA模型，该模型通过输出track-wise CLAP嵌入和相应的活动-联合Cartesian DOA来解决问题。在我们的实验评估中，embeds-ACCDOA模型在零和几个shot SELD任务中表现出色，其location-dependent scores比直接将CLAP音频编码器和DOA估算模型组合的 scores更高。此外，我们将embeds-ACCDOA模型和CLAP音频编码器与零或几个shot样本组合起来，与完整的训练数据进行评估，结果与官方基eline系统在评估数据集中的表现相当。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.SD_2023_09_17/" data-id="clogxf3qf00v15xrabf9b17dv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.CV_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T13:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.CV_2023_09_17/">cs.CV - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-conditional-generative-models-for-longitudinal-single-slice-abdominal-computed-tomography-harmonization"><a href="#Deep-conditional-generative-models-for-longitudinal-single-slice-abdominal-computed-tomography-harmonization" class="headerlink" title="Deep conditional generative models for longitudinal single-slice abdominal computed tomography harmonization"></a>Deep conditional generative models for longitudinal single-slice abdominal computed tomography harmonization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09392">http://arxiv.org/abs/2309.09392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masilab/c-slicegen">https://github.com/masilab/c-slicegen</a></li>
<li>paper_authors: Xin Yu, Qi Yang, Yucheng Tang, Riqiang Gao, Shunxing Bao, Leon Y. Cai, Ho Hin Lee, Yuankai Huo, Ann Zenobia Moore, Luigi Ferrucci, Bennett A. Landman</li>
<li>for: 用于解决对腹部CT成像数据进行长期分析时，因为不同年份获取的扫描 slice 位置不同，导致不同的器官&#x2F;组织被捕捉的问题。</li>
<li>methods: 我们提出了 C-SliceGen 方法，可以将任意的腹部 axial slice 作为输入，并在 latent space 中估算结构变化，以生成预定义的vertebral level slice。</li>
<li>results: 我们的实验表明，C-SliceGen 方法可以生成高质量的图像，具有真实性和相似性。此外，我们还证明了该方法可以减少腹部 CT 数据的 slice 位置差异，并且在1033名参与者的 Baltimore Longitudinal Study of Aging (BLSA) 数据集上进行了评估，并证明了该方法可以减少腹部 slice 的位置差异。<details>
<summary>Abstract</summary>
Two-dimensional single-slice abdominal computed tomography (CT) provides a detailed tissue map with high resolution allowing quantitative characterization of relationships between health conditions and aging. However, longitudinal analysis of body composition changes using these scans is difficult due to positional variation between slices acquired in different years, which leading to different organs/tissues captured. To address this issue, we propose C-SliceGen, which takes an arbitrary axial slice in the abdominal region as a condition and generates a pre-defined vertebral level slice by estimating structural changes in the latent space. Our experiments on 2608 volumetric CT data from two in-house datasets and 50 subjects from the 2015 Multi-Atlas Abdomen Labeling Challenge dataset (BTCV) Challenge demonstrate that our model can generate high-quality images that are realistic and similar. We further evaluate our method's capability to harmonize longitudinal positional variation on 1033 subjects from the Baltimore Longitudinal Study of Aging (BLSA) dataset, which contains longitudinal single abdominal slices, and confirmed that our method can harmonize the slice positional variance in terms of visceral fat area. This approach provides a promising direction for mapping slices from different vertebral levels to a target slice and reducing positional variance for single-slice longitudinal analysis. The source code is available at: https://github.com/MASILab/C-SliceGen.
</details>
<details>
<summary>摘要</summary>
“两维单片腹部计算机断层成像（CT）提供了高分辨率的组织地图，可以量化健康状况和年龄之间的关系。然而，使用这些扫描数据进行 longitudinal 分析的 Body 组成变化困难，因为不同年份扫描时的 slice 位置会有偏移。为解决这个问题，我们提出了 C-SliceGen，它可以将任意腹部 Axial slice 作为输入，并生成预定的vertebral level slice，通过估计 latent space 中的结构变化。我们的实验表明，我们的模型可以生成高质量的图像，具有实际和相似的特征。我们进一步评估了我们的方法在1033名参与者的 Baltimore Longitudinal Study of Aging（BLSA）数据集上的能力，并证明了我们的方法可以减少 slice 位置偏移的方差。这种方法提供了一个可行的方向，可以将不同 vertebral level 的 slice 映射到目标 slice 上，并减少 longitudinal 分析中的 slice 位置偏移。源代码可以在以下链接中获取：https://github.com/MASILab/C-SliceGen。”
</details></li>
</ul>
<hr>
<h2 id="a-critical-analysis-of-internal-reliability-for-uncertainty-quantification-of-dense-image-matching-in-multi-view-stereo"><a href="#a-critical-analysis-of-internal-reliability-for-uncertainty-quantification-of-dense-image-matching-in-multi-view-stereo" class="headerlink" title="a critical analysis of internal reliability for uncertainty quantification of dense image matching in multi-view stereo"></a>a critical analysis of internal reliability for uncertainty quantification of dense image matching in multi-view stereo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09379">http://arxiv.org/abs/2309.09379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debao Huang, Rongjun Qin</li>
<li>for: 该研究用于分析多视角摄影探测数据的内部可靠性，尤其是在无referencedata情况下。</li>
<li>methods: 该研究使用了多视角摄影幂等的内部匹配度量，包括射线聚合统计、交叉角度统计、DIM能量等。</li>
<li>results: 研究发现，使用不同的内部匹配度量可以对多视角摄影探测数据的内部可靠性进行评估，尤其是在LiDAR参照数据不 disponible情况下。<details>
<summary>Abstract</summary>
Nowadays, photogrammetrically derived point clouds are widely used in many civilian applications due to their low cost and flexibility in acquisition. Typically, photogrammetric point clouds are assessed through reference data such as LiDAR point clouds. However, when reference data are not available, the assessment of photogrammetric point clouds may be challenging. Since these point clouds are algorithmically derived, their accuracies and precisions are highly varying with the camera networks, scene complexity, and dense image matching (DIM) algorithms, and there is no standard error metric to determine per-point errors. The theory of internal reliability of camera networks has been well studied through first-order error estimation of Bundle Adjustment (BA), which is used to understand the errors of 3D points assuming known measurement errors. However, the measurement errors of the DIM algorithms are intricate to an extent that every single point may have its error function determined by factors such as pixel intensity, texture entropy, and surface smoothness. Despite the complexity, there exist a few common metrics that may aid the process of estimating the posterior reliability of the derived points, especially in a multi-view stereo (MVS) setup when redundancies are present. In this paper, by using an aerial oblique photogrammetric block with LiDAR reference data, we analyze several internal matching metrics within a common MVS framework, including statistics in ray convergence, intersection angles, DIM energy, etc.
</details>
<details>
<summary>摘要</summary>
现在，由光ogrammetry derive的点云在多种民用应用中广泛使用，主要因为它们的成本低廉和捕捉方式灵活。通常，光ogrammetric点云通过参考数据 such as LiDAR点云进行评估。然而，当参考数据不 disponible时，评估光ogrammetric点云可能具有挑战。由于这些点云是算法 derive的，其准确性和精度与摄像机网络、场景复杂度和密集图像匹配（DIM）算法有高度相关。而且没有标准的错误度量来确定每个点的错误。在摄像机网络的内部可靠性理论方面，已经进行了广泛的研究，包括第一个错误估计的Bundle Adjustment（BA）理论，以便理解3D点的错误。然而，DIM算法中的测量错误是复杂到每个点都有自己的错误函数，它们取决于因素如像素强度、текстура杂度和表面平滑性。尽管如此，还是有一些常见的度量可以帮助估计 derivated 点云的 posterior 可靠性，特别是在多视点雷达（MVS）设置中，当redundancy 存在时。在本文中，我们使用了一个飞行倾斜的光ogrammetric块，与LiDAR参考数据进行比较，分析了一些内部匹配度量，包括射线整合度、交叉角度、DIM能量等。
</details></li>
</ul>
<hr>
<h2 id="MOVIN-Real-time-Motion-Capture-using-a-Single-LiDAR"><a href="#MOVIN-Real-time-Motion-Capture-using-a-Single-LiDAR" class="headerlink" title="MOVIN: Real-time Motion Capture using a Single LiDAR"></a>MOVIN: Real-time Motion Capture using a Single LiDAR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09314">http://arxiv.org/abs/2309.09314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deok-Kyeong Jang, Dongseok Yang, Deok-Yun Jang, Byeoli Choi, Taeil Jin, Sung-Hee Lee<br>for:* 这个论文是为了解决现有的全身跟踪系统过于昂贵、需要专业技能运行或者穿着不适的问题，提供一种数据驱动的生成方法来实现实时全身跟踪。methods:* 这个方法使用了一个LiDAR传感器来获取3D点云数据，并使用一个自适应 conditional variational autoencoder（CVAE）模型来学习全身 pose 的分布。results:* 该方法可以准确地预测表演者的3D全身信息和局部关节细节，同时具有考虑时间相关性移动的能力。Here’s the simplified Chinese text in the format you requested:for:* 这个论文是为了解决现有的全身跟踪系统过于昂贵、需要专业技能运行或者穿着不适的问题，提供一种数据驱动的生成方法来实现实时全身跟踪。methods:* 这个方法使用了一个LiDAR传感器来获取3D点云数据，并使用一个自适应 conditional variational autoencoder（CVAE）模型来学习全身 pose 的分布。results:* 该方法可以准确地预测表演者的3D全身信息和局部关节细节，同时具有考虑时间相关性移动的能力。<details>
<summary>Abstract</summary>
Recent advancements in technology have brought forth new forms of interactive applications, such as the social metaverse, where end users interact with each other through their virtual avatars. In such applications, precise full-body tracking is essential for an immersive experience and a sense of embodiment with the virtual avatar. However, current motion capture systems are not easily accessible to end users due to their high cost, the requirement for special skills to operate them, or the discomfort associated with wearable devices. In this paper, we present MOVIN, the data-driven generative method for real-time motion capture with global tracking, using a single LiDAR sensor. Our autoregressive conditional variational autoencoder (CVAE) model learns the distribution of pose variations conditioned on the given 3D point cloud from LiDAR.As a central factor for high-accuracy motion capture, we propose a novel feature encoder to learn the correlation between the historical 3D point cloud data and global, local pose features, resulting in effective learning of the pose prior. Global pose features include root translation, rotation, and foot contacts, while local features comprise joint positions and rotations. Subsequently, a pose generator takes into account the sampled latent variable along with the features from the previous frame to generate a plausible current pose. Our framework accurately predicts the performer's 3D global information and local joint details while effectively considering temporally coherent movements across frames. We demonstrate the effectiveness of our architecture through quantitative and qualitative evaluations, comparing it against state-of-the-art methods. Additionally, we implement a real-time application to showcase our method in real-world scenarios. MOVIN dataset is available at \url{https://movin3d.github.io/movin_pg2023/}.
</details>
<details>
<summary>摘要</summary>
现代技术的发展带来了新的互动应用程序，如社交Metaverse，在这些应用程序中，用户通过他们的虚拟人物进行互动。在这些应用程序中，精准全身跟踪是实现卷积体验和虚拟人物embodying的关键。然而，目前的动作捕捉系统因为高价格、需要特殊技能运行以及穿着设备不舒适而不太可 accessible。在这篇论文中，我们提出了MOVIN，一种基于数据驱动的生成方法，通过单个LiDAR感知器实现实时动作捕捉，包括全身跟踪。我们的 autoencoder 模型学习了基于给定的 3D 点云数据的动作变化的分布，并通过一种新的特征编码器来学习 pose 的相关性。全身pose特征包括根据翻译、旋转和脚 contacts，而地方特征包括 JOINT 位置和旋转。然后，一个 pose 生成器将考虑 Sampled 随机变量以及前一帧的特征来生成一个可能的当前 pose。我们的框架可以准确预测演员的 3D 全身信息和局部关节细节，同时考虑了在帧之间的时间准确性。我们通过量化和质量评估来证明我们的架构的有效性，并与当前的方法进行比较。此外，我们还实现了一个实时应用，以示出我们的方法在实际场景中的应用。MOVIN 数据集可以在 \url{https://movin3d.github.io/movin_pg2023/} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-Debiasing-Frame-Length-Bias-in-Text-Video-Retrieval-via-Causal-Intervention"><a href="#Towards-Debiasing-Frame-Length-Bias-in-Text-Video-Retrieval-via-Causal-Intervention" class="headerlink" title="Towards Debiasing Frame Length Bias in Text-Video Retrieval via Causal Intervention"></a>Towards Debiasing Frame Length Bias in Text-Video Retrieval via Causal Intervention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09311">http://arxiv.org/abs/2309.09311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Burak Satar, Hongyuan Zhu, Hanwang Zhang, Joo Hwee Lim</li>
<li>for: 本研究旨在解决文本视频检索中的学习和推理偏见问题。</li>
<li>methods: 本研究使用了一种独特的方法，即考虑视频帧长度差异对于训练和测试集的影响，以避免学习和推理偏见。</li>
<li>results: 研究发现，该方法可以减少文本视频检索模型中的偏见问题，并在 Epic-Kitchens-100、YouCook2 和 MSR-VTT 等 datasets 上达到了领先的成绩。<details>
<summary>Abstract</summary>
Many studies focus on improving pretraining or developing new backbones in text-video retrieval. However, existing methods may suffer from the learning and inference bias issue, as recent research suggests in other text-video-related tasks. For instance, spatial appearance features on action recognition or temporal object co-occurrences on video scene graph generation could induce spurious correlations. In this work, we present a unique and systematic study of a temporal bias due to frame length discrepancy between training and test sets of trimmed video clips, which is the first such attempt for a text-video retrieval task, to the best of our knowledge. We first hypothesise and verify the bias on how it would affect the model illustrated with a baseline study. Then, we propose a causal debiasing approach and perform extensive experiments and ablation studies on the Epic-Kitchens-100, YouCook2, and MSR-VTT datasets. Our model overpasses the baseline and SOTA on nDCG, a semantic-relevancy-focused evaluation metric which proves the bias is mitigated, as well as on the other conventional metrics.
</details>
<details>
<summary>摘要</summary>
In this work, we present a unique and systematic study of a temporal bias due to frame length discrepancy between training and test sets of trimmed video clips, which is the first such attempt for a text-video retrieval task, to the best of our knowledge. We first hypothesize and verify the bias using a baseline study. Then, we propose a causal debiasing approach and perform extensive experiments and ablation studies on the Epic-Kitchens-100, YouCook2, and MSR-VTT datasets. Our model outperforms the baseline and SOTA on nDCG, a semantic-relevancy-focused evaluation metric, which proves that the bias is mitigated, as well as on other conventional metrics.
</details></li>
</ul>
<hr>
<h2 id="UGC-Unified-GAN-Compression-for-Efficient-Image-to-Image-Translation"><a href="#UGC-Unified-GAN-Compression-for-Efficient-Image-to-Image-Translation" class="headerlink" title="UGC: Unified GAN Compression for Efficient Image-to-Image Translation"></a>UGC: Unified GAN Compression for Efficient Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09310">http://arxiv.org/abs/2309.09310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxi Ren, Jie Wu, Peng Zhang, Manlin Zhang, Xuefeng Xiao, Qian He, Rui Wang, Min Zheng, Xin Pan</li>
<li>for: 本研究旨在提出一种新的学习模式，即统一gan压缩（UGC），以实现模型高效和标签高效的同时满足。</li>
<li>methods: 本研究使用 semi-supervised-driven 网络架构搜索和自适应在线 semi-supervised distillation 两个阶段，共同实现一种多样化、标签高效和性能优秀的模型。</li>
<li>results: 实验结果表明，UGC 可以在各种图像识别和生成任务上达到比较高的性能水平，而且比传统的 GAN 模型具有更好的计算效率和数据使用效率。<details>
<summary>Abstract</summary>
Recent years have witnessed the prevailing progress of Generative Adversarial Networks (GANs) in image-to-image translation. However, the success of these GAN models hinges on ponderous computational costs and labor-expensive training data. Current efficient GAN learning techniques often fall into two orthogonal aspects: i) model slimming via reduced calculation costs; ii)data/label-efficient learning with fewer training data/labels. To combine the best of both worlds, we propose a new learning paradigm, Unified GAN Compression (UGC), with a unified optimization objective to seamlessly prompt the synergy of model-efficient and label-efficient learning. UGC sets up semi-supervised-driven network architecture search and adaptive online semi-supervised distillation stages sequentially, which formulates a heterogeneous mutual learning scheme to obtain an architecture-flexible, label-efficient, and performance-excellent model.
</details>
<details>
<summary>摘要</summary>
近年来，人工智能领域内的生成对抗网络（GAN）在图像到图像翻译方面取得了很大进步。然而，GAN模型的成功受到了计算成本的约束和训练数据的劳动成本。当前的高效GAN学习技术通常分为两个垂直方面：i）模型缩减通过减少计算成本；ii）数据/标签高效学习 fewer 训练数据/标签。为了结合这两个方面的优点，我们提出了一种新的学习理念，即统一GAN压缩（UGC），它通过统一优化目标来融合模型高效和标签高效的学习。UGC设计了顺序执行 semi-supervised 驱动网络搜索和自适应在线 semi-supervised 熔化阶段，这种异质共同学习方案可以从数据中提取出高效、标签高效和表现优秀的模型。
</details></li>
</ul>
<hr>
<h2 id="Effective-Image-Tampering-Localization-via-Enhanced-Transformer-and-Co-attention-Fusion"><a href="#Effective-Image-Tampering-Localization-via-Enhanced-Transformer-and-Co-attention-Fusion" class="headerlink" title="Effective Image Tampering Localization via Enhanced Transformer and Co-attention Fusion"></a>Effective Image Tampering Localization via Enhanced Transformer and Co-attention Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09306">http://arxiv.org/abs/2309.09306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Guo, Haochen Zhu, Gang Cao</li>
<li>for: 本文提出了一种基于两极性增强变换encoder的图像修改地点检测网络（EITLNet），以提高图像修改检测的精度和Robustness。</li>
<li>methods: 本文使用了一种两极性增强变换encoder，并设计了一个特性增强模块来提高变换器encoder的特征表示能力。另外，通过均匀注意力模块在多个级别进行特征对比，以实现RGB和杂变流中提取的特征的有效拼接。</li>
<li>results: 实验结果表明，提出的方案在多个标准数据集上达到了当前最佳的总体化能力和Robustness。代码将于<a target="_blank" rel="noopener" href="https://github.com/multimediaFor/EITLNet%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/multimediaFor/EITLNet公开。</a><details>
<summary>Abstract</summary>
Powerful manipulation techniques have made digital image forgeries be easily created and widespread without leaving visual anomalies. The blind localization of tampered regions becomes quite significant for image forensics. In this paper, we propose an effective image tampering localization network (EITLNet) based on a two-branch enhanced transformer encoder with attention-based feature fusion. Specifically, a feature enhancement module is designed to enhance the feature representation ability of the transformer encoder. The features extracted from RGB and noise streams are fused effectively by the coordinate attention-based fusion module at multiple scales. Extensive experimental results verify that the proposed scheme achieves the state-of-the-art generalization ability and robustness in various benchmark datasets. Code will be public at https://github.com/multimediaFor/EITLNet.
</details>
<details>
<summary>摘要</summary>
powerful manipulation techniques have made digital image forgeries easily created and widespread without leaving visual anomalies. The blind localization of tampered regions becomes quite significant for image forensics. In this paper, we propose an effective image tampering localization network (EITLNet) based on a two-branch enhanced transformer encoder with attention-based feature fusion. Specifically, a feature enhancement module is designed to enhance the feature representation ability of the transformer encoder. The features extracted from RGB and noise streams are fused effectively by the coordinate attention-based fusion module at multiple scales. Extensive experimental results verify that the proposed scheme achieves the state-of-the-art generalization ability and robustness in various benchmark datasets. Code will be public at https://github.com/multimediaFor/EITLNet.Here's the translation in Traditional Chinese:powerful manipulation techniques have made digital image forgeries easily created and widespread without leaving visual anomalies. The blind localization of tampered regions becomes quite significant for image forensics. In this paper, we propose an effective image tampering localization network (EITLNet) based on a two-branch enhanced transformer encoder with attention-based feature fusion. Specifically, a feature enhancement module is designed to enhance the feature representation ability of the transformer encoder. The features extracted from RGB and noise streams are fused effectively by the coordinate attention-based fusion module at multiple scales. Extensive experimental results verify that the proposed scheme achieves the state-of-the-art generalization ability and robustness in various benchmark datasets. Code will be public at https://github.com/multimediaFor/EITLNet.
</details></li>
</ul>
<hr>
<h2 id="RenderIH-A-Large-scale-Synthetic-Dataset-for-3D-Interacting-Hand-Pose-Estimation"><a href="#RenderIH-A-Large-scale-Synthetic-Dataset-for-3D-Interacting-Hand-Pose-Estimation" class="headerlink" title="RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation"></a>RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09301">http://arxiv.org/abs/2309.09301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adwardlee/renderih">https://github.com/adwardlee/renderih</a></li>
<li>paper_authors: Lijun Li, Linrui Tian, Xindi Zhang, Qi Wang, Bang Zhang, Liefeng Bo, Mengyuan Liu, Chen Chen</li>
<li>for: 提高手势估计精度，增加数据的多样性和自然性。</li>
<li>methods: 使用 RenderIH 大规模生成 Synthetic 数据，并提出一种新的 pose 优化算法和一种基于 transformer 的 pose 估计网络 TransHand。</li>
<li>results: 实验表明，预训练在 RenderIH 数据上可以显著降低误差，从 6.76mm 降低至 5.79mm，并且 TransHand 超越了当前的方法。<details>
<summary>Abstract</summary>
The current interacting hand (IH) datasets are relatively simplistic in terms of background and texture, with hand joints being annotated by a machine annotator, which may result in inaccuracies, and the diversity of pose distribution is limited. However, the variability of background, pose distribution, and texture can greatly influence the generalization ability. Therefore, we present a large-scale synthetic dataset RenderIH for interacting hands with accurate and diverse pose annotations. The dataset contains 1M photo-realistic images with varied backgrounds, perspectives, and hand textures. To generate natural and diverse interacting poses, we propose a new pose optimization algorithm. Additionally, for better pose estimation accuracy, we introduce a transformer-based pose estimation network, TransHand, to leverage the correlation between interacting hands and verify the effectiveness of RenderIH in improving results. Our dataset is model-agnostic and can improve more accuracy of any hand pose estimation method in comparison to other real or synthetic datasets. Experiments have shown that pretraining on our synthetic data can significantly decrease the error from 6.76mm to 5.79mm, and our Transhand surpasses contemporary methods. Our dataset and code are available at https://github.com/adwardlee/RenderIH.
</details>
<details>
<summary>摘要</summary>
当前的互动手（IH）数据集相对简单，背景和文化环境相对有限，手关节被机器注意者注解，可能会导致错误，手姿 distribuition 的多样性也很有限。然而，背景、手姿分布和文化环境的变化可以对泛化能力产生很大的影响。因此，我们提供了一个大规模的 sintetic 数据集 RenderIH，包含100万个真实的、多样的互动手图像，具有多种背景、视角和手 texture。为生成自然和多样的互动姿势，我们提议了一个新的pose优化算法。此外，为更好地优化pose估计精度，我们引入了一种基于 transformer 的 pose估计网络 TransHand，以利用互动手之间的相关性。我们的数据集是model-agnostic，可以提高任何手姿估计方法的准确性，比较其他真实或 sintetic 数据集。我们的实验表明，预训练于我们的 sintetic 数据可以显著降低错误率，从6.76mm降低到5.79mm，而我们的 TransHand 突破了当今方法。我们的数据集和代码可以在 https://github.com/adwardlee/RenderIH 上下载。
</details></li>
</ul>
<hr>
<h2 id="Chasing-Day-and-Night-Towards-Robust-and-Efficient-All-Day-Object-Detection-Guided-by-an-Event-Camera"><a href="#Chasing-Day-and-Night-Towards-Robust-and-Efficient-All-Day-Object-Detection-Guided-by-an-Event-Camera" class="headerlink" title="Chasing Day and Night: Towards Robust and Efficient All-Day Object Detection Guided by an Event Camera"></a>Chasing Day and Night: Towards Robust and Efficient All-Day Object Detection Guided by an Event Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09297">http://arxiv.org/abs/2309.09297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahang Cao, Xu Zheng, Yuanhuiyi Lyu, Jiaxu Wang, Renjing Xu, Lin Wang</li>
<li>for: 这篇论文目的是提出一种robust和高效的all-day对象检测方法，以适应实际应用中的各种照明条件。</li>
<li>methods: 该方法基于一个轻量级的射频神经网络（SNN），并使用事件模式来高效地利用异步性。另外，我们还提出了一个事件时间注意力（ETA）模块，以学习事件中的高时间信息，同时保持重要的边缘信息。最后，我们提出了一种新的对RGB-事件特征的Symmetric RGB-Event Fusion（SREF）模块，以有效地融合RGB-事件特征，无需依赖于特定的感知模式。</li>
<li>results: 我们的EOLO方法在各种照明条件下表现出色，与现状的最佳方法（RENet）相比，增加了3.74%的mAP50。我们还建立了两个新的数据集，E-MSCOCO和E-VOC，以便进一步验证和改进我们的方法。<details>
<summary>Abstract</summary>
The ability to detect objects in all lighting (i.e., normal-, over-, and under-exposed) conditions is crucial for real-world applications, such as self-driving.Traditional RGB-based detectors often fail under such varying lighting conditions.Therefore, recent works utilize novel event cameras to supplement or guide the RGB modality; however, these methods typically adopt asymmetric network structures that rely predominantly on the RGB modality, resulting in limited robustness for all-day detection. In this paper, we propose EOLO, a novel object detection framework that achieves robust and efficient all-day detection by fusing both RGB and event modalities. Our EOLO framework is built based on a lightweight spiking neural network (SNN) to efficiently leverage the asynchronous property of events. Buttressed by it, we first introduce an Event Temporal Attention (ETA) module to learn the high temporal information from events while preserving crucial edge information. Secondly, as different modalities exhibit varying levels of importance under diverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion (SREF) module to effectively fuse RGB-Event features without relying on a specific modality, thus ensuring a balanced and adaptive fusion for all-day detection. In addition, to compensate for the lack of paired RGB-Event datasets for all-day training and evaluation, we propose an event synthesis approach based on the randomized optical flow that allows for directly generating the event frame from a single exposure image. We further build two new datasets, E-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC. Extensive experiments demonstrate that our EOLO outperforms the state-of-the-art detectors,e.g.,RENet,by a substantial margin (+3.74% mAP50) in all lighting conditions.Our code and datasets will be available at https://vlislab22.github.io/EOLO/
</details>
<details>
<summary>摘要</summary>
“能够检测各种照明（正常、过颤、和UNDER-EXPOSED）的能力是实际应用中的重要要素，例如自动驾驶。传统的RGB基于的探测器经常在这些不同的照明条件下失败。因此，现有的工作将使用新的事件摄像机来补充或导引RGB模式；然而，这些方法通常运用不对称的网络结构，导致仅对RGB模式进行有限的可靠性。在这篇文章中，我们提出了EOLO框架，一个可靠且高效的实现了全天探测的物体探测框架。我们的EOLO框架基于轻量级的神经网络（SNN），以有效地利用事件的异步性。此外，我们首先引入了一个事件时间注意力（ETA）模组，以学习事件中的高时间信息，同时保留重要的边缘信息。其次，由于不同的模式在不同的照明条件下展现出不同的重要性，我们提出了一个新的对称RGB-事件融合（SREF）模组，以有效地融合RGB-事件特征，并确保在所有照明条件下实现平衡和适应的融合。此外，为了补充缺乏RGB-事件的对称训练和评估数据，我们提出了一个基于随机抽象流的事件生成方法，允许从单一曝光图像中直接生成事件帧。 finally，我们建立了两个新的数据集，E-MSCOCO和E-VOC，基于知名的benchmark MSCOCO和PASCAL VOC。实验结果显示，我们的EOLO在所有照明条件下明显超过了现有的检测器，例如RENet，+3.74% mAP50。我们的代码和数据将在https://vlislab22.github.io/EOLO/ 网页上公开。”
</details></li>
</ul>
<hr>
<h2 id="LivelySpeaker-Towards-Semantic-Aware-Co-Speech-Gesture-Generation"><a href="#LivelySpeaker-Towards-Semantic-Aware-Co-Speech-Gesture-Generation" class="headerlink" title="LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation"></a>LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09294">http://arxiv.org/abs/2309.09294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyhbili/LivelySpeaker">https://github.com/zyhbili/LivelySpeaker</a></li>
<li>paper_authors: Yihao Zhi, Xiaodong Cun, Xuelin Chen, Xi Shen, Wen Guo, Shaoli Huang, Shenghua Gao</li>
<li>For: The paper is focused on developing a framework for generating co-speech gestures that are semantically aligned with the speech content, and it aims to provide several control handles for various applications.* Methods: The proposed framework consists of two stages: script-based gesture generation and audio-guided rhythm refinement. The script-based gesture generation uses pre-trained CLIP text embeddings as guidance, while the audio-guided rhythm refinement uses a simple but effective diffusion-based gesture generation backbone conditioned on audio signals.* Results: The proposed framework outperforms competing methods in terms of semantic awareness and rhythm alignment, and it also achieves state-of-the-art performance on two benchmarks. Additionally, the framework provides several applications such as changing the gesticulation style, editing co-speech gestures via textual prompting, and controlling the semantic awareness and rhythm alignment with guided diffusion.Here’s the simplified Chinese version of the three key points:* For: 这篇论文是关于开发一种基于语音内容的协调姿势生成框架，并提供了多种控制处理的目的。* Methods: 该框架包括两个阶段：脚本基于的姿势生成和音频导向的协调姿势细化。脚本基于的姿势生成使用预训练的 CLIP 文本嵌入为导航，而音频导向的姿势细化使用简单 yet effective 的扩散基本模型， conditioned on 音频信号。* Results: 该框架比前一代方法更加具有 semantics-aware 和 rhythm alignment 的优势，并在两个标准测试集上达到了领先的性能。此外，该框架还提供了多种应用，例如修改姿势风格、通过文本提示编辑协调姿势、控制semantic awareness和rhythm alignment with guided diffusion。<details>
<summary>Abstract</summary>
Gestures are non-verbal but important behaviors accompanying people's speech. While previous methods are able to generate speech rhythm-synchronized gestures, the semantic context of the speech is generally lacking in the gesticulations. Although semantic gestures do not occur very regularly in human speech, they are indeed the key for the audience to understand the speech context in a more immersive environment. Hence, we introduce LivelySpeaker, a framework that realizes semantics-aware co-speech gesture generation and offers several control handles. In particular, our method decouples the task into two stages: script-based gesture generation and audio-guided rhythm refinement. Specifically, the script-based gesture generation leverages the pre-trained CLIP text embeddings as the guidance for generating gestures that are highly semantically aligned with the script. Then, we devise a simple but effective diffusion-based gesture generation backbone simply using pure MLPs, that is conditioned on only audio signals and learns to gesticulate with realistic motions. We utilize such powerful prior to rhyme the script-guided gestures with the audio signals, notably in a zero-shot setting. Our novel two-stage generation framework also enables several applications, such as changing the gesticulation style, editing the co-speech gestures via textual prompting, and controlling the semantic awareness and rhythm alignment with guided diffusion. Extensive experiments demonstrate the advantages of the proposed framework over competing methods. In addition, our core diffusion-based generative model also achieves state-of-the-art performance on two benchmarks. The code and model will be released to facilitate future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统方法可以生成与语音节奏同步的手势，但是它们缺乏语音Semantic上的信息。虽然semantic手势在人类语言交流中并不太常见，但它们对听众理解语言场景的更深入的含义是非常重要的。因此，我们介绍了LivelySpeaker框架，它可以实现语音Semantic-aware co-speech手势生成，并提供了多个控制把柄。具体来说，我们的方法分为两个阶段：脚本基于的手势生成和音频导航的rhythm refinement。特别是，我们使用预训练的CLIP文本嵌入为生成高度semantic相align的手势的指导。然后，我们设计了一种简单 yet powerful的扩散基于多层perceptron（MLP）的手势生成模型，该模型通过只使用音频信号来生成真实的手势动作。我们利用这种强大的优先来谱匹配script-guided手势与音频信号，特别是在零扩展设定下。我们的新的两个阶段生成框架还具有多种应用，例如改变手势风格，通过文本提示编辑副音频手势，以及控制semantic awareness和rhythm alignment的扩散导航。我们的实验表明，我们的提议的框架比前一代方法具有更多的优势。此外，我们的核心扩散生成模型也在两个标准测试集上达到了状态的艺术性表现。我们计划将代码和模型发布，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="MVP-Meta-Visual-Prompt-Tuning-for-Few-Shot-Remote-Sensing-Image-Scene-Classification"><a href="#MVP-Meta-Visual-Prompt-Tuning-for-Few-Shot-Remote-Sensing-Image-Scene-Classification" class="headerlink" title="MVP: Meta Visual Prompt Tuning for Few-Shot Remote Sensing Image Scene Classification"></a>MVP: Meta Visual Prompt Tuning for Few-Shot Remote Sensing Image Scene Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09276">http://arxiv.org/abs/2309.09276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Zhu, Yiying Li, Chunping Qiu, Ke Yang, Naiyang Guan, Xiaodong Yi</li>
<li>for: 这个研究的目的是提出一个高效且灵活的几步演练类别模型，专门适用于遥感图像分类 задачі。</li>
<li>methods: 这个研究使用了已经预训的视觉对应器模型，并将其调整为适合遥感图像分类任务。另外，研究人员还提出了一个基于patch嵌入重复的资料增强技术，以增强Scene的表现和多样性。</li>
<li>results: 实验结果显示，提出的MVP方法在不同的设定下（包括不同的way和shot）均具有较好的性能，并且在跨领域适应中也具有良好的表现。<details>
<summary>Abstract</summary>
Vision Transformer (ViT) models have recently emerged as powerful and versatile models for various visual tasks. Recently, a work called PMF has achieved promising results in few-shot image classification by utilizing pre-trained vision transformer models. However, PMF employs full fine-tuning for learning the downstream tasks, leading to significant overfitting and storage issues, especially in the remote sensing domain. In order to tackle these issues, we turn to the recently proposed parameter-efficient tuning methods, such as VPT, which updates only the newly added prompt parameters while keeping the pre-trained backbone frozen. Inspired by VPT, we propose the Meta Visual Prompt Tuning (MVP) method. Specifically, we integrate the VPT method into the meta-learning framework and tailor it to the remote sensing domain, resulting in an efficient framework for Few-Shot Remote Sensing Scene Classification (FS-RSSC). Furthermore, we introduce a novel data augmentation strategy based on patch embedding recombination to enhance the representation and diversity of scenes for classification purposes. Experiment results on the FS-RSSC benchmark demonstrate the superior performance of the proposed MVP over existing methods in various settings, such as various-way-various-shot, various-way-one-shot, and cross-domain adaptation.
</details>
<details>
<summary>摘要</summary>
视野变换器（ViT）模型最近在视觉任务中表现出了强大和通用的能力。其中，一项工作名为PMF在几个shot图像分类中获得了可观的结果，但PMF使用全部精度调整，导致重要遗传和存储问题，尤其在远程感知领域。为解决这些问题，我们转向 reciently proposed 参数精度调整方法，如VPT，该方法只更新添加的提示参数，而保留预训练的背部锁定。 inspirited by VPT，我们提出了元视觉提示调整方法（MVP）。specifically，我们将VPT方法 integrate into 元学习框架，并适应远程感知领域，从而实现了高效的几个shot远程感知场景分类（FS-RSSC）。此外，我们引入了一种新的数据增强策略基于贴图嵌入重编，以提高分类目的场景表示和多样性。FS-RSSC标准测试集实验结果表明，我们提出的MVP方法在不同的设置下（如多种多shot、一种多shot和跨领域适应）都与现有方法进行了比较，并达到了更好的表现。
</details></li>
</ul>
<hr>
<h2 id="LiDAR-Data-Synthesis-with-Denoising-Diffusion-Probabilistic-Models"><a href="#LiDAR-Data-Synthesis-with-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models"></a>LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09256">http://arxiv.org/abs/2309.09256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuto Nakashima, Ryo Kurazume</li>
<li>for: 本研究旨在生成高精度的3D LiDAR数据，用于自适应移动机器人的规划和控制。</li>
<li>methods: 我们提出了一种基于DDPMs的生成模型，通过对图像表示的距离和反射强度进行描述，生成多种和高精度的3D场景点云。我们还提出了一种灵活的LiDAR完成管线，使用DDPMs的强大特性来实现。</li>
<li>results: 我们的方法在KITTI-360和KITTI-Raw数据集上的生成任务和KITTI-360数据集上的upsampling任务中表现出色，超过了基eline。我们的代码和预训练参数将在<a target="_blank" rel="noopener" href="https://github.com/kazuto1011/r2dm%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/kazuto1011/r2dm上提供。</a><details>
<summary>Abstract</summary>
Generative modeling of 3D LiDAR data is an emerging task with promising applications for autonomous mobile robots, such as scalable simulation, scene manipulation, and sparse-to-dense completion of LiDAR point clouds. Existing approaches have shown the feasibility of image-based LiDAR data generation using deep generative models while still struggling with the fidelity of generated data and training instability. In this work, we present R2DM, a novel generative model for LiDAR data that can generate diverse and high-fidelity 3D scene point clouds based on the image representation of range and reflectance intensity. Our method is based on the denoising diffusion probabilistic models (DDPMs), which have demonstrated impressive results among generative model frameworks and have been significantly progressing in recent years. To effectively train DDPMs on the LiDAR domain, we first conduct an in-depth analysis regarding data representation, training objective, and spatial inductive bias. Based on our designed model R2DM, we also introduce a flexible LiDAR completion pipeline using the powerful properties of DDPMs. We demonstrate that our method outperforms the baselines on the generation task of KITTI-360 and KITTI-Raw datasets and the upsampling task of KITTI-360 datasets. Our code and pre-trained weights will be available at https://github.com/kazuto1011/r2dm.
</details>
<details>
<summary>摘要</summary>
“三维LiDAR数据生成是一个emerging任务，具有吸引人的应用前景，如自动移动 robot的广泛simulation、scene操作和LiDAR点云的簇范 completion。现有的方法已经显示了对于深度生成模型的LiDAR数据生成的可能性，但是仍然面临生成数据的实际性和训练稳定性问题。在这个工作中，我们提出了R2DM，一个新的LiDAR数据生成模型，可以生成多样和高实际性的三维Scene点云，基于影像表现的距离和反射intensity。我们的方法基于减误散射概率模型（DDPMs），这些模型在生成模型框架中已经显示出了杰出的成果，并在最近几年内有所进步。为了有效地对LiDAR领域训练 DDPMs，我们首先进行了LiDAR领域的深入分析，包括数据表现、训练目标和空间传递偏好。基于我们的设计的R2DM模型，我们也提出了一个灵活的LiDAR完备管线，使用DDPMs的强大特性。我们的方法在KITTI-360和KITTI-Rawdataset上的生成和upsampling任务中表现出色，较基eline的方法更好。我们的代码和预训练 веса将在https://github.com/kazuto1011/r2dm上公开。”
</details></li>
</ul>
<hr>
<h2 id="Convex-Latent-Optimized-Adversarial-Regularizers-for-Imaging-Inverse-Problems"><a href="#Convex-Latent-Optimized-Adversarial-Regularizers-for-Imaging-Inverse-Problems" class="headerlink" title="Convex Latent-Optimized Adversarial Regularizers for Imaging Inverse Problems"></a>Convex Latent-Optimized Adversarial Regularizers for Imaging Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09250">http://arxiv.org/abs/2309.09250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huayu Wang, Chen Luo, Taofeng Xie, Qiyu Jin, Guoqing Chen, Zhuo-Xu Cui, Dong Liang</li>
<li>for: 这个研究旨在提出一个新的、可解释的数据驱动技术，以解决Magnetic Resonance Imaging（MRI）逆问题中的挑战。</li>
<li>methods: 这个方法使用了深度学习（DL）和条件调节的融合，并使用了潜在优化技术来对抗训练一个输入圆体神经网络。</li>
<li>results: 这个研究展示了CLEAR-informed的调节模型能够在真实数据上运行，并且能够稳定地重建图像，即使在测量干扰的情况下。此外，这个方法比 conventinal data-driven技术和传统调节方法更好，具有更高的重建质量和更好的稳定性。<details>
<summary>Abstract</summary>
Recently, data-driven techniques have demonstrated remarkable effectiveness in addressing challenges related to MR imaging inverse problems. However, these methods still exhibit certain limitations in terms of interpretability and robustness. In response, we introduce Convex Latent-Optimized Adversarial Regularizers (CLEAR), a novel and interpretable data-driven paradigm. CLEAR represents a fusion of deep learning (DL) and variational regularization. Specifically, we employ a latent optimization technique to adversarially train an input convex neural network, and its set of minima can fully represent the real data manifold. We utilize it as a convex regularizer to formulate a CLEAR-informed variational regularization model that guides the solution of the imaging inverse problem on the real data manifold. Leveraging its inherent convexity, we have established the convergence of the projected subgradient descent algorithm for the CLEAR-informed regularization model. This convergence guarantees the attainment of a unique solution to the imaging inverse problem, subject to certain assumptions. Furthermore, we have demonstrated the robustness of our CLEAR-informed model, explicitly showcasing its capacity to achieve stable reconstruction even in the presence of measurement interference. Finally, we illustrate the superiority of our approach using MRI reconstruction as an example. Our method consistently outperforms conventional data-driven techniques and traditional regularization approaches, excelling in both reconstruction quality and robustness.
</details>
<details>
<summary>摘要</summary>
最近，数据驱动技术已经在MR图像反问题中表现出了非常出色的效果。然而，这些方法仍然存在一定的可解释性和稳定性的限制。为回应，我们介绍了一种新的可解释的数据驱动方法，即Convex Latent-Optimized Adversarial Regularizers（CLEAR）。CLEAR是一种将深度学习（DL）和变量正则化相结合的新型数据驱动方法。我们使用了潜在优化技术来对输入的凸神经网络进行对抗训练，并将其集的最小值用作一种凸正则化模型，以指导图像反问题的解决。由于其内置的凸性，我们已经证明了对CLEAR-informed的正则化模型的投影下滤链落点梯度下降算法的 converges，这 garanties the attainment of a unique solution to the imaging inverse problem, subject to certain assumptions。此外，我们还证明了我们的CLEAR-informed模型的稳定性，并证明其能够在测量干扰存在时仍然实现稳定的重建。最后，我们使用MRI重建为例子，并证明了我们的方法可以与传统的数据驱动技术和正则化方法相比，在重建质量和稳定性两个方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="LiteTrack-Layer-Pruning-with-Asynchronous-Feature-Extraction-for-Lightweight-and-Efficient-Visual-Tracking"><a href="#LiteTrack-Layer-Pruning-with-Asynchronous-Feature-Extraction-for-Lightweight-and-Efficient-Visual-Tracking" class="headerlink" title="LiteTrack: Layer Pruning with Asynchronous Feature Extraction for Lightweight and Efficient Visual Tracking"></a>LiteTrack: Layer Pruning with Asynchronous Feature Extraction for Lightweight and Efficient Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09249">http://arxiv.org/abs/2309.09249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingmao Wei, Bi Zeng, Jianqi Liu, Li He, Guotian Zeng</li>
<li>for: 这个论文是为了提出一种高速的 transformer-based 视觉跟踪模型，以满足实时 робоτICS应用的需求。</li>
<li>methods: 该模型使用 asynchronous feature extraction 和模版和搜索区域之间的交互来提高特征融合和减少无用计算，并对加载的encoder层进行剪辑以达到更好的性能和速度平衡。</li>
<li>results: 模型在 GOT-10k 测试集上 achiev 65.2% AO，超过了所有之前的轻量级跟踪模型，并在 ONNX 上的 Jetson Orin NX 边缘设备上运行速度超过 100 fps。此外，模型在 NVIDIA 2080Ti GPU 上达到了 171 fps 的运行速度，并在 TrackingNet 测试集上达到了 72.2% AO 和 82.4% AUC。<details>
<summary>Abstract</summary>
The recent advancements in transformer-based visual trackers have led to significant progress, attributed to their strong modeling capabilities. However, as performance improves, running latency correspondingly increases, presenting a challenge for real-time robotics applications, especially on edge devices with computational constraints. In response to this, we introduce LiteTrack, an efficient transformer-based tracking model optimized for high-speed operations across various devices. It achieves a more favorable trade-off between accuracy and efficiency than the other lightweight trackers. The main innovations of LiteTrack encompass: 1) asynchronous feature extraction and interaction between the template and search region for better feature fushion and cutting redundant computation, and 2) pruning encoder layers from a heavy tracker to refine the balnace between performance and speed. As an example, our fastest variant, LiteTrack-B4, achieves 65.2% AO on the GOT-10k benchmark, surpassing all preceding efficient trackers, while running over 100 fps with ONNX on the Jetson Orin NX edge device. Moreover, our LiteTrack-B9 reaches competitive 72.2% AO on GOT-10k and 82.4% AUC on TrackingNet, and operates at 171 fps on an NVIDIA 2080Ti GPU. The code and demo materials will be available at https://github.com/TsingWei/LiteTrack.
</details>
<details>
<summary>摘要</summary>
Recent advancements in transformer-based visual trackers have led to significant progress, thanks to their strong modeling capabilities. However, as performance improves, running latency correspondingly increases, presenting a challenge for real-time robotics applications, especially on edge devices with computational constraints. In response to this, we introduce LiteTrack, an efficient transformer-based tracking model optimized for high-speed operations across various devices. It achieves a more favorable trade-off between accuracy and efficiency than other lightweight trackers. The main innovations of LiteTrack include:1. 异步特征提取和模板与搜索区域之间的交互，以实现更好的特征融合和减少 redundancy computation。2. 从一个重量级的跟踪器中剪辑encoder层，以进一步整合性和速度的平衡。例如，我们最快的变体LiteTrack-B4在GOT-10k标准测试集上达到了65.2%的AO，超过了所有之前的高效跟踪器，并在ONNX上的Jetson Orin NX边缘设备上运行速度达100帧/秒。此外，我们的LiteTrack-B9在GOT-10k和TrackingNet标准测试集上达到了72.2%的AO和82.4%的AUC，并在NVIDIA 2080Ti GPU上运行速度达171帧/秒。我们将在https://github.com/TsingWei/LiteTrack中提供代码和示例材料。
</details></li>
</ul>
<hr>
<h2 id="Image-level-supervision-and-self-training-for-transformer-based-cross-modality-tumor-segmentation"><a href="#Image-level-supervision-and-self-training-for-transformer-based-cross-modality-tumor-segmentation" class="headerlink" title="Image-level supervision and self-training for transformer-based cross-modality tumor segmentation"></a>Image-level supervision and self-training for transformer-based cross-modality tumor segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09246">http://arxiv.org/abs/2309.09246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malo de Boisredon, Eugene Vorontsov, William Trung Le, Samuel Kadoury<br>for:* 这个研究旨在提高医疗影像分割领域中的自动化医疗影像分类，特别是跨modalities的情况下。methods:* 提出了一个新的半supervised训练策略called MoDATTS，可以实现精准的跨modalities 3D肿瘤分类。* 使用了一个image-to-image translation策略来将不同modalities的影像转换为弹性target volume，以提高对不同modalities的普遍化。* 还引入了一个迭代自训程序来进一步关闭modalities之间的领域差。results:* MoDATTS在CrossMoDA 2022挑战中的reported top Dice score为0.87+&#x2F;-0.04，较其他参赛队伍的方法高。* MoDATTS在跨modalities的Brain Tumor Segmentation任务上显示了consistent的提高，其Dice score比baseline高出10%以上。* MoDATTS可以实现95%的目标supervised模型性能，并且可以透过更多的标注资料来进一步提高性能。<details>
<summary>Abstract</summary>
Deep neural networks are commonly used for automated medical image segmentation, but models will frequently struggle to generalize well across different imaging modalities. This issue is particularly problematic due to the limited availability of annotated data, making it difficult to deploy these models on a larger scale. To overcome these challenges, we propose a new semi-supervised training strategy called MoDATTS. Our approach is designed for accurate cross-modality 3D tumor segmentation on unpaired bi-modal datasets. An image-to-image translation strategy between imaging modalities is used to produce annotated pseudo-target volumes and improve generalization to the unannotated target modality. We also use powerful vision transformer architectures and introduce an iterative self-training procedure to further close the domain gap between modalities. MoDATTS additionally allows the possibility to extend the training to unannotated target data by exploiting image-level labels with an unsupervised objective that encourages the model to perform 3D diseased-to-healthy translation by disentangling tumors from the background. The proposed model achieves superior performance compared to other methods from participating teams in the CrossMoDA 2022 challenge, as evidenced by its reported top Dice score of 0.87+/-0.04 for the VS segmentation. MoDATTS also yields consistent improvements in Dice scores over baselines on a cross-modality brain tumor segmentation task composed of four different contrasts from the BraTS 2020 challenge dataset, where 95% of a target supervised model performance is reached. We report that 99% and 100% of this maximum performance can be attained if 20% and 50% of the target data is additionally annotated, which further demonstrates that MoDATTS can be leveraged to reduce the annotation burden.
</details>
<details>
<summary>摘要</summary>
深度神经网络通常用于自动医疗影像分割，但模型很难泛化到不同的成像方式。这个问题特别是由于有限的标注数据，使得这些模型在更大规模上部署变得困难。为了解决这些挑战，我们提出了一种新的半监督训练策略called MoDATTS。我们的方法适用于精准的三维肿瘤分割不同成像方式的不协调数据集。我们使用 между成像模式之间的图像转换策略生成标注 pseudo-目标Volume，以改善对目标成像模式的泛化。此外，我们使用强大的视觉转换架构，并引入迭代自我训练过程，以进一步减小成像模式之间的领域差。MoDATTS还允许在未标注目标数据上继续训练，通过利用图像水平标签来鼓励模型进行三维疾病到健康的图像翻译，从而分离肿瘤和背景。我们的模型在CrossMoDA 2022挑战中与其他参赛队列表示的方法相比，实现了最高的Dice分数0.87+/-0.04 для VS分割。MoDATTS还在四个不同的脑肿瘤分割任务中实现了相对于基eline的稳定改进，其中95%的目标监督模型性能可以达到。如果添加20%和50%的目标数据，则可以达到99%和100%的最大性能，这进一步证明了MoDATTS可以减少标注占用。
</details></li>
</ul>
<hr>
<h2 id="CaSAR-Contact-aware-Skeletal-Action-Recognition"><a href="#CaSAR-Contact-aware-Skeletal-Action-Recognition" class="headerlink" title="CaSAR: Contact-aware Skeletal Action Recognition"></a>CaSAR: Contact-aware Skeletal Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10001">http://arxiv.org/abs/2309.10001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junan Lin, Zhichao Sun, Enjie Cao, Taein Kwon, Mahdi Rad, Marc Pollefeys</li>
<li>for: 这篇论文的目的是提出一种新的skeletal action recognition方法，以便在AR&#x2F;VR镜头和人机器人交互中使用。</li>
<li>methods: 这篇论文使用了一种新的表示方法，即手指和物体之间的接触点和远离点，以捕捉手指和物体之间的空间关系。</li>
<li>results: 该方法在两个公共数据集上达到了91.3%和98.4%的高精度，比之前的最佳方法高出了10%以上。<details>
<summary>Abstract</summary>
Skeletal Action recognition from an egocentric view is important for applications such as interfaces in AR/VR glasses and human-robot interaction, where the device has limited resources. Most of the existing skeletal action recognition approaches use 3D coordinates of hand joints and 8-corner rectangular bounding boxes of objects as inputs, but they do not capture how the hands and objects interact with each other within the spatial context. In this paper, we present a new framework called Contact-aware Skeletal Action Recognition (CaSAR). It uses novel representations of hand-object interaction that encompass spatial information: 1) contact points where the hand joints meet the objects, 2) distant points where the hand joints are far away from the object and nearly not involved in the current action. Our framework is able to learn how the hands touch or stay away from the objects for each frame of the action sequence, and use this information to predict the action class. We demonstrate that our approach achieves the state-of-the-art accuracy of 91.3% and 98.4% on two public datasets, H2O and FPHA, respectively.
</details>
<details>
<summary>摘要</summary>
skeletal action recognition from an egocentric view 是很重要的，因为它们可以用于AR/VR镜头和人机交互，而这些设备具有有限的资源。现有的大多数skeletal action recognition方法使用手 JOINTS的3D坐标和物体的8个顶点 rectangle bounding box作为输入，但是它们不能捕捉手和物体之间的空间关系。在这篇论文中，我们提出了一个新的框架，即Contact-aware Skeletal Action Recognition（CaSAR）。它使用了一些新的手-物体交互表示，包括：1）手 JOINTS与物体之间的接触点，2）手 JOINTS在物体之外的远离点，这些点在当前动作序列中并不直接参与动作。我们的框架可以在每帧动作序列中学习手与物体之间的接触和远离情况，并使用这些信息预测动作类别。我们证明了我们的方法可以在两个公共数据集上达到91.3%和98.4%的状态态的准确率。
</details></li>
</ul>
<hr>
<h2 id="CryoAlign-feature-based-method-for-global-and-local-3D-alignment-of-EM-density-maps"><a href="#CryoAlign-feature-based-method-for-global-and-local-3D-alignment-of-EM-density-maps" class="headerlink" title="CryoAlign: feature-based method for global and local 3D alignment of EM density maps"></a>CryoAlign: feature-based method for global and local 3D alignment of EM density maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09217">http://arxiv.org/abs/2309.09217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bintao He, Fa Zhang, Chenjie Feng, Jianyi Yang, Xin Gao, Renmin Han</li>
<li>for: density maps的对Alignment和比较，以解释结构信息，如结构不一致性分析和原子模型组装。</li>
<li>methods: 使用本地密度特征描述符来捕捉空间结构相似性，快速建立点对点匹配和稳定定制参数。</li>
<li>results: 在实验评估中，CryoAlign表现出了较高的对Alignment精度和速度，胜过现有的方法。<details>
<summary>Abstract</summary>
Advances on cryo-electron imaging technologies have led to a rapidly increasing number of density maps. Alignment and comparison of density maps play a crucial role in interpreting structural information, such as conformational heterogeneity analysis using global alignment and atomic model assembly through local alignment. Here, we propose a fast and accurate global and local cryo-electron microscopy density map alignment method CryoAlign, which leverages local density feature descriptors to capture spatial structure similarities. CryoAlign is the first feature-based EM map alignment tool, in which the employment of feature-based architecture enables the rapid establishment of point pair correspondences and robust estimation of alignment parameters. Extensive experimental evaluations demonstrate the superiority of CryoAlign over the existing methods in both alignment accuracy and speed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="All-optical-image-denoising-using-a-diffractive-visual-processor"><a href="#All-optical-image-denoising-using-a-diffractive-visual-processor" class="headerlink" title="All-optical image denoising using a diffractive visual processor"></a>All-optical image denoising using a diffractive visual processor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09215">http://arxiv.org/abs/2309.09215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cagatay Isıl, Tianyi Gan, F. Onuralp Ardic, Koray Mentesoglu, Jagrit Digani, Huseyin Karaca, Hanlong Chen, Jingxi Li, Deniz Mengu, Mona Jarrahi, Kaan Akşit, Aydogan Ozcan</li>
<li>for:  removes noise&#x2F;artifacts from input images</li>
<li>methods:  all-optical and non-iterative, using deep learning-enabled analog diffractive image denoiser</li>
<li>results:  efficiently removes salt and pepper noise and image rendering-related spatial artifacts, with an output power efficiency of ~30-40%<details>
<summary>Abstract</summary>
Image denoising, one of the essential inverse problems, targets to remove noise/artifacts from input images. In general, digital image denoising algorithms, executed on computers, present latency due to several iterations implemented in, e.g., graphics processing units (GPUs). While deep learning-enabled methods can operate non-iteratively, they also introduce latency and impose a significant computational burden, leading to increased power consumption. Here, we introduce an analog diffractive image denoiser to all-optically and non-iteratively clean various forms of noise and artifacts from input images - implemented at the speed of light propagation within a thin diffractive visual processor. This all-optical image denoiser comprises passive transmissive layers optimized using deep learning to physically scatter the optical modes that represent various noise features, causing them to miss the output image Field-of-View (FoV) while retaining the object features of interest. Our results show that these diffractive denoisers can efficiently remove salt and pepper noise and image rendering-related spatial artifacts from input phase or intensity images while achieving an output power efficiency of ~30-40%. We experimentally demonstrated the effectiveness of this analog denoiser architecture using a 3D-printed diffractive visual processor operating at the terahertz spectrum. Owing to their speed, power-efficiency, and minimal computational overhead, all-optical diffractive denoisers can be transformative for various image display and projection systems, including, e.g., holographic displays.
</details>
<details>
<summary>摘要</summary>
图像去噪，是一个fundamental inverse problem，目标是从输入图像中除去噪声/特征。通常，计算机执行的数字图像去噪算法会出现延迟，因为它们在图像处理中需要许多迭代。深度学习启用的方法也会引入延迟和计算负担，导致增加的电力消耗。在这里，我们引入了一种光学diffractive图像去噪器，可以非 iteratively 和all-optically清理输入图像中的各种噪声和特征。这种光学图像去噪器包括优化的通过深度学习的透明层，以physically 扰动光模式，使其在输出图像Field-of-View（FoV）中产生折射。我们的结果表明，这种diffractive denoiser可以高效地除去盐和细颗粒噪声，以及图像渲染相关的空间artefacts，从输入相位或Intensity图像中获得 ~30-40%的输出功率效率。我们实验采用了一个3D打印的diffractive视觉处理器，在teraHz频谱下运行。由于它们的速度、功率效率和计算负担很低，光学diffractive denoiser可能会对各种图像显示和投影系统做出巨大的变革，例如投影式显示器。
</details></li>
</ul>
<hr>
<h2 id="Neural-Gradient-Learning-and-Optimization-for-Oriented-Point-Normal-Estimation"><a href="#Neural-Gradient-Learning-and-Optimization-for-Oriented-Point-Normal-Estimation" class="headerlink" title="Neural Gradient Learning and Optimization for Oriented Point Normal Estimation"></a>Neural Gradient Learning and Optimization for Oriented Point Normal Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09211">http://arxiv.org/abs/2309.09211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LeoQLi/NGLO">https://github.com/LeoQLi/NGLO</a></li>
<li>paper_authors: Qing Li, Huifang Feng, Kanle Shi, Yi Fang, Yu-Shen Liu, Zhizhong Han</li>
<li>for: 学习3D点云中的向量场，用于normal估计。</li>
<li>methods: 使用深度学习方法， parameterize对象函数生成点云中的导向量，并使用地方几何学习 angular distance field 进行精细化。</li>
<li>results: 提供了一种robust和精细的normal估计方法，可以抗抗噪、点云异常和点云分布变化。对比 précédents works，提高了normal估计的精度和泛化能力。<details>
<summary>Abstract</summary>
We propose Neural Gradient Learning (NGL), a deep learning approach to learn gradient vectors with consistent orientation from 3D point clouds for normal estimation. It has excellent gradient approximation properties for the underlying geometry of the data. We utilize a simple neural network to parameterize the objective function to produce gradients at points using a global implicit representation. However, the derived gradients usually drift away from the ground-truth oriented normals due to the lack of local detail descriptions. Therefore, we introduce Gradient Vector Optimization (GVO) to learn an angular distance field based on local plane geometry to refine the coarse gradient vectors. Finally, we formulate our method with a two-phase pipeline of coarse estimation followed by refinement. Moreover, we integrate two weighting functions, i.e., anisotropic kernel and inlier score, into the optimization to improve the robust and detail-preserving performance. Our method efficiently conducts global gradient approximation while achieving better accuracy and generalization ability of local feature description. This leads to a state-of-the-art normal estimator that is robust to noise, outliers and point density variations. Extensive evaluations show that our method outperforms previous works in both unoriented and oriented normal estimation on widely used benchmarks. The source code and pre-trained models are available at https://github.com/LeoQLi/NGLO.
</details>
<details>
<summary>摘要</summary>
我们提出了神经Gradient学习（NGL），一种深度学习方法，用于从3D点云中学习具有一致方向的梯度 вектор。它具有优秀的梯度近似性特性，用于下面的数据结构。我们使用了简单的神经网络来参数化目标函数，以生成点上的梯度。然而， derivated梯度通常会偏离实际的正见方向的 норма，因为缺乏本地细节描述。因此，我们引入了梯度向量优化（GVO），以学习基于本地平面几何的angular distance场，以重фине粗略梯度向量。最后，我们将方法拟合成为两阶段管道，首先进行粗略估计，然后进行细化。此外，我们将两个权重函数，即不同权重的kernel和准确度分数，integrated into the optimization，以提高方法的稳定性和细节描述能力。我们的方法可以高效地进行全局梯度近似，同时实现更高的准确性和地方特征描述能力。这使得我们的方法在噪声、异常点和点云变化等问题上具有更高的Robustness和普遍性。我们对广泛使用的标准准点Cloud进行了广泛的评估，并证明了我们的方法在不oriented和oriented normal estimation中具有state-of-the-art的性能。源代码和预训练模型可以在https://github.com/LeoQLi/NGLO中下载。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-SLAM-Helps-Deep-Learning-based-LiDAR-Perception-Tasks"><a href="#Differentiable-SLAM-Helps-Deep-Learning-based-LiDAR-Perception-Tasks" class="headerlink" title="Differentiable SLAM Helps Deep Learning-based LiDAR Perception Tasks"></a>Differentiable SLAM Helps Deep Learning-based LiDAR Perception Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09206">http://arxiv.org/abs/2309.09206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prashant Kumar, Dheeraj Vattikonda, Vedang Bhupesh Shenvi Nadkarni, Erqun Dong, Sabyasachi Sahoo</li>
<li>for: 这paper是为了研究一种新的自然语言处理方法，使用可微分SLAM架构来训练深度学习模型。</li>
<li>methods: 这paper使用了一种新的自然语言处理方法，即使用可微分SLAM架构来训练深度学习模型。</li>
<li>results: 实验结果表明，使用可微分SLAM架构可以提高两种深度学习应用程序（地面水平估计和动态到静止LiDAR翻译）的性能。总的来说，这些发现提供了重要的navidad的提高LiDAR基于导航系统性能的新方法。<details>
<summary>Abstract</summary>
We investigate a new paradigm that uses differentiable SLAM architectures in a self-supervised manner to train end-to-end deep learning models in various LiDAR based applications. To the best of our knowledge there does not exist any work that leverages SLAM as a training signal for deep learning based models. We explore new ways to improve the efficiency, robustness, and adaptability of LiDAR systems with deep learning techniques. We focus on the potential benefits of differentiable SLAM architectures for improving performance of deep learning tasks such as classification, regression as well as SLAM. Our experimental results demonstrate a non-trivial increase in the performance of two deep learning applications - Ground Level Estimation and Dynamic to Static LiDAR Translation, when used with differentiable SLAM architectures. Overall, our findings provide important insights that enhance the performance of LiDAR based navigation systems. We demonstrate that this new paradigm of using SLAM Loss signal while training LiDAR based models can be easily adopted by the community.
</details>
<details>
<summary>摘要</summary>
我们investigates一种新的思想，利用可微分SLAM架构来在自我超vised的方式下训练深度学习模型，用于各种LiDAR应用程序中。我们知道到目前为止，没有任何工作利用SLAM作为训练深度学习模型的信号。我们探索新的方法，以提高LiDAR系统的效率、可靠性和适应性。我们关注使用可微分SLAM架构来改善深度学习任务的性能，如分类、回归以及SLAM。我们的实验结果表明，将SLAM损失信号作为训练深度学习模型的一部分，可以提高Ground Level Estimation和Dynamic to Static LiDAR Translation两个深度学习应用程序的性能。总的来说，我们的发现提供了重要的意见，使LiDAR基于导航系统的性能得到了提高。我们示示了这新的思想可以轻松地被社区采纳。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Pyramid-Channel-Attention-Network-for-Pathological-Myopia-Detection"><a href="#Efficient-Pyramid-Channel-Attention-Network-for-Pathological-Myopia-Detection" class="headerlink" title="Efficient Pyramid Channel Attention Network for Pathological Myopia Detection"></a>Efficient Pyramid Channel Attention Network for Pathological Myopia Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09196">http://arxiv.org/abs/2309.09196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tommylitlle/epcanet">https://github.com/tommylitlle/epcanet</a></li>
<li>paper_authors: Xiaoqing Zhang, Jilu Zhao, Richu Jin, Yan Li, Hao Wu, Xiangtian Zhou, Jiang Liu</li>
<li>for: 检测普遍性近视（PM）的早期检测，以避免视力和失明。</li>
<li>methods: 利用注意模块设计，包括全面通道注意模块（EPCA），在特征图中高效地检测全球和本地病理信息。</li>
<li>results: 在三个数据集上进行了广泛的实验，证明了我们的EPCA-Net在检测PM方面的表现超过了现有方法。此外，我们还尝试了预训练和终端调整方法，并证明了与传统终端调整方法相比，我们的方法在参数更少的情况下实现了竞争力的表现。<details>
<summary>Abstract</summary>
Pathological myopia (PM) is the leading ocular disease for impaired vision and blindness worldwide. The key to detecting PM as early as possible is to detect informative features in global and local lesion regions, such as fundus tessellation, atrophy and maculopathy. However, applying classical convolutional neural networks (CNNs) to efficiently highlight global and local lesion context information in feature maps is quite challenging. To tackle this issue, we aim to fully leverage the potential of global and local lesion information with attention module design. Based on this, we propose an efficient pyramid channel attention (EPCA) module, which dynamically explores the relative importance of global and local lesion context information in feature maps. Then we combine the EPCA module with the backbone network to construct EPCA-Net for automatic PM detection based on fundus images. In addition, we construct a PM dataset termed PM-fundus by collecting fundus images of PM from publicly available datasets (e.g., the PALM dataset and ODIR dataset). The comprehensive experiments are conducted on three datasets, demonstrating that our EPCA-Net outperforms state-of-the-art methods in detecting PM. Furthermore, motivated by the recent pretraining-and-finetuning paradigm, we attempt to adapt pre-trained natural image models for PM detection by freezing them and treating the EPCA module and other attention modules as the adapters. The results show that our method with the pretraining-and-finetuning paradigm achieves competitive performance through comparisons to part of methods with traditional fine-tuning methods with fewer tunable parameters.
</details>
<details>
<summary>摘要</summary>
全球最主要的眼睛疾病之一是病理型近视（PM），它是全球视力和失明的主要原因。检测PM的关键在于检测背景和局部 lesion 区域中有用的特征。然而，使用传统的卷积神经网络（CNNs）来快速提取全球和局部 lesion 上下文信息在特征图中是很困难的。为了解决这个问题，我们想要全面利用全球和局部 lesion 信息的潜在能力，并设计了一种受注意模块（Attention Module）。基于这种设计，我们提出了一种高效的pyramid channel attention（EPCA）模块，它可以动态探索特征图中全球和局部 lesion 上下文信息的相对重要性。然后，我们将EPCA模块与背部网络结合，构建EPCA-Net以自动检测基于眼科图像的PM。此外，我们还构建了PM-fundus数据集，收集了PM眼科图像数据。我们在三个数据集上进行了广泛的实验，结果表明，我们的EPCA-Net可以超越现有方法的检测PM性能。此外，我们还尝试了采用先training-and-finetuning的方法，将预训练的自然图像模型适应PM检测。结果表明，我们的方法可以通过与一些传统 fine-tuning 方法进行比较，实现类似的性能。
</details></li>
</ul>
<hr>
<h2 id="CLIPUNetr-Assisting-Human-robot-Interface-for-Uncalibrated-Visual-Servoing-Control-with-CLIP-driven-Referring-Expression-Segmentation"><a href="#CLIPUNetr-Assisting-Human-robot-Interface-for-Uncalibrated-Visual-Servoing-Control-with-CLIP-driven-Referring-Expression-Segmentation" class="headerlink" title="CLIPUNetr: Assisting Human-robot Interface for Uncalibrated Visual Servoing Control with CLIP-driven Referring Expression Segmentation"></a>CLIPUNetr: Assisting Human-robot Interface for Uncalibrated Visual Servoing Control with CLIP-driven Referring Expression Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09183">http://arxiv.org/abs/2309.09183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Jiang, Yuchen Yang, Martin Jagersand</li>
<li>for: 这个论文目的是提高人机交互式视觉服务（UIBVS）中的图像基于视觉服务（image-based visual servoing，IBVS）精度和效率，使用 Referring Expression Segmentation（RES）技术提供更多的准确信息，以帮助机器人在 manipulate 任务中更好地理解人类的意图。</li>
<li>methods: 这个论文使用了一种新的 Referring Expression Segmentation 网络（CLIPUNetr），该网络利用 CLIP 的强大视觉语言表示能力来 segment 引用表达中的区域，同时利用其“U-shaped”编码器-解码器架构来生成更加精确的预测。此外，论文还提出了一种新的整体管道，用于将 CLIPUNetr 集成到 UIBVS 中，并在实际世界环境中应用。</li>
<li>results: 实验表明，使用 CLIPUNetr 可以提高边界和结构测量的准确性，平均提高120%，并成功地帮助实际世界中的 UIBVS 控制。<details>
<summary>Abstract</summary>
The classical human-robot interface in uncalibrated image-based visual servoing (UIBVS) relies on either human annotations or semantic segmentation with categorical labels. Both methods fail to match natural human communication and convey rich semantics in manipulation tasks as effectively as natural language expressions. In this paper, we tackle this problem by using referring expression segmentation, which is a prompt-based approach, to provide more in-depth information for robot perception. To generate high-quality segmentation predictions from referring expressions, we propose CLIPUNetr - a new CLIP-driven referring expression segmentation network. CLIPUNetr leverages CLIP's strong vision-language representations to segment regions from referring expressions, while utilizing its ``U-shaped'' encoder-decoder architecture to generate predictions with sharper boundaries and finer structures. Furthermore, we propose a new pipeline to integrate CLIPUNetr into UIBVS and apply it to control robots in real-world environments. In experiments, our method improves boundary and structure measurements by an average of 120% and can successfully assist real-world UIBVS control in an unstructured manipulation environment.
</details>
<details>
<summary>摘要</summary>
传统的人机交互界面在无调整图像基于视觉服务（UIBVS）中依赖于人类注释或semantic segmentation WITH categorical标签。这两种方法无法匹配人类自然的沟通方式，并且不能够具备rich semantics在操作任务中。在这篇论文中，我们解决这个问题，使用引用表达分 segmentation，以提供更多的信息来提高机器人的感知。为生成高质量的分 segmentation预测，我们提出了CLIPUNetr，一种基于CLIP的引用表达分 segmentation网络。CLIPUNetr利用CLIP的强视语表示能力，从引用表达中提取区域，同时利用其“U字形”编码器-解码器架构，生成预测更加精细的边界和结构。此外，我们提出了一种新的管道，将CLIPUNetr纳入UIBVS中，并在实际世界环境中控制机器人。在实验中，我们发现，我们的方法可以提高边界和结构测量的平均值120%，并成功地帮助实际世界中的UIBVS控制。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.CV_2023_09_17/" data-id="clogxf3n400hi5xraa5hp9a5d" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.AI_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T12:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.AI_2023_09_17/">cs.AI - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ChatGPT-Hallucinates-when-Attributing-Answers"><a href="#ChatGPT-Hallucinates-when-Attributing-Answers" class="headerlink" title="ChatGPT Hallucinates when Attributing Answers"></a>ChatGPT Hallucinates when Attributing Answers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09401">http://arxiv.org/abs/2309.09401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guido Zuccon, Bevan Koopman, Razia Shaik</li>
<li>for: The paper aims to investigate the ability of ChatGPT to provide evidence to support its answers and to analyze the quality of the references it suggests.</li>
<li>methods: The paper uses a collection of domain-specific knowledge-based questions to prompt ChatGPT to provide answers and supporting evidence in the form of references to external sources.</li>
<li>results: The paper finds that ChatGPT provides correct or partially correct answers in about half of the cases (50.6% of the times), but its suggested references only exist 14% of the times. The generated references reveal common traits and often do not support the claims ChatGPT attributes to them.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是调查ChatGPT是否能够提供证据支持其答案，以及其建议的参考文献的质量。</li>
<li>methods: 论文使用具体领域知识基础问题来让ChatGPT提供答案和相关参考文献。</li>
<li>results: 论文发现ChatGPT在50.6%的情况下提供正确或部分正确的答案，但建议的参考文献只有14%存在。生成的参考文献显示出共同特征，并常常不支持ChatGPT所归功于它们的说法。<details>
<summary>Abstract</summary>
Can ChatGPT provide evidence to support its answers? Does the evidence it suggests actually exist and does it really support its answer? We investigate these questions using a collection of domain-specific knowledge-based questions, specifically prompting ChatGPT to provide both an answer and supporting evidence in the form of references to external sources. We also investigate how different prompts impact answers and evidence. We find that ChatGPT provides correct or partially correct answers in about half of the cases (50.6% of the times), but its suggested references only exist 14% of the times. We further provide insights on the generated references that reveal common traits among the references that ChatGPT generates, and show how even if a reference provided by the model does exist, this reference often does not support the claims ChatGPT attributes to it. Our findings are important because (1) they are the first systematic analysis of the references created by ChatGPT in its answers; (2) they suggest that the model may leverage good quality information in producing correct answers, but is unable to attribute real evidence to support its answers. Prompts, raw result files and manual analysis are made publicly available.
</details>
<details>
<summary>摘要</summary>
Can ChatGPT 提供证据支持其答案？Does the evidence it suggests 真的存在，并且确实支持其答案？我们通过一个领域专门知识基础的问题集来调查这些问题，具体来说是让 ChatGPT 提供答案和证据的形式为外部源引用。我们还发现了不同的提问对答案和证据的影响。我们发现 ChatGPT 在50.6% 的情况下提供正确或部分正确的答案，但是提供的参考文献只有14% 的情况下存在。我们进一步分析生成的参考文献，发现这些参考文献具有共同特征，并且即使参考文献确实存在，它们通常不支持 ChatGPT 所归功于它们的说法。我们的发现对（1）是首次系统性地分析 ChatGPT 答案中的参考文献，（2）表明模型可能在生成正确答案时利用了好几个信息，但是无法归功于它们的证据。我们提供的提问、 raw 结果文件和手动分析将公开发布。
</details></li>
</ul>
<hr>
<h2 id="CulturaX-A-Cleaned-Enormous-and-Multilingual-Dataset-for-Large-Language-Models-in-167-Languages"><a href="#CulturaX-A-Cleaned-Enormous-and-Multilingual-Dataset-for-Large-Language-Models-in-167-Languages" class="headerlink" title="CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages"></a>CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09400">http://arxiv.org/abs/2309.09400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen</li>
<li>for: 这个论文的目的是为了提高大型自然语言处理（LLM）模型的学习能力，并且将其让到公众使用，以促进更深入的研究和应用。</li>
<li>methods: 该论文使用了严格的整理和筛选程序来清洁和精确地准确地训练大型自然语言处理模型。</li>
<li>results: 该论文发现，这些精确地训练的大型自然语言处理模型在多种语言中的表现优化，并且可以用于多种应用。<details>
<summary>Abstract</summary>
The driving factors behind the development of large language models (LLMs) with impressive learning capabilities are their colossal model sizes and extensive training datasets. Along with the progress in natural language processing, LLMs have been frequently made accessible to the public to foster deeper investigation and applications. However, when it comes to training datasets for these LLMs, especially the recent state-of-the-art models, they are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is fully released to the public in HuggingFace to facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的发展因素包括其巨大的模型大小和广泛的训练数据。随着自然语言处理的进步，LLMs 经常被公开提供，以便更深入的研究和应用。然而，LLMs 的训练数据的进展却有一定的障碍。特别是最新的状态艺术模型，其训练数据经常不是完全公布的。为了创建高性能 LLMS 的训练数据，需要进行广泛的清洗和去重，以确保必要的质量水平。由于训练数据的不透明度，因此对 LLMS 的幻觉和偏见问题的研究和解决受到了阻碍，这也阻碍了社区的进一步发展。在多语言学习场景下，可以用的多语言文本数据库往往不够，而且清洗和去重的过程也往往不充分。为了解决这个问题，我们介绍了 CulturaX，一个具有6.3亿个字的167种语言的大型多语言数据集，适用于 LLMS 的开发。我们的数据集经过了多 Stage 的严格清洗和去重管道，包括语言标识、URL 基于的筛选、度量基于的清洗、文档级别和数据去重，以确保模型训练时的最佳质量。CulturaX 被完全公开发布到 HuggingFace，以便社区的研究和发展：https://huggingface.co/datasets/uonlp/CulturaX。
</details></li>
</ul>
<hr>
<h2 id="Do-Large-GPT-Models-Discover-Moral-Dimensions-in-Language-Representations-A-Topological-Study-Of-Sentence-Embeddings"><a href="#Do-Large-GPT-Models-Discover-Moral-Dimensions-in-Language-Representations-A-Topological-Study-Of-Sentence-Embeddings" class="headerlink" title="Do Large GPT Models Discover Moral Dimensions in Language Representations? A Topological Study Of Sentence Embeddings"></a>Do Large GPT Models Discover Moral Dimensions in Language Representations? A Topological Study Of Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09397">http://arxiv.org/abs/2309.09397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Fitz</li>
<li>for: 这篇论文的目的是研究基于GPT-3.5的语言模型 internal structure，以及这些模型在训练过程中对公平性的影响。</li>
<li>methods: 该论文使用了GPT-3.5语言模型的基础模型，并使用了社会心理学 литературе中的公平度度量来分析模型的内部结构。</li>
<li>results: 研究发现，基于GPT-3.5的语言模型可以将 sentences embedding decomposed into two submanifolds，表示公平和不公平的道德判断。这表明GPT-基于语言模型在其表示空间中发展了道德维度，并在训练过程中学习了公平性的概念。<details>
<summary>Abstract</summary>
As Large Language Models are deployed within Artificial Intelligence systems, that are increasingly integrated with human society, it becomes more important than ever to study their internal structures. Higher level abilities of LLMs such as GPT-3.5 emerge in large part due to informative language representations they induce from raw text data during pre-training on trillions of words. These embeddings exist in vector spaces of several thousand dimensions, and their processing involves mapping between multiple vector spaces, with total number of parameters on the order of trillions. Furthermore, these language representations are induced by gradient optimization, resulting in a black box system that is hard to interpret. In this paper, we take a look at the topological structure of neuronal activity in the "brain" of Chat-GPT's foundation language model, and analyze it with respect to a metric representing the notion of fairness. We develop a novel approach to visualize GPT's moral dimensions. We first compute a fairness metric, inspired by social psychology literature, to identify factors that typically influence fairness assessments in humans, such as legitimacy, need, and responsibility. Subsequently, we summarize the manifold's shape using a lower-dimensional simplicial complex, whose topology is derived from this metric. We color it with a heat map associated with this fairness metric, producing human-readable visualizations of the high-dimensional sentence manifold. Our results show that sentence embeddings based on GPT-3.5 can be decomposed into two submanifolds corresponding to fair and unfair moral judgments. This indicates that GPT-based language models develop a moral dimension within their representation spaces and induce an understanding of fairness during their training process.
</details>
<details>
<summary>摘要</summary>
As Large Language Models (LLMs) 在人工智能系统中部署，与人类社会越来越紧密相连，研究其内部结构变得更加重要。 GPT-3.5 等高级 LLM 的高级能力在大部分由 Raw text data 中抽象出的语言表示所带来，这些表示在 vector space 中存在数千维度的向量空间中，并且处理涉及多个 vector space 之间的映射，总参数数在万亿级别。此外，这些语言表示是通过梯度优化来实现的黑盒系统，具有很难理解的特点。在这篇论文中，我们将研究 Chat-GPT 的基础语言模型中神经元活动的 topological structure，并对其进行分析，以了解它们是如何实现公平性的。我们开发了一种新的方法，可以将 GPT 的道德维度视觉化。我们首先计算了一种公平度量， drawing inspiration from social psychology literature，以确定在人类中常见的公平评价因素，如合法性、需求和责任。然后，我们将 manifold 的形状摘要为一个几何体，其Topology是基于这种公平度量来确定的。我们将这个几何体用一个与公平度量相关的热图进行颜色标记，从而生成可读的 visualization 图像，以便更好地理解高级语言模型中的道德维度。我们的结果表明，基于 GPT-3.5 的 sentence embeddings 可以分解为两个子 manifold，每个子 manifold 都表示一种公平或不公平的道德评价。这表明 GPT 基于的语言模型在其表示空间中发展了道德维度，并在训练过程中学习了公平性。
</details></li>
</ul>
<hr>
<h2 id="Talk2Care-Facilitating-Asynchronous-Patient-Provider-Communication-with-Large-Language-Model"><a href="#Talk2Care-Facilitating-Asynchronous-Patient-Provider-Communication-with-Large-Language-Model" class="headerlink" title="Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model"></a>Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09357">http://arxiv.org/abs/2309.09357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Yang, Xuhai Xu, Bingsheng Yao, Shao Zhang, Ethan Rogers, Stephen Intille, Nawar Shara, Guodong, Gao, Dakuo Wang</li>
<li>for: 这个研究的目的是为了探索大语言模型（LLMs）在长期健康照顾和跨代通信中的潜在作用。</li>
<li>methods: 这个研究使用了两个访谈研究，一个是older adults（N&#x3D;10），另一个是健康提供者（N&#x3D;9），以了解他们对LLMs在患者-医疗提供者异步通信中的需求和机会。然后，他们根据这些洞察，建立了一个LLM-powered通信系统，名为Talk2Care，并设计了与两个群体互动的元件：一个是为older adults，他们使用语音助手（VAs）来实现有效的资讯收集；另一个是为健康提供者，他们建立了LLM-基于的互动画面，以 SUMMARIZE和显示老年人对VAs的访谈中重要的健康资讯。</li>
<li>results: 这两个使用者研究显示，Talk2Care可以帮助患者和医疗提供者之间的通信 процес，增加患者对健康信息的收集，并对医疗提供者的努力和时间节省几成。我们视这个研究为LLMs在医疗和人际通信之间的初步探索。<details>
<summary>Abstract</summary>
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults' conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers' efforts and time. We envision our work as an initial exploration of LLMs' capability in the intersection of healthcare and interpersonal communication.
</details>
<details>
<summary>摘要</summary>
尽管现有许多电健康应用程序可以帮助家庭older adults和医疗提供者，但是基本的消息和电话仍然是最常用的通信方法，这些方法受到有限的可用性、信息损失和流程不充分的限制。一种有前途的解决方案是利用大语言模型（LLMs），它们具有强大的自然语言对话和总结能力。然而，对LLMs在通信中的角色的理解还很有限。我们首先进行了10名older adults和9名医疗提供者的两次采访，以了解他们的需求和LLMs在患者-医生异步通信中的机会。基于这些发现，我们建立了一个LLM-powered通信系统，名为Talk2Care，并为两个群体设计了互动组件：1.  дляolder adults，我们利用了voice assistant（VA）的便捷和可达性，并为他们建立了LLM-powered VA界面，以便有效地收集健康信息。2.  для医疗提供者，我们建立了LLM基于的概要摘要界面，以显示older adults和VA之间的对话中重要的医疗信息。我们进行了两次用户研究，以评估Talk2Care的可用性。结果显示，Talk2Care可以改善患者-医生的通信过程，拓宽来自older adults的健康信息，并大幅减少医疗提供者的努力和时间。我们认为，我们的工作是LLMs在医疗和人际通信的交叉点的初步探索。
</details></li>
</ul>
<hr>
<h2 id="Speech-Gesture-GAN-Gesture-Generation-for-Robots-and-Embodied-Agents"><a href="#Speech-Gesture-GAN-Gesture-Generation-for-Robots-and-Embodied-Agents" class="headerlink" title="Speech-Gesture GAN: Gesture Generation for Robots and Embodied Agents"></a>Speech-Gesture GAN: Gesture Generation for Robots and Embodied Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09346">http://arxiv.org/abs/2309.09346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carson Yu Liu, Gelareh Mohammadi, Yang Song, Wafa Johal</li>
<li>for: 这 paper 的目的是为了帮助机器人和embodied agents在人类与人类交互中更好地表达他们的态度、情感和意图。</li>
<li>methods: 这 paper 使用了一种基于 Conditional Generative Adversarial Network (GAN) 的神经网络模型，学习了语音输入中的协作姿势和语音特征之间的关系。</li>
<li>results: 试验结果表明，这个姿势生成框架可以帮助机器人和embodied agents更好地与人类交互，并且可以在对话中表达他们的态度和情感。<details>
<summary>Abstract</summary>
Embodied agents, in the form of virtual agents or social robots, are rapidly becoming more widespread. In human-human interactions, humans use nonverbal behaviours to convey their attitudes, feelings, and intentions. Therefore, this capability is also required for embodied agents in order to enhance the quality and effectiveness of their interactions with humans. In this paper, we propose a novel framework that can generate sequences of joint angles from the speech text and speech audio utterances. Based on a conditional Generative Adversarial Network (GAN), our proposed neural network model learns the relationships between the co-speech gestures and both semantic and acoustic features from the speech input. In order to train our neural network model, we employ a public dataset containing co-speech gestures with corresponding speech audio utterances, which were captured from a single male native English speaker. The results from both objective and subjective evaluations demonstrate the efficacy of our gesture-generation framework for Robots and Embodied Agents.
</details>
<details>
<summary>摘要</summary>
现代智能机器人和虚拟代理人正在广泛应用。人类在人际交流中使用非语言行为表达自己的态度、情感和意图。因此，这种能力也是智能机器人需要的，以提高与人类交流的质量和效率。在这篇论文中，我们提出了一种新的姿势生成框架，可以根据语音文本和语音音频utterances生成肢体姿势。我们的提议的神经网络模型学习了语音输入中的听力和语义特征与手势之间的关系。为了训练我们的神经网络模型，我们使用了一个公共数据集，包括与语音音频utterances对应的手势记录，从单一的男性Native English speaker中获取。经过对象和主观评估，我们的姿势生成框架在机器人和虚拟代理人中得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Dynamic-Mode-Decomposition-and-Deep-Learning-for-Rainfall-Prediction-in-North-East-India"><a href="#Unleashing-the-Power-of-Dynamic-Mode-Decomposition-and-Deep-Learning-for-Rainfall-Prediction-in-North-East-India" class="headerlink" title="Unleashing the Power of Dynamic Mode Decomposition and Deep Learning for Rainfall Prediction in North-East India"></a>Unleashing the Power of Dynamic Mode Decomposition and Deep Learning for Rainfall Prediction in North-East India</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09336">http://arxiv.org/abs/2309.09336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paleti Nikhil Chowdary, Sathvika P, Pranav U, Rohan S, Sowmya V, Gopalakrishnan E A, Dhanya M</li>
<li>for: 预测北东部印度的降水量，提高灾害防御和减轻气候变化的影响</li>
<li>methods: 使用数据驱动方法Dynamic Mode Decomposition (DMD)和深度学习方法Long Short-Term Memory (LSTM)进行降水量预测，使用印度气象部门每天降水数据进行训练和验证</li>
<li>results: LSTM方法比DMD方法更加准确地预测降水量，表明LSTM方法可以更好地捕捉数据中的复杂非线性关系，这些发现可以帮助提高北东部印度的降水量预测精度，降低气候变化的影响<details>
<summary>Abstract</summary>
Accurate rainfall forecasting is crucial for effective disaster preparedness and mitigation in the North-East region of India, which is prone to extreme weather events such as floods and landslides. In this study, we investigated the use of two data-driven methods, Dynamic Mode Decomposition (DMD) and Long Short-Term Memory (LSTM), for rainfall forecasting using daily rainfall data collected from India Meteorological Department in northeast region over a period of 118 years. We conducted a comparative analysis of these methods to determine their relative effectiveness in predicting rainfall patterns. Using historical rainfall data from multiple weather stations, we trained and validated our models to forecast future rainfall patterns. Our results indicate that both DMD and LSTM are effective in forecasting rainfall, with LSTM outperforming DMD in terms of accuracy, revealing that LSTM has the ability to capture complex nonlinear relationships in the data, making it a powerful tool for rainfall forecasting. Our findings suggest that data-driven methods such as DMD and deep learning approaches like LSTM can significantly improve rainfall forecasting accuracy in the North-East region of India, helping to mitigate the impact of extreme weather events and enhance the region's resilience to climate change.
</details>
<details>
<summary>摘要</summary>
准确预测降水是北东地区灾害准备和mitigation的关键，这里容易受到洪水和山崩等极端天气事件的影响。在这项研究中，我们调查了使用两种数据驱动方法：动态模式分解（DMD）和长期记忆（LSTM），以预测降水。我们使用印度气象部门在北东地区收集的日常降水数据进行训练和验证。我们的结果表明，DMD和LSTM都有效地预测降水，但LSTM在准确性方面表现更好，表明LSTM可以捕捉数据中的复杂非线性关系，使其成为预测降水的强大工具。我们的发现表明，使用数据驱动方法如DMD和深度学习方法如LSTM可以大幅提高降水预测精度，帮助北东地区 Mitigate the impact of extreme weather events and enhance its resilience to climate change.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Knee-Osteoarthritis-severity-level-classification-using-diffusion-augmented-images"><a href="#Enhancing-Knee-Osteoarthritis-severity-level-classification-using-diffusion-augmented-images" class="headerlink" title="Enhancing Knee Osteoarthritis severity level classification using diffusion augmented images"></a>Enhancing Knee Osteoarthritis severity level classification using diffusion augmented images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09328">http://arxiv.org/abs/2309.09328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AKSHAY24-tech/Enhancing-Knee-Osteoarthritis-severity-level-classification-using-diffusion-augmented-images">https://github.com/AKSHAY24-tech/Enhancing-Knee-Osteoarthritis-severity-level-classification-using-diffusion-augmented-images</a></li>
<li>paper_authors: Paleti Nikhil Chowdary, Gorantla V N S L Vishnu Vardhan, Menta Sai Akshay, Menta Sai Aashish, Vadlapudi Sai Aravind, Garapati Venkata Krishna Rayalu, Aswathy P</li>
<li>for: 这个研究 paper 探讨了使用高级计算机视觉模型和扩充技术来分类膝部骨关节炎（OA）严重程度。</li>
<li>methods: 该研究使用了数据预处理，包括强化对比限定静止 histogram 平衡（CLAHE），以及数据扩充使用扩散模型。三个实验进行了：在原始数据集上训练模型，在预处理后的数据集上训练模型，以及在扩充后的数据集上训练模型。</li>
<li>results: 结果显示，数据预处理和扩充可以大幅提高模型的准确率。EfficientNetB3 模型在扩充后的数据集上达到了 84% 的最高准确率。此外，使用 Grad-CAM 等注意力视觉技术，可以提供详细的注意力地图，提高了模型的理解和信任性。这些发现指出，将高级模型与扩充数据和注意力视觉相结合，可以准确地分类膝部骨关节炎严重程度。<details>
<summary>Abstract</summary>
This research paper explores the classification of knee osteoarthritis (OA) severity levels using advanced computer vision models and augmentation techniques. The study investigates the effectiveness of data preprocessing, including Contrast-Limited Adaptive Histogram Equalization (CLAHE), and data augmentation using diffusion models. Three experiments were conducted: training models on the original dataset, training models on the preprocessed dataset, and training models on the augmented dataset. The results show that data preprocessing and augmentation significantly improve the accuracy of the models. The EfficientNetB3 model achieved the highest accuracy of 84\% on the augmented dataset. Additionally, attention visualization techniques, such as Grad-CAM, are utilized to provide detailed attention maps, enhancing the understanding and trustworthiness of the models. These findings highlight the potential of combining advanced models with augmented data and attention visualization for accurate knee OA severity classification.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇研究论文探讨了使用高级计算机视觉模型和扩充技术来分类膝关节风扁病（OA）严重程度的方法。研究检查了数据预处理，包括对比限适的自适应压缩（CLAHE），以及使用扩充模型来进行数据扩充。研究进行了三个实验：在原始数据集上训练模型，在预处理后的数据集上训练模型，以及在扩充后的数据集上训练模型。结果显示，数据预处理和扩充可以显著提高模型的准确率。EfficientNetB3模型在扩充后的数据集上达到了84%的最高准确率。此外，使用Grad-CAM等注意力可视化技术，为模型提供了详细的注意力地图，提高了对模型的理解和信任性。这些发现表明可以通过结合高级模型、扩充数据和注意力可视化来实现精准的膝关节风扁病严重程度分类。
</details></li>
</ul>
<hr>
<h2 id="Answering-Layer-3-queries-with-DiscoSCMs"><a href="#Answering-Layer-3-queries-with-DiscoSCMs" class="headerlink" title="Answering Layer 3 queries with DiscoSCMs"></a>Answering Layer 3 queries with DiscoSCMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09323">http://arxiv.org/abs/2309.09323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heyang Gong</li>
<li>For: This paper aims to address the issue of counterfactual degeneration in causal inference, specifically in the context of Layer 3 valuations and individual-level semantics.* Methods: The paper proposes a novel framework called DiscoSCM, which combines the strengths of both Potential Outcome (PO) and Structural Causal Model (SCM) frameworks, and could be seen as an extension of them. The DiscoSCM framework leverages the philosophy of individual causality to tackle the counterfactual degeneration problem.* Results: The paper demonstrates the superior performance of the DiscoSCM framework in answering counterfactual questions through several key results in the topic of unit select problems. The results show that the DiscoSCM framework can effectively address the issue of counterfactual degeneration and provide more accurate estimates of counterfactual parameters.<details>
<summary>Abstract</summary>
In the realm of causal inference, the primary frameworks are the Potential Outcome (PO) and the Structural Causal Model (SCM), both predicated on the consistency rule. However, when facing Layer 3 valuations, i.e., counterfactual queries that inherently belong to individual-level semantics, they both seem inadequate due to the issue of degeneration caused by the consistency rule. For instance, in personalized incentive scenarios within the internet industry, the probability of one particular user being a complier, denoted as $P(y_x, y'_{x'})$, degenerates to a parameter that can only take values of 0 or 1. This paper leverages the DiscoSCM framework to theoretically tackle the aforementioned counterfactual degeneration problem, which is a novel framework for causal modeling that combines the strengths of both PO and SCM, and could be seen as an extension of them. The paper starts with a brief introduction to the background of causal modeling frameworks. It then illustrates, through an example, the difficulty in recovering counterfactual parameters from data without imposing strong assumptions. Following this, we propose the DiscoSCM with independent potential noise framework to address this problem. Subsequently, the superior performance of the DiscoSCM framework in answering counterfactual questions is demonstrated by several key results in the topic of unit select problems. We then elucidate that this superiority stems from the philosophy of individual causality. In conclusion, we suggest that DiscoSCM may serve as a significant milestone in the causal modeling field for addressing counterfactual queries.
</details>
<details>
<summary>摘要</summary>
在 causal inference 领域，主要框架是 potential outcome (PO) 和 structural causal model (SCM)，都是基于一致性规则。但在面临层 3 评估（counterfactual queries）时，它们都显得不够，这是因为一致性规则导致的半极化问题。例如，在个性化奖励场景下，用户 $x$ 的行为 $y_x$ 的潜在结果 $P(y_x, y'_{x'})$ 会半极化为一个只能取 0 或 1 的参数。本文使用 DiscoSCM 框架来解决上述 counterfactual 半极化问题，这是一种结合 PO 和 SCM 的新框架，可以看作是它们的扩展。文章首先介绍了 causal modeling 框架的背景，然后通过一个例子示出在数据中无法回归 counterfactual 参数的困难。接着，我们提出了 DiscoSCM 独立潜在噪声框架来解决这个问题。文章后续展示了 DiscoSCM 框架在 unit select 问题上的优秀表现，并证明了这种优秀性源于个体 causality 哲学。最后，我们建议 DiscoSCM 可能是 causal modeling 领域内 Answering counterfactual questions 的一个重要突破口。
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-for-Semantic-Segmentation-with-Multi-class-Label-Query"><a href="#Active-Learning-for-Semantic-Segmentation-with-Multi-class-Label-Query" class="headerlink" title="Active Learning for Semantic Segmentation with Multi-class Label Query"></a>Active Learning for Semantic Segmentation with Multi-class Label Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09319">http://arxiv.org/abs/2309.09319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sehyun Hwang, Sohyun Lee, Hoyoung Kim, Minhyeon Oh, Jungseul Ok, Suha Kwak</li>
<li>for: 提出了一种新的活动学习方法 дляsemantic segmentation</li>
<li>methods: 使用了一种新的标注查询设计，采样了本地图像区域（例如超 pix），并向 oracle 请求每个区域的多类标签vector，以解决存在多类标签问题</li>
<li>results: 在Cityscapes和PASCAL VOC 2012上比前一个方法减少了标注成本，并且达到了更高的 segmentation 性能<details>
<summary>Abstract</summary>
This paper proposes a new active learning method for semantic segmentation. The core of our method lies in a new annotation query design. It samples informative local image regions (e.g., superpixels), and for each of such regions, asks an oracle for a multi-hot vector indicating all classes existing in the region. This multi-class labeling strategy is substantially more efficient than existing ones like segmentation, polygon, and even dominant class labeling in terms of annotation time per click. However, it introduces the class ambiguity issue in training since it assigns partial labels (i.e., a set of candidate classes) to individual pixels. We thus propose a new algorithm for learning semantic segmentation while disambiguating the partial labels in two stages. In the first stage, it trains a segmentation model directly with the partial labels through two new loss functions motivated by partial label learning and multiple instance learning. In the second stage, it disambiguates the partial labels by generating pixel-wise pseudo labels, which are used for supervised learning of the model. Equipped with a new acquisition function dedicated to the multi-class labeling, our method outperformed previous work on Cityscapes and PASCAL VOC 2012 while spending less annotation cost.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>In the first stage, the method trains a segmentation model directly with the partial labels using two new loss functions inspired by partial label learning and multiple instance learning.2. In the second stage, the method disambiguates the partial labels by generating pixel-wise pseudo labels, which are used for supervised learning of the model.The proposed method is equipped with a new acquisition function tailored to the multi-class labeling, which leads to better performance on Cityscapes and PASCAL VOC 2012 while reducing the annotation cost.In simplified Chinese, the text can be translated as:这篇论文提出了一种新的活动学习方法 дляsemantic segmentation。这个方法的核心在于一种新的注释查询设计，它将地图区域（例如superpixel）作为批处理，并问 oracle 提供多个类别的多hot вектор。这种多类标注策略比传统的分类、多边形和主导类标注更加高效，但是它会在训练中引入类别不确定性问题，因为每个像素都会被分配一组候选类。为了解决这个问题，该方法提出了两个阶段的方法：1. 在第一阶段，方法直接使用多个类别的partial label进行分类模型的训练，使用两种新的损失函数，启发自partial label学习和多个实例学习。2. 在第二阶段，方法使用像素级别的pseudo标签来解决类别不确定性问题，并将其用于模型的超级vised学习。该方法采用了一种专门为多类标注而设计的新的收购函数，使其在Cityscapes和PASCAL VOC 2012上表现优于之前的工作，同时减少了注释成本。</details></li>
</ol>
<hr>
<h2 id="Deep-Neighbor-Layer-Aggregation-for-Lightweight-Self-Supervised-Monocular-Depth-Estimation"><a href="#Deep-Neighbor-Layer-Aggregation-for-Lightweight-Self-Supervised-Monocular-Depth-Estimation" class="headerlink" title="Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation"></a>Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09272">http://arxiv.org/abs/2309.09272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boya Wang, Shuo Wang, Ziwen Dou, Dong Ye</li>
<li>for: 提高自主导航和机器人视觉系统中 depth estimation 模型的效率，避免使用大型和复杂的网络。</li>
<li>methods: 使用全连接层 fusion 技术，将高分辨率和低分辨率特征相互融合，以保留小目标和快移动物体的信息。采用轻量级频道注意力 Mechanism 来提高depth estimation结果。</li>
<li>results: 在 KITTI 数据集上进行实验，与许多大型模型相比，如 Monodepth2，我们的方法可以达到更高的准确率，仅使用 30 个参数。<details>
<summary>Abstract</summary>
With the frequent use of self-supervised monocular depth estimation in robotics and autonomous driving, the model's efficiency is becoming increasingly important. Most current approaches apply much larger and more complex networks to improve the precision of depth estimation. Some researchers incorporated Transformer into self-supervised monocular depth estimation to achieve better performance. However, this method leads to high parameters and high computation. We present a fully convolutional depth estimation network using contextual feature fusion. Compared to UNet++ and HRNet, we use high-resolution and low-resolution features to reserve information on small targets and fast-moving objects instead of long-range fusion. We further promote depth estimation results employing lightweight channel attention based on convolution in the decoder stage. Our method reduces the parameters without sacrificing accuracy. Experiments on the KITTI benchmark show that our method can get better results than many large models, such as Monodepth2, with only 30 parameters. The source code is available at https://github.com/boyagesmile/DNA-Depth.
</details>
<details>
<summary>摘要</summary>
随着自主导航和机器人领域中深度估计的频繁使用，模型的效率变得越来越重要。现有大多数方法使用更大和更复杂的网络来提高深度估计的精度。一些研究人员在自主导航中采用了Transformer来提高性能，但这会导致高参数和高计算量。我们提出了一种完全 convolutional 的深度估计网络，通过Contextual feature fusion来提高性能。我们使用高分辨率和低分辨率的特征来保留小目标和快速移动的信息，而不是长距离融合。我们还使用轻量级的通道注意力来提高decoder stage中的深度估计结果。我们的方法可以降低参数量不 sacrificing 精度。在 KITTI 标准测试集上，我们的方法可以超越许多大型模型，如 Monodepth2，并且只需30个参数。代码可以在 https://github.com/boyagesmile/DNA-Depth 上获取。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Modeling-of-the-Denoising-Process-for-Speech-Enhancement-Based-on-Deep-Learning"><a href="#Continuous-Modeling-of-the-Denoising-Process-for-Speech-Enhancement-Based-on-Deep-Learning" class="headerlink" title="Continuous Modeling of the Denoising Process for Speech Enhancement Based on Deep Learning"></a>Continuous Modeling of the Denoising Process for Speech Enhancement Based on Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09270">http://arxiv.org/abs/2309.09270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilu Guo, Jun Du, CHin-Hui Lee</li>
<li>for: 这个论文旨在提出一种连续模型基于深度学习的语音听写提升方法，关注听写过程中的干净化。</li>
<li>methods: 这个方法使用状态变量来表示听写过程，起始状态是噪声语音，结束状态是干净的语音。噪声分量在状态变量中随状态索引的变化而减少，直到噪声分量为0。在训练中，一个UNet-like神经网络学习估算每个状态变量，从连续听写过程中抽取样本。在测试中，我们引入一个控制因子作为嵌入，让神经网络控制噪声减少的水平。这种方法实现可控的语音提升和适应不同应用场景。</li>
<li>results: 实验结果表明，保留一小amount的噪声在干净目标中对语音提升具有利于效果，证明了对语音评价指标和自动语音识别性能的改进。<details>
<summary>Abstract</summary>
In this paper, we explore a continuous modeling approach for deep-learning-based speech enhancement, focusing on the denoising process. We use a state variable to indicate the denoising process. The starting state is noisy speech and the ending state is clean speech. The noise component in the state variable decreases with the change of the state index until the noise component is 0. During training, a UNet-like neural network learns to estimate every state variable sampled from the continuous denoising process. In testing, we introduce a controlling factor as an embedding, ranging from zero to one, to the neural network, allowing us to control the level of noise reduction. This approach enables controllable speech enhancement and is adaptable to various application scenarios. Experimental results indicate that preserving a small amount of noise in the clean target benefits speech enhancement, as evidenced by improvements in both objective speech measures and automatic speech recognition performance.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了深度学习基于的连续模型化 speech 减噪方法，强调减噪过程。我们使用状态变量来表示减噪过程。起始状态是噪声的 speech，结束状态是干净的 speech。噪声组分在状态变量中随状态索引的变化而减少，直到噪声组分为 0。在训练中，一种 UNet-like 神经网络学习 estimate 每个来自连续减噪过程的状态变量。在测试中，我们将一个控制因子作为嵌入，范围从 0 到 1，提供给神经网络，以控制噪声减少的水平。这种方法允许可控制的 speech 减噪和适应不同应用场景。实验结果表明，保留一些噪声在干净目标中有助于 speech 减噪，证明了对象测试 Speech 测量指标和自动语音识别性能的改进。
</details></li>
</ul>
<hr>
<h2 id="Sim-to-Real-Deep-Reinforcement-Learning-with-Manipulators-for-Pick-and-place"><a href="#Sim-to-Real-Deep-Reinforcement-Learning-with-Manipulators-for-Pick-and-place" class="headerlink" title="Sim-to-Real Deep Reinforcement Learning with Manipulators for Pick-and-place"></a>Sim-to-Real Deep Reinforcement Learning with Manipulators for Pick-and-place</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09247">http://arxiv.org/abs/2309.09247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxing Liu, Hanlin Niu, Robert Skilton, Joaquin Carrasco</li>
<li>For: This paper proposes a self-supervised vision-based deep reinforcement learning (DRL) method to improve the performance of transferring a DRL model from simulation to the real world.* Methods: The proposed method uses a height-sensitive action policy to deal with crowded and stacked objects in challenging environments. The training model is applied directly to a real suction task without any fine-tuning from the real world.* Results: The proposed method achieves a high suction success rate of 90% in a real experiment with novel objects, without any real-world fine-tuning. An experimental video is available at: <a target="_blank" rel="noopener" href="https://youtu.be/jSTC-EGsoFA.Here">https://youtu.be/jSTC-EGsoFA.Here</a> are the three key points in Simplified Chinese:* For: 这篇论文提出了一种基于自我监督视觉深度学习（DRL）方法，以提高将DRL模型从模拟世界转移到实际世界的性能。* Methods: 该方法使用了高度敏感的动作策略来处理充满杂物和堆叠物的复杂环境。模型直接应用于实际吸取任务，不需要任何实际世界细调。* Results: 该方法在实际实验中以90%的吸取成功率成功地应用于新的物体，无需任何实际世界细调。实验视频可以在：<a target="_blank" rel="noopener" href="https://youtu.be/jSTC-EGsoFA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://youtu.be/jSTC-EGsoFA中找到。</a><details>
<summary>Abstract</summary>
When transferring a Deep Reinforcement Learning model from simulation to the real world, the performance could be unsatisfactory since the simulation cannot imitate the real world well in many circumstances. This results in a long period of fine-tuning in the real world. This paper proposes a self-supervised vision-based DRL method that allows robots to pick and place objects effectively and efficiently when directly transferring a training model from simulation to the real world. A height-sensitive action policy is specially designed for the proposed method to deal with crowded and stacked objects in challenging environments. The training model with the proposed approach can be applied directly to a real suction task without any fine-tuning from the real world while maintaining a high suction success rate. It is also validated that our model can be deployed to suction novel objects in a real experiment with a suction success rate of 90\% without any real-world fine-tuning. The experimental video is available at: https://youtu.be/jSTC-EGsoFA.
</details>
<details>
<summary>摘要</summary>
Note:* " simulation" => 模拟 (mó dì)* "real world"  => 真实世界 (zhēn shí shì jì)* "fine-tuning"  => 微调 (wēi tiān)* "success rate"  => 成功率 (chéng gōng ràng)* "suction"  => 吸引 (xīu yì)* "novel objects"  => 新型物品 (xīn xíng wù jī)
</details></li>
</ul>
<hr>
<h2 id="Detection-and-Localization-of-Firearm-Carriers-in-Complex-Scenes-for-Improved-Safety-Measures"><a href="#Detection-and-Localization-of-Firearm-Carriers-in-Complex-Scenes-for-Improved-Safety-Measures" class="headerlink" title="Detection and Localization of Firearm Carriers in Complex Scenes for Improved Safety Measures"></a>Detection and Localization of Firearm Carriers in Complex Scenes for Improved Safety Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09236">http://arxiv.org/abs/2309.09236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intelligentMachines-ITU/LFC-Dataset">https://github.com/intelligentMachines-ITU/LFC-Dataset</a></li>
<li>paper_authors: Arif Mahmood, Abdul Basit, M. Akhtar Munir, Mohsen Ali</li>
<li>For: 本研究旨在提高人员携带武器的检测和精确定位，以提高安全和监控领域的效果。* Methods: 本研究提出了一种新的方法，利用人员与武器之间的互动信息，以提高携带武器人员的定位。该方法包括一个注意力机制，可以准确分别人员和背景，以及一种稳定性驱动的地方保持约束，以学习重要特征。* Results: 对比基eline方法，本研究的方法在新建的数据集上实现了显著更高的准精度（AP&#x3D;77.8%）。这表明，利用注意力机制和稳定性驱动的地方保持约束可以提高人员携带武器的检测精度。<details>
<summary>Abstract</summary>
Detecting firearms and accurately localizing individuals carrying them in images or videos is of paramount importance in security, surveillance, and content customization. However, this task presents significant challenges in complex environments due to clutter and the diverse shapes of firearms. To address this problem, we propose a novel approach that leverages human-firearm interaction information, which provides valuable clues for localizing firearm carriers. Our approach incorporates an attention mechanism that effectively distinguishes humans and firearms from the background by focusing on relevant areas. Additionally, we introduce a saliency-driven locality-preserving constraint to learn essential features while preserving foreground information in the input image. By combining these components, our approach achieves exceptional results on a newly proposed dataset. To handle inputs of varying sizes, we pass paired human-firearm instances with attention masks as channels through a deep network for feature computation, utilizing an adaptive average pooling layer. We extensively evaluate our approach against existing methods in human-object interaction detection and achieve significant results (AP=77.8\%) compared to the baseline approach (AP=63.1\%). This demonstrates the effectiveness of leveraging attention mechanisms and saliency-driven locality preservation for accurate human-firearm interaction detection. Our findings contribute to advancing the fields of security and surveillance, enabling more efficient firearm localization and identification in diverse scenarios.
</details>
<details>
<summary>摘要</summary>
探测火器和准确地local化携带火器的人在图像或视频中是安全监测和内容个性化的关键问题。然而，这个问题在复杂环境中存在 significativetranslation missing 挑战，主要是因为背景干扰和火器的多样化形状。为解决这个问题，我们提出了一种新的方法，利用人与火器互动信息，该信息提供了关键的携带人Localization的信息。我们的方法包括一个注意力机制，有效地从背景中分离人和火器，并且引入了一个带有Saliency的地方填充约束，以学习 essencial 特征。通过这些组件，我们的方法实现了出色的结果在一个新提出的数据集上。为处理不同大小的输入，我们通过一个深度网络传递paired人与火器实例，并使用适应平均抽取层来计算特征。我们对现有方法进行了广泛的评估，并实现了相比基eline方法（AP=63.1%）显著的结果（AP=77.8%）。这表明了注意力机制和Saliency-driven的地方填充约束对人与火器互动探测的精度具有重要作用。我们的发现对安全监测和识别领域的进步做出了贡献，可以在多种enario中更有效地检测和识别携带火器的人。
</details></li>
</ul>
<hr>
<h2 id="Improving-Speech-Inversion-Through-Self-Supervised-Embeddings-and-Enhanced-Tract-Variables"><a href="#Improving-Speech-Inversion-Through-Self-Supervised-Embeddings-and-Enhanced-Tract-Variables" class="headerlink" title="Improving Speech Inversion Through Self-Supervised Embeddings and Enhanced Tract Variables"></a>Improving Speech Inversion Through Self-Supervised Embeddings and Enhanced Tract Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09220">http://arxiv.org/abs/2309.09220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Adel Attia, Yashish M. Siriwardena, Carol Espy-Wilson</li>
<li>for: 这个论文主要是为了研究吗？</li>
<li>methods: 这个论文使用了哪些方法？</li>
<li>results: 这个论文得到了什么结果？Here are the answers in Simplified Chinese:</li>
<li>for: 这个论文主要是为了研究Acoustic-to-articulatory speech inversion（SI）系统的性能。</li>
<li>methods: 这个论文使用了自动学习模型（SSL） HuBERT 和改进的几何变换模型。</li>
<li>results: 通过结合这两种方法，SI 系统的 Pearson 产品积分相关性（PPMC）分数提高了从 0.7452 到 0.8141，即6.9% 的提高。<details>
<summary>Abstract</summary>
The performance of deep learning models depends significantly on their capacity to encode input features efficiently and decode them into meaningful outputs. Better input and output representation has the potential to boost models' performance and generalization. In the context of acoustic-to-articulatory speech inversion (SI) systems, we study the impact of utilizing speech representations acquired via self-supervised learning (SSL) models, such as HuBERT compared to conventional acoustic features. Additionally, we investigate the incorporation of novel tract variables (TVs) through an improved geometric transformation model. By combining these two approaches, we improve the Pearson product-moment correlation (PPMC) scores which evaluate the accuracy of TV estimation of the SI system from 0.7452 to 0.8141, a 6.9% increase. Our findings underscore the profound influence of rich feature representations from SSL models and improved geometric transformations with target TVs on the enhanced functionality of SI systems.
</details>
<details>
<summary>摘要</summary>
深度学习模型的性能受输入特征编码和输出解码的效率影响很大。更好的输入和输出表示有助于提高模型的性能和泛化。在声音逆转（SI）系统中，我们研究了使用自动监督学习（SSL）模型获得的声音表示，比如胡蜡BERT，与传统的声音特征相比。此外，我们还研究了通过改进的几何变换模型中的新的轨迹变量（TV）的添加。将这两种方法结合起来，可以提高声音逆转系统的归一化积分相互 correlatio（PPMC）分数，从0.7452提高到0.8141，即6.9%提高。我们的发现表明，rich的特征表示从SSL模型和改进的几何变换模型中的target TVs具有杰出的影响，提高了SI系统的功能。
</details></li>
</ul>
<hr>
<h2 id="SplitEE-Early-Exit-in-Deep-Neural-Networks-with-Split-Computing"><a href="#SplitEE-Early-Exit-in-Deep-Neural-Networks-with-Split-Computing" class="headerlink" title="SplitEE: Early Exit in Deep Neural Networks with Split Computing"></a>SplitEE: Early Exit in Deep Neural Networks with Split Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09195">http://arxiv.org/abs/2309.09195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Divya J. Bajpai, Vivek K. Trivedi, Sohan L. Yadav, Manjesh K. Hanawal</li>
<li>for: 这个研究的目的是提高资源受限的设备（边缘、移动、IoT）中深度神经网络（DNNs）的运行性能。</li>
<li>methods: 这个研究使用了分 computed 和早期终结的方法来解决这个问题。具体来说，它们使用了将 computation 分给云端进行最终推断（split computing），并在推断过程中选择性地终结推断。</li>
<li>results: 这个研究获得了较高的成本优化（&gt;50%），并且仅导致了小于2%的准确性下降。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have drawn attention because of their outstanding performance on various tasks. However, deploying full-fledged DNNs in resource-constrained devices (edge, mobile, IoT) is difficult due to their large size. To overcome the issue, various approaches are considered, like offloading part of the computation to the cloud for final inference (split computing) or performing the inference at an intermediary layer without passing through all layers (early exits). In this work, we propose combining both approaches by using early exits in split computing. In our approach, we decide up to what depth of DNNs computation to perform on the device (splitting layer) and whether a sample can exit from this layer or need to be offloaded. The decisions are based on a weighted combination of accuracy, computational, and communication costs. We develop an algorithm named SplitEE to learn an optimal policy. Since pre-trained DNNs are often deployed in new domains where the ground truths may be unavailable and samples arrive in a streaming fashion, SplitEE works in an online and unsupervised setup. We extensively perform experiments on five different datasets. SplitEE achieves a significant cost reduction ($>50\%$) with a slight drop in accuracy ($<2\%$) as compared to the case when all samples are inferred at the final layer. The anonymized source code is available at \url{https://anonymous.4open.science/r/SplitEE_M-B989/README.md}.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）吸引了关注，因其在多种任务上表现出色。然而，在资源有限的设备（边缘、移动、物联网）中部署完整的DNNs困难，因为它们的大小较大。为解决这个问题，一些方法被考虑，如将计算部分提取到云端进行最终推理（分 computation）或在设备上进行推理，而不是将所有层传递。在这种情况下，我们提出了结合这两种方法的方法，即使用早期退出在分 computation中。在我们的方法中，我们可以在设备上进行DNNs计算的深度层（分层），并决定一个样本是否可以在这层退出，或者需要被上传。这些决定是基于精度、计算和通信成本的权重平均值。我们开发了一个名为SplitEE的算法，用于学习优化策略。由于预训练的DNNs常常在新领域中部署，采用新的批处理方式和不可预测的样本流入，SplitEE在线上和无监督的设置下工作。我们对五个不同的数据集进行了广泛的实验。SplitEE可以在计算成本方面实现大于50%的减少，同时减少精度少于2%。详细的源代码可以在 \url{https://anonymous.4open.science/r/SplitEE_M-B989/README.md} 中找到。
</details></li>
</ul>
<hr>
<h2 id="From-Cooking-Recipes-to-Robot-Task-Trees-–-Improving-Planning-Correctness-and-Task-Efficiency-by-Leveraging-LLMs-with-a-Knowledge-Network"><a href="#From-Cooking-Recipes-to-Robot-Task-Trees-–-Improving-Planning-Correctness-and-Task-Efficiency-by-Leveraging-LLMs-with-a-Knowledge-Network" class="headerlink" title="From Cooking Recipes to Robot Task Trees – Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network"></a>From Cooking Recipes to Robot Task Trees – Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09181">http://arxiv.org/abs/2309.09181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sadman Sakib, Yu Sun</li>
<li>For: This paper is written for the task of robotic cooking, specifically in generating a sequence of actions for a robot to prepare a meal successfully.* Methods: The paper introduces a novel task tree generation pipeline that uses a large language model (LLM) to retrieve recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a task tree, capturing sequential and parallel dependencies among subtasks. The pipeline also mitigates the uncertainty and unreliable features of LLM outputs using task tree retrieval.* Results: The paper shows superior performance compared to previous works in task planning accuracy and efficiency, with improved planning correctness and improved execution efficiency.Here is the information in Simplified Chinese text:* For: 这篇论文是为了 robotic cooking 任务进行生成一个Successful sequence of actions。* Methods: 论文提出了一种新的任务树生成管道，使用大语言模型 (LLM)  retrieve recipe instructions，然后使用精度调整后的 GPT-3 将其转换为任务树，捕捉下一个和平行依赖关系。管道还使用任务树检索来降低 LLM 输出的不确定性和不可靠性。* Results: 论文的评估结果显示，与前一些工作相比，它在任务规划正确率和执行效率方面表现出色，得到了改进的规划正确率和执行效率。<details>
<summary>Abstract</summary>
Task planning for robotic cooking involves generating a sequence of actions for a robot to prepare a meal successfully. This paper introduces a novel task tree generation pipeline producing correct planning and efficient execution for cooking tasks. Our method first uses a large language model (LLM) to retrieve recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a task tree, capturing sequential and parallel dependencies among subtasks. The pipeline then mitigates the uncertainty and unreliable features of LLM outputs using task tree retrieval. We combine multiple LLM task tree outputs into a graph and perform a task tree retrieval to avoid questionable nodes and high-cost nodes to improve planning correctness and improve execution efficiency. Our evaluation results show its superior performance compared to previous works in task planning accuracy and efficiency.
</details>
<details>
<summary>摘要</summary>
任务观念生成 для robotic cooking 包括生成一系列动作来成功实现料理任务。本研究提出了一个新的任务树生成管线，可以生成正确的观念和高效的执行 для cooking 任务。我们的方法首先使用大型自然语言模型（LLM） retrieve 菜单 instrucions，然后使用精度调整的 GPT-3 将其转换为任务树，捕捉组成组件和平行依赖关系。然后，我们的管线将 LLM 输出中的不确定和高成本特征使用任务树搜寻来缓解。我们将多个 LLM 任务树输出融合为一个图形，并使用任务树搜寻来避免问题节点和高成本节点，以提高观念正确性和执行效率。我们的评估结果显示，该管线在任务观念正确性和执行效率方面表现更好于先前的工作。
</details></li>
</ul>
<hr>
<h2 id="Neural-Speaker-Diarization-Using-Memory-Aware-Multi-Speaker-Embedding-with-Sequence-to-Sequence-Architecture"><a href="#Neural-Speaker-Diarization-Using-Memory-Aware-Multi-Speaker-Embedding-with-Sequence-to-Sequence-Architecture" class="headerlink" title="Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding with Sequence-to-Sequence Architecture"></a>Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding with Sequence-to-Sequence Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09180">http://arxiv.org/abs/2309.09180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyunlongaaa/NSD-MS2S">https://github.com/liyunlongaaa/NSD-MS2S</a></li>
<li>paper_authors: Gaobin Yang, Maokui He, Shutong Niu, Ruoyu Wang, Yanyan Yue, Shuangqing Qian, Shilong Wu, Jun Du, Chin-Hui Lee</li>
<li>for: 这个论文旨在提出一种基于记忆感知多话者嵌入和序列到序列架构的新型神经网络演说者识别系统（NSD-MS2S），以提高效率和性能。</li>
<li>methods: 这个系统使用了记忆感知多话者嵌入（MA-MSE）和序列到序列架构（Seq2Seq）的优点，并将它们结合在一起，从而提高了效率和性能。在解码过程中，还采用了输入特征融合和多头注意力机制，以提高特征的捕捉和融合。</li>
<li>results: 根据CHiME-7 EVAL集的macro演说者识别错误率（DER）的评估结果，NSD-MS2S实现了15.9%的DER，相比官方基eline系统的49%的提升，表明该模型在主要轨道上实现了CHiME-7 DASR挑战赛的最佳性能。此外，我们还引入了深度互动模块（DIM），以更好地重新获取更清晰和更特征化的多话者嵌入，使当前模型超越了我们在CHiME-7 DASR挑战赛中使用的系统。<details>
<summary>Abstract</summary>
We propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates the strengths of memory-aware multi-speaker embedding (MA-MSE) and sequence-to-sequence (Seq2Seq) architecture, leading to improvement in both efficiency and performance. Next, we further decrease the memory occupation of decoding by incorporating input features fusion and then employ a multi-head attention mechanism to capture features at different levels. NSD-MS2S achieved a macro diarization error rate (DER) of 15.9% on the CHiME-7 EVAL set, which signifies a relative improvement of 49% over the official baseline system, and is the key technique for us to achieve the best performance for the main track of CHiME-7 DASR Challenge. Additionally, we introduce a deep interactive module (DIM) in MA-MSE module to better retrieve a cleaner and more discriminative multi-speaker embedding, enabling the current model to outperform the system we used in the CHiME-7 DASR Challenge. Our code will be available at https://github.com/liyunlongaaa/NSD-MS2S.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的神经网络发言人分类系统，使用记忆感知多 speaker嵌入Sequence-to-Sequence架构（NSD-MS2S），这种系统结合记忆感知多 speaker嵌入（MA-MSE）和Sequence-to-Sequence（Seq2Seq）架构的优点，从而提高效率和性能。然后，我们进一步减少解码过程中的内存占用量，通过输入特征融合和多头注意机制来捕捉不同级别的特征。NSD-MS2S在CHiME-7 EVAL集上达到了15.9%的macro分类错误率（DER），相比官方基eline系统，表示提高49%的相对提升，是我们在CHiME-7 DASR挑战的主轨上实现最佳性能的关键技术。此外，我们在MA-MSE模块中引入了深度互动模块（DIM），以更好地提取 cleaner和更特征化的多 speaker嵌入，使现在的模型超越了在CHiME-7 DASR挑战中使用的系统。我们的代码将于https://github.com/liyunlongaaa/NSD-MS2S上发布。
</details></li>
</ul>
<hr>
<h2 id="Syntax-Tree-Constrained-Graph-Network-for-Visual-Question-Answering"><a href="#Syntax-Tree-Constrained-Graph-Network-for-Visual-Question-Answering" class="headerlink" title="Syntax Tree Constrained Graph Network for Visual Question Answering"></a>Syntax Tree Constrained Graph Network for Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09179">http://arxiv.org/abs/2309.09179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangrui Su, Qi Zhang, Chongyang Shi, Jiachang Liu, Liang Hu</li>
<li>for: 本研究旨在提高Visual Question Answering（VQA）的精度，以便自动回答基于图像内容的自然语言问题。</li>
<li>methods: 本研究提议了一种新的Syntax Tree Constrained Graph Network（STCGN）模型，基于实体消息传递和语法树。该模型可以从问题中提取更加精确的语法树信息，并通过消息传递机制捕捉更加精确的实体特征。</li>
<li>results: 对VQA2.0数据集进行了广泛的实验，显示了我们提议的模型的超越性。<details>
<summary>Abstract</summary>
Visual Question Answering (VQA) aims to automatically answer natural language questions related to given image content. Existing VQA methods integrate vision modeling and language understanding to explore the deep semantics of the question. However, these methods ignore the significant syntax information of the question, which plays a vital role in understanding the essential semantics of the question and guiding the visual feature refinement. To fill the gap, we suggested a novel Syntax Tree Constrained Graph Network (STCGN) for VQA based on entity message passing and syntax tree. This model is able to extract a syntax tree from questions and obtain more precise syntax information. Specifically, we parse questions and obtain the question syntax tree using the Stanford syntax parsing tool. From the word level and phrase level, syntactic phrase features and question features are extracted using a hierarchical tree convolutional network. We then design a message-passing mechanism for phrase-aware visual entities and capture entity features according to a given visual context. Extensive experiments on VQA2.0 datasets demonstrate the superiority of our proposed model.
</details>
<details>
<summary>摘要</summary>
visual question answering (VQA) 目标是自动回答基于图像内容的自然语言问题。现有的 VQA 方法将视觉模型和语言理解结合以探索问题深层 semantics。然而，这些方法忽略了问题语法信息，这种信息在理解问题基本 semantics 和指导视觉特征细化方面发挥重要作用。为了填补这个空白，我们提议了一种基于实体消息传递和语法树的新方法，即Syntax Tree Constrained Graph Network (STCGN)。这个模型可以从问题中提取语法树，并且通过 hierarchy 的 convolutional network 提取Word 和 phrase 层次特征。然后，我们设计了一种message-passing机制，以便捕捉基于给定视觉上下文的视觉实体特征。我们在 VQA2.0 数据集上进行了广泛的实验，并证明了我们的提议模型的优越性。
</details></li>
</ul>
<hr>
<h2 id="Imbalanced-Data-Stream-Classification-using-Dynamic-Ensemble-Selection"><a href="#Imbalanced-Data-Stream-Classification-using-Dynamic-Ensemble-Selection" class="headerlink" title="Imbalanced Data Stream Classification using Dynamic Ensemble Selection"></a>Imbalanced Data Stream Classification using Dynamic Ensemble Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09175">http://arxiv.org/abs/2309.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priya. S, Haribharathi Sivakumar, Vijay Arvind. R</li>
<li>for: 这篇论文旨在解决现代流动数据分类中面临的概念迁移和类别不均等问题，以提高分类器的精度和正确性。</li>
<li>methods: 本文提出了一个新的框架，将数据预处理和动态集合选择组合使用，以适应非站点过渡不均等数据流。这个框架使用了七种数据预处理技术和两种动态集合选择方法。</li>
<li>results: 实验结果显示，将数据预处理与动态集合选择组合使用可以在不均等数据流中提高分类精度和正确性。<details>
<summary>Abstract</summary>
Modern streaming data categorization faces significant challenges from concept drift and class imbalanced data. This negatively impacts the output of the classifier, leading to improper classification. Furthermore, other factors such as the overlapping of multiple classes limit the extent of the correctness of the output. This work proposes a novel framework for integrating data pre-processing and dynamic ensemble selection, by formulating the classification framework for the nonstationary drifting imbalanced data stream, which employs the data pre-processing and dynamic ensemble selection techniques. The proposed framework was evaluated using six artificially generated data streams with differing imbalance ratios in combination with two different types of concept drifts. Each stream is composed of 200 chunks of 500 objects described by eight features and contains five concept drifts. Seven pre-processing techniques and two dynamic ensemble selection methods were considered. According to experimental results, data pre-processing combined with Dynamic Ensemble Selection techniques significantly delivers more accuracy when dealing with imbalanced data streams.
</details>
<details>
<summary>摘要</summary>
现代流处理数据分类面临着概念飘移和数据偏好问题，这会负面影响分类器的输出，导致不正确的分类。此外，多个类别的重叠也限制了正确性的范围。这项工作提出了一种新的框架，通过将数据预处理和动态ensemble选择相结合，对非站ARY飘移偏好数据流进行分类框架，该框架使用了数据预处理和动态ensemble选择技术。该提案在六个人工生成的数据流中进行了评估，每个流程包含200个块，每个块有500个对象，描述了八个特征。每个流程包含五次概念飘移。试验结果表明，数据预处理和动态ensemble选择技术的结合可以在偏好数据流中提供更高的准确性。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Understand-Real-World-Complex-Instructions"><a href="#Can-Large-Language-Models-Understand-Real-World-Complex-Instructions" class="headerlink" title="Can Large Language Models Understand Real-World Complex Instructions?"></a>Can Large Language Models Understand Real-World Complex Instructions?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09150">http://arxiv.org/abs/2309.09150</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abbey4799/cello">https://github.com/abbey4799/cello</a></li>
<li>paper_authors: Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen, Xintao Wang, Yuncheng Huang, Haoning Ye, Zihan Li, Shisong Chen, Yikai Zhang, Zhouhong Gu, Jiaqing Liang, Yanghua Xiao</li>
<li>for: 评估大型自然语言模型（LLM）能否系统地遵循复杂的指令，并可以应用于实际场景。</li>
<li>methods: 提出了八种复杂指令特征，并从实际场景中构建了评估数据集。还开发了四个评估标准和相应的指标，以替代现有的不充分、偏向或过于粗糙的评估方法。</li>
<li>results: 通过广泛的实验，发现代表中文和英文领域的模型在遵循复杂指令时表现不佳，具体的问题包括忽略语义约束、生成错误格式、违反长度或样本数约束、不准确反映输入文本等。<details>
<summary>Abstract</summary>
Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs' ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four criteria and develop corresponding metrics, as current ones are inadequate, biased or too strict and coarse-grained. We compare the performance of representative Chinese-oriented and English-oriented models in following complex instructions through extensive experiments. Resources of CELLO are publicly available at https://github.com/Abbey4799/CELLO.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Quantised-End-to-End-ASR-Models-via-Personalisation"><a href="#Enhancing-Quantised-End-to-End-ASR-Models-via-Personalisation" class="headerlink" title="Enhancing Quantised End-to-End ASR Models via Personalisation"></a>Enhancing Quantised End-to-End ASR Models via Personalisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09136">http://arxiv.org/abs/2309.09136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qmgzhao/Enhancing-Quantised-End-to-End-ASR-Models-via-Personalisation">https://github.com/qmgzhao/Enhancing-Quantised-End-to-End-ASR-Models-via-Personalisation</a></li>
<li>paper_authors: Qiuming Zhao, Guangzhi Sun, Chao Zhang, Mingxing Xu, Thomas Fang Zheng</li>
<li>for: 提高资源受限设备上自动语音识别（ASR）模型的性能。</li>
<li>methods: 提出了一种新的个性化策略（PQM），结合 speaker adaptive training（SAT）和模型归一化来改进压缩模型的性能。PQM使用了4位 NormalFloat Quantisation（NF4）方法进行模型归一化，并使用low-rank adaptation（LoRA）进行SAT。</li>
<li>results: 对于 LibriSpeech 和 TED-LIUM 3 数据集，PQM 可以在压缩模型中实现15.1%和23.3%的相对WRR（word error rate）下降，相比原始精度模型。此外，PQM 只需要加入1%的 speaker-specific 参数，可以实现7倍的模型大小减少。<details>
<summary>Abstract</summary>
Recent end-to-end automatic speech recognition (ASR) models have become increasingly larger, making them particularly challenging to be deployed on resource-constrained devices. Model quantisation is an effective solution that sometimes causes the word error rate (WER) to increase. In this paper, a novel strategy of personalisation for a quantised model (PQM) is proposed, which combines speaker adaptive training (SAT) with model quantisation to improve the performance of heavily compressed models. Specifically, PQM uses a 4-bit NormalFloat Quantisation (NF4) approach for model quantisation and low-rank adaptation (LoRA) for SAT. Experiments have been performed on the LibriSpeech and the TED-LIUM 3 corpora. Remarkably, with a 7x reduction in model size and 1% additional speaker-specific parameters, 15.1% and 23.3% relative WER reductions were achieved on quantised Whisper and Conformer-based attention-based encoder-decoder ASR models respectively, comparing to the original full precision models.
</details>
<details>
<summary>摘要</summary>
现代自动声音识别（ASR）模型已经变得越来越大，这使得它们在有限资源设备上部署变得更加困难。模型量化是一种有效的解决方案，但是有时会导致单词错误率（WER）增加。本文提出了一种个性化quantized模型（PQM）策略，该策略结合说话人适应训练（SAT）和模型量化来提高压缩模型的性能。具体来说，PQM使用4位NormalFloat量化（NF4）方法进行模型量化，并使用低级适应（LoRA）来实现SAT。在LibriSpeech和TED-LIUM 3 corpora上进行了实验，结果显示：与原始精度模型相比，使用PQM可以实现7倍压缩和1%额外的说话人特定参数，而WER的相对下降为15.1%和23.3%。
</details></li>
</ul>
<hr>
<h2 id="ChainForge-A-Visual-Toolkit-for-Prompt-Engineering-and-LLM-Hypothesis-Testing"><a href="#ChainForge-A-Visual-Toolkit-for-Prompt-Engineering-and-LLM-Hypothesis-Testing" class="headerlink" title="ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing"></a>ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09128">http://arxiv.org/abs/2309.09128</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ianarawjo/ChainForge">https://github.com/ianarawjo/ChainForge</a></li>
<li>paper_authors: Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman</li>
<li>for: 这篇论文是为了评估大型自然语言模型（LLM）的输出而写的。</li>
<li>methods: 这篇论文使用了一个开源的视觉工具箱，用于比较不同模型和提示的响应。</li>
<li>results: 研究发现，通过使用这个工具箱，不同的人可以Investigate各种假设，包括实际世界中的应用。<details>
<summary>Abstract</summary>
Evaluating outputs of large language models (LLMs) is challenging, requiring making -- and making sense of -- many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.
</details>
<details>
<summary>摘要</summary>
评估大语言模型（LLM）的输出具有挑战性，需要对多个响应进行评估和理解。然而，评估工具通常需要编程API知识，专注于窄领域，或者是关闭源代码。我们提出了链forge，一个开源的视觉工具箱，用于文本生成LLM的推荐工程和即时假设测试。链forge提供了一个图形化界面，用于比较不同模型和提示变化的响应。我们的系统旨在支持三个任务：模型选择、提示模板设计和假设测试（例如，审核）。我们在开发的早期就发布了链forge，并与学术界和在线用户合作进行了设计征提高。经过实验室和专访研究，我们发现了许多人可以使用链forge来调查他们关心的假设，包括在实际场景中。我们将评估工具分为三种模式：机会性探索、有限评估和迭代优化。
</details></li>
</ul>
<hr>
<h2 id="How-much-can-ChatGPT-really-help-Computational-Biologists-in-Programming"><a href="#How-much-can-ChatGPT-really-help-Computational-Biologists-in-Programming" class="headerlink" title="How much can ChatGPT really help Computational Biologists in Programming?"></a>How much can ChatGPT really help Computational Biologists in Programming?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09126">http://arxiv.org/abs/2309.09126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chowdhury Rafeed Rahman, Limsoon Wong</li>
<li>for: 本研究探讨了 chatGPT 在生物计算领域的潜在影响，包括代码生成、数据分析、机器学习模型和特征提取等方面。</li>
<li>methods: 本研究使用了 chatGPT 来执行不同的生物计算任务，如代码写作、代码审查、错误检测、代码转换、代码重构和管道化等。</li>
<li>results: 研究发现 chatGPT 在生物计算领域有多种可能的影响，包括代码生成、数据分析和机器学习模型的建立等方面，同时也存在一些潜在的负面影响，如代码质量和数据隐私等问题。<details>
<summary>Abstract</summary>
ChatGPT, a recently developed product by openAI, is successfully leaving its mark as a multi-purpose natural language based chatbot. In this paper, we are more interested in analyzing its potential in the field of computational biology. A major share of work done by computational biologists these days involve coding up Bioinformatics algorithms, analyzing data, creating pipelining scripts and even machine learning modeling & feature extraction. This paper focuses on the potential influence (both positive and negative) of ChatGPT in the mentioned aspects with illustrative examples from different perspectives. Compared to other fields of Computer Science, Computational Biology has: (1) less coding resources, (2) more sensitivity and bias issues (deals with medical data) and (3) more necessity of coding assistance (people from diverse background come to this field). Keeping such issues in mind, we cover use cases such as code writing, reviewing, debugging, converting, refactoring and pipelining using ChatGPT from the perspective of computational biologists in this paper.
</details>
<details>
<summary>摘要</summary>
chatGPT，由openAI开发的一款多功能自然语言基础的聊天机器人，在这篇论文中，我们更关心其在生物计算领域的潜在影响。计算生物学家们的工作中大量包括编程、数据分析、创建管道脚本以及机器学习模型和特征提取。本论文将对chatGPT在这些方面的影响进行分析，并通过不同角度提供示例。与其他计算机科学领域不同，计算生物学领域有以下特点：（1） coding资源更少，（2）更敏感和偏见问题（与医疗数据相关），（3）更需要编程帮助（人们来自不同背景）。基于这些问题，本文将从计算生物学家的视角来探讨使用chatGPT进行代码写作、审查、调试、转换、重构和管道化的可能性。
</details></li>
</ul>
<hr>
<h2 id="Using-Reinforcement-Learning-to-Simplify-Mealtime-Insulin-Dosing-for-People-with-Type-1-Diabetes-In-Silico-Experiments"><a href="#Using-Reinforcement-Learning-to-Simplify-Mealtime-Insulin-Dosing-for-People-with-Type-1-Diabetes-In-Silico-Experiments" class="headerlink" title="Using Reinforcement Learning to Simplify Mealtime Insulin Dosing for People with Type 1 Diabetes: In-Silico Experiments"></a>Using Reinforcement Learning to Simplify Mealtime Insulin Dosing for People with Type 1 Diabetes: In-Silico Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09125">http://arxiv.org/abs/2309.09125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anas El Fathi, Marc D. Breton</li>
<li>for: 这个论文的目的是为人类类型1糖尿病患者提供一种简单、可靠的药物剂量计算方法，以改善他们的糖尿病控制和生活质量。</li>
<li>methods: 这个研究使用了 actor-critic方法和长期记忆（LSTM）神经网络，通过训练80个虚拟Subject（VS）来实现。</li>
<li>results: 研究结果表明，提出的RL方法在26周的enario中表现出色，可以取代标准糖尿病控制方法，并且可以提高糖尿病患者的生活质量和血糖控制水平。特别是，在26周的enario中，时间在范围($70-180$mg&#x2F;dL)和时间在低糖（$&lt;70$mg&#x2F;dL）分别为73.1±11.6%和2.0±1.8%，与标准CC方法相比有所提高。<details>
<summary>Abstract</summary>
People with type 1 diabetes (T1D) struggle to calculate the optimal insulin dose at mealtime, especially when under multiple daily injections (MDI) therapy. Effectively, they will not always perform rigorous and precise calculations, but occasionally, they might rely on intuition and previous experience. Reinforcement learning (RL) has shown outstanding results in outperforming humans on tasks requiring intuition and learning from experience. In this work, we propose an RL agent that recommends the optimal meal-accompanying insulin dose corresponding to a qualitative meal (QM) strategy that does not require precise carbohydrate counting (CC) (e.g., a usual meal at noon.). The agent is trained using the soft actor-critic approach and comprises long short-term memory (LSTM) neurons. For training, eighty virtual subjects (VS) of the FDA-accepted UVA/Padova T1D adult population were simulated using MDI therapy and QM strategy. For validation, the remaining twenty VS were examined in 26-week scenarios, including intra- and inter-day variabilities in glucose. \textit{In-silico} results showed that the proposed RL approach outperforms a baseline run-to-run approach and can replace the standard CC approach. Specifically, after 26 weeks, the time-in-range ($70-180$mg/dL) and time-in-hypoglycemia ($<70$mg/dL) were $73.1\pm11.6$% and $ 2.0\pm 1.8$% using the RL-optimized QM strategy compared to $70.6\pm14.8$% and $ 1.5\pm 1.5$% using CC. Such an approach can simplify diabetes treatment, resulting in improved quality of life and glycemic outcomes.
</details>
<details>
<summary>摘要</summary>
人们 WITH 类型 1  диабеetes (T1D) 困难计算最佳注射剂量，特别是在多 daily 注射 (MDI) 治疗下。实际上，他们并不总是做出精确的计算，而是倾向于依靠直觉和之前的经验。人工学习 (RL) 已经表现出惊人的成绩，可以在需要直觉和经验学习的任务上超越人类。在这项工作中，我们提出一种RL代理人，可以根据qualitative meal (QM) 策略提供最佳陪食时注射剂量。该代理人使用 soft actor-critic 方法和长Short-Term Memory (LSTM) 神经元进行训练。为了训练，我们 simulate 了八十名虚拟Subject (VS)，分别采用 MDI 治疗和 QM 策略。为验证，剩下的二十 VS 在26周的enario中进行了检验，包括内部和外部的变化。 results showed that our proposed RL approach outperforms a baseline run-to-run approach and can replace the standard carbohydrate counting (CC) approach. Specifically, after 26 weeks, the time-in-range ($70-180$mg/dL) and time-in-hypoglycemia ($<70$mg/dL) were $73.1\pm11.6$% and $ 2.0\pm 1.8$% using the RL-optimized QM strategy compared to $70.6\pm14.8$% and $ 1.5\pm 1.5$% using CC. Such an approach can simplify diabetes treatment, resulting in improved quality of life and glycemic outcomes.
</details></li>
</ul>
<hr>
<h2 id="Conditional-Mutual-Information-Constrained-Deep-Learning-for-Classification"><a href="#Conditional-Mutual-Information-Constrained-Deep-Learning-for-Classification" class="headerlink" title="Conditional Mutual Information Constrained Deep Learning for Classification"></a>Conditional Mutual Information Constrained Deep Learning for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09123">http://arxiv.org/abs/2309.09123</a></li>
<li>repo_url: None</li>
<li>paper_authors: En-Hui Yang, Shayan Mohajer Hamidi, Linfeng Ye, Renhao Tan, Beverly Yang</li>
<li>for: 本文使用 conditional mutual information (CMI) 和 normalized conditional mutual information (NCMI) 来衡量深度神经网络 (DNN) 输出概率分布中的集中度和分化性能。</li>
<li>methods: 本文使用 NCMI 来评估 popular DNNs 在 ImageNet 文献中预训练的性能，并发现这些模型的验证精度与 NCMI 值成对关系。基于这一观察， authors 修改了标准深度学习 (DL) 框架，使其具有 CMI 约束，称为 CMI constrained deep learning (CMIC-DL)。</li>
<li>results: 对 CMIC-DL 的束定优化问题，提出了一种新的交叉学习算法。实验结果表明，使用 CMIC-DL 训练的 DNN 模型在精度和对抗攻击性能方面都高于文献中其他模型和损失函数。此外，通过 visualizing 学习过程中 CMI 和 NCMI 的变化，也能够更好地理解 DNN 的学习过程。<details>
<summary>Abstract</summary>
The concepts of conditional mutual information (CMI) and normalized conditional mutual information (NCMI) are introduced to measure the concentration and separation performance of a classification deep neural network (DNN) in the output probability distribution space of the DNN, where CMI and the ratio between CMI and NCMI represent the intra-class concentration and inter-class separation of the DNN, respectively. By using NCMI to evaluate popular DNNs pretrained over ImageNet in the literature, it is shown that their validation accuracies over ImageNet validation data set are more or less inversely proportional to their NCMI values. Based on this observation, the standard deep learning (DL) framework is further modified to minimize the standard cross entropy function subject to an NCMI constraint, yielding CMI constrained deep learning (CMIC-DL). A novel alternating learning algorithm is proposed to solve such a constrained optimization problem. Extensive experiment results show that DNNs trained within CMIC-DL outperform the state-of-the-art models trained within the standard DL and other loss functions in the literature in terms of both accuracy and robustness against adversarial attacks. In addition, visualizing the evolution of learning process through the lens of CMI and NCMI is also advocated.
</details>
<details>
<summary>摘要</summary>
《条件共mutual information（CMI）和正常化条件共mutual information（NCMI）的概念在深度学习（DL）中被引入，用于衡量深度神经网络（DNN）在输出概率分布空间中的集中度和分化性。CMI和NCMI之比分别表示DNN的内类集中度和对类分化程度。通过对popular DNNs在ImageNet文献中的验证性能进行NCMI评估，发现这些模型的验证性能与NCMI值相对负相关。基于此观察，我们提出了一种受CMI约束的DL框架（CMIC-DL），并提出了一种新的交互学习算法来解决这种受约束优化问题。实验结果表明，在CMIC-DL中训练的DNN模型比标准DL框架和其他文献中的损失函数训练模型在鲁棒性和鲁棒性对抗攻击方面表现更好。此外，通过CMI和NCMI的视觉化来描述学习过程的演化也被提出。》Note that Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="FDCNet-Feature-Drift-Compensation-Network-for-Class-Incremental-Weakly-Supervised-Object-Localization"><a href="#FDCNet-Feature-Drift-Compensation-Network-for-Class-Incremental-Weakly-Supervised-Object-Localization" class="headerlink" title="FDCNet: Feature Drift Compensation Network for Class-Incremental Weakly Supervised Object Localization"></a>FDCNet: Feature Drift Compensation Network for Class-Incremental Weakly Supervised Object Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09122">http://arxiv.org/abs/2309.09122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Vision-sejin/FDCNet">https://github.com/Vision-sejin/FDCNet</a></li>
<li>paper_authors: Sejin Park, Taehyung Lee, Yeejin Lee, Byeongkeun Kang</li>
<li>for: 这个研究旨在解决类增量弱监督物体Localization（CI-WSOL）任务，即逐渐学习新类的物体Localization，只使用图像级别的标签，而保持先前学习的类的Localization能力。</li>
<li>methods: 我们采用了类增量类ifiers的策略来抑制忘记现象，包括知识填充、保留先前任务中的小数据集、以及使用cosine normalization。此外，我们还提出了特征漂移补偿网络，以补偿因Feature Drift而导致的类分数和Localization图像的影响。</li>
<li>results: 我们通过在ImageNet-100和CUB-200两个公开available datasets上进行实验，发现我们的提议方法比基eline方法高效。<details>
<summary>Abstract</summary>
This work addresses the task of class-incremental weakly supervised object localization (CI-WSOL). The goal is to incrementally learn object localization for novel classes using only image-level annotations while retaining the ability to localize previously learned classes. This task is important because annotating bounding boxes for every new incoming data is expensive, although object localization is crucial in various applications. To the best of our knowledge, we are the first to address this task. Thus, we first present a strong baseline method for CI-WSOL by adapting the strategies of class-incremental classifiers to mitigate catastrophic forgetting. These strategies include applying knowledge distillation, maintaining a small data set from previous tasks, and using cosine normalization. We then propose the feature drift compensation network to compensate for the effects of feature drifts on class scores and localization maps. Since updating network parameters to learn new tasks causes feature drifts, compensating for the final outputs is necessary. Finally, we evaluate our proposed method by conducting experiments on two publicly available datasets (ImageNet-100 and CUB-200). The experimental results demonstrate that the proposed method outperforms other baseline methods.
</details>
<details>
<summary>摘要</summary>
这个研究强调了类增量弱监督对象定位任务（CI-WSOL）。目标是逐步学习新类对象定位，只使用图像级别的注释，保留之前学习的类别定位能力。这项任务非常重要，因为为每个新来的数据都要注释 bounding box 是非常昂贵的，但对象定位是许多应用场景中非常重要的。根据我们所知，我们是第一个对这项任务进行研究的。因此，我们首先提出了一个强大的基线方法 для CI-WSOL，通过适应类增量分类器的策略来减轻忘却性。这些策略包括应用知识传播、维护前一任务中的小数据集，以及使用仓颉 норamlization。然后，我们提议了特征漂移补做网络，以补做因更新网络参数学习新任务而导致的特征漂移的效果。最后，我们通过在 ImageNet-100 和 CUB-200 两个公共可用的数据集上进行实验，证明我们提出的方法在基eline方法上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Public-Perceptions-of-Gender-Bias-in-Large-Language-Models-Cases-of-ChatGPT-and-Ernie"><a href="#Public-Perceptions-of-Gender-Bias-in-Large-Language-Models-Cases-of-ChatGPT-and-Ernie" class="headerlink" title="Public Perceptions of Gender Bias in Large Language Models: Cases of ChatGPT and Ernie"></a>Public Perceptions of Gender Bias in Large Language Models: Cases of ChatGPT and Ernie</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09120">http://arxiv.org/abs/2309.09120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyrie Zhixuan Zhou, Madelyn Rose Sanfilippo</li>
<li>for: This paper aims to investigate and analyze public perceptions of gender bias in large language models (LLMs) trained in different cultural contexts.</li>
<li>methods: The authors conducted a content analysis of social media discussions to gather data on people’s observations of gender bias in their personal use of LLMs and scientific findings about gender bias in LLMs.</li>
<li>results: The study found that ChatGPT, a US-based LLM, exhibited more implicit gender bias, while Ernie, a China-based LLM, showed more explicit gender bias. The findings suggest that culture plays a significant role in shaping gender bias in LLMs and propose governance recommendations to regulate gender bias in these models.<details>
<summary>Abstract</summary>
Large language models are quickly gaining momentum, yet are found to demonstrate gender bias in their responses. In this paper, we conducted a content analysis of social media discussions to gauge public perceptions of gender bias in LLMs which are trained in different cultural contexts, i.e., ChatGPT, a US-based LLM, or Ernie, a China-based LLM. People shared both observations of gender bias in their personal use and scientific findings about gender bias in LLMs. A difference between the two LLMs was seen -- ChatGPT was more often found to carry implicit gender bias, e.g., associating men and women with different profession titles, while explicit gender bias was found in Ernie's responses, e.g., overly promoting women's pursuit of marriage over career. Based on the findings, we reflect on the impact of culture on gender bias and propose governance recommendations to regulate gender bias in LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型快速增长，但它们在回应中显示出性别偏见。在这篇研究中，我们通过社交媒体讨论分析公众对于语言模型中的性别偏见。人们分享了对于性别偏见的personal使用经验和科学发现，包括两个不同的语言模型：ChatGPT和Ernie。我们发现ChatGPT更常见偏见，例如将男女分配不同的职业标题，而Ernie的回应则显示了明显的性别偏见，例如过度推荐女性追求婚姻而不是职业。根据结果，我们思考了文化对性别偏见的影响和建议管理性别偏见在语言模型中。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-3D-Object-Level-Mapping-with-Deep-Shape-Priors"><a href="#Uncertainty-aware-3D-Object-Level-Mapping-with-Deep-Shape-Priors" class="headerlink" title="Uncertainty-aware 3D Object-Level Mapping with Deep Shape Priors"></a>Uncertainty-aware 3D Object-Level Mapping with Deep Shape Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09118">http://arxiv.org/abs/2309.09118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TRAILab/UncertainShapePose">https://github.com/TRAILab/UncertainShapePose</a></li>
<li>paper_authors: Ziwei Liao, Jun Yang, Jingxing Qian, Angela P. Schoellig, Steven L. Waslander<br>for:这篇论文的目的是提出一种高质量的物体级别地图生成方法，用于在探测过程中对未知物体进行高精度的重建。methods:该方法使用多个RGB-D图像作为输入，并输出高精度的3D形状和9DoF姿态（包括3个涂抹参数）。该方法利用学习的生成模型来作为对shape类别的先验知识，并使用概率uncertainty-aware优化框架来进行3D重建。results:我们在实验中取得了substantial的提高，与现有的方法相比。我们还示出了对下游机器人任务的活跃探测等方面的应用。<details>
<summary>Abstract</summary>
3D object-level mapping is a fundamental problem in robotics, which is especially challenging when object CAD models are unavailable during inference. In this work, we propose a framework that can reconstruct high-quality object-level maps for unknown objects. Our approach takes multiple RGB-D images as input and outputs dense 3D shapes and 9-DoF poses (including 3 scale parameters) for detected objects. The core idea of our approach is to leverage a learnt generative model for shape categories as a prior and to formulate a probabilistic, uncertainty-aware optimization framework for 3D reconstruction. We derive a probabilistic formulation that propagates shape and pose uncertainty through two novel loss functions. Unlike current state-of-the-art approaches, we explicitly model the uncertainty of the object shapes and poses during our optimization, resulting in a high-quality object-level mapping system. Moreover, the resulting shape and pose uncertainties, which we demonstrate can accurately reflect the true errors of our object maps, can also be useful for downstream robotics tasks such as active vision. We perform extensive evaluations on indoor and outdoor real-world datasets, achieving achieves substantial improvements over state-of-the-art methods. Our code will be available at https://github.com/TRAILab/UncertainShapePose.
</details>
<details>
<summary>摘要</summary>
三维对象级映射是Robotics中的基本问题，特别是当对象CAD模型不可用于推理时更加棘手。在这项工作中，我们提出了一个框架，可以重建高质量的对象级映射。我们的方法接受多个RGB-D图像作为输入，并输出密集的3D形状和9个DoF姿态（包括3个涉及度参数）。我们的核心思想是利用学习的生成模型来作为类别的先验，并使用概率、不确定性意识推导的优化框架来重建3D映射。我们 derive了一个概率形式，将形状和姿态不确定性传递给两个新的损失函数。不同于当前状态的方法，我们显式地模型对象的形状和姿态不确定性，从而实现了高质量的对象级映射系统。此外，我们所获得的形状和姿态不确定性，可以准确地反映我们对象映射的真实错误，同时也可以用于下游机器人任务，如活动视觉。我们在室内和室外实际数据集上进行了广泛的评估，实现了明显的提高。我们的代码将在https://github.com/TRAILab/UncertainShapePose上提供。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Decoding-Improves-Reasoning-in-Large-Language-Models"><a href="#Contrastive-Decoding-Improves-Reasoning-in-Large-Language-Models" class="headerlink" title="Contrastive Decoding Improves Reasoning in Large Language Models"></a>Contrastive Decoding Improves Reasoning in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09117">http://arxiv.org/abs/2309.09117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean O’Brien, Mike Lewis</li>
<li>for: 该研究旨在证明 Contrastive Decoding 可以大幅提高语言模型生成文本的质量，特别是在逻辑 reasoning 任务上。</li>
<li>methods: 该研究使用 Contrastive Decoding 方法，这是一种简单、计算上轻便，无需训练的文本生成方法。</li>
<li>results: 研究发现，Contrastive Decoding 可以在多种逻辑 reasoning 任务上大幅超越 greedy decoding，并且在 HellaSwag 常识逻辑测试 benchmark 和 GSM8K 数学单词逻辑测试 benchmark 上表现出色。<details>
<summary>Abstract</summary>
We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models.
</details>
<details>
<summary>摘要</summary>
我们证明了对比解码（Contrastive Decoding）——一种简单、计算机轻量级、训练无需的文本生成方法，提出于李等2022年的研究——在多种逻辑任务上实现了大量的出vat的改进。原本用于改善长文本生成的 perceived 质量，对比解码搜索最大化一个权重 diferencial of likelihood  между强度模型和弱度模型中的字符串。我们表明，对比解码使 LLama-65B 超过 LLama 2、GPT-3.5 和 PaLM 2-L 在 hellaSwag 常识逻辑标准 bencmark 上表现出色，以及在 GSM8K 数学单词逻辑标准 bencmark 上超过 LLaMA 2、GPT-3.5 和 PaLM-540B。此外，对比解码还能在其他任务上提高表现。分析表明，对比解码可以避免一些抽象逻辑错误，以及避免简单的模式，如输入段落复制。总之，对比解码超过 nucleus sampling  для长文本生成和 greedy decoding  para reasoning tasks，成为一种强大的通用方法 для生成语言模型中的文本。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.AI_2023_09_17/" data-id="clogxf3kk00455xradaljct3e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.CL_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T11:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.CL_2023_09_17/">cs.CL - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Augmenting-text-for-spoken-language-understanding-with-Large-Language-Models"><a href="#Augmenting-text-for-spoken-language-understanding-with-Large-Language-Models" class="headerlink" title="Augmenting text for spoken language understanding with Large Language Models"></a>Augmenting text for spoken language understanding with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09390">http://arxiv.org/abs/2309.09390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roshan Sharma, Suyoun Kim, Daniel Lazar, Trang Le, Akshat Shrivastava, Kwanghoon Ahn, Piyush Kansal, Leda Sari, Ozlem Kalinli, Michael Seltzer</li>
<li>for: 该研究的目的是解决现有应用领域中的Robust模型训练问题，需要对应的 triplets 数据，但获取这些数据可以是非常复杂和昂贵的。</li>
<li>methods: 该研究使用了不同的方法来使用无对应的文本数据进行模型训练，包括Joint Audio Text (JAT)和Text-to-Speech (TTS)等方法。</li>
<li>results: 实验结果表明，使用无对应的文本数据可以提高模型的性能，对于现有领域和新领域来说，使用LLMs生成的文本可以提高EM的精度 by 1.4%和2.6%。<details>
<summary>Abstract</summary>
Spoken semantic parsing (SSP) involves generating machine-comprehensible parses from input speech. Training robust models for existing application domains represented in training data or extending to new domains requires corresponding triplets of speech-transcript-semantic parse data, which is expensive to obtain. In this paper, we address this challenge by examining methods that can use transcript-semantic parse data (unpaired text) without corresponding speech. First, when unpaired text is drawn from existing textual corpora, Joint Audio Text (JAT) and Text-to-Speech (TTS) are compared as ways to generate speech representations for unpaired text. Experiments on the STOP dataset show that unpaired text from existing and new domains improves performance by 2% and 30% in absolute Exact Match (EM) respectively. Second, we consider the setting when unpaired text is not available in existing textual corpora. We propose to prompt Large Language Models (LLMs) to generate unpaired text for existing and new domains. Experiments show that examples and words that co-occur with intents can be used to generate unpaired text with Llama 2.0. Using the generated text with JAT and TTS for spoken semantic parsing improves EM on STOP by 1.4% and 2.6% absolute for existing and new domains respectively.
</details>
<details>
<summary>摘要</summary>
spoken semantic parsing (SSP) 即从语音输入生成机器可理解的架构。为已有应用领域的训练数据或扩展到新领域训练模型而需要对应的三元组（语音、 trascript、semantic parse）数据，这是 expensive 的。在这篇论文中，我们解决这个挑战，通过不使用对应的语音，使用 trascript-semantic parse 数据（无对应的语音）来生成 speech 表示。首先，当 unpaired text 从现有的文本库中提取时，我们比较使用 Joint Audio Text (JAT) 和 Text-to-Speech (TTS) 生成 speech 表示。STOP 数据集上的实验结果显示，从现有和新领域的 unpaired text 提取可以提高表达的性能，相对于无提取情况下，减少了2%和30%的精确匹配（EM）。其次，当 unpaired text 不在现有的文本库中时，我们提议使用 Large Language Models (LLMs) 生成 unpaired text。我们发现，可以使用 intents 相关的例子和单词来生成 unpaired text。使用生成的文本与 JAT 和 TTS 生成的 speech 进行 spoken semantic parsing，可以提高 STOP 数据集上的 EM 表达，相对于无提取情况下，提高了1.4%和2.6%的精确匹配。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Shortcuts-in-Language-Models-with-Soft-Label-Encoding"><a href="#Mitigating-Shortcuts-in-Language-Models-with-Soft-Label-Encoding" class="headerlink" title="Mitigating Shortcuts in Language Models with Soft Label Encoding"></a>Mitigating Shortcuts in Language Models with Soft Label Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09380">http://arxiv.org/abs/2309.09380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirui He, Huiqi Deng, Haiyan Zhao, Ninghao Liu, Mengnan Du</li>
<li>for: 本研究的目的是解决大语言模型在自然语言理解（NLU）任务上依赖偶合关系的问题。</li>
<li>methods: 我们提出了一种简单 yet effective的减弱架构，名为软标签编码（SoftLE）。我们首先使用师模型通过硬标签训练每个样本的偶合度。然后，我们添加了一个假类来编码偶合度，并将其用于融合其他维度的真实标签生成软标签。这个新的真实标签用于训练更加可靠的学生模型。</li>
<li>results: 我们在两个 NLU  benchmark 任务上进行了广泛的实验，结果表明，SoftLE 可以显著提高对于样本集外的泛化性能，而保持满意的在样本集内的准确率。<details>
<summary>Abstract</summary>
Recent research has shown that large language models rely on spurious correlations in the data for natural language understanding (NLU) tasks. In this work, we aim to answer the following research question: Can we reduce spurious correlations by modifying the ground truth labels of the training data? Specifically, we propose a simple yet effective debiasing framework, named Soft Label Encoding (SoftLE). We first train a teacher model with hard labels to determine each sample's degree of relying on shortcuts. We then add one dummy class to encode the shortcut degree, which is used to smooth other dimensions in the ground truth label to generate soft labels. This new ground truth label is used to train a more robust student model. Extensive experiments on two NLU benchmark tasks demonstrate that SoftLE significantly improves out-of-distribution generalization while maintaining satisfactory in-distribution accuracy.
</details>
<details>
<summary>摘要</summary>
近期研究发现大语言模型在自然语言理解任务上依赖于偶合关系。在这项工作中，我们想回答以下研究问题：可以通过修改训练数据的真实标签来减少偶合关系吗？我们提出了一种简单 yet effective的减偶框架，名为软标签编码（SoftLE）。我们首先使用师模型通过硬标签确定每个样本的偶合度。然后，我们添加了一个幌子类来编码偶合度，并将其用于软标签的缓冲处理。这个新的真实标签用于训练更加可靠的学生模型。我们在两个NLU benchmark任务上进行了广泛的实验，结果表明，SoftLE可以明显提高异常情况泛化性，同时保持满意的内部准确率。
</details></li>
</ul>
<hr>
<h2 id="Embrace-Divergence-for-Richer-Insights-A-Multi-document-Summarization-Benchmark-and-a-Case-Study-on-Summarizing-Diverse-Information-from-News-Articles"><a href="#Embrace-Divergence-for-Richer-Insights-A-Multi-document-Summarization-Benchmark-and-a-Case-Study-on-Summarizing-Diverse-Information-from-News-Articles" class="headerlink" title="Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles"></a>Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09369">http://arxiv.org/abs/2309.09369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kung-Hsiang Huang, Philippe Laban, Alexander R. Fabbri, Prafulla Kumar Choubey, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu</li>
<li>for: 本研究旨在提出一种多文摘要新闻Summarization任务，即对多篇新闻文章中的不同信息进行摘要。</li>
<li>methods: 我们提出了一种数据收集方案，并开发了一个名为DiverseSumm的数据集，包括245篇新闻故事和10篇新闻文章。此外，我们还进行了全面的分析，探讨使用大语言模型（LLM）测试摘要的覆盖率和准确性的问题，以及与人类评估的相关性。</li>
<li>results: 我们的分析发现，使用LLM进行多文摘要时，主要存在覆盖率和准确性的问题，GPT-4只能覆盖不同信息的 Less than 40% 的情况。<details>
<summary>Abstract</summary>
Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, to our knowledge, the summarization of diverse information dispersed across multiple articles about an event has not been previously investigated. The latter imposes a different set of challenges for a summarization model. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Moreover, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of the summaries, as well as their correlation with human assessments. We applied our findings to study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover less than 40% of the diverse information on average.
</details>
<details>
<summary>摘要</summary>
To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Moreover, we conducted a comprehensive analysis to identify the position and verbosity biases when using Large Language Model (LLM)-based metrics to evaluate the coverage and faithfulness of the summaries, as well as their correlation with human assessments.We applied our findings to study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that while LLMs have extraordinary capabilities in single-document summarization, the proposed task remains a complex challenge for them, with GPT-4 only able to cover less than 40% of the diverse information on average.
</details></li>
</ul>
<hr>
<h2 id="Language-models-are-susceptible-to-incorrect-patient-self-diagnosis-in-medical-applications"><a href="#Language-models-are-susceptible-to-incorrect-patient-self-diagnosis-in-medical-applications" class="headerlink" title="Language models are susceptible to incorrect patient self-diagnosis in medical applications"></a>Language models are susceptible to incorrect patient self-diagnosis in medical applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09362">http://arxiv.org/abs/2309.09362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rojin Ziaei, Samuel Schmidgall</li>
<li>for: 这个研究旨在检验大型自然语言模型（LLMs）在医疗领域中的可用性，以及它们在实际患者-医生交流中的表现。</li>
<li>methods: 研究使用了多种LLMs，并对它们进行了多项选择和修改，以模拟实际患者自诊的情况。研究还使用了美国医学会考试的多选题来评估LLMs的诊断准确率。</li>
<li>results: 研究发现，当患者提供了错误的偏见验证信息时，LLMs的诊断准确率会受到极大的影响，表明LLMs在自诊情况下存在高度易错的问题。<details>
<summary>Abstract</summary>
Large language models (LLMs) are becoming increasingly relevant as a potential tool for healthcare, aiding communication between clinicians, researchers, and patients. However, traditional evaluations of LLMs on medical exam questions do not reflect the complexity of real patient-doctor interactions. An example of this complexity is the introduction of patient self-diagnosis, where a patient attempts to diagnose their own medical conditions from various sources. While the patient sometimes arrives at an accurate conclusion, they more often are led toward misdiagnosis due to the patient's over-emphasis on bias validating information. In this work we present a variety of LLMs with multiple-choice questions from United States medical board exams which are modified to include self-diagnostic reports from patients. Our findings highlight that when a patient proposes incorrect bias-validating information, the diagnostic accuracy of LLMs drop dramatically, revealing a high susceptibility to errors in self-diagnosis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Performance-of-the-Pre-Trained-Large-Language-Model-GPT-4-on-Automated-Short-Answer-Grading"><a href="#Performance-of-the-Pre-Trained-Large-Language-Model-GPT-4-on-Automated-Short-Answer-Grading" class="headerlink" title="Performance of the Pre-Trained Large Language Model GPT-4 on Automated Short Answer Grading"></a>Performance of the Pre-Trained Large Language Model GPT-4 on Automated Short Answer Grading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09338">http://arxiv.org/abs/2309.09338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerd Kortemeyer</li>
<li>for: 这 paper 是 investigate GPT-4 LLM 在 Automated Short Answer Grading (ASAG) 领域的性能。</li>
<li>methods: 这 paper 使用了 GPT-4 LLM 和手动设计的模型，对 SciEntsBank 和 Beetle 数据集进行测试。</li>
<li>results: 结果表明，GPT-4 LLM 的性能相对于手动设计的模型类似，但与特殊训练的 LLM 相比较差。<details>
<summary>Abstract</summary>
Automated Short Answer Grading (ASAG) has been an active area of machine-learning research for over a decade. It promises to let educators grade and give feedback on free-form responses in large-enrollment courses in spite of limited availability of human graders. Over the years, carefully trained models have achieved increasingly higher levels of performance. More recently, pre-trained Large Language Models (LLMs) emerged as a commodity, and an intriguing question is how a general-purpose tool without additional training compares to specialized models. We studied the performance of GPT-4 on the standard benchmark 2-way and 3-way datasets SciEntsBank and Beetle, where in addition to the standard task of grading the alignment of the student answer with a reference answer, we also investigated withholding the reference answer. We found that overall, the performance of the pre-trained general-purpose GPT-4 LLM is comparable to hand-engineered models, but worse than pre-trained LLMs that had specialized training.
</details>
<details>
<summary>摘要</summary>
自动化简答评分（ASAG）已经是机器学习研究的活跃领域之一，时间已经超过十年。它承诺能让教育工作者在大规模课程中评分和给出反馈，尽管人工评分员的可用性有限。总之，仔细训练的模型在过去的几年中取得了不断提高的性能。更加最近，大型自然语言模型（LLM）出现了，一个有趣的问题是一个通用工具没有进行特殊训练是如何与特殊训练的模型相比。我们研究了GPT-4在标准benchmark上的性能，包括SciEntsBank和Beetle dataset，我们还研究了不公开参考答案的情况。我们发现，总的来说，预训练的通用GPT-4 LLM的性能和手工设计的模型相比较，但是比特殊训练的LLM更差。
</details></li>
</ul>
<hr>
<h2 id="A-Few-Shot-Approach-to-Dysarthric-Speech-Intelligibility-Level-Classification-Using-Transformers"><a href="#A-Few-Shot-Approach-to-Dysarthric-Speech-Intelligibility-Level-Classification-Using-Transformers" class="headerlink" title="A Few-Shot Approach to Dysarthric Speech Intelligibility Level Classification Using Transformers"></a>A Few-Shot Approach to Dysarthric Speech Intelligibility Level Classification Using Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09329">http://arxiv.org/abs/2309.09329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paleti Nikhil Chowdary, Vadlapudi Sai Aravind, Gorantla V N S L Vishnu Vardhan, Menta Sai Akshay, Menta Sai Aashish, Jyothish Lal. G</li>
<li>for: 检测异常speech（dysarthria），以便开发治疗计划，提高人们的交流能力和生活质量。</li>
<li>methods: 使用transformer模型，采用几招shot学习法，对有限数据进行分类，并且解决之前研究中的数据泄露问题。</li>
<li>results: 使用whisper-large-v2 transformer模型，在UASpeech数据集中的中度智能水平患者上达到了85%的准确率，0.92的精度、0.8的准确率、0.85的F1分数和0.91的特异性。模型在’words’数据集上表现较好，比’letters’和’digits’数据集更好。多类模型达到67%的准确率。<details>
<summary>Abstract</summary>
Dysarthria is a speech disorder that hinders communication due to difficulties in articulating words. Detection of dysarthria is important for several reasons as it can be used to develop a treatment plan and help improve a person's quality of life and ability to communicate effectively. Much of the literature focused on improving ASR systems for dysarthric speech. The objective of the current work is to develop models that can accurately classify the presence of dysarthria and also give information about the intelligibility level using limited data by employing a few-shot approach using a transformer model. This work also aims to tackle the data leakage that is present in previous studies. Our whisper-large-v2 transformer model trained on a subset of the UASpeech dataset containing medium intelligibility level patients achieved an accuracy of 85%, precision of 0.92, recall of 0.8 F1-score of 0.85, and specificity of 0.91. Experimental results also demonstrate that the model trained using the 'words' dataset performed better compared to the model trained on the 'letters' and 'digits' dataset. Moreover, the multiclass model achieved an accuracy of 67%.
</details>
<details>
<summary>摘要</summary>
《异常speech》是一种语言障碍，影响人们的交流能力，由于说话困难。检测异常speech的重要性在于可以开发治疗计划，帮助人们提高交流能力和语言表达效果。许多文献关注提高ASR系统对异常speech的识别能力。目前研究的目标是开发一种可以准确地识别异常speech并提供语音elligibility水平信息的模型，使用少量数据和transformer模型进行几架 Approach。此外，本研究还希望解决过去研究中存在的数据泄露问题。我们使用UASpeech数据集中的中度语音智能水平患者训练了我们的whisper-large-v2 transformer模型，达到了85%的准确率、0.92的精度、0.8的 recall、0.85的F1得分和0.91的特异性。实验结果还表明，使用“words”数据集训练的模型比使用“letters”和“digits”数据集训练的模型性能更高。此外，多类模型达到了67%的准确率。
</details></li>
</ul>
<hr>
<h2 id="How-People-Perceive-The-Dynamic-Zero-COVID-Policy-A-Retrospective-Analysis-From-The-Perspective-of-Appraisal-Theory"><a href="#How-People-Perceive-The-Dynamic-Zero-COVID-Policy-A-Retrospective-Analysis-From-The-Perspective-of-Appraisal-Theory" class="headerlink" title="How People Perceive The Dynamic Zero-COVID Policy: A Retrospective Analysis From The Perspective of Appraisal Theory"></a>How People Perceive The Dynamic Zero-COVID Policy: A Retrospective Analysis From The Perspective of Appraisal Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09324">http://arxiv.org/abs/2309.09324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Na Yang, Kyrie Zhixuan Zhou, Yunzhe Li</li>
<li>for: 这 paper 是为了研究中国的动态零病政策三年来的情感反应和观点变化。</li>
<li>methods: 这 paper 使用了 sentiment analysis 方法来分析了 2,358 篇微博文章中的情感，并从 appraisal theory 的视角进行了深入的诠释。</li>
<li>results: 研究发现了四个代表点：政策初始化、情感态度快速变化、情感分数最低、政策终止。这些结果可能有助于未来卫生防疫控制措施的开发。<details>
<summary>Abstract</summary>
The Dynamic Zero-COVID Policy in China spanned three years and diverse emotional responses have been observed at different times. In this paper, we retrospectively analyzed public sentiments and perceptions of the policy, especially regarding how they evolved over time, and how they related to people's lived experiences. Through sentiment analysis of 2,358 collected Weibo posts, we identified four representative points, i.e., policy initialization, sharp sentiment change, lowest sentiment score, and policy termination, for an in-depth discourse analysis through the lens of appraisal theory. In the end, we reflected on the evolving public sentiments toward the Dynamic Zero-COVID Policy and proposed implications for effective epidemic prevention and control measures for future crises.
</details>
<details>
<summary>摘要</summary>
中国的动态零病政策覆盖了三年的时间，而人们对这政策的情感响应也在不同时间 exhibited 多样化的情感。在这篇论文中，我们通过对2358篇微博文章的情感分析，发现了四个代表点：政策 initialize，快速情感变化，最低情感分数，和政策终止，并通过评价理论进行深入的讨论。最后，我们回顾了公众对动态零病政策的情感发展，并提出了未来危机管理的有效措施。Note: "微博" (Weibo) is a popular social media platform in China, similar to Twitter.
</details></li>
</ul>
<hr>
<h2 id="A-novel-approach-to-measuring-patent-claim-scope-based-on-probabilities-obtained-from-large-language-models"><a href="#A-novel-approach-to-measuring-patent-claim-scope-based-on-probabilities-obtained-from-large-language-models" class="headerlink" title="A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models"></a>A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10003">http://arxiv.org/abs/2309.10003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sébastien Ragot</li>
<li>for: 这种方法用于测量专利索引的范围，它基于信息理论，假设罕见的概念更有启示性，因为它更有启示性。</li>
<li>methods: 该方法基于语言模型，从报告语言模型中计算自信息，从而计算专利索引的范围。五种语言模型被考虑，从 simplest 模型（每个词或字符从均匀分布中选择）到中间模型（使用平均词或字符频率），最后一个模型是 GPT2。</li>
<li>results: 结果表明，随着语言模型的复杂程度的增加，模型的性能也得到了改善。GPT2模型在多个静态测试中表现出色，超过了基于词和字符频率的模型，而这些模型本身也超过了基于词和字符计数的模型。<details>
<summary>Abstract</summary>
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope. The performance of the language models is then assessed with respect to several ad hoc tests. The more sophisticated the model, the better the results. The GPT2 model outperforms models based on word and character frequencies, which are themselves ahead of models based on word and character counts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AutoAM-An-End-To-End-Neural-Model-for-Automatic-and-Universal-Argument-Mining"><a href="#AutoAM-An-End-To-End-Neural-Model-for-Automatic-and-Universal-Argument-Mining" class="headerlink" title="AutoAM: An End-To-End Neural Model for Automatic and Universal Argument Mining"></a>AutoAM: An End-To-End Neural Model for Automatic and Universal Argument Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09300">http://arxiv.org/abs/2309.09300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lang Cao</li>
<li>for: 本文是为了提出一种基于神经网络的自动论证挖掘模型（AutoAM），以解决现有的论证挖掘技术尚未成熟和不具备准确描述论证关系的问题。</li>
<li>methods: 本文提出了一种新的论证组件注意力机制，可以更好地捕捉论证中相关的信息，从而提高论证挖掘的性能。此外，本文还提出了一种通用的终端框架，可以无需Constraints like tree structure，完成论证挖掘的三个子任务。</li>
<li>results: 实验结果表明， compared with现有的工作，我们的模型在两个公共数据集上的多个维度上表现出色，达到了更高的性能。<details>
<summary>Abstract</summary>
Argument mining is to analyze argument structure and extract important argument information from unstructured text. An argument mining system can help people automatically gain causal and logical information behind the text. As argumentative corpus gradually increases, like more people begin to argue and debate on social media, argument mining from them is becoming increasingly critical. However, argument mining is still a big challenge in natural language tasks due to its difficulty, and relative techniques are not mature. For example, research on non-tree argument mining needs to be done more. Most works just focus on extracting tree structure argument information. Moreover, current methods cannot accurately describe and capture argument relations and do not predict their types. In this paper, we propose a novel neural model called AutoAM to solve these problems. We first introduce the argument component attention mechanism in our model. It can capture the relevant information between argument components, so our model can better perform argument mining. Our model is a universal end-to-end framework, which can analyze argument structure without constraints like tree structure and complete three subtasks of argument mining in one model. The experiment results show that our model outperforms the existing works on several metrics in two public datasets.
</details>
<details>
<summary>摘要</summary>
Argument mining是分析对话结构，从不结构化文本中提取重要的论据信息的技术。一个有效的论据挖掘系统可以帮助人们自动获得文本中的 causal 和逻辑信息。随着社交媒体上的论据资源的不断增加，论据挖掘在自然语言任务中变得越来越重要。然而，论据挖掘仍然是自然语言任务中的大挑战，因为它的难度和相关技术还没有成熟。例如，研究非树结构论据挖掘还需要做得更多。大多数工作都是只关注EXTRACTING 树结构论据信息。此外，当前的方法无法准确地描述和捕捉论据关系，也无法预测它们的类型。在这篇论文中，我们提出了一种新的神经网络模型called AutoAM，以解决这些问题。我们首先介绍了我们模型中的论据组件注意力机制。它可以捕捉论据组件之间的相关信息，因此我们的模型可以更好地进行论据挖掘。我们的模型是一个通用的端到端框架，可以不受树结构的限制，完成论据挖掘中的三个子任务。实验结果表明，我们的模型在两个公共数据集上的多个纪录度量上都高于现有的作品。
</details></li>
</ul>
<hr>
<h2 id="OWL-A-Large-Language-Model-for-IT-Operations"><a href="#OWL-A-Large-Language-Model-for-IT-Operations" class="headerlink" title="OWL: A Large Language Model for IT Operations"></a>OWL: A Large Language Model for IT Operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09298">http://arxiv.org/abs/2309.09298</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Other-sources">https://github.com/Aryia-Behroziuan/Other-sources</a></li>
<li>paper_authors: Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, Xu Shi, Tieqiao Zheng, Liangfan Zheng, Bo Zhang, Ke Xu, Zhoujun Li</li>
<li>for: 这篇论文旨在探讨特有的大自然语言处理技术（NLP）在信息技术（IT）操作中的应用。</li>
<li>methods: 本论文使用了一个名为OWL的大语言模型，该模型在我们收集的OWL-Instruct数据集上进行了训练，该数据集包含了各种IT相关信息。在训练过程中，提出了混合适应器策略来提高参数效率的调整 across different domains or tasks。</li>
<li>results: 根据我们在OWL-Bench和开放IT相关的benchmark上进行的评估，OWL模型在IT任务上表现出色，与现有模型相比，具有显著的性能优势。此外，我们希望这些发现能够为IT操作技术的发展提供更多的思路和灵感。<details>
<summary>Abstract</summary>
With the rapid development of IT operations, it has become increasingly crucial to efficiently manage and analyze large volumes of data for practical applications. The techniques of Natural Language Processing (NLP) have shown remarkable capabilities for various tasks, including named entity recognition, machine translation and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various NLP downstream tasks. However, there is a lack of specialized LLMs for IT operations. In this paper, we introduce the OWL, a large language model trained on our collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks. Furthermore, we evaluate the performance of our OWL on the OWL-Bench established by us and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs.
</details>
<details>
<summary>摘要</summary>
随着信息技术运营的快速发展，管理和分析大量数据的效率已成为非常重要的。自然语言处理（NLP）技术在各种任务上表现出了惊人的能力，包括命名实体识别、机器翻译和对话系统。最近，大型自然语言模型（LLMs）在各种 NLP 下渠道任务上实现了显著的改进。然而，对 IT 运营的特殊化 LLMs 缺乏。本文介绍了 OWL，一个基于我们收集的 OWL-Instruct 数据集的大型自然语言模型，其中混合 adapter 策略可以在不同的领域或任务中进行参数高效调整。此外，我们评估了 OWL 在 OWL-Bench 和开放的 IT 相关benchmark上的性能，并发现 OWL 在 IT 任务上表现出了显著的优异性。此外，我们希望通过这些研究成果，为 IT 运营技术的发展提供更多的新思路和灵感。
</details></li>
</ul>
<hr>
<h2 id="Model-based-Subsampling-for-Knowledge-Graph-Completion"><a href="#Model-based-Subsampling-for-Knowledge-Graph-Completion" class="headerlink" title="Model-based Subsampling for Knowledge Graph Completion"></a>Model-based Subsampling for Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09296">http://arxiv.org/abs/2309.09296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xincanfeng/ms_kge">https://github.com/xincanfeng/ms_kge</a></li>
<li>paper_authors: Xincan Feng, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe</li>
<li>for: 提高 Knowledge Graph Embedding (KGE) 模型的适应性和性能</li>
<li>methods: 提出 Model-based Subsampling (MBS) 和 Mixed Subsampling (MIX) 方法，通过 KGE 模型的预测来估计不够频繁的查询的出现概率</li>
<li>results: 对 FB15k-237、WN18RR 和 YAGO3-10 等 dataset 进行评估，显示我们的提案的抽样方法可以提高受欢迎 KGE 模型的 KG 完成性能<details>
<summary>Abstract</summary>
Subsampling is effective in Knowledge Graph Embedding (KGE) for reducing overfitting caused by the sparsity in Knowledge Graph (KG) datasets. However, current subsampling approaches consider only frequencies of queries that consist of entities and their relations. Thus, the existing subsampling potentially underestimates the appearance probabilities of infrequent queries even if the frequencies of their entities or relations are high. To address this problem, we propose Model-based Subsampling (MBS) and Mixed Subsampling (MIX) to estimate their appearance probabilities through predictions of KGE models. Evaluation results on datasets FB15k-237, WN18RR, and YAGO3-10 showed that our proposed subsampling methods actually improved the KG completion performances for popular KGE models, RotatE, TransE, HAKE, ComplEx, and DistMult.
</details>
<details>
<summary>摘要</summary>
通过抽样可以有效地降低知识图（KG）数据集中的过拟合问题，但现有的抽样方法只考虑了实体和关系之间的频率。这可能会下降不常见的查询的出现概率，即使实体或关系的频率很高。为解决这个问题，我们提出了基于模型的抽样（MBS）和混合抽样（MIX）方法，通过KGE模型的预测来估计它们的出现概率。我们在FB15k-237、WN18RR和YAGO3-10等 dataset上进行了评估，结果表明，我们的提议的抽样方法实际上提高了各种KGE模型（RotatE、TransE、HAKE、ComplEx、DistMult）的KG完成性能。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Social-Discourse-to-Measure-Check-worthiness-of-Claims-for-Fact-checking"><a href="#Leveraging-Social-Discourse-to-Measure-Check-worthiness-of-Claims-for-Fact-checking" class="headerlink" title="Leveraging Social Discourse to Measure Check-worthiness of Claims for Fact-checking"></a>Leveraging Social Discourse to Measure Check-worthiness of Claims for Fact-checking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09274">http://arxiv.org/abs/2309.09274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megha Sundriyal, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: 本研究旨在提出一种细化的声明检查价值 Task，以优化现有的声明检查系统。</li>
<li>methods: 该研究使用了一个大量人工标注的Twitter数据集，并提出了一种基于人工评估的CheckMate方法，以 JOINTLY 确定声明是否值得检查，以及导致这个结论的因素。</li>
<li>results: 研究表明， integrating 多种因素可以提高声明检查的准确率和效率，并且人工评估 validate 了这些结论。<details>
<summary>Abstract</summary>
The expansion of online social media platforms has led to a surge in online content consumption. However, this has also paved the way for disseminating false claims and misinformation. As a result, there is an escalating demand for a substantial workforce to sift through and validate such unverified claims. Currently, these claims are manually verified by fact-checkers. Still, the volume of online content often outweighs their potency, making it difficult for them to validate every single claim in a timely manner. Thus, it is critical to determine which assertions are worth fact-checking and prioritize claims that require immediate attention. Multiple factors contribute to determining whether a claim necessitates fact-checking, encompassing factors such as its factual correctness, potential impact on the public, the probability of inciting hatred, and more. Despite several efforts to address claim check-worthiness, a systematic approach to identify these factors remains an open challenge. To this end, we introduce a new task of fine-grained claim check-worthiness, which underpins all of these factors and provides probable human grounds for identifying a claim as check-worthy. We present CheckIt, a manually annotated large Twitter dataset for fine-grained claim check-worthiness. We benchmark our dataset against a unified approach, CheckMate, that jointly determines whether a claim is check-worthy and the factors that led to that conclusion. We compare our suggested system with several baseline systems. Finally, we report a thorough analysis of results and human assessment, validating the efficacy of integrating check-worthiness factors in detecting claims worth fact-checking.
</details>
<details>
<summary>摘要</summary>
在线社交媒体平台的扩展导致在线内容的摄 consumption 增加，但也导致了假宣传和不准确信息的传播。因此，需要一大量的人力来筛选和验证这些未经证实的宣言。现在，这些宣言都是由 фак-checker 手动验证的。然而，在线内容的量太大，fact-checker 无法在有限时间内验证每一个宣言。因此，是非常重要的确定哪些宣言值得验证，并且需要立即验证的宣言。多种因素会影响哪些宣言需要验证，包括宣言的准确性、公众影响、激进的可能性和更多。虽然有很多努力来解决宣言验证价值的问题，但是一个系统atic approach 来确定这些因素仍然是一个开放的挑战。为此，我们介绍了一个新的细化的宣言验证任务，这个任务涵盖了所有这些因素，并提供了人类可靠的判据来确定宣言是否值得验证。我们提供了 CheckIt，一个手动标注的大型 Twitter 数据集，用于细化的宣言验证。我们对我们的数据集进行了对joint 方法 CheckMate 的比较，joint 方法可以同时判断一个宣言是否值得验证，以及这个宣言被验证的原因。我们与多个基线系统进行比较。最后，我们进行了详细的分析结果和人类评估，证明了将验证价值因素纳入检测宣言值得验证的效果。
</details></li>
</ul>
<hr>
<h2 id="Code-quality-assessment-using-transformers"><a href="#Code-quality-assessment-using-transformers" class="headerlink" title="Code quality assessment using transformers"></a>Code quality assessment using transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09264">http://arxiv.org/abs/2309.09264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mosleh Mahamud, Isak Samsten</li>
<li>for: 这个论文的目的是自动评估编程作业的正确性，但是编程任务可以有多种解决方案，其中许多方案尽管正确，但是代码具有冗长、糟糕的命名和重复等� субjective 质量。</li>
<li>methods: 这篇论文使用了 CodeBERT 来自动分配代码质量分数。作者们尝试了不同的模型和训练方法，并对一个新的代码质量评估数据集进行了实验。</li>
<li>results: 研究发现，代码质量有一定的可预测性，并且使用 transformer 基于的模型，使用任务适应预训练可以更有效地解决这个问题。<details>
<summary>Abstract</summary>
Automatically evaluate the correctness of programming assignments is rather straightforward using unit and integration tests. However, programming tasks can be solved in multiple ways, many of which, although correct, are inelegant. For instance, excessive branching, poor naming or repetitiveness make the code hard to understand and maintain. These subjective qualities of code are hard to automatically assess using current techniques. In this work we investigate the use of CodeBERT to automatically assign quality score to Java code. We experiment with different models and training paradigms. We explore the accuracy of the models on a novel dataset for code quality assessment. Finally, we assess the quality of the predictions using saliency maps. We find that code quality to some extent is predictable and that transformer based models using task adapted pre-training can solve the task more efficiently than other techniques.
</details>
<details>
<summary>摘要</summary>
自动评估程序作业正确性 relativamente直 forward 使用单元测试和集成测试。然而，程序任务可以通过多种方式解决，许多方法都是正确的，但是命名不佳、重复性强或分支过多，导致代码难以理解和维护。这些编程质量的subjective特征难以通过当前技术自动评估。在这种工作中，我们 investigate使用CodeBERT自动为Java代码分配质量分数。我们试用不同的模型和训练方法。我们探索一个新的代码质量评估数据集的准确性。最后，我们评估预测的质量使用Saliency Map。我们发现代码质量有一定的预测性，并且基于 transformer 模型的任务适应预处理可以更有效地解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-for-Text-Expansion-Datasets-Metrics-and-Baselines"><a href="#A-Benchmark-for-Text-Expansion-Datasets-Metrics-and-Baselines" class="headerlink" title="A Benchmark for Text Expansion: Datasets, Metrics, and Baselines"></a>A Benchmark for Text Expansion: Datasets, Metrics, and Baselines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09198">http://arxiv.org/abs/2309.09198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Chen, Haiyun Jiang, Wei Bi, Rui Wang, Longyue Wang, Shuming Shi, Ruifeng Xu</li>
<li>for: This paper presents a new task called Text Expansion (TE), which aims to insert fine-grained modifiers into plain text to make it more concrete and vivid.</li>
<li>methods: The authors use four complementary approaches to construct a dataset of 12 million automatically generated instances and 2K human-annotated references for both English and Chinese. They also design various metrics to evaluate the effectiveness of the expansions.</li>
<li>results: The proposed Locate&amp;Infill models demonstrate superiority over the Text2Text baselines, especially in expansion informativeness. Experiments verify the feasibility of the TE task and point out potential directions for future research.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文提出了一个新的文本扩展任务（TE），旨在插入细化修饰符到文本中，使其更加具体和生动。</li>
<li>methods: 作者使用了四种补充方法构建了一个包含12万自动生成实例和2000个人工标注的数据集，以及多种评价指标。</li>
<li>results: 提出的 Locate&amp;Infill 模型在扩展信息量方面表现出色，特别是在对 Text2Text 基elines 的比较中。实验证明了文本扩展任务的可行性，并指出了未来研究的可能性。<details>
<summary>Abstract</summary>
This work presents a new task of Text Expansion (TE), which aims to insert fine-grained modifiers into proper locations of the plain text to concretize or vivify human writings. Different from existing insertion-based writing assistance tasks, TE requires the model to be more flexible in both locating and generation, and also more cautious in keeping basic semantics. We leverage four complementary approaches to construct a dataset with 12 million automatically generated instances and 2K human-annotated references for both English and Chinese. To facilitate automatic evaluation, we design various metrics from multiple perspectives. In particular, we propose Info-Gain to effectively measure the informativeness of expansions, which is an important quality dimension in TE. On top of a pre-trained text-infilling model, we build both pipelined and joint Locate&Infill models, which demonstrate the superiority over the Text2Text baselines, especially in expansion informativeness. Experiments verify the feasibility of the TE task and point out potential directions for future research toward better automatic text expansion.
</details>
<details>
<summary>摘要</summary>
To create a dataset for this task, we use four complementary approaches to generate 12 million automatically generated instances and 2,000 human-annotated references for both English and Chinese. We also design various metrics to evaluate the effectiveness of the expansions, including a new metric called Info-Gain that measures the informativeness of the expansions.We build both pipelined and joint Locate&Infill models on top of a pre-trained text-infilling model, and compare them with Text2Text baselines. Our experiments show that the proposed models outperform the baselines, especially in terms of expansion informativeness. This demonstrates the feasibility of the TE task and provides a new direction for future research in automatic text expansion.
</details></li>
</ul>
<hr>
<h2 id="Detecting-covariate-drift-in-text-data-using-document-embeddings-and-dimensionality-reduction"><a href="#Detecting-covariate-drift-in-text-data-using-document-embeddings-and-dimensionality-reduction" class="headerlink" title="Detecting covariate drift in text data using document embeddings and dimensionality reduction"></a>Detecting covariate drift in text data using document embeddings and dimensionality reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10000">http://arxiv.org/abs/2309.10000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinayaksodar/nlp_drift_paper_code">https://github.com/vinayaksodar/nlp_drift_paper_code</a></li>
<li>paper_authors: Vinayak Sodar, Ankit Sekseria</li>
<li>for: 本研究旨在提高文本分析模型的可靠性和性能，通过检测文本数据中的covariate漂移。</li>
<li>methods: 本研究使用了三种文档嵌入：TF-IDF使用LSA进行维度减少， Doc2Vec和BERT嵌入，以及使用PCA进行维度减少。检测covariate漂移的方法包括KS统计和MMD测试。</li>
<li>results: 实验结果表明，某些嵌入、维度减少方法和漂移检测方法的组合表现较好，可以有效地检测文本数据中的covariate漂移。这些结果对于提高可靠的文本分析模型做出了贡献。<details>
<summary>Abstract</summary>
Detecting covariate drift in text data is essential for maintaining the reliability and performance of text analysis models. In this research, we investigate the effectiveness of different document embeddings, dimensionality reduction techniques, and drift detection methods for identifying covariate drift in text data. We explore three popular document embeddings: term frequency-inverse document frequency (TF-IDF) using Latent semantic analysis(LSA) for dimentionality reduction and Doc2Vec, and BERT embeddings, with and without using principal component analysis (PCA) for dimensionality reduction. To quantify the divergence between training and test data distributions, we employ the Kolmogorov-Smirnov (KS) statistic and the Maximum Mean Discrepancy (MMD) test as drift detection methods. Experimental results demonstrate that certain combinations of embeddings, dimensionality reduction techniques, and drift detection methods outperform others in detecting covariate drift. Our findings contribute to the advancement of reliable text analysis models by providing insights into effective approaches for addressing covariate drift in text data.
</details>
<details>
<summary>摘要</summary>
检测文本数据中的变量漂移是维护文本分析模型的可靠性和性能的关键。在这个研究中，我们研究了不同的文档嵌入、维度减少技术和变量漂移检测方法，以确定在文本数据中检测变量漂移的效果。我们探索了三种流行的文档嵌入：term frequency-inverse document frequency（TF-IDF）使用隐藏语义分析（LSA）进行维度减少，Doc2Vec和BERT嵌入，并使用主成分分析（PCA）进行维度减少。为了量化训练和测试数据分布之间的差异，我们采用了科维莫洛夫-斯密涅夫（KS）统计和最大均值差（MMD）测试作为变量漂移检测方法。实验结果表明，某些组合的嵌入、维度减少技术和变量漂移检测方法在检测变量漂移方面表现出色。我们的发现对于建立可靠的文本分析模型做出了贡献，提供了有关有效地在文本数据中检测变量漂移的信息。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.CL_2023_09_17/" data-id="clogxf3lv00at5xrahpn2bcmd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.LG_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T10:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.LG_2023_09_17/">cs.LG - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-Over-Smoothing-and-Over-Squashing-using-Augmentations-of-Forman-Ricci-Curvature"><a href="#Mitigating-Over-Smoothing-and-Over-Squashing-using-Augmentations-of-Forman-Ricci-Curvature" class="headerlink" title="Mitigating Over-Smoothing and Over-Squashing using Augmentations of Forman-Ricci Curvature"></a>Mitigating Over-Smoothing and Over-Squashing using Augmentations of Forman-Ricci Curvature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09384">http://arxiv.org/abs/2309.09384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Fesser, Melanie Weber</li>
<li>For: This paper proposes a rewiring technique based on Augmented Forman-Ricci curvature (AFRC) to mitigate over-smoothing and over-squashing effects in message-passing Graph Neural Networks (GNNs).* Methods: The proposed technique uses AFRC, a scalable curvature notation that can be computed in linear time, to characterize over-smoothing and over-squashing effects in GNNs.* Results: The proposed approach achieves state-of-the-art performance while significantly reducing the computational cost in comparison with other methods. The paper also provides effective heuristics for hyperparameters in curvature-based rewiring, which avoids expensive hyperparameter searches.<details>
<summary>Abstract</summary>
While Graph Neural Networks (GNNs) have been successfully leveraged for learning on graph-structured data across domains, several potential pitfalls have been described recently. Those include the inability to accurately leverage information encoded in long-range connections (over-squashing), as well as difficulties distinguishing the learned representations of nearby nodes with growing network depth (over-smoothing). An effective way to characterize both effects is discrete curvature: Long-range connections that underlie over-squashing effects have low curvature, whereas edges that contribute to over-smoothing have high curvature. This observation has given rise to rewiring techniques, which add or remove edges to mitigate over-smoothing and over-squashing. Several rewiring approaches utilizing graph characteristics, such as curvature or the spectrum of the graph Laplacian, have been proposed. However, existing methods, especially those based on curvature, often require expensive subroutines and careful hyperparameter tuning, which limits their applicability to large-scale graphs. Here we propose a rewiring technique based on Augmented Forman-Ricci curvature (AFRC), a scalable curvature notation, which can be computed in linear time. We prove that AFRC effectively characterizes over-smoothing and over-squashing effects in message-passing GNNs. We complement our theoretical results with experiments, which demonstrate that the proposed approach achieves state-of-the-art performance while significantly reducing the computational cost in comparison with other methods. Utilizing fundamental properties of discrete curvature, we propose effective heuristics for hyperparameters in curvature-based rewiring, which avoids expensive hyperparameter searches, further improving the scalability of the proposed approach.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经成功地应用于不同领域的图数据上，但最近有一些潜在的坑害被描述了。这些坑害包括不能准确利用图中长距离连接中的信息 (过滤)，以及随着网络深度增加而导致近节点的学习表现相似化 (过滤)。一种有效的方式来描述这两种效果是离散曲率：图中长距离连接的离散曲率较低，而导致过滤的边的离散曲率较高。这一观察引起了重新连接技术的出现，这些技术通过添加或 removing 边来缓解过滤和过滤的问题。已有一些基于图特性的重新连接方法，如曲率或图laplacian的谱，被提出。然而，现有的方法，特别是基于曲率的方法，经常需要费时的优化和精心调整 hyperparameter，这限制了它们在大规模图上的应用。我们提出了基于 Augmented Forman-Ricci curvature (AFRC) 的重新连接技术，AFRC 是一种可以在线时间内计算的离散曲率表示法。我们证明 AFRC 能够有效地描述 GNN 中的过滤和过滤效果。我们通过实验证明，我们的方法可以 achieve state-of-the-art 性能，同时减少了与其他方法相比的计算成本。利用离散曲率的基本属性，我们提出了一些有效的启发式 hyperparameter 优化方法，以避免费时的寻找优化方法，进一步提高了我们的方法的可扩展性。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-in-Temporal-Heterogeneity"><a href="#Federated-Learning-in-Temporal-Heterogeneity" class="headerlink" title="Federated Learning in Temporal Heterogeneity"></a>Federated Learning in Temporal Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09381">http://arxiv.org/abs/2309.09381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junghwan Lee</li>
<li>for: 本研究探讨了 federated learning 中的时间不同客户端上的 temporally 不一致问题。</li>
<li>methods: 我们提出了一种基于 empirical 观察的方法来 mitigate temporally 不一致问题，以便更有效地进行 federated learning。</li>
<li>results: 我们发现 global model 使用 fix-length sequence 更快地 converges than varying-length sequence。<details>
<summary>Abstract</summary>
In this work, we explored federated learning in temporal heterogeneity across clients. We observed that global model obtained by \texttt{FedAvg} trained with fixed-length sequences shows faster convergence than varying-length sequences. We proposed methods to mitigate temporal heterogeneity for efficient federated learning based on the empirical observation.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们探索了联邦学习中的时间不同客户端之间的差异。我们发现，使用固定长度序列的\texttt{FedAvg}训练的全局模型在更快地尝试了。我们提出了一些缓解时间差异的方法，以便有效地进行联邦学习，基于实际观察。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Fully-Convolutional-Generative-Machine-Learning-Method-for-Accelerating-Non-Equilibrium-Greens-Function-Simulations"><a href="#Fully-Convolutional-Generative-Machine-Learning-Method-for-Accelerating-Non-Equilibrium-Greens-Function-Simulations" class="headerlink" title="Fully Convolutional Generative Machine Learning Method for Accelerating Non-Equilibrium Greens Function Simulations"></a>Fully Convolutional Generative Machine Learning Method for Accelerating Non-Equilibrium Greens Function Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09374">http://arxiv.org/abs/2309.09374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Preslav Aleksandrov, Ali Rezaei, Nikolas Xeni, Tapas Dutta, Asen Asenov, Vihar Georgiev</li>
<li>for: 这篇论文描述了一种新的模拟方法，它结合机器学习和设备模拟。这种模拟方法基于量子力学非平衡绿函数（NEGF）方法，并使用扩展到卷积生成网络。我们称之为ML-NEGF方法，并在我们的内部 simulate 软件（NESS）中实现。</li>
<li>methods: 这种模拟方法使用了机器学习方法，卷积生成网络来学习奈米薄膜晶体管的物理行为。</li>
<li>results: 据报告，ML-NEGF方法可以在相同精度下提高模拟速度，减少计算时间，平均提高了60%。<details>
<summary>Abstract</summary>
This work describes a novel simulation approach that combines machine learning and device modelling simulations. The device simulations are based on the quantum mechanical non-equilibrium Greens function (NEGF) approach and the machine learning method is an extension to a convolutional generative network. We have named our new simulation approach ML-NEGF and we have implemented it in our in-house simulator called NESS (nano-electronics simulations software). The reported results demonstrate the improved convergence speed of the ML-NEGF method in comparison to the standard NEGF approach. The trained ML model effectively learns the underlying physics of nano-sheet transistor behaviour, resulting in faster convergence of the coupled Poisson-NEGF simulations. Quantitatively, our ML- NEGF approach achieves an average convergence acceleration of 60%, substantially reducing the computational time while maintaining the same accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-on-Congestion-Control-and-Scheduling-for-Multipath-TCP-Machine-Learning-vs-Classical-Approaches"><a href="#A-Survey-on-Congestion-Control-and-Scheduling-for-Multipath-TCP-Machine-Learning-vs-Classical-Approaches" class="headerlink" title="A Survey on Congestion Control and Scheduling for Multipath TCP: Machine Learning vs Classical Approaches"></a>A Survey on Congestion Control and Scheduling for Multipath TCP: Machine Learning vs Classical Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09372">http://arxiv.org/abs/2309.09372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maisha Maliha, Golnaz Habibi, Mohammed Atiquzzaman</li>
<li>for: 本研究旨在解决多路 TCP (MPTCP) 中的几个问题，包括流量占用和延迟控制。</li>
<li>methods: 本研究使用两种主要方法：非数据驱动（传统）方法和数据驱动（机器学习）方法。</li>
<li>results: 本研究对这两种方法的优缺点进行比较，并提供实际环境中 MPCTP 的实现和模拟。<details>
<summary>Abstract</summary>
Multipath TCP (MPTCP) has been widely used as an efficient way for communication in many applications. Data centers, smartphones, and network operators use MPTCP to balance the traffic in a network efficiently. MPTCP is an extension of TCP (Transmission Control Protocol), which provides multiple paths, leading to higher throughput and low latency. Although MPTCP has shown better performance than TCP in many applications, it has its own challenges. The network can become congested due to heavy traffic in the multiple paths (subflows) if the subflow rates are not determined correctly. Moreover, communication latency can occur if the packets are not scheduled correctly between the subflows. This paper reviews techniques to solve the above-mentioned problems based on two main approaches; non data-driven (classical) and data-driven (Machine Learning) approaches. This paper compares these two approaches and highlights their strengths and weaknesses with a view to motivating future researchers in this exciting area of machine learning for communications. This paper also provides details on the simulation of MPTCP and its implementations in real environments.
</details>
<details>
<summary>摘要</summary>
multipath TCP (MPTCP) 已经广泛应用于许多应用程序中，以提高网络吞吐量和低延迟。数据中心、智能手机和网络运营商都使用 MPTCP 来均衡网络流量。MPTCP 是 TCP（传输控制协议）的扩展，它提供多个路径，从而实现更高的吞吐量和低延迟。虽然 MPTCP 在许多应用中表现了更好的性能，但它还存在一些挑战。如果多个流（subflow）的流量不是正确地确定的话，网络就可能变得拥堵。此外，如果包没有正确地安排的话，则会出现交通延迟。本文评论了解决上述问题的两种方法：非数据驱动（传统）方法和数据驱动（机器学习）方法。本文比较这两种方法的优劣，并强调它们在这一领域的挑战和未来研究的可能性。此外，本文还提供了 MPTCP 的模拟和实现在真实环境中的细节。
</details></li>
</ul>
<hr>
<h2 id="An-Automatic-Tuning-MPC-with-Application-to-Ecological-Cruise-Control"><a href="#An-Automatic-Tuning-MPC-with-Application-to-Ecological-Cruise-Control" class="headerlink" title="An Automatic Tuning MPC with Application to Ecological Cruise Control"></a>An Automatic Tuning MPC with Application to Ecological Cruise Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09358">http://arxiv.org/abs/2309.09358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Abtahi, Mahdis Rabbani, Shima Nazari</li>
<li>for: 这个论文是为了研究模型预测控制（MPC）的自动调整方法，以优化MPC控制器的性能。</li>
<li>methods: 该论文使用了动态Programming和神经网络来解决MPC控制器的自动调整问题，并在online操作中使用预览信息来适应道路坡度。</li>
<li>results:  simulations results show that the proposed approach can effectively optimize the fuel consumption of the ecological cruise control system under different road geometries.<details>
<summary>Abstract</summary>
Model predictive control (MPC) is a powerful tool for planning and controlling dynamical systems due to its capacity for handling constraints and taking advantage of preview information. Nevertheless, MPC performance is highly dependent on the choice of cost function tuning parameters. In this work, we demonstrate an approach for online automatic tuning of an MPC controller with an example application to an ecological cruise control system that saves fuel by using a preview of road grade. We solve the global fuel consumption minimization problem offline using dynamic programming and find the corresponding MPC cost function by solving the inverse optimization problem. A neural network fitted to these offline results is used to generate the desired MPC cost function weight during online operation. The effectiveness of the proposed approach is verified in simulation for different road geometries.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Structure-to-Property-Chemical-Element-Embeddings-and-a-Deep-Learning-Approach-for-Accurate-Prediction-of-Chemical-Properties"><a href="#Structure-to-Property-Chemical-Element-Embeddings-and-a-Deep-Learning-Approach-for-Accurate-Prediction-of-Chemical-Properties" class="headerlink" title="Structure to Property: Chemical Element Embeddings and a Deep Learning Approach for Accurate Prediction of Chemical Properties"></a>Structure to Property: Chemical Element Embeddings and a Deep Learning Approach for Accurate Prediction of Chemical Properties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09355">http://arxiv.org/abs/2309.09355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmamur/elembert">https://github.com/dmamur/elembert</a></li>
<li>paper_authors: Shokirbek Shermukhamedov, Dilorom Mamurjonova, Michael Probst</li>
<li>for: 本研究使用机器学习技术来预测分子性质，加速药物发现和材料设计。</li>
<li>methods: 本研究使用深度学习技术，包括多层编码器和解码器架构，进行分类任务。</li>
<li>results: 我们通过应用这种方法，在不同的输入数据上 achiev 高度预测力，例如在Matbench和Moleculenetbenchmarks上，并进行了分子数据表示 вектор的全面分析，揭示了分子数据中的下意识模式。<details>
<summary>Abstract</summary>
The application of machine learning (ML) techniques in computational chemistry has led to significant advances in predicting molecular properties, accelerating drug discovery, and material design. ML models can extract hidden patterns and relationships from complex and large datasets, allowing for the prediction of various chemical properties with high accuracy. The use of such methods has enabled the discovery of molecules and materials that were previously difficult to identify. This paper introduces a new ML model based on deep learning techniques, such as a multilayer encoder and decoder architecture, for classification tasks. We demonstrate the opportunities offered by our approach by applying it to various types of input data, including organic and inorganic compounds. In particular, we developed and tested the model using the Matbench and Moleculenet benchmarks, which include crystal properties and drug design-related benchmarks. We also conduct a comprehensive analysis of vector representations of chemical compounds, shedding light on the underlying patterns in molecular data. The models used in this work exhibit a high degree of predictive power, underscoring the progress that can be made with refined machine learning when applied to molecular and material datasets. For instance, on the Tox21 dataset, we achieved an average accuracy of 96%, surpassing the previous best result by 10%. Our code is publicly available at https://github.com/dmamur/elembert.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）技术在计算化学中应用得到了显著的进步，包括预测分子性质、加速药物发现和材料设计。ML模型可以从复杂大量数据中提取隐藏的模式和关系，以高度准确地预测各种化学性质。这种方法的应用使得可以发现 previously difficult to identify的分子和材料。本文介绍了一种基于深度学习技术的新ML模型，包括多层编码器和解码器建筑，用于分类任务。我们通过应用这种方法于不同类型的输入数据，包括有机和无机化合物，证明了我们的方法的可行性。具体来说，我们使用了Matbench和Moleculenetbenchmark，包括晶体性和药物设计相关的benchmark，进行了全面的分子表示vector分析，揭示了分子数据中的下面纲。使用的模型在这个工作中表现出了高度预测力，这将进一步推动了对分子和材料数据的机器学习应用。例如，在Tox21dataset上，我们实现了96%的平均准确率，比前一个最佳结果高出10%。我们的代码可以在https://github.com/dmamur/elembert上下载。
</details></li>
</ul>
<hr>
<h2 id="Simulation-based-Inference-for-Exoplanet-Atmospheric-Retrieval-Insights-from-winning-the-Ariel-Data-Challenge-2023-using-Normalizing-Flows"><a href="#Simulation-based-Inference-for-Exoplanet-Atmospheric-Retrieval-Insights-from-winning-the-Ariel-Data-Challenge-2023-using-Normalizing-Flows" class="headerlink" title="Simulation-based Inference for Exoplanet Atmospheric Retrieval: Insights from winning the Ariel Data Challenge 2023 using Normalizing Flows"></a>Simulation-based Inference for Exoplanet Atmospheric Retrieval: Insights from winning the Ariel Data Challenge 2023 using Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09337">http://arxiv.org/abs/2309.09337</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/astroai-cfa/ariel_data_challenge_2023_solution">https://github.com/astroai-cfa/ariel_data_challenge_2023_solution</a></li>
<li>paper_authors: Mayeul Aubin, Carolina Cuesta-Lazaro, Ethan Tregidga, Javier Viaña, Cecilia Garraffo, Iouli E. Gordon, Mercedes López-Morales, Robert J. Hargreaves, Vladimir Yu. Makhnev, Jeremy J. Drake, Douglas P. Finkbeiner, Phillip Cargile</li>
<li>for: 这项研究旨在提出新的机器学习模型，用于分析外层行星大气层谱。</li>
<li>methods: 该研究使用了 Normalizing Flows 技术，预测大气参数的 posterior 分布下不同大气假设。</li>
<li>results: 研究发现了一种新的机器学习模型，可以更高效地分析外层行星大气层谱。此外，研究还发现了一种更高性能的模型，即使在挑战中获得较低分而然。这些发现表明需要重新评估评价指标，并且探索更加高效和准确的方法来分析外层行星大气层谱。<details>
<summary>Abstract</summary>
Advancements in space telescopes have opened new avenues for gathering vast amounts of data on exoplanet atmosphere spectra. However, accurately extracting chemical and physical properties from these spectra poses significant challenges due to the non-linear nature of the underlying physics.   This paper presents novel machine learning models developed by the AstroAI team for the Ariel Data Challenge 2023, where one of the models secured the top position among 293 competitors. Leveraging Normalizing Flows, our models predict the posterior probability distribution of atmospheric parameters under different atmospheric assumptions.   Moreover, we introduce an alternative model that exhibits higher performance potential than the winning model, despite scoring lower in the challenge. These findings highlight the need to reevaluate the evaluation metric and prompt further exploration of more efficient and accurate approaches for exoplanet atmosphere spectra analysis.   Finally, we present recommendations to enhance the challenge and models, providing valuable insights for future applications on real observational data. These advancements pave the way for more effective and timely analysis of exoplanet atmospheric properties, advancing our understanding of these distant worlds.
</details>
<details>
<summary>摘要</summary>
This paper presents new machine learning models developed by the AstroAI team for the Ariel Data Challenge 2023. One of our models achieved the top position among 293 competitors by leveraging Normalizing Flows to predict the posterior probability distribution of atmospheric parameters under different atmospheric assumptions.Furthermore, we introduce an alternative model that exhibits higher performance potential than the winning model, despite scoring lower in the challenge. These findings highlight the need to reevaluate the evaluation metric and prompt further exploration of more efficient and accurate approaches for analyzing exoplanet atmosphere spectra.Finally, we provide recommendations to enhance the challenge and models, offering valuable insights for future applications on real observational data. These advancements pave the way for more effective and timely analysis of exoplanet atmospheric properties, deepening our understanding of these distant worlds.
</details></li>
</ul>
<hr>
<h2 id="Experiential-Informed-Data-Reconstruction-for-Fishery-Sustainability-and-Policies-in-the-Azores"><a href="#Experiential-Informed-Data-Reconstruction-for-Fishery-Sustainability-and-Policies-in-the-Azores" class="headerlink" title="Experiential-Informed Data Reconstruction for Fishery Sustainability and Policies in the Azores"></a>Experiential-Informed Data Reconstruction for Fishery Sustainability and Policies in the Azores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09326">http://arxiv.org/abs/2309.09326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brenda Nogueira, Gui M. Menezes, Nuno Moniz</li>
<li>for: 本研究的目的是重建附近葡萄牙阿鲁亚群岛渔业数据集（2010-2017年），以便更好地了解渔业捕捞方法对海洋生态系统的影响。</li>
<li>methods: 本研究使用了域知和机器学习方法来恢复数据集，并通过对每个鱼类捕捞数据进行分析来推断渔业工具的使用情况。</li>
<li>results: 研究结果表明，通过使用不同的模型方法可以有效地重建数据集，并提供了新的视角对不同渔业的行为和时间的影响，这些信息对未来鱼类人口评估和管理具有重要意义。<details>
<summary>Abstract</summary>
Fishery analysis is critical in maintaining the long-term sustainability of species and the livelihoods of millions of people who depend on fishing for food and income. The fishing gear, or metier, is a key factor significantly impacting marine habitats, selectively targeting species and fish sizes. Analysis of commercial catches or landings by metier in fishery stock assessment and management is crucial, providing robust estimates of fishing efforts and their impact on marine ecosystems. In this paper, we focus on a unique data set from the Azores' fishing data collection programs between 2010 and 2017, where little information on metiers is available and sparse throughout our timeline. Our main objective is to tackle the task of data set reconstruction, leveraging domain knowledge and machine learning methods to retrieve or associate metier-related information to each fish landing. We empirically validate the feasibility of this task using a diverse set of modeling approaches and demonstrate how it provides new insights into different fisheries' behavior and the impact of metiers over time, which are essential for future fish population assessments, management, and conservation efforts.
</details>
<details>
<summary>摘要</summary>
鱼业分析是维护生物种和渔业生产的长期可持续性的关键。鱼网（metier）是影响海洋生态系统的关键因素，可以选择性地目标种类和鱼的大小。在鱼业资源评估和管理中，商业捕捞数据的分析是非常重要的，可以提供坚实的捕捞努力和海洋生态系统的影响。本文关注Azores鱼业数据收集计划在2010年至2017年之间的独特数据集，因为这个数据集中有少量的鱼网信息，并且这些信息在时间线上是罕见的。我们的主要目标是使用领域知识和机器学习方法来重建这个数据集，并将鱼网相关信息与每个鱼投射相关联。我们通过多种模型方法进行实验验证，并证明这个任务的可行性，从而提供新的鱼业行为和鱼网的影响情况，这些信息对未来鱼种评估、管理和保护具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Kinematics-aware-Trajectory-Generation-and-Prediction-with-Latent-Stochastic-Differential-Modeling"><a href="#Kinematics-aware-Trajectory-Generation-and-Prediction-with-Latent-Stochastic-Differential-Modeling" class="headerlink" title="Kinematics-aware Trajectory Generation and Prediction with Latent Stochastic Differential Modeling"></a>Kinematics-aware Trajectory Generation and Prediction with Latent Stochastic Differential Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09317">http://arxiv.org/abs/2309.09317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruochen Jiao, Yixuan Wang, Xiangguo Liu, Chao Huang, Qi Zhu</li>
<li>for: 本研究旨在提高自动驾驶车辆的路径生成和预测能力，以便在开发和运行过程中更好地处理复杂的交通enario。</li>
<li>methods: 我们 integrate了机械知识和神经网络泊brace(SDE)，并基于novel latent kinematics-aware SDE（LK-SDE）开发了一种variational autoencoder，以生成车辆运动。我们的方法结合了模型基于和深度学习基于技术的优点。</li>
<li>results: 我们的方法在生成和预测车辆路径时比基eline方法表现出色，生成的路径更加真实、物理可行和精度可控。<details>
<summary>Abstract</summary>
Trajectory generation and trajectory prediction are two critical tasks for autonomous vehicles, which generate various trajectories during development and predict the trajectories of surrounding vehicles during operation, respectively. However, despite significant advances in improving their performance, it remains a challenging problem to ensure that the generated/predicted trajectories are realistic, explainable, and physically feasible. Existing model-based methods provide explainable results, but are constrained by predefined model structures, limiting their capabilities to address complex scenarios. Conversely, existing deep learning-based methods have shown great promise in learning various traffic scenarios and improving overall performance, but they often act as opaque black boxes and lack explainability. In this work, we integrate kinematic knowledge with neural stochastic differential equations (SDE) and develop a variational autoencoder based on a novel latent kinematics-aware SDE (LK-SDE) to generate vehicle motions. Our approach combines the advantages of both model-based and deep learning-based techniques. Experimental results demonstrate that our method significantly outperforms baseline approaches in producing realistic, physically-feasible, and precisely-controllable vehicle trajectories, benefiting both generation and prediction tasks.
</details>
<details>
<summary>摘要</summary>
几何轨迹生成和预测是自动车的两个关键任务，它们在开发过程中产生了许多轨迹，并在运行过程中预测周围车辆的轨迹。然而，即使有了重要的进步，仍然是一个挑战性的问题，确保生成/预测的轨迹是现实、可解释和物理可行的。现有的模型基方法可以提供可解释的结果，但它们受限于预先定义的模型结构，导致它们无法处理复杂的enario。相反，现有的深度学习基本方法在学习不同的交通enario中表现出色，但它们经常作为透明的黑盒子，无法提供可解释的结果。在这个工作中，我们结合了几何知识和神经统计学 differential equation (SDE)，开发了一个基于novel latent kinematics-aware SDE (LK-SDE)的抽象自动车动作统计模型。我们的方法结合了模型基的优点和深度学习基的优点。实验结果显示，我们的方法与基准方法相比，在生成和预测轨迹任务中表现出色，具有现实、物理可行和精确控制的轨迹。
</details></li>
</ul>
<hr>
<h2 id="Energy-stable-neural-network-for-gradient-flow-equations"><a href="#Energy-stable-neural-network-for-gradient-flow-equations" class="headerlink" title="Energy stable neural network for gradient flow equations"></a>Energy stable neural network for gradient flow equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10002">http://arxiv.org/abs/2309.10002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganghua Fan, Tianyu Jin, Yuan Lan, Yang Xiang, Luchan Zhang</li>
<li>for: 解决梯度流方程 equations 的方法</li>
<li>methods: 使用一种基于副变量的等价形式来更新解决方案，并使用一些能量衰退块来实现梯度流方程的演化过程中的稳定性</li>
<li>results: 通过实验证明，该网络能够生成高精度和稳定的预测结果<details>
<summary>Abstract</summary>
In this paper, we propose an energy stable network (EStable-Net) for solving gradient flow equations. The solution update scheme in our neural network EStable-Net is inspired by a proposed auxiliary variable based equivalent form of the gradient flow equation. EStable-Net enables decreasing of a discrete energy along the neural network, which is consistent with the property in the evolution process of the gradient flow equation. The architecture of the neural network EStable-Net consists of a few energy decay blocks, and the output of each block can be interpreted as an intermediate state of the evolution process of the gradient flow equation. This design provides a stable, efficient and interpretable network structure. Numerical experimental results demonstrate that our network is able to generate high accuracy and stable predictions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种能量稳定网络（EStable-Net），用于解决梯度流方程。我们的神经网络EStable-Net中的解决方案是基于提出的辅助变量基于等效形式的梯度流方程的想法。EStable-Net使得梯度流方程中的能量逐渐减少，与演化过程中的性质相一致。神经网络EStable-Net的架构包括一些能量衰减块，每个块的输出可以被解释为梯度流方程的演化过程中的中间状态。这种设计提供了稳定、高效和可解释的网络结构。数值实验结果表明，我们的网络能够生成高精度和稳定的预测。
</details></li>
</ul>
<hr>
<h2 id="Global-Convergence-of-SGD-For-Logistic-Loss-on-Two-Layer-Neural-Nets"><a href="#Global-Convergence-of-SGD-For-Logistic-Loss-on-Two-Layer-Neural-Nets" class="headerlink" title="Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets"></a>Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09258">http://arxiv.org/abs/2309.09258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pulkit Gopalani, Samyak Jha, Anirbit Mukherjee</li>
<li>for: 这个论文目的是证明SGD可以快速收敛到深度为2的抽象函数网络的全局最低点，无论数据是什么样的，Activation函数是否是sigmoid或tanh。</li>
<li>methods: 这个论文使用了SGD算法，并证明了其在 kontinuous time 下的快速收敛速率，包括使用SoftPlus activation function。</li>
<li>results: 论文证明了SGD在这些对象函数上的快速收敛，并且适用于任何数据和Activation函数。<details>
<summary>Abstract</summary>
In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates with adequately smooth and bounded activations like sigmoid and tanh. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence of Frobenius norm regularized logistic loss functions on constant-sized neural nets which are "Villani functions" and thus be able to build on recent progress with analyzing SGD on such objectives.
</details>
<details>
<summary>摘要</summary>
在这份备忘录中，我们证明了SGD在适当规则化的Logistic Empirical Risk函数的深度为2的神经网络上具有首次性的可证明收敛性，包括任意数据和任意数量的门控件，以及具有适当的平滑和缓冲的活化函数，如sigmoid和tanh。我们还证明了SGD在继续时间下的快速收敛速率，其适用于饱和不bounded的活化函数，如SoftPlus。我们的关键想法是证明常量大小神经网络上的Frobenius norm规则化Logistic loss函数是"Villani函数"，因此可以基于最近的SGD分析进程建立。
</details></li>
</ul>
<hr>
<h2 id="User-Assignment-and-Resource-Allocation-for-Hierarchical-Federated-Learning-over-Wireless-Networks"><a href="#User-Assignment-and-Resource-Allocation-for-Hierarchical-Federated-Learning-over-Wireless-Networks" class="headerlink" title="User Assignment and Resource Allocation for Hierarchical Federated Learning over Wireless Networks"></a>User Assignment and Resource Allocation for Hierarchical Federated Learning over Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09253">http://arxiv.org/abs/2309.09253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tinghao Zhang, Kwok-Yan Lam, Jun Zhao</li>
<li>for: 这篇论文主要关注于提高处理器的能效性和延迟时间，并且解决资料隐私问题。</li>
<li>methods: 这篇论文提出了一个弹性 Federated Learning（HFL）架构，并且提出了两个算法来优化资源分配和用户分配。</li>
<li>results: 实验结果显示，这个提案的HFL架构可以对现有的研究进行能源和延迟时间的明显优化。<details>
<summary>Abstract</summary>
The large population of wireless users is a key driver of data-crowdsourced Machine Learning (ML). However, data privacy remains a significant concern. Federated Learning (FL) encourages data sharing in ML without requiring data to leave users' devices but imposes heavy computation and communications overheads on mobile devices. Hierarchical FL (HFL) alleviates this problem by performing partial model aggregation at edge servers. HFL can effectively reduce energy consumption and latency through effective resource allocation and appropriate user assignment. Nevertheless, resource allocation in HFL involves optimizing multiple variables, and the objective function should consider both energy consumption and latency, making the development of resource allocation algorithms very complicated. Moreover, it is challenging to perform user assignment, which is a combinatorial optimization problem in a large search space. This article proposes a spectrum resource optimization algorithm (SROA) and a two-stage iterative algorithm (TSIA) for HFL. Given an arbitrary user assignment pattern, SROA optimizes CPU frequency, transmit power, and bandwidth to minimize system cost. TSIA aims to find a user assignment pattern that considerably reduces the total system cost. Experimental results demonstrate the superiority of the proposed HFL framework over existing studies in energy and latency reduction.
</details>
<details>
<summary>摘要</summary>
大量无线用户人群是数据拥有者学习（ML）的关键驱动力，但数据隐私保护仍然是一大问题。联邦学习（FL）鼓励数据在用户设备上进行学习，而不需要数据离开用户设备，但是在移动设备上进行计算和通信 overhead 占用了大量资源。层次联邦学习（HFL）解决了这个问题，通过在边缘服务器进行部分模型聚合来减少计算和通信 overhead。HFL 可以有效降低能源消耗和延迟，通过有效的资源分配和合适的用户分配。但是，资源分配在 HFL 中包括优化多个变量的问题，并且目标函数应该考虑到能源消耗和延迟两个方面，这使得资源分配算法的开发变得非常复杂。另外，用户分配是一个具有大量搜索空间的启发式优化问题。本文提出了一种spectrum resource optimization algorithm（SROA）和一种两个阶段迭代算法（TSIA）来解决 HFL 中的资源分配和用户分配问题。给定任意用户分配模式，SROA 将在用户设备上优化 CPU 频率、发射功率和带宽，以最小化系统成本。TSIA 则是一种希望找到一个考虑到总系统成本的用户分配模式。实验结果表明，提出的 HFL 框架在能源和延迟两个方面都有较好的性能。
</details></li>
</ul>
<hr>
<h2 id="High-dimensional-manifold-of-solutions-in-neural-networks-insights-from-statistical-physics"><a href="#High-dimensional-manifold-of-solutions-in-neural-networks-insights-from-statistical-physics" class="headerlink" title="High-dimensional manifold of solutions in neural networks: insights from statistical physics"></a>High-dimensional manifold of solutions in neural networks: insights from statistical physics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09240">http://arxiv.org/abs/2309.09240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enrico M. Malatesta</li>
<li>for: 这篇论文探讨了神经网络的统计力学方法，尤其是使用 binary 和连续权重的 perceptron 架构，在分类设定下。</li>
<li>methods: 论文使用了 Gardner 的 replica 方法，derived SAT&#x2F;UNSAT 转变在存储设定下。</li>
<li>results: 论文发现了 zero 训练错误配置的几何排序，并如何这种排序随训练集大小的增加而改变。 论文还证明了，在 binary 权重模型中，算法困难是因为解决区域的消失，这个区域可以到非常大的距离。最后，论文表明了研究线性模式连接 между解决方案可以提供解决批处理的平均形状的信息。<details>
<summary>Abstract</summary>
In these pedagogic notes I review the statistical mechanics approach to neural networks, focusing on the paradigmatic example of the perceptron architecture with binary an continuous weights, in the classification setting. I will review the Gardner's approach based on replica method and the derivation of the SAT/UNSAT transition in the storage setting. Then, I discuss some recent works that unveiled how the zero training error configurations are geometrically arranged, and how this arrangement changes as the size of the training set increases. I also illustrate how different regions of solution space can be explored analytically and how the landscape in the vicinity of a solution can be characterized. I give evidence how, in binary weight models, algorithmic hardness is a consequence of the disappearance of a clustered region of solutions that extends to very large distances. Finally, I demonstrate how the study of linear mode connectivity between solutions can give insights into the average shape of the solution manifold.
</details>
<details>
<summary>摘要</summary>
这些教学笔记中，我将对神经网络的统计力学方法进行介绍，以某种类型的感知器架构为例，并将着眼于分类设置下的情况。我将详细介绍加德纳的方法，包括使用复制方法的 derivation，以及存储设置下的 SAT/UNSAT 转变。然后，我会讨论一些最近的研究，描述了 zero training error 配置的几何排布，以及这个排布如何随训练集大小的变化。我还会说明如何在解决空间中分析不同区域的解，以及在解近 vicinity 中描述解决方案的场景。最后，我会展示如何在 binary weight 模型中，算法困难性是因为解决空间中的集中区域消失。此外，我还会讨论如何通过 linear mode 连接 между解来了解解决方案的平均形状。
</details></li>
</ul>
<hr>
<h2 id="Globally-Convergent-Accelerated-Algorithms-for-Multilinear-Sparse-Logistic-Regression-with-ell-0-constraints"><a href="#Globally-Convergent-Accelerated-Algorithms-for-Multilinear-Sparse-Logistic-Regression-with-ell-0-constraints" class="headerlink" title="Globally Convergent Accelerated Algorithms for Multilinear Sparse Logistic Regression with $\ell_0$-constraints"></a>Globally Convergent Accelerated Algorithms for Multilinear Sparse Logistic Regression with $\ell_0$-constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09239">http://arxiv.org/abs/2309.09239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weifeng-yang/mlsr">https://github.com/weifeng-yang/mlsr</a></li>
<li>paper_authors: Weifeng Yang, Wenwen Min</li>
<li>For: The paper is written for analyzing multidimensional data using a Multilinear Sparse Logistic Regression model with $\ell_0$-constraints (MLSR).* Methods: The paper proposes an Accelerated Proximal Alternating Linearized Minimization with Adaptive Momentum (APALM$^+$) method to solve the $\ell_0$-MLSR model, which is a novel approach that combines the advantages of both the $\ell_1$-norm and the $\ell_2$-norm.* Results: The paper demonstrates the superior performance of the proposed APALM$^+$ method in terms of both accuracy and speed, compared to other state-of-the-art methods, on synthetic and real-world datasets. Additionally, the paper provides a proof of convergence for the objective function of the $\ell_0$-MLSR model using the Kurdyka-Lojasiewicz property.<details>
<summary>Abstract</summary>
Tensor data represents a multidimensional array. Regression methods based on low-rank tensor decomposition leverage structural information to reduce the parameter count. Multilinear logistic regression serves as a powerful tool for the analysis of multidimensional data. To improve its efficacy and interpretability, we present a Multilinear Sparse Logistic Regression model with $\ell_0$-constraints ($\ell_0$-MLSR). In contrast to the $\ell_1$-norm and $\ell_2$-norm, the $\ell_0$-norm constraint is better suited for feature selection. However, due to its nonconvex and nonsmooth properties, solving it is challenging and convergence guarantees are lacking. Additionally, the multilinear operation in $\ell_0$-MLSR also brings non-convexity. To tackle these challenges, we propose an Accelerated Proximal Alternating Linearized Minimization with Adaptive Momentum (APALM$^+$) method to solve the $\ell_0$-MLSR model. We provide a proof that APALM$^+$ can ensure the convergence of the objective function of $\ell_0$-MLSR. We also demonstrate that APALM$^+$ is globally convergent to a first-order critical point as well as establish convergence rate by using the Kurdyka-Lojasiewicz property. Empirical results obtained from synthetic and real-world datasets validate the superior performance of our algorithm in terms of both accuracy and speed compared to other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
tensor数据表示多维数组。基于低维张量分解的回归方法利用结构信息来减少参数数。多线性логистиック回归作为多维数据分析的powerful工具。为了提高其效果和可解性，我们提出了多线性稀缺LOGISTIC回归模型（$\ell_0$-MLSR）。在$\ell_1$-norm和$\ell_2$-norm之外，$\ell_0$-norm约束更适合特征选择。然而，由于其非拟 convex和非均匀性质，解决它的困难重大，并且存在无法确保的收敛保证。此外，多线性操作在$\ell_0$-MLSR中也带来了非拟 convex性。为了解决这些挑战，我们提出了一种加速 proximal alternating linearized minimization with adaptive momentum（APALM$^+）方法来解决$\ell_0$-MLSR模型。我们提供了一个证明，表明APALM$^+$可以确保$\ell_0$-MLSR模型的目标函数收敛。此外，我们还证明APALM$^+$是全球收敛到一个第一阶关键点，并且使用库德ijk Lojasiewicz性质来确定收敛速率。实验结果表明，基于实验和实际数据，我们的算法在精度和速度方面与当前状态艺术方法相比具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Provable-learning-of-quantum-states-with-graphical-models"><a href="#Provable-learning-of-quantum-states-with-graphical-models" class="headerlink" title="Provable learning of quantum states with graphical models"></a>Provable learning of quantum states with graphical models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09235">http://arxiv.org/abs/2309.09235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liming Zhao, Naixu Guo, Ming-Xing Luo, Patrick Rebentrost</li>
<li>for: 本文研究了一种可以快速学习的量子状态的方法，具体来说是Restricted Boltzmann Machines（RBMs）。</li>
<li>methods: 本文使用了两个邻域学习算法，即 ferromagnetic 邻域学习算法和 locally consistent 邻域学习算法，以实现高效的量子状态学习。</li>
<li>results: 本文证明了使用这两种邻域学习算法可以在一定的情况下实现对量子状态的高效学习，并且可以比逻辑学习更快。<details>
<summary>Abstract</summary>
The complete learning of an $n$-qubit quantum state requires samples exponentially in $n$. Several works consider subclasses of quantum states that can be learned in polynomial sample complexity such as stabilizer states or high-temperature Gibbs states. Other works consider a weaker sense of learning, such as PAC learning and shadow tomography. In this work, we consider learning states that are close to neural network quantum states, which can efficiently be represented by a graphical model called restricted Boltzmann machines (RBMs). To this end, we exhibit robustness results for efficient provable two-hop neighborhood learning algorithms for ferromagnetic and locally consistent RBMs. We consider the $L_p$-norm as a measure of closeness, including both total variation distance and max-norm distance in the limit. Our results allow certain quantum states to be learned with a sample complexity \textit{exponentially} better than naive tomography. We hence provide new classes of efficiently learnable quantum states and apply new strategies to learn them.
</details>
<details>
<summary>摘要</summary>
完全学习一个 $n$-qubit量子状态需要样本数量呈指数函数关系于 $n$。一些作品考虑了一些量子状态的子集，可以在 polynomial 样本复杂性下学习，如稳定器状态或高温 Gibbs 状态。其他作品考虑了一种弱一种学习方式，如 PAC 学习和影子测试。在这项工作中，我们考虑了学习与神经网络状态相似的量子状态，可以有效地表示为受限 Boltzmann 机制（RBM）。为此，我们展示了二步邻居学习算法的Robustness 结果，包括 ferromagnetic 和本地一致 RBM。我们使用 $L_p$-norm 作为距离度量，包括总变分距离和最大 нор距离在限制中。我们的结果表示可以使用更好的样本复杂性学习一些量子状态，比Naive 测试更好。我们因此提供了新的有效地学习量子状态的类别，并应用新的策略来学习它们。
</details></li>
</ul>
<hr>
<h2 id="Double-Normalizing-Flows-Flexible-Bayesian-Gaussian-Process-ODEs-Learning"><a href="#Double-Normalizing-Flows-Flexible-Bayesian-Gaussian-Process-ODEs-Learning" class="headerlink" title="Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning"></a>Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09222">http://arxiv.org/abs/2309.09222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Xu, Shian Du, Junmei Yang, Xinghao Ding, John Paisley, Delu Zeng</li>
<li>for: 模型vector field continuous dynamical systems的bayesian inference</li>
<li>methods:  incorporate normalizing flows to reparameterize the vector field of ODEs, 使用normalizing flows进行 posterior inference</li>
<li>results: 提高了模型 uncertainty和精度 estimates, 在 simulate dynamical systems and real-world human motion data中得到了更好的结果<details>
<summary>Abstract</summary>
Recently, Gaussian processes have been utilized to model the vector field of continuous dynamical systems. Bayesian inference for such models \cite{hegde2022variational} has been extensively studied and has been applied in tasks such as time series prediction, providing uncertain estimates. However, previous Gaussian Process Ordinary Differential Equation (ODE) models may underperform on datasets with non-Gaussian process priors, as their constrained priors and mean-field posteriors may lack flexibility. To address this limitation, we incorporate normalizing flows to reparameterize the vector field of ODEs, resulting in a more flexible and expressive prior distribution. Additionally, due to the analytically tractable probability density functions of normalizing flows, we apply them to the posterior inference of GP ODEs, generating a non-Gaussian posterior. Through these dual applications of normalizing flows, our model improves accuracy and uncertainty estimates for Bayesian Gaussian Process ODEs. The effectiveness of our approach is demonstrated on simulated dynamical systems and real-world human motion data, including tasks such as time series prediction and missing data recovery. Experimental results indicate that our proposed method effectively captures model uncertainty while improving accuracy.
</details>
<details>
<summary>摘要</summary>
近期，Gaussian processes 被应用于连续动力系统的vector field模型中。Bayesian推理 для这些模型 \cite{hegde2022variational} 已经得到了广泛的研究，并在任务如时间序列预测中提供了不确定估计。然而，之前的Gaussian ProcessOrdinary Differential Equation（ODE）模型可能在非Gaussian process priors的数据集上表现不佳，因为它们的受限的先验和媒介质POSTerior可能缺乏灵活性。为了解决这个限制，我们将normalizing flows integration到了ODE的vector field中，从而获得了更灵活和表达力强的先验分布。此外，由于normalizing flows的概率密度函数是可微分的，我们可以将其应用到GP ODEs的后验推理中，生成一个非Gaussian posterior。通过这种双重应用normalizing flows，我们的模型可以提高Bayesian Gaussian Process ODEs的准确性和uncertainty估计。我们的方法在模拟动力系统和真实世界人类运动数据上进行了实验，包括时间序列预测和缺失数据恢复等任务，结果表明我们的提案方法可以有效地捕捉模型uncertainty，同时提高准确性。
</details></li>
</ul>
<hr>
<h2 id="MFRL-BI-Design-of-a-Model-free-Reinforcement-Learning-Process-Control-Scheme-by-Using-Bayesian-Inference"><a href="#MFRL-BI-Design-of-a-Model-free-Reinforcement-Learning-Process-Control-Scheme-by-Using-Bayesian-Inference" class="headerlink" title="MFRL-BI: Design of a Model-free Reinforcement Learning Process Control Scheme by Using Bayesian Inference"></a>MFRL-BI: Design of a Model-free Reinforcement Learning Process Control Scheme by Using Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09205">http://arxiv.org/abs/2309.09205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanrong Li, Juan Du, Wei Jiang</li>
<li>for: 本研究旨在提出一种基于模型自由强化学习（MFRL）的控制方案，以实时数据进行实验和控制优化，以适应实际生产系统中模型不准确的问题。</li>
<li>methods: 本研究使用了MFRL控制方案，通过bayesian推理更新干扰分布，以降低生产过程中干扰的大量变化。</li>
<li>results: 研究结果显示，提议的MFRL控制方案在无知过程模型情况下能够实现良好的控制性能，并且在数学性质上也得到了保证。计算研究也证明了我们的方法的有效性和效率。<details>
<summary>Abstract</summary>
Design of process control scheme is critical for quality assurance to reduce variations in manufacturing systems. Taking semiconductor manufacturing as an example, extensive literature focuses on control optimization based on certain process models (usually linear models), which are obtained by experiments before a manufacturing process starts. However, in real applications, pre-defined models may not be accurate, especially for a complex manufacturing system. To tackle model inaccuracy, we propose a model-free reinforcement learning (MFRL) approach to conduct experiments and optimize control simultaneously according to real-time data. Specifically, we design a novel MFRL control scheme by updating the distribution of disturbances using Bayesian inference to reduce their large variations during manufacturing processes. As a result, the proposed MFRL controller is demonstrated to perform well in a nonlinear chemical mechanical planarization (CMP) process when the process model is unknown. Theoretical properties are also guaranteed when disturbances are additive. The numerical studies also demonstrate the effectiveness and efficiency of our methodology.
</details>
<details>
<summary>摘要</summary>
制程控制方案的设计对制造系统质量保证具有关键性。以半导体制造为例，广泛的文献关注控制优化基于certain process models（通常是线性模型），这些模型通常通过实验 перед制造过程开始获得。然而，在实际应用中，预定义的模型可能不准确，特别是对于复杂的制造系统。为解决模型不准确的问题，我们提议使用无模型反馈学习（MFRL）方法来进行实验和控制优化同时，根据实时数据进行调整。 Specifically, we design a novel MFRL control scheme by updating the distribution of disturbances using Bayesian inference to reduce their large variations during manufacturing processes. As a result, the proposed MFRL controller is demonstrated to perform well in a nonlinear chemical mechanical planarization (CMP) process when the process model is unknown. 理论性质也得到保证，当干扰是加性的时候。 numerical studies also demonstrate the effectiveness and efficiency of our methodology.
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Optimized-Pipeline-for-Prediction-of-Protein-Folding-Kinetics"><a href="#End-to-End-Optimized-Pipeline-for-Prediction-of-Protein-Folding-Kinetics" class="headerlink" title="End-to-End Optimized Pipeline for Prediction of Protein Folding Kinetics"></a>End-to-End Optimized Pipeline for Prediction of Protein Folding Kinetics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09191">http://arxiv.org/abs/2309.09191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vijay Arvind. R, Haribharathi Sivakumar, Brindha. R</li>
<li>for: 预测蛋白质折叠动力学的高精度且低占用内存的算法 pipeline。</li>
<li>methods: 使用机器学习模型进行预测。</li>
<li>results: 比预先状态艺术模型高4.8%的准确率，并且占用内存327倍少和运行速度7.3%快。<details>
<summary>Abstract</summary>
Protein folding is the intricate process by which a linear sequence of amino acids self-assembles into a unique three-dimensional structure. Protein folding kinetics is the study of pathways and time-dependent mechanisms a protein undergoes when it folds. Understanding protein kinetics is essential as a protein needs to fold correctly for it to perform its biological functions optimally, and a misfolded protein can sometimes be contorted into shapes that are not ideal for a cellular environment giving rise to many degenerative, neuro-degenerative disorders and amyloid diseases. Monitoring at-risk individuals and detecting protein discrepancies in a protein's folding kinetics at the early stages could majorly result in public health benefits, as preventive measures can be taken. This research proposes an efficient pipeline for predicting protein folding kinetics with high accuracy and low memory footprint. The deployed machine learning (ML) model outperformed the state-of-the-art ML models by 4.8% in terms of accuracy while consuming 327x lesser memory and being 7.3% faster.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Driven-Reachability-Analysis-of-Stochastic-Dynamical-Systems-with-Conformal-Inference"><a href="#Data-Driven-Reachability-Analysis-of-Stochastic-Dynamical-Systems-with-Conformal-Inference" class="headerlink" title="Data-Driven Reachability Analysis of Stochastic Dynamical Systems with Conformal Inference"></a>Data-Driven Reachability Analysis of Stochastic Dynamical Systems with Conformal Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09187">http://arxiv.org/abs/2309.09187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Hashemi, Xin Qin, Lars Lindemann, Jyotirmoy V. Deshmukh</li>
<li>for: 本文针对数据驱动的抽象时间概率动力系统进行可达性分析，使用协Forms inference。</li>
<li>methods: 本文使用数据学习来建立一个代理模型，然后使用代理模型进行可达性分析，并使用协Forms inference来评估代理模型所受的误差。</li>
<li>results: 本文可以为learning-enabled控制系统提供可达性保证，并且可以处理复杂的closed-loop dynamics。<details>
<summary>Abstract</summary>
We consider data-driven reachability analysis of discrete-time stochastic dynamical systems using conformal inference. We assume that we are not provided with a symbolic representation of the stochastic system, but instead have access to a dataset of $K$-step trajectories. The reachability problem is to construct a probabilistic flowpipe such that the probability that a $K$-step trajectory can violate the bounds of the flowpipe does not exceed a user-specified failure probability threshold. The key ideas in this paper are: (1) to learn a surrogate predictor model from data, (2) to perform reachability analysis using the surrogate model, and (3) to quantify the surrogate model's incurred error using conformal inference in order to give probabilistic reachability guarantees. We focus on learning-enabled control systems with complex closed-loop dynamics that are difficult to model symbolically, but where state transition pairs can be queried, e.g., using a simulator. We demonstrate the applicability of our method on examples from the domain of learning-enabled cyber-physical systems.
</details>
<details>
<summary>摘要</summary>
我们考虑了数据驱动的可达性分析，用于离散时间渐进系统。我们假设我们没有符号表示法，而是有一个$K$-步轨迹数据集。我们的目标是构建一个流管，使得流管中的概率超过用户指定的失败概率阈值。我们的关键想法是：（1）从数据学习一个代理预测模型，（2）使用代理模型进行可达性分析，（3）使用凤凰推理来评估代理模型所吃进的误差，以提供可达性保证。我们关注learning-enabled控制系统，其中具有复杂的关闭环境，但可以通过 simulate 来查询状态转移对。我们在学习启发系统中的示例上进行了应用。
</details></li>
</ul>
<hr>
<h2 id="On-the-Connection-Between-Riemann-Hypothesis-and-a-Special-Class-of-Neural-Networks"><a href="#On-the-Connection-Between-Riemann-Hypothesis-and-a-Special-Class-of-Neural-Networks" class="headerlink" title="On the Connection Between Riemann Hypothesis and a Special Class of Neural Networks"></a>On the Connection Between Riemann Hypothesis and a Special Class of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09171">http://arxiv.org/abs/2309.09171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soufiane Hayou</li>
<li>for: 这份论文是关于里曼假设（RH）的一个检查和扩展。</li>
<li>methods: 论文使用了一种已知的分析条件，称为尼曼-贝尔灵 критерион，连接RH与一种特殊的神经网络最小化问题。</li>
<li>results: 论文提供了一种扩展的分析条件，以及一种新的方法来检查RH。<details>
<summary>Abstract</summary>
The Riemann hypothesis (RH) is a long-standing open problem in mathematics. It conjectures that non-trivial zeros of the zeta function all have real part equal to 1/2. The extent of the consequences of RH is far-reaching and touches a wide spectrum of topics including the distribution of prime numbers, the growth of arithmetic functions, the growth of Euler totient, etc. In this note, we revisit and extend an old analytic criterion of the RH known as the Nyman-Beurling criterion which connects the RH to a minimization problem that involves a special class of neural networks. This note is intended for an audience unfamiliar with RH. A gentle introduction to RH is provided.
</details>
<details>
<summary>摘要</summary>
里曼假设（RH）是数学中一个长期开放的问题。它假设非质数函数的非质数部分都是1/2的实数部分。这个假设的影响是广泛的，覆盖了许多数学领域，包括整数分布、算术函数的增长、欧拉 totient 函数的增长等。在这份notes中，我们重新访问和扩展了一个古老的分析 критерий，称为尼曼-欧拉 criterion，它将RH与一种特殊的神经网络相连接。这份notes是为那些不熟悉RH的读者而设计的。我们会提供一个温顺的引入，以便读者更好地了解RH。
</details></li>
</ul>
<hr>
<h2 id="Integration-of-geoelectric-and-geochemical-data-using-Self-Organizing-Maps-SOM-to-characterize-a-landfill"><a href="#Integration-of-geoelectric-and-geochemical-data-using-Self-Organizing-Maps-SOM-to-characterize-a-landfill" class="headerlink" title="Integration of geoelectric and geochemical data using Self-Organizing Maps (SOM) to characterize a landfill"></a>Integration of geoelectric and geochemical data using Self-Organizing Maps (SOM) to characterize a landfill</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09164">http://arxiv.org/abs/2309.09164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Camila Juliao, Johan Diaz, Yosmely BermÚdez, Milagrosa Aldana</li>
<li>For: 这个研究的目的是确定垃圾掩埋场周围区域是否存在潜在的污染风险，并通过不同方法来实现这一目的。* Methods: 本研究使用了地球电性资料（抗阻和IP）和表面甲烷测量数据，并使用了一个不supervised Neural Network（ Kohonen 型）来处理和分类这些数据。* Results: 研究结果显示，通过使用 Self-Organizing Classification Maps（SOM），可以实现精确地定义潜在污染风险区域，并将其分为不同的类别。两个图像出力被 obtiened 从训练过程中，每个图像都代表了不同的潜在污染风险区域。<details>
<summary>Abstract</summary>
Leachates from garbage dumps can significantly compromise their surrounding area. Even if the distance between these and the populated areas could be considerable, the risk of affecting the aquifers for public use is imminent in most cases. For this reason, the delimitation and monitoring of the leachate plume are of significant importance. Geoelectric data (resistivity and IP), and surface methane measurements, are integrated and classified using an unsupervised Neural Network to identify possible risk zones in areas surrounding a landfill. The Neural Network used is a Kohonen type, which generates; as a result, Self-Organizing Classification Maps or SOM (Self-Organizing Map). Two graphic outputs were obtained from the training performed in which groups of neurons that presented a similar behaviour were selected. Contour maps corresponding to the location of these groups and the individual variables were generated to compare the classification obtained and the different anomalies associated with each of these variables. Two of the groups resulting from the classification are related to typical values of liquids percolated in the landfill for the parameters evaluated individually. In this way, a precise delimitation of the affected areas in the studied landfill was obtained, integrating the input variables via SOMs. The location of the study area is not detailed for confidentiality reasons.
</details>
<details>
<summary>摘要</summary>
垃圾排泄物可以很大程度地对周围环境造成影响。即使垃圾排泄物和人口集中区之间的距离相对较远，但是影响公共饮水储存层的风险仍然很高。因此，垃圾排泄物泄洪和监测的重要性非常大。在这种情况下，利用不超级网络（Kohonen类）进行无监督学习，并将抵抗性和IP测量数据集成，以生成自组织分类地图（SOM）。在训练过程中，选择了表现相似的神经元组，并生成了对应的Contour地图，以比较不同变量之间的分类结果和异常相关性。两个组 resulting from the classification are related to typical liquid values percolated in the landfill for the parameters evaluated individually. In this way, a precise delimitation of the affected areas in the studied landfill was obtained, integrating the input variables via SOMs.  The location of the study area is not detailed for confidentiality reasons.
</details></li>
</ul>
<hr>
<h2 id="Total-Variation-Distance-Estimation-Is-as-Easy-as-Probabilistic-Inference"><a href="#Total-Variation-Distance-Estimation-Is-as-Easy-as-Probabilistic-Inference" class="headerlink" title="Total Variation Distance Estimation Is as Easy as Probabilistic Inference"></a>Total Variation Distance Estimation Is as Easy as Probabilistic Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09134">http://arxiv.org/abs/2309.09134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S. Meel, Dimitrios Myrisiotis, A. Pavan, N. V. Vinodchandran</li>
<li>for: 这个论文是关于total variation（TV）距离估计和概率推理之间的新连接。</li>
<li>methods: 这篇论文使用了一种有效、结构保持的减少方法，将相对的TV距离估计转化为概率推理 над指定的导航图模型中。</li>
<li>results: 这篇论文提出了一种基于Bayes网的FPRAS估计TV距离 между任意类型的分布，并且只需要有效的概率推理算法。此外，这种方法还可以用于估计高维分布的TV距离。<details>
<summary>Abstract</summary>
In this paper, we establish a novel connection between total variation (TV) distance estimation and probabilistic inference. In particular, we present an efficient, structure-preserving reduction from relative approximation of TV distance to probabilistic inference over directed graphical models. This reduction leads to a fully polynomial randomized approximation scheme (FPRAS) for estimating TV distances between distributions over any class of Bayes nets for which there is an efficient probabilistic inference algorithm. In particular, it leads to an FPRAS for estimating TV distances between distributions that are defined by Bayes nets of bounded treewidth. Prior to this work, such approximation schemes only existed for estimating TV distances between product distributions. Our approach employs a new notion of $partial$ couplings of high-dimensional distributions, which might be of independent interest.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们建立了一种新的连接，即全量变量（TV）距离估计和概率推理之间的连接。我们 Specifically, we present a structure-preserving reduction from relative approximation of TV distance to probabilistic inference over directed graphical models. This reduction leads to a fully polynomial randomized approximation scheme (FPRAS) for estimating TV distances between distributions over any class of Bayes nets for which there is an efficient probabilistic inference algorithm. In particular, it leads to an FPRAS for estimating TV distances between distributions that are defined by Bayes nets of bounded treewidth. Prior to this work, such approximation schemes only existed for estimating TV distances between product distributions. Our approach employs a new notion of $partial$ couplings of high-dimensional distributions, which might be of independent interest.Here's the translation in Traditional Chinese:在这篇论文中，我们建立了一种新的连接，即全量变量（TV）距离估计和概率推理之间的连接。我们 Specifically, we present a structure-preserving reduction from relative approximation of TV distance to probabilistic inference over directed graphical models. This reduction leads to a fully polynomial randomized approximation scheme (FPRAS) for estimating TV distances between distributions over any class of Bayes nets for which there is an efficient probabilistic inference algorithm. In particular, it leads to an FPRAS for estimating TV distances between distributions that are defined by Bayes nets of bounded treewidth. Prior to this work, such approximation schemes only existed for estimating TV distances between product distributions. Our approach employs a new notion of $partial$ couplings of high-dimensional distributions, which might be of independent interest.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.LG_2023_09_17/" data-id="clogxf3oi00of5xradltaedg6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/eess.SP_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T08:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/eess.SP_2023_09_17/">eess.SP - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Climate-Resilient-UAVs-Enhancing-Energy-Efficient-B5G-Communication-in-Harsh-Environments"><a href="#Climate-Resilient-UAVs-Enhancing-Energy-Efficient-B5G-Communication-in-Harsh-Environments" class="headerlink" title="Climate-Resilient UAVs: Enhancing Energy-Efficient B5G Communication in Harsh Environments"></a>Climate-Resilient UAVs: Enhancing Energy-Efficient B5G Communication in Harsh Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09387">http://arxiv.org/abs/2309.09387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdu Saif, Saeed Hamood Alsamhi, Edward Curry</li>
<li>for: 这篇论文探讨了无人机在超 fifth generation（B5G）通信网络中的重要作用，尤其是在雨、雾、雪等不利天气条件下。</li>
<li>methods: 这篇研究探讨了气候鲜度无人机和能效B5G通信之间的相互作用，并分析了各种天气元素对无人机覆盖和通信动态的影响。</li>
<li>results: 研究发现，气候鲜度无人机可以在不同的天气条件下提供更高的能效性、降低干扰、提高数据传输率，并且在不同的天气条件下可以获得最佳通道增强。<details>
<summary>Abstract</summary>
This paper explores the crucial role of Unmanned Aerial Vehicles (UAVs) in advancing Beyond Fifth Generation (B5G) communication networks, especially in adverse weather conditions like rain, fog, and snow.   The study investigates the synergy between climate-resilient UAVs and energy-efficient B5G communication.   Key findings include the impact of weather elements on UAV coverage and communication dynamics. The research demonstrates significant enhancements in energy efficiency, reduced interference, increased data transmission rates, and optimal channel gain under various weather conditions.   Overall, this paper emphasizes the potential of climate-resilient UAVs to improve energy-efficient B5G communication and highlights technology's role in mitigating climate change's impact on communication systems, promoting sustainability and resilience.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨无人飞行器（UAV）在 fifth Generation 以上通信网络中的重要作用，特别是在雨、雾和雪等不利天气 услови下。 研究发现了气候适应UAV和能效B5G通信之间的共同作用，以及不同天气条件下UAV覆盖和通信动态的影响。 研究显示在不同的天气条件下，气候适应UAV可以提高能效率，减少干扰，提高数据传输速率，并且在不同的天气条件下实现最佳通道增强。总的来说，这篇论文强调气候适应UAV在能效B5G通信中的潜在作用，并 highlights 技术在气候变化对通信系统的影响下的作用，推动可持续发展和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Domain-Detection-for-Molecular-Communication-with-Cross-Reactive-Receptors"><a href="#Frequency-Domain-Detection-for-Molecular-Communication-with-Cross-Reactive-Receptors" class="headerlink" title="Frequency-Domain Detection for Molecular Communication with Cross-Reactive Receptors"></a>Frequency-Domain Detection for Molecular Communication with Cross-Reactive Receptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09377">http://arxiv.org/abs/2309.09377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meltem Civas, Murat Kuscu, Ozgur B. Akan</li>
<li>for: This paper is written for the development of a frequency-domain detection (FDD) technique for bioFET-based molecular communication receivers (MC-Rxs) to overcome molecular cross-talk in the time domain.</li>
<li>methods: The paper proposes the use of a frequency-domain detection technique that exploits the difference in binding reaction rates of different ligand types reflected in the power spectrum of the ligand-receptor binding noise to decode transmitted concentration signals.</li>
<li>results: The paper demonstrates the effectiveness of the proposed FDD technique in decoding transmitted concentration signals under stochastic molecular interference compared to a widely used time-domain detection (TDD) technique, and verifies the analytical performance bounds of the FDD through a particle-based spatial stochastic simulator simulating reactions on the MC-Rx in microfluidic channels.<details>
<summary>Abstract</summary>
Molecular Communications (MC) is a bio-inspired communication paradigm that uses molecules as information carriers, requiring unconventional transceivers and modulation/detection techniques. Practical MC receivers (MC-Rxs) can be implemented using field-effect transistor biosensor (bioFET) architectures, where surface receptors reversibly react with ligands. The time-varying concentration of ligand-bound receptors is translated into electrical signals via field effect, which is used to decode the transmitted information. However, ligand-receptor interactions do not provide an ideal molecular selectivity, as similar ligand types, i.e., interferers, co-existing in the MC channel, can interact with the same type of receptors. Overcoming this molecular cross-talk in the time domain can be challenging, especially when Rx has no knowledge of the interferer statistics or operates near saturation. Therefore, we propose a frequency-domain detection (FDD) technique for bioFET-based MC-Rxs that exploits the difference in binding reaction rates of different ligand types reflected in the power spectrum of the ligand-receptor binding noise. We derive the bit error probability (BEP) of the FDD technique and demonstrate its effectiveness in decoding transmitted concentration signals under stochastic molecular interference compared to a widely used time-domain detection (TDD) technique. We then verified the analytical performance bounds of the FDD through a particle-based spatial stochastic simulator simulating reactions on the MC-Rx in microfluidic channels.
</details>
<details>
<summary>摘要</summary>
молекулярcommunications（MC）是一种生物体注意的通信模式，使用分子作为信息传递者，需要不同寻常的接收器和模ulation/探测技术。实际的MC接收器（MC-Rx）可以通过场效 транзистор生物感应（bioFET）建筑实现，其表面受体逆转受体与抗体结合。时间变化的抗体结合的分子浓度通过场效转换成电学信号，用于解码传输的信息。但是，抗体-受体交互不提供理想的分子选择性，因为同类抗体在MC通道中可能与同类受体结合。在时间频谱中解决这种分子交叉通信可以是挑战，特别是当接收器没有知道干扰者统计或在满载状态下操作时。因此，我们提出了频率域检测（FDD）技术，该技术利用不同抗体类型在绑定反应速率上的差异，反映在抗体-受体绑定噪声的能量спектrum中。我们计算了FDD技术的比特错误概率（BEP），并证明其在对抗杂噪声的情况下比时频域检测（TDD）技术更有效地解码传输的浓度信号。我们然后通过使用粒子基的空间随机仿真器模拟MC-Rx在微 fluidic通道中的反应，验证了我们的分析性能下限。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Estimation-Using-Complex-Valued-Shifted-Window-Transformer"><a href="#Frequency-Estimation-Using-Complex-Valued-Shifted-Window-Transformer" class="headerlink" title="Frequency Estimation Using Complex-Valued Shifted Window Transformer"></a>Frequency Estimation Using Complex-Valued Shifted Window Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09352">http://arxiv.org/abs/2309.09352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/josiahwsmith10/spectral-super-resolution-swin">https://github.com/josiahwsmith10/spectral-super-resolution-swin</a></li>
<li>paper_authors: Josiah W. Smith, Murat Torlak</li>
<li>for: 本研究targets at estimating closely spaced frequency components of a signal, which is a fundamental problem in statistical signal processing.</li>
<li>methods: 本文提出了一种基于Swin transformer的新方法，包括1-D real-valued和复数值Shifted window transformer（SwinFreq和CVSwinFreq），用于1-D复数值信号的线spectra频率估计。</li>
<li>results: 对比传统的Periodogram、MUSIC和OMP算法以及state-of-the-art的deep learning方法cResFreq，SwinFreq和CVSwinFreq具有更高的性能、更好的分辨率和更少的模型参数，因此更适合边缘和移动应用。此外， authors发现了real-valued Swin-Freq在某些任务上表现更好，而且具有较小的模型大小。最后， authors应用了提posed方法于实际雷达范profile超分辨率 task中，实验结果 validate了SwinFreq和CVSwinFreq的数值和实验上的优越性。<details>
<summary>Abstract</summary>
Estimating closely spaced frequency components of a signal is a fundamental problem in statistical signal processing. In this letter, we introduce 1-D real-valued and complex-valued shifted window (Swin) transformers, referred to as SwinFreq and CVSwinFreq, respectively, for line-spectra frequency estimation on 1-D complex-valued signals. Whereas 2-D Swin transformer-based models have gained traction for optical image super-resolution, we introduce for the first time a complex-valued Swin module designed to leverage the complex-valued nature of signals for a wide array of applications. The proposed approach overcomes the limitations of the classical algorithms such as the periodogram, MUSIC, and OMP in addition to state-of-the-art deep learning approach cResFreq. SwinFreq and CVSwinFreq boast superior performance at low signal-to-noise ratio SNR and improved resolution capability while requiring fewer model parameters than cResFreq, thus deeming it more suitable for edge and mobile applications. We find that the real-valued Swin-Freq outperforms its complex-valued counterpart CVSwinFreq for several tasks while touting a smaller model size. Finally, we apply the proposed techniques for radar range profile super-resolution using real data. The results from both synthetic and real experimentation validate the numerical and empirical superiority of SwinFreq and CVSwinFreq to the state-of-the-art deep learning-based techniques and traditional frequency estimation algorithms. The code and models are publicly available at https://github.com/josiahwsmith10/spectral-super-resolution-swin.
</details>
<details>
<summary>摘要</summary>
估计 closely spaced frequency component of a signal 是统计信号处理中的基本问题。在这封信件中，我们介绍了1维实数值和复数值偏移窗变换器（Swin），称为SwinFreq和CVSwinFreq，用于1维复数值信号的线pectra频率估计。而2维Swin transformer-based模型在光学超分解中获得了进步，我们是第一次引入一种用于信号的复数值Swin模块，用以利用信号的复数值特性，并且可以应用于各种应用程序。我们的方法可以超越经典算法，如期ogram、MUSIC和OMP，以及当前的深度学习方法cResFreq。SwinFreq和CVSwinFreq具有低SNR和提高分辨率的优势，同时需要 fewer model parameter than cResFreq，因此适用于边缘和移动应用。我们发现实数值Swin-Freq在一些任务上表现出色，而且具有较小的模型大小。最后，我们应用了提议的技术于雷达距离Profile超分解中使用实际数据。实验结果证明了SwinFreq和CVSwinFreq的数学和实验准确性，并超越了当前的深度学习基于技术和经典频率估计算法。代码和模型可以在https://github.com/josiahwsmith10/spectral-super-resolution-swin上获得。
</details></li>
</ul>
<hr>
<h2 id="Asymptotic-Analysis-of-the-Downlink-in-Cooperative-Massive-MIMO-Systems"><a href="#Asymptotic-Analysis-of-the-Downlink-in-Cooperative-Massive-MIMO-Systems" class="headerlink" title="Asymptotic Analysis of the Downlink in Cooperative Massive MIMO Systems"></a>Asymptotic Analysis of the Downlink in Cooperative Massive MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09273">http://arxiv.org/abs/2309.09273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itsik Bergel, Siddhartan Govindasamy</li>
<li>For: The paper is written for a cooperative cellular communications system, where multiple base stations around each mobile station cooperate to reduce interference and improve system performance.* Methods: The paper uses closed-form expressions to derive the asymptotic performance of the network as the number of antennas per base station increases, and includes Monte Carlo simulations to verify the results. The paper also proposes a power allocation algorithm that achieves near-optimal performance with reduced coordination overhead between base stations.* Results: The paper shows that the asymptotic results capture the trade-off between various system parameters, and characterize the joint effect of noise and interference. The results are useful even when the number of antennas per base station is only moderately large, and the proposed power allocation algorithm achieves near-optimal performance with reduced coordination overhead.Here is the format you requested for the simplified Chinese text:* For: 这篇论文是关于协同通信系统的下行频率调制，多个基站周围每个移动站协同进行零干扰，以降低接收到移动站的干扰。* Methods: 论文使用closed-form表达式来 deriv asymptotic性能表达式，并通过 Monte Carlo仿真来验证结果。论文还提出了一种功率分配算法，可以在基站之间减少协调开销。* Results: 论文显示， asymptotic结果 capture了系统参数之间的贸易off，并characterize了干扰和噪声的共同效应。结果是用于 moderately large antenna数per base station，并且提出的功率分配算法可以实现 near-optimal性能，同时减少协调开销。<details>
<summary>Abstract</summary>
We consider the downlink of a cooperative cellular communications system, where several base-stations around each mobile cooperate and perform zero-forcing to reduce the received interference at the mobile. We derive closed-form expressions for the asymptotic performance of the network as the number of antennas per base station grows large. These expressions capture the trade off between various system parameters, and characterize the joint effect of noise and interference (where either noise or interference is asymptotically dominant and where both are asymptotically relevant). The asymptotic results are verified using Monte Carlo simulations, which indicate that they are useful even when the number of antennas per base station is only moderately large. Additionally, we show that when the number of antennas per base station grows large, power allocation can be optimized locally at each base station. We hence present a power allocation algorithm that achieves near optimal performance while significantly reducing the coordination overhead between base stations. The presented analysis is significantly more challenging than the uplink analysis, due to the dependence between beamforming vectors of nearby base stations. This statistical dependence is handled by introducing novel bounds on marked shot-noise point processes with dependent marks, which are also useful in other contexts.
</details>
<details>
<summary>摘要</summary>
我团队考虑了一个合作的移动通信系统的下链，其中各个基站附近的移动站合作，并通过零干扰来减少移动站接收的干扰。我们 deriv了大面积表达式，用于描述系统的极限性能，这些表达式捕捉了系统参数之间的贸易OFF和干扰的共同作用，以及干扰和噪声之间的交互作用。我们使用Monte Carlo仿真来验证我们的结论，并发现这些结论在只有 moderately 大的antenna数时仍然有用。此外，我们还证明了当antenna数量增大时，每个基站可以地方Optimize its power allocation，以达到近似优化性能的目的，同时减少基站之间协调 overhead。在这个分析中，我们面临的挑战在于附近基站的扩散波形矩阵之间的统计依赖关系。我们使用了新的 bounds on marked shot-noise point processes with dependent marks来处理这种统计依赖关系。这些bounds也可以在其他 contexts 中使用。
</details></li>
</ul>
<hr>
<h2 id="Toward-Beamfocusing-Aided-Near-Field-Communications-Research-Advances-Potential-and-Challenges"><a href="#Toward-Beamfocusing-Aided-Near-Field-Communications-Research-Advances-Potential-and-Challenges" class="headerlink" title="Toward Beamfocusing-Aided Near-Field Communications: Research Advances, Potential, and Challenges"></a>Toward Beamfocusing-Aided Near-Field Communications: Research Advances, Potential, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09242">http://arxiv.org/abs/2309.09242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancheng An, Chau Yuen, Linglong Dai, Marco Di Renzo, Merouane Debbah, Lajos Hanzo</li>
<li>for: 这篇论文旨在探讨未来无线通信技术的发展，尤其是EXTREMELY大规模天线阵列（ELAA）和tera响communications（NFC）的潜在应用。</li>
<li>methods: 论文使用基于圆柱形波front的模型来准确描述近场无线传播通道特性。同时，论文还提出了NFC与传统远场通信的比较，并评估了NFC的频率响应和硬件设计等挑战。</li>
<li>results: 数值结果表明NFC可以提高空间多重化增量和位置准确性。此外，论文还提出了一些未来研究的开问，以便进一步探索NFC的潜在应用和发展。<details>
<summary>Abstract</summary>
Next-generation mobile networks promise to support high throughput, massive connectivity, and improved energy efficiency. To achieve these ambitious goals, extremely large-scale antenna arrays (ELAAs) and terahertz communications constitute a pair of promising technologies. This will result in future wireless communications occurring in the near-field regions. To accurately portray the channel characteristics of near-field wireless propagation, spherical wavefront-based models are required and present both opportunities as well as challenges. Following the basics of near-field communications (NFC), we contrast it to conventional far-field communications. Moreover, we cover the key challenges of NFC, including its channel modeling and estimation, near-field beamfocusing, as well as hardware design. Our numerical results demonstrate the potential of NFC in improving the spatial multiplexing gain and positioning accuracy. Finally, a suite of open issues are identified for motivating future research.
</details>
<details>
<summary>摘要</summary>
Next-generation mobile networks 将支持高速、大量连接和改善能源效率。为实现这些目标，极大规模天线阵列（ELAAs）和teraHz通信技术是两种承诺技术。这将导致未来无线通信发生在近场区域。为准确描述近场无线媒体特性，球形波front基于模型是必需的，并提供了机会和挑战。根据近场通信（NFC）的基本原理，我们对它与传统远场通信进行了比较。此外，我们还讨论了NFC的主要挑战，包括通道模型化和估计、近场焦点定向以及硬件设计。我们的数字结果表明NFC可以提高空间复用增量和位置准确性。最后，我们确定了一些未解决的问题，以便激励未来的研究。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Cramer-Rao-Bound-Optimization-for-Active-RIS-Empowered-ISAC-Systems"><a href="#Cramer-Rao-Bound-Optimization-for-Active-RIS-Empowered-ISAC-Systems" class="headerlink" title="Cramer-Rao Bound Optimization for Active RIS-Empowered ISAC Systems"></a>Cramer-Rao Bound Optimization for Active RIS-Empowered ISAC Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09207">http://arxiv.org/abs/2309.09207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Zhu, Ming Li, Rang Liu, Qian Liu</li>
<li>for: 这篇论文的目的是探讨使用活动智能表面（RIS）强化探测信号质量和通信性能的Integrated sensing and communication（ISAC）系统。</li>
<li>methods: 论文使用了基站传输预编码和活动RIS反射扩散 beamforming的共同设计来优化参数估算性能，并提出了一种高效的算法基于块坐标降解（BCD）、半definite relaxation（SDR）和主导化最小化（MM）来解决非核心问题。</li>
<li>results:  simulation results validate the effectiveness of the developed algorithm and the potential of employing active RIS in ISAC systems to enhance direct-of-arrival（DoA）估算性能。<details>
<summary>Abstract</summary>
Integrated sensing and communication (ISAC), which simultaneously performs sensing and communication functions using the same frequency band and hardware platform, has emerged as a promising technology for future wireless systems. However, the weak echo signal received by the low-sensitivity ISAC receiver severely limits the sensing performance. Active reconfigurable intelligent surface (RIS) has become a prospective solution by situationally manipulating the wireless propagations and amplifying the signals. In this paper, we investigate the deployment of active RIS-empowered ISAC systems to enhance radar echo signal quality as well as communication performance. In particular, we focus on the joint design of the base station (BS) transmit precoding and the active RIS reflection beamforming to optimize the parameter estimation performance in terms of Cramer-Rao bound (CRB) subject to the service users' signal-to-interference-plus-noise ratio (SINR) requirements. An efficient algorithm based on block coordinate descent (BCD), semidefinite relaxation (SDR), and majorization-minimization (MM) is proposed to solve the formulated challenging non-convex problem. Finally, simulation results validate the effectiveness of the developed algorithm and the potential of employing active RIS in ISAC systems to enhance direct-of-arrival (DoA) estimation performance.
</details>
<details>
<summary>摘要</summary>
Integrated sensing and communication (ISAC)技术，它同时执行感知和通信功能，使用同一个频率带和硬件平台，已经成为未来无线系统的一种优秀技术。然而，低敏感度ISAC接收器接收到的弱回声信号，严重限制了感知性能。活动可重配置表面（RIS）已成为一种可能的解决方案，通过 Situationally manipulating wireless propagation和增强信号。在这篇论文中，我们 investigate了活动RIS-empowered ISAC系统的部署，以提高雷达回声信号质量以及通信性能。具体来说，我们关注了基站（BS）传输 precoding 和活动RIS反射扩散的共同设计，以优化参数估计性能，并达到服务用户的信号干扰plus noise ratio（SINR）要求。我们提出了一种高效的算法，基于块坐标降解（BCD）、凸relaxation（SDR）和主要化-最小化（MM）来解决复杂非对称问题。最后，我们的实验结果证明了我们提出的算法的有效性，以及活动RIS在ISAC系统中的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="NOMA-Based-Coexistence-of-Near-Field-and-Far-Field-Massive-MIMO-Communications"><a href="#NOMA-Based-Coexistence-of-Near-Field-and-Far-Field-Massive-MIMO-Communications" class="headerlink" title="NOMA-Based Coexistence of Near-Field and Far-Field Massive MIMO Communications"></a>NOMA-Based Coexistence of Near-Field and Far-Field Massive MIMO Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09185">http://arxiv.org/abs/2309.09185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiguo Ding, Robert Schober, H. Vincent Poor</li>
<li>for: 提供了一种使用NOMA原理来支持传统近场用户的合作，以提高大量MIMO网络的性能。</li>
<li>methods: 使用预先配置的近场用户的空间束来服务更多的远场用户，并通过增加基站antenna数来提高NOMA-assisted大量MIMO网络的性能。</li>
<li>results: 研究结果表明，通过NOMA原理可以有效地支持近场和远场通信的合作，并且可以通过增加基站antenna数来提高NOMA-assisted大量MIMO网络的性能。<details>
<summary>Abstract</summary>
This letter considers a legacy massive multiple-input multiple-output (MIMO) network, in which spatial beams have been preconfigured for near-field users, and proposes to use the non-orthogonal multiple access (NOMA) principle to serve additional far-field users by exploiting the spatial beams preconfigured for the legacy near-field users. Our results reveal that the coexistence between near-field and far-field communications can be effectively supported via NOMA, and that the performance of NOMA-assisted massive MIMO can be efficiently improved by increasing the number of antennas at the base station.
</details>
<details>
<summary>摘要</summary>
这封信件考虑了一个传统的大规模多输入多输出（MIMO）网络，在near-field用户中预先配置了空间扩散，并提议使用非对称多access（NOMA）原则来为更远的far-field用户提供服务，利用预先配置的near-field用户的空间扩散。我们的结果表明，near-field和far-field通信的共存可以通过NOMA进行有效支持，并且通过提高基站antenna数量来提高NOMA-assisted大规模MIMO的性能。Note that Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Throughput-Analysis-of-IEEE-802-11bn-Coordinated-Spatial-Reuse"><a href="#Throughput-Analysis-of-IEEE-802-11bn-Coordinated-Spatial-Reuse" class="headerlink" title="Throughput Analysis of IEEE 802.11bn Coordinated Spatial Reuse"></a>Throughput Analysis of IEEE 802.11bn Coordinated Spatial Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09169">http://arxiv.org/abs/2309.09169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesc Wilhelmi, Lorenzo Galati-Giordano, Giovanni Geraci, Boris Bellalta, Gianluca Fontanesi, David Nuñez</li>
<li>for:  This paper focuses on the Coordinated Spatial Reuse (C-SR) feature of the Multi-Access Point Coordination (MAPC) in the IEEE 802.11bn amendment (Wi-Fi 8).</li>
<li>methods:  The authors use an analytical model based on Continuous Time Markov Chains (CTMCs) to characterize the throughput and spatial efficiency of C-SR.</li>
<li>results:  The authors show that C-SR can opportunistically enable parallel high-quality transmissions and achieve an average throughput gain of up to 59% compared to the legacy 802.11 Distributed Coordination Function (DCF) and up to 42% compared to the 802.11ax Overlapping Basic Service Set Packet Detect (OBSS&#x2F;PD) mechanism.Here is the result in Simplified Chinese text:</li>
<li>for: 这篇论文关注了802.11bn规定（Wi-Fi 8）中的多个访问点协调（MAPC）功能之一——协调空间重用（C-SR）。</li>
<li>methods: 作者们使用基于 kontinuous Time Markov Chains（CTMCs）的分析模型来描述C-SR的吞吐量和空间效率。</li>
<li>results: 作者们表明，C-SR可以机会地实现并发高质量传输，并实现与legacy 802.11分布协调函数（DCF）和802.11ax Overlapping Basic Service Set Packet Detect（OBSS&#x2F;PD）机制相比的吞吐量提高，最高达59%。<details>
<summary>Abstract</summary>
Multi-Access Point Coordination (MAPC) is becoming the cornerstone of the IEEE 802.11bn amendment, alias Wi-Fi 8. Among the MAPC features, Coordinated Spatial Reuse (C-SR) stands as one of the most appealing due to its capability to orchestrate simultaneous access point transmissions at a low implementation complexity. In this paper, we contribute to the understanding of C-SR by introducing an analytical model based on Continuous Time Markov Chains (CTMCs) to characterize its throughput and spatial efficiency. Applying the proposed model to several network topologies, we show that C-SR opportunistically enables parallel high-quality transmissions and yields an average throughput gain of up to 59% in comparison to the legacy 802.11 Distributed Coordination Function (DCF) and up to 42% when compared to the 802.11ax Overlapping Basic Service Set Packet Detect (OBSS/PD) mechanism.
</details>
<details>
<summary>摘要</summary>
多点存取协调（MAP）正成为IEEE 802.11bn 修订（即Wi-Fi 8）的核心。其中，协调空间重复（C-SR）是最吸引人的功能之一，因为它可以实现低实现 Complexity 下的同时多点变数通信。在本文中，我们对C-SR进行了分析，并基于状态空间过程（CTMC）引入了一个分析模型，以描述它的吞吐率和空间效率。我们将这个模型应用到了多个网络架构上，结果显示，C-SR可以允许高质量的平行传输，并产生了与传统802.11 DCF和802.11ax OBSS/PD Mechanism 相比的吞吐率增加，具体是59%。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Code-Multiple-Access-SCMA-Technique"><a href="#Sparse-Code-Multiple-Access-SCMA-Technique" class="headerlink" title="Sparse Code Multiple Access (SCMA) Technique"></a>Sparse Code Multiple Access (SCMA) Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09127">http://arxiv.org/abs/2309.09127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjeev Sharma, Kuntal Deka</li>
<li>For: This paper is written to introduce and analyze the code domain-based sparse code multiple access (SCMA) non-orthogonal multiple access (NOMA) scheme to enhance the spectral efficiency of wireless networks.* Methods: The paper uses code domain-based SCMA, which is designed and detected using a hybrid multiple access scheme that combines code-domain and power-domain NOMA. The paper also discusses the method for codebooks design and its impact on system performance.* Results: The paper includes simulation results to show the impact of various SCMA system parameters, such as the number of users, the spreading factor, and the code length, on the system performance. The results demonstrate the potential of SCMA to enhance the spectral efficiency of wireless networks.<details>
<summary>Abstract</summary>
Next-generation wireless networks require higher spectral efficiency and lower latency to meet the demands of various upcoming applications. Recently, non-orthogonal multiple access (NOMA) schemes are introduced in the literature for 5G and beyond. Various forms of NOMA are considered like power domain, code domain, pattern division multiple access, etc. to enhance the spectral efficiency of wireless networks. In this chapter, we introduce the code domain-based sparse code multiple access (SCMA) NOMA scheme to enhance the spectral efficiency of a wireless network. The design and detection of an SCMA system are analyzed in this chapter. Also, the method for codebooks design and its impact on system performance are highlighted. A hybrid multiple access scheme is also introduced using both code-domain and power-domain NOMA. Furthermore, simulation results are included to show the impact of various SCMA system parameters.ext-generation wireless networks require higher spectral efficiency and lower latency to meet the demands of various upcoming applications. Recently, non-orthogonal multiple access (NOMA) schemes are introduced in the literature for 5G and beyond. Various forms of NOMA are considered like power domain, code domain, pattern division multiple access, etc. to enhance the spectral efficiency of wireless networks. In this chapter, we introduce the code domainbased sparse code multiple access (SCMA) NOMA scheme to enhance the spectral efficiency of a wireless network. The design and detection of an SCMA system are analyzed in this chapter. Also, the method for codebooks design and its impact on system performance are highlighted. A hybrid multiple access scheme is also introduced using both code-domain and power-domain NOMA. Furthermore, simulation results are included to show the impact of various SCMA system parameters.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/eess.SP_2023_09_17/" data-id="clogxf3tk017q5xra4f8c6m4x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/16/cs.SD_2023_09_16/" class="article-date">
  <time datetime="2023-09-16T15:00:00.000Z" itemprop="datePublished">2023-09-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/16/cs.SD_2023_09_16/">cs.SD - 2023-09-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-GAN-Based-Vocoders-with-Contrastive-Learning-Under-Data-limited-Condition"><a href="#Enhancing-GAN-Based-Vocoders-with-Contrastive-Learning-Under-Data-limited-Condition" class="headerlink" title="Enhancing GAN-Based Vocoders with Contrastive Learning Under Data-limited Condition"></a>Enhancing GAN-Based Vocoders with Contrastive Learning Under Data-limited Condition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09088">http://arxiv.org/abs/2309.09088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoming Guo, Seth Z. Zhao, Jiachen Lian, Gopala Anumanchipalli, Gerald Friedland</li>
<li>for: 本研究旨在提高 vocoder 模型在数据有限情况下的质量，不修改模型结构或添加更多数据。</li>
<li>methods: 本研究使用了对mel-spectrogram进行对比学习，以提高 vocoder 模型的语音质量。此外， authors 还尝试了在多Modal情况下使用waveform进行学习，以解决权值逐出问题。</li>
<li>results: 研究结果表明，通过对 vocoder 模型进行对比学习，可以在数据有限情况下提高模型性能，并且分析结果表明，提posed方法可以successfully解决权值逐出问题，并生成高质量的语音。<details>
<summary>Abstract</summary>
Vocoder models have recently achieved substantial progress in generating authentic audio comparable to human quality while significantly reducing memory requirement and inference time. However, these data-hungry generative models require large-scale audio data for learning good representations. In this paper, we apply contrastive learning methods in training the vocoder to improve the perceptual quality of the vocoder without modifying its architecture or adding more data. We design an auxiliary task with mel-spectrogram contrastive learning to enhance the utterance-level quality of the vocoder model under data-limited conditions. We also extend the task to include waveforms to improve the multi-modality comprehension of the model and address the discriminator overfitting problem. We optimize the additional task simultaneously with GAN training objectives. Our result shows that the tasks improve model performance substantially in data-limited settings. Our analysis based on the result indicates that the proposed design successfully alleviates discriminator overfitting and produces audio of higher fidelity.
</details>
<details>
<summary>摘要</summary>
很多最新的 vocoder 模型已经取得了很大的进步，可以生成比人类质量更高的真实音频，同时减少了内存需求和计算时间。然而，这些数据夹带的生成模型需要大量的音频数据来学习良好的表示。在这篇论文中，我们使用了对比学习方法来在训练 vocoder 模型中提高模型的感知质量，无需修改模型结构或添加更多的数据。我们设计了一项 auxiliary 任务，通过 mel-spectrogram 对比学习来提高 vocoder 模型在数据有限的情况下的话语质量。我们还将这项任务扩展到包括波形，以提高模型的多模式理解和解决探测器过拟合问题。我们同时优化了这些额外任务和 GAN 训练目标。我们的结果表明，这些任务可以在数据有限情况下提高模型性能的极大程度。我们的分析表明，我们的设计成功解决了探测器过拟合问题，并生成了更高的准确性和音频质量。
</details></li>
</ul>
<hr>
<h2 id="SynthTab-Leveraging-Synthesized-Data-for-Guitar-Tablature-Transcription"><a href="#SynthTab-Leveraging-Synthesized-Data-for-Guitar-Tablature-Transcription" class="headerlink" title="SynthTab: Leveraging Synthesized Data for Guitar Tablature Transcription"></a>SynthTab: Leveraging Synthesized Data for Guitar Tablature Transcription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09085">http://arxiv.org/abs/2309.09085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyi Zang, Yi Zhong, Frank Cwitkowitz, Zhiyao Duan</li>
<li>for: 这篇论文的目的是提高电子琴 Tablature Transcription (GTT) 模型的准确性和通用性，以应对现有的数据集规模和范围有限，导致现有的 GTT 模型容易过滤和没有通用性。</li>
<li>methods: 作者采用了多个商业电子琴和普通琴插件来生成 SynthTab，一个大规模的电子琴 Tablature Transcription 数据集。这个数据集是基于 DadaGP 提供的广泛的 Tablature 集，并且具有丰富的特征和技巧。</li>
<li>results: 实验显示，将先进 GTT 模型在 SynthTab 上进行预训后，可以提高同数据集的准确性，并且在跨数据集评估中具有较好的适应性和减少了过滤问题。<details>
<summary>Abstract</summary>
Guitar tablature is a form of music notation widely used among guitarists. It captures not only the musical content of a piece, but also its implementation and ornamentation on the instrument. Guitar Tablature Transcription (GTT) is an important task with broad applications in music education and entertainment. Existing datasets are limited in size and scope, causing state-of-the-art GTT models trained on such datasets to suffer from overfitting and to fail in generalization across datasets. To address this issue, we developed a methodology for synthesizing SynthTab, a large-scale guitar tablature transcription dataset using multiple commercial acoustic and electric guitar plugins. This dataset is built on tablatures from DadaGP, which offers a vast collection and the degree of specificity we wish to transcribe. The proposed synthesis pipeline produces audio which faithfully adheres to the original fingerings, styles, and techniques specified in the tablature with diverse timbre. Experiments show that pre-training state-of-the-art GTT model on SynthTab improves transcription accuracy in same-dataset tests. More importantly, it significantly mitigates overfitting problems of GTT models in cross-dataset evaluation.
</details>
<details>
<summary>摘要</summary>
吉他标谱是一种广泛用于吉他演奏的音乐notation，不仅记录了音乐内容，还包括 instru Meyer 和ornamentation。吉他标谱转写（GTT）是一个重要的任务，有广泛的应用在音乐教育和娱乐领域。现有的数据集 limitation 的size和scope，导致现有的GTT模型在这些数据集上进行训练后会出现过拟合和泛化问题。为解决这个问题，我们开发了一种方法ологи для生成 SynthTab，一个大规模的吉他标谱转写数据集，使用多种商业钢琴和电吉他插件。这个数据集基于DadaGP提供的大量标谱，我们可以根据我们的要求进行特定的转写。我们的合成管道可以生成具有多样 timbre 的音频，忠实地实现原始的手套、风格和技巧 specified in the tablature。实验表明，在 SynthTab 上先行训练 state-of-the-art GTT 模型可以提高同一个数据集的转写精度。更重要的是，它可以减轻 GTT 模型在不同数据集之间的过拟合问题。
</details></li>
</ul>
<hr>
<h2 id="Music-Generation-based-on-Generative-Adversarial-Networks-with-Transformer"><a href="#Music-Generation-based-on-Generative-Adversarial-Networks-with-Transformer" class="headerlink" title="Music Generation based on Generative Adversarial Networks with Transformer"></a>Music Generation based on Generative Adversarial Networks with Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09075">http://arxiv.org/abs/2309.09075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Jiang, Yi Zhong, Ruoxue Wu, Zhenghan Chen, Xiaoxuan Liang</li>
<li>for: 本研究旨在提高基于Transformers的自动生成音乐作品的质量，并且减少曝光偏见的影响。</li>
<li>methods: 我们使用了一种基于GAN框架的敌方损失函数，并使用了一个预训练的Span-BERT模型作为推论器。我们还使用了Gumbel-Softmax trick来实现整数序列的可微分化。</li>
<li>results: 我们通过人工评估和引入一种新的探测指标，证明了我们的方法比基于likelihood最大化的基eline模型具有更高的质量。<details>
<summary>Abstract</summary>
Autoregressive models based on Transformers have become the prevailing approach for generating music compositions that exhibit comprehensive musical structure. These models are typically trained by minimizing the negative log-likelihood (NLL) of the observed sequence in an autoregressive manner. However, when generating long sequences, the quality of samples from these models tends to significantly deteriorate due to exposure bias. To address this issue, we leverage classifiers trained to differentiate between real and sampled sequences to identify these failures. This observation motivates our exploration of adversarial losses as a complement to the NLL objective. We employ a pre-trained Span-BERT model as the discriminator in the Generative Adversarial Network (GAN) framework, which enhances training stability in our experiments. To optimize discrete sequences within the GAN framework, we utilize the Gumbel-Softmax trick to obtain a differentiable approximation of the sampling process. Additionally, we partition the sequences into smaller chunks to ensure that memory constraints are met. Through human evaluations and the introduction of a novel discriminative metric, we demonstrate that our approach outperforms a baseline model trained solely on likelihood maximization.
</details>
<details>
<summary>摘要</summary>
自适应模型基于Transformers已成为生成具有完整音乐结构的乐曲主要方法。这些模型通常通过逐步式拟合方式进行训练，以最小化负对数梯度（NLL）为目标。然而，在生成长序列时，这些模型的样本质量往往会受到曝光偏见的影响，导致样本质量下降。为 Addressing this issue, we leveraged classifiers trained to distinguish between real and sampled sequences to identify these failures. This observation motivates our exploration of adversarial losses as a complement to the NLL objective. We employed a pre-trained Span-BERT model as the discriminator in the Generative Adversarial Network (GAN) framework, which enhances training stability in our experiments. To optimize discrete sequences within the GAN framework, we utilized the Gumbel-Softmax trick to obtain a differentiable approximation of the sampling process. Additionally, we partitioned the sequences into smaller chunks to ensure that memory constraints were met. Through human evaluations and the introduction of a novel discriminative metric, we demonstrated that our approach outperformed a baseline model trained solely on likelihood maximization.Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Unifying-Robustness-and-Fidelity-A-Comprehensive-Study-of-Pretrained-Generative-Methods-for-Speech-Enhancement-in-Adverse-Conditions"><a href="#Unifying-Robustness-and-Fidelity-A-Comprehensive-Study-of-Pretrained-Generative-Methods-for-Speech-Enhancement-in-Adverse-Conditions" class="headerlink" title="Unifying Robustness and Fidelity: A Comprehensive Study of Pretrained Generative Methods for Speech Enhancement in Adverse Conditions"></a>Unifying Robustness and Fidelity: A Comprehensive Study of Pretrained Generative Methods for Speech Enhancement in Adverse Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09028">http://arxiv.org/abs/2309.09028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heming Wang, Meng Yu, Hao Zhang, Chunlei Zhang, Zhongweiyang Xu, Muqiao Yang, Yixuan Zhang, Dong Yu</li>
<li>for: 提高噪音环境下的语音信号质量</li>
<li>methods: 使用预训练的生成方法重新生成干净的语音信号</li>
<li>results: 实验表明，使用代码生成器可以获得更高的主观分数，并且生成的语音质量更高，噪音和反射减少。<details>
<summary>Abstract</summary>
Enhancing speech signal quality in adverse acoustic environments is a persistent challenge in speech processing. Existing deep learning based enhancement methods often struggle to effectively remove background noise and reverberation in real-world scenarios, hampering listening experiences. To address these challenges, we propose a novel approach that uses pre-trained generative methods to resynthesize clean, anechoic speech from degraded inputs. This study leverages pre-trained vocoder or codec models to synthesize high-quality speech while enhancing robustness in challenging scenarios. Generative methods effectively handle information loss in speech signals, resulting in regenerated speech that has improved fidelity and reduced artifacts. By harnessing the capabilities of pre-trained models, we achieve faithful reproduction of the original speech in adverse conditions. Experimental evaluations on both simulated datasets and realistic samples demonstrate the effectiveness and robustness of our proposed methods. Especially by leveraging codec, we achieve superior subjective scores for both simulated and realistic recordings. The generated speech exhibits enhanced audio quality, reduced background noise, and reverberation. Our findings highlight the potential of pre-trained generative techniques in speech processing, particularly in scenarios where traditional methods falter. Demos are available at https://whmrtm.github.io/SoundResynthesis.
</details>
<details>
<summary>摘要</summary>
增强语音信号质量在不利的听音环境中是一个长期挑战的问题。现有的深度学习基于的增强方法经常在实际场景中不能有效地除去背景噪声和反射，从而影响听众体验。为解决这些挑战，我们提出了一种新的方法，使用预训练的生成方法将清晰、无反射的语音重新生成出来。这项研究利用预训练的 vocoder 或 codec 模型来生成高质量的语音，同时提高了对挑战性场景的抗性。生成方法可以有效处理语音信号中的信息损失，从而生成具有提高的听音质量和减少的artefacts的语音。通过利用预训练模型的能力，我们实现了原始语音的忠实复制在不利条件下。实验评估表明，我们的提议方法在模拟数据集和实际采样中具有显著的效果和稳定性。特别是通过利用 codec，我们在模拟和实际录音中获得了更高的主观评分。生成的语音具有提高的听音质量、减少的背景噪声和反射。我们的发现表明，预训练的生成技术在语音处理中具有潜在的潜力，特别是在传统方法失效的场景下。 Demo 可以在 <https://whmrtm.github.io/SoundResynthesis> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Decoder-only-Architecture-for-Speech-Recognition-with-CTC-Prompts-and-Text-Data-Augmentation"><a href="#Decoder-only-Architecture-for-Speech-Recognition-with-CTC-Prompts-and-Text-Data-Augmentation" class="headerlink" title="Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation"></a>Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08876">http://arxiv.org/abs/2309.08876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe</li>
<li>for: 提高自动语音识别（ASR）模型的精度和效率，使其可以使用文本数据进行训练。</li>
<li>methods: 采用decoder-only架构，使用简单的文本扩充，并使用CTC预测来提供音频信息。</li>
<li>results: 在LibriSpeech和Switchboard datasets上，提出的模型比普通CTC预测减少了0.3%和1.4%的单词错误率，并在LibriSpeech 100h和Switchboard训练场景中超过了传统的encoder-decoder ASR模型。<details>
<summary>Abstract</summary>
Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.
</details>
<details>
<summary>摘要</summary>
收集音频文本对是costly的;但是可以轻松地获取文本数据。除非使用浅层融合，否则末端自动语音识别（ASR）模型需要建筑修改或额外训练方式来使用文本数据。受最近的语言模型（LM）的进步启发，我们提议使用decoder-only架构 для ASR，并使用简单的文本扩展。为了提供音频信息，encoder特征被CTC预测压缩后用作decoder的激活器，可以视为通过decoder-only模型来更正CTC预测。由于decoder架构与 autoregressive LM 相同，因此可以通过外部文本数据进行LM训练来增强模型。我们在LibriSpeech和Switchboard上进行了实验比较，发现我们提议的模型在文本扩展训练下降低了word error rate（PER）by 0.3%和1.4%在LibriSpeech test-clean和test-otherSet上，并且在Switchboard和CallHome上降低了2.9%和5.0%。此外，我们的模型在计算效率方面具有优势，并在LibriSpeech 100h和Switchboard训练enario上超过了传统的末端encoder-decoder ASR模型。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Latent-Space-Reconstruction-Learning-for-Audio-Text-Retrieval"><a href="#Contrastive-Latent-Space-Reconstruction-Learning-for-Audio-Text-Retrieval" class="headerlink" title="Contrastive Latent Space Reconstruction Learning for Audio-Text Retrieval"></a>Contrastive Latent Space Reconstruction Learning for Audio-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08839">http://arxiv.org/abs/2309.08839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiyi Luo, Xulong Zhang, Jianzong Wang, Huaxiong Li, Ning Cheng, Jing Xiao</li>
<li>for: 这个论文主要针对 audio-to-text 模式下的跨模态检索问题，即使用 audio clips 和文本进行对应。</li>
<li>methods: 该论文提出了一种新的 Contrastive Latent Space Reconstruction Learning (CLSR) 方法，它在对比表示学习中考虑了内模态分离性，并采用了 adaptive temperature control 策略。此外，该方法还包含了模态交互的latent representation reconstruction模块。</li>
<li>results: 对两个 audio-text 数据集进行比较，CLSR 方法表现出了较高的效果，胜过了一些当前最佳方法。<details>
<summary>Abstract</summary>
Cross-modal retrieval (CMR) has been extensively applied in various domains, such as multimedia search engines and recommendation systems. Most existing CMR methods focus on image-to-text retrieval, whereas audio-to-text retrieval, a less explored domain, has posed a great challenge due to the difficulty to uncover discriminative features from audio clips and texts. Existing studies are restricted in the following two ways: 1) Most researchers utilize contrastive learning to construct a common subspace where similarities among data can be measured. However, they considers only cross-modal transformation, neglecting the intra-modal separability. Besides, the temperature parameter is not adaptively adjusted along with semantic guidance, which degrades the performance. 2) These methods do not take latent representation reconstruction into account, which is essential for semantic alignment. This paper introduces a novel audio-text oriented CMR approach, termed Contrastive Latent Space Reconstruction Learning (CLSR). CLSR improves contrastive representation learning by taking intra-modal separability into account and adopting an adaptive temperature control strategy. Moreover, the latent representation reconstruction modules are embedded into the CMR framework, which improves modal interaction. Experiments in comparison with some state-of-the-art methods on two audio-text datasets have validated the superiority of CLSR.
</details>
<details>
<summary>摘要</summary>
跨模式检索（CMR）在不同领域得到了广泛应用，如多媒体搜索引擎和推荐系统。现有的大多数CMR方法强调图像到文本检索，而听音到文本检索则是一个未得到充分发展的领域，这主要是因为听音clip和文本之间找到特征点具有很大的挑战性。现有的研究受到以下两种限制：1. 大多数研究人员采用对偶学习来构建共同的特征空间，以便在数据之间可以测量相似性。然而，他们只考虑了跨模式变换，忽略了内模态分离性。此外，温度参数不适应性地调整，这会下降性能。2. 这些方法不会考虑隐藏表示的重建，这是必要的 дляsemantic alignment。本文提出了一种新的听音到文本 oriented CMR方法，称为对偶特征空间重建学习（CLSR）。CLSR方法改进了对偶表示学习，通过考虑内模态分离性和适应性温度控制策略。此外，模态交互模块被引入到CMR框架中，以提高模态交互。对两个音频到文本数据集进行比较 экспериментирова， Validated the superiority of CLSR。
</details></li>
</ul>
<hr>
<h2 id="FastGraphTTS-An-Ultrafast-Syntax-Aware-Speech-Synthesis-Framework"><a href="#FastGraphTTS-An-Ultrafast-Syntax-Aware-Speech-Synthesis-Framework" class="headerlink" title="FastGraphTTS: An Ultrafast Syntax-Aware Speech Synthesis Framework"></a>FastGraphTTS: An Ultrafast Syntax-Aware Speech Synthesis Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08837">http://arxiv.org/abs/2309.08837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzong Wang, Xulong Zhang, Aolan Sun, Ning Cheng, Jing Xiao</li>
<li>for: 这篇论文旨在 integrate graph-to-sequence 到一个终端文本至语音框架中，以实现 syntax-aware 模型化。</li>
<li>methods: 这篇论文使用了 dependency parsing 模块将输入文本解析成一个 sintactic graph，然后使用 graph encoder 对这个 sintactic graph 进行编码，提取 sintactic hidden information，并与 phoneme embedding 进行拼接，并输入到 alignment 和 flow-based decoding 模块中，生成 raw audio waveform。</li>
<li>results: 实验结果表明，这种模型可以提供更好的语音合成效果，并且在 subjective prosodic evaluation 中获得了更高的分数。此外，模型还可以进行voice conversion。此外，通过 AI chip operator 的设计，模型的效率得到了5x的加速。<details>
<summary>Abstract</summary>
This paper integrates graph-to-sequence into an end-to-end text-to-speech framework for syntax-aware modelling with syntactic information of input text. Specifically, the input text is parsed by a dependency parsing module to form a syntactic graph. The syntactic graph is then encoded by a graph encoder to extract the syntactic hidden information, which is concatenated with phoneme embedding and input to the alignment and flow-based decoding modules to generate the raw audio waveform. The model is experimented on two languages, English and Mandarin, using single-speaker, few samples of target speakers, and multi-speaker datasets, respectively. Experimental results show better prosodic consistency performance between input text and generated audio, and also get higher scores in the subjective prosodic evaluation, and show the ability of voice conversion. Besides, the efficiency of the model is largely boosted through the design of the AI chip operator with 5x acceleration.
</details>
<details>
<summary>摘要</summary>
这篇论文将graph-to-sequence integrate到了一个端到端的文本到语音框架中，以实现文本的 syntax-aware 模型化。具体来说，输入文本首先被依赖分析模块解析，形成一个语法图。然后，语法图被图编码器编码，以提取语法隐藏信息。这些隐藏信息与phoneme embedding相加，并输入到对齐和流程基于解码模块中，以生成原始的音频波形。模型在英语和普通话两种语言上进行了实验，使用单个说话者、少量目标说话者和多个说话者的数据集，分别进行了实验。实验结果表明，模型可以更好地保持输入文本和生成的音频波形之间的PROSODIC 一致性，并在主观的PROSODIC 评价中获得更高的分数。此外，模型的效率得到了通过AI芯片运算符的5倍加速的大幅提升。
</details></li>
</ul>
<hr>
<h2 id="Boosting-End-to-End-Multilingual-Phoneme-Recognition-through-Exploiting-Universal-Speech-Attributes-Constraints"><a href="#Boosting-End-to-End-Multilingual-Phoneme-Recognition-through-Exploiting-Universal-Speech-Attributes-Constraints" class="headerlink" title="Boosting End-to-End Multilingual Phoneme Recognition through Exploiting Universal Speech Attributes Constraints"></a>Boosting End-to-End Multilingual Phoneme Recognition through Exploiting Universal Speech Attributes Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08828">http://arxiv.org/abs/2309.08828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Yen, Sabato Marco Siniscalchi, Chin-Hui Lee</li>
<li>for: 该研究旨在提出一种多语言自动语音识别（ASR）系统，以利用语音生成器的知识来提高系统的性能。</li>
<li>methods: 该研究使用了一种基于语音特征的 attribute-to-phoneme 映射方法，将知识基于生成器的特征映射到输出phoneme上，以限制系统的预测。</li>
<li>results: 该研究在多种语言的测试数据上进行了比较，并发现了与传统多语言方法相比，提出的解决方案能够提高系统的性能，平均提高6.85%。此外，研究还发现了该解决方案能够消除与特征不一致的phoneme预测。<details>
<summary>Abstract</summary>
We propose a first step toward multilingual end-to-end automatic speech recognition (ASR) by integrating knowledge about speech articulators. The key idea is to leverage a rich set of fundamental units that can be defined "universally" across all spoken languages, referred to as speech attributes, namely manner and place of articulation. Specifically, several deterministic attribute-to-phoneme mapping matrices are constructed based on the predefined set of universal attribute inventory, which projects the knowledge-rich articulatory attribute logits, into output phoneme logits. The mapping puts knowledge-based constraints to limit inconsistency with acoustic-phonetic evidence in the integrated prediction. Combined with phoneme recognition, our phone recognizer is able to infer from both attribute and phoneme information. The proposed joint multilingual model is evaluated through phoneme recognition. In multilingual experiments over 6 languages on benchmark datasets LibriSpeech and CommonVoice, we find that our proposed solution outperforms conventional multilingual approaches with a relative improvement of 6.85% on average, and it also demonstrates a much better performance compared to monolingual model. Further analysis conclusively demonstrates that the proposed solution eliminates phoneme predictions that are inconsistent with attributes.
</details>
<details>
<summary>摘要</summary>
我们提出一个初步的多语言端到端自动语音识别（ASR）方法，通过 интеGRATE知识About speech articulators。关键思想是利用一个丰富的基本单元，可以在所有的口语语言中 Universally defined，称为speech attributes，namely manner and place of articulation。特别是，我们构建了一些决定性的 attribute-to-phoneme mapping矩阵，基于预定的universal attribute inventory，将知识医学特征logits项目到输出phoneme logits。这种映射带有知识基础的约束，以限制与语音-phonetic证据的不一致。与phoneme recognition结合，我们的电话识别器能够从both attribute和phoneme信息中进行推理。我们提出的联合多语言模型在LibriSpeech和CommonVoice多语言测试集上进行了phoneme recognition测试，并 obtAIN了相对改善6.85%的平均提升，以及和单语言模型的较好表现。进一步的分析表明，我们的解决方案可以消除与 attribute不一致的phoneme预测。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/16/cs.SD_2023_09_16/" data-id="clogxf3qf00uz5xraeo5k30om" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/32/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/31/">31</a><a class="page-number" href="/page/32/">32</a><span class="page-number current">33</span><a class="page-number" href="/page/34/">34</a><a class="page-number" href="/page/35/">35</a><span class="space">&hellip;</span><a class="page-number" href="/page/83/">83</a><a class="extend next" rel="next" href="/page/34/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
