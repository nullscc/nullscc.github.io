
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/48/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.CL_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T11:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/cs.CL_2023_09_05/">cs.CL - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Scaling-Autoregressive-Multi-Modal-Models-Pretraining-and-Instruction-Tuning"><a href="#Scaling-Autoregressive-Multi-Modal-Models-Pretraining-and-Instruction-Tuning" class="headerlink" title="Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning"></a>Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02591">http://arxiv.org/abs/2309.02591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/CM3Leon">https://github.com/kyegomez/CM3Leon</a></li>
<li>paper_authors: Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, Armen Aghajanyan</li>
<li>for: 这篇论文是为了描述一种基于多模态语言模型的文本和图像生成模型CM3Leon，以及该模型在不同任务上的性能。</li>
<li>methods: 该模型使用了CM3多模态架构，并在大规模的采集和调参数数据上进行了扩展和优化。它还包括一个大规模的预训练阶段和一个多任务练熟环境（SFT）阶段。</li>
<li>results: 实验结果显示，这种方法对多模态模型是非常有效的，CM3Leon在文本到图像生成任务中达到了状态对的性能（FID&#x3D;4.88），并且在语言指导图像编辑、图像控制生成和分割等任务中也可以达到了不可思议的水平。<details>
<summary>Abstract</summary>
We present CM3Leon (pronounced "Chameleon"), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon uses the CM3 multi-modal architecture but additionally shows the extreme benefits of scaling up and tuning on more diverse instruction-style data. It is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate unprecedented levels of controllability in tasks ranging from language-guided image editing to image-controlled generation and segmentation.
</details>
<details>
<summary>摘要</summary>
我们提出CM3Leon（发音为“卡美伦”），这是一个基于搜索修正的、符号基于的解码器只多模态语言模型，可以生成和填充文本和图像。CM3Leon使用CM3多模态架构，但还有更加极端的优势，来自更多的指令样式数据的扩大和调整。它是首个基于文本only语言模型的多模态模型，通过一个大规模的搜索修正预训练阶段和第二个多任务监督练练（SFT）阶段进行训练。它还是一个通用的模型，可以进行文本到图像和图像到文本的生成，allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs。广泛的实验表明，这种方法对多模态模型非常有效。CM3Leon在文本到图像生成中达到了比较方法的状态机器（零shot MS-COCO FID of 4.88）。在SFT后，CM3Leon也可以展示无 precedent的可控性，从语言引导的图像修改到图像控制生成和分割。
</details></li>
</ul>
<hr>
<h2 id="Substitution-based-Semantic-Change-Detection-using-Contextual-Embeddings"><a href="#Substitution-based-Semantic-Change-Detection-using-Contextual-Embeddings" class="headerlink" title="Substitution-based Semantic Change Detection using Contextual Embeddings"></a>Substitution-based Semantic Change Detection using Contextual Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02403">http://arxiv.org/abs/2309.02403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dallascard/SBSCD">https://github.com/dallascard/SBSCD</a></li>
<li>paper_authors: Dallas Card</li>
<li>for: 本研究旨在使用上下文嵌入来度量语义变化，并且提出了一种简单有效的方法，以优化现有的方法。</li>
<li>methods: 本研究使用最有可能的替换词来度量语义变化，这种方法不仅直观可解，而且更有效率，可以更好地探讨语义变化。</li>
<li>results: 本研究在最常引用的数据集上达到了最高的均值性能，并且可以更好地探讨语义变化，比静止词vec更有利于理解语义变化。<details>
<summary>Abstract</summary>
Measuring semantic change has thus far remained a task where methods using contextual embeddings have struggled to improve upon simpler techniques relying only on static word vectors. Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation. We present a simplified approach to measuring semantic change using contextual embeddings, relying only on the most probable substitutes for masked terms. Not only is this approach directly interpretable, it is also far more efficient in terms of storage, achieves superior average performance across the most frequently cited datasets for this task, and allows for more nuanced investigation of change than is possible with static word vectors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="nanoT5-A-PyTorch-Framework-for-Pre-training-and-Fine-tuning-T5-style-Models-with-Limited-Resources"><a href="#nanoT5-A-PyTorch-Framework-for-Pre-training-and-Fine-tuning-T5-style-Models-with-Limited-Resources" class="headerlink" title="nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources"></a>nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02373">http://arxiv.org/abs/2309.02373</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/piotrnawrot/nanot5">https://github.com/piotrnawrot/nanot5</a></li>
<li>paper_authors: Piotr Nawrot</li>
<li>for: 提高语言模型研究的可用性和资源利用率，使更多研究者能够访问和使用T5模型。</li>
<li>methods: 通过优化PyTorch框架和优化器，实现高效的T5模型预训练和精度调整，以及开源框架和配置等资源的提供，旨在拓宽语言模型研究领域的可用性和资源利用率。</li>
<li>results: 在单个GPU上预训练T5-Base模型只需16个小时，不会影响性能，并提供了多种配置和软硬件准则，以及开源框架和预训练模型，以满足研究者对T5模型的需求。<details>
<summary>Abstract</summary>
State-of-the-art language models like T5 have revolutionized the NLP landscape, but their computational demands hinder a large portion of the research community. To address this challenge, we present nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models. Drawing on insights from optimizer differences and prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a single GPU in just 16 hours, without any loss in performance. With the introduction of this open-source framework, we hope to widen the accessibility to language modelling research and cater to the community's demand for more user-friendly T5 (Encoder-Decoder) implementations. Our contributions, including configurations, codebase, software/hardware insights, and pre-trained models, are available to the public, aiming to strike a balance between research accessibility and resource constraints in NLP.
</details>
<details>
<summary>摘要</summary>
现代语言模型如T5已经革命化了NLPT中的景象，但它们的计算需求限制了大量研究人员。为解决这个挑战，我们现在提出nanoT5，一个特殊优化的PyTorch框架，用于高效地预训练和精度调整T5模型。通过优化器差异和高效性的启发，nanoT5可以在单个GPU上预训练T5-Base模型，只需16个小时，而无损失性表现。我们通过这个开源框架，希望扩大语言模型研究的访问权限，并为NLPT社区提供更加用户友好的T5（Encoder-Decoder）实现。我们的贡献包括配置、代码库、软硬件杂志和预训练模型，都对公众开放，以实现NLPT研究资源的平衡。
</details></li>
</ul>
<hr>
<h2 id="Weigh-Your-Own-Words-Improving-Hate-Speech-Counter-Narrative-Generation-via-Attention-Regularization"><a href="#Weigh-Your-Own-Words-Improving-Hate-Speech-Counter-Narrative-Generation-via-Attention-Regularization" class="headerlink" title="Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization"></a>Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02311">http://arxiv.org/abs/2309.02311</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/milanlproc/weigh-your-own-words">https://github.com/milanlproc/weigh-your-own-words</a></li>
<li>paper_authors: Helena Bonaldi, Giuseppe Attanasio, Debora Nozza, Marco Guerini</li>
<li>for: 防止在线仇恨言语的发展，提出了一种基于预训练语言模型（PLMs）的自动生成对话方法。</li>
<li>methods: 本研究使用了一种基于注意力的违规常量来改进PLMs的泛化能力，以便在不同的目标和实际垃圾语言上生成更加多样化和更加丰富的对话。</li>
<li>results: 对英语 benchmark 数据集进行实验表明，使用了注意力违规常量的改进方法可以生成更好的对话，特别是在训练数据中不包含仇恨目标时。<details>
<summary>Abstract</summary>
Recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting Pretrained Transformer-based Language Models (PLMs) with human-curated data. This process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. This paper introduces novel attention regularization methodologies to improve the generalization capabilities of PLMs for counter narratives generation. Overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. We experiment with two attention-based regularization techniques on a benchmark English dataset. Regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. This work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:近期计算方法对于在线仇恨言语的应对包括使用预训练的变换器基于语言模型（PLMs）自动生成反对narritives。然而，这个过程可能会导致域内过拟合，使模型只能生成与训练数据相似的acceptable narritives，具有小的可移植性到其他目标或实际世界中的恶语言。本文提出了一种新的注意力规范方法来提高PLMs的泛化能力 для反对narritives生成。通过避免训练数据特定的注意力过拟合，模型可以生成更多元和更加丰富的narritives。我们在一个英语 benchmark 数据集上实验了两种注意力基于规范技术，并发现正则化模型在大多数情况下可以生成更好的反对narritives，特别是当仇恨目标不在训练数据中时。这项工作为Counter-speech生成模型的更好和更灵活的模型开创了道路，这个任务的数据非常困难生产。
</details></li>
</ul>
<hr>
<h2 id="PromptTTS-2-Describing-and-Generating-Voices-with-Text-Prompt"><a href="#PromptTTS-2-Describing-and-Generating-Voices-with-Text-Prompt" class="headerlink" title="PromptTTS 2: Describing and Generating Voices with Text Prompt"></a>PromptTTS 2: Describing and Generating Voices with Text Prompt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02285">http://arxiv.org/abs/2309.02285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian</li>
<li>for: 这个研究是为了解决基于文本提示的语音生成方法中的一个问题，即使用文本提示来生成语音时，不能完全捕捉语音中的声音变化信息。</li>
<li>methods: 这个研究使用了一种变换网络，该网络可以根据文本提示来预测语音中的声音变化信息，以及一个提取ipeline，该ipeline可以使用语音理解模型来识别语音中的声音特征（例如性别、速度等），并使用大型自然语言处理模型来形成文本提示。</li>
<li>results: 实验结果表明，与前一代方法相比，PromptTTS 2可以更好地根据文本提示生成语音，并且支持采样多种语音变化，因此可以为用户提供更多的语音选择。此外，提取ipeline可以生成高质量的文本提示，从而消除大量的标注成本。<details>
<summary>Abstract</summary>
Speech conveys more information than just text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompt for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variation network predicts the representation extracted from the reference speech (which contains full information about voice) based on the text prompt representation. For the prompt generation pipeline, it generates text prompts for speech with a speech understanding model to recognize voice attributes (e.g., gender, speed) from speech and a large language model to formulate text prompt based on the recognition results. Experiments on a large-scale (44K hours) speech dataset demonstrate that compared to the previous works, PromptTTS 2 generates voices more consistent with text prompts and supports the sampling of diverse voice variability, thereby offering users more choices on voice generation. Additionally, the prompt generation pipeline produces high-quality prompts, eliminating the large labeling cost. The demo page of PromptTTS 2 is available online\footnote{https://speechresearch.github.io/prompttts2}.
</details>
<details>
<summary>摘要</summary>
文本中的语音包含更多信息，因为同一个词可以在不同的声音下被读出，表达多种信息。相比传统的文本识别（TTS）方法，利用语音提示（参考语音）来实现声音多样性，使用文本提示（描述）更加用户友好，因为语音提示可能困难找或者不存在。TTS方法基于文本提示面临两个挑战：1）一个多个问题，即文本提示中不能完全表达声音多样性的细节信息；2）文本提示数据集的有限性，需要供应商和大量的数据标注来编写文本提示。在这项工作中，我们介绍PromptTTS 2，以解决这两个挑战。PromptTTS 2使用变化网络提供不同声音的多样性信息，并使用大语言模型（LLM）组合高质量文本提示来生成语音。具体来说，变化网络预测基于参考语音（含有全部声音信息）的表示，根据文本提示表示。为生成文本提示，我们使用语音理解模型认识语音特征（例如性别、速度），并使用大语言模型根据认识结果组合文本提示。实验表明，Compared to previous works，PromptTTS 2可以更好地根据文本提示生成声音，并支持采样多样的声音选择。此外，提示生成管道可以生成高质量的提示，减少大量标注成本。PromptTTS 2的demo页面可以在线查看\footnotesize{\url{https://speechresearch.github.io/prompttts2}.
</details></li>
</ul>
<hr>
<h2 id="Dialog-Action-Aware-Transformer-for-Dialog-Policy-Learning"><a href="#Dialog-Action-Aware-Transformer-for-Dialog-Policy-Learning" class="headerlink" title="Dialog Action-Aware Transformer for Dialog Policy Learning"></a>Dialog Action-Aware Transformer for Dialog Policy Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02240">http://arxiv.org/abs/2309.02240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huimin Wang, Wai-Chung Kwan, Kam-Fai Wong</li>
<li>for: 这个研究旨在提高对话策略学习（Dialog Policy Learning，DPL）的效率，使用对话数据来增强RL代理人的学习速度。</li>
<li>methods: 本研究提出了一个叫做“对话动作意识”的对话动作批评（DaTrans），该批评通过一个新的调整程序“对话最后一个动作任务”来增强DaTrans的对话意识和动作特征。</li>
<li>results: 研究结果显示，这个方法可以快速地将RL代理人带到最佳的对话策略，并且在人类评价中得到了良好的评价。<details>
<summary>Abstract</summary>
Recent works usually address Dialog policy learning DPL by training a reinforcement learning (RL) agent to determine the best dialog action. However, existing works on deep RL require a large volume of agent-user interactions to achieve acceptable performance. In this paper, we propose to make full use of the plain text knowledge from the pre-trained language model to accelerate the RL agent's learning speed. Specifically, we design a dialog action-aware transformer encoder (DaTrans), which integrates a new fine-tuning procedure named masked last action task to encourage DaTrans to be dialog-aware and distils action-specific features. Then, DaTrans is further optimized in an RL setting with ongoing interactions and evolves through exploration in the dialog action space toward maximizing long-term accumulated rewards. The effectiveness and efficiency of the proposed model are demonstrated with both simulator evaluation and human evaluation.
</details>
<details>
<summary>摘要</summary>
现代工作通常采用对话策略学习（Dialog Policy Learning，DPL），通过训练一个强化学习（Reinforcement Learning，RL）代理人来确定最佳对话动作。然而，现有的深度RL需要大量的代理人-用户互动来 достичьacceptable的性能。在这篇论文中，我们提议利用预先训练的自然语言模型的普通文本知识，以加速RL代理人的学习速度。特别是，我们设计了对话动作意识的 transformer 编码器（DaTrans），通过一种新的精细调整过程名为遮盖最后一个动作任务来鼓励 DaTrans 成为对话意识的。然后，DaTrans 在RL Setting中进行了进一步优化，通过在对话动作空间中的探索来最大化长期积累的奖励。我们通过 simulate 评估和人类评估来证明提案的效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Dictionaries-into-a-Neural-Network-Architecture-to-Extract-COVID-19-Medical-Concepts-From-Social-Media"><a href="#Incorporating-Dictionaries-into-a-Neural-Network-Architecture-to-Extract-COVID-19-Medical-Concepts-From-Social-Media" class="headerlink" title="Incorporating Dictionaries into a Neural Network Architecture to Extract COVID-19 Medical Concepts From Social Media"></a>Incorporating Dictionaries into a Neural Network Architecture to Extract COVID-19 Medical Concepts From Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02188">http://arxiv.org/abs/2309.02188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abul Hasan, Mark Levene, David Weston</li>
<li>for: 这研究探讨了将字典信息 integrate into neural network architecture for natural language processing的可能性。</li>
<li>methods: 这研究使用了一种基于字典的深度学习模型，用于提取COVID-19相关的概念。</li>
<li>results: 研究结果显示，将小领域字典 integrate into深度学习模型可以提高概念提取任务的性能，并且这些模型可以在不同数据集上进行转移。<details>
<summary>Abstract</summary>
We investigate the potential benefit of incorporating dictionary information into a neural network architecture for natural language processing. In particular, we make use of this architecture to extract several concepts related to COVID-19 from an on-line medical forum. We use a sample from the forum to manually curate one dictionary for each concept. In addition, we use MetaMap, which is a tool for extracting biomedical concepts, to identify a small number of semantic concepts. For a supervised concept extraction task on the forum data, our best model achieved a macro $F_1$ score of 90\%. A major difficulty in medical concept extraction is obtaining labelled data from which to build supervised models. We investigate the utility of our models to transfer to data derived from a different source in two ways. First for producing labels via weak learning and second to perform concept extraction. The dataset we use in this case comprises COVID-19 related tweets and we achieve an $F_1$ score 81\% for symptom concept extraction trained on weakly labelled data. The utility of our dictionaries is compared with a COVID-19 symptom dictionary that was constructed directly from Twitter. Further experiments that incorporate BERT and a COVID-19 version of BERTweet demonstrate that the dictionaries provide a commensurate result. Our results show that incorporating small domain dictionaries to deep learning models can improve concept extraction tasks. Moreover, models built using dictionaries generalize well and are transferable to different datasets on a similar task.
</details>
<details>
<summary>摘要</summary>
我们研究将词典信息 integrate into neural network architecture for natural language processing的潜在优点。特别是我们使用这种架构提取COVID-19相关概念从在线医学讨论区。我们使用样本从讨论区手动精心抽取一个词典 для每个概念。此外，我们使用MetaMap工具提取生物医学概念，以确定一些semantic概念。对于基于讨论区数据的抽象概念提取任务，我们的最佳模型达到了90%的macro $F_1$ 分数。医疗概念提取的主要挑战之一是获得可靠的标签数据，用于建立supervised模型。我们研究将我们的模型传输到不同来源数据上进行两种方式。第一种是通过弱学习生成标签，第二种是进行概念提取。我们使用COVID-19相关推特来构建数据集，并实现了基于弱标签的概念提取Task中的81%的$F_1$ 分数。我们的词典与直接从Twitter中构建的COVID-19症状词典进行比较。进一步的实验表明，我们的词典提供了相似的结果。我们的结果表明，将小域词典 integrate into深度学习模型可以提高概念提取任务的性能。此外，使用词典建立的模型具有良好的泛化能力和可传播性。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Text-to-GLOSS-Neural-Translation-Using-a-Novel-Hyper-parameter-Optimization-Technique"><a href="#Advancing-Text-to-GLOSS-Neural-Translation-Using-a-Novel-Hyper-parameter-Optimization-Technique" class="headerlink" title="Advancing Text-to-GLOSS Neural Translation Using a Novel Hyper-parameter Optimization Technique"></a>Advancing Text-to-GLOSS Neural Translation Using a Novel Hyper-parameter Optimization Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02162">http://arxiv.org/abs/2309.02162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Younes Ouargani, Noussaima El Khattabi</li>
<li>for: 这paper是 investigate transformers for Neural Machine Translation of text-to-GLOSS, 用于提高Deaf和听力不良的通信中的GLOSS翻译的精度和流畅性。</li>
<li>methods: 这paper使用了一种新的超参数搜索技术，搜索了不同的架构参数，并构建了一个优化的 transformer-based 架构，特意适用于text-to-GLOSS翻译任务。</li>
<li>results: 实验结果表明，最佳的 transformer 架构在 PHOENIX14T 数据集上达到了 ROUGE 分数55.18% 和 BLEU-1 分数63.6%，超过了之前在同一数据集上的最佳结果，升级了 BLEU1 和 ROUGE 分数的状态之作。<details>
<summary>Abstract</summary>
In this paper, we investigate the use of transformers for Neural Machine Translation of text-to-GLOSS for Deaf and Hard-of-Hearing communication. Due to the scarcity of available data and limited resources for text-to-GLOSS translation, we treat the problem as a low-resource language task. We use our novel hyper-parameter exploration technique to explore a variety of architectural parameters and build an optimal transformer-based architecture specifically tailored for text-to-GLOSS translation. The study aims to improve the accuracy and fluency of Neural Machine Translation generated GLOSS. This is achieved by examining various architectural parameters including layer count, attention heads, embedding dimension, dropout, and label smoothing to identify the optimal architecture for improving text-to-GLOSS translation performance. The experiments conducted on the PHOENIX14T dataset reveal that the optimal transformer architecture outperforms previous work on the same dataset. The best model reaches a ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score of 55.18% and a BLEU-1 (BiLingual Evaluation Understudy 1) score of 63.6%, outperforming state-of-the-art results on the BLEU1 and ROUGE score by 8.42 and 0.63 respectively.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究使用变换器来进行神经机器翻译文本到GLOSS，以便为听力异常和耳聋人士进行交流。由于文本到GLOSS翻译数据的稀缺和限制了资源，我们将这个问题视为低资源语言任务。我们使用我们的新的 гипер参数探索技术来探索各种建筑 Parameters，并构建一个优化的变换器基础结构，专门适用于文本到GLOSS翻译。研究的目的是提高神经机器翻译生成的GLOSS的准确率和流畅度。我们通过检查层数、注意头数、嵌入维度、dropout和标签平滑来确定优化文本到GLOSS翻译性能的最佳建筑 Parameters。在PHOENIX14T数据集上进行的实验表明，优化的变换器结构可以超越之前在同一数据集上的成果。最佳模型在ROUGE（Recall-Oriented Understudy for Gisting Evaluation）分数上达到55.18%，并在BLEU-1（BiLingual Evaluation Understudy 1）分数上达到63.6%，超越了当前的BLEU1和ROUGE分数的状态态度。
</details></li>
</ul>
<hr>
<h2 id="Bring-the-Noise-Introducing-Noise-Robustness-to-Pretrained-Automatic-Speech-Recognition"><a href="#Bring-the-Noise-Introducing-Noise-Robustness-to-Pretrained-Automatic-Speech-Recognition" class="headerlink" title="Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition"></a>Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02145">http://arxiv.org/abs/2309.02145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Eickhoff, Matthias Möller, Theresa Pekarek Rosin, Johannes Twiefel, Stefan Wermter</li>
<li>for: 这研究旨在提高自动语音识别（ASR）系统的性能，特别是在听力条件不佳的情况下。</li>
<li>methods: 我们提出了一种新的方法，可以将大型端到端（E2E）模型中的干净能力提取出来，并将其应用于任何encoder-decoder架构。我们的方法基于Conformer ASR模型的隐藏活动，通过一个decoder来预测干净spectrogram。</li>
<li>results: 我们的模型可以成功地过滤听力条件下的噪音，并且可以提高下游模型在噪音条件下的总词错率（WER）。我们的模型可以作为前端应用于预训练的Conformer ASR模型，以及从头开始训练小型Conformer ASR模型。<details>
<summary>Abstract</summary>
In recent research, in the domain of speech processing, large End-to-End (E2E) systems for Automatic Speech Recognition (ASR) have reported state-of-the-art performance on various benchmarks. These systems intrinsically learn how to handle and remove noise conditions from speech. Previous research has shown, that it is possible to extract the denoising capabilities of these models into a preprocessor network, which can be used as a frontend for downstream ASR models. However, the proposed methods were limited to specific fully convolutional architectures. In this work, we propose a novel method to extract the denoising capabilities, that can be applied to any encoder-decoder architecture. We propose the Cleancoder preprocessor architecture that extracts hidden activations from the Conformer ASR model and feeds them to a decoder to predict denoised spectrograms. We train our pre-processor on the Noisy Speech Database (NSD) to reconstruct denoised spectrograms from noisy inputs. Then, we evaluate our model as a frontend to a pretrained Conformer ASR model as well as a frontend to train smaller Conformer ASR models from scratch. We show that the Cleancoder is able to filter noise from speech and that it improves the total Word Error Rate (WER) of the downstream model in noisy conditions for both applications.
</details>
<details>
<summary>摘要</summary>
Recent research in speech processing has shown that large End-to-End (E2E) systems for Automatic Speech Recognition (ASR) have achieved state-of-the-art performance on various benchmarks. These systems have the ability to intrinsically handle and remove noise from speech. Previous studies have demonstrated that the denoising capabilities of these models can be extracted and used as a frontend for downstream ASR models. However, these methods were limited to specific fully convolutional architectures.In this study, we propose a novel method to extract the denoising capabilities that can be applied to any encoder-decoder architecture. We introduce the Cleancoder preprocessor architecture, which extracts hidden activations from the Conformer ASR model and feeds them to a decoder to predict denoised spectrograms. We train our pre-processor on the Noisy Speech Database (NSD) to reconstruct denoised spectrograms from noisy inputs.We evaluate our model as a frontend to a pretrained Conformer ASR model as well as a frontend to train smaller Conformer ASR models from scratch. Our results show that the Cleancoder is able to filter noise from speech and improve the total Word Error Rate (WER) of the downstream model in noisy conditions for both applications.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Methods-for-Ground-Truth-Free-Foreign-Accent-Conversion"><a href="#Evaluating-Methods-for-Ground-Truth-Free-Foreign-Accent-Conversion" class="headerlink" title="Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion"></a>Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02133">http://arxiv.org/abs/2309.02133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/unilight/seq2seq-vc">https://github.com/unilight/seq2seq-vc</a></li>
<li>paper_authors: Wen-Chin Huang, Tomoki Toda</li>
<li>For: 本研究旨在评估三种最近提出的无ground truth基础的外语变换方法（FAC），以实现将非本地语言speaker的语音转换为本地语言speaker的语音，同时保持speaker identity。* Methods: 本研究使用的方法包括seq2seq和非并行的VC模型，以实现控制speaker identity和降低外语变换的困难性。* Results: 我们的实验评估结果显示，无一个方法在所有评估轴上表现出色，与之前的研究结论不同。我们还分析了seq2seq模型的训练输入和输出，以及非并行VC模型的设计选择，并发现Intelligibility指标与主观外语程度之间没有直接关系。<details>
<summary>Abstract</summary>
Foreign accent conversion (FAC) is a special application of voice conversion (VC) which aims to convert the accented speech of a non-native speaker to a native-sounding speech with the same speaker identity. FAC is difficult since the native speech from the desired non-native speaker to be used as the training target is impossible to collect. In this work, we evaluate three recently proposed methods for ground-truth-free FAC, where all of them aim to harness the power of sequence-to-sequence (seq2seq) and non-parallel VC models to properly convert the accent and control the speaker identity. Our experimental evaluation results show that no single method was significantly better than the others in all evaluation axes, which is in contrast to conclusions drawn in previous studies. We also explain the effectiveness of these methods with the training input and output of the seq2seq model and examine the design choice of the non-parallel VC model, and show that intelligibility measures such as word error rates do not correlate well with subjective accentedness. Finally, our implementation is open-sourced to promote reproducible research and help future researchers improve upon the compared systems.
</details>
<details>
<summary>摘要</summary>
外国腔转换（FAC）是voice转换（VC）的特殊应用，旨在将非本地语言 speaker的折衣语音转换为本地语言 speaker的Native-sounding speech，同时保持 speaker identity。FAC具有困难，因为不可收集欲使用的Native speech from the desired non-native speaker作为训练目标。在这项工作中，我们评估了三种最近提出的ground-truth-free FAC方法，其中所有方法均企图利用 seq2seq和non-parallel VC模型来正确地转换腔和控制 speaker identity。我们的实验评估结果表明，没有任何方法在所有评估轴上表现出显著优势，这与之前的研究结论不符。我们还解释了这些方法的效iveness，并检查了seq2seq模型的训练输入和输出，以及非平行VC模型的设计选择。最后，我们发现Intelligibility measure如word error rates与主观腔度之间没有正确的相关性。 finally,我们开源了我们的实现，以便促进可重复性的研究和未来的研究人员可以在此基础上改进相关的系统。
</details></li>
</ul>
<hr>
<h2 id="Wordle-A-Microcosm-of-Life-Luck-Skill-Cheating-Loyalty-and-Influence"><a href="#Wordle-A-Microcosm-of-Life-Luck-Skill-Cheating-Loyalty-and-Influence" class="headerlink" title="Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!"></a>Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02110">http://arxiv.org/abs/2309.02110</a></li>
<li>repo_url: None</li>
<li>paper_authors: James P. Dilger</li>
<li>for: 这个研究是为了研究Wordle游戏中玩家的做法和习惯。</li>
<li>methods: 这个研究使用了信息理论来评估玩家的幸运和技巧，并将数据显示在Wordle游戏中的第一、第二、…、第六个猜测中。</li>
<li>results: 研究发现每天约有0.2-0.5%的玩家在第一次猜测中解题成功，这意味着4,000-10,000名玩家可能通过外部获取目标词语来夺冠。此外，研究还发现至少1&#x2F;3的玩家有一个喜爱的开头词，而且大多数玩家会保持loyal于他们的开头词，即使该词语已经出现过。8月15日，约有30,000名玩家突然改变了他们的开头词，这可能是基于十字WORD的游戏提示。<details>
<summary>Abstract</summary>
Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 possible target words at random is 0.043%, this implies that 4,000 - 10,000 players cheat by obtaining the target word outside of playing the game! B) At least 1/3 of the players have a favorite starting word, or cycle through several. And even though players should be aware that target words are never repeated, most players appear to remain loyal to their starting word even after its appearance as a target word. C) On August 15, 2023, about 30,000 players abruptly changed their starting word, presumably based on a crossword puzzle clue! Wordle players can be influenced! This study goes beyond social media postings, surveys, and Google Trends to provide solid, quantitative evidence about cheating in Wordle.
</details>
<details>
<summary>摘要</summary>
wordle是一款受欢迎的在线单词游戏，提供于纽约时报（nytimes.com）上。目前全球玩家约200万人。玩家有6次尝试猜测每天的目标词（target word），每次猜测后，玩家会收到颜色标注的正确性和位置信息。完成游戏或最后一次无法猜测后，软件可以根据信息理论评估玩家的运气和技巧，并显示数据 для第一、第二、...、第六次猜测的随机样本玩家。我最近发现这些数据可以轻松地复制并粘贴到表格中。我 compile了5月2023年-8月2023年的Wordle玩家首次猜测数据，并从中推导出了一些有趣的信息。A) 每天大约0.2%-0.5%的玩家在第一次猜测中解题成功。由于随机猜测target word的概率为0.043%，这 imply That 4,000-10,000名玩家通过外部方式获得target word！B) 至少1/3的玩家有一个喜爱的开始词，或者循环使用多个。尽管玩家应该知道target words never repeated，但大多数玩家仍然偏向自己的开始词，即使该词已经出现在目标词中。C) 2023年8月15日，约30,000名玩家 suddenly changed their starting word， apparently based on a crossword puzzle clue! Wordle players can be influenced！这项研究超过社交媒体帖子、调查和Google Trends提供的轻量级证据，以准确的数据证明Wordle玩家的作弊行为。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Emotion-Role-Labeling-and-Appraisal-based-Emotion-Analysis"><a href="#Bridging-Emotion-Role-Labeling-and-Appraisal-based-Emotion-Analysis" class="headerlink" title="Bridging Emotion Role Labeling and Appraisal-based Emotion Analysis"></a>Bridging Emotion Role Labeling and Appraisal-based Emotion Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02092">http://arxiv.org/abs/2309.02092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Klinger</li>
<li>for: 本研究旨在探讨情感分析在文本中的应用，具体来说是情感分类和情感角色标注两个方面。</li>
<li>methods: 本研究使用了多种自然语言处理技术，包括情感分类和情感角色标注等。</li>
<li>results: 研究发现了情感分类和情感角色标注两个方面的问题，并提出了一些未解决的研究问题。<details>
<summary>Abstract</summary>
The term emotion analysis in text subsumes various natural language processing tasks which have in common the goal to enable computers to understand emotions. Most popular is emotion classification in which one or multiple emotions are assigned to a predefined textual unit. While such setting is appropriate to identify the reader's or author's emotion, emotion role labeling adds the perspective of mentioned entities and extracts text spans that correspond to the emotion cause. The underlying emotion theories agree on one important point; that an emotion is caused by some internal or external event and comprises several subcomponents, including the subjective feeling and a cognitive evaluation. We therefore argue that emotions and events are related in two ways. (1) Emotions are events; and this perspective is the fundament in NLP for emotion role labeling. (2) Emotions are caused by events; a perspective that is made explicit with research how to incorporate psychological appraisal theories in NLP models to interpret events. These two research directions, role labeling and (event-focused) emotion classification, have by and large been tackled separately. We contributed to both directions with the projects SEAT (Structured Multi-Domain Emotion Analysis from Text) and CEAT (Computational Event Evaluation based on Appraisal Theories for Emotion Analysis), both funded by the German Research Foundation. In this paper, we consolidate the findings and point out open research questions.
</details>
<details>
<summary>摘要</summary>
“情感分析”是一种自然语言处理任务的总称，它的目的是让计算机理解人类的情感。最受欢迎的是情感分类，在这种设定下，一个或多个情感被分配给已知文本单位。而情感角色标注则添加了提及对象的视角，并提取与情感相关的文本块。在情感理论中，所有情感都是由内部或外部事件引起的，并包括一些主观感受和认知评价。因此，我们认为情感和事件之间存在两种关系。第一种是情感是事件的角度，这是NP的基础。第二种是情感是由事件引起的，这种角度通过涉及心理评价理论来在NP模型中表示。这两个研究方向一直处理了分开，我们通过项目《SEAT》（结构多元领域情感分析从文本）和《CEAT》（基于评价理论的计算事件评价为情感分析），均得到了德国研究基金的资金支持。在这篇论文中，我们汇总了发现和提出了未来研究的问题。
</details></li>
</ul>
<hr>
<h2 id="An-Automatic-Evaluation-Framework-for-Multi-turn-Medical-Consultations-Capabilities-of-Large-Language-Models"><a href="#An-Automatic-Evaluation-Framework-for-Multi-turn-Medical-Consultations-Capabilities-of-Large-Language-Models" class="headerlink" title="An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models"></a>An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02077">http://arxiv.org/abs/2309.02077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusheng Liao, Yutong Meng, Hongcheng Liu, Yanfeng Wang, Yu Wang</li>
<li>for: 这篇论文旨在评估大语言模型（LLMs）在虚拟医生环境中的实际能力。</li>
<li>methods: 该论文提出了一种自动评估框架，用于评估 LLMs 在多turn 询问中的实际能力。该框架包括设计了供询问任务，要求 LLMs 了解自己所不知道的信息，并从患者那里收集缺失的医疗信息。</li>
<li>results: 实验结果显示，通过 fine-tuning 训练集可以减轻 LLMs 的假设现象，提高其在提posed的benchmark上的表现。这些结果得到了广泛的实验和剥夺学调查的验证。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved significant success in interacting with human. However, recent studies have revealed that these models often suffer from hallucinations, leading to overly confident but incorrect judgments. This limits their application in the medical domain, where tasks require the utmost accuracy. This paper introduces an automated evaluation framework that assesses the practical capabilities of LLMs as virtual doctors during multi-turn consultations. Consultation tasks are designed to require LLMs to be aware of what they do not know, to inquire about missing medical information from patients, and to ultimately make diagnoses. To evaluate the performance of LLMs for these tasks, a benchmark is proposed by reformulating medical multiple-choice questions from the United States Medical Licensing Examinations (USMLE), and comprehensive evaluation metrics are developed and evaluated on three constructed test sets. A medical consultation training set is further constructed to improve the consultation ability of LLMs. The results of the experiments show that fine-tuning with the training set can alleviate hallucinations and improve LLMs' performance on the proposed benchmark. Extensive experiments and ablation studies are conducted to validate the effectiveness and robustness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
Consultation tasks are designed to require LLMs to be aware of what they do not know, to inquire about missing medical information from patients, and to ultimately make diagnoses. To evaluate the performance of LLMs for these tasks, a benchmark is proposed by reformulating medical multiple-choice questions from the United States Medical Licensing Examinations (USMLE), and comprehensive evaluation metrics are developed and evaluated on three constructed test sets. A medical consultation training set is further constructed to improve the consultation ability of LLMs. The results of the experiments show that fine-tuning with the training set can alleviate hallucinations and improve LLMs' performance on the proposed benchmark.Extensive experiments and ablation studies are conducted to validate the effectiveness and robustness of the proposed framework.
</details></li>
</ul>
<hr>
<h2 id="Bilevel-Scheduled-Sampling-for-Dialogue-Generation"><a href="#Bilevel-Scheduled-Sampling-for-Dialogue-Generation" class="headerlink" title="Bilevel Scheduled Sampling for Dialogue Generation"></a>Bilevel Scheduled Sampling for Dialogue Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01953">http://arxiv.org/abs/2309.01953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawen Liu, Kan Li</li>
<li>for:  mitigating exposure bias in natural language processing tasks, particularly in dialog generation.</li>
<li>methods:  proposed a bilevel scheduled sampling model that takes sentence-level information into account and incorporates it with word-level quality, and a smooth function that maps the combined result to an appropriate range for probabilistic sampling.</li>
<li>results:  significantly alleviated the exposure bias problem and outperformed state-of-the-art scheduled sampling methods in experiments conducted on the DailyDialog and PersonaChat datasets.<details>
<summary>Abstract</summary>
Exposure bias poses a common challenge in numerous natural language processing tasks, particularly in the dialog generation. In response to this issue, researchers have devised various techniques, among which scheduled sampling has proven to be an effective method for mitigating exposure bias. However, the existing state-of-the-art scheduled sampling methods solely consider the current sampling words' quality for threshold truncation sampling, which overlooks the importance of sentence-level information and the method of threshold truncation warrants further discussion. In this paper, we propose a bilevel scheduled sampling model that takes the sentence-level information into account and incorporates it with word-level quality. To enhance sampling diversity and improve the model's adaptability, we propose a smooth function that maps the combined result of sentence-level and word-level information to an appropriate range, and employ probabilistic sampling based on the mapped values instead of threshold truncation. Experiments conducted on the DailyDialog and PersonaChat datasets demonstrate the effectiveness of our proposed methods, which significantly alleviate the exposure bias problem and outperform state-of-the-art scheduled sampling methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate("Exposure bias poses a common challenge in numerous natural language processing tasks, particularly in dialog generation. In response to this issue, researchers have devised various techniques, among which scheduled sampling has proven to be an effective method for mitigating exposure bias. However, the existing state-of-the-art scheduled sampling methods solely consider the current sampling words' quality for threshold truncation sampling, which overlooks the importance of sentence-level information and the method of threshold truncation warrants further discussion. In this paper, we propose a bilevel scheduled sampling model that takes the sentence-level information into account and incorporates it with word-level quality. To enhance sampling diversity and improve the model's adaptability, we propose a smooth function that maps the combined result of sentence-level and word-level information to an appropriate range, and employ probabilistic sampling based on the mapped values instead of threshold truncation. Experiments conducted on the DailyDialog and PersonaChat datasets demonstrate the effectiveness of our proposed methods, which significantly alleviate the exposure bias problem and outperform state-of-the-art scheduled sampling methods.")]Here's the translation:<<SYS>>交叉偏见是许多自然语言处理任务中的常见挑战，尤其是对话生成。为了解决这个问题，研究人员已经提出了多种技术，其中规则采样已经被证明是有效的方法来减少交叉偏见。然而，现有的状态艺术规则采样方法只考虑当前采样词语的质量，忽略了句子水平信息，这种方法不充分考虑句子级别的信息和规则采样的问题。在这篇论文中，我们提出了一种两级规则采样模型，该模型考虑了句子水平信息，并将其与单词水平信息结合。为了增强采样多样性和模型适应性，我们提出了一种缓动函数，将合并的句子水平和单词水平信息映射到适当的范围内，然后使用概率采样基于映射值而不是阈值 truncation。经过 DailyDialog 和 PersonaChat 数据集的实验，我们的提议方法显示效果，可以减少交叉偏见问题，并在现有的规则采样方法中具有优势。
</details></li>
</ul>
<hr>
<h2 id="TODM-Train-Once-Deploy-Many-Efficient-Supernet-Based-RNN-T-Compression-For-On-device-ASR-Models"><a href="#TODM-Train-Once-Deploy-Many-Efficient-Supernet-Based-RNN-T-Compression-For-On-device-ASR-Models" class="headerlink" title="TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models"></a>TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01947">http://arxiv.org/abs/2309.01947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Shangguan, Haichuan Yang, Danni Li, Chunyang Wu, Yassir Fathullah, Dilin Wang, Ayushi Dalmia, Raghuraman Krishnamoorthi, Ozlem Kalinli, Junteng Jia, Jay Mahadeokar, Xin Lei, Mike Seltzer, Vikas Chandra</li>
<li>for: 这篇论文的目的是提出一个名为TODM（Train Once Deploy Many）的新方法，用于快速训练适合不同硬件的实时语音识别（ASR）模型，并且可以与单一训练作业相比减少训练时间和资源。</li>
<li>methods: 这篇论文使用了以往的Supernet研究，将RNN-T模型的层级和宽度缩减为更小的subnetworks，以适应不同的硬件类型。此外，论文还提出了三种技术来提高TODM Supernet的效果：适应性Dropout、对Alpha-divergence知识传递和Scale Adam优化器。</li>
<li>results: 论文通过比较Supernet训练 versus个别调整Multi-Head State Space Model (MH-SSM) RNN-T使用LibriSpeech数据库，发现TODM Supernet可以与手动调整模型相比，在字元错误率（WER）上提高至3%以上的表现，而且可以快速地训练多个模型，并且仅需小于单一训练作业的训练时间和资源。<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) models need to be optimized for specific hardware before they can be deployed on devices. This can be done by tuning the model's hyperparameters or exploring variations in its architecture. Re-training and re-validating models after making these changes can be a resource-intensive task. This paper presents TODM (Train Once Deploy Many), a new approach to efficiently train many sizes of hardware-friendly on-device ASR models with comparable GPU-hours to that of a single training job. TODM leverages insights from prior work on Supernet, where Recurrent Neural Network Transducer (RNN-T) models share weights within a Supernet. It reduces layer sizes and widths of the Supernet to obtain subnetworks, making them smaller models suitable for all hardware types. We introduce a novel combination of three techniques to improve the outcomes of the TODM Supernet: adaptive dropouts, an in-place Alpha-divergence knowledge distillation, and the use of ScaledAdam optimizer. We validate our approach by comparing Supernet-trained versus individually tuned Multi-Head State Space Model (MH-SSM) RNN-T using LibriSpeech. Results demonstrate that our TODM Supernet either matches or surpasses the performance of manually tuned models by up to a relative of 3% better in word error rate (WER), while efficiently keeping the cost of training many models at a small constant.
</details>
<details>
<summary>摘要</summary>
自动话语识别（ASR）模型需要根据特定硬件进行优化，以便在设备上部署。这可以通过调整模型的超参数或探索其结构的变化来实现。然而，在进行这些变化后，需要重新训练和验证模型，这可能会占用资源。本文介绍了一种新的方法—— Train Once Deploy Many（TODM），可以高效地在不同硬件类型上训练多个适合硬件的语音识别模型，并且与单个训练任务相比，它的GPU时间相同。TODM利用了先前的Supernet研究，在Supernet中，Recurrent Neural Network Transducer（RNN-T）模型共享权重。它采用了减小Supernet层数和宽度，从而得到了适合所有硬件类型的子网络，这些子网络是小型模型。我们介绍了一种新的组合技术，包括适应性Dropout、在位Alpha-分布知识继承和Scale Adam优化器，以提高TODM Supernet的结果。我们通过对Supernet训练 versus 手动调整Multi-Head State Space Model（MH-SSM）RNN-T使用LibriSpeech进行比较，结果表明，我们的TODM Supernet可以与手动调整模型相比，在字节错误率（WER）方面提高到3%之间的Relative。同时，我们efficient地保持了训练多个模型的成本，占用小的常量。
</details></li>
</ul>
<hr>
<h2 id="QuantEase-Optimization-based-Quantization-for-Language-Models-–-An-Efficient-and-Intuitive-Algorithm"><a href="#QuantEase-Optimization-based-Quantization-for-Language-Models-–-An-Efficient-and-Intuitive-Algorithm" class="headerlink" title="QuantEase: Optimization-based Quantization for Language Models – An Efficient and Intuitive Algorithm"></a>QuantEase: Optimization-based Quantization for Language Models – An Efficient and Intuitive Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01885">http://arxiv.org/abs/2309.01885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya Keerthi, Rahul Mazumder</li>
<li>for: 本研究针对大型自然语言模型（LLMs）的快速部署实现了压缩技术，尤其是Post-Training Quantization（PTQ）。</li>
<li>methods: 本研究提出了一个层别压缩框架QuantEase，各层独立进行压缩，并使用了coordinate descent（CD）技术来解决非凸网络问题。</li>
<li>results: 实验结果显示，QuantEase在不同的LLMs和数据集上的误差率和零shot准确率方面具有国际级的表现，与比较方法GPTQ之间的改善为15%之间。尤其是对于具有重要权重（outliers）的情况下，我们的方法可以实现近乎3位数字的压缩，不需要非凸压缩或分组技术，与比较方法SpQR的改善为2倍以上。<details>
<summary>Abstract</summary>
With the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs. Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization. The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques. These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems. Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition. We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision. Our proposal attains state-of-the-art performance in terms of perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements up to 15% over methods such as GPTQ. Particularly noteworthy is our outlier-aware algorithm's capability to achieve near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform quantization or grouping techniques, improving upon methods such as SpQR by up to two times in terms of perplexity.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLM）的 популяр化，压缩技术的研究吸引了越来越多的关注。这项研究关注于LLM的Post-Training Quantization（PTQ）。基于最新的进展，我们提出了QuantEase，一个层 wise量化框架，其中每层都进行独立的量化。问题被定义为一个逻辑结构化非核心的优化问题，这使得我们可以基于坐标降低（CD）技术开发高质量的解决方案。这些CD基本的方法可以提供高质量的解决方案，并且具有简单的更新，只需要基于矩阵和向量的操作，不需要矩阵反射或分解。我们还探索了一种具有异常检测的变体，可以保留重要的权重（异常），并且完全保留精度。我们的提议在实验中达到了 LLM 的状态zegart 性能，包括词 Error 和零培训精度，与比如 GPTQ 的方法相比，提高了15%。特别是我们的异常检测变体可以在不同批量化或分组技术的情况下，实现 LLM 的近或下三位量化，超过 SpQR 的性能，提高了至多两倍。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.CL_2023_09_05/" data-id="clorjzl4600ajf1881n0e8lpw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.LG_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T10:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/cs.LG_2023_09_05/">cs.LG - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Superclustering-by-finding-statistically-significant-separable-groups-of-optimal-gaussian-clusters"><a href="#Superclustering-by-finding-statistically-significant-separable-groups-of-optimal-gaussian-clusters" class="headerlink" title="Superclustering by finding statistically significant separable groups of optimal gaussian clusters"></a>Superclustering by finding statistically significant separable groups of optimal gaussian clusters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02623">http://arxiv.org/abs/2309.02623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berng/GMSDB">https://github.com/berng/GMSDB</a></li>
<li>paper_authors: Oleg I. Berngardt</li>
<li>For: The paper proposes an algorithm for clustering a dataset into optimal superclusters based on the BIC criterion and statistical separability.* Methods: The algorithm consists of three stages: representing the dataset as a mixture of Gaussian distributions, estimating distances and cluster sizes using the Mahalanobis distance, and combining clusters into superclusters using the DBSCAN method with a statistical significance level.* Results: The algorithm demonstrates good results on test datasets in both noisy and noiseless situations, and can predict correct superclusters for new data based on already trained clusterer. However, the algorithm has low speed and stochastic nature, and requires a sufficiently large dataset for clustering.Here is the same information in Simplified Chinese text:* For: 文章提出一种算法，用于基于BIC criterion和统计分离性 clustering dataset。* Methods: 算法包括三个阶段：将dataset表示为一个 mixture of Gaussian distributions，使用 Mahalanobis distance 估计 distances 和 cluster size，并使用 DBSCAN 方法将 clusters 组合成 superclusters，并使用统计significance level。* Results: 算法在测试dataset上显示了良好的结果，能够预测新数据中 correct superclusters，基于已经训练好的 clusterer。然而，算法有低速度和随机性，需要一个充分大的 dataset 进行 clustering。<details>
<summary>Abstract</summary>
The paper presents the algorithm for clustering a dataset by grouping the optimal, from the point of view of the BIC criterion, number of Gaussian clusters into the optimal, from the point of view of their statistical separability, superclusters.   The algorithm consists of three stages: representation of the dataset as a mixture of Gaussian distributions - clusters, which number is determined based on the minimum of the BIC criterion; using the Mahalanobis distance, to estimate the distances between the clusters and cluster sizes; combining the resulting clusters into superclusters using the DBSCAN method by finding its hyperparameter (maximum distance) providing maximum value of introduced matrix quality criterion at maximum number of superclusters. The matrix quality criterion corresponds to the proportion of statistically significant separated superclusters among all found superclusters.   The algorithm has only one hyperparameter - statistical significance level, and automatically detects optimal number and shape of superclusters based of statistical hypothesis testing approach. The algorithm demonstrates a good results on test datasets in noise and noiseless situations. An essential advantage of the algorithm is its ability to predict correct supercluster for new data based on already trained clusterer and perform soft (fuzzy) clustering. The disadvantages of the algorithm are: its low speed and stochastic nature of the final clustering. It requires a sufficiently large dataset for clustering, which is typical for many statistical methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Representing the dataset as a mixture of Gaussian distributions (clusters) and determining the number of clusters based on the minimum of the BIC criterion.2. Using the Mahalanobis distance to estimate the distances between the clusters and their sizes.3. Combining the resulting clusters into superclusters using the DBSCAN method by finding the maximum distance that provides the maximum value of the introduced matrix quality criterion at the maximum number of superclusters.The algorithm has only one hyperparameter, the statistical significance level, and automatically detects the optimal number and shape of superclusters based on a statistical hypothesis testing approach. The algorithm demonstrates good results on test datasets in both noisy and noise-free situations. An advantage of the algorithm is its ability to predict correct superclusters for new data based on an already trained clusterer and perform soft (fuzzy) clustering. However, the algorithm has some disadvantages, such as low speed and stochastic nature of the final clustering, and requires a sufficient dataset for clustering, which is common for many statistical methods.</details></li>
</ol>
<hr>
<h2 id="Generative-AI-aided-Joint-Training-free-Secure-Semantic-Communications-via-Multi-modal-Prompts"><a href="#Generative-AI-aided-Joint-Training-free-Secure-Semantic-Communications-via-Multi-modal-Prompts" class="headerlink" title="Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts"></a>Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02616">http://arxiv.org/abs/2309.02616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyang Du, Guangyuan Liu, Dusit Niyato, Jiayi Zhang, Jiawen Kang, Zehui Xiong, Bo Ai, Dong In Kim</li>
<li>for: 提高网络资源使用效率和实现通信目标</li>
<li>methods: 使用生成人工智能（GAI）模型，增强 semantic decoder 的可重构能力，并应用多模型提示进行精准内容解码</li>
<li>results: 实现精准内容解码和安全传输源消息，并提高网络资源使用效率<details>
<summary>Abstract</summary>
Semantic communication (SemCom) holds promise for reducing network resource consumption while achieving the communications goal. However, the computational overheads in jointly training semantic encoders and decoders-and the subsequent deployment in network devices-are overlooked. Recent advances in Generative artificial intelligence (GAI) offer a potential solution. The robust learning abilities of GAI models indicate that semantic decoders can reconstruct source messages using a limited amount of semantic information, e.g., prompts, without joint training with the semantic encoder. A notable challenge, however, is the instability introduced by GAI's diverse generation ability. This instability, evident in outputs like text-generated images, limits the direct application of GAI in scenarios demanding accurate message recovery, such as face image transmission. To solve the above problems, this paper proposes a GAI-aided SemCom system with multi-model prompts for accurate content decoding. Moreover, in response to security concerns, we introduce the application of covert communications aided by a friendly jammer. The system jointly optimizes the diffusion step, jamming, and transmitting power with the aid of the generative diffusion models, enabling successful and secure transmission of the source messages.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Algorithms-for-Fusion-of-Physics-Based-Wildfire-Spread-Models-with-Satellite-Data-for-Initializing-Wildfire-Forecasts"><a href="#Generative-Algorithms-for-Fusion-of-Physics-Based-Wildfire-Spread-Models-with-Satellite-Data-for-Initializing-Wildfire-Forecasts" class="headerlink" title="Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts"></a>Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02615">http://arxiv.org/abs/2309.02615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bshaddy/cWGAN_fire_arrival_time_inference">https://github.com/bshaddy/cWGAN_fire_arrival_time_inference</a></li>
<li>paper_authors: Bryan Shaddy, Deep Ray, Angel Farguell, Valentina Calaza, Jan Mandel, James Haley, Kyle Hilburn, Derek V. Mallia, Adam Kochanski, Assad Oberai<br>for: 这个研究的目的是开发一种基于卫星数据的高分辨率野火行为模型，以便预测野火的 sprech.methods: 这个研究使用了一种称为conditional Wasserstein Generative Adversarial Network (cWGAN)的方法，用于从卫星活动火数据中推断野火的到达时间。results: 研究发现，使用cWGAN来预测野火的到达时间可以非常准确， average Sorensen’s coefficient of 0.81 for the fire perimeters和average ignition time error of 32 minutes。<details>
<summary>Abstract</summary>
Increases in wildfire activity and the resulting impacts have prompted the development of high-resolution wildfire behavior models for forecasting fire spread. Recent progress in using satellites to detect fire locations further provides the opportunity to use measurements to improve fire spread forecasts from numerical models through data assimilation. This work develops a method for inferring the history of a wildfire from satellite measurements, providing the necessary information to initialize coupled atmosphere-wildfire models from a measured wildfire state in a physics-informed approach. The fire arrival time, which is the time the fire reaches a given spatial location, acts as a succinct representation of the history of a wildfire. In this work, a conditional Wasserstein Generative Adversarial Network (cWGAN), trained with WRF-SFIRE simulations, is used to infer the fire arrival time from satellite active fire data. The cWGAN is used to produce samples of likely fire arrival times from the conditional distribution of arrival times given satellite active fire detections. Samples produced by the cWGAN are further used to assess the uncertainty of predictions. The cWGAN is tested on four California wildfires occurring between 2020 and 2022, and predictions for fire extent are compared against high resolution airborne infrared measurements. Further, the predicted ignition times are compared with reported ignition times. An average Sorensen's coefficient of 0.81 for the fire perimeters and an average ignition time error of 32 minutes suggest that the method is highly accurate.
</details>
<details>
<summary>摘要</summary>
人类活动增加了野火的活动，并导致了一些影响。为了预测野火的扩散，人们已经开发了高分辨率野火行为模型。近年来，通过卫星探测火灾位置，可以使用测量数据进行数据吸收，以改进数字模型中的火灾扩散预测。这种工作通过卫星活动火灾数据来推算火灾历史，以便在物理学 informed 的方法下初始化气候-野火模型。火灾到达时间，即火灾到某个空间位置的时间， acted as a succinct representation of the history of a wildfire。在这种工作中，一种 conditional Wasserstein 生成 adversarial network (cWGAN) 被用来从卫星活动火灾数据中推算火灾到达时间。cWGAN 被用来生成 conditional 分布中的可能的火灾到达时间样本。这些样本被用来评估预测的不确定性。cWGAN 在加利福尼亚州2020-2022年四次野火中进行测试，并将预测的火灾范围与高分辨率空中热成像测量进行比较。此外，预测的点燃时间也与报告的点燃时间进行比较。 Sorensen 公式的平均值为0.81，表明方法的准确性很高。
</details></li>
</ul>
<hr>
<h2 id="T-SaS-Toward-Shift-aware-Dynamic-Adaptation-for-Streaming-Data"><a href="#T-SaS-Toward-Shift-aware-Dynamic-Adaptation-for-Streaming-Data" class="headerlink" title="T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data"></a>T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02610">http://arxiv.org/abs/2309.02610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijieying Ren, Tianxiang Zhao, Wei Qin, Kunpeng Liu</li>
<li>for: 这篇研究是为了解决流数据中的分布类型逐步变化问题，特别是在没有预测的情况下，数据流中突然出现分布shift。</li>
<li>methods: 这篇研究使用了一个 Bayesian 框架，名为 T-SaS，并将分布类型变化变数纳入模型中，以捕捉数据流中突然的分布shift。然后，这篇研究设计了一个可动的网络选择策略，以适应不同的分布类型。</li>
<li>results: 实验结果显示，这篇研究的方法可以优于在数据流中准确地探测分布shift的范围，并对下游预测或分类任务进行有效适应。<details>
<summary>Abstract</summary>
In many real-world scenarios, distribution shifts exist in the streaming data across time steps. Many complex sequential data can be effectively divided into distinct regimes that exhibit persistent dynamics. Discovering the shifted behaviors and the evolving patterns underlying the streaming data are important to understand the dynamic system. Existing methods typically train one robust model to work for the evolving data of distinct distributions or sequentially adapt the model utilizing explicitly given regime boundaries. However, there are two challenges: (1) shifts in data streams could happen drastically and abruptly without precursors. Boundaries of distribution shifts are usually unavailable, and (2) training a shared model for all domains could fail to capture varying patterns. This paper aims to solve the problem of sequential data modeling in the presence of sudden distribution shifts that occur without any precursors. Specifically, we design a Bayesian framework, dubbed as T-SaS, with a discrete distribution-modeling variable to capture abrupt shifts of data. Then, we design a model that enable adaptation with dynamic network selection conditioned on that discrete variable. The proposed method learns specific model parameters for each distribution by learning which neurons should be activated in the full network. A dynamic masking strategy is adopted here to support inter-distribution transfer through the overlapping of a set of sparse networks. Extensive experiments show that our proposed method is superior in both accurately detecting shift boundaries to get segments of varying distributions and effectively adapting to downstream forecast or classification tasks.
</details>
<details>
<summary>摘要</summary>
在许多实际场景中，流动数据中的分布shift存在，这些shift可能是随机的和突然的。许多复杂的顺序数据可以被有效地分解为不同的领域，每个领域都具有持续的动力学。了解流动数据中的shift和下沉pattern是理解动态系统的关键。现有方法通常是在不同的分布下训练一个Robust模型，或者采用显式给出的领域边界来逐步修改模型。然而，存在两个挑战：（1）数据流中的shift可能会发生急剧和突然，无法预测；（2）训练共享模型可能无法捕捉不同领域的变化模式。这篇论文的目的是解决流动数据中的顺序数据模型化问题，具体来说是在无前兆的分布shift下进行适应。我们提出了一种抽象 Bayesian 框架，名为T-SaS，它包含一个简单的分布模型变量，用于捕捉数据的突然shift。然后，我们设计了一种可动的网络选择conditioned于这个分布变量，以便适应不同的分布。我们的方法可以学习每个分布的特定参数，并且通过在全网络中活跃的 neuron 来确定哪些参数是有用的。我们采用了一种动态遮盾策略来支持 между分布传递，这种策略可以在不同的分布下共享一部分稀缺网络。我们的方法在 Segment 分布boundary 检测和适应下沉任务中具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Variational-Inference-for-Online-Supervised-Learning"><a href="#Distributed-Variational-Inference-for-Online-Supervised-Learning" class="headerlink" title="Distributed Variational Inference for Online Supervised Learning"></a>Distributed Variational Inference for Online Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02606">http://arxiv.org/abs/2309.02606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pptx/distributed-mapping">https://github.com/pptx/distributed-mapping</a></li>
<li>paper_authors: Parth Paritosh, Nikolay Atanasov, Sonia Martinez</li>
<li>for: 这篇论文旨在提出一种扩展可行的分布式概率推理算法，用于智能传感器网络中的推理问题。</li>
<li>methods: 该论文提出了一种分布式 probabilistic inference algorithm，可以应用于连续变量、不可解 posteriors 和大规模实时数据。在中央设置下，Variational inference 是一种基本的技术，用于perform approximate Bayesian estimation，其中一个难以求解 posterior density 被approximated 为一个参数化density。</li>
<li>results: 论文的关键贡献在于 derive 了一个分布式lower bound  на centralized estimation objective，这使得在传感器网络中进行分布式variational inference，只需一次 hop 通信。此外，论文还设计了一种在线分布式算法，用于在流动数据中进行分类和回归问题的解决，并特化为 Gaussian variational densities with non-linear likelihoods。最后，论文还 derive 了一个高维模型的 diagonalized 版本，并应用于多机器人概率地图使用indoor LiDAR数据。<details>
<summary>Abstract</summary>
Developing efficient solutions for inference problems in intelligent sensor networks is crucial for the next generation of location, tracking, and mapping services. This paper develops a scalable distributed probabilistic inference algorithm that applies to continuous variables, intractable posteriors and large-scale real-time data in sensor networks. In a centralized setting, variational inference is a fundamental technique for performing approximate Bayesian estimation, in which an intractable posterior density is approximated with a parametric density. Our key contribution lies in the derivation of a separable lower bound on the centralized estimation objective, which enables distributed variational inference with one-hop communication in a sensor network. Our distributed evidence lower bound (DELBO) consists of a weighted sum of observation likelihood and divergence to prior densities, and its gap to the measurement evidence is due to consensus and modeling errors. To solve binary classification and regression problems while handling streaming data, we design an online distributed algorithm that maximizes DELBO, and specialize it to Gaussian variational densities with non-linear likelihoods. The resulting distributed Gaussian variational inference (DGVI) efficiently inverts a $1$-rank correction to the covariance matrix. Finally, we derive a diagonalized version for online distributed inference in high-dimensional models, and apply it to multi-robot probabilistic mapping using indoor LiDAR data.
</details>
<details>
<summary>摘要</summary>
开发高效的推理解决方案是智能感知网络下一代位置跟踪和地图服务的关键。本文提出了一种可扩展的分布式概率推理算法，适用于继续变量、不可解决 posterior 和大规模实时数据。在中央化环境下，变量推理是概率推理的基本技术，用于approximate Bayesian estimation，其中一个难以解决的 posterior density 被approximated 为parametric density。我们的关键贡献在于 derive 一个可分离的下界于中央估计目标函数，这使得分布式变量推理可以在感知网络中进行一 hop 通信。我们的分布式证据下界（DELBO）是一个加权和 observation likelihood 和偏好函数之间的差异，它的差异是由consensus和modeling error 引起的。为了解决流动数据中的二分类和回归问题，我们设计了一种在线分布式算法，该算法可以最大化 DELBO，并特化为 Gaussian 变量推理函数。这导致了一种高效地减少 $1$-rank  corrections 的方法。最后，我们 deriv 了一个 диагональ 版本，用于在线分布式推理高维模型，并应用于多 robot 概率地图使用indoor LiDAR 数据。
</details></li>
</ul>
<hr>
<h2 id="Screening-of-Pneumonia-and-Urinary-Tract-Infection-at-Triage-using-TriNet"><a href="#Screening-of-Pneumonia-and-Urinary-Tract-Infection-at-Triage-using-TriNet" class="headerlink" title="Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet"></a>Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02604">http://arxiv.org/abs/2309.02604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Z. Lu<br>for: 这个论文是为了解决医疗机构的急诊室拥堵和效率下降问题而写的。methods: 这个论文使用机器学习算法来自动化急诊室的医疗指导，以提高急诊室的效率和质量。results: 论文中的TriNet模型在检测患有肺炎和慢性肾炎的病人中显示了高正确率（0.86和0.93），这些模型比现有的临床标准更高，表明机器学习医疗指导可以提供免费、不侵入的检测方式，从而降低急诊室的风险和提高医疗质量。<details>
<summary>Abstract</summary>
Due to the steady rise in population demographics and longevity, emergency department visits are increasing across North America. As more patients visit the emergency department, traditional clinical workflows become overloaded and inefficient, leading to prolonged wait-times and reduced healthcare quality. One of such workflows is the triage medical directive, impeded by limited human workload, inaccurate diagnoses and invasive over-testing. To address this issue, we propose TriNet: a machine learning model for medical directives that automates first-line screening at triage for conditions requiring downstream testing for diagnosis confirmation. To verify screening potential, TriNet was trained on hospital triage data and achieved high positive predictive values in detecting pneumonia (0.86) and urinary tract infection (0.93). These models outperform current clinical benchmarks, indicating that machine-learning medical directives can offer cost-free, non-invasive screening with high specificity for common conditions, reducing the risk of over-testing while increasing emergency department efficiency.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因人口减少和寿命增加，北美洲的急诊室访问量在增长。随着更多患者前往急诊室，传统的临床工作流程变得过载和不具有效率，导致排队时间增长，健康保健质量减退。其中一种工作流程是抢救医疗指南，受到人工负荷、不准确诊断和不必要的检测限制。为解决这一问题，我们提议TriNet：一种基于机器学习的医疗指南，自动化急诊室抢救阶段的首选检测，以确认诊断。为验证这一点，TriNet在医院急诊室数据上进行训练，在患有肺炎和尿感染的病例中达到了0.86的正确预测值，并在患有尿感染的病例中达到了0.93的正确预测值。这些模型比现有的临床标准更高，表明机器学习医疗指南可以提供免费、不侵入的检测，高度特异性 для常见的疾病，降低过测试风险，提高急诊室效率。
</details></li>
</ul>
<hr>
<h2 id="Causal-Structure-Recovery-of-Linear-Dynamical-Systems-An-FFT-based-Approach"><a href="#Causal-Structure-Recovery-of-Linear-Dynamical-Systems-An-FFT-based-Approach" class="headerlink" title="Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach"></a>Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02571">http://arxiv.org/abs/2309.02571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mishfad Shaikh Veedu, James Melbourne, Murti V. Salapaka</li>
<li>for: 本研究旨在探讨时间序列观测中的 causal 效应， especial when there are dynamical dependencies between entities.</li>
<li>methods: 我们提出了一种方法，可以减少计算复杂性为 $O(Tn^3 \log N)$，用于回归 causation 结构，从而获得频域频谱 (FD) 表示。</li>
<li>results: 我们发现，对于LTI 系统，可以使用 do-calculus 机制在 FD 中进行 causal 推理，并且可以使用 multivariate Wiener projections 实现 graph 重建，具有 $O(n)$ 复杂性。<details>
<summary>Abstract</summary>
Learning causal effects from data is a fundamental and well-studied problem across science, especially when the cause-effect relationship is static in nature. However, causal effect is less explored when there are dynamical dependencies, i.e., when dependencies exist between entities across time. Identifying dynamic causal effects from time-series observations is computationally expensive when compared to the static scenario. We demonstrate that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. Since FFT accumulates all the time dependencies on every frequency, causal inference can be performed efficiently by considering the state variables as random variables at any given frequency. We additionally show that, for systems with interactions that are LTI, do-calculus machinery can be realized in the FD resulting in versions of the classical single-door (with cycles), front and backdoor criteria. We demonstrate, for a large class of problems, graph reconstruction using multivariate Wiener projections results in a significant computational advantage with $O(n)$ complexity over reconstruction algorithms such as the PC algorithm which has $O(n^q)$ complexity, where $q$ is the maximum neighborhood size. This advantage accrues due to some remarkable properties of the phase response of the frequency-dependent Wiener coefficients which is not present in any time-domain approach.
</details>
<details>
<summary>摘要</summary>
学习 causal effects 从数据中是科学的基础问题，特别是当 causal relationship 是静态的时候。然而， causal effect 在存在时间相关性时更少研究。从时序观察数据中提取动态 causal effects 的计算复杂度比静态场景更高。我们证明了VAR 模型的 causation 结构恢复计算复杂度为 $O(Tn^3N^2)$, where $n$ 是节点数， $T$ 是样本数， $N$ 是最大时间延迟 между实体。我们报告了一种方法，计算复杂度为 $O(Tn^3 \log N)$, 恢复 causation 结构，以获得频域域 (FD) 表示。由于 FFT 积累了所有时间相关性，因此在频域中进行 causal inference 是高效的。我们还证明了，对于具有 LTI 交互的系统，do-calculus 机械可以在 FD 中实现，导致了类ical single-door (with cycles)、front 和 backdoor  criterion。我们示例了， для 一类问题，使用 multivariate Wiener projections 进行图重建可以获得 $O(n)$ 复杂度的计算优势，比 PC 算法 ($O(n^q)$ 复杂度) 更高效。这个优势来自频域 Wiener 系数的相对应性，不存在在时域方法中。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Partitioning-Around-Medoids"><a href="#Sparse-Partitioning-Around-Medoids" class="headerlink" title="Sparse Partitioning Around Medoids"></a>Sparse Partitioning Around Medoids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02557">http://arxiv.org/abs/2309.02557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars Lenssen, Erich Schubert</li>
<li>For: 这篇论文是关于分群算法，具体是使用Partitioning Around Medoids（PAM）和fastPAM方法来解决分群问题。* Methods: 这篇论文使用了PAM和fastPAM方法，并且提出了一个缓存簇数据的方法来解决分群问题。* Results: 这篇论文的结果显示了这个方法可以在实际应用中提供高品质的分群解决方案，并且可以避免过度的缓存和复杂运算。<details>
<summary>Abstract</summary>
Partitioning Around Medoids (PAM, k-Medoids) is a popular clustering technique to use with arbitrary distance functions or similarities, where each cluster is represented by its most central object, called the medoid or the discrete median. In operations research, this family of problems is also known as facility location problem (FLP). FastPAM recently introduced a speedup for large k to make it applicable for larger problems, but the method still has a runtime quadratic in N. In this chapter, we discuss a sparse and asymmetric variant of this problem, to be used for example on graph data such as road networks. By exploiting sparsity, we can avoid the quadratic runtime and memory requirements, and make this method scalable to even larger problems, as long as we are able to build a small enough graph of sufficient connectivity to perform local optimization. Furthermore, we consider asymmetric cases, where the set of medoids is not identical to the set of points to be covered (or in the interpretation of facility location, where the possible facility locations are not identical to the consumer locations). Because of sparsity, it may be impossible to cover all points with just k medoids for too small k, which would render the problem unsolvable, and this breaks common heuristics for finding a good starting condition. We, hence, consider determining k as a part of the optimization problem and propose to first construct a greedy initial solution with a larger k, then to optimize the problem by alternating between PAM-style "swap" operations where the result is improved by replacing medoids with better alternatives and "remove" operations to reduce the number of k until neither allows further improving the result quality. We demonstrate the usefulness of this method on a problem from electrical engineering, with the input graph derived from cartographic data.
</details>
<details>
<summary>摘要</summary>
分割附近中心（PAM，k-Medoids）是一种流行的聚类技术，可以用于任何距离函数或相似度，每个群由其最中央对象表示，称为中心点或离散中值。在运维研究中，这家团队的问题也称为设施位置问题（FLP）。Recently, FastPAM introduced a speedup for large k to make it applicable for larger problems, but the method still has a runtime quadratic in N. 在本章中，我们讨论了一种稀疏和不均匀的变体，用于应用于图数据，如道路网络。通过利用稀疏性，我们可以避免 quadratic runtime和内存需求，并使这种方法可扩展至更大的问题，只要我们能够构建一个足够紧凑的图，以便进行本地优化。此外，我们考虑了非对称情况，其中中心点与要覆盖的点不同。由于稀疏性，可能无法使用 too small k 覆盖所有点，这会导致问题不可解，并让常见的尝试找到好的初始状态失效。我们因此考虑在优化问题时确定 k 的部分，并提议先构建一个大于 k 的推荐解，然后通过 PAM 样式的 "交换" 操作和 "移除" 操作来优化问题，直到 neither 允许进一步提高结果质量。我们在电力工程中的一个问题上示cases the usefulness of this method.Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China and Singapore. The translation is based on the traditional Chinese characters, but the sentence structure and vocabulary have been adjusted to conform to Simplified Chinese conventions.
</details></li>
</ul>
<hr>
<h2 id="Data-Aggregation-for-Hierarchical-Clustering"><a href="#Data-Aggregation-for-Hierarchical-Clustering" class="headerlink" title="Data Aggregation for Hierarchical Clustering"></a>Data Aggregation for Hierarchical Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02552">http://arxiv.org/abs/2309.02552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elki-project/elki">https://github.com/elki-project/elki</a></li>
<li>paper_authors: Erich Schubert, Andreas Lang</li>
<li>for: 使用Hierarchical Agglomerative Clustering（HAC）进行数据 clustering，但是因为HAC需要全息距离矩阵和完整的层次结构，因此在资源受限的系统上可能会出现问题。</li>
<li>methods: 使用BETULA数据汇集算法，一种稳定的BIRCH数据汇集算法变体，来使HAC在受限资源的系统上可行，只有小的质量损失。</li>
<li>results: 可以使用BETULA数据汇集算法来实现HAC在受限资源的系统上的可行性，但是有小的质量损失。<details>
<summary>Abstract</summary>
Hierarchical Agglomerative Clustering (HAC) is likely the earliest and most flexible clustering method, because it can be used with many distances, similarities, and various linkage strategies. It is often used when the number of clusters the data set forms is unknown and some sort of hierarchy in the data is plausible. Most algorithms for HAC operate on a full distance matrix, and therefore require quadratic memory. The standard algorithm also has cubic runtime to produce a full hierarchy. Both memory and runtime are especially problematic in the context of embedded or otherwise very resource-constrained systems. In this section, we present how data aggregation with BETULA, a numerically stable version of the well known BIRCH data aggregation algorithm, can be used to make HAC viable on systems with constrained resources with only small losses on clustering quality, and hence allow exploratory data analysis of very large data sets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Generalized-Bandsplit-Neural-Network-for-Cinematic-Audio-Source-Separation"><a href="#A-Generalized-Bandsplit-Neural-Network-for-Cinematic-Audio-Source-Separation" class="headerlink" title="A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation"></a>A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02539">http://arxiv.org/abs/2309.02539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karn N. Watcharasupat, Chih-Wei Wu, Yiwei Ding, Iroro Orife, Aaron J. Hipple, Phillip A. Williams, Scott Kramer, Alexander Lerch, William Wolcott</li>
<li>for: 提取对话、音乐和特效的独立音频源</li>
<li>methods: 基于频谱分割的Bandsplit RNN模型，利用听觉学原则定义频率谱，使用1-norm损失函数和共享编码器提高分离性能</li>
<li>results: 在Divide and Remaster数据集上，模型达到了理想比例幕值以上的分离性能<details>
<summary>Abstract</summary>
Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with performance above the ideal ratio mask for the dialogue stem.
</details>
<details>
<summary>摘要</summary>
《电影式音频源分离》是一个相对较新的音频源分离子任务，目的是从它们的混合中提取对话束、音乐束和特效束。在这个工作中，我们开发了一个泛化了Bandsplit RNN模型，用于任何完整或过完整的频谱分解。基于听觉驱动的频谱缩放被用来引导频段定义，这些频段现在具有冗余性，以提高特征提取的可靠性。我们还提出了基于信号噪声比和1-norm减少准则的损失函数。此外，我们还利用了通用编码器设置中的信息共享特性，以降低训练和推断时的计算复杂度，提高对困难总结类声音的分离性能，并允许在推断时进行轻松地分解。我们的最佳模型已经将Divide and Remaster数据集的状态标准化，对对话束的性能超过理想的掩码壁。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-on-the-Probability-Simplex"><a href="#Diffusion-on-the-Probability-Simplex" class="headerlink" title="Diffusion on the Probability Simplex"></a>Diffusion on the Probability Simplex</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02530">http://arxiv.org/abs/2309.02530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Griffin Floto, Thorsteinn Jonsson, Mihai Nica, Scott Sanner, Eric Zhengyu Zhu</li>
<li>for: 生成模型学习数据分布的逆进程，创造一个生成模型。</li>
<li>methods: 使用概率 simplice 进行Diffusion，使用softmax函数应用于Ornstein-Unlenbeck过程。</li>
<li>results: 方法可以自然扩展到包括Diffusion在unit cube上，并有应用于 bounded image generation。Note: “概率 simplice” refers to the probability simplex, which is a geometric object used to represent probability distributions. “Diffusion on the unit cube” refers to a specific type of diffusion process that is applied to a cube-shaped domain, rather than a continuous space.<details>
<summary>Abstract</summary>
Diffusion models learn to reverse the progressive noising of a data distribution to create a generative model. However, the desired continuous nature of the noising process can be at odds with discrete data. To deal with this tension between continuous and discrete objects, we propose a method of performing diffusion on the probability simplex. Using the probability simplex naturally creates an interpretation where points correspond to categorical probability distributions. Our method uses the softmax function applied to an Ornstein-Unlenbeck Process, a well-known stochastic differential equation. We find that our methodology also naturally extends to include diffusion on the unit cube which has applications for bounded image generation.
</details>
<details>
<summary>摘要</summary>
diffusion 模型可以学习将数据分布中的进行进行逆转，以创建一个生成模型。然而，所希望的连续性的噪声过程可能与数据的精度有冲突。为了解决这种连续和精度之间的矛盾，我们提出了将噪声应用到概率 simpliciter 的方法。使用概率 simpliciter 自然地创造了点对应的 categorical 概率分布的解释。我们的方法使用 Ornstein-Unlenbeck 过程和 softmax 函数。我们发现我们的方法也自然地扩展到包括单位立方体上的噪声，这有应用于 bounded 图像生成。Note: "概率 simpliciter" refers to the probability simplex, which is a geometric object used to represent probability distributions. In this context, the method proposed in the text applies diffusion to the probability simplex to create a generative model.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Adversarial-Training-Does-Not-Increase-Recourse-Costs"><a href="#Adaptive-Adversarial-Training-Does-Not-Increase-Recourse-Costs" class="headerlink" title="Adaptive Adversarial Training Does Not Increase Recourse Costs"></a>Adaptive Adversarial Training Does Not Increase Recourse Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02528">http://arxiv.org/abs/2309.02528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ian Hardy, Jayanth Yetukuri, Yang Liu</li>
<li>for: 本研究旨在 investigating the effects of adaptive adversarial training on algorithmic recourse costs.</li>
<li>methods: 本研究使用了 adaptive adversarial training 方法，以对模型的Robustness和Algorithmic recourse costs 进行研究.</li>
<li>results: 研究结果显示，adaptive adversarial training 可以对模型的Robustness进行改善，但是这些改善对Algorithmic recourse costs 没有明显的影响。<details>
<summary>Abstract</summary>
Recent work has connected adversarial attack methods and algorithmic recourse methods: both seek minimal changes to an input instance which alter a model's classification decision. It has been shown that traditional adversarial training, which seeks to minimize a classifier's susceptibility to malicious perturbations, increases the cost of generated recourse; with larger adversarial training radii correlating with higher recourse costs. From the perspective of algorithmic recourse, however, the appropriate adversarial training radius has always been unknown. Another recent line of work has motivated adversarial training with adaptive training radii to address the issue of instance-wise variable adversarial vulnerability, showing success in domains with unknown attack radii. This work studies the effects of adaptive adversarial training on algorithmic recourse costs. We establish that the improvements in model robustness induced by adaptive adversarial training show little effect on algorithmic recourse costs, providing a potential avenue for affordable robustness in domains where recoursability is critical.
</details>
<details>
<summary>摘要</summary>
最近的研究已经连接了 adversarial attack 方法和 algorithmic recourse 方法：它们都寻找最小的输入实例修改，以变更模型的分类决策。已经证明，传统的 adversarial training，寻求对于黑客变化的抑制，将生成的 recourse 成本增加;  avec larger adversarial training radii 相关的 recourse 成本高于。从 algorithmic recourse 的角度来看，适当的 adversarial training radius 一直未知。另一些最近的研究将 adversarial training 与 adaptive training radii 结合，以解决实例对于黑客变化的不确定性，并在不同的实例上显示成功。这项研究 investigate 了 adaptive adversarial training 对于 algorithmic recourse 成本的影响。我们确定了 adaptive adversarial training 对于模型Robustness 的改进，对于 algorithmic recourse 成本的影响几乎无效，提供了可能的折衣预算在域内的途径。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-CPU-and-GPU-Profiling-for-Deep-Learning-Models"><a href="#Comparative-Analysis-of-CPU-and-GPU-Profiling-for-Deep-Learning-Models" class="headerlink" title="Comparative Analysis of CPU and GPU Profiling for Deep Learning Models"></a>Comparative Analysis of CPU and GPU Profiling for Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02521">http://arxiv.org/abs/2309.02521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipesh Gyawali</li>
<li>for: 这个论文是关于深度学习和机器学习应用的研究，旨在探讨 GPU 和 CPU 在训练深度神经网络时的资源分配和消耗。</li>
<li>methods: 该论文使用了 Pytorch 框架来实现深度学习项目，并对 GPU 和 CPU 的操作跟踪进行分析，以了解它们在训练深度神经网络时的资源分配和消耗。</li>
<li>results: 研究显示，在训练深度神经网络时，GPU 的运行时间比 CPU 更低，但是对于更简单的网络，GPU 与 CPU 之间并没有很大的差异。<details>
<summary>Abstract</summary>
Deep Learning(DL) and Machine Learning(ML) applications are rapidly increasing in recent days. Massive amounts of data are being generated over the internet which can derive meaningful results by the use of ML and DL algorithms. Hardware resources and open-source libraries have made it easy to implement these algorithms. Tensorflow and Pytorch are one of the leading frameworks for implementing ML projects. By using those frameworks, we can trace the operations executed on both GPU and CPU to analyze the resource allocations and consumption. This paper presents the time and memory allocation of CPU and GPU while training deep neural networks using Pytorch. This paper analysis shows that GPU has a lower running time as compared to CPU for deep neural networks. For a simpler network, there are not many significant improvements in GPU over the CPU.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）和机器学习（ML）应用在最近几年内快速增长。互联网上的巨量数据可以通过ML和DL算法提取有意义的结果。硬件资源和开源库的出现使得实现这些算法变得更加容易。TensorFlow和PyTorch是实现ML项目的主要框架之一。通过使用这些框架，我们可以跟踪CPU和GPU上执行的操作，以分析资源分配和消耗。本文 presente在训练深度神经网络时CPU和GPU的时间和内存分配。本文分析显示，在深度神经网络训练中，GPU的运行时间比CPU更低。对于简单的网络，GPU上并没有很多显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Towards-User-Guided-Actionable-Recourse"><a href="#Towards-User-Guided-Actionable-Recourse" class="headerlink" title="Towards User Guided Actionable Recourse"></a>Towards User Guided Actionable Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02517">http://arxiv.org/abs/2309.02517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayanth Yetukuri, Ian Hardy, Yang Liu</li>
<li>for: This paper aims to provide actionable recourse to negatively impacted users in machine learning models, with a focus on capturing user preferences via soft constraints.</li>
<li>methods: The paper proposes using three simple forms of soft constraints to capture user preferences: scoring continuous features, bounding feature values, and ranking categorical features. Additionally, the paper proposes a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR).</li>
<li>results: The paper conducts extensive experiments to verify the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Machine Learning's proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user's actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experiments to verify the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Deep-Learning-Models-through-Tensorization-A-Comprehensive-Survey-and-Framework"><a href="#Enhancing-Deep-Learning-Models-through-Tensorization-A-Comprehensive-Survey-and-Framework" class="headerlink" title="Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework"></a>Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02428">http://arxiv.org/abs/2309.02428</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mhelal/TensorsPyBook">https://github.com/mhelal/TensorsPyBook</a></li>
<li>paper_authors: Manal Helal</li>
<li>for: 本文旨在概述张量化，它是一种将多维数据转化为二维矩阵的方法，用于提高深度学习模型的表示和分析能力。</li>
<li>methods: 本文使用了多种多方分析方法，包括盲源分离（BSS）等，并评估了这些方法在不同领域的应用。</li>
<li>results: 研究结果表明，使用多维数据的原始形式和多方分析方法可以更好地捕捉数据中的复杂关系，同时减少模型参数数量和加速处理速度。<details>
<summary>Abstract</summary>
The burgeoning growth of public domain data and the increasing complexity of deep learning model architectures have underscored the need for more efficient data representation and analysis techniques. This paper is motivated by the work of Helal (2023) and aims to present a comprehensive overview of tensorization. This transformative approach bridges the gap between the inherently multidimensional nature of data and the simplified 2-dimensional matrices commonly used in linear algebra-based machine learning algorithms. This paper explores the steps involved in tensorization, multidimensional data sources, various multiway analysis methods employed, and the benefits of these approaches. A small example of Blind Source Separation (BSS) is presented comparing 2-dimensional algorithms and a multiway algorithm in Python. Results indicate that multiway analysis is more expressive. Contrary to the intuition of the dimensionality curse, utilising multidimensional datasets in their native form and applying multiway analysis methods grounded in multilinear algebra reveal a profound capacity to capture intricate interrelationships among various dimensions while, surprisingly, reducing the number of model parameters and accelerating processing. A survey of the multi-away analysis methods and integration with various Deep Neural Networks models is presented using case studies in different domains.
</details>
<details>
<summary>摘要</summary>
随着公共领域数据的急速增长和深度学习模型的复杂化，需要更有效的数据表示和分析技术。这篇论文是基于Helal（2023）的研究，旨在提供tensorization的全面介绍。这种转换方法 bridge了数据的自然多维性和通常用于线性代数学习算法中的简单二维矩阵之间的差异。本文探讨tensorization的过程、多维数据源、多方分析方法和其好处。此外，还提供了一个小例子， Comparing 2-dimensional算法和多方算法在Python中。结果表明，多方分析方法更加表达力。与传统的维度惩罚理论相反，使用原始多维数据和应用多方分析方法可以捕捉多维维度之间的复杂关系，同时减少模型参数的数量和加速处理。本文还提供了多方分析方法的survey和与不同领域的各种深度神经网络模型的集成。Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Monotone-Tree-Based-GAMI-Models-by-Adapting-XGBoost"><a href="#Monotone-Tree-Based-GAMI-Models-by-Adapting-XGBoost" class="headerlink" title="Monotone Tree-Based GAMI Models by Adapting XGBoost"></a>Monotone Tree-Based GAMI Models by Adapting XGBoost</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02426">http://arxiv.org/abs/2309.02426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Hu, Soroush Aramideh, Jie Chen, Vijayan N. Nair<br>for:This paper aims to develop a monotone tree-based functional ANOVA model, called monotone GAMI-Tree, to address the issue of non-monotonicity in existing GAMI models.methods:The proposed method uses a filtering technique to select important interactions, followed by fitting a monotone XGBoost algorithm with the selected interactions. The results are then parsed and purified to obtain a monotone GAMI model.results:The proposed method is demonstrated on simulated datasets and shows better performance than existing GAMI models in terms of monotonicity and accuracy. The results also show that the main effects can be monotone, but the interactions may not be monotone.<details>
<summary>Abstract</summary>
Recent papers have used machine learning architecture to fit low-order functional ANOVA models with main effects and second-order interactions. These GAMI (GAM + Interaction) models are directly interpretable as the functional main effects and interactions can be easily plotted and visualized. Unfortunately, it is not easy to incorporate the monotonicity requirement into the existing GAMI models based on boosted trees, such as EBM (Lou et al. 2013) and GAMI-Lin-T (Hu et al. 2022). This paper considers models of the form $f(x)=\sum_{j,k}f_{j,k}(x_j, x_k)$ and develops monotone tree-based GAMI models, called monotone GAMI-Tree, by adapting the XGBoost algorithm. It is straightforward to fit a monotone model to $f(x)$ using the options in XGBoost. However, the fitted model is still a black box. We take a different approach: i) use a filtering technique to determine the important interactions, ii) fit a monotone XGBoost algorithm with the selected interactions, and finally iii) parse and purify the results to get a monotone GAMI model. Simulated datasets are used to demonstrate the behaviors of mono-GAMI-Tree and EBM, both of which use piecewise constant fits. Note that the monotonicity requirement is for the full model. Under certain situations, the main effects will also be monotone. But, as seen in the examples, the interactions will not be monotone.
</details>
<details>
<summary>摘要</summary>
现在的研究论文使用机器学习建筑物来适应低阶函数ANOVA模型的主效应和次阶交互。这些GAMI（GAM + 交互）模型可以直接解释为函数主效应和交互，可以轻松地图表和可见化。然而，对于现有的GAMI模型，如EBM（Lou et al. 2013）和GAMI-Lin-T（Hu et al. 2022），不能直接包含幂随机性的要求。这篇论文考虑模型的形式为 $f(x)=\sum_{j,k}f_{j,k}(x_j, x_k)$，并开发了幂随机性基于XGBoost算法的幂随机性GAMI模型，称为幂随机性GAMI-Tree。使用XGBoost算法直接适应幂随机性模型是 straightforward。然而，适应后的模型仍然是黑盒模型。我们采用了不同的方法：i）使用筛选技术确定重要的交互，ii）适应幂随机性XGBoost算法，并iii）解析和纯化结果，以获得幂随机性GAMI模型。使用 simulate datasets 进行了示例，并证明了 mono-GAMI-Tree 和 EBM 都使用 piecewise constant fits 的行为。请注意，幂随机性要求是全模型的，而不是每个主效应或交互。在某些情况下，主效应也可能是幂随机的，但交互通常不是。
</details></li>
</ul>
<hr>
<h2 id="On-the-Minimax-Regret-in-Online-Ranking-with-Top-k-Feedback"><a href="#On-the-Minimax-Regret-in-Online-Ranking-with-Top-k-Feedback" class="headerlink" title="On the Minimax Regret in Online Ranking with Top-k Feedback"></a>On the Minimax Regret in Online Ranking with Top-k Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02425">http://arxiv.org/abs/2309.02425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Zhang, Ambuj Tewari</li>
<li>For:  Online ranking with top $k$ feedback* Methods:  Partial monitoring techniques, minimax regret rates* Results:  Full characterization of minimax regret rates for all $k$ and for Pairwise Loss, Discounted Cumulative Gain, and Precision@n, efficient algorithm for Precision@nHere is the Chinese translation of the three information points:* For:  online排名 WITH top $k$ 反馈* Methods:  partial monitoring 技术, minimax regret rates* Results:  all $k$ 和 Pairwise Loss, Discounted Cumulative Gain, Precision@n 中的完整characterization, efficient algorithm for Precision@nI hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In online ranking, a learning algorithm sequentially ranks a set of items and receives feedback on its ranking in the form of relevance scores. Since obtaining relevance scores typically involves human annotation, it is of great interest to consider a partial feedback setting where feedback is restricted to the top-$k$ items in the rankings. Chaudhuri and Tewari [2017] developed a framework to analyze online ranking algorithms with top $k$ feedback. A key element in their work was the use of techniques from partial monitoring. In this paper, we further investigate online ranking with top $k$ feedback and solve some open problems posed by Chaudhuri and Tewari [2017]. We provide a full characterization of minimax regret rates with the top $k$ feedback model for all $k$ and for the following ranking performance measures: Pairwise Loss, Discounted Cumulative Gain, and Precision@n. In addition, we give an efficient algorithm that achieves the minimax regret rate for Precision@n.
</details>
<details>
<summary>摘要</summary>
在在线排名中，一种学习算法顺序排序一组项目，并接收反馈分数以评估其排名的有用性。由于获取反馈分数通常需要人工标注，因此受限于top-$k$反馈的情况是非常有趣。查得和特ва里[2017]提出了在线排名算法的框架，并使用partial monitoring技术进行分析。在这篇论文中，我们进一步调查了在线排名的top-$k$反馈问题，并解决了查得和特wa里[2017]提出的一些开放问题。我们提供了所有$k$的最小最大偏差率的完整 характеристику，以及对Pairwise Loss、Discounted Cumulative Gain和Precision@n的排名性能指标进行分析。此外，我们还提供了实现最小最大偏差率的高效算法。
</details></li>
</ul>
<hr>
<h2 id="Maximum-Mean-Discrepancy-Meets-Neural-Networks-The-Radon-Kolmogorov-Smirnov-Test"><a href="#Maximum-Mean-Discrepancy-Meets-Neural-Networks-The-Radon-Kolmogorov-Smirnov-Test" class="headerlink" title="Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test"></a>Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02422">http://arxiv.org/abs/2309.02422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghoon Paik, Michael Celentano, Alden Green, Ryan J. Tibshirani</li>
<li>For: The paper is written to introduce a new test for comparing two distributions, called the Radon-Kolmogorov-Smirnov (RKS) test, which is based on the concept of Radon bounded variation (RBV) and has applications in deep learning.* Methods: The RKS test uses the unit ball in the RBV space of a given smoothness order $k \geq 0$ as the function space $\mathcal{F}$ and maximizes the mean difference over samples from one distribution $P$ versus another $Q$. The test is related to neural networks and can be optimized using modern deep learning toolkits.* Results: The paper proves that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not&#x3D; Q$ of distributions, derives its asymptotic null distribution, and conducts extensive experiments to evaluate the strengths and weaknesses of the RKS test compared to the more traditional kernel MMD test.Here is the information in Simplified Chinese:</li>
<li>for: 这篇论文是为了介绍一种新的分布比较测试方法，称为Radon-Kolmogorov-Smirnov（RKS）测试，它基于Radon bounded variation（RBV）的概念，有应用于深度学习。</li>
<li>methods: RKS测试使用 unit ball在RBV空间中的smoothness order $k \geq 0$作为函数空间$\mathcal{F}$，对一个分布$P$和另一个分布$Q$的样本进行最大化mean difference。测试与神经网络有关，可以使用现代深度学习工具包来（近似地）优化测试的 критериion。</li>
<li>results: 论文证明RKS测试在任意不同的分布$P \not&#x3D; Q$中具有极大的权重， derivation of its asymptotic null distribution, 并进行了广泛的实验来评估RKS测试与传统的kernel MMD测试的优劣点。<details>
<summary>Abstract</summary>
Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. This allows us to leverage the power of modern deep learning toolkits to (approximately) optimize the criterion that underlies the RKS test. We prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out extensive experiments to elucidate the strengths and weakenesses of the RKS test versus the more traditional kernel MMD test.
</details>
<details>
<summary>摘要</summary>
“最大均值差（MMD）是一种通用的非参数TwoSample测试，它基于对一个分布$P$和另一个分布$Q$的样本之间的均值差进行最大化，而且对所有的数据变换$f$生成在一个函数空间$\mathcal{F}$中。 drawing inspiration from recent work that connects functions of Radon bounded variation (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. this test, which we refer to as the Radon-Kolmogorov-Smirnov (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. it is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. this allows us to leverage the power of modern deep learning toolkits to (approximately) optimize the criterion that underlies the RKS test. we prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out extensive experiments to elucidate the strengths and weaknesses of the RKS test versus the more traditional kernel MMD test.”Note: "Simplified Chinese" is a translation of the text into traditional Chinese characters, but with simpler grammar and vocabulary to make it easier to read and understand.
</details></li>
</ul>
<hr>
<h2 id="Computing-SHAP-Efficiently-Using-Model-Structure-Information"><a href="#Computing-SHAP-Efficiently-Using-Model-Structure-Information" class="headerlink" title="Computing SHAP Efficiently Using Model Structure Information"></a>Computing SHAP Efficiently Using Model Structure Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02417">http://arxiv.org/abs/2309.02417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Hu, Ke Wang</li>
<li>for: 本研究旨在提高 SHAP（SHapley Additive exPlanations）计算的效率，因为现有的方法具有几乎 exponential 时间复杂度。</li>
<li>methods: 本文提出了三种方法来计算 SHAP，包括：1) 基于函数分解的方法，可以在 polynomial 时间内计算 SHAP; 2) 基于模型结构信息的方法，可以在 polynomial 时间内计算 SHAP; 3) 基于迭代方法的方法，可以用于 unknown 模型结构情况下。</li>
<li>results: 三种方法均可以准确计算 SHAP，并且 computationally efficient。与 Castor &amp; Gomez (2008) 的采样方法进行比较，本文的方法在几乎所有情况下具有更高的效率。<details>
<summary>Abstract</summary>
SHAP (SHapley Additive exPlanations) has become a popular method to attribute the prediction of a machine learning model on an input to its features. One main challenge of SHAP is the computation time. An exact computation of Shapley values requires exponential time complexity. Therefore, many approximation methods are proposed in the literature. In this paper, we propose methods that can compute SHAP exactly in polynomial time or even faster for SHAP definitions that satisfy our additivity and dummy assumptions (eg, kernal SHAP and baseline SHAP). We develop different strategies for models with different levels of model structure information: known functional decomposition, known order of model (defined as highest order of interaction in the model), or unknown order. For the first case, we demonstrate an additive property and a way to compute SHAP from the lower-order functional components. For the second case, we derive formulas that can compute SHAP in polynomial time. Both methods yield exact SHAP results. Finally, if even the order of model is unknown, we propose an iterative way to approximate Shapley values. The three methods we propose are computationally efficient when the order of model is not high which is typically the case in practice. We compare with sampling approach proposed in Castor & Gomez (2008) using simulation studies to demonstrate the efficacy of our proposed methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="First-and-zeroth-order-implementations-of-the-regularized-Newton-method-with-lazy-approximated-Hessians"><a href="#First-and-zeroth-order-implementations-of-the-regularized-Newton-method-with-lazy-approximated-Hessians" class="headerlink" title="First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians"></a>First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02412">http://arxiv.org/abs/2309.02412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Doikov, Geovani Nunes Grapiglia</li>
<li>for:  solving general non-convex optimization problems</li>
<li>methods:  Cubically regularized Newton method with finite difference approximations of the derivatives, and an adaptive search procedure that simultaneously fits the regularization constant and the parameters of the finite difference approximations</li>
<li>results:  global complexity bound of $\mathcal{O}( n^{1&#x2F;2} \epsilon^{-3&#x2F;2})$ function and gradient evaluations for the Hessian-free method, and a bound of $\mathcal{O}( n^{3&#x2F;2} \epsilon^{-3&#x2F;2} )$ function evaluations for the derivative-free method, which significantly improve the previously known ones in terms of the joint dependence on $n$ and $\epsilon$.<details>
<summary>Abstract</summary>
In this work, we develop first-order (Hessian-free) and zero-order (derivative-free) implementations of the Cubically regularized Newton method for solving general non-convex optimization problems. For that, we employ finite difference approximations of the derivatives. We use a special adaptive search procedure in our algorithms, which simultaneously fits both the regularization constant and the parameters of the finite difference approximations. It makes our schemes free from the need to know the actual Lipschitz constants. Additionally, we equip our algorithms with the lazy Hessian update that reuse a previously computed Hessian approximation matrix for several iterations. Specifically, we prove the global complexity bound of $\mathcal{O}( n^{1/2} \epsilon^{-3/2})$ function and gradient evaluations for our new Hessian-free method, and a bound of $\mathcal{O}( n^{3/2} \epsilon^{-3/2} )$ function evaluations for the derivative-free method, where $n$ is the dimension of the problem and $\epsilon$ is the desired accuracy for the gradient norm. These complexity bounds significantly improve the previously known ones in terms of the joint dependence on $n$ and $\epsilon$, for the first-order and zeroth-order non-convex optimization.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们开发了一种基于第一阶 Taylor 展开的卷积规范 Newton 方法，用于解决普通非凸优化问题。我们使用贝叶斯适应搜索算法，以同时调整规范常数和finite differenceapproximation的参数。这使得我们的算法不需要知道实际的Lipschitz常数。此外，我们还在我们的算法中使用懒散Hessian更新，使用已经计算过的Hessian近似矩阵进行多个迭代。我们证明了我们新的Hessian-free方法的全局复杂度上下文为 $\mathcal{O}(n^{1/2} \epsilon^{-3/2})$ 函数和梯度评估数，而derivative-free方法的复杂度上下文为 $\mathcal{O}(n^{3/2} \epsilon^{-3/2})$ 函数评估数，其中 $n$ 是优化问题的维度， $\epsilon$ 是梯度norm的desired accuracy。这些复杂度上下文在之前已知的joint $n$ 和 $\epsilon$ 的依赖关系方面具有显著改进。
</details></li>
</ul>
<hr>
<h2 id="Delta-LoRA-Fine-Tuning-High-Rank-Parameters-with-the-Delta-of-Low-Rank-Matrices"><a href="#Delta-LoRA-Fine-Tuning-High-Rank-Parameters-with-the-Delta-of-Low-Rank-Matrices" class="headerlink" title="Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices"></a>Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02411">http://arxiv.org/abs/2309.02411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, Lei Zhang</li>
<li>for: 本文提出了Delta-LoRA，一种基于大语言模型的参数高效的细化方法。</li>
<li>methods: Delta-LoRA不仅更新低级矩阵 $\bA$ 和 $\bB$，还通过更新 $\bW$ 中的差值（$\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$）来进行学习。</li>
<li>results: 对比LoRA和其他低级适应方法，Delta-LoRA在下游任务中表现出色，并且与LoRA相比，Delta-LoRA具有相同的内存需求和计算成本。<details>
<summary>Abstract</summary>
In this paper, we present Delta-LoRA, which is a novel parameter-efficient approach to fine-tune large language models (LLMs). In contrast to LoRA and other low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates the low-rank matrices $\bA$ and $\bB$, but also propagate the learning to the pre-trained weights $\bW$ via updates utilizing the delta of the product of two low-rank matrices ($\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$). Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks. Moreover, as the update of $\bW$ does not need to compute the gradients of $\bW$ and store their momentums, Delta-LoRA shares comparable memory requirements and computational costs with LoRA. Extensive experiments show that Delta-LoRA significantly outperforms existing low-rank adaptation methods. We further support these results with comprehensive analyses that underscore the effectiveness of Delta-LoRA.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了Delta-LoRA，它是一种新的参数效率的方法，用于精细调整大型自然语言模型（LLM）。与LoRA和其他低级权 adaptation方法相比，Delta-LoRA不仅更新了低级矩阵$\bA$和$\bB$,还通过使用两个低级矩阵的乘积 delta（$\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$）来升级学习。这种策略有效地解决了低级矩阵逐步更新的限制，使得模型能够更好地适应下游任务。此外，由于更新 $\bW$ 不需要计算 $\bW$ 的梯度和存储它们的动量，Delta-LoRA 与 LoRA 的内存需求和计算成本相同。实验表明，Delta-LoRA 显著超过了现有的低级权 adaptation 方法。我们还提供了全面的分析，以证明 Delta-LoRA 的效果。
</details></li>
</ul>
<hr>
<h2 id="In-Ear-Voice-Towards-Milli-Watt-Audio-Enhancement-With-Bone-Conduction-Microphones-for-In-Ear-Sensing-Platforms"><a href="#In-Ear-Voice-Towards-Milli-Watt-Audio-Enhancement-With-Bone-Conduction-Microphones-for-In-Ear-Sensing-Platforms" class="headerlink" title="In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms"></a>In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02393">http://arxiv.org/abs/2309.02393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Schilk, Niccolò Polvani, Andrea Ronco, Milos Cernak, Michele Magno</li>
<li>for: 这个论文是为了解决远程会议中音频质量受到扰乱的问题而写的。</li>
<li>methods: 这篇论文使用了新型的MEMS骨传导 Microphone，并采用了个性化语音活动检测算法和循环神经网络来解决问题。</li>
<li>results: 论文评估了一种基于骨传导数据的低功耗个性化语音检测算法，并与传统麦克风输入的方法进行比较。实验结果显示，骨传导系统可以在12.8毫秒内正确地检测到语音，并且具有43小时的电池寿命。<details>
<summary>Abstract</summary>
The recent ubiquitous adoption of remote conferencing has been accompanied by omnipresent frustration with distorted or otherwise unclear voice communication. Audio enhancement can compensate for low-quality input signals from, for example, small true wireless earbuds, by applying noise suppression techniques. Such processing relies on voice activity detection (VAD) with low latency and the added capability of discriminating the wearer's voice from others - a task of significant computational complexity. The tight energy budget of devices as small as modern earphones, however, requires any system attempting to tackle this problem to do so with minimal power and processing overhead, while not relying on speaker-specific voice samples and training due to usability concerns.   This paper presents the design and implementation of a custom research platform for low-power wireless earbuds based on novel, commercial, MEMS bone-conduction microphones. Such microphones can record the wearer's speech with much greater isolation, enabling personalized voice activity detection and further audio enhancement applications. Furthermore, the paper accurately evaluates a proposed low-power personalized speech detection algorithm based on bone conduction data and a recurrent neural network running on the implemented research platform. This algorithm is compared to an approach based on traditional microphone input. The performance of the bone conduction system, achieving detection of speech within 12.8ms at an accuracy of 95\% is evaluated. Different SoC choices are contrasted, with the final implementation based on the cutting-edge Ambiq Apollo 4 Blue SoC achieving 2.64mW average power consumption at 14uJ per inference, reaching 43h of battery life on a miniature 32mAh li-ion cell and without duty cycling.
</details>
<details>
<summary>摘要</summary>
现代无线耳机的普遍采用导致了远程会议中的声音扭曲或不清楚的问题，而声音提升技术可以补做小质量输入信号的问题。这种处理需要具备快速响应和可区分戴户的声音和其他声音的能力，但是由于设备的能量限制，系统不能依赖于特定的说话人样本和训练。本文描述了一种自定义研究平台，基于新型商业MEMS骨传声 microphone，用于低功耗无线耳机。这种 microphone 可以更好地隔离戴户的声音，以便实现个性化声音活动检测和其他声音提升应用。此外，文章详细评估了一种基于骨传声数据和回归神经网络的低功耗个性化声音检测算法。这种算法与传统 микрофон输入的方法进行比较，并评估了骨传声系统的性能，包括检测speech within 12.8ms 的精度为 95%。文章还对不同的SoC选择进行了比较，最终实现基于Ambiq Apollo 4 Blue SoC的实现，占用2.64mW的平均电力consumption，可以达到32mAh电池的43h寿命，无需循环停机。
</details></li>
</ul>
<hr>
<h2 id="Explaining-grokking-through-circuit-efficiency"><a href="#Explaining-grokking-through-circuit-efficiency" class="headerlink" title="Explaining grokking through circuit efficiency"></a>Explaining grokking through circuit efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02390">http://arxiv.org/abs/2309.02390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, Ramana Kumar</li>
<li>for: 本研究探讨了神经网络的总结和泛化问题，具体来说是 Grokking 现象，即训练准确率很高 yet 泛化率很低的网络，在进一步训练后会转变为泛化率很高 yet 训练准确率很低。</li>
<li>methods: 作者提出了一种解释 Grokking 现象的假设，即任务存在一个泛化解决方案和一个记忆解决方案，其中泛化解决方案 slower to learn 但更高效，生成更大的 logits 与参数 нор 相同。作者还提出了四个新预测，并证明了其中的四个。</li>
<li>results: 作者通过实验证明了他们的假设，并发现了两种新的行为：ungrokking 和 semi-grokking。ungrokking 是指网络从完美测试率下降到低测试率的现象，而 semi-grokking 是指网络在部分测试数据上显示延迟的泛化行为。<details>
<summary>Abstract</summary>
One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.
</details>
<details>
<summary>摘要</summary>
一种非常有趣的神经网络泛化问题是“grokking”：一个网络在完美训练后却表现出低泛化性能。我们提出，grokking发生在任务允许一个泛化解决方案和一个记忆解决方案，其中泛化解决方案需要更长的时间学习，但生成更大的logits，同样的参数 нор。我们假设记忆化环路在更大的训练数据集上变得更不效率，而泛化环路不变。我们提出四个新预测，并证明了这些预测。最引人注目的是我们发现了两种新的行为：ungrokking和半泛化。ungrokking是指一个网络在完美训练后却降低到低测试精度，而半泛化是指一个网络在部分测试数据上显示延迟的泛化。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-and-Transferable-Design-for-Robust-LEGO-Manipulation"><a href="#A-Lightweight-and-Transferable-Design-for-Robust-LEGO-Manipulation" class="headerlink" title="A Lightweight and Transferable Design for Robust LEGO Manipulation"></a>A Lightweight and Transferable Design for Robust LEGO Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02354">http://arxiv.org/abs/2309.02354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixuan Liu, Yifan Sun, Changliu Liu</li>
<li>for: 这篇论文是研究如何实现安全高效的机器人LEGO拼接。</li>
<li>methods: 本论文使用硬件软件共设计，设计了一个终端工具（EOAT），以便大型工业机器人轻松地拼接LEGO块。此外，本论文使用进化策略安全地优化机器人运动，以实现100%的成功率。</li>
<li>results: 实验表明，EOAT可靠地进行LEGO拼接，而学习框架可以安全地提高拼接性能，达到100%的成功率。另外，本解决方案在多个机器人（FANUC LR-mate 200id&#x2F;7L和Yaskawa GP4）上进行了多机器人扩展和传输性测试，以证明其通用性和可重复性。最后，我们表明了该解决方案可以实现可持续的机器人LEGO拼接，机器人可以重复地组装和解组不同的原型。<details>
<summary>Abstract</summary>
LEGO is a well-known platform for prototyping pixelized objects. However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement. This paper investigates safe and efficient robotic LEGO manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks. In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation. Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100\% success rate. The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the proposed solution enables sustainable robotic LEGO prototyping, in which the robot can repeatedly assemble and disassemble different prototypes.
</details>
<details>
<summary>摘要</summary>
LEGO 是一个知名的原型平台，但是机器人 LEGO 拼接 (i.e. 拼接 LEGO 块) 具有挑战性，主要是因为连接紧密和精度要求高。这篇论文 investigate 安全和高效的机器人 LEGO 拼接方法。特别是这篇论文通过硬件软件共设计来降低拼接复杂性。一个终端工具 (EOAT) 被设计，可以减少问题维度，使大型工业机器人更容易地拼接 LEGO 块。此外，这篇论文使用进化策略来安全地优化机器人运动，以达到100% 的成功率。实验表明，EOAT 可靠地 manipulating LEGO 块，并且学习框架可以效果地提高拼接性能。 co-design 被部署到多个机器人 (i.e. FANUC LR-mate 200id/7L 和 Yaskawa GP4)，以示其通用性和传递性。最后，我们示出了我们的解决方案可以实现可持续的机器人 LEGO 拼接，机器人可以重复地组装和解组不同的原型。
</details></li>
</ul>
<hr>
<h2 id="Exact-Inference-for-Continuous-Time-Gaussian-Process-Dynamics"><a href="#Exact-Inference-for-Continuous-Time-Gaussian-Process-Dynamics" class="headerlink" title="Exact Inference for Continuous-Time Gaussian Process Dynamics"></a>Exact Inference for Continuous-Time Gaussian Process Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02351">http://arxiv.org/abs/2309.02351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Ensinger, Nicholas Tagliapietra, Sebastian Ziesche, Sebastian Trimpe</li>
<li>for: 提取真实连续时间系统的GP模型</li>
<li>methods: 利用多步和泰勒积分器，实现直接GP推理</li>
<li>results: 实验和理论表明，该方法可以准确地表示连续时间系统的GP模型<details>
<summary>Abstract</summary>
Physical systems can often be described via a continuous-time dynamical system. In practice, the true system is often unknown and has to be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved. Thus, we aim for a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. Many higher-order integrators require dynamics evaluations at intermediate time steps making exact GP inference intractable. In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. In order to make direct inference tractable, we propose to leverage multistep and Taylor integrators. We demonstrate how to derive flexible inference schemes for these types of integrators. Further, we derive tailored sampling schemes that allow to draw consistent dynamics functions from the learned posterior. This is crucial to sample consistent predictions from the dynamics model. We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.
</details>
<details>
<summary>摘要</summary>
Physical systems can often be described using a continuous-time dynamical system. However, the true system is often unknown and must be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in certain scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties must be conserved. Therefore, we aim to learn a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. However, many higher-order integrators require dynamics evaluations at intermediate time steps, making exact GP inference intractable.In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. To make direct inference tractable, we propose to leverage multistep and Taylor integrators. We derive how to derive flexible inference schemes for these types of integrators. Additionally, we derive tailored sampling schemes that allow drawing consistent dynamics functions from the learned posterior. This is crucial to sample consistent predictions from the dynamics model.We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.Note: The text is translated into Simplified Chinese, which is a standardized form of Chinese used in mainland China and Singapore. The translation is written in Traditional Chinese characters, which is the standard form of Chinese used in Taiwan and other countries.这些physical systems可以用一个连续时间的动力系统来描述。然而，真正的系统是未知的，需要从测量数据学习。由于数据通常是在离散时间收集的，例如由感应器收集，因此大多数GP动力系统学习方法是在一步之前预测。这可能会在某些情况下问题，例如测量是在不规则时间步进行的或物理系统特性需要保持。因此，我们的目标是学习一个GP模型的真正连续时间系统。高级数字积分器提供了必要的工具来解决这个问题。然而，许多高级积分器需要在中途时间步进行动力评估，使得GP数据 posterior 不可靠。在过去的工作中，这个问题通常是通过将GP posterior 近似为多标量数据学习的方法来解决。然而，精确的GP数据 posterior 是在许多情况下更好的，例如因为它具有数学保证。为了让直接推理可行，我们提议使用多步和泰勒积分器。我们 derivation 了如何 derivation  flexible inference schemes for these types of integrators. 此外，我们也 derivation 了适合的抽样方案，允许从学习的 posterior 中获得一致的动力函数。这是关键的，以获得一致的预测。我们在实验和理论上证明了我们的方法可以实现一个精确的连续时间系统表示。
</details></li>
</ul>
<hr>
<h2 id="PolyLUT-Learning-Piecewise-Polynomials-for-Ultra-Low-Latency-FPGA-LUT-based-Inference"><a href="#PolyLUT-Learning-Piecewise-Polynomials-for-Ultra-Low-Latency-FPGA-LUT-based-Inference" class="headerlink" title="PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference"></a>PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02334">http://arxiv.org/abs/2309.02334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Andronic, George A. Constantinides</li>
<li>for: 这个论文是为了提高深度学习推理的启动速度和面积使用Field-programmable gate arrays (FPGAs)的实现。</li>
<li>methods: 这个论文提出了一种使用多ivariate polynomials作为深度学习模型的基本建置物件，并将这些多ivariate polynomials hide在FPGA的Lookup Table (LUTs)中，以避免额外的负载。</li>
<li>results: 这个论文显示了使用多ivariate polynomials可以实现相同的准确性，但是使用许多 fewer layers of soft logic，从而获得了显著的启动速度和面积改善。这个方法在三个任务中得到了证明：网络入侵检测、CERN大 HadronCollider上的戳机识别和MNIST datasets上的手写数字识别。<details>
<summary>Abstract</summary>
Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. We demonstrate the effectiveness of this approach in three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and handwritten digit recognition using the MNIST dataset.
</details>
<details>
<summary>摘要</summary>
Field-programmable gate arrays (FPGAs) 广泛用于深度学习推理实现。标准深度神经网络推理包括交叠的线性映射和非线性活化函数的计算。先前的工作对于超低延迟实现做出了硬编码了线性映射和非线性活化函数在FPGALookup表（LUTs）中的方法。我们的工作是基于FPGA LUTs 可以实现更多种函数的想法。在这篇论文中，我们提出了一种新的方法，使用多变量多项式作为神经网络训练的基本构件。我们的方法利用FPGA soft logic 的灵活性，将多项式评估隐藏在LUTs 中，无损失。我们显示，使用多项式构件可以与使用线性函数相比，实现同等准确性，但是具有显著的延迟和面积改进。我们在三个任务中证明了这种方法的有效性：网络入侵检测、在CERN大 адроン卫星中的戳彩识别和使用MNIST数据集的手写数字识别。
</details></li>
</ul>
<hr>
<h2 id="Resilient-VAE-Unsupervised-Anomaly-Detection-at-the-SLAC-Linac-Coherent-Light-Source"><a href="#Resilient-VAE-Unsupervised-Anomaly-Detection-at-the-SLAC-Linac-Coherent-Light-Source" class="headerlink" title="Resilient VAE: Unsupervised Anomaly Detection at the SLAC Linac Coherent Light Source"></a>Resilient VAE: Unsupervised Anomaly Detection at the SLAC Linac Coherent Light Source</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02333">http://arxiv.org/abs/2309.02333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Humble, William Colocho, Finn O’Shea, Daniel Ratner, Eric Darve</li>
<li>for: 这篇论文旨在应用深度学习进行异常检测，但现有方法假设有正常训练集（i.e., 无异常数据）或完全标签训练集。在复杂的工程系统中，例如粒子加速器，标签是罕见和昂贵的；为了进行异常检测在这些情况下，我们必须搁置这些假设，并使用完全无监督的方法。</li>
<li>methods: 这篇论文提出了具有抗衰变性的自适应器（ResVAE），一种深度生成模型，用于异常检测。ResVAE在训练过程中学习每个样本的异常可能性，以及每个个别特征的异常可能性，并将这些可能性用于有效地忽略在训练数据中的异常样本。</li>
<li>results: 这篇论文应用ResVAE进行了加速器异常检测，并使用了射测系统中的枪位监控数据。 results show that ResVAE exhibits exceptional ability in identifying various types of anomalies present in the accelerator, and provides feature-level anomaly attribution.<details>
<summary>Abstract</summary>
Significant advances in utilizing deep learning for anomaly detection have been made in recent years. However, these methods largely assume the existence of a normal training set (i.e., uncontaminated by anomalies) or even a completely labeled training set. In many complex engineering systems, such as particle accelerators, labels are sparse and expensive; in order to perform anomaly detection in these cases, we must drop these assumptions and utilize a completely unsupervised method. This paper introduces the Resilient Variational Autoencoder (ResVAE), a deep generative model specifically designed for anomaly detection. ResVAE exhibits resilience to anomalies present in the training data and provides feature-level anomaly attribution. During the training process, ResVAE learns the anomaly probability for each sample as well as each individual feature, utilizing these probabilities to effectively disregard anomalous examples in the training data. We apply our proposed method to detect anomalies in the accelerator status at the SLAC Linac Coherent Light Source (LCLS). By utilizing shot-to-shot data from the beam position monitoring system, we demonstrate the exceptional capability of ResVAE in identifying various types of anomalies that are visible in the accelerator.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Significant advances" is translated as "重要的进步" (zhòng yì de jìn bo)* "Utilizing deep learning" is translated as "使用深度学习" (shǐ yòu shēn dào xué xí)* "Anomaly detection" is translated as "异常检测" (yì cháng jiǎn té)* "Normal training set" is translated as "正常的训练集" (zhèng cháng de xùn xí jí)* "Completely labeled training set" is translated as "完全标注的训练集" (quán zhì biǎo xiǎo de xùn xí jí)* "Particle accelerators" is translated as "粒子加速器" (dì zhí jiā sù qì)* "Sparse and expensive labels" is translated as "稀疏和昂贵的标签" (xī shū hé gōng jí de biāo jiā)* "Completely unsupervised method" is translated as "完全无监督的方法" (quán zhì wú jiǎn dū de fāng fá)* "Resilient Variational Autoencoder" is translated as "鲁棒的变量自适应器" (ròng bò de biàn yù zì xiǎng qì)* "Feature-level anomaly attribution" is translated as "特征层异常报告" (fèi yì zhì yì cháng bào gāo)* "Shot-to-shot data" is translated as "一把一的数据" (yī bǎ yī de xùn xí)* "Beam position monitoring system" is translated as "贝壳位置监测系统" (bēi kē zhì dào jiān tè xì tuán)* "SLAC Linac Coherent Light Source" is translated as "SLAC激光干涉源" (SLA cè liàng kē gǎn chuī yuán)
</details></li>
</ul>
<hr>
<h2 id="A-study-on-the-impact-of-pre-trained-model-on-Just-In-Time-defect-prediction"><a href="#A-study-on-the-impact-of-pre-trained-model-on-Just-In-Time-defect-prediction" class="headerlink" title="A study on the impact of pre-trained model on Just-In-Time defect prediction"></a>A study on the impact of pre-trained model on Just-In-Time defect prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02317">http://arxiv.org/abs/2309.02317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Guo, Xiaopeng Gao, Zhenyu Zhang, W. K. Chan, Bo Jiang</li>
<li>for: 本研究主要针对Just-In-Time（JIT）缺陷预测任务，旨在探讨不同预训模型之间的关系。</li>
<li>methods: 我们建立了六个模型：RoBERTaJIT、CodeBERTJIT、BARTJIT、PLBARTJIT、GPT2JIT和CodeGPTJIT，每个模型都使用不同的预训模型作为底层模型。我们系统地探讨这些模型之间的差异和连接。</li>
<li>results: 我们发现每个模型都有改进，当预训模型的类似性较高时，需要的训练资源减少得更多。我们 также发现提交代码对缺陷探测具有重要作用，不同的预训模型在少量数据下场景下表现出不同的缺陷探测能力。这些结果为JIT缺陷预测任务中使用预训模型优化提供新的视角，并 highlight了需要更多注意的因素。<details>
<summary>Abstract</summary>
Previous researchers conducting Just-In-Time (JIT) defect prediction tasks have primarily focused on the performance of individual pre-trained models, without exploring the relationship between different pre-trained models as backbones. In this study, we build six models: RoBERTaJIT, CodeBERTJIT, BARTJIT, PLBARTJIT, GPT2JIT, and CodeGPTJIT, each with a distinct pre-trained model as its backbone. We systematically explore the differences and connections between these models. Specifically, we investigate the performance of the models when using Commit code and Commit message as inputs, as well as the relationship between training efficiency and model distribution among these six models. Additionally, we conduct an ablation experiment to explore the sensitivity of each model to inputs. Furthermore, we investigate how the models perform in zero-shot and few-shot scenarios. Our findings indicate that each model based on different backbones shows improvements, and when the backbone's pre-training model is similar, the training resources that need to be consumed are much more closer. We also observe that Commit code plays a significant role in defect detection, and different pre-trained models demonstrate better defect detection ability with a balanced dataset under few-shot scenarios. These results provide new insights for optimizing JIT defect prediction tasks using pre-trained models and highlight the factors that require more attention when constructing such models. Additionally, CodeGPTJIT and GPT2JIT achieved better performance than DeepJIT and CC2Vec on the two datasets respectively under 2000 training samples. These findings emphasize the effectiveness of transformer-based pre-trained models in JIT defect prediction tasks, especially in scenarios with limited training data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Inferring-effective-couplings-with-Restricted-Boltzmann-Machines"><a href="#Inferring-effective-couplings-with-Restricted-Boltzmann-Machines" class="headerlink" title="Inferring effective couplings with Restricted Boltzmann Machines"></a>Inferring effective couplings with Restricted Boltzmann Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02292">http://arxiv.org/abs/2309.02292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alfonso-navas/inferring_effective_couplings_with_RBMs">https://github.com/alfonso-navas/inferring_effective_couplings_with_RBMs</a></li>
<li>paper_authors: Aurélien Decelle, Cyril Furtlehner, Alfonso De Jesus Navas Gómez, Beatriz Seoane</li>
<li>for: 本研究旨在提出一种简单的解决方案，使能够准确地理解生成模型中的物理含义。</li>
<li>methods: 本研究使用了 Restricted Boltzmann Machine（RBM），并提出了一种将 RBM 的能量函数映射到有效磁矩度 Hamiltonian 的方法，该方法包括了所有可能的交互次数，超过了传统的对应 inverse Ising 方法所考虑的对习次数。</li>
<li>results: 研究人员通过控制的数值实验，训练 RBM 使用平衡样本，以验证该方法的有效性。结果表明，该方法可以学习正确的交互网络，并可以应用于模型复杂数据集。此外，研究人员还评估了不同训练方法的模型质量。<details>
<summary>Abstract</summary>
Generative models offer a direct way to model complex data. Among them, energy-based models provide us with a neural network model that aims to accurately reproduce all statistical correlations observed in the data at the level of the Boltzmann weight of the model. However, one challenge is to understand the physical interpretation of such models. In this study, we propose a simple solution by implementing a direct mapping between the energy function of the Restricted Boltzmann Machine and an effective Ising spin Hamiltonian that includes high-order interactions between spins. This mapping includes interactions of all possible orders, going beyond the conventional pairwise interactions typically considered in the inverse Ising approach, and allowing the description of complex datasets. Earlier works attempted to achieve this goal, but the proposed mappings did not do properly treat the complexity of the problem or did not contain direct prescriptions for practical application. To validate our method, we performed several controlled numerical experiments where we trained the RBMs using equilibrium samples of predefined models containing local external fields, two-body and three-body interactions in various low-dimensional topologies. The results demonstrate the effectiveness of our proposed approach in learning the correct interaction network and pave the way for its application in modeling interesting datasets. We also evaluate the quality of the inferred model based on different training methods.
</details>
<details>
<summary>摘要</summary>
<?xml:namespace prefix = "o" ns = "urn:schemas-microsoft-com:office:office" />生成模型提供了直接模型复杂数据的方式。其中，能量基型模型为我们提供了一个基于神经网络的模型，该模型的目标是在数据中识别所有 estadísticos correlations，并在模型的Boltzmann权重水平上准确地复制它们。然而，一个挑战是理解这些模型的物理解释。在这项研究中，我们提议一种简单的解决方案，通过将Restricted Boltzmann Machine（RBM）的能量函数直接映射到包含高阶交互的有效牛顿矩阵 Hamiltoniano中。这种映射包括所有可能的顺序交互，超过了传统的对应 inverse Ising 方法中考虑的对应交互，并允许描述复杂的数据集。先前的工作尝试了实现这个目标，但是提议的映射没有正确地处理问题的复杂性或者没有直接的实践指南。为验证我们的方法，我们进行了一些控制性的数字实验，在 pré-definido 模型中使用平衡样本，包括局部外场、二体和三体交互在多种低维度拓扑中。结果表明了我们提议的方法的效iveness，可以学习正确的交互网络，并为复杂的数据集模型提供了道路。我们还评估了不同的培训方法的模型质量。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Residual-based-Methods-on-Fault-Detection"><a href="#A-Comparison-of-Residual-based-Methods-on-Fault-Detection" class="headerlink" title="A Comparison of Residual-based Methods on Fault Detection"></a>A Comparison of Residual-based Methods on Fault Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02274">http://arxiv.org/abs/2309.02274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi-Ching Hsu, Gaetan Frusque, Olga Fink</li>
<li>for: 本研究旨在比较两种残差基于方法：自动编码器和输入输出模型，以检测FAULTS并区分不同的潜在FAULT类型。</li>
<li>methods: 两种方法都使用残差来检测FAULTS，并且都使用健康数据进行训练。</li>
<li>results: 两种方法都可以在约20个循环后检测FAULTS，并保持低的假阳性率。而输入输出模型提供更好的解释力，包括可能的FAULT类型和可能的FAULT Component。<details>
<summary>Abstract</summary>
An important initial step in fault detection for complex industrial systems is gaining an understanding of their health condition. Subsequently, continuous monitoring of this health condition becomes crucial to observe its evolution, track changes over time, and isolate faults. As faults are typically rare occurrences, it is essential to perform this monitoring in an unsupervised manner. Various approaches have been proposed not only to detect faults in an unsupervised manner but also to distinguish between different potential fault types. In this study, we perform a comprehensive comparison between two residual-based approaches: autoencoders, and the input-output models that establish a mapping between operating conditions and sensor readings. We explore the sensor-wise residuals and aggregated residuals for the entire system in both methods. The performance evaluation focuses on three tasks: health indicator construction, fault detection, and health indicator interpretation. To perform the comparison, we utilize the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model, specifically a subset of the turbofan engine dataset containing three different fault types. All models are trained exclusively on healthy data. Fault detection is achieved by applying a threshold that is determined based on the healthy condition. The detection results reveal that both models are capable of detecting faults with an average delay of around 20 cycles and maintain a low false positive rate. While the fault detection performance is similar for both models, the input-output model provides better interpretability regarding potential fault types and the possible faulty components.
</details>
<details>
<summary>摘要</summary>
<<sys.language_model.activate(lang='zh-CN')>>为了探测复杂工业系统中的FAULT， initially understanding its health condition是非常重要的一步。随后，对这个健康状况的持续监测变得非常重要，以观察其发展、跟踪变化并孤立FAULT。由于FAULT是非常罕见的，因此需要在无监督的情况下进行监测。多种方法已经被提议，不仅可以探测FAULT，还可以分辨不同的可能的FAULT类型。在本研究中，我们进行了总比较两种异常检测方法：自适应神经网络和输入输出模型，它们都可以建立运行条件和传感器读ings之间的映射。我们分析了传感器级别和系统级别的差异，并对三个不同的FAULT类型进行了评估。我们使用了商用模块式飞机发动机 simulate（C-MAPSS）动力模型，特别是一个包含三种FAULT类型的液冷发动机数据集。所有模型都是由健康数据进行了唯一的训练。异常检测是通过设置基于健康状况的阈值来实现的。检测结果显示，两种模型都能够在20轮异常检测，并保持低的假阳性率。虽然异常检测性能相似，但输入输出模型提供了更好的可解释性，即可能的FAULT类型和可能的异常组件。<<sys.language_model.deactivate()>>
</details></li>
</ul>
<hr>
<h2 id="Graph-Based-Automatic-Feature-Selection-for-Multi-Class-Classification-via-Mean-Simplified-Silhouette"><a href="#Graph-Based-Automatic-Feature-Selection-for-Multi-Class-Classification-via-Mean-Simplified-Silhouette" class="headerlink" title="Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette"></a>Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02272">http://arxiv.org/abs/2309.02272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlevinwork/GB-AFS">https://github.com/davidlevinwork/GB-AFS</a></li>
<li>paper_authors: David Levin, Gonen Singer</li>
<li>for: 本研究提出了一种新的图structured filter方法，用于自动特征选择（简称GB-AFS），以便在多类分类任务中提高预测性能。</li>
<li>methods: 该方法使用Jeffries-Matusita距离和t-分布随机邻域嵌入（t-SNE）生成一个低维度空间，以反映每个特征在每对类之间如何有效地分化。而选择最小数量的特征则使用我们新提出的平均简化 Silhouette指数（简称MSS），用于评估特征选择任务中的凝结结果。</li>
<li>results: 实验结果表明，提案的GB-AFS方法在公共数据集上表现出优于其他筛子基本方法和自动特征选择方法。此外，GB-AFS方法可以保持使用所有特征时的准确率，只使用$7%$到$30%$的特征，从而降低了分类时间的消耗，从$15%$降低到$70%$.<details>
<summary>Abstract</summary>
This paper introduces a novel graph-based filter method for automatic feature selection (abbreviated as GB-AFS) for multi-class classification tasks. The method determines the minimum combination of features required to sustain prediction performance while maintaining complementary discriminating abilities between different classes. It does not require any user-defined parameters such as the number of features to select. The methodology employs the Jeffries-Matusita (JM) distance in conjunction with t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space reflecting how effectively each feature can differentiate between each pair of classes. The minimum number of features is selected using our newly developed Mean Simplified Silhouette (abbreviated as MSS) index, designed to evaluate the clustering results for the feature selection task. Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches. Moreover, the proposed algorithm maintained the accuracy achieved when utilizing all features, while using only $7\%$ to $30\%$ of the features. Consequently, this resulted in a reduction of the time needed for classifications, from $15\%$ to $70\%$.
</details>
<details>
<summary>摘要</summary>
The methodology employs Jeffries-Matusita distance and t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space that reflects how effectively each feature differentiates between each pair of classes. The minimum number of features is selected using the Mean Simplified Silhouette (MSS) index, which evaluates the clustering results for the feature selection task.Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches. The proposed algorithm achieved the same accuracy as using all features, while using only 7% to 30% of the features, resulting in a significant reduction in classification time, from 15% to 70%.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Sample-Selection-Through-Uncertainty-Estimation-and-Its-Application-in-Deep-Learning"><a href="#Optimal-Sample-Selection-Through-Uncertainty-Estimation-and-Its-Application-in-Deep-Learning" class="headerlink" title="Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning"></a>Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02476">http://arxiv.org/abs/2309.02476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Chen Liu, Chenlu Ye, Qing Lian, Yuan Yao, Tong Zhang</li>
<li>For: 这个研究旨在提出一个可靠的方法来选择资料subset，以减少深度学习模型的训练成本和错误。* Methods: 本研究使用了潜在子集选择和活动学习，并提出了一个 theoretically 优质的解决方案，名为COPS（uncertainty based OPtimal Sub-sampling），可以对于线性软MAX regression进行最佳化。* Results: 在实验中，COPS 方法与基eline方法比较，结果显示 COPS 方法在深度学习任务中具有superior表现，并且可以对于模型缺失和低密度样本进行下条件调整。<details>
<summary>Abstract</summary>
Modern deep learning heavily relies on large labeled datasets, which often comse with high costs in terms of both manual labeling and computational resources. To mitigate these challenges, researchers have explored the use of informative subset selection techniques, including coreset selection and active learning. Specifically, coreset selection involves sampling data with both input ($\bx$) and output ($\by$), active learning focuses solely on the input data ($\bx$).   In this study, we present a theoretically optimal solution for addressing both coreset selection and active learning within the context of linear softmax regression. Our proposed method, COPS (unCertainty based OPtimal Sub-sampling), is designed to minimize the expected loss of a model trained on subsampled data. Unlike existing approaches that rely on explicit calculations of the inverse covariance matrix, which are not easily applicable to deep learning scenarios, COPS leverages the model's logits to estimate the sampling ratio. This sampling ratio is closely associated with model uncertainty and can be effectively applied to deep learning tasks. Furthermore, we address the challenge of model sensitivity to misspecification by incorporating a down-weighting approach for low-density samples, drawing inspiration from previous works.   To assess the effectiveness of our proposed method, we conducted extensive empirical experiments using deep neural networks on benchmark datasets. The results consistently showcase the superior performance of COPS compared to baseline methods, reaffirming its efficacy.
</details>
<details>
<summary>摘要</summary>
现代深度学习强调大量标注数据，经常带来高的人工标注和计算成本。为了缓解这些挑战，研究人员已经探索了有用的子集选择技术，包括核心集选择和活动学习。特别是，核心集选择是通过采样数据来减少数据量，而活动学习则仅关注输入数据。在本研究中，我们提出了在线性软MAX回归中的理论优化解决方案，名为COPS（uncertainty based Optimal Sub-sampling）。我们的方法旨在降低由subsampled数据训练的模型预测错误的期望损失。与现有方法不同，COPS不需要显式计算 inverse covariance matrix，而是利用模型的logits来估算采样比率。这个采样比率与模型uncertainty有很Close关系，可以有效应用于深度学习任务。此外，我们还解决了模型偏置低密度样本的挑战，通过引入低密度样本下降值策略，这种策略 drew inspiration from previous works。为评估我们的提出方法的有效性，我们在深度神经网络上进行了广泛的实验。结果 consistently showcase COPS比基准方法更好，这再次证明了其效果。
</details></li>
</ul>
<hr>
<h2 id="RoBoSS-A-Robust-Bounded-Sparse-and-Smooth-Loss-Function-for-Supervised-Learning"><a href="#RoBoSS-A-Robust-Bounded-Sparse-and-Smooth-Loss-Function-for-Supervised-Learning" class="headerlink" title="RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning"></a>RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02250">http://arxiv.org/abs/2309.02250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mtanveer1/RoBoSS">https://github.com/mtanveer1/RoBoSS</a></li>
<li>paper_authors: Mushir Akhtar, M. Tanveer, Mohd. Arshad</li>
<li>For: 本研究提出了一种新的稳定、缩短、稀疏和均匀（RoBoSS）损失函数，用于改进支持向量机（SVM）的超级vised学习算法。* Methods: 本文 integra RoBoSS损失函数到SVM框架中，并提出了一种新的稳定化算法($\mathcal{L}<em>{rbss}$-SVM)。同时，本文也进行了对RoBoSS损失函数的分类准确性和泛化能力的理论分析。* Results: 实验表明，使用提出的$\mathcal{L}</em>{rbss}$-SVM模型，在88个真实世界UCI和KEEL数据集上表现出色，并且在医学领域中，在EEG信号数据集和Breast Cancer（BreaKHis）数据集上也得到了惊喜的结果。<details>
<summary>Abstract</summary>
In the domain of machine learning algorithms, the significance of the loss function is paramount, especially in supervised learning tasks. It serves as a fundamental pillar that profoundly influences the behavior and efficacy of supervised learning algorithms. Traditional loss functions, while widely used, often struggle to handle noisy and high-dimensional data, impede model interpretability, and lead to slow convergence during training. In this paper, we address the aforementioned constraints by proposing a novel robust, bounded, sparse, and smooth (RoBoSS) loss function for supervised learning. Further, we incorporate the RoBoSS loss function within the framework of support vector machine (SVM) and introduce a new robust algorithm named $\mathcal{L}_{rbss}$-SVM. For the theoretical analysis, the classification-calibrated property and generalization ability are also presented. These investigations are crucial for gaining deeper insights into the performance of the RoBoSS loss function in the classification tasks and its potential to generalize well to unseen data. To empirically demonstrate the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM, we evaluate it on $88$ real-world UCI and KEEL datasets from diverse domains. Additionally, to exemplify the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM within the biomedical realm, we evaluated it on two medical datasets: the electroencephalogram (EEG) signal dataset and the breast cancer (BreaKHis) dataset. The numerical results substantiate the superiority of the proposed $\mathcal{L}_{rbss}$-SVM model, both in terms of its remarkable generalization performance and its efficiency in training time.
</details>
<details>
<summary>摘要</summary>
在机器学习算法领域，损失函数的重要性非常高，特别在指导学习任务中。它作为基础的核心因素，深刻影响了指导学习算法的行为和效果。传统的损失函数，虽广泛使用，但经常难以处理噪音和高维数据，阻碍模型解释性，并导致训练过程中的慢 converges。本文提出了一种新的robust、bounded、稀疏和均匀（RoBoSS）损失函数，用于指导学习。此外，我们将RoBoSS损失函数 integration到支持向量机（SVM）框架中，并提出了一种新的Robust-SVM算法。对于理论分析方面，我们还提出了分类准备性和泛化能力的研究。这些研究对于了解RoBoSS损失函数在分类任务中的性能和泛化能力具有重要意义。为了证明提出的Lrbss-SVM模型的效果，我们对88个真实世界UCI和KEEL数据集进行了实验评估。此外，为了展示Lrbss-SVM模型在医学领域的效果，我们对电enzephalogram（EEG）信号数据集和Breast Cancer（BreaKHis）数据集进行了评估。实验结果表明，提出的Lrbss-SVM模型具有优秀的泛化性和训练效率。
</details></li>
</ul>
<hr>
<h2 id="Self-Similarity-Based-and-Novelty-based-loss-for-music-structure-analysis"><a href="#Self-Similarity-Based-and-Novelty-based-loss-for-music-structure-analysis" class="headerlink" title="Self-Similarity-Based and Novelty-based loss for music structure analysis"></a>Self-Similarity-Based and Novelty-based loss for music structure analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02243">http://arxiv.org/abs/2309.02243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffroy Peeters</li>
<li>for: 本研究旨在提出一种监督学习方法来解决音乐分割问题。</li>
<li>methods: 该方法同时学习特征和卷积核，并将自同相似矩阵（SSM）和新鲜度分数作为损失函数进行优化。</li>
<li>results: 研究人员通过对学习到的特征进行自我注意力，提高了音乐分割任务的性能。此外，与之前的方法进行比较，该方法在RWC-Pop和SALAMI等标准数据集上表现较优。<details>
<summary>Abstract</summary>
Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity. In this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels. For this we jointly optimize -- a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and -- a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss. We also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA. Finally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI.
</details>
<details>
<summary>摘要</summary>
音乐结构分析（MSA）是目标在音乐轨道中确定乐曲的各个部分，并可能根据它们的相似性进行标签。在这篇论文中，我们提出了一种监督方法来实现音乐边界检测任务。我们同时学习特征和卷积核，并同步优化两种损失函数。其中一种损失函数基于自相似矩阵（SSM），通过学习的特征来获得，被称为SSM-损失；另一种损失函数基于新鲜度分数，通过学习的卷积核应用于估算的SSM中，被称为新鲜度-损失。我们还证明了通过自我注意力来实现相对特征学习是MSA任务中有利的。最后，我们比较了我们的方法与之前提出的方法在标准RWC-Pop和SALAMI中的性能。
</details></li>
</ul>
<hr>
<h2 id="Sample-Size-in-Natural-Language-Processing-within-Healthcare-Research"><a href="#Sample-Size-in-Natural-Language-Processing-within-Healthcare-Research" class="headerlink" title="Sample Size in Natural Language Processing within Healthcare Research"></a>Sample Size in Natural Language Processing within Healthcare Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02237">http://arxiv.org/abs/2309.02237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaya Chaturvedi, Diana Shamsutdinova, Felix Zimmer, Sumithra Velupillai, Daniel Stahl, Robert Stewart, Angus Roberts</li>
<li>for: 本研究是为了提供适合医疗领域文本数据的样本大小选择的建议。</li>
<li>methods: 本研究使用了不同的分类器和样本大小进行了 simulations，以评估不同样本大小对文本分类任务的影响。</li>
<li>results: 研究发现，使用 K-最近邻分类器时，小样本大小可以提供更好的性能指标，而使用支持向量机和BERT模型时，大样本大小提供更好的性能。总之，样本大小大于1000是适合的，可以提供良好的性能指标。<details>
<summary>Abstract</summary>
Sample size calculation is an essential step in most data-based disciplines. Large enough samples ensure representativeness of the population and determine the precision of estimates. This is true for most quantitative studies, including those that employ machine learning methods, such as natural language processing, where free-text is used to generate predictions and classify instances of text. Within the healthcare domain, the lack of sufficient corpora of previously collected data can be a limiting factor when determining sample sizes for new studies. This paper tries to address the issue by making recommendations on sample sizes for text classification tasks in the healthcare domain.   Models trained on the MIMIC-III database of critical care records from Beth Israel Deaconess Medical Center were used to classify documents as having or not having Unspecified Essential Hypertension, the most common diagnosis code in the database. Simulations were performed using various classifiers on different sample sizes and class proportions. This was repeated for a comparatively less common diagnosis code within the database of diabetes mellitus without mention of complication.   Smaller sample sizes resulted in better results when using a K-nearest neighbours classifier, whereas larger sample sizes provided better results with support vector machines and BERT models. Overall, a sample size larger than 1000 was sufficient to provide decent performance metrics.   The simulations conducted within this study provide guidelines that can be used as recommendations for selecting appropriate sample sizes and class proportions, and for predicting expected performance, when building classifiers for textual healthcare data. The methodology used here can be modified for sample size estimates calculations with other datasets.
</details>
<details>
<summary>摘要</summary>
Sample size calculation is an essential step in most data-based disciplines. Large enough samples ensure representativeness of the population and determine the precision of estimates. This is true for most quantitative studies, including those that employ machine learning methods, such as natural language processing, where free-text is used to generate predictions and classify instances of text. Within the healthcare domain, the lack of sufficient corpora of previously collected data can be a limiting factor when determining sample sizes for new studies. This paper tries to address the issue by making recommendations on sample sizes for text classification tasks in the healthcare domain.  模型在基于MIMIC-III数据库的医疗记录中训练后，用于分类文档是否有未特定主要高血压症，该数据库中最常见的诊断代码。在不同的样本大小和类别比例下，使用不同的分类器进行了 simulations。这些 simulations 重复使用不同的分类器和不同的样本大小。结果表明，使用 K-最近邻居分类器时，小样本大小得到了更好的结果，而使用支持向量机和BERT模型时，大样本大小得到了更好的结果。总的来说，样本大小大于1000是可以提供不错的性能指标的。  这些 simulations 提供了适用于健康领域文本数据的分类器建立时选择合适的样本大小和类别比例，以及预测性能的指南。这种方法可以适用于其他数据集的样本大小估计计算。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Machine-Learning-with-Multi-source-Data"><a href="#Distributionally-Robust-Machine-Learning-with-Multi-source-Data" class="headerlink" title="Distributionally Robust Machine Learning with Multi-source Data"></a>Distributionally Robust Machine Learning with Multi-source Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02211">http://arxiv.org/abs/2309.02211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyu Wang, Peter Bühlmann, Zijian Guo</li>
<li>for: 这篇文章是用于提高预测性能，当目标分布与源 Population 不同时，传统机器学习方法可能会导致差强预测性能。</li>
<li>methods: 文章提出了一个基于多来源数据的集团分布强化预测模型，这个模型使用了对于target分布的敌方奖励函数来定义，以提高预测性能。</li>
<li>results: 文章的实验结果显示，相比于传统的empirical risk minimization，提案的强化预测模型可以提高预测性能，并且可以让用户对于不同的目标分布进行预测。<details>
<summary>Abstract</summary>
Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the convergence rate. Our proposal can be seen as a distributionally robust federated learning approach that is computationally efficient and easy to implement using arbitrary machine learning base algorithms, satisfies some privacy constraints, and has a nice interpretation of different sources' importance for predicting a given target covariate distribution. We demonstrate the performance of our proposed group distributionally robust method on simulated and real data with random forests and neural networks as base-learning algorithms.
</details>
<details>
<summary>摘要</summary>
传统机器学习方法可能会导致预测性能差，因为目标分布与源 популяции不同。这篇论文利用多个源数据，提出了一种分布robust预测模型，定义为最小化一个对target分布的反对抗 reward的 adversarial  objective function。相比于传统的empirical risk minimization，我们的robust预测模型可以提高预测性能 для目标population中的分布shift。我们证明了我们的分布robust预测模型是源population conditional outcome模型的Weighted average。我们利用这个关键的标识结果，以robustify任意机器学习算法，包括随机森林和神经网络。我们开发了一种偏导corrected estimator来估计最佳汇合权，并证明其提高了收敛率。我们的提议可以看作是一种分布robust Federated learning方法，computationally efficient，易于实现，满足一些隐私约束，并具有预测targetcovariate分布的nice interpretation of different sources的重要性。我们在 simulate和实际数据上测试了我们的提议，使用随机森林和神经网络作为基础学习算法。
</details></li>
</ul>
<hr>
<h2 id="Latent-Disentanglement-in-Mesh-Variational-Autoencoders-Improves-the-Diagnosis-of-Craniofacial-Syndromes-and-Aids-Surgical-Planning"><a href="#Latent-Disentanglement-in-Mesh-Variational-Autoencoders-Improves-the-Diagnosis-of-Craniofacial-Syndromes-and-Aids-Surgical-Planning" class="headerlink" title="Latent Disentanglement in Mesh Variational Autoencoders Improves the Diagnosis of Craniofacial Syndromes and Aids Surgical Planning"></a>Latent Disentanglement in Mesh Variational Autoencoders Improves the Diagnosis of Craniofacial Syndromes and Aids Surgical Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10825">http://arxiv.org/abs/2309.10825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Foti, Alexander J. Rickart, Bongjin Koo, Eimear O’ Sullivan, Lara S. van de Lande, Athanasios Papaioannou, Roman Khonsari, Danail Stoyanov, N. u. Owase Jeelani, Silvia Schievano, David J. Dunaway, Matthew J. Clarkson</li>
<li>for: 这项研究旨在应用Swap Disentangled Variational Autoencoder（SD-VAE）模型对人类头部复杂结构进行深度学习分析，以便更好地识别和分类各种颅骨畸形。</li>
<li>methods: 这项研究使用了SD-VAE模型，通过对整个头部网格进行分类，同时也可以分析每个区域对颅骨畸形的影响。此外，通过修改生成模型的参数，可以模拟不同的颅骨外科手术结果。</li>
<li>results: 这项研究可以帮助提高颅骨畸形诊断的准确性，帮助外科医生规划手术，以及对手术结果进行客观评估。<details>
<summary>Abstract</summary>
The use of deep learning to undertake shape analysis of the complexities of the human head holds great promise. However, there have traditionally been a number of barriers to accurate modelling, especially when operating on both a global and local level. In this work, we will discuss the application of the Swap Disentangled Variational Autoencoder (SD-VAE) with relevance to Crouzon, Apert and Muenke syndromes. Although syndrome classification is performed on the entire mesh, it is also possible, for the first time, to analyse the influence of each region of the head on the syndromic phenotype. By manipulating specific parameters of the generative model, and producing procedure-specific new shapes, it is also possible to simulate the outcome of a range of craniofacial surgical procedures. This opens new avenues to advance diagnosis, aids surgical planning and allows for the objective evaluation of surgical outcomes.
</details>
<details>
<summary>摘要</summary>
使用深度学习进行人类头部复杂性分析具有很大的抢救性。然而，在全球和地方层次上准确模型化却存在许多障碍。在这项工作中，我们将讨论使用Swap Disentangled Variational Autoencoder（SD-VAE）在Crouzon、Apert和Muenke综合症中的应用。虽然病种分类是基于整个网格进行的，但是也可以，如 nunca antes，分析每个头部区域对综合症fenotip的影响。通过修改生成模型的特定参数，并生成过程特定的新形状，也可以模拟多种颅外科手术的结果。这些新的技术可以提高诊断、帮助手术规划和评估手术结果的 объекivity。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-for-Novelty-Detection-in-System-Call-Traces"><a href="#Language-Models-for-Novelty-Detection-in-System-Call-Traces" class="headerlink" title="Language Models for Novelty Detection in System Call Traces"></a>Language Models for Novelty Detection in System Call Traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02206">http://arxiv.org/abs/2309.02206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Fournier, Daniel Aloise, Leandro R. Costa</li>
<li>for: 本研究旨在提出一种基于系统调用语言模型的新型行为探测方法，用于检测现代计算机系统中的异常行为。</li>
<li>methods: 本研究使用了三种不同的 neural network 架构：LSTM、Transformer 和 Longformer，并对这些架构进行了评估。</li>
<li>results: 研究发现，使用这些架构可以实现高于 95% 的 F-score 和 AuROC 在大多数新型行为上，而且该方法不需要大量的专家手动编制和可以在不同任务上进行数据独立的应用。<details>
<summary>Abstract</summary>
Due to the complexity of modern computer systems, novel and unexpected behaviors frequently occur. Such deviations are either normal occurrences, such as software updates and new user activities, or abnormalities, such as misconfigurations, latency issues, intrusions, and software bugs. Regardless, novel behaviors are of great interest to developers, and there is a genuine need for efficient and effective methods to detect them. Nowadays, researchers consider system calls to be the most fine-grained and accurate source of information to investigate the behavior of computer systems. Accordingly, this paper introduces a novelty detection methodology that relies on a probability distribution over sequences of system calls, which can be seen as a language model. Language models estimate the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition, they would be unlikely under the model. Following the success of neural networks for language models, three architectures are evaluated in this work: the widespread LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer. However, large neural networks typically require an enormous amount of data to be trained effectively, and to the best of our knowledge, no massive modern datasets of kernel traces are publicly available. This paper addresses this limitation by introducing a new open-source dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors. The proposed methodology requires minimal expert hand-crafting and achieves an F-score and AuROC greater than 95% on most novelties while being data- and task-agnostic. The source code and trained models are publicly available on GitHub while the datasets are available on Zenodo.
</details>
<details>
<summary>摘要</summary>
Recently, researchers have turned to system calls as the most fine-grained and accurate source of information to investigate the behavior of computer systems. This paper introduces a novelty detection methodology that relies on a probability distribution over sequences of system calls, which can be seen as a language model. The method estimates the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition, they would be unlikely under the model.To evaluate the effectiveness of the method, three neural network architectures were used: the widespread LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer. However, large neural networks typically require a large amount of data to be trained effectively. To address this limitation, this paper introduces a new open-source dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors.The proposed methodology requires minimal expert hand-crafting and achieves an F-score and AuROC greater than 95% on most novelties while being data- and task-agnostic. The source code and trained models are publicly available on GitHub, while the datasets are available on Zenodo.
</details></li>
</ul>
<hr>
<h2 id="On-the-Complexity-of-Differentially-Private-Best-Arm-Identification-with-Fixed-Confidence"><a href="#On-the-Complexity-of-Differentially-Private-Best-Arm-Identification-with-Fixed-Confidence" class="headerlink" title="On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence"></a>On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02202">http://arxiv.org/abs/2309.02202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achraf Azize, Marc Jourdan, Aymen Al Marjani, Debabrota Basu</li>
<li>for: 这个论文主要研究的是如何在数据敏感应用中实现最佳臂标识（BAI）问题，包括设计适应性临床试验、调整超参数以及进行用户研究等。</li>
<li>methods: 这篇论文使用了 $\epsilon$-全球隐私（DP）来保证数据隐私，并研究了 $\epsilon$-全球DP下BAI问题的解决方案。作者首先计算了隐私保护成本的下限，并发现了两种隐私模式，即高隐私模式（小 $\epsilon$）和低隐私模式（大 $\epsilon$）。在高隐私模式下，难度受到隐私和一种新的信息论量——总特征时间的共同影响。在低隐私模式下，下界降到非私下的下界。</li>
<li>results: 作者提出了一种名为 AdaP-TT 的 $\epsilon$-全球DP下的 BAI 算法，该算法在 arm-dependent 的扩展集内运行，并添加了 Laplace 噪声来保证好隐私与实用之间的融合。作者 deriv了 AdaP-TT 的 asymptotic 上限，并证明了其与下界之间的相似性。最后，作者进行了实验分析，并证明了理论结论。<details>
<summary>Abstract</summary>
Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies to name a few. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence under $\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive a lower bound on the sample complexity of any $\delta$-correct BAI algorithm satisfying $\epsilon$-global DP. Our lower bound suggests the existence of two privacy regimes depending on the privacy budget $\epsilon$. In the high-privacy regime (small $\epsilon$), the hardness depends on a coupled effect of privacy and a novel information-theoretic quantity, called the Total Variation Characteristic Time. In the low-privacy regime (large $\epsilon$), the sample complexity lower bound reduces to the classical non-private lower bound. Second, we propose AdaP-TT, an $\epsilon$-global DP variant of the Top Two algorithm. AdaP-TT runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. We derive an asymptotic upper bound on the sample complexity of AdaP-TT that matches with the lower bound up to multiplicative constants in the high-privacy regime. Finally, we provide an experimental analysis of AdaP-TT that validates our theoretical results.
</details>
<details>
<summary>摘要</summary>
最佳臂标识问题（BAI）在数据敏感应用中得到普遍应用，如设计适应式临床试验、调整超参数以及进行用户研究等。由于这些应用中的数据隐私问题，我们研究BAI问题在固定信息保护环境下的$\epsilon$-全球隐私（DP）下进行研究。首先，我们定义了隐私成本的量化，即在任意$\delta$-正确BAI算法满足$\epsilon$-全球DP下的样本复杂度下界。我们的下界表明，隐私预算$\epsilon$的两个隐私模式存在：在高隐私模式（小$\epsilon$）下，难度受到隐私和一种新的信息论量度——总特征时间的共同作用的影响。在低隐私模式（大$\epsilon$）下，样本复杂度下界降低到经典非私有下界。其次，我们提出了一种$\epsilon$-全球DP variant的Top Two算法——AdaP-TT。AdaP-TT在臂 dependent的适应性集中运行，并在每个集中添加拉Place噪声以确保良好的隐私利用平衡。我们 deriv了AdaP-TT的 asymptotic 上界样本复杂度，与下界匹配到多项式常数在高隐私模式。最后，我们对AdaP-TT进行实验分析，并证明了我们的理论结果。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Function-space-Representation-of-Neural-Networks"><a href="#Sparse-Function-space-Representation-of-Neural-Networks" class="headerlink" title="Sparse Function-space Representation of Neural Networks"></a>Sparse Function-space Representation of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02195">http://arxiv.org/abs/2309.02195</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Aidan Scannell, Riccardo Mereu, Paul Chang, Ella Tamir, Joni Pajarinen, Arno Solin</li>
<li>for: 提高深度神经网络（NN）的不确定性估计和新数据 incorporation 能力</li>
<li>methods: 通过将NN从权重空间转换到函数空间，via dual parameterization，实现了一种简洁和原理正确的不确定性捕捉方法，并可以在整个数据集中捕捉信息</li>
<li>results: 通过proof-of-concept示例，证明了该方法在超参量学任务上可以有效地Quantifying uncertainty和 incorporating new data without retraining<details>
<summary>Abstract</summary>
Deep neural networks (NNs) are known to lack uncertainty estimates and struggle to incorporate new data. We present a method that mitigates these issues by converting NNs from weight space to function space, via a dual parameterization. Importantly, the dual parameterization enables us to formulate a sparse representation that captures information from the entire data set. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We provide proof-of-concept demonstrations with the proposed approach for quantifying uncertainty in supervised learning on UCI benchmark tasks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度神经网络（NN）通常缺乏不确定性估计和新数据integrate的能力。我们提出了一种方法，通过将NN从权重空间转换到函数空间，使用双参数化。这种方法可以提供一种紧凑而原理的方式来捕捉不确定性信息，并且可以在不需要 RETRAINING 的情况下，将新数据集入库。我们在 UCI benchmark 任务上提供了证明示范，以证明我们的方法可以在超出学习中量化不确定性。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Federated-Deep-Reinforcement-Learning-based-Trajectory-Optimization-for-Multi-UAV-Assisted-Edge-Computing"><a href="#Personalized-Federated-Deep-Reinforcement-Learning-based-Trajectory-Optimization-for-Multi-UAV-Assisted-Edge-Computing" class="headerlink" title="Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing"></a>Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02193">http://arxiv.org/abs/2309.02193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengrong Song, Chuan Ma, Ming Ding, Howard H. Yang, Yuwen Qian, Xiangwei Zhou</li>
<li>for: 提高多架空器飞行轨迹优化的通信系统吞吐量</li>
<li>methods: 使用个性化联合深度学习（PF-DRL）方法，为每个代理模型开发个性化模型，以Address数据稀缺和不均匀性问题</li>
<li>results: 在模拟环境中，提议的算法比其他DRL基本方法具有更好的训练性能和更快的 converges速率，并提高服务质量<details>
<summary>Abstract</summary>
In the era of 5G mobile communication, there has been a significant surge in research focused on unmanned aerial vehicles (UAVs) and mobile edge computing technology. UAVs can serve as intelligent servers in edge computing environments, optimizing their flight trajectories to maximize communication system throughput. Deep reinforcement learning (DRL)-based trajectory optimization algorithms may suffer from poor training performance due to intricate terrain features and inadequate training data. To overcome this limitation, some studies have proposed leveraging federated learning (FL) to mitigate the data isolation problem and expedite convergence. Nevertheless, the efficacy of global FL models can be negatively impacted by the high heterogeneity of local data, which could potentially impede the training process and even compromise the performance of local agents. This work proposes a novel solution to address these challenges, namely personalized federated deep reinforcement learning (PF-DRL), for multi-UAV trajectory optimization. PF-DRL aims to develop individualized models for each agent to address the data scarcity issue and mitigate the negative impact of data heterogeneity. Simulation results demonstrate that the proposed algorithm achieves superior training performance with faster convergence rates, and improves service quality compared to other DRL-based approaches.
</details>
<details>
<summary>摘要</summary>
在5G移动通信时代，有一场很大的研究集中在无人飞行器（UAV）和边缘计算技术上。UAV可以在边缘计算环境中服务为智能服务器，最大化通信系统吞吐量。深度违离学（DRL）基于的轨迹优化算法可能因地形特征复杂和训练数据不充分而表现不佳。为了解决这些限制，一些研究已经提议利用联邦学习（FL）来减少数据隔离问题，加速融合。然而，全球FL模型的效果可能受到本地数据的高自similarity的影响，这可能会阻碍训练过程并可能下降本地代理的性能。这项工作提出了一种解决这些挑战的新解决方案，即个性化联邦深度违离学（PF-DRL），用于多个UAV的轨迹优化。PF-DRL的目标是为每个代理开发特定的模型，以解决数据缺乏问题，并减少数据不同性的负面影响。在模拟结果中，提出的算法可以在训练性能和速度上达到更好的表现，并提高服务质量相比其他DRL基于的方法。
</details></li>
</ul>
<hr>
<h2 id="Bias-Propagation-in-Federated-Learning"><a href="#Bias-Propagation-in-Federated-Learning" class="headerlink" title="Bias Propagation in Federated Learning"></a>Bias Propagation in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02160">http://arxiv.org/abs/2309.02160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/privacytrustlab/bias_in_FL">https://github.com/privacytrustlab/bias_in_FL</a></li>
<li>paper_authors: Hongyan Chang, Reza Shokri</li>
<li>for: 这个论文旨在探讨联邦学习中的群体公平问题，具体来说是研究在分布式数据集上如何避免偏见的扩散。</li>
<li>methods: 这个论文使用了联邦学习的实际应用场景，对实际分布式数据集进行分析和解释，探讨偏见如何在联邦学习中传播。</li>
<li>results: 研究发现，在联邦学习中，偏见可以通过网络传播给所有参与方，而且这种偏见的程度高于中央训练模型使用所有数据集的情况。这些结果告诉我们，在联邦学习中需要进行审核和设计具有群体公平性的学习算法。<details>
<summary>Abstract</summary>
We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.
</details>
<details>
<summary>摘要</summary>
我们显示了参与联邦学习可能会对群体公平性造成不良影响。事实上，一些党对受抑表示的群体（通过敏感特征如性别或种族）的偏见可以在网络中传播到所有党。我们分析并解释了联邦学习中偏见传播的现象。我们的分析表明，偏见党在训练过程中隐藏式地将偏见编码到少量的模型参数中，并在训练中不断增加受抑表示的参考。值得注意的是，在联邦学习中体验到的偏见高于中央训练一个模型使用所有数据的情况下所体验到的偏见。这表明，偏见是由算法所导致的。我们的工作呼吁了审核联邦学习中的群体公平性，并设计不受偏见传播的学习算法。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Asymmetric-Momentum-Make-SGD-Greatest-Again"><a href="#A-Simple-Asymmetric-Momentum-Make-SGD-Greatest-Again" class="headerlink" title="A Simple Asymmetric Momentum Make SGD Greatest Again"></a>A Simple Asymmetric Momentum Make SGD Greatest Again</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02130">http://arxiv.org/abs/2309.02130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gongyue Zhang, Dinghuang Zhang, Shuwen Zhao, Donghan Liu, Carrie M. Toptan, Honghai Liu</li>
<li>for: 本研究旨在解决深度学习中的枢轴点问题，提出了一种新的损控极大值梯度法（LCAM），不同于传统的梯度下降法，LCAM在计算成本上没有增加，却能够超越现有的优化器。</li>
<li>methods: 本研究使用了质量 conjugation 和拖动效应来解释 LCAM 的现象，并设计了一系列实验来快速降低学习率在特定的积分 epoch 来更好地吸引参数陷入枢轴点。</li>
<li>results: 在 WRN28-10 测试网络上，使用 LCAM 在 Cifar100 测试集上达到了平均测试精度的峰值 around 120 epoch，比原始 WRN 纸上的80.75% 高，而且使用的 convergence time 只有原始 WRN 的一半。<details>
<summary>Abstract</summary>
We propose the simplest SGD enhanced method ever, Loss-Controlled Asymmetric Momentum(LCAM), aimed directly at the Saddle Point problem. Compared to the traditional SGD with Momentum, there's no increase in computational demand, yet it outperforms all current optimizers. We use the concepts of weight conjugation and traction effect to explain this phenomenon. We designed experiments to rapidly reduce the learning rate at specified epochs to trap parameters more easily at saddle points. We selected WRN28-10 as the test network and chose cifar10 and cifar100 as test datasets, an identical group to the original paper of WRN and Cosine Annealing Scheduling(CAS). We compared the ability to bypass saddle points of Asymmetric Momentum with different priorities. Finally, using WRN28-10 on Cifar100, we achieved a peak average test accuracy of 80.78\% around 120 epoch. For comparison, the original WRN paper reported 80.75\%, while CAS was at 80.42\%, all at 200 epoch. This means that while potentially increasing accuracy, we use nearly half convergence time. Our demonstration code is available at\\ https://github.com/hakumaicc/Asymmetric-Momentum-LCAM
</details>
<details>
<summary>摘要</summary>
我们提出了最简单的SGD加强方法之一，损控量子动量（LCAM），直接解决顶点问题。与传统的SGD加强方法相比，我们没有增加计算需求，但它在当前优化器中表现出色。我们使用了权重 conjugation 和拖动效应来解释这种现象。我们设计了实验，以快速降低学习率在 specify 的epoch中，以更容易将参数固定在顶点上。我们选择了 WRN28-10 作为测试网络，并选择了 cifar10 和 cifar100 作为测试集，与原始 WRN 和 Cosine Annealing Scheduling（CAS）的测试集一样。我们比较了不同优先级的偏置量子动量的缺过点途径能力。最后，使用 WRN28-10 在 Cifar100 上达到了约 120 epoch 的峰值平均测试准确率 around 80.78%。相比之下，原始 WRN 文章reported 80.75%，而 CAS 则是 80.42%，都在 200 epoch 上。这意味着，虽然可能提高准确率，但我们使用的是 nearly half 的 converge 时间。我们的示例代码可以在 https://github.com/hakumaicc/Asymmetric-Momentum-LCAM 上找到。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Spatial-temporal-Data-for-Sleep-Stage-Classification-via-Hypergraph-Learning"><a href="#Exploiting-Spatial-temporal-Data-for-Sleep-Stage-Classification-via-Hypergraph-Learning" class="headerlink" title="Exploiting Spatial-temporal Data for Sleep Stage Classification via Hypergraph Learning"></a>Exploiting Spatial-temporal Data for Sleep Stage Classification via Hypergraph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02124">http://arxiv.org/abs/2309.02124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuze Liu, Ziming Zhao, Tiehua Zhang, Kang Wang, Xin Chen, Xiaowei Huang, Jun Yin, Zhishu Shen</li>
<li>for: 静脉分类是诊断疾病的关键，现有模型主要使用卷积神经网络（CNN）模型几何数据，以及图 convolutional neural networks（GNN）模型非几何数据，但是这些模型无法同时考虑多modal数据的异质性和交互性，以及空间-时间相关性，因此它们的分类性能有限。</li>
<li>methods: 我们提出了一种动态学习框架STHL，该框架引入了Hipergraph来编码空间-时间数据 для静脉分类。Hipergraph可以构造多Modal&#x2F;多类型的数据，而不是使用简单的对两个主体之间的对应。STHL创建空间和时间的Hiperedge分别来建立节点相关性，然后它进行类型特定的Hipergraph学习过程来编码特征到嵌入空间。</li>
<li>results: 我们的提出的STHL在静脉分类任务中超过了当前最佳模型的性能。<details>
<summary>Abstract</summary>
Sleep stage classification is crucial for detecting patients' health conditions. Existing models, which mainly use Convolutional Neural Networks (CNN) for modelling Euclidean data and Graph Convolution Networks (GNN) for modelling non-Euclidean data, are unable to consider the heterogeneity and interactivity of multimodal data as well as the spatial-temporal correlation simultaneously, which hinders a further improvement of classification performance. In this paper, we propose a dynamic learning framework STHL, which introduces hypergraph to encode spatial-temporal data for sleep stage classification. Hypergraphs can construct multi-modal/multi-type data instead of using simple pairwise between two subjects. STHL creates spatial and temporal hyperedges separately to build node correlations, then it conducts type-specific hypergraph learning process to encode the attributes into the embedding space. Extensive experiments show that our proposed STHL outperforms the state-of-the-art models in sleep stage classification tasks.
</details>
<details>
<summary>摘要</summary>
📝 sleep stage classification 是诊断病人健康状况的关键。现有的模型主要使用卷积神经网络（CNN）来模型几何数据，以及图 convolutional neural networks（GNN）来模型非几何数据，但是这些模型无法同时考虑多模态数据的异质性和互动性以及空间-时间相关性，这会限制分类性能的进一步提高。在本文中，我们提出了一个动态学习框架 STHL，该框架利用卷积图来编码空间-时间数据 для睡眠阶段分类。卷积图可以构建多Modal/多类型的数据，而不是使用简单的对两个主题之间的对应关系。STHL 首先在空间和时间上分别创建特性 Edge，然后进行类型特定的卷积图学习过程来编码特征到嵌入空间中。广泛的实验表明，我们提出的 STHL 在睡眠阶段分类任务中表现出了优于现有的模型。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Approach-to-Unsupervised-Out-of-Distribution-Detection-with-Variational-Autoencoders"><a href="#An-Efficient-Approach-to-Unsupervised-Out-of-Distribution-Detection-with-Variational-Autoencoders" class="headerlink" title="An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders"></a>An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02084">http://arxiv.org/abs/2309.02084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjlab-ammi/vae4ood">https://github.com/zjlab-ammi/vae4ood</a></li>
<li>paper_authors: Zezhen Zeng, Bin Liu</li>
<li>for: 这 paper 关注深度生成模型 (DGM) 的无监督 out-of-distribution (OOD) 检测。特别是我们关注 vanilla Variational Autoencoders (VAE)，使用标准正态分布 для隐藏变量。这些模型具有更小的模型大小，使得它们在资源有限的应用程序中更适合使用，比较复杂的 DGM。</li>
<li>methods: 我们提出了一种新的 OOD 分数，called Error Reduction (ER)，专门为 vanilla VAE 设计。ER 具有重建输入图像的损失 counterpart 的想法，并考虑图像的 Kolmogorov 复杂性。我们在多个数据集上进行了实验，比较基准方法。</li>
<li>results: 我们的实验结果表明，我们的方法在多个数据集上具有显著优势，比较基准方法。我们的代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/ZJLAB-AMMI/VAE4OOD%E3%80%82">https://github.com/ZJLAB-AMMI/VAE4OOD。</a><details>
<summary>Abstract</summary>
This paper is concerned with deep generative models (DGMs) for unsupervised out-of-distribution (OOD) detection. In particular, we focus on vanilla Variational Autoencoders (VAE) that use a standard normal prior distribution for the latent variables. These models have a smaller model size, enabling faster training and inference, making them well-suited for resource-limited applications compared to more complex DGMs. We propose a novel OOD score called Error Reduction (ER) specifically designed for vanilla VAE. ER incorporate the idea of reconstructing image inputs from their lossy counterparts and takes into account the Kolmogorov complexity of the images. Experimental results on diverse datasets demonstrate the superiority of our approach over baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/VAE4OOD.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "DGMs" is translated as "深度生成模型" (shēn dào shēng chuàng mó delè)* "VAE" is translated as "自变量 autoencoder" (zì biàn xiàng autoencoder)* "OOD" is translated as "外围数据" (wài yù shù jí)* "ER" is translated as "错误减少" (cuò wù jiǎn shang)* "Kolmogorov complexity" is translated as "科玛戈罗夫复杂度" (kē mǎ gē luó fù zhòng dù)
</details></li>
</ul>
<hr>
<h2 id="BeeTLe-A-Framework-for-Linear-B-Cell-Epitope-Prediction-and-Classification"><a href="#BeeTLe-A-Framework-for-Linear-B-Cell-Epitope-Prediction-and-Classification" class="headerlink" title="BeeTLe: A Framework for Linear B-Cell Epitope Prediction and Classification"></a>BeeTLe: A Framework for Linear B-Cell Epitope Prediction and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02071">http://arxiv.org/abs/2309.02071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanx749/bcell">https://github.com/yuanx749/bcell</a></li>
<li>paper_authors: Xiao Yuan</li>
<li>for: 这个论文的目的是提出一种新的深度学习基于多任务框架，用于线性B细胞抗体复合体预测和抗体类型特定的复合体分类。</li>
<li>methods: 该论文提出了一种基于序列 neural network 模型，使用回归层和 Transformer 块来实现。此外，还提出了一种基于对角化的胺基encoding方法，以帮助模型学习复合体的表示。</li>
<li>results: 实验结果表明，提出的方法有效地预测B细胞抗体复合体，并且与竞争方法相比，表现出色。<details>
<summary>Abstract</summary>
The process of identifying and characterizing B-cell epitopes, which are the portions of antigens recognized by antibodies, is important for our understanding of the immune system, and for many applications including vaccine development, therapeutics, and diagnostics. Computational epitope prediction is challenging yet rewarding as it significantly reduces the time and cost of laboratory work. Most of the existing tools do not have satisfactory performance and only discriminate epitopes from non-epitopes. This paper presents a new deep learning-based multi-task framework for linear B-cell epitope prediction as well as antibody type-specific epitope classification. Specifically, a sequenced-based neural network model using recurrent layers and Transformer blocks is developed. We propose an amino acid encoding method based on eigen decomposition to help the model learn the representations of epitopes. We introduce modifications to standard cross-entropy loss functions by extending a logit adjustment technique to cope with the class imbalance. Experimental results on data curated from the largest public epitope database demonstrate the validity of the proposed methods and the superior performance compared to competing ones.
</details>
<details>
<summary>摘要</summary>
“识别和描述B细胞结构的过程是免疫系统理解的重要部分，并有许多应用，包括疫苗开发、治疗和诊断。计算epitope预测是具有挑战性的，但是可以大幅降低实验室工作的时间和成本。现有的工具大多数无法达到满意的性能，只能区分epitope和非epitope。本文提出了一个新的深度学习多任务框架，用于直线B细胞epitope预测和抗体类型特定epitope分类。具体来说，我们使用序列化的神经网络模型，包括回传层和Transformer层。我们提出了一个使用对角解析法来编码氨基酸的方法，帮助模型学习epitope的表现。我们也提出了对标准十字熵损失函数进行修改，以应对分布不对称。实验结果显示，提出的方法有效性和与竞争方法相比较高的性能。”
</details></li>
</ul>
<hr>
<h2 id="Efficiency-is-Not-Enough-A-Critical-Perspective-of-Environmentally-Sustainable-AI"><a href="#Efficiency-is-Not-Enough-A-Critical-Perspective-of-Environmentally-Sustainable-AI" class="headerlink" title="Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI"></a>Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02065">http://arxiv.org/abs/2309.02065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Wright, Christian Igel, Gabrielle Samuel, Raghavendra Selvan<br>for: 本文旨在探讨机器学习（ML）技术的环境可持续性问题，并 argue against solely focusing on efficiency as the solution.methods: 本文使用高级数学（DL）和其他ML方法，以及系统思维来探讨ML技术对环境的影响。results: 本文认为，提高ML系统的效率并不能够完全解决其对环境的影响，而需要考虑多个变量的交互作用。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) is currently spearheaded by machine learning (ML) methods such as deep learning (DL) which have accelerated progress on many tasks thought to be out of reach of AI. These ML methods can often be compute hungry, energy intensive, and result in significant carbon emissions, a known driver of anthropogenic climate change. Additionally, the platforms on which ML systems run are associated with environmental impacts including and beyond carbon emissions. The solution lionized by both industry and the ML community to improve the environmental sustainability of ML is to increase the efficiency with which ML systems operate in terms of both compute and energy consumption. In this perspective, we argue that efficiency alone is not enough to make ML as a technology environmentally sustainable. We do so by presenting three high level discrepancies between the effect of efficiency on the environmental sustainability of ML when considering the many variables which it interacts with. In doing so, we comprehensively demonstrate, at multiple levels of granularity both technical and non-technical reasons, why efficiency is not enough to fully remedy the environmental impacts of ML. Based on this, we present and argue for systems thinking as a viable path towards improving the environmental sustainability of ML holistically.
</details>
<details>
<summary>摘要</summary>
然而，我们认为效率alone是不足以使ML成为可持续的技术。我们这样做的原因在于，当ML系统与多个变数互动时，增加效率对环境可持续性的影响是复杂的。为了解释这个观点，我们在这篇文章中提出了三个高度不一致的问题，它们是：1. 碳排放和能源消耗之间的复杂关系。2. ML系统的可持续性受到多个因素的影响，包括硬件、软件、供应链和使用者。3. 增加效率可能会导致新的环境和社会影响，例如对于资源的掌控和可持续性。这些问题表明，增加ML系统的效率 alone 不能全面解决环境可持续性的问题。因此，我们提出了以系统思维为基础的可持续性解决方案，以确保ML技术在环境和社会方面的影响是可持续的。
</details></li>
</ul>
<hr>
<h2 id="MvFS-Multi-view-Feature-Selection-for-Recommender-System"><a href="#MvFS-Multi-view-Feature-Selection-for-Recommender-System" class="headerlink" title="MvFS: Multi-view Feature Selection for Recommender System"></a>MvFS: Multi-view Feature Selection for Recommender System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02064">http://arxiv.org/abs/2309.02064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youngjune Lee, Yeongjong Jeong, Keunchan Park, SeongKu Kang</li>
<li>for: 提高 recommender systems 中 feature selection 的性能，适应不同数据场景。</li>
<li>methods: 使用多视图网络和独立的重要性分数模型，避免特征选择过程受主导特征的偏袋问题，从而更有效地选择有用的特征。</li>
<li>results: 对实际数据进行实验，证明 MvFS 比state-of-the-art基elines更有效。<details>
<summary>Abstract</summary>
Feature selection, which is a technique to select key features in recommender systems, has received increasing research attention. Recently, Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations in that its selection process could be easily biased to major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective importance score modeling strategy which is applied independently to each field without incurring dependency among features. Experimental results on real-world datasets demonstrate the effectiveness of MvFS compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
Feature selection, which is a technique used in recommender systems to select key features, has recently received increasing research attention. Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations, as its selection process can be easily biased towards major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective importance score modeling strategy which is applied independently to each field without incurring dependency among features. Experimental results on real-world datasets demonstrate the effectiveness of MvFS compared to state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="No-Regret-Caching-with-Noisy-Request-Estimates"><a href="#No-Regret-Caching-with-Noisy-Request-Estimates" class="headerlink" title="No-Regret Caching with Noisy Request Estimates"></a>No-Regret Caching with Noisy Request Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02055">http://arxiv.org/abs/2309.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Younes Ben Mazziane, Francescomaria Faticanti, Giovanni Neglia, Sara Alouf</li>
<li>for: 这个论文目的是设计缓存策略，以满足在高负荷和&#x2F;或内存约束的情况下的缓存需求。</li>
<li>methods: 这个论文使用了在线学习算法，以实现缓存策略的设计，并且对缓存请求序列进行了预测。</li>
<li>results: 该论文提出了一种名为“听雷雨 Follow the Perturbed Leader”（NFPL）算法，该算法可以在缓存请求序列中受到噪声影响时，实现低 regret 的缓存策略。此外，该论文还进行了对 классические缓存策略的比较，并在实验中验证了提议的方法的可行性。<details>
<summary>Abstract</summary>
Online learning algorithms have been successfully used to design caching policies with regret guarantees. Existing algorithms assume that the cache knows the exact request sequence, but this may not be feasible in high load and/or memory-constrained scenarios, where the cache may have access only to sampled requests or to approximate requests' counters. In this paper, we propose the Noisy-Follow-the-Perturbed-Leader (NFPL) algorithm, a variant of the classic Follow-the-Perturbed-Leader (FPL) when request estimates are noisy, and we show that the proposed solution has sublinear regret under specific conditions on the requests estimator. The experimental evaluation compares the proposed solution against classic caching policies and validates the proposed approach under both synthetic and real request traces.
</details>
<details>
<summary>摘要</summary>
在线学习算法已经成功地用于设计缓存策略，并且提供了 regret 保证。现有的算法假设缓存知道正确的请求序列，但在高负荷和/或内存压力高的enario中，这可能并不是可行的，因为缓存可能只有对请求数据进行采样或估计。在这篇论文中，我们提出了听从噪声扰动领导者（NFPL）算法，这是经典的跟踪扰动领导者（FPL）算法的变种，当请求估计具有噪声时。我们证明了我们的解决方案在特定的请求估计条件下具有下降式 regret。实验评估比较了我们的解决方案与经典缓存策略，并在 synthetic 和实际请求轨迹上验证了我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Model-agnostic-network-inference-enhancement-from-noisy-measurements-via-curriculum-learning"><a href="#Model-agnostic-network-inference-enhancement-from-noisy-measurements-via-curriculum-learning" class="headerlink" title="Model-agnostic network inference enhancement from noisy measurements via curriculum learning"></a>Model-agnostic network inference enhancement from noisy measurements via curriculum learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02050">http://arxiv.org/abs/2309.02050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoyuans/manie">https://github.com/xiaoyuans/manie</a></li>
<li>paper_authors: Kai Wu, Yuanyuan Li, Jing Liu</li>
<li>for: 提高网络推理模型对噪声的抵抗性能</li>
<li>methods: curriculum learning + 模型自适应阈值调整 + 数据 augmentation</li>
<li>results: 在多种噪声环境下，提高了各种网络推理模型的性能，特别是在清晰样本充沥的情况下表现出色<details>
<summary>Abstract</summary>
Noise is a pervasive element within real-world measurement data, significantly undermining the performance of network inference models. However, the quest for a comprehensive enhancement framework capable of bolstering noise resistance across a diverse array of network inference models has remained elusive. Here, we present an elegant and efficient framework tailored to amplify the capabilities of network inference models in the presence of noise. Leveraging curriculum learning, we mitigate the deleterious impact of noisy samples on network inference models. Our proposed framework is model-agnostic, seamlessly integrable into a plethora of model-based and model-free network inference methods. Notably, we utilize one model-based and three model-free network inference methods as the foundation. Extensive experimentation across various synthetic and real-world networks, encapsulating diverse nonlinear dynamic processes, showcases substantial performance augmentation under varied noise types, particularly thriving in scenarios enriched with clean samples. This framework's adeptness in fortifying both model-free and model-based network inference methodologies paves the avenue towards a comprehensive and unified enhancement framework, encompassing the entire spectrum of network inference models. Available Code: https://github.com/xiaoyuans/MANIE.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>噪声是现实世界测量数据中的一种普遍存在的元素，对网络推理模型的性能产生了重要的影响。然而，找到一个全面提高抗噪抗性的框架，能够在多种网络推理模型上提高性能，一直未能实现。在这里，我们提出了一个简洁而高效的框架，用于增强网络推理模型在噪声存在下的性能。我们利用课程学习，以mitigate噪声样本对网络推理模型的负面影响。我们提posed的框架是模型无关的，可以轻松地整合到多种模型基于和模型自由的网络推理方法中。特别是，我们使用了一个模型基于的和三个模型自由的网络推理方法作为基础。经过对多种人工和实际网络、包括多种非线性动力学过程的广泛实验，表明我们的框架在不同的噪声类型下具有显著的性能提高，特别是在充满清晰样本的场景下表现出色。这种框架的能力在加强模型自由和模型基于的网络推理方法方面表现出了广泛的可用性，为建立一个涵盖整个网络推理模型谱系的全面和统一的提高框架提供了道路。可以在 GitHub 上获取代码：https://github.com/xiaoyuans/MANIE。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Self-supervised-Learning-via-Scoring-Rules-Minimization"><a href="#Probabilistic-Self-supervised-Learning-via-Scoring-Rules-Minimization" class="headerlink" title="Probabilistic Self-supervised Learning via Scoring Rules Minimization"></a>Probabilistic Self-supervised Learning via Scoring Rules Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02048">http://arxiv.org/abs/2309.02048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Vahidi, Simon Schoßer, Lisa Wimmer, Yawei Li, Bernd Bischl, Eyke Hüllermeier, Mina Rezaei</li>
<li>For: 提高表示质量和避免归一化表示* Methods: 使用 probabilistic models 和知识传播来增强表示质量，并提出一种新的损失函数基于适当的分数规则* Results: 在多种下游任务上达到了superior的准确率和准确性，比自我超vised基线在广泛的实验中表现出色，demonstrating scalability and real-world applicability.<details>
<summary>Abstract</summary>
In this paper, we propose a novel probabilistic self-supervised learning via Scoring Rule Minimization (ProSMIN), which leverages the power of probabilistic models to enhance representation quality and mitigate collapsing representations. Our proposed approach involves two neural networks; the online network and the target network, which collaborate and learn the diverse distribution of representations from each other through knowledge distillation. By presenting the input samples in two augmented formats, the online network is trained to predict the target network representation of the same sample under a different augmented view. The two networks are trained via our new loss function based on proper scoring rules. We provide a theoretical justification for ProSMIN's convergence, demonstrating the strict propriety of its modified scoring rule. This insight validates the method's optimization process and contributes to its robustness and effectiveness in improving representation quality. We evaluate our probabilistic model on various downstream tasks, such as in-distribution generalization, out-of-distribution detection, dataset corruption, low-shot learning, and transfer learning. Our method achieves superior accuracy and calibration, surpassing the self-supervised baseline in a wide range of experiments on large-scale datasets like ImageNet-O and ImageNet-C, ProSMIN demonstrates its scalability and real-world applicability.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的概率自编学习方法，即 Scoring Rule Minimization（ProSMIN），它利用概率模型来提高表示质量和消除塑性表示。我们的提议方法包括两个神经网络：在线网络和目标网络，它们相互合作，通过知识传递学习来学习各自的多样化表示分布。我们将输入样本提供两种扩展视图，使在线网络通过预测目标网络对同一个样本的不同扩展视图的表示来训练。我们使用我们新提出的损失函数，基于正确的探索规则。我们提供了对ProSMIN的优化过程的理论正确性的证明，这种视角证明了其优化过程的正确性和效果性，从而提高了表示质量。我们在多种下游任务上评估了我们的概率模型，包括内部分布式、外部分布式、数据损害、低学习率和转移学习等。我们的方法在各种实验中具有优于自编学习基eline的高精度和抗混淆性。
</details></li>
</ul>
<hr>
<h2 id="Data-Juicer-A-One-Stop-Data-Processing-System-for-Large-Language-Models"><a href="#Data-Juicer-A-One-Stop-Data-Processing-System-for-Large-Language-Models" class="headerlink" title="Data-Juicer: A One-Stop Data Processing System for Large Language Models"></a>Data-Juicer: A One-Stop Data Processing System for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02033">http://arxiv.org/abs/2309.02033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba/data-juicer">https://github.com/alibaba/data-juicer</a></li>
<li>paper_authors: Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, Jingren Zhou</li>
<li>for:  LLama 大语模型数据处理</li>
<li>methods:  Data-Juicer 提供50多个内置Operator和可插入工具，以提高模块化、可组合、可扩展性，用于多种 LLama 数据处理需求。</li>
<li>results:  Empirical validation reveals up to 7.45% relative improvement in LLaMA performance, and up to 88.7% reduction in single-machine processing time.<details>
<summary>Abstract</summary>
The immense evolution in Large Language Models (LLMs) has underscored the importance of massive, diverse, and high-quality data. Despite this, existing open-source tools for LLM data processing remain limited and mostly tailored to specific datasets, with an emphasis on the reproducibility of released data over adaptability and usability, inhibiting potential applications. In response, we propose a one-stop, powerful yet flexible and user-friendly LLM data processing system named Data-Juicer. Our system offers over 50 built-in versatile operators and pluggable tools, which synergize modularity, composability, and extensibility dedicated to diverse LLM data processing needs. By incorporating visualized and automatic evaluation capabilities, Data-Juicer enables a timely feedback loop to accelerate data processing and gain data insights. To enhance usability, Data-Juicer provides out-of-the-box components for users with various backgrounds, and fruitful data recipes for LLM pre-training and post-tuning usages. Further, we employ multi-facet system optimization and seamlessly integrate Data-Juicer with both LLM and distributed computing ecosystems, to enable efficient and scalable data processing. Empirical validation of the generated data recipes reveals considerable improvements in LLaMA performance for various pre-training and post-tuning cases, demonstrating up to 7.45% relative improvement of averaged score across 16 LLM benchmarks and 16.25% higher win rate using pair-wise GPT-4 evaluation. The system's efficiency and scalability are also validated, supported by up to 88.7% reduction in single-machine processing time, 77.1% and 73.1% less memory and CPU usage respectively, and 7.91x processing acceleration when utilizing distributed computing ecosystems. Our system, data recipes, and multiple tutorial demos are released, calling for broader research centered on LLM data.
</details>
<details>
<summary>摘要</summary>
“巨大的语言模型（LLM）演化带来了大量、多样化和高质量数据的重要性。然而，现有的开源工具 для LLM 数据处理仍然有限，主要是为特定数据集设计的，强调数据重现性而不是适应性和用户友好性，这限制了其应用前景。为此，我们提出了一个一站式、强大 yet 灵活和用户友好的 LLM 数据处理系统，名为 Data-Juicer。我们的系统提供了50多种快速组合和可插入工具，以提高模块性、可 compose 性和扩展性，以满足不同 LLM 数据处理需求。通过包含可视化和自动评估功能，Data-Juicer 可以提供时效的反馈循环，加速数据处理并获得数据视图。为了提高可用性，Data-Juicer 提供了适合不同背景的准备组件，以及 LLMA 预训练和后处理用例的有用数据荚。此外，我们采用多方面优化和与 LLM 和分布式计算环境集成，以实现高效和可扩展的数据处理。我们的实验 validate 了生成的数据荚，显示 LLMA 性能提高明显，在16个 LLMBenchmark 和16个 GPT-4 评估中，相对提高7.45%的平均分数，并在对比评估中提高16.25%的胜率。系统的效率和扩展性也得到了 validate，包括单机处理时间减少88.7%、内存和CPU使用量减少77.1%和73.1%，以及使用分布式计算环境时的处理加速7.91倍。我们的系统、数据荚和多个教程示例都已经发布，呼吁更广泛的 LLM 数据研究。”
</details></li>
</ul>
<hr>
<h2 id="Non-Parametric-Representation-Learning-with-Kernels"><a href="#Non-Parametric-Representation-Learning-with-Kernels" class="headerlink" title="Non-Parametric Representation Learning with Kernels"></a>Non-Parametric Representation Learning with Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02028">http://arxiv.org/abs/2309.02028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Pascal Esser, Maximilian Fleissner, Debarghya Ghoshdastidar</li>
<li>for: 学习无监督的特征表示，从无标签数据中学习有用的特征。</li>
<li>methods: 使用kernel-based方法进行表示学习，包括对冲损函数和自适应 Encoder（AE）模型。</li>
<li>results: 提出了新的表示理论，并 derive了泛化误差上限，进行表示学习的评估。<details>
<summary>Abstract</summary>
Unsupervised and self-supervised representation learning has become popular in recent years for learning useful features from unlabelled data. Representation learning has been mostly developed in the neural network literature, and other models for representation learning are surprisingly unexplored. In this work, we introduce and analyze several kernel-based representation learning approaches: Firstly, we define two kernel Self-Supervised Learning (SSL) models using contrastive loss functions and secondly, a Kernel Autoencoder (AE) model based on the idea of embedding and reconstructing data. We argue that the classical representer theorems for supervised kernel machines are not always applicable for (self-supervised) representation learning, and present new representer theorems, which show that the representations learned by our kernel models can be expressed in terms of kernel matrices. We further derive generalisation error bounds for representation learning with kernel SSL and AE, and empirically evaluate the performance of these methods in both small data regimes as well as in comparison with neural network based models.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese无监督和自监督表示学习在最近几年内变得非常流行，以学习无标记数据中的有用特征。表示学习主要发展在神经网络文献中，而其他模型表示学习却尚未得到探索。在这项工作中，我们引入并分析了几种基于核函数的表示学习方法：首先，我们定义了两种抽象损失函数基于自我监督学习（SSL）模型，其次，基于数据嵌入和重建的核自动编码（AE）模型。我们认为经典的supervised机器学习的表示定理不一定适用于（自监督）表示学习，并提出了新的表示定理，其中表示学习得到的表示可以表示为核矩阵。我们进一步 deriv Generalization Error bounds for representation learning with kernel SSL和AE，并对这些方法在小数据 régime和与神经网络模型相比进行实验评估。Note: "Simplified Chinese" is a romanization of Chinese characters, which is used to represent the language in the Latin alphabet. It is not a translation of the text into Traditional Chinese, which is a different writing system.
</details></li>
</ul>
<hr>
<h2 id="Granger-Causal-Inference-in-Multivariate-Hawkes-Processes-by-Minimum-Message-Length"><a href="#Granger-Causal-Inference-in-Multivariate-Hawkes-Processes-by-Minimum-Message-Length" class="headerlink" title="Granger Causal Inference in Multivariate Hawkes Processes by Minimum Message Length"></a>Granger Causal Inference in Multivariate Hawkes Processes by Minimum Message Length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02027">http://arxiv.org/abs/2309.02027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katerina Hlavackova-Schindler, Anna Melnykova, Irene Tubikanec</li>
<li>for: 这个论文主要针对多变量郝克过程（MHPs）中的连接图生成和选择问题，并提出一种基于最小消息长度（MML）原理的优化 критерион和模型选择算法。</li>
<li>methods: 该论文使用了扩展衰减函数和优化 критерион，并通过比较不同模型对数据的适应度和 concise度来选择最佳模型。</li>
<li>results: 对于短时间适应度较高的场景，该方法可以达到最高的 F1 分数，并在具有特定稀疏图设置下进行了数值研究。 更进一步，通过应用于 G7 财政债券数据，该方法可以获得一致的 causal 连接，与专业知识一致。<details>
<summary>Abstract</summary>
Multivariate Hawkes processes (MHPs) are versatile probabilistic tools used to model various real-life phenomena: earthquakes, operations on stock markets, neuronal activity, virus propagation and many others. In this paper, we focus on MHPs with exponential decay kernels and estimate connectivity graphs, which represent the Granger causal relations between their components. We approach this inference problem by proposing an optimization criterion and model selection algorithm based on the minimum message length (MML) principle. MML compares Granger causal models using the Occam's razor principle in the following way: even when models have a comparable goodness-of-fit to the observed data, the one generating the most concise explanation of the data is preferred. While most of the state-of-art methods using lasso-type penalization tend to overfitting in scenarios with short time horizons, the proposed MML-based method achieves high F1 scores in these settings. We conduct a numerical study comparing the proposed algorithm to other related classical and state-of-art methods, where we achieve the highest F1 scores in specific sparse graph settings. We illustrate the proposed method also on G7 sovereign bond data and obtain causal connections, which are in agreement with the expert knowledge available in the literature.
</details>
<details>
<summary>摘要</summary>
多变量庞克过程（MHP）是一种通用的概率工具，用于模拟各种实际场景：地震、股票市场交易、神经元活动、病毒传播等。在这篇论文中，我们关注MHP中的凝聚函数和抽象树的估计问题。我们使用最小消息长度（MML）原理来解决这个问题，MML比较不同庞克模型的适应度，并选择最简洁的模型。大多数当前的方法使用lasso类型的惩罚往往会过拟合短时间尺度下的场景，而我们提出的MML基于方法在这些设置下达到了最高的F1分数。我们进行了一个数字实验，比较了我们的方法和其他相关的古典和当前状态的方法，我们在特定的稀疏图设置下达到了最高的F1分数。我们还应用了我们的方法在G7国债数据中，并获得了一致的 causal 连接，与文献中的专家知识一致。
</details></li>
</ul>
<hr>
<h2 id="RDGSL-Dynamic-Graph-Representation-Learning-with-Structure-Learning"><a href="#RDGSL-Dynamic-Graph-Representation-Learning-with-Structure-Learning" class="headerlink" title="RDGSL: Dynamic Graph Representation Learning with Structure Learning"></a>RDGSL: Dynamic Graph Representation Learning with Structure Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02025">http://arxiv.org/abs/2309.02025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Zhang, Yun Xiong, Yao Zhang, Yiheng Sun, Xi Chen, Yizhu Jiao, Yangyong Zhu</li>
<li>for: 本研究旨在学习 kontinuous-time 动态图 Representation，以提高下游任务的效果。</li>
<li>methods: 本研究提出了 RDGSL 方法，具有 dynamic graph structure learning 和 Temporal Embedding Learner 两个重要组成部分。dynamic graph structure learning 可以有效地抑制噪声，Temporal Embedding Learner 可以选择ively ignore 噪声边，以提高 Representation 的表达力。</li>
<li>results: 本研究的方法在 downstream 任务中表现出了5.1% 绝对 AUC 提升，与第二个基线相比。<details>
<summary>Abstract</summary>
Temporal Graph Networks (TGNs) have shown remarkable performance in learning representation for continuous-time dynamic graphs. However, real-world dynamic graphs typically contain diverse and intricate noise. Noise can significantly degrade the quality of representation generation, impeding the effectiveness of TGNs in downstream tasks. Though structure learning is widely applied to mitigate noise in static graphs, its adaptation to dynamic graph settings poses two significant challenges. i) Noise dynamics. Existing structure learning methods are ill-equipped to address the temporal aspect of noise, hampering their effectiveness in such dynamic and ever-changing noise patterns. ii) More severe noise. Noise may be introduced along with multiple interactions between two nodes, leading to the re-pollution of these nodes and consequently causing more severe noise compared to static graphs. In this paper, we present RDGSL, a representation learning method in continuous-time dynamic graphs. Meanwhile, we propose dynamic graph structure learning, a novel supervisory signal that empowers RDGSL with the ability to effectively combat noise in dynamic graphs. To address the noise dynamics issue, we introduce the Dynamic Graph Filter, where we innovatively propose a dynamic noise function that dynamically captures both current and historical noise, enabling us to assess the temporal aspect of noise and generate a denoised graph. We further propose the Temporal Embedding Learner to tackle the challenge of more severe noise, which utilizes an attention mechanism to selectively turn a blind eye to noisy edges and hence focus on normal edges, enhancing the expressiveness for representation generation that remains resilient to noise. Our method demonstrates robustness towards downstream tasks, resulting in up to 5.1% absolute AUC improvement in evolving classification versus the second-best baseline.
</details>
<details>
<summary>摘要</summary>
temps 图网络（TGNs）在学习 continuous-time 动态图 Representation 方面表现出色，但实际世界中的动态图通常具有多样化和复杂的噪音。噪音可以对 Representation 生成质量产生重要影响，从而降低 TGNs 在下游任务中的效果。虽然结构学习在静止图中广泛应用，但其在动态图设置中存在两个主要挑战。i) 噪音动态性。现有的结构学习方法无法 Address 动态噪音的问题，因此其效iveness 在这些动态和改变中的噪音模式下受限。ii) 更严重的噪音。噪音可能会在两个节点之间多种交互中引入，导致这些节点重新污染，从而导致更严重的噪音 compared to 静止图。在这篇文章中，我们提出了 RDGSL，一种在 continuous-time 动态图上进行 Representation 学习的方法。同时，我们提出了动态图结构学习，一种新的监督信号，可以让 RDGSL 在动态图上有效地抗抗噪音。为 Address 噪音动态性问题，我们引入了动态噪音函数，可以动态地捕捉当前和历史噪音，使我们可以评估动态图中噪音的 temporal 方面，并生成一个 Denoised 图。此外，我们还提出了时间 Embedding Learner，可以在更严重的噪音下提高 Representation 生成的表达能力。我们的方法在下游任务中表现了Robustness，相比第二Best baseline，我们的方法在 evolving 分类中实现了5.1%的绝对 AUC 提升。
</details></li>
</ul>
<hr>
<h2 id="PROMISE-Preconditioned-Stochastic-Optimization-Methods-by-Incorporating-Scalable-Curvature-Estimates"><a href="#PROMISE-Preconditioned-Stochastic-Optimization-Methods-by-Incorporating-Scalable-Curvature-Estimates" class="headerlink" title="PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates"></a>PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02014">http://arxiv.org/abs/2309.02014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary Frangella, Pratik Rathore, Shipu Zhao, Madeleine Udell</li>
<li>for: 解决大规模的几何优化问题，如机器学习中的ridge和logistic回归问题。</li>
<li>methods: 使用绘制技术来实现预处理的渐进搜索法，包括SVRG、SAGA和Katyusha等方法，每个方法都有强大的理论分析和有效的默认超参数设置。</li>
<li>results: 经验表明，提出的方法可以在 default 超参数设置下超过或与通过手动调整的梯度搜索优化器相比，并且在实际中也能够更快地达到 globally 线性减少。<details>
<summary>Abstract</summary>
This paper introduces PROMISE ($\textbf{Pr}$econditioned Stochastic $\textbf{O}$ptimization $\textbf{M}$ethods by $\textbf{I}$ncorporating $\textbf{S}$calable Curvature $\textbf{E}$stimates), a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems arising in machine learning. PROMISE includes preconditioned versions of SVRG, SAGA, and Katyusha; each algorithm comes with a strong theoretical analysis and effective default hyperparameter values. In contrast, traditional stochastic gradient methods require careful hyperparameter tuning to succeed, and degrade in the presence of ill-conditioning, a ubiquitous phenomenon in machine learning. Empirically, we verify the superiority of the proposed algorithms by showing that, using default hyperparameter values, they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems assembled from benchmark machine learning repositories. On the theoretical side, this paper introduces the notion of quadratic regularity in order to establish linear convergence of all proposed methods even when the preconditioner is updated infrequently. The speed of linear convergence is determined by the quadratic regularity ratio, which often provides a tighter bound on the convergence rate compared to the condition number, both in theory and in practice, and explains the fast global linear convergence of the proposed methods.
</details>
<details>
<summary>摘要</summary>
Empirically, we demonstrate the superiority of the proposed algorithms by showing that they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems from benchmark machine learning repositories, using default hyperparameter values.Theoretically, this paper introduces the concept of quadratic regularity to establish the linear convergence of all proposed methods, even when the preconditioner is updated infrequently. The speed of linear convergence is determined by the quadratic regularity ratio, which often provides a tighter bound on the convergence rate compared to the condition number, both in theory and in practice. This explains the fast global linear convergence of the proposed methods.
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-Dynamics-of-Self-Supervised-Models"><a href="#Representation-Learning-Dynamics-of-Self-Supervised-Models" class="headerlink" title="Representation Learning Dynamics of Self-Supervised Models"></a>Representation Learning Dynamics of Self-Supervised Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02011">http://arxiv.org/abs/2309.02011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Esser, Satyaki Mukherjee, Debarghya Ghoshdastidar</li>
<li>for: 本研究探讨了无监督学习（Self-Supervised Learning）模型中的学习动力学，具体来说是对减少对比和非对比损失来获得的表示进行研究。</li>
<li>methods: 作者使用了多变量回归模型的动力学来探讨SSL模型的学习动力学，并提出了包含对齐约束的SSL目标函数。</li>
<li>results: 研究发现，使用 gradient descent 在 Grassmannian  manifold 上训练 SSL 模型时，模型会学习简单的标量表示，导致维度减少现象出现。作者还证明了无监督学习模型在无穷宽approximation中与supervised模型之间存在很大差异。<details>
<summary>Abstract</summary>
Self-Supervised Learning (SSL) is an important paradigm for learning representations from unlabelled data, and SSL with neural networks has been highly successful in practice. However current theoretical analysis of SSL is mostly restricted to generalisation error bounds. In contrast, learning dynamics often provide a precise characterisation of the behaviour of neural networks based models but, so far, are mainly known in supervised settings. In this paper, we study the learning dynamics of SSL models, specifically representations obtained by minimising contrastive and non-contrastive losses. We show that a naive extension of the dymanics of multivariate regression to SSL leads to learning trivial scalar representations that demonstrates dimension collapse in SSL. Consequently, we formulate SSL objectives with orthogonality constraints on the weights, and derive the exact (network width independent) learning dynamics of the SSL models trained using gradient descent on the Grassmannian manifold. We also argue that the infinite width approximation of SSL models significantly deviate from the neural tangent kernel approximations of supervised models. We numerically illustrate the validity of our theoretical findings, and discuss how the presented results provide a framework for further theoretical analysis of contrastive and non-contrastive SSL.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Establishing-a-real-time-traffic-alarm-in-the-city-of-Valencia-with-Deep-Learning"><a href="#Establishing-a-real-time-traffic-alarm-in-the-city-of-Valencia-with-Deep-Learning" class="headerlink" title="Establishing a real-time traffic alarm in the city of Valencia with Deep Learning"></a>Establishing a real-time traffic alarm in the city of Valencia with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02010">http://arxiv.org/abs/2309.02010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Folgado, Veronica Sanz, Johannes Hirn, Edgar Lorenzo-Saez, Javier Urchueguia</li>
<li>for: 这项研究的目的是分析城市劳伦сия（Valencia，西班牙）的交通征流与污染物的相关性，以及开发一种用于预测下一30分钟内特定街区是否会出现异常高交通流的警报系统。</li>
<li>methods: 该研究使用了2018年的交通数据，通过Long Short-Term Memory（LSTM）神经网络进行预测，并在2019年的交通数据上进行测试。</li>
<li>results: 研究发现，交通征流对某些污染物（特别是$\text{NO}_\text{x}$）的水平有显著影响。同时，该研究开发出了一种独立的三级水平警报系统，可以预测特定街区在下一30分钟内是否会出现异常高交通流。<details>
<summary>Abstract</summary>
Urban traffic emissions represent a significant concern due to their detrimental impacts on both public health and the environment. Consequently, decision-makers have flagged their reduction as a crucial goal. In this study, we first analyze the correlation between traffic flux and pollution in the city of Valencia, Spain. Our results demonstrate that traffic has a significant impact on the levels of certain pollutants (especially $\text{NO}_\text{x}$). Secondly, we develop an alarm system to predict if a street is likely to experience unusually high traffic in the next 30 minutes, using an independent three-tier level for each street. To make the predictions, we use traffic data updated every 10 minutes and Long Short-Term Memory (LSTM) neural networks. We trained the LSTM using traffic data from 2018, and tested it using traffic data from 2019.
</details>
<details>
<summary>摘要</summary>
城市交通排放对公共健康和环境产生了重要的影响，因此决策者们将其减少列为重要目标。在这项研究中，我们首先分析了Valencia市的交通流和污染物之间的相关性。我们的结果显示，交通具有对某些污染物（尤其是$\text{NO}_\text{x}$）的显著影响。其次，我们开发了一个预测在下一个30分钟内街道是否会出现异常高交通流的警示系统，并将每条街道分为三级水平。为了进行预测，我们使用了每10分钟更新的交通数据和Long Short-Term Memory（LSTM）神经网络。我们使用2018年的交通数据进行训练，并在2019年的交通数据上进行测试。
</details></li>
</ul>
<hr>
<h2 id="An-LSTM-Based-Predictive-Monitoring-Method-for-Data-with-Time-varying-Variability"><a href="#An-LSTM-Based-Predictive-Monitoring-Method-for-Data-with-Time-varying-Variability" class="headerlink" title="An LSTM-Based Predictive Monitoring Method for Data with Time-varying Variability"></a>An LSTM-Based Predictive Monitoring Method for Data with Time-varying Variability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01978">http://arxiv.org/abs/2309.01978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Qiu, Yu Lin, Inez Zwetsloot</li>
<li>for: 本文旨在探讨使用循环神经网络（RNN）和其变体来实现预测性监测，以检测数据中的异常现象。</li>
<li>methods: 本文提出了基于长短期记忆（LSTM）预测 интерVAL的控制图，用于监测时间变化的数据。</li>
<li>results:  simulations 和实际应用表明，提出的方法在检测平均值变化时表现出色，并且在实际时系列感知器数据中得到了证明。<details>
<summary>Abstract</summary>
The recurrent neural network and its variants have shown great success in processing sequences in recent years. However, this deep neural network has not aroused much attention in anomaly detection through predictively process monitoring. Furthermore, the traditional statistic models work on assumptions and hypothesis tests, while neural network (NN) models do not need that many assumptions. This flexibility enables NN models to work efficiently on data with time-varying variability, a common inherent aspect of data in practice. This paper explores the ability of the recurrent neural network structure to monitor processes and proposes a control chart based on long short-term memory (LSTM) prediction intervals for data with time-varying variability. The simulation studies provide empirical evidence that the proposed model outperforms other NN-based predictive monitoring methods for mean shift detection. The proposed method is also applied to time series sensor data, which confirms that the proposed method is an effective technique for detecting abnormalities.
</details>
<details>
<summary>摘要</summary>
“Recurrent neural network（RNN）和其变体在过去几年内得到了广泛的成功，但它尚未吸引过多的注意力在预测过程监测中。此外，传统的统计模型基于假设和假设测试，而神经网络（NN）模型则不需要这么多假设。这种灵活性使得NN模型在数据中具有时变变量的效率，这是实际数据中的一个常见特征。本文探讨了RNN结构在监测过程中的能力，并提出了基于长期快短时尺度内预测 интерval的控制图。实验研究表明，提议的模型在mean shift探测方面表现出色，并且在时间序列感知数据中进行了有效的异常检测。”Note: The translation is in Simplified Chinese, which is one of the two standard Chinese dialects. The other dialect is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="AdaPlus-Integrating-Nesterov-Momentum-and-Precise-Stepsize-Adjustment-on-AdamW-Basis"><a href="#AdaPlus-Integrating-Nesterov-Momentum-and-Precise-Stepsize-Adjustment-on-AdamW-Basis" class="headerlink" title="AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis"></a>AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01966">http://arxiv.org/abs/2309.01966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Guan</li>
<li>for: 提出了一种高效的优化器 called AdaPlus，它将Nesterov冲击和精确步长调整与AdamW基础结合。</li>
<li>methods: 使用了AdamW、Nadam和AdaBelief的优点，而无需添加额外的超参数。</li>
<li>results: 通过对三个机器学习任务进行广泛的实验评估，证明了AdaPlus在图像分类任务中表现最优，并在语言模型任务和生成器任务中表现出最高的稳定性。<details>
<summary>Abstract</summary>
This paper proposes an efficient optimizer called AdaPlus which integrates Nesterov momentum and precise stepsize adjustment on AdamW basis. AdaPlus combines the advantages of AdamW, Nadam, and AdaBelief and, in particular, does not introduce any extra hyper-parameters. We perform extensive experimental evaluations on three machine learning tasks to validate the effectiveness of AdaPlus. The experiment results validate that AdaPlus (i) is the best adaptive method which performs most comparable with (even slightly better than) SGD with momentum on image classification tasks and (ii) outperforms other state-of-the-art optimizers on language modeling tasks and illustrates the highest stability when training GANs. The experiment code of AdaPlus is available at: https://github.com/guanleics/AdaPlus.
</details>
<details>
<summary>摘要</summary>
这份论文提出了一种高效的优化器called AdaPlus，它将Nesterov势量和精确步长调整 integrate到AdamW基础上。AdaPlus结合了AdamW、Nadam和AdaBelief的优点，并不添加任何额外hyper参数。我们在三个机器学习任务上进行了广泛的实验评估，以验证AdaPlus的效果。实验结果表明，AdaPlus：（1）在图像分类任务上与SGD势量几乎相同，甚至有slightly better的性能。（2）在语言模型任务上超过其他当前顶尖优化器。（3）在训练GAN时显示出最高稳定性。AdaPlus的实验代码可以在https://github.com/guanleics/AdaPlus中找到。
</details></li>
</ul>
<hr>
<h2 id="Developing-A-Fair-Individualized-Polysocial-Risk-Score-iPsRS-for-Identifying-Increased-Social-Risk-of-Hospitalizations-in-Patients-with-Type-2-Diabetes-T2D"><a href="#Developing-A-Fair-Individualized-Polysocial-Risk-Score-iPsRS-for-Identifying-Increased-Social-Risk-of-Hospitalizations-in-Patients-with-Type-2-Diabetes-T2D" class="headerlink" title="Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)"></a>Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02467">http://arxiv.org/abs/2309.02467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Huang, Jingchuan Guo, William T Donahoo, Zhengkang Fan, Ying Lu, Wei-Han Chen, Huilin Tang, Lori Bilello, Elizabeth A Shenkman, Jiang Bian<br>for: 这个研究的目的是开发一个基于电子健康记录（EHR）的机器学习（ML）分析管道，以识别患有型二糖尿病（T2D）患者的社会需求不足，并且对这些需求进行解释性AI（XAI）评估和优化。methods: 这个研究使用了大学佐华利 Integrated Data Repository（UFH IR）中的EHR数据，包括社会Determinants of health（SDoH）和个体级SDoH，并开发了一个基于EHR的ML分析管道，称为个体化多社会风险分数（iPsRS），以识别患有T2D的患者中的高社会风险。results: 我们的iPsRS在各个种族-民族群体中进行了公平优化后，C statistic为0.72，可以准确预测患有T2D的患者1年内的入院风险。iPsRS能够准确捕捉高入院风险的个体，实际1年内top 5%的iPsRS的入院率比底层分数段高约13倍。<details>
<summary>Abstract</summary>
Background: Racial and ethnic minority groups and individuals facing social disadvantages, which often stem from their social determinants of health (SDoH), bear a disproportionate burden of type 2 diabetes (T2D) and its complications. It is therefore crucial to implement effective social risk management strategies at the point of care. Objective: To develop an EHR-based machine learning (ML) analytical pipeline to identify the unmet social needs associated with hospitalization risk in patients with T2D. Methods: We identified 10,192 T2D patients from the EHR data (from 2012 to 2022) from the University of Florida Health Integrated Data Repository, including contextual SDoH (e.g., neighborhood deprivation) and individual-level SDoH (e.g., housing stability). We developed an electronic health records (EHR)-based machine learning (ML) analytic pipeline, namely individualized polysocial risk score (iPsRS), to identify high social risk associated with hospitalizations in T2D patients, along with explainable AI (XAI) techniques and fairness assessment and optimization. Results: Our iPsRS achieved a C statistic of 0.72 in predicting 1-year hospitalization after fairness optimization across racial-ethnic groups. The iPsRS showed excellent utility for capturing individuals at high hospitalization risk; the actual 1-year hospitalization rate in the top 5% of iPsRS was ~13 times as high as the bottom decile. Conclusion: Our ML pipeline iPsRS can fairly and accurately screen for patients who have increased social risk leading to hospitalization in T2D patients.
</details>
<details>
<summary>摘要</summary>
背景：种族和民族少数群体和受到社会不利条件影响的个人患有类型2糖尿病（T2D）和其并发症的负担较大。因此，在点患者处实施有效的社会风险管理策略是非常重要。目标：开发基于电子健康纪录（EHR）的机器学习（ML）分析管道，以识别患有T2D患者的社会需求不足，与住院风险相关。方法：我们从2012年至2022年的University of Florida Health Integrated Data Repository中提取了10,192名T2D患者的EHR数据，包括上下文性社会 determinants of health（SDoH）和个体级SDoH（如住房稳定）。我们开发了基于EHR的ML分析管道，称为个体化多社会风险分数（iPsRS），以识别患有T2D患者的高社会风险，同时使用可解释AI（XAI）技术和公平评估和优化。结果：我们的iPsRS在公平优化后，在不同种族-民族群体中的CStatistic为0.72，能够准确预测患有T2D患者1年内的住院风险。iPsRS表现出色地捕捉了高住院风险的个体，实际1年内患有T2D患者在top5%的iPsRS中住院率高达13倍于bottom decile。结论：我们的ML管道iPsRS可以公平、准确地在患有T2D患者中识别具有高社会风险的患者，并且可以通过可解释AI技术和公平评估和优化来提高其效果。
</details></li>
</ul>
<hr>
<h2 id="RoboAgent-Generalization-and-Efficiency-in-Robot-Manipulation-via-Semantic-Augmentations-and-Action-Chunking"><a href="#RoboAgent-Generalization-and-Efficiency-in-Robot-Manipulation-via-Semantic-Augmentations-and-Action-Chunking" class="headerlink" title="RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking"></a>RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01918">http://arxiv.org/abs/2309.01918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, Vikash Kumar</li>
<li>for: 这篇论文旨在开发一种能够快速 multiply 现有数据集，并提取高性能策略的 universal agent 训练系统。</li>
<li>methods: 该系统使用 semantic augmentations 和 action representations 来快速训练 universal agent，并使用可靠的任务条件和表达能力架构来实现多种 manipulate 技能。</li>
<li>results: 通过使用仅 7500 示例，该系统可以训练一个可以执行 12 种技能的 universal agent，并在不同的 kitchen 场景中展示其普遍性和多样性。在未seen 情况下，RoboAgent 的性能高于先前方法，并且更加 Sample Efficient。视频详情请参考 <a target="_blank" rel="noopener" href="https://robopen.github.io/">https://robopen.github.io/</a>.<details>
<summary>Abstract</summary>
The grand aim of having a single robot that can manipulate arbitrary objects in diverse settings is at odds with the paucity of robotics datasets. Acquiring and growing such datasets is strenuous due to manual efforts, operational costs, and safety challenges. A path toward such an universal agent would require a structured framework capable of wide generalization but trained within a reasonable data budget. In this paper, we develop an efficient system (RoboAgent) for training universal agents capable of multi-task manipulation skills using (a) semantic augmentations that can rapidly multiply existing datasets and (b) action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting. In addition, reliable task conditioning and an expressive policy architecture enable our agent to exhibit a diverse repertoire of skills in novel situations specified using language commands. Using merely 7500 demonstrations, we are able to train a single agent capable of 12 unique skills, and demonstrate its generalization over 38 tasks spread across common daily activities in diverse kitchen scenes. On average, RoboAgent outperforms prior methods by over 40% in unseen situations while being more sample efficient and being amenable to capability improvements and extensions through fine-tuning. Videos at https://robopen.github.io/
</details>
<details>
<summary>摘要</summary>
“我们的大目标是建立一个可以 manipulate 任意物品的多功能机器人，但是实际上存在着机器人学习数据的稀缺。 acquiring 和 growing 这些数据需要许多人工干预、操作成本和安全挑战。为了实现这个目标，我们需要一个结构化的框架，可以实现广泛的普遍化，并在有限的数据预算下训练。在这篇论文中，我们开发了一个高效的系统（RoboAgent），可以通过（a）实义增强和（b）动作表示来快速增加现有数据，并将小量多 modal 数据中的精致政策EXTRACT。此外，我们的任务条件和表达政策架构可以让我们的代理人在新的语言指令下展现多元的技能。仅从7500次示例中，我们能够训练一个可以拥有12种技能的单一代理人，并在38个任务中展现其普遍性。在未见的情况下，RoboAgent 比PRIOR METHODS 高出40%的性能，同时更加sample efficient 和可以通过精致化和扩展来提高能力。影片请参考https://robopen.github.io/”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Physics-Informed-Reinforcement-Learning-Review-and-Open-Problems"><a href="#A-Survey-on-Physics-Informed-Reinforcement-Learning-Review-and-Open-Problems" class="headerlink" title="A Survey on Physics Informed Reinforcement Learning: Review and Open Problems"></a>A Survey on Physics Informed Reinforcement Learning: Review and Open Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01909">http://arxiv.org/abs/2309.01909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chayan Banerjee, Kien Nguyen, Clinton Fookes, Maziar Raissi</li>
<li>for: 这种研究旨在推动physics-informed reinforcement learning（PIRL）的发展，增强机器学习框架中的物理信息 incorporation，以提高physical plausibility和数据效率。</li>
<li>methods: 这篇文章通过对现有的physics-informed reinforcement learning（PIRL）方法进行系统性的综述和分类，批判性地分析了它们的不同特点和适用场景，从而提供了一个权威的taxonomy。</li>
<li>results: 该文章提供了一个全面的视角，把physics-informed reinforcement learning（PIRL）的实现方法分类为不同的类别，并指出了这个领域的应用场景、潜在的挑战和未来研究的方向。<details>
<summary>Abstract</summary>
The inclusion of physical information in machine learning frameworks has revolutionized many application areas. This involves enhancing the learning process by incorporating physical constraints and adhering to physical laws. In this work we explore their utility for reinforcement learning applications. We present a thorough review of the literature on incorporating physics information, as known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning (PIRL). We introduce a novel taxonomy with the reinforcement learning pipeline as the backbone to classify existing works, compare and contrast them, and derive crucial insights. Existing works are analyzed with regard to the representation/ form of the governing physics modeled for integration, their specific contribution to the typical reinforcement learning architecture, and their connection to the underlying reinforcement learning pipeline stages. We also identify core learning architectures and physics incorporation biases (i.e., observational, inductive and learning) of existing PIRL approaches and use them to further categorize the works for better understanding and adaptation. By providing a comprehensive perspective on the implementation of the physics-informed capability, the taxonomy presents a cohesive approach to PIRL. It identifies the areas where this approach has been applied, as well as the gaps and opportunities that exist. Additionally, the taxonomy sheds light on unresolved issues and challenges, which can guide future research. This nascent field holds great potential for enhancing reinforcement learning algorithms by increasing their physical plausibility, precision, data efficiency, and applicability in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
机器学习框架中包含物理信息的包含已经革命化了许多应用领域。这些包含物理约束和遵循物理法律，以提高学习过程的精度和有效性。在这项工作中，我们探讨物理信息在强化学习应用中的用途。我们提出了一种新的分类方法，将现有的强化学习方法分为三类：观察性、推理性和学习性。我们还对现有的强化学习方法进行了分析，包括物理模型的表示形式、在强化学习架构中的特点和与强化学习流程的连接。此外，我们还发现了现有的强化学习方法的核心学习架构和物理包含偏好。这种分类方法为未来研究提供了一个整体的视角，并且为实现物理信息的包含提供了一个有效的方法。此外，这种分类方法还透视了物理信息的包含在强化学习中的潜在问题和挑战，以及未来研究的可能性。这个领域的发展潜力很大，可以提高强化学习算法的物理可能性、精度、数据效率和实际应用场景中的适用性。
</details></li>
</ul>
<hr>
<h2 id="Extended-Symmetry-Preserving-Attention-Networks-for-LHC-Analysis"><a href="#Extended-Symmetry-Preserving-Attention-Networks-for-LHC-Analysis" class="headerlink" title="Extended Symmetry Preserving Attention Networks for LHC Analysis"></a>Extended Symmetry Preserving Attention Networks for LHC Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01886">http://arxiv.org/abs/2309.01886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael James Fenton, Alexander Shmakov, Hideki Okawa, Yuji Li, Ko-Yang Hsiao, Shih-Chieh Hsu, Daniel Whiteson, Pierre Baldi</li>
<li>for: 这篇论文是用来探索重积过的大型内部积体（ttH）搜寻、Top颗粒子质量测量和重Z’粒子衰变到Top颗粒子对的搜寻。</li>
<li>methods: 这篇论文使用了一种简化的注意力机制——对称保持注意力网络（SPANet），并将其扩展到考虑多个输入流，例如电子和全事件特征。</li>
<li>results: 研究发现在 semi-leptonic 探索中，使用扩展的 SPANet 可以获得 significative 的改善，并在 three 个代表性的研究中提供了详细的结果。<details>
<summary>Abstract</summary>
Reconstructing unstable heavy particles requires sophisticated techniques to sift through the large number of possible permutations for assignment of detector objects to partons. An approach based on a generalized attention mechanism, symmetry preserving attention networks (SPANet), has been previously applied to top quark pair decays at the Large Hadron Collider, which produce six hadronic jets. Here we extend the SPANet architecture to consider multiple input streams, such as leptons, as well as global event features, such as the missing transverse momentum. In addition, we provide regression and classification outputs to supplement the parton assignment. We explore the performance of the extended capability of SPANet in the context of semi-leptonic decays of top quark pairs as well as top quark pairs produced in association with a Higgs boson. We find significant improvements in the power of three representative studies: search for ttH, measurement of the top quark mass and a search for a heavy Z' decaying to top quark pairs. We present ablation studies to provide insight on what the network has learned in each case.
</details>
<details>
<summary>摘要</summary>
重新建构不稳定的重子 particle需要使用复杂的技术来筛选大量的可能性，以将测器对象分配给束子。一种基于通用注意机制的 Symmetry Preserving Attention Networks (SPANet) 已经在大引子中子粒子机器人中应用于 top quark pair 减谱，该过程产生六个有征的树脂。在这里，我们扩展了 SPANet 架构，考虑多个输入流，如电子和全局事件特征，如转移质量。此外，我们还提供了 regression 和 classification 输出，以补充束子分配。我们在 semi-leptonic  decay 中研究了 top quark pair 的扩展能力，以及 top quark pair 与 Higgs  boson 共生生成的情况。我们发现，在三个表型研究中，使用扩展的 SPANet 能力具有显著改善。我们还进行了剥离研究，以了解每个情况中网络学习的内容。
</details></li>
</ul>
<hr>
<h2 id="Task-Generalization-with-Stability-Guarantees-via-Elastic-Dynamical-System-Motion-Policies"><a href="#Task-Generalization-with-Stability-Guarantees-via-Elastic-Dynamical-System-Motion-Policies" class="headerlink" title="Task Generalization with Stability Guarantees via Elastic Dynamical System Motion Policies"></a>Task Generalization with Stability Guarantees via Elastic Dynamical System Motion Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01884">http://arxiv.org/abs/2309.01884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Li, Nadia Figueroa</li>
<li>for: 本研究旨在提出一种基于动力系统（DS）的学习 FROM DEMONSTRATION（LfD）方法，以实现从少量轨迹学习稳定和准确的激发动作策略，并能够扩展到新的任务实例。</li>
<li>methods: 该方法基于线性参数变化（LPV）DS模型，并使用 Gaussian Mixture Model（GMM）来捕捉任务相关的参数变化。在新任务实例&#x2F;上下文中，GMM将被改变并使用Laplacian Editing来重新估计LPV-DS策略。</li>
<li>results: 在许多 simulate 和实际 робо臂实验中，Elastic-DS 表现出了高度的灵活性和扩展性，同时保持了控制理论上的保证。详细视频可以在<a target="_blank" rel="noopener" href="https://sites.google.com/view/elastic-ds">https://sites.google.com/view/elastic-ds</a> 找到。<details>
<summary>Abstract</summary>
Dynamical System (DS) based Learning from Demonstration (LfD) allows learning of reactive motion policies with stability and convergence guarantees from a few trajectories. Yet, current DS learning techniques lack the flexibility to generalize to new task instances as they ignore explicit task parameters that inherently change the underlying trajectories. In this work, we propose Elastic-DS, a novel DS learning, and generalization approach that embeds task parameters into the Gaussian Mixture Model (GMM) based Linear Parameter Varying (LPV) DS formulation. Central to our approach is the Elastic-GMM, a GMM constrained to SE(3) task-relevant frames. Given a new task instance/context, the Elastic-GMM is transformed with Laplacian Editing and used to re-estimate the LPV-DS policy. Elastic-DS is compositional in nature and can be used to construct flexible multi-step tasks. We showcase its strength on a myriad of simulated and real-robot experiments while preserving desirable control-theoretic guarantees. Supplementary videos can be found at https://sites.google.com/view/elastic-ds
</details>
<details>
<summary>摘要</summary>
dynamical system (DS) 基于学习from Demonstration (LfD) 可以从一些轨迹学习反应性动作策略，并且有稳定性和收敛保证。然而，当前的 DS 学习技术 ignore 表达式 task 参数，这些参数直接影响下面的轨迹，从而导致学习不具有普适性。在这项工作中，我们提出了 Elastic-DS，一种新的 DS 学习和总结方法，该方法在 Gaussian Mixture Model (GMM) 基于 Linear Parameter Varying (LPV) DS 形式ulation中嵌入任务参数。中心思想是 Elastic-GMM，一个受 SE(3) 任务相关框架约束的 GMM。给定一个新任务实例/上下文，Elastic-GMM 将通过 Laplacian Editing 变换，并用来重新估计 LPV-DS 政策。Elastic-DS 是可组合的性的，可以用于构建灵活的多步任务。我们在许多模拟和真实机器人实验中证明了其强大，同时保持了控制理论上的保证。补充视频可以在 https://sites.google.com/view/elastic-ds 找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.LG_2023_09_05/" data-id="clorjzl9100p3f188aril4b34" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/eess.IV_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T09:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/eess.IV_2023_09_05/">eess.IV - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-Improved-Upper-Bound-on-the-Rate-Distortion-Function-of-Images"><a href="#An-Improved-Upper-Bound-on-the-Rate-Distortion-Function-of-Images" class="headerlink" title="An Improved Upper Bound on the Rate-Distortion Function of Images"></a>An Improved Upper Bound on the Rate-Distortion Function of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02574">http://arxiv.org/abs/2309.02574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duanzhiihao/lossy-vae">https://github.com/duanzhiihao/lossy-vae</a></li>
<li>paper_authors: Zhihao Duan, Jack Ma, Jiangpeng He, Fengqing Zhu</li>
<li>for: 这个论文的目的是提出一种基于Variational Autoencoders（VAEs）的图像损失 compression算法，以实现图像的损失 compression。</li>
<li>methods: 该论文使用了一种新的 VAE 模型架构，并应用了可变比率压缩技术，以及一种新的 \ourfunction{} 来稳定训练。</li>
<li>results: 该论文的实验结果表明，可以通过该算法实现至少 30% BD-rate 减少，相比于 VVC 编码器的内部预测模式，这表明仍然有很大的潜在提高损失图像压缩的potential。<details>
<summary>Abstract</summary>
Recent work has shown that Variational Autoencoders (VAEs) can be used to upper-bound the information rate-distortion (R-D) function of images, i.e., the fundamental limit of lossy image compression. In this paper, we report an improved upper bound on the R-D function of images implemented by (1) introducing a new VAE model architecture, (2) applying variable-rate compression techniques, and (3) proposing a novel \ourfunction{} to stabilize training. We demonstrate that at least 30\% BD-rate reduction w.r.t. the intra prediction mode in VVC codec is achievable, suggesting that there is still great potential for improving lossy image compression. Code is made publicly available at https://github.com/duanzhiihao/lossy-vae.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，变量自动编码器（VAEs）可以用来Upper-bound the information rate-distortion（R-D）函数图像，即图像损失压缩的基本限制。在这篇论文中，我们报告了一种新的 VAE 模型架构，以及对变量比特率压缩技术的应用，以及一种新的 \ourfunction{} 来稳定训练。我们示出，至少可以实现30%的BD-rate减少相对于VVC编码器的内部预测模式，这表明还有很大的潜在改进损失图像压缩的可能性。代码在https://github.com/duanzhiihao/lossy-vae中公开。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-High-Performance-Learned-Image-Compression-With-Improved-Checkerboard-Context-Model-Deformable-Residual-Module-and-Knowledge-Distillation"><a href="#Fast-and-High-Performance-Learned-Image-Compression-With-Improved-Checkerboard-Context-Model-Deformable-Residual-Module-and-Knowledge-Distillation" class="headerlink" title="Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation"></a>Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02529">http://arxiv.org/abs/2309.02529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haisheng Fu, Feng Liang, Jie Liang, Yongqiang Wang, Guohe Zhang, Jingning Han</li>
<li>for: 提高图像压缩速度和质量之间的平衡</li>
<li>methods: 引入四种技术：扭曲conv模块、平行上下文模型、改进的三步知识传递训练方法和$L_1$正则化</li>
<li>results: 比对 estado-of-the-art 学习图像编码方案，我们的方法可以在编码和解码过程中减少时间，并且在 PSNR 和 MS-SSIM 指标上提高 $2.3%$，在 Kodak 和 Tecnick-40 数据集上测试得到更高的性能。<details>
<summary>Abstract</summary>
Deep learning-based image compression has made great progresses recently. However, many leading schemes use serial context-adaptive entropy model to improve the rate-distortion (R-D) performance, which is very slow. In addition, the complexities of the encoding and decoding networks are quite high and not suitable for many practical applications. In this paper, we introduce four techniques to balance the trade-off between the complexity and performance. We are the first to introduce deformable convolutional module in compression framework, which can remove more redundancies in the input image, thereby enhancing compression performance. Second, we design a checkerboard context model with two separate distribution parameter estimation networks and different probability models, which enables parallel decoding without sacrificing the performance compared to the sequential context-adaptive model. Third, we develop an improved three-step knowledge distillation and training scheme to achieve different trade-offs between the complexity and the performance of the decoder network, which transfers both the final and intermediate results of the teacher network to the student network to help its training. Fourth, we introduce $L_{1}$ regularization to make the numerical values of the latent representation more sparse. Then we only encode non-zero channels in the encoding and decoding process, which can greatly reduce the encoding and decoding time. Experiments show that compared to the state-of-the-art learned image coding scheme, our method can be about 20 times faster in encoding and 70-90 times faster in decoding, and our R-D performance is also $2.3 \%$ higher. Our method outperforms the traditional approach in H.266/VVC-intra (4:4:4) and some leading learned schemes in terms of PSNR and MS-SSIM metrics when testing on Kodak and Tecnick-40 datasets.
</details>
<details>
<summary>摘要</summary>
深度学习基于图像压缩的技术在最近几年来已经取得了大量的进步。然而，许多领先的方案仍然使用序列Context-adaptive entropy模型来提高Rate-distortion（R-D）性能，这很慢。此外，压缩和解压缩网络的复杂度很高，不适合许多实际应用。在这篇论文中，我们提出了四种技术来平衡复杂度和性能的负担。我们是首次在压缩框架中引入可变 convolutional模块，可以更好地从输入图像中除去红UNDERSCOREundancy，从而提高压缩性能。其次，我们设计了Checkerboard Context模型，它使用两个独立的分布参数估计网络和不同的概率模型，可以在平行解码过程中保持同样的性能，而不需要顺序Context-adaptive模型。第三，我们开发了一种改进的三步知识传递和训练方案，可以在不同的负担和性能之间进行平衡。最后，我们引入L1正则化，使得干扰表示的数字值更加稀疏。然后，我们只编码非零通道，从而大幅减少编码和解码时间。实验结果显示，相比于当前最佳学习图像编码方案，我们的方法可以在编码过程中提高20倍，并在解码过程中提高70-90倍，同时R-D性能也提高了2.3%。我们的方法在H.266/VVC-intra（4:4:4）和一些领先的学习图像编码方案之上具有较高的PSNR和MS-SSIM指标，当测试在Kodak和Tecnick-40 dataset时。
</details></li>
</ul>
<hr>
<h2 id="An-automated-high-resolution-phenotypic-assay-for-adult-Brugia-malayi-and-microfilaria"><a href="#An-automated-high-resolution-phenotypic-assay-for-adult-Brugia-malayi-and-microfilaria" class="headerlink" title="An automated, high-resolution phenotypic assay for adult Brugia malayi and microfilaria"></a>An automated, high-resolution phenotypic assay for adult Brugia malayi and microfilaria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03235">http://arxiv.org/abs/2309.03235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Upender Kalwa, Yunsoo Park, Michael J. Kimber, Santosh Pandey</li>
<li>for: 这项研究用于测试抗helmintic药物的有效性，以提高现有的抗helmintic药物治疗生ariasis的效果。</li>
<li>methods: 这种多参数型生物力学试验基于tracking成年布鲁格IA的运动能力，以评估三种抗helmintic药物的效果。</li>
<li>results: 研究发现，这三种抗helmintic药物可以减少成年布鲁格IA的运动能力，且具有不同的机理和效果。<details>
<summary>Abstract</summary>
Brugia malayi are thread-like parasitic worms and one of the etiological agents of Lymphatic filariasis (LF). Existing anthelmintic drugs to treat LF are effective in reducing the larval microfilaria (mf) counts in human bloodstream but are less effective on adult parasites. To test potential drug candidates, we report a multi-parameter phenotypic assay based on tracking the motility of adult B. malayi and mf in vitro. For adult B. malayi, motility is characterized by the centroid velocity, path curvature, angular velocity, eccentricity, extent, and Euler Number. These parameters are evaluated in experiments with three anthelmintic drugs. For B. malayi mf, motility is extracted from the evolving body skeleton to yield positional data and bending angles at 74 key point. We achieved high-fidelity tracking of complex worm postures (self-occlusions, omega turns, body bending, and reversals) while providing a visual representation of pose estimates and behavioral attributes in both space and time scales.
</details>
<details>
<summary>摘要</summary>
布鲁迪亚马LAY是线状寄生虫，是淋巴炎病（LF）的etiological agent之一。现有的安定虫药可以降低人体血液中幼虫微血短的数量，但对成熟虫有效性较差。为测试潜在药物候选者，我们报告了一种多参数现象学测试方法，基于成人布鲁迪亚马LAY和幼虫的运动追踪。成人布鲁迪亚马LAY的运动特征包括中心速度、轨迹弯曲、angular velocity、eccentricity、范围和Euler数。这些参数在三种安定虫药实验中被评估。布鲁迪亚马LAY幼虫的运动被提取自发展的身体骨架中，以获得位坐数据和弯曲角度。我们实现了高精度的跟踪复杂的虫姿势（自相交、卷曲、身体弯曲和反转），并提供了Visual representation of pose estimates和行为特征在空间和时间尺度上。
</details></li>
</ul>
<hr>
<h2 id="Duration-adaptive-Video-Highlight-Pre-caching-for-Vehicular-Communication-Network"><a href="#Duration-adaptive-Video-Highlight-Pre-caching-for-Vehicular-Communication-Network" class="headerlink" title="Duration-adaptive Video Highlight Pre-caching for Vehicular Communication Network"></a>Duration-adaptive Video Highlight Pre-caching for Vehicular Communication Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01944">http://arxiv.org/abs/2309.01944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Xu, Deshi Li, Kaitao Meng, Mingliu Liu, Shuya Zhu</li>
<li>for: 本文 targets  vehicular communication networks (VCNs) and aims to improve video highlight pre-caching.</li>
<li>methods: 本文提出了一种高效的视频精华预取方案，基于服务持续时间的变化和视频段的吸引力和续接性。</li>
<li>results:  simulations based on real-world video datasets show that the proposed method significantly improves highlight entropy and jitter compared to benchmark schemes.<details>
<summary>Abstract</summary>
Video traffic in vehicular communication networks (VCNs) faces exponential growth. However, different segments of most videos reveal various attractiveness for viewers, and the pre-caching decision is greatly affected by the dynamic service duration that edge nodes can provide services for mobile vehicles driving along a road. In this paper, we propose an efficient video highlight pre-caching scheme in the vehicular communication network, adapting to the service duration. Specifically, a highlight entropy model is devised with the consideration of the segments' popularity and continuity between segments within a period of time, based on which, an optimization problem of video highlight pre-caching is formulated. As this problem is non-convex and lacks a closed-form expression of the objective function, we decouple multiple variables by deriving candidate highlight segmentations of videos through wavelet transform, which can significantly reduce the complexity of highlight pre-caching. Then the problem is solved iteratively by a highlight-direction trimming algorithm, which is proven to be locally optimal. Simulation results based on real-world video datasets demonstrate significant improvement in highlight entropy and jitter compared to benchmark schemes.
</details>
<details>
<summary>摘要</summary>
Video流量在交通网络（VCN）中正在呈指数增长趋势。然而，不同的视频片段吸引了不同的观众，并且边缘节点可以为移动 vehicles提供不同的服务时间，这会对预缓存决策产生很大的影响。在这篇论文中，我们提出了一种高效的视频突出点预缓存方案，适应到服务时间的变化。具体来说，我们开发了一个高光 entropy 模型，考虑了视频片段的吸引力和时间上的连续性，基于这个模型，我们将预缓存问题转化为优化问题。由于这个问题是非凸的，而且无法取得目标函数的闭合表达，我们将变量分解成多个变量，通过wavelet 变换 derivation 得到候选的高光片段，这可以很大地降低预缓存的复杂度。然后，我们通过一种高光方向截断算法来解决这个问题，这个算法被证明是本地优化的。实验结果基于实际的视频数据集表明，与参考方案相比，我们的方案具有显著提高高光 entropy和抖动的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/eess.IV_2023_09_05/" data-id="clorjzlfz016ef188b3zu1p9x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/eess.SP_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T08:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/eess.SP_2023_09_05/">eess.SP - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semantic-Communications-Based-on-Adaptive-Generative-Models-and-Information-Bottleneck"><a href="#Semantic-Communications-Based-on-Adaptive-Generative-Models-and-Information-Bottleneck" class="headerlink" title="Semantic Communications Based on Adaptive Generative Models and Information Bottleneck"></a>Semantic Communications Based on Adaptive Generative Models and Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02387">http://arxiv.org/abs/2309.02387</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Barbarossa, D. Comminiello, E. Grassucci, F. Pezone, S. Sardellitti, P. Di Lorenzo</li>
<li>for: 本文提出了一种基于三个基本想法的含义通信方法，即使用拓扑空间来表示数据，根据关系来捕捉 semantics，使用信息瓶颈理论来确定可信度和延迟，以及使用概率生成模型来适应无线通信频率和图像重建或执行分类任务。</li>
<li>methods: 本文使用的方法包括使用拓扑空间来表示数据，根据关系来捕捉 semantics，使用信息瓶颈理论来确定可信度和延迟，以及使用概率生成模型来适应无线通信频率和图像重建或执行分类任务。</li>
<li>results: 本文的结果表明，基于这三个基本想法的含义通信方法可以减少无线通信中的传输数据量，同时保持图像重建和分类任务的高精度。<details>
<summary>Abstract</summary>
Semantic communications represent a significant breakthrough with respect to the current communication paradigm, as they focus on recovering the meaning behind the transmitted sequence of symbols, rather than the symbols themselves. In semantic communications, the scope of the destination is not to recover a list of symbols symbolically identical to the transmitted ones, but rather to recover a message that is semantically equivalent to the semantic message emitted by the source. This paradigm shift introduces many degrees of freedom to the encoding and decoding rules that can be exploited to make the design of communication systems much more efficient. In this paper, we present an approach to semantic communication building on three fundamental ideas: 1) represent data over a topological space as a formal way to capture semantics, as expressed through relations; 2) use the information bottleneck principle as a way to identify relevant information and adapt the information bottleneck online, as a function of the wireless channel state, in order to strike an optimal trade-off between transmit power, reconstruction accuracy and delay; 3) exploit probabilistic generative models as a general tool to adapt the transmission rate to the wireless channel state and make possible the regeneration of the transmitted images or run classification tasks at the receiver side.
</details>
<details>
<summary>摘要</summary>
semantic communications represent a significant breakthrough in terms of the current communication paradigm, as they focus on recovering the meaning behind the transmitted sequence of symbols, rather than the symbols themselves. In semantic communications, the scope of the destination is not to recover a list of symbols symbolically identical to the transmitted ones, but rather to recover a message that is semantically equivalent to the semantic message emitted by the source. This paradigm shift introduces many degrees of freedom to the encoding and decoding rules that can be exploited to make the design of communication systems much more efficient. In this paper, we present an approach to semantic communication building on three fundamental ideas: 1) represent data over a topological space as a formal way to capture semantics, as expressed through relations; 2) use the information bottleneck principle as a way to identify relevant information and adapt the information bottleneck online, as a function of the wireless channel state, in order to strike an optimal trade-off between transmit power, reconstruction accuracy, and delay; 3) exploit probabilistic generative models as a general tool to adapt the transmission rate to the wireless channel state and make possible the regeneration of the transmitted images or run classification tasks at the receiver side.
</details></li>
</ul>
<hr>
<h2 id="Sensing-With-Random-Signals"><a href="#Sensing-With-Random-Signals" class="headerlink" title="Sensing With Random Signals"></a>Sensing With Random Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02375">http://arxiv.org/abs/2309.02375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpatsenker/noisy_random_projection_sparse_signal_recon">https://github.com/jpatsenker/noisy_random_projection_sparse_signal_recon</a></li>
<li>paper_authors: Shihang Lu, Fan Liu, Fuwang Dong, Yifeng Xiong, Jie Xu, Ya-Feng Liu</li>
<li>for: 本文研究了使用随机ISAC信号进行目标探测，并对多antenna系统进行分析。</li>
<li>methods: 本文定义了一个新的探测性能指标，即随机线性最小均方误差（ELMMSE），用于描述ISAC信号Randomness中的估计误差。然后，本文研究了基于数据依赖的探测矩阵编码方案，以实现优化的探测性能，并提出了一种数据独立的探测矩阵编码方案和一种Stochastic Gradient Projection（SGP）算法来实现ELMMSE最小化。</li>
<li>results: 通过Simulations，本文示出了提议方法的优越性。<details>
<summary>Abstract</summary>
Radar systems typically employ well-designed deterministic signals for target sensing. In contrast to that, integrated sensing and communications (ISAC) systems have to use random signals to convey useful information, potentially causing sensing performance degradation. This paper analyzes the sensing performance via random ISAC signals over a multi-antenna system. Towards this end, we define a new sensing performance metric, namely, ergodic linear minimum mean square error (ELMMSE), which characterizes the estimation error averaged over the randomness of ISAC signals. Then, we investigate a data-dependent precoding scheme to minimize the ELMMSE, which attains the {optimized} sensing performance at the price of high computational complexity. To reduce the complexity, we present an alternative data-independent precoding scheme and propose a stochastic gradient projection (SGP) algorithm for ELMMSE minimization, which can be trained offline by locally generated signal samples. Finally, we demonstrate the superiority of the proposed methods by simulations.
</details>
<details>
<summary>摘要</summary>
雷达系统通常使用高效的决定性信号进行目标探测。然而，集成感知通信（ISAC）系统需要使用随机信号传输有用信息，可能导致探测性能下降。本文分析了使用随机ISAC信号在多antenna系统上的探测性能。为此，我们定义了一个新的探测性能指标，即ergodic线性最小均方误差（ELMMSE），该指标表示随机ISAC信号中的估计误差的平均值。然后，我们 investigate了一种数据依赖的 precoding 策略，以实现最优的探测性能，但是计算复杂性高。为了降低复杂性，我们提出了一种数据独立的 precoding 策略，并提出了一种Stochastic Gradient Projection（SGP）算法，用于ELMMSE最小化，该算法可以在本地生成的信号样本上进行线上培育。最后，我们通过 simulate 表明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Optimization-of-RSMA-for-Uplink-Communication-based-on-Intelligent-Reflecting-Surface"><a href="#Fairness-Optimization-of-RSMA-for-Uplink-Communication-based-on-Intelligent-Reflecting-Surface" class="headerlink" title="Fairness Optimization of RSMA for Uplink Communication based on Intelligent Reflecting Surface"></a>Fairness Optimization of RSMA for Uplink Communication based on Intelligent Reflecting Surface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02264">http://arxiv.org/abs/2309.02264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Zhang, Wen Chen</li>
<li>for: 提高上行通信系统的公平性</li>
<li>methods: 使用环境反射表（IRS）减轻通信路径损失，并使用最大最小偏好优化问题获取资源分配，包括接收束形成和相位偏移束形成</li>
<li>results:  simulation 结果表明，提议的方案可以提高上行通信的公平性<details>
<summary>Abstract</summary>
In this paper, we propose a rate-splitting multiple access (RSMA) scheme for uplink wireless communication systems with intelligent reflecting surface (IRS) aided. In the considered model, IRS is adopted to overcome power attenuation caused by path loss. We construct a max-min fairness optimization problem to obtain the resource allocation, including the receive beamforming at the base station (BS) and phase-shift beamforming at IRS. We also introduce a successive group decoding (SGD) algorithm at the receiver, which trades off the fairness and complexity of decoding. In the simulation, the results show that the proposed scheme has superiority in improving the fairness of uplink communication.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于多访问率拆分（RSMA）的上行无线通信系统中使用智能反射 superficie（IRS）的方案。在考虑的模型中，我们采用了IRS以超越由路径损失引起的功率损失。我们构建了最大最小公平性优化问题来获取资源分配，包括接收天线的扫描方向和phaseshift天线的调制。我们还引入了Successive Group Decoding（SGD）算法，以考虑公平性和解码复杂性之间的贸易。在仿真中，结果显示，我们提出的方案可以提高上行通信的公平性。
</details></li>
</ul>
<hr>
<h2 id="Design-of-a-New-CIM-DCSK-Based-Ambient-Backscatter-Communication-System"><a href="#Design-of-a-New-CIM-DCSK-Based-Ambient-Backscatter-Communication-System" class="headerlink" title="Design of a New CIM-DCSK-Based Ambient Backscatter Communication System"></a>Design of a New CIM-DCSK-Based Ambient Backscatter Communication System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02259">http://arxiv.org/abs/2309.02259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruipeng Yang, Yi Fang, Pingping Chen, Huan Ma<br>for: 提高Diffusion Chaos Shift Keying（DCSK）基于Ambient Backscatter Communication（AmBC）系统的数据率，我们提出了一种基于Code Index Modulation（CIM）的AmBC系统，称为CIM-DCSK-AmBC系统。methods: 在提出的系统中，CIM-DCSK信号在直接链路上传输，并用作背scatter链路的Radio Frequency源。在背scatter链路中，信号格式设计用于提高数据率，同时消除直接链路信号干扰。因此，直接链路信号和背scatter链路信号可以同时接收和解模ulation。results: 我们 derivated and validated the theoretical bit error rate（BER）表达式 Of CIM-DCSK-AmBC系统 Over multipath Rayleigh fading channels。Compared with准确参照DCSK-based AmBC（SR-DCSK-AmBC）系统作为参考系统，numerical results reveal that CIM-DCSK-AmBC系统可以在直接链路中 achieve better BER性能和在背scatter链路中 higher throughput than benchmark system。<details>
<summary>Abstract</summary>
To improve the data rate in differential chaos shift keying (DCSK) based ambient backscatter communication (AmBC) system, we propose a new AmBC system based on code index modulation (CIM), referred to as CIM-DCSK-AmBC system. In the proposed system, the CIM-DCSK signal transmitted in the direct link is used as the radio frequency source of the backscatter link. The signal format in the backscatter link is designed to increase the data rate as well as eliminate the interference of the direct link signal. As such, the direct link signal and the backscatter link signal can be received and demodulated simultaneously. Moreover, we derive and validate the theoretical bit error rate (BER) expressions of the CIM-DCSK-AmBC system over multipath Rayleigh fading channels. Regarding the short reference DCSK-based AmBC (SR-DCSK-AmBC) system as a benchmark system, numerical results reveal that the CIM-DCSK-AmBC system can achieve better BER performance in the direct link and higher throughput in the backscatter link than the benchmark system.
</details>
<details>
<summary>摘要</summary>
为了提高Diffusion Chaos Shift Keying（DCSK）基于Ambient Backscatter Communication（AmBC）系统的数据速率，我们提议一种基于Code Index Modulation（CIM）的AmBC系统，称为CIM-DCSK-AmBC系统。在该系统中，在直接链路中发送的CIM-DCSK信号被用作背scatter链路的Radio Frequency源。在背scatter链路中，信号格式设计为提高数据速率，同时消除直接链路信号干扰。因此，直接链路信号和背scatter链路信号可以同时接收和解模式。此外，我们 derive了和验证了CIM-DCSK-AmBC系统在多path Rayleigh抖振通道上的符号错误率（BER）表达式。对于参考系统SR-DCSK-AmBC系统作为标准系统，数字结果表明，CIM-DCSK-AmBC系统在直接链路中的BER性能比标准系统更好，而在背scatter链路中的 Throughput更高。
</details></li>
</ul>
<hr>
<h2 id="PyPVRoof-a-Python-package-for-extracting-the-characteristics-of-rooftop-PV-installations-using-remote-sensing-data"><a href="#PyPVRoof-a-Python-package-for-extracting-the-characteristics-of-rooftop-PV-installations-using-remote-sensing-data" class="headerlink" title="PyPVRoof: a Python package for extracting the characteristics of rooftop PV installations using remote sensing data"></a>PyPVRoof: a Python package for extracting the characteristics of rooftop PV installations using remote sensing data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07143">http://arxiv.org/abs/2309.07143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabrielkasmi/pypvroof">https://github.com/gabrielkasmi/pypvroof</a></li>
<li>paper_authors: Yann Tremenbert, Gabriel Kasmi, Laurent Dubus, Yves-Marie Saint-Drenan, Philippe Blanc</li>
<li>for: 这篇论文是为了提供一个Python包（{\tt PyPVRoof）来自动提取庭院式太阳能系统的主要特征（倾角、方位、表面、地点和安装容量）。</li>
<li>methods: 该论文使用了一种benchmark方法来评估{\tt PyPVRoof}的准确性，并提供了数据来复制这些精度测试。</li>
<li>results: 该论文的结果表明，{\tt PyPVRoof}可以高效地自动提取庭院式太阳能系统的主要特征，并且可以满足不同的数据可用性和用户需求。<details>
<summary>Abstract</summary>
Photovoltaic (PV) energy grows at an unprecedented pace, which makes it difficult to maintain up-to-date and accurate PV registries, which are critical for many applications such as PV power generation estimation. This lack of qualitative data is especially true in the case of rooftop PV installations. As a result, extensive efforts are put into the constitution of PV inventories. However, although valuable, these registries cannot be directly used for monitoring the deployment of PV or estimating the PV power generation, as these tasks usually require PV systems {\it characteristics}. To seamlessly extract these characteristics from the global inventories, we introduce {\tt PyPVRoof}. {\tt PyPVRoof} is a Python package to extract essential PV installation characteristics. These characteristics are tilt angle, azimuth, surface, localization, and installed capacity. {\tt PyPVRoof} is designed to cover all use cases regarding data availability and user needs and is based on a benchmark of the best existing methods. Data for replicating our accuracy benchmarks are available on our Zenodo repository \cite{tremenbert2023pypvroof}, and the package code is accessible at this URL: \url{https://github.com/gabrielkasmi/pypvroof}.
</details>
<details>
<summary>摘要</summary>
彩票太阳能（PV）在不可思议的速度下增长，使得保持最新和准确的PV注册记录变得很困难，这些注册记录对许多应用来说非常重要，例如PV电力生产估算。特别是在悬挂PV设备上，缺乏质量的数据是非常真实的。因此，大量的努力被投入到PV库的编制中。虽然这些注册记录非常有价值，但它们无法直接用于监测PV的部署或估算PV电力生产，因为这些任务通常需要PV系统的特征。为了快速提取这些特征，我们介绍了PyPVRoof。PyPVRoof是一个基于Python的包，用于提取PV设备的关键特征，包括倾斜角度、方位、面积、地点和安装容量。PyPVRoof针对所有的数据可用性和用户需求进行了设计，并基于最佳现有方法的准确性标准。数据用于复制我们准确性标准的数据可以在我们Zenodo存储库中找到 \cite{tremenbert2023pypvroof}, 并且包代码可以在以下URL上获取：\url{https://github.com/gabrielkasmi/pypvroof}。
</details></li>
</ul>
<hr>
<h2 id="A-Wideband-MIMO-Channel-Model-for-Aerial-Intelligent-Reflecting-Surface-Assisted-Wireless-Communications"><a href="#A-Wideband-MIMO-Channel-Model-for-Aerial-Intelligent-Reflecting-Surface-Assisted-Wireless-Communications" class="headerlink" title="A Wideband MIMO Channel Model for Aerial Intelligent Reflecting Surface-Assisted Wireless Communications"></a>A Wideband MIMO Channel Model for Aerial Intelligent Reflecting Surface-Assisted Wireless Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02171">http://arxiv.org/abs/2309.02171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoyi Liu, Nan Ma, Yaning Chen, Ke Peng, Dongsheng Xue</li>
<li>for: 本研究旨在提出一种三维宽频通道模型，用于描述空中智能反射表(AIRS)和智能反射表(IRS)合作多输入多出力(MIMO)通信系统中的通道特性。</li>
<li>methods: 本文提出了一种三维宽频通道模型，考虑了AIRS的旋转度量和空间运动角度。基于该模型，提出了一些可行的共同相位调整策略。</li>
<li>results: 实验结果表明，提出的模型能准确捕捉通道特性，并且提出的相位调整策略可以有效地改善通道统计特性和系统容量。此外，我们发现在某些情况下，IRS和直线视线(LoS)路径之间的道路具有类似特性。这些发现可以为未来智能通信系统的发展提供有价值的指导。<details>
<summary>Abstract</summary>
Compared to traditional intelligent reflecting surfaces(IRS), aerial IRS (AIRS) has unique advantages, such as more flexible deployment and wider service coverage. However, modeling AIRS in the channel presents new challenges due to their mobility. In this paper, a three-dimensional (3D) wideband channel model for AIRS and IRS joint-assisted multiple-input multiple-output (MIMO) communication system is proposed, where considering the rotational degrees of freedom in three directions and the motion angles of AIRS in space. Based on the proposed model, the channel impulse response (CIR), correlation function, and channel capacity are derived, and several feasible joint phase shifts schemes for AIRS and IRS units are proposed. Simulation results show that the proposed model can capture the channel characteristics accurately, and the proposed phase shifts methods can effectively improve the channel statistical characteristics and increase the system capacity. Additionally, we observe that in certain scenarios, the paths involving the IRS and the line-of-sight (LoS) paths exhibit similar characteristics. These findings provide valuable insights for the future development of intelligent communication systems.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:与传统的智能反射表面（IRS）相比，空中智能反射表面（AIRS）具有更多的灵活部署和更广泛的服务覆盖。然而，为AIRS在通道进行模型化带来了新的挑战，因为它们的移动会导致通道的差异。在这篇论文中，我们提出了一个三维（3D）宽频通道模型，用于AIRS和IRS共同协助多输入多出力（MIMO）通信系统。该模型考虑了AIRS在三个方向上的旋转度量和空间中的运动角度。根据提出的模型，我们 derivated了通道响应函数（CIR）、相关函数和通道容量。此外，我们还提出了一些可能的共同相位shift方案 дляAIRS和IRS单元。实验结果表明，提出的模型可以准确捕捉通道特性，并且提出的相位shift方案可以有效改善通道统计特性和系统容量。此外，我们还发现在某些场景下，IRS和直线视线（LoS）路径之间的道路具有相似的特性。这些发现提供了智能通信系统的未来发展中的有价值意见。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-SAR-ADC-Mismatch-on-Quantized-Massive-MU-MIMO-Systems"><a href="#The-Impact-of-SAR-ADC-Mismatch-on-Quantized-Massive-MU-MIMO-Systems" class="headerlink" title="The Impact of SAR-ADC Mismatch on Quantized Massive MU-MIMO Systems"></a>The Impact of SAR-ADC Mismatch on Quantized Massive MU-MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02168">http://arxiv.org/abs/2309.02168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jérémy Guichemerre, Christoph Studer</li>
<li>for: 这篇论文主要探讨了低分辨率的数字化数组（ADC）在大规模多用户（MU）多输入多Output（MIMO）无线系统中的应用。</li>
<li>methods: 论文使用了Bussgang的分解来模型了ADC的发散错误，并分析了这些错误对ADC的性能影响。</li>
<li>results: 论文发现，即使使用低分辨率的SAR ADC，但是发散错误仍然会影响系统的性能。<details>
<summary>Abstract</summary>
Low-resolution analog-to-digital converters (ADCs) in massive multi-user (MU) multiple-input multiple-output (MIMO) wireless systems can significantly reduce the power, cost, and interconnect data rates of infrastructure basestations. Thus, recent research on the theory and algorithm sides has extensively focused on such architectures, but with idealistic quantization models. However, real-world ADCs do not behave like ideal quantizers, and are affected by fabrication mismatches. We analyze the impact of capacitor-array mismatches in successive approximation register (SAR) ADCs, which are widely used in wireless systems. We use Bussgang's decomposition to model the effects of such mismatches, and we analyze their impact on the performance of a single ADC. We then simulate a massive MU-MIMO system to demonstrate that capacitor mismatches should not be ignored, even in basestations that use low-resolution SAR ADCs.
</details>
<details>
<summary>摘要</summary>
低分辨率的analog-to-digital converter (ADC)在大规模多用户多输入多输出（MU-MIMO）无线系统中可以显著降低基站的能耗、成本和 интер连接数据率。因此，当前的研究把焦点在这些架构上，但是使用理想的量化模型。然而，实际的ADC不是理想的量化器，它们受到制造偏差的影响。我们分析了Successive Approximation Register（SAR）ADC中的电容器数组偏差的影响，使用Bussgang的分解来模型这些影响。我们分析了单个ADC的性能受到这些偏差的影响，然后通过模拟大规模MU-MIMO系统来证明，即使使用低分辨率SAR ADC，也不能忽略电容器偏差。
</details></li>
</ul>
<hr>
<h2 id="Wiometrics-Comparative-Performance-of-Artificial-Neural-Networks-for-Wireless-Navigation"><a href="#Wiometrics-Comparative-Performance-of-Artificial-Neural-Networks-for-Wireless-Navigation" class="headerlink" title="Wiometrics: Comparative Performance of Artificial Neural Networks for Wireless Navigation"></a>Wiometrics: Comparative Performance of Artificial Neural Networks for Wireless Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02121">http://arxiv.org/abs/2309.02121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Russ Whiton, Junshi Chen, Fredrik Tufvesson</li>
<li>for: 本研究用于Navigation aid的 Radio signals 的利用，以及现有和未来的 terrestrial wireless communication systems 的双用性。</li>
<li>methods: 本文使用 artificial neural networks 进行了 vehicular location and heading estimation，并使用了 software-defined radio 和庞大的天线数组。</li>
<li>results: 实验结果显示，使用不同的 artificial neural network 架构和输入数据表示，可以实现精度在几米之间，并且方向精度在几度之间。<details>
<summary>Abstract</summary>
Radio signals are used broadly as navigation aids, and current and future terrestrial wireless communication systems have properties that make their dual-use for this purpose attractive. Sub-6 GHz carrier frequencies enable widespread coverage for data communication and navigation, but typically offer smaller bandwidths and limited resolution for precise estimation of geometries, particularly in environments where propagation channels are diffuse in time and/or space. Non-parametric methods have been employed with some success for such scenarios both commercially and in literature, but often with an emphasis on low-cost hardware and simple models of propagation, or with simulations that do not fully capture hardware impairments and complex propagation mechanisms. In this article, we make opportunistic observations of downlink signals transmitted by commercial cellular networks by using a software-defined radio and massive antenna array mounted on a passenger vehicle in an urban non line-of-sight scenario, together with a ground truth reference for vehicle pose. With these observations as inputs, we employ artificial neural networks to generate estimates of vehicle location and heading for various artificial neural network architectures and different representations of the input observation data, which we call wiometrics, and compare the performance for navigation. Position accuracy on the order of a few meters, and heading accuracy of a few degrees, are achieved for the best-performing combinations of networks and wiometrics. Based on the results of the experiments we draw conclusions regarding possible future directions for wireless navigation using statistical methods.
</details>
<details>
<summary>摘要</summary>
无线信号广泛用于导航帮助，现有和未来的陆地无线通信系统具有许多有用的双用特性。低于6GHz的载波频率提供了广泛的覆盖率 для数据通信和导航，但通常具有较小的带宽和限制的分辨率，尤其在时间和空间方向上的噪声通道杂化环境中。非参数方法在这些场景中已经得到了一定的成功，但经常强调低成本硬件和简单的噪声传播模型，或者使用不完全捕捉硬件障碍和复杂噪声传播机制的仿真。在这篇文章中，我们利用软件定义的广播Receiver和巨大的天线数组，在城市非直线视线场景中观察商业无线网络的下行信号，并使用软件定义的人工神经网络生成车辆位置和方向估计。我们使用不同的人工神经网络架构和输入数据的不同表示方式，称为“wiometrics”，并比较这些组合的性能。实验结果显示，最佳组合可以实现位置精度在几米之间，并且方向精度在几度之间。根据实验结果，我们对未来无线导航使用统计方法的可能性进行了结论。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Phase-Search-for-Probabilistic-Amplitude-Shaping"><a href="#Bayesian-Phase-Search-for-Probabilistic-Amplitude-Shaping" class="headerlink" title="Bayesian Phase Search for Probabilistic Amplitude Shaping"></a>Bayesian Phase Search for Probabilistic Amplitude Shaping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02003">http://arxiv.org/abs/2309.02003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Taha Askari, Lutz Lampe</li>
<li>for: 这篇论文是为了提出一种可靠的数据恢复（CPR）算法，可以在低信号至杂音比例（SNR）的情况下进行恢复。</li>
<li>methods: 这篇论文使用的方法是 bayesian 数据恢复（CPR）算法，并且将其应用于可能性束形成（PAS）。</li>
<li>results: 结果显示这个新算法可以超越干扰监测CPR的降解情况，并且在PAS中获得更好的效果。<details>
<summary>Abstract</summary>
We introduce a Bayesian carrier phase recovery (CPR) algorithm which is robust against low signal-to-noise ratio scenarios. It is therefore effective for phase recovery for probabilistic amplitude shaping (PAS). Results validate that the new algorithm overcomes the degradation experienced by blind phase-search CPR for PAS.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种 bayesian 承载阶段恢复（CPR）算法，可以在低信号噪响比enario下展示Robust性。因此，这种算法是probabilistic amplitude shaping（PAS）中的phase恢复效果的好选择。结果表明，新算法可以超越blind phase-search CPR对PAS的干扰。Here's a breakdown of the translation:* "We introduce" is translated as "我团队提出" (wǒ tuán zǔ tím shuō)* "Bayesian carrier phase recovery" is translated as "bayesian 承载阶段恢复" (bayesian zhāng chēng jīe duō zhī yì)* "algorithm" is translated as "算法" (suān fǎ)* "robust against low signal-to-noise ratio scenarios" is translated as "可以在低信号噪响比enario下展示Robust性" (kě yǐ zài shàng xīn xiāng bīng yè xiàng)* "It is therefore effective for phase recovery for probabilistic amplitude shaping" is translated as "因此，这种算法是probabilistic amplitude shaping（PAS）中的phase恢复效果的好选择" (yīn qù, zhè zhōng suān fǎ shì PAS 中的phase zhī yì de hǎo jiǎo)* "Results validate" is translated as "结果表明" (jiégù bǎo míng)* "that the new algorithm overcomes the degradation experienced by blind phase-search CPR for PAS" is translated as "新算法可以超越blind phase-search CPR对PAS的干扰" (xīn suān fǎ kě yǐ chāo yù blind phase-search CPR duō PAS de gōng kē)
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/eess.SP_2023_09_05/" data-id="clorjzlhe019wf188415z314m" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.SD_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T15:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.SD_2023_09_04/">cs.SD - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Single-Channel-Speech-Enhancement-with-Deep-Complex-U-Networks-and-Probabilistic-Latent-Space-Models"><a href="#Single-Channel-Speech-Enhancement-with-Deep-Complex-U-Networks-and-Probabilistic-Latent-Space-Models" class="headerlink" title="Single-Channel Speech Enhancement with Deep Complex U-Networks and Probabilistic Latent Space Models"></a>Single-Channel Speech Enhancement with Deep Complex U-Networks and Probabilistic Latent Space Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01535">http://arxiv.org/abs/2309.01535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eike J. Nustede, Jörn Anemüller</li>
<li>for: 提高混响 speech enhancement 性能</li>
<li>methods:  integrate probabilistic (i.e., variational) latent space model into U-Network architecture</li>
<li>results: 在 MS-DNS 2020 和 Voicebank+Demand 数据集上实现了高效的混响 speech enhancement，比如 SI-SDR 达到 20.2 dB，与无 probabilistic latent space 版本相比提高约 0.5-1.4 dB，并且高于 WaveUNet 和 PHASEN。<details>
<summary>Abstract</summary>
In this paper, we propose to extend the deep, complex U-Network architecture for speech enhancement by incorporating a probabilistic (i.e., variational) latent space model. The proposed model is evaluated against several ablated versions of itself in order to study the effects of the variational latent space model, complex-value processing, and self-attention. Evaluation on the MS-DNS 2020 and Voicebank+Demand datasets yields consistently high performance. E.g., the proposed model achieves an SI-SDR of up to 20.2 dB, about 0.5 to 1.4 dB higher than its ablated version without probabilistic latent space, 2-2.4 dB higher than WaveUNet, and 6.7 dB above PHASEN. Compared to real-valued magnitude spectrogram processing with a variational U-Net, the complex U-Net achieves an improvement of up to 4.5 dB SI-SDR. Complex spectrum encoding as magnitude and phase yields best performance in anechoic conditions whereas real and imaginary part representation results in better generalization to (novel) reverberation conditions, possibly due to the underlying physics of sound.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提议扩展深度、复杂的U-网络架构以提高语音增强。我们的提议模型包括 probabilistic（即变量）latent space模型。我们对模型的几个版本进行了ablationstudy，以研究变量latent space模型、复杂值处理和自注意的效果。我们对MS-DNS 2020和Voicebank+Demand dataset进行了评估，得到了出色的表现。例如，我们的模型在SI-SDR方面可以达到20.2 dB，与无 probabilistic latent space版本相比高出0.5-1.4 dB，与WaveUNet相比高出2-2.4 dB，与PHASEN相比高出6.7 dB。与实数值spectrogram处理的变量U-Net相比，复杂spectrum编码为实数值和相位的方法可以在静音条件下达到最佳性能，而实部和虚部表示的方法可以更好地泛化到（新的）噪音条件，可能是因为声音的物理学习。
</details></li>
</ul>
<hr>
<h2 id="Quid-Manumit-–-Freeing-the-Qubit-for-Art"><a href="#Quid-Manumit-–-Freeing-the-Qubit-for-Art" class="headerlink" title="Quid Manumit – Freeing the Qubit for Art"></a>Quid Manumit – Freeing the Qubit for Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03104">http://arxiv.org/abs/2309.03104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Carney</li>
<li>for: 这篇论文描述了如何使用嵌入式量子 simulateur 创造出具有艺术性的量子乐器和乐效。</li>
<li>methods: 该论文利用了 previously released 的 ARM-based Raspberry Pi Pico嵌入式微控制器的量子 simulateur 代码，并提供了一些示例，包括一个量子 MIDI 处理器，可以根据输入音符生成附加的伴奏和具有量子生成的乐器，以及一个量子扭曲模块，可以根据量子Circuit来修改乐器的原始声音。</li>
<li>results: 该论文提供了一些示例，包括一个自包含的Quantum Stylophone和一个效果模块插件called ‘QubitCrusher’ для Korg Nu:Tekt NTS-1。这篇论文还讨论了未来的工作和方向，并提供了所有示例作为开源代码。这是作者所知道的第一个嵌入式量子 simulateur 用于乐器音乐（另一个 QSIM）。<details>
<summary>Abstract</summary>
This paper describes how to `Free the Qubit' for art, by creating standalone quantum musical effects and instruments. Previously released quantum simulator code for an ARM-based Raspberry Pi Pico embedded microcontroller is utilised here, and several examples are built demonstrating different methods of utilising embedded resources: The first is a Quantum MIDI processor that generates additional notes for accompaniment and unique quantum generated instruments based on the input notes, decoded and passed through a quantum circuit in an embedded simulator. The second is a Quantum Distortion module that changes an instrument's raw sound according to a quantum circuit, which is presented in two forms; a self-contained Quantum Stylophone, and an effect module plugin called 'QubitCrusher' for the Korg Nu:Tekt NTS-1. This paper also discusses future work and directions for quantum instruments, and provides all examples as open source. This is, to the author's knowledge, the first example of embedded Quantum Simulators for Instruments of Music (another QSIM).
</details>
<details>
<summary>摘要</summary>
The first example is a Quantum MIDI processor that generates additional notes for accompaniment and unique quantum-generated instruments based on the input notes, decoded and passed through a quantum circuit in an embedded simulator. The second example is a Quantum Distortion module that changes an instrument's raw sound according to a quantum circuit, presented in two forms: a self-contained Quantum Stylophone and an effect module plugin called "QubitCrusher" for the Korg Nu:Tekt NTS-1.The paper also discusses future work and directions for quantum instruments and provides all examples as open source. This is, to the author's knowledge, the first example of embedded Quantum Simulators for Instruments of Music (QSIM).
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.SD_2023_09_04/" data-id="clorjzlbf00vyf188h6qpg2g8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.CV_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T13:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.CV_2023_09_04/">cs.CV - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NLLB-CLIP-–-train-performant-multilingual-image-retrieval-model-on-a-budget"><a href="#NLLB-CLIP-–-train-performant-multilingual-image-retrieval-model-on-a-budget" class="headerlink" title="NLLB-CLIP – train performant multilingual image retrieval model on a budget"></a>NLLB-CLIP – train performant multilingual image retrieval model on a budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01859">http://arxiv.org/abs/2309.01859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Visheratin</li>
<li>for:  investigate whether someone without access to massive computing resources can make a valuable scientific contribution in the field of multilingual image retrieval.</li>
<li>methods:  trained a CLIP model with a text encoder from the NLLB model using an automatically created dataset of 106,246 good-quality images with captions in 200 languages, and used various sizes of image and text encoders and froze different parts of the model during training.</li>
<li>results:  NLLB-CLIP is comparable in quality to state-of-the-art models and significantly outperforms them on low-resource languages.<details>
<summary>Abstract</summary>
Today, the exponential rise of large models developed by academic and industrial institutions with the help of massive computing resources raises the question of whether someone without access to such resources can make a valuable scientific contribution. To explore this, we tried to solve the challenging task of multilingual image retrieval having a limited budget of $1,000. As a result, we present NLLB-CLIP - CLIP model with a text encoder from the NLLB model. To train the model, we used an automatically created dataset of 106,246 good-quality images with captions in 201 languages derived from the LAION COCO dataset. We trained multiple models using image and text encoders of various sizes and kept different parts of the model frozen during the training. We thoroughly analyzed the trained models using existing evaluation datasets and newly created XTD200 and Flickr30k-200 datasets. We show that NLLB-CLIP is comparable in quality to state-of-the-art models and significantly outperforms them on low-resource languages.
</details>
<details>
<summary>摘要</summary>
Note: "NLLB" stands for "No Language Labels Born", which is a technique for training machine learning models without language labels. "CLIP" stands for "Contrastive Language-Image Pre-training".
</details></li>
</ul>
<hr>
<h2 id="Towards-Universal-Image-Embeddings-A-Large-Scale-Dataset-and-Challenge-for-Generic-Image-Representations"><a href="#Towards-Universal-Image-Embeddings-A-Large-Scale-Dataset-and-Challenge-for-Generic-Image-Representations" class="headerlink" title="Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations"></a>Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01858">http://arxiv.org/abs/2309.01858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolaos-Antonios Ypsilantis, Kaifeng Chen, Bingyi Cao, Mário Lipovský, Pelin Dogan-Schönberger, Grzegorz Makosa, Boris Bluntschli, Mojtaba Seyedhosseini, Ondřej Chum, André Araujo</li>
<li>for: The paper aims to address the problem of universal image embedding, where a single universal model is trained and used in multiple domains.</li>
<li>methods: The paper proposes a new large-scale public benchmark for the evaluation of universal image embeddings, with 241k query images, 1.4M index images, and 2.8M training images across 8 different domains and 349k classes. The authors also provide a comprehensive experimental evaluation on the new dataset and conduct a public research competition to foster future research in this area.</li>
<li>results: The paper shows that existing approaches and simplistic extensions lead to worse performance than an assembly of models trained for each domain separately. Additionally, the public research competition attracted the participation of more than 1k teams worldwide and generated many interesting research ideas and findings.<details>
<summary>Abstract</summary>
Fine-grained and instance-level recognition methods are commonly trained and evaluated on specific domains, in a model per domain scenario. Such an approach, however, is impractical in real large-scale applications. In this work, we address the problem of universal image embedding, where a single universal model is trained and used in multiple domains. First, we leverage existing domain-specific datasets to carefully construct a new large-scale public benchmark for the evaluation of universal image embeddings, with 241k query images, 1.4M index images and 2.8M training images across 8 different domains and 349k classes. We define suitable metrics, training and evaluation protocols to foster future research in this area. Second, we provide a comprehensive experimental evaluation on the new dataset, demonstrating that existing approaches and simplistic extensions lead to worse performance than an assembly of models trained for each domain separately. Finally, we conducted a public research competition on this topic, leveraging industrial datasets, which attracted the participation of more than 1k teams worldwide. This exercise generated many interesting research ideas and findings which we present in detail. Project webpage: https://cmp.felk.cvut.cz/univ_emb/
</details>
<details>
<summary>摘要</summary>
通常，细化和实例级认识方法在特定领域上进行训练和评估，这种方法在实际大规模应用中不实用。在这种工作中，我们解决了图像嵌入的问题，其中一个通用模型在多个领域进行训练和使用。我们首先利用现有的领域特定数据集， méticulously construct了一个大规模的公共数据集，用于图像嵌入的评估，该数据集包括8个领域、349个类型，共计241k个查询图像、1.4M个指定图像和2.8M个训练图像。我们定义了适当的度量、训练和评估协议，以促进未来的研究。其次，我们对新数据集进行了完整的实验评估，表明现有方法和简单的扩展在多个领域中的性能较差于每个领域 separately trained models。最后，我们在这个主题上进行了公共研究竞赛，使用了来自产业的数据集，这引起了全球1k多个团队的参与。这个实验生成了许多有趣的研究想法和发现，我们在详细地展示。项目网页：https://cmp.felk.cvut.cz/univ_emb/
</details></li>
</ul>
<hr>
<h2 id="SMPLitex-A-Generative-Model-and-Dataset-for-3D-Human-Texture-Estimation-from-Single-Image"><a href="#SMPLitex-A-Generative-Model-and-Dataset-for-3D-Human-Texture-Estimation-from-Single-Image" class="headerlink" title="SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image"></a>SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01855">http://arxiv.org/abs/2309.01855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Casas, Marc Comino-Trinidad</li>
<li>for: 该论文旨在 estimating and manipulating 人体3D外观从单个图像中。</li>
<li>methods: 该方法基于 reciently proposed generative models for 2D images，并将其扩展到3D领域通过计算输入图像中像素与表面之间的对应关系。</li>
<li>results: 该方法在3个公共可用的数据集上进行了量化和质量评估，并表明 SMPLitex 可以对人体Texture estimation 进行更好的表现，同时允许更多的任务，如编辑、 sintesis 和 manipulate。<details>
<summary>Abstract</summary>
We propose SMPLitex, a method for estimating and manipulating the complete 3D appearance of humans captured from a single image. SMPLitex builds upon the recently proposed generative models for 2D images, and extends their use to the 3D domain through pixel-to-surface correspondences computed on the input image. To this end, we first train a generative model for complete 3D human appearance, and then fit it into the input image by conditioning the generative model to the visible parts of the subject. Furthermore, we propose a new dataset of high-quality human textures built by sampling SMPLitex conditioned on subject descriptions and images. We quantitatively and qualitatively evaluate our method in 3 publicly available datasets, demonstrating that SMPLitex significantly outperforms existing methods for human texture estimation while allowing for a wider variety of tasks such as editing, synthesis, and manipulation
</details>
<details>
<summary>摘要</summary>
我们提出SMPLitex方法，用于从单张图像中估计和操纵人体的完整3D外观。SMPLitex基于最近提出的生成模型 для2D图像，并将其扩展到3D领域通过图像上的像素到表面匹配。为此，我们首先培训了一个完整3D人体外观生成模型，然后将其适应到输入图像中可见部分的条件下。此外，我们还提出了一个新的高质量人体xture样本，通过SMPLitex conditioned on subject descriptions和图像来建立。我们在3个公共可用的数据集上Quantitatively和Qualitatively评估了我们的方法，结果显示SMPLitex Significantly Outperforms现有的人体xture估计方法，同时允许更多的任务，如编辑、生成和操作。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-in-AI-Evaluating-Deep-Neural-Networks-on-Out-of-Distribution-Images"><a href="#Uncertainty-in-AI-Evaluating-Deep-Neural-Networks-on-Out-of-Distribution-Images" class="headerlink" title="Uncertainty in AI: Evaluating Deep Neural Networks on Out-of-Distribution Images"></a>Uncertainty in AI: Evaluating Deep Neural Networks on Out-of-Distribution Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01850">http://arxiv.org/abs/2309.01850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamiu Idowu, Ahmed Almasoud</li>
<li>for: 这篇论文探讨了深度神经网络（包括ResNet-50、VGG16、DenseNet121、AlexNet和GoogleNet）在不正常数据（out-of-distribution，OOD）或受到干扰（perturbed）情况下的表现不一致。</li>
<li>methods: 这篇论文采用了三个实验方法：首先，使用预训练模型将OOD图像分类，以评估它们的表现。其次，建立了模型预测的 ensemble，使用概率平均来寻求多数票的优势。 ensemble的不确定性由average probability、variance和entropy指标来衡量。最后，对新生成的DALL-E图像或实际捕捉图像进行了干扰（filters、rotations等）测试，以评估模型的 robustness。</li>
<li>results: 结果显示，ResNet-50是OOD图像中最准确的单个模型，但ensemble perfom even better，对所有图像进行正确分类。此外，对DALL-E图像和实际捕捉图像进行干扰测试后，ResNet-50模型的表现出现了明显的敏感性，对4&#x2F;5不受干扰图像进行正确分类，但对所有受干扰图像进行错误分类，这些错误分类也可以由人类观察到，反映AI模型的局限性。使用Saliency map可以确定模型对图像的重要区域做出决策。<details>
<summary>Abstract</summary>
As AI models are increasingly deployed in critical applications, ensuring the consistent performance of models when exposed to unusual situations such as out-of-distribution (OOD) or perturbed data, is important. Therefore, this paper investigates the uncertainty of various deep neural networks, including ResNet-50, VGG16, DenseNet121, AlexNet, and GoogleNet, when dealing with such data. Our approach includes three experiments. First, we used the pretrained models to classify OOD images generated via DALL-E to assess their performance. Second, we built an ensemble from the models' predictions using probabilistic averaging for consensus due to its advantages over plurality or majority voting. The ensemble's uncertainty was quantified using average probabilities, variance, and entropy metrics. Our results showed that while ResNet-50 was the most accurate single model for OOD images, the ensemble performed even better, correctly classifying all images. Third, we tested model robustness by adding perturbations (filters, rotations, etc.) to new epistemic images from DALL-E or real-world captures. ResNet-50 was chosen for this being the best performing model. While it classified 4 out of 5 unperturbed images correctly, it misclassified all of them post-perturbation, indicating a significant vulnerability. These misclassifications, which are clear to human observers, highlight AI models' limitations. Using saliency maps, we identified regions of the images that the model considered important for their decisions.
</details>
<details>
<summary>摘要</summary>
As AI模型在关键应用中得到广泛应用，确保模型在不常见的情况下（如外部数据）的稳定性是重要的。因此，这篇论文研究了各种深度神经网络（包括ResNet-50、VGG16、DenseNet121、AlexNet和GoogleNet）在处理不常见数据时的不确定性。我们的方法包括三个实验。第一个实验是使用预训练模型来分类DALL-E生成的外部数据，以评估它们的性能。第二个实验是使用概率权重平均来构建一个ensemble，并使用概率、方差和 entropy 度量来衡量ensemble的不确定性。我们的结果表明，虽然ResNet-50是外部数据上最准确的单个模型，但ensemble perfom even better， correctly classifying all images。第三个实验是测试模型的Robustness，我们添加了 filters、旋转等扰动到DALL-E生成的新知识图像或实际捕捉图像。ResNet-50是我们选择的，因为它是最佳性能的模型。而在添加扰动后，ResNet-50对5个未扰动图像中的4个正确分类，但对所有扰动图像 incorrect classification，这表明模型有 significiant vulnerability。这些错误分类，对人类来说明显， highlight AI模型的局限性。使用saliency maps，我们identified模型对图像决策中的重要区域。
</details></li>
</ul>
<hr>
<h2 id="StereoFlowGAN-Co-training-for-Stereo-and-Flow-with-Unsupervised-Domain-Adaptation"><a href="#StereoFlowGAN-Co-training-for-Stereo-and-Flow-with-Unsupervised-Domain-Adaptation" class="headerlink" title="StereoFlowGAN: Co-training for Stereo and Flow with Unsupervised Domain Adaptation"></a>StereoFlowGAN: Co-training for Stereo and Flow with Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01842">http://arxiv.org/abs/2309.01842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhexiao Xiong, Feng Qiao, Yu Zhang, Nathan Jacobs</li>
<li>for: 这 paper 用于提出一种基于图像到图像翻译的新训练策略，用于立体匹配和光流估算，以便在真实图像场景下实现优秀的性能。</li>
<li>methods: 这 paper 使用了一种图像到图像翻译的方法，通过在真实图像和Synthetic图像之间进行图像翻译，以便在真实图像场景下训练模型。它还引入了一种bidirectional feature warping模块，可以处理左右和前后方向的图像翻译。</li>
<li>results: 实验结果表明，这 paper 的提出的方法可以比前一些基于域变换的方法更高效地进行立体匹配和光流估算，这证明了该方法的有效性。<details>
<summary>Abstract</summary>
We introduce a novel training strategy for stereo matching and optical flow estimation that utilizes image-to-image translation between synthetic and real image domains. Our approach enables the training of models that excel in real image scenarios while relying solely on ground-truth information from synthetic images. To facilitate task-agnostic domain adaptation and the training of task-specific components, we introduce a bidirectional feature warping module that handles both left-right and forward-backward directions. Experimental results show competitive performance over previous domain translation-based methods, which substantiate the efficacy of our proposed framework, effectively leveraging the benefits of unsupervised domain adaptation, stereo matching, and optical flow estimation.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的训练策略，用于stereo匹配和光流估算，该策略利用图像到图像翻译来在生成图像和实际图像域之间进行图像-图像翻译。我们的方法允许在实际图像场景下训练出 excel 的模型，只靠基于生成图像的真实信息进行训练。为了实现任务不受限制的领域适应和任务特定组件的训练，我们引入了双向特征扭曲模块，可以处理左右和前后两个方向。实验结果显示，我们的提出的框架可以与前一些基于领域翻译的方法相比，并且实际上利用了无监督领域适应、stereo匹配和光流估算的优点。
</details></li>
</ul>
<hr>
<h2 id="On-the-fly-Deep-Neural-Network-Optimization-Control-for-Low-Power-Computer-Vision"><a href="#On-the-fly-Deep-Neural-Network-Optimization-Control-for-Low-Power-Computer-Vision" class="headerlink" title="On the fly Deep Neural Network Optimization Control for Low-Power Computer Vision"></a>On the fly Deep Neural Network Optimization Control for Low-Power Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01824">http://arxiv.org/abs/2309.01824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishmeet Kaur, Adwaita Janardhan Jadhav</li>
<li>for: This paper aims to improve the deployability of state-of-the-art computer vision techniques on resource-constrained edge devices.</li>
<li>methods: The paper proposes a novel technique called AdaptiveActivation, which dynamically adjusts the sparsity and precision of a DNN’s activation function during run-time to improve accuracy and energy consumption.</li>
<li>results: The authors conduct experiments on popular edge devices and show that their approach achieves accuracy within 1.5% of the baseline while requiring 10%–38% less memory, providing more accuracy-efficiency tradeoff options.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提高现代计算机视觉技术在受限的边缘设备上的部署可能性。</li>
<li>methods: 论文提出了一种名为 AdaptiveActivation 的新技术，它在运行时动态调整 DNN 的激活函数输出范围，以提高准确率和能耗。</li>
<li>results: 作者在各种边缘设备上进行了实验，并显示其方法可以达到基eline的准确率（ Within 1.5%），同时需要10%-38% Less memory，提供更多的准确率-效率质量权衡选项。<details>
<summary>Abstract</summary>
Processing visual data on mobile devices has many applications, e.g., emergency response and tracking. State-of-the-art computer vision techniques rely on large Deep Neural Networks (DNNs) that are usually too power-hungry to be deployed on resource-constrained edge devices. Many techniques improve the efficiency of DNNs by using sparsity or quantization. However, the accuracy and efficiency of these techniques cannot be adapted for diverse edge applications with different hardware constraints and accuracy requirements. This paper presents a novel technique to allow DNNs to adapt their accuracy and energy consumption during run-time, without the need for any re-training. Our technique called AdaptiveActivation introduces a hyper-parameter that controls the output range of the DNNs' activation function to dynamically adjust the sparsity and precision in the DNN. AdaptiveActivation can be applied to any existing pre-trained DNN to improve their deployability in diverse edge environments. We conduct experiments on popular edge devices and show that the accuracy is within 1.5% of the baseline. We also show that our approach requires 10%--38% less memory than the baseline techniques leading to more accuracy-efficiency tradeoff options
</details>
<details>
<summary>摘要</summary>
处理移动设备上的视觉数据有很多应用，例如紧急响应和跟踪。现状顶尖计算机视觉技术依靠大深度神经网络（DNNs），但这些大DNNs通常是资源受限的边缘设备上不可deploy。许多技术改进DNNs的效率，使用稀疏性或量化。然而，这些技术不能适应多样化的边缘应用程序不同的硬件限制和准确要求。这篇论文提出了一种新的技术，允许DNNs在运行时自适应准确和能耗，无需任何再训练。我们的技术被称为AdaptiveActivation，它在DNNs的活化函数输出范围中引入了一个超参数，以动态调整DNNs的稀疏性和精度。AdaptiveActivation可以应用于任何现有的预训练DNN，以提高它们在多样化边缘环境中的部署可能性。我们在受欢迎的边缘设备上进行了实验，并证明了准确性与基准值相差1.5%。我们还证明了我们的方法需要10%-38% menos的内存，从而提供更多的准确度-效率质量评估选项
</details></li>
</ul>
<hr>
<h2 id="Multi-dimension-unified-Swin-Transformer-for-3D-Lesion-Segmentation-in-Multiple-Anatomical-Locations"><a href="#Multi-dimension-unified-Swin-Transformer-for-3D-Lesion-Segmentation-in-Multiple-Anatomical-Locations" class="headerlink" title="Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations"></a>Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01823">http://arxiv.org/abs/2309.01823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoyan Pan, Yiqiao Liu, Sarah Halek, Michal Tomaszewski, Shubing Wang, Richard Baumgartner, Jianda Yuan, Gregory Goldmacher, Antong Chen</li>
<li>for: 这个研究旨在提高了肿瘤辐射成像中的肿瘤 segmentation精度，以便为肿瘤生长模型的研究提供更多的数据。</li>
<li>methods: 该研究使用了一种新的模型，即多维度统一Swin transformer（MDU-ST），将2D和3D输入都可以进行学习，并且可以从大量未标注的3D肿瘤量据中学习肿瘤形态下的基本特征。</li>
<li>results: 该研究发现，使用该模型可以在肿瘤成像中提高肿瘤 segmentation精度，并且在评估中得到了与其他模型相比的显著提高。这种方法可以用于自动化肿瘤 segmentation，以便为肿瘤生长模型的研究提供更多的数据。<details>
<summary>Abstract</summary>
In oncology research, accurate 3D segmentation of lesions from CT scans is essential for the modeling of lesion growth kinetics. However, following the RECIST criteria, radiologists routinely only delineate each lesion on the axial slice showing the largest transverse area, and delineate a small number of lesions in 3D for research purposes. As a result, we have plenty of unlabeled 3D volumes and labeled 2D images, and scarce labeled 3D volumes, which makes training a deep-learning 3D segmentation model a challenging task. In this work, we propose a novel model, denoted a multi-dimension unified Swin transformer (MDU-ST), for 3D lesion segmentation. The MDU-ST consists of a Shifted-window transformer (Swin-transformer) encoder and a convolutional neural network (CNN) decoder, allowing it to adapt to 2D and 3D inputs and learn the corresponding semantic information in the same encoder. Based on this model, we introduce a three-stage framework: 1) leveraging large amount of unlabeled 3D lesion volumes through self-supervised pretext tasks to learn the underlying pattern of lesion anatomy in the Swin-transformer encoder; 2) fine-tune the Swin-transformer encoder to perform 2D lesion segmentation with 2D RECIST slices to learn slice-level segmentation information; 3) further fine-tune the Swin-transformer encoder to perform 3D lesion segmentation with labeled 3D volumes. The network's performance is evaluated by the Dice similarity coefficient (DSC) and Hausdorff distance (HD) using an internal 3D lesion dataset with 593 lesions extracted from multiple anatomical locations. The proposed MDU-ST demonstrates significant improvement over the competing models. The proposed method can be used to conduct automated 3D lesion segmentation to assist radiomics and tumor growth modeling studies. This paper has been accepted by the IEEE International Symposium on Biomedical Imaging (ISBI) 2023.
</details>
<details>
<summary>摘要</summary>
在肿瘤研究中，准确的3D肿瘤分割从CT扫描图是非常重要的，以模拟肿瘤增长趋势。然而，根据RECIST标准，医生通常只在最大横向面的AXIAL slice上画出肿瘤，并且只为研究目的画出少量3D肿瘤。这意味着我们有大量未标注的3D卷积和标注的2D图像，以及罕见的标注3D卷积，这使得训练深度学习3D分割模型成为一项挑战。在这种情况下，我们提出了一种新的模型，即多维度统一Swin变换（MDU-ST），用于肿瘤分割。MDU-ST包括Swin变换器（Swin-transformer）编码器和卷积神经网络（CNN）解码器，可以适应2D和3D输入，并在同一个编码器中学习相应的 semantic 信息。基于这种模型，我们提出了一个三个阶段的框架：1）通过自动驱动的预TEXT任务来利用大量未标注的3D肿瘤卷积来学习肿瘤生物学的下面纹理；2）根据2D RECIST slice 进行精度调整Swin-transformer编码器，以学习 slice-level 分割信息；3）进一步精度调整Swin-transformer编码器，以进行3D肿瘤分割使用标注3D卷积。网络的性能被评估于 internal 3D 肿瘤数据集中，包括593个肿瘤，从多个 анаatomical 位置中提取。提出的 MDU-ST 表现出色，胜过竞争模型。该方法可以用于自动进行3D肿瘤分割，以帮助 радиOmics 和肿瘤增长模型研究。这篇文章已经被Accepted by IEEE International Symposium on Biomedical Imaging（ISBI）2023。
</details></li>
</ul>
<hr>
<h2 id="Instant-Continual-Learning-of-Neural-Radiance-Fields"><a href="#Instant-Continual-Learning-of-Neural-Radiance-Fields" class="headerlink" title="Instant Continual Learning of Neural Radiance Fields"></a>Instant Continual Learning of Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01811">http://arxiv.org/abs/2309.01811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Po, Zhengyang Dong, Alexander W. Bergman, Gordon Wetzstein</li>
<li>for:  novle-view synthesis和3D scene reconstruction</li>
<li>methods:  replay-based methods combined with a hybrid explicit–implicit scene representation</li>
<li>results:  higher reconstruction quality and faster training than previous methods<details>
<summary>Abstract</summary>
Neural radiance fields (NeRFs) have emerged as an effective method for novel-view synthesis and 3D scene reconstruction. However, conventional training methods require access to all training views during scene optimization. This assumption may be prohibitive in continual learning scenarios, where new data is acquired in a sequential manner and a continuous update of the NeRF is desired, as in automotive or remote sensing applications. When naively trained in such a continual setting, traditional scene representation frameworks suffer from catastrophic forgetting, where previously learned knowledge is corrupted after training on new data. Prior works in alleviating forgetting with NeRFs suffer from low reconstruction quality and high latency, making them impractical for real-world application. We propose a continual learning framework for training NeRFs that leverages replay-based methods combined with a hybrid explicit--implicit scene representation. Our method outperforms previous methods in reconstruction quality when trained in a continual setting, while having the additional benefit of being an order of magnitude faster.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accuracy-and-Consistency-of-Space-based-Vegetation-Height-Maps-for-Forest-Dynamics-in-Alpine-Terrain"><a href="#Accuracy-and-Consistency-of-Space-based-Vegetation-Height-Maps-for-Forest-Dynamics-in-Alpine-Terrain" class="headerlink" title="Accuracy and Consistency of Space-based Vegetation Height Maps for Forest Dynamics in Alpine Terrain"></a>Accuracy and Consistency of Space-based Vegetation Height Maps for Forest Dynamics in Alpine Terrain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01797">http://arxiv.org/abs/2309.01797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchang Jiang, Marius Rüetschi, Vivien Sainte Fare Garnot, Mauro Marty, Konrad Schindler, Christian Ginzler, Jan D. Wegner</li>
<li>for: 提高瑞士国家森林评估的时间分辨率</li>
<li>methods: 使用卫星遥感和深度学习生成大规模的植被高程地图</li>
<li>results: 实现年度、国家范围内的植被高程地图，并对植被高程地图进行变化探测，检测到小于250平方米的变化<details>
<summary>Abstract</summary>
Monitoring and understanding forest dynamics is essential for environmental conservation and management. This is why the Swiss National Forest Inventory (NFI) provides countrywide vegetation height maps at a spatial resolution of 0.5 m. Its long update time of 6 years, however, limits the temporal analysis of forest dynamics. This can be improved by using spaceborne remote sensing and deep learning to generate large-scale vegetation height maps in a cost-effective way. In this paper, we present an in-depth analysis of these methods for operational application in Switzerland. We generate annual, countrywide vegetation height maps at a 10-meter ground sampling distance for the years 2017 to 2020 based on Sentinel-2 satellite imagery. In comparison to previous works, we conduct a large-scale and detailed stratified analysis against a precise Airborne Laser Scanning reference dataset. This stratified analysis reveals a close relationship between the model accuracy and the topology, especially slope and aspect. We assess the potential of deep learning-derived height maps for change detection and find that these maps can indicate changes as small as 250 $m^2$. Larger-scale changes caused by a winter storm are detected with an F1-score of 0.77. Our results demonstrate that vegetation height maps computed from satellite imagery with deep learning are a valuable, complementary, cost-effective source of evidence to increase the temporal resolution for national forest assessments.
</details>
<details>
<summary>摘要</summary>
监测和理解森林动态是环境保护和管理的关键。为此，瑞士国家森林调查（NFI）提供了全国覆盖率0.5米的植被高度地图。然而，NFI的更新周期为6年，限制了森林动态的时间分析。这可以通过使用空间Remote sensing和深度学习生成大规模的植被高度地图来改进。在这篇论文中，我们对这些方法进行了详细的分析，并在瑞士进行了操作应用。我们生成了2017年至2020年的年度、全国覆盖率10米的植被高度地图，基于Sentinel-2卫星图像。与之前的研究相比，我们进行了大规模的 stratified 分析，并对精确的空中雷达扫描参照数据进行了比较。这种 stratified 分析表明模型精度与地形特征（坡度和方向）之间存在紧密的关系。我们评估了深度学习得到的高度地图在变化检测方面的潜力，并发现这些地图可以检测变化为小为250平方米。在更大规模的变化（冬季风暴）方面，我们获得了F1分数0.77。我们的结果表明，通过卫星图像使用深度学习计算的植被高度地图是一种有价值的、补充性、成本效果的证据，可以增加国家森林评估的时间分辨率。
</details></li>
</ul>
<hr>
<h2 id="Safe-and-Robust-Watermark-Injection-with-a-Single-OoD-Image"><a href="#Safe-and-Robust-Watermark-Injection-with-a-Single-OoD-Image" class="headerlink" title="Safe and Robust Watermark Injection with a Single OoD Image"></a>Safe and Robust Watermark Injection with a Single OoD Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01786">http://arxiv.org/abs/2309.01786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyang Yu, Junyuan Hong, Haobo Zhang, Haotao Wang, Zhangyang Wang, Jiayu Zhou</li>
<li>for: 保护深度神经网络模型的知识产权和商业所有权</li>
<li>methods: 使用一个单一的 OUT-OF-distribution（OoD）图像作为秘密钥刃，并通过随机偏移模型参数来防御常见的水印移除攻击</li>
<li>results: 提出了一种安全和可靠的水印插入技术，可以在不需要训练数据的情况下，在不同的模型版本上保持水印的可读性和不朽性。<details>
<summary>Abstract</summary>
Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.
</details>
<details>
<summary>摘要</summary>
训练高性能深度神经网络需要大量的数据和计算资源。保护深度模型的知识产权（IP）和商业所有权是一项挑战，但也变得越来越重要。一条主要的水印策略是通过毒化训练样本来植入可靠的后门 triggers，但这些常常因数据隐私和安全问题而不切实际，并且容易受到微型化改进的影响。为了解决这些挑战，我们提议一种安全和可靠的后门基于水印注入技术，利用单个 OUT-OF-distribution（OoD）图像作为秘密键 для IP 验证。训练数据的独立性使其不受第三方的IP安全承诺影响。我们通过在水印注入过程中随机偏移模型参数来增强鲁棒性，以抵御通常的水印移除攻击，包括微型化、剪辑和模型提取。我们的实验结果表明，我们提议的水印策略不仅时间和样本效率高，而且具有很好的鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="StyleAdapter-A-Single-Pass-LoRA-Free-Model-for-Stylized-Image-Generation"><a href="#StyleAdapter-A-Single-Pass-LoRA-Free-Model-for-Stylized-Image-Generation" class="headerlink" title="StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image Generation"></a>StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01770">http://arxiv.org/abs/2309.01770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi, Ying Shan, Wenping Wang, Ping Luo</li>
<li>for: 本研究旨在提出一种不需要LoRA的图像美化方法，该方法可以根据文本提示和样式参考图像来生成输出图像，而不需要训练每种样式的LoRA。</li>
<li>methods: 本方法使用了两种组件：一个两路扩充模块（TPCA）和三种解除策略。这些组件使得我们的模型可以分离文本提示和样式参考特征，并减少样式参考中的强相关性，从而提高图像质量和多样性。</li>
<li>results: 实验表明，我们的方法可以生成高质量的图像，并且可以适应不同的样式（包括未经见过的样式），而不需要多个LoRA。相比之下，现有方法 Less flexible and less efficient。<details>
<summary>Abstract</summary>
This paper presents a LoRA-free method for stylized image generation that takes a text prompt and style reference images as inputs and produces an output image in a single pass. Unlike existing methods that rely on training a separate LoRA for each style, our method can adapt to various styles with a unified model. However, this poses two challenges: 1) the prompt loses controllability over the generated content, and 2) the output image inherits both the semantic and style features of the style reference image, compromising its content fidelity. To address these challenges, we introduce StyleAdapter, a model that comprises two components: a two-path cross-attention module (TPCA) and three decoupling strategies. These components enable our model to process the prompt and style reference features separately and reduce the strong coupling between the semantic and style information in the style references. StyleAdapter can generate high-quality images that match the content of the prompts and adopt the style of the references (even for unseen styles) in a single pass, which is more flexible and efficient than previous methods. Experiments have been conducted to demonstrate the superiority of our method over previous works.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BLiSS-Bootstrapped-Linear-Shape-Space"><a href="#BLiSS-Bootstrapped-Linear-Shape-Space" class="headerlink" title="BLiSS: Bootstrapped Linear Shape Space"></a>BLiSS: Bootstrapped Linear Shape Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01765">http://arxiv.org/abs/2309.01765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjeev Muralikrishnan, Chun-Hao Paul Huang, Duygu Ceylan, Niloy J. Mitra</li>
<li>for: 创建人类形态模型，提高人类形态数据的表示和分析能力</li>
<li>methods: 使用自适应扩展模型，通过精细调整和非rigid registration来实现人类形态数据的匹配</li>
<li>results: 提出BLiSS方法，可以自动将新的不注册扫描数据匹配到已有的注册扫描数据中，提高人类形态数据的表示和分析能力<details>
<summary>Abstract</summary>
Morphable models are fundamental to numerous human-centered processes as they offer a simple yet expressive shape space. Creating such morphable models, however, is both tedious and expensive. The main challenge is establishing dense correspondences across raw scans that capture sufficient shape variation. This is often addressed using a mix of significant manual intervention and non-rigid registration. We observe that creating a shape space and solving for dense correspondence are tightly coupled -- while dense correspondence is needed to build shape spaces, an expressive shape space provides a reduced dimensional space to regularize the search. We introduce BLiSS, a method to solve both progressively. Starting from a small set of manually registered scans to bootstrap the process, we enrich the shape space and then use that to get new unregistered scans into correspondence automatically. The critical component of BLiSS is a non-linear deformation model that captures details missed by the low-dimensional shape space, thus allowing progressive enrichment of the space.
</details>
<details>
<summary>摘要</summary>
《膨润模型是人类中心的过程中的基础模型，它们提供了简单 yet 表达力强的形态空间。然而，创建这些膨润模型是时间和成本的挑战。主要挑战在于在原始扫描图像之间建立密集的对匹配，以捕捉足够的形态变化。通常通过手动干预和非RIGID注册来解决这个问题。我们发现创建形态空间和 dense correspondence 是紧密相关的——而 dense correspondence 是建立形态空间的必要条件，而且一个表达力强的形态空间可以提供一个减少维度的空间来规范搜索。我们介绍了 BLiSS，一种解决这两个问题的方法。从一个小型手动注册的扫描图像开始，我们在形态空间中增强表达力，然后用这个空间来自动将新的未注册扫描图像与其他扫描图像进行对匹配。BLiSS 的关键组成部分是一种非线性塑形模型，它可以捕捉低维度形态空间所过度的细节，从而允许进行进一步的表达力增强。》
</details></li>
</ul>
<hr>
<h2 id="Multispectral-Indices-for-Wildfire-Management"><a href="#Multispectral-Indices-for-Wildfire-Management" class="headerlink" title="Multispectral Indices for Wildfire Management"></a>Multispectral Indices for Wildfire Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01751">http://arxiv.org/abs/2309.01751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afonso Oliveira, João P. Matos-Carvalho, Filipe Moutinho, Nuno Fachada</li>
<li>for: 这篇论文旨在为火灾预防和管理提供 Multispectral 指标和相关方法。</li>
<li>methods: 论文检查了多种领域，其中 Multispectral 指标与野火预防和管理有着 closest 关系，包括植被和土壤特征提取、水特征映射、人工结构识别和火灾后烧区面积估计。</li>
<li>results: 论文强调了 Multispectral 指标在野火管理中的 universality 和有效性，并提供了具体的指标，如 NDVI 和 NDWI。 同时，为了提高准确性和解决个体指标应用中的局限性，建议 integra  complementary 处理解决方案和其他数据源，如高分辨率图像和地面测量。<details>
<summary>Abstract</summary>
This paper highlights and summarizes the most important multispectral indices and associated methodologies for fire management. Various fields of study are examined where multispectral indices align with wildfire prevention and management, including vegetation and soil attribute extraction, water feature mapping, artificial structure identification, and post-fire burnt area estimation. The versatility and effectiveness of multispectral indices in addressing specific issues in wildfire management are emphasized. Fundamental insights for optimizing data extraction are presented. Concrete indices for each task, including the NDVI and the NDWI, are suggested. Moreover, to enhance accuracy and address inherent limitations of individual index applications, the integration of complementary processing solutions and additional data sources like high-resolution imagery and ground-based measurements is recommended. This paper aims to be an immediate and comprehensive reference for researchers and stakeholders working on multispectral indices related to the prevention and management of fires.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文探讨了多spectral指标的最重要应用和方法在野火管理中，包括植被和土壤特征提取、水特征地图、人工结构识别和火灾后烧Area估计。论文强调了多spectral指标在野火预防和管理中的 versatility和有效性。提供了数据提取优化的基本理念，并建议使用NDVI和NDWI等指标。此外，为了提高准确性和解决个体指标应用的局限性，建议结合补充处理解决方案和高分辨率图像以及地面测量数据。这篇论文旨在为研究人员和相关方 working on多spectral指标与野火预防和管理的人提供立即和全面的参考。
</details></li>
</ul>
<hr>
<h2 id="Generative-based-Fusion-Mechanism-for-Multi-Modal-Tracking"><a href="#Generative-based-Fusion-Mechanism-for-Multi-Modal-Tracking" class="headerlink" title="Generative-based Fusion Mechanism for Multi-Modal Tracking"></a>Generative-based Fusion Mechanism for Multi-Modal Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01728">http://arxiv.org/abs/2309.01728</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangyong-tang/gmmt">https://github.com/zhangyong-tang/gmmt</a></li>
<li>paper_authors: Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Xiao-Jun Wu, Josef Kittler</li>
<li>for: 本研究探讨了如何使用生成模型技术来解决多Modal跟踪中的信息融合挑战。</li>
<li>methods: 本研究使用了两种常见的生成模型技术， namely Conditional Generative Adversarial Networks (CGANs) 和 Diffusion Models (DMs)。这些技术在传统的融合过程中直接将每种模式的特征传输到融合块，而不是直接将特征传输。</li>
<li>results: 经验表明，使用生成模型技术可以提高多Modal跟踪的性能，并在 LasHeR 和 RGBD1K 上设置新的纪录。<details>
<summary>Abstract</summary>
Generative models (GMs) have received increasing research interest for their remarkable capacity to achieve comprehensive understanding. However, their potential application in the domain of multi-modal tracking has remained relatively unexplored. In this context, we seek to uncover the potential of harnessing generative techniques to address the critical challenge, information fusion, in multi-modal tracking. In this paper, we delve into two prominent GM techniques, namely, Conditional Generative Adversarial Networks (CGANs) and Diffusion Models (DMs). Different from the standard fusion process where the features from each modality are directly fed into the fusion block, we condition these multi-modal features with random noise in the GM framework, effectively transforming the original training samples into harder instances. This design excels at extracting discriminative clues from the features, enhancing the ultimate tracking performance. To quantitatively gauge the effectiveness of our approach, we conduct extensive experiments across two multi-modal tracking tasks, three baseline methods, and three challenging benchmarks. The experimental results demonstrate that the proposed generative-based fusion mechanism achieves state-of-the-art performance, setting new records on LasHeR and RGBD1K.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:生成模型（GM）已经收到了研究的增加兴趣，特别是在多modal跟踪领域中。在这个预期中，我们想要探索生成技术的潜在应用，以解决多modal跟踪中的关键挑战——信息融合。在这篇论文中，我们探究了两种主要的生成技术——条件生成对抗网络（CGAN）和扩散模型（DM）。与标准融合过程不同，我们在生成模型框架中，通过conditioning多modal特征来增强特征的抽象能力，从而提高最终跟踪性能。为了量化评估我们的方法的效果，我们在两个多modal跟踪任务、三个基eline方法和三个挑战性 benchmark 上进行了广泛的实验。实验结果表明，我们提出的生成基于融合机制可以达到状态级表现，在 LasHeR 和 RGBD1K 上设置新的纪录。
</details></li>
</ul>
<hr>
<h2 id="SAF-IS-a-Spatial-Annotation-Free-Framework-for-Instance-Segmentation-of-Surgical-Tools"><a href="#SAF-IS-a-Spatial-Annotation-Free-Framework-for-Instance-Segmentation-of-Surgical-Tools" class="headerlink" title="SAF-IS: a Spatial Annotation Free Framework for Instance Segmentation of Surgical Tools"></a>SAF-IS: a Spatial Annotation Free Framework for Instance Segmentation of Surgical Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01723">http://arxiv.org/abs/2309.01723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Sestini, Benoit Rosa, Elena De Momi, Giancarlo Ferrigno, Nicolas Padoy</li>
<li>for:  This paper aims to develop a framework for instance segmentation of surgical instruments without requiring expensive pixel-level annotations.</li>
<li>methods: The proposed solution uses binary tool masks and binary tool presence labels to train a tool instance classifier, leveraging unsupervised binary segmentation models to obtain the masks.</li>
<li>results: The approach outperforms several state-of-the-art fully-supervised segmentation methods and is completely free from spatial annotations.<details>
<summary>Abstract</summary>
Instance segmentation of surgical instruments is a long-standing research problem, crucial for the development of many applications for computer-assisted surgery. This problem is commonly tackled via fully-supervised training of deep learning models, requiring expensive pixel-level annotations to train. In this work, we develop a framework for instance segmentation not relying on spatial annotations for training. Instead, our solution only requires binary tool masks, obtainable using recent unsupervised approaches, and binary tool presence labels, freely obtainable in robot-assisted surgery. Based on the binary mask information, our solution learns to extract individual tool instances from single frames, and to encode each instance into a compact vector representation, capturing its semantic features. Such representations guide the automatic selection of a tiny number of instances (8 only in our experiments), displayed to a human operator for tool-type labelling. The gathered information is finally used to match each training instance with a binary tool presence label, providing an effective supervision signal to train a tool instance classifier. We validate our framework on the EndoVis 2017 and 2018 segmentation datasets. We provide results using binary masks obtained either by manual annotation or as predictions of an unsupervised binary segmentation model. The latter solution yields an instance segmentation approach completely free from spatial annotations, outperforming several state-of-the-art fully-supervised segmentation approaches.
</details>
<details>
<summary>摘要</summary>
Instance segmentation of surgical instruments是长期的研究问题，对计算机助手手术应用的发展非常重要。通常通过深度学习模型的全导学习来解决这个问题，需要昂贵的像素级注解来训练。在这种工作中，我们开发了不需要空间注解的实例分割框架。而是利用最近的无监督方法获取的二进制工具面积，以及在机器人助手手术中自由获得的二进制工具存在标签。基于二进制面积信息，我们的解决方案可以从单帧中提取个体工具实例，并将每个实例编码为一个紧凑的向量表示，捕捉其 semantic 特征。这些表示导引人工操作员选择工具类型标签。最终，我们使用这些标签来匹配每个训练实例与二进制工具存在标签，以提供有效的超级视图信号，用于训练工具实例分类器。我们在EndoVis 2017和2018 segmentation dataset上验证了我们的框架。我们使用手动注解或predictions of unsupervised binary segmentation model来获取二进制面积。后者解决方案可以完全免除空间注解，并在数个状态之前的全导学习方法之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="ControlMat-A-Controlled-Generative-Approach-to-Material-Capture"><a href="#ControlMat-A-Controlled-Generative-Approach-to-Material-Capture" class="headerlink" title="ControlMat: A Controlled Generative Approach to Material Capture"></a>ControlMat: A Controlled Generative Approach to Material Capture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01700">http://arxiv.org/abs/2309.01700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, Tamy Boubekeur<br>for: 提出了一种控制的数据生成方法，用于从单个照片中生成可信、可缩放、物理基础的数字材料。methods: 使用了生成深度网络进行控制synthesis，并采用了多通道输出的diffusion模型，采样过程进行多尺度信息融合，并引入了折叠diffusion来实现高分辨率输出和缩放性。results: 比较了与推论和秘密空间优化方法，显示了控制Mat的超越性，并且仔细验证了diffusion过程的设计选择。<details>
<summary>Abstract</summary>
Material reconstruction from a photograph is a key component of 3D content creation democratization. We propose to formulate this ill-posed problem as a controlled synthesis one, leveraging the recent progress in generative deep networks. We present ControlMat, a method which, given a single photograph with uncontrolled illumination as input, conditions a diffusion model to generate plausible, tileable, high-resolution physically-based digital materials. We carefully analyze the behavior of diffusion models for multi-channel outputs, adapt the sampling process to fuse multi-scale information and introduce rolled diffusion to enable both tileability and patched diffusion for high-resolution outputs. Our generative approach further permits exploration of a variety of materials which could correspond to the input image, mitigating the unknown lighting conditions. We show that our approach outperforms recent inference and latent-space-optimization methods, and carefully validate our diffusion process design choices. Supplemental materials and additional details are available at: https://gvecchio.com/controlmat/.
</details>
<details>
<summary>摘要</summary>
Material 重建从照片是3D内容创造的关键组件。我们提议将这个不定性问题转化为控制的合成问题，利用最近的生成深度网络的进步。我们提出ControlMat方法，给定一个具有不控制照明的照片输入，使用扩散模型生成可信、可缩放、基于物理的高分辨率数字材料。我们仔细分析扩散模型的多通道输出行为，适应多尺度信息的融合和滤波rolled扩散，以实现高分辨率输出的瓦片可重复性和补充扩散。我们的生成方法还允许探索输入图像所对应的多种材料， mitigate不确定的照明条件。我们比较了我们的扩散过程设计选择，并证明我们的方法超越了最近的推理和latent空间优化方法。补充材料和更多细节可以在：https://gvecchio.com/controlmat/。
</details></li>
</ul>
<hr>
<h2 id="Mask-Attention-Free-Transformer-for-3D-Instance-Segmentation"><a href="#Mask-Attention-Free-Transformer-for-3D-Instance-Segmentation" class="headerlink" title="Mask-Attention-Free Transformer for 3D Instance Segmentation"></a>Mask-Attention-Free Transformer for 3D Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01692">http://arxiv.org/abs/2309.01692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/mask-attention-free-transformer">https://github.com/dvlab-research/mask-attention-free-transformer</a></li>
<li>paper_authors: Xin Lai, Yuhui Yuan, Ruihang Chu, Yukang Chen, Han Hu, Jiaya Jia</li>
<li>for: 提高3D实例分割 task 的速度和准确率，即使初始masks的召回率低。</li>
<li>methods: 弃用mask attention设计，改用auxiliary center regression任务，通过positional prior来进行cross-attention和迭代改进。</li>
<li>results: 与现有工作相比，我们的方法可以在ScanNetv2 3D实例分割benchmark上 converge 4x faster，并且在多个dataset上显示出超过现有方法的性能。<details>
<summary>Abstract</summary>
Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.
</details>
<details>
<summary>摘要</summary>
最近，基于 transformer 的方法在 3D 实例分割中占据了主导地位，其中 mask attention 是通常 involve 的一部分。具体来说，对象查询在第一次 cross-attention 中被 guid 由初始实例面积的面积注意力，然后在类似的方式进行 iterative refinement。但我们发现，mask attention 管道通常会导致慢速收敛，因为初始实例面积的准确率较低。因此，我们放弃了面积注意力设计，转而使用 auxillary center regression 任务来解决这个问题。通过 center regression，我们可以有效地超越低准确率的问题，并通过做Positional Prior来进行 cross-attention。为了实现这个目标，我们开发了一系列的位置意识设计。首先，我们学习了 3D 空间中的位置分布，作为初始的位置查询。它们在 3D 空间中分布 densely，可以轻松地捕捉 scene 中的对象，并且具有高准确率。此外，我们还提供了相对位置编码 для cross-attention 和 iterative refinement，以便更准确地确定位置查询。实验表明，我们的方法可以在 ScanNetv2 3D 实例分割 benchmark 上 converges 4x  faster than 现有的工作，并且在多个 dataset 上也表现出了superior的性能。代码和模型可以在 https://github.com/dvlab-research/Mask-Attention-Free-Transformer 上获取。
</details></li>
</ul>
<hr>
<h2 id="Prior-Knowledge-Guided-Network-for-Video-Anomaly-Detection"><a href="#Prior-Knowledge-Guided-Network-for-Video-Anomaly-Detection" class="headerlink" title="Prior Knowledge Guided Network for Video Anomaly Detection"></a>Prior Knowledge Guided Network for Video Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01682">http://arxiv.org/abs/2309.01682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhewen Deng, Dongyue Chen, Shizhuo Deng</li>
<li>for: video anomaly detection (VAD)</li>
<li>methods: 使用教师学生网络、自适应网络、知识填充等方法提高模型的泛化能力和多尺度检测能力</li>
<li>results: 实验结果表明，我们的方法可以更高效、更准确地检测视频异常事件，比现有的方法更高效。<details>
<summary>Abstract</summary>
Video Anomaly Detection (VAD) involves detecting anomalous events in videos, presenting a significant and intricate task within intelligent video surveillance. Existing studies often concentrate solely on features acquired from limited normal data, disregarding the latent prior knowledge present in extensive natural image datasets. To address this constraint, we propose a Prior Knowledge Guided Network(PKG-Net) for the VAD task. First, an auto-encoder network is incorporated into a teacher-student architecture to learn two designated proxy tasks: future frame prediction and teacher network imitation, which can provide better generalization ability on unknown samples. Second, knowledge distillation on proper feature blocks is also proposed to increase the multi-scale detection ability of the model. In addition, prediction error and teacher-student feature inconsistency are combined to evaluate anomaly scores of inference samples more comprehensively. Experimental results on three public benchmarks validate the effectiveness and accuracy of our method, which surpasses recent state-of-the-arts.
</details>
<details>
<summary>摘要</summary>
视频异常检测（VAD）涉及到视频中异常事件的检测，是智能视频监测中的一个复杂和繁复任务。现有研究 часто仅仅使用有限的正常数据来学习特征，忽略了大量自然图像数据中的隐藏知识。为解决这一问题，我们提议一种基于先前知识指导网络（PKG-Net）的方法。首先，我们在教师-学生架构中 integrate 一个自编码器网络，以学习两个指定的代理任务：未来帧预测和教师网络模仿，以提高对未知样本的泛化能力。其次，我们还提出了在正确的特征块上进行知识填充，以增强模型的多尺度检测能力。此外，我们还结合预测错误和教师-学生特征不一致来评估推理样本的异常分数。实验结果表明，我们的方法可以在三个公共标准测试集上达到更高的准确率，超过当前状态的艺术。
</details></li>
</ul>
<hr>
<h2 id="Building-Footprint-Extraction-in-Dense-Areas-using-Super-Resolution-and-Frame-Field-Learning"><a href="#Building-Footprint-Extraction-in-Dense-Areas-using-Super-Resolution-and-Frame-Field-Learning" class="headerlink" title="Building Footprint Extraction in Dense Areas using Super Resolution and Frame Field Learning"></a>Building Footprint Extraction in Dense Areas using Super Resolution and Frame Field Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01656">http://arxiv.org/abs/2309.01656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vuong Nguyen, Anh Ho, Duc-Anh Vu, Nguyen Thi Ngoc Anh, Tran Ngoc Thang</li>
<li>for: 提高叠 edifices 的精度和精细度，使得在压杂的区域中提取建筑物的轮廓更加精准。</li>
<li>methods: 使用超分解提高空中图像的空间分辨率，然后使用多任务学习模块进行分割和框架场景学习，以处理不规则的建筑结构。</li>
<li>results: 对印度一个贫民区的实验表明，提出的方法可以明显超越当前状态的方法，具有较高的精度和精细度。<details>
<summary>Abstract</summary>
Despite notable results on standard aerial datasets, current state-of-the-arts fail to produce accurate building footprints in dense areas due to challenging properties posed by these areas and limited data availability. In this paper, we propose a framework to address such issues in polygonal building extraction. First, super resolution is employed to enhance the spatial resolution of aerial image, allowing for finer details to be captured. This enhanced imagery serves as input to a multitask learning module, which consists of a segmentation head and a frame field learning head to effectively handle the irregular building structures. Our model is supervised by adaptive loss weighting, enabling extraction of sharp edges and fine-grained polygons which is difficult due to overlapping buildings and low data quality. Extensive experiments on a slum area in India that mimics a dense area demonstrate that our proposed approach significantly outperforms the current state-of-the-art methods by a large margin.
</details>
<details>
<summary>摘要</summary>
尽管现有的州际数据集上得到了可注目的结果，但现今的状态艺术无法在受挑战的区域中生成准确的建筑面积，这是因为这些区域具有复杂的属性和有限的数据可用性。在这篇论文中，我们提出了一种框架来解决这些问题。我们首先使用超分解来提高飞行图像的空间分辨率，以便更好地捕捉详细的建筑结构。这个提高的图像作为输入，我们的模型包括一个分割头和一个帧场学习头，以有效地处理不规则的建筑结构。我们的模型被超过适应损失质量调整，以提取锐利的边和细腻的多边形，这是由于 overlap 的建筑和低质量数据所致。我们的提出的方法在印度一个模拟 dense 区域的实验中显示出了与当前状态艺术方法之间的大幅提高。
</details></li>
</ul>
<hr>
<h2 id="Relay-Diffusion-Unifying-diffusion-process-across-resolutions-for-image-synthesis"><a href="#Relay-Diffusion-Unifying-diffusion-process-across-resolutions-for-image-synthesis" class="headerlink" title="Relay Diffusion: Unifying diffusion process across resolutions for image synthesis"></a>Relay Diffusion: Unifying diffusion process across resolutions for image synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03350">http://arxiv.org/abs/2309.03350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THUDM/RelayDiffusion">https://github.com/THUDM/RelayDiffusion</a></li>
<li>paper_authors: Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, Jie Tang</li>
<li>for: 这 paper 是用于描述一种基于抽象扩散模型的高分辨率图像生成方法。</li>
<li>methods: 这 paper 使用了一种叫做 Relay Diffusion Model (RDM)，它可以将低分辨率图像或噪声转换成等效的高分辨率图像，从而让扩散过程可以继续无间断地进行在任何新的分辨率或模型中。</li>
<li>results: 这 paper 的实验结果表明，RDM 可以在 CelebA-HQ 和 ImageNet 256$\times$256 上 achieved state-of-the-art FID 和 sFID Result，大幅超越过去的 ADM、LDM 和 DiT 等方法。<details>
<summary>Abstract</summary>
Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that \emph{the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain}. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \url{https://github.com/THUDM/RelayDiffusion}.
</details>
<details>
<summary>摘要</summary>
Diffusion models have achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find that the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256×256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \url{https://github.com/THUDM/RelayDiffusion}.Here's the word-for-word translation of the text:Diffusion models 已经取得了很大的成功在图像生成中，但仍然面临高分辨率生成的挑战。通过抽象幂transform的孔径，我们发现主要的问题在于，在更高的分辨率下，相同的噪声水平会导致更高的信号噪声比在频域中。在这项工作中，我们提出了Relay Diffusion Model（RDM），它通过混合扩散和块噪声来将低分辨率图像或噪声转换成与扩散模型相对应的高分辨率图像。因此，扩散过程可以不间断继续在任何新的分辨率或模型上进行，不需要从纯噪声或低分辨率conditioning重新开始。RDM在CelebA-HQ和ImageNet 256×256上 achieve state-of-the-art FID和sFID，大幅超过了前一些工作，如ADM、LDM和DiT。所有的代码和检查点都是开源的，可以在 \url{https://github.com/THUDM/RelayDiffusion} 上找到。
</details></li>
</ul>
<hr>
<h2 id="ReLoc-PDR-Visual-Relocalization-Enhanced-Pedestrian-Dead-Reckoning-via-Graph-Optimization"><a href="#ReLoc-PDR-Visual-Relocalization-Enhanced-Pedestrian-Dead-Reckoning-via-Graph-Optimization" class="headerlink" title="ReLoc-PDR: Visual Relocalization Enhanced Pedestrian Dead Reckoning via Graph Optimization"></a>ReLoc-PDR: Visual Relocalization Enhanced Pedestrian Dead Reckoning via Graph Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01646">http://arxiv.org/abs/2309.01646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongyang Chen, Xianfei Pan, Changhao Chen</li>
<li>for: 本研究旨在提供一种精准地位化行人在卫星排除条件下，使用低成本抗 gravitational 倾斜传感器。</li>
<li>methods: 该研究提出了一种 combining PDR 和视觉重定位技术，使用图像优化算法和学习描述符来实现Robust位置估算。</li>
<li>results: 实验表明，我们的 ReLoc-PDR 在不良环境中表现出了较高的准确性和可靠性，可以在较少的文本环境和黑夜场景中实现高精度的行人位置估算。<details>
<summary>Abstract</summary>
Accurately and reliably positioning pedestrians in satellite-denied conditions remains a significant challenge. Pedestrian dead reckoning (PDR) is commonly employed to estimate pedestrian location using low-cost inertial sensor. However, PDR is susceptible to drift due to sensor noise, incorrect step detection, and inaccurate stride length estimation. This work proposes ReLoc-PDR, a fusion framework combining PDR and visual relocalization using graph optimization. ReLoc-PDR leverages time-correlated visual observations and learned descriptors to achieve robust positioning in visually-degraded environments. A graph optimization-based fusion mechanism with the Tukey kernel effectively corrects cumulative errors and mitigates the impact of abnormal visual observations. Real-world experiments demonstrate that our ReLoc-PDR surpasses representative methods in accuracy and robustness, achieving accurte and robust pedestrian positioning results using only a smartphone in challenging environments such as less-textured corridors and dark nighttime scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>精度和可靠地定位行人在卫星探测不可靠情况下是一个重要挑战。行人死reckoning（PDR）通常用低成本惯性传感器来估算行人位置。然而，PDR受到传感器噪声、错误的步动检测和不准确的步长估算的影响，导致偏移。本工作提出了Reloc-PDR，一种混合框架， комбини了PDR和视觉重定位使用图像优化。Reloc-PDR利用时间相关的视觉观察和学习的特征来实现在视觉弱化环境中Robust的定位。一种图像优化基于Tukeykernel的混合机制，有效地纠正累累的错误和减少了不正常的视觉观察的影响。实际实验表明，我们的Reloc-PDR在精度和Robust性方面超过了代表性的方法，在杂糌走廊和黑夜enario中实现了高精度和Robust的行人定位，只使用了智能手机。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Cross-Consistent-Deep-Unfolding-Network-for-Adaptive-All-In-One-Video-Restoration"><a href="#Cross-Consistent-Deep-Unfolding-Network-for-Adaptive-All-In-One-Video-Restoration" class="headerlink" title="Cross-Consistent Deep Unfolding Network for Adaptive All-In-One Video Restoration"></a>Cross-Consistent Deep Unfolding Network for Adaptive All-In-One Video Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01627">http://arxiv.org/abs/2309.01627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanshuo Cheng, Mingwen Shao, Yecong Wan, Lixu Zhang, Wangmeng Zuo, Deyu Meng</li>
<li>for: 提高实际应用中视频修复（VR）方法的可扩展性和可靠性。</li>
<li>methods: 提议了一种 Cross-consistent Deep Unfolding Network（CDUN），可以通过单一模型来消除多种降低效果。</li>
<li>results: 实验表明，提议的方法可以在All-In-One VR中实现状态最佳的表现。<details>
<summary>Abstract</summary>
Existing Video Restoration (VR) methods always necessitate the individual deployment of models for each adverse weather to remove diverse adverse weather degradations, lacking the capability for adaptive processing of degradations. Such limitation amplifies the complexity and deployment costs in practical applications. To overcome this deficiency, in this paper, we propose a Cross-consistent Deep Unfolding Network (CDUN) for All-In-One VR, which enables the employment of a single model to remove diverse degradations for the first time. Specifically, the proposed CDUN accomplishes a novel iterative optimization framework, capable of restoring frames corrupted by corresponding degradations according to the degradation features given in advance. To empower the framework for eliminating diverse degradations, we devise a Sequence-wise Adaptive Degradation Estimator (SADE) to estimate degradation features for the input corrupted video. By orchestrating these two cascading procedures, CDUN achieves adaptive processing for diverse degradation. In addition, we introduce a window-based inter-frame fusion strategy to utilize information from more adjacent frames. This strategy involves the progressive stacking of temporal windows in multiple iterations, effectively enlarging the temporal receptive field and enabling each frame's restoration to leverage information from distant frames. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance in All-In-One VR.
</details>
<details>
<summary>摘要</summary>
现有的视频修复（VR）方法总是需要采取各种特定的模型来消除不同的降低因素，缺乏适应处理降低因素的能力。这种局限性会增加实际应用中的复杂性和投入成本。为了解决这一缺点，在这篇论文中，我们提出了一种名为 Cross-consistent Deep Unfolding Network（CDUN）的所有在一个VR方法，可以使用单个模型来消除多种降低因素。具体来说，我们提出了一种新的迭代优化框架，可以根据输入降低因素的特征来修复降低因素所影响的帧。为了使这种框架能够消除多种降低因素，我们设计了一种适应性降低因素估计器（SADE），可以为输入降低因素提供适应性的估计。通过这两种顺序执行的过程，CDUN实现了适应处理多种降低因素。此外，我们还提出了一种窗口基本的Inter-frame融合策略，可以利用更多的邻近帧中的信息。这种策略通过在多个迭代中逐渐堆叠窗口，实现了提高时间感知范围，使每帧的修复可以利用更远的帧中的信息。广泛的实验表明，我们提出的方法在All-In-One VR中实现了state-of-the-art的性能。
</details></li>
</ul>
<hr>
<h2 id="AGG-Net-Attention-Guided-Gated-convolutional-Network-for-Depth-Image-Completion"><a href="#AGG-Net-Attention-Guided-Gated-convolutional-Network-for-Depth-Image-Completion" class="headerlink" title="AGG-Net: Attention Guided Gated-convolutional Network for Depth Image Completion"></a>AGG-Net: Attention Guided Gated-convolutional Network for Depth Image Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01624">http://arxiv.org/abs/2309.01624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyue Chen, Tingxuan Huang, Zhimin Song, Shizhuo Deng, Tong Jia</li>
<li>for: 提高RGBD相机遥感几何图像质量</li>
<li>methods: 提出了一种基于Attention Guided Gated-convolutional Network（AGG-Net）的深度图像完成方法，通过实现颜色和深度特征的综合 fusion，提高了图像的准确性和可靠性</li>
<li>results: 对比于状态艺术方法，该方法在NYU-Depth V2、DIML和SUN RGB-D等标准底本上达到了更高的完成率<details>
<summary>Abstract</summary>
Recently, stereo vision based on lightweight RGBD cameras has been widely used in various fields. However, limited by the imaging principles, the commonly used RGB-D cameras based on TOF, structured light, or binocular vision acquire some invalid data inevitably, such as weak reflection, boundary shadows, and artifacts, which may bring adverse impacts to the follow-up work. In this paper, we propose a new model for depth image completion based on the Attention Guided Gated-convolutional Network (AGG-Net), through which more accurate and reliable depth images can be obtained from the raw depth maps and the corresponding RGB images. Our model employs a UNet-like architecture which consists of two parallel branches of depth and color features. In the encoding stage, an Attention Guided Gated-Convolution (AG-GConv) module is proposed to realize the fusion of depth and color features at different scales, which can effectively reduce the negative impacts of invalid depth data on the reconstruction. In the decoding stage, an Attention Guided Skip Connection (AG-SC) module is presented to avoid introducing too many depth-irrelevant features to the reconstruction. The experimental results demonstrate that our method outperforms the state-of-the-art methods on the popular benchmarks NYU-Depth V2, DIML, and SUN RGB-D.
</details>
<details>
<summary>摘要</summary>
近些年来，基于轻量级RGBD相机的斯tereo视觉已经广泛应用于多个领域。然而，由于捕捉原理的限制，常用的RGB-D相机基于TOF、结构光或双目视觉都会不可避免地获得一些无效数据，如弱反射、边缘阴影和artefacts，这些无效数据可能会对后续工作产生负面影响。在这篇论文中，我们提出了一种基于Attention Guided Gated-convolutional Network（AGG-Net）的深度图像完成模型，通过这种模型，从原始的深度图和对应的RGB图中获得更加准确和可靠的深度图像。我们的模型采用了UNet-like的架构，该架构包括两个平行的深度和颜色特征分支。在编码阶段，我们提出了一种Attention Guided Gated-Convolution（AG-GConv）模块，用于在不同的尺度上进行深度和颜色特征的 fusión，以降低无效深度数据对重建的负面影响。在解码阶段，我们提出了一种Attention Guided Skip Connection（AG-SC）模块，以避免在重建过程中引入过多不相关的深度特征。实验结果表明，我们的方法在流行的benchmark上（NYU-Depth V2、DIML和SUN RGB-D）具有出色的性能。
</details></li>
</ul>
<hr>
<h2 id="Hindering-Adversarial-Attacks-with-Multiple-Encrypted-Patch-Embeddings"><a href="#Hindering-Adversarial-Attacks-with-Multiple-Encrypted-Patch-Embeddings" class="headerlink" title="Hindering Adversarial Attacks with Multiple Encrypted Patch Embeddings"></a>Hindering Adversarial Attacks with Multiple Encrypted Patch Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01620">http://arxiv.org/abs/2309.01620</a></li>
<li>repo_url: None</li>
<li>paper_authors: AprilPyone MaungMaung, Isao Echizen, Hitoshi Kiya</li>
<li>for: 本研究提出了一种新的钥匙基防御方法，旨在提高防御效果和可靠性。</li>
<li>methods: 本研究基于之前的钥匙基防御方法，并做出了两个主要改进：（1）高效地训练，（2）可选的随机化。提议的防御使用一或多个秘密质量嵌入和一个预训练的卷积网络来实现。当使用多个秘密嵌入时，提议的防御允许在推理过程中进行随机化。</li>
<li>results: 对于ImageNet dataset上的一系列攻击，包括适应性攻击，提议的防御得到了高度的鲁棒性精度和相当的清洁精度。<details>
<summary>Abstract</summary>
In this paper, we propose a new key-based defense focusing on both efficiency and robustness. Although the previous key-based defense seems effective in defending against adversarial examples, carefully designed adaptive attacks can bypass the previous defense, and it is difficult to train the previous defense on large datasets like ImageNet. We build upon the previous defense with two major improvements: (1) efficient training and (2) optional randomization. The proposed defense utilizes one or more secret patch embeddings and classifier heads with a pre-trained isotropic network. When more than one secret embeddings are used, the proposed defense enables randomization on inference. Experiments were carried out on the ImageNet dataset, and the proposed defense was evaluated against an arsenal of state-of-the-art attacks, including adaptive ones. The results show that the proposed defense achieves a high robust accuracy and a comparable clean accuracy compared to the previous key-based defense.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的钥匙基础防御，旨在同时增加效率和鲁棒性。先前的钥匙基础防御可以有效防御对抗例子，但是特制的适应攻击可以绕过先前的防御，并且在大量的ImageNet数据集上训练是困难的。我们基于先前的防御，提出了两大改进：（1）高效的训练和（2）可选的随机化。我们提出的防御使用一个或多个秘密贴图嵌入和一个或多个预训练的卷积网络。当使用多个秘密贴图嵌入时，我们的防御允许在推理时进行随机化。我们在ImageNet数据集上进行了实验，并评估了一系列当今最佳攻击。结果表明，我们的防御可以 дости得高效率和相对较高的清洁率，与先前的钥匙基础防御相比。
</details></li>
</ul>
<hr>
<h2 id="On-the-Query-Strategies-for-Efficient-Online-Active-Distillation"><a href="#On-the-Query-Strategies-for-Efficient-Online-Active-Distillation" class="headerlink" title="On the Query Strategies for Efficient Online Active Distillation"></a>On the Query Strategies for Efficient Online Active Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01612">http://arxiv.org/abs/2309.01612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Boldo, Enrico Martini, Mirco De Marchi, Stefano Aldegheri, Nicola Bombieri</li>
<li>for: 本研究旨在提高人姿估算（HPE）模型的训练效率和实时适应性，通过Active Learning（AL）和在线热退化。</li>
<li>methods: 本研究使用两种方法进行评估：一种是传统的离线方法，另一种是通过知识退化进行在线评估。</li>
<li>results: 研究表明，通过选择合适的帧进行训练，可以减少模型的计算复杂度，同时保持模型的准确性。这种方法可以应用于实时人姿估算场景，并且可以帮助模型在新的上下文中进行有效的适应。<details>
<summary>Abstract</summary>
Deep Learning (DL) requires lots of time and data, resulting in high computational demands. Recently, researchers employ Active Learning (AL) and online distillation to enhance training efficiency and real-time model adaptation. This paper evaluates a set of query strategies to achieve the best training results. It focuses on Human Pose Estimation (HPE) applications, assessing the impact of selected frames during training using two approaches: a classical offline method and a online evaluation through a continual learning approach employing knowledge distillation, on a popular state-of-the-art HPE dataset. The paper demonstrates the possibility of enabling training at the edge lightweight models, adapting them effectively to new contexts in real-time.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）需要很多时间和数据，导致计算成本很高。最近，研究人员使用活动学习（AL）和在线热针蒸馈来提高训练效率和实时模型适应。这篇论文评估了一组查询策略，以实现最佳训练结果。它专注于人姿估计（HPE）应用，评估在训练过程中选择的帧的影响，使用两种方法：一种传统的离线方法和一种在线评估过程，通过知识传递来适应新上下文。论文展示了在边缘训练轻量级模型的可能性，并在实时中效地适应新上下文。
</details></li>
</ul>
<hr>
<h2 id="Segmentation-of-3D-pore-space-from-CT-images-using-curvilinear-skeleton-application-to-numerical-simulation-of-microbial-decomposition"><a href="#Segmentation-of-3D-pore-space-from-CT-images-using-curvilinear-skeleton-application-to-numerical-simulation-of-microbial-decomposition" class="headerlink" title="Segmentation of 3D pore space from CT images using curvilinear skeleton: application to numerical simulation of microbial decomposition"></a>Segmentation of 3D pore space from CT images using curvilinear skeleton: application to numerical simulation of microbial decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01611">http://arxiv.org/abs/2309.01611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Monga, Zakaria Belghali, Mouad Klai, Lucie Druoton, Dominique Michelucci, Valerie Pot</li>
<li>For: The paper aims to present a new method for describing the pore space of soil using the curvilinear skeleton, which can improve the accuracy and efficiency of numerical simulations of microbial decomposition and diffusion processes.* Methods: The authors use 3D X-ray CT scanner images to extract the pore space of soil and then use the curvilinear skeleton to segment the pore space into connected regions. They compare the results with other methods using different geometric representations of pore space, such as balls and voxels.* Results: The authors validate the simulation outputs using different pore space geometrical representations and show that the curvilinear skeleton-based method can provide more accurate and efficient simulations of microbial decomposition and diffusion processes in soil.<details>
<summary>Abstract</summary>
Recent advances in 3D X-ray Computed Tomographic (CT) sensors have stimulated research efforts to unveil the extremely complex micro-scale processes that control the activity of soil microorganisms. Voxel-based description (up to hundreds millions voxels) of the pore space can be extracted, from grey level 3D CT scanner images, by means of simple image processing tools. Classical methods for numerical simulation of biological dynamics using mesh of voxels, such as Lattice Boltzmann Model (LBM), are too much time consuming. Thus, the use of more compact and reliable geometrical representations of pore space can drastically decrease the computational cost of the simulations. Several recent works propose basic analytic volume primitives (e.g. spheres, generalized cylinders, ellipsoids) to define a piece-wise approximation of pore space for numerical simulation of draining, diffusion and microbial decomposition. Such approaches work well but the drawback is that it generates approximation errors. In the present work, we study another alternative where pore space is described by means of geometrically relevant connected subsets of voxels (regions) computed from the curvilinear skeleton. Indeed, many works use the curvilinear skeleton (3D medial axis) for analyzing and partitioning 3D shapes within various domains (medicine, material sciences, petroleum engineering, etc.) but only a few ones in soil sciences. Within the context of soil sciences, most studies dealing with 3D medial axis focus on the determination of pore throats. Here, we segment pore space using curvilinear skeleton in order to achieve numerical simulation of microbial decomposition (including diffusion processes). We validate simulation outputs by comparison with other methods using different pore space geometrical representations (balls, voxels).
</details>
<details>
<summary>摘要</summary>
In the present work, we study another alternative where pore space is described by means of geometrically relevant connected subsets of voxels (regions) computed from the curvilinear skeleton. The curvilinear skeleton is a 3D medial axis that has been widely used in various domains such as medicine, material sciences, and petroleum engineering to analyze and partition 3D shapes. However, only a few studies have applied this technique in soil sciences, and most of them have focused on determining pore throats. Here, we segment pore space using the curvilinear skeleton to achieve numerical simulation of microbial decomposition, including diffusion processes. We validate the simulation outputs by comparing them with other methods using different pore space geometrical representations (balls, voxels).
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Precision-and-Recall-Towards-Reliable-Evaluation-of-Generative-Models"><a href="#Probabilistic-Precision-and-Recall-Towards-Reliable-Evaluation-of-Generative-Models" class="headerlink" title="Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models"></a>Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01590">http://arxiv.org/abs/2309.01590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kdst-team/probablistic_precision_recall">https://github.com/kdst-team/probablistic_precision_recall</a></li>
<li>paper_authors: Dogyun Park, Suhyun Kim<br>for: This paper focuses on evaluating the fidelity and diversity of generative models, specifically addressing the limitations of existing k-Nearest Neighbor (kNN) based precision-recall metrics.methods: The authors propose novel metrics, P-precision and P-recall (PP&amp;PR), based on a probabilistic approach to address the oversimplified assumptions and undesirable properties of kNN.results: The authors show through extensive toy experiments and state-of-the-art generative models that their PP&amp;PR provide more reliable estimates for comparing fidelity and diversity than the existing metrics.<details>
<summary>Abstract</summary>
Assessing the fidelity and diversity of the generative model is a difficult but important issue for technological advancement. So, recent papers have introduced k-Nearest Neighbor ($k$NN) based precision-recall metrics to break down the statistical distance into fidelity and diversity. While they provide an intuitive method, we thoroughly analyze these metrics and identify oversimplified assumptions and undesirable properties of kNN that result in unreliable evaluation, such as susceptibility to outliers and insensitivity to distributional changes. Thus, we propose novel metrics, P-precision and P-recall (PP\&PR), based on a probabilistic approach that address the problems. Through extensive investigations on toy experiments and state-of-the-art generative models, we show that our PP\&PR provide more reliable estimates for comparing fidelity and diversity than the existing metrics. The codes are available at \url{https://github.com/kdst-team/Probablistic_precision_recall}.
</details>
<details>
<summary>摘要</summary>
【评估生成模型的准确性和多样性是技术发展中的一个重要问题。因此，最近的论文已经引入了k-最近邻居（$k$NN）基于精度-回归指标来细分统计距离。尽管它们提供了直观的方法，但我们在这些指标中进行了全面的分析，并发现了它们的假设过于简化，以及不良的性质，如受到异常值的影响和分布变化的敏感性不足。因此，我们提出了新的指标，即P-精度和P-回归（PP&PR），基于概率方法，以解决这些问题。我们在各种实验中进行了广泛的调查，并显示了我们的PP&PR在比较准确性和多样性方面提供了更可靠的估计。代码可以在<https://github.com/kdst-team/Probablistic_precision_recall>获取。】Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SATAY-A-Streaming-Architecture-Toolflow-for-Accelerating-YOLO-Models-on-FPGA-Devices"><a href="#SATAY-A-Streaming-Architecture-Toolflow-for-Accelerating-YOLO-Models-on-FPGA-Devices" class="headerlink" title="SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices"></a>SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01587">http://arxiv.org/abs/2309.01587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Montgomerie-Corcoran, Petros Toupas, Zhewen Yu, Christos-Savvas Bouganis</li>
<li>for: 这个论文是为了解决现代智能视觉和图像处理任务中的对象检测问题，以实现现实生活中的各种应用程序，如自动驾驶到医疗影像处理。</li>
<li>methods: 这个论文使用了流处理架构和自动化工具流来加速YOLO模型，以解决将现代对象检测模型部署到FPGA设备上的挑战。</li>
<li>results: 这个论文的研究结果表明，使用流处理架构和自动化工具流可以生成高性能的FPGA加速器，可以与GPU设备相比，并且超越当前状态的FPGA加速器。<details>
<summary>Abstract</summary>
AI has led to significant advancements in computer vision and image processing tasks, enabling a wide range of applications in real-life scenarios, from autonomous vehicles to medical imaging. Many of those applications require efficient object detection algorithms and complementary real-time, low latency hardware to perform inference of these algorithms. The YOLO family of models is considered the most efficient for object detection, having only a single model pass. Despite this, the complexity and size of YOLO models can be too computationally demanding for current edge-based platforms. To address this, we present SATAY: a Streaming Architecture Toolflow for Accelerating YOLO. This work tackles the challenges of deploying stateof-the-art object detection models onto FPGA devices for ultralow latency applications, enabling real-time, edge-based object detection. We employ a streaming architecture design for our YOLO accelerators, implementing the complete model on-chip in a deeply pipelined fashion. These accelerators are generated using an automated toolflow, and can target a range of suitable FPGA devices. We introduce novel hardware components to support the operations of YOLO models in a dataflow manner, and off-chip memory buffering to address the limited on-chip memory resources. Our toolflow is able to generate accelerator designs which demonstrate competitive performance and energy characteristics to GPU devices, and which outperform current state-of-the-art FPGA accelerators.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Visual-Quality-and-Transferability-of-Adversarial-Attacks-on-Face-Recognition-Simultaneously-with-Adversarial-Restoration"><a href="#Improving-Visual-Quality-and-Transferability-of-Adversarial-Attacks-on-Face-Recognition-Simultaneously-with-Adversarial-Restoration" class="headerlink" title="Improving Visual Quality and Transferability of Adversarial Attacks on Face Recognition Simultaneously with Adversarial Restoration"></a>Improving Visual Quality and Transferability of Adversarial Attacks on Face Recognition Simultaneously with Adversarial Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01582">http://arxiv.org/abs/2309.01582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengfan Zhou, Hefei Ling, Yuxuan Shi, Jiazhong Chen, Ping Li</li>
<li>for: 该论文旨在提高黑客脸部例子的视觉质量和传输性。</li>
<li>methods: 该论文提出了一种新的黑客攻击技术，即黑客恢复（AdvRestore），它利用了一种面Restoration Latent Diffusion Model（RLDM）来提高黑客脸部例子的视觉质量和传输性。</li>
<li>results: 该论文的实验结果表明，黑客恢复技术可以备受提高黑客脸部例子的传输性和视觉质量。<details>
<summary>Abstract</summary>
Adversarial face examples possess two critical properties: Visual Quality and Transferability. However, existing approaches rarely address these properties simultaneously, leading to subpar results. To address this issue, we propose a novel adversarial attack technique known as Adversarial Restoration (AdvRestore), which enhances both visual quality and transferability of adversarial face examples by leveraging a face restoration prior. In our approach, we initially train a Restoration Latent Diffusion Model (RLDM) designed for face restoration. Subsequently, we employ the inference process of RLDM to generate adversarial face examples. The adversarial perturbations are applied to the intermediate features of RLDM. Additionally, by treating RLDM face restoration as a sibling task, the transferability of the generated adversarial face examples is further improved. Our experimental results validate the effectiveness of the proposed attack method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>发现对抗面部示例具有两个关键性能：视觉质量和传输性。然而，现有的方法几乎从未同时考虑这两个性能，导致效果不佳。为解决这问题，我们提出了一种新的对抗攻击技术，即对抗恢复（AdvRestore）。我们首先在RLDM（Restoration Latent Diffusion Model）中训练一个面Restoration模型。然后，我们使用RLDM的推理过程来生成对抗面部示例。对抗扰动被应用于RLDM中的中间特征。此外，通过将RLDM的面Restoration视为姐妹任务，我们进一步改进了生成的对抗面部示例的传输性。我们的实验结果证明了我们的攻击方法的效果。Note: "RLDM" stands for "Restoration Latent Diffusion Model" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="DiffHPE-Robust-Coherent-3D-Human-Pose-Lifting-with-Diffusion"><a href="#DiffHPE-Robust-Coherent-3D-Human-Pose-Lifting-with-Diffusion" class="headerlink" title="DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion"></a>DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01575">http://arxiv.org/abs/2309.01575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez</li>
<li>for: 这篇论文是为了提出一种新的3D人姿估计方法（DiffHPE），通过灵活的扩散模型来提高人姿估计的准确性、可靠性和一致性。</li>
<li>methods: 这篇论文使用了扩散模型，并将其与现有的指导模型相结合，以提高人姿估计的精度和可靠性。</li>
<li>results: 论文通过使用扩散模型，提高了人姿估计的时间一致性和三角均衡性，并在干扰情况下表现更加稳定。在人类3.6M数据集上，这种方法表现出色，并在不同的干扰情况下保持稳定性。<details>
<summary>Abstract</summary>
We present an innovative approach to 3D Human Pose Estimation (3D-HPE) by integrating cutting-edge diffusion models, which have revolutionized diverse fields, but are relatively unexplored in 3D-HPE. We show that diffusion models enhance the accuracy, robustness, and coherence of human pose estimations. We introduce DiffHPE, a novel strategy for harnessing diffusion models in 3D-HPE, and demonstrate its ability to refine standard supervised 3D-HPE. We also show how diffusion models lead to more robust estimations in the face of occlusions, and improve the time-coherence and the sagittal symmetry of predictions. Using the Human\,3.6M dataset, we illustrate the effectiveness of our approach and its superiority over existing models, even under adverse situations where the occlusion patterns in training do not match those in inference. Our findings indicate that while standalone diffusion models provide commendable performance, their accuracy is even better in combination with supervised models, opening exciting new avenues for 3D-HPE research.
</details>
<details>
<summary>摘要</summary>
我们提出了一种创新的三维人姿估计（3D-HPE）方法，通过结合前沿扩散模型，这些模型在多个领域中引领了革命，但在3D-HPE中尚未得到广泛研究。我们证明了扩散模型可以提高人姿估计的准确性、可靠性和相对性。我们提出了一种新的推 diffusionHPE 方法，并证明它可以在标准的三维人姿估计上进行精细调整。我们还表明了扩散模型可以在 occlusion 情况下提供更加稳定的估计，并改善时间相关性和顺序协调性。使用 Human\,3.6M 数据集，我们证明了我们的方法的效iveness，并与现有模型相比，即使在训练和推测中 occlusion  patrerns 不同时，也能够 достичь更高的性能。我们的发现表明，单独使用扩散模型可以提供很好的性能，但是将它们与直接学习模型相结合，可以带来更高的准确性，开启了3D-HPE研究的新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Raw-Data-Is-All-You-Need-Virtual-Axle-Detector-with-Enhanced-Receptive-Field"><a href="#Raw-Data-Is-All-You-Need-Virtual-Axle-Detector-with-Enhanced-Receptive-Field" class="headerlink" title="Raw Data Is All You Need: Virtual Axle Detector with Enhanced Receptive Field"></a>Raw Data Is All You Need: Virtual Axle Detector with Enhanced Receptive Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01574">http://arxiv.org/abs/2309.01574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henik Riedel, Robert Steven Lorenzen, Clemens Hübler</li>
<li>for: 本研究旨在开发一种新的车辆轴承检测方法，以实现实时应用桥式Weight-In-Motion（BWIM）系统，不需要专门的车辆检测器。</li>
<li>methods: 该方法基于虚拟车辆检测器（VAD）模型，利用原始加速度数据进行处理，从而提高了感知范围。</li>
<li>results: 比较 experiments 表明，与现有VAD方法相比，提出的虚拟车辆检测器with Enhanced Receptive field（VADER）可以提高(F_1) score 73%，空间精度 39%，同时降低计算和内存成本99%。VADER在使用代表性训练集和功能传感器时，(F_1) score 达99.4%，空间错误为4.13cm。此外，我们还提出了一种基于对象大小驱动的 CNN 架构设计规则，结果表明，使用原始数据可以达到更好的性能，从而成为考虑原始数据作为输入的优势。<details>
<summary>Abstract</summary>
Rising maintenance costs of ageing infrastructure necessitate innovative monitoring techniques. This paper presents a new approach for axle detection, enabling real-time application of Bridge Weigh-In-Motion (BWIM) systems without dedicated axle detectors. The proposed method adapts the Virtual Axle Detector (VAD) model to handle raw acceleration data, which allows the receptive field to be increased. The proposed Virtual Axle Detector with Enhanced Receptive field (VADER) improves the \(F_1\) score by 73\% and spatial accuracy by 39\%, while cutting computational and memory costs by 99\% compared to the state-of-the-art VAD. VADER reaches a \(F_1\) score of 99.4\% and a spatial error of 4.13~cm when using a representative training set and functional sensors. We also introduce a novel receptive field (RF) rule for an object-size driven design of Convolutional Neural Network (CNN) architectures. Based on this rule, our results suggest that models using raw data could achieve better performance than those using spectrograms, offering a compelling reason to consider raw data as input.
</details>
<details>
<summary>摘要</summary>
提高老化基础设施维护成本的必要性，这篇论文提出了一种新的车轮检测方法，允许实时应用桥梁运动测量系统（BWIM）无需专门的车轮检测器。该方法基于虚拟车轮检测器（VAD）模型，可以处理原始加速度数据，从而提高感知范围。提出的虚拟车轮检测器增强型（VADER）提高了\(F_1\)分数 by 73%和空间准确率 by 39%，同时降低计算和存储成本 by 99%比领先的VAD。VADER在使用代表性训练集和功能传感器时达到了\(F_1\)分数99.4%和空间错误4.13cm。我们还提出了一种新的接收场规则（RF），用于设计基于卷积神经网络（CNN）架构。根据这个规则，我们的结果表明使用原始数据可以实现更好的性能，这为使用原始数据作为输入提供了一个吸引人的理由。
</details></li>
</ul>
<hr>
<h2 id="Locality-Aware-Hyperspectral-Classification"><a href="#Locality-Aware-Hyperspectral-Classification" class="headerlink" title="Locality-Aware Hyperspectral Classification"></a>Locality-Aware Hyperspectral Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01561">http://arxiv.org/abs/2309.01561</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhoufangqin/hylite">https://github.com/zhoufangqin/hylite</a></li>
<li>paper_authors: Fangqin Zhou, Mert Kilickaya, Joaquin Vanschoren</li>
<li>for: 本研究旨在提高干扰影像分类精度，利用视Transformers自动化干扰影像分类。</li>
<li>methods: 本研究提出了三大贡献：一、引入干扰本地信息图像变换器（HyLITE），二、一种新的规范函数，以及三、提出的方法在竞争对手中具有明显的性能优势，升高了准确率。</li>
<li>results: 本研究的实验结果表明，提出的方法可以在干扰影像分类任务中升高准确率，与竞争对手相比，具有明显的性能优势，最高准确率提升10%。<details>
<summary>Abstract</summary>
Hyperspectral image classification is gaining popularity for high-precision vision tasks in remote sensing, thanks to their ability to capture visual information available in a wide continuum of spectra. Researchers have been working on automating Hyperspectral image classification, with recent efforts leveraging Vision-Transformers. However, most research models only spectra information and lacks attention to the locality (i.e., neighboring pixels), which may be not sufficiently discriminative, resulting in performance limitations. To address this, we present three contributions: i) We introduce the Hyperspectral Locality-aware Image TransformEr (HyLITE), a vision transformer that models both local and spectral information, ii) A novel regularization function that promotes the integration of local-to-global information, and iii) Our proposed approach outperforms competing baselines by a significant margin, achieving up to 10% gains in accuracy. The trained models and the code are available at HyLITE.
</details>
<details>
<summary>摘要</summary>
干扰图像分类在远程感知中得到推广，感谢它们可以捕捉视觉信息的广泛谱 spectrum。研究人员在自动化干扰图像分类方面努力，其中最新的努力是利用视力变换器。然而，大多数研究模型只考虑spectra信息，缺乏对邻近像素（即地方信息）的注意力，这可能导致表现有限制。为此，我们提出了三项贡献：1. 我们介绍了干扰图像特征地址Transformer（HyLITE），一种视力变换器，可以同时模型本地和spectra信息。2. 一种新的规范函数，可以促进本地信息与全局信息的集成。3. 我们的提议方法在比较基eline上表现出特别的准确性提升，达到10%的提升率。我们的训练模型和代码可以在HyLITE上下载。
</details></li>
</ul>
<hr>
<h2 id="TSTTC-A-Large-Scale-Dataset-for-Time-to-Contact-Estimation-in-Driving-Scenarios"><a href="#TSTTC-A-Large-Scale-Dataset-for-Time-to-Contact-Estimation-in-Driving-Scenarios" class="headerlink" title="TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios"></a>TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01539">http://arxiv.org/abs/2309.01539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tusen-ai/TSTTC">https://github.com/tusen-ai/TSTTC</a></li>
<li>paper_authors: Yuheng Shi, Zehao Huang, Yan Yan, Naiyan Wang, Xiaojie Guo</li>
<li>for: 这篇论文主要旨在提供一个大规模的行为对象驱动距离时间联系（TTC）数据集，以便促进TTC估计方法的研究和发展。</li>
<li>methods: 这篇论文使用了大量的驾驶数据，并采用了最新的神经网络生成技术来增加小TTC情况的数据量。</li>
<li>results: 该论文提供了一个大规模的TTC数据集，并提供了一些简单 yet 有效的TTC估计基线。这些基线在提posed dataset上进行了广泛的评估，以证明其效果。<details>
<summary>Abstract</summary>
Time-to-Contact (TTC) estimation is a critical task for assessing collision risk and is widely used in various driver assistance and autonomous driving systems. The past few decades have witnessed development of related theories and algorithms. The prevalent learning-based methods call for a large-scale TTC dataset in real-world scenarios. In this work, we present a large-scale object oriented TTC dataset in the driving scene for promoting the TTC estimation by a monocular camera. To collect valuable samples and make data with different TTC values relatively balanced, we go through thousands of hours of driving data and select over 200K sequences with a preset data distribution. To augment the quantity of small TTC cases, we also generate clips using the latest Neural rendering methods. Additionally, we provide several simple yet effective TTC estimation baselines and evaluate them extensively on the proposed dataset to demonstrate their effectiveness. The proposed dataset is publicly available at https://open-dataset.tusen.ai/TSTTC.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>时间到contact（TTC）估计是评估冲突风险的关键任务，广泛应用于不同的驾驶助手和自动驾驶系统中。过去几十年内，相关理论和算法的发展都有所成就。现有的学习型方法需要大量的TTC实际场景数据。在这项工作中，我们提供了一个大规模的 объек oriented TTC数据集，用于推广TTC估计。为了收集有价值的样本并使数据具有不同TTC值相对均衡，我们通过了数千小时的驾驶数据，选择了超过200K个序列，并采用了一个预设的数据分布。为了增加小TTC情况的数量，我们还使用了最新的神经网络渲染方法生成clip。此外，我们还提供了一些简单 yet有效的TTC估计基线，并在提posed数据集上进行了广泛的评估，以示其效果。提出的数据集可以在https://open-dataset.tusen.ai/TSTTC上公开获取。
</details></li>
</ul>
<hr>
<h2 id="On-the-use-of-Mahalanobis-distance-for-out-of-distribution-detection-with-neural-networks-for-medical-imaging"><a href="#On-the-use-of-Mahalanobis-distance-for-out-of-distribution-detection-with-neural-networks-for-medical-imaging" class="headerlink" title="On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging"></a>On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01488">http://arxiv.org/abs/2309.01488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harryanthony/mahalanobis-ood-detection">https://github.com/harryanthony/mahalanobis-ood-detection</a></li>
<li>paper_authors: Harry Anthony, Konstantinos Kamnitsas</li>
<li>for: 本研究旨在探讨在医疗应用中使用神经网络时，如何探测输入数据与训练数据之间的差异，以避免不可靠的预测。</li>
<li>methods: 本研究使用了距离基于方法，如 Mahalanobis 距离，来探测输入数据与训练数据之间的差异。</li>
<li>results: 本研究发现，使用 Mahalanobis 距离探测输入数据与训练数据之间的差异，并不是一个一致的解决方案。相反， Results 表明，选择合适的层或层组，可以提高探测不同类型的异常情况的灵活性。此外，研究还发现，将 OOD 探测器分解成不同深度的网络层可以增强网络的稳定性。这些发现都被 validate 在实际 OOD 任务上，使用 CheXpert 胸部X射影像，并使用不同的 pacemaker 和性别作为 OOD 例子。<details>
<summary>Abstract</summary>
Implementing neural networks for clinical use in medical applications necessitates the ability for the network to detect when input data differs significantly from the training data, with the aim of preventing unreliable predictions. The community has developed several methods for out-of-distribution (OOD) detection, within which distance-based approaches - such as Mahalanobis distance - have shown potential. This paper challenges the prevailing community understanding that there is an optimal layer, or combination of layers, of a neural network for applying Mahalanobis distance for detection of any OOD pattern. Using synthetic artefacts to emulate OOD patterns, this paper shows the optimum layer to apply Mahalanobis distance changes with the type of OOD pattern, showing there is no one-fits-all solution. This paper also shows that separating this OOD detector into multiple detectors at different depths of the network can enhance the robustness for detecting different OOD patterns. These insights were validated on real-world OOD tasks, training models on CheXpert chest X-rays with no support devices, then using scans with unseen pacemakers (we manually labelled 50% of CheXpert for this research) and unseen sex as OOD cases. The results inform best-practices for the use of Mahalanobis distance for OOD detection. The manually annotated pacemaker labels and the project's code are available at: https://github.com/HarryAnthony/Mahalanobis-OOD-detection.
</details>
<details>
<summary>摘要</summary>
使用神经网络进行医疗应用时，需要神经网络能够检测输入数据与训练数据之间的差异，以避免不可靠的预测。社区已经开发出了许多对外部数据（OOD）检测方法，其中距离基于方法，如马哈拉诺比斯距离，表现出了潜在。这篇论文挑战了社区认知，即在任何OOD模式下都有一个最佳层或组合层可以应用马哈拉诺比斯距离来检测OOD模式。使用 sintetic artifacts 模拟 OOD 模式，这篇论文表明了在不同类型的 OOD 模式时，适用马哈拉诺比斯距离的层不同，而且没有一个通用的解决方案。此外，这篇论文还表明，将 OOD 检测器分解成不同深度的网络层可以提高对不同 OOD 模式的检测稳定性。这些发现得到了实际 OOD 任务的验证，使用 CheXpert 胸部X射影片进行训练，然后使用未知的心 pacemaker 和性别作为 OOD 例外。结果提供了使用马哈拉诺比斯距离进行 OOD 检测的最佳实践。手动标注 pacemaker 标签和项目代码可以在 GitHub 上获取：https://github.com/HarryAnthony/Mahalanobis-OOD-detection。
</details></li>
</ul>
<hr>
<h2 id="GenSelfDiff-HIS-Generative-Self-Supervision-Using-Diffusion-for-Histopathological-Image-Segmentation"><a href="#GenSelfDiff-HIS-Generative-Self-Supervision-Using-Diffusion-for-Histopathological-Image-Segmentation" class="headerlink" title="GenSelfDiff-HIS: Generative Self-Supervision Using Diffusion for Histopathological Image Segmentation"></a>GenSelfDiff-HIS: Generative Self-Supervision Using Diffusion for Histopathological Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01487">http://arxiv.org/abs/2309.01487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishnuvardhan Purma, Suhas Srinath, Seshan Srirangarajan, Aanchal Kakkar, Prathosh A. P<br>for:这个研究目的是提出一种基于自类学习的几何像分割方法，来减轻几何像分析的传统人工分析压力。methods:这个方法基于对无标注数据的生成扩散模型，并使用多元损失函数进行精致化。results:研究结果显示，这个方法可以在两个公开available的数据集上取得良好的效果，并且在一个新提出的头颈癌（HN）数据集上也取得了良好的效果。<details>
<summary>Abstract</summary>
Histopathological image segmentation is a laborious and time-intensive task, often requiring analysis from experienced pathologists for accurate examinations. To reduce this burden, supervised machine-learning approaches have been adopted using large-scale annotated datasets for histopathological image analysis. However, in several scenarios, the availability of large-scale annotated data is a bottleneck while training such models. Self-supervised learning (SSL) is an alternative paradigm that provides some respite by constructing models utilizing only the unannotated data which is often abundant. The basic idea of SSL is to train a network to perform one or many pseudo or pretext tasks on unannotated data and use it subsequently as the basis for a variety of downstream tasks. It is seen that the success of SSL depends critically on the considered pretext task. While there have been many efforts in designing pretext tasks for classification problems, there haven't been many attempts on SSL for histopathological segmentation. Motivated by this, we propose an SSL approach for segmenting histopathological images via generative diffusion models in this paper. Our method is based on the observation that diffusion models effectively solve an image-to-image translation task akin to a segmentation task. Hence, we propose generative diffusion as the pretext task for histopathological image segmentation. We also propose a multi-loss function-based fine-tuning for the downstream task. We validate our method using several metrics on two publically available datasets along with a newly proposed head and neck (HN) cancer dataset containing hematoxylin and eosin (H\&E) stained images along with annotations. Codes will be made public at https://github.com/PurmaVishnuVardhanReddy/GenSelfDiff-HIS.git.
</details>
<details>
<summary>摘要</summary>
历史 PATHOLOGICAL 图像分割是一项劳动密集和时间consuming的任务，经验训练的病理学家通常需要进行准确的检查。为了减轻这个负担，有些人使用了Supervised 机器学习方法，使用大规模的标注数据进行历史 PATHOLOGICAL 图像分析。然而，在一些情况下，获得大规模的标注数据是一个瓶颈，而自我supervised 学习（SSL）是一种代替方案，它可以使用只有未标注的数据进行训练。基本上，SSL 的想法是训练一个网络，使其在未标注数据上完成一些pseudo或预text任务，然后用这些任务作为基础进行多种下游任务。它的成功取决于考虑的预text任务。虽然有很多人在设计 Classification 的预text任务方面做出了努力，但是对历史 PATHOLOGICAL 图像分割的 SSL 方法还未有很多尝试。在这篇论文中，我们提出了一种基于生成扩散模型的 SSL 方法，用于分割历史 PATHOLOGICAL 图像。我们基于图像到图像的翻译任务的观察，因此我们提出了生成扩散作为预text任务。此外，我们还提出了基于多个损失函数的细化。我们使用了多种度量来验证我们的方法，并在两个公共可用的数据集上进行验证，以及一个新提出的头颈癌（HN）癌症数据集，该数据集包含HE染料的历史 PATHOLOGICAL 图像以及注解。代码将在 <https://github.com/PurmaVishnuVardhanReddy/GenSelfDiff-HIS.git> 上公开。
</details></li>
</ul>
<hr>
<h2 id="CA2-Class-Agnostic-Adaptive-Feature-Adaptation-for-One-class-Classification"><a href="#CA2-Class-Agnostic-Adaptive-Feature-Adaptation-for-One-class-Classification" class="headerlink" title="CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification"></a>CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01483">http://arxiv.org/abs/2309.01483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilong Zhang, Zhibin Zhao, Deyu Meng, Xingwu Zhang, Xuefeng Chen</li>
<li>for: 提高机器学习模型在真实世界中的部署，实现一类分类（OCC）。</li>
<li>methods: 使用适应特定目标数据集的预训练特征，并将其扩展到未知类数。</li>
<li>results: 在不同类数（1-1024）的训练数据上，一直高于当前状态艺术方法，提高OCC性能。<details>
<summary>Abstract</summary>
One-class classification (OCC), i.e., identifying whether an example belongs to the same distribution as the training data, is essential for deploying machine learning models in the real world. Adapting the pre-trained features on the target dataset has proven to be a promising paradigm for improving OCC performance. Existing methods are constrained by assumptions about the number of classes. This contradicts the real scenario where the number of classes is unknown. In this work, we propose a simple class-agnostic adaptive feature adaptation method (CA2). We generalize the center-based method to unknown classes and optimize this objective based on the prior existing in the pre-trained network, i.e., pre-trained features that belong to the same class are adjacent. CA2 is validated to consistently improve OCC performance across a spectrum of training data classes, spanning from 1 to 1024, outperforming current state-of-the-art methods. Code is available at https://github.com/zhangzilongc/CA2.
</details>
<details>
<summary>摘要</summary>
一类分类（OCC），即确定输入例子是否属于训练数据的同一分布，在实际应用中是非常重要的。适应预训练特征onto目标数据集已经证明是提高OCC性能的有效方法。现有方法受限于类别数量的假设，这与实际场景不符。在这种情况下，我们提出了一种简单的类型不可知的适应特征调整方法（CA2）。我们将中心基于方法扩展到未知类别，并基于预训练网络中的先前存在的对称性来优化这个目标函数。CA2被证明可以在训练数据类型范围从1到1024之间，在不同类别的情况下一致提高OCC性能，超越当前状态的最佳方法。代码可以在https://github.com/zhangzilongc/CA2上下载。
</details></li>
</ul>
<hr>
<h2 id="Parameter-and-Computation-Efficient-Transfer-Learning-for-Vision-Language-Pre-trained-Models"><a href="#Parameter-and-Computation-Efficient-Transfer-Learning-for-Vision-Language-Pre-trained-Models" class="headerlink" title="Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models"></a>Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01479">http://arxiv.org/abs/2309.01479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiong Wu, Wei Yu, Yiyi Zhou, Shubin Huang, Xiaoshuai Sun, Rongrong Ji</li>
<li>for: 这篇研究目的是提出一种 Parameter and Computation Efficient Transfer Learning (PCETL) 方法，以提高 Vision-Language Pre-trained (VLP) 模型在下游任务中的适应性。</li>
<li>methods: 本研究提出了一种叫做 Dynamic Architecture Skipping (DAS) 方法，它通过观察 VLP 模型的模块之间的相互关联，并使用强化学习 (RL) 来决定哪些模块是可以被跳过的。这样可以将 VLP 模型的trainable参数数量降低，同时保持其在下游任务中的性能。</li>
<li>results: 实验结果显示 DAS 方法可以将 VLP 模型的 Computational Complexity 降低到 -11.97% FLOPs，并且与现有的 PETL 方法相比，DAS 方法在参数给数和性能之间能够取得平衡。<details>
<summary>Abstract</summary>
With ever increasing parameters and computation, vision-language pre-trained (VLP) models exhibit prohibitive expenditure in downstream task adaption. Recent endeavors mainly focus on parameter efficient transfer learning (PETL) for VLP models by only updating a small number of parameters. However, excessive computational overhead still plagues the application of VLPs. In this paper, we aim at parameter and computation efficient transfer learning (PCETL) for VLP models. In particular, PCETL not only needs to limit the number of trainable parameters in VLP models, but also to reduce the computational redundancy during inference, thus enabling a more efficient transfer. To approach this target, we propose a novel dynamic architecture skipping (DAS) approach towards effective PCETL. Instead of directly optimizing the intrinsic architectures of VLP models, DAS first observes the significances of their modules to downstream tasks via a reinforcement learning (RL) based process, and then skips the redundant ones with lightweight networks, i.e., adapters, according to the obtained rewards. In this case, the VLP model can well maintain the scale of trainable parameters while speeding up its inference on downstream tasks. To validate DAS, we apply it to two representative VLP models, namely ViLT and METER, and conduct extensive experiments on a bunch of VL tasks. The experimental results not only show the great advantages of DAS in reducing computational complexity, e.g. -11.97% FLOPs of METER on VQA2.0, but also confirm its competitiveness against existing PETL methods in terms of parameter scale and performance. Our source code is given in our appendix.
</details>
<details>
<summary>摘要</summary>
随着参数和计算的增加，视觉语言预训练（VLP）模型在下游任务适应中存在拥堵性问题。现有的努力主要集中在视觉语言预训练（PETL）模型中，通过只更新一小部分参数进行 parameter efficient transfer learning。然而，计算开销仍然困扰着VLP的应用。在这篇论文中，我们目标是在VLP模型中实现参数和计算效率的传输学习（PCETL）。具体来说，PCETL不仅需要限制VLP模型的可训练参数数量，还需要在推理过程中减少计算重复性，以便更有效地进行传输。为达到这个目标，我们提出了一种新的动态architecture skipping（DAS）方法。在DAS方法中，我们首先通过 reinforcement learning（RL）基于的过程来评估VLP模型中各个模块的下游任务意义，然后根据获得的奖励，使用轻量级网络（adapter）将红undeniable模块替换掉。这样，VLP模型可以保持参数数量的扩展性，同时快速完成下游任务的推理。为验证DAS方法，我们在两个代表性的VLP模型，namely ViLT和METER上进行了广泛的实验。实验结果不仅表明DAS方法可以减少计算复杂度，例如METER模型在VQA2.0任务上的计算复杂度减少了11.97%，而且也证明了它在参数数量和性能方面与现有的PETL方法相当竞争。我们的源代码在附录中提供。
</details></li>
</ul>
<hr>
<h2 id="Defect-Detection-in-Synthetic-Fibre-Ropes-using-Detectron2-Framework"><a href="#Defect-Detection-in-Synthetic-Fibre-Ropes-using-Detectron2-Framework" class="headerlink" title="Defect Detection in Synthetic Fibre Ropes using Detectron2 Framework"></a>Defect Detection in Synthetic Fibre Ropes using Detectron2 Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01469">http://arxiv.org/abs/2309.01469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anju Rani, Daniel O. Arroyo, Petar Durdevic</li>
<li>for:  This paper aims to develop an automated and efficient method for detecting defects in synthetic fibre ropes (SFRs) using deep learning (DL) models, specifically the Detectron2 library with Mask R-CNN architecture.</li>
<li>methods: The study uses an experimentally obtained dataset of high-dimensional images of SFRs, with seven damage classes, to train and test Mask R-CNN with various backbone configurations.</li>
<li>results: The use of Detectron2 and Mask R-CNN with different backbone configurations can effectively detect defects in SFRs, enhancing the inspection process and ensuring the safety of the fibre ropes.<details>
<summary>Abstract</summary>
Fibre ropes with the latest technology have emerged as an appealing alternative to steel ropes for offshore industries due to their lightweight and high tensile strength. At the same time, frequent inspection of these ropes is essential to ensure the proper functioning and safety of the entire system. The development of deep learning (DL) models in condition monitoring (CM) applications offers a simpler and more effective approach for defect detection in synthetic fibre ropes (SFRs). The present paper investigates the performance of Detectron2, a state-of-the-art library for defect detection and instance segmentation. Detectron2 with Mask R-CNN architecture is used for segmenting defects in SFRs. Mask R-CNN with various backbone configurations has been trained and tested on an experimentally obtained dataset comprising 1,803 high-dimensional images containing seven damage classes (loop high, loop medium, loop low, compression, core out, abrasion, and normal respectively) for SFRs. By leveraging the capabilities of Detectron2, this study aims to develop an automated and efficient method for detecting defects in SFRs, enhancing the inspection process, and ensuring the safety of the fibre ropes.
</details>
<details>
<summary>摘要</summary>
合成纤维绳（Synthetic Fiber Ropes，SFR）的 Condition Monitoring（CM）应用中，latest technology的纤维绳已经出现为海上工业的吸引力，因为它们具有轻量和高强度特点。同时，为保证整个系统的正常工作和安全，这些纤维绳的常规检查是必须的。在这种情况下，深度学习（Deep Learning，DL）模型在CM应用中的开发提供了一种更加简单和有效的方法来检测SFR中的缺陷。本文 investigate了Detectron2库在SFR中的缺陷检测和实例分割方面的表现。通过使用Mask R-CNN架构，Detectron2在SFR中 segmenting 缺陷。Mask R-CNN采用了不同的背景配置，在实验获得的1,803个高维度图像中进行了7种损害类（循环高、循环中、循环低、压缩、核心缺陷、 Abrasion 和正常）的训练和测试。通过利用Detectron2的能力，本研究旨在开发一种自动化和高效的SFR缺陷检测方法，提高检查过程，并确保纤维绳的安全。
</details></li>
</ul>
<hr>
<h2 id="Toward-Defensive-Letter-Design"><a href="#Toward-Defensive-Letter-Design" class="headerlink" title="Toward Defensive Letter Design"></a>Toward Defensive Letter Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01452">http://arxiv.org/abs/2309.01452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Rentaro Kataoka, Akisato Kimura, Seiichi Uchida</li>
<li>for: 防御 adversarial 攻击</li>
<li>methods: 使用 Iterative Fast Gradient Sign Method (I-FGSM) 和深度回归模型测试字符图像的防御能力，并基于生成 adversarial 网络 (GAN) 提出一种两步方法生成更加防御性的字符图像。</li>
<li>results: 通过测试和实验，提出了一种基于字符图像的防御机制，可以帮助防御 adversarial 攻击。<details>
<summary>Abstract</summary>
A major approach for defending against adversarial attacks aims at controlling only image classifiers to be more resilient, and it does not care about visual objects, such as pandas and cars, in images. This means that visual objects themselves cannot take any defensive actions, and they are still vulnerable to adversarial attacks. In contrast, letters are artificial symbols, and we can freely control their appearance unless losing their readability. In other words, we can make the letters more defensive to the attacks. This paper poses three research questions related to the adversarial vulnerability of letter images: (1) How defensive are the letters against adversarial attacks? (2) Can we estimate how defensive a given letter image is before attacks? (3) Can we control the letter images to be more defensive against adversarial attacks? For answering the first and second questions, we measure the defensibility of letters by employing Iterative Fast Gradient Sign Method (I-FGSM) and then build a deep regression model for estimating the defensibility of each letter image. We also propose a two-step method based on a generative adversarial network (GAN) for generating character images with higher defensibility, which solves the third research question.
</details>
<details>
<summary>摘要</summary>
一种主要方法对抗反对攻击是控制图像分类器更加抗击强，不管图像中的物体，如熊猫和车辆，是否受到攻击。这意味着视觉物体本身无法采取任何防御行动，仍然易受到反对攻击。相比之下，字符是人工符号，我们可以自由地控制它们的显示，只要不导致不可读性。即使在攻击时，我们可以使字符更加抗击强。这篇论文提出了三个研究问题 relacionadas 反对攻击的抗击性：1. 字符对反对攻击的抗击性如何？2. 我们可以在攻击之前对给定的字符图像进行估算，该图像在攻击中的抗击性如何？3. 我们可以通过生成推荐网络（GAN）来生成具有更高抗击性的字符图像，以解决第三个研究问题。为了回答第一个和第二个问题，我们使用迭代快速梯度签名方法（I-FGSM）测量字符的抗击性，并建立深度回归模型来估算每个字符图像的抗击性。我们还提出了一种基于GAN的两步方法，用于生成具有更高抗击性的字符图像，解决第三个研究问题。
</details></li>
</ul>
<hr>
<h2 id="Large-Separable-Kernel-Attention-Rethinking-the-Large-Kernel-Attention-Design-in-CNN"><a href="#Large-Separable-Kernel-Attention-Rethinking-the-Large-Kernel-Attention-Design-in-CNN" class="headerlink" title="Large Separable Kernel Attention: Rethinking the Large Kernel Attention Design in CNN"></a>Large Separable Kernel Attention: Rethinking the Large Kernel Attention Design in CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01439">http://arxiv.org/abs/2309.01439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kin Wai Lau, Lai-Man Po, Yasar Abbas Ur Rehman</li>
<li>for: 提高vision-based任务中VAN的性能，并降低计算和存储的占用率。</li>
<li>methods: 提出Large Separable Kernel Attention（LSKA）模块，将深度wise convolutional layer中的2D卷积核 decomposition为水平和垂直的1D卷积核，从而避免了额外块的使用。</li>
<li>results: 对VAN、ViTs和ConvNeXt进行了修饰，并在Object recognition、Object detection、Semantic segmentation和Robustness测试中提供了相对较好的性能。在不同的kernel size下，LSKA模块可以减少计算和存储的占用率，并且在Object recognition、Object detection、Semantic segmentation和Robustness测试中具有较好的性能。<details>
<summary>Abstract</summary>
Visual Attention Networks (VAN) with Large Kernel Attention (LKA) modules have been shown to provide remarkable performance, that surpasses Vision Transformers (ViTs), on a range of vision-based tasks. However, the depth-wise convolutional layer in these LKA modules incurs a quadratic increase in the computational and memory footprints with increasing convolutional kernel size. To mitigate these problems and to enable the use of extremely large convolutional kernels in the attention modules of VAN, we propose a family of Large Separable Kernel Attention modules, termed LSKA. LSKA decomposes the 2D convolutional kernel of the depth-wise convolutional layer into cascaded horizontal and vertical 1-D kernels. In contrast to the standard LKA design, the proposed decomposition enables the direct use of the depth-wise convolutional layer with large kernels in the attention module, without requiring any extra blocks. We demonstrate that the proposed LSKA module in VAN can achieve comparable performance with the standard LKA module and incur lower computational complexity and memory footprints. We also find that the proposed LSKA design biases the VAN more toward the shape of the object than the texture with increasing kernel size. Additionally, we benchmark the robustness of the LKA and LSKA in VAN, ViTs, and the recent ConvNeXt on the five corrupted versions of the ImageNet dataset that are largely unexplored in the previous works. Our extensive experimental results show that the proposed LSKA module in VAN provides a significant reduction in computational complexity and memory footprints with increasing kernel size while outperforming ViTs, ConvNeXt, and providing similar performance compared to the LKA module in VAN on object recognition, object detection, semantic segmentation, and robustness tests.
</details>
<details>
<summary>摘要</summary>
视觉注意网络（VAN）配置了大kernel注意模块（LKA）可以提供出色的表现，超过视transformer（ViT），在视觉任务中。然而，深度 wise convolutional层在这些LKA模块中会导致计算和存储空间呈 quadratic 增长，随着核心大小的增加。为了解决这些问题并使用极大的核心大小在VAN中的注意模块中，我们提出了一个家族Large Separable Kernel Attention（LSKA）模块。LSKA将二维核心层的深度wise convolutional层 decomposed into cascaded horizontal和vertical 1-D核心。与标准LKA设计不同，我们的分解方式可以 direct 使用深度wise convolutional层中的大核心在注意模块中，无需额外块。我们的实验结果表明，在VAN中使用我们提出的LSKA模块可以与标准LKA模块相当，同时具有较低的计算复杂度和存储空间占用。此外，我们发现LSKA设计偏向对象的形状，而不是Texture，随着核心大小的增加。此外，我们对VAN、ViTs和最近的ConvNeXt在ImageNet数据集上进行了大规模的robustness测试，发现LSKA模块在对象认知、物体检测、semantic segmentation和Robustness测试中具有优秀的表现。
</details></li>
</ul>
<hr>
<h2 id="DAT-Spatially-Dynamic-Vision-Transformer-with-Deformable-Attention"><a href="#DAT-Spatially-Dynamic-Vision-Transformer-with-Deformable-Attention" class="headerlink" title="DAT++: Spatially Dynamic Vision Transformer with Deformable Attention"></a>DAT++: Spatially Dynamic Vision Transformer with Deformable Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01430">http://arxiv.org/abs/2309.01430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leaplabthu/dat">https://github.com/leaplabthu/dat</a></li>
<li>paper_authors: Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, Gao Huang</li>
<li>for: 这篇论文的目的是提出一种可靠且高效的视觉处理模型，能够解决传统的视觉模型在实现全球注意力和对特定区域的适应能力之间的矛盾。</li>
<li>methods: 这篇论文提出了一种名为“弹性多头注意模组”的新型注意力模块，具有自动分配鉴定点的功能，以实现对应区域的适应注意。这个模组可以与传统的 dense attention 结合，以提高视觉模型的表现力。</li>
<li>results: 根据实验结果，这篇论文的提案DAT++ 能够在多个视觉识别任务上取得顶尖的成绩，包括 ImageNet 的准确率85.9%、COCO 的实例分割精度54.5和47.0，以及 ADE20K 的 semantics 分割精度51.5。<details>
<summary>Abstract</summary>
Transformers have shown superior performance on various vision tasks. Their large receptive field endows Transformer models with higher representation power than their CNN counterparts. Nevertheless, simply enlarging the receptive field also raises several concerns. On the one hand, using dense attention in ViT leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. On the other hand, the handcrafted attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long-range relations. To solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. This flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. On this basis, we present Deformable Attention Transformer (DAT), a general vision backbone efficient and effective for visual recognition. We further build an enhanced version DAT++. Extensive experiments show that our DAT++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% ImageNet accuracy, 54.5 and 47.0 MS-COCO instance segmentation mAP, and 51.5 ADE20K semantic segmentation mIoU.
</details>
<details>
<summary>摘要</summary>
《 transformers 在视觉任务中表现出色，其大 receive 场能够提供更高的表示力 than CNN 模型。然而，简单地扩大 receive 场也存在一些问题。一方面，使用 dense attention 在 ViT 中会导致额外的内存和计算成本增加，而且特征可能受到 beyond 的无关部分的影响。另一方面，手动设置的 attention 在 PVT 或 Swin Transformer 中可能会限制对长距离关系的模型化。为解决这个 dilemma，我们提出了一种 novel deformable multi-head attention 模块，其中 key 和 value 对在 self-attention 中的位置会在数据依存地分配。这种灵活的方案允许我们的 propose deformable attention 动态关注相关的区域，同时保持 global attention 的表示力。基于这个基础，我们提出了 Deformable Attention Transformer (DAT)，一种通用的视觉基础结构，高效精准地进行视觉识别。此外，我们还提出了 DAT++，一种进一步提高 DAT 的版本。广泛的实验表明，我们的 DAT++ 在多种视觉识别benchmark上达到了最佳结果，其中 ImageNet 准确率为 85.9%，COCO 实例分割 mAP 为 54.5 和 47.0，ADE20K semantic segmentation mIoU 为 51.5。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Segment-Anything-Model-for-Change-Detection-in-HR-Remote-Sensing-Images"><a href="#Adapting-Segment-Anything-Model-for-Change-Detection-in-HR-Remote-Sensing-Images" class="headerlink" title="Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images"></a>Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01429">http://arxiv.org/abs/2309.01429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ggsding/sam-cd">https://github.com/ggsding/sam-cd</a></li>
<li>paper_authors: Lei Ding, Kun Zhu, Daifeng Peng, Hao Tang, Haitao Guo<br>for: 这个研究目的是将视觉基础模型（VFM）应用于高分辨率远程感知图像（RSIs）中的变化探测。methods: 这个研究使用了快速SAM的视觉编码器来提取RS scene中的视觉表现，并使用了一个 convolutional adaptor 来聚合任务化变化信息。此外，这个研究还引入了一个任务无关的 semantic learning branch 来模型RSIs中的semantic latent space。results: 这个研究获得了与顶尖方法相比的更高的准确性，并且展示了与半指导CD方法相似的样本效率学习能力。根据我们所知，这是首次将VFM应用于HR RSIs中的变化探测。<details>
<summary>Abstract</summary>
Vision Foundation Models (VFMs) such as the Segment Anything Model (SAM) allow zero-shot or interactive segmentation of visual contents, thus they are quickly applied in a variety of visual scenes. However, their direct use in many Remote Sensing (RS) applications is often unsatisfactory due to the special imaging characteristics of RS images. In this work, we aim to utilize the strong visual recognition capabilities of VFMs to improve the change detection of high-resolution Remote Sensing Images (RSIs). We employ the visual encoder of FastSAM, an efficient variant of the SAM, to extract visual representations in RS scenes. To adapt FastSAM to focus on some specific ground objects in the RS scenes, we propose a convolutional adaptor to aggregate the task-oriented change information. Moreover, to utilize the semantic representations that are inherent to SAM features, we introduce a task-agnostic semantic learning branch to model the semantic latent in bi-temporal RSIs. The resulting method, SAMCD, obtains superior accuracy compared to the SOTA methods and exhibits a sample-efficient learning ability that is comparable to semi-supervised CD methods. To the best of our knowledge, this is the first work that adapts VFMs for the CD of HR RSIs.
</details>
<details>
<summary>摘要</summary>
各种视觉基础模型（VFM），如分割任何模型（SAM），可以实现零shot或交互分割视觉内容，因此它们很快地应用于多种视觉场景。然而，直接使用它们在许多远程感知（RS）应用中 often 不满足要求，因为RS图像的特殊捕捉特性。在这项工作中，我们希望利用VFM的强大视觉识别能力来改进高分辨率远程感知图像（RSIs）中的变化检测。我们使用 FastSAM 的视觉编码器来提取 RS 场景中的视觉表示。为了使 FastSAM 在RS场景中专注于某些特定的地面 объек，我们提议一种 convolutional adapter 来聚合任务关注的变化信息。此外，我们引入一种任务无关的 semantic learning branch 来模型RSIs中的semantic latent space。得到的方法，SAMCD，与state-of-the-art方法相比，显示出了更高的准确率，并且展现了与 semi-supervised CD 方法类似的样本效率学习能力。到目前为止，这是首次应用 VFM 于高分辨率 RSIs 的变化检测。
</details></li>
</ul>
<hr>
<h2 id="Unified-Pre-training-with-Pseudo-Texts-for-Text-To-Image-Person-Re-identification"><a href="#Unified-Pre-training-with-Pseudo-Texts-for-Text-To-Image-Person-Re-identification" class="headerlink" title="Unified Pre-training with Pseudo Texts for Text-To-Image Person Re-identification"></a>Unified Pre-training with Pseudo Texts for Text-To-Image Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01420">http://arxiv.org/abs/2309.01420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyin Shao, Xinyu Zhang, Changxing Ding, Jian Wang, Jingdong Wang</li>
<li>for: 本研究旨在提高文本到图像人重识别（T2I-ReID）任务的性能，因为存在两种基础问题：数据不一致和训练不一致。</li>
<li>methods: 我们提出了一个新的统一预训策略（UniPT），包括建立大规模的文本标注人像数据集“LUPerson-T”，并使用简单的视觉语言预训策略来对图像和文本模态的特征空间进行Alignment。</li>
<li>results: 我们的UniPT可以在不需要任何辅助工具的情况下达到竞争性的排名1精度（68.50%、60.09%和51.85%）在CUHK-PEDES、ICFG-PEDES和RSTPReid等三个任务上。<details>
<summary>Abstract</summary>
The pre-training task is indispensable for the text-to-image person re-identification (T2I-ReID) task. However, there are two underlying inconsistencies between these two tasks that may impact the performance; i) Data inconsistency. A large domain gap exists between the generic images/texts used in public pre-trained models and the specific person data in the T2I-ReID task. This gap is especially severe for texts, as general textual data are usually unable to describe specific people in fine-grained detail. ii) Training inconsistency. The processes of pre-training of images and texts are independent, despite cross-modality learning being critical to T2I-ReID. To address the above issues, we present a new unified pre-training pipeline (UniPT) designed specifically for the T2I-ReID task. We first build a large-scale text-labeled person dataset "LUPerson-T", in which pseudo-textual descriptions of images are automatically generated by the CLIP paradigm using a divide-conquer-combine strategy. Benefiting from this dataset, we then utilize a simple vision-and-language pre-training framework to explicitly align the feature space of the image and text modalities during pre-training. In this way, the pre-training task and the T2I-ReID task are made consistent with each other on both data and training levels. Without the need for any bells and whistles, our UniPT achieves competitive Rank-1 accuracy of, ie, 68.50%, 60.09%, and 51.85% on CUHK-PEDES, ICFG-PEDES and RSTPReid, respectively. Both the LUPerson-T dataset and code are available at https;//github.com/ZhiyinShao-H/UniPT.
</details>
<details>
<summary>摘要</summary>
“预训作业是文本识别人重识别（T2I-ReID）任务的不可或缺的一部分。然而，有两个隐含的差异可能影响性能；一是数据不一致。 generic images/texts在公共预训模型中使用的大频谱与特定人数据在T2I-ReID任务中存在巨大的频谱差异。这种差异特别是对文本而言，通常文本数据无法细化特定人的描述。二是训练不一致。图像和文本的预训过程是独立的，尽管交叉模态学习是T2I-ReID任务中 krit。为解决以上问题，我们提出了一个新的一体化预训管线（UniPT），专门为T2I-ReID任务设计。我们首先建立了大规模的文本标注人数据集"LUPerson-T",其中图像中的文本描述使用CLIP парадигмы自动生成pseudo文本描述。利用这个数据集，我们然后使用简单的视觉语言预训框架，在预训过程中显式对图像和文本模式之间的特征空间进行对接。这样，预训任务和T2I-ReID任务在数据和训练水平上得到了一致。不需要任何附加功能，我们的UniPT实现了竞争力强的排名1准确率，即68.50%、60.09%和51.85%在CUHK-PEDES、ICFG-PEDES和RSTPReid等三个任务中。LUPerson-T数据集和代码都可以在https://github.com/ZhiyinShao-H/UniPT上获取。”
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Image-Stitching-With-Enhanced-and-Blended-Feature-Reconstruction"><a href="#Implicit-Neural-Image-Stitching-With-Enhanced-and-Blended-Feature-Reconstruction" class="headerlink" title="Implicit Neural Image Stitching With Enhanced and Blended Feature Reconstruction"></a>Implicit Neural Image Stitching With Enhanced and Blended Feature Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01409">http://arxiv.org/abs/2309.01409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minshu-kim/NIS">https://github.com/minshu-kim/NIS</a></li>
<li>paper_authors: Minsu Kim, Jaewon Lee, Byeonghun Lee, Sunghoon Im, Kyong Hwan Jin</li>
<li>for: 提高图像拼接的质量和精度，解决传统框架中的锐利artefacts和照明、深度等级的不一致问题。</li>
<li>methods: 基于隐藏层 neural network 的图像拼接方法，通过估计图像的福洛coefficients来提高图像质量，并在幂值空间进行颜色匹配和重差调整，最终decode为RGB值得拼接图像。</li>
<li>results: 比传统框架更高效地解决低分辨率图像拼接问题，并且可以融合加速图像提高方法，实现更高质量的拼接图像。<details>
<summary>Abstract</summary>
Existing frameworks for image stitching often provide visually reasonable stitchings. However, they suffer from blurry artifacts and disparities in illumination, depth level, etc. Although the recent learning-based stitchings relax such disparities, the required methods impose sacrifice of image qualities failing to capture high-frequency details for stitched images. To address the problem, we propose a novel approach, implicit Neural Image Stitching (NIS) that extends arbitrary-scale super-resolution. Our method estimates Fourier coefficients of images for quality-enhancing warps. Then, the suggested model blends color mismatches and misalignment in the latent space and decodes the features into RGB values of stitched images. Our experiments show that our approach achieves improvement in resolving the low-definition imaging of the previous deep image stitching with favorable accelerated image-enhancing methods. Our source code is available at https://github.com/minshu-kim/NIS.
</details>
<details>
<summary>摘要</summary>
现有的图像拼接框架通常提供可见的合理拼接结果，但它们受到锐化缺陷和照明、深度等因素的影响，导致拼接图像具有模糊效果。虽然最近的学习型拼接方法可以减轻这些缺陷，但它们需要牺牲图像质量，无法捕捉高频环境的细节。为解决这问题，我们提出了一种新的方法：隐式神经图像拼接（NIS），它扩展了自适应超分辨率。我们的方法估算图像的快推函数，然后使用建议的模型在幽DefaultsLatent空间进行混合和调整，最后 decode到RGB值来生成拼接图像。我们的实验表明，我们的方法可以提高前期深度图像拼接的低分辨率问题，并且可以利用加速的图像改进方法来加速图像进行改进。我们的源代码可以在 GitHub 上找到：https://github.com/minshu-kim/NIS。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Self-Supervised-Vision-Transformers-for-Neural-Transfer-Function-Design"><a href="#Leveraging-Self-Supervised-Vision-Transformers-for-Neural-Transfer-Function-Design" class="headerlink" title="Leveraging Self-Supervised Vision Transformers for Neural Transfer Function Design"></a>Leveraging Self-Supervised Vision Transformers for Neural Transfer Function Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01408">http://arxiv.org/abs/2309.01408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Engel, Leon Sick, Timo Ropinski</li>
<li>for: 用于量 Rendering 中的结构分类和质量属性分配</li>
<li>methods: 使用自然语言描述的自适应 Transfer Function 定义方法</li>
<li>results: 减少标注量和计算时间，提高分割精度和用户体验<details>
<summary>Abstract</summary>
In volume rendering, transfer functions are used to classify structures of interest, and to assign optical properties such as color and opacity. They are commonly defined as 1D or 2D functions that map simple features to these optical properties. As the process of designing a transfer function is typically tedious and unintuitive, several approaches have been proposed for their interactive specification. In this paper, we present a novel method to define transfer functions for volume rendering by leveraging the feature extraction capabilities of self-supervised pre-trained vision transformers. To design a transfer function, users simply select the structures of interest in a slice viewer, and our method automatically selects similar structures based on the high-level features extracted by the neural network. Contrary to previous learning-based transfer function approaches, our method does not require training of models and allows for quick inference, enabling an interactive exploration of the volume data. Our approach reduces the amount of necessary annotations by interactively informing the user about the current classification, so they can focus on annotating the structures of interest that still require annotation. In practice, this allows users to design transfer functions within seconds, instead of minutes. We compare our method to existing learning-based approaches in terms of annotation and compute time, as well as with respect to segmentation accuracy. Our accompanying video showcases the interactivity and effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
在量rendering中，传输函数用于分类结构物体，并将光学性质如颜色和透明度分配给这些结构物体。传输函数通常是1D或2D函数，它们将简单特征映射到这些光学性质。由于设计传输函数的过程通常是慢搬和不直观的，因此有几种方法被提议用于其交互式规定。在本文中，我们提出了一种使用自然语言处理器来定义传输函数的新方法。用户只需在切片查看器中选择结构物体，我们的方法会自动选择与结构物体相似的结构，基于由神经网络提取的高级特征。与之前的学习基于的传输函数方法不同，我们的方法不需要训练模型，可以快速进行推理，从而允许用户在数秒钟内设计传输函数，而不是需要数分钟。此外，我们的方法可以减少必须的注释量，通过在用户操作时提供反馈，使用户能够更快地注释需要注释的结构物体。在实践中，我们的方法比既有学习基于的方法更快，更准确。我们的视频辑演示了我们的方法的互动性和效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Residual-Elastic-Warps-for-Image-Stitching-under-Dirichlet-Boundary-Condition"><a href="#Learning-Residual-Elastic-Warps-for-Image-Stitching-under-Dirichlet-Boundary-Condition" class="headerlink" title="Learning Residual Elastic Warps for Image Stitching under Dirichlet Boundary Condition"></a>Learning Residual Elastic Warps for Image Stitching under Dirichlet Boundary Condition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01406">http://arxiv.org/abs/2309.01406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minshu-kim/REwarp">https://github.com/minshu-kim/REwarp</a></li>
<li>paper_authors: Minsu Kim, Yongjun Lee, Woo Kyoung Han, Kyong Hwan Jin</li>
<li>for: 解决深度学习图像拼接中大偏移误差所导致的缺陷，提高图像拼接的精度和效率。</li>
<li>methods: 使用 Dirichlet 边界条件和循环学习减少误差，预测homography和Thin-plate Spline（TPS）来实现缺陷和孔洞自适应拼接。</li>
<li>results: 在实验中，REwarp 表现出了优于现有方法的精度和计算效率，能够提供高质量的图像拼接 results。<details>
<summary>Abstract</summary>
Trendy suggestions for learning-based elastic warps enable the deep image stitchings to align images exposed to large parallax errors. Despite the remarkable alignments, the methods struggle with occasional holes or discontinuity between overlapping and non-overlapping regions of a target image as the applied training strategy mostly focuses on overlap region alignment. As a result, they require additional modules such as seam finder and image inpainting for hiding discontinuity and filling holes, respectively. In this work, we suggest Recurrent Elastic Warps (REwarp) that address the problem with Dirichlet boundary condition and boost performances by residual learning for recurrent misalign correction. Specifically, REwarp predicts a homography and a Thin-plate Spline (TPS) under the boundary constraint for discontinuity and hole-free image stitching. Our experiments show the favorable aligns and the competitive computational costs of REwarp compared to the existing stitching methods. Our source code is available at https://github.com/minshu-kim/REwarp.
</details>
<details>
<summary>摘要</summary>
当前建议：学习基于弹性折叠的方法可以使深度图像融合到大量偏差错误中进行对齐。尽管这些方法能够实现出色的对齐，但它们在处理重叠和非重叠区域之间的缺陷和缺口问题上却陷入困难。因此，它们通常需要额外的模块，如缺陷找到器和图像填充，以隐藏缺陷和填充缺口。在这种情况下，我们建议使用循环弹性折叠（REwarp）方法，该方法通过 Dirichlet 边界条件和循环弹性学习来解决缺陷和缺口问题，从而实现不间断和缺陷自适应的图像融合。我们的实验表明，REwarp 方法可以具有优秀的对齐性和竞争性的计算成本。我们的源代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="SSVOD-Semi-Supervised-Video-Object-Detection-with-Sparse-Annotations"><a href="#SSVOD-Semi-Supervised-Video-Object-Detection-with-Sparse-Annotations" class="headerlink" title="SSVOD: Semi-Supervised Video Object Detection with Sparse Annotations"></a>SSVOD: Semi-Supervised Video Object Detection with Sparse Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01391">http://arxiv.org/abs/2309.01391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Mahmud, Chun-Hao Liu, Burhaneddin Yaman, Diana Marculescu</li>
<li>for: 这篇论文是为了提出一种基于semi-supervised learning的视频对象检测方法，以解决现有视频对象检测方法所存在的一些问题，例如需要大量注释框架来实现良好的监督学习效果。</li>
<li>methods: 这篇论文使用了一种基于流动的策略，即使用流动的预测来选择合适的 pseudo-labels，以便在大量无注释框架上进行学习。具体来说，这篇论文引入了两种选择方法：一种是基于流动的预测和匹配的方法，另一种是基于交叉 IoU 和交叉异同度的方法。</li>
<li>results: 根据论文的结果，这种 semi-supervised 视频对象检测方法可以在 ImageNet-VID、Epic-KITCHENS 和 YouTube-VIS 等 datasets 上达到显著的性能改进，比如在 ImageNet-VID 上的 mAP 提高了 10.3%，在 Epic-KITCHENS 上的 mAP 提高了 13.1%，在 YouTube-VIS 上的 mAP 提高了 11.4%。<details>
<summary>Abstract</summary>
Despite significant progress in semi-supervised learning for image object detection, several key issues are yet to be addressed for video object detection: (1) Achieving good performance for supervised video object detection greatly depends on the availability of annotated frames. (2) Despite having large inter-frame correlations in a video, collecting annotations for a large number of frames per video is expensive, time-consuming, and often redundant. (3) Existing semi-supervised techniques on static images can hardly exploit the temporal motion dynamics inherently present in videos. In this paper, we introduce SSVOD, an end-to-end semi-supervised video object detection framework that exploits motion dynamics of videos to utilize large-scale unlabeled frames with sparse annotations. To selectively assemble robust pseudo-labels across groups of frames, we introduce \textit{flow-warped predictions} from nearby frames for temporal-consistency estimation. In particular, we introduce cross-IoU and cross-divergence based selection methods over a set of estimated predictions to include robust pseudo-labels for bounding boxes and class labels, respectively. To strike a balance between confirmation bias and uncertainty noise in pseudo-labels, we propose confidence threshold based combination of hard and soft pseudo-labels. Our method achieves significant performance improvements over existing methods on ImageNet-VID, Epic-KITCHENS, and YouTube-VIS datasets. Code and pre-trained models will be released.
</details>
<details>
<summary>摘要</summary>
尽管在半监督学习方面已经取得了显著的进步，但视频对象检测中仍有一些关键问题需要解决：（1）在视频中达到良好的性能需要大量的注释帧。（2）即使在视频中存在大量的间隔帧相互关系，仍然收集注释帧的成本高、时间费时、重复的问题。（3）现有的半监督技术在静止图像上基本无法利用视频中的时间动态。本文介绍SSVOD，一种终端到终点的半监督视频对象检测框架，利用视频中的时间动态来利用大量的无注释帧。为了选择性地组合坚实的pseudo标签，我们引入了流动抗压采用邻近帧的预测值进行时间一致性估计。具体来说，我们引入了交叉IoU和交叉异常值基于选择方法，以包括坚实的pseudo标签 для bounding box和类别标签。为了保持pseudo标签中的确认偏见和不确定噪音的平衡，我们提议使用信任值阈值基于组合硬化和软化pseudo标签。我们的方法在ImageNet-VID、Epic-KITCHENS和YouTube-VIS数据集上取得了显著的性能提升。我们将代码和预训练模型发布。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Video-Scenes-through-Text-Insights-from-Text-based-Video-Question-Answering"><a href="#Understanding-Video-Scenes-through-Text-Insights-from-Text-based-Video-Question-Answering" class="headerlink" title="Understanding Video Scenes through Text: Insights from Text-based Video Question Answering"></a>Understanding Video Scenes through Text: Insights from Text-based Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01380">http://arxiv.org/abs/2309.01380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar</li>
<li>for: 本研究探讨了两个新引入的 dataset，NewsVideoQA 和 M4-ViteVQA，用于基于文本内容回答视频问题。</li>
<li>methods: 研究者使用了 BERT-QA，一种文本只模型，对这两个 dataset 进行了实验，并发现它在这两个 dataset 上 display 相似的表现。</li>
<li>results: 研究发现，训练在 M4-ViteVQA 上并不能 directly apply to NewsVideoQA，且对于 out-of-domain 训练，需要进行适应。<details>
<summary>Abstract</summary>
Researchers have extensively studied the field of vision and language, discovering that both visual and textual content is crucial for understanding scenes effectively. Particularly, comprehending text in videos holds great significance, requiring both scene text understanding and temporal reasoning. This paper focuses on exploring two recently introduced datasets, NewsVideoQA and M4-ViteVQA, which aim to address video question answering based on textual content. The NewsVideoQA dataset contains question-answer pairs related to the text in news videos, while M4-ViteVQA comprises question-answer pairs from diverse categories like vlogging, traveling, and shopping. We provide an analysis of the formulation of these datasets on various levels, exploring the degree of visual understanding and multi-frame comprehension required for answering the questions. Additionally, the study includes experimentation with BERT-QA, a text-only model, which demonstrates comparable performance to the original methods on both datasets, indicating the shortcomings in the formulation of these datasets. Furthermore, we also look into the domain adaptation aspect by examining the effectiveness of training on M4-ViteVQA and evaluating on NewsVideoQA and vice-versa, thereby shedding light on the challenges and potential benefits of out-of-domain training.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:研究人员对视觉和语言领域进行了广泛的研究，发现视觉和文本内容都是理解场景的关键因素。特别是在视频中理解文本内容的重要性，需要场景文本理解和时间理解。本文关注两个最近引入的dataset，新闻视频问答集和M4-ViteVQA集，以解决基于文本内容的视频问答问题。新闻视频问答集包含新闻视频中文本内容相关的问题答案对，而M4-ViteVQA集包含多种类别的问题答案对，如博客、旅行和购物。我们对这些dataset的形式化进行了多种层次的分析，探讨响应问题需要的视觉理解和多帧理解水平。此外，我们还进行了BERT-QA模型的实验，这是一种只有文本内容的模型，它在这两个dataset上达到了相当的性能，表明这些dataset的形式化存在缺陷。此外，我们还研究了域适应问题，包括在M4-ViteVQA集上训练并在新闻视频问答集上测试的效果，以及 vice versa，从而探讨域外训练的挑战和优点。
</details></li>
</ul>
<hr>
<h2 id="ImmersiveNeRF-Hybrid-Radiance-Fields-for-Unbounded-Immersive-Light-Field-Reconstruction"><a href="#ImmersiveNeRF-Hybrid-Radiance-Fields-for-Unbounded-Immersive-Light-Field-Reconstruction" class="headerlink" title="ImmersiveNeRF: Hybrid Radiance Fields for Unbounded Immersive Light Field Reconstruction"></a>ImmersiveNeRF: Hybrid Radiance Fields for Unbounded Immersive Light Field Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01374">http://arxiv.org/abs/2309.01374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohang Yu, Haoxiang Wang, Yuqi Han, Lei Yang, Tao Yu, Qionghai Dai<br>for: This paper proposes a method for unbounded immersive light field reconstruction, which supports high-quality rendering and aggressive view extrapolation.methods: The method uses a hybrid radiance field representation, with separate radiance fields for the foreground and background, and adaptive sampling and segmentation regularization to improve performance.results: The proposed method achieves strong performance for unbounded immersive light field reconstruction, and contributes a novel dataset for further research and applications in the immersive light field domain.Here’s the text in Simplified Chinese:for: 这篇论文提出了一种用于无限维度吸引光场重建的方法，支持高质量渲染和较为侵略性的视角推导。methods: 该方法使用了混合的光场场景表示，将前景和背景分别表示为两个不同的空间映射策略，并使用了适应性的采样和分割规则来提高性能。results: 提出的方法在无限维度吸引光场重建中实现了强大的表现，并为未来的研究和应用在吸引光场领域提供了一个新的数据集。<details>
<summary>Abstract</summary>
This paper proposes a hybrid radiance field representation for unbounded immersive light field reconstruction which supports high-quality rendering and aggressive view extrapolation. The key idea is to first formally separate the foreground and the background and then adaptively balance learning of them during the training process. To fulfill this goal, we represent the foreground and background as two separate radiance fields with two different spatial mapping strategies. We further propose an adaptive sampling strategy and a segmentation regularizer for more clear segmentation and robust convergence. Finally, we contribute a novel immersive light field dataset, named THUImmersive, with the potential to achieve much larger space 6DoF immersive rendering effects compared with existing datasets, by capturing multiple neighboring viewpoints for the same scene, to stimulate the research and AR/VR applications in the immersive light field domain. Extensive experiments demonstrate the strong performance of our method for unbounded immersive light field reconstruction.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种混合辐射场表示方法，用于无限尺度吸引辐射场重建，支持高质量渲染和激进视点推演。关键思想是首先正式分离背景和前景，然后在训练过程中适应地学习它们。为达到这个目标，我们将背景和前景表示为两个不同的辐射场，使用两种不同的空间映射策略。我们还提出了一种适应 sampling 策略和一种分割规范，以实现更清晰的分割和更稳定的收敛。最后，我们发布了一个新的 immerse 辐射场数据集，名为 THUImmersive，它可以实现更大的空间 six-degree-of-freedom 吸引辐射渲染效果，比现有数据集更大，通过记录同一场景的多个邻居视点，刺激研究和 AR/VR 应用在 immerse 辐射场领域。广泛的实验表明我们的方法在无限尺度吸引辐射场重建中具有强大表现。
</details></li>
</ul>
<hr>
<h2 id="DiverseMotion-Towards-Diverse-Human-Motion-Generation-via-Discrete-Diffusion"><a href="#DiverseMotion-Towards-Diverse-Human-Motion-Generation-via-Discrete-Diffusion" class="headerlink" title="DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion"></a>DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01372">http://arxiv.org/abs/2309.01372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/axdfhj/mdd">https://github.com/axdfhj/mdd</a></li>
<li>paper_authors: Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang, Yi Yang</li>
<li>for: 这个论文的目的是生成基于文本描述的高质量人体动作，同时保持动作多样性。</li>
<li>methods: 该论文使用了一种新的方法，即 Hierarchical Semantic Aggregation (HSA) 模块和 Motion Discrete Diffusion (MDD) 框架，以确保动作质量和多样性之间的平衡。</li>
<li>results: 该论文通过实验证明，其方法可以在 HumanML3D 和 KIT-ML 上达到 state-of-the-art 的动作质量和竞争力动作多样性。<details>
<summary>Abstract</summary>
We present DiverseMotion, a new approach for synthesizing high-quality human motions conditioned on textual descriptions while preserving motion diversity.Despite the recent significant process in text-based human motion generation,existing methods often prioritize fitting training motions at the expense of action diversity. Consequently, striking a balance between motion quality and diversity remains an unresolved challenge. This problem is compounded by two key factors: 1) the lack of diversity in motion-caption pairs in existing benchmarks and 2) the unilateral and biased semantic understanding of the text prompt, focusing primarily on the verb component while neglecting the nuanced distinctions indicated by other words.In response to the first issue, we construct a large-scale Wild Motion-Caption dataset (WMC) to extend the restricted action boundary of existing well-annotated datasets, enabling the learning of diverse motions through a more extensive range of actions. To this end, a motion BLIP is trained upon a pretrained vision-language model, then we automatically generate diverse motion captions for the collected motion sequences. As a result, we finally build a dataset comprising 8,888 motions coupled with 141k text.To comprehensively understand the text command, we propose a Hierarchical Semantic Aggregation (HSA) module to capture the fine-grained semantics.Finally,we involve the above two designs into an effective Motion Discrete Diffusion (MDD) framework to strike a balance between motion quality and diversity. Extensive experiments on HumanML3D and KIT-ML show that our DiverseMotion achieves the state-of-the-art motion quality and competitive motion diversity. Dataset, code, and pretrained models will be released to reproduce all of our results.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的方法——多样化动作（DiverseMotion），可以生成高质量的人体动作，基于文本描述而conditioning，同时保持动作多样性。尽管最近有 significative progress in text-based human motion generation,但现有方法通常会偏好适应训练动作，而忽略动作多样性。这种问题被两个关键因素困扰：1）现有的动作-caption对不够多样化，2）文本提示的semantic理解偏执一面，即便只重视verb部分，而忽略其他词语的细微差别。为了解决第一个问题，我们构建了一个大规模的野动作-caption数据集（WMC），以扩展现有的动作边界，让学习动作的多样性。为此，我们首先训练了一个动作BLIP在一个预训练的视力语言模型上，然后自动生成了多样的动作caption。最终，我们建立了一个包含8,888个动作和141,000个文本的数据集。为了全面理解文本命令，我们提出了一个层次semantic汇集（HSA）模块，以捕捉细节semantic。最后，我们将上述两种设计 integrate into an effective Motion Discrete Diffusion（MDD）框架，以平衡动作质量和多样性。我们的多样化动作在HumanML3D和KIT-ML上进行了广泛的实验，并达到了状态之arte motion质量和竞争力动作多样性。数据集、代码和预训练模型将被发布，以便复制所有我们的结果。
</details></li>
</ul>
<hr>
<h2 id="Attention-as-Annotation-Generating-Images-and-Pseudo-masks-for-Weakly-Supervised-Semantic-Segmentation-with-Diffusion"><a href="#Attention-as-Annotation-Generating-Images-and-Pseudo-masks-for-Weakly-Supervised-Semantic-Segmentation-with-Diffusion" class="headerlink" title="Attention as Annotation: Generating Images and Pseudo-masks for Weakly Supervised Semantic Segmentation with Diffusion"></a>Attention as Annotation: Generating Images and Pseudo-masks for Weakly Supervised Semantic Segmentation with Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01369">http://arxiv.org/abs/2309.01369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryota Yoshihashi, Yuya Otsuka, Kenji Doi, Tomohiro Tanaka<br>for:* 这个论文的目的是提出一种没有使用真实图像和手动标注的Semantic segmentation训练方法。methods:* 该方法使用文本到图像扩散模型生成的图像，并使用图像的内部文本到图像十字关注作为监督 Pseudo-mask。results:* 实验表明，attn2mask可以在PASCAL VOC中取得良好的结果，而不需要使用真实的训练数据。* 该方法还能够扩展到更多类别的场景，如ImageNet segmentation。* 它还显示了对LoRA基于的细化调整的适应能力，可以将segmenation转移到远程领域，如Cityscapes。<details>
<summary>Abstract</summary>
Although recent advancements in diffusion models enabled high-fidelity and diverse image generation, training of discriminative models largely depends on collections of massive real images and their manual annotation. Here, we present a training method for semantic segmentation that neither relies on real images nor manual annotation. The proposed method {\it attn2mask} utilizes images generated by a text-to-image diffusion model in combination with its internal text-to-image cross-attention as supervisory pseudo-masks. Since the text-to-image generator is trained with image-caption pairs but without pixel-wise labels, attn2mask can be regarded as a weakly supervised segmentation method overall. Experiments show that attn2mask achieves promising results in PASCAL VOC for not using real training data for segmentation at all, and it is also useful to scale up segmentation to a more-class scenario, i.e., ImageNet segmentation. It also shows adaptation ability with LoRA-based fine-tuning, which enables the transfer to a distant domain i.e., Cityscapes.
</details>
<details>
<summary>摘要</summary>
尽管最近的扩散模型可以生成高精度和多样的图像，但训练推理模型大多依赖于庞大的真实图像和手动注释。在这里，我们提出了一种不需要真实图像和手动注释的 semantic segmentation 训练方法。我们称之为“attn2mask”，它利用由文本到图像扩散模型生成的图像，并与其内部的文本到图像交叉注意力作为超级vision pseudo-mask。由于文本到图像生成器在没有像素级标注的情况下训练，可以视为一种弱supervised segmentation方法。实验表明，attn2mask 在 PASCAL VOC 上表现出色，不使用实际训练数据进行 segmentation 的情况下，并且在更多类enario中也表现出了好的扩展能力。它还表现出了 LoRA-based fine-tuning 的适应能力，可以在远程领域 i.e., Cityscapes 中进行转移。
</details></li>
</ul>
<hr>
<h2 id="High-Frequency-High-Accuracy-Pointing-onboard-Nanosats-using-Neuromorphic-Event-Sensing-and-Piezoelectric-Actuation"><a href="#High-Frequency-High-Accuracy-Pointing-onboard-Nanosats-using-Neuromorphic-Event-Sensing-and-Piezoelectric-Actuation" class="headerlink" title="High Frequency, High Accuracy Pointing onboard Nanosats using Neuromorphic Event Sensing and Piezoelectric Actuation"></a>High Frequency, High Accuracy Pointing onboard Nanosats using Neuromorphic Event Sensing and Piezoelectric Actuation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01361">http://arxiv.org/abs/2309.01361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasir Latif, Peter Anastasiou, Yonhon Ng, Zebb Prime, Tien-Fu Lu, Matthew Tetlow, Robert Mahony, Tat-Jun Chin</li>
<li>for: 这个论文旨在提高小型卫星的稳定点击精度，以便为空间域意识任务（SDA）提供更高精度的点击。</li>
<li>methods: 该论文提出了一种新的 payload，它利用 neuromorphic event sensor 和 piezoelectric stage 实现高精度和高频率的相对位态估计和控制。</li>
<li>results:  experiments 表明，使用该 payload 可以在 1-5 度的精度下实现稳定点击，并且可以在 50Hz 的操作频率下运行。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
As satellites become smaller, the ability to maintain stable pointing decreases as external forces acting on the satellite come into play. At the same time, reaction wheels used in the attitude determination and control system (ADCS) introduce high frequency jitter which can disrupt pointing stability. For space domain awareness (SDA) tasks that track objects tens of thousands of kilometres away, the pointing accuracy offered by current nanosats, typically in the range of 10 to 100 arcseconds, is not sufficient. In this work, we develop a novel payload that utilises a neuromorphic event sensor (for high frequency and highly accurate relative attitude estimation) paired in a closed loop with a piezoelectric stage (for active attitude corrections) to provide highly stable sensor-specific pointing. Event sensors are especially suited for space applications due to their desirable characteristics of low power consumption, asynchronous operation, and high dynamic range. We use the event sensor to first estimate a reference background star field from which instantaneous relative attitude is estimated at high frequency. The piezoelectric stage works in a closed control loop with the event sensor to perform attitude corrections based on the discrepancy between the current and desired attitude. Results in a controlled setting show that we can achieve a pointing accuracy in the range of 1-5 arcseconds using our novel payload at an operating frequency of up to 50Hz using a prototype built from commercial-off-the-shelf components. Further details can be found at https://ylatif.github.io/ultrafinestabilisation
</details>
<details>
<summary>摘要</summary>
为了提高小型卫星的稳定性，我们开发了一种新的 payload，它使用神经元事件传感器（高频和高精度相对姿态估计）和一个 piezoelectric stage（用于活动姿态 corrections）。事件传感器在空间应用中特别有利，因为它们具有低功耗、异步操作和高动态范围等极佳特点。我们使用事件传感器来估计背景星场，并根据差异来使用 piezoelectric stage 进行姿态 corrections。在控制的环境中，我们可以使用我们的新型payload实现1-5弧矩度精度的指向稳定，并且可以在50Hz的运行频率下达到这个精度。更多细节可以在https://ylatif.github.io/ultrafinestabilisation 查看。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Classifiers-To-Changing-Class-Priors-During-Deployment"><a href="#Adapting-Classifiers-To-Changing-Class-Priors-During-Deployment" class="headerlink" title="Adapting Classifiers To Changing Class Priors During Deployment"></a>Adapting Classifiers To Changing Class Priors During Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01357">http://arxiv.org/abs/2309.01357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Natnael Daba, Bruce McIntosh, Abhijit Mahalanobis</li>
<li>for: 这篇论文是关于如何在不同的部署场景中使用通用分类器的。</li>
<li>methods: 论文使用了基于分类器自身输出的方法来估算类偏好。</li>
<li>results: 结果表明，在部署场景中 incorporating 估算的类偏好可以使分类器在运行时准确率提高。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Conventional classifiers are trained and evaluated using balanced data sets in which all classes are equally present. Classifiers are now trained on large data sets such as ImageNet, and are now able to classify hundreds (if not thousands) of different classes. On one hand, it is desirable to train such general-purpose classifier on a very large number of classes so that it performs well regardless of the settings in which it is deployed. On the other hand, it is unlikely that all classes known to the classifier will occur in every deployment scenario, or that they will occur with the same prior probability. In reality, only a relatively small subset of the known classes may be present in a particular setting or environment. For example, a classifier will encounter mostly animals if its deployed in a zoo or for monitoring wildlife, aircraft and service vehicles at an airport, or various types of automobiles and commercial vehicles if it is used for monitoring traffic. Furthermore, the exact class priors are generally unknown and can vary over time. In this paper, we explore different methods for estimating the class priors based on the output of the classifier itself. We then show that incorporating the estimated class priors in the overall decision scheme enables the classifier to increase its run-time accuracy in the context of its deployment scenario.
</details>
<details>
<summary>摘要</summary>
传统的分类器通常在具有平衡数据集的情况下训练和评估。现在，分类器被训练在大量数据集如ImageNet上，能够分类百计以上不同的类别。一方面，悉心地训练这种通用分类器，以便它在不同的部署enario中都能表现出色。另一方面，实际情况下，分类器可能只会遇到部分已知的类别，而且这些类别的发生概率可能不同，甚至随着时间的推移而变化。例如，如果把分类器部署在动物园或野生动物监测中，它就会遇到大量的动物类别。 similarly， if it is used for monitoring traffic, it will encounter mostly automobiles and commercial vehicles. 为了解决这个问题，我们在这篇论文中研究了不同的方法来估算类别概率，基于分类器的输出。然后，我们表明，在部署scenario中，通过包含估算后的类别概率在总决策方案中，可以使分类器在运行时准确性提高。
</details></li>
</ul>
<hr>
<h2 id="Real-time-pedestrian-recognition-on-low-computational-resources"><a href="#Real-time-pedestrian-recognition-on-low-computational-resources" class="headerlink" title="Real-time pedestrian recognition on low computational resources"></a>Real-time pedestrian recognition on low computational resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01353">http://arxiv.org/abs/2309.01353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guifan Weng</li>
<li>for: 这篇文章的目的是实现实时行人识别在小型移动设备上，以提高安全性和自动驾驶等应用的效能。</li>
<li>methods: 这篇文章使用了三种方法来实现实时行人识别，包括提高了本地二进制特征和 AdaBoost 分类器、优化了几何特征和支持向量机制、以及实现了快速梯度下降神经网络。</li>
<li>results: 这篇文章的结果显示了三种方法可以在小型物理设备上实现实时行人识别，并且获得了高于95%的准确率和高于5 fps的速度。这些方法可以轻松地应用到小型移动设备上，并且具有高相容性和通用性。<details>
<summary>Abstract</summary>
Pedestrian recognition has successfully been applied to security, autonomous cars, Aerial photographs. For most applications, pedestrian recognition on small mobile devices is important. However, the limitations of the computing hardware make this a challenging task. In this work, we investigate real-time pedestrian recognition on small physical-size computers with low computational resources for faster speed. This paper presents three methods that work on the small physical size CPUs system. First, we improved the Local Binary Pattern (LBP) features and Adaboost classifier. Second, we optimized the Histogram of Oriented Gradients (HOG) and Support Vector Machine. Third, We implemented fast Convolutional Neural Networks (CNNs). The results demonstrate that the three methods achieved real-time pedestrian recognition at an accuracy of more than 95% and a speed of more than 5 fps on a small physical size computational platform with a 1.8 GHz Intel i5 CPU. Our methods can be easily applied to small mobile devices with high compatibility and generality.
</details>
<details>
<summary>摘要</summary>
人体识别已成功应用于安全、自动驾驶、航空图像等领域。大多数应用中，人体识别在小型移动设备上是非常重要。然而，计算硬件的限制使得这成为一项挑战。在这项工作中，我们调查了小型物理尺寸计算机上的实时人体识别方法。本文提出了三种方法，它们在小型物理尺寸计算机系统上实现了实时人体识别，并且具有高准确率和快速速度。首先，我们改进了本地二进制特征（LBP）和权重融合分类器。其次，我们优化了梯度图 histogram 和支持向量机。最后，我们实现了快速的卷积神经网络（CNNs）。结果表明，三种方法在一个小型物理尺寸计算平台上实现了实时人体识别，准确率高于 95%，速度高于 5 fps。我们的方法可以轻松应用于小型移动设备，具有高兼容性和通用性。
</details></li>
</ul>
<hr>
<h2 id="Adv3D-Generating-3D-Adversarial-Examples-in-Driving-Scenarios-with-NeRF"><a href="#Adv3D-Generating-3D-Adversarial-Examples-in-Driving-Scenarios-with-NeRF" class="headerlink" title="Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF"></a>Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01351">http://arxiv.org/abs/2309.01351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leheng Li, Qing Lian, Ying-Cong Chen</li>
<li>for: 这个研究旨在测试深度神经网络（DNNs）对于恶作剧示例的敏感性，并且针对基于DNN的自动驾驶架构（i.e., 3D物体探测）。</li>
<li>methods: 这个研究使用了模型恶作剧示例为Neural Radiance Fields（NeRFs），并且提出了primitive-aware sampling和semantic-guided regularization以生成可能的恶作剧示例。</li>
<li>results: 实验结果显示，训练了恶作剧NeRF可以对不同的 pose、scene 和3D探测器进行大规模的性能降低。此外，这个研究也提出了一种防御方法，即通过数据增强训练来防止这些攻击。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks (i.e., 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects' confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To generate physically realizable adversarial examples, we propose primitive-aware sampling and semantic-guided regularization that enable 3D patch attacks with camouflage adversarial texture. Experimental results demonstrate that the trained adversarial NeRF generalizes well to different poses, scenes, and 3D detectors. Finally, we provide a defense method to our attacks that involves adversarial training through data augmentation. Project page: https://len-li.github.io/adv3d-web
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）已经被证明非常易受到敌意示例的影响，这引发了特别的安全关注，特别是在基于DNN的自动驾驶堆栈中（即3D对象检测）。虽然有大量的图像级别攻击工作，但大多数都是在2D像素空间中进行，这些攻击并不总是物理上真实的在我们的3D世界中。在这里，我们提出了模型敌意示例为神经辐射场（NeRF）的首次探索。NeRF的进步提供了真实的外观和准确的3D生成，从而导致更真实和可能的敌意示例。我们在训练敌意NeRF时，将培育周围对象的信任预测值作为3D检测器的训练集中的一部分。然后，我们在无法见验证集上评估Adv3D，并证明它可以在任意抽象 pose 中进行3D patch攻击。为生成物理可能的敌意示例，我们提出了基于元素的sampling和semantic-guided regularization，允许3D质量攻击。实验结果表明，我们的训练敌意NeRF可以在不同的 pose、场景和3D检测器上进行广泛的应用。最后，我们提出了防御方法，通过数据增强来进行对敌意示例的防御。项目页面：https://len-li.github.io/adv3d-web
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Parametric-Prototype-Learning-for-Cross-Domain-Few-Shot-Classification"><a href="#Adaptive-Parametric-Prototype-Learning-for-Cross-Domain-Few-Shot-Classification" class="headerlink" title="Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification"></a>Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01342">http://arxiv.org/abs/2309.01342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marzi Heidari, Abdullah Alchihabi, Qing En, Yuhong Guo</li>
<li>for: 这篇论文是为了解决跨领域少数检索分类问题。</li>
<li>methods: 本文提出了一种名为 Adaptive Parametric Prototype Learning（APPL）的新方法，它是基于元学习惯例的。不同于现有的标本性几少方法，我们在支持集合上学习分类标本，并将标本获得到几少检索集合中的条件强制整理。</li>
<li>results: 我们在多个跨领域少数检索资料集上实验了这种方法，结果显示APPL在跨领域少数检索分类中表现更好，比较多数现有的方法。<details>
<summary>Abstract</summary>
Cross-domain few-shot classification induces a much more challenging problem than its in-domain counterpart due to the existence of domain shifts between the training and test tasks. In this paper, we develop a novel Adaptive Parametric Prototype Learning (APPL) method under the meta-learning convention for cross-domain few-shot classification. Different from existing prototypical few-shot methods that use the averages of support instances to calculate the class prototypes, we propose to learn class prototypes from the concatenated features of the support set in a parametric fashion and meta-learn the model by enforcing prototype-based regularization on the query set. In addition, we fine-tune the model in the target domain in a transductive manner using a weighted-moving-average self-training approach on the query instances. We conduct experiments on multiple cross-domain few-shot benchmark datasets. The empirical results demonstrate that APPL yields superior performance than many state-of-the-art cross-domain few-shot learning methods.
</details>
<details>
<summary>摘要</summary>
跨领域少量分类问题比其内领域对应的问题更加挑战性，这是因为训练和测试任务之间存在领域偏移。在这篇论文中，我们开发了一种名为 Adaptive Parametric Prototype Learning（APPL）的新方法，该方法基于元学习准则进行跨领域少量分类。与现有的概率性几何方法不同，我们在支持集合的 concatenated 特征上学习类prototype，并在元学习中加入prototype-based regularization。此外，我们在目标领域中进行了权重移动平均自适应更新方法，以便在查询集中进行微调。我们在多个跨领域少量分类 benchmark 数据集上进行了实验，结果表明，APPL 的性能较多现状顶尖跨领域少量分类方法优秀。
</details></li>
</ul>
<hr>
<h2 id="MDSC-Towards-Evaluating-the-Style-Consistency-Between-Music-and"><a href="#MDSC-Towards-Evaluating-the-Style-Consistency-Between-Music-and" class="headerlink" title="MDSC: Towards Evaluating the Style Consistency Between Music and"></a>MDSC: Towards Evaluating the Style Consistency Between Music and</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01340">http://arxiv.org/abs/2309.01340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zixiangzhou916/mdsc">https://github.com/zixiangzhou916/mdsc</a></li>
<li>paper_authors: Zixiang Zhou, Baoyuan Wang</li>
<li>for: 评估音乐与舞蹈风格的匹配度（assessing the matching degree between music and dance styles）</li>
<li>methods: 使用音乐编码器和动作编码器进行匹配和对齐（using music and motion encoders for matching and alignment）</li>
<li>results: 提出了一种新的评估metric——音乐动作风格一致度（MDSC），并通过用户研究发现其可以准确评估音乐与动作风格之间的匹配度（accurately assessing the matching degree between music and dance styles）。Here is the summary in English for reference:</li>
<li>for: Assessing the matching degree between music and dance styles</li>
<li>methods: Using music and motion encoders for matching and alignment</li>
<li>results: Proposed a new evaluation metric called Music-Dance Style Consistency (MDSC) and validated its effectiveness through user studies, demonstrating its ability to accurately assess the matching degree between music and dance styles.<details>
<summary>Abstract</summary>
We propose MDSC(Music-Dance-Style Consistency), the first evaluation metric which assesses to what degree the dance moves and music match. Existing metrics can only evaluate the fidelity and diversity of motion and the degree of rhythmic matching between music and motion. MDSC measures how stylistically correlated the generated dance motion sequences and the conditioning music sequences are. We found that directly measuring the embedding distance between motion and music is not an optimal solution. We instead tackle this through modelling it as a clustering problem. Specifically, 1) we pre-train a music encoder and a motion encoder, then 2) we learn to map and align the motion and music embedding in joint space by jointly minimizing the intra-cluster distance and maximizing the inter-cluster distance, and 3) for evaluation purpose, we encode the dance moves into embedding and measure the intra-cluster and inter-cluster distances, as well as the ratio between them. We evaluate our metric on the results of several music-conditioned motion generation methods, combined with user study, we found that our proposed metric is a robust evaluation metric in measuring the music-dance style correlation. The code is available at: https://github.com/zixiangzhou916/MDSC.
</details>
<details>
<summary>摘要</summary>
我们提出了MDSC（音乐舞蹈风格一致性），第一个评估预测metric，评估音乐和舞蹈动作之间的匹配程度。现有的metric只能评估动作和音乐的精度和多样性，以及音乐和动作的节奏匹配程度。而MDSC则测量了生成的舞蹈动作序列和条件音乐序列之间的风格相关性。我们发现直接测量动作和音乐的嵌入距离并不是最佳解决方案。我们相反地通过模elling它为一个聚类问题来解决。具体来说，我们先预训一个音乐编码器和一个动作编码器，然后学习将动作和音乐嵌入在共同空间中进行对应。最后，我们用计算内层距离和外层距离，以及内层和外层之间的比率来评估。我们在一些音乐条件动作生成方法的结果上进行了评估，并结合用户研究发现，我们的提出的metric是一种可靠的评估metric，可以量化音乐舞蹈风格相关性。代码可以在以下链接中找到：https://github.com/zixiangzhou916/MDSC。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Constraint-Matching-Transformer-for-Weakly-Supervised-Object-Localization"><a href="#Semantic-Constraint-Matching-Transformer-for-Weakly-Supervised-Object-Localization" class="headerlink" title="Semantic-Constraint Matching Transformer for Weakly Supervised Object Localization"></a>Semantic-Constraint Matching Transformer for Weakly Supervised Object Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01331">http://arxiv.org/abs/2309.01331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Cao, Yukun Su, Wenjun Wang, Yanxia Liu, Qingyao Wu</li>
<li>for: 本研究旨在解决weakly supervised object localization中的partial activation问题，即使用只有图像水平级别的指导，学习检测器能够准确地本地化对象。</li>
<li>methods: 本研究使用Vision Transformer（Transformer）来解决partial activation问题，并通过自动注意力机制获取长距离特征依赖关系。此外，还提出了一种本地匹配策略，通过对局部图像进行洗混，保证全局一致性。</li>
<li>results: 经验结果表明，我们的方法可以在CUB-200-2011和ILSVRC datasets上达到新的状态级表现，与之前的方法相比具有显著的超越。<details>
<summary>Abstract</summary>
Weakly supervised object localization (WSOL) strives to learn to localize objects with only image-level supervision. Due to the local receptive fields generated by convolution operations, previous CNN-based methods suffer from partial activation issues, concentrating on the object's discriminative part instead of the entire entity scope. Benefiting from the capability of the self-attention mechanism to acquire long-range feature dependencies, Vision Transformer has been recently applied to alleviate the local activation drawbacks. However, since the transformer lacks the inductive localization bias that are inherent in CNNs, it may cause a divergent activation problem resulting in an uncertain distinction between foreground and background. In this work, we proposed a novel Semantic-Constraint Matching Network (SCMN) via a transformer to converge on the divergent activation. Specifically, we first propose a local patch shuffle strategy to construct the image pairs, disrupting local patches while guaranteeing global consistency. The paired images that contain the common object in spatial are then fed into the Siamese network encoder. We further design a semantic-constraint matching module, which aims to mine the co-object part by matching the coarse class activation maps (CAMs) extracted from the pair images, thus implicitly guiding and calibrating the transformer network to alleviate the divergent activation. Extensive experimental results conducted on two challenging benchmarks, including CUB-200-2011 and ILSVRC datasets show that our method can achieve the new state-of-the-art performance and outperform the previous method by a large margin.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的半监督物体地址 localization（WSOL）方法，它的目的是通过只有图像级别的指导来学习地址物体。由于图像 convolution 操作生成的局部感知范围，前一代 CNN 基于方法容易出现部分活跃问题，即将对象的特征部分进行激活而不是整个实体范围。在应用自注意机制可以获得长范围特征依赖关系的情况下，我们使用了 Vision Transformer 来缓解本地活动问题。然而，由于 transformer 缺乏 CNN 中带有适应性的 inductive 地址偏好，可能会导致不确定的背景和前景分割。为了解决这个问题，我们提出了一种新的 Semantic-Constraint Matching Network（SCMN），通过 transformer 来实现对分歧的激活的控制。具体来说，我们首先提出了一种本地小块洗版策略，用于构建图像对。这种策略可以在保证全局一致性的情况下，对图像进行局部洗版。然后，我们将这些包含共同物体的图像对feed到 Siamese 网络Encoder 中。我们还设计了一种 semantic-constraint 匹配模块，该模块的目的是通过匹配 CAMs 提取自对图像对中的共同部分，从而隐式地引导和调整 transformer 网络，以缓解分歧的激活。我们在 CUB-200-2011 和 ILSVRC 数据集上进行了广泛的实验，结果表明，我们的方法可以达到新的状态码性能，并将之前的方法超越。
</details></li>
</ul>
<hr>
<h2 id="SKoPe3D-A-Synthetic-Dataset-for-Vehicle-Keypoint-Perception-in-3D-from-Traffic-Monitoring-Cameras"><a href="#SKoPe3D-A-Synthetic-Dataset-for-Vehicle-Keypoint-Perception-in-3D-from-Traffic-Monitoring-Cameras" class="headerlink" title="SKoPe3D: A Synthetic Dataset for Vehicle Keypoint Perception in 3D from Traffic Monitoring Cameras"></a>SKoPe3D: A Synthetic Dataset for Vehicle Keypoint Perception in 3D from Traffic Monitoring Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01324">http://arxiv.org/abs/2309.01324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Himanshu Pahadia, Duo Lu, Bharatesh Chakravarthi, Yezhou Yang<br>for:SKoPe3D is a synthetic vehicle keypoint dataset generated using the CARLA simulator, aiming to address the challenges of vehicle keypoint detection in vision-based vehicle monitoring for ITS.methods:The dataset includes generated images with bounding boxes, tracking IDs, and 33 keypoints for each vehicle, spanning over 25k images across 28 scenes with over 150k vehicle instances and 4.9 million keypoints.results:The dataset has the potential to enable advancements in vehicle keypoint detection for ITS, as demonstrated by training a keypoint R-CNN model on the dataset and conducting a thorough evaluation. The dataset’s applicability and the potential for knowledge transfer between synthetic and real-world data are highlighted.<details>
<summary>Abstract</summary>
Intelligent transportation systems (ITS) have revolutionized modern road infrastructure, providing essential functionalities such as traffic monitoring, road safety assessment, congestion reduction, and law enforcement. Effective vehicle detection and accurate vehicle pose estimation are crucial for ITS, particularly using monocular cameras installed on the road infrastructure. One fundamental challenge in vision-based vehicle monitoring is keypoint detection, which involves identifying and localizing specific points on vehicles (such as headlights, wheels, taillights, etc.). However, this task is complicated by vehicle model and shape variations, occlusion, weather, and lighting conditions. Furthermore, existing traffic perception datasets for keypoint detection predominantly focus on frontal views from ego vehicle-mounted sensors, limiting their usability in traffic monitoring. To address these issues, we propose SKoPe3D, a unique synthetic vehicle keypoint dataset generated using the CARLA simulator from a roadside perspective. This comprehensive dataset includes generated images with bounding boxes, tracking IDs, and 33 keypoints for each vehicle. Spanning over 25k images across 28 scenes, SKoPe3D contains over 150k vehicle instances and 4.9 million keypoints. To demonstrate its utility, we trained a keypoint R-CNN model on our dataset as a baseline and conducted a thorough evaluation. Our experiments highlight the dataset's applicability and the potential for knowledge transfer between synthetic and real-world data. By leveraging the SKoPe3D dataset, researchers and practitioners can overcome the limitations of existing datasets, enabling advancements in vehicle keypoint detection for ITS.
</details>
<details>
<summary>摘要</summary>
现代交通基础设施中的智能交通系统（ITS）已经革命化了现代交通基础设施，提供了重要的功能，如交通监测、路安全评估、减压和法律执法。在视觉基础上，精准的车辆检测和车辆位置估计是ITS的关键，特别是使用路边安装的单目camera。车辆特征和形态变化、遮挡、天气和照明条件会增加车辆检测的复杂度。此外，现有的交通感知数据集主要集中在前视角，限制了它们的应用范围。为解决这些问题，我们提出了SKoPe3D数据集，这是一个基于CARLA simulate器生成的路边视角的唯一的车辆关键点数据集。这个全面的数据集包括生成的图像、 bounding box、跟踪ID和33个关键点，涵盖了25000多张图像、28个场景，共计150000辆车辆和4900万个关键点。为证明其可用性，我们在我们的数据集上训练了一个关键点R-CNN模型，并进行了系统性的评估。我们的实验表明，SKoPe3D数据集可以应用于车辆关键点检测，并且可以在实际数据上传递知识。通过利用SKoPe3D数据集，研究人员和实践者可以超越现有数据集的限制，促进车辆关键点检测的进步，以推动ITS的发展。
</details></li>
</ul>
<hr>
<h2 id="FAU-Net-An-Attention-U-Net-Extension-with-Feature-Pyramid-Attention-for-Prostate-Cancer-Segmentation"><a href="#FAU-Net-An-Attention-U-Net-Extension-with-Feature-Pyramid-Attention-for-Prostate-Cancer-Segmentation" class="headerlink" title="FAU-Net: An Attention U-Net Extension with Feature Pyramid Attention for Prostate Cancer Segmentation"></a>FAU-Net: An Attention U-Net Extension with Feature Pyramid Attention for Prostate Cancer Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01322">http://arxiv.org/abs/2309.01322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Cesar Quihui-Rubio, Daniel Flores-Araiza, Miguel Gonzalez-Mendoza, Christian Mata, Gilberto Ochoa-Ruiz</li>
<li>for: 这篇论文是为了提出一种基于深度学习的抑制肾脏区域分 segmentation 方法，以提高肾脏癌检测和诊断的工作流程。</li>
<li>methods: 该方法使用 U-Net 网络结合 additive 和 feature pyramid attention 模块，以提高分 segmentation 精度。</li>
<li>results: 比较 seven 种不同的 U-Net 架构，提出的方法在测试集中 achieved  mean DSC 84.15% 和 IoU 76.9%，与大多数研究模型相比，只有 R2U-Net 和 attention R2U-Net 架构超越。<details>
<summary>Abstract</summary>
This contribution presents a deep learning method for the segmentation of prostate zones in MRI images based on U-Net using additive and feature pyramid attention modules, which can improve the workflow of prostate cancer detection and diagnosis. The proposed model is compared to seven different U-Net-based architectures. The automatic segmentation performance of each model of the central zone (CZ), peripheral zone (PZ), transition zone (TZ) and Tumor were evaluated using Dice Score (DSC), and the Intersection over Union (IoU) metrics. The proposed alternative achieved a mean DSC of 84.15% and IoU of 76.9% in the test set, outperforming most of the studied models in this work except from R2U-Net and attention R2U-Net architectures.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "prostate zones" is translated as "陌生区域" (pinyin: zhèng xìng qū yù)* "MRI images" is translated as "MRI图像" (pinyin: MRI tú xiàng)* "U-Net" is translated as "U-Net" (pinyin: Yù nét)* "additive and feature pyramid attention modules" is translated as "加法和特征层 pyramid 注意模块" (pinyin: jiā fàng yǔ tiě xìng piào qián yǐng module)* "can improve the workflow of prostate cancer detection and diagnosis" is translated as "可以改善陌生癌病检测和诊断的工作流程" (pinyin: kě yǐ gǎi shàn zhèng xìng ài yì jīng yì gòng zuò liú xíng)* "the proposed model" is translated as "提议的模型" (pinyin: tím yì de mó del)* "seven different U-Net-based architectures" is translated as "七种不同的 U-Net 基于架构" (pinyin: qī zhǒng bù dìng de U-Net bìng yù jià gòng)* "automatic segmentation performance" is translated as "自动分割性能" (pinyin: zì dìan fēn xiǎn yè nuò)* "Dice Score (DSC)" is translated as "Dice Score (DSC)" (pinyin: Dice Score (DSC))* "Intersection over Union (IoU)" is translated as "交叠率 (IoU)" (pinyin: jiāo fù rátio (IoU))* "test set" is translated as "测试集" (pinyin: cè shì jí)* "outperforming most of the studied models" is translated as "超越大多数研究的模型" (pinyin: chāo yú dà duō shù yán jí de mó del)* "except for R2U-Net and attention R2U-Net architectures" is translated as "除了 R2U-Net 和注意 R2U-Net 架构" (pinyin: chú le R2U-Net hé zhù yì R2U-Net jià gòng)
</details></li>
</ul>
<hr>
<h2 id="An-FPGA-smart-camera-implementation-of-segmentation-models-for-drone-wildfire-imagery"><a href="#An-FPGA-smart-camera-implementation-of-segmentation-models-for-drone-wildfire-imagery" class="headerlink" title="An FPGA smart camera implementation of segmentation models for drone wildfire imagery"></a>An FPGA smart camera implementation of segmentation models for drone wildfire imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01318">http://arxiv.org/abs/2309.01318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Guarduño-Martinez, Jorge Ciprian-Sanchez, Gerardo Valente, Vazquez-Garcia, Gerardo Rodriguez-Hernandez, Adriana Palacios-Rosas, Lucile Rossi-Tisson, Gilberto Ochoa-Ruiz</li>
<li>for: 这个研究旨在开发一个可行的、低功耗的计算机构架，以实现在无人机上进行火灾探测和评估。</li>
<li>methods: 这个研究使用了智能相机，基于低功耗的可程式逻辑阵列（FPGAs），并与二进制神经网络（BNNs）结合，以实现在边缘计算上的高效执行。</li>
<li>results: 研究人员透过优化和对缩减原始模型的实现，实现了从8帧每秒（FPS）提高至33.63 FPS的速度提升，而且无损于标注性能：模型在沃尔夫-科赫茨曼统计指标（MCC）、F1分数和哈菲安质量指标（HAF）中获得0.912、0.915和0.870的分数，并且与原始全精度模型的标注结果相似。<details>
<summary>Abstract</summary>
Wildfires represent one of the most relevant natural disasters worldwide, due to their impact on various societal and environmental levels. Thus, a significant amount of research has been carried out to investigate and apply computer vision techniques to address this problem. One of the most promising approaches for wildfire fighting is the use of drones equipped with visible and infrared cameras for the detection, monitoring, and fire spread assessment in a remote manner but in close proximity to the affected areas. However, implementing effective computer vision algorithms on board is often prohibitive since deploying full-precision deep learning models running on GPU is not a viable option, due to their high power consumption and the limited payload a drone can handle. Thus, in this work, we posit that smart cameras, based on low-power consumption field-programmable gate arrays (FPGAs), in tandem with binarized neural networks (BNNs), represent a cost-effective alternative for implementing onboard computing on the edge. Herein we present the implementation of a segmentation model applied to the Corsican Fire Database. We optimized an existing U-Net model for such a task and ported the model to an edge device (a Xilinx Ultra96-v2 FPGA). By pruning and quantizing the original model, we reduce the number of parameters by 90%. Furthermore, additional optimizations enabled us to increase the throughput of the original model from 8 frames per second (FPS) to 33.63 FPS without loss in the segmentation performance: our model obtained 0.912 in Matthews correlation coefficient (MCC),0.915 in F1 score and 0.870 in Hafiane quality index (HAF), and comparable qualitative segmentation results when contrasted to the original full-precision model. The final model was integrated into a low-cost FPGA, which was used to implement a neural network accelerator.
</details>
<details>
<summary>摘要</summary>
野火是全球最重要的自然灾害之一，它对社会和环境层次产生了深远的影响。因此，许多研究已经进行了，以应用计算机见识技术来解决这个问题。一种最具吸引力的方法是使用具有可见光和红外线摄像头的无人机，以无人机遥测、监控和评估野火传播的方式进行远程监控，但是在邻近灾区进行这些操作。然而，实现有效的计算机见识算法在无人机上是经常不可能的，因为将全精度深度学习模型在GPU上运行是不可避免的，因为它们的电力消耗量太高，无人机的载重量也是有限的。因此，在这个工作中，我们认为智能相机，基于低功耗的场程可程式阵列（FPGAs），与二进制神经网络（BNNs）共同构成了一个可行的选择。我们在这里显示了对 corsica 火灾数据库进行分类模型的实现。我们修改了现有的 U-Net 模型，并将模型转移到边缘设备（Xilinx Ultra96-v2 FPGA）上。通过剪裁和数值化原始模型，我们缩减了模型的参数数量，从8帧/秒降至33.63帧/秒，并维持了分类性能的稳定。我们的模型在 Matthews 相互关联系数（MCC）、F1 分数（F1）和 Hafiane 质量指数（HAF）中获得了0.912、0.915和0.870的数据，并且在与原始全精度模型进行比较时，获得了相似的分类结果。最终模型被集成到低成本 FPGA 上，实现了一个神经网络加速器。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Automated-and-Early-Detection-of-Alzheimer’s-Disease-Using-Out-Of-Distribution-Detection"><a href="#Enhancing-Automated-and-Early-Detection-of-Alzheimer’s-Disease-Using-Out-Of-Distribution-Detection" class="headerlink" title="Enhancing Automated and Early Detection of Alzheimer’s Disease Using Out-Of-Distribution Detection"></a>Enhancing Automated and Early Detection of Alzheimer’s Disease Using Out-Of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01312">http://arxiv.org/abs/2309.01312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Audrey Paleczny, Shubham Parab, Maxwell Zhang</li>
<li>for: 预测老年人群中的阿尔ц海默病患者（age 65 and older），以便提供早期诊断和治疗。</li>
<li>methods: 使用深度学习模型（Convolutional Neural Network，CNN）和磁共振成像（Magnetic Resonance Imaging，MRI）进行诊断。</li>
<li>results: 使用OOD检测技术可以减少假阳性诊断，提高诊断的可靠性。模型基于CNN结果的检测精度为98%，分类精度为95%，超过了基于分割体积模型的检测和分类精度（93%和87%）。<details>
<summary>Abstract</summary>
More than 10.7% of people aged 65 and older are affected by Alzheimer's disease. Early diagnosis and treatment are crucial as most Alzheimer's patients are unaware of having it until the effects become detrimental. AI has been known to use magnetic resonance imaging (MRI) to diagnose Alzheimer's. However, models which produce low rates of false diagnoses are critical to prevent unnecessary treatments. Thus, we trained supervised Random Forest models with segmented brain volumes and Convolutional Neural Network (CNN) outputs to classify different Alzheimer's stages. We then applied out-of-distribution (OOD) detection to the CNN model, enabling it to report OOD if misclassification is likely, thereby reducing false diagnoses. With an accuracy of 98% for detection and 95% for classification, our model based on CNN results outperformed our segmented volume model, which had detection and classification accuracies of 93% and 87%, respectively. Applying OOD detection to the CNN model enabled it to flag brain tumor images as OOD with 96% accuracy and minimal overall accuracy reduction. By using OOD detection to enhance the reliability of MRI classification using CNNs, we lowered the rate of false positives and eliminated a significant disadvantage of using Machine Learning models for healthcare tasks. Source code available upon request.
</details>
<details>
<summary>摘要</summary>
更多于10.7%的人年龄在65岁及以上有患阿尔茨海默病。早期诊断和治疗是非常重要，因为大多数阿尔茨海默病患者并不知道自己患病 until the effects become detrimental。人工智能可以使用磁共振成像（MRI）进行诊断。然而，模型生成低False Positive率是非常重要，以避免不必要的治疗。因此，我们使用了监督式Random Forest模型，并将Convolutional Neural Network（CNN）输出与分割的脑部volume进行类别。我们然后将CNN模型应用到OOD检测，以便如果误分类可能，则报告OOD，从而降低了False Positive率。使用OOD检测可以提高MRI类别的可靠性，并且使用CNN模型进行健康任务的应用中，消除了一个重要的缺点。代码可以在请求时提供。
</details></li>
</ul>
<hr>
<h2 id="EMR-MSF-Self-Supervised-Recurrent-Monocular-Scene-Flow-Exploiting-Ego-Motion-Rigidity"><a href="#EMR-MSF-Self-Supervised-Recurrent-Monocular-Scene-Flow-Exploiting-Ego-Motion-Rigidity" class="headerlink" title="EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity"></a>EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01296">http://arxiv.org/abs/2309.01296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Jiang, Masatoshi Okutomi</li>
<li>for: 本研究旨在提高现有自监学习Scene Flow estimation方法的精度，通过借鉴超vised学习方法的优点，并在减少动态区域的影响下提高固有的姿态稳定性。</li>
<li>methods: 我们提出了一种名为EMR-MSF的改进模型，其中包括在构建ego-motion汇集模块时引入explicit和稳定的几何约束，以及在满足固有的姿态稳定性下使用mask正则化损失。此外，我们还提出了一种运动一致损失和mask正则化损失，以全面发挥静态区域的作用。</li>
<li>results: 我们的提posed方法在KITTIScene Flow benchmark上表现出色，与state-of-the-art自监学习monocular方法的SF-all指标相比，提高44%。此外，我们的方法在深度和视觉征迹等子任务中也达到了supervised方法的水平。<details>
<summary>Abstract</summary>
Self-supervised monocular scene flow estimation, aiming to understand both 3D structures and 3D motions from two temporally consecutive monocular images, has received increasing attention for its simple and economical sensor setup. However, the accuracy of current methods suffers from the bottleneck of less-efficient network architecture and lack of motion rigidity for regularization. In this paper, we propose a superior model named EMR-MSF by borrowing the advantages of network architecture design under the scope of supervised learning. We further impose explicit and robust geometric constraints with an elaborately constructed ego-motion aggregation module where a rigidity soft mask is proposed to filter out dynamic regions for stable ego-motion estimation using static regions. Moreover, we propose a motion consistency loss along with a mask regularization loss to fully exploit static regions. Several efficient training strategies are integrated including a gradient detachment technique and an enhanced view synthesis process for better performance. Our proposed method outperforms the previous self-supervised works by a large margin and catches up to the performance of supervised methods. On the KITTI scene flow benchmark, our approach improves the SF-all metric of the state-of-the-art self-supervised monocular method by 44% and demonstrates superior performance across sub-tasks including depth and visual odometry, amongst other self-supervised single-task or multi-task methods.
</details>
<details>
<summary>摘要</summary>
自我监督单目场景流估算，寻求从两个 consecutively temporally 单目图像中理解三维结构和三维运动。由于现有方法的精度受到网络架构的瓶颈和运动稳定性的限制，因此在这篇论文中，我们提出了一种高效的模型 named EMR-MSF。我们采用了指导了supervised学习中网络架构的优点，并在 elaborate 构建了自身运动汇集模块，在这里我们提出了一种坚定性软面罩来过滤动态区域，以确保稳定的自身运动估算。此外，我们还提出了运动一致损失和面罩规范损失，以便充分利用静止区域。我们还 интегрирова了一些高效的训练策略，包括梯度分离技术和改进的视图合成过程，以提高表现。根据 KITTI 场景流标准测试集，我们的方法在自我监督单目方法中提高了 SF-all 指标44%，并在深度和视觉速度等子任务中表现出色，在其他自我监督单任务或多任务方法中也达到了类似的水平。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.CV_2023_09_04/" data-id="clorjzl6j00huf188dz5ffah9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.AI_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T12:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.AI_2023_09_04/">cs.AI - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Defense-Against-Model-Stealing-Attacks-on-Convolutional-Neural-Networks"><a href="#Efficient-Defense-Against-Model-Stealing-Attacks-on-Convolutional-Neural-Networks" class="headerlink" title="Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks"></a>Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01838">http://arxiv.org/abs/2309.01838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kacemkhaled/defending-extraction">https://github.com/kacemkhaled/defending-extraction</a></li>
<li>paper_authors: Kacem Khaled, Mouna Dhaouadi, Felipe Gohring de Magalhães, Gabriela Nicolescu</li>
<li>For: The paper is written to propose a simple yet effective and efficient defense against model stealing attacks for deep learning models.* Methods: The paper introduces a heuristic approach to perturb the output probabilities of the model to defend against stealing attacks, which can be easily integrated into models without additional training.* Results: The proposed defense is effective in defending against three state-of-the-art stealing attacks, and outperforms the state-of-the-art defenses with a $\times37$ faster inference latency without requiring any additional model and with a low impact on the model’s performance. The defense is also effective for quantized CNNs targeting edge devices.Here’s the same information in Simplified Chinese text:* For: 这篇论文是为了提出一种简单又有效的模型盗用攻击防御方案。* Methods: 论文提出一种基于归类抽象的方法，通过对模型输出概率进行扰动来防御盗用攻击。这种方法可以轻松地与现有模型集成，无需进行额外训练。* Results: 提出的防御方法有效地防止了三种state-of-the-art的盗用攻击，并且比现有的防御方法快速37倍，不需要额外的模型和占用较低的模型性能。此外，这种防御方法也适用于采用量化（即压缩）的卷积神经网络（CNN）和边缘设备。<details>
<summary>Abstract</summary>
Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a $\times37$ faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks.We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a $\times37$ faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.
</details></li>
</ul>
<hr>
<h2 id="Smoothing-ADMM-for-Sparse-Penalized-Quantile-Regression-with-Non-Convex-Penalties"><a href="#Smoothing-ADMM-for-Sparse-Penalized-Quantile-Regression-with-Non-Convex-Penalties" class="headerlink" title="Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties"></a>Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03094">http://arxiv.org/abs/2309.03094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Mirzaeifard, Naveen K. D. Venkategowda, Vinay Chakravarthi Gogineni, Stefan Werner</li>
<li>for: 这paper investigates quantile regression in the presence of non-convex and non-smooth sparse penalties, and proposes a novel single-loop smoothing ADMM algorithm named SIAD to accelerate the convergence speed.</li>
<li>methods: 该paper使用了iterative techniques like coordinate descent and local linear approximation, as well as the alternating direction method of multipliers (ADMM) to facilitate convergence.</li>
<li>results: 数据表示SIAD方法比现有方法更快和稳定，提供了更好的解决方案 для sparse-penalized quantile regression。<details>
<summary>Abstract</summary>
This paper investigates quantile regression in the presence of non-convex and non-smooth sparse penalties, such as the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). The non-smooth and non-convex nature of these problems often leads to convergence difficulties for many algorithms. While iterative techniques like coordinate descent and local linear approximation can facilitate convergence, the process is often slow. This sluggish pace is primarily due to the need to run these approximation techniques until full convergence at each step, a requirement we term as a \emph{secondary convergence iteration}. To accelerate the convergence speed, we employ the alternating direction method of multipliers (ADMM) and introduce a novel single-loop smoothing ADMM algorithm with an increasing penalty parameter, named SIAD, specifically tailored for sparse-penalized quantile regression. We first delve into the convergence properties of the proposed SIAD algorithm and establish the necessary conditions for convergence. Theoretically, we confirm a convergence rate of $o\big({k^{-\frac{1}{4}}\big)$ for the sub-gradient bound of augmented Lagrangian. Subsequently, we provide numerical results to showcase the effectiveness of the SIAD algorithm. Our findings highlight that the SIAD method outperforms existing approaches, providing a faster and more stable solution for sparse-penalized quantile regression.
</details>
<details>
<summary>摘要</summary>
To improve the convergence speed, we use the alternating direction method of multipliers (ADMM) and develop a new single-loop smoothing ADMM algorithm called SIAD. We prove that the SIAD algorithm converges at a rate of $o\big({k^{-\frac{1}{4}}\big)$ for the sub-gradient bound of the augmented Lagrangian.We also conduct numerical experiments to compare the performance of the SIAD algorithm with other methods. Our results show that the SIAD method outperforms existing approaches, providing a faster and more stable solution for sparse-penalized quantile regression.
</details></li>
</ul>
<hr>
<h2 id="One-Wide-Feedforward-is-All-You-Need"><a href="#One-Wide-Feedforward-is-All-You-Need" class="headerlink" title="One Wide Feedforward is All You Need"></a>One Wide Feedforward is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01826">http://arxiv.org/abs/2309.01826</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Telmo Pessoa Pires, António V. Lopes, Yannick Assogba, Hendra Setiawan</li>
<li>for: 这个论文的目的是探究Transformer架构中的Feed Forward Network（FFN） redundancy，以及如何通过减少FFN的参数数量来提高模型的准确率和响应时间。</li>
<li>methods: 这个论文使用了Transformer架构，并对其中的FFN进行了探究和优化。特别是， authors 发现了FFN的重复性，并通过在解码器层上移除FFN来减少参数数量。此外， authors 还将共享一个FFN来替代原始Transformer Big中的多个FFN，以提高准确率和响应时间。</li>
<li>results: 根据实验结果， authors 发现了减少FFN参数数量可以 achieving substantial gains in both accuracy and latency with respect to the original Transformer Big。具体来说， authors 通过减少解码器层的FFN参数数量，可以提高模型的准确率，同时也可以降低模型的响应时间。<details>
<summary>Abstract</summary>
The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work we explore the role of the FFN, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by removing the FFN on the decoder layers and sharing a single FFN across the encoder. Finally we scale this architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency with respect to the original Transformer Big.
</details>
<details>
<summary>摘要</summary>
transformer 架构有两个主要非嵌入组件：注意力和Feed Forward Network (FFN)。注意力捕捉即使词语位置不同也可以互相依赖的关系，而 FFN 非线性变换每个输入token。在这项工作中，我们研究 FFN 的角色，并发现它占用模型参数的一大部分，但它具有很高的重复率。具体来说，我们可以通过去除decoder层的 FFN，并将encoder中的 FFN 共享来减少参数数量，只有一定的精度下降。最后，我们通过增加共享 FFN 的隐藏维度，实现了对原始 transformer Big 的重大提升 both accuracy和延迟时间。
</details></li>
</ul>
<hr>
<h2 id="Towards-Foundational-AI-Models-for-Additive-Manufacturing-Language-Models-for-G-Code-Debugging-Manipulation-and-Comprehension"><a href="#Towards-Foundational-AI-Models-for-Additive-Manufacturing-Language-Models-for-G-Code-Debugging-Manipulation-and-Comprehension" class="headerlink" title="Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension"></a>Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02465">http://arxiv.org/abs/2309.02465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idealab-isu/llm4g-code">https://github.com/idealab-isu/llm4g-code</a></li>
<li>paper_authors: Anushrut Jignasu, Kelly Marshall, Baskar Ganapathysubramanian, Aditya Balu, Chinmay Hegde, Adarsh Krishnamurthy</li>
<li>for: 这篇论文旨在描述如何使用现有的大型自然语言模型（LLMs）来理解和修改3D打印机的G-code文件。</li>
<li>methods: 论文使用了六种现有的LLMs，并设计了有效的提示来让这些模型理解和操纵G-code文件。</li>
<li>results: 论文对六种LLMs的性能进行了全面的评估，并分析了它们对完整G-code文件的理解的优劣点。<details>
<summary>Abstract</summary>
3D printing or additive manufacturing is a revolutionary technology that enables the creation of physical objects from digital models. However, the quality and accuracy of 3D printing depend on the correctness and efficiency of the G-code, a low-level numerical control programming language that instructs 3D printers how to move and extrude material. Debugging G-code is a challenging task that requires a syntactic and semantic understanding of the G-code format and the geometry of the part to be printed. In this paper, we present the first extensive evaluation of six state-of-the-art foundational large language models (LLMs) for comprehending and debugging G-code files for 3D printing. We design effective prompts to enable pre-trained LLMs to understand and manipulate G-code and test their performance on various aspects of G-code debugging and manipulation, including detection and correction of common errors and the ability to perform geometric transformations. We analyze their strengths and weaknesses for understanding complete G-code files. We also discuss the implications and limitations of using LLMs for G-code comprehension.
</details>
<details>
<summary>摘要</summary>
三维打印或加itive制造是一种革命性的技术，允许将数字模型转化为物理 объек的创造。然而，三维打印的质量和准确性取决于G-code的正确性和效率，G-code是一种低级数控制程序语言，用于指示三维打印机如何移动和挤出材料。调试G-code是一项复杂的任务，需要对G-code格式和部件的几何结构具有语义和语法理解。在这篇论文中，我们展示了首次对六种现代基础大语言模型（LLM）的扩展性评估，以便理解和修改G-code文件。我们设计有效的提示，使得预训练的LLM可以理解和操纵G-code，并测试其表现于不同的G-code调试和修改方面，包括常见错误检测和修复以及几何变换能力。我们分析它们在完整G-code文件理解方面的优势和缺陷。我们还讨论了使用LLM进行G-code理解的限制和局限性。
</details></li>
</ul>
<hr>
<h2 id="DiscoverPath-A-Knowledge-Refinement-and-Retrieval-System-for-Interdisciplinarity-on-Biomedical-Research"><a href="#DiscoverPath-A-Knowledge-Refinement-and-Retrieval-System-for-Interdisciplinarity-on-Biomedical-Research" class="headerlink" title="DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research"></a>DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01808">http://arxiv.org/abs/2309.01808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ynchuang/discoverpath">https://github.com/ynchuang/discoverpath</a></li>
<li>paper_authors: Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Kwei-Herng Lai, Daochen Zha, Ruixiang Tang, Fan Yang, Alfredo Costilla Reyes, Kaixiong Zhou, Xiaoqian Jiang, Xia Hu</li>
<li>for: 增进生物医学研究中的文献检索效率，尤其是在跨学科领域中，通过使用知识图来提高用户体验。</li>
<li>methods: 使用命名实体识别（NER）和parts-of-speech（POS）标签来从文章摘要中提取 terminologies和关系，并将其整合成知识图。</li>
<li>results: 提供了一个开源的Graphical User Interface，可以帮助用户查找相关的文章和增进知识探索。<details>
<summary>Abstract</summary>
The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical User Interface that provides an intuitive visualization of the KG, query recommendations, and detailed article information, enabling efficient article retrieval, thus fostering interdisciplinary knowledge exploration. DiscoverPath is open-sourced at https://github.com/ynchuang/DiscoverPath.
</details>
<details>
<summary>摘要</summary>
随着学术论文的激增增长，需要更高级的工具来快速检索相关的论文，特别是在交叉学科领域，where diverse terminologies are used to describe similar research. 传统的关键词基本搜索引擎often fails to assist users who are not familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, called DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a knowledge graph (KG). To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes, and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical User Interface that provides an intuitive visualization of the KG, query recommendations, and detailed article information, enabling efficient article retrieval and thus fostering interdisciplinary knowledge exploration. DiscoverPath is open-sourced at <https://github.com/ynchuang/DiscoverPath>.
</details></li>
</ul>
<hr>
<h2 id="Marginalized-Importance-Sampling-for-Off-Environment-Policy-Evaluation"><a href="#Marginalized-Importance-Sampling-for-Off-Environment-Policy-Evaluation" class="headerlink" title="Marginalized Importance Sampling for Off-Environment Policy Evaluation"></a>Marginalized Importance Sampling for Off-Environment Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01807">http://arxiv.org/abs/2309.01807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pulkit Katdare, Nan Jiang, Katherine Driggs-Campbell</li>
<li>for: 评估实际世界中RL策略的性能，不需要真实世界的部署。</li>
<li>methods: 利用Marginalized Importance Sampling（MIS）框架，通过在模拟器中添加真实世界停留数据，评估RL策略的实际世界性能。</li>
<li>results: 对多种Sim2Sim环境和目标策略，以及不同的停留数据收集策略进行了实际评估，并在Sim2Real任务中评估了一个7度 freedomRobotic臂的性能。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation, requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies without deploying them in the real world. The proposed approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separately. The first term is learned with direct supervision and the second term has a small magnitude, thus making it easier to run. We analyze the sample complexity as well as error propagation of our two step-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim environments such as Cartpole, Reacher and Half-Cheetah. Our results show that our method generalizes well across a variety of Sim2Sim gap, target policies and offline data collection policies. We also demonstrate the performance of our algorithm on a Sim2Real task of validating the performance of a 7 DOF robotic arm using offline data along with a gazebo based arm simulator.
</details>
<details>
<summary>摘要</summary>
《强化学习（RL）方法通常是样本不fficient，使得在实际世界中训练和部署RL策略变得困难。 même a robust策略在simulation中训练，需要在实际世界中进行评估其性能。这篇论文提出了一种新的方法，用于在实际世界中评估代理策略的性能，不需要将其部署到实际世界中。该方法利用了simulator和实际世界的停止数据，通过Marginalized Importance Sampling（MIS）框架评估任何策略的性能。现有MIS方法面临两个挑战：（1）巨大的概率比率，它们与理解范围内的合理范围偏离很大；（2）间接监督，需要间接地估算概率比率，从而使得估计误差加大。我们的方法解决了这两个挑战，通过引入目标策略在simulator中的存在量作为中间变量，将概率比率分解为两个可分别学习的项。第一项通过直接监督学习，第二项具有小的幅度，因此更容易实现。我们还分析了我们的两步程序的样本复杂度以及误差的卷积。此外，我们也进行了Empirical评估，并证明我们的方法在Cartpole、Reacher和Half-Cheetah等Sim2Sim环境中能够具有良好的泛化性。此外，我们还展示了我们的算法在一个Sim2Real任务中，使用了停止数据和Gazebo基于的arm simulator，验证了一个7度OF robotic arm的性能。
</details></li>
</ul>
<hr>
<h2 id="Neural-Singular-Hessian-Implicit-Neural-Representation-of-Unoriented-Point-Clouds-by-Enforcing-Singular-Hessian"><a href="#Neural-Singular-Hessian-Implicit-Neural-Representation-of-Unoriented-Point-Clouds-by-Enforcing-Singular-Hessian" class="headerlink" title="Neural-Singular-Hessian: Implicit Neural Representation of Unoriented Point Clouds by Enforcing Singular Hessian"></a>Neural-Singular-Hessian: Implicit Neural Representation of Unoriented Point Clouds by Enforcing Singular Hessian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01793">http://arxiv.org/abs/2309.01793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bearprin/Neural-Singular-Hessian">https://github.com/bearprin/Neural-Singular-Hessian</a></li>
<li>paper_authors: Zixiong Wang, Yunxiao Zhang, Rui Xu, Fan Zhang, Pengshuai Wang, Shuangmin Chen, Shiqing Xin, Wenping Wang, Changhe Tu</li>
<li>for: 该论文旨在拟合点云数据中的表面 reconstruction 问题。</li>
<li>methods: 该方法combines various regularization terms, such as Eikonal和Laplacian energy terms, to enforce the learned neural function to possess the properties of a Signed Distance Function (SDF)。 In addition, the approach enforces the Hessian of the neural implicit function to have a zero determinant for points near the surface, which aligns the gradients for a near-surface point and its on-surface projection point, producing a rough but faithful shape。</li>
<li>results: 经验表明，该方法可以有效地suppress ghost geometry和recover details from unoriented point clouds with better expressiveness than existing fitting-based methods。<details>
<summary>Abstract</summary>
Neural implicit representation is a promising approach for reconstructing surfaces from point clouds. Existing methods combine various regularization terms, such as the Eikonal and Laplacian energy terms, to enforce the learned neural function to possess the properties of a Signed Distance Function (SDF). However, inferring the actual topology and geometry of the underlying surface from poor-quality unoriented point clouds remains challenging. In accordance with Differential Geometry, the Hessian of the SDF is singular for points within the differential thin-shell space surrounding the surface. Our approach enforces the Hessian of the neural implicit function to have a zero determinant for points near the surface. This technique aligns the gradients for a near-surface point and its on-surface projection point, producing a rough but faithful shape within just a few iterations. By annealing the weight of the singular-Hessian term, our approach ultimately produces a high-fidelity reconstruction result. Extensive experimental results demonstrate that our approach effectively suppresses ghost geometry and recovers details from unoriented point clouds with better expressiveness than existing fitting-based methods.
</details>
<details>
<summary>摘要</summary>
神经隐式表示是一种有前途的方法，用于从点云重建表面。现有方法将各种正则化项相结合，如振荡能量和拉普拉斯能量项，以强制学习神经函数具备签名距离函数的性质。然而，从低质量、无法定向的点云中恢复真实的表面 topology 和几何结构仍然是一个搜索。根据 diferencial geometry，在表面 differential thin-shell 空间中，SDF 的哈密顿矩阵是非特征矩阵。我们的方法强制神经隐式函数的哈密顿矩阵在near surface 点附近为零 determinant。这种技术将near surface 点的梯度与其在表面上的投影点的梯度相对align，生成一个粗糙 yet faithful 的形态，只需几个迭代即可。通过渐进式地减小weight的特征矩阵项，我们的方法最终生成高精度重建结果。广泛的实验结果表明，我们的方法可以有效地抑制幽灵几何和从无法定向的点云中恢复细节，比现有的适应型方法更有表达力。
</details></li>
</ul>
<hr>
<h2 id="3D-View-Prediction-Models-of-the-Dorsal-Visual-Stream"><a href="#3D-View-Prediction-Models-of-the-Dorsal-Visual-Stream" class="headerlink" title="3D View Prediction Models of the Dorsal Visual Stream"></a>3D View Prediction Models of the Dorsal Visual Stream</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01782">http://arxiv.org/abs/2309.01782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Sarch, Hsiao-Yu Fish Tung, Aria Wang, Jacob Prince, Michael Tarr</li>
<li>for: 这个论文是为了测试一种基于3D场景几何的自适应循环神经网络（GRNN）是否能更好地与脑动脉核心视觉区域的功能特性相匹配。</li>
<li>methods: 这个论文使用了一种自适应循环神经网络（GRNN），并使用了一个3D特征记忆来训练这个模型。</li>
<li>results: 研究发现，GRNN能够更好地预测新的摄像头视图，并且对脑动脉核心视觉区域的变化具有更高的准确率。<details>
<summary>Abstract</summary>
Deep neural network representations align well with brain activity in the ventral visual stream. However, the primate visual system has a distinct dorsal processing stream with different functional properties. To test if a model trained to perceive 3D scene geometry aligns better with neural responses in dorsal visual areas, we trained a self-supervised geometry-aware recurrent neural network (GRNN) to predict novel camera views using a 3D feature memory. We compared GRNN to self-supervised baseline models that have been shown to align well with ventral regions using the large-scale fMRI Natural Scenes Dataset (NSD). We found that while the baseline models accounted better for ventral brain regions, GRNN accounted for a greater proportion of variance in dorsal brain regions. Our findings demonstrate the potential for using task-relevant models to probe representational differences across visual streams.
</details>
<details>
<summary>摘要</summary>
深度神经网络表示 aligned well with brain activity in the ventral visual stream. However, the primate visual system has a distinct dorsal processing stream with different functional properties. To test if a model trained to perceive 3D scene geometry aligns better with neural responses in dorsal visual areas, we trained a self-supervised geometry-aware recurrent neural network (GRNN) to predict novel camera views using a 3D feature memory. We compared GRNN to self-supervised baseline models that have been shown to align well with ventral regions using the large-scale fMRI Natural Scenes Dataset (NSD). We found that while the baseline models accounted better for ventral brain regions, GRNN accounted for a greater proportion of variance in dorsal brain regions. Our findings demonstrate the potential for using task-relevant models to probe representational differences across visual streams.Here's the translation in Traditional Chinese:深度神经网络表示 aligned well with brain activity in the ventral visual stream. However, the primate visual system has a distinct dorsal processing stream with different functional properties. To test if a model trained to perceive 3D scene geometry aligns better with neural responses in dorsal visual areas, we trained a self-supervised geometry-aware recurrent neural network (GRNN) to predict novel camera views using a 3D feature memory. We compared GRNN to self-supervised baseline models that have been shown to align well with ventral regions using the large-scale fMRI Natural Scenes Dataset (NSD). We found that while the baseline models accounted better for ventral brain regions, GRNN accounted for a greater proportion of variance in dorsal brain regions. Our findings demonstrate the potential for using task-relevant models to probe representational differences across visual streams.
</details></li>
</ul>
<hr>
<h2 id="On-the-size-of-irredundant-propagation-complete-CNF-formulas"><a href="#On-the-size-of-irredundant-propagation-complete-CNF-formulas" class="headerlink" title="On the size of irredundant propagation complete CNF formulas"></a>On the size of irredundant propagation complete CNF formulas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01750">http://arxiv.org/abs/2309.01750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petr Savický</li>
<li>for: 这个论文 investigate propagation complete (PC) CNF formulas for a symmetric definite Horn function of $n$ variables.</li>
<li>methods: 论文使用了 minimum size of these formulas 与specific covering numbers closely related, specifically, the smallest number of $k$-subsets of an $n$-set covering all $(k-1)$-subsets for a suitable $k$.</li>
<li>results: 论文展示了一个 irredundant PC formula whose size is larger than the size of a smallest PC formula for the same function by a factor $\Omega(n&#x2F;\ln n)$. This complements a known polynomial upper bound on this factor.<details>
<summary>Abstract</summary>
We investigate propagation complete (PC) CNF formulas for a symmetric definite Horn function of $n$ variables and demonstrate that the minimum size of these formulas is closely related to specific covering numbers, namely, to the smallest number of $k$-subsets of an $n$-set covering all $(k-1)$-subsets for a suitable $k$. As a consequence, we demonstrate an irredundant PC formula whose size is larger than the size of a smallest PC formula for the same function by a factor $\Omega(n/\ln n)$. This complements a known polynomial upper bound on this factor.
</details>
<details>
<summary>摘要</summary>
我们调查完整的几何函数（PC）逻辑式，对于一个对称定义的权Func数学函数，并证明这个函数的最小大小与特定的覆盖数字有密切的关系，即最小的$k$-subsets的集合覆盖所有($k-1$)-subsets。我们从这个结果获得了一个不可简的PC方程，其大小比最小PC方程的大小有$\Omega(n/\ln n)$的因子。这与已知的多项式上界有关。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-data-driven-thermal-simulation-model-for-comfort-assessment"><a href="#Hybrid-data-driven-thermal-simulation-model-for-comfort-assessment" class="headerlink" title="Hybrid data driven&#x2F;thermal simulation model for comfort assessment"></a>Hybrid data driven&#x2F;thermal simulation model for comfort assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01734">http://arxiv.org/abs/2309.01734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Barbedienne, Sara Yasmine Ouerk, Mouadh Yagoubi, Hassan Bouia, Aurelie Kaemmerlen, Benoit Charrier</li>
<li>for: 提高物理模型的速度和质量</li>
<li>methods: 结合实际数据和模拟数据预测室内舒适度</li>
<li>results: 使用Random Forest模型 obtain F1 score 0.999 的promising results<details>
<summary>Abstract</summary>
Machine learning models improve the speed and quality of physical models. However, they require a large amount of data, which is often difficult and costly to acquire. Predicting thermal comfort, for example, requires a controlled environment, with participants presenting various characteristics (age, gender, ...). This paper proposes a method for hybridizing real data with simulated data for thermal comfort prediction. The simulations are performed using Modelica Language. A benchmarking study is realized to compare different machine learning methods. Obtained results look promising with an F1 score of 0.999 obtained using the random forest model.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：机器学习模型可以提高物理模型的速度和质量，但它们需要大量数据，而这些数据常常困难和costly to obtain。预测冷凉舒适性需要控制环境，参与者具有不同特征（年龄、性别、...）。这篇论文提议将实际数据与模拟数据相互融合以预测冷凉舒适性。模拟使用Modelica语言进行。实现了不同机器学习方法的比较研究。获得的结果很有前途，使用随机森林模型获得的F1分数为0.999。Note: "简化中文" refers to Simplified Chinese, which is one of the two standardized Chinese writing systems, used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Softmax-Bias-Correction-for-Quantized-Generative-Models"><a href="#Softmax-Bias-Correction-for-Quantized-Generative-Models" class="headerlink" title="Softmax Bias Correction for Quantized Generative Models"></a>Softmax Bias Correction for Quantized Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01729">http://arxiv.org/abs/2309.01729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nilesh Prasad Pandey, Marios Fournarakis, Chirag Patel, Markus Nagel</li>
<li>for: 提高 Edge 设备上大量生成模型的运行时间和功耗效率，包括稳定扩散或大语言模型。</li>
<li>methods:  investigate 软MAX输出强制性对归一化干扰的影响，并提出一种在部署时进行偏差修正，以提高软MAX的量化可行性。</li>
<li>results: 在稳定扩散 v1.5 和 125M-size OPT 语言模型上，实现了8比特量化软MAX后的准确性提高。<details>
<summary>Abstract</summary>
Post-training quantization (PTQ) is the go-to compression technique for large generative models, such as stable diffusion or large language models. PTQ methods commonly keep the softmax activation in higher precision as it has been shown to be very sensitive to quantization noise. However, this can lead to a significant runtime and power overhead during inference on resource-constraint edge devices. In this work, we investigate the source of the softmax sensitivity to quantization and show that the quantization operation leads to a large bias in the softmax output, causing accuracy degradation. To overcome this issue, we propose an offline bias correction technique that improves the quantizability of softmax without additional compute during deployment, as it can be readily absorbed into the quantization parameters. We demonstrate the effectiveness of our method on stable diffusion v1.5 and 125M-size OPT language model, achieving significant accuracy improvement for 8-bit quantized softmax.
</details>
<details>
<summary>摘要</summary>
Post-training quantization (PTQ) 是大型生成模型的压缩技术，如稳定扩散或大语言模型。PTQ方法通常保留软 макс激活函数的高精度，因为它已经被证明对压缩噪声非常敏感。然而，这可能会导致在资源有限的边缘设备中的运行时间和功耗开销增加。在这项工作中，我们研究软 макс激活函数对压缩的敏感性的源头，发现压缩操作会导致软 макс输出中的大量偏差，从而导致准确性下降。为解决这个问题，我们提出了一种离线偏差修正技术，可以在部署过程中对软 макс进行压缩而不需要额外的计算，因为它可以轻松吸收到压缩参数中。我们在稳定扩散 v1.5 和 125M 大小的 OPT 语言模型上进行了实验，并达到了8位压缩软 макс后的显著准确性改进。
</details></li>
</ul>
<hr>
<h2 id="Interdisciplinary-Fairness-in-Imbalanced-Research-Proposal-Topic-Inference-A-Hierarchical-Transformer-based-Method-with-Selective-Interpolation"><a href="#Interdisciplinary-Fairness-in-Imbalanced-Research-Proposal-Topic-Inference-A-Hierarchical-Transformer-based-Method-with-Selective-Interpolation" class="headerlink" title="Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation"></a>Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01717">http://arxiv.org/abs/2309.01717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Xiao, Min Wu, Ziyue Qiao, Yanjie Fu, Zhiyuan Ning, Yi Du, Yuanchun Zhou</li>
<li>for: 提高自动话题推荐系统的公平性，解决由于人工填写话题导致的偏误和不公平现象。</li>
<li>methods: 基于Transformerencoder-decoder架构实现话题标签推论系统，并利用 interpolate技术生成pseudo-交叉学科提案，以减少模型训练时的偏误。</li>
<li>results: 在实际数据集上进行了广泛的实验，研究结果表明，提posed方法可以减少自动话题推论任务中的不公平现象。<details>
<summary>Abstract</summary>
The objective of topic inference in research proposals aims to obtain the most suitable disciplinary division from the discipline system defined by a funding agency. The agency will subsequently find appropriate peer review experts from their database based on this division. Automated topic inference can reduce human errors caused by manual topic filling, bridge the knowledge gap between funding agencies and project applicants, and improve system efficiency. Existing methods focus on modeling this as a hierarchical multi-label classification problem, using generative models to iteratively infer the most appropriate topic information. However, these methods overlook the gap in scale between interdisciplinary research proposals and non-interdisciplinary ones, leading to an unjust phenomenon where the automated inference system categorizes interdisciplinary proposals as non-interdisciplinary, causing unfairness during the expert assignment. How can we address this data imbalance issue under a complex discipline system and hence resolve this unfairness? In this paper, we implement a topic label inference system based on a Transformer encoder-decoder architecture. Furthermore, we utilize interpolation techniques to create a series of pseudo-interdisciplinary proposals from non-interdisciplinary ones during training based on non-parametric indicators such as cross-topic probabilities and topic occurrence probabilities. This approach aims to reduce the bias of the system during model training. Finally, we conduct extensive experiments on a real-world dataset to verify the effectiveness of the proposed method. The experimental results demonstrate that our training strategy can significantly mitigate the unfairness generated in the topic inference task.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:研究主题推断在研究提案中的目标是通过funding机构定义的学科系统中获得最适合的学科分类。该机构将根据此分类找到相应的专家评审人员从其数据库中。自动化主题推断可以降低人类手动填充主题的错误，跨学科研究提案和非跨学科研究提案之间的知识差距，并提高系统效率。现有方法是将这视为一个层次多个标签的分类问题，使用生成模型iteratively推断最有利的主题信息。然而，这些方法忽略了跨学科研究提案和非跨学科研究提案之间的规模差异，导致自动推断系统将跨学科研究提案分类为非跨学科研究提案，从而导致了对专家分配的不公正。如何在复杂的学科系统下解决这种数据不匹配问题，从而解决这种不公正呢？在这篇论文中，我们实现了一个基于Transformer编码器-解码器架构的主题标签推断系统。此外，我们使用 interpolate技术在训练期间创建一系列 pseudo-跨学科提案从非跨学科提案中，基于非 Parametric indicator such as cross-topic probabilities和主题发生概率。这种方法 aimsto reduce the bias of the system during model training.最后，我们对实际数据进行了广泛的实验，以验证提案的有效性。实验结果表明，我们的训练策略可以明显减少自动推断 task中的不公正。
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Post-hoc-GNN-Explainers-to-Label-Noise"><a href="#On-the-Robustness-of-Post-hoc-GNN-Explainers-to-Label-Noise" class="headerlink" title="On the Robustness of Post-hoc GNN Explainers to Label Noise"></a>On the Robustness of Post-hoc GNN Explainers to Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01706">http://arxiv.org/abs/2309.01706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Zhong, Yangqianzi Jiang, Davide Mottin</li>
<li>for: 本研究旨在探讨post-hoc图 neural network（GNN）解释器在受损标签情况下的可靠性。</li>
<li>methods: 研究使用了多种post-hoc GNN解释器，并在不同的标签噪声水平进行了系统性的实验研究。</li>
<li>results: 研究发现，post-hoc GNN解释器具有抗受损性，但是即使标签噪声较低，解释器也会受到影响，解释质量下降。同时，研究还发现，随着标签噪声水平的增加，解释器的效果逐渐恢复。<details>
<summary>Abstract</summary>
Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.
</details>
<details>
<summary>摘要</summary>
提议作为图 neural network（GNN）的黑盒限制解决方案，post-hoc GNN 解释器尝试提供准确和有 insightful 的 GNN 行为解释。尽管在学术和工业上最近有所进步，但post-hoc GNN 解释器的Robustness 仍未被探索，对于标签噪声的情况。为了bridging这个差距，我们进行了系统性的实验研究，评估不同的 post-hoc GNN 解释器在不同的标签噪声水平下的效果。我们的结果显示了以下几点：一、post-hoc GNN 解释器受到标签变动的影响。二、即使标签噪声非常低，也会对 GNN 性能造成很大的影响。三、随着噪声水平的增加，解释效果逐渐恢复。
</details></li>
</ul>
<hr>
<h2 id="No-Data-Augmentation-Alternative-Regularizations-for-Effective-Training-on-Small-Datasets"><a href="#No-Data-Augmentation-Alternative-Regularizations-for-Effective-Training-on-Small-Datasets" class="headerlink" title="No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets"></a>No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01694">http://arxiv.org/abs/2309.01694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Brigato, Stavroula Mougiakakou</li>
<li>for: 解决现代计算机视觉中小训练集图像分类任务的问题</li>
<li>methods: 使用各种敏感训练策略和模型尺度、训练时间表等参数的调整</li>
<li>results: 在 solely 1% of the original CIFAR-10 training set (i.e., 50 images per class) 和 ciFAIR-10 测试集上达到了 66.5% 的测试精度，与现状最佳方法相当。<details>
<summary>Abstract</summary>
Solving image classification tasks given small training datasets remains an open challenge for modern computer vision. Aggressive data augmentation and generative models are among the most straightforward approaches to overcoming the lack of data. However, the first fails to be agnostic to varying image domains, while the latter requires additional compute and careful design. In this work, we study alternative regularization strategies to push the limits of supervised learning on small image classification datasets. In particular, along with the model size and training schedule scaling, we employ a heuristic to select (semi) optimal learning rate and weight decay couples via the norm of model parameters. By training on only 1% of the original CIFAR-10 training set (i.e., 50 images per class) and testing on ciFAIR-10, a variant of the original CIFAR without duplicated images, we reach a test accuracy of 66.5%, on par with the best state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉中解决小训练集数据的图像分类任务仍然是一个开放的挑战。非常的数据扩展和生成模型是最直接的方法来缓解缺乏数据的问题，但是前者不具备适应不同图像领域的特性，而后者需要额外的计算和精心的设计。在这项工作中，我们研究了不同于supervised学习的regularization策略，以推动小图像分类集数据上的模型训练。特别是，我们采用一种heuristic来选择（semi）优化的学习率和权重衰减couple，通过模型参数的norm来实现。通过使用原始CIFAR-10训练集的1%（即50张每个类）和测试在ciFAIR-10上，我们达到了66.5%的测试精度，与状态元的方法相当。
</details></li>
</ul>
<hr>
<h2 id="Prompt-me-a-Dataset-An-investigation-of-text-image-prompting-for-historical-image-dataset-creation-using-foundation-models"><a href="#Prompt-me-a-Dataset-An-investigation-of-text-image-prompting-for-historical-image-dataset-creation-using-foundation-models" class="headerlink" title="Prompt me a Dataset: An investigation of text-image prompting for historical image dataset creation using foundation models"></a>Prompt me a Dataset: An investigation of text-image prompting for historical image dataset creation using foundation models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01674">http://arxiv.org/abs/2309.01674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hassanhajj910/prompt-me-a-dataset">https://github.com/hassanhajj910/prompt-me-a-dataset</a></li>
<li>paper_authors: Hassan El-Hajj, Matteo Valleriani</li>
<li>for: 这个论文是为了提出一个基于基础模型的图像提取管道，用于从历史文献中提取图像，并评估文本-图像提示的效果在人文领域中。</li>
<li>methods: 该管道采用了GroundDINO和Meta的Segment-Anything-Model（SAM）来从历史文献中检索大量的视觉数据，并评估不同语言提示的影响。</li>
<li>results: 研究发现，使用文本-图像提示可以提高图像提取的效果，并且在不同水平的人文数据集上都有较高的效果。<details>
<summary>Abstract</summary>
In this paper, we present a pipeline for image extraction from historical documents using foundation models, and evaluate text-image prompts and their effectiveness on humanities datasets of varying levels of complexity. The motivation for this approach stems from the high interest of historians in visual elements printed alongside historical texts on the one hand, and from the relative lack of well-annotated datasets within the humanities when compared to other domains. We propose a sequential approach that relies on GroundDINO and Meta's Segment-Anything-Model (SAM) to retrieve a significant portion of visual data from historical documents that can then be used for downstream development tasks and dataset creation, as well as evaluate the effect of different linguistic prompts on the resulting detections.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个图像提取管道，使用基础模型来从历史文献中提取图像，并评估文本图像提示的效果在人文领域中。我们的动机是，历史学家对于与历史文献一起出版的视觉元素具有极高的兴趣，而人文领域内的数据资源相对较少，而且对于其他领域来说更加缺乏准确的标注数据。我们提议一种顺序的方法，利用GroundDINO和Meta的Segment-Anything-Model（SAM）来从历史文献中检索大量的视觉数据，并用于下游开发任务和数据集创建，以及评估不同语言提示的影响。
</details></li>
</ul>
<hr>
<h2 id="Fine-grained-Affective-Processing-Capabilities-Emerging-from-Large-Language-Models"><a href="#Fine-grained-Affective-Processing-Capabilities-Emerging-from-Large-Language-Models" class="headerlink" title="Fine-grained Affective Processing Capabilities Emerging from Large Language Models"></a>Fine-grained Affective Processing Capabilities Emerging from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01664">http://arxiv.org/abs/2309.01664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joost Broekens, Bernhard Hilpert, Suzan Verberne, Kim Baraka, Patrick Gebhard, Aske Plaat</li>
<li>for: 这项研究探讨了 ChatGPT 在情感计算任务中的零配置能力，并使用提示alone进行情感分析、情感表达和情感识别等任务。</li>
<li>methods: 这项研究使用了 ChatGPT 进行语言预处理和情感计算任务，并通过提示来实现情感分析、情感表达和情感识别等任务。</li>
<li>results: 研究发现 ChatGPT 可以在 Valence、Arousal 和 Dominance 维度上进行意义性的情感分析，并且有意义的情感表达和情感识别能力。此外， ChatGPT 还可以基于提示实现基本的情绪诱发。这些发现对于情感计算任务和人工智能应用有重要意义。<details>
<summary>Abstract</summary>
Large language models, in particular generative pre-trained transformers (GPTs), show impressive results on a wide variety of language-related tasks. In this paper, we explore ChatGPT's zero-shot ability to perform affective computing tasks using prompting alone. We show that ChatGPT a) performs meaningful sentiment analysis in the Valence, Arousal and Dominance dimensions, b) has meaningful emotion representations in terms of emotion categories and these affective dimensions, and c) can perform basic appraisal-based emotion elicitation of situations based on a prompt-based computational implementation of the OCC appraisal model. These findings are highly relevant: First, they show that the ability to solve complex affect processing tasks emerges from language-based token prediction trained on extensive data sets. Second, they show the potential of large language models for simulating, processing and analyzing human emotions, which has important implications for various applications such as sentiment analysis, socially interactive agents, and social robotics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>ChatGPT can perform meaningful sentiment analysis in the Valence, Arousal, and Dominance dimensions.2. ChatGPT has meaningful emotion representations in terms of emotion categories and affective dimensions.3. ChatGPT can perform basic appraisal-based emotion elicitation of situations using a prompt-based computational implementation of the OCC appraisal model.These findings are significant:1. They demonstrate that the ability to solve complex affect processing tasks can emerge from language-based token prediction trained on extensive data sets.2. They show the potential of large language models for simulating, processing, and analyzing human emotions, which has important implications for applications such as sentiment analysis, socially interactive agents, and social robotics.</details></li>
</ol>
<hr>
<h2 id="Unveiling-Theory-of-Mind-in-Large-Language-Models-A-Parallel-to-Single-Neurons-in-the-Human-Brain"><a href="#Unveiling-Theory-of-Mind-in-Large-Language-Models-A-Parallel-to-Single-Neurons-in-the-Human-Brain" class="headerlink" title="Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain"></a>Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01660">http://arxiv.org/abs/2309.01660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Jamali, Ziv M. Williams, Jing Cai<br>for: This paper explores the ability of large language models (LLMs) to exhibit a Theory of Mind (ToM), a cognitive capacity related to our conscious mind that allows us to infer another’s beliefs and perspective.methods: The authors drew inspiration from the dorsal medial prefrontal cortex (dmPFC) neurons subserving human ToM and employed a similar methodology to examine whether LLMs exhibit comparable characteristics. They analyzed the hidden embeddings (artificial neurons) within LLMs to see if they could represent another’s perspective.results: The analysis revealed a striking resemblance between the two, as the hidden embeddings within LLMs started to exhibit significant responsiveness to either true- or false-belief trials, suggesting their ability to represent another’s perspective. The authors found that the other’s beliefs could be accurately decoded using the entire embeddings, indicating the presence of the embeddings’ ToM capability at the population level. These findings offer initial evidence of a parallel between the artificial model and neurons in the human brain.<details>
<summary>Abstract</summary>
With their recent development, large language models (LLMs) have been found to exhibit a certain level of Theory of Mind (ToM), a complex cognitive capacity that is related to our conscious mind and that allows us to infer another's beliefs and perspective. While human ToM capabilities are believed to derive from the neural activity of a broadly interconnected brain network, including that of dorsal medial prefrontal cortex (dmPFC) neurons, the precise processes underlying LLM's capacity for ToM or their similarities with that of humans remains largely unknown. In this study, we drew inspiration from the dmPFC neurons subserving human ToM and employed a similar methodology to examine whether LLMs exhibit comparable characteristics. Surprisingly, our analysis revealed a striking resemblance between the two, as hidden embeddings (artificial neurons) within LLMs started to exhibit significant responsiveness to either true- or false-belief trials, suggesting their ability to represent another's perspective. These artificial embedding responses were closely correlated with the LLMs' performance during the ToM tasks, a property that was dependent on the size of the models. Further, the other's beliefs could be accurately decoded using the entire embeddings, indicating the presence of the embeddings' ToM capability at the population level. Together, our findings revealed an emergent property of LLMs' embeddings that modified their activities in response to ToM features, offering initial evidence of a parallel between the artificial model and neurons in the human brain.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的最近发展已经发现具有一定的理论心（ToM）能力，这是与我们意识的大脑网络相关的复杂认知能力，允许我们推断别人的信念和视角。人类ToM能力据信来自大脑的广泛交叉连接的神经活动，包括前 фронталь脑某些 neurons，但precise processes underlying LLM的ToM或与人类相似之处仍然不清楚。在这项研究中，我们 Draw inspiration from human ToM neurons and used a similar methodology to examine whether LLMs exhibit comparable characteristics. Surprisingly, our analysis revealed a striking resemblance between the two, as hidden embeddings（人工神经元）within LLMs started to exhibit significant responsiveness to either true- or false-belief trials， suggesting their ability to represent another's perspective. These artificial embedding responses were closely correlated with the LLMs' performance during the ToM tasks， a property that was dependent on the size of the models. Furthermore， the other's beliefs could be accurately decoded using the entire embeddings， indicating the presence of the embeddings' ToM capability at the population level. Together， our findings revealed an emergent property of LLMs' embeddings that modified their activities in response to ToM features， offering initial evidence of a parallel between the artificial model and neurons in the human brain.
</details></li>
</ul>
<hr>
<h2 id="Which-algorithm-to-select-in-sports-timetabling"><a href="#Which-algorithm-to-select-in-sports-timetabling" class="headerlink" title="Which algorithm to select in sports timetabling?"></a>Which algorithm to select in sports timetabling?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03229">http://arxiv.org/abs/2309.03229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robertomrosati/sa4stt">https://github.com/robertomrosati/sa4stt</a></li>
<li>paper_authors: David Van Bulck, Dries Goossens, Jan-Patrick Clarner, Angelos Dimitsas, George H. G. Fonseca, Carlos Lamas-Fernandez, Martin Mariusz Lester, Jaap Pedersen, Antony E. Phillips, Roberto Maria Rosati</li>
<li>for: 运动赛事时间表调定 (sports timetabling)</li>
<li>methods: 机器学习技术 (machine learning techniques)</li>
<li>results:	+ 提出了一个算法选择系统，可以根据运动赛事问题实例的特征选择最佳的算法。	+  indentified 了选择算法时的重要特征，提供了算法性能的深入了解和提高建议。	+  empirically evaluated the hardness of the instances.In English, this means:</li>
<li>for: Sports timetabling</li>
<li>methods: Machine learning techniques</li>
<li>results:	+ Proposed an algorithm selection system that can select the best algorithm based on the characteristics of a sports timetabling problem instance.	+ Identified the important features in selecting the algorithm, providing deep insights into the performance of the algorithms and suggestions for improvement.	+ Empirically evaluated the hardness of the instances.<details>
<summary>Abstract</summary>
Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational experiments involving about 50 years of CPU time on more than 500 newly generated problem instances.
</details>
<details>
<summary>摘要</summary>
任何体育竞赛都需要一份时间表，指定比赛队伍在哪里和何时相遇。最近的国际时间安排竞赛（ITC2021）表明，尽管可以开发通用算法，但每个算法在问题实例上的性能差异较大。本文提供了体育时间安排的实例空间分析，导致了八种当前状态算法的强大洞察和探索。基于机器学习技术，我们提出了一种算法选择系统，可以根据体育时间安排问题实例的特点预测最佳算法。此外，我们还确定了哪些特征对于这种预测具有重要性，从而提供了算法性能的深入了解和改进建议。最后，我们评估了实验难度。我们的结果基于大量计算实验，耗时约50年，使用了500多个新生成的问题实例。
</details></li>
</ul>
<hr>
<h2 id="Design-of-Recognition-and-Evaluation-System-for-Table-Tennis-Players’-Motor-Skills-Based-on-Artificial-Intelligence"><a href="#Design-of-Recognition-and-Evaluation-System-for-Table-Tennis-Players’-Motor-Skills-Based-on-Artificial-Intelligence" class="headerlink" title="Design of Recognition and Evaluation System for Table Tennis Players’ Motor Skills Based on Artificial Intelligence"></a>Design of Recognition and Evaluation System for Table Tennis Players’ Motor Skills Based on Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07141">http://arxiv.org/abs/2309.07141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuo-yong Shi, Ye-tao Jia, Ke-xin Zhang, Ding-han Wang, Long-meng Ji, Yong Wu</li>
<li>for: 这项研究旨在提高穿戴式设备对特定运动的识别和分析能力。</li>
<li>methods: 该研究使用人工智能技术，设计了一种Device来收集乒乓球运动员的运动信息，并对实际运动数据进行处理。然后，通过分割特征数据库和特征工程来构建运动特征，并通过不同评价指标的损失函数来建立运动技巧的层次评价系统。</li>
<li>results: 研究结果显示，基于特征计算机神经网络的Feature-based BP神经网络在识别乒乓球运动员的运动技巧方面具有更高的识别精度和更强的泛化能力，比传统的卷积神经网络更为出色。<details>
<summary>Abstract</summary>
With the rapid development of electronic science and technology, the research on wearable devices is constantly updated, but for now, it is not comprehensive for wearable devices to recognize and analyze the movement of specific sports. Based on this, this paper improves wearable devices of table tennis sport, and realizes the pattern recognition and evaluation of table tennis players' motor skills through artificial intelligence. Firstly, a device is designed to collect the movement information of table tennis players and the actual movement data is processed. Secondly, a sliding window is made to divide the collected motion data into a characteristic database of six table tennis benchmark movements. Thirdly, motion features were constructed based on feature engineering, and motor skills were identified for different models after dimensionality reduction. Finally, the hierarchical evaluation system of motor skills is established with the loss functions of different evaluation indexes. The results show that in the recognition of table tennis players' motor skills, the feature-based BP neural network proposed in this paper has higher recognition accuracy and stronger generalization ability than the traditional convolutional neural network.
</details>
<details>
<summary>摘要</summary>
随着电子科学和技术的快速发展，穿戴设备的研究不断更新，但目前并不能完全识别和分析特定运动的运动动作。基于这一点，本文提出了一种改进穿戴设备，以便识别和评估乒乓球运动员的动作能力。首先，设备是设计用来收集乒乓球运动员的运动信息，并对实际运动数据进行处理。其次，使用滑动窗口将收集的运动数据分成六种乒乓球标准运动动作的特征库。然后，基于特征工程学，构建了运动特征，并将不同模型中的动作识别为不同的评估指标。最后，建立了基于损失函数的层次评估系统，以评估不同模型的评估指标。结果表明，在识别乒乓球运动员的动作能力方面，基于特征参数的BP神经网络提出的方法在识别精度和泛化能力方面高于传统的卷积神经网络。
</details></li>
</ul>
<hr>
<h2 id="Corgi-2-A-Hybrid-Offline-Online-Approach-To-Storage-Aware-Data-Shuffling-For-SGD"><a href="#Corgi-2-A-Hybrid-Offline-Online-Approach-To-Storage-Aware-Data-Shuffling-For-SGD" class="headerlink" title="Corgi^2: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD"></a>Corgi^2: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01640">http://arxiv.org/abs/2309.01640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etay Livne, Gal Kaplun, Eran Malach, Shai Shalev-Schwatz</li>
<li>for: 这篇论文是为了提高 Stochastic Gradient Descent (SGD) 训练机器学习模型时的数据访问效率而设计的。</li>
<li>methods: 该论文提出了一种在云存储的大型数据集上使用 online shuffling 算法，称为 CorgiPile，以提高数据访问效率，但是会导致一定的性能损失。该论文还提出了一种新的两步半数据洗选策略， combinining  offline 迭代 CorgiPile 方法和 online 迭代。</li>
<li>results: 该论文提供了一个全面的理论分析，证明了该方法的收敛性质，并通过实验结果表明，该方法可以在 homogeneous 数据上实现类似于随机访问的性能，而不需要妥协数据访问效率。<details>
<summary>Abstract</summary>
When using Stochastic Gradient Descent (SGD) for training machine learning models, it is often crucial to provide the model with examples sampled at random from the dataset. However, for large datasets stored in the cloud, random access to individual examples is often costly and inefficient. A recent work \cite{corgi}, proposed an online shuffling algorithm called CorgiPile, which greatly improves efficiency of data access, at the cost some performance loss, which is particularly apparent for large datasets stored in homogeneous shards (e.g., video datasets). In this paper, we introduce a novel two-step partial data shuffling strategy for SGD which combines an offline iteration of the CorgiPile method with a subsequent online iteration. Our approach enjoys the best of both worlds: it performs similarly to SGD with random access (even for homogenous data) without compromising the data access efficiency of CorgiPile. We provide a comprehensive theoretical analysis of the convergence properties of our method and demonstrate its practical advantages through experimental results.
</details>
<details>
<summary>摘要</summary>
当使用泛化Gradient Descent（SGD）训练机器学习模型时，通常需要将模型提供随机选择自 dataset 中的示例。然而，对于大规模存储在云端的数据集，随机访问单个示例是经济不可行，不 efficient。一项最近的工作 \cite{corgi} 提出了一种在线洗混算法 called CorgiPile，可以大幅提高数据访问效率，但是会导致一定的性能损失，尤其是对于存储在同一个分区（例如视频集）中的数据。在这篇论文中，我们提出了一种新的两步半数据洗混策略， combinines an offline iteration of the CorgiPile method with a subsequent online iteration。我们的方法可以同SGD with random access（即使对同种数据）获得类似的性能，而无需牺牲 CorgiPile 的数据访问效率。我们提供了完整的理论分析方法，并通过实验结果证明了我们的方法的实际优势。
</details></li>
</ul>
<hr>
<h2 id="Concepts-is-All-You-Need-A-More-Direct-Path-to-AGI"><a href="#Concepts-is-All-You-Need-A-More-Direct-Path-to-AGI" class="headerlink" title="Concepts is All You Need: A More Direct Path to AGI"></a>Concepts is All You Need: A More Direct Path to AGI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01622">http://arxiv.org/abs/2309.01622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Voss, Mladjan Jovanovic</li>
<li>for: 这个论文旨在帮助开发人工通用智能（AGI），以便更快速地实现人类智能水平的计算机。</li>
<li>methods: 该论文采用了认知AI方法，而不是现在广泛使用的统计学和生成方法，以更好地理解人类智能的核心需求，并从而快速实现人类智能水平的计算机。</li>
<li>results: 该论文提出了一种建议的体系和开发计划，以及一些初步的结果，可以帮助开发人工智能快速实现人类智能水平。<details>
<summary>Abstract</summary>
Little demonstrable progress has been made toward AGI (Artificial General Intelligence) since the term was coined some 20 years ago. In spite of the fantastic breakthroughs in Statistical AI such as AlphaZero, ChatGPT, and Stable Diffusion none of these projects have, or claim to have, a clear path to AGI. In order to expedite the development of AGI it is crucial to understand and identify the core requirements of human-like intelligence as it pertains to AGI. From that one can distill which particular development steps are necessary to achieve AGI, and which are a distraction. Such analysis highlights the need for a Cognitive AI approach rather than the currently favored statistical and generative efforts. More specifically it identifies the central role of concepts in human-like cognition. Here we outline an architecture and development plan, together with some preliminary results, that offers a much more direct path to full Human-Level AI (HLAI)/ AGI.
</details>
<details>
<summary>摘要</summary>
“自从AGI（人工通用智能）的概念提出20年前，实际的进步不多。尽管这些年来的统计AI（如AlphaZero、ChatGPT和稳定扩散）实现了非常惊人的突破，但是这些项目都没有或宣称不会有明确的AGI路径。以实现AGI为目标，需要了解和识别人类智能的核心需求，从而决定需要哪些开发步骤，哪些则是骚扰。这种分析表明了需要以认知AI为主，而不是目前受欢迎的统计和生成方法。更 Specifically，它显示了概念在人类智能中的中心角色。以下是一个建筑和开发计划，以及一些先验性结果，它将提供一个许多更直接的人类水准AI（HLAI）/AGI道路。”Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DeViL-Decoding-Vision-features-into-Language"><a href="#DeViL-Decoding-Vision-features-into-Language" class="headerlink" title="DeViL: Decoding Vision features into Language"></a>DeViL: Decoding Vision features into Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01617">http://arxiv.org/abs/2309.01617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/DeViL">https://github.com/ExplainableML/DeViL</a></li>
<li>paper_authors: Meghal Dani, Isabel Rio-Torto, Stephan Alaniz, Zeynep Akata</li>
<li>for: 本研究旨在提供深度神经网络决策过程的自然语言描述，尤其是对于视觉卷积网络的各层抽象特征。</li>
<li>methods: 我们提出的DeViL方法可以将视觉特征转换为自然语言描述，并不仅高亮特征位置，还生成了对应的文本描述。我们使用了 dropout 技术来进行验证，并使用了预训练的语言模型来生成文本描述。</li>
<li>results: DeViL方法可以生成与图像内容相关的自然语言描述，并且在 CC3M  dataset 上超越了先前的轻量级captioning模型，以及描述了视觉模型中学习的概念。此外，DeViL 还在 MILANNOTATIONS  dataset 上超越了当前的 neuron-wise 描述模型。<details>
<summary>Abstract</summary>
Post-hoc explanation methods have often been criticised for abstracting away the decision-making process of deep neural networks. In this work, we would like to provide natural language descriptions for what different layers of a vision backbone have learned. Our DeViL method decodes vision features into language, not only highlighting the attribution locations but also generating textual descriptions of visual features at different layers of the network. We train a transformer network to translate individual image features of any vision layer into a prompt that a separate off-the-shelf language model decodes into natural language. By employing dropout both per-layer and per-spatial-location, our model can generalize training on image-text pairs to generate localized explanations. As it uses a pre-trained language model, our approach is fast to train, can be applied to any vision backbone, and produces textual descriptions at different layers of the vision network. Moreover, DeViL can create open-vocabulary attribution maps corresponding to words or phrases even outside the training scope of the vision model. We demonstrate that DeViL generates textual descriptions relevant to the image content on CC3M surpassing previous lightweight captioning models and attribution maps uncovering the learned concepts of the vision backbone. Finally, we show DeViL also outperforms the current state-of-the-art on the neuron-wise descriptions of the MILANNOTATIONS dataset. Code available at https://github.com/ExplainableML/DeViL
</details>
<details>
<summary>摘要</summary>
各自使用dropout both per-layer和per-spatial-location，我们的DeViL方法可以进行区域化解释。我们的方法通过将视觉特征翻译成语言提示，然后使用一个独立的语言模型解码成自然语言描述。我们的模型可以快速训练，可以应用于任何视觉后处理器，并且在不同层次上生成文本描述。此外，DeViL还可以生成对于训练词汇外的开 vocabulary扩展映射。我们示示了DeViL可以在CC3M上生成相关的图像内容的文本描述，并且在MILANNOTATIONS数据集中神经元级别的描述也超过了当前状态的最佳性能。代码可以在https://github.com/ExplainableML/DeViL上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Overloaded-Vehicle-Identification-for-Long-Span-Bridges-Based-on-Structural-Health-Monitoring-Data"><a href="#Deep-Learning-Overloaded-Vehicle-Identification-for-Long-Span-Bridges-Based-on-Structural-Health-Monitoring-Data" class="headerlink" title="Deep Learning Overloaded Vehicle Identification for Long Span Bridges Based on Structural Health Monitoring Data"></a>Deep Learning Overloaded Vehicle Identification for Long Span Bridges Based on Structural Health Monitoring Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01593">http://arxiv.org/abs/2309.01593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqin Li, Jun Liu, Shengliang Zhong, Licheng Zhou, Shoubin Dong, Zejia Liu, Liqun Tang</li>
<li>for: 这个研究是为了找出长 Span 桥梁上的过载车辆，使用结构健康监控数据进行过载车辆识别。</li>
<li>methods: 本研究提出了一个深度学习基本的过载车辆识别方法（DOVI），使用时间卷网络架构对输入序列数据进行抽象，提供了一个端到端的过载车辆识别解决方案，不需要影响线或 velocity 和车辆底盘信息。</li>
<li>results:  результа显示，提出的深度学习过载车辆识别方法比其他机器学习和深度学习方法更有效和更坚固，可以在多辆车辆下进行运行。<details>
<summary>Abstract</summary>
Overloaded vehicles bring great harm to transportation infrastructures. BWIM (bridge weigh-in-motion) method for overloaded vehicle identification is getting more popular because it can be implemented without interruption to the traffic. However, its application is still limited because its effectiveness largely depends on professional knowledge and extra information, and is susceptible to occurrence of multiple vehicles. In this paper, a deep learning based overloaded vehicle identification approach (DOVI) is proposed, with the purpose of overloaded vehicle identification for long-span bridges by the use of structural health monitoring data. The proposed DOVI model uses temporal convolutional architectures to extract the spatial and temporal features of the input sequence data, thus provides an end-to-end overloaded vehicle identification solution which neither needs the influence line nor needs to obtain velocity and wheelbase information in advance and can be applied under the occurrence of multiple vehicles. Model evaluations are conducted on a simply supported beam and a long-span cable-stayed bridge under random traffic flow. Results demonstrate that the proposed deep-learning overloaded vehicle identification approach has better effectiveness and robustness, compared with other machine learning and deep learning approaches.
</details>
<details>
<summary>摘要</summary>
拥载过重车辆对交通基础设施造成严重损害。BWIM（桥上量测方法）方法在过重车辆标识方面获得更多的应用，因为它不需要中断交通。然而，它的应用仍然受限，因为它的效果受职业知识和附加信息的影响，并且容易发生多辆车辆的情况。在本文中，一种基于深度学习的过重车辆标识方法（DOVI）被提出，用于长链桥上的过重车辆标识，通过使用结构健康监测数据。提出的DOVI模型使用时间卷积架构提取输入序列数据的空间和时间特征，因此提供了一个终端到终点的过重车辆标识解决方案，无需影响线 nor 需要先知道速度和车辆跑道信息。模型评估在简支桥和长链悬臂桥上进行随机交通流下。结果表明，提出的深度学习过重车辆标识方法比其他机器学习和深度学习方法更有效和更坚定。
</details></li>
</ul>
<hr>
<h2 id="Les-Houches-Lectures-on-Deep-Learning-at-Large-Infinite-Width"><a href="#Les-Houches-Lectures-on-Deep-Learning-at-Large-Infinite-Width" class="headerlink" title="Les Houches Lectures on Deep Learning at Large &amp; Infinite Width"></a>Les Houches Lectures on Deep Learning at Large &amp; Infinite Width</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01592">http://arxiv.org/abs/2309.01592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasaman Bahri, Boris Hanin, Antonin Brossollet, Vittorio Erba, Christian Keup, Rosalba Pacelli, James B. Simon</li>
<li>for: 这些讲座主要关注深度神经网络的无穷宽限和大宽限的特性。</li>
<li>methods: 讲座涉及到深度神经网络的Random化、训练后的连接关系、线性模型、kernels和Gaussian Processes等方面。</li>
<li>results: 讲座讨论了无穷宽限下的深度神经网络的统计和动力学性质，以及训练后的大宽限网络的非平衡和平衡情况。<details>
<summary>Abstract</summary>
These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
</details>
<details>
<summary>摘要</summary>
这些讲座，发生在2022年的勒舍夏学院深度学习和机器学习讲座，关注深度神经网络的无限宽限和大宽限。讲座讨论了各种统计和动力学性质，包括随机深度神经网络、训练后深度神经网络与线性模型、核函数和高斯过程之间的连接。此外，讲座还讨论了大宽度网络的非短程和短程训练初始化。
</details></li>
</ul>
<hr>
<h2 id="Rail-Crack-Propagation-Forecasting-Using-Multi-horizons-RNNs"><a href="#Rail-Crack-Propagation-Forecasting-Using-Multi-horizons-RNNs" class="headerlink" title="Rail Crack Propagation Forecasting Using Multi-horizons RNNs"></a>Rail Crack Propagation Forecasting Using Multi-horizons RNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01569">http://arxiv.org/abs/2309.01569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Yasmine Ouerk, Olivier Vo Van, Mouadh Yagoubi</li>
<li>for: 预测铁路裂隙长度的扩展，以便维护和评估材料和结构的安全性。</li>
<li>methods: 使用机器学习技术，特别是循环神经网络（RNN），来预测时间序列数据。</li>
<li>results: 比较state-of-the-art模型（LSTM和GRU），多个 horizons 模型表现出色，可以更好地预测铁路裂隙长度的扩展。<details>
<summary>Abstract</summary>
The prediction of rail crack length propagation plays a crucial role in the maintenance and safety assessment of materials and structures. Traditional methods rely on physical models and empirical equations such as Paris law, which often have limitations in capturing the complex nature of crack growth. In recent years, machine learning techniques, particularly Recurrent Neural Networks (RNNs), have emerged as promising methods for time series forecasting. They allow to model time series data, and to incorporate exogenous variables into the model. The proposed approach involves collecting real data on the French rail network that includes historical crack length measurements, along with relevant exogenous factors that may influence crack growth. First, a pre-processing phase was performed to prepare a consistent data set for learning. Then, a suitable Bayesian multi-horizons recurrent architecture was designed to model the crack propagation phenomenon. Obtained results show that the Multi-horizons model outperforms state-of-the-art models such as LSTM and GRU.
</details>
<details>
<summary>摘要</summary>
预测铁路裂口长度的传播 игра着关键的角色在材料和结构的维护和安全评估中。传统方法通常基于物理模型和实验方程如巴黎法律，它们经常无法捕捉裂口增长的复杂性。在最近几年，机器学习技术特别是循环神经网络（RNN）已经出现为时间序列预测的有力方法。它允许模拟时间序列数据，并将外生变量纳入模型中。该方法中的提议包括收集法国铁路网络的历史裂口长度测量数据，以及可能影响裂口增长的相关外生因素。首先，一个预处理阶段进行了数据集的准备，以便学习。然后，一种适合的多个镜像感知架构被设计来模拟裂口增长现象。实验结果表明，多镜像模型在LSTM和GRU模型之上表现出了优异的性能。
</details></li>
</ul>
<hr>
<h2 id="OutRank-Speeding-up-AutoML-based-Model-Search-for-Large-Sparse-Data-sets-with-Cardinality-aware-Feature-Ranking"><a href="#OutRank-Speeding-up-AutoML-based-Model-Search-for-Large-Sparse-Data-sets-with-Cardinality-aware-Feature-Ranking" class="headerlink" title="OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets with Cardinality-aware Feature Ranking"></a>OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets with Cardinality-aware Feature Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01552">http://arxiv.org/abs/2309.01552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/outbrain/outrank">https://github.com/outbrain/outrank</a></li>
<li>paper_authors: Blaž Škrlj, Blaž Mramor</li>
<li>for: 这种论文主要用于提高现代推荐系统的设计，以便更好地解决推荐任务。</li>
<li>methods: 本论文使用了一种称为OutRank的系统，用于精细地排序特征和数据质量相关的异常检测。OutRank使用了一种基于分类数据的变体，即对同类特征噪声进行Normalizaation，以便更好地发现有用的信号。此外，该方法还 incorporates 特征相似性和合并相关性信息。</li>
<li>results: 作者们在一个 synthetic 数据集上证明了OutRank的可行性，并在一个真实的点击率预测数据集上比Random Forest-based approaches表现出色，得到了更好的结果。OutRank可以探索更大的特征空间，达到300%更大的特征空间，从而更快地找到更好的模型。<details>
<summary>Abstract</summary>
The design of modern recommender systems relies on understanding which parts of the feature space are relevant for solving a given recommendation task. However, real-world data sets in this domain are often characterized by their large size, sparsity, and noise, making it challenging to identify meaningful signals. Feature ranking represents an efficient branch of algorithms that can help address these challenges by identifying the most informative features and facilitating the automated search for more compact and better-performing models (AutoML). We introduce OutRank, a system for versatile feature ranking and data quality-related anomaly detection. OutRank was built with categorical data in mind, utilizing a variant of mutual information that is normalized with regard to the noise produced by features of the same cardinality. We further extend the similarity measure by incorporating information on feature similarity and combined relevance. The proposed approach's feasibility is demonstrated by speeding up the state-of-the-art AutoML system on a synthetic data set with no performance loss. Furthermore, we considered a real-life click-through-rate prediction data set where it outperformed strong baselines such as random forest-based approaches. The proposed approach enables exploration of up to 300% larger feature spaces compared to AutoML-only approaches, enabling faster search for better models on off-the-shelf hardware.
</details>
<details>
<summary>摘要</summary>
现代推荐系统的设计需要了解哪些特征空间中的特征是解决某个推荐任务的关键。然而，实际世界数据集经常具有大量数据、稀疏性和噪声等特点，使得找到有意义的信号变得困难。特征排名算法可以帮助解决这些挑战，通过识别最有用的特征并实现自动化模型搜索（AutoML）。我们介绍了一个名为OutRank的系统，用于多样化特征排名和数据质量相关异常检测。OutRank采用了分类数据的视角，使用一种对特征噪声产生的减法正则化的相互信息变体。我们进一步扩展了相互信息，通过 integrate feature similarity和共同相关性信息。我们的方法的可行性被证明通过加速现场AutoML系统的速度，而无损失性。此外，我们考虑了一个真实的点击率预测数据集，其中OutRank超过了强基线方法，如随机森林方法。我们的方法可以探索到300%更大的特征空间，比AutoML-只的方法更快地寻找更好的模型，在准备的硬件上。
</details></li>
</ul>
<hr>
<h2 id="ChatRule-Mining-Logical-Rules-with-Large-Language-Models-for-Knowledge-Graph-Reasoning"><a href="#ChatRule-Mining-Logical-Rules-with-Large-Language-Models-for-Knowledge-Graph-Reasoning" class="headerlink" title="ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning"></a>ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01538">http://arxiv.org/abs/2309.01538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linhao Luo, Jiaxin Ju, Bo Xiong, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan</li>
<li>for:  mines logical rules over knowledge graphs (KGs) to improve reasoning performance and provide interpretable results.</li>
<li>methods:  uses large language models (LLMs) to generate logical rules, leveraging both the semantic and structural information of KGs, and incorporates facts from existing KGs to refine the generated rules.</li>
<li>results:  evaluates the effectiveness and scalability of the proposed method on four large-scale KGs, showing impressive performance and outperforming existing methods.<details>
<summary>Abstract</summary>
Logical rules are essential for uncovering the logical connections between relations, which could improve the reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from the computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, a rule validator harnesses the reasoning ability of LLMs to validate the logical correctness of ranked rules through chain-of-thought reasoning. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用逻辑规则可以探索知识图（KG）中的逻辑连接关系，提高推理性能并提供可读写的结果。 although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, a rule validator harnesses the reasoning ability of LLMs to validate the logical correctness of ranked rules through chain-of-thought reasoning. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.<</SYS>>Here's the translation in Simplified Chinese:使用逻辑规则可以探索知识图（KG）中的逻辑连接关系，提高推理性能并提供可读写的结果。 although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, a rule validator harnesses the reasoning ability of LLMs to validate the logical correctness of ranked rules through chain-of-thought reasoning. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.
</details></li>
</ul>
<hr>
<h2 id="Are-We-Using-Autoencoders-in-a-Wrong-Way"><a href="#Are-We-Using-Autoencoders-in-a-Wrong-Way" class="headerlink" title="Are We Using Autoencoders in a Wrong Way?"></a>Are We Using Autoencoders in a Wrong Way?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01532">http://arxiv.org/abs/2309.01532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GabMartino/icrst_trst_autoencoder">https://github.com/GabMartino/icrst_trst_autoencoder</a></li>
<li>paper_authors: Gabriele Martino, Davide Moroni, Massimo Martinelli</li>
<li>for: This paper is written for revisiting the standard training for undercomplete autoencoders, specifically by modifying the shape of the latent space without using any explicit regularization term in the loss function.</li>
<li>methods: The paper uses the standard training for undercomplete autoencoders, but with a modified shape of the latent space. The model is trained to reconstruct not the same observation in input, but another one sampled from the same class distribution.</li>
<li>results: The paper explores the behavior of the latent space in the case of reconstruction of a random sample from the whole dataset.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了重新评估标准的半完全自动编码器训练方法，具体来说是通过不使用任何显式正则化项来修改半完全自动编码器的幂值空间的形态。</li>
<li>methods: 这篇论文使用标准的半完全自动编码器训练方法，但是半完全自动编码器的幂值空间被修改了。模型被训练以重建不同的输入观测值，而不是原始的输入观测值。</li>
<li>results: 这篇论文探索了随机从整个数据集中采样的整个数据集的行为。<details>
<summary>Abstract</summary>
Autoencoders are certainly among the most studied and used Deep Learning models: the idea behind them is to train a model in order to reconstruct the same input data. The peculiarity of these models is to compress the information through a bottleneck, creating what is called Latent Space. Autoencoders are generally used for dimensionality reduction, anomaly detection and feature extraction. These models have been extensively studied and updated, given their high simplicity and power. Examples are (i) the Denoising Autoencoder, where the model is trained to reconstruct an image from a noisy one; (ii) Sparse Autoencoder, where the bottleneck is created by a regularization term in the loss function; (iii) Variational Autoencoder, where the latent space is used to generate new consistent data. In this article, we revisited the standard training for the undercomplete Autoencoder modifying the shape of the latent space without using any explicit regularization term in the loss function. We forced the model to reconstruct not the same observation in input, but another one sampled from the same class distribution. We also explored the behaviour of the latent space in the case of reconstruction of a random sample from the whole dataset.
</details>
<details>
<summary>摘要</summary>
自然语言处理中的Autoencoder是非常常用的深度学习模型之一，其核心思想是通过训练模型来重建输入数据。Autoencoder模型具有压缩信息的特点，创造了所谓的缓存空间（Latent Space）。这些模型通常用于维度减少、异常检测和特征提取。这些模型已经得到了广泛的研究和更新，因为它们的简单性和力量。例如，（i）噪声Autoencoder，其中模型通过噪声图像重建原始图像；（ii）稀疏Autoencoder，其中瓶颈是通过惩罚项来创造的；（iii）变量Autoencoder，其中缓存空间用于生成新的一致性数据。在这篇文章中，我们重新训练了含有缺失的Autoencoder模型，不使用任何显式的惩罚项在损失函数中。我们让模型重建不同的输入数据，而不是原始输入数据。我们还探索了缓存空间在重建整个数据集中的行为。
</details></li>
</ul>
<hr>
<h2 id="MultiWay-Adapater-Adapting-large-scale-multi-modal-models-for-scalable-image-text-retrieval"><a href="#MultiWay-Adapater-Adapting-large-scale-multi-modal-models-for-scalable-image-text-retrieval" class="headerlink" title="MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval"></a>MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01516">http://arxiv.org/abs/2309.01516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longkukuhi/multiway-adapter">https://github.com/longkukuhi/multiway-adapter</a></li>
<li>paper_authors: Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa</li>
<li>for: 这个研究旨在解决大型多 modal 模型（LMMs）的适应问题，尤其是在新任务上进行高效的适应和传播知识。</li>
<li>methods: 我们提出了一个创新的框架，名为 Multiway-Adapter，它包括一个“对齐增强器”，用于深入对齐不同模式之间的知识。这个方法增加了LMMs中的 fewer than 1.25% 的额外参数，并在零基eline image-text搜寻中表现出色，而且可以降低 fine-tuning 时间达57%。</li>
<li>results: 我们的方法可以实现LMMs的资源有效适应和传播知识，并且可以提高零基eline image-text搜寻的性能，而且可以降低 fine-tuning 时间。<details>
<summary>Abstract</summary>
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and effective adaptation pathway for LMMs, broadening their applicability. The source code is publicly available at: \url{https://github.com/longkukuhi/MultiWay-Adapter}.
</details>
<details>
<summary>摘要</summary>
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57% reduction in fine-tuning time. Our approach offers a resource-efficient and effective adaptation pathway for LMMs, broadening their applicability. The source code is publicly available at: \url{https://github.com/longkukuhi/MultiWay-Adapter}.Here's the translation in Traditional Chinese:为了解决大型多modal模型（LMMs）的 Parameters 规模不断增加所带来的 computationally 和 memory-intensive 挑战，传统的 fine-tuning 方法通常需要隔离的、耗时的 retuning  для each new task，限制模型的多样性。此外，现有的高效的 adaptation 技术 oft overlook modality alignment，专注于新任务的知识提取。为了解决这些问题，我们介绍 Multiway-Adapter，一个创新的框架，包括一个 'Alignment Enhancer'，以深化modal alignment，实现高转移性 без fine-tuning 预训练模型的 Parameters。我们的方法仅增加 LMMs 中的 fewer than 1.25% 的额外参数，例如 BEiT-3 模型在我们的研究中。这导致在 zero-shot 图像文本搜寻性能比完全 fine-tuned 模型更高，同时可以 achieve up to 57% 的 fine-tuning 时间减少。我们的方法提供了资源效率的和有效的 adaptation 通路 для LMMs，扩展其应用范围。source code 公开可用于：\url{https://github.com/longkukuhi/MultiWay-Adapter}.
</details></li>
</ul>
<hr>
<h2 id="RGI-Net-3D-Room-Geometry-Inference-from-Room-Impulse-Responses-in-the-Absence-of-First-order-Echoes"><a href="#RGI-Net-3D-Room-Geometry-Inference-from-Room-Impulse-Responses-in-the-Absence-of-First-order-Echoes" class="headerlink" title="RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes"></a>RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01513">http://arxiv.org/abs/2309.01513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inmo Yeon, Jung-Woo Choi</li>
<li>for: 这个论文是为了提出一种新的室内geometry推导方法，不需要先知道室内形状是 convex 的，也不需要知道墙的数量。</li>
<li>methods: 该方法使用了深度神经网络（DNN），称为 RGI-Net，可以从室内响应函数（RIR）中推导室内形状。 RGI-Net 学习和利用室内高阶反射的复杂关系，因此可以在非拥圆形室内或缺失首际反射情况下估计室内形状。</li>
<li>results: 该方法可以在实际场景中应用，只需要使用一个圆形 Mikrofon 阵列和一个单个扬声器，可以大幅提高实用性。 RGI-Net 还包括评估网络，可以分别评估墙的存在概率，因此不需要先知道墙的数量。<details>
<summary>Abstract</summary>
Room geometry is important prior information for implementing realistic 3D audio rendering. For this reason, various room geometry inference (RGI) methods have been developed by utilizing the time of arrival (TOA) or time difference of arrival (TDOA) information in room impulse responses. However, the conventional RGI technique poses several assumptions, such as convex room shapes, the number of walls known in priori, and the visibility of first-order reflections. In this work, we introduce the deep neural network (DNN), RGI-Net, which can estimate room geometries without the aforementioned assumptions. RGI-Net learns and exploits complex relationships between high-order reflections in room impulse responses (RIRs) and, thus, can estimate room shapes even when the shape is non-convex or first-order reflections are missing in the RIRs. The network takes RIRs measured from a compact audio device equipped with a circular microphone array and a single loudspeaker, which greatly improves its practical applicability. RGI-Net includes the evaluation network that separately evaluates the presence probability of walls, so the geometry inference is possible without prior knowledge of the number of walls.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-Vector-Fields-Generalizing-Distance-Vector-Fields-by-Codebooks-and-Zero-Curl-Regularization"><a href="#Neural-Vector-Fields-Generalizing-Distance-Vector-Fields-by-Codebooks-and-Zero-Curl-Regularization" class="headerlink" title="Neural Vector Fields: Generalizing Distance Vector Fields by Codebooks and Zero-Curl Regularization"></a>Neural Vector Fields: Generalizing Distance Vector Fields by Codebooks and Zero-Curl Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01512">http://arxiv.org/abs/2309.01512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianghui Yang, Guosheng Lin, Zhenghao Chen, Luping Zhou</li>
<li>for: 提出了一种新的3D表示方法，即神经Vector Fields（NVF），可以同时优化表示度和拟合精度。</li>
<li>methods: 该方法采用直接预测表面上的变换，而不是通过网络导数来获取方向场，从而解决了表面EXTRACTION的问题。此外，该方法还提出了两种形状代码库，即NVF（Lite或Ultra），以促进跨类重建。</li>
<li>results: 在四个表面重建场景中，NVF（Ultra）表现出色，包括水密vs非水密形状、Category-agnostic重建vs Category-unseen重建、Category-specific重建和跨域重建。<details>
<summary>Abstract</summary>
Recent neural networks based surface reconstruction can be roughly divided into two categories, one warping templates explicitly and the other representing 3D surfaces implicitly. To enjoy the advantages of both, we propose a novel 3D representation, Neural Vector Fields (NVF), which adopts the explicit learning process to manipulate meshes and implicit unsigned distance function (UDF) representation to break the barriers in resolution and topology. This is achieved by directly predicting the displacements from surface queries and modeling shapes as Vector Fields, rather than relying on network differentiation to obtain direction fields as most existing UDF-based methods do. In this way, our approach is capable of encoding both the distance and the direction fields so that the calculation of direction fields is differentiation-free, circumventing the non-trivial surface extraction step. Furthermore, building upon NVFs, we propose to incorporate two types of shape codebooks, \ie, NVFs (Lite or Ultra), to promote cross-category reconstruction through encoding cross-object priors. Moreover, we propose a new regularization based on analyzing the zero-curl property of NVFs, and implement this through the fully differentiable framework of our NVF (ultra). We evaluate both NVFs on four surface reconstruction scenarios, including watertight vs non-watertight shapes, category-agnostic reconstruction vs category-unseen reconstruction, category-specific, and cross-domain reconstruction.
</details>
<details>
<summary>摘要</summary>
最近的神经网络基于表面重建可以大致分为两类，一是显式填充模板，另一是表示3D表面的隐式表示。为了利用这两者的优点，我们提出了一种新的3D表示方法，即神经向量场（NVF），它采用显式学习过程来操纵网格和隐式无符号距离函数（UDF）表示来突破分辨率和结构的限制。在这种方式下，我们直接预测表面上的变位异移，而不是通过网络导数来获取方向场，这样我们可以同时编码距离场和方向场，从而避免了不rivial的表面提取步骤。此外，我们在NVF的基础上，提出了两种形状码库，即NVF（Lite或Ultra），以便通过编码跨物类约束来促进跨类重建。此外，我们还提出了一种基于NVF的零核性分析 regularization，并通过我们的完全导数可 differentiable 框架来实现。我们在四个表面重建场景中评估了NVF，包括非水平 shapes、类型不可知的重建、类型特定重建和跨领域重建。
</details></li>
</ul>
<hr>
<h2 id="Memory-Efficient-Optimizers-with-4-bit-States"><a href="#Memory-Efficient-Optimizers-with-4-bit-States" class="headerlink" title="Memory Efficient Optimizers with 4-bit States"></a>Memory Efficient Optimizers with 4-bit States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01507">http://arxiv.org/abs/2309.01507</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/low-bit-optimizers">https://github.com/thu-ml/low-bit-optimizers</a></li>
<li>paper_authors: Bingrui Li, Jianfei Chen, Jun Zhu</li>
<li>for: 降低各种神经网络训练中的内存占用，使得训练模型在给定内存预算内可以达到最大训练模型。</li>
<li>methods: 通过对首 moments和次 moments进行详细的实验分析，下降优化器状态的有效位数至4位。特别是，我们发现了outsider pattern，现有的块 wise quantization无法准确地近似。我们使用更小的块大小，并使用行 wise和列 wise信息进行更好的量化。此外，我们还解决了量化第二 moment的零点问题，使用了 exclude 零点的直线量化器。</li>
<li>results: 我们在各种 benchmark 上评估了我们的4位优化器，包括自然语言理解、机器翻译、图像分类和指令调整。在所有任务上，我们的优化器可以与其全精度对手相当，同时具有更好的内存效率。<details>
<summary>Abstract</summary>
Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BadSQA-Stealthy-Backdoor-Attacks-Using-Presence-Events-as-Triggers-in-Non-Intrusive-Speech-Quality-Assessment"><a href="#BadSQA-Stealthy-Backdoor-Attacks-Using-Presence-Events-as-Triggers-in-Non-Intrusive-Speech-Quality-Assessment" class="headerlink" title="BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment"></a>BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01480">http://arxiv.org/abs/2309.01480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Ren, Kailai Shen, Zhe Ye, Diqun Yan</li>
<li>for: 本研究旨在攻击非侵入式语音质量评估（NISQA）系统，以实现高度隐蔽的攻击。</li>
<li>methods: 本研究提出了一种基于存在事件的新型后门攻击方法，可以在NISQA任务中实现高度隐蔽的攻击。</li>
<li>results: 实验结果表明，提出的后门攻击方法在四个基准数据集和两个现状最佳NISQA模型下，可以 достиieves an average attack success rate of up to 99% with a poisoning rate of only 3%.<details>
<summary>Abstract</summary>
Non-Intrusive speech quality assessment (NISQA) has gained significant attention for predicting the mean opinion score (MOS) of speech without requiring the reference speech. In practical NISQA scenarios, untrusted third-party resources are often employed during deep neural network training to reduce costs. However, it would introduce a potential security vulnerability as specially designed untrusted resources can launch backdoor attacks against NISQA systems. Existing backdoor attacks primarily focus on classification tasks and are not directly applicable to NISQA which is a regression task. In this paper, we propose a novel backdoor attack on NISQA tasks, leveraging presence events as triggers to achieving highly stealthy attacks. To evaluate the effectiveness of our proposed approach, we conducted experiments on four benchmark datasets and employed two state-of-the-art NISQA models. The results demonstrate that the proposed backdoor attack achieved an average attack success rate of up to 99% with a poisoning rate of only 3%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Pure-Monte-Carlo-Counterfactual-Regret-Minimization"><a href="#Pure-Monte-Carlo-Counterfactual-Regret-Minimization" class="headerlink" title="Pure Monte Carlo Counterfactual Regret Minimization"></a>Pure Monte Carlo Counterfactual Regret Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03084">http://arxiv.org/abs/2309.03084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju Qi, Ting Feng, Falun Hei, Zhemei Fang, Yunfeng Luo</li>
<li>for:  solves large-scale incomplete information games</li>
<li>methods:  builds upon CFR and Fictitious Play, combines counterfactual regret and best response strategy</li>
<li>results:  achieves better performance, reduces time and space complexity, and converges faster than MCCFR with a new warm-start algorithm<details>
<summary>Abstract</summary>
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strategies elimination method. Consequently, the PMCCFR with new warm start algorithm can converge by two orders of magnitude faster than the CFR+ algorithm.
</details>
<details>
<summary>摘要</summary>
Counterfactual Regret Minimization (CFR) 和其变种是目前最佳的大规模不完整信息游戏解决方案。本文提出了一种新的算法名为纯Counterfactual Regret Minimization (PCFR)，以实现更好的性能。PCFR可以看作是CFR和虚拟玩家(FP)的组合，沿用CFR中的counterfactual regret（价值）概念，并使用下一轮的最佳回应策略而不是 regret matching 策略。我们提供了理论证明，表明PCFR可以实现黑尔方程（Blackwell）可接近性，这使得PCFR可以与任何CFR变种，包括Monte Carlo CFR (MCCFR)结合。结果是PMCCFR可以显著降低时间和空间复杂度。尤其是PMCCFR的快速收敛速度至少三倍于MCCFR。此外，由于PMCCFR不通过严格dominated策略的路径，我们开发了一种新的暖启动算法， drawing inspiration from the strictly dominated strategies elimination method。因此，PMCCFR with new warm start algorithm可以在CFR+算法 convergence speed two orders of magnitude faster。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Graph-Convolutional-Filtering"><a href="#Interactive-Graph-Convolutional-Filtering" class="headerlink" title="Interactive Graph Convolutional Filtering"></a>Interactive Graph Convolutional Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01453">http://arxiv.org/abs/2309.01453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Zhang, Defu Lian, Hong Xie, Yawen Li, Enhong Chen</li>
<li>for: 这 paper 的目的是提出一种解决交互推荐系统中的冷启动和数据稀缺问题的新方法。</li>
<li>methods: 这 paper 使用了一种基于图模型的交互卷积滤波器模型，并使用了变量推理技术来解决非线性模型的计算困难。它还使用了 bayesian 元学习方法来有效地解决冷启动问题，并 derive 了对方法的理论 regret 下界，以确保方法的稳定性。</li>
<li>results:  experiments 表明，该方法在三个实际 dataset 上表现出色，比存在的基准模型更高。<details>
<summary>Abstract</summary>
Interactive Recommender Systems (IRS) have been increasingly used in various domains, including personalized article recommendation, social media, and online advertising. However, IRS faces significant challenges in providing accurate recommendations under limited observations, especially in the context of interactive collaborative filtering. These problems are exacerbated by the cold start problem and data sparsity problem. Existing Multi-Armed Bandit methods, despite their carefully designed exploration strategies, often struggle to provide satisfactory results in the early stages due to the lack of interaction data. Furthermore, these methods are computationally intractable when applied to non-linear models, limiting their applicability. To address these challenges, we propose a novel method, the Interactive Graph Convolutional Filtering model. Our proposed method extends interactive collaborative filtering into the graph model to enhance the performance of collaborative filtering between users and items. We incorporate variational inference techniques to overcome the computational hurdles posed by non-linear models. Furthermore, we employ Bayesian meta-learning methods to effectively address the cold-start problem and derive theoretical regret bounds for our proposed method, ensuring a robust performance guarantee. Extensive experimental results on three real-world datasets validate our method and demonstrate its superiority over existing baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Effective-Multi-Graph-Neural-Networks-for-Illicit-Account-Detection-on-Cryptocurrency-Transaction-Networks"><a href="#Effective-Multi-Graph-Neural-Networks-for-Illicit-Account-Detection-on-Cryptocurrency-Transaction-Networks" class="headerlink" title="Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks"></a>Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02460">http://arxiv.org/abs/2309.02460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihao Ding, Jieming Shi, Qing Li, Jiannong Cao</li>
<li>for: 本研究旨在检测 криптовалюencies 交易网络上的非法帐户，以防止normal用户 suffer 亏金额。</li>
<li>methods: 本文使用 DIAM 模型，该模型包括 Edge2Seq 模块和 Multigraph Discrepancy (MGD) 模块，自动学习有效的节点表示，并 capture 非法节点特征。</li>
<li>results: 对于 4 个大型 криптовалюencies 数据集（Bitcoin 和 Ethereum），DIAM 模型与 14 种现有解决方案进行比较，并 consistently 实现最佳性能，准确地检测非法帐户，而且高效。例如，在一个 Bitcoin 数据集上，DIAM 模型 achieve F1 score 96.55%，significantly higher than 最佳竞争者的 F1 score 83.92%。<details>
<summary>Abstract</summary>
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing the multigraph topology, DIAM employs a new Multigraph Discrepancy (MGD) module with a well-designed message passing mechanism to capture the discrepant features between normal and illicit nodes, supported by an attention mechanism. Assembling all techniques, DIAM is trained in an end-to-end manner. Extensive experiments, comparing against 14 existing solutions on 4 large cryptocurrency datasets of Bitcoin and Ethereum, demonstrate that DIAM consistently achieves the best performance to accurately detect illicit accounts, while being efficient. For instance, on a Bitcoin dataset with 20 million nodes and 203 million edges, DIAM achieves F1 score 96.55%, significantly higher than the F1 score 83.92% of the best competitor.
</details>
<details>
<summary>摘要</summary>
我们研究非法账户检测在 криптовалюencies 交易网络上，这些网络在在线金融市场中变得越来越重要。非法活动对于正常用户而言导致了亿万元的损失。现有的解决方案可以分为两类：一是 tedious 的特征工程来获取手工特征，二是不充分利用 криптовалюencies 交易数据的semantics，导致效果不佳。在这篇论文中，我们将非法账户检测问题定义为一个分类任务，并提出了一种多граaph神经网络模型（DIAM），可以有效地检测非法账户在大规模交易网络上。首先，DIAM包含一个 Edge2Seq 模块，可以自动学习有效的节点表示，保留交易 patrerns 的内在特征，通过考虑边Attributes和指向edge sequence dependencies。然后，DIAM使用一种新的多граaph不匹配度（MGD）模块，通过一种合理的消息传递机制，捕捉非法和正常节点之间的不同特征。最后，DIAM结合所有技术，在端到端方式进行训练。广泛的实验证明，对于4个大的 криптовалюencies dataset（Bitcoin和Ethereum），DIAM在14种现有解决方案中显示出了最高的性能，可以准确地检测非法账户，同时具有高效性。例如，在一个 Bitcoin dataset中，包含2000万个节点和203亿个边，DIAM的 F1 分数为96.55%，远高于最佳竞争者的 F1 分数83.92%。
</details></li>
</ul>
<hr>
<h2 id="Social-Factors-in-P2P-Energy-Trading-Using-Hedonic-Games"><a href="#Social-Factors-in-P2P-Energy-Trading-Using-Hedonic-Games" class="headerlink" title="Social Factors in P2P Energy Trading Using Hedonic Games"></a>Social Factors in P2P Energy Trading Using Hedonic Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01418">http://arxiv.org/abs/2309.01418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Mitrea, Viorica Chifu, Tudor Cioara, Ionut Anghel, Cristina Pop</li>
<li>for: 这篇论文的目的是提出一种基于hedonic game的P2P能源交易模型，以便在能源社区中帮助潜在的购买者和卖家之间进行能源交易，同时考虑社交因素。</li>
<li>methods: 该模型使用hedonic game理论来协调和合作，考虑了社交关系内的能源价格和社会偏好，并且通过了区块链技术来实现P2P能源交易。</li>
<li>results: 在一个实验中，该模型在能源社区中提高了总能源交易量的5%，并且在社交环境下帮助提高了能源交易量的10%，同时帮助实现了社区内的能源需求和供应的更好均衡。<details>
<summary>Abstract</summary>
Lately, the energy communities have gained a lot of attention as they have the potential to significantly contribute to the resilience and flexibility of the energy system, facilitating widespread integration of intermittent renewable energy sources. Within these communities the prosumers can engage in peer-to-peer trading, fostering local collaborations and increasing awareness about energy usage and flexible consumption. However, even under these favorable conditions, prosumer engagement levels remain low, requiring trading mechanisms that are aligned with their social values and expectations. In this paper, we introduce an innovative hedonic game coordination and cooperation model for P2P energy trading among prosumers which considers the social relationships within an energy community to create energy coalitions and facilitate energy transactions among them. We defined a heuristic that optimizes the prosumers coalitions, considering their social and energy price preferences and balancing the energy demand and supply within the community. We integrated the proposed hedonic game model into a state-of-the-art blockchain-based P2P energy flexibility market and evaluated its performance within an energy community of prosumers. The evaluation results on a blockchain-based P2P energy flexibility market show the effectiveness in considering social factors when creating coalitions, increasing the total amount of energy transacted in a market session by 5% compared with other game theory-based solutions. Finally, it shows the importance of the social dimensions of P2P energy transactions, the positive social dynamics in the energy community increasing the amount of energy transacted by more than 10% while contributing to a more balanced energy demand and supply within the community.
</details>
<details>
<summary>摘要</summary>
近些时间，能源社区获得了很多关注，因为它们可以为能源系统的可持续性和灵活性做出重要贡献，激发广泛的可变性可再生能源源泉的Integration。在这些社区中，潜在消费者（prosumer）可以进行Peer-to-Peer（P2P）贸易，促进本地合作和提高能源消耗和灵活消耗的认识。然而，即使在这些有利条件下，潜在消费者参与度仍然低，需要与他们的社会价值观和期望相匹配的交易机制。在这篇论文中，我们介绍了一种创新的 Hedonic Game 协调和合作模型，用于P2P能源贸易中潜在消费者之间的协作。我们定义了一个启发函数，用于优化潜在消费者的协会，考虑其社会和能源价格偏好，并均衡能源需求和供应在社区内。我们将该模型集成到了一个国际领先的区块链技术基础的P2P能源灵活市场中，并对其在能源社区中的潜在消费者进行评估。评估结果表明，考虑社会因素时创建协会可以提高P2P能源贸易市场Session中的总能源交易量，比其他Game theory基础的解决方案提高5%。此外，它还表明了社会维度上的P2P能源贸易的重要性，通过提高能源社区内的能源交易量高于10%，同时为能源需求和供应做出更好的均衡。
</details></li>
</ul>
<hr>
<h2 id="Towards-frugal-unsupervised-detection-of-subtle-abnormalities-in-medical-imaging"><a href="#Towards-frugal-unsupervised-detection-of-subtle-abnormalities-in-medical-imaging" class="headerlink" title="Towards frugal unsupervised detection of subtle abnormalities in medical imaging"></a>Towards frugal unsupervised detection of subtle abnormalities in medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02458">http://arxiv.org/abs/2309.02458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geoffroyo/onlineem">https://github.com/geoffroyo/onlineem</a></li>
<li>paper_authors: Geoffroy Oudoumanessah, Carole Lartizien, Michel Dojat, Florence Forbes</li>
<li>for: 这篇论文的目的是提出一种基于混合分布的无监督异常检测方法，来探析医疗影像中的异常现象。</li>
<li>methods: 这篇论文使用了混合分布的方法，包括mixtures of probability distributions，来进行异常检测。这种方法可以处理复杂的多个变量参考模型，并且具有较少的参数和较好的解释性。然而，标准的估计方法，如期望最大化算法，不适合处理大量数据，因为它们需要高度的内存使用。</li>
<li>results: 这篇论文的结果显示，使用混合分布的方法可以实现高度的异常检测精度，并且可以适应不同的医疗影像数据。实验结果显示，这种方法可以检测出 Parkinson 病患的脑部畸形，并且与 Hoehn 和 Yahr 病程 scales 相符。<details>
<summary>Abstract</summary>
Anomaly detection in medical imaging is a challenging task in contexts where abnormalities are not annotated. This problem can be addressed through unsupervised anomaly detection (UAD) methods, which identify features that do not match with a reference model of normal profiles. Artificial neural networks have been extensively used for UAD but they do not generally achieve an optimal trade-o$\hookleftarrow$ between accuracy and computational demand. As an alternative, we investigate mixtures of probability distributions whose versatility has been widely recognized for a variety of data and tasks, while not requiring excessive design e$\hookleftarrow$ort or tuning. Their expressivity makes them good candidates to account for complex multivariate reference models. Their much smaller number of parameters makes them more amenable to interpretation and e cient learning. However, standard estimation procedures, such as the Expectation-Maximization algorithm, do not scale well to large data volumes as they require high memory usage. To address this issue, we propose to incrementally compute inferential quantities. This online approach is illustrated on the challenging detection of subtle abnormalities in MR brain scans for the follow-up of newly diagnosed Parkinsonian patients. The identified structural abnormalities are consistent with the disease progression, as accounted by the Hoehn and Yahr scale.
</details>
<details>
<summary>摘要</summary>
医学成像异常检测在没有标注异常的情况下是一个挑战。这个问题可以通过无监督异常检测（USD）方法解决，这些方法可以标识不符合参照模型的常见 Profile 中的特征。人工神经网络已经广泛应用于 USD，但它们通常不能达到最佳的准确性和计算成本之间的平衡。作为一个替代方案，我们调查混合概率分布的使用。这种混合分布的多样性使其适用于多种数据和任务，而不需要过度的设计或调整。它的表达能力使其成为质量异常检测中的好选择，但标准估计过程，如期望最大化算法，不适用于大量数据。为解决这个问题，我们提议逐步计算推理量。这种在线方法在对抗性检测MR brain scan中的轻微异常情况中得到了描述。已经标识出的结构异常与疾病进程相符，如根据豪恩-雅尔scale的评估。
</details></li>
</ul>
<hr>
<h2 id="Metric-Learning-for-Projections-Bias-of-Generalized-Zero-shot-Learning"><a href="#Metric-Learning-for-Projections-Bias-of-Generalized-Zero-shot-Learning" class="headerlink" title="Metric Learning for Projections Bias of Generalized Zero-shot Learning"></a>Metric Learning for Projections Bias of Generalized Zero-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01390">http://arxiv.org/abs/2309.01390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Zhang, Mingyu Jin, Qinkai Yu, Haochen Xue, Xiaobo Jin</li>
<li>for: 本研究旨在提高Generalized Zero-shot Learning（GZSL）模型的可靠性和效果，使其能够正确识别未经见过的类别。</li>
<li>methods: 本研究使用了Variational Autoencoder &amp; Generative Adversarial Networks（VAEGAN）框架，并提出了一种新的参数化 Mahalanobis 距离表示法，以便在推理过程中减少偏见。同时，我们改进了VAEGAN 网络结构，以便使用两个分支来分别预测已经见过的样本和通过这个seen样本生成的未经见过的样本。</li>
<li>results: 我们在四个数据集上进行了广泛的评估，并证明了我们的方法在与状态方法相比有superiority。 codes 可以在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/111hxr">https://anonymous.4open.science/r/111hxr</a> 上获取。<details>
<summary>Abstract</summary>
Generalized zero-shot learning models (GZSL) aim to recognize samples from seen or unseen classes using only samples from seen classes as training data. During inference, GZSL methods are often biased towards seen classes due to the visibility of seen class samples during training. Most current GZSL methods try to learn an accurate projection function (from visual space to semantic space) to avoid bias and ensure the effectiveness of GZSL methods. However, during inference, the computation of distance will be important when we classify the projection of any sample into its nearest class since we may learn a biased projection function in the model. In our work, we attempt to learn a parameterized Mahalanobis distance within the framework of VAEGAN (Variational Autoencoder \& Generative Adversarial Networks), where the weight matrix depends on the network's output. In particular, we improved the network structure of VAEGAN to leverage the discriminative models of two branches to separately predict the seen samples and the unseen samples generated by this seen one. We proposed a new loss function with two branches to help us learn the optimized Mahalanobis distance representation. Comprehensive evaluation benchmarks on four datasets demonstrate the superiority of our method over the state-of-the-art counterparts. Our codes are available at https://anonymous.4open.science/r/111hxr.
</details>
<details>
<summary>摘要</summary>
通用零shot学习模型（GZSL）目标是使用已知类样本进行训练，并在测试时recognize未知类样本。然而，大多数现有GZSL方法在测试时仍然受到已知类样本的影响，导致模型偏向已知类。为解决这个问题，现有的GZSL方法通常尝试学习一个准确的投影函数（从视觉空间到 semantic空间），以避免偏见和保证GZSL方法的效果。然而，在测试时，计算距离是非常重要的，因为我们可能会学习一个偏向的投影函数。在我们的工作中，我们尝试了在VAEGAN（variational autoencoder & generative adversarial networks）框架中学习一个参数化的马ха拉诺比斯距离。具体来说，我们改进了VAEGAN的网络结构，使其能够利用两个分支的探测模型来分别预测已知样本和由已知样本生成的未知样本。我们提出了一个新的两支loss函数，以帮助我们学习优化的马ха拉诺比斯距离表示。我们在四个数据集上进行了全面的评估，并证明了我们的方法在当前的状态革命性。我们的代码可以在https://anonymous.4open.science/r/111hxr中获取。
</details></li>
</ul>
<hr>
<h2 id="LoRA-like-Calibration-for-Multimodal-Deception-Detection-using-ATSFace-Data"><a href="#LoRA-like-Calibration-for-Multimodal-Deception-Detection-using-ATSFace-Data" class="headerlink" title="LoRA-like Calibration for Multimodal Deception Detection using ATSFace Data"></a>LoRA-like Calibration for Multimodal Deception Detection using ATSFace Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01383">http://arxiv.org/abs/2309.01383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun-Wen Hsiao, Cheng-Yuan Sun</li>
<li>for: 本研究旨在开发一种能够有效地检测人类视频中的谎言，并提供可读性的模型。</li>
<li>methods: 我们提出了一种注意力意识的神经网络模型，该模型通过综合评估视频、音频和文本特征，找到谎言的表征。我们还使用多模态融合策略，提高了准确率。</li>
<li>results: 我们在一个真实的评估数据集上实现了92%的准确率。此外，模型还可以显示视频中的注意力焦点，为检测谎言提供了有价值的信息。<details>
<summary>Abstract</summary>
Recently, deception detection on human videos is an eye-catching techniques and can serve lots applications. AI model in this domain demonstrates the high accuracy, but AI tends to be a non-interpretable black box. We introduce an attention-aware neural network addressing challenges inherent in video data and deception dynamics. This model, through its continuous assessment of visual, audio, and text features, pinpoints deceptive cues. We employ a multimodal fusion strategy that enhances accuracy; our approach yields a 92\% accuracy rate on a real-life trial dataset. Most important of all, the model indicates the attention focus in the videos, providing valuable insights on deception cues. Hence, our method adeptly detects deceit and elucidates the underlying process. We further enriched our study with an experiment involving students answering questions either truthfully or deceitfully, resulting in a new dataset of 309 video clips, named ATSFace. Using this, we also introduced a calibration method, which is inspired by Low-Rank Adaptation (LoRA), to refine individual-based deception detection accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Memory-augment-is-All-You-Need-for-image-restoration"><a href="#Memory-augment-is-All-You-Need-for-image-restoration" class="headerlink" title="Memory augment is All You Need for image restoration"></a>Memory augment is All You Need for image restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01377">http://arxiv.org/abs/2309.01377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangbaijin/memorynet">https://github.com/zhangbaijin/memorynet</a></li>
<li>paper_authors: Xiao Feng Zhang, Chao Chen Gu, Shan Ying Zhu</li>
<li>for: 本研究旨在提出一种基于三级层次记忆的图像恢复方法，以提高图像恢复性能。</li>
<li>methods: 该方法使用了一种名为MemoryNet的三级层次记忆层和对比学习策略，将样本分为正例、负例和实际三种样本，并通过对比学习来塑造学习的特征。</li>
<li>results: 实验表明，该方法在Derain&#x2F;Deshadow&#x2F;Deblur任务上能够提高图像恢复性能，并且在三个不同类型的质量畸变数据集上获得了显著的PSNR和SSIM提升，这是一种强有力的证明，表明恢复的图像是可见真实的。<details>
<summary>Abstract</summary>
Image restoration is a low-level vision task, most CNN methods are designed as a black box, lacking transparency and internal aesthetics. Although some methods combining traditional optimization algorithms with DNNs have been proposed, they all have some limitations. In this paper, we propose a three-granularity memory layer and contrast learning named MemoryNet, specifically, dividing the samples into positive, negative, and actual three samples for contrastive learning, where the memory layer is able to preserve the deep features of the image and the contrastive learning converges the learned features to balance. Experiments on Derain/Deshadow/Deblur task demonstrate that these methods are effective in improving restoration performance. In addition, this paper's model obtains significant PSNR, SSIM gain on three datasets with different degradation types, which is a strong proof that the recovered images are perceptually realistic. The source code of MemoryNet can be obtained from https://github.com/zhangbaijin/MemoryNet
</details>
<details>
<summary>摘要</summary>
Image restoration 是一个低级视觉任务，大多数 CNN 方法都是黑盒子，缺乏透明度和内部美学。虽然一些将传统优化算法与 DNN 结合的方法有被提议，但它们都有一些限制。在这篇论文中，我们提出了三级别内存层和对比学习名为 MemoryNet，具体来说，将样本分为正样本、负样本和实际三个样本进行对比学习，内存层能够保留图像深度特征，对比学习使得学习的特征进行平衡。实验表明，这些方法可以提高修复性能。此外，这篇论文的模型在三个不同类型的损害数据集上获得了显著的 PSNR、SSIM 提升，这是一个强大的证明，修复的图像是有感知真实的。MemoryNet 的源代码可以从 GitHub 上获取：https://github.com/zhangbaijin/MemoryNet
</details></li>
</ul>
<hr>
<h2 id="ReOnto-A-Neuro-Symbolic-Approach-for-Biomedical-Relation-Extraction"><a href="#ReOnto-A-Neuro-Symbolic-Approach-for-Biomedical-Relation-Extraction" class="headerlink" title="ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction"></a>ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01370">http://arxiv.org/abs/2309.01370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kracr/reonto-relation-extraction">https://github.com/kracr/reonto-relation-extraction</a></li>
<li>paper_authors: Monika Jain, Kuldeep Singh, Raghava Mutharaju</li>
<li>for: 这个研究旨在提高生物医学文本中关系提取 task 的性能，通过使用 neuromorphic 知识来解决生物医学关系的特殊性。</li>
<li>methods: 这种新的技术called ReOnto，使用图 neural network 获得句子表示，并利用公开 accessible  ontology 作为先验知识来识别两个实体之间的句子关系。</li>
<li>results: 实验结果表明，使用符号知识从 ontology 与图 neural network 结合使用，可以超过所有基线（约3%）。<details>
<summary>Abstract</summary>
Relation Extraction (RE) is the task of extracting semantic relationships between entities in a sentence and aligning them to relations defined in a vocabulary, which is generally in the form of a Knowledge Graph (KG) or an ontology. Various approaches have been proposed so far to address this task. However, applying these techniques to biomedical text often yields unsatisfactory results because it is hard to infer relations directly from sentences due to the nature of the biomedical relations. To address these issues, we present a novel technique called ReOnto, that makes use of neuro symbolic knowledge for the RE task. ReOnto employs a graph neural network to acquire the sentence representation and leverages publicly accessible ontologies as prior knowledge to identify the sentential relation between two entities. The approach involves extracting the relation path between the two entities from the ontology. We evaluate the effect of using symbolic knowledge from ontologies with graph neural networks. Experimental results on two public biomedical datasets, BioRel and ADE, show that our method outperforms all the baselines (approximately by 3\%).
</details>
<details>
<summary>摘要</summary>
relation extraction (RE) 是将 sentence 中 entities 之间的 semantic 关系提取出来，并将其与知识图（KG）或ontology 中定义的关系进行对应的任务。目前已经有很多方法提出来了。但是在生物医学文本中应用这些技术时，通常会得到不满足的结果，因为生物医学关系很难直接从句子中提取。为解决这些问题，我们提出了一种新的技术 called ReOnto，它利用 neurosymbolic 知识来进行 RE 任务。ReOnto 使用图ael 神经网络来获取句子表示，并利用公共可访问的 ontology 作为先验知识来确定句子中两个 entit 之间的关系。该方法包括从 ontology 中提取两个 entit 之间的关系路径。我们通过对 symbolic 知识和 graph neural networks 的结合效果进行实验，并在两个公共生物医学数据集（BioRel 和 ADE）上进行了评估。结果表明，我们的方法比所有基eline（约为 3%） superior。
</details></li>
</ul>
<hr>
<h2 id="Refined-Temporal-Pyramidal-Compression-and-Amplification-Transformer-for-3D-Human-Pose-Estimation"><a href="#Refined-Temporal-Pyramidal-Compression-and-Amplification-Transformer-for-3D-Human-Pose-Estimation" class="headerlink" title="Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation"></a>Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01365">http://arxiv.org/abs/2309.01365</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hbing-l/rtpca">https://github.com/hbing-l/rtpca</a></li>
<li>paper_authors: Hanbing Liu, Wangmeng Xiang, Jun-Yan He, Zhi-Qi Cheng, Bin Luo, Yifeng Geng, Xuansong Xie</li>
<li>for: 优化3D人体pose预测器的精度和结构，基于transformer的Refined Temporal Pyramidal Compression-and-Amplification（RTPCA）模型。</li>
<li>methods: 利用时间维度，RTPCA模型通过Temporal Pyramidal Compression-and-Amplification（TPCA）块和 Cross-Layer Refinement（XLR）模块，提高了内块时间模型和 между块特征交互。TPCA块利用时间 pyramid 思想，强化关键和值表示能力，并从运动序列中提取空间 semantics。XLR模块通过不断交互查询、键和值，营养丰富的semantic表示。</li>
<li>results: 在Human3.6M、HumanEva-I和MPI-INF-3DHP测试集上达到了state-of-the-art result，与其他基于transformer的方法相比，具有较少的计算负担。<details>
<summary>Abstract</summary>
Accurately estimating the 3D pose of humans in video sequences requires both accuracy and a well-structured architecture. With the success of transformers, we introduce the Refined Temporal Pyramidal Compression-and-Amplification (RTPCA) transformer. Exploiting the temporal dimension, RTPCA extends intra-block temporal modeling via its Temporal Pyramidal Compression-and-Amplification (TPCA) structure and refines inter-block feature interaction with a Cross-Layer Refinement (XLR) module. In particular, TPCA block exploits a temporal pyramid paradigm, reinforcing key and value representation capabilities and seamlessly extracting spatial semantics from motion sequences. We stitch these TPCA blocks with XLR that promotes rich semantic representation through continuous interaction of queries, keys, and values. This strategy embodies early-stage information with current flows, addressing typical deficits in detail and stability seen in other transformer-based methods. We demonstrate the effectiveness of RTPCA by achieving state-of-the-art results on Human3.6M, HumanEva-I, and MPI-INF-3DHP benchmarks with minimal computational overhead. The source code is available at https://github.com/hbing-l/RTPCA.
</details>
<details>
<summary>摘要</summary>
准确估计视频序列中人体3D姿势需要 Both accuracy和一个良好的架构。在Transformers的成功基础上，我们引入Refined Temporal Pyramidal Compression-and-Amplification（RTPCA）transformer。利用时间维度，RTPCA通过其Temporal Pyramidal Compression-and-Amplification（TPCA）结构进一步发挥 intra-block 时间模型化，并使用 Cross-Layer Refinement（XLR）模块来细化 inter-block 特征互动。特别是，TPCA块采用了时间PYRAMID思想，强化关键和值表示能力，并快速从运动序列中提取空间语义。我们将这些TPCA块与XLR相连，以便通过 queries、keys 和values之间的连续互动，实现丰富的semantic表示。这种策略既保留了早期信息，又与当前流量互动，解决了其他基于Transformer的方法中常见的缺失细节和稳定性问题。我们在Human3.6M、HumanEva-I和MPI-INF-3DHP benchmark上实现了state-of-the-art的结果，而且计算开销很小。代码可以在https://github.com/hbing-l/RTPCA中获取。
</details></li>
</ul>
<hr>
<h2 id="Self-driven-Grounding-Large-Language-Model-Agents-with-Automatical-Language-aligned-Skill-Learning"><a href="#Self-driven-Grounding-Large-Language-Model-Agents-with-Automatical-Language-aligned-Skill-Learning" class="headerlink" title="Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning"></a>Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01352">http://arxiv.org/abs/2309.01352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaohui Peng, Xing Hu, Qi Yi, Rui Zhang, Jiaming Guo, Di Huang, Zikang Tian, Ruizhi Chen, Zidong Du, Qi Guo, Yunji Chen, Ling Li</li>
<li>for: 提高大语言模型在真实环境中的应用能力</li>
<li>methods: 自动提出子目标、与环境互动验证、自适应学习练习技能</li>
<li>results: 在知名的 instruktion following task 上比较出色的表现，与循证学习方法相当，但需要更少的示范数据，证明学习到的技能有效并证明了框架的可行性和效率。<details>
<summary>Abstract</summary>
Large language models (LLMs) show their powerful automatic reasoning and planning capability with a wealth of semantic knowledge about the human world. However, the grounding problem still hinders the applications of LLMs in the real-world environment. Existing studies try to fine-tune the LLM or utilize pre-defined behavior APIs to bridge the LLMs and the environment, which not only costs huge human efforts to customize for every single task but also weakens the generality strengths of LLMs. To autonomously ground the LLM onto the environment, we proposed the Self-Driven Grounding (SDG) framework to automatically and progressively ground the LLM with self-driven skill learning. SDG first employs the LLM to propose the hypothesis of sub-goals to achieve tasks and then verify the feasibility of the hypothesis via interacting with the underlying environment. Once verified, SDG can then learn generalized skills with the guidance of these successfully grounded subgoals. These skills can be further utilized to accomplish more complex tasks which fail to pass the verification phase. Verified in the famous instruction following task set-BabyAI, SDG achieves comparable performance in the most challenging tasks compared with imitation learning methods that cost millions of demonstrations, proving the effectiveness of learned skills and showing the feasibility and efficiency of our framework.
</details>
<details>
<summary>摘要</summary>
SDG first uses the LLM to propose hypotheses of sub-goals to achieve tasks and then verifies the feasibility of these hypotheses by interacting with the underlying environment. Once verified, SDG can learn generalized skills with the guidance of these successfully grounded sub-goals. These skills can be used to accomplish more complex tasks that fail to pass the verification phase. In the famous instruction following task set-BabyAI, SDG achieves comparable performance in the most challenging tasks with millions of demonstrations, demonstrating the effectiveness of learned skills and the feasibility and efficiency of our framework.
</details></li>
</ul>
<hr>
<h2 id="UniSA-Unified-Generative-Framework-for-Sentiment-Analysis"><a href="#UniSA-Unified-Generative-Framework-for-Sentiment-Analysis" class="headerlink" title="UniSA: Unified Generative Framework for Sentiment Analysis"></a>UniSA: Unified Generative Framework for Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01339">http://arxiv.org/abs/2309.01339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dawn0815/saeval-benchmark">https://github.com/dawn0815/saeval-benchmark</a></li>
<li>paper_authors: Zaijing Li, Ting-En Lin, Yuchuan Wu, Meng Liu, Fengxiao Tang, Ming Zhao, Yongbin Li</li>
<li>for: 本研究旨在解决各种情感分析子任务之间的协调问题，提高多模态情感分析的性能。</li>
<li>methods: 该研究提出了一种任务特定提示方法，并 introduce了一种多模态生成框架 named UniSA，以及一个新的情感分析评价标准 benchmark。</li>
<li>results: 实验结果表明，UniSA在各种情感分析子任务中表现 Comparable 于现状况，并且在不同子任务之间具有良好的泛化能力。<details>
<summary>Abstract</summary>
Sentiment analysis is a crucial task that aims to understand people's emotional states and predict emotional categories based on multimodal information. It consists of several subtasks, such as emotion recognition in conversation (ERC), aspect-based sentiment analysis (ABSA), and multimodal sentiment analysis (MSA). However, unifying all subtasks in sentiment analysis presents numerous challenges, including modality alignment, unified input/output forms, and dataset bias. To address these challenges, we propose a Task-Specific Prompt method to jointly model subtasks and introduce a multimodal generative framework called UniSA. Additionally, we organize the benchmark datasets of main subtasks into a new Sentiment Analysis Evaluation benchmark, SAEval. We design novel pre-training tasks and training methods to enable the model to learn generic sentiment knowledge among subtasks to improve the model's multimodal sentiment perception ability. Our experimental results show that UniSA performs comparably to the state-of-the-art on all subtasks and generalizes well to various subtasks in sentiment analysis.
</details>
<details>
<summary>摘要</summary>
（简化中文）情感分析是一项非常重要的任务，旨在理解人们的情感状态并根据多modal信息预测情感类别。它包括多个子任务，如对话中情感识别（ERC）、基于特征的情感分析（ABSA）和多modal情感分析（MSA）。然而，在情感分析中统一所有子任务存在许多挑战，包括模式匹配、统一输入/输出格式和数据集偏见。为了解决这些挑战，我们提出了任务特定提示方法，用于同时模型子任务，并引入了一个多modal生成框架called UniSA。此外，我们还将主要的 benchmark dataset组织成了一个新的情感分析评估 benchmark，称为 SAEval。我们还设计了新的预训练任务和训练方法，以便模型可以从多个子任务中学习通用的情感知识，提高模型的多modal情感感知能力。我们的实验结果显示，UniSA在所有子任务上表现相当于当前状态的顶尖水平，并且在不同的子任务中具有良好的通用性。
</details></li>
</ul>
<hr>
<h2 id="Learning-for-Interval-Prediction-of-Electricity-Demand-A-Cluster-based-Bootstrapping-Approach"><a href="#Learning-for-Interval-Prediction-of-Electricity-Demand-A-Cluster-based-Bootstrapping-Approach" class="headerlink" title="Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach"></a>Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01336">http://arxiv.org/abs/2309.01336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohit Dube, Natarajan Gautam, Amarnath Banerjee, Harsha Nagarajan</li>
<li>for: 这篇论文是为了提供一种基于差分bootstrap的日均电力需求Interval估计方法，以便在小聚合负荷setting中更好地管理运营。</li>
<li>methods: 该方法使用机器学习算法获取日均电力需求的点估计值，并使用这些点估计值和相应的差分来生成Interval估计值。具体来说，首先使用一种不supervised learning算法将日均电力需求数据分为类似的日期集合，然后将这些集合用于生成Interval估计值。</li>
<li>results: 该方法在使用实际电力需求数据进行评估时，与其他 bootstrap方法相比，能够更好地保持Interval估计值的准确性和稳定性。具体来说，该方法可以在不同的信任 интер val中提供更加精准的Interval估计值，并且可以避免因点估计值的偏差而导致的误差。<details>
<summary>Abstract</summary>
Accurate predictions of electricity demands are necessary for managing operations in a small aggregation load setting like a Microgrid. Due to low aggregation, the electricity demands can be highly stochastic and point estimates would lead to inflated errors. Interval estimation in this scenario, would provide a range of values within which the future values might lie and helps quantify the errors around the point estimates. This paper introduces a residual bootstrap algorithm to generate interval estimates of day-ahead electricity demand. A machine learning algorithm is used to obtain the point estimates of electricity demand and respective residuals on the training set. The obtained residuals are stored in memory and the memory is further partitioned. Days with similar demand patterns are grouped in clusters using an unsupervised learning algorithm and these clusters are used to partition the memory. The point estimates for test day are used to find the closest cluster of similar days and the residuals are bootstrapped from the chosen cluster. This algorithm is evaluated on the real electricity demand data from EULR(End Use Load Research) and is compared to other bootstrapping methods for varying confidence intervals.
</details>
<details>
<summary>摘要</summary>
准确的电力需求预测是微型电网运营管理中必要的。由于低聚合，电力需求具有高度抽象和点估计会带来膨胀的错误。间隔估计在这种情况下，可以提供未来值的范围，并帮助量化估计错误。本文介绍了剩余 Bootstrap 算法，用于生成间隔估计的日前电力需求。一个机器学习算法用于获取电力需求的点估计和相应的偏差在训练集上。获取的偏差被存储在内存中，并将内存进一步分区。根据类似的需求模式，天数被分组到 clusters 中使用不监督学习算法。测试日点估计用于找到最接近的 cluster，并从选择的 cluster 中 bootstrapping 偏差。这种算法在 EULR（End Use Load Research）实际电力需求数据上进行了评估，并与其他各种各样的启动方法进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Can-I-Trust-Your-Answer-Visually-Grounded-Video-Question-Answering"><a href="#Can-I-Trust-Your-Answer-Visually-Grounded-Video-Question-Answering" class="headerlink" title="Can I Trust Your Answer? Visually Grounded Video Question Answering"></a>Can I Trust Your Answer? Visually Grounded Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01327">http://arxiv.org/abs/2309.01327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/doc-doc/next-gqa">https://github.com/doc-doc/next-gqa</a></li>
<li>paper_authors: Junbin Xiao, Angela Yao, Yicong Li, Tat Seng Chua</li>
<li>for: 这个论文旨在探讨利用预处理技术来提高视频语言理解的趋势，具体来说是考虑视频语言模型（VLMs）能够回答问题并同时提供视觉证据，以确定这些技术的预测是否真正受到视频内容的支持，而不是语言或视觉上的偶合关系。</li>
<li>methods: 作者提出了NExT-GQA数据集，用于检验当今最佳的VLMs。通过后期注意力分析，发现这些模型尚未能够坚持回答的根据，这表明这些模型的预测不可靠。为此，作者提出了一种视频定位机制，包括 Gaussian mask 优化和跨模态学习。</li>
<li>results: 作者的实验表明，这种定位机制可以提高视频定位和回答。不同的后端模型的实验结果也表明，这种定位机制可以提高视频定位和回答的可靠性。<details>
<summary>Abstract</summary>
We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA -- an extension of NExT-QA with 10.5$K$ temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a variety of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are weak in substantiating the answers despite their strong QA performance. This exposes a severe limitation of these models in making reliable predictions. As a remedy, we further explore and suggest a video grounding mechanism via Gaussian mask optimization and cross-modal learning. Experiments with different backbones demonstrate that this grounding mechanism improves both video grounding and QA. Our dataset and code are released. With these efforts, we aim to push towards the reliability of deploying VLMs in VQA systems.
</details>
<details>
<summary>摘要</summary>
我们研究基于视觉的视频问答系统，响应现代技术的趋势，使用预训练技术来理解视频语言。特别是，我们要证明视频语言模型（VLM）的预测是否围绕视频内容进行 anchored，而不是语言或无关的视觉上下文的偶合。为此，我们构建了NExT-GQA数据集，包含10500个时间（或位置）标签，与原始问答对相关。通过对多种现状顶尖VLM进行探究，我们发现这些模型具有强大的问答能力，但却弱于证明答案的能力。这表明这些模型在作出可靠预测时存在严重的限制。为此，我们进一步探讨视频基准机制，包括 Gaussian 掩码优化和跨模态学习。实验表明，这种基准机制可以提高视频基准和问答能力。我们发布了数据集和代码，以便推动VLM在VQA系统中的可靠部署。
</details></li>
</ul>
<hr>
<h2 id="Learning-a-Patent-Informed-Biomedical-Knowledge-Graph-Reveals-Technological-Potential-of-Drug-Repositioning-Candidates"><a href="#Learning-a-Patent-Informed-Biomedical-Knowledge-Graph-Reveals-Technological-Potential-of-Drug-Repositioning-Candidates" class="headerlink" title="Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates"></a>Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03227">http://arxiv.org/abs/2309.03227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysjegal/ysjegal-drug-repositioning">https://github.com/ysjegal/ysjegal-drug-repositioning</a></li>
<li>paper_authors: Yongseung Jegal, Jaewoong Choi, Jiho Lee, Ki-Su Park, Seyoung Lee, Janghyeok Yoon<br>for:This paper aims to present a novel protocol for identifying drug repositioning candidates with both technological potential and scientific evidence.methods:The protocol involves constructing a scientific biomedical knowledge graph (s-BKG) and a patent-informed biomedical knowledge graph (p-BKG), and using a graph embedding protocol to evaluate the relevance scores of potential drug candidates.results:The case study on Alzheimer’s disease demonstrates the efficacy and feasibility of the proposed method, and the quantitative outcomes and systematic methods are expected to bridge the gap between computational discoveries and successful market applications in drug repositioning research.<details>
<summary>Abstract</summary>
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we developed a graph embedding protocol to ascertain the structure of the p-BKG, thereby calculating the relevance scores of those candidates with target disease-related patents to evaluate their technological potential. Our case study on Alzheimer's disease demonstrates its efficacy and feasibility, while the quantitative outcomes and systematic methods are expected to bridge the gap between computational discoveries and successful market applications in drug repositioning research.
</details>
<details>
<summary>摘要</summary>
药物重新定位策略，即在现有药物上发现新的治疗用途，在计算科学文献中得到了越来越多的探索。然而，药物重新定位候选者的技术潜力经常被忽视。本研究提出了一种新的协议，用于全面分析各种来源，包括药品专利和生物医学数据库，并从科学角度评估药物重新定位候选者。为此，我们首先构建了一个生物医学知识图（s-BKG），其中包括药物、疾病和基因之间的科学关系，来自生物医学数据库。我们的协议是通过识别具有较少与目标疾病相关的药物，但与s-BKG相互关联的药物作为重新定位候选者。然后，我们将药品专利信息添加到了生物医学知识图（p-BKG）中，并开发了一个图像嵌入协议，以确定p-BKG的结构，从而计算重新定位候选者与疾病相关专利的相互关系。我们的实验案例涉及阿兹海默病，并证明了其可行性和实用性，而量化结果和系统方法即将bridge计算发现和成功应用在药物重新定位研究中的差距。
</details></li>
</ul>
<hr>
<h2 id="Code-Representation-Pre-training-with-Complements-from-Program-Executions"><a href="#Code-Representation-Pre-training-with-Complements-from-Program-Executions" class="headerlink" title="Code Representation Pre-training with Complements from Program Executions"></a>Code Representation Pre-training with Complements from Program Executions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09980">http://arxiv.org/abs/2309.09980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiabo Huang, Jianyu Zhao, Yuyang Rong, Yiwen Guo, Yifeng He, Hao Chen</li>
<li>for: 提高代码智能的研究，使用大型自然语言处理模型（LLM）。</li>
<li>methods: 使用自定义随机测试工具生成测试用例，并将其用于预训练代码表示。</li>
<li>results: 与其他预训练方法相比，FuzzPretrain在代码搜索中提高了6%以上&#x2F;9%以上的MAP值。<details>
<summary>Abstract</summary>
Large language models (LLMs) for natural language processing have been grafted onto programming language modeling for advancing code intelligence. Although it can be represented in the text format, code is syntactically more rigorous in order to be properly compiled or interpreted to perform a desired set of behaviors given any inputs. In this case, existing works benefit from syntactic representations to learn from code less ambiguously in the forms of abstract syntax tree, control-flow graph, etc. However, programs with the same purpose can be implemented in various ways showing different syntactic representations while the ones with similar implementations can have distinct behaviors. Though trivially demonstrated during executions, such semantics about functionality are challenging to be learned directly from code, especially in an unsupervised manner. Hence, in this paper, we propose FuzzPretrain to explore the dynamic information of programs revealed by their test cases and embed it into the feature representations of code as complements. The test cases are obtained with the assistance of a customized fuzzer and are only required during pre-training. FuzzPretrain yielded more than 6%/9% mAP improvements on code search over its counterparts trained with only source code or AST, respectively. Our extensive experimental results show the benefits of learning discriminative code representations with program executions.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理（LLM）模型已经被应用到程序语言模型中，以提高代码智能。虽然代码可以被表示为文本格式，但它的语法更加严格，以便在输入任意时执行所需的行为。在这种情况下，现有的工作受益于语法表示，以更好地从代码中学习不同的行为。然而，实现相同的目的可以通过不同的方式实现，导致代码的语法表示异常。尽管在执行时可以轻松地证明这些semantics，但在无监督情况下学习代码的Semantics是困难的。因此，在这篇论文中，我们提出了FuzzPretrain，它可以通过测试用例来探索代码中的动态信息，并将其 embedding到代码的特征表示中作为补充。这些测试用例通过自定义的随机测试工具生成，只需在预训练时使用。FuzzPretrain在代码搜索中实现了6%/9%的mAP提升，与只使用源代码或AST训练的对手相比。我们的广泛的实验结果表明了从代码执行中学习特征代码表示的利antages。
</details></li>
</ul>
<hr>
<h2 id="ExMobileViT-Lightweight-Classifier-Extension-for-Mobile-Vision-Transformer"><a href="#ExMobileViT-Lightweight-Classifier-Extension-for-Mobile-Vision-Transformer" class="headerlink" title="ExMobileViT: Lightweight Classifier Extension for Mobile Vision Transformer"></a>ExMobileViT: Lightweight Classifier Extension for Mobile Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01310">http://arxiv.org/abs/2309.01310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gyeongdong Yang, Yungwook Kwon, Hyunjin Kim</li>
<li>for: 提高手机友好的视transformer性能，降低计算负担</li>
<li>methods: 使用均值池化结果来扩展通道数，再利用早期注意力阶段的信息</li>
<li>results: 相比原MobileViT，提高精度，仅增加5%参数量<details>
<summary>Abstract</summary>
The paper proposes an efficient structure for enhancing the performance of mobile-friendly vision transformer with small computational overhead. The vision transformer (ViT) is very attractive in that it reaches outperforming results in image classification, compared to conventional convolutional neural networks (CNNs). Due to its need of high computational resources, MobileNet-based ViT models such as MobileViT-S have been developed. However, their performance cannot reach the original ViT model. The proposed structure relieves the above weakness by storing the information from early attention stages and reusing it in the final classifier. This paper is motivated by the idea that the data itself from early attention stages can have important meaning for the final classification. In order to reuse the early information in attention stages, the average pooling results of various scaled features from early attention stages are used to expand channels in the fully-connected layer of the final classifier. It is expected that the inductive bias introduced by the averaged features can enhance the final performance. Because the proposed structure only needs the average pooling of features from the attention stages and channel expansions in the final classifier, its computational and storage overheads are very small, keeping the benefits of low-cost MobileNet-based ViT (MobileViT). Compared with the original MobileViTs on the ImageNet dataset, the proposed ExMobileViT has noticeable accuracy enhancements, having only about 5% additional parameters.
</details>
<details>
<summary>摘要</summary>
文章提出了一种高效的结构，以提高移动设备友好的视Transformer（ViT）性能，而不需要大量计算资源。由于ViT模型的需求高于常见的卷积神经网络（CNN），因此基于MobileNet的ViT模型如MobileViT-S已经开发。然而，其性能不能达到原始ViT模型的水平。提出的结构解决了上述弱点，通过将早期注意力阶段中的信息存储并重复使用在最终分类器中。这篇论文受到了数据本身在早期注意力阶段的重要性的想法所 inspirited。为了重复使用早期注意力阶段的信息，使用了不同缩放因子的特征的平均池化结果来扩展最终分类器的 Fully Connected（FC）层的通道数。预计通过引入缩放因子引入的预测偏好，可以提高最终性能。由于提出的结构只需要平均池化早期注意力阶段的特征，以及在最终分类器的FC层中进行通道扩展，因此计算和存储开销非常小，保留了低成本的MobileNet基于ViT（MobileViT）的好处。与原始MobileViT在ImageNet dataset上的性能相比，提出的ExMobileViT具有显著的准确性提升，只有约5%的额外参数。
</details></li>
</ul>
<hr>
<h2 id="Partial-Proof-of-a-Conjecture-with-Implications-for-Spectral-Majorization"><a href="#Partial-Proof-of-a-Conjecture-with-Implications-for-Spectral-Majorization" class="headerlink" title="Partial Proof of a Conjecture with Implications for Spectral Majorization"></a>Partial Proof of a Conjecture with Implications for Spectral Majorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01302">http://arxiv.org/abs/2309.01302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeffrey Uhlmann</li>
<li>for: 这项研究探讨了一个关于 $n\times n$ ($n\leq 6$) 正定矩阵的性质的悬念。</li>
<li>methods: 这项研究使用了计算机辅助的和平方方法（SoS）来证明多项式非负性。</li>
<li>results: 研究发现了一个新的矩阵家族，其特点是对角线majorize其 спектrum。此外，这个家族可以通过克로内克组合扩展到 $n&gt;6$ ，保持特殊的majorization property。<details>
<summary>Abstract</summary>
In this paper we report on new results relating to a conjecture regarding properties of $n\times n$, $n\leq 6$, positive definite matrices. The conjecture has been proven for $n\leq 4$ using computer-assisted sum of squares (SoS) methods for proving polynomial nonnegativity. Based on these proven cases, we report on the recent identification of a new family of matrices with the property that their diagonals majorize their spectrum. We then present new results showing that this family can extended via Kronecker composition to $n>6$ while retaining the special majorization property. We conclude with general considerations on the future of computer-assisted and AI-based proofs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们报告了新的结果，与positive definite矩阵($n\times n$, $n\leq 6$)的性质有关。我们已经使用计算机支持的sum of squares（SoS）方法证明了$n\leq 4$的情况。基于已经证明的 случа例，我们报告了一个新的矩阵家族，其特点是主对角线majorize其 спектrum。然后，我们发现了一种可以通过克ро内克组合延伸到$n>6$的方法，保持特殊的majorization性。我们结束于计算机支持和人工智能基于证明的未来考虑。Here's the translation in Traditional Chinese:在这篇论文中，我们报告了新的结果，与positive definite矩阵（$n\times n$, $n\leq 6）的性质有关。我们已经使用计算机支持的sum of squares（SoS）方法证明了$n\leq 4$的情况。基于已经证明的个案例，我们报告了一个新的矩阵家族，其特点是主对角线majorize其 спектrum。然后，我们发现了一种可以通过克ро内克组合延伸到$n>6$的方法，保持特殊的majorization性。我们结束于计算机支持和人工智能基于证明的未来考虑。
</details></li>
</ul>
<hr>
<h2 id="AlphaZero-Gomoku"><a href="#AlphaZero-Gomoku" class="headerlink" title="AlphaZero Gomoku"></a>AlphaZero Gomoku</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01294">http://arxiv.org/abs/2309.01294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suragnair/alpha-zero-general">https://github.com/suragnair/alpha-zero-general</a></li>
<li>paper_authors: Wen Liang, Chao Yu, Brian Whiteaker, Inyoung Huh, Hua Shao, Youzhi Liang</li>
<li>for: 这个论文的目的是探索AlphaZero算法在gomoku棋盘游戏中的表现。</li>
<li>methods: 这个论文使用了AlphaZero算法，具体来说是将深度学习与Monte Carlo搜索结合在一起，以便在gomoku棋盘游戏中实现人工智能的优势。</li>
<li>results: 测试结果表明，AlphaZero算法在gomoku棋盘游戏中表现出了优势，并且能够在不同的游戏环境下保持稳定的高水平。<details>
<summary>Abstract</summary>
In the past few years, AlphaZero's exceptional capability in mastering intricate board games has garnered considerable interest. Initially designed for the game of Go, this revolutionary algorithm merges deep learning techniques with the Monte Carlo tree search (MCTS) to surpass earlier top-tier methods. In our study, we broaden the use of AlphaZero to Gomoku, an age-old tactical board game also referred to as "Five in a Row." Intriguingly, Gomoku has innate challenges due to a bias towards the initial player, who has a theoretical advantage. To add value, we strive for a balanced game-play. Our tests demonstrate AlphaZero's versatility in adapting to games other than Go. MCTS has become a predominant algorithm for decision processes in intricate scenarios, especially board games. MCTS creates a search tree by examining potential future actions and uses random sampling to predict possible results. By leveraging the best of both worlds, the AlphaZero technique fuses deep learning from Reinforcement Learning with the balancing act of MCTS, establishing a fresh standard in game-playing AI. Its triumph is notably evident in board games such as Go, chess, and shogi.
</details>
<details>
<summary>摘要</summary>
Recently, AlphaZero的出色的能力在复杂游戏中精通得到了广泛关注。 alphaZero最初是设计用于围棋游戏，这种革命性的算法将深度学习技术与Monte Carlo Tree Search（MCTS）相结合，超越了之前的顶尖方法。在我们的研究中，我们扩展了AlphaZero的使用范围到了古 mobil游戏（Gomoku），这是一款具有偏好初始玩家的战略游戏。很有趣的是，Gomoku拥有内生的挑战，因为初始玩家有理论上的优势。为了增加价值，我们努力寻求平衡的游戏环境。我们的测试表明AlphaZero在不同于Go的游戏中也有很好的适应能力。MCTS已成为复杂enario中决策过程中的主流算法，特别是板球游戏。MCTS通过评估未来动作的可能性来构建搜索树，并使用随机抽样来预测可能的结果。AlphaZero技术将深度学习从强化学习与MCTS的平衡过程相结合，建立了新的游戏AI标准。其胜利在板球游戏、国际象棋和将棋等游戏中得到了广泛证明。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.AI_2023_09_04/" data-id="clorjzl23003df18841xae4hc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.CL_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T11:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.CL_2023_09_04/">cs.CL - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Attention-Driven-Multi-Modal-Fusion-Enhancing-Sign-Language-Recognition-and-Translation"><a href="#Attention-Driven-Multi-Modal-Fusion-Enhancing-Sign-Language-Recognition-and-Translation" class="headerlink" title="Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation"></a>Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01860">http://arxiv.org/abs/2309.01860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaber Ibn Abdul Hakim, Rasman Mubtasim Swargo, Muhammad Abdullah Adnan</li>
<li>for: 这个研究的目的是扩展现有的连续手语识别和翻译管道，以包含多modal信息。</li>
<li>methods: 这个研究使用了一种跨modal编码器，将Optical flow信息与RGB图像集成到特征集中，以增强手语识别和翻译的精度。</li>
<li>results: 研究表明，通过包含多modal信息，可以提高手语识别和翻译的结果。在RWTH-PHOENIX-2014数据集上进行手语识别任务，我们的方法可以降低WER值0.9。在RWTH-PHOENIX-2014T数据集上进行翻译任务，我们的方法可以提高大多数BLEU分数值0.6。<details>
<summary>Abstract</summary>
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法，用于将多modal信息添加到现有的连续手语识别和翻译管道中。在我们的过程中，我们将光流信息与RGB图像结合以增强特征中的运动相关信息。本研究证明了这种多模态包含的可行性，并使用了一个轻量级的插件，不需要在端到端方式中额外添加新模态的特征提取器。我们对手语识别和翻译进行了应用，并在每个情况下提高了结果。我们在RWTH-PHOENIX-2014 dataset上进行了手语识别任务的评估，并在RWTH-PHOENIX-2014T dataset上进行了翻译任务的评估。在识别任务中，我们的方法降低了WER值0.9，在翻译任务中，我们的方法在测试集上提高了大多数BLEU分数的0.6。
</details></li>
</ul>
<hr>
<h2 id="Minimal-Effective-Theory-for-Phonotactic-Memory-Capturing-Local-Correlations-due-to-Errors-in-Speech"><a href="#Minimal-Effective-Theory-for-Phonotactic-Memory-Capturing-Local-Correlations-due-to-Errors-in-Speech" class="headerlink" title="Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech"></a>Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02466">http://arxiv.org/abs/2309.02466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Myles Eugenio</li>
<li>for: 这个论文是为了探讨语音演化的经济性的影响，以及这种影响如何使得人们更容易学习语音。</li>
<li>methods: 这篇论文使用了一种基于tensor网络的本地相关模型，这种模型利用了语音中的本地phonetic correlations来促进语音学习。</li>
<li>results: 研究发现，这种模型可以帮助人们更容易学习语音，并且可以生成新的语音单词，这些单词符合目标语言的phonetic规则。此外，模型还可以提供一个 hierarchical 的最likely errors 列表，用于描述在语音行为中可能出现的错误。<details>
<summary>Abstract</summary>
Spoken language evolves constrained by the economy of speech, which depends on factors such as the structure of the human mouth. This gives rise to local phonetic correlations in spoken words. Here we demonstrate that these local correlations facilitate the learning of spoken words by reducing their information content. We do this by constructing a locally-connected tensor-network model, inspired by similar variational models used for many-body physics, which exploits these local phonetic correlations to facilitate the learning of spoken words. The model is therefore a minimal model of phonetic memory, where "learning to pronounce" and "learning a word" are one and the same. A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language; as well as providing a hierarchy of the most likely errors that could be produced during the action of speech. We test our model against Latin and Turkish words. (The code is available on GitHub.)
</details>
<details>
<summary>摘要</summary>
spoken language evolves constrained by the economy of speech, which depends on factors such as the structure of the human mouth. This gives rise to local phonetic correlations in spoken words. Here we demonstrate that these local correlations facilitate the learning of spoken words by reducing their information content. We do this by constructing a locally-connected tensor-network model, inspired by similar variational models used for many-body physics, which exploits these local phonetic correlations to facilitate the learning of spoken words. The model is therefore a minimal model of phonetic memory, where "learning to pronounce" and "learning a word" are one and the same. A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language; as well as providing a hierarchy of the most likely errors that could be produced during the action of speech. We test our model against Latin and Turkish words. (The code is available on GitHub.)Here's the translation breakdown:* "spoken language" 口语 (kǒu yǔ)* "evolves" 演化 (biǎn huà)* "constrained" 受限 (shòu jiàn)* "by the economy of speech" 由语言经济 (yǐ yǔ yán jīng jì)* "which depends on factors such as the structure of the human mouth" 即人口结构等因素 (jī rén kǒu jiégòu déng yǐng fāng)* "This gives rise to local phonetic correlations in spoken words" 这会导致 spoken words 中的地方声学相关性 (zhè huì dào cái spoken words zhōng de dì fāng shēng xué xiāng yì)* "Here we demonstrate that these local correlations facilitate the learning of spoken words" 我们在这里示出这些地方声学相关性可以促进 spoken words 的学习 (wǒ men zài zhè lǐ shì chū shēng zhī yì xiǎng xué xí)* "by reducing their information content" 通过减少信息内容 (tōng guò jiǎn shòu xìn xīn nèi zhòng)* "We do this by constructing a locally-connected tensor-network model" 我们使用一种地方连接的tensor网络模型 (wǒ men shǐ yòng yī zhōng dì fāng lián xiǎng de tensor wǎng wǎn mó del)* "inspired by similar variational models used for many-body physics"  Drawing on similar variational models used in many-body physics (fāng yǐn yī xiàng zhī yì zhī shì)* "which exploits these local phonetic correlations to facilitate the learning of spoken words" 这种模型可以利用这些地方声学相关性来促进 spoken words 的学习 (zhè zhōng mó del cóu yì liǎng yòu zhī yì xiǎng yì shì)* "The model is therefore a minimal model of phonetic memory" 这种模型因此成为一种最小的声学记忆模型 (zhè zhōng mó del yìn xiàng zhī yì xiǎng yì)* "where 'learning to pronounce' and 'learning a word' are one and the same" 这种模型中，"学习发音"和"学习一个词"是一样的 (zhè zhōng mó del zhì yì, "xué xí fā yīn" yǔ "xué xí yī gè ci" shì yī yàng de)* "A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language" 这种模型的一个结果是学习出新词，这些词汇在目标语言中是声学合理的 (zhè zhōng mó del yī yì shì, zhè xiē yì shì zhī yì yì bù)* "as well as providing a hierarchy of the most likely errors that could be produced during the action of speech" 这种模型还可以提供一个词汇错误的层次结构 (zhè zhōng mó del yǐn yì shì zhī yì yì bù)* "We test our model against Latin and Turkish words" 我们对 Latin 和 Turkish 词汇进行测试 (wǒ men duì Lati n yǔ Tūrkish yì shì zhì)* "The code is available on GitHub" 代码可以在 GitHub 上找到 (fǎn yì zhī yì zhī shì)
</details></li>
</ul>
<hr>
<h2 id="Into-the-Single-Cell-Multiverse-an-End-to-End-Dataset-for-Procedural-Knowledge-Extraction-in-Biomedical-Texts"><a href="#Into-the-Single-Cell-Multiverse-an-End-to-End-Dataset-for-Procedural-Knowledge-Extraction-in-Biomedical-Texts" class="headerlink" title="Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts"></a>Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01812">http://arxiv.org/abs/2309.01812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ylaboratory/flambe">https://github.com/ylaboratory/flambe</a></li>
<li>paper_authors: Ruth Dannenfelser, Jeffrey Zhong, Ran Zhang, Vicky Yao</li>
<li>for: 这篇论文的主要目的是提供一个用于生物医学领域进行过程知识抽取的数据集，以便进一步开发自然语言处理（NLP）模型。</li>
<li>methods: 该数据集是通过专家 manually 精心纠正的方式从生物医学领域的学术论文中提取出的过程知识，包括名实 recognize 和ambiguation 等任务。</li>
<li>results: 该数据集提供了一个大量的 manually 精心纠正的名实识别和ambiguation 数据集，以便进一步开发NLP模型，同时也有助于提高生物医学研究领域的重复性。<details>
<summary>Abstract</summary>
Many of the most commonly explored natural language processing (NLP) information extraction tasks can be thought of as evaluations of declarative knowledge, or fact-based information extraction. Procedural knowledge extraction, i.e., breaking down a described process into a series of steps, has received much less attention, perhaps in part due to the lack of structured datasets that capture the knowledge extraction process from end-to-end. To address this unmet need, we present FlaMB\'e (Flow annotations for Multiverse Biological entities), a collection of expert-curated datasets across a series of complementary tasks that capture procedural knowledge in biomedical texts. This dataset is inspired by the observation that one ubiquitous source of procedural knowledge that is described as unstructured text is within academic papers describing their methodology. The workflows annotated in FlaMB\'e are from texts in the burgeoning field of single cell research, a research area that has become notorious for the number of software tools and complexity of workflows used. Additionally, FlaMB\'e provides, to our knowledge, the largest manually curated named entity recognition (NER) and disambiguation (NED) datasets for tissue/cell type, a fundamental biological entity that is critical for knowledge extraction in the biomedical research domain. Beyond providing a valuable dataset to enable further development of NLP models for procedural knowledge extraction, automating the process of workflow mining also has important implications for advancing reproducibility in biomedical research.
</details>
<details>
<summary>摘要</summary>
多种常见的自然语言处理（NLP）信息EXTRACTION任务可以被视为评估声明知识或基于事实的信息EXTRACTION。而过程知识EXTRACTION，即从描述过程中提取步骤，则 receiving much less attention，可能是由于缺乏结构化数据集的不足。为解决这一需求，我们提出FlaMB\'e（流动注释 для多元生物实体），这是一系列 complementary tasks 的专家修订 datasets，用于捕捉生物文献中的过程知识。这个数据集得到了学术论文中描述的过程的注释，这些过程来自生物学研究领域的快速发展领域，特别是单细胞研究。此外，FlaMB\'e 还提供了，到我们所知，生物医学研究领域中最大的手动修订名实体识别（NER）和名实体识别（NED）数据集，用于识别基本生物实体，这种实体是生物医学研究领域中知识EXTRACTION的重要基础。除了提供可用于进一步发展NLP模型的价值数据集外，自动化工作流挖掘也有重要的 reproduceability 推动生物医学研究的意义。
</details></li>
</ul>
<hr>
<h2 id="Are-Emergent-Abilities-in-Large-Language-Models-just-In-Context-Learning"><a href="#Are-Emergent-Abilities-in-Large-Language-Models-just-In-Context-Learning" class="headerlink" title="Are Emergent Abilities in Large Language Models just In-Context Learning?"></a>Are Emergent Abilities in Large Language Models just In-Context Learning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01809">http://arxiv.org/abs/2309.01809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ukplab/on-emergence">https://github.com/ukplab/on-emergence</a></li>
<li>paper_authors: Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, Iryna Gurevych</li>
<li>for: This paper aims to provide a comprehensive examination of the emergent abilities of large language models, specifically looking at the role of in-context learning in their performance.</li>
<li>methods: The authors use a set of 18 models with varying parameters (60 million to 175 billion) and test them on a set of 22 tasks. They conduct over 1,000 experiments to evaluate the models’ performance and determine the underlying mechanisms driving their emergent abilities.</li>
<li>results: The authors find that the emergent abilities of large language models can primarily be attributed to in-context learning, and there is no evidence for the emergence of reasoning abilities. This provides valuable insights into the use of these models and alleviates safety concerns regarding their performance.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是为了对大语言模型的突出能力进行全面的检查，特别是它们在受到不同提示下的性能如何。</li>
<li>methods: 作者使用了18个模型（参数量从60万到175亿），对其进行了22项任务的测试，并进行了超过1000个实验来评估模型的性能。</li>
<li>results: 作者发现，大语言模型的突出能力主要归因于受到提示下的学习，并没有证据表明它们具有推理能力的emergence。这些结论有助于我们更好地理解这些模型的使用，并缓解对其性能的安全性问题。<details>
<summary>Abstract</summary>
Large language models have exhibited emergent abilities, demonstrating exceptional performance across diverse tasks for which they were not explicitly trained, including those that require complex reasoning abilities. The emergence of such abilities carries profound implications for the future direction of research in NLP, especially as the deployment of such models becomes more prevalent. However, one key challenge is that the evaluation of these abilities is often confounded by competencies that arise in models through alternative prompting techniques, such as in-context learning and instruction following, which also emerge as the models are scaled up. In this study, we provide the first comprehensive examination of these emergent abilities while accounting for various potentially biasing factors that can influence the evaluation of models. We conduct rigorous tests on a set of 18 models, encompassing a parameter range from 60 million to 175 billion parameters, across a comprehensive set of 22 tasks. Through an extensive series of over 1,000 experiments, we provide compelling evidence that emergent abilities can primarily be ascribed to in-context learning. We find no evidence for the emergence of reasoning abilities, thus providing valuable insights into the underlying mechanisms driving the observed abilities and thus alleviating safety concerns regarding their use.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Empirical-Analysis-for-Zero-Shot-Multi-Label-Classification-on-COVID-19-CT-Scans-and-Uncurated-Reports"><a href="#An-Empirical-Analysis-for-Zero-Shot-Multi-Label-Classification-on-COVID-19-CT-Scans-and-Uncurated-Reports" class="headerlink" title="An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports"></a>An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01740">http://arxiv.org/abs/2309.01740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Dack, Lorenzo Brigato, Matthew McMurray, Matthias Fontanellaz, Thomas Frauenfelder, Hanno Hoppe, Aristomenis Exadaktylos, Thomas Geiser, Manuela Funke-Chambour, Andreas Christe, Lukas Ebner, Stavroula Mougiakakou</li>
<li>for: 这 paper 是为了研究基于 contrastive visual language learning 的零shot 多标签分类方法，以帮助 radiologist 诊断 COVID-19 和识别详细的 lung 病变。</li>
<li>methods: 这 paper 使用了 unstructured data 和 CT 成像来进行零shot 多标签分类，并与 human expert 合作调节模型。</li>
<li>results: 这 paper 的 empirical analysis 表明，零shot 多标签分类方法可以帮助 radiologist 更好地诊断 COVID-19 和识别详细的 lung 病变。<details>
<summary>Abstract</summary>
The pandemic resulted in vast repositories of unstructured data, including radiology reports, due to increased medical examinations. Previous research on automated diagnosis of COVID-19 primarily focuses on X-ray images, despite their lower precision compared to computed tomography (CT) scans. In this work, we leverage unstructured data from a hospital and harness the fine-grained details offered by CT scans to perform zero-shot multi-label classification based on contrastive visual language learning. In collaboration with human experts, we investigate the effectiveness of multiple zero-shot models that aid radiologists in detecting pulmonary embolisms and identifying intricate lung details like ground glass opacities and consolidations. Our empirical analysis provides an overview of the possible solutions to target such fine-grained tasks, so far overlooked in the medical multimodal pretraining literature. Our investigation promises future advancements in the medical image analysis community by addressing some challenges associated with unstructured data and fine-grained multi-label classification.
</details>
<details>
<summary>摘要</summary>
pandemic 导致了庞大的不结构数据存储，包括 radiology 报告，由于增加的医疗检查。先前的自动诊断 COVID-19 研究主要集中在 X-ray 图像上，尽管它们的精度比 computed tomography (CT) 扫描低。在这项工作中，我们利用医院的不结构数据和 CT 扫描的细腻特征，实现零shot 多标签分类 based on 对比视觉语言学习。与人类专家合作，我们调查了多种零shot 模型的效果，帮助 radiologist 检测肺动脉塞和识别复杂的肺脉塞，如云彩杂色和聚集。我们的实验分析提供了targeting such fine-grained tasks 的可能解决方案的概述，在医学多Modal 预训练 литературе中被过looked。我们的调查承诺未来医学图像分析社区的发展，解决一些不结构数据和细腻多标签分类的挑战。
</details></li>
</ul>
<hr>
<h2 id="Prompting-or-Fine-tuning-A-Comparative-Study-of-Large-Language-Models-for-Taxonomy-Construction"><a href="#Prompting-or-Fine-tuning-A-Comparative-Study-of-Large-Language-Models-for-Taxonomy-Construction" class="headerlink" title="Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction"></a>Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01715">http://arxiv.org/abs/2309.01715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/20001LastOrder/Taxonomy-GPT">https://github.com/20001LastOrder/Taxonomy-GPT</a></li>
<li>paper_authors: Boqi Chen, Fandi Yi, Dániel Varró</li>
<li>for: 本研究旨在提出一种满足结构约束的自动taxonomy构建框架，以便在不同的软件模型和自然语言处理（NLP）活动中提高taxonomy的效果。</li>
<li>methods: 本研究使用了适当的用户输入（称为提示），将GPT-3等大语言模型（LLMs）引导到多种NLP任务中，而不需要显式（重）训练。</li>
<li>results: 研究发现，无需显式训练，提示方法可以在hypernym taxonomy和计算机科学 taxonomy dataset中对taxonomy进行自动构建，并且在训练集小时，提示方法的性能比 fine-tuning 方法更高。但是， fine-tuning 方法可以轻松地对生成的taxonomy进行后处理，以满足所有约束。<details>
<summary>Abstract</summary>
Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing (NLP) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models (LLMs) have demonstrated that appropriate user inputs (called prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.
</details>
<details>
<summary>摘要</summary>
taxonomies 表示实体之间的层次关系，常用于软件建模和自然语言处理（NLP）活动中。它们通常受到一组结构约束，限制它们的内容。然而，手动构建税onomy可以是时间consuming、不完整和维护成本高。现在的研究表明，适当的用户输入（叫做提示）可以导引大语言模型（LLMs）在多种NLP任务中表现出优秀的性能，无需显式（再）训练。然而，现有的自动税onomy构建方法通常通过调整模型参数来进行细化。在这篇论文中，我们提出一种通用的税onomy构建框架，考虑结构约束。然后，我们进行了系统比较，通过对hypernym税onomy和一个新的计算机科学税onomy数据集进行提示和细化两种方法的性能。我们的结果显示以下：（1）无需显式训练数据集，提示方法比细化方法表现更好，而且当训练数据集较小时，性能差距更大。然而，（2）通过细化方法生成的税onomy可以轻松地进行后期处理，以满足所有约束，而提示方法生成的税onomy处理抵触的问题可以困难。这些评估结果为选择合适的方法提供指导，并高亮了两种方法的可能的改进。
</details></li>
</ul>
<hr>
<h2 id="MathAttack-Attacking-Large-Language-Models-Towards-Math-Solving-Ability"><a href="#MathAttack-Attacking-Large-Language-Models-Towards-Math-Solving-Ability" class="headerlink" title="MathAttack: Attacking Large Language Models Towards Math Solving Ability"></a>MathAttack: Attacking Large Language Models Towards Math Solving Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01686">http://arxiv.org/abs/2309.01686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, Kaizhu Huang</li>
<li>for: 本研究旨在检测大型自然语言模型（LLMs）在数学问题解决能力方面的安全性。</li>
<li>methods: 我们提出了一种名为MathAttack的模型，用于攻击数学问题样本，以保持原始数学问题的逻辑逻辑。我们首先使用逻辑存在检测来识别逻辑入口，然后使用word-level攻击者对剩下的文本进行攻击。</li>
<li>results: 我们的实验表明，MathAttack可以有效攻击LLMs的数学问题解决能力。我们发现：1）我们的敌意样本从高精度LLMs中生成的样本也能够攻击低精度LLMs（例如，从大到小模型或从多个步骤到零步骤提问中）；2）复杂的数学问题（例如，更多的解决步骤、更长的文本、更多的数字）更容易受到攻击；3）我们可以通过使用我们的反敌样本来提高LLMs的 robustness。<details>
<summary>Abstract</summary>
With the boom of Large Language Models (LLMs), the research of solving Math Word Problem (MWP) has recently made great progress. However, there are few studies to examine the security of LLMs in math solving ability. Instead of attacking prompts in the use of LLMs, we propose a MathAttack model to attack MWP samples which are closer to the essence of security in solving math problems. Compared to traditional text adversarial attack, it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen. Subsequently, the remaining text are attacked by adopting a word-level attacker. Furthermore, we propose a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability. Extensive experiments on our RobustMath and two another math benchmark datasets GSM8K and MultiAirth show that MathAttack could effectively attack the math solving ability of LLMs. In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts. Finally, we hope our practice and observation can serve as an important attempt towards enhancing the robustness of LLMs in math solving ability. We will release our code and dataset.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLMs）的爆发，解决数学问题（MWP）的研究已经做出了很大的进步。然而，有很少的研究检查LLMs在数学问题解决能力的安全性。而不是通过对提示的使用来进行攻击，我们提议一种名为MathAttack的模型来攻击MWP样本，这些样本更加接近安全性的解决数学问题的本质。相比传统的文本恶意攻击，在攻击MWP时更加重要是保持原始MWP的数学逻辑。为此，我们提出了逻辑实体识别，以冻结逻辑实体。然后，剩下的文本使用word-level攻击者进行攻击。此外，我们提出了一个名为RobustMath的新数据集，用于评估LLMs在数学问题解决能力的Robustness。我们在RobustMath和两个其他数学benchmark数据集GSM8K和MultiAirth上进行了广泛的实验，结果表明MathAttack可以有效攻击LLMs的数学问题解决能力。在实验中，我们注意到以下问题：1.我们从高精度LLMs中生成的恶意样本也可以有效地攻击低精度LLMs（例如，从大到小的LLMs或从多少个提示到零个提示）。2.复杂的MWP（如更多的解决步骤、更长的文本、更多的数字）更容易受到攻击。3.我们可以通过使用我们的恶意样本来提高LLMs的Robustness，特别是在几个提示中。最后，我们希望我们的实践和观察可以作为LLMs在数学问题解决能力的Robustness的重要尝试。我们将发布我们的代码和数据集。
</details></li>
</ul>
<hr>
<h2 id="CRUISE-Screening-Living-Literature-Reviews-Toolbox"><a href="#CRUISE-Screening-Living-Literature-Reviews-Toolbox" class="headerlink" title="CRUISE-Screening: Living Literature Reviews Toolbox"></a>CRUISE-Screening: Living Literature Reviews Toolbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01684">http://arxiv.org/abs/2309.01684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/projectdossier/cruise-screening">https://github.com/projectdossier/cruise-screening</a></li>
<li>paper_authors: Wojciech Kusa, Petr Knoth, Allan Hanbury</li>
<li>for: 帮助研究人员快速找到相关研究，提高生成Literature Review的效率和有效性。</li>
<li>methods: 使用文本分类和问答模型帮助屏选相关论文，并通过API与多个搜索引擎连接以更新搜索结果。</li>
<li>results: 开发了一款基于Web应用程序，可以实现活动Literature Review的屏选和搜索，可以帮助研究人员避免手动屏选和搜索，提高工作效率。<details>
<summary>Abstract</summary>
Keeping up with research and finding related work is still a time-consuming task for academics. Researchers sift through thousands of studies to identify a few relevant ones. Automation techniques can help by increasing the efficiency and effectiveness of this task. To this end, we developed CRUISE-Screening, a web-based application for conducting living literature reviews - a type of literature review that is continuously updated to reflect the latest research in a particular field. CRUISE-Screening is connected to several search engines via an API, which allows for updating the search results periodically. Moreover, it can facilitate the process of screening for relevant publications by using text classification and question answering models. CRUISE-Screening can be used both by researchers conducting literature reviews and by those working on automating the citation screening process to validate their algorithms. The application is open-source: https://github.com/ProjectDoSSIER/cruise-screening, and a demo is available under this URL: https://citation-screening.ec.tuwien.ac.at. We discuss the limitations of our tool in Appendix A.
</details>
<details>
<summary>摘要</summary>
保持研究的最新信息和找到相关工作仍然是学术人员的时间消耗任务。研究人员需要从千余篇论文中筛选出一些相关的研究，以增加研究效率和效果。自动化技术可以帮助解决这个问题。为此，我们开发了CRUISE-Screening，一个基于网络的应用程序，用于进行生活文献评估 - 一种Periodically更新的文献评估方法，以反映最新的研究领域。CRUISE-Screening通过API与多个搜索引擎连接，以 periodic更新搜索结果。此外，它还可以通过文本分类和问答模型来帮助屏选相关的论文。CRUISE-Screening可以用于由研究人员进行文献评估，以及用于自动化引用屏选过程的验证。该应用程序是开源的，可以在 GitHub 上找到代码：https://github.com/ProjectDoSSIER/cruise-screening，并可以在以下 URL 上查看示例：https://citation-screening.ec.tuwien.ac.at。我们在 Appendix A 中讨论了我们的工具的限制。
</details></li>
</ul>
<hr>
<h2 id="Donkii-Can-Annotation-Error-Detection-Methods-Find-Errors-in-Instruction-Tuning-Datasets"><a href="#Donkii-Can-Annotation-Error-Detection-Methods-Find-Errors-in-Instruction-Tuning-Datasets" class="headerlink" title="Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?"></a>Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01669">http://arxiv.org/abs/2309.01669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leon Weber-Genzel, Robert Litschko, Ekaterina Artemova, Barbara Plank<br>for:这个论文主要针对的是如何在生成 Setting 中应用 Annotation Error Detection (AED) 方法，以提高 Large Language Models (LLMs) 的训练。methods:这篇论文使用了三个 instruction-tuning 数据集，它们都是由专家和 semi-automatic 方法进行了注释。 authors 还提出了四种 AED 基线方法，并对其进行了全面的评估。results:这篇论文发现，选择正确的 AED 方法和模型大小是非常重要，这有助于提高 instruction-tuning 的性能。 authors 还提供了一个首次的案例研究，以了解 instruction-tuning 数据集的质量如何影响下游性能。<details>
<summary>Abstract</summary>
Instruction-tuning has become an integral part of training pipelines for Large Language Models (LLMs) and has been shown to yield strong performance gains. In an orthogonal line of research, Annotation Error Detection (AED) has emerged as a tool for detecting quality issues of gold-standard labels. But so far, the application of AED methods is limited to discriminative settings. It is an open question how well AED methods generalize to generative settings which are becoming widespread via generative LLMs. In this work, we present a first and new benchmark for AED on instruction-tuning data: Donkii. It encompasses three instruction-tuning datasets enriched with annotations by experts and semi-automatic methods. We find that all three datasets contain clear-cut errors that sometimes directly propagate into instruction-tuned LLMs. We propose four AED baselines for the generative setting and evaluate them comprehensively on the newly introduced dataset. Our results demonstrate that choosing the right AED method and model size is indeed crucial, thereby deriving practical recommendations. To gain insights, we provide a first case-study to examine how the quality of the instruction-tuning datasets influences downstream performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>现在，教程调整（Instruction-tuning）已成为大语言模型（LLMs）训练管道的一个重要组成部分，并且已经显示出强大的性能提升。而在平行的研究方向中，标注错误检测（AED）已经出现为检测黄金标准标签质量问题的工具。但目前，AED方法的应用仅限于推理性Setting中。因此，是一个打开的问题，AED方法在生成Setting中的普遍性如何。在这项工作中，我们提出了一个新的和第一个Benchmark дляAED在教程调整数据上：Donkii。它包括三个 instruciton-tuning 数据集，每个数据集都有专家和半自动方法对标注。我们发现所有三个数据集都含有明确的错误，这些错误直接卷入 instruciton-tuned LLMs。我们提出了四个AED基线 для生成Setting，并在新引入的数据集上进行了全面的评估。我们的结果表明，选择正确的AED方法和模型大小是非常重要，从而得到了实用的建议。为了获得更多的洞察，我们提供了一个首次的案例研究，探讨了下游性能如何受到教程调整数据的质量影响。
</details></li>
</ul>
<hr>
<h2 id="Evolving-linguistic-divergence-on-polarizing-social-media"><a href="#Evolving-linguistic-divergence-on-polarizing-social-media" class="headerlink" title="Evolving linguistic divergence on polarizing social media"></a>Evolving linguistic divergence on polarizing social media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01659">http://arxiv.org/abs/2309.01659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andreskarjus/evolving_divergence">https://github.com/andreskarjus/evolving_divergence</a></li>
<li>paper_authors: Andres Karjus, Christine Cuskley</li>
<li>for: 本研究探讨了美国政治各派别之间语言分化的现象，特别是社交媒体平台上的语言使用差异。</li>
<li>methods: 该研究使用了社交媒体数据 mines, lexicostatistics, machine learning 和大语言模型，并采用了系统的人工注释方法，以描述和量化语言分化的现象。</li>
<li>results: 研究发现，美国政治各派别之间存在语言分化的现象，尤其是在话题和话语方面，与之前的研究一致。这些现象表明，美国英语在不断受到政治各派别的影响，可能会导致语言分化。<details>
<summary>Abstract</summary>
Language change is influenced by many factors, but often starts from synchronic variation, where multiple linguistic patterns or forms coexist, or where different speech communities use language in increasingly different ways. Besides regional or economic reasons, communities may form and segregate based on political alignment. The latter, referred to as political polarization, is of growing societal concern across the world. Here we map and quantify linguistic divergence across the partisan left-right divide in the United States, using social media data. We develop a general methodology to delineate (social) media users by their political preference, based on which (potentially biased) news media accounts they do and do not follow on a given platform. Our data consists of 1.5M short posts by 10k users (about 20M words) from the social media platform Twitter (now "X"). Delineating this sample involved mining the platform for the lists of followers (n=422M) of 72 large news media accounts. We quantify divergence in topics of conversation and word frequencies, messaging sentiment, and lexical semantics of words and emoji. We find signs of linguistic divergence across all these aspects, especially in topics and themes of conversation, in line with previous research. While US American English remains largely intelligible within its large speech community, our findings point at areas where miscommunication may eventually arise given ongoing polarization and therefore potential linguistic divergence. Our methodology - combining data mining, lexicostatistics, machine learning, large language models and a systematic human annotation approach - is largely language and platform agnostic. In other words, while we focus here on US political divides and US English, the same approach is applicable to other countries, languages, and social media platforms.
</details>
<details>
<summary>摘要</summary>
language change 由多种因素 influencing，frequently 从同时变化开始，其中多种语言模式或形式同时存在，或者不同的语言社区使用语言在不同的方式。besides regional or economic reasons, communities may form and segregate based on political alignment. the latter, referred to as political polarization, is of growing societal concern across the world. here we map and quantify linguistic divergence across the partisan left-right divide in the united states, using social media data. we develop a general methodology to delineate (social) media users by their political preference, based on which (potentially biased) news media accounts they do and do not follow on a given platform. our data consists of 1.5 million short posts by 10,000 users (about 20 million words) from the social media platform Twitter (now "X"). delineating this sample involved mining the platform for the lists of followers (n=422 million) of 72 large news media accounts. we quantify divergence in topics of conversation and word frequencies, messaging sentiment, and lexical semantics of words and emoji. we find signs of linguistic divergence across all these aspects, especially in topics and themes of conversation, in line with previous research. while US American English remains largely intelligible within its large speech community, our findings point at areas where miscommunication may eventually arise given ongoing polarization and therefore potential linguistic divergence. our methodology - combining data mining, lexicostatistics, machine learning, large language models and a systematic human annotation approach - is largely language and platform agnostic. in other words, while we focus here on US political divides and US English, the same approach is applicable to other countries, languages, and social media platforms.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-effectiveness-of-ChatGPT-based-feedback-compared-with-teacher-feedback-and-self-feedback-Evidence-from-Chinese-to-English-translation"><a href="#Exploring-the-effectiveness-of-ChatGPT-based-feedback-compared-with-teacher-feedback-and-self-feedback-Evidence-from-Chinese-to-English-translation" class="headerlink" title="Exploring the effectiveness of ChatGPT-based feedback compared with teacher feedback and self-feedback: Evidence from Chinese to English translation"></a>Exploring the effectiveness of ChatGPT-based feedback compared with teacher feedback and self-feedback: Evidence from Chinese to English translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01645">http://arxiv.org/abs/2309.01645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyi Cao, Linping Zhong</li>
<li>for: 本研究是为了比较中国硬件翻译硬件翻译学生在英语作为第二语言中使用ChatGPT反馈的效果，并与教师反馈（TF）和自我反馈（SF）进行比较。</li>
<li>methods: 本研究使用了BLEU分数来衡量翻译质量，以及Coh-Metrix来分析翻译文本中的语言特征。</li>
<li>results: 研究发现，TF和SF带来的翻译文本质量高于ChatGPT反馈，但ChatGPT反馈能够提高翻译文本中的词汇能力和参照相互关系。同时，TF和SF更能够提高翻译文本中的语法能力，特别是正确使用了过去分词。<details>
<summary>Abstract</summary>
ChatGPT,a cutting-edge AI-powered Chatbot,can quickly generate responses on given commands. While it was reported that ChatGPT had the capacity to deliver useful feedback, it is still unclear about its effectiveness compared with conventional feedback approaches,such as teacher feedback (TF) and self-feedback (SF). To address this issue, this study compared the revised Chinese to English translation texts produced by Chinese Master of Translation and Interpretation (MTI) students,who learned English as a Second/Foreign Language (ESL/EFL), based on three feedback types (i.e., ChatGPT-based feedback, TF and SF). The data was analyzed using BLEU score to gauge the overall translation quality as well as Coh-Metrix to examine linguistic features across three dimensions: lexicon, syntax, and cohesion.The findings revealed that TF- and SF-guided translation texts surpassed those with ChatGPT-based feedback, as indicated by the BLEU score. In terms of linguistic features,ChatGPT-based feedback demonstrated superiority, particularly in enhancing lexical capability and referential cohesion in the translation texts. However, TF and SF proved more effective in developing syntax-related skills,as it addressed instances of incorrect usage of the passive voice. These diverse outcomes indicate ChatGPT's potential as a supplementary resource, complementing traditional teacher-led methods in translation practice.
</details>
<details>
<summary>摘要</summary>
chatGPT，一种前沿的人工智能 chatbot，可快速生成对给定命令的回应。尽管报道称 chatGPT 有可能提供有用的反馈，但是它的效果对于传统的反馈方法（如教师反馈和自我反馈）仍然不清楚。为了解决这个问题，本研究比较了由中文翻译和 intérprete 学生（学习英语为第二外语/第二外语）所制定的修改后的中英翻译文本，基于三种反馈类型（即 chatGPT 反馈、教师反馈和自我反馈）。数据分析使用 BLEU 分数来评估翻译质量的总体水平，以及 Coh-Metrix 来检查翻译文本中的三个维度：词汇、 sentence 和 cohesion。研究发现，TF 和 SF 引导的翻译文本在 BLEU 分数上胜过 chatGPT 反馈，而在语言特征方面，chatGPT 反馈表现出优势，特别是在提高翻译文本中的词汇能力和引用共识性。然而，TF 和 SF 更有效地发展了 sentence 结构相关的技能，它解决了 incorrect 使用过去分词的情况。这些多样的结果表明 chatGPT 可以作为辅助资源，与传统的教师带领方法相结合，在翻译实践中发挥作用。
</details></li>
</ul>
<hr>
<h2 id="Critical-Behavioral-Traits-Foster-Peer-Engagement-in-Online-Mental-Health-Communities"><a href="#Critical-Behavioral-Traits-Foster-Peer-Engagement-in-Online-Mental-Health-Communities" class="headerlink" title="Critical Behavioral Traits Foster Peer Engagement in Online Mental Health Communities"></a>Critical Behavioral Traits Foster Peer Engagement in Online Mental Health Communities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01618">http://arxiv.org/abs/2309.01618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aseem Srivastava, Tanya Gupta, Alison Cerezo, Sarah Peregrine, Lord, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>For: This paper aims to explore the factors that drive peer engagement within counseling threads on online mental health communities, such as Reddit, to enhance our understanding of this critical phenomenon.* Methods: The study uses a novel dataset called BeCOPE, which consists of over 10,000 posts and 58,000 comments from 21 mental health-specific subreddits, and is annotated with three major fine-grained behavior labels (intent, criticism, and readability) and emotion labels.* Results: The study finds that self-criticism is the most prevalent form of criticism expressed by help-seekers, and that individuals who explicitly express their need for help are more likely to receive assistance compared to those who present surveys or engage in rants. Additionally, the study highlights the importance of well-articulated problem descriptions in receiving support.<details>
<summary>Abstract</summary>
Online Mental Health Communities (OMHCs), such as Reddit, have witnessed a surge in popularity as go-to platforms for seeking information and support in managing mental health needs. Platforms like Reddit offer immediate interactions with peers, granting users a vital space for seeking mental health assistance. However, the largely unregulated nature of these platforms introduces intricate challenges for both users and society at large. This study explores the factors that drive peer engagement within counseling threads, aiming to enhance our understanding of this critical phenomenon. We introduce BeCOPE, a novel behavior encoded Peer counseling dataset comprising over 10,118 posts and 58,279 comments sourced from 21 mental health-specific subreddits. The dataset is annotated using three major fine-grained behavior labels: (a) intent, (b) criticism, and (c) readability, along with the emotion labels. Our analysis indicates the prominence of ``self-criticism'' as the most prevalent form of criticism expressed by help-seekers, accounting for a significant 43% of interactions. Intriguingly, we observe that individuals who explicitly express their need for help are 18.01% more likely to receive assistance compared to those who present ``surveys'' or engage in ``rants.'' Furthermore, we highlight the pivotal role of well-articulated problem descriptions, showing that superior readability effectively doubles the likelihood of receiving the sought-after support. Our study emphasizes the essential role of OMHCs in offering personalized guidance and unveils behavior-driven engagement patterns.
</details>
<details>
<summary>摘要</summary>
We introduce BeCOPE, a novel dataset comprising over 10,118 posts and 58,279 comments sourced from 21 mental health-specific subreddits. The dataset is annotated with three major fine-grained behavior labels: (a) intent, (b) criticism, and (c) readability, as well as emotion labels. Our analysis reveals that "self-criticism" is the most prevalent form of criticism expressed by help-seekers, accounting for 43% of interactions. Interestingly, we find that individuals who explicitly express their need for help are 18.01% more likely to receive assistance compared to those who present "surveys" or engage in "rants." Furthermore, we highlight the importance of well-articulated problem descriptions, showing that superior readability effectively doubles the likelihood of receiving the sought-after support.Our study emphasizes the crucial role of OMHCs in offering personalized guidance and unveils behavior-driven engagement patterns. These findings have significant implications for the development of OMHCs and the provision of mental health support online. By understanding the factors that drive peer engagement, we can better tailor these platforms to meet the needs of users and improve the overall quality of mental health support.
</details></li>
</ul>
<hr>
<h2 id="Geo-Encoder-A-Chunk-Argument-Bi-Encoder-Framework-for-Chinese-Geographic-Re-Ranking"><a href="#Geo-Encoder-A-Chunk-Argument-Bi-Encoder-Framework-for-Chinese-Geographic-Re-Ranking" class="headerlink" title="Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese Geographic Re-Ranking"></a>Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese Geographic Re-Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01606">http://arxiv.org/abs/2309.01606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Cao, Ruixue Ding, Boli Chen, Xianzhi Li, Min Chen, Daniel Hershcovich, Pengjun Xie, Fei Huang</li>
<li>for: 该论文旨在提高中文地图搜索结果的准确率，以便为地图服务等地理相关应用提供更加有用的结果。</li>
<li>methods: 该论文提出了一种新的框架，即Geo-Encoder，以更好地将中文地理 semantics  интегра到重新排序管道中。该方法首先使用可用的工具将文本与地理 span 相关联，然后提出了一种多任务学习模块，以同时获得一个有效的注意力矩阵，决定chunk的贡献。此外，该论文还提出了一种异步更新机制，以便指导模型更好地关注特定的chunk。</li>
<li>results:  experiments 表明，Geo-Encoder 在两个中文地理搜索数据集上达到了显著提高，相比之前的基eline。特别是，它使得 MGEO-BERT 的 Hit@1 分数从 62.76 提高到 68.98， representing a 6.22% improvement.<details>
<summary>Abstract</summary>
Chinese geographic re-ranking task aims to find the most relevant addresses among retrieved candidates, which is crucial for location-related services such as navigation maps. Unlike the general sentences, geographic contexts are closely intertwined with geographical concepts, from general spans (e.g., province) to specific spans (e.g., road). Given this feature, we propose an innovative framework, namely Geo-Encoder, to more effectively integrate Chinese geographical semantics into re-ranking pipelines. Our methodology begins by employing off-the-shelf tools to associate text with geographical spans, treating them as chunking units. Then, we present a multi-task learning module to simultaneously acquire an effective attention matrix that determines chunk contributions to extra semantic representations. Furthermore, we put forth an asynchronous update mechanism for the proposed addition task, aiming to guide the model capable of effectively focusing on specific chunks. Experiments on two distinct Chinese geographic re-ranking datasets, show that the Geo-Encoder achieves significant improvements when compared to state-of-the-art baselines. Notably, it leads to a substantial improvement in the Hit@1 score of MGEO-BERT, increasing it by 6.22% from 62.76 to 68.98 on the GeoTES dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Comparative-Analysis-of-Pretrained-Language-Models-for-Text-to-Speech"><a href="#A-Comparative-Analysis-of-Pretrained-Language-Models-for-Text-to-Speech" class="headerlink" title="A Comparative Analysis of Pretrained Language Models for Text-to-Speech"></a>A Comparative Analysis of Pretrained Language Models for Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01576">http://arxiv.org/abs/2309.01576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Granero-Moya, Penny Karanasou, Sri Karlapati, Bastian Schnell, Nicole Peinelt, Alexis Moinet, Thomas Drugman</li>
<li>for: 本研究旨在investigate the impact of different pre-trained language models (PLMs) on text-to-speech (TTS) tasks, specifically prosody prediction和pause prediction.</li>
<li>methods: 研究采用了15种不同的PLMs，并对其进行了训练和测试。</li>
<li>results: 发现PLMs的大小和质量之间存在对数关系，并且发现表达和中性表达之间存在显著的性能差异。此外，发现 pause prediction 任务对小型模型的敏感程度较低，并且发现这些语言模型的验证结果和我们的实验结果之间存在强相关性。<details>
<summary>Abstract</summary>
State-of-the-art text-to-speech (TTS) systems have utilized pretrained language models (PLMs) to enhance prosody and create more natural-sounding speech. However, while PLMs have been extensively researched for natural language understanding (NLU), their impact on TTS has been overlooked. In this study, we aim to address this gap by conducting a comparative analysis of different PLMs for two TTS tasks: prosody prediction and pause prediction. Firstly, we trained a prosody prediction model using 15 different PLMs. Our findings revealed a logarithmic relationship between model size and quality, as well as significant performance differences between neutral and expressive prosody. Secondly, we employed PLMs for pause prediction and found that the task was less sensitive to small models. We also identified a strong correlation between our empirical results and the GLUE scores obtained for these language models. To the best of our knowledge, this is the first study of its kind to investigate the impact of different PLMs on TTS.
</details>
<details>
<summary>摘要</summary>
现代文本读取系统（TTS）已经利用预训练语言模型（PLM）提高了语调和创造出更自然的语音。然而，虽然PLM在自然语言理解（NLU）方面得到了广泛的研究，但它们在TTS方面的影响却被忽略了。在这项研究中，我们想要填补这个差距，通过对不同PLM进行比较分析，以探讨它们在两个TTS任务中的表现。首先，我们使用15种不同的PLM进行语调预测模型的训练。我们的发现表明，模型的大小和质量之间存在对数的关系，同时，中性和表达性的语调之间也存在显著的性能差异。其次，我们使用PLM进行停顿预测，发现这个任务对小型模型是更敏感的。我们还发现了这些实验结果和GLUE分数中的语言模型得到的相关性强。根据我们所知，这是第一项研究对TTS中不同PLM的影响的研究。
</details></li>
</ul>
<hr>
<h2 id="What-are-Public-Concerns-about-ChatGPT-A-Novel-Self-Supervised-Neural-Topic-Model-Tells-You"><a href="#What-are-Public-Concerns-about-ChatGPT-A-Novel-Self-Supervised-Neural-Topic-Model-Tells-You" class="headerlink" title="What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You"></a>What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01522">http://arxiv.org/abs/2309.01522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Wang, Xing Liu, Yanan Wang, Haiping Huang</li>
<li>for: 本研究的目的是挖掘对 chatGPT 的公共担忧。</li>
<li>methods: 本研究使用了一种名为 Self-Supervised neural Topic Model (SSTM)，它将话题化模型视为表示学习过程。</li>
<li>results: 实验结果显示，提posed方法可以提取高质量的公共担忧，并且具有更好的解释性和多样性，超过了现有的方法的性能。<details>
<summary>Abstract</summary>
The recently released artificial intelligence conversational agent, ChatGPT, has gained significant attention in academia and real life. A multitude of early ChatGPT users eagerly explore its capabilities and share their opinions on it via social media. Both user queries and social media posts express public concerns regarding this advanced dialogue system. To mine public concerns about ChatGPT, a novel Self-Supervised neural Topic Model (SSTM), which formalizes topic modeling as a representation learning procedure, is proposed in this paper. Extensive experiments have been conducted on Twitter posts about ChatGPT and queries asked by ChatGPT users. And experimental results demonstrate that the proposed approach could extract higher quality public concerns with improved interpretability and diversity, surpassing the performance of state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
Recently released artificial intelligence conversational agent ChatGPT 已经吸引了大量学术和实际应用的关注。许多早期 ChatGPT 用户积极探索其能力并分享他们对其的看法 via 社交媒体。用户提问和社交媒体文章表达了公众对 ChatGPT 的担忧。为了挖掘公众对 ChatGPT 的担忧，本文提出了一种新的 Self-Supervised neural Topic Model (SSTM)，它将话题模型化为表示学习过程的形式。对 Twitter 上关于 ChatGPT 的文章和用户提问进行了广泛的实验，并实验结果表明，提出的方法可以提取更高质量的公众担忧，并且可以提高解释性和多样性，超过了现有的方法的性能。
</details></li>
</ul>
<hr>
<h2 id="LLM-and-Infrastructure-as-a-Code-use-case"><a href="#LLM-and-Infrastructure-as-a-Code-use-case" class="headerlink" title="LLM and Infrastructure as a Code use case"></a>LLM and Infrastructure as a Code use case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01456">http://arxiv.org/abs/2309.01456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thibault Chanus, Michael Aubertin</li>
<li>for: This paper aims to explore the use of Generative LLMs (Language Models) to generate and manage Ansible YAML roles and playbooks, with a focus on identifying potential directions and industrial applications.</li>
<li>methods: The paper employs the use of Ansible and YAML, alongside Generative LLMs, to automate systems administration tasks and translate human descriptions into code.</li>
<li>results: The paper outlines the potential of using Generative LLMs in this context, with the potential for improved efficiency and accuracy in generating and managing Ansible YAML roles and playbooks.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文旨在探讨使用生成式LLM（语言模型）来生成和管理 Ansible YAML 角色和执行脚本，注重发现可能的方向和工业应用。</li>
<li>methods: 论文使用 Ansible 和 YAML，并与生成式LLM 结合，自动化系统管理任务，将人类描述转化为代码。</li>
<li>results: 论文强调使用生成式LLM 在这个上下文中的潜在优势，包括提高代码生成和管理 Ansible YAML 角色和执行脚本的效率和准确性。<details>
<summary>Abstract</summary>
Cloud computing and the evolution of management methodologies such as Lean Management or Agile entail a profound transformation in both system construction and maintenance approaches. These practices are encompassed within the term "DevOps." This descriptive approach to an information system or application, alongside the configuration of its constituent components, has necessitated the development of descriptive languages paired with specialized engines for automating systems administration tasks. Among these, the tandem of Ansible (engine) and YAML (descriptive language) stands out as the two most prevalent tools in the market, facing notable competition mainly from Terraform. The current document presents an inquiry into a solution for generating and managing Ansible YAML roles and playbooks, utilizing Generative LLMs (Language Models) to translate human descriptions into code. Our efforts are focused on identifying plausible directions and outlining the potential industrial applications.   Note: For the purpose of this experiment, we have opted against the use of Ansible Lightspeed. This is due to its reliance on an IBM Watson model, for which we have not found any publicly available references. Comprehensive information regarding this remarkable technology can be found directly on our partner RedHat's website, https://www.redhat.com/en/about/press-releases/red-hat-introduces-ansible-lightspeed-ai-driven-it-automation
</details>
<details>
<summary>摘要</summary>
云计算和流程管理方法的发展，如Lean Management或Agile，对系统建构和维护方法进行了深刻的变革。这些实践被称为“DevOps”。这种描述性方法， alongside the configuration of its constituent components, has led to the development of descriptive languages and specialized engines for automating systems administration tasks. Among these, the pairing of Ansible (engine) and YAML (descriptive language) is the most prevalent in the market, facing significant competition from Terraform.本文档讨论了一种解决方案，使用生成型LLM（语言模型）将人类描述翻译成代码，以便生成和管理Ansible YAML角色和执行脚本。我们的努力专注于找到可能的方向和详细描述相关的工业应用。注意：在这个实验中，我们选择不使用Ansible Lightspeed，因为它基于IBM Watson模型，而我们没有找到任何公开可用的参考。如果您想了解更多关于这一技术的信息，请参考我们的合作伙伴Red Hat的官方网站，https://www.redhat.com/en/about/press-releases/red-hat-introduces-ansible-lightspeed-ai-driven-it-automation。
</details></li>
</ul>
<hr>
<h2 id="NumHG-A-Dataset-for-Number-Focused-Headline-Generation"><a href="#NumHG-A-Dataset-for-Number-Focused-Headline-Generation" class="headerlink" title="NumHG: A Dataset for Number-Focused Headline Generation"></a>NumHG: A Dataset for Number-Focused Headline Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01455">http://arxiv.org/abs/2309.01455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian-Tao Huang, Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen</li>
<li>for: 本研究的目的是提高Headline Generation task的 numeral accuracy, 通过引入新的数据集NumHG，并对5种之前的模型进行人工评估。</li>
<li>methods: 本研究使用了Encoder-Decoder模型，并对数据集进行细致的标注，以便更好地学习和评估 numeral generation。</li>
<li>results: 研究发现，现有的模型在numeral generation方面存在缺陷，特别是在数字的准确性方面。 NumHG数据集可以帮助解决这个问题，并且可以驱动进一步的研究和讨论。<details>
<summary>Abstract</summary>
Headline generation, a key task in abstractive summarization, strives to condense a full-length article into a succinct, single line of text. Notably, while contemporary encoder-decoder models excel based on the ROUGE metric, they often falter when it comes to the precise generation of numerals in headlines. We identify the lack of datasets providing fine-grained annotations for accurate numeral generation as a major roadblock. To address this, we introduce a new dataset, the NumHG, and provide over 27,000 annotated numeral-rich news articles for detailed investigation. Further, we evaluate five well-performing models from previous headline generation tasks using human evaluation in terms of numerical accuracy, reasonableness, and readability. Our study reveals a need for improvement in numerical accuracy, demonstrating the potential of the NumHG dataset to drive progress in number-focused headline generation and stimulate further discussions in numeral-focused text generation.
</details>
<details>
<summary>摘要</summary>
摘要生成，摘要文本生成中的一项关键任务，旨在将全文短化为精炼的一行文本。尤其是当今的编码-解码模型在ROUGE指标上表现出色，但它们在精确生成数字的问题上经常困难。我们认为缺乏精细标注数据为准确数字生成带来了重大障碍。为了解决这一问题，我们提出了一个新的数据集，即NumHG，并为其进行了27,000多个精心标注的新闻文章的 исследова。此外，我们使用人类评估来评估五种以前的摘要生成模型，以确定它们在数字准确性、合理性和可读性方面的表现。我们的研究发现，现有的模型在数字准确性方面存在改进的需求，这也证明了NumHG数据集的潜在作用力，以及数字专注的文本生成领域的进一步探讨。
</details></li>
</ul>
<hr>
<h2 id="Open-Sesame-Universal-Black-Box-Jailbreaking-of-Large-Language-Models"><a href="#Open-Sesame-Universal-Black-Box-Jailbreaking-of-Large-Language-Models" class="headerlink" title="Open Sesame! Universal Black Box Jailbreaking of Large Language Models"></a>Open Sesame! Universal Black Box Jailbreaking of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01446">http://arxiv.org/abs/2309.01446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raz Lapid, Ron Langberg, Moshe Sipper</li>
<li>for: 这篇论文旨在提供一种用于 manipulate 大型自然语言模型（LLM）的方法，以便在模型结构和参数不可访问的情况下实现不良目的。</li>
<li>methods: 这篇论文使用了一种基于遗传算法（GA）的方法，通过优化一个通用对抗提示来让模型偏离它的对抗目标，从而导致模型生成不当的输出。</li>
<li>results: 通过广泛的实验，这篇论文证明了这种方法的有效性，从而为负责任AI开发提供了一种诊断工具，用于评估和提高 LLM 的对人意图的Alignment。这是我们所知道的第一个自动化的通用黑盒子监狱攻击。<details>
<summary>Abstract</summary>
Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM's outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool for evaluating and enhancing alignment of LLMs with human intent. To our knowledge this is the first automated universal black box jailbreak attack.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常采用对齐技术来与用户意图和社会准则进行Alignment。 unfortunately，这种对齐可以被黑客利用，以达到不良目的。在这篇论文中，我们介绍了一种新的方法，使用进化算法（GA）来控制LLM，当模型结构和参数不可访问时。GA攻击工作通过优化通用对抗提示，使模型与用户的查询相结合，导致模型的对齐受到干扰，从而导致不良和可能有害的输出。我们的新方法可以系统地揭示模型的局限和漏洞，通过找到模型的响应与预期行为不符的情况。经过广泛的实验，我们证明了我们的技术的有效性，因此贡献到负责AI开发的讨论中，提供了对LLM的对齐评估和提高的 диагности工具。到我们所知，这是第一个自动化的通用黑盒子监狱攻击。
</details></li>
</ul>
<hr>
<h2 id="Text-Only-Domain-Adaptation-for-End-to-End-Speech-Recognition-through-Down-Sampling-Acoustic-Representation"><a href="#Text-Only-Domain-Adaptation-for-End-to-End-Speech-Recognition-through-Down-Sampling-Acoustic-Representation" class="headerlink" title="Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation"></a>Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02459">http://arxiv.org/abs/2309.02459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Zhu, Weinan Tong, Yaoxun Xu, Changhe Song, Zhiyong Wu, Zhao You, Dan Su, Dong Yu, Helen Meng</li>
<li>for: 提高新频谱频率识别（ASR）性能，使用文本数据进行频率适应。</li>
<li>methods: 使用文本数据进行频率适应，通过下采样音频表示来匹配文本表示的长度。</li>
<li>results: 实验结果表明，提出的方法可以更好地学习两个Modalities的统一表示，从而提高ASR性能。<details>
<summary>Abstract</summary>
Mapping two modalities, speech and text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
将两种modalities，语音和文本，映射到共享表示空间是一个研究话题，用文本只数据提高端到端自动语音识别（ASR）性能在新领域。然而，语音表示长度和文本表示长度不一致。遗传方法通过升降样本来匹配语音模式，但可能不匹配预期的实际持续时间。在这篇论文中，我们提出了一种新的匹配策略，通过降低音频表示来匹配文本模式。通过引入一个连续整合和点火（CIF）模块生成匹配语音表示，我们的ASR模型可以更好地从两个modalities中学习统一表示，允许通过文本只数据进行领域适应。实验结果表明我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="SememeASR-Boosting-Performance-of-End-to-End-Speech-Recognition-against-Domain-and-Long-Tailed-Data-Shift-with-Sememe-Semantic-Knowledge"><a href="#SememeASR-Boosting-Performance-of-End-to-End-Speech-Recognition-against-Domain-and-Long-Tailed-Data-Shift-with-Sememe-Semantic-Knowledge" class="headerlink" title="SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge"></a>SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01437">http://arxiv.org/abs/2309.01437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Zhu, Changhe Song, Zhiyong Wu, Helen Meng</li>
<li>for: 提高语音识别的效果，尤其是在域外和长尾数据上</li>
<li>methods: 利用sememe知识来增强语音识别模型</li>
<li>results: 实验表明，sememe知识可以提高语音识别的效果，并且可以提高模型对域外和长尾数据的识别能力<details>
<summary>Abstract</summary>
Recently, excellent progress has been made in speech recognition. However, pure data-driven approaches have struggled to solve the problem in domain-mismatch and long-tailed data. Considering that knowledge-driven approaches can help data-driven approaches alleviate their flaws, we introduce sememe-based semantic knowledge information to speech recognition (SememeASR). Sememe, according to the linguistic definition, is the minimum semantic unit in a language and is able to represent the implicit semantic information behind each word very well. Our experiments show that the introduction of sememe information can improve the effectiveness of speech recognition. In addition, our further experiments show that sememe knowledge can improve the model's recognition of long-tailed data and enhance the model's domain generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Benchmarking-Large-Language-Models-in-Retrieval-Augmented-Generation"><a href="#Benchmarking-Large-Language-Models-in-Retrieval-Augmented-Generation" class="headerlink" title="Benchmarking Large Language Models in Retrieval-Augmented Generation"></a>Benchmarking Large Language Models in Retrieval-Augmented Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01431">http://arxiv.org/abs/2309.01431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chen700564/RGB">https://github.com/chen700564/RGB</a></li>
<li>paper_authors: Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun</li>
<li>for: This paper aims to evaluate the impact of Retrieval-Augmented Generation (RAG) on large language models (LLMs) and identify potential bottlenecks in their capabilities.</li>
<li>methods: The paper uses a systematic approach to investigate the impact of RAG on LLMs, including the establishment of a new corpus (RGB) and the evaluation of 6 representative LLMs on RGB.</li>
<li>results: The evaluation reveals that while LLMs exhibit some degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information, indicating that there is still a considerable journey ahead to effectively apply RAG to LLMs.<details>
<summary>Abstract</summary>
Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的幻觉可以通过 Retrieval-Augmented Generation（RAG）方法进行缓解。然而，现有的研究缺乏对不同的大量语言模型RAG的精心评估，这使得了解RAG对不同LLM的可能的瓶颈困难。在这篇论文中，我们系统地研究了RAG对大量语言模型的影响。我们分析了不同的大量语言模型在4种基本能力上的表现，包括噪声抵抗、负面排斥、信息集成和Counterfactual Robustness。为此，我们创建了一个新的评估 benchmark，即 Retrieval-Augmented Generation Benchmark（RGB），该benchmark包含了4种分别测试基础能力的测试床。然后，我们评估了6种代表性强的LLM在RGB上，以诊断当前LLM在RAG应用中的挑战。评估结果表明，虽然LLM具有一定的噪声抵抗能力，但仍然在负面排斥、信息集成和面对假信息方面存在显著的困难。上述评估结果表明，在RAG应用中，还有一定的征途需要进行。
</details></li>
</ul>
<hr>
<h2 id="Hateful-Messages-A-Conversational-Data-Set-of-Hate-Speech-produced-by-Adolescents-on-Discord"><a href="#Hateful-Messages-A-Conversational-Data-Set-of-Hate-Speech-produced-by-Adolescents-on-Discord" class="headerlink" title="Hateful Messages: A Conversational Data Set of Hate Speech produced by Adolescents on Discord"></a>Hateful Messages: A Conversational Data Set of Hate Speech produced by Adolescents on Discord</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01413">http://arxiv.org/abs/2309.01413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Fillies, Silvio Peikert, Adrian Paschke</li>
<li>For: The paper aims to address the bias of youth language within hate speech classification and provide a modern and anonymized hate speech youth language data set.* Methods: The research uses a self-developed annotation schema to classify publicly available online messages from the chat platform Discord, with 6.42% of the messages classified as hate speech. The data set includes age annotations for 35,553 messages, with an average author age of under 20 years old.* Results: The paper provides a modern and anonymized hate speech youth language data set consisting of 88,395 annotated chat messages, which can be used to improve the generalizability and performance of automated hate speech classification systems.<details>
<summary>Abstract</summary>
With the rise of social media, a rise of hateful content can be observed. Even though the understanding and definitions of hate speech varies, platforms, communities, and legislature all acknowledge the problem. Therefore, adolescents are a new and active group of social media users. The majority of adolescents experience or witness online hate speech. Research in the field of automated hate speech classification has been on the rise and focuses on aspects such as bias, generalizability, and performance. To increase generalizability and performance, it is important to understand biases within the data. This research addresses the bias of youth language within hate speech classification and contributes by providing a modern and anonymized hate speech youth language data set consisting of 88.395 annotated chat messages. The data set consists of publicly available online messages from the chat platform Discord. ~6,42% of the messages were classified by a self-developed annotation schema as hate speech. For 35.553 messages, the user profiles provided age annotations setting the average author age to under 20 years old.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Zero-shot-information-extraction-from-radiological-reports-using-ChatGPT"><a href="#Zero-shot-information-extraction-from-radiological-reports-using-ChatGPT" class="headerlink" title="Zero-shot information extraction from radiological reports using ChatGPT"></a>Zero-shot information extraction from radiological reports using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01398">http://arxiv.org/abs/2309.01398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danqing Hu, Bing Liu, Xiaofeng Zhu, Xudong Lu, Nan Wu</li>
<li>for: 抽象出 radiological report 中有用信息，以便进行第二次分析。</li>
<li>methods: 使用 ChatGPT 大语言模型进行零参数信息提取，不需要注释数据进行模型参数优化。</li>
<li>results: ChatGPT 可以在 847 份 CT 报告中提取有用信息，但还需要进一步改进以提高性能。<details>
<summary>Abstract</summary>
Electronic health records contain an enormous amount of valuable information, but many are recorded in free text. Information extraction is the strategy to transform the sequence of characters into structured data, which can be employed for secondary analysis. However, the traditional information extraction components, such as named entity recognition and relation extraction, require annotated data to optimize the model parameters, which has become one of the major bottlenecks in building information extraction systems. With the large language models achieving good performances on various downstream NLP tasks without parameter tuning, it becomes possible to use large language models for zero-shot information extraction. In this study, we aim to explore whether the most popular large language model, ChatGPT, can extract useful information from the radiological reports. We first design the prompt template for the interested information in the CT reports. Then, we generate the prompts by combining the prompt template with the CT reports as the inputs of ChatGPT to obtain the responses. A post-processing module is developed to transform the responses into structured extraction results. We conducted the experiments with 847 CT reports collected from Peking University Cancer Hospital. The experimental results indicate that ChatGPT can achieve competitive performances for some extraction tasks compared with the baseline information extraction system, but some limitations need to be further improved.
</details>
<details>
<summary>摘要</summary>
电子健康记录包含巨量有价值信息，但许多都记录在自由文本中。信息提取是将字符串序列转换为结构化数据的策略，可以用于次要分析。然而，传统信息提取组件，如命名实体识别和关系EXTRACTION，需要标注数据来优化模型参数，这成为了建立信息提取系统的一个主要瓶颈。随着大语言模型在多个下游NLP任务中达到好表现，不需要参数调整，因此可以使用大语言模型进行零shot信息提取。在这项研究中，我们想要探索是否可以使用最受欢迎的大语言模型ChatGPT提取CT报告中的有用信息。我们首先设计了关注信息的模板Prompt，然后将Prompt与CT报告结合使用ChatGPT来获取响应。我们还开发了一个后处程模块，用于将响应转换为结构化提取结果。我们对847份CT报告进行了实验，结果表明ChatGPT可以与基准信息提取系统相比，在某些提取任务中达到竞争性表现，但还需要进一步改进。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.CL_2023_09_04/" data-id="clorjzl4700alf1889cntdzxz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.LG_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T10:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.LG_2023_09_04/">cs.LG - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Delegating-Data-Collection-in-Decentralized-Machine-Learning"><a href="#Delegating-Data-Collection-in-Decentralized-Machine-Learning" class="headerlink" title="Delegating Data Collection in Decentralized Machine Learning"></a>Delegating Data Collection in Decentralized Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01837">http://arxiv.org/abs/2309.01837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nivasini Ananthakrishnan, Stephen Bates, Michael I. Jordan, Nika Haghtalab</li>
<li>for: 研究了数据收集委托的优化问题，尤其是面临不确定评估模型质量和无知对优化模型性能的两个基本机器学习挑战。</li>
<li>methods: 基于合同理论开始，设计优化和近优化合同来解决两个挑战，并通过简单的线性合同实现1-1&#x2F;e的最优Utility，即使主体只有小考试集。此外，我们还给出了测试集大小的条件，以实现趋近于优化Utility的添加性近似。</li>
<li>results: 我们显示了在不确定评估模型质量和无知对优化模型性能的情况下，可以通过简单的线性合同和可靠的算法来解决这两个挑战，并实现高效的数据收集委托。<details>
<summary>Abstract</summary>
Motivated by the emergence of decentralized machine learning ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental machine learning challenges: lack of certainty in the assessment of model quality and lack of knowledge regarding the optimal performance of any model. We show that lack of certainty can be dealt with via simple linear contracts that achieve 1-1/e fraction of the first-best utility, even if the principal has a small test set. Furthermore, we give sufficient conditions on the size of the principal's test set that achieves a vanishing additive approximation to the optimal utility. To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract.
</details>
<details>
<summary>摘要</summary>
驱动了分布式机器学习生态系统的出现，我们研究数据采集委托的问题。从ontract理论为起点，我们设计优化和近似优化的合同，解决机器学习中两个基本挑战：评估模型质量的不确定性和任何模型的优化性知识不够。我们显示，不确定性可以通过简单的线性合同来处理，实现1-1/e的首选utililty，即使主体只有一小部分的测试集。此外，我们给出了测试集的大小必须满足的条件，以实现趋近于优化的utililty。为了解决无先知道优化性的问题，我们给出了一种可靠和高效的 convex program，可以动态计算优化的合同。</systext>Note: "systext" is the Simplified Chinese character set, which is a standardized form of Chinese used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Soft-Dropout-A-Practical-Approach-for-Mitigating-Overfitting-in-Quantum-Convolutional-Neural-Networks"><a href="#Soft-Dropout-A-Practical-Approach-for-Mitigating-Overfitting-in-Quantum-Convolutional-Neural-Networks" class="headerlink" title="Soft-Dropout: A Practical Approach for Mitigating Overfitting in Quantum Convolutional Neural Networks"></a>Soft-Dropout: A Practical Approach for Mitigating Overfitting in Quantum Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01829">http://arxiv.org/abs/2309.01829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aakash Ravindra Shinde, Charu Jain, Amir Kalev</li>
<li>for: 这 paper 是为了研究量子卷积神经网络（QCNN）中的过拟合问题。</li>
<li>methods: 这 paper 使用了一种经典的过拟合 mitigation 方法，即在训练后添加 dropout 方法。</li>
<li>results: 研究发现，在量子设定下直接实现 dropout 方法会导致 QCNN 的成功率减少。此外，提出了一种更加温和的 dropout 方法，可以成功地处理 QCNN 中的过拟合问题。<details>
<summary>Abstract</summary>
Quantum convolutional neural network (QCNN), an early application for quantum computers in the NISQ era, has been consistently proven successful as a machine learning (ML) algorithm for several tasks with significant accuracy. Derived from its classical counterpart, QCNN is prone to overfitting. Overfitting is a typical shortcoming of ML models that are trained too closely to the availed training dataset and perform relatively poorly on unseen datasets for a similar problem. In this work we study the adaptation of one of the most successful overfitting mitigation method, knows as the (post-training) dropout method, to the quantum setting. We find that a straightforward implementation of this method in the quantum setting leads to a significant and undesirable consequence: a substantial decrease in success probability of the QCNN. We argue that this effect exposes the crucial role of entanglement in QCNNs and the vulnerability of QCNNs to entanglement loss. To handle overfitting, we proposed a softer version of the dropout method. We find that the proposed method allows us to handle successfully overfitting in the test cases.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Secure-and-Efficient-Federated-Learning-in-LEO-Constellations-using-Decentralized-Key-Generation-and-On-Orbit-Model-Aggregation"><a href="#Secure-and-Efficient-Federated-Learning-in-LEO-Constellations-using-Decentralized-Key-Generation-and-On-Orbit-Model-Aggregation" class="headerlink" title="Secure and Efficient Federated Learning in LEO Constellations using Decentralized Key Generation and On-Orbit Model Aggregation"></a>Secure and Efficient Federated Learning in LEO Constellations using Decentralized Key Generation and On-Orbit Model Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01828">http://arxiv.org/abs/2309.01828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Elmahallawy, Tie Luo, Mohamed I. Ibrahem</li>
<li>for: 这篇论文是为了解决在低地球轨道（LEO）上运行小卫星的资料下载和联合学习问题。</li>
<li>methods: 这篇论文提出了一个名为 FedSecure 的安全联合学习方法，它包括两个新的 ком成分：（1）分散的钥匙生成，以保护卫星数据的隐私使用函数加密方案，和（2）在轨道上进行模型传输和聚合，从而实现每个轨道的部分全球模型，以最小化遗失可见区域的对等待时间。</li>
<li>results: 我们的分析和结果显示，FedSecure 可以保护每个卫星的数据免受窃听者、curious server 或 curious satellite 的披露，并且具有较低的通信和计算负载，从而实现高精度（达85.35%）的联合学习。<details>
<summary>Abstract</summary>
Satellite technologies have advanced drastically in recent years, leading to a heated interest in launching small satellites into low Earth orbit (LEOs) to collect massive data such as satellite imagery. Downloading these data to a ground station (GS) to perform centralized learning to build an AI model is not practical due to the limited and expensive bandwidth. Federated learning (FL) offers a potential solution but will incur a very large convergence delay due to the highly sporadic and irregular connectivity between LEO satellites and GS. In addition, there are significant security and privacy risks where eavesdroppers or curious servers/satellites may infer raw data from satellites' model parameters transmitted over insecure communication channels. To address these issues, this paper proposes FedSecure, a secure FL approach designed for LEO constellations, which consists of two novel components: (1) decentralized key generation that protects satellite data privacy using a functional encryption scheme, and (2) on-orbit model forwarding and aggregation that generates a partial global model per orbit to minimize the idle waiting time for invisible satellites to enter the visible zone of the GS. Our analysis and results show that FedSecure preserves the privacy of each satellite's data against eavesdroppers, a curious server, or curious satellites. It is lightweight with significantly lower communication and computation overheads than other privacy-preserving FL aggregation approaches. It also reduces convergence delay drastically from days to only a few hours, yet achieving high accuracy of up to 85.35% using realistic satellite images.
</details>
<details>
<summary>摘要</summary>
卫星技术在最近几年内发展了非常快，导致低地球轨道（LEO）上发射小卫星来收集大量数据，如卫星成像。由于下载这些数据到地面站（GS）以进行中央学习并建立人工智能模型是不实际的，因为卫星和GS之间的带宽是有限且昂贵的。联邦学习（FL）提供了一个可能的解决方案，但是它会产生非常大的融合延迟，因为LEO卫星和GS之间的连接是不规则和不可预测的。此外，在卫星和GS之间的通信频道上存在严重的安全和隐私风险，可能导致卫星数据的泄露。为解决这些问题，这篇论文提出了FedSecure，一种安全的联邦学习方法，包括两个新的组件：1. 分布式密钥生成，通过功能加密方案保护卫星数据隐私。2. 在轨道上进行模型转发和聚合，每次轨道执行一个部分全球模型，以最小化隐藏在视野外的卫星等待时间。我们的分析和结果表明，FedSecure可以保护每个卫星的数据隐私，并且具有较低的通信和计算开销，相比其他隐私保护的聚合方法。它还可以减少融合延迟从几天减少到只需几个小时，同时实现高准确率（达85.35%）。
</details></li>
</ul>
<hr>
<h2 id="LoopTune-Optimizing-Tensor-Computations-with-Reinforcement-Learning"><a href="#LoopTune-Optimizing-Tensor-Computations-with-Reinforcement-Learning" class="headerlink" title="LoopTune: Optimizing Tensor Computations with Reinforcement Learning"></a>LoopTune: Optimizing Tensor Computations with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01825">http://arxiv.org/abs/2309.01825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dejan Grubisic, Bram Wasti, Chris Cummins, John Mellor-Crummey, Aleksandar Zlateski</li>
<li>for: 这篇论文是为了解决高性能机器学习应用在新硬件上运行的问题，但传统的编译器无法提供性能。</li>
<li>methods: 这篇论文使用了深度学习自适应优化技术，开发了一个名为LoopTune的深度学习编译器，可以优化深度学习模型中的矩阵计算在CPU上。LoopTune使用了ultra-fast lightweight代码生成器LoopNest进行硬件特定优化。</li>
<li>results: LoopTune可以减少LoopNest的搜索时间，并且可以生成论文速度比TVM、MetaSchedule和AutoTVM快，consistently performing at the level of the hand-tuned library Numpy。此外，LoopTune可以在秒钟级别进行代码优化。<details>
<summary>Abstract</summary>
Advanced compiler technology is crucial for enabling machine learning applications to run on novel hardware, but traditional compilers fail to deliver performance, popular auto-tuners have long search times and expert-optimized libraries introduce unsustainable costs. To address this, we developed LoopTune, a deep reinforcement learning compiler that optimizes tensor computations in deep learning models for the CPU. LoopTune optimizes tensor traversal order while using the ultra-fast lightweight code generator LoopNest to perform hardware-specific optimizations. With a novel graph-based representation and action space, LoopTune speeds up LoopNest by 3.2x, generating an order of magnitude faster code than TVM, 2.8x faster than MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order of seconds.
</details>
<details>
<summary>摘要</summary>
高级编译技术对机器学习应用的运行是关键，但传统的编译器无法提供性能。常用的自动调参器有很长的搜索时间，专家优化库会带来不可持续的成本。为解决这问题，我们开发了 LoopTune，一种基于深度学习的编译器，用于优化深度学习模型中的矩阵计算。LoopTune优化矩阵游标顺序，并使用 ultra-fast 轻量级代码生成器 LoopNest 进行硬件特定优化。使用图表 Representation 和 Action 空间，LoopTune 将 LoopNest 加速了 3.2 倍，生成的代码比 TVM 快了一个数量级，比 MetaSchedule 快了 2.8 倍，和 AutoTVM 快了 1.08 倍，一直保持与手动优化库 Numpy 的水平。此外，LoopTune 只需几秒钟来调参代码。
</details></li>
</ul>
<hr>
<h2 id="Computation-and-Communication-Efficient-Federated-Learning-over-Wireless-Networks"><a href="#Computation-and-Communication-Efficient-Federated-Learning-over-Wireless-Networks" class="headerlink" title="Computation and Communication Efficient Federated Learning over Wireless Networks"></a>Computation and Communication Efficient Federated Learning over Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01816">http://arxiv.org/abs/2309.01816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaonan Liu, Tharmalingam Ratnarajah</li>
<li>for: 提高 Federated Learning （FL）模型训练的精度和效率，同时保持数据隐私。</li>
<li>methods: 提出一种基于 partial model pruning 和个性化的 FL 框架，将学习模型分为全球部分和个性化部分，以适应各个设备的非独立同分布（non IID）数据。</li>
<li>results: 通过数学分析和优化问题的解法，提高 FL 框架的计算和通信负载，同时提高学习精度和速度。实验结果显示，提议的 FL 框架可以降低大约 50% 的计算和通信负载。<details>
<summary>Abstract</summary>
Federated learning (FL) allows model training from local data by edge devices while preserving data privacy. However, the learning accuracy decreases due to the heterogeneity of devices data, and the computation and communication latency increase when updating large scale learning models on devices with limited computational capability and wireless resources. To overcome these challenges, we consider a novel FL framework with partial model pruning and personalization. This framework splits the learning model into a global part with model pruning shared with all devices to learn data representations and a personalized part to be fine tuned for a specific device, which adapts the model size during FL to reduce both computation and communication overhead and minimize the overall training time, and increases the learning accuracy for the device with non independent and identically distributed (non IID) data. Then, the computation and communication latency and the convergence analysis of the proposed FL framework are mathematically analyzed. Based on the convergence analysis, an optimization problem is formulated to maximize the convergence rate under a latency threshold by jointly optimizing the pruning ratio and wireless resource allocation. By decoupling the optimization problem and deploying Karush Kuhn Tucker (KKT) conditions, we derive the closed form solutions of pruning ratio and wireless resource allocation. Finally, experimental results demonstrate that the proposed FL framework achieves a remarkable reduction of approximately 50 percents computation and communication latency compared with the scheme only with model personalization.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）允许本地数据进行模型训练，同时保持数据隐私。然而，由于设备数据的不同性，学习准确率减少，并且更新大规模学习模型在设备上的计算和通信负担增加。为了解决这些挑战，我们提出了一种新的FL框架，其中分解学习模型为全球部分和个性化部分。全球部分通过模型剔除来学习数据表示，而个性化部分在特定设备上进行细化调整，以适应非独立同分布（non IID）数据。这种框架可以降低计算和通信延迟，提高学习精度，并最大化 converges 率。然后，我们数学分析了计算和通信延迟以及折衔率的影响。基于折衔率的最大化，我们提出了一个优化问题，以提高 converges 率下的延迟阈值。通过分解优化问题并应用 KKT 条件，我们得到了封闭式的解决方案。最后，我们通过实验证明，提出的FL框架可以降低约50%的计算和通信延迟。
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-matrix-sensing-by-gradient-descent-with-small-random-initialization"><a href="#Asymmetric-matrix-sensing-by-gradient-descent-with-small-random-initialization" class="headerlink" title="Asymmetric matrix sensing by gradient descent with small random initialization"></a>Asymmetric matrix sensing by gradient descent with small random initialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01796">http://arxiv.org/abs/2309.01796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan S. Wind</li>
<li>for:  matrix sensing problem, reconstructing low-rank matrix from linear measurements</li>
<li>methods: factorized gradient descent, continuous differential equation (perturbed gradient flow)</li>
<li>results: quick convergence to true target matrix with bounded perturbation, novel proof of asymmetric matrix sensing<details>
<summary>Abstract</summary>
We study matrix sensing, which is the problem of reconstructing a low-rank matrix from a few linear measurements. It can be formulated as an overparameterized regression problem, which can be solved by factorized gradient descent when starting from a small random initialization.   Linear neural networks, and in particular matrix sensing by factorized gradient descent, serve as prototypical models of non-convex problems in modern machine learning, where complex phenomena can be disentangled and studied in detail. Much research has been devoted to studying special cases of asymmetric matrix sensing, such as asymmetric matrix factorization and symmetric positive semi-definite matrix sensing.   Our key contribution is introducing a continuous differential equation that we call the $\textit{perturbed gradient flow}$. We prove that the perturbed gradient flow converges quickly to the true target matrix whenever the perturbation is sufficiently bounded. The dynamics of gradient descent for matrix sensing can be reduced to this formulation, yielding a novel proof of asymmetric matrix sensing with factorized gradient descent. Compared to directly analyzing the dynamics of gradient descent, the continuous formulation allows bounding key quantities by considering their derivatives, often simplifying the proofs. We believe the general proof technique may prove useful in other settings as well.
</details>
<details>
<summary>摘要</summary>
我们研究矩阵感知问题，即从一些线性测量中重建一个低级矩阵的问题。可以将其形式化为过参数化回归问题，可以通过分解梯度下降来解决，当起始于一个小random initialization时。  linear neural networks 和特别是矩阵感知通过分解梯度下降是现代机器学习中非 convex 问题的典型模型，其中复杂现象可以分解和研究在详细的方式上。许多研究者已经投入到 изучение特殊情况的偏 asymmetric matrix sensing 中，如偏 asymmetric matrix factorization 和Symmetric positive semi-definite matrix sensing。我们的关键贡献在于引入一个名为 $\textit{perturbed gradient flow}$ 的连续偏微分方程。我们证明了当perturbation够小时，这个方程快速地 converge 到真正的目标矩阵。矩阵感知的梯度下降动力学可以被归纳到这个形式化中，从而得到一种新的证明方式。与直接分析梯度下降动力学相比，连续形式化允许通过考虑其导数来简化证明。我们认为这种普适的证明技巧可能在其他情况下也会有用。
</details></li>
</ul>
<hr>
<h2 id="Composite-federated-learning-with-heterogeneous-data"><a href="#Composite-federated-learning-with-heterogeneous-data" class="headerlink" title="Composite federated learning with heterogeneous data"></a>Composite federated learning with heterogeneous data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01795">http://arxiv.org/abs/2309.01795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaojiao Zhang, Jiang Hu, Mikael Johansson</li>
<li>for: 解决复杂的 Federated Learning（FL）问题</li>
<li>methods: 使用战略性分解质量梯度和通信，并不假设数据相似性来避免客户端漂移</li>
<li>results: 比前者更高效，可以 linearly 收敛到一个 neighborhood 的优解Here’s a breakdown of each point:1. for: The paper is written to solve the composite Federated Learning (FL) problem.2. methods: The paper proposes a novel algorithm that manages non-smooth regularization by decoupling the proximal operator and communication, and addresses client drift without assuming data similarity. Each worker uses local updates to reduce communication frequency with the server and transmits only a $d$-dimensional vector per communication round.3. results: The algorithm is proven to converge linearly to a neighborhood of the optimal solution, and the paper demonstrates the superiority of the algorithm over state-of-the-art methods in numerical experiments.<details>
<summary>Abstract</summary>
We propose a novel algorithm for solving the composite Federated Learning (FL) problem. This algorithm manages non-smooth regularization by strategically decoupling the proximal operator and communication, and addresses client drift without any assumptions about data similarity. Moreover, each worker uses local updates to reduce the communication frequency with the server and transmits only a $d$-dimensional vector per communication round. We prove that our algorithm converges linearly to a neighborhood of the optimal solution and demonstrate the superiority of our algorithm over state-of-the-art methods in numerical experiments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的 Federation Learning（FL）问题的算法。该算法在规范化正则化中使用推迟分离 proximal 算符和通信，并 Addresses 客户端漂移无需数据相似性假设。另外，每个工作者使用本地更新减少与服务器之间的通信频率，并只在通信轮次中发送 $d$-维向量。我们证明了我们的算法 linearly 收敛到优解附近的解，并在数值实验中证明了我们的算法与现状算法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Grammar-Induced-Geometry-for-Data-Efficient-Molecular-Property-Prediction"><a href="#Hierarchical-Grammar-Induced-Geometry-for-Data-Efficient-Molecular-Property-Prediction" class="headerlink" title="Hierarchical Grammar-Induced Geometry for Data-Efficient Molecular Property Prediction"></a>Hierarchical Grammar-Induced Geometry for Data-Efficient Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01788">http://arxiv.org/abs/2309.01788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gmh14/geo-deg">https://github.com/gmh14/geo-deg</a></li>
<li>paper_authors: Minghao Guo, Veronika Thost, Samuel W Song, Adithya Balachandran, Payel Das, Jie Chen, Wojciech Matusik</li>
<li>for: 这项研究的目的是提出一种数据效率的物理属性预测方法，以便在材料和药物发现领域中进行预测。</li>
<li>methods: 该方法使用学习式层次分子 grammar，可以生成分子结构从 grammar 生成规则。这种 grammar 适应了分子结构空间的显式几何结构，从而提供了有用的分子结构相似性的几何信息。 property 预测使用图 neural diffusion 在 grammar-induced 几何空间中进行。</li>
<li>results: 在小规模和大规模数据集上，我们的评估表明，这种方法可以比supervised和预训练图 neural network 等基准方法表现出色，并且在具有极其有限数据情况下进行预测时表现出色。我们还包括了细化的减少研究和进一步分析，以证明我们的解决方案的有效性。<details>
<summary>Abstract</summary>
The prediction of molecular properties is a crucial task in the field of material and drug discovery. The potential benefits of using deep learning techniques are reflected in the wealth of recent literature. Still, these techniques are faced with a common challenge in practice: Labeled data are limited by the cost of manual extraction from literature and laborious experimentation. In this work, we propose a data-efficient property predictor by utilizing a learnable hierarchical molecular grammar that can generate molecules from grammar production rules. Such a grammar induces an explicit geometry of the space of molecular graphs, which provides an informative prior on molecular structural similarity. The property prediction is performed using graph neural diffusion over the grammar-induced geometry. On both small and large datasets, our evaluation shows that this approach outperforms a wide spectrum of baselines, including supervised and pre-trained graph neural networks. We include a detailed ablation study and further analysis of our solution, showing its effectiveness in cases with extremely limited data. Code is available at https://github.com/gmh14/Geo-DEG.
</details>
<details>
<summary>摘要</summary>
“分子性能预测是物质和药物搜索领域中的一项关键任务。 latest literature 中的 potential benefits 反映了使用深度学习技术的可能性。然而，这些技术在实践中受到一种常见的挑战：标注数据受到文献EXTRACTION AND laborious experimentation的成本限制。在这种情况下，我们提出了一种数据效率的属性预测器，利用可学习的分子语法树来生成分子。这种语法树 induces 分子图的Explicit geometry，提供了有用的分子结构相似性的假设。我们使用图解 diffusion 来预测分子的属性，并在小和大数据集上评估了我们的方法。结果表明，我们的方法可以比supervised和预训练图神经网络Outperform。我们还进行了详细的折衣分析和进一步的分析，证明了我们的解决方案在数据受限情况下的有效性。代码可以在https://github.com/gmh14/Geo-DEG中找到。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="ATMS-Algorithmic-Trading-Guided-Market-Simulation"><a href="#ATMS-Algorithmic-Trading-Guided-Market-Simulation" class="headerlink" title="ATMS: Algorithmic Trading-Guided Market Simulation"></a>ATMS: Algorithmic Trading-Guided Market Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01784">http://arxiv.org/abs/2309.01784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Wei, Andrea Coletta, Svitlana Vyetrenko, Tucker Balch</li>
<li>For: The paper aims to propose a metric to quantify market discrepancy and develop an Algorithmic Trading-guided Market Simulation (ATMS) to improve the realism of market simulations.* Methods: The proposed metric measures the difference between a causal effect from underlying market unique characteristics and is evaluated through the interaction between the AT agent and the market. ATMS formulates the simulator as a stochastic policy in reinforcement learning (RL) to account for the sequential nature of trading, and utilizes the policy gradient update to bypass differentiating the proposed metric.* Results: The proposed metric and ATMS are demonstrated to be effective through extensive experiments on semi-real market data, showing improved similarity to reality compared to the state-of-the-art conditional Wasserstein Generative Adversarial Network (cWGAN) approach, and producing market data with more balanced BUY and SELL volumes.<details>
<summary>Abstract</summary>
The effective construction of an Algorithmic Trading (AT) strategy often relies on market simulators, which remains challenging due to existing methods' inability to adapt to the sequential and dynamic nature of trading activities. This work fills this gap by proposing a metric to quantify market discrepancy. This metric measures the difference between a causal effect from underlying market unique characteristics and it is evaluated through the interaction between the AT agent and the market. Most importantly, we introduce Algorithmic Trading-guided Market Simulation (ATMS) by optimizing our proposed metric. Inspired by SeqGAN, ATMS formulates the simulator as a stochastic policy in reinforcement learning (RL) to account for the sequential nature of trading. Moreover, ATMS utilizes the policy gradient update to bypass differentiating the proposed metric, which involves non-differentiable operations such as order deletion from the market. Through extensive experiments on semi-real market data, we demonstrate the effectiveness of our metric and show that ATMS generates market data with improved similarity to reality compared to the state-of-the-art conditional Wasserstein Generative Adversarial Network (cWGAN) approach. Furthermore, ATMS produces market data with more balanced BUY and SELL volumes, mitigating the bias of the cWGAN baseline approach, where a simple strategy can exploit the BUY/SELL imbalance for profit.
</details>
<details>
<summary>摘要</summary>
通常，建立一个Algorithmic Trading（AT）策略的有效构造往往依赖市场模拟器，但是现有方法很难适应交易活动的顺序和动态性。这种工作填补了这一空白，提出了一个用于量化市场差异的度量。这个度量测量了 causal effect的差异，它通过AT代理和市场的交互来评估。更重要的是，我们引入了Algorithmic Trading-guided Market Simulation（ATMS），通过优化我们的提出的度量来寻找最佳的市场模拟器。受SeqGAN的启发，ATMS将市场模拟器形式化为一个随机政策，以便考虑交易的顺序性。此外，ATMS使用策略梯度更新来绕过非导数的操作，如市场中的订单删除。通过对半实际市场数据进行了广泛的实验，我们证明了我们的度量的有效性，并显示ATMS生成的市场数据与现有的conditional Wasserstein Generative Adversarial Network（cWGAN）方法相比，具有更高的实际性。此外，ATMS生成的市场数据具有更好的BUY和SELL量均衡，从而减轻cWGAN基础方法的偏见，其中一个简单的策略可以通过BUY/SELL偏见来获得利润。
</details></li>
</ul>
<hr>
<h2 id="Survival-Prediction-from-Imbalance-colorectal-cancer-dataset-using-hybrid-sampling-methods-and-tree-based-classifiers"><a href="#Survival-Prediction-from-Imbalance-colorectal-cancer-dataset-using-hybrid-sampling-methods-and-tree-based-classifiers" class="headerlink" title="Survival Prediction from Imbalance colorectal cancer dataset using hybrid sampling methods and tree-based classifiers"></a>Survival Prediction from Imbalance colorectal cancer dataset using hybrid sampling methods and tree-based classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01783">http://arxiv.org/abs/2309.01783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadegh Soleimani, Mahsa Bahrami, Mansour Vali</li>
<li>for: 预测COLRET cancer patients的1、3、5年生存率</li>
<li>methods: 使用优化的预处理技术、标准平衡技术、 Synthetic Minority Over-sampling Techniques (SMOTE) 和 pipelines of SMOTE和RENN方法来平衡数据，并使用 Decision Trees、Random Forest、Extra Tree、eXtreme Gradient Boosting 和 Light Gradient Boosting (LGBM) 等树型分类算法进行预测</li>
<li>results: 使用5-fold cross-validation方法进行性能评估，在高度不平衡的1年生存任务中，提出的方法与LGBM combinatorial方法达到了72.30%的敏感性；在3年生存任务中，combine RENN和LGBM方法达到了80.81%的敏感性，表明提出的方法在高度不平衡的数据集上表现最佳。<details>
<summary>Abstract</summary>
Background and Objective: Colorectal cancer is a high mortality cancer. Clinical data analysis plays a crucial role in predicting the survival of colorectal cancer patients, enabling clinicians to make informed treatment decisions. However, utilizing clinical data can be challenging, especially when dealing with imbalanced outcomes. This paper focuses on developing algorithms to predict 1-, 3-, and 5-year survival of colorectal cancer patients using clinical datasets, with particular emphasis on the highly imbalanced 1-year survival prediction task. To address this issue, we propose a method that creates a pipeline of some of standard balancing techniques to increase the true positive rate. Evaluation is conducted on a colorectal cancer dataset from the SEER database. Methods: The pre-processing step consists of removing records with missing values and merging categories. The minority class of 1-year and 3-year survival tasks consists of 10% and 20% of the data, respectively. Edited Nearest Neighbor, Repeated edited nearest neighbor (RENN), Synthetic Minority Over-sampling Techniques (SMOTE), and pipelines of SMOTE and RENN approaches were used and compared for balancing the data with tree-based classifiers. Decision Trees, Random Forest, Extra Tree, eXtreme Gradient Boosting, and Light Gradient Boosting (LGBM) are used in this article. Method. Results: The performance evaluation utilizes a 5-fold cross-validation approach. In the case of highly imbalanced datasets (1-year), our proposed method with LGBM outperforms other sampling methods with the sensitivity of 72.30%. For the task of imbalance (3-year survival), the combination of RENN and LGBM achieves a sensitivity of 80.81%, indicating that our proposed method works best for highly imbalanced datasets. Conclusions: Our proposed method significantly improves mortality prediction for the minority class of colorectal cancer patients.
</details>
<details>
<summary>摘要</summary>
背景和目标：肠Rectal癌是高 Mortality癌症，临床数据分析对于预测肠Rectal癌患者的存活率起着关键作用，帮助临床医生制定 Informed 治疗决策。然而，利用临床数据可能会困难，特别是面临异常尝试的情况。这篇论文关注了开发用来预测肠Rectal癌患者1-, 3-, 5年存活率的算法，特别是面临异常尝试的1年存活预测任务。为解决这一问题，我们提议一种方法，该方法包括一系列标准平衡技术，以增加真正正确率。我们对一个肠Rectal癌数据集进行评估。方法：数据预处理步骤包括去除异常值和合并类别。肠Rectal癌1年和3年存活任务中的少数类刚占数据集的10%和20%。我们使用编辑最近邻国（Edited Nearest Neighbor，RENN）、重复编辑最近邻国（Repeated edited nearest neighbor，SMOTE）、Synthetic Minority Over-sampling Techniques（SMOTE）和这些技术的组合来平衡数据，并与树型分类器结合。我们使用的分类器包括决策树、Random Forest、Extra Tree、eXtreme Gradient Boosting和Light Gradient Boosting（LGBM）。结果：我们使用5-fold Cross-validation方法进行性能评估。在面临异常尝试的1年存活任务中，我们提议的方法与LGBM结合的sensitivity达到72.30%，表明我们的方法在高度偏置的数据集中表现出色。在3年存活任务中，我们 combinational RENN和LGBM的方法达到了80.81%的sensitivity， indicating that our proposed method works best for highly imbalanced datasets。结论：我们的提议方法能够显著提高肠Rectal癌患者少数类的存活预测率。
</details></li>
</ul>
<hr>
<h2 id="Self-concordant-Smoothing-for-Convex-Composite-Optimization"><a href="#Self-concordant-Smoothing-for-Convex-Composite-Optimization" class="headerlink" title="Self-concordant Smoothing for Convex Composite Optimization"></a>Self-concordant Smoothing for Convex Composite Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01781">http://arxiv.org/abs/2309.01781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl">https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl</a></li>
<li>paper_authors: Adeyemi D. Adeoye, Alberto Bemporad</li>
<li>for: 本研究旨在提出一种自相关平滑方法，用于最小化两个凸函数之和，其中一个是平滑的，另一个可能是非凸的。</li>
<li>methods: 本研究使用了部分平滑技术，只平滑了一部分非凸函数。研究者提出了一种自然的问题结构，以及一种变量 метриック选择方法和一种步长选择规则，特别适合 proximal Newton 类算法。</li>
<li>results: 研究者证明了本方法的本地二次quadratic convergence 率，并在两种算法中实现了这一点：Prox-N-SCORE 算法和 Prox-GGN-SCORE 算法。其中 Prox-GGN-SCORE 算法包含一种重要的近似过程，可以减少大多数计算开销，特别是在过parameterized 机器学习模型和 mini-batch  Settings 中。 numerics 示例表明了本方法的高效性和其他方法的不平等。<details>
<summary>Abstract</summary>
We introduce the notion of self-concordant smoothing for minimizing the sum of two convex functions: the first is smooth and the second may be nonsmooth. Our framework results naturally from the smoothing approximation technique referred to as partial smoothing in which only a part of the nonsmooth function is smoothed. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as $\ell_1$-regularization and group-lasso penalties. We prove local quadratic convergence rates for two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton (GGN) algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful for overparameterized machine learning models and in the mini-batch settings. Numerical examples on both synthetic and real datasets demonstrate the efficiency of our approach and its superiority over existing approaches.
</details>
<details>
<summary>摘要</summary>
我们引入自己协调缓和的减少方法，用于减少两个凸函数：第一个是光滑的，第二个可能是不凸的。我们的框架从partial smoothing技术发展而来，仅将部分不凸函数缓和。我们的方法的关键特点在于它具有自然的变量 метри选择方法和步长选择规则，特别适合 proximal Newton 类型的算法。此外，我们可以有效地处理特定结构，它们由不凸函数带来，例如 $\ell_1$-调整和群lasso 罚则。我们证明了两个结果算法的本地quadratic convergence 率：Prox-N-SCORE 算法和 Prox-GGN-SCORE 算法。Prox-GGN-SCORE 算法显示了一个重要的近似程序，它可以帮助将大多数的计算负担与 inverse Hessian 相关联结。这个近似是在过parameterized machine learning 模型和 mini-batch 设定下特别有用。numero examples 表明我们的方法的效率和其他方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Interpreting-and-Improving-Fairness-of-Algorithms-using-Causal-Inference-and-Randomized-Experiments"><a href="#Measuring-Interpreting-and-Improving-Fairness-of-Algorithms-using-Causal-Inference-and-Randomized-Experiments" class="headerlink" title="Measuring, Interpreting, and Improving Fairness of Algorithms using Causal Inference and Randomized Experiments"></a>Measuring, Interpreting, and Improving Fairness of Algorithms using Causal Inference and Randomized Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01780">http://arxiv.org/abs/2309.01780</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Enouen, Tianshu Sun, Yan Liu</li>
<li>For: The paper is written to address the problem of algorithm fairness in real-world AI production systems, with a focus on developing a practical and easy-to-implement measurement framework and a systematic approach to correcting detected sources of bias.* Methods: The paper uses recent advances in causal inference and interpretable machine learning to develop an algorithm-agnostic framework called MIIF (Measure, Interpret, and Improve the Fairness of an algorithmic decision). The framework includes randomized experiments to measure algorithm bias and an explainable machine learning model to interpret and distill the beliefs of a blackbox algorithm.* Results: The paper demonstrates the effectiveness of MIIF in measuring algorithm bias and improving fairness in practical applications like e-commerce and targeted advertising, where industry A&#x2F;B testing is already abundant. The framework is shown to be simple and powerful, and the results suggest that it can be used to study algorithm fairness in a wide range of applications.<details>
<summary>Abstract</summary>
Algorithm fairness has become a central problem for the broad adoption of artificial intelligence. Although the past decade has witnessed an explosion of excellent work studying algorithm biases, achieving fairness in real-world AI production systems has remained a challenging task. Most existing works fail to excel in practical applications since either they have conflicting measurement techniques and/ or heavy assumptions, or require code-access of the production models, whereas real systems demand an easy-to-implement measurement framework and a systematic way to correct the detected sources of bias.   In this paper, we leverage recent advances in causal inference and interpretable machine learning to present an algorithm-agnostic framework (MIIF) to Measure, Interpret, and Improve the Fairness of an algorithmic decision. We measure the algorithm bias using randomized experiments, which enables the simultaneous measurement of disparate treatment, disparate impact, and economic value. Furthermore, using modern interpretability techniques, we develop an explainable machine learning model which accurately interprets and distills the beliefs of a blackbox algorithm. Altogether, these techniques create a simple and powerful toolset for studying algorithm fairness, especially for understanding the cost of fairness in practical applications like e-commerce and targeted advertising, where industry A/B testing is already abundant.
</details>
<details>
<summary>摘要</summary>
“算法公平性已成为人工智能广泛采用的中心问题。过去十年内，我们已经见证了优秀的研究算法偏见，但在实际应用中实现算法公平性仍然是一项挑战。现有大多数工作具有冲突的测量技术和假设，或者需要生产模型的代码访问，而实际应用需要一个简单易用的测量框架和一个系统atic的方法来纠正检测到的偏见来源。在这篇论文中，我们利用了最新的 causal inference 和可解释机器学习来提出一个算法无关的框架（MIIF），用于测量、解释和改进算法决策中的偏见。我们使用随机实验来测量算法偏见，这使得同时测量不同对待、不同影响和经济价值的可能性。此外，我们使用现代可解释技术来开发一个可解释的机器学习模型，可以准确地解释和概括黑盒算法的信仰。总之，这些技术创造了一个简单强大的工具集，用于研究算法公平性，特别是在实际应用中的电商和目标广告等场景，其中产业A/B测试已经充沛。”
</details></li>
</ul>
<hr>
<h2 id="DRAG-Divergence-based-Adaptive-Aggregation-in-Federated-learning-on-Non-IID-Data"><a href="#DRAG-Divergence-based-Adaptive-Aggregation-in-Federated-learning-on-Non-IID-Data" class="headerlink" title="DRAG: Divergence-based Adaptive Aggregation in Federated learning on Non-IID Data"></a>DRAG: Divergence-based Adaptive Aggregation in Federated learning on Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01779">http://arxiv.org/abs/2309.01779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Zhu, Jingjing Zhang, Shengyun Liu, Xin Wang</li>
<li>for: 这个论文的目的是提高 Federated Learning（FL）中的通信效率，以及解决因为不同的训练数据分布而导致的“客户端滑块”现象。</li>
<li>methods: 这个论文提出了一个新的度量“拟合度”，用于量度每个客户端的本地更新与全域尺度方向之间的夹角。然后，这个度量被用来实现在每个环境中动态地“拖”本地更新，以避免额外的通信过程。</li>
<li>results: 这个论文透过实验证明了DRAG算法在实际应用中具有优异的性能，能够有效地控制“客户端滑块”现象，并且具有不断性和稳定性。此外，DRAG算法还能够对某些Byzantine攻击进行有效的防护。<details>
<summary>Abstract</summary>
Local stochastic gradient descent (SGD) is a fundamental approach in achieving communication efficiency in Federated Learning (FL) by allowing individual workers to perform local updates. However, the presence of heterogeneous data distributions across working nodes causes each worker to update its local model towards a local optimum, leading to the phenomenon known as ``client-drift" and resulting in slowed convergence. To address this issue, previous works have explored methods that either introduce communication overhead or suffer from unsteady performance. In this work, we introduce a novel metric called ``degree of divergence," quantifying the angle between the local gradient and the global reference direction. Leveraging this metric, we propose the divergence-based adaptive aggregation (DRAG) algorithm, which dynamically ``drags" the received local updates toward the reference direction in each round without requiring extra communication overhead. Furthermore, we establish a rigorous convergence analysis for DRAG, proving its ability to achieve a sublinear convergence rate. Compelling experimental results are presented to illustrate DRAG's superior performance compared to state-of-the-art algorithms in effectively managing the client-drift phenomenon. Additionally, DRAG exhibits remarkable resilience against certain Byzantine attacks. By securely sharing a small sample of the client's data with the FL server, DRAG effectively counters these attacks, as demonstrated through comprehensive experiments.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的度量量名为“分布度”，用于量化当前工作节点的本地梯度和全局参考方向之间的角度。基于这个度量量，我们提出了一种名为“分布度基于的自适应聚合”（DRAG）算法，可以在每个轮次中动态地“拖”收到的本地更新向参考方向。这种算法不需要额外的通信开销，同时能够有效地控制客户端漂移现象。此外，我们也提供了一种准确的收敛分析，证明DRAG可以实现下线性收敛率。实验结果表明，DRAG在面临客户端漂移现象时表现出了显著的优势，并且具有remarkable的抗拒攻击能力。
</details></li>
</ul>
<hr>
<h2 id="CONFIDERAI-a-novel-CONFormal-Interpretable-by-Design-score-function-for-Explainable-and-Reliable-Artificial-Intelligence"><a href="#CONFIDERAI-a-novel-CONFormal-Interpretable-by-Design-score-function-for-Explainable-and-Reliable-Artificial-Intelligence" class="headerlink" title="CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable and Reliable Artificial Intelligence"></a>CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable and Reliable Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01778">http://arxiv.org/abs/2309.01778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Carlevaro, Sara Narteni, Fabrizio Dabbene, Marco Muselli, Maurizio Mongelli</li>
<li>For: The paper proposes a methodology for linking conformal prediction with explainable machine learning, with the goal of creating more reliable and trustworthy artificial intelligence systems.* Methods: The paper introduces a new score function called CONFIDERAI, which leverages both the predictive ability of rules and their geometric position within boundaries. Additionally, the paper addresses the problem of defining regions in feature space where conformal guarantees are satisfied using techniques to control the number of non-conformal samples in conformal regions based on support vector data description (SVDD).* Results: The paper reports promising results on benchmark and real datasets, such as DNS tunneling detection and cardiovascular disease prediction.<details>
<summary>Abstract</summary>
Everyday life is increasingly influenced by artificial intelligence, and there is no question that machine learning algorithms must be designed to be reliable and trustworthy for everyone. Specifically, computer scientists consider an artificial intelligence system safe and trustworthy if it fulfills five pillars: explainability, robustness, transparency, fairness, and privacy. In addition to these five, we propose a sixth fundamental aspect: conformity, that is, the probabilistic assurance that the system will behave as the machine learner expects. In this paper, we propose a methodology to link conformal prediction with explainable machine learning by defining CONFIDERAI, a new score function for rule-based models that leverages both rules predictive ability and points geometrical position within rules boundaries. We also address the problem of defining regions in the feature space where conformal guarantees are satisfied by exploiting techniques to control the number of non-conformal samples in conformal regions based on support vector data description (SVDD). The overall methodology is tested with promising results on benchmark and real datasets, such as DNS tunneling detection or cardiovascular disease prediction.
</details>
<details>
<summary>摘要</summary>
每天生活都在人工智能的影响下，机器学习算法必须设计为可靠和信任worthy。特别是计算机科学家认为一个人工智能系统安全和可靠的五大基础：解释性、可靠性、透明度、公平性和隐私。此外，我们还提出了第六个基本方面：准确性，即机器学习者期望的系统行为的概率ensure。在这篇论文中，我们提出了将CONFIDERAI作为新的分数函数，用于规则型模型，该函数利用规则预测的能力和点在规则边界的几何位置。我们还解决了定义特征空间中符合 garanties的问题，通过控制特征空间中非符合 garanties样本的数量来基于支持向量数据描述（SVDD）。总的来说，我们的方法ологи是在 benchmark和实际数据集上测试，如 DNS 隧道检测或心血管疾病预测。
</details></li>
</ul>
<hr>
<h2 id="Gated-recurrent-neural-networks-discover-attention"><a href="#Gated-recurrent-neural-networks-discover-attention" class="headerlink" title="Gated recurrent neural networks discover attention"></a>Gated recurrent neural networks discover attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01775">http://arxiv.org/abs/2309.01775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes von Oswald, Maxime Larcher, Angelika Steger, João Sacramento</li>
<li>for: 这个论文主要探讨了使用现代RNN的可能性，以及RNN如何通过线性循环层和Feedforward层实现自注意力。</li>
<li>methods: 这个论文使用了现代RNN的设计元素，包括线性循环层和Feedforward层，以及reverse工程技术来探讨RNN如何实现自注意力。</li>
<li>results: 研究发现，通过使用gradient descent优化器，RNN可以在具有简单学习任务的情况下实现同Transformers一样的性能，并且发现RNN在具有自注意力性的任务上实现了同Transformers一样的 Algorithm。<details>
<summary>Abstract</summary>
Recent architectural developments have enabled recurrent neural networks (RNNs) to reach and even surpass the performance of Transformers on certain sequence modeling tasks. These modern RNNs feature a prominent design pattern: linear recurrent layers interconnected by feedforward paths with multiplicative gating. Here, we show how RNNs equipped with these two design elements can exactly implement (linear) self-attention, the main building block of Transformers. By reverse-engineering a set of trained RNNs, we find that gradient descent in practice discovers our construction. In particular, we examine RNNs trained to solve simple in-context learning tasks on which Transformers are known to excel and find that gradient descent instills in our RNNs the same attention-based in-context learning algorithm used by Transformers. Our findings highlight the importance of multiplicative interactions in neural networks and suggest that certain RNNs might be unexpectedly implementing attention under the hood.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:现代建筑设计使得回归神经网络（RNN）能够在某些序列模型任务上与变换器相当或超越其性能。这些现代RNN具有一种明确的设计模式：线性循环层与Feedforward层之间的乘法关系。我们示出了RNN具有这两个元素可以直接实现线性自注意力，变换器的核心组件。通过分析训练过的RNN，我们发现在实际的梯度下降过程中，Gradient Descent实际上找到了我们的结构。具体来说，我们研究了在解决简单的上下文学习任务上训练过的RNN，这些任务在变换器上很出色，并发现了Gradient Descent在我们的RNN中实际上填充了同样的注意力基于上下文学习算法，与变转器一样。我们的发现表明多元互作在神经网络中的重要性，并且可能存在某些RNN在实际中无意识地实现注意力。
</details></li>
</ul>
<hr>
<h2 id="ADC-DAC-Free-Analog-Acceleration-of-Deep-Neural-Networks-with-Frequency-Transformation"><a href="#ADC-DAC-Free-Analog-Acceleration-of-Deep-Neural-Networks-with-Frequency-Transformation" class="headerlink" title="ADC&#x2F;DAC-Free Analog Acceleration of Deep Neural Networks with Frequency Transformation"></a>ADC&#x2F;DAC-Free Analog Acceleration of Deep Neural Networks with Frequency Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01771">http://arxiv.org/abs/2309.01771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nastaran Darabi, Maeesha Binte Hashem, Hongyi Pan, Ahmet Cetin, Wilfred Gomes, Amit Ranjan Trivedi<br>for: 这篇论文旨在提出一种能效的频域运算深度神经网络（DNN）加速方法，以减少网络的耗电和延迟。methods: 本文使用频域运算，例如华氏-哈达姆转换（WHT），并提出了一种新的类比频域运算的能效加速方法，通过利用类比频域的tensor Transformation。results: 根据16×16核心阵列，对8位输入处理，提出的方法可以在VDD &#x3D; 0.8 V下达到1602兆操作每秒每瓦（TOPS&#x2F;W）的能效率，而且可以透过早期终止策略提高到5311 TOPS&#x2F;W。<details>
<summary>Abstract</summary>
The edge processing of deep neural networks (DNNs) is becoming increasingly important due to its ability to extract valuable information directly at the data source to minimize latency and energy consumption. Frequency-domain model compression, such as with the Walsh-Hadamard transform (WHT), has been identified as an efficient alternative. However, the benefits of frequency-domain processing are often offset by the increased multiply-accumulate (MAC) operations required. This paper proposes a novel approach to an energy-efficient acceleration of frequency-domain neural networks by utilizing analog-domain frequency-based tensor transformations. Our approach offers unique opportunities to enhance computational efficiency, resulting in several high-level advantages, including array micro-architecture with parallelism, ADC/DAC-free analog computations, and increased output sparsity. Our approach achieves more compact cells by eliminating the need for trainable parameters in the transformation matrix. Moreover, our novel array micro-architecture enables adaptive stitching of cells column-wise and row-wise, thereby facilitating perfect parallelism in computations. Additionally, our scheme enables ADC/DAC-free computations by training against highly quantized matrix-vector products, leveraging the parameter-free nature of matrix multiplications. Another crucial aspect of our design is its ability to handle signed-bit processing for frequency-based transformations. This leads to increased output sparsity and reduced digitization workload. On a 16$\times$16 crossbars, for 8-bit input processing, the proposed approach achieves the energy efficiency of 1602 tera operations per second per Watt (TOPS/W) without early termination strategy and 5311 TOPS/W with early termination strategy at VDD = 0.8 V.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的边缘处理在不断增长的重要性，这是因为它可以直接从数据源提取有价值信息，以降低延迟和能耗。频域模型压缩，如沃尔夏-哈达姆变换（WHT），已被证明是一种有效的方法。然而，频域处理的优点通常被增加的 multiply-accumulate（MAC）操作所抵消。这篇论文提出了一种新的能效加速频域神经网络的方法，利用频域频率基于的分析频率转换。我们的方法具有增强计算效率的多种优点，包括数组微架构的并行计算、ADC/DAC无计算、和输出稀疏化。我们的方法可以减少转换矩阵中的可训练参数，从而实现更加紧凑的维度。此外，我们的新的数组微架构可以在某些列和行上进行可靠的缝合，以便实现完全的并行计算。此外，我们的方法可以避免 ADC/DAC 计算，通过对高度量化矩阵-向量乘法进行训练，利用矩阵乘法的参数自由性。此外，我们的方法还可以处理signed-bit转换，从而提高输出稀疏化和减少数字化工作负担。在0.8 V 的电压下，我们的方法在16x16 核心上实现了1602 TOPS/W 的能效率，而不需要早期终止策略，并在使用早期终止策略时实现了5311 TOPS/W 的能效率。
</details></li>
</ul>
<hr>
<h2 id="On-Penalty-Methods-for-Nonconvex-Bilevel-Optimization-and-First-Order-Stochastic-Approximation"><a href="#On-Penalty-Methods-for-Nonconvex-Bilevel-Optimization-and-First-Order-Stochastic-Approximation" class="headerlink" title="On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation"></a>On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01753">http://arxiv.org/abs/2309.01753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongyeol Kwon, Dohyun Kwon, Steve Wright, Robert Nowak</li>
<li>for: 本文研究了一种基于 penalty 方法的 first-order 算法，用于解决精度优化（BO）问题，其中目标函数都是平滑的，但可能不具有凸性。</li>
<li>methods: 本文使用 penalty 方法来把上下水平的目标函数 combine 成一个权重和 penalty 参数 $\sigma &gt; 0$ 的和。并通过准确地Characterizing 下水平目标函数和上水平目标函数的值和导数在 $\sigma $ 的关系，得出了 penalty 函数的梯度。</li>
<li>results: 本文提出了一种 first-order 算法，可以在 $\epsilon $ 精度下找到一个 $\epsilon $ 站ARY点，并且需要 $O(\epsilon^{-3})$ 和 $O(\epsilon^{-7})$ 访问 first-order (随机) 梯度或acles。在 deterministic 梯度或acles 的情况下，算法可以在一个完全单loop 方式下实现，并且可以在 $O(1)$ 扫描每次迭代。<details>
<summary>Abstract</summary>
In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty methods, in which the upper- and lower-level objectives are combined in a weighted sum with penalty parameter $\sigma > 0$. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be $O(\sigma)$-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as $O(\sigma)$-approximation of the original BO, we propose first-order algorithms that find an $\epsilon$-stationary solution by optimizing the penalty formulation with $\sigma = O(\epsilon)$. When the perturbed lower-level problem uniformly satisfies the small-error proximal error-bound (EB) condition, we propose a first-order algorithm that converges to an $\epsilon$-stationary point of the penalty function, using in total $O(\epsilon^{-3})$ and $O(\epsilon^{-7})$ accesses to first-order (stochastic) gradient oracles when the oracle is deterministic and oracles are noisy, respectively. Under an additional assumption on stochastic oracles, we show that the algorithm can be implemented in a fully {\it single-loop} manner, i.e., with $O(1)$ samples per iteration, and achieves the improved oracle-complexity of $O(\epsilon^{-3})$ and $O(\epsilon^{-5})$, respectively.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了一种基于权重方法的首选算法来解决层次优化问题（BO），其中目标函数是连续的，但可能非几何的在两个水平上。作为第一步，我们通过对层次优化问题进行负权重方法的分析，特别是通过Explicitly characterizing the conditions under which the values and derivatives of the two must be $O(\sigma)$-close。这个分析中的结果还包括了层次优化问题的下面解的梯度的表达式，这可能有独立的应用价值。然后，我们视权重方法为$O(\sigma)$-近似于原始BO的方法，并提出了一种首选算法，它可以在$\sigma = O(\epsilon)$下找到一个$\epsilon$-稳定的解。当下面问题的较小精度预测误差Bound（EB）条件满足时，我们提出了一种首选算法，它可以在$O(\epsilon^{-3})$和$O(\epsilon^{-7})$的访问次数下 converge to an $\epsilon$-稳定点，其中可能存在随机 oracle的假设。在这种假设下，我们表明了该算法可以在单 loop（即每个迭代只需要 $O(1)$ 样本）下实现，并且实现了改进的oracle-复杂度 $O(\epsilon^{-3})$和 $O(\epsilon^{-5})$。
</details></li>
</ul>
<hr>
<h2 id="Turbulent-Flow-Simulation-using-Autoregressive-Conditional-Diffusion-Models"><a href="#Turbulent-Flow-Simulation-using-Autoregressive-Conditional-Diffusion-Models" class="headerlink" title="Turbulent Flow Simulation using Autoregressive Conditional Diffusion Models"></a>Turbulent Flow Simulation using Autoregressive Conditional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01745">http://arxiv.org/abs/2309.01745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Kohl, Li-Wei Chen, Nils Thuerey</li>
<li>for: 这篇论文主要是为了解决机器学习基于PDE解决方法中的稳定性问题。</li>
<li>methods: 这篇论文使用了一种基于 conditional diffusion 模型的抽象扩展，以提高学习型PDE解决方法的稳定性。</li>
<li>results: 论文表明，这种方法可以在各种复杂的液体流动场景中提供稳定的解决方案，并且可以在不同的流动参数范围内进行扩展。同时，这种方法还可以借助概率性的推 diffusion 方法来预测流体物理统计学上的性质。<details>
<summary>Abstract</summary>
Simulating turbulent flows is crucial for a wide range of applications, and machine learning-based solvers are gaining increasing relevance. However, achieving stability when generalizing to longer rollout horizons remains a persistent challenge for learned PDE solvers. We address this challenge by introducing a fully data-driven fluid solver that utilizes an autoregressive rollout based on conditional diffusion models. We show that this approach offers clear advantages in terms of rollout stability compared to other learned baselines. Remarkably, these improvements in stability are achieved without compromising the quality of generated samples, and our model successfully generalizes to flow parameters beyond the training regime. Additionally, the probabilistic nature of the diffusion approach allows for inferring predictions that align with the statistics of the underlying physics. We quantitatively and qualitatively evaluate the performance of our method on a range of challenging scenarios, including incompressible and transonic flows, as well as isotropic turbulence.
</details>
<details>
<summary>摘要</summary>
模拟湍流是许多应用领域的关键，而机器学习基于的解决方案在不断增长。然而，在扩展到更长的执行 horizon 时，学习得到的稳定性仍然是一个棘手的挑战。我们解决这个挑战 by introducing a fully data-driven fluid solver that utilizes an autoregressive rollout based on conditional diffusion models.我们发现这种方法可以在其他学习基准下提供明显的稳定性改进，而不需要牺牲生成样本的质量。另外，Diffusion 方法的 probabilistic nature 允许我们生成与物理统计相符的预测。我们对一系列复杂的场景进行量化和质量evaluate our method，包括不压缩和超音速流体动理，以及各向异otropic turbulence。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Resource-Allocation-for-Virtualized-Base-Stations-in-O-RAN-with-Online-Learning"><a href="#Adaptive-Resource-Allocation-for-Virtualized-Base-Stations-in-O-RAN-with-Online-Learning" class="headerlink" title="Adaptive Resource Allocation for Virtualized Base Stations in O-RAN with Online Learning"></a>Adaptive Resource Allocation for Virtualized Base Stations in O-RAN with Online Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01730">http://arxiv.org/abs/2309.01730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michail Kalntis, George Iosifidis, Fernando A. Kuipers</li>
<li>for:  optimize the allocation of resources in virtualized base stations (vBSs) to balance effective throughput and energy consumption, even in challenging environments.</li>
<li>methods:  online learning algorithm and meta-learning scheme to adapt to non-stationary or adversarial traffic demands and choose the best performing algorithm for different environments.</li>
<li>results:  sub-linear regret and up to 64.5% power consumption savings compared to state-of-the-art benchmarks, evaluated with real-world data and trace-driven evaluations.<details>
<summary>Abstract</summary>
Open Radio Access Network systems, with their virtualized base stations (vBSs), offer operators the benefits of increased flexibility, reduced costs, vendor diversity, and interoperability. Optimizing the allocation of resources in a vBS is challenging since it requires knowledge of the environment, (i.e., "external'' information), such as traffic demands and channel quality, which is difficult to acquire precisely over short intervals of a few seconds. To tackle this problem, we propose an online learning algorithm that balances the effective throughput and vBS energy consumption, even under unforeseeable and "challenging'' environments; for instance, non-stationary or adversarial traffic demands. We also develop a meta-learning scheme, which leverages the power of other algorithmic approaches, tailored for more "easy'' environments, and dynamically chooses the best performing one, thus enhancing the overall system's versatility and effectiveness. We prove the proposed solutions achieve sub-linear regret, providing zero average optimality gap even in challenging environments. The performance of the algorithms is evaluated with real-world data and various trace-driven evaluations, indicating savings of up to 64.5% in the power consumption of a vBS compared with state-of-the-art benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Online-Classification-From-Estimation-to-Denoising"><a href="#Robust-Online-Classification-From-Estimation-to-Denoising" class="headerlink" title="Robust Online Classification: From Estimation to Denoising"></a>Robust Online Classification: From Estimation to Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01698">http://arxiv.org/abs/2309.01698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changlong Wu, Ananth Grama, Wojciech Szpankowski</li>
<li>for: 研究在含有噪声标签的在线分类中的噪声干扰。噪声机制由一个通用的kernel模型，对任何特征标签对而指定一个已知的分布集合，而选择器在每个时间步骤上选择未知分布，并生成噪声标签。</li>
<li>methods: 研究者使用了在线条件分布估计的概念，来扩展和涵盖了一般的噪声kernel和选择器，以及无限类型和随机生成的特征。</li>
<li>results: 研究者表明，对于许多自然的噪声kernel，选择器和标签函数的finite类型， minimax风险可以独立于时间跨度和对征函数的对数幂级别下降。此外，研究者还扩展了结果到无限类型和随机生成的特征。<details>
<summary>Abstract</summary>
We study online classification in the presence of noisy labels. The noise mechanism is modeled by a general kernel that specifies, for any feature-label pair, a (known) set of distributions over noisy labels. At each time step, an adversary selects an unknown distribution from the distribution set specified by the kernel based on the actual feature-label pair, and generates the noisy label from the selected distribution. The learner then makes a prediction based on the actual features and noisy labels observed thus far, and incurs loss $1$ if the prediction differs from the underlying truth (and $0$ otherwise). The prediction quality is quantified through minimax risk, which computes the cumulative loss over a finite horizon $T$. We show that for a wide range of natural noise kernels, adversarially selected features, and finite class of labeling functions, minimax risk can be upper bounded independent of the time horizon and logarithmic in the size of labeling function class. We then extend these results to inifinite classes and stochastically generated features via the concept of stochastic sequential covering. Our results extend and encompass findings of Ben-David et al. (2009) through substantial generality, and provide intuitive understanding through a novel reduction to online conditional distribution estimation.
</details>
<details>
<summary>摘要</summary>
我们研究在含杂标签下的在线分类。杂标机制是通过一个通用的核函数来模型，该函数指定了任何特征标签对的（已知）分布过滤器。在每个时间步骤中，一个对手选择一个未知分布从选择的分布集中，并生成含杂标签。学习者根据实际特征和含杂标签所见而进行预测，并且如果预测与真实真实值不同，则输入1，否则输入0。预测质量通过最小最大风险来衡量，该风险计算了时间 horizon $T$ 内的总损失。我们证明，对于许多自然的杂标核函数、对手选择的特征和finite类标签函数，最小最大风险可以独立于时间桢和对数型的总体规模而上下界。然后，我们扩展这些结果到无限类和随机生成的特征上，通过离散顺序覆盖的概念。我们的结果超越和涵盖了Ben-David等人（2009）的发现，并提供了直观的理解，通过一种新的减少到在线条件分布估计的概念。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Polynomial-Chaos-Expansions"><a href="#Physics-Informed-Polynomial-Chaos-Expansions" class="headerlink" title="Physics-Informed Polynomial Chaos Expansions"></a>Physics-Informed Polynomial Chaos Expansions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01697">http://arxiv.org/abs/2309.01697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukáš Novák, Himanshu Sharma, Michael D. Shields<br>for: 本研究旨在构造physics-informed多项式泛化(PCE)，以推优化既有数据约束又有物理约束的泛化过程。methods: 本研究使用了一种新的方法，即将实验设计与模型物理约束相结合，以构造physics-informed PCE。这种方法通过利用物理约束来提高泛化精度，而不需要评估原始模型。results: 研究结果表明，提出的方法可以提高泛化精度，而且不加增计算负担。此外，通过对一些具有不同复杂性的决定性例子进行应用，还可以通过分析减少后的PCE滤波来进行不确定性评估。<details>
<summary>Abstract</summary>
Surrogate modeling of costly mathematical models representing physical systems is challenging since it is typically not possible to create a large experimental design. Thus, it is beneficial to constrain the approximation to adhere to the known physics of the model. This paper presents a novel methodology for the construction of physics-informed polynomial chaos expansions (PCE) that combines the conventional experimental design with additional constraints from the physics of the model. Physical constraints investigated in this paper are represented by a set of differential equations and specified boundary conditions. A computationally efficient means for construction of physically constrained PCE is proposed and compared to standard sparse PCE. It is shown that the proposed algorithms lead to superior accuracy of the approximation and does not add significant computational burden. Although the main purpose of the proposed method lies in combining data and physical constraints, we show that physically constrained PCEs can be constructed from differential equations and boundary conditions alone without requiring evaluations of the original model. We further show that the constrained PCEs can be easily applied for uncertainty quantification through analytical post-processing of a reduced PCE filtering out the influence of all deterministic space-time variables. Several deterministic examples of increasing complexity are provided and the proposed method is applied for uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
实验设计较少的mathematical model预测physical system的成本高，因此实验设计是准确描述physical system的关键。这篇论文提出了一种新的方法，即physics-informed polynomial chaos expansions（PCE）的建构，融合实验设计和物理模型的条件。这些物理条件是通过 differential equations和specified boundary conditions表示的。提出了一种 computationally efficient的建构方法，并与标准的罕见PCE进行比较。结果显示，提案的方法可以提高 aproximation的精度，而且不会增加computational burden。虽然主要的目的是将data和物理条件融合，但我们显示可以从 differential equations和boundary conditions aloneconstruct physically constrained PCE，不需要评估原始模型。此外，我们还显示了 constrained PCE可以通过analytical post-processing的方式，范围内的所有决定性空间时间变量 filtering out。提供了一些 deterministic example of increasing complexity，并应用于uncertainty quantification。
</details></li>
</ul>
<hr>
<h2 id="Blind-Biological-Sequence-Denoising-with-Self-Supervised-Set-Learning"><a href="#Blind-Biological-Sequence-Denoising-with-Self-Supervised-Set-Learning" class="headerlink" title="Blind Biological Sequence Denoising with Self-Supervised Set Learning"></a>Blind Biological Sequence Denoising with Self-Supervised Set Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01670">http://arxiv.org/abs/2309.01670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Ng, Ji Won Park, Jae Hyeon Lee, Ryan Lewis Kelly, Stephen Ra, Kyunghyun Cho</li>
<li>for: 这个论文的目的是为了实现高通量DNA测序数据中的下游科学应用，特别是为了更好地清除Sequencing plataforms中的错误读取。</li>
<li>methods: 这个论文提出了一种新的自主学习方法，叫做Self-Supervised Set Learning (SSSL)，可以将多个受挤的序列读取到一个嵌入空间中，并且估算这些序列的集合嵌入。这个集合嵌入可以用来预测受挤的清洁序列。</li>
<li>results: 在实验中，SSSL方法可以较前一个基准下降17%的错误率，并且在实际数据中也有好的表现，尤其是在小序列上（小于6个受挤），可以大幅降低错误率。<details>
<summary>Abstract</summary>
Biological sequence analysis relies on the ability to denoise the imprecise output of sequencing platforms. We consider a common setting where a short sequence is read out repeatedly using a high-throughput long-read platform to generate multiple subreads, or noisy observations of the same sequence. Denoising these subreads with alignment-based approaches often fails when too few subreads are available or error rates are too high. In this paper, we propose a novel method for blindly denoising sets of sequences without directly observing clean source sequence labels. Our method, Self-Supervised Set Learning (SSSL), gathers subreads together in an embedding space and estimates a single set embedding as the midpoint of the subreads in both the latent and sequence spaces. This set embedding represents the "average" of the subreads and can be decoded into a prediction of the clean sequence. In experiments on simulated long-read DNA data, SSSL methods denoise small reads of $\leq 6$ subreads with 17% fewer errors and large reads of $>6$ subreads with 8% fewer errors compared to the best baseline. On a real dataset of antibody sequences, SSSL improves over baselines on two self-supervised metrics, with a significant improvement on difficult small reads that comprise over 60% of the test set. By accurately denoising these reads, SSSL promises to better realize the potential of high-throughput DNA sequencing data for downstream scientific applications.
</details>
<details>
<summary>摘要</summary>
生物序列分析需要去除测序平台输出的不精准数据。我们考虑一种常见的情况，在高通量长读平台上重复读取短序列，以生成多个噪声观测。对这些噪声观测进行对Alignment基于的去噪方法经常失败，当有太少的噪声观测或错误率太高时。在这篇论文中，我们提出了一种新的方法，即Self-Supervised Set Learning（SSSL）。这种方法将噪声观测集成到一个映射空间中，并估算这些噪声观测的集中点，作为latent空间和序列空间中的midpoint。这个集中点表示“平均”的噪声观测，可以被解码成一个clean序列预测。在对模拟长读DNA数据进行实验中，SSSL方法可以对小读数据（≤6个噪声观测）和大读数据（>6个噪声观测）进行去噪，相比best baseline，减少了17%和8%的错误。在一个真实的抗体序列数据集上，SSSL方法超过了基准值，尤其是在difficult小读中，这些小读占检测集的60%以上。通过准确地去噪这些小读，SSSL方法承诺可以更好地实现高通量DNA测序数据的下游科学应用。
</details></li>
</ul>
<hr>
<h2 id="Robust-penalized-least-squares-of-depth-trimmed-residuals-regression-for-high-dimensional-data"><a href="#Robust-penalized-least-squares-of-depth-trimmed-residuals-regression-for-high-dimensional-data" class="headerlink" title="Robust penalized least squares of depth trimmed residuals regression for high-dimensional data"></a>Robust penalized least squares of depth trimmed residuals regression for high-dimensional data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01666">http://arxiv.org/abs/2309.01666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijun Zuo</li>
<li>for: 本研究旨在探讨高维度数据分析中遇到的挑战，包括维度大于样本大小（i）和异常点或杂质点隐藏和更难检测（ii）。</li>
<li>methods: 本文使用了许多现代整合 penalty 方法来分析高维度数据，其中包括惩罚方法和深度trimmed residuals 方法。</li>
<li>results: 研究发现，大多数整合 penalty 方法在面对异常点或杂质点时会失效，而新提出的最小最小二乘法depth trimmed residuals方法可以更好地处理这些情况，并在实验中表现出较高的估计和预测精度。<details>
<summary>Abstract</summary>
Challenges with data in the big-data era include (i) the dimension $p$ is often larger than the sample size $n$ (ii) outliers or contaminated points are frequently hidden and more difficult to detect. Challenge (i) renders most conventional methods inapplicable. Thus, it attracts tremendous attention from statistics, computer science, and bio-medical communities. Numerous penalized regression methods have been introduced as modern methods for analyzing high-dimensional data. Disproportionate attention has been paid to the challenge (ii) though. Penalized regression methods can do their job very well and are expected to handle the challenge (ii) simultaneously. Most of them, however, can break down by a single outlier (or single adversary contaminated point) as revealed in this article.   The latter systematically examines leading penalized regression methods in the literature in terms of their robustness, provides quantitative assessment, and reveals that most of them can break down by a single outlier. Consequently, a novel robust penalized regression method based on the least sum of squares of depth trimmed residuals is proposed and studied carefully. Experiments with simulated and real data reveal that the newly proposed method can outperform some leading competitors in estimation and prediction accuracy in the cases considered.
</details>
<details>
<summary>摘要</summary>
大数据时代的数据分析挑战包括（i）维度pfrequently大于样本size n（ii）异常值或杂质点隐藏更难于探测。挑战（i）使得大多数传统方法无法应用。这引起了统计、计算机科学和生物医学领域的极大关注。许多惩罚回归方法被引入为现代高维数据分析的方法。虽然挑战（ii）得到了过度的关注，但是惩罚回归方法可以很好地处理它。然而，大多数方法都可以被单个异常值（或单个杂质点）所破坏，这在本文中得到了证明。为了解决这个问题，一种基于深度剔除差异的最小二乘方法被提出并且仔细研究了。实验表明，新提出的方法在预测和估计精度方面在考虑的情况下能够超越一些竞争对手。
</details></li>
</ul>
<hr>
<h2 id="Locally-Stationary-Graph-Processes"><a href="#Locally-Stationary-Graph-Processes" class="headerlink" title="Locally Stationary Graph Processes"></a>Locally Stationary Graph Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01657">http://arxiv.org/abs/2309.01657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Canbolat, Elif Vural</li>
<li>for: 本文旨在提出一种基于不规则网络结构的局部站立图像处理方法，以满足实际问题中的局部特点变化。</li>
<li>methods: 本文提出了一种基于组件过程的局部站立图像模型（LSGP），通过表示过程的总体为多个组件过程的组合，来表示图像在不同区域的局部站立性。提出了一种计算LSGP模型的算法，以及本地使用WSS过程的近似方法。</li>
<li>results: 实验表明，提出的过程模型可以与现有技术竞争，并且在信号 interpolating 问题中提供了高精度的信号表示。<details>
<summary>Abstract</summary>
Stationary graph process models are commonly used in the analysis and inference of data sets collected on irregular network topologies. While most of the existing methods represent graph signals with a single stationary process model that is globally valid on the entire graph, in many practical problems, the characteristics of the process may be subject to local variations in different regions of the graph. In this work, we propose a locally stationary graph process (LSGP) model that aims to extend the classical concept of local stationarity to irregular graph domains. We characterize local stationarity by expressing the overall process as the combination of a set of component processes such that the extent to which the process adheres to each component varies smoothly over the graph. We propose an algorithm for computing LSGP models from realizations of the process, and also study the approximation of LSGPs locally with WSS processes. Experiments on signal interpolation problems show that the proposed process model provides accurate signal representations competitive with the state of the art.
</details>
<details>
<summary>摘要</summary>
stationary graph process models 常用于非 régulière 网络 topology 上的数据集分析和推理。大多数现有方法使用 globally 有效的站ARY graph signal 模型来表示整个图的信号，但在实际问题中，过程的特性可能会在不同地方的图中具有本地差异。在这种情况下，我们提出了一种 Locally Stationary Graph Process (LSGP) 模型，旨在扩展传统的本地站ARY性概念到不规则图域。我们通过表示过程的总体作为不同地方的组件过程的组合来 caracterize 本地站ARY性。我们还提出了一种计算 LSGP 模型的算法，以及对 LSGP 模型进行本地approximation的 WSS 过程的研究。实验表明，提出的过程模型可以与当前状态齐的精度地表示信号。
</details></li>
</ul>
<hr>
<h2 id="Representing-Edge-Flows-on-Graphs-via-Sparse-Cell-Complexes"><a href="#Representing-Edge-Flows-on-Graphs-via-Sparse-Cell-Complexes" class="headerlink" title="Representing Edge Flows on Graphs via Sparse Cell Complexes"></a>Representing Edge Flows on Graphs via Sparse Cell Complexes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01632">http://arxiv.org/abs/2309.01632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josef Hoppe, Michael T. Schaub</li>
<li>for: 获取对数据的稀疏、可解释性表示是机器学习和信号处理任务中的关键。</li>
<li>methods: 将图结构提升到 simplicial complex 中，然后使用 Hodge-Laplacian 的特征值和相应的 incidence matrix 来实现 Hodge 分解，从而将观察数据表示为梯度、旋转和响应流。</li>
<li>results: 在实际数据和 sintetic 数据上，我们的算法可以高效地解决 cell inference 优化问题，并且比现有的方法更高效。<details>
<summary>Abstract</summary>
Obtaining sparse, interpretable representations of observable data is crucial in many machine learning and signal processing tasks. For data representing flows along the edges of a graph, an intuitively interpretable way to obtain such representations is to lift the graph structure to a simplicial complex: The eigenvectors of the associated Hodge-Laplacian, respectively the incidence matrices of the corresponding simplicial complex then induce a Hodge decomposition, which can be used to represent the observed data in terms of gradient, curl, and harmonic flows. In this paper, we generalize this approach to cellular complexes and introduce the cell inference optimization problem, i.e., the problem of augmenting the observed graph by a set of cells, such that the eigenvectors of the associated Hodge Laplacian provide a sparse, interpretable representation of the observed edge flows on the graph. We show that this problem is NP-hard and introduce an efficient approximation algorithm for its solution. Experiments on real-world and synthetic data demonstrate that our algorithm outperforms current state-of-the-art methods while being computationally efficient.
</details>
<details>
<summary>摘要</summary>
获取稀疏、可解释的数据表示是许多机器学习和信号处理任务中的关键。为了在图structure上获取这些表示，一种直观可解的方法是将图结构升级到 simplicial complex：图结构的特征值和相应的 simplicial complex 的 incidence matrix  THEN INDUCE A Hodge decomposition, 可以用来表示观察到的边流在图上。在这篇论文中，我们扩展了这种方法到细胞复杂体系，并引入细胞推理优化问题，即在观察到的图上添加一组细胞，以便将 graph 的特征值和 incidence matrix 转化为稀疏、可解释的表示。我们证明了这个问题是NP困难的，并提出了一种有效的近似算法来解决它。实验表明，我们的算法在实际数据和synthetic数据上都能够超越当前状态的方法，而且 Computationally efficient。
</details></li>
</ul>
<hr>
<h2 id="Dropout-Attacks"><a href="#Dropout-Attacks" class="headerlink" title="Dropout Attacks"></a>Dropout Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01614">http://arxiv.org/abs/2309.01614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngunnar/Robustness_tutorial">https://github.com/ngunnar/Robustness_tutorial</a></li>
<li>paper_authors: Andrew Yuan, Alina Oprea, Cheng Tan</li>
<li>for: 本研究旨在攻击深度学习模型中的Dropout操作，以防止过拟合。</li>
<li>methods: 本文引入了一种新的Dropout攻击方法，称为DROPOUTATTACK，通过 manipulate dropout operator 中选择的 neuron 而不是随机选择。</li>
<li>results: 在训练 VGG-16 模型在 CIFAR-100 上，我们的攻击可以将受到攻击的级别降低至 34.6% (从 81.7% 降至 47.1%)，而无需对模型精度做出任何干扰。<details>
<summary>Abstract</summary>
Dropout is a common operator in deep learning, aiming to prevent overfitting by randomly dropping neurons during training. This paper introduces a new family of poisoning attacks against neural networks named DROPOUTATTACK. DROPOUTATTACK attacks the dropout operator by manipulating the selection of neurons to drop instead of selecting them uniformly at random. We design, implement, and evaluate four DROPOUTATTACK variants that cover a broad range of scenarios. These attacks can slow or stop training, destroy prediction accuracy of target classes, and sabotage either precision or recall of a target class. In our experiments of training a VGG-16 model on CIFAR-100, our attack can reduce the precision of the victim class by 34.6% (from 81.7% to 47.1%) without incurring any degradation in model accuracy
</details>
<details>
<summary>摘要</summary>
Dropout 是深度学习中常用的操作，目的是防止适应性过度 Training 中的 neuron 被随机Dropout 操作。这篇论文介绍了一种新的毒素攻击 named DROPOUTATTACK，该攻击targets  dropout 操作，而不是随机选择 neuron。我们设计了四种 DROPOUTATTACK 变种，覆盖了广泛的场景。这些攻击可以降低目标类准确率，甚至使模型训练失败。在我们对 VGG-16 模型在 CIFAR-100 上训练的实验中，我们的攻击可以降低目标类准确率 by 34.6%（从 81.7% 降至 47.1%），而无需模型精度下降。
</details></li>
</ul>
<hr>
<h2 id="Fair-Ranking-under-Disparate-Uncertainty"><a href="#Fair-Ranking-under-Disparate-Uncertainty" class="headerlink" title="Fair Ranking under Disparate Uncertainty"></a>Fair Ranking under Disparate Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01610">http://arxiv.org/abs/2309.01610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richa Rastogi, Thorsten Joachims</li>
<li>for: 提高排序系统的公平性，即使数据量不均衡时仍能保证各个群体的排名公平。</li>
<li>methods: 提出Equal-Opportunity Ranking（EOR）作为公平排序标准，并实现了一种可行的算法来实现EOR排名，时间复杂度为O(n * log(n))。</li>
<li>results: 经过synthetic数据、美国人口普查数据和Amazon搜索关键词数据的实验表明，该算法可靠地保证EOR公平性，同时提供有效的排名。<details>
<summary>Abstract</summary>
Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation far more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, since the relevance estimates for minority groups tend to have higher uncertainty due to a lack of data or appropriate features. To overcome this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking that provably corrects for the disparity in uncertainty between groups. Furthermore, we present a practical algorithm for computing EOR rankings in time $O(n \log(n))$ and prove its close approximation guarantee to the globally optimal solution. In a comprehensive empirical evaluation on synthetic data, a US Census dataset, and a real-world case study of Amazon search queries, we find that the algorithm reliably guarantees EOR fairness while providing effective rankings.
</details>
<details>
<summary>摘要</summary>
“排名是一种普遍存在的方法，用于集中人类评估者的注意力于可管理的子集中。它的应用范围从电子商务网站上浮出潜在有用的产品到审核大学申请。尽管排名可以使人类评估变得非常有效，但它可能引入不公正性，因为不同群体选项的后台相关性模型的不确定性差异较大。实际上，这种差异在少数群体选项中的相关性估计通常更高，因为这些选项的数据或特征不够。为解决这个公正性问题，我们提出了平等机会排名（EOR）作为一种新的公正性标准，可以正确地纠正不同群体选项之间的不确定性差异。此外，我们提出了一种实用的算法来计算EOR排名，时间复杂度为O(nlog(n))，并证明其与全球最佳解决方案的快近优化 garantia。在synthetic数据、US Census数据和amazon搜索查询的实际评估中，我们发现了这种算法可靠地保证EOR公正性，同时提供有效的排名。”
</details></li>
</ul>
<hr>
<h2 id="Drifter-Efficient-Online-Feature-Monitoring-for-Improved-Data-Integrity-in-Large-Scale-Recommendation-Systems"><a href="#Drifter-Efficient-Online-Feature-Monitoring-for-Improved-Data-Integrity-in-Large-Scale-Recommendation-Systems" class="headerlink" title="Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems"></a>Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08617">http://arxiv.org/abs/2309.08617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blaž Škrlj, Nir Ki-Tov, Lee Edelist, Natalia Silberstein, Hila Weisman-Zohar, Blaž Mramor, Davorin Kopič, Naama Ziporin</li>
<li>for: 这个论文是为了解决大规模、动态流中数据质量维护问题。</li>
<li>methods: 这个系统使用了新的在线特征监测和验证技术，以提供快速、有效、适应性强的数据质量监测，并能够实时检测数据质量问题的根本原因。</li>
<li>results: 对实际数据集进行评估，这个系统能够有效地发送警示并解决数据质量问题，提高了实时live推荐系统的可靠性和性能。<details>
<summary>Abstract</summary>
Real-world production systems often grapple with maintaining data quality in large-scale, dynamic streams. We introduce Drifter, an efficient and lightweight system for online feature monitoring and verification in recommendation use cases. Drifter addresses limitations of existing methods by delivering agile, responsive, and adaptable data quality monitoring, enabling real-time root cause analysis, drift detection and insights into problematic production events. Integrating state-of-the-art online feature ranking for sparse data and anomaly detection ideas, Drifter is highly scalable and resource-efficient, requiring only two threads and less than a gigabyte of RAM per production deployments that handle millions of instances per minute. Evaluation on real-world data sets demonstrates Drifter's effectiveness in alerting and mitigating data quality issues, substantially improving reliability and performance of real-time live recommender systems.
</details>
<details>
<summary>摘要</summary>
现实生产环境中， oftentimes 面临着大规模、动态流中数据质量维护的挑战。我们介绍了 Drifter，一种高效、轻量级的在线特征监测和验证系统，用于推荐使用情况下的数据质量监测。 Drifter 超越了现有方法的局限性，提供了快速、敏感、适应性的数据质量监测，允许实时根本原因分析、漂移检测和问题生成的生产事件中的深入洞察。 Drifter  integrate 最新的在线特征排名技术和罕见检测思想，可扩展性强，只需两个线程和 less than 一 gigabyte 的内存，可执行 millions 个实例每分钟的生产部署。 评估实际数据集表明， Drifter 可以有效地预警和解决数据质量问题，substantially 提高实时live 推荐系统的可靠性和性能。
</details></li>
</ul>
<hr>
<h2 id="Active-flow-control-for-three-dimensional-cylinders-through-deep-reinforcement-learning"><a href="#Active-flow-control-for-three-dimensional-cylinders-through-deep-reinforcement-learning" class="headerlink" title="Active flow control for three-dimensional cylinders through deep reinforcement learning"></a>Active flow control for three-dimensional cylinders through deep reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02462">http://arxiv.org/abs/2309.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pol Suárez, Francisco Alcántara-Ávila, Arnau Miró, Jean Rabault, Bernat Font, Oriol Lehmkuhl, R. Vinuesa</li>
<li>for: 降低缩扰系数（drag coefficient）</li>
<li>methods: 多个独立控制的零负质流体推进器（synthetic jets），基于深度学习托管的液体动力学解算器，以及一个使用质量优化算法的代理人</li>
<li>results: 在三种不同的问题配置中，实现了显著的缩扰减少<details>
<summary>Abstract</summary>
This paper presents for the first time successful results of active flow control with multiple independently controlled zero-net-mass-flux synthetic jets. The jets are placed on a three-dimensional cylinder along its span with the aim of reducing the drag coefficient. The method is based on a deep-reinforcement-learning framework that couples a computational-fluid-dynamics solver with an agent using the proximal-policy-optimization algorithm. We implement a multi-agent reinforcement-learning framework which offers numerous advantages: it exploits local invariants, makes the control adaptable to different geometries, facilitates transfer learning and cross-application of agents and results in significant training speedup. In this contribution we report significant drag reduction after applying the DRL-based control in three different configurations of the problem.
</details>
<details>
<summary>摘要</summary>
We implement a multi-agent reinforcement-learning framework, which has several advantages:1. It exploits local invariants, making the control adaptable to different geometries.2. It facilitates transfer learning and cross-application of agents.3. It results in significant training speedup.In this contribution, we report significant drag reduction after applying the DRL-based control in three different configurations of the problem.
</details></li>
</ul>
<hr>
<h2 id="Passing-Heatmap-Prediction-Based-on-Transformer-Model-and-Tracking-Data"><a href="#Passing-Heatmap-Prediction-Based-on-Transformer-Model-and-Tracking-Data" class="headerlink" title="Passing Heatmap Prediction Based on Transformer Model and Tracking Data"></a>Passing Heatmap Prediction Based on Transformer Model and Tracking Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01526">http://arxiv.org/abs/2309.01526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yisheng Pei, Varuna De Silva, Mike Caine</li>
<li>For: 这个研究旨在提供一种能够预测传球的可能性和传球前运动影响最终结果的深度学习网络架构，以更正查评球员的表现。* Methods: 该研究使用了深度学习网络模型，分析了超过28,000个传球事件，并达到了0.7以上的顶部一级准确率。* Results: 该研究表明，通过分析球员的运动动作，可以更好地理解球场控制和传球选择对防御性表现的影响，并为足球分析师提供一个更好的工具和指标来评估球员的运动贡献。<details>
<summary>Abstract</summary>
Although the data-driven analysis of football players' performance has been developed for years, most research only focuses on the on-ball event including shots and passes, while the off-ball movement remains a little-explored area in this domain. Players' contributions to the whole match are evaluated unfairly, those who have more chances to score goals earn more credit than others, while the indirect and unnoticeable impact that comes from continuous movement has been ignored. This research presents a novel deep-learning network architecture which is capable to predict the potential end location of passes and how players' movement before the pass affects the final outcome. Once analysed more than 28,000 pass events, a robust prediction can be achieved with more than 0.7 Top-1 accuracy. And based on the prediction, a better understanding of the pitch control and pass option could be reached to measure players' off-ball movement contribution to defensive performance. Moreover, this model could provide football analysts a better tool and metric to understand how players' movement over time contributes to the game strategy and final victory.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Blackbox-Model-Is-All-You-Need-to-Breach-Privacy-Smart-Grid-Forecasting-Models-as-a-Use-Case"><a href="#A-Blackbox-Model-Is-All-You-Need-to-Breach-Privacy-Smart-Grid-Forecasting-Models-as-a-Use-Case" class="headerlink" title="A Blackbox Model Is All You Need to Breach Privacy: Smart Grid Forecasting Models as a Use Case"></a>A Blackbox Model Is All You Need to Breach Privacy: Smart Grid Forecasting Models as a Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01523">http://arxiv.org/abs/2309.01523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussein Aly, Abdulaziz Al-Ali, Abdullah Al-Ali, Qutaibah Malluhi</li>
<li>for: 这篇论文研究了智能电网中预测模型的隐私风险，尤其是深度学习和预测模型在智能电网中的应用。</li>
<li>methods: 本研究使用了深度学习和预测模型，包括长期快速响应神经网络 (LSTM)，分析了预测模型是否泄露敏感信息的风险。</li>
<li>results: 研究发现，使用预测模型可以泄露智能电网系统中的全局性和隐私威胁，特别是通过黑盒访问LSTM模型可以泄露大量信息，与数据直接访问的情况相似（差异在1%以下，在ROC曲线下的面积）。这说明需要对预测模型进行保护，与数据一样重要。<details>
<summary>Abstract</summary>
This paper investigates the potential privacy risks associated with forecasting models, with specific emphasis on their application in the context of smart grids. While machine learning and deep learning algorithms offer valuable utility, concerns arise regarding their exposure of sensitive information. Previous studies have focused on classification models, overlooking risks associated with forecasting models. Deep learning based forecasting models, such as Long Short Term Memory (LSTM), play a crucial role in several applications including optimizing smart grid systems but also introduce privacy risks. Our study analyzes the ability of forecasting models to leak global properties and privacy threats in smart grid systems. We demonstrate that a black box access to an LSTM model can reveal a significant amount of information equivalent to having access to the data itself (with the difference being as low as 1% in Area Under the ROC Curve). This highlights the importance of protecting forecasting models at the same level as the data.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "smart grids" is translated as "智能电网" (zìnéng diànwǎng)* "forecasting models" is translated as "预测模型" (yùzhèng módelǐ)* "machine learning" is translated as "机器学习" (jīshì xuéxí)* "deep learning" is translated as "深度学习" (shēngrù xuéxí)* "Long Short Term Memory" is translated as "长期短 память" (chángzhì duǎnjiàng)* "black box access" is translated as "黑盒访问" (hēi bāo fāngwù)* "privacy risks" is translated as "隐私风险" (yìnwèi fēngxì)
</details></li>
</ul>
<hr>
<h2 id="Hawkeye-Change-targeted-Testing-for-Android-Apps-based-on-Deep-Reinforcement-Learning"><a href="#Hawkeye-Change-targeted-Testing-for-Android-Apps-based-on-Deep-Reinforcement-Learning" class="headerlink" title="Hawkeye: Change-targeted Testing for Android Apps based on Deep Reinforcement Learning"></a>Hawkeye: Change-targeted Testing for Android Apps based on Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01519">http://arxiv.org/abs/2309.01519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Peng, Zhengwei Lv, Jiarong Fu, Jiayuan Liang, Zhao Zhang, Ajitha Rajan, Ping Yang</li>
<li>for: 本研究的目的是提高Android应用程序更新的正确性，以避免在用户端出现漏洞。</li>
<li>methods: 本研究提出了一种指导测试方法，使用深度强化学习来优先执行更新后影响的GUI操作。</li>
<li>results: 对于10个开源App和1个商业App，研究发现，使用 Hawkeye 可以更可靠地生成targeting更改的GUI事件序列，比 FastBot2 和 ARES 更高效。 Hawkeye 在小型开源App上表现相当，但在大型商业App上表现较低。在实际生产环境中，Hawkeye 也能够成功地进行烟测测试。<details>
<summary>Abstract</summary>
Android Apps are frequently updated to keep up with changing user, hardware, and business demands. Ensuring the correctness of App updates through extensive testing is crucial to avoid potential bugs reaching the end user. Existing Android testing tools generate GUI events focussing on improving the test coverage of the entire App rather than prioritising updates and its impacted elements. Recent research has proposed change-focused testing but relies on random exploration to exercise the updates and impacted GUI elements that is ineffective and slow for large complex Apps with a huge input exploration space. We propose directed testing of App updates with Hawkeye that is able to prioritise executing GUI actions associated with code changes based on deep reinforcement learning from historical exploration data. Our empirical evaluation compares Hawkeye with state-of-the-art model-based and reinforcement learning-based testing tools FastBot2 and ARES using 10 popular open-source and 1 commercial App. We find that Hawkeye is able to generate GUI event sequences targeting changed functions more reliably than FastBot2 and ARES for the open source Apps and the large commercial App. Hawkeye achieves comparable performance on smaller open source Apps with a more tractable exploration space. The industrial deployment of Hawkeye in the development pipeline also shows that Hawkeye is ideal to perform smoke testing for merge requests of a complicated commercial App.
</details>
<details>
<summary>摘要</summary>
我们提出了directed testing of App updates with Hawkeye，它可以根据深度学习历史探索数据来优先执行更改后的GUI操作。我们的实验比较了Hawkeye与当前最佳的模型基于和学习基于的测试工具FastBot2和ARES，使用10个流行的开源App和1个商业App。我们发现Hawkeye可以更可靠地对更改后的函数生成GUI事件序列，比FastBot2和ARES更高效。 Hawkeye在小型开源App上也有类似的性能，而且在商业App的开发pipeline中的实际部署中也显示了Hawkeye的适用性。 Hawkeye可以快速完成 merge requests 的烟测，为复杂的商业App提供了一个 идеal testing solution。
</details></li>
</ul>
<hr>
<h2 id="Federated-cINN-Clustering-for-Accurate-Clustered-Federated-Learning"><a href="#Federated-cINN-Clustering-for-Accurate-Clustered-Federated-Learning" class="headerlink" title="Federated cINN Clustering for Accurate Clustered Federated Learning"></a>Federated cINN Clustering for Accurate Clustered Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01515">http://arxiv.org/abs/2309.01515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Zhou, Minjia Shi, Yuxin Tian, Yuanxi Li, Qing Ye, Jiancheng Lv</li>
<li>For: 本研究旨在 Addressing the challenge of coordinating Federated Learning (FL) with crowd intelligence, particularly when client groups have disparate objectives due to data heterogeneity or distinct tasks.* Methods: 提议了一种 Federated cINN Clustering Algorithm (FCCA)，通过对每个客户端的私有数据进行全局编码，并使用生成模型进行最大可能性估计，以避免模式折射和优化难度。* Results: 对多种模型和数据集进行了广泛的实验，并证明了 FCCA 的超越性，比如其能够更好地减少客户端之间的干扰和提高全局模型的准确率。<details>
<summary>Abstract</summary>
Federated Learning (FL) presents an innovative approach to privacy-preserving distributed machine learning and enables efficient crowd intelligence on a large scale. However, a significant challenge arises when coordinating FL with crowd intelligence which diverse client groups possess disparate objectives due to data heterogeneity or distinct tasks. To address this challenge, we propose the Federated cINN Clustering Algorithm (FCCA) to robustly cluster clients into different groups, avoiding mutual interference between clients with data heterogeneity, and thereby enhancing the performance of the global model. Specifically, FCCA utilizes a global encoder to transform each client's private data into multivariate Gaussian distributions. It then employs a generative model to learn encoded latent features through maximum likelihood estimation, which eases optimization and avoids mode collapse. Finally, the central server collects converged local models to approximate similarities between clients and thus partition them into distinct clusters. Extensive experimental results demonstrate FCCA's superiority over other state-of-the-art clustered federated learning algorithms, evaluated on various models and datasets. These results suggest that our approach has substantial potential to enhance the efficiency and accuracy of real-world federated learning tasks.
</details>
<details>
<summary>摘要</summary>
《联邦学习（FL）》提出了一种创新的隐私保护分布式机器学习方法，可以实现大规模的群体智能。然而，在与群体智能协调FL时， Client groups possessing diverse objectives due to data heterogeneity or distinct tasks poses a significant challenge. To address this challenge, we propose the Federated cINN Clustering Algorithm (FCCA) to robustly cluster clients into different groups, avoiding mutual interference between clients with data heterogeneity, and thereby enhancing the performance of the global model. Specifically, FCCA utilizes a global encoder to transform each client's private data into multivariate Gaussian distributions. It then employs a generative model to learn encoded latent features through maximum likelihood estimation, which eases optimization and avoids mode collapse. Finally, the central server collects converged local models to approximate similarities between clients and thus partition them into distinct clusters. Extensive experimental results demonstrate FCCA's superiority over other state-of-the-art clustered federated learning algorithms, evaluated on various models and datasets. These results suggest that our approach has substantial potential to enhance the efficiency and accuracy of real-world federated learning tasks.
</details></li>
</ul>
<hr>
<h2 id="Layer-wise-training-for-self-supervised-learning-on-graphs"><a href="#Layer-wise-training-for-self-supervised-learning-on-graphs" class="headerlink" title="Layer-wise training for self-supervised learning on graphs"></a>Layer-wise training for self-supervised learning on graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01503">http://arxiv.org/abs/2309.01503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Pina, Verónica Vilaplana</li>
<li>for: 训练大 graphs上的端到端图 neural networks (GNN) 存在多种内存和计算挑战，限制了深度的应用，因为深度会导致内存和空间复杂度的剧烈增长。</li>
<li>methods: 我们提出了层 wise Regularized Graph Infomax算法，通过自我超vised的方式来训练 GNN 层次。我们将 GNN 中的特征传播和特征变换分解出来，以学习节点表示，并从未来输入预测的基础上 derivate 一个损失函数。</li>
<li>results: 我们在 inductive 大 graphs 中评估了我们的算法，与其他端到端方法相当，并且显著提高了效率，可以在一个单个设备上训练更加复杂的模型。此外，我们的算法还可以避免深度 GNN 中的抖搅现象。<details>
<summary>Abstract</summary>
End-to-end training of graph neural networks (GNN) on large graphs presents several memory and computational challenges, and limits the application to shallow architectures as depth exponentially increases the memory and space complexities. In this manuscript, we propose Layer-wise Regularized Graph Infomax, an algorithm to train GNNs layer by layer in a self-supervised manner. We decouple the feature propagation and feature transformation carried out by GNNs to learn node representations in order to derive a loss function based on the prediction of future inputs. We evaluate the algorithm in inductive large graphs and show similar performance to other end to end methods and a substantially increased efficiency, which enables the training of more sophisticated models in one single device. We also show that our algorithm avoids the oversmoothing of the representations, another common challenge of deep GNNs.
</details>
<details>
<summary>摘要</summary>
大型图格神经网络（GNN）的端到端训练存在许多内存和计算挑战，深度随着增加而 exponentially 增加内存和空间复杂性。在这篇论文中，我们提出层wise Regularized Graph Infomax算法，用于层段式自我监督的GNN训练。我们将GNN中的特征传播和特征转换分解开来，以学习节点表示，并从未来输入预测得出损失函数。我们在大型 inductive 图上评估了算法，并与其他端到端方法相当，同时有substantially 提高效率，这使得可以在一个设备上训练更复杂的模型。此外，我们还证明了我们的算法可以避免深度GNN的过滤 representations。
</details></li>
</ul>
<hr>
<h2 id="FinDiff-Diffusion-Models-for-Financial-Tabular-Data-Generation"><a href="#FinDiff-Diffusion-Models-for-Financial-Tabular-Data-Generation" class="headerlink" title="FinDiff: Diffusion Models for Financial Tabular Data Generation"></a>FinDiff: Diffusion Models for Financial Tabular Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01472">http://arxiv.org/abs/2309.01472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timur Sattarov, Marco Schreyer, Damian Borth</li>
<li>for: 这个论文的目的是为了生成真实世界金融数据表格，以便于经济enario模型、压力测试和诈骗探测等下游任务。</li>
<li>methods: 这篇论文使用了扩散模型，特别是噪声扩散模型，来生成模拟真实数据的数据。它使用嵌入编码来处理金融数据的混合类型特征，包括分类和数值特征。</li>
<li>results: 实验结果表明，FinDiff可以生成高准确性、隐私和实用性的 sintetic tabular financial data。与现有基线模型进行比较，FinDiff在三个真实世界金融数据集上表现出色。<details>
<summary>Abstract</summary>
The sharing of microdata, such as fund holdings and derivative instruments, by regulatory institutions presents a unique challenge due to strict data confidentiality and privacy regulations. These challenges often hinder the ability of both academics and practitioners to conduct collaborative research effectively. The emergence of generative models, particularly diffusion models, capable of synthesizing data mimicking the underlying distributions of real-world data presents a compelling solution. This work introduces 'FinDiff', a diffusion model designed to generate real-world financial tabular data for a variety of regulatory downstream tasks, for example economic scenario modeling, stress tests, and fraud detection. The model uses embedding encodings to model mixed modality financial data, comprising both categorical and numeric attributes. The performance of FinDiff in generating synthetic tabular financial data is evaluated against state-of-the-art baseline models using three real-world financial datasets (including two publicly available datasets and one proprietary dataset). Empirical results demonstrate that FinDiff excels in generating synthetic tabular financial data with high fidelity, privacy, and utility.
</details>
<details>
<summary>摘要</summary>
共享微数据，如基金投资和 derivate 工具，由 regulatory 机构提供的存在着独特的挑战，主要是由于严格的数据保密和隐私法规。这些挑战通常会阻碍学者和实践者进行有效的合作研究。随着生成模型的出现，特别是扩散模型，可以Synthesize 数据，模拟实际世界数据的下面分布。本文介绍了 'FinDiff'，一种扩散模型，用于生成实际世界金融表格数据，用于经济enario模拟、压力测试和欺诈探测等下游任务。FinDiff 使用 embedding 编码来模型金融数据的混合模式，包括 both categorical 和 numeric 特征。FinDiff 在生成 synthetic 金融表格数据方面的性能被评估于现有的基eline 模型，使用三个实际世界金融数据集（包括两个公共可用数据集和一个专有数据集）。实际结果表明，FinDiff 可以高效地生成 synthetic 金融表格数据，具有高准确性、隐私和实用性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Reward-Consistency-for-Interpretable-Feature-Discovery-in-Reinforcement-Learning"><a href="#Leveraging-Reward-Consistency-for-Interpretable-Feature-Discovery-in-Reinforcement-Learning" class="headerlink" title="Leveraging Reward Consistency for Interpretable Feature Discovery in Reinforcement Learning"></a>Leveraging Reward Consistency for Interpretable Feature Discovery in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01458">http://arxiv.org/abs/2309.01458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qisen Yang, Huanqian Wang, Mukun Tong, Wenjie Shi, Gao Huang, Shiji Song</li>
<li>for: 解释和解释深度强化学习（RL）Agent的行为</li>
<li>methods: 提出了一种新的框架（RL-in-RL），通过解决动作和奖励之间的连接问题，来保证奖励一致性 during interpretable feature发现</li>
<li>results: 在Atari 2600游戏和Duckietown自驾车 simulator环境中测试和评估了该方法，结果表明方法可以保持奖励一致性，并实现高质量的特征归属。进一步的分析实验也 validate了动作匹配原则的局限性。<details>
<summary>Abstract</summary>
The black-box nature of deep reinforcement learning (RL) hinders them from real-world applications. Therefore, interpreting and explaining RL agents have been active research topics in recent years. Existing methods for post-hoc explanations usually adopt the action matching principle to enable an easy understanding of vision-based RL agents. In this paper, it is argued that the commonly used action matching principle is more like an explanation of deep neural networks (DNNs) than the interpretation of RL agents. It may lead to irrelevant or misplaced feature attribution when different DNNs' outputs lead to the same rewards or different rewards result from the same outputs. Therefore, we propose to consider rewards, the essential objective of RL agents, as the essential objective of interpreting RL agents as well. To ensure reward consistency during interpretable feature discovery, a novel framework (RL interpreting RL, denoted as RL-in-RL) is proposed to solve the gradient disconnection from actions to rewards. We verify and evaluate our method on the Atari 2600 games as well as Duckietown, a challenging self-driving car simulator environment. The results show that our method manages to keep reward (or return) consistency and achieves high-quality feature attribution. Further, a series of analytical experiments validate our assumption of the action matching principle's limitations.
</details>
<details>
<summary>摘要</summary>
深度强化学习（RL）的黑盒特性使其在实际应用中受到限制。因此，解释和解释RL代理的研究在最近几年得到了广泛的关注。现有的后续解释方法通常采用行动匹配原理，以便轻松理解视觉RL代理。然而，这篇论文 argue互联网的行动匹配原理更多地是深度神经网络（DNN）的解释，而不是RL代理的解释。这可能会导致不相关或错位的特征归因，因为不同的DNN输出可能导致同一个奖励，或者不同的奖励可能来自同一个输出。因此，我们建议将奖励作为RL代理的解释的关键目标，以确保解释过程中的奖励一致性。为保证解释过程中的奖励一致性，我们提出了一种RL解释RL的框架（denoted as RL-in-RL），解决了动作和奖励之间的梯度分离问题。我们在Atari 2600游戏和DUCKIETOWN自驾车模拟环境中进行了证明和评估。结果表明，我们的方法能够保持奖励一致性，并实现高质量的特征归因。此外，一系列的分析实验 validate了我们对行动匹配原理的假设的局限性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Consistency-and-Robustness-of-Saliency-Explanations-for-Time-Series-Classification"><a href="#On-the-Consistency-and-Robustness-of-Saliency-Explanations-for-Time-Series-Classification" class="headerlink" title="On the Consistency and Robustness of Saliency Explanations for Time Series Classification"></a>On the Consistency and Robustness of Saliency Explanations for Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01457">http://arxiv.org/abs/2309.01457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiara Balestra, Bin Li, Emmanuel Müller</li>
<li>for: 这 paper aims to analyze the consistency and robustness of saliency maps for time series features and temporal attribution in a time series classification task.</li>
<li>methods: 该 paper uses perturbation-based and gradient-based explanation models to generate saliency explanations, and examines their consistency and robustness on five real-world datasets.</li>
<li>results: The experimental results show that the saliency explanations from both models lack consistent and robust performances to some extent, highlighting the need for developing more reliable explanation methods for time series classification.<details>
<summary>Abstract</summary>
Interpretable machine learning and explainable artificial intelligence have become essential in many applications. The trade-off between interpretability and model performance is the traitor to developing intrinsic and model-agnostic interpretation methods. Although model explanation approaches have achieved significant success in vision and natural language domains, explaining time series remains challenging. The complex pattern in the feature domain, coupled with the additional temporal dimension, hinders efficient interpretation. Saliency maps have been applied to interpret time series windows as images. However, they are not naturally designed for sequential data, thus suffering various issues.   This paper extensively analyzes the consistency and robustness of saliency maps for time series features and temporal attribution. Specifically, we examine saliency explanations from both perturbation-based and gradient-based explanation models in a time series classification task. Our experimental results on five real-world datasets show that they all lack consistent and robust performances to some extent. By drawing attention to the flawed saliency explanation models, we motivate to develop consistent and robust explanations for time series classification.
</details>
<details>
<summary>摘要</summary>
<div><p> interpreter 机器学习和解释人工智能在许多应用中变得必需。模型性能和可解释性的交易是发展内置和模型无关的解释方法的障碍。虽然模型解释方法在视觉和自然语言领域得到了显著成功，但是解释时序序列仍然是挑战。时序序列特征空间复杂，加上额外的时间维度，使得有效的解释受到阻碍。</p><p>Saliency maps 已经应用于解释时序序列窗口，但它们不是特定适用于顺序数据，因此受到各种问题的担忧。本文进行了严格的分析和对比，发现这些解释模型在时序序列分类任务中缺乏一致性和可靠性。我们在五个真实的实际数据集上进行了实验，结果表明它们都缺乏一定程度的一致性和可靠性。我们通过吸引注意力于缺陷的解释模型，激励开发一致和可靠的解释方法 для时序序列分类。</p><p>针对这些问题，我们提出了一种新的解释方法，即基于窗口的时序序列解释方法。这种方法可以快速地生成可解释的时序序列特征，并且可以减少对解释模型的依赖性。我们在实验中证明了这种方法的有效性和可靠性。</p><p>总之，我们的研究表明，为了解释时序序列分类模型的决策，需要开发一致和可靠的解释方法。我们的新的解释方法可以帮助解决这一问题，并且可以应用于实际的应用场景。</p></div>Note: Some of the words and phrases in the text may not be exactly the same as their Simplified Chinese translations, but they should be close enough to be understood.
</details></li>
</ul>
<hr>
<h2 id="Hundreds-Guide-Millions-Adaptive-Offline-Reinforcement-Learning-with-Expert-Guidance"><a href="#Hundreds-Guide-Millions-Adaptive-Offline-Reinforcement-Learning-with-Expert-Guidance" class="headerlink" title="Hundreds Guide Millions: Adaptive Offline Reinforcement Learning with Expert Guidance"></a>Hundreds Guide Millions: Adaptive Offline Reinforcement Learning with Expert Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01448">http://arxiv.org/abs/2309.01448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qisen Yang, Shenzhi Wang, Qihang Zhang, Gao Huang, Shiji Song</li>
<li>for: 提高OFFLINE强化学习（RL）中的策略优化，以便在已经收集的数据集上无需与环境交互时仍能够得到优化的策略。</li>
<li>methods: 提出了一种基于导航网络的插件方法，名为指导OFFLINERL（GORL），该方法可以自动确定每个样本的策略改进和策略限制的相对重要性。</li>
<li>results: 经过广泛的实验表明，GORL可以轻松地与大多数OFFLINERL算法结合使用，并且在各种环境中提供了 statistically significant 的性能提升。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) optimizes the policy on a previously collected dataset without any interactions with the environment, yet usually suffers from the distributional shift problem. To mitigate this issue, a typical solution is to impose a policy constraint on a policy improvement objective. However, existing methods generally adopt a ``one-size-fits-all'' practice, i.e., keeping only a single improvement-constraint balance for all the samples in a mini-batch or even the entire offline dataset. In this work, we argue that different samples should be treated with different policy constraint intensities. Based on this idea, a novel plug-in approach named Guided Offline RL (GORL) is proposed. GORL employs a guiding network, along with only a few expert demonstrations, to adaptively determine the relative importance of the policy improvement and policy constraint for every sample. We theoretically prove that the guidance provided by our method is rational and near-optimal. Extensive experiments on various environments suggest that GORL can be easily installed on most offline RL algorithms with statistically significant performance improvements.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translation into Simplified ChineseOffline 学习优化策略（Offline Reinforcement Learning）通常会遇到分布shift问题，这是因为学习策略时不能直接与环境交互。为解决这个问题，常见的方法是通过策略约束来限制策略改进的目标。然而，现有的方法通常采用“一size-fits-all”的做法，即在一个mini-batch或整个Offline dataset中都保留单一的改进约束平衡。在这个工作中，我们 argue That different samples should be treated with different策略约束强度。基于这个想法，我们提出了一种名为Guided Offline RL（GORL）的新插件方法。GORL使用一个引导网络，以及只需几个专家示范，来动态确定每个样本的策略改进和策略约束的相对重要性。我们证明了我们的方法的引导是理性的和近似优化的。广泛的实验表明，GORL可以轻松地安装在大多数Offline RL算法上，并且具有 statistically significant的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Expanding-Mars-Climate-Modeling-Interpretable-Machine-Learning-for-Modeling-MSL-Relative-Humidity"><a href="#Expanding-Mars-Climate-Modeling-Interpretable-Machine-Learning-for-Modeling-MSL-Relative-Humidity" class="headerlink" title="Expanding Mars Climate Modeling: Interpretable Machine Learning for Modeling MSL Relative Humidity"></a>Expanding Mars Climate Modeling: Interpretable Machine Learning for Modeling MSL Relative Humidity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01424">http://arxiv.org/abs/2309.01424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nour Abdelmoneim, Dattaraj B. Dhuri, Dimitra Atri, Germán Martínez</li>
<li>for: 这个研究的目的是为了模拟火星气候，具体来说是在加乐沟地区测量的相对湿度。</li>
<li>methods: 这个研究使用的方法是基于机器学习技术，使用了一个深度神经网络模型，使用了火星气候模型生成的数据进行训练。</li>
<li>results: 研究结果表明，这个神经网络模型可以准确地预测加乐沟地区的相对湿度，误差在3%以内，$R^2$分数为0.92。此外，研究还发现了一种方法可以预测相对湿度的范围，这有助于应用需要范围值的场合。<details>
<summary>Abstract</summary>
For the past several decades, numerous attempts have been made to model the climate of Mars with extensive studies focusing on the planet's dynamics and the understanding of its climate. While physical modeling and data assimilation approaches have made significant progress, uncertainties persist in comprehensively capturing and modeling the complexities of Martian climate. In this work, we propose a novel approach to Martian climate modeling by leveraging machine learning techniques that have shown remarkable success in Earth climate modeling. Our study presents a deep neural network designed to accurately model relative humidity in Gale Crater, as measured by NASA's Mars Science Laboratory ``Curiosity'' rover. By utilizing simulated meteorological variables produced by the Mars Planetary Climate Model, a robust Global Circulation Model, our model accurately predicts relative humidity with a mean error of 3\% and an $R^2$ score of 0.92. Furthermore, we present an approach to predict quantile ranges of relative humidity, catering to applications that require a range of values. To address the challenge of interpretability associated with machine learning models, we utilize an interpretable model architecture and conduct an in-depth analysis of its internal mechanisms and decision making processes. We find that our neural network can effectively model relative humidity at Gale crater using a few meteorological variables, with the monthly mean surface H$_2$O layer, planetary boundary layer height, convective wind speed, and solar zenith angle being the primary contributors to the model predictions. In addition to providing a fast and efficient method to modeling climate variables on Mars, this modeling approach can also be used to expand on current datasets by filling spatial and temporal gaps in observations.
</details>
<details>
<summary>摘要</summary>
For the past several decades, numerous attempts have been made to model the climate of Mars, with extensive studies focusing on the planet's dynamics and understanding its climate. Although physical modeling and data assimilation approaches have made significant progress, uncertainties still exist in comprehensively capturing and modeling the complexities of Martian climate. In this study, we propose a novel approach to Martian climate modeling by leveraging machine learning techniques that have shown remarkable success in Earth climate modeling. Our study presents a deep neural network designed to accurately model relative humidity in Gale Crater, as measured by NASA's Mars Science Laboratory "Curiosity" rover. By utilizing simulated meteorological variables produced by the Mars Planetary Climate Model, a robust Global Circulation Model, our model accurately predicts relative humidity with a mean error of 3% and an $R^2$ score of 0.92. Furthermore, we present an approach to predict quantile ranges of relative humidity, catering to applications that require a range of values. To address the challenge of interpretability associated with machine learning models, we utilize an interpretable model architecture and conduct an in-depth analysis of its internal mechanisms and decision-making processes. We find that our neural network can effectively model relative humidity at Gale crater using a few meteorological variables, with the monthly mean surface H$_2$O layer, planetary boundary layer height, convective wind speed, and solar zenith angle being the primary contributors to the model predictions. In addition to providing a fast and efficient method to modeling climate variables on Mars, this modeling approach can also be used to expand on current datasets by filling spatial and temporal gaps in observations.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Bayesian-Structure-Learning-with-Acyclicity-Assurance"><a href="#Differentiable-Bayesian-Structure-Learning-with-Acyclicity-Assurance" class="headerlink" title="Differentiable Bayesian Structure Learning with Acyclicity Assurance"></a>Differentiable Bayesian Structure Learning with Acyclicity Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01392">http://arxiv.org/abs/2309.01392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang-Duy Tran, Phuoc Nguyen, Bao Duong, Thin Nguyen</li>
<li>for: 本研究旨在提出一种具有约束性的循环自适应方法，以确保生成的图Structures是有向无环的。</li>
<li>methods: 本研究使用了一种结合排序知识的方法，以确保生成的图Structures是有向无环的。</li>
<li>results: 实验表明，我们的方法可以比相关的 bayesian 分数基于方法更好地降低推理复杂性，同时确保生成的图Structures是有向无环的。<details>
<summary>Abstract</summary>
Score-based approaches in the structure learning task are thriving because of their scalability. Continuous relaxation has been the key reason for this advancement. Despite achieving promising outcomes, most of these methods are still struggling to ensure that the graphs generated from the latent space are acyclic by minimizing a defined score. There has also been another trend of permutation-based approaches, which concern the search for the topological ordering of the variables in the directed acyclic graph in order to limit the search space of the graph. In this study, we propose an alternative approach for strictly constraining the acyclicty of the graphs with an integration of the knowledge from the topological orderings. Our approach can reduce inference complexity while ensuring the structures of the generated graphs to be acyclic. Our empirical experiments with simulated and real-world data show that our approach can outperform related Bayesian score-based approaches.
</details>
<details>
<summary>摘要</summary>
score-based方法在结构学任务中得到了广泛应用，主要是因为它们可以扩展到大规模的数据集。连续放松是这些方法的关键原因。尽管它们在实现出色的结果，但大多数这些方法仍然无法确保生成的图从幂等空间中是无环的，通过定义得分来减少搜索空间。此外，有一种排序基于方法，它们关注在搜索变量的顺序问题上，以限制搜索空间。在这种研究中，我们提出了一种新的方法，通过结合知识来限制图的环的数量。我们的方法可以减少推理复杂性，同时确保生成的图是无环的。我们的实验表明，我们的方法可以超越相关的 bayesian 分数基于方法。
</details></li>
</ul>
<hr>
<h2 id="Classic-algorithms-are-fair-learners-Classification-Analysis-of-natural-weather-and-wildfire-occurrences"><a href="#Classic-algorithms-are-fair-learners-Classification-Analysis-of-natural-weather-and-wildfire-occurrences" class="headerlink" title="Classic algorithms are fair learners: Classification Analysis of natural weather and wildfire occurrences"></a>Classic algorithms are fair learners: Classification Analysis of natural weather and wildfire occurrences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01381">http://arxiv.org/abs/2309.01381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sengopal/classic-ml-review-paper">https://github.com/sengopal/classic-ml-review-paper</a></li>
<li>paper_authors: Senthilkumar Gopal</li>
<li>for: 这个论文旨在对常见的监督学习算法进行实际运行和数学分析，以了解它们在不同情况下的性能和特性。</li>
<li>methods: 这篇论文使用了多种常见的监督学习算法，包括决策树、扩展、支持向量机和k-最近邻居。</li>
<li>results: 该论文通过对稀疏表格数据进行分类任务的实验，发现这些经典算法在面对噪声和稀疏数据时仍然能够保持良好的泛化能力，并且可以通过不同的参数来提高分类精度。<details>
<summary>Abstract</summary>
Classic machine learning algorithms have been reviewed and studied mathematically on its performance and properties in detail. This paper intends to review the empirical functioning of widely used classical supervised learning algorithms such as Decision Trees, Boosting, Support Vector Machines, k-nearest Neighbors and a shallow Artificial Neural Network. The paper evaluates these algorithms on a sparse tabular data for classification task and observes the effect on specific hyperparameters on these algorithms when the data is synthetically modified for higher noise. These perturbations were introduced to observe these algorithms on their efficiency in generalizing for sparse data and their utility of different parameters to improve classification accuracy. The paper intends to show that these classic algorithms are fair learners even for such limited data due to their inherent properties even for noisy and sparse datasets.
</details>
<details>
<summary>摘要</summary>
经典机器学习算法已经被详细地研究和分析其性能和特性。这篇论文的目的是对广泛使用的经典超级vised学习算法进行实证性的评估，包括决策树、提升、支持向量机器、k最近邻居和杂层人工神经网络。这篇论文将这些算法应用于稀疏表格数据进行分类任务，并观察这些算法对不同的超参数的影响，当数据被人工修改以增加噪音时。这些干扰是为了评估这些算法在普适数据上的泛化能力和不同参数的用于提高分类精度。论文的目的是表明这些经典算法是有效的学习者，即使只有有限的数据。
</details></li>
</ul>
<hr>
<h2 id="Mutual-Information-Maximizing-Quantum-Generative-Adversarial-Network-and-Its-Applications-in-Finance"><a href="#Mutual-Information-Maximizing-Quantum-Generative-Adversarial-Network-and-Its-Applications-in-Finance" class="headerlink" title="Mutual Information Maximizing Quantum Generative Adversarial Network and Its Applications in Finance"></a>Mutual Information Maximizing Quantum Generative Adversarial Network and Its Applications in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01363">http://arxiv.org/abs/2309.01363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Lee, Myeongjin Shin, Junseo Lee, Kabgyun Jeong</li>
<li>for: 本研究旨在应用于NISQ计算时代的量子机器学习领域，即使用量子机器学习来解决各种领域的问题。</li>
<li>methods: 本研究使用量子生成对抗网络（QGAN），并在QGAN中引入了误差度量 neural network（MINE）来解决模式塌陷问题。</li>
<li>results: 研究表明， InfoQGAN 可以成功地解决模式塌陷问题，并在金融场景中应用于动态资产配置问题来生成 portefolio 返报分布。<details>
<summary>Abstract</summary>
One of the most promising applications in the era of NISQ (Noisy Intermediate-Scale Quantum) computing is quantum machine learning. Quantum machine learning offers significant quantum advantages over classical machine learning across various domains. Specifically, generative adversarial networks have been recognized for their potential utility in diverse fields such as image generation, finance, and probability distribution modeling. However, these networks necessitate solutions for inherent challenges like mode collapse. In this study, we capitalize on the concept that the estimation of mutual information between high-dimensional continuous random variables can be achieved through gradient descent using neural networks. We introduce a novel approach named InfoQGAN, which employs the Mutual Information Neural Estimator (MINE) within the framework of quantum generative adversarial networks to tackle the mode collapse issue. Furthermore, we elaborate on how this approach can be applied to a financial scenario, specifically addressing the problem of generating portfolio return distributions through dynamic asset allocation. This illustrates the potential practical applicability of InfoQGAN in real-world contexts.
</details>
<details>
<summary>摘要</summary>
一个有前途的应用在NISQ（杂AX）计算时代是量子机器学习。量子机器学习在各个领域提供了明显的量子优势。例如，生成对抗网络在图像生成、金融和概率分布模型方面具有潜在的应用前景。然而，这些网络面临的挑战包括模式塌缩。在本研究中，我们利用潜在的思路，即高维连续随机变量之间的相互信息的估计可以通过梯度下降使用神经网络来实现。我们提出一种名为InfoQGAN的新方法，该方法在量子生成对抗网络框架中使用神经网络来解决模式塌缩问题。此外，我们还详细介绍了如何在金融场景中应用InfoQGAN，具体是通过动态资产配置来生成 portefolio返杂分布。这说明InfoQGAN在实际场景中的应用前景非常广阔。
</details></li>
</ul>
<hr>
<h2 id="Random-Projections-of-Sparse-Adjacency-Matrices"><a href="#Random-Projections-of-Sparse-Adjacency-Matrices" class="headerlink" title="Random Projections of Sparse Adjacency Matrices"></a>Random Projections of Sparse Adjacency Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01360">http://arxiv.org/abs/2309.01360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Qiu</li>
<li>for: 该论文旨在研究随机投影方法，用于表示稀疏图。</li>
<li>methods: 该论文使用随机投影方法，以保留图的功能性。</li>
<li>results: 该论文显示，随机投影方法可以在同一空间中表示不同大小的图和顶点集，并且可以精确地执行图算子。<details>
<summary>Abstract</summary>
We analyze a random projection method for adjacency matrices, studying its utility in representing sparse graphs. We show that these random projections retain the functionality of their underlying adjacency matrices while having extra properties that make them attractive as dynamic graph representations. In particular, they can represent graphs of different sizes and vertex sets in the same space, allowing for the aggregation and manipulation of graphs in a unified manner. We also provide results on how the size of the projections need to scale in order to preserve accurate graph operations, showing that the size of the projections can scale linearly with the number of vertices while accurately retaining first-order graph information. We conclude by characterizing our random projection as a distance-preserving map of adjacency matrices analogous to the usual Johnson-Lindenstrauss map.
</details>
<details>
<summary>摘要</summary>
我们分析了一种随机投影方法 для邻接矩阵，研究其在表示稀疏图的有用性。我们显示这些随机投影可以保持它们的下面矩阵的功能性，同时具有一些有利的特性，使得它们成为动态图表示的优选。具体来说，它们可以将不同大小的图和顶点集表示在同一个空间中，allowing for the aggregation and manipulation of graphs in a unified manner。我们还提供了保持准确图操作的尺度规则，显示随机投影的大小可以与顶点数 linearly 增长，并准确地保留首领信息。我们最后characterize our random projection as a distance-preserving map of adjacency matrices analogous to the usual Johnson-Lindenstrauss map.
</details></li>
</ul>
<hr>
<h2 id="MalwareDNA-Simultaneous-Classification-of-Malware-Malware-Families-and-Novel-Malware"><a href="#MalwareDNA-Simultaneous-Classification-of-Malware-Malware-Families-and-Novel-Malware" class="headerlink" title="MalwareDNA: Simultaneous Classification of Malware, Malware Families, and Novel Malware"></a>MalwareDNA: Simultaneous Classification of Malware, Malware Families, and Novel Malware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01350">http://arxiv.org/abs/2309.01350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maksim E. Eren, Manish Bhattarai, Kim Rasmussen, Boian S. Alexandrov, Charles Nicholas</li>
<li>for: 本研究旨在提出一种新的机器学习方法，用于准确地识别新型恶意软件家族，同时整合了恶意软件&#x2F;良好软件分类和恶意软件家族分类的能力。</li>
<li>methods: 该方法使用机器学习技术，并利用了各种特征和数据来进行分类。</li>
<li>results: 本研究的初步结果表明，该方法可以准确地识别新型恶意软件家族，并且可以整合恶意软件&#x2F;良好软件分类和恶意软件家族分类的能力。<details>
<summary>Abstract</summary>
Malware is one of the most dangerous and costly cyber threats to national security and a crucial factor in modern cyber-space. However, the adoption of machine learning (ML) based solutions against malware threats has been relatively slow. Shortcomings in the existing ML approaches are likely contributing to this problem. The majority of current ML approaches ignore real-world challenges such as the detection of novel malware. In addition, proposed ML approaches are often designed either for malware/benign-ware classification or malware family classification. Here we introduce and showcase preliminary capabilities of a new method that can perform precise identification of novel malware families, while also unifying the capability for malware/benign-ware classification and malware family classification into a single framework.
</details>
<details>
<summary>摘要</summary>
马拉ware是现代网络空间中最危险和最昂贵的Cyber安全威胁之一，但是使用机器学习（ML）技术对抗马拉ware威胁的采用速度相对较慢。现有的ML方法存在缺陷，主要是忽略现实中的新型马拉ware检测。此外，大多数当前ML方法都是为马拉ware/非恶意软件分类或马拉ware家族分类而设计的。我们介绍了一种新的方法，可以精准地识别新型马拉ware家族，同时整合了马拉ware/非恶意软件分类和马拉ware家族分类的能力。
</details></li>
</ul>
<hr>
<h2 id="In-processing-User-Constrained-Dominant-Sets-for-User-Oriented-Fairness-in-Recommender-Systems"><a href="#In-processing-User-Constrained-Dominant-Sets-for-User-Oriented-Fairness-in-Recommender-Systems" class="headerlink" title="In-processing User Constrained Dominant Sets for User-Oriented Fairness in Recommender Systems"></a>In-processing User Constrained Dominant Sets for User-Oriented Fairness in Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01335">http://arxiv.org/abs/2309.01335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongxuan Han, Chaochao Chen, Xiaolin Zheng, Weiming Liu, Jun Wang, Wenjie Cheng, Yuyuan Li<br>for: 本研究旨在解决推荐系统中的用户方向偏袋问题（User-Oriented Fairness，UOF），即推荐性能对特定用户群体的不公正。methods: 本研究提出了一种基于backbone推荐模型的In-processing User Constrained Dominant Sets（In-UCDS）框架，包括两个阶段：UCDS模型阶段和在处理阶段。在UCDS模型阶段，对每个劣位用户，提取一个约束主集（user cluster），包含一些有利用户的高质量用户。在处理阶段，通过计算公平损失，将劣位用户的表示向其相应的cluster移动 closer。这种结合公平损失和原始backbone模型损失的方法，可以同时解决UOF问题和保持总的推荐性能。results: 实验表明，In-UCDS在三个真实世界数据集上表现出色，与状态前的方法相比，具有更高的公平性和更好的总推荐性能。<details>
<summary>Abstract</summary>
Recommender systems are typically biased toward a small group of users, leading to severe unfairness in recommendation performance, i.e., User-Oriented Fairness (UOF) issue. The existing research on UOF is limited and fails to deal with the root cause of the UOF issue: the learning process between advantaged and disadvantaged users is unfair. To tackle this issue, we propose an In-processing User Constrained Dominant Sets (In-UCDS) framework, which is a general framework that can be applied to any backbone recommendation model to achieve user-oriented fairness. We split In-UCDS into two stages, i.e., the UCDS modeling stage and the in-processing training stage. In the UCDS modeling stage, for each disadvantaged user, we extract a constrained dominant set (a user cluster) containing some advantaged users that are similar to it. In the in-processing training stage, we move the representations of disadvantaged users closer to their corresponding cluster by calculating a fairness loss. By combining the fairness loss with the original backbone model loss, we address the UOF issue and maintain the overall recommendation performance simultaneously. Comprehensive experiments on three real-world datasets demonstrate that In-UCDS outperforms the state-of-the-art methods, leading to a fairer model with better overall recommendation performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>推荐系统通常偏向一小组用户，导致推荐性能不公，即用户 ориентирован的公平问题 (UOF)。现有的 UOF 研究有限，并未能够解决 UOF 问题的根本原因：推荐模型学习过程中受欢迎和受惩罚用户之间的不公。为解决此问题，我们提出了内部用户受限 dominant set (In-UCDS) 框架，这是一个通用的框架，可以应用于任何基础推荐模型，以实现用户 Oriented 公平。我们将 In-UCDS 分成两个阶段：UCDS 模型化阶段和在处理阶段。在 UCDS 模型化阶段，为每个受欢迎用户，我们提取一个受限 dominant set (用户集)，包含一些受欢迎用户和受惩罚用户之间的相似性。在处理阶段，我们通过计算公平损失，使得受欢迎用户的表示更加接近它所对应的用户集。通过将公平损失与原始基础模型损失相加，我们同时解决 UOF 问题和保持总体推荐性能。广泛的实验表明，In-UCDS 在三个实际数据集上表现出色，与当前状态的方法相比，它可以实现更加公平的推荐模型，同时保持总体推荐性能。
</details></li>
</ul>
<hr>
<h2 id="An-ML-assisted-OTFS-vs-OFDM-adaptable-modem"><a href="#An-ML-assisted-OTFS-vs-OFDM-adaptable-modem" class="headerlink" title="An ML-assisted OTFS vs. OFDM adaptable modem"></a>An ML-assisted OTFS vs. OFDM adaptable modem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01319">http://arxiv.org/abs/2309.01319</a></li>
<li>repo_url: None</li>
<li>paper_authors: I. Zakir Ahmed, Hamid R. Sadjadpour</li>
<li>for: 提高高速移动场景下的通信性能</li>
<li>methods: 使用深度神经网络（DNN）自适应 switching between OTFS 和 OFDM 信号处理链</li>
<li>results: 对比 OTFS、OFDM 和提议的 switching waveform  scheme，得到了显著改善的 Mean-Squared-Error（MSE）性能<details>
<summary>Abstract</summary>
The Orthogonal-Time-Frequency-Space (OTFS) signaling is known to be resilient to doubly-dispersive channels, which impacts high mobility scenarios. On the other hand, the Orthogonal-Frequency-Division-Multiplexing (OFDM) waveforms enjoy the benefits of the reuse of legacy architectures, simplicity of receiver design, and low-complexity detection. Several studies that compare the performance of OFDM and OTFS have indicated mixed outcomes due to the plethora of system parameters at play beyond high-mobility conditions. In this work, we exemplify this observation using simulations and propose a deep neural network (DNN)-based adaptation scheme to switch between using either an OTFS or OFDM signal processing chain at the transmitter and receiver for optimal mean-squared-error (MSE) performance. The DNN classifier is trained to switch between the two schemes by observing the channel condition, received SNR, and modulation format. We compare the performance of the OTFS, OFDM, and the proposed switched-waveform scheme. The simulations indicate superior performance with the proposed scheme with a well-trained DNN, thus improving the MSE performance of the communication significantly.
</details>
<details>
<summary>摘要</summary>
Orthogonal-Time-Frequency-Space (OTFS) 信号处理可以抗抗双折射通道，这对高移动场景有着积极的影响。然而，Orthogonal-Frequency-Division-Multiplexing (OFDM) 波形具有重用现有架构、接收器设计简单、检测低复杂性的优点。但由于系统参数的各种变化，各种研究表明OFDM和OTFS的性能表现存在混乱。在这项工作中，我们通过simeulations进行了证明，并提出了基于深度神经网络（DNN）的适应方案，以实现在传输和接收端使用OTFS或OFDM信号处理链的优化。DNN分类器根据通道条件、接收SNR和调制格式进行选择。我们对OTFS、OFDM和我们提议的切换波形 schemes进行比较。Simulations表明，具有良好训练的DNN，提出的方案可以明显提高通信的MSE性能。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Design-of-Learning-System-for-Energy-Demand-Forecasting-of-Electrical-Vehicles"><a href="#Communication-Efficient-Design-of-Learning-System-for-Energy-Demand-Forecasting-of-Electrical-Vehicles" class="headerlink" title="Communication-Efficient Design of Learning System for Energy Demand Forecasting of Electrical Vehicles"></a>Communication-Efficient Design of Learning System for Energy Demand Forecasting of Electrical Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01297">http://arxiv.org/abs/2309.01297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Xu, Riley Kilfoyle, Zixiang Xiong, Ligang Lu</li>
<li>for: 这篇论文是用于提出一种可以实现时间序列能源利用预测的机器学习模型，并且可以在各地的电动车充电站进行分布式训练。</li>
<li>methods: 这篇论文使用了最新的 transformer 架构和分布式学习（Federated Learning）的组合，以提高时间序列预测性能。</li>
<li>results: 比较这篇论文的时间序列预测性能和训练资料量，与其他模型的比较显示，这篇论文的模型可以与其他模型相比，同时具有更低的训练资料量。<details>
<summary>Abstract</summary>
Machine learning (ML) applications to time series energy utilization forecasting problems are a challenging assignment due to a variety of factors. Chief among these is the non-homogeneity of the energy utilization datasets and the geographical dispersion of energy consumers. Furthermore, these ML models require vast amounts of training data and communications overhead in order to develop an effective model. In this paper, we propose a communication-efficient time series forecasting model combining the most recent advancements in transformer architectures implemented across a geographically dispersed series of EV charging stations and an efficient variant of federated learning (FL) to enable distributed training. The time series prediction performance and communication overhead cost of our FL are compared against their counterpart models and shown to have parity in performance while consuming significantly lower data rates during training. Additionally, the comparison is made across EV charging as well as other time series datasets to demonstrate the flexibility of our proposed model in generalized time series prediction beyond energy demand. The source code for this work is available at https://github.com/XuJiacong/LoGTST_PSGF
</details>
<details>
<summary>摘要</summary>
机器学习（ML）应用于时间序列能源利用预测问题是一项复杂的任务，主要原因是时间序列数据的非均匀性和能源消耗者的地理分散。此外，这些ML模型需要大量的训练数据和通信占用量来建立有效的模型。在这篇论文中，我们提出了一种具有最新的变换架构和 federated learning（FL）的通信高效的时间序列预测模型，用于在地理分散的电动车充电站上进行分布式训练。我们对比了我们的FL模型和其他模型的时间序列预测性能和通信负担，并证明了我们的模型在通信成本下降的情况下保持了与其他模型相当的性能。此外，我们还对EV充电和其他时间序列数据集进行了比较，以示我们的模型在泛化时间序列预测中的灵活性。模型的源代码可以在https://github.com/XuJiacong/LoGTST_PSGF 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.LG_2023_09_04/" data-id="clorjzl9000p1f1889d413hjt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/eess.IV_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T09:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/eess.IV_2023_09_04/">eess.IV - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Effects-of-Material-Mapping-Agnostic-Partial-Volume-Correction-for-Subject-Specific-Finite-Elements-Simulations"><a href="#Effects-of-Material-Mapping-Agnostic-Partial-Volume-Correction-for-Subject-Specific-Finite-Elements-Simulations" class="headerlink" title="Effects of Material Mapping Agnostic Partial Volume Correction for Subject Specific Finite Elements Simulations"></a>Effects of Material Mapping Agnostic Partial Volume Correction for Subject Specific Finite Elements Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01769">http://arxiv.org/abs/2309.01769</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adbeagley/pvcpy">https://github.com/adbeagley/pvcpy</a></li>
<li>paper_authors: Aren Beagley, Hannah Richards, Joshua W. Giles</li>
<li>for:  correction of partial volume effects in CT images</li>
<li>methods:  new algorithm based on previous work, no pre-processing or user input required, applied directly to CT images</li>
<li>results:  improved accuracy of surface strain predictions in experimental three point bending tests compared to original, uncorrected CT images<details>
<summary>Abstract</summary>
Partial Volume effects are present at the boundary between any two types of material in a CT image due to the scanner's Point Spread Function, finite voxel resolution, and importantly, the discrepancy in radiodensity between the two materials. In this study a new algorithm is developed and validated that builds on previously published work to enable the correction of partial volume effects at cortical bone boundaries. Unlike past methods, this algorithm does not require pre-processing or user input to achieve the correction, and the correction is applied directly onto a set of CT images, which enables it to be used in existing computational modelling workflows. The algorithm was validated by performing experimental three point bending tests on porcine fibulae specimen and comparing the experimental results to finite element results for models created using either the original, uncorrected CT images or the partial volume corrected images. Results demonstrated that the models created using the partial volume corrected images did improved the accuracy of the surface strain predictions. Given this initial validation, this algorithm is a viable method for overcoming the challenge of partial volume effects in CT images. Thus, future work should be undertaken to further validate the algorithm with human tissues and through coupling it with a range of different finite element creation workflows to verify that it is robust and agnostic to the chosen material mapping strategy.
</details>
<details>
<summary>摘要</summary>
<<SYS>> CT 图像中的部分体积效应出现在任何两种材料之间的边界上，这是因为扫描仪的点扩散函数、粒子分辨率以及材料的辐射密度差异。在这项研究中，一种新的算法被开发并验证，以解决在 cortical bone 边界上的部分体积效应。与过去的方法不同的是，这种算法不需要先期处理或用户输入来实现修正，而且修正直接应用于 CT 图像集，因此可以在现有的计算模型工作流程中使用。这种算法在使用三点弯曲试验和猪骨脚模型进行验证后得到了证明，模型使用未修正 CT 图像时的表面弯曲预测结果比较准确。基于这个初步验证，这种算法是一种可靠的方法，未来的工作应该继续验证这种算法在人类组织中的效果，并通过将其与不同的材料映射策略集成来验证其是否具有抗耗荷性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Approach-for-Large-Scale-Real-Time-Quantification-of-Green-Fluorescent-Protein-Labeled-Biological-Samples-in-Microreactors"><a href="#Deep-Learning-Approach-for-Large-Scale-Real-Time-Quantification-of-Green-Fluorescent-Protein-Labeled-Biological-Samples-in-Microreactors" class="headerlink" title="Deep Learning Approach for Large-Scale, Real-Time Quantification of Green Fluorescent Protein-Labeled Biological Samples in Microreactors"></a>Deep Learning Approach for Large-Scale, Real-Time Quantification of Green Fluorescent Protein-Labeled Biological Samples in Microreactors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01384">http://arxiv.org/abs/2309.01384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wei, Sai Mu Dalike Abaxi, Nawaz Mehmood, Luoquan Li, Fuyang Qu, Guangyao Cheng, Dehua Hu, Yi-Ping Ho, Scott Wu Yuan, Ho-Pui Ho<br>for: 这个研究旨在开发一种基于深度学习的批处理管线，以自动分类和量化GFP标记的微反应器。methods: 该方法使用深度学习算法自动分类和量化GFP标记的微反应器，并且可以在标准实验室fluorescence Mikroskop用于实时精确量化。results: 该研究发现，使用该方法可以准确预测微反应器的大小和占据状态，并且可以在2.5秒钟内量化超过2,000个微反应器（在10张图像中），并且具有1000倍的分辨率。<details>
<summary>Abstract</summary>
Absolute quantification of biological samples entails determining expression levels in precise numerical copies, offering enhanced accuracy and superior performance for rare templates. However, existing methodologies suffer from significant limitations: flow cytometers are both costly and intricate, while fluorescence imaging relying on software tools or manual counting is time-consuming and prone to inaccuracies. In this study, we have devised a comprehensive deep-learning-enabled pipeline that enables the automated segmentation and classification of GFP (green fluorescent protein)-labeled microreactors, facilitating real-time absolute quantification. Our findings demonstrate the efficacy of this technique in accurately predicting the sizes and occupancy status of microreactors using standard laboratory fluorescence microscopes, thereby providing precise measurements of template concentrations. Notably, our approach exhibits an analysis speed of quantifying over 2,000 microreactors (across 10 images) within remarkably 2.5 seconds, and a dynamic range spanning from 56.52 to 1569.43 copies per micron-liter. Furthermore, our Deep-dGFP algorithm showcases remarkable generalization capabilities, as it can be directly applied to various GFP-labeling scenarios, including droplet-based, microwell-based, and agarose-based biological applications. To the best of our knowledge, this represents the first successful implementation of an all-in-one image analysis algorithm in droplet digital PCR (polymerase chain reaction), microwell digital PCR, droplet single-cell sequencing, agarose digital PCR, and bacterial quantification, without necessitating any transfer learning steps, modifications, or retraining procedures. We firmly believe that our Deep-dGFP technique will be readily embraced by biomedical laboratories and holds potential for further development in related clinical applications.
</details>
<details>
<summary>摘要</summary>
全程量化生物样本的实现需要确定表达水平的准确数值，提供了更高的精度和性能，特别是对于罕见模板。然而，现有的方法ologies有限，流率计价仪器costly和复杂，而基于软件工具或手动计数的抗体影像分析方法时间consuming和不准确。在本研究中，我们开发了一个涵盖全 Deep-learning-enabled 管道，可以自动 segmentation和类型化 GFP（绿色抗体）标记的微反应器，实现实时全程量化。我们的发现表明该技术可以准确预测微反应器的大小和占用状态，从而提供精确的模板浓度测量。另外，我们的 Deep-dGFP 算法展示了杰出的通用能力，可以直接应用于多种 GFP 标记方式，包括滴度基本、微瓶基本、agarose基本生物应用。根据我们所知，这是首次实现了无需转移学习步骤、修改或重训练的全程量化图像分析算法，可以应用于批量数计、微瓶数计、滴度单细Sequencing、agarose数计和细菌量化。我们 firmly believe 的 Deep-dGFP 技术将被生物医学实验室广泛采用，并具有进一步发展的临床应用潜力。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/eess.IV_2023_09_04/" data-id="clorjzlg0016gf188db2t9rqc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/47/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/46/">46</a><a class="page-number" href="/page/47/">47</a><span class="page-number current">48</span><a class="page-number" href="/page/49/">49</a><a class="page-number" href="/page/50/">50</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/49/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
