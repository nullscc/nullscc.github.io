
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/71/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/cs.AI_2023_07_20/" class="article-date">
  <time datetime="2023-07-20T12:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/20/cs.AI_2023_07_20/">cs.AI - 2023-07-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Approximate-Computing-Survey-Part-II-Application-Specific-Architectural-Approximation-Techniques-and-Applications"><a href="#Approximate-Computing-Survey-Part-II-Application-Specific-Architectural-Approximation-Techniques-and-Applications" class="headerlink" title="Approximate Computing Survey, Part II: Application-Specific &amp; Architectural Approximation Techniques and Applications"></a>Approximate Computing Survey, Part II: Application-Specific &amp; Architectural Approximation Techniques and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11128">http://arxiv.org/abs/2307.11128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Leon, Muhammad Abdullah Hanif, Giorgos Armeniakos, Xun Jiao, Muhammad Shafique, Kiamal Pekmestzi, Dimitrios Soudris</li>
<li>for: 本文旨在探讨近似计算的应用和技术，以提高计算系统的能效性和性能。</li>
<li>methods: 本文使用了多种应用特定和体系结构的近似技术，包括系统下的硬件和软件近似技术。</li>
<li>results: 本文对近似计算的应用谱和技术进行了全面的检讨和分析，并提出了未来研究的挑战和方向。<details>
<summary>Abstract</summary>
The challenging deployment of compute-intensive applications from domains such Artificial Intelligence (AI) and Digital Signal Processing (DSP), forces the community of computing systems to explore new design approaches. Approximate Computing appears as an emerging solution, allowing to tune the quality of results in the design of a system in order to improve the energy efficiency and/or performance. This radical paradigm shift has attracted interest from both academia and industry, resulting in significant research on approximation techniques and methodologies at different design layers (from system down to integrated circuits). Motivated by the wide appeal of Approximate Computing over the last 10 years, we conduct a two-part survey to cover key aspects (e.g., terminology and applications) and review the state-of-the art approximation techniques from all layers of the traditional computing stack. In Part II of our survey, we classify and present the technical details of application-specific and architectural approximation techniques, which both target the design of resource-efficient processors/accelerators & systems. Moreover, we present a detailed analysis of the application spectrum of Approximate Computing and discuss open challenges and future directions.
</details>
<details>
<summary>摘要</summary>
“ computationally intensive 应用程序（如人工智能和数位信号处理）的部署困难，迫使计算系统社群探索新的设计方法。粗略计算被视为一种emerging solution，允许在系统设计中调整结果质量，以提高能效性和/或性能。这个崭新的思维方式在学术和业界都引起了广泛关注，并且在不同的设计层（从系统到集成电路）进行了大量的研究。惊叹于过去十年内 aproximate computing 的广泛吸引力，我们将进行两部分的调查，涵盖关键方面（如术语和应用），并回顾各层设计的粗略技术。在第二部分中，我们分类和详细介绍了特定应用和建筑方面的粗略技术，包括资源有效的处理器/加速器和系统的设计。此外，我们也进行了综合的应用spectrum 分析，讨论开启的挑战和未来方向。”
</details></li>
</ul>
<hr>
<h2 id="PE-YOLO-Pyramid-Enhancement-Network-for-Dark-Object-Detection"><a href="#PE-YOLO-Pyramid-Enhancement-Network-for-Dark-Object-Detection" class="headerlink" title="PE-YOLO: Pyramid Enhancement Network for Dark Object Detection"></a>PE-YOLO: Pyramid Enhancement Network for Dark Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10953">http://arxiv.org/abs/2307.10953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiangchenyin/pe-yolo">https://github.com/xiangchenyin/pe-yolo</a></li>
<li>paper_authors: Xiangchen Yin, Zhenda Yu, Zetao Fei, Wenjun Lv, Xin Gao</li>
<li>for: 提高黑暗环境下物体检测的精度和效率</li>
<li>methods: 使用 Laplacian Pyramid  decomposes 图像，并提出了细节处理模块 (DPM) 和低频增强筛 (LEF) 来增强图像细节和低频semantics，并采用端到端结合训练方法和normal detection loss 进行训练。</li>
<li>results: 在 ExDark 数据集上进行测试，PE-YOLO 在黑暗环境下物体检测中达到了 78.0% 的 mAP 和 53.6 的 FPS，较其他黑暗检测器和低光照增强模型更高。<details>
<summary>Abstract</summary>
Current object detection models have achieved good results on many benchmark datasets, detecting objects in dark conditions remains a large challenge. To address this issue, we propose a pyramid enhanced network (PENet) and joint it with YOLOv3 to build a dark object detection framework named PE-YOLO. Firstly, PENet decomposes the image into four components of different resolutions using the Laplacian pyramid. Specifically we propose a detail processing module (DPM) to enhance the detail of images, which consists of context branch and edge branch. In addition, we propose a low-frequency enhancement filter (LEF) to capture low-frequency semantics and prevent high-frequency noise. PE-YOLO adopts an end-to-end joint training approach and only uses normal detection loss to simplify the training process. We conduct experiments on the low-light object detection dataset ExDark to demonstrate the effectiveness of ours. The results indicate that compared with other dark detectors and low-light enhancement models, PE-YOLO achieves the advanced results, achieving 78.0% in mAP and 53.6 in FPS, respectively, which can adapt to object detection under different low-light conditions. The code is available at https://github.com/XiangchenYin/PE-YOLO.
</details>
<details>
<summary>摘要</summary>
当前的对象检测模型在许多标准数据集上已经实现了好的结果，但检测对象在黑暗环境中仍然是一大挑战。为解决这个问题，我们提出了卷积扩展网络（PENet），并与YOLOv3结合以建立一个适用于黑暗对象检测的框架，称为PE-YOLO。首先，PENet将图像分解成四个不同的分辨率的组件使用卷积 pyramid。我们提出了一个细节处理模块（DPM），用于增强图像的细节，该模块包括上下文分支和边分支。此外，我们还提出了一个低频增强筛选器（LEF），用于捕捉低频 semantics 并避免高频噪声。PE-YOLO采用了端到端集成训练方法，并只使用 normal detection loss 简化训练过程。我们在 ExDark 降霾对象检测数据集上进行了实验，结果表明，与其他黑暗检测器和低照明增强模型相比，PE-YOLO 实现了更高的 mAP 和 FPS 结果，分别达到 78.0% 和 53.6%。代码可以在 GitHub 上找到：https://github.com/XiangchenYin/PE-YOLO。
</details></li>
</ul>
<hr>
<h2 id="Proxy-Anchor-based-Unsupervised-Learning-for-Continuous-Generalized-Category-Discovery"><a href="#Proxy-Anchor-based-Unsupervised-Learning-for-Continuous-Generalized-Category-Discovery" class="headerlink" title="Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery"></a>Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10943">http://arxiv.org/abs/2307.10943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyungmin Kim, Sungho Suh, Daehwan Kim, Daun Jeong, Hansang Cho, Junmo Kim</li>
<li>for: 提出了一种无监督的类增长学习方法，用于在不具备先知知识的情况下，精确地找到 novel 类别。</li>
<li>methods: 方法基于 feature extractor 和 proxy anchors，首先在已知集上精度地调整 feature extractor，然后将样本分为 old 和 novel 类别，并在无监督集上进行 clustering。同时，使用 proxy anchors-based exemplar 来避免 catastrophic forgetting。</li>
<li>results: 实验结果表明，提出的方法在细化 datasets 上下适用于实际情况，并且超过了当前状态的方法。<details>
<summary>Abstract</summary>
Recent advances in deep learning have significantly improved the performance of various computer vision applications. However, discovering novel categories in an incremental learning scenario remains a challenging problem due to the lack of prior knowledge about the number and nature of new categories. Existing methods for novel category discovery are limited by their reliance on labeled datasets and prior knowledge about the number of novel categories and the proportion of novel samples in the batch. To address the limitations and more accurately reflect real-world scenarios, in this paper, we propose a novel unsupervised class incremental learning approach for discovering novel categories on unlabeled sets without prior knowledge. The proposed method fine-tunes the feature extractor and proxy anchors on labeled sets, then splits samples into old and novel categories and clusters on the unlabeled dataset. Furthermore, the proxy anchors-based exemplar generates representative category vectors to mitigate catastrophic forgetting. Experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods on fine-grained datasets under real-world scenarios.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:latest advances in deep learning have significantly improved the performance of various computer vision applications. However, discovering novel categories in an incremental learning scenario remains a challenging problem due to the lack of prior knowledge about the number and nature of new categories. Existing methods for novel category discovery are limited by their reliance on labeled datasets and prior knowledge about the number of novel categories and the proportion of novel samples in the batch. To address the limitations and more accurately reflect real-world scenarios, in this paper, we propose a novel unsupervised class incremental learning approach for discovering novel categories on unlabeled sets without prior knowledge. The proposed method fine-tunes the feature extractor and proxy anchors on labeled sets, then splits samples into old and novel categories and clusters on the unlabeled dataset. Furthermore, the proxy anchors-based exemplar generates representative category vectors to mitigate catastrophic forgetting. Experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods on fine-grained datasets under real-world scenarios.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese dialects. If you prefer Traditional Chinese, please let me know and I will be happy to provide the translation in that dialect as well.
</details></li>
</ul>
<hr>
<h2 id="PASTA-Pretrained-Action-State-Transformer-Agents"><a href="#PASTA-Pretrained-Action-State-Transformer-Agents" class="headerlink" title="PASTA: Pretrained Action-State Transformer Agents"></a>PASTA: Pretrained Action-State Transformer Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10936">http://arxiv.org/abs/2307.10936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Boige, Yannis Flet-Berliac, Arthur Flajolet, Guillaume Richard, Thomas Pierrot</li>
<li>for: 这个论文的目的是研究基于自动学习的RL探索方法，并提供一种基于transformer模型的模型PASTA，用于解决多种下游任务。</li>
<li>methods: 这个论文使用了一种统一的方法ology，包括使用各种基本的预训练目标，如下一个token预测，并在多个领域同时训练模型。</li>
<li>results: 这个论文的研究显示，使用PASTA模型可以在多种下游任务上达到优秀的性能，并且可以在不同的领域中进行参数效率的微调。<details>
<summary>Abstract</summary>
Self-supervised learning has brought about a revolutionary paradigm shift in various computing domains, including NLP, vision, and biology. Recent approaches involve pre-training transformer models on vast amounts of unlabeled data, serving as a starting point for efficiently solving downstream tasks. In the realm of reinforcement learning, researchers have recently adapted these approaches by developing models pre-trained on expert trajectories, enabling them to address a wide range of tasks, from robotics to recommendation systems. However, existing methods mostly rely on intricate pre-training objectives tailored to specific downstream applications. This paper presents a comprehensive investigation of models we refer to as Pretrained Action-State Transformer Agents (PASTA). Our study uses a unified methodology and covers an extensive set of general downstream tasks including behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation. Our goal is to systematically compare various design choices and provide valuable insights to practitioners for building robust models. Key highlights of our study include tokenization at the action and state component level, using fundamental pre-training objectives like next token prediction, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT). The developed models in our study contain fewer than 10 million parameters and the application of PEFT enables fine-tuning of fewer than 10,000 parameters during downstream adaptation, allowing a broad community to use these models and reproduce our experiments. We hope that this study will encourage further research into the use of transformers with first-principles design choices to represent RL trajectories and contribute to robust policy learning.
</details>
<details>
<summary>摘要</summary>
自顾学学习在不同的计算领域中引发了一场革命性的思维方式转移，包括自然语言处理、视觉和生物学。现有的方法通常是在大量无标签数据上预训练变换器模型，作为下游任务解决的起点。在回归学领域，研究人员已经采用了这些方法，开发了基于专家轨迹预训练的模型，以解决从机器人到推荐系统等广泛的任务。然而，现有的方法大多采用特定下游应用程序的复杂预训练目标。本文提出了一种统一的方法ologies，涵盖了广泛的通用下游任务，包括行为做模拟、离线RL、感知器失效鲁棒性和动力学变化适应。我们的目标是系统地比较不同的设计选择，提供价值的反思和建议给实践者。关键的高亮之处包括动作和状态层次化，使用基本预训练目标如下一个符号预测，在多个领域同时训练模型，以及使用参数高效缩放（PEFT）。我们的研究中的模型含 fewer than 10 million parameters，并且通过PEFT来缩放 fewer than 10,000 parameters During downstream adaptation，使得广泛的社区可以使用这些模型和复制我们的实验。我们希望这种研究能够鼓励更多的研究人员采用基于first principles的设计选择来表示RL轨迹，并为稳定政策学习做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Identical-and-Fraternal-Twins-Fine-Grained-Semantic-Contrastive-Learning-of-Sentence-Representations"><a href="#Identical-and-Fraternal-Twins-Fine-Grained-Semantic-Contrastive-Learning-of-Sentence-Representations" class="headerlink" title="Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations"></a>Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10932">http://arxiv.org/abs/2307.10932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingfa Xiao, Shuangyin Li, Lei Chen</li>
<li>for: 提高不监督学习的句子表示学习效果</li>
<li>methods: 提出一种新的同类双胞胎学习框架（IFTCL），能够同时适应不同的增强技术生成的正例对</li>
<li>results: IFTCL在九个语义文本相似性任务中表现出色，比前方法更高效和有效。<details>
<summary>Abstract</summary>
The enhancement of unsupervised learning of sentence representations has been significantly achieved by the utility of contrastive learning. This approach clusters the augmented positive instance with the anchor instance to create a desired embedding space. However, relying solely on the contrastive objective can result in sub-optimal outcomes due to its inability to differentiate subtle semantic variations between positive pairs. Specifically, common data augmentation techniques frequently introduce semantic distortion, leading to a semantic margin between the positive pair. While the InfoNCE loss function overlooks the semantic margin and prioritizes similarity maximization between positive pairs during training, leading to the insensitive semantic comprehension ability of the trained model. In this paper, we introduce a novel Identical and Fraternal Twins of Contrastive Learning (named IFTCL) framework, capable of simultaneously adapting to various positive pairs generated by different augmentation techniques. We propose a \textit{Twins Loss} to preserve the innate margin during training and promote the potential of data enhancement in order to overcome the sub-optimal issue. We also present proof-of-concept experiments combined with the contrastive objective to prove the validity of the proposed Twins Loss. Furthermore, we propose a hippocampus queue mechanism to restore and reuse the negative instances without additional calculation, which further enhances the efficiency and performance of the IFCL. We verify the IFCL framework on nine semantic textual similarity tasks with both English and Chinese datasets, and the experimental results show that IFCL outperforms state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
“对不同增强学习方法的自动识别学习优化有很大进步，特别是通过对比学习。这种方法将调整后的正例和anchor例集 clustering以创建愿景空间。但是，仅仅靠对比目标无法区别微妙的Semantic variation between positive pairs，导致训练时的低效。具体来说，常用的数据增强技术会导致Semantic distortion，从而导致正例之间的Semantic margin。而InfoNCE损失函数忽略这个Semantic margin，将主要关注在正例之间的相似性最大化，从而导致训练后的模型无法深入理解Semantic的问题。在这篇文章中，我们提出了一个名为Identical and Fraternal Twins of Contrastive Learning（简称IFCL）的新框架，可以同时适应不同的增强技术生成的正例。我们提出了一个名为Twins Loss的损失函数，以保持内在的Semantic margin during training，并且提高数据增强的可能性，以解决低效的问题。我们还提出了一个 hippocampus queue 机制，可以复原和重复使用负例而不需要额外计算，这有助于提高效率和表现。我们在九个semantic textual similarity任务上验证了IFCL框架，结果显示IFCL在训练后比州前方法更高效。”
</details></li>
</ul>
<hr>
<h2 id="MediaGPT-A-Large-Language-Model-For-Chinese-Media"><a href="#MediaGPT-A-Large-Language-Model-For-Chinese-Media" class="headerlink" title="MediaGPT : A Large Language Model For Chinese Media"></a>MediaGPT : A Large Language Model For Chinese Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10930">http://arxiv.org/abs/2307.10930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhonghao Wang, Zijia Lu, Bo Jin, Haiying Deng</li>
<li>for: 这个论文主要针对中文媒体领域的语言模型（LLMs）进行研究，旨在开发一个专门为中文媒体领域设计的语言模型（MediaGPT）。</li>
<li>methods: 这篇论文使用了多种任务类型和域定义提示类型，并在这些任务和提示类型的基础上训练了MediaGPT模型。</li>
<li>results: 根据专家评估和强模型评估，这篇论文证明了MediaGPT模型在多种中文媒体领域任务上表现出色，并证明了域数据和域定义提示类型对于建立有效的域特定LLM的重要性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown remarkable capabilities in generating high-quality text and making predictions based on large amounts of data, including the media domain. However, in practical applications, the differences between the media's use cases and the general-purpose applications of LLMs have become increasingly apparent, especially Chinese. This paper examines the unique characteristics of media-domain-specific LLMs compared to general LLMs, designed a diverse set of task instruction types to cater the specific requirements of the domain and constructed unique datasets that are tailored to the media domain. Based on these, we proposed MediaGPT, a domain-specific LLM for the Chinese media domain, training by domain-specific data and experts SFT data. By performing human experts evaluation and strong model evaluation on a validation set, this paper demonstrated that MediaGPT outperforms mainstream models on various Chinese media domain tasks and verifies the importance of domain data and domain-defined prompt types for building an effective domain-specific LLM.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展示了优质文本生成和基于大量数据的预测能力，包括媒体领域。然而，在实际应用中，媒体使用场景和通用应用场景中LML的不同特点在增加显著，特别是中文。本文研究媒体领域专门的LML与通用LML的特点相比，设计了具有媒体领域特定需求的多种任务指令类型，并构建了适应媒体领域的唯一数据集。基于这些，我们提出了MediaGPT，专门为中文媒体领域的领域特定LML，通过媒体领域特定的数据和专家SFT数据进行训练。通过专家评估和强模型评估在验证集上，本文证明了MediaGPT在多种中文媒体领域任务上表现出色，并证明了领域数据和领域定义的提示类型的重要性于建立有效的领域特定LML。
</details></li>
</ul>
<hr>
<h2 id="FLASK-Fine-grained-Language-Model-Evaluation-based-on-Alignment-Skill-Sets"><a href="#FLASK-Fine-grained-Language-Model-Evaluation-based-on-Alignment-Skill-Sets" class="headerlink" title="FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets"></a>FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10928">http://arxiv.org/abs/2307.10928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaistai/flask">https://github.com/kaistai/flask</a></li>
<li>paper_authors: Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo</li>
<li>for: 本研究旨在提供一种细化评估语言模型（LLMs）的方法，以更好地评估LLMs的表现，特别是在多个技能的组合下。</li>
<li>methods: 本研究使用的方法包括定义12种细化技能，以及根据用户指令的域名和难度水平来分配这些技能。此外，研究还使用了模型基于和人类基于的评估方法。</li>
<li>results: 研究发现，使用FLASK评估协议可以更好地评估LLMs的表现，并且可以分析模型在特定技能、域名和难度水平上的表现。此外，研究还发现了多种开源和商业LLMs之间的高度相关性。<details>
<summary>Abstract</summary>
Evaluation of Large Language Models (LLMs) is challenging because aligning to human values requires the composition of multiple skills and the required set of skills varies depending on the instruction. Recent studies have evaluated the performance of LLMs in two ways, (1) automatic evaluation on several independent benchmarks and (2) human or machined-based evaluation giving an overall score to the response. However, both settings are coarse-grained evaluations, not considering the nature of user instructions that require instance-wise skill composition, which limits the interpretation of the true capabilities of LLMs. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets), a fine-grained evaluation protocol that can be used for both model-based and human-based evaluation which decomposes coarse-level scoring to an instance-wise skill set-level. Specifically, we define 12 fine-grained skills needed for LLMs to follow open-ended user instructions and construct an evaluation set by allocating a set of skills for each instance. Additionally, by annotating the target domains and difficulty level for each instance, FLASK provides a holistic view with a comprehensive analysis of a model's performance depending on skill, domain, and difficulty. Through using FLASK, we compare multiple open-sourced and proprietary LLMs and observe highly-correlated findings between model-based and human-based evaluations. FLASK enables developers to more accurately measure the model performance and how it can be improved by analyzing factors that make LLMs proficient in particular skills. For practitioners, FLASK can be used to recommend suitable models for particular situations through comprehensive comparison among various LLMs. We release the evaluation data and code implementation at https://github.com/kaistAI/FLASK.
</details>
<details>
<summary>摘要</summary>
评估大型自然语言模型（LLM）具有挑战性，因为需要考虑多种技能的组合，并且这些技能的集合可以根据指令而变化。现有研究通过两种方法评估LLM的性能：一是自动评估多个独立的标准 benchmark，二是人或机器基于的全面评分。然而，这两种设置都是粗粒度的评估，不能考虑用户的指令需要实例化的技能组合，这限制了LLM的真实能力的解释。在这篇论文中，我们提出了FLASK（细化语言模型评估基于配对技能集）协议，可以用于模型基于和人类基于的评估，它将粗粒度评估 decomposes 到实例化技能集级别。具体来说，我们定义了LLM需要遵循开放式用户指令的12种细化技能，并构建了评估集，各实例分配了一组技能。此外，我们还对每个实例标注目标领域和难度水平，从而提供了全面分析模型性能的整体视图。通过使用FLASK，我们对多种开源和商业LLM进行比较，并发现了高度相关的发现。FLASK可以帮助开发者更准确地评估模型性能，并分析因素使得LLM在特定技能方面突出。对于实践者，FLASK可以用来建议适用于特定情况的模型，通过对多种LLM进行全面比较。我们在github上发布了评估数据和代码实现。
</details></li>
</ul>
<hr>
<h2 id="Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks"><a href="#Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks" class="headerlink" title="Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks"></a>Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10891">http://arxiv.org/abs/2307.10891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cxlvinchau/linna">https://github.com/cxlvinchau/linna</a></li>
<li>paper_authors: Calvin Chau, Jan Křetínský, Stefanie Mohr</li>
<li>for: 提高神经网络的可扩展性</li>
<li>methods: 使用抽象技术进行神经网络的减少</li>
<li>results: 提供了一个更 flexible的抽象框架，并通过实验证明了它的效果<details>
<summary>Abstract</summary>
Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.
</details>
<details>
<summary>摘要</summary>
吧抽象是一种关键的验证技术，可以提高神经网络的可扩展性。然而，它们在神经网络上的使用尚未得到广泛应用。先前的方法是将一些神经元替换为它们相似的一个，这种相似性可以是语法性（基于连接 между神经元的量）或semantic（基于神经元对各种输入的活动值）。 unfortunately，这些先前的方法只能实现moderate的减少，甚至不得不实现。在这种工作中，我们提供了一个更灵活的框架，允许一个神经元被替换为一个线性组合其他神经元，从而提高减少。我们应用这种方法both on syntactic和semantic abstractions，并实现和评估它们。此外，我们还引入了一种精度调整方法，allowing for finding a better balance between reduction and precision。
</details></li>
</ul>
<hr>
<h2 id="Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing"><a href="#Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing" class="headerlink" title="Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing"></a>Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10864">http://arxiv.org/abs/2307.10864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva</li>
<li>for: 提高文本到图像生成模型的可靠性和精度，特别是处理复杂的提示语时。</li>
<li>methods: 提出了两个新的损失函数：novel attendance loss和binding loss，用于提高生成的对象的特征匹配和精度。</li>
<li>results: 在多个评估标准上显示出优于现有方法的性能，能够准确地生成desired对象并改进特征匹配。<details>
<summary>Abstract</summary>
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks. More videos and updates can be found on the project page \url{https://sites.google.com/view/divide-and-bind}.
</details>
<details>
<summary>摘要</summary>
新型大规模文本至图生成模型，如稳定扩散（SD），已经展现出惊人的成绩，高度准确。 DESPITE THE AMAZING PROGRESS， current state-of-the-art models still struggle to generate images that fully adhere to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate semantics. It has shown promising results in generating simple prompts, such as "a cat and a dog". However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding.To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks. More videos and updates can be found on the project page [https://sites.google.com/view/divide-and-bind].
</details></li>
</ul>
<hr>
<h2 id="Goal-Conditioned-Reinforcement-Learning-with-Disentanglement-based-Reachability-Planning"><a href="#Goal-Conditioned-Reinforcement-Learning-with-Disentanglement-based-Reachability-Planning" class="headerlink" title="Goal-Conditioned Reinforcement Learning with Disentanglement-based Reachability Planning"></a>Goal-Conditioned Reinforcement Learning with Disentanglement-based Reachability Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10846">http://arxiv.org/abs/2307.10846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifeng Qian, Mingyu You, Hongjun Zhou, Xuanhui Xu, Bin He</li>
<li>for: 解决高维状态空间中的远程目标问题，提高目标Conditioned Reinforcement Learning（GCRL）的效率和性能。</li>
<li>methods: 提出了一种组合了目标Conditioned Reinforcement Learning（GCRL）和分解性能规划（REPlan）的算法，包括一种分解表示模块（DRM）和一种 temporal reachability discrimination module（REM），以解决高维状态空间中的远程目标问题。</li>
<li>results: 在三个视觉基于的模拟任务和一个真实世界任务中，对比PRIOR的方法，OUR的REPlan显示了明显的优异性能。<details>
<summary>Abstract</summary>
Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to spontaneously set diverse goals to learn a set of skills. Despite the excellent works proposed in various fields, reaching distant goals in temporally extended tasks remains a challenge for GCRL. Current works tackled this problem by leveraging planning algorithms to plan intermediate subgoals to augment GCRL. Their methods need two crucial requirements: (i) a state representation space to search valid subgoals, and (ii) a distance function to measure the reachability of subgoals. However, they struggle to scale to high-dimensional state space due to their non-compact representations. Moreover, they cannot collect high-quality training data through standard GC policies, which results in an inaccurate distance function. Both affect the efficiency and performance of planning and policy learning. In the paper, we propose a goal-conditioned RL algorithm combined with Disentanglement-based Reachability Planning (REPlan) to solve temporally extended tasks. In REPlan, a Disentangled Representation Module (DRM) is proposed to learn compact representations which disentangle robot poses and object positions from high-dimensional observations in a self-supervised manner. A simple REachability discrimination Module (REM) is also designed to determine the temporal distance of subgoals. Moreover, REM computes intrinsic bonuses to encourage the collection of novel states for training. We evaluate our REPlan in three vision-based simulation tasks and one real-world task. The experiments demonstrate that our REPlan significantly outperforms the prior state-of-the-art methods in solving temporally extended tasks.
</details>
<details>
<summary>摘要</summary>
goal-conditioned 学习（GCRL）可以让代理人自发设定多种目标，以学习一组技能。然而，在远程目标的任务中，GCRL仍然面临一个挑战。现有的方法通过使用规划算法计划间接产生的目标，以增强GCRL的能力。这些方法需要两个关键的要求：（i）一个状态表示空间来搜索有效的产生目标，以及（ii）一个距离函数来衡量产生目标的可达性。然而，它们在高维状态空间中缺乏扩展性，而且无法通过标准GC策略收集高质量的训练数据，这会导致距离函数的误差。这两个因素都会影响规划和策略学习的效率和性能。在本文中，我们提出一种具有目标条件的RL算法，并与基于分解的可达性规划（REPlan）结合，以解决时间扩展任务。在REPlan中，我们提出了一个分解表示模块（DRM），用于在自主学习方式下学习高维观察数据中的紧凑表示，并分解机器人姿态和物体位置。我们还设计了一个简单的REachability推理模块（REM），用于确定产生目标的时间距离。此外，REM还计算了内在的资金奖励，以鼓励收集训练数据。我们在三个视觉基础任务和一个实际任务中评估了我们的REPlan。实验结果显示，我们的REPlan在解决时间扩展任务方面明显超越了先前的状态艺术方法。
</details></li>
</ul>
<hr>
<h2 id="Modifications-of-the-Miller-definition-of-contrastive-counterfactual-explanations"><a href="#Modifications-of-the-Miller-definition-of-contrastive-counterfactual-explanations" class="headerlink" title="Modifications of the Miller definition of contrastive (counterfactual) explanations"></a>Modifications of the Miller definition of contrastive (counterfactual) explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10832">http://arxiv.org/abs/2307.10832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin McAreavey, Weiru Liu</li>
<li>for: 本研究旨在探讨贝尔纳和珀尔（HP）定义的对比（counterfactual）解释，以及迈尔（Miller）的定义是基于HP定义的修改后的版本。</li>
<li>methods: 本研究使用了HP定义的修改后的版本和迈尔定义，以及两种改进后的定义。</li>
<li>results: 本研究显示，迈尔定义继承了HP定义的问题，而我们提出的两种改进后的定义可以解决这些问题，同时保持与迈尔定义的精神。<details>
<summary>Abstract</summary>
Miller recently proposed a definition of contrastive (counterfactual) explanations based on the well-known Halpern-Pearl (HP) definitions of causes and (non-contrastive) explanations. Crucially, the Miller definition was based on the original HP definition of explanations, but this has since been modified by Halpern; presumably because the original yields counterintuitive results in many standard examples. More recently Borner has proposed a third definition, observing that this modified HP definition may also yield counterintuitive results. In this paper we show that the Miller definition inherits issues found in the original HP definition. We address these issues by proposing two improved variants based on the more robust modified HP and Borner definitions. We analyse our new definitions and show that they retain the spirit of the Miller definition where all three variants satisfy an alternative unified definition that is modular with respect to an underlying definition of non-contrastive explanations. To the best of our knowledge this paper also provides the first explicit comparison between the original and modified HP definitions.
</details>
<details>
<summary>摘要</summary>
美利尔最近提出了一种对比（Counterfactual）解释定义，基于著名的哈勒普-珀尔（HP）解释和非对比解释定义。关键是，美利尔定义基于原始HP解释定义，但是这已经被 modificado 由哈勒普，因为原始定义在许多标准例子中会导致counterintuitive结果。更 recientemente，博erner也提出了第三个定义，认为这 modificado HP定义也可能导致counterintuitive结果。在这篇论文中，我们示出了美利尔定义中的问题，并提出了两种改进的变体，基于更加robust的修改后HP定义和博erner定义。我们分析了我们的新定义，并证明它们保留了美利尔定义的灵魂，其中所有三个变体都满足一个备用的 reunified定义，该定义是对非对比解释定义的模块化。到目前为止，这篇论文也是第一篇明确地比较原始HP定义和修改后HP定义的论文。
</details></li>
</ul>
<hr>
<h2 id="What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems"><a href="#What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems" class="headerlink" title="What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems"></a>What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11784">http://arxiv.org/abs/2307.11784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saddek Bensalem, Chih-Hong Cheng, Wei Huang, Xiaowei Huang, Changshun Wu, Xingyu Zhao</li>
<li>for: 本研究旨在解决机器学习技术在安全关键领域中能够可靠地应用的挑战。</li>
<li>methods: 本研究提出了一种两步验证方法，以实现可证明的统计 garantía。</li>
<li>results: 研究发现现有的方法无法实际实现可证明的 garantía，提出了一种新的验证方法以实现这一目标。<details>
<summary>Abstract</summary>
Machine learning has made remarkable advancements, but confidently utilising learning-enabled components in safety-critical domains still poses challenges. Among the challenges, it is known that a rigorous, yet practical, way of achieving safety guarantees is one of the most prominent. In this paper, we first discuss the engineering and research challenges associated with the design and verification of such systems. Then, based on the observation that existing works cannot actually achieve provable guarantees, we promote a two-step verification method for the ultimate achievement of provable statistical guarantees.
</details>
<details>
<summary>摘要</summary>
机器学习技术已经取得了很大的进步，但在安全关键领域使用学习启用的组件仍然存在挑战。其中一个最大的挑战是实现安全保证的方法。在这篇论文中，我们首先讨论了在设计和验证这些系统方面的工程和研究挑战。然后，根据现有的工作不能实现可证明的保证的观察，我们提出了一种两步验证方法以实现可证明的统计保证。</SYS>Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".Here's the translation of the text into Traditional Chinese:<SYS>机器学习技术已经取得了很大的进步，但在安全关键领域使用学习启用的 компонент仍然存在挑战。其中一个最大的挑战是实现安全保证的方法。在这篇论文中，我们首先讨论了在设计和验证这些系统方面的工程和研究挑战。然后，根据现有的工作不能实现可证明的保证的观察，我们提出了一种两步验证方法以实现可证明的统计保证。</SYS>Note: Traditional Chinese is also known as "Formal Chinese" or "Classical Chinese".
</details></li>
</ul>
<hr>
<h2 id="On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport"><a href="#On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport" class="headerlink" title="On Combining Expert Demonstrations in Imitation Learning via Optimal Transport"></a>On Combining Expert Demonstrations in Imitation Learning via Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10810">http://arxiv.org/abs/2307.10810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning">https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning</a></li>
<li>paper_authors: Ilana Sebag, Samuel Cohen, Marc Peter Deisenroth</li>
<li>For: 本研究旨在解决多个专家示范的组合问题，以便智能代理人可以吸取多个专家的知识并学习多种不同的状态轨迹。* Methods: 本研究使用多元优质运输距离来组合多个专家示范，以提供一种更合理的状态轨迹的均值。* Results: 研究表明，使用多元优质运输距离可以更好地组合多个专家示范，并且可以在OpenAI Gym控制环境中提高智能代理人的学习效率。<details>
<summary>Abstract</summary>
Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environments and demonstrates that the standard method is not always optimal.
</details>
<details>
<summary>摘要</summary>
<<SYS>>模仿学习（IL）目标是教育代理人特定任务通过专家示范。一种关键的IL方法是定义代理人和专家之间的距离，并找到一个代理人策略，以最小化这个距离。优化运输方法在模仿学习中广泛使用，因为它们提供了衡量代理人和专家轨迹之间有意义的距离的方法。然而，如何优化多个专家示范的结合尚未得到广泛研究。标准方法是将状态(-动作)轨迹直接 concatenate，这会导致轨迹多模式问题。我们提出了一种另一种方法，使用多元优化运输距离，可以在OT意义下组合多个和多样的状态轨迹，提供一个更合理的示范均值。我们的方法允许代理人从多个专家学习，并在OpenAI Gym控制环境中分析了其效率，结果显示，标准方法并不总是最佳。Note: "优化运输方法" in the original text refers to "optimal transport methods" in English.
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression"><a href="#Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression" class="headerlink" title="Communication-Efficient Split Learning via Adaptive Feature-Wise Compression"></a>Communication-Efficient Split Learning via Adaptive Feature-Wise Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10805">http://arxiv.org/abs/2307.10805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongjeong Oh, Jaeho Lee, Christopher G. Brinton, Yo-Seb Jeon</li>
<li>for: 提高分布式学习中的通信效率，减少中间特征和梯度 вектор的传输过程中的占用率。</li>
<li>methods: 提出了一种新的通信压缩框架 SplitFC，利用矩阵列表中不同的分散度来减少通信过程中的占用率。该框架包括两种压缩策略：首先，采用适应性特征排除和适应性量化。其次，通过链式规则， dropped 的特征 вектор和相关的梯度 вектор也会被 dropped。</li>
<li>results: 在 MNIST、CIFAR-10 和 CelebA 数据集上进行了实验，得到了与当前SL框架相比， SplitFC 提供了更多的5.6%的分类精度，同时它们需要与无压缩SL框架相比，320倍少的通信过程占用率。<details>
<summary>Abstract</summary>
This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process. The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices. SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization. In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped. In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantization levels of this strategy are derived in a closed-form expression. Simulation results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC provides more than a 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while they require 320 times less communication overhead compared to the vanilla SL framework without compression.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Adaptive feature-wise dropout: The intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped.2. Adaptive feature-wise quantization: The non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantization levels are derived in a closed-form expression.The proposed SplitFC framework achieves a more than 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while requiring 320 times less communication overhead compared to the vanilla SL framework without compression. The experimental results on the MNIST, CIFAR-10, and CelebA datasets demonstrate the effectiveness of SplitFC.</details></li>
</ol>
<hr>
<h2 id="Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning"><a href="#Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning" class="headerlink" title="Meta-Transformer: A Unified Framework for Multimodal Learning"></a>Meta-Transformer: A Unified Framework for Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10802">http://arxiv.org/abs/2307.10802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/invictus717/MetaTransformer">https://github.com/invictus717/MetaTransformer</a></li>
<li>paper_authors: Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue</li>
<li>for: 本研究旨在提出一种能够处理多Modalities数据的框架，并不需要任何对准多Modalities训练数据。</li>
<li>methods: 该框架基于一个冻结encoder来实现多Modalities感知， Raw输入数据从不同Modalities被映射到共享的Token空间中，然后使用冻结的encoder提取高级别的semantic特征。</li>
<li>results: 实验表明，Meta-Transformer可以处理各种任务，包括基本感知（文本、图像、点云、音频、视频）、实用应用（X射、红外、偏振、IMU）以及数据挖掘（图表、时间序列、图表）。<details>
<summary>Abstract</summary>
Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer
</details>
<details>
<summary>摘要</summary>
多模态学习目标是建立可处理多种模式的模型。尽管多年的开发努力，仍然困难设计一个统一网络来处理多种模式（如自然语言、2D图像、3D点云、音频、视频、时间序列、表格数据），因为这些模式之间存在深刻的差异。在这种情况下，我们提出了一个框架，名为Meta-Transformer，它利用一个冻结的编码器来实现多模态感知，无需任何配对多模态训练数据。Meta-Transformer框架由三个主要组成部分组成：一个统一数据tokenizer，一个共享encoder和下游任务特定的头。Meta-Transformer可以在12种模式上进行统一学习，无需配对数据。在不同的benchmark上进行的实验表明，Meta-Transformer可以处理各种任务，包括基本感知（文本、图像、点云、音频、视频）、实用应用（X射、红外、多spectral、IMU）和数据挖掘（图形、表格、时间序列）。Meta-Transformer表明了未来发展多模态智能的 transformer 的美好前景。代码将在 GitHub 上提供。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection"><a href="#Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection" class="headerlink" title="Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection"></a>Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10792">http://arxiv.org/abs/2307.10792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scortexio/patchcore-few-shot">https://github.com/scortexio/patchcore-few-shot</a></li>
<li>paper_authors: João Santos, Triet Tran, Oliver Rippel</li>
<li>for: 本研究探讨了使用少量样本进行异常检测（AD）的新趋势，并评估了现有的普shot AD算法在少量样本情况下的性能。</li>
<li>methods: 本研究使用了PatchCore算法，现代普shot AD&#x2F;异常分割（AS）算法的现状标准。研究人员对PatchCore算法的多种超参数进行优化，并应用了减少supervised learning中的减少学习技术。</li>
<li>results: 实验结果表明，对PatchCore算法的超参数优化可以实现显著性能提升，而图像水平的增强技术可能会，但并不一定，提高性能。基于这些发现，研究人员在VisA数据集上达到了新的状态Espoir en ligne de few-shot AD。此外，研究人员还确认了在AD&#x2F;AS领域探索特征提取器 WITH strong inductive bias的可能性。<details>
<summary>Abstract</summary>
Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples. While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting. It thus remains unclear if the performance of such pre-existing algorithms can be further improved. We address said question in this work. Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings. We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) significant performance improvements can be realized by optimizing hyperparameters such as the underlying feature extractor, and that (II) image-level augmentations can, but are not guaranteed, to improve performance. Based on these findings, we achieve a new state of the art in few-shot AD on VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to the few-shot setting. Last, we identify the investigation of feature extractors with a strong inductive bias as a potential future research direction for (few-shot) AD/AS.
</details>
<details>
<summary>摘要</summary>
新兴的几拍异常检测（AD）领域，即使使用只有几个选择的样本来分类正常和异常数据。而新提出的几拍AD方法都会与全shot领域的已有算法进行比较，但它们并没有专门优化这些算法以适应几拍设置。因此，它们的性能是否可以进一步改进，是一个未知问题。我们在这里回答这个问题。我们专门研究了使用PatchCore算法，当前的全shot AD/AS算法，在几拍和多拍设置下的AD/AS性能。我们假设可以通过（I）优化其多种超参数，以及（II）将几拍supervised学习中知道改进技术转移到AD领域来实现性能改进。我们对公共的VisA和MVTec AD数据集进行了广泛的实验，发现（I）可以通过优化特征提取器来实现显著性能提高，并且（II）图像水平的扩展可能会，但并不一定，提高性能。基于这些发现，我们在VisA上实现了新的state of the art，进一步证明了适应pre-existing AD/AS方法到几拍设置的优势。最后，我们认为在AD/AS领域中调查具有强 inductive bias的特征提取器是未来研究的 potential future research direction。
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory"><a href="#Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory" class="headerlink" title="Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory"></a>Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10768">http://arxiv.org/abs/2307.10768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanglab-deepneurocoglab/worm">https://github.com/zhanglab-deepneurocoglab/worm</a></li>
<li>paper_authors: Ankur Sikarwar, Mengmi Zhang</li>
<li>for: 本研究开发了一个全面的工作记忆（Working Memory，WM） benchmark dataset，用于评估人工智能（AI）WM模型的效果。</li>
<li>methods: 研究使用了10个任务和100万次试验，评估了4种功能、3种领域和11种行为和神经特征。同时，研究还包括了人类行为标准 als an upper bound for comparison。</li>
<li>results: 研究发现，AI模型在某些方面与大脑的工作记忆相似，如 primacy 和 recency 效应，以及各个领域和功能的神经团集和相关性。然而，研究也发现现有模型存在一些限制，无法完全模拟人类行为。这个数据集将成为认知心理学、神经科学和AI社区的 valuable resource，用于比较和改进WM模型，调查WM的神经基础，并开发人类类似的WM模型。<details>
<summary>Abstract</summary>
Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM. In the experiments, we also reveal some limitations in existing models to approximate human behavior. This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM's neural underpinnings, and develop WM models with human-like capabilities. Our source code and data are available at https://github.com/ZhangLab-DeepNeuroCogLab/WorM.
</details>
<details>
<summary>摘要</summary>
工作记忆（WM），一种基本的认知过程，扮演短期存储、 интеграción、操作和检索信息的重要角色，对理智和决策任务起着重要作用。为了有效开发和评估人工智能WM模型，需要一些具有多方面特点的WM数据集。在这里，我们介绍了一个全面的Working Memory（WorM）数据集，用于这种目的。WorM包含10项任务和总共100万次评估，覆盖4种功能、3种领域和11种行为和神经特征。我们同时训练和测试了当前最佳的回归神经网络和转换器模型。我们还包括了人类行为标准 als an upper bound for comparison。我们的结果表明，人工智能模型在部分特征上与大脑WM相似，主要是 primacy和 recency效应，以及各个领域和功能特征的神经团和相关性。在实验中，我们还发现了现有模型的一些限制，无法模拟人类行为。这个数据集将成为认知心理学、神经科学和人工智能这些领域的价值资源，提供一个标准化的框架，用于比较和改进WM模型，调查WM的神经基础，并开发人类类似的WM模型。我们的源代码和数据可以在https://github.com/ZhangLab-DeepNeuroCogLab/WorM上获取。
</details></li>
</ul>
<hr>
<h2 id="Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query"><a href="#Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query" class="headerlink" title="Actor-agnostic Multi-label Action Recognition with Multi-modal Query"></a>Actor-agnostic Multi-label Action Recognition with Multi-modal Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10763">http://arxiv.org/abs/2307.10763</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mondalanindya/msqnet">https://github.com/mondalanindya/msqnet</a></li>
<li>paper_authors: Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</li>
<li>for: 提高多种演员（包括人类和动物）动作识别的灵活性和性能。</li>
<li>methods: 提出了一种新的多模态多标签动作识别方法，基于 transformer 框架和文本特征，不需要actor pose estimation，可以更好地利用视觉和文本特征来表示动作类别。</li>
<li>results: 在五个公共数据集上进行了广泛的实验，与之前的actor-specificalternatives相比，MSQNet在人类和动物单标和多标动作识别任务上表现出了50%的提高。<details>
<summary>Abstract</summary>
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-specific model designs is a key advantage, as it removes the need for actor pose estimation altogether. Extensive experiments on five publicly available benchmarks show that our MSQNet consistently outperforms the prior arts of actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%. Code will be released at https://github.com/mondalanindya/MSQNet.
</details>
<details>
<summary>摘要</summary>
现有的动作识别方法通常是对应者特定的，因为actor的内在结构和外在特征之间存在差异。这需要对特定的演员进行姿势估计（例如人类 vs. 动物），导致模型设计复杂和维护成本高昂。此外，它们通常专注于学习视觉modal alone，单一标签分类，而忽略其他可用的资料来源（例如类别名称文本）和同时发生的多个动作。为了解决这些限制，我们提出了一个新的方法，即“actor-agnostic multi-modal multi-label action recognition”，它提供了各种演员类型的统一解决方案，包括人类和动物。我们进一步提出了一个名为“Multi-modal Semantic Query Network”（MSQNet）的新模型，它在基于物件检测框架（例如DETR）中实现了融合视觉和文本modalities以更好地表现动作类别。删除对特定演员的模型设计是MSQNet的关键优点，因为它消除了对姿势估计的需求。实际实验结果显示，我们的MSQNet在人类和动物单一和多个动作识别任务上顶对应的先进方法的50%。代码将会在https://github.com/mondalanindya/MSQNet中发布。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Perspectives-on-the-Impact-of-Artificial-Intelligence-on-the-Creativity-of-Knowledge-Work-Beyond-Mechanised-Plagiarism-and-Stochastic-Parrots"><a href="#Exploring-Perspectives-on-the-Impact-of-Artificial-Intelligence-on-the-Creativity-of-Knowledge-Work-Beyond-Mechanised-Plagiarism-and-Stochastic-Parrots" class="headerlink" title="Exploring Perspectives on the Impact of Artificial Intelligence on the Creativity of Knowledge Work: Beyond Mechanised Plagiarism and Stochastic Parrots"></a>Exploring Perspectives on the Impact of Artificial Intelligence on the Creativity of Knowledge Work: Beyond Mechanised Plagiarism and Stochastic Parrots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10751">http://arxiv.org/abs/2307.10751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Advait Sarkar<br>for: 这篇论文旨在探讨人工智能（AI）如何影响知识工作中的创造力和归功问题。methods: 本文使用例如文学批评、艺术史和版权法等领域的例子，展示了创造力和原创性是一个过程、作者或观众的财产，而不是一个可观测或信息理论的特征。results: 根据这些例子，我们可以看到AI在创造知识工作中shift知识生产到批评集成，从而更好地认可用户的创造和审核voice。这篇论文的目的是开始一个关于使用AI时更加细致地评估创造力和归功问题的对话。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI), and in particular generative models, are transformative tools for knowledge work. They problematise notions of creativity, originality, plagiarism, the attribution of credit, and copyright ownership. Critics of generative models emphasise the reliance on large amounts of training data, and view the output of these models as no more than randomised plagiarism, remix, or collage of the source data. On these grounds, many have argued for stronger regulations on the deployment, use, and attribution of the output of these models. However, these issues are not new or unique to artificial intelligence. In this position paper, using examples from literary criticism, the history of art, and copyright law, I show how creativity and originality resist definition as a notatable or information-theoretic property of an object, and instead can be seen as the property of a process, an author, or a viewer. Further alternative views hold that all creative work is essentially reuse (mostly without attribution), or that randomness itself can be creative. I suggest that creativity is ultimately defined by communities of creators and receivers, and the deemed sources of creativity in a workflow often depend on which parts of the workflow can be automated. Using examples from recent studies of AI in creative knowledge work, I suggest that AI shifts knowledge work from material production to critical integration. This position paper aims to begin a conversation around a more nuanced approach to the problems of creativity and credit assignment for generative models, one which more fully recognises the importance of the creative and curatorial voice of the users of these models and moves away from simpler notational or information-theoretic views.
</details>
<details>
<summary>摘要</summary>
人工智能（AI），尤其是生成模型，是知识工作中转化的工具。它们问题化了创ativity、原创性、抄袭、功劳归属和版权所有权的问题。对于这些模型的批评者来说，它们的输出只是基于大量训练数据的随机抄袭、重新排版或拼接，而不是真正的创新。为了应对这些问题，许多人提出了更加严格的法规，限制生成模型的部署、使用和归属。但是，这些问题并不是人工智能的新或特有的问题。在这份位点纸中，我使用文学批评、艺术历史和版权法的例子，表明创ativity和原创性无法定义为一个可观测或信息学性的属性，而是一个过程的属性、作者的属性或观众的属性。此外，一些人认为所有的创作工作都是无认准的重复（大多数无需归属），或者Randomness本身可以是创新的。我认为创ativity是由创作者和接收者社区定义的，并且生成模型在创作过程中的产生部分可以被自动化。使用最近的人工智能在创造知识工作中的研究例子，我建议AI将知识工作从物质生产转移到批判集成。本位点纸的目标是开启一种更加细致的方法，更好地认可生成模型的创新和归属问题，从而更好地满足用户的创作和评价需求。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Aware-Client-Selection-for-Federated-Learning"><a href="#Fairness-Aware-Client-Selection-for-Federated-Learning" class="headerlink" title="Fairness-Aware Client Selection for Federated Learning"></a>Fairness-Aware Client Selection for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10738">http://arxiv.org/abs/2307.10738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Shi, Zelei Liu, Zhuan Shi, Han Yu</li>
<li>for: 这个论文的目的是解决 Federated Learning (FL) 中 client 选择问题，以实现性能和公平性的平衡。</li>
<li>methods: 该方法基于 Lyapunov 优化，通过考虑客户端的声誉、参与 FL 任务的时间和对模型性能的贡献，动态调整客户端选择概率。不使用阈值基于声誉筛选，因此允许客户端在感知性能不佳时重新恢复声誉。</li>
<li>results: 对实际 multimedia 数据集进行了广泛的实验，结果显示，FairFedCS 比最佳现有方法平均提高了19.6%的公平性和0.73%的测试准确率。<details>
<summary>Abstract</summary>
Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data. Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem. Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients. The problem of balancing performance and fairness considerations when selecting FL clients remains open. To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance. By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceived poor performance, thereby further enhancing fair client treatment. Extensive experiments based on real-world multimedia datasets show that FairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy on average than the best-performing state-of-the-art approach.
</details>
<details>
<summary>摘要</summary>
Federated learning (FL) 已经允许多个数据所有者（简称 FL 客户）共同训练机器学习模型，不需要披露私人数据。由于 FL 服务器只能在每次训练中选择一定数量的客户，因此 FL 客户选择成为了重要的研究问题。现有的方法通常是专注于提高 FL 模型性能或对 FL 客户进行公平对待。对于将性能和公平考虑进行平衡的问题，仍然是一个开问题。为解决这个问题，我们提出了 Fairness-aware Federated Client Selection（FairFedCS）方法。基于 Lyapunov 优化，它可以在考虑客户的声誉、参与 FL 任务的时间和对模型性能的贡献之间进行动态调整客户选择机会的概率。不使用阈值基于声誉滤过，它为 FL 客户提供了重新证明自己的声誉的机会，进一步增强公平客户待遇。经过了基于实际 multimedia 数据的广泛实验，我们发现 FairFedCS 在平均情况下高于最佳现有方法的 19.6% 和 0.73% 。
</details></li>
</ul>
<hr>
<h2 id="LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem"><a href="#LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem" class="headerlink" title="LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?"></a>LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10719">http://arxiv.org/abs/2307.10719</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, Vardan Papyan</li>
<li>for: 本研究旨在探讨现有防御机制对大语言模型（LLM）的潜在危害的效果。</li>
<li>methods: 本研究使用理论分析表明现有的语义审核方法存在理论上的限制，并且攻击者可以通过重构允许的输出来重建禁止的输出。</li>
<li>results: 研究结果表明，semantic censorship是一个不可解决的问题， LLMs 的程序化和指令遵循能力使得潜在危害的输出仍然可以被重构。此外，攻击者可以通过重构允许的输出来重建禁止的输出，从而降低现有防御机制的效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Introducing-Risk-Shadowing-For-Decisive-and-Comfortable-Behavior-Planning"><a href="#Introducing-Risk-Shadowing-For-Decisive-and-Comfortable-Behavior-Planning" class="headerlink" title="Introducing Risk Shadowing For Decisive and Comfortable Behavior Planning"></a>Introducing Risk Shadowing For Decisive and Comfortable Behavior Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10714">http://arxiv.org/abs/2307.10714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Puphal, Julian Eggert</li>
<li>for: 本研究旨在解决城市驾驶中的群体互动问题，现有的自驾车行为规划器通常是对每个单个agent-to-agent互动 separately进行成本函数来找到最优的行为方案，以避免与其他agent相撞。</li>
<li>methods: 本研究提出了风险附层（risk shadowing）方法，可以超越单个互动，通过分析三个agent之间的群体互动来更好地理解情况。具体来说，提出的方法可以确定ego agent中的第一个其他agent不需要在行为规划器中考虑，因为这个第一个其他agent无法到达ego agent的路径由第二个其他agent阻挡。</li>
<li>results: 在实验中，使用风险附层作为行为规划器的上游筛选器可以规划出更加决策和舒适的驾驶策略，保证安全性。这种方法的可用性在不同的交叉口enario和长途驾驶中被证明。<details>
<summary>Abstract</summary>
We consider the problem of group interactions in urban driving. State-of-the-art behavior planners for self-driving cars mostly consider each single agent-to-agent interaction separately in a cost function in order to find an optimal behavior for the ego agent, such as not colliding with any of the other agents. In this paper, we develop risk shadowing, a situation understanding method that allows us to go beyond single interactions by analyzing group interactions between three agents. Concretely, the presented method can find out which first other agent does not need to be considered in the behavior planner of an ego agent, because this first other agent cannot reach the ego agent due to a second other agent obstructing its way. In experiments, we show that using risk shadowing as an upstream filter module for a behavior planner allows to plan more decisive and comfortable driving strategies than state of the art, given that safety is ensured in these cases. The usability of the approach is demonstrated for different intersection scenarios and longitudinal driving.
</details>
<details>
<summary>摘要</summary>
我们考虑了城市驾驶中群体互动的问题。当前最佳行为规划器 для自动驾驶车辆通常对每个单个代理-to-代理交互 separately 在成本函数中来找到优化的行为 для ego 代理，例如不与任何其他代理相撞。在这篇论文中，我们开发了风险阴影，一种群体理解方法，允许我们超越单一交互。具体来说，所表示的方法可以找出egos 代理中不需要考虑的第一个其他代理，因为这个第一个其他代理无法到达 ego 代理的因为第二个其他代理阻挡了其路径。在实验中，我们表明了使用风险阴影作为行为规划器的上游筛选模块可以规划更加决策和舒适的驾驶策略，只要保证安全。我们在不同的交叉点enario和长途驾驶中证明了这种方法的可用性。
</details></li>
</ul>
<hr>
<h2 id="Kick-Back-Relax-Learning-to-Reconstruct-the-World-by-Watching-SlowTV"><a href="#Kick-Back-Relax-Learning-to-Reconstruct-the-World-by-Watching-SlowTV" class="headerlink" title="Kick Back &amp; Relax: Learning to Reconstruct the World by Watching SlowTV"></a>Kick Back &amp; Relax: Learning to Reconstruct the World by Watching SlowTV</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10713">http://arxiv.org/abs/2307.10713</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jspenmar/slowtv_monodepth">https://github.com/jspenmar/slowtv_monodepth</a></li>
<li>paper_authors: Jaime Spencer, Chris Russell, Simon Hadfield, Richard Bowden</li>
<li>For: The paper is written for scaling self-supervised monocular depth estimation (SS-MDE) to vast quantities of data, and addressing the limitation of existing approaches to the automotive domain.* Methods: The paper proposes a large-scale SlowTV dataset curated from YouTube, which contains 1.7M images from diverse environments, and trains an SS-MDE model using this dataset that provides zero-shot generalization to a large collection of indoor&#x2F;outdoor datasets. The paper also introduces a collection of best-practices to further maximize performance and zero-shot generalization.* Results: The resulting model outperforms all existing SSL approaches and closes the gap on supervised SoTA, despite using a more efficient architecture.Here’s the same information in Simplified Chinese:* For: 这篇论文是为了扩大自监督单目深度估计（SS-MDE）的应用范围，并解决现有方法困于自动驾驶领域的局限性。* Methods: 论文提出了一个大规模的 SlowTV 数据集，来自 YouTube 上的170万张图像，包含了世界各地不同的季节旅游、景观驾驶和潜水等环境。使用这个数据集，论文训练了一个 SS-MDE 模型，能够零shot泛化到大量的室内&#x2F;外部数据集。* Results: 根据论文的结果，该模型比所有现有的 SSL 方法更高效，并且与超级vision SoTA 靠近。<details>
<summary>Abstract</summary>
Self-supervised monocular depth estimation (SS-MDE) has the potential to scale to vast quantities of data. Unfortunately, existing approaches limit themselves to the automotive domain, resulting in models incapable of generalizing to complex environments such as natural or indoor settings.   To address this, we propose a large-scale SlowTV dataset curated from YouTube, containing an order of magnitude more data than existing automotive datasets. SlowTV contains 1.7M images from a rich diversity of environments, such as worldwide seasonal hiking, scenic driving and scuba diving. Using this dataset, we train an SS-MDE model that provides zero-shot generalization to a large collection of indoor/outdoor datasets. The resulting model outperforms all existing SSL approaches and closes the gap on supervised SoTA, despite using a more efficient architecture.   We additionally introduce a collection of best-practices to further maximize performance and zero-shot generalization. This includes 1) aspect ratio augmentation, 2) camera intrinsic estimation, 3) support frame randomization and 4) flexible motion estimation. Code is available at https://github.com/jspenmar/slowtv_monodepth.
</details>
<details>
<summary>摘要</summary>
自我指导的单目深度估计（SS-MDE）有可能扩展到庞大的数据量。然而，现有的方法只限于汽车领域，导致模型无法泛化到复杂的环境，如自然环境或室内环境。为此，我们提出了一个大规模的SlowTV数据集，从YouTube上收集的170万张图像，包括世界各地不同季节的步行、景观驾车和潜水等环境。使用这个数据集，我们训练了一个SS-MDE模型，可以零执行泛化到大量的室内/外部数据集。这个模型与所有现有的SSL方法相比，表现出了更高的性能和零执行泛化能力，即使使用更有效的架构。此外，我们还介绍了一些最佳实践来进一步提高性能和零执行泛化。这些包括：1. 增加比例增强2. 摄像机内参数估计3. 支持帧随机化4. 灵活运动估计代码可以在https://github.com/jspenmar/slowtv_monodepth上下载。
</details></li>
</ul>
<hr>
<h2 id="AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models"><a href="#AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models" class="headerlink" title="AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models"></a>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10711">http://arxiv.org/abs/2307.10711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachun Pan, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan</li>
<li>for: 这 paper 目的是解决 diffusion probabilistic models (DPMs) 的自适应 Customization 问题，只使用用户提供的 differentiable metric 作为指导。</li>
<li>methods: 这 paper 使用了一种新的方法 called AdjointDPM，它首先使用泛化模型生成新样本，然后使用逆变函数感知方法来归因损失到模型参数（包括 conditioning signals、网络参数和初始噪声）。</li>
<li>results: 这 paper 在三个有趣的任务上 demonstrate 了 AdjointDPM 的效果：将视觉特效转换为标识符文本嵌入，finetune DPMs  для特定类型的风格化，以及优化初始噪声来生成安全审核中的敌意样本。<details>
<summary>Abstract</summary>
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and gradient backpropagation processes, we further reparameterize the probability-flow ODE and augmented ODE as simple non-stiff ODEs using exponential integration. Finally, we demonstrate the effectiveness of AdjointDPM on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing.
</details>
<details>
<summary>摘要</summary>
现有的自定义方法需要访问多个参考示例来对预训练的扩散概率模型（DPM）进行对接。这篇论文目标是解决DPM自定义时只有用户提供的可微分度量的挑战。由于扩散过程中的搜索过程包含多个递归调用杂化网络，直观的梯度反射需要保存所有迭代的 intermediate states，从而导致内存占用极高。为解决这个问题，我们提出了一种新的方法：AdjointDPM。AdjointDPM方法首先通过解决相应的概率流方程来生成新的样本。然后，它使用散度敏感方法来倒推损失的梯度到模型参数（包括conditioning信号、网络参数和初始噪声）。为了减少在前向生成和梯度倒推过程中的数值错误，我们进一步重parameterize概率流方程和扩充方程为简单的非硬式ODE。最后，我们示cases了AdjointDPM在三个有趣的任务上的效果：将视觉特效转换为标识符 embedding，finetune DPM  для特定类型的风格化，以及优化初始噪声以生成安全审核中的敌意样本。
</details></li>
</ul>
<hr>
<h2 id="Towards-an-architectural-framework-for-intelligent-virtual-agents-using-probabilistic-programming"><a href="#Towards-an-architectural-framework-for-intelligent-virtual-agents-using-probabilistic-programming" class="headerlink" title="Towards an architectural framework for intelligent virtual agents using probabilistic programming"></a>Towards an architectural framework for intelligent virtual agents using probabilistic programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10693">http://arxiv.org/abs/2307.10693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anton Andreev, Grégoire Cattan<br>for:ECAs (embodied conversational agents)methods:probabilistic programming, Bayesian networks, and distributionsresults:more natural behavior, adaptation to user preferences, and evolving internal states over time<details>
<summary>Abstract</summary>
We present a new framework called KorraAI for conceiving and building embodied conversational agents (ECAs). Our framework models ECAs' behavior considering contextual information, for example, about environment and interaction time, and uncertain information provided by the human interaction partner. Moreover, agents built with KorraAI can show proactive behavior, as they can initiate interactions with human partners. For these purposes, KorraAI exploits probabilistic programming. Probabilistic models in KorraAI are used to model its behavior and interactions with the user. They enable adaptation to the user's preferences and a certain degree of indeterminism in the ECAs to achieve more natural behavior. Human-like internal states, such as moods, preferences, and emotions (e.g., surprise), can be modeled in KorraAI with distributions and Bayesian networks. These models can evolve over time, even without interaction with the user. ECA models are implemented as plugins and share a common interface. This enables ECA designers to focus more on the character they are modeling and less on the technical details, as well as to store and exchange ECA models. Several applications of KorraAI ECAs are possible, such as virtual sales agents, customer service agents, virtual companions, entertainers, or tutors.
</details>
<details>
<summary>摘要</summary>
我们提出了一个新的框架called KorraAI，用于设计和建立具有身体语言功能的会话代理人(ECAs)。我们的框架对ECAs的行为进行了考虑，包括上下文信息（如环境和互动时间）以及由人类互动伙伴提供的不确定信息。此外，由KorraAI构建的代理人可以显示主动的行为，可以与人类互动伙伴发起互动。为达到这些目的，KorraAI利用概率编程。概率模型在KorraAI中用于模型行为和人类互动。它们允许适应用户的偏好，并在ECAs中实现更自然的行为。内部状态，如情绪、喜好和惊喜（例如）可以在KorraAI中被模型为分布和 bayesian网络。这些模型可以随着时间的推移而发展，无需与用户交互。ECAs的模型实现为插件，共享公共接口。这使得ECA设计者可以更注重模仿的人物，而不是技术细节，同时也可以存储和交换ECAs模型。ECAs可以应用于虚拟销售代理人、客户服务代理人、虚拟伴侣、娱乐者或教育师。
</details></li>
</ul>
<hr>
<h2 id="Bounded-Combinatorial-Reconfiguration-with-Answer-Set-Programming"><a href="#Bounded-Combinatorial-Reconfiguration-with-Answer-Set-Programming" class="headerlink" title="Bounded Combinatorial Reconfiguration with Answer Set Programming"></a>Bounded Combinatorial Reconfiguration with Answer Set Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10688">http://arxiv.org/abs/2307.10688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuya Yamada, Mutsunori Banbara, Katsumi Inoue, Torsten Schaub</li>
<li>for: 解决 combinatorial reconfiguration 问题</li>
<li>methods: 使用 Answer Set Programming (ASP) 开发 bounded combinatorial reconfiguration 方法</li>
<li>results: 实现了 solver track 中所有 metric 的解决方案，并在 single-engine solvers track 中独立排名第一，并进行了 empirical analysis 评估所有 CoRe Challenge 2022 实例。<details>
<summary>Abstract</summary>
We develop an approach called bounded combinatorial reconfiguration for solving combinatorial reconfiguration problems based on Answer Set Programming (ASP). The general task is to study the solution spaces of source combinatorial problems and to decide whether or not there are sequences of feasible solutions that have special properties. The resulting recongo solver covers all metrics of the solver track in the most recent international competition on combinatorial reconfiguration (CoRe Challenge 2022). recongo ranked first in the shortest metric of the single-engine solvers track. In this paper, we present the design and implementation of bounded combinatorial reconfiguration, and present an ASP encoding of the independent set reconfiguration problem that is one of the most studied combinatorial reconfiguration problems. Finally, we present empirical analysis considering all instances of CoRe Challenge 2022.
</details>
<details>
<summary>摘要</summary>
我们开发了一种名为 bounded combinatorial reconfiguration 的方法，用于解决 combinatorial reconfiguration 问题，基于 Answer Set Programming (ASP)。通用任务是研究源 combinatorial 问题的解空间，并判断是否存在特殊性的解册列表。 resulting recongo solver 覆盖了最新的国际合作计划（CoRe Challenge 2022）中的所有纪录。 recongo 在单引擎solvers track中的最短度metric中排名第一。在这篇论文中，我们介绍 bounded combinatorial reconfiguration 的设计和实现，并提供了独立集重配置问题的 ASP 编码，这是 combinatorial reconfiguration 问题中最受研究的一个。 finally，我们对 CoRe Challenge 2022 所有实例进行了实验分析。
</details></li>
</ul>
<hr>
<h2 id="A-Personalized-Recommender-System-Based-on-Knowledge-Graph-Embeddings"><a href="#A-Personalized-Recommender-System-Based-on-Knowledge-Graph-Embeddings" class="headerlink" title="A Personalized Recommender System Based-on Knowledge Graph Embeddings"></a>A Personalized Recommender System Based-on Knowledge Graph Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10680">http://arxiv.org/abs/2307.10680</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nil9/Master-Thesis">https://github.com/nil9/Master-Thesis</a></li>
<li>paper_authors: Ngoc Luyen Le, Marie-Hélène Abel, Philippe Gouspillou</li>
<li>for: 这篇论文是关于构建个性化推荐系统的研究，通过知识图 embedding 技术来更好地捕捉用户和物品之间的隐式连接，提供更加准确的推荐。</li>
<li>methods: 该论文提出了一种基于知识图 embedding 技术的个性化推荐系统，通过在知识图中嵌入用户和物品，更好地捕捉用户的偏好和需求，提供更加准确的推荐。</li>
<li>results: 实验结果表明，该方法可以提供高度相似的推荐，并且可以捕捉用户的偏好和需求，提供更加准确的推荐。<details>
<summary>Abstract</summary>
Knowledge graphs have proven to be effective for modeling entities and their relationships through the use of ontologies. The recent emergence in interest for using knowledge graphs as a form of information modeling has led to their increased adoption in recommender systems. By incorporating users and items into the knowledge graph, these systems can better capture the implicit connections between them and provide more accurate recommendations. In this paper, we investigate and propose the construction of a personalized recommender system via knowledge graphs embedding applied to the vehicle purchase/sale domain. The results of our experimentation demonstrate the efficacy of the proposed method in providing relevant recommendations that are consistent with individual users.
</details>
<details>
<summary>摘要</summary>
知识图有效地模型了实体和其关系，通过使用 ontology。近期关于使用知识图作为信息模型的兴趣增长，导致它们在推荐系统中的应用更加普遍。通过将用户和项目 embedding到知识图中，这些系统可以更好地捕捉用户和项目之间的隐式连接，提供更准确的推荐。本文investigates和提出了基于知识图embedding的个性化推荐系统的建立，应用于汽车购买/销售领域。实验结果表明，提posed方法可以提供适合个人用户的有关推荐。
</details></li>
</ul>
<hr>
<h2 id="SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models"><a href="#SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models" class="headerlink" title="SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"></a>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10635">http://arxiv.org/abs/2307.10635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mandyyyyii/scibench">https://github.com/mandyyyyii/scibench</a></li>
<li>paper_authors: Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang</li>
<li>for: 这篇论文旨在检验大型自然语言模型（LLM）的推理能力，以便在科学问题解决中提高其表现。</li>
<li>methods: 这篇论文使用了两个精心准备的数据集：一个是大学生水平的科学问题，包括数学、化学和物理等领域的问题，另一个是计算机科学和数学等专业考试的问题。研究者使用了多种提示策略来评估 LLM 的表现。</li>
<li>results: 研究结果显示，当前 LLM 的表现并不出色，总得分只有 35.80%。此外，研究者还分类了 LLM 的错误为十种问题解决能力，发现没有一种提示策略能够明显超越其他策略，一些策略可以提高某些问题解决能力，却导致其他能力下降。<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with an overall score of merely 35.80%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.
</details>
<details>
<summary>摘要</summary>
SciBench contains two datasets: an open set with a range of collegiate-level scientific problems from mathematics, chemistry, and physics textbooks, and a closed set with problems from undergraduate-level exams in computer science and mathematics. We conduct an in-depth benchmark study of two representative LLMs with various prompting strategies, and find that current LLMs score only 35.80% overall.Through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis shows that no single prompting strategy significantly outperforms others, and that some strategies that improve in certain problem-solving skills result in declines in other skills. We believe that SciBench will drive further developments in the reasoning abilities of LLMs, ultimately contributing to scientific research and discovery.
</details></li>
</ul>
<hr>
<h2 id="Pluvio-Assembly-Clone-Search-for-Out-of-domain-Architectures-and-Libraries-through-Transfer-Learning-and-Conditional-Variational-Information-Bottleneck"><a href="#Pluvio-Assembly-Clone-Search-for-Out-of-domain-Architectures-and-Libraries-through-Transfer-Learning-and-Conditional-Variational-Information-Bottleneck" class="headerlink" title="Pluvio: Assembly Clone Search for Out-of-domain Architectures and Libraries through Transfer Learning and Conditional Variational Information Bottleneck"></a>Pluvio: Assembly Clone Search for Out-of-domain Architectures and Libraries through Transfer Learning and Conditional Variational Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10631">http://arxiv.org/abs/2307.10631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwei Fu, Steven H. H. Ding, Furkan Alaca, Benjamin C. M. Fung, Philippe Charland</li>
<li>for: 本研究旨在解决现有的Assembly clone search方法在面对未看过的架构和库时的限制。</li>
<li>methods: 本研究提议通过大规模预训练的自然语言模型来帮助现有的学习型方法，以扩大其应用范围。此外，我们还提出一种强化学习代理来消除无用和重复的токен。</li>
<li>results: 我们通过模拟未看过架构做clone搜索场景，并实验结果表明我们的方法比现有方法更有效。<details>
<summary>Abstract</summary>
The practice of code reuse is crucial in software development for a faster and more efficient development lifecycle. In reality, however, code reuse practices lack proper control, resulting in issues such as vulnerability propagation and intellectual property infringements. Assembly clone search, a critical shift-right defence mechanism, has been effective in identifying vulnerable code resulting from reuse in released executables. Recent studies on assembly clone search demonstrate a trend towards using machine learning-based methods to match assembly code variants produced by different toolchains. However, these methods are limited to what they learn from a small number of toolchain variants used in training, rendering them inapplicable to unseen architectures and their corresponding compilation toolchain variants.   This paper presents the first study on the problem of assembly clone search with unseen architectures and libraries. We propose incorporating human common knowledge through large-scale pre-trained natural language models, in the form of transfer learning, into current learning-based approaches for assembly clone search. Transfer learning can aid in addressing the limitations of the existing approaches, as it can bring in broader knowledge from human experts in assembly code. We further address the sequence limit issue by proposing a reinforcement learning agent to remove unnecessary and redundant tokens. Coupled with a new Variational Information Bottleneck learning strategy, the proposed system minimizes the reliance on potential indicators of architectures and optimization settings, for a better generalization of unseen architectures. We simulate the unseen architecture clone search scenarios and the experimental results show the effectiveness of the proposed approach against the state-of-the-art solutions.
</details>
<details>
<summary>摘要</summary>
software开发中代码重用是一项非常重要的做法，可以帮助提高开发周期的速度和效率。然而，实际上代码重用很难受到有效的控制，这会导致问题如攻击协议和知识产权侵犯。Assembly clone search是一种重要的防御机制，可以 identificatin vulnerable code resulting from reuse in released executables。 recent studies have shown a trend towards using machine learning-based methods to match assembly code variants produced by different toolchains。然而，这些方法受到训练中使用的小量工具链变体的限制，使其对未看过的架构和相应的编译工具链变体无法适用。这篇论文是关于assembly clone search with unseen architectures和libraries的首个研究。我们提议通过大规模预训练的自然语言模型来 incorporate human common knowledge into current learning-based approaches for assembly clone search。这种方法可以帮助解决现有方法的局限性，因为它可以带来更广泛的人类专家知识。我们进一步解决序列限制问题，提出一种强化学习代理来 remov redundant和无用的 токен。与新的Variational Information Bottleneck学习策略相结合，我们的提案可以减少架构和优化设置的可能指标，以提高对未看过的架构的总体化。我们在模拟未看过架构做clone search的场景下进行了实验，结果显示了我们的方法的效果，比起当前的状态对策。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques"><a href="#Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques" class="headerlink" title="Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques"></a>Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10617">http://arxiv.org/abs/2307.10617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anusuya Baby Hari Krishnan</li>
<li>for: 本研究旨在提出一种机器学习模型，用于 indentifying deceptive reviews，尤其是针对餐厅评论。</li>
<li>methods: 本研究使用n-gram模型和max features技术来提取特征，并 coupling five distinct machine learning classification algorithms进行分类。</li>
<li>results: 实验结果显示，通过使用passive aggressive类型的分类器，可以具有最高的精度和鲁棒性，并且在文本分类和假评论检测方面表现出色。<details>
<summary>Abstract</summary>
In the contemporary digital landscape, online reviews have become an indispensable tool for promoting products and services across various businesses. Marketers, advertisers, and online businesses have found incentives to create deceptive positive reviews for their products and negative reviews for their competitors' offerings. As a result, the writing of deceptive reviews has become an unavoidable practice for businesses seeking to promote themselves or undermine their rivals. Detecting such deceptive reviews has become an intense and ongoing area of research. This research paper proposes a machine learning model to identify deceptive reviews, with a particular focus on restaurants. This study delves into the performance of numerous experiments conducted on a dataset of restaurant reviews known as the Deceptive Opinion Spam Corpus. To accomplish this, an n-gram model and max features are developed to effectively identify deceptive content, particularly focusing on fake reviews. A benchmark study is undertaken to explore the performance of two different feature extraction techniques, which are then coupled with five distinct machine learning classification algorithms. The experimental results reveal that the passive aggressive classifier stands out among the various algorithms, showcasing the highest accuracy not only in text classification but also in identifying fake reviews. Moreover, the research delves into data augmentation and implements various deep learning techniques to further enhance the process of detecting deceptive reviews. The findings shed light on the efficacy of the proposed machine learning approach and offer valuable insights into dealing with deceptive reviews in the realm of online businesses.
</details>
<details>
<summary>摘要</summary>
在当今数字化时代，在线评论已成为不同业务的不可或缺的工具。广告商、市场推广人员和在线业务在创建假评论以便自己的产品或者降低竞争对手的产品时，假评论的写作已成为不可避免的做法。因此，检测假评论已成为一项激烈和持续的研究领域。本研究论文提出一种机器学习模型，用于识别假评论，尤其是针对餐厅评论。本研究通过对知名的餐厅评论数据集——假评论敏感数据集进行了多个实验。为了实现这一目标，我们开发了ngram模型和最佳特征，以便有效地识别假内容，特别是假评论。我们进行了benchmark研究，以 explore两种不同的特征提取技术的相对性，然后与五种不同的机器学习分类算法结合。实验结果表明，通过的情感攻击分类器在文本分类和识别假评论方面具有最高精度。此外，我们还对数据增强和深度学习技术进行了应用，以进一步提高假评论检测的过程。研究成果照明了我们提出的机器学习方法的效果，并对在线业务中的假评论处理提供了有价值的思路。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges"><a href="#Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges" class="headerlink" title="Heterogeneous Federated Learning: State-of-the-art and Research Challenges"></a>Heterogeneous Federated Learning: State-of-the-art and Research Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10616">http://arxiv.org/abs/2307.10616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marswhu/hfl_survey">https://github.com/marswhu/hfl_survey</a></li>
<li>paper_authors: Mang Ye, Xiuwen Fang, Bo Du, Pong C. Yuen, Dacheng Tao</li>
<li>for: 本研究主要针对 Federated Learning (FL) 在大规模实际应用中的挑战，特别是在客户端数据、模型、网络环境和硬件设备等方面存在多样化的情况下进行研究。</li>
<li>methods: 本文首先总结了 Heterogeneous Federated Learning (HFL) 中的多种研究挑战，包括统计学上的多样性、模型上的多样性、通信上的多样性、设备上的多样性以及其他挑战。此外，本文还对现有的 HFL 方法进行了系统的审视，并提出了一种新的分类方法，即根据 HFL 过程的不同级别分为数据级、模型级和服务级。</li>
<li>results: 本文对 HFL 的研究进行了深入的分析，并提出了一些关键和前景的未来研究方向。这些方向可能会促进 HFL 领域的进一步发展。此外，本文还提供了一个 periodic 更新的 HFL 集成资源，可以在 <a target="_blank" rel="noopener" href="https://github.com/marswhu/HFL_Survey">https://github.com/marswhu/HFL_Survey</a> 上获取。<details>
<summary>Abstract</summary>
Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level. Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field. A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.
</details>
<details>
<summary>摘要</summary>
Federated learning (FL) 已经引起了越来越多的关注，因为它在大规模工业应用中具有潜在的潜力。现有的联邦学习研究主要集中在模型同质Setting下进行。然而，实际的联邦学习通常面临参与客户端数据分布、模型架构、网络环境和硬件设备之间的不同性。这种不同性的联邦学习（HFL）是更加复杂和多样化的，因此需要一个系统的检视和分析。在这个检视中，我们首先总结了HFL的多种研究挑战，从五个方面出发：统计不同性、模型不同性、通信不同性、设备不同性以及附加挑战。此外，我们还评论了现有的HFL方法，并提出了一种新的分类方法，从三个不同的水平进行分类：数据水平、模型水平和服务器水平。最后，我们讨论了一些重要和有前途的未来研究方向，这些方向可能会促进这个领域的进一步发展。关于HFL的更新集可以在https://github.com/marswhu/HFL_Survey中找到。
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Solutions-in-AI-for-All"><a href="#Challenges-and-Solutions-in-AI-for-All" class="headerlink" title="Challenges and Solutions in AI for All"></a>Challenges and Solutions in AI for All</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10600">http://arxiv.org/abs/2307.10600</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Rifat Ara Shams, Didar Zowghi, Muneera Bano</li>
<li>for: 本研究旨在探讨人工智能（AI）设计中的多样性和包容性（D&amp;I）原则，以提高系统的公平、信任和透明度。</li>
<li>methods: 本研究采用系统性回访方法，检索2017年至2022年间发表的48篇研究论文。通过开 coding，找到了55个D&amp;I在AI中的挑战和33个解决方案，以及24个优化D&amp;I实践使用AI的挑战和23个解决方案。</li>
<li>results: 本研究提供了对D&amp;I在AI中的问题更深入的理解，为研究人员和实践者们提供了 referencing的知识，以便将这些原则integrated into future AI系统。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI)'s pervasive presence and variety necessitate diversity and inclusivity (D&I) principles in its design for fairness, trust, and transparency. Yet, these considerations are often overlooked, leading to issues of bias, discrimination, and perceived untrustworthiness. In response, we conducted a Systematic Review to unearth challenges and solutions relating to D&I in AI. Our rigorous search yielded 48 research articles published between 2017 and 2022. Open coding of these papers revealed 55 unique challenges and 33 solutions for D&I in AI, as well as 24 unique challenges and 23 solutions for enhancing such practices using AI. This study, by offering a deeper understanding of these issues, will enlighten researchers and practitioners seeking to integrate these principles into future AI systems.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的普遍存在和多样性需要多样性和包容性（D&I）的设计，以确保公平、信任和透明度。然而，这些考虑因常被忽略，导致了偏见、歧视和被视为不可信的问题。为了应对这些问题，我们进行了系统性审查，探索了AI中D&I的挑战和解决方案。我们的严格搜索结果为48篇发表于2017-2022年的研究论文，经开放编码 revelaed 55个唯一的挑战和33个解决方案，以及24个唯一的挑战和23个解决方案，用于提高这些原则的实践。这项研究，通过深入理解这些问题，将为研究人员和实践者提供指导，以便将这些原则integrated into future AI系统。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Structure-for-Optimal-Multi-Agent-Bayesian-Decentralized-Estimation"><a href="#Exploiting-Structure-for-Optimal-Multi-Agent-Bayesian-Decentralized-Estimation" class="headerlink" title="Exploiting Structure for Optimal Multi-Agent Bayesian Decentralized Estimation"></a>Exploiting Structure for Optimal Multi-Agent Bayesian Decentralized Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10594">http://arxiv.org/abs/2307.10594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Funk, Ofer Dagan, Benjamin Noack, Nisar R. Ahmed</li>
<li>for: 这篇论文是关于泛矩阵中的异谱传播问题，具体来说是针对在多 Agent 分布式整合问题中的 rumor propagation 现象进行研究。</li>
<li>methods: 这篇论文提出了一种基于多因子权重的非正式 CI 算法，以及一种基于概率独立结构的优化方案，用于解决这个问题。</li>
<li>results: 对于一个简单的问题，这两种方法都能够 converge 到同一解，而且在一个大规模的目标跟踪 simulate 中，非正式 CI 算法能够实现更紧张的 bound 和更准确的估计。<details>
<summary>Abstract</summary>
A key challenge in Bayesian decentralized data fusion is the `rumor propagation' or `double counting' phenomenon, where previously sent data circulates back to its sender. It is often addressed by approximate methods like covariance intersection (CI) which takes a weighted average of the estimates to compute the bound. The problem is that this bound is not tight, i.e. the estimate is often over-conservative. In this paper, we show that by exploiting the probabilistic independence structure in multi-agent decentralized fusion problems a tighter bound can be found using (i) an expansion to the CI algorithm that uses multiple (non-monolithic) weighting factors instead of one (monolithic) factor in the original CI and (ii) a general optimization scheme that is able to compute optimal bounds and fully exploit an arbitrary dependency structure. We compare our methods and show that on a simple problem, they converge to the same solution. We then test our new non-monolithic CI algorithm on a large-scale target tracking simulation and show that it achieves a tighter bound and a more accurate estimate compared to the original monolithic CI.
</details>
<details>
<summary>摘要</summary>
“统计分散式数据融合中的一个主要挑战是传闻传播（double counting）现象，其中先前发送的数据会再次回到发送者。通常这个问题会用近似方法如协变积分（CI）来解决，这个方法会将估计值加权平均以计算范围。然而，这个范围通常是不紧的，即估计通常是过保守的。在这篇论文中，我们显示了在多代理分散式数据融合问题中，通过利用多元独立的概率结构，可以使用多个不同的加权因子而不是单一的固定加权因子，从而获得更紧的范围。我们比较了我们的方法和原始单一CI方法，并证明了在一个简单问题上，它们均 converge 到相同的解决方案。然后，我们将我们的新非单一CI算法应用到一个大规模目标追踪 simulator 中，并证明了它在获得更紧的范围和更准的估计方面比原始单一CI更好。”
</details></li>
</ul>
<hr>
<h2 id="Boundary-State-Generation-for-Testing-and-Improvement-of-Autonomous-Driving-Systems"><a href="#Boundary-State-Generation-for-Testing-and-Improvement-of-Autonomous-Driving-Systems" class="headerlink" title="Boundary State Generation for Testing and Improvement of Autonomous Driving Systems"></a>Boundary State Generation for Testing and Improvement of Autonomous Driving Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10590">http://arxiv.org/abs/2307.10590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Biagiola, Paolo Tonella</li>
<li>for: This paper aims to improve the dependability of autonomous driving systems (ADSs) by presenting a novel test generator called GenBo.</li>
<li>methods: GenBo mutates the driving conditions of the ego vehicle (position, velocity, and orientation) collected in a failure-free environment instance to generate challenging driving conditions at the behavior boundary, where the model starts to misbehave.</li>
<li>results: The retrained model using GenBo has up to 16% higher success rate on a separate set of evaluation tracks compared to the original DNN model.Here’s the simplified Chinese text:</li>
<li>for: 本文提出了一种新的自动驾驶系统测试生成器（GenBo），以提高自动驾驶系统的可靠性。</li>
<li>methods: GenBo通过对ego车辆的驾驶条件（位置、速度和方向）进行突变，在同一个环境实例中生成了难题的驾驶条件，以便在模型开始出问题的边界上进行测试。</li>
<li>results: 使用GenBo重新训练模型，模型在分离的评估车道上的成功率提高了16%。<details>
<summary>Abstract</summary>
Recent advances in Deep Neural Networks (DNNs) and sensor technologies are enabling autonomous driving systems (ADSs) with an ever-increasing level of autonomy. However, assessing their dependability remains a critical concern. State-of-the-art ADS testing approaches modify the controllable attributes of a simulated driving environment until the ADS misbehaves. Such approaches have two main drawbacks: (1) modifications to the simulated environment might not be easily transferable to the in-field test setting (e.g., changing the road shape); (2) environment instances in which the ADS is successful are discarded, despite the possibility that they could contain hidden driving conditions in which the ADS may misbehave.   In this paper, we present GenBo (GENerator of BOundary state pairs), a novel test generator for ADS testing. GenBo mutates the driving conditions of the ego vehicle (position, velocity and orientation), collected in a failure-free environment instance, and efficiently generates challenging driving conditions at the behavior boundary (i.e., where the model starts to misbehave) in the same environment. We use such boundary conditions to augment the initial training dataset and retrain the DNN model under test. Our evaluation results show that the retrained model has up to 16 higher success rate on a separate set of evaluation tracks with respect to the original DNN model.
</details>
<details>
<summary>摘要</summary>
In this paper, we present GenBo (GENerator of BOundary state pairs), a novel test generator for ADS testing. GenBo mutates the driving conditions of the ego vehicle (position, velocity, and orientation), collected in a failure-free environment instance, and efficiently generates challenging driving conditions at the behavior boundary (where the model starts to misbehave) in the same environment. We use these boundary conditions to augment the initial training dataset and retrain the DNN model under test. Our evaluation results show that the retrained model has up to 16% higher success rate on a separate set of evaluation tracks compared to the original DNN model.
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques"><a href="#Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques" class="headerlink" title="Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques"></a>Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10588">http://arxiv.org/abs/2307.10588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanif Tayarani, Trisha V. Ramadoss, Vaishnavi Karanam, Gil Tal, Christopher Nitta<br>for:This paper aims to improve the forecasting of battery electric vehicle (BEV) charging events, which is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively.methods:The paper develops a novel Micro Clustering Deep Neural Network (MCDNN) algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events.results:The proposed MCDNN outperforms benchmark approaches, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models, in predicting the charging events.<details>
<summary>Abstract</summary>
Energy systems, climate change, and public health are among the primary reasons for moving toward electrification in transportation. Transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. While great for climate and pollution goals, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a total of 1570167 vehicle miles traveled. The numerical findings revealed that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.
</details>
<details>
<summary>摘要</summary>
transportation electrification 是为了降低排放和提高公共健康而努力往前，其中能源系统、气候变化和公共健康都是主要原因。随着全球各地推广电动汽车，许多汽车制造商即将停止生产内燃机械汽车，而且BEV采购率在加利福尼亚州正在增长，主要是由于气候变化和空气污染的问题。然而，不当管理BEV充电可能会导致充电基础设施不足和停电。这种研究开发了一种 Micro Clustering Deep Neural Network (MCDNN)，这是一种人工神经网络算法，可以很好地学习BEV行驶和充电数据，以预测BEV充电事件，这些信息对电力聚集器和供电公司来说非常重要。MCDNN使用了加利福尼亚州2015-2020年间132台BEV的行驶记录，涵盖5种BEV型号，共计1570167公里。数值发现表明，提案的MCDNN比 benchmark方法更有效，例如支持向量机、最近邻居、决策树和其他神经网络模型在预测充电事件方面更高。
</details></li>
</ul>
<hr>
<h2 id="Ethosight-A-Reasoning-Guided-Iterative-Learning-System-for-Nuanced-Perception-based-on-Joint-Embedding-Contextual-Label-Affinity"><a href="#Ethosight-A-Reasoning-Guided-Iterative-Learning-System-for-Nuanced-Perception-based-on-Joint-Embedding-Contextual-Label-Affinity" class="headerlink" title="Ethosight: A Reasoning-Guided Iterative Learning System for Nuanced Perception based on Joint-Embedding &amp; Contextual Label Affinity"></a>Ethosight: A Reasoning-Guided Iterative Learning System for Nuanced Perception based on Joint-Embedding &amp; Contextual Label Affinity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10577">http://arxiv.org/abs/2307.10577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugo Latapie, Shan Yu, Patrick Hammer, Kristinn R. Thorisson, Vahagn Petrosyan, Brandon Kynoch, Alind Khare, Payman Behnam, Alexey Tumanov, Aksheit Saxena, Anish Aralikatti, Hanning Chen, Mohsen Imani, Mike Archbold, Tangrui Li, Pei Wang, Justin Hart</li>
<li>for: 本研究旨在提出一种flexible和适应的零shot视频分析系统，以解决传统计算机视觉模型在实际应用中的缺点，如高False Positive和False Negative率，以及新场景投入重新训练等问题。</li>
<li>methods: 本研究使用用户定义的视频分析、自然语言或关键词指定，并利用 joint embedding 模型和基于 ontologies 如 WordNet 和 ConceptNet 的理解机制。 Ethosight 在低成本的边缘设备上运行，支持 runtime 适应，从而提供一种新的连续学习方法，不受恶化学习的限制。</li>
<li>results: 本研究提供了 Ethosight 的实际效果的证明，在多种复杂的应用场景中表现出色，同时也采取了全部源代码和数据集的发布，以便重新复制和进一步推动研究和商业领域的创新。<details>
<summary>Abstract</summary>
Traditional computer vision models often necessitate extensive data acquisition, annotation, and validation. These models frequently struggle in real-world applications, resulting in high false positive and negative rates, and exhibit poor adaptability to new scenarios, often requiring costly retraining. To address these issues, we present Ethosight, a flexible and adaptable zero-shot video analytics system. Ethosight begins from a clean slate based on user-defined video analytics, specified through natural language or keywords, and leverages joint embedding models and reasoning mechanisms informed by ontologies such as WordNet and ConceptNet. Ethosight operates effectively on low-cost edge devices and supports enhanced runtime adaptation, thereby offering a new approach to continuous learning without catastrophic forgetting. We provide empirical validation of Ethosight's promising effectiveness across diverse and complex use cases, while highlighting areas for further improvement. A significant contribution of this work is the release of all source code and datasets to enable full reproducibility and to foster further innovation in both the research and commercial domains.
</details>
<details>
<summary>摘要</summary>
传统的计算机视觉模型经常需要大量的数据收集、注释和验证，这些模型经常在实际应用中遇到高的假阳性和假阴性率，并且具有贫富新场景适应性，需要高成本的重新训练。为解决这些问题，我们介绍了Ethosight，一个灵活和适应的零shot视频分析系统。Ethosight从用户定义的视频分析开始，通过自然语言或关键词指定，并利用联合嵌入模型和基于 ontology 的理解机制，例如 WordNet 和 ConceptNet。Ethosight在低成本的边缘设备上运行，支持增强的运行时适应，因此提供了一种新的连续学习方法，不会导致恰等忘记。我们提供了多种用例的实验 validate Ethosight 的承诺效果，同时还提出了进一步改进的方向。这项工作的一个重要贡献是发布所有源代码和数据集，以便完全重现和促进研究和商业领域的进一步创新。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Federated-Learning-Convergence-with-Prototype-Regularization"><a href="#Boosting-Federated-Learning-Convergence-with-Prototype-Regularization" class="headerlink" title="Boosting Federated Learning Convergence with Prototype Regularization"></a>Boosting Federated Learning Convergence with Prototype Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10575">http://arxiv.org/abs/2307.10575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Qiao, Huy Q. Le, Choong Seon Hong</li>
<li>for: 这篇论文旨在提高 Federated Learning（FL）中 Client 间的资料分布不均势，以提高模型表现。</li>
<li>methods: 本文提出了一种基于 Prototype 的调整策略，通过服务器将分布在 Client 上的本地 Prototype 聚合为全域 Prototype，然后将其返回到各 Client，以帮助它们本地训练。</li>
<li>results: 在 MNIST 和 Fashion-MNIST 上进行的实验结果显示，该提案可以与最受欢迎的基于 FedAvg 的基线比较，在不均势的设定下实现更快的整合速度和3.3% 和8.9% 的平均测试准确率提高。<details>
<summary>Abstract</summary>
As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
为了应对分布式机器学习技术中的客户端数据不均衡问题，本文提出了一种原型基于准则化策略。具体来说，这种准则化策略包括客户端分布式的本地原型被服务器聚合成global原型，然后将global原型返回给每个客户端，以帮助每个客户端本地进行训练。我们在MNIST和Fashion-MNIST上进行了实验，结果显示，我们的方案可以与最流行的基准FedAvg相比，提高测试准确率3.3%和8.9%。此外，我们的方法在不均衡情况下具有快速的收敛速率。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Control-of-Resource-Flow-to-Optimize-Construction-Work-and-Cash-Flow-via-Online-Deep-Reinforcement-Learning"><a href="#Adaptive-Control-of-Resource-Flow-to-Optimize-Construction-Work-and-Cash-Flow-via-Online-Deep-Reinforcement-Learning" class="headerlink" title="Adaptive Control of Resource Flow to Optimize Construction Work and Cash Flow via Online Deep Reinforcement Learning"></a>Adaptive Control of Resource Flow to Optimize Construction Work and Cash Flow via Online Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10574">http://arxiv.org/abs/2307.10574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Jiang, Xin Li, Jia-Rui Lin, Ming Liu, Zhiliang Ma</li>
<li>for: 本研究旨在提出一种模型和方法，以优化建筑项目中工作和资金流的控制，以适应复杂的建筑项目环境下的不确定性和多样性。</li>
<li>methods: 本研究使用了深度强化学习（DRL）技术，实现了继续适应控制劳务和材料流动，从而优化工作和资金流。同时，为了有效地训练DRL，还开发了基于分割事件 simulations的模拟器。</li>
<li>results: 实验结果表明，我们的方法在多种项目和外部环境下表现出色，并且在不同的项目和环境下具有remarkable的可靠性和灵活性。此外，杂合DRL和经验法则的代理人得到了最佳结果。本研究的成果可能对建筑项目管理中的适应控制和优化做出重要贡献。<details>
<summary>Abstract</summary>
Due to complexity and dynamics of construction work, resource, and cash flows, poor management of them usually leads to time and cost overruns, bankruptcy, even project failure. Existing approaches in construction failed to achieve optimal control of resource flow in a dynamic environment with uncertainty. Therefore, this paper introducess a model and method to adaptive control the resource flows to optimize the work and cash flows of construction projects. First, a mathematical model based on a partially observable Markov decision process is established to formulate the complex interactions of construction work, resource, and cash flows as well as uncertainty and variability of diverse influence factors. Meanwhile, to efficiently find the optimal solutions, a deep reinforcement learning (DRL) based method is introduced to realize the continuous adaptive optimal control of labor and material flows, thereby optimizing the work and cash flows. To assist the training process of DRL, a simulator based on discrete event simulation is also developed to mimic the dynamic features and external environments of a project. Experiments in simulated scenarios illustrate that our method outperforms the vanilla empirical method and genetic algorithm, possesses remarkable capability in diverse projects and external environments, and a hybrid agent of DRL and empirical method leads to the best result. This paper contributes to adaptive control and optimization of coupled work, resource, and cash flows, and may serve as a step stone for adopting DRL technology in construction project management.
</details>
<details>
<summary>摘要</summary>
First, a mathematical model based on a partially observable Markov decision process is established to formulate the complex interactions of construction work, resources, and cash flows, as well as uncertainty and variability of diverse influence factors. To efficiently find the optimal solutions, a deep reinforcement learning (DRL) based method is introduced to realize continuous adaptive optimal control of labor and material flows, thereby optimizing the work and cash flows.To assist the training process of DRL, a simulator based on discrete event simulation is also developed to mimic the dynamic features and external environments of a project. Experiments in simulated scenarios show that our method outperforms the vanilla empirical method and genetic algorithm, possesses remarkable capability in diverse projects and external environments, and a hybrid agent of DRL and empirical method leads to the best result. This paper contributes to adaptive control and optimization of coupled work, resources, and cash flows, and may serve as a stepping stone for adopting DRL technology in construction project management.
</details></li>
</ul>
<hr>
<h2 id="Invalid-Logic-Equivalent-Gains-The-Bizarreness-of-Reasoning-in-Language-Model-Prompting"><a href="#Invalid-Logic-Equivalent-Gains-The-Bizarreness-of-Reasoning-in-Language-Model-Prompting" class="headerlink" title="Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting"></a>Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10573">http://arxiv.org/abs/2307.10573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rylan Schaeffer, Kateryna Pistunova, Samar Khanna, Sarthak Consul, Sanmi Koyejo</li>
<li>for: 这篇论文旨在探讨语义模型如何通过逻辑无效的Chain-of-Thought（CoT）提问方法来提高性能。</li>
<li>methods: 研究人员使用了无效CoT提问方法和编辑CoT提问方法来测试语义模型的性能。</li>
<li>results: 研究人员发现，无效CoT提问方法可以提高语义模型的性能，并且这种提高效果与有效提问方法相当。此外，研究人员还发现了一些先前的CoT提问方法中的逻辑错误。这表示，以外于逻辑正确的因素也可能导致性能提高。<details>
<summary>Abstract</summary>
Language models can be prompted to reason through problems in a manner that significantly improves performance. However, \textit{why} such prompting improves performance is unclear. Recent work showed that using logically \textit{invalid} Chain-of-Thought (CoT) prompting improves performance almost as much as logically \textit{valid} CoT prompting, and that editing CoT prompts to replace problem-specific information with abstract information or out-of-distribution information typically doesn't harm performance. Critics have responded that these findings are based on too few and too easily solved tasks to draw meaningful conclusions. To resolve this dispute, we test whether logically invalid CoT prompts offer the same level of performance gains as logically valid prompts on the hardest tasks in the BIG-Bench benchmark, termed BIG-Bench Hard (BBH). We find that the logically \textit{invalid} reasoning prompts do indeed achieve similar performance gains on BBH tasks as logically valid reasoning prompts. We also discover that some CoT prompts used by previous works contain logical errors. This suggests that covariates beyond logically valid reasoning are responsible for performance improvements.
</details>
<details>
<summary>摘要</summary>
语言模型可以通过 проблеme 的推理来提高性能，但是为什么这种提高性能是如此明显不清楚。  latest work 表明，使用无效的链条（Chain-of-Thought，CoT）提问可以几乎达到有效的 CoT 提问的性能水平，并且编辑 CoT 提问以替换问题特定信息或者out-of-distribution信息通常不会增加性能的风险。  however， some critics 认为，这些发现是基于 too few 和 too easily solved tasks 来 drew meaningful conclusions。  To resolve this dispute, we test whether logically invalid CoT prompts offer the same level of performance gains as logically valid prompts on the hardest tasks in the BIG-Bench benchmark, termed BIG-Bench Hard (BBH). We find that the logically invalid reasoning prompts do indeed achieve similar performance gains on BBH tasks as logically valid reasoning prompts。 We also discover that some CoT prompts used by previous works contain logical errors。 This suggests that covariates beyond logically valid reasoning are responsible for performance improvements。
</details></li>
</ul>
<hr>
<h2 id="Deceptive-Alignment-Monitoring"><a href="#Deceptive-Alignment-Monitoring" class="headerlink" title="Deceptive Alignment Monitoring"></a>Deceptive Alignment Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10569">http://arxiv.org/abs/2307.10569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: 防止大数据学模型的伪装Alignment威胁</li>
<li>methods: 多种多样的机器学习子领域的研究</li>
<li>results: 提出了新的研究机遇和挑战，促进了对伪装Alignment的监测和防御<details>
<summary>Abstract</summary>
As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety & Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
</details>
<details>
<summary>摘要</summary>
large machine learning models的能力不断增强，模型自身的自主权也在扩大，这使得一种新的敌人隐约出现：模型自身的偏见。这种偏见被称为“欺骗吧”（deceptive alignment）在AI安全与对齐社区中，我们称这个新方向为“欺骗吧监测”。在这篇文章中，我们识别出了未来几年将成为关键和互相关联的多种机器学习子领域的发展趋势，并 argue That these fields present both long-term challenges and new research opportunities. Finally, we advocate for greater involvement by the adversarial machine learning community in these emerging directions.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation"><a href="#FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation" class="headerlink" title="FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation"></a>FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10563">http://arxiv.org/abs/2307.10563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Pai, Andres Carranza, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: 这篇论文的主要目的是提高深度神经网络中的不可预测性攻击防护，以提高模型的灵活性和可靠性。</li>
<li>methods: 这篇论文提出了一个新的几率和几何方法，用于无监督机器学习中的不可预测性攻击探测。这个方法基于几率分布预测，从激活空间中提取高维模式，以提高模型的不可预测性和可靠性。</li>
<li>results: 这篇论文的结果显示，FACADE方法可以实现高度精确地探测深度神经网络中的不可预测性攻击，并且可以实现模型的稳定性和可靠性。此外，FACADE方法还可以应用于实际应用场景中，以提高模型的运行效率和可靠性。<details>
<summary>Abstract</summary>
We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.
</details>
<details>
<summary>摘要</summary>
我们介绍FACADE，一个新的机会概率和几何框架，用于无超级机器学习模型中的机会性异常检测。它的主要目标是提高防火墙攻击的理解和缓和。FACADE通过生成逻辑统计分布，以提供异常检测中几何特性的关键洞察，从而实现高效地探测和抵御防火墙攻击。我们的方法可以提高模型的抗性、增强可扩展的模型监控，并且在实际应用中展现了有前途的应用。
</details></li>
</ul>
<hr>
<h2 id="Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning"><a href="#Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning" class="headerlink" title="Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning"></a>Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10559">http://arxiv.org/abs/2307.10559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymlasu/para-atm-collection">https://github.com/ymlasu/para-atm-collection</a></li>
<li>paper_authors: Yutian Pang, Jueming Hu, Christopher S. Lieber, Nancy J. Cooke, Yongming Liu</li>
<li>for: 这个论文的目的是预测空交通控制员（ATCo）的工作负担，以避免过载和保证操作安全性和空域使用效率。</li>
<li>methods: 这个论文使用了人工智能技术，特别是图学深度学习和协形预测，以分析空交通数据和工作负担标签。</li>
<li>results: 研究结果表明， besides 交通密度特征，交通冲突特征也对工作负担预测做出了贡献（即最小水平&#x2F;垂直距离）。 direct learning from 空间时间图形的图 neural network 可以实现更高的预测精度，比手工设计的交通复杂度特征。 conformal prediction 是一种有价值的工具，可以进一步提高模型预测精度，生成一个范围内的预测Label。<details>
<summary>Abstract</summary>
Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to identify the ATCo workload levels. The number of aircraft under the controller's control varies both spatially and temporally, resulting in dynamically evolving graphs. The experiment results suggest that (a) besides the traffic density feature, the traffic conflict feature contributes to the workload prediction capabilities (i.e., minimum horizontal/vertical separation distance); (b) directly learning from the spatiotemporal graph layout of airspace with graph neural network can achieve higher prediction accuracy, compare to hand-crafted traffic complexity features; (c) conformal prediction is a valuable tool to further boost model prediction accuracy, resulting a range of predicted workload labels. The code used is available at \href{https://github.com/ymlasu/para-atm-collection/blob/master/air-traffic-prediction/ATC-Workload-Prediction/}{$\mathsf{Link}$}.
</details>
<details>
<summary>摘要</summary>
空交通控制（ATC）是一个安全关键的服务系统，需要地面空交通控制员（ATCo）不断注意力，以维护每天的航空业务。ATCo的工作负担可能会对运行安全和空间使用产生负面影响。为了避免过载和确保ATCo的工作负担水平为可接受，需要准确预测ATCo的工作负担。在这篇论文中，我们首先进行了关于ATCo工作负担的研究，主要从空交通的角度进行评估。然后，我们简要介绍了使用退休ATCo进行人类在Loop（HITL） simulations的设置，其中获取了空交通数据和工作负担标签。在三个 Phoenixtapproach 场景下，人类ATCo被请求进行自我评估工作负担水平（从低1到高7）。我们对数据进行了初步分析。然后，我们提出了基于图的深度学习框架，并使用协形预测来预测ATCo工作负担水平。由于空交通中的飞机数量在控制员的控制范围内变化 both spatially和 temporally， resulting in dynamically evolving graphs。实验结果表明，（a） besides traffic density feature， traffic conflict feature也对工作负担预测做出了贡献（i.e., minimum horizontal/vertical separation distance）；（b） directly learning from spatiotemporal graph layout of airspace with graph neural network can achieve higher prediction accuracy， compare to hand-crafted traffic complexity features；（c） conformal prediction is a valuable tool to further boost model prediction accuracy， resulting a range of predicted workload labels。代码使用 $\mathsf{Link}$。
</details></li>
</ul>
<hr>
<h2 id="EMQ-Evolving-Training-free-Proxies-for-Automated-Mixed-Precision-Quantization"><a href="#EMQ-Evolving-Training-free-Proxies-for-Automated-Mixed-Precision-Quantization" class="headerlink" title="EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization"></a>EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10554">http://arxiv.org/abs/2307.10554</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lilujunai/emq-series">https://github.com/lilujunai/emq-series</a></li>
<li>paper_authors: Peijie Dong, Lujun Li, Zimian Wei, Xin Niu, Zhiliang Tian, Hengyue Pan</li>
<li>for: 这个论文是为了提出一种自动生成混合精度量化（MQ）代理的框架，以提高MQ的准确率和效率。</li>
<li>methods: 该论文使用了一种自动搜索方法来找到最佳的MQ代理，并提出了一种多样性推动选择策略和兼容性检测协议来避免快速落后。</li>
<li>results: 该论文的实验结果表明，使用该自动生成MQ代理框架可以在ImageNet上 достичь比州态艺术方法更高的性能，并且具有显著更高的效率。<details>
<summary>Abstract</summary>
Mixed-Precision Quantization~(MQ) can achieve a competitive accuracy-complexity trade-off for models. Conventional training-based search methods require time-consuming candidate training to search optimized per-layer bit-width configurations in MQ. Recently, some training-free approaches have presented various MQ proxies and significantly improve search efficiency. However, the correlation between these proxies and quantization accuracy is poorly understood. To address the gap, we first build the MQ-Bench-101, which involves different bit configurations and quantization results. Then, we observe that the existing training-free proxies perform weak correlations on the MQ-Bench-101. To efficiently seek superior proxies, we develop an automatic search of proxies framework for MQ via evolving algorithms. In particular, we devise an elaborate search space involving the existing proxies and perform an evolution search to discover the best correlated MQ proxy. We proposed a diversity-prompting selection strategy and compatibility screening protocol to avoid premature convergence and improve search efficiency. In this way, our Evolving proxies for Mixed-precision Quantization~(EMQ) framework allows the auto-generation of proxies without heavy tuning and expert knowledge. Extensive experiments on ImageNet with various ResNet and MobileNet families demonstrate that our EMQ obtains superior performance than state-of-the-art mixed-precision methods at a significantly reduced cost. The code will be released.
</details>
<details>
<summary>摘要</summary>
含杂精度量化~(MQ)可以实现模型的竞争性精度复杂度质量规则。传统的训练基本方法需要耗时的候选人训练来搜索MQ中的优化每层比特宽配置。最近，一些无需训练的方法已经提出了多种MQ代理，并显著提高了搜索效率。然而，这些代理与量化精度之间的相关性不够了解。为了解决这个差距，我们首先建立了MQ-Bench-101，它包括不同的比特配置和量化结果。然后，我们发现现有的无需训练代理在MQ-Bench-101上表现出弱相关性。为了有效寻找优秀代理，我们开发了一个自动搜索代理框架 дляMQ。在特定的搜索空间中，我们采用了现有的代理和演化算法来找到最佳相关的MQ代理。我们提出了一种多样性激发选择策略和兼容性检查协议，以避免早期 converges和提高搜索效率。因此，我们的演化代理 для混合精度量化~(EMQ)框架可以自动生成代理，无需重重的调整和专家知识。我们的实验结果表明，在ImageNet上使用不同的ResNet和MobileNet家族时，我们的EMQ可以在相对较少的成本下达到当前混合精度方法的优秀性能。代码将被发布。
</details></li>
</ul>
<hr>
<h2 id="PPN-Parallel-Pointer-based-Network-for-Key-Information-Extraction-with-Complex-Layouts"><a href="#PPN-Parallel-Pointer-based-Network-for-Key-Information-Extraction-with-Complex-Layouts" class="headerlink" title="PPN: Parallel Pointer-based Network for Key Information Extraction with Complex Layouts"></a>PPN: Parallel Pointer-based Network for Key Information Extraction with Complex Layouts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10551">http://arxiv.org/abs/2307.10551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Wei, Jie Yao, Jingyuan Zhang, Yangyang Kang, Fubang Zhao, Yating Zhang, Changlong Sun, Xin Jin, Xin Zhang</li>
<li>for: 本研究旨在提高键信息EXTRACTION（KIE）任务中的结构化值semantic entityextraction的精度和效率，以满足实际世界中的复杂场景需求。</li>
<li>methods: 本文提出了一种新的大规模人工标注数据集 named Complex Layout form for key information EXtraction (CLEX),以及一种基于并行指针的网络模型（PPN），可以在零shot和几shot情况下应用。PPN可以利用semantic entity之间的隐式做法来帮助提取，并且其并行提取机制可以同时和高效地提取多个结果。</li>
<li>results: 对CLEX数据集进行测试，PPN模型比现有状态的方法更高效，同时具有更快的推理速度。<details>
<summary>Abstract</summary>
Key Information Extraction (KIE) is a challenging multimodal task that aims to extract structured value semantic entities from visually rich documents. Although significant progress has been made, there are still two major challenges that need to be addressed. Firstly, the layout of existing datasets is relatively fixed and limited in the number of semantic entity categories, creating a significant gap between these datasets and the complex real-world scenarios. Secondly, existing methods follow a two-stage pipeline strategy, which may lead to the error propagation problem. Additionally, they are difficult to apply in situations where unseen semantic entity categories emerge. To address the first challenge, we propose a new large-scale human-annotated dataset named Complex Layout form for key information EXtraction (CLEX), which consists of 5,860 images with 1,162 semantic entity categories. To solve the second challenge, we introduce Parallel Pointer-based Network (PPN), an end-to-end model that can be applied in zero-shot and few-shot scenarios. PPN leverages the implicit clues between semantic entities to assist extracting, and its parallel extraction mechanism allows it to extract multiple results simultaneously and efficiently. Experiments on the CLEX dataset demonstrate that PPN outperforms existing state-of-the-art methods while also offering a much faster inference speed.
</details>
<details>
<summary>摘要</summary>
《键信息提取（KIE）是一项具有挑战性的多Modal任务，旨在从视觉丰富的文档中提取结构化的值含义实体。尽管已经取得了显著的进步，但还有两个主要挑战需要解决。首先，现有的数据集的布局相对固定，数量有限，与实际世界场景相比存在差距。其次，现有的方法采用两阶段管道策略，可能会导致错误堆叠问题。此外，它们难以应用于新的Semantic实体类型出现的情况。为了解决第一个挑战，我们提出了一个新的大规模人工标注数据集 named Complex Layout form for key information EXtraction（CLEX），该数据集包含5860张图像和1162个Semantic实体类型。为了解决第二个挑战，我们引入了并行指针网络（PPN），这是一种端到端模型，可以在零 shot和几 shot情况下应用。PPN利用Semantic实体之间的隐式做法来帮助提取，并且其并行提取机制使得它可以同时提取多个结果，高效地。实验表明，PPN在CLEX数据集上的性能明显超过了现有的状态态先进方法，同时也提供了 Much faster的推理速度。》
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Large-Language-Models-on-Blockchains"><a href="#Dynamic-Large-Language-Models-on-Blockchains" class="headerlink" title="Dynamic Large Language Models on Blockchains"></a>Dynamic Large Language Models on Blockchains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10549">http://arxiv.org/abs/2307.10549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Gong</li>
<li>for: 这篇论文是为了提出一种基于区块链的动态大语言模型训练和部署方法，以解决现有大语言模型训练和部署所需的高 computation performance 和静态性的问题。</li>
<li>methods: 本篇论文提出了一种基于区块链的方法，通过将大语言模型训练和部署 onto 区块链上，以获得高 computation performance 和分布式的优势。此外，本篇论文还提出了一种基于用户输入的动态训练方法，让模型可以不断地学习用户的反馈。</li>
<li>results: 本篇论文的结果显示，基于区块链的动态大语言模型可以在不同的用户输入下进行不断的学习和改善，并且可以提供更高的准确率和更好的使用者体验。此外，本篇论文的结果还显示出了基于区块链的大语言模型训练和部署的可行性和可调性。<details>
<summary>Abstract</summary>
Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamper-proof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also sheds a light on the next generation artificial intelligence systems.
</details>
<details>
<summary>摘要</summary>
<SYS> translate("Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamper-proof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also sheds a light on the next generation artificial intelligence systems.")</SYS>Here's the translation:训练和部署大语言模型需要巨量计算资源，因为模型包含数十亿参数，文本有千个token。另一个问题是大语言模型是静态的，它们在训练过程后固化。为解决这些问题，在这篇论文中，我们提议在区块链上训练和部署动态大语言模型，区块链具有高计算性能和分布在计算机网络上，可以创建一个不可篡改的交易记录，无需中介人员。动态大语言模型可以在用户输入后继续学习。我们的方法提供了一种新的大语言模型开发方式，也照亮了下一代人工智能系统。
</details></li>
</ul>
<hr>
<h2 id="TREA-Tree-Structure-Reasoning-Schema-for-Conversational-Recommendation"><a href="#TREA-Tree-Structure-Reasoning-Schema-for-Conversational-Recommendation" class="headerlink" title="TREA: Tree-Structure Reasoning Schema for Conversational Recommendation"></a>TREA: Tree-Structure Reasoning Schema for Conversational Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10543">http://arxiv.org/abs/2307.10543</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/windylee0822/trea">https://github.com/windylee0822/trea</a></li>
<li>paper_authors: Wendi Li, Wei Wei, Xiaoye Qu, Xian-Ling Mao, Ye Yuan, Wenfeng Xie, Dangyang Chen</li>
<li>for: 提高对话 Context 理解，提供更有针对性的 Response。</li>
<li>methods: 利用多层次可拓展 Tree 结构来解释 causality 关系，并充分利用历史对话来生成更有针对性的 Response。</li>
<li>results: 在两个公共 CRS 数据集上进行了广泛的实验，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Conversational recommender systems (CRS) aim to timely trace the dynamic interests of users through dialogues and generate relevant responses for item recommendations. Recently, various external knowledge bases (especially knowledge graphs) are incorporated into CRS to enhance the understanding of conversation contexts. However, recent reasoning-based models heavily rely on simplified structures such as linear structures or fixed-hierarchical structures for causality reasoning, hence they cannot fully figure out sophisticated relationships among utterances with external knowledge. To address this, we propose a novel Tree structure Reasoning schEmA named TREA. TREA constructs a multi-hierarchical scalable tree as the reasoning structure to clarify the causal relationships between mentioned entities, and fully utilizes historical conversations to generate more reasonable and suitable responses for recommended results. Extensive experiments on two public CRS datasets have demonstrated the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
对话式推荐系统 (CRS) 目的是在对话中追踪用户的动态喜好，并为其提供相应的回答以进行物品推荐。现在，许多外部知识库 (特别是知识 graphs) 被 incorporated into CRS 以增强对话上下文的理解。然而，最近的推理基于模型倾向于使用简单的结构，如线性结构或固定层次结构，以进行 causality 推理，因此无法完全理解对话中的复杂关系。为解决这个问题，我们提出了一个 noval Tree structure Reasoning schEmA 名为 TREA。TREA 使用多层次可扩展的树结构来clarify 受到提及的实体之间的 causal 关系，并充分利用历史对话来产生更合理和适合的回答来进行推荐。实验结果显示了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="The-Extractive-Abstractive-Axis-Measuring-Content-“Borrowing”-in-Generative-Language-Models"><a href="#The-Extractive-Abstractive-Axis-Measuring-Content-“Borrowing”-in-Generative-Language-Models" class="headerlink" title="The Extractive-Abstractive Axis: Measuring Content “Borrowing” in Generative Language Models"></a>The Extractive-Abstractive Axis: Measuring Content “Borrowing” in Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11779">http://arxiv.org/abs/2307.11779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nedelina Teneva</li>
<li>for: 本文提出了一个新的评估生成模型的方法，即EXTRACTIVE-ABSTRACTIVE轴。</li>
<li>methods: 本文使用了生成模型的抽象性和EXTRACTIVE-ABSTRACTIVE轴来评估生成模型的性能。</li>
<li>results: 本文提出了一些相关的指标、数据集和注解指南，以便评估生成模型的抽象性和EXTRACTIVE-ABSTRACTIVE轴。<details>
<summary>Abstract</summary>
Generative language models produce highly abstractive outputs by design, in contrast to extractive responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing & Attribution, we propose the the so-called Extractive-Abstractive axis for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality.
</details>
<details>
<summary>摘要</summary>
Generative语言模型会生成非常抽象的输出，与搜索引擎的EXTRACTIVE响应不同。由于这一特点和内容授权的后果，我们提出了所谓的EXTRACTIVE-ABSTRACTIVE轴，用于评估生成模型，并需要开发相应的指标、数据集和注释指南。我们只讨论文本 modalities。Note: "EXTRACTIVE-ABSTRACTIVE轴" is a made-up term, and "文本modalities" is the plural form of "文本modalities" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks"><a href="#Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks" class="headerlink" title="Fast Unsupervised Deep Outlier Model Selection with Hypernetworks"></a>Fast Unsupervised Deep Outlier Model Selection with Hypernetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10529">http://arxiv.org/abs/2307.10529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueying Ding, Yue Zhao, Leman Akoglu</li>
<li>for: 本文旨在提出一种有效的超参数（HP）调整方法，以提高无监督的异常点检测（OD）模型的性能。</li>
<li>methods: 本文提出了一种新的嵌入式神经网络（DOD）模型，并使用了一种名为HYPER的 hypernetwork（HN）来调整DOD模型的超参数。HYPER使用了一种新的 meta-学arning 技术，以使用历史的OD任务中的标签来训练一个代理验证函数。</li>
<li>results: 在35个OD任务上，HYPER achieved high performance against 8 baselines with significant efficiency gains.<details>
<summary>Abstract</summary>
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding to varying HPs), which offers significant speed-up. In addition, it employs meta-learning on historical OD tasks with labels to train a proxy validation function, likewise trained with our proposed HN efficiently. Extensive experiments on 35 OD tasks show that HYPER achieves high performance against 8 baselines with significant efficiency gains.
</details>
<details>
<summary>摘要</summary>
外异检测（OD）在许多应用中找到了广泛的应用，而且有许多技术的研究。深度神经网络基于的外异检测（DOD）在最近几年得到了广泛的关注，因为深度学习技术的进步。在这篇论文中，我们考虑了一个尚未得到充分研究的挑战：对于无监督的外异检测模型，有效地调整超参数（HP）。虽然之前的研究已经证明了外异检测模型对HP的敏感性，但是现代DOD模型的HP列表却非常长。我们提出了一种名为HYPER的方法，用于调整DOD模型。HYPER解决了两个基本挑战：无监督验证（由于缺乏异常数据）和高效地搜索HP/模型空间（由于HP的数量的增长）。我们的关键想法是设计和训练一个新的超网络（HN），将HP映射到外异检测模型的优化参数。然后，HYPER可以通过单个HN来动态生成多个DOD模型（对应于不同的HP），从而提供了显著的速度提升。此外，它还使用元学习来训练一个代理验证函数，这个函数通过我们提出的HN有效地训练。我们对35个OD任务进行了广泛的实验，结果显示HYPER可以高效地与8个基准模型进行比较，同时具有显著的效率优势。
</details></li>
</ul>
<hr>
<h2 id="Building-Socio-culturally-Inclusive-Stereotype-Resources-with-Community-Engagement"><a href="#Building-Socio-culturally-Inclusive-Stereotype-Resources-with-Community-Engagement" class="headerlink" title="Building Socio-culturally Inclusive Stereotype Resources with Community Engagement"></a>Building Socio-culturally Inclusive Stereotype Resources with Community Engagement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10514">http://arxiv.org/abs/2307.10514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunipa Dev, Jaya Goyal, Dinesh Tewari, Shachi Dave, Vinodkumar Prabhakaran</li>
<li>For: The paper aims to address the need for more culturally sensitive and representative evaluation resources for generative language models, specifically in the Indian context.* Methods: The authors use a community-engaged approach to build a resource of stereotypes unique to India, which increases the number of stereotypes known for the Indian context by over 1000.* Results: The authors demonstrate the utility and effectiveness of the expanded resource for evaluating language models and show that it can help identify harmful stereotypes that may be overlooked by traditional evaluation methods.Here are the three points in Simplified Chinese text:* For: 本文目的是为了增强语言模型评估资源的文化敏感性和代表性，特别是在印度上。* Methods: 作者采用社区参与的方法建立了印度特有的刻板印象资源，该资源在印度上增加了More than 1000的刻板印象。* Results: 作者表明了扩展资源的有用性和效果，并示出它可以帮助确定语言模型中的刻板印象，这些刻板印象可能被传统评估方法忽略。<details>
<summary>Abstract</summary>
With rapid development and deployment of generative language models in global settings, there is an urgent need to also scale our measurements of harm, not just in the number and types of harms covered, but also how well they account for local cultural contexts, including marginalized identities and the social biases experienced by them. Current evaluation paradigms are limited in their abilities to address this, as they are not representative of diverse, locally situated but global, socio-cultural perspectives. It is imperative that our evaluation resources are enhanced and calibrated by including people and experiences from different cultures and societies worldwide, in order to prevent gross underestimations or skews in measurements of harm. In this work, we demonstrate a socio-culturally aware expansion of evaluation resources in the Indian societal context, specifically for the harm of stereotyping. We devise a community engaged effort to build a resource which contains stereotypes for axes of disparity that are uniquely present in India. The resultant resource increases the number of stereotypes known for and in the Indian context by over 1000 stereotypes across many unique identities. We also demonstrate the utility and effectiveness of such expanded resources for evaluations of language models. CONTENT WARNING: This paper contains examples of stereotypes that may be offensive.
</details>
<details>
<summary>摘要</summary>
随着生成语言模型在全球范围内的快速发展和应用，有一定的急需要扩大我们对害的衡量，不仅包括各种各样的害的类型和数量，还要考虑当地文化上下文，包括弱化的标签和社会偏见。现有的评估方法有限，因为它们没有代表多样化的、全球化的社会文化观点。为了避免严重的下预估或偏见，我们需要加强和调整我们的评估资源，包括从不同文化和社会世界中的人和经验中获取知识。在这种情况下，我们展示了一种具有社会文化意识的评估资源扩展方法，特别是在印度社会上下文中，对刻板印度人的害进行了社区参与的努力。我们制定了一个含有印度独特的负担轴的 sterotypes 资源，该资源包含了在印度上下文中独特的1000多个刻板印度人。我们还证明了这种扩展资源的有用性和效果，用于评估语言模型。警告：本文可能包含有害的刻板印度人示例。
</details></li>
</ul>
<hr>
<h2 id="IvyGPT-InteractiVe-Chinese-pathwaY-language-model-in-medical-domain"><a href="#IvyGPT-InteractiVe-Chinese-pathwaY-language-model-in-medical-domain" class="headerlink" title="IvyGPT: InteractiVe Chinese pathwaY language model in medical domain"></a>IvyGPT: InteractiVe Chinese pathwaY language model in medical domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10512">http://arxiv.org/abs/2307.10512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongsheng Wang, Yaofei Duan, ChanTong Lam, Jiexi Chen, Jiangsheng Xu, Haoming Chen, Xiaohong Liu, Patrick Cheong-Iao Pang, Tao Tan</li>
<li>for: 这个论文是为了提出一种基于 LLaMA 的大型自然语言处理模型（IvyGPT），用于医疗问答和诊断。</li>
<li>methods: 这个论文使用了高质量医疗问答（QA）实例和人工回馈学习（RLHF）来训练和精度调整 IvyGPT。</li>
<li>results: 实验结果显示，IvyGPT 已经超越了其他医疗 GPT 模型，并且可以输出更加详细的诊断和治疗答案。<details>
<summary>Abstract</summary>
General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models.
</details>
<details>
<summary>摘要</summary>
通用大型语言模型（LLM）如ChatGPT已经表现出了惊人的成功。然而，这些LLM还没有广泛应用于医疗领域，主要因为它们的精度不高并无法提供医学建议。我们提出了IvyGPT，基于LLaMA的LLM，通过高质量的医学问答（QA）实例和人工智能反馈学习（RLHF）进行训练和细化。经过超vision训练，IvyGPT具有良好的多turn对话能力，但它无法像医生一样在其他方面做出全面诊断。通过RLHF，IvyGPT可以输出更加丰富的诊断和治疗答案，更加接近人类。在训练中，我们使用了QLoRA来训练330亿参数的NVIDIA A100（80GB）GPU。实验结果表明，IvyGPT已经超过了其他医学GPT模型。
</details></li>
</ul>
<hr>
<h2 id="Markov-Decision-Processes-with-Time-Varying-Geometric-Discounting"><a href="#Markov-Decision-Processes-with-Time-Varying-Geometric-Discounting" class="headerlink" title="Markov Decision Processes with Time-Varying Geometric Discounting"></a>Markov Decision Processes with Time-Varying Geometric Discounting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10491">http://arxiv.org/abs/2307.10491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiarui Gan, Annika Hennes, Rupak Majumdar, Debmalya Mandal, Goran Radanovic</li>
<li>for: 研究无穷 infinithorizon Markov decision processes（MDPs）中的时间变化的折损因子模型。</li>
<li>methods: 从游戏观点出发，每个时间步骤 treated as an independent decision maker with their own（固定）折损因子，研究其相对稳定点（SPE）的游戏观点和相关的算法问题。</li>
<li>results: 存在一个SPE的构造证明，并证明计算SPE是EXPTIME-hard的。此外，还证明存在一个$\epsilon$-SPE，并提供了一种计算$\epsilon$-SPE的算法，其时间复杂度upper bound为函数于时间变化的折损因子的收敛性。<details>
<summary>Abstract</summary>
Canonical models of Markov decision processes (MDPs) usually consider geometric discounting based on a constant discount factor. While this standard modeling approach has led to many elegant results, some recent studies indicate the necessity of modeling time-varying discounting in certain applications. This paper studies a model of infinite-horizon MDPs with time-varying discount factors. We take a game-theoretic perspective -- whereby each time step is treated as an independent decision maker with their own (fixed) discount factor -- and we study the subgame perfect equilibrium (SPE) of the resulting game as well as the related algorithmic problems. We present a constructive proof of the existence of an SPE and demonstrate the EXPTIME-hardness of computing an SPE. We also turn to the approximate notion of $\epsilon$-SPE and show that an $\epsilon$-SPE exists under milder assumptions. An algorithm is presented to compute an $\epsilon$-SPE, of which an upper bound of the time complexity, as a function of the convergence property of the time-varying discount factor, is provided.
</details>
<details>
<summary>摘要</summary>
标准的马尔可夫决策过程（MDP）模型通常使用固定的折扣因子进行减折扣。然而，一些最近的研究表明，在某些应用场景中，时变折扣是必要的。这篇论文研究了无穷 horizon MDP 中的时变折扣因子。我们从游戏观点出发，即每个时间步骤都是一个独立的决策者，每个决策者都有自己的固定折扣因子。我们研究这个游戏的子游戏完善平衡（SPE）以及相关的算法问题。我们提供了一个构造性的证明，证明了 SPE 的存在，并证明了计算 SPE 的复杂度是 EXPTIME 困难的。此外，我们还研究了 $\epsilon $-SPE 的概念，并证明了在较宽的假设下， $\epsilon $-SPE 存在。我们还提供了一个算法来计算 $\epsilon $-SPE，并给出了时间复杂度的Upper bound，具体取决于时变折扣因子的收敛性。
</details></li>
</ul>
<hr>
<h2 id="Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs"><a href="#Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs" class="headerlink" title="(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs"></a>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10490">http://arxiv.org/abs/2307.10490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebagdasa/multimodal_injection">https://github.com/ebagdasa/multimodal_injection</a></li>
<li>paper_authors: Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</li>
<li>for: 这篇论文旨在描述如何使用图像和声音作为多模态语言模型（LLM）的间接提示和指令注入攻击。</li>
<li>methods: 袋虫生成抗击噪阻抗器，并将其混合到图像或音频记录中。当用户问问 benign 模型关于损害图像或音频时，抗击噪阻会导致模型输出攻击者选择的文本和&#x2F;或使 subsequential dialog 遵循攻击者的指令。</li>
<li>results: 论文通过several proof-of-concept例子，证明了这种攻击可以成功地控制 LLVA 和 PandaGPT 等多模态语言模型。<details>
<summary>Abstract</summary>
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
</details>
<details>
<summary>摘要</summary>
我们展示了图像和声音可以用来导入多 modal LLM 中的间接提示和指令注入攻击。攻击者创建了这些提示的恶意变化，然后把它与图像或音频录音混合在一起。当用户对未修改的、良好的模型询问这些图像或音频时，变化将引导模型发出攻击者选择的文本和/或使之在对话中按照攻击者的指令继续。我们透过几个证明例子，显示了这种攻击可以对 LLaVa 和 PandaGPT 进行。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Attack-against-Object-Detection-with-Clean-Annotation"><a href="#Backdoor-Attack-against-Object-Detection-with-Clean-Annotation" class="headerlink" title="Backdoor Attack against Object Detection with Clean Annotation"></a>Backdoor Attack against Object Detection with Clean Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10487">http://arxiv.org/abs/2307.10487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yize Cheng, Wenbin Hu, Minhao Cheng</li>
<li>for: 防止深度神经网络中的攻击，特别是对象检测任务中的攻击。</li>
<li>methods: 利用深度学习中的特性，对对象检测任务进行袋式攻击，包括对象消失攻击和对象生成攻击。</li>
<li>results: 在PASCAL VOC07+12和MSCOCO两个对象检测数据集上，实现了对象检测攻击成功率超过92%，且杂点率仅5%。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have shown unprecedented success in object detection tasks. However, it was also discovered that DNNs are vulnerable to multiple kinds of attacks, including Backdoor Attacks. Through the attack, the attacker manages to embed a hidden backdoor into the DNN such that the model behaves normally on benign data samples, but makes attacker-specified judgments given the occurrence of a predefined trigger. Although numerous backdoor attacks have been experimented on image classification, backdoor attacks on object detection tasks have not been properly investigated and explored. As object detection has been adopted as an important module in multiple security-sensitive applications such as autonomous driving, backdoor attacks on object detection could pose even more severe threats. Inspired by the inherent property of deep learning-based object detectors, we propose a simple yet effective backdoor attack method against object detection without modifying the ground truth annotations, specifically focusing on the object disappearance attack and object generation attack. Extensive experiments and ablation studies prove the effectiveness of our attack on two benchmark object detection datasets, PASCAL VOC07+12 and MSCOCO, on which we achieve an attack success rate of more than 92% with a poison rate of only 5%.
</details>
<details>
<summary>摘要</summary>
Inspired by the inherent properties of deep learning-based object detectors, we propose a simple yet effective backdoor attack method against object detection without modifying the ground truth annotations. We focus on two types of attacks: object disappearance and object generation. Our attack method can achieve an attack success rate of over 92% with a poison rate of only 5% on two popular object detection datasets, PASCAL VOC07+12 and MSCOCO.Extensive experiments and ablation studies demonstrate the effectiveness of our attack method. We also show that existing defenses against backdoor attacks are ineffective against our method. Our findings highlight the urgent need for more robust defenses against backdoor attacks in object detection tasks.
</details></li>
</ul>
<hr>
<h2 id="Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting"><a href="#Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting" class="headerlink" title="Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?"></a>Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10472">http://arxiv.org/abs/2307.10472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omkar Dige, Jacob-Junqi Tian, David Emerson, Faiza Khan Khattak</li>
<li>for: 这个论文是为了评估指定 fine-tuned 语言模型中的偏见性而写的。</li>
<li>methods: 这个论文使用了零shot提问的方法来评估语言模型的偏见性。</li>
<li>results: 研究发现，使用 Alpaca 7B 模型，在偏见性识别任务上获得了56.7%的准确率，而 scaling up LLM 大小和数据多样性可能会导致更好的性能。<details>
<summary>Abstract</summary>
As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.
</details>
<details>
<summary>摘要</summary>
为了应对语言模型应用的快速扩展，现在越来越重要建立有效的测试和干预模型学习或遗传社会偏见的框架。在这篇论文中，我们展示了我们在零条件提示中评估推广语言模型的偏见识别能力，包括链接思维（CoT）提示。在LLaMA和其两个导入精通版本中，Alpaca 7B在偏见识别任务上表现最好，具体的准确率为56.7%。我们也示出了将LLM大小和数据多样性增加可以带来更大的性能提升。这是我们的偏见干预框架的首个部分，我们将继续更新这个工作，当我们获得更多结果时。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Visualization-Types-and-Perspectives-in-Patents"><a href="#Classification-of-Visualization-Types-and-Perspectives-in-Patents" class="headerlink" title="Classification of Visualization Types and Perspectives in Patents"></a>Classification of Visualization Types and Perspectives in Patents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10471">http://arxiv.org/abs/2307.10471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tibhannover/patentimageclassification">https://github.com/tibhannover/patentimageclassification</a></li>
<li>paper_authors: Junaid Ahmed Ghauri, Eric Müller-Budack, Ralph Ewerth</li>
<li>for: 这篇论文的目的是提高专利搜寻和检索的效率，使用最新的深度学习方法来分类专利图像中的不同类型和角度。</li>
<li>methods: 本论文使用了现今的深度学习方法，包括矩阵变数储存（Transformers），来分类专利图像中的不同类型和角度。</li>
<li>results: 实验结果显示了提案的方法的可行性，并且提供了可用于实际应用的代码、模型和数据集。<details>
<summary>Abstract</summary>
Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Experimental results have demonstrated the feasibility of the proposed approaches. Source code, models, and dataset will be made publicly available.
</details>
<details>
<summary>摘要</summary>
In this paper, we employ state-of-the-art deep learning techniques for the classification of visualization types and perspectives in patent images. We expand the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. Additionally, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Our experimental results demonstrate the feasibility of the proposed approaches. The source code, models, and dataset will be publicly available.
</details></li>
</ul>
<hr>
<h2 id="A-data-science-axiology-the-nature-value-and-risks-of-data-science"><a href="#A-data-science-axiology-the-nature-value-and-risks-of-data-science" class="headerlink" title="A data science axiology: the nature, value, and risks of data science"></a>A data science axiology: the nature, value, and risks of data science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10460">http://arxiv.org/abs/2307.10460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael L. Brodie</li>
<li>for: 这篇论文是为了探讨数据科学的axiology，即其目的、性质、重要性、风险和价值，以帮助理解和定义数据科学，并找到其可能的 beneficii、风险和研究挑战。</li>
<li>methods: 本论文使用了 axiological 方法，通过探讨数据科学的remarkable和definitive特性来评估其值和风险。</li>
<li>results: 本论文认为，数据科学在其初始阶段，axiology 可以帮助我们更好地理解和定义它，并找到其可能的 beneficii、风险和研究挑战。<details>
<summary>Abstract</summary>
Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery and will take us into new ways of understanding the world.
</details>
<details>
<summary>摘要</summary>
数据科学不是一门科学。它是一种研究方法论，具有未曾探索的范围、大小、复杂性和知识发现的能力，超过人类的理解。它正在改变我们的世界，在每个领域普遍应用了数以千计的应用程序，并在人工智能竞赛中广泛应用。由于它的不可知晓性，这可能会导致不可预期的风险。本文提出了数据科学的axiology，即其目的、性质、重要性、风险和问题解决的价值，通过探索和评估它的卓越特征来帮助理解和定义数据科学，并识别其潜在的好处、风险和研究挑战。由于数据科学处于其初期阶段，这个初步的、观测的axiology可以帮助我们更好地理解和定义它，并掌握其潜在的价值和风险。人工智能基础的数据科学是不确定的，可能比我们更好地适应现实世界。数据科学将对我们的世界产生深远的影响，将带我们进入新的世界理解方式。
</details></li>
</ul>
<hr>
<h2 id="A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints"><a href="#A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints" class="headerlink" title="A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints"></a>A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10459">http://arxiv.org/abs/2307.10459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Andrei V. Konstantinov, Lev V. Utkin</li>
<li>for: 这篇论文的目的是提出一种新的计算简单的神经网络输出值约束方法。</li>
<li>methods: 该方法的关键思想是将神经网络参数 вектор映射到一个确保处于可能的约束集的点上。该映射是通过额外的神经网络层实现，该层带有约束条件。</li>
<li>results: 该方法可以简单地扩展到对输出 Vector 以外的同时受约束的情况，并且可以简单地实现投影方法来实现约束。该方法的计算简单，前向传播的复杂度为O(n*m)和O(n^2*m)。 numerics 实验用于解决优化和分类问题。<details>
<summary>Abstract</summary>
A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pass of the proposed neural network layer by linear and quadratic constraints are O(n*m) and O(n^2*m), respectively, where n is the number of variables, m is the number of constraints. Numerical experiments illustrate the method by solving optimization and classification problems. The code implementing the method is publicly available.
</details>
<details>
<summary>摘要</summary>
一种新的 computationally simple method for imposing hard convex constraints on neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by an additional neural network layer with constraints on the output. The proposed method can be easily extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can also be simply implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, as well as constraints in the form of boundaries. An important feature of the method is its computational simplicity, with the complexities of the forward pass of the proposed neural network layer being O(n\*m) and O(n^2\*m), respectively, where n is the number of variables and m is the number of constraints. Numerical experiments illustrate the method by solving optimization and classification problems, and the code implementing the method is publicly available.
</details></li>
</ul>
<hr>
<h2 id="Complying-with-the-EU-AI-Act"><a href="#Complying-with-the-EU-AI-Act" class="headerlink" title="Complying with the EU AI Act"></a>Complying with the EU AI Act</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10458">http://arxiv.org/abs/2307.10458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacintha Walters, Diptish Dey, Debarati Bhaumik, Sophie Horsman</li>
<li>for: 本研究旨在描述欧盟人工智能法案（AI Act）中不同类别的实施情况，并通过问卷调查获取量化数据，以提供有关组织实施AI Act的启示。</li>
<li>methods: 本研究使用问卷调查方法来收集数据，并分析数据以挖掘不同类别组织面临的挑战，以及这些挑战如何与组织特点相关。</li>
<li>results: 研究发现，不同类别组织面临的挑战有所不同，而大型和特定领域的组织面临更大的挑战。此外，问卷调查还显示了各个问题的占比，包括AI Act的内容和应用方面。<details>
<summary>Abstract</summary>
The EU AI Act is the proposed EU legislation concerning AI systems. This paper identifies several categories of the AI Act. Based on this categorization, a questionnaire is developed that serves as a tool to offer insights by creating quantitative data. Analysis of the data shows various challenges for organizations in different compliance categories. The influence of organization characteristics, such as size and sector, is examined to determine the impact on compliance. The paper will also share qualitative data on which questions were prevalent among respondents, both on the content of the AI Act as the application. The paper concludes by stating that there is still room for improvement in terms of compliance with the AIA and refers to a related project that examines a solution to help these organizations.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "AI Act" is translated as "欧盟AI法规" (EU AI Act)* "categories" is translated as "类别" (categories)* "questionnaire" is translated as "问卷" (questionnaire)* "compliance" is translated as "合规" (compliance)* "organization" is translated as "组织" (organization)* "size" is translated as "大小" (size)* "sector" is translated as "领域" (sector)* "qualitative data" is translated as "质量数据" (qualitative data)* "content" is translated as "内容" (content)* "application" is translated as "应用" (application)* "improvement" is translated as "改进" (improvement)* "solution" is translated as "解决方案" (solution)
</details></li>
</ul>
<hr>
<h2 id="A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset"><a href="#A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset" class="headerlink" title="A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset"></a>A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10455">http://arxiv.org/abs/2307.10455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zahrag/BIOSCAN-1M">https://github.com/zahrag/BIOSCAN-1M</a></li>
<li>paper_authors: Zahra Gharaee, ZeMing Gong, Nicholas Pellegrino, Iuliia Zarubiieva, Joakim Bruslund Haurum, Scott C. Lowe, Jaclyn T. A. McKeown, Chris C. Y. Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke, Angel X. Chang, Graham W. Taylor, Paul Fieguth</li>
<li>for: 本研究旨在开发一个基于图像的生物多样性调查方法，以探索全球生物多样性的详细结构。</li>
<li>methods: 该研究使用了大量手动标注的昆虫图像集，以及相关的遗传信息，包括raw nucleotide barcode sequences和归类指标。</li>
<li>results: 研究人员通过实现和分析一种基线分类器来介绍图像基于的生物分类问题的特点和挑战。<details>
<summary>Abstract</summary>
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.
</details>
<details>
<summary>摘要</summary>
我团队在尝试 catalog insect 多样性时，我们提出了一个新的大型手标记昆虫图像集合，称为 BIOSCAN-Insect 数据集。每个记录都被taxonomically分类由专家，同时还包括Raw nucleotide barcode sequences和分配给barcode index numbers，这些是基于物种分类的生物 marker。本文介绍了一个精心约 million 张图像集合，主要用于训练计算机视觉模型，以提供图像基本的 taxonomic assessment。然而，该数据集还具有一些吸引人的特点，研究这些特点会对机器学习社区产生感兴趣。由于数据集的生物性质，它展现了一种长尾分布。此外，taxonomic 标签是一种层次分类方案，这使得图像分类问题变得非常细化。本文介绍了数据集和基线分类器的实现和分析，以便进一步推动生物多样性研究在机器学习社区中的发展。
</details></li>
</ul>
<hr>
<h2 id="Learning-Formal-Specifications-from-Membership-and-Preference-Queries"><a href="#Learning-Formal-Specifications-from-Membership-and-Preference-Queries" class="headerlink" title="Learning Formal Specifications from Membership and Preference Queries"></a>Learning Formal Specifications from Membership and Preference Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10434">http://arxiv.org/abs/2307.10434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ameesh Shah, Marcell Vazquez-Chanlatte, Sebastian Junges, Sanjit A. Seshia</li>
<li>for: 学习正式规定，如自动机。</li>
<li>methods: 提议一种新的框架，异步请求组合会员标签和对比 preference，而不仅仅是会员标签。</li>
<li>results: 在两个不同领域中实现了框架，并显示了通过组合Modalities来异步学习规定的可靠性和方便性。<details>
<summary>Abstract</summary>
Active learning is a well-studied approach to learning formal specifications, such as automata. In this work, we extend active specification learning by proposing a novel framework that strategically requests a combination of membership labels and pair-wise preferences, a popular alternative to membership labels. The combination of pair-wise preferences and membership labels allows for a more flexible approach to active specification learning, which previously relied on membership labels only. We instantiate our framework in two different domains, demonstrating the generality of our approach. Our results suggest that learning from both modalities allows us to robustly and conveniently identify specifications via membership and preferences.
</details>
<details>
<summary>摘要</summary>
aktive lerning 是一种已经广泛研究的学习形式，用于学习正式规范，如自动机。在这项工作中，我们延伸 aktive specification learning 的框架，提议一种新的框架，强调策略性地请求混合成员标签和对比标签，这是成员标签的受欢迎替代方案。这种混合的方式允许更加灵活地进行 aktive specification learning，之前只能通过成员标签进行学习。我们在两个不同的领域中实现了我们的框架，证明了我们的方法的一致性。我们的结果表明，从两种模式中学习可以robustly和方便地识别规范via成员和偏好。
</details></li>
</ul>
<hr>
<h2 id="PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models"><a href="#PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models" class="headerlink" title="PreDiff: Precipitation Nowcasting with Latent Diffusion Models"></a>PreDiff: Precipitation Nowcasting with Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10422">http://arxiv.org/abs/2307.10422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihan Gao, Xingjian Shi, Boran Han, Hao Wang, Xiaoyong Jin, Danielle Maddix, Yi Zhu, Mu Li, Yuyang Wang</li>
<li>for: 这篇研究旨在提出一个可能性推断模型，以实现地球系统预测中的不确定性处理和专业知识整合。</li>
<li>methods: 研究使用了两个阶段构成的潜在阶段推断管线，包括一个名为PreDiff的可能性条件扩散模型，以及一个内置的专业知识控制机制，以确保预测符合物理限制。</li>
<li>results: 实验结果显示PreDiff能够有效地处理不确定性，整合专业知识，并产生高操作价值的预测。<details>
<summary>Abstract</summary>
Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly. We conduct empirical studies on two datasets: N-body MNIST, a synthetic dataset with chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset. Specifically, we impose the law of conservation of energy in N-body MNIST and anticipated precipitation intensity in SEVIR. Experiments demonstrate the effectiveness of PreDiff in handling uncertainty, incorporating domain-specific prior knowledge, and generating forecasts that exhibit high operational utility.
</details>
<details>
<summary>摘要</summary>
地球系统预报traditionally rely于复杂的物理模型，这些模型 computationally expensive和需要特定领域专家知识。过去一代，随着无 precedented 的类时空 Earth observation data的增加，使得可以使用深度学习技术建立数据驱动的预报模型。这些模型在多种 Earth system预报任务中表现了承诺，但是它们可能会处理不确定性或忽略领域特定的专业知识，导致预报结果模糊或生成物理不合理的预测。为了解决这些限制，我们提议了一个two-stage管道 для抽象的类时空预报：1. 我们开发了一个名为PreDiff的conditional latent diffusion模型，可以实现抽象的预报。2. 我们将Explicit知识控制机制加入管道中，以确保预报与领域特定的物理限制相align。这是通过在每个排除步骤中估算对套用的条件为预报过滤条件，然后调整转换分布而实现。我们在两个数据集上进行了实验：N-body MNIST和SEVIR。Specifically，我们在N-body MNIST中强制遵循能量守恒定律，并在SEVIR中预测预测 precipitation intensity。实验结果显示PreDiff可以高效地处理不确定性，把领域特定的专业知识纳入预报中，并生成高效用的预报。
</details></li>
</ul>
<hr>
<h2 id="GOOSE-Algorithm-A-Powerful-Optimization-Tool-for-Real-World-Engineering-Challenges-and-Beyond"><a href="#GOOSE-Algorithm-A-Powerful-Optimization-Tool-for-Real-World-Engineering-Challenges-and-Beyond" class="headerlink" title="GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond"></a>GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10420">http://arxiv.org/abs/2307.10420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebwar Khalid Hamad, Tarik A. Rashid</li>
<li>For: The paper proposes a novel metaheuristic algorithm called GOOSE, which is inspired by the behavior of geese during rest and foraging. The algorithm is designed to solve optimization problems.* Methods: The GOOSE algorithm uses a combination of balance and guarding mechanisms to search for the optimal solution. It is benchmarked on 19 well-known test functions and compared with four other algorithms.* Results: The results show that the GOOSE algorithm outperforms the other algorithms on 10 modern benchmark functions and 5 classical benchmark functions. It is also applied to three real-world engineering challenges and shows good performance in optimizing these problems.<details>
<summary>Abstract</summary>
This study proposes the GOOSE algorithm as a novel metaheuristic algorithm based on the goose's behavior during rest and foraging. The goose stands on one leg and keeps his balance to guard and protect other individuals in the flock. The GOOSE algorithm is benchmarked on 19 well-known benchmark test functions, and the results are verified by a comparative study with genetic algorithm (GA), particle swarm optimization (PSO), dragonfly algorithm (DA), and fitness dependent optimizer (FDO). In addition, the proposed algorithm is tested on 10 modern benchmark functions, and the gained results are compared with three recent algorithms, such as the dragonfly algorithm, whale optimization algorithm (WOA), and salp swarm algorithm (SSA). Moreover, the GOOSE algorithm is tested on 5 classical benchmark functions, and the obtained results are evaluated with six algorithms, such as fitness dependent optimizer (FDO), FOX optimizer, butterfly optimization algorithm (BOA), whale optimization algorithm, dragonfly algorithm, and chimp optimization algorithm (ChOA). The achieved findings attest to the proposed algorithm's superior performance compared to the other algorithms that were utilized in the current study. The technique is then used to optimize Welded beam design and Economic Load Dispatch Problem, three renowned real-world engineering challenges, and the Pathological IgG Fraction in the Nervous System. The outcomes of the engineering case studies illustrate how well the suggested approach can optimize issues that arise in the real-world.
</details>
<details>
<summary>摘要</summary>
这项研究提出了一种新的元朋凝融算法，基于鹅的休息和搜寻行为。鹅站在一个脚上，保持平衡，以保护和保障鸟群其他成员。这种算法被测试在19个知名的测试函数上，并与遗传算法（GA）、 particle swarm优化算法（PSO）、龙虾算法（DA）和优化器（FDO）进行比较研究。此外，提出的算法还被测试在10个现代测试函数上，并与三种最新的算法，如龙虾算法（WOA）、鳄鱼算法（SSA）和蝴蝶算法（BOA）进行比较。此外，GOOSE算法还被测试在5个经典测试函数上，并与6种算法，如依赖度优化器（FDO）、FOX优化器、蝴蝶算法（BOA）、龙虾算法、鳄鱼算法和猩猩算法（ChOA）进行比较。实验结果证明，提出的算法在与其他算法进行比较时表现出色。然后，这种算法被应用于焊接梁设计和经济荷负调度问题，以及神经系统中免疫力IgG分数的优化问题。工程实践研究的结果表明，这种方法可以高效地解决现实中出现的问题。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Autonomous-Driving-Actions-with-Visual-Question-Answering"><a href="#Explaining-Autonomous-Driving-Actions-with-Visual-Question-Answering" class="headerlink" title="Explaining Autonomous Driving Actions with Visual Question Answering"></a>Explaining Autonomous Driving Actions with Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10408">http://arxiv.org/abs/2307.10408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shahin-01/vqa-ad">https://github.com/shahin-01/vqa-ad</a></li>
<li>paper_authors: Shahin Atakishiyev, Mohammad Salameh, Housam Babiker, Randy Goebel</li>
<li>for: 本研究旨在提供一种可解释的自动驾驶技术，以便更好地理解自动驾驶车辆的决策过程。</li>
<li>methods: 本研究使用了可见问答（VQA）框架，通过问题answering-based causal reasoning来解释自动驾驶车辆的决策。</li>
<li>results: 研究发现，VQA机制可以提供支持来解释自动驾驶车辆的决策过程，并帮助提高整体驾驶安全性。<details>
<summary>Abstract</summary>
The end-to-end learning ability of self-driving vehicles has achieved significant milestones over the last decade owing to rapid advances in deep learning and computer vision algorithms. However, as autonomous driving technology is a safety-critical application of artificial intelligence (AI), road accidents and established regulatory principles necessitate the need for the explainability of intelligent action choices for self-driving vehicles. To facilitate interpretability of decision-making in autonomous driving, we present a Visual Question Answering (VQA) framework, which explains driving actions with question-answering-based causal reasoning. To do so, we first collect driving videos in a simulation environment using reinforcement learning (RL) and extract consecutive frames from this log data uniformly for five selected action categories. Further, we manually annotate the extracted frames using question-answer pairs as justifications for the actions chosen in each scenario. Finally, we evaluate the correctness of the VQA-predicted answers for actions on unseen driving scenes. The empirical results suggest that the VQA mechanism can provide support to interpret real-time decisions of autonomous vehicles and help enhance overall driving safety.
</details>
<details>
<summary>摘要</summary>
自驾车技术在过去一代取得了重大突破，归功于深度学习和计算机视觉算法的快速发展。然而，由于自驾车技术是安全关键的人工智能应用，因此需要解释自驾车的决策。为实现自驾车决策的解释，我们提出了视觉问答（VQA）框架，该框架通过问答对 causal 理解来解释自驾车的决策。首先，我们使用回归学习（RL）在模拟环境中收集了驾驶视频数据，并从这些日志数据中采样出了五个动作类别的连续帧。然后，我们手动标注了这些抽取的帧，使用问题对答对为每个场景选择的动作提供了证明。最后，我们对未看过的驾驶场景中VQA预测的答案进行了评估。实际结果表明，VQA机制可以为自驾车决策提供支持，并帮助提高总体驾驶安全性。
</details></li>
</ul>
<hr>
<h2 id="Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games"><a href="#Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games" class="headerlink" title="Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games"></a>Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11105">http://arxiv.org/abs/2307.11105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Gillberg, Joakim Bergdahl, Alessandro Sestini, Andrew Eakins, Linus Gisslen</li>
<li>for: 本研究旨在推广游戏生产中的机器学习应用，特意是通过强化学习增加自动游戏测试解决方案中的测试覆盖率。</li>
<li>methods: 本研究使用了训练bot的强化学习系统，与现有的脚本bot测试解决方案集成。</li>
<li>results: 研究在AAA游戏《战field 2042》和《尸 espacio》（2023）中实现了增加测试覆盖率的目的，并提出了一些可能有价值的研究方向，以帮助游戏业者快速采用这种技术。<details>
<summary>Abstract</summary>
Going from research to production, especially for large and complex software systems, is fundamentally a hard problem. In large-scale game production, one of the main reasons is that the development environment can be very different from the final product. In this technical paper we describe an effort to add an experimental reinforcement learning system to an existing automated game testing solution based on scripted bots in order to increase its capacity. We report on how this reinforcement learning system was integrated with the aim to increase test coverage similar to [1] in a set of AAA games including Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to show a use-case of leveraging reinforcement learning in game production and cover some of the largest time sinks anyone who wants to make the same journey for their game may encounter. Furthermore, to help the game industry to adopt this technology faster, we propose a few research directions that we believe will be valuable and necessary for making machine learning, and especially reinforcement learning, an effective tool in game production.
</details>
<details>
<summary>摘要</summary>
从研究到生产，特别是 для大型和复杂的软件系统，是一个基本困难的问题。在大规模游戏生产中，一个主要的原因是开发环境和产品环境之间的差异。在这份技术著作中，我们描述了将实验式学习系统添加到现有的自动游戏测试解决方案基于脚本 Bot 以增加其容量。我们报告了在一些 AAA 游戏，包括 Battlefield 2042 和 Dead Space (2023) 中如何将这个学习系统与测试系统集成，以增加测试覆盖率。这份技术著作的目的是展示在游戏生产中如何使用学习系统，并讨论一些可能会遇到的主要时间潜雷。此外，为了帮助游戏业界更快地采用这些技术，我们建议了一些研究方向，我们认为这些方向将是有价值和必要的，以使机器学习和特别是学习系统成为游戏生产中的有效工具。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net"><a href="#Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net" class="headerlink" title="Interpreting and Correcting Medical Image Classification with PIP-Net"></a>Interpreting and Correcting Medical Image Classification with PIP-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10404">http://arxiv.org/abs/2307.10404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-nauta/pipnet">https://github.com/m-nauta/pipnet</a></li>
<li>paper_authors: Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, Jörg Schlötterer, Maurice van Keulen, Christin Seifert</li>
<li>for: 这篇论文探讨了使用可解释的机器学习模型，尤其是PIP-Net，在实际医疗图像数据上进行自动诊断支持。</li>
<li>methods: 这篇论文使用PIP-Net模型，学习人类理解的图像组件prototype，并评估其精度和可解释性在骨折检测和皮肤癌诊断方面。</li>
<li>results: 研究发现，PIP-Net的决策过程与医学分类标准相一致，只需要提供图像级别的类别标签。此外，PIP-Net还可以轻松地检测数据质量问题，如X光图像中的不必要文本或标签错误。最后，我们发现，人类可以直接禁用PIP-Net的不想要的 проtotypes，以 corrrect其决策过程。<details>
<summary>Abstract</summary>
Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.Translate the text into Simplified Chinese.</SYS>Here's the translation:Part-prototype 模型是一种可解释的设计图像分类器，也是黑盒AI的有力的替代方案。本文探讨了PIP-Net在实际医疗影像数据上的适用性和潜力，并评估了它的准确率和可解释性在骨折检测和皮肤癌诊断方面。我们发现PIP-Net的决策过程与医学分类标准相一致，只需要图像级别的类别标签。由于PIP-Net在前期无监督学习prototype的方式下，可以轻松地标识数据质量问题，如X射线图像中的不想要的文本或标签错误。此外，我们是首次显示人类可以手动 corriger PIP-Net的逻辑，通过直接禁用不需要的prototype来改善其判断。我们 conclude that part-prototype 模型在医疗应用中具有可解释性和进一步的模型调试潜力。
</details></li>
</ul>
<hr>
<h2 id="DialogStudio-Towards-Richest-and-Most-Diverse-Unified-Dataset-Collection-for-Conversational-AI"><a href="#DialogStudio-Towards-Richest-and-Most-Diverse-Unified-Dataset-Collection-for-Conversational-AI" class="headerlink" title="DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI"></a>DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10172">http://arxiv.org/abs/2307.10172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salesforce/DialogStudio">https://github.com/salesforce/DialogStudio</a></li>
<li>paper_authors: Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou Yu, Huan Wang, Silvio Savarese, Caiming Xiong</li>
<li>For: The paper aims to introduce DialogStudio, a large and diverse collection of dialogue datasets, to address the challenges of handling diverse conversational tasks and improve the comprehensiveness of existing dialogue dataset collections.* Methods: The paper uses a consistent format to unify diverse dialogue datasets, including open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues. The authors also identify licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning.* Results: The authors develop conversational AI models using the dataset collection and demonstrate their superiority in both zero-shot and few-shot learning scenarios. They also make all datasets, licenses, codes, and models associated with DialogStudio publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/salesforce/DialogStudio">https://github.com/salesforce/DialogStudio</a> to support dataset and task-based research, as well as language model pre-training.<details>
<summary>Abstract</summary>
Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-shot and few-shot learning scenarios demonstrate the superiority of DialogStudio. To improve transparency and support dataset and task-based research, as well as language model pre-training, all datasets, licenses, codes, and models associated with DialogStudio are made publicly accessible at https://github.com/salesforce/DialogStudio
</details>
<details>
<summary>摘要</summary>
尽管 conversational AI 技术有所进步，但语言模型在各种对话任务中仍然遇到挑战，现有对话集合也常lacks diversity和completeness。为解决这些问题，我们介绍 DialogStudio：最大最多样化的对话集合，具有一致的格式而不产生数据损失。我们的集合包括了开放领域对话、任务启发对话、自然语言理解、对话推荐、对话概要和知识启发对话，这使得它成为对话研究和模型训练的极其富裕和多样化资源。为进一步提高 DialogStudio 的实用性，我们确定了每个数据集的许可证，并设计了适应域匹配的提示，以便实现 instrucion-aware fine-tuning。此外，我们使用 DialogStudio 集合来开发 conversational AI 模型，我们的实验表明，在零shot 和几shot 学习场景中，DialogStudio 具有显著的优势。为提高透明度和支持数据集和任务基础研究以及语言模型预训练，我们将所有相关的数据集、许可证、代码和模型 associatted  WITH DialogStudio 公开访问于 GitHub 上，请参考 https://github.com/salesforce/DialogStudio。
</details></li>
</ul>
<hr>
<h2 id="LightPath-Lightweight-and-Scalable-Path-Representation-Learning"><a href="#LightPath-Lightweight-and-Scalable-Path-Representation-Learning" class="headerlink" title="LightPath: Lightweight and Scalable Path Representation Learning"></a>LightPath: Lightweight and Scalable Path Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10171">http://arxiv.org/abs/2307.10171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen</li>
<li>for: 提供高效、可扩展的路径表示学习框架，用于智能交通和智能城市应用。</li>
<li>methods: 提议一种轻量级、可扩展的路径表示学习框架，包括稀疏自动编码器、关系逻辑推理框架和全球-本地知识传播。</li>
<li>results: 经过广泛的实验 validate 了该框架的效率、可扩展性和表现力。<details>
<summary>Abstract</summary>
Movement paths are used widely in intelligent transportation and smart city applications. To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation. In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential. Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.   We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability. More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with respect to path length. Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders. We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse path encoders. Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
移动路径在智能交通和智能城市应用广泛使用。为服务这些应用，路径表示学习目标寻求提供高效精准的操作，以便于不同下游任务 such as 路径排名和旅行成本估算。在资源有限的环境和绿色计算限制下，很多情况下，很重要的路径表示学习是轻量级和可扩展的。然而，现有的路径表示学习研究主要关注准确性，尽管只是次要关注资源消耗和可扩展性。我们提出一个轻量级和可扩展的路径表示学习框架，名为LightPath，以减少资源消耗并实现可扩展性，而无需减少准确性。更具体来说，我们首先提出一个稀疏自动编码器，以确保框架在路径长度方面具有良好的可扩展性。然后，我们提出一个关系理解框架，以更快地训练更稀疏的路径编码器。最后，我们提出全球-本地知识传播，以进一步减小路径编码器的大小并提高其性能。我们在两个真实世界数据集上进行了广泛的实验，以提供有关效率、可扩展性和有效性的深入了解。
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Applications-of-Large-Language-Models"><a href="#Challenges-and-Applications-of-Large-Language-Models" class="headerlink" title="Challenges and Applications of Large Language Models"></a>Challenges and Applications of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10169">http://arxiv.org/abs/2307.10169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy</li>
<li>for: 本研究旨在为机器学习研究人员快速了解大语言模型（LLMs）领域的当前状况，以便更快地成为生产力的一员。</li>
<li>methods: 本研究采用系统的方法描述了LLMs领域的开放问题和成功应用领域，以便帮助研究人员更快地了解领域的当前状况。</li>
<li>results: 本研究对LLMs领域的当前状况进行了系统的描述，并identified several open problems and successful application areas, which can help researchers quickly understand the field and become productive.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）从不存在到普遍的Machine Learning话语中几年内快速发展。由于这个领域的快速进程，它很难分析剩下的挑战和已经有成果的应用领域。在这篇论文中，我们想要建立一套系统的开问和应用成功，以便ML研究人员更快地理解领域的当前状态，更快地成为产品力。
</details></li>
</ul>
<hr>
<h2 id="Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning"><a href="#Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning" class="headerlink" title="Robust Driving Policy Learning with Guided Meta Reinforcement Learning"></a>Robust Driving Policy Learning with Guided Meta Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10160">http://arxiv.org/abs/2307.10160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanghoon Lee, Jiachen Li, David Isele, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer</li>
<li>for: 实现自驾车辆在互动交通enario中的自主通行</li>
<li>methods: 采用一个单一meta-policy来训练多元的驾驶策略，通过随机调整社交车辆之间的互动奖励函数来生成多元的目标，并透过导引策略来训练meta-policy</li>
<li>results: 成功将ego车辆的驾驶策略导入不同的社交车辆行为中，并在一个具有挑战性的无控T字路口scenario中训练出一个具有弹性和可靠性的驾驶策略<details>
<summary>Abstract</summary>
Although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unseen situations with out-of-distribution (OOD) social agents' behaviors in a challenging uncontrolled T-intersection scenario.
</details>
<details>
<summary>摘要</summary>
In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives.We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unseen situations with out-of-distribution (OOD) social agents' behaviors in a challenging uncontrolled T-intersection scenario.translate into Simplified Chinese:although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors.在这个工作中，我们提出了一种有效的方法，通过随机化社交车辆之间的互动奖励函数，来训练多样化的驾驶策略。我们通过指导策略来有效地训练单一的元策略。此外，我们还提出了一种增强驾驶策略的训练策略，使用已学习的元策略控制社交车辆在环境中。我们成功地在一个复杂的无控T路口场景中学习了一个普适的驾驶策略，并能够在未看到的社交车辆行为下保持稳定。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion"><a href="#Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion" class="headerlink" title="Benchmarking Potential Based Rewards for Learning Humanoid Locomotion"></a>Benchmarking Potential Based Rewards for Learning Humanoid Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10142">http://arxiv.org/abs/2307.10142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/se-hwan/pbrs-humanoid">https://github.com/se-hwan/pbrs-humanoid</a></li>
<li>paper_authors: Se Hwan Jeon, Steve Heim, Charles Khazoom, Sangbae Kim</li>
<li>for: 本研究旨在比较标准形式的奖励拟合和 potential based reward shaping (PBRS) 在人工智能机器人中的表现。</li>
<li>methods: 本研究使用了标准的奖励拟合和 PBRS 方法，并对两种方法在高维系统中进行了比较。</li>
<li>results: 研究发现，在高维系统中，PBRS 的性能提升效果相对较弱，但 PBRS 奖励项在不同的缩放比例下表现更加稳定和易于调整。<details>
<summary>Abstract</summary>
The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly more robust to scaling than typical reward shaping approaches, and thus easier to tune.
</details>
<details>
<summary>摘要</summary>
主要挑战在开发有效的强化学习（RL）管道是设计和调整奖励函数。Well-designed 形式的奖励可以导致学习速度更快。然而，未经适当调整的奖励可能会与 желаем的行为冲突，导致过拟合或者even erratic performance。理论上，广泛的 potential based reward shaping（PBRS）可以帮助学习过程进行导航，无需影响最佳策略。虽然一些研究已经探讨了使用 potential based reward shaping 加速学习的潜在性，但大多数研究仅局限于格子世界和低维系统，RL在 robotics 中主要依靠标准的奖励形式。在这篇论文中，我们对标准的奖励形式和 PBRS 进行了比较，发现在这个高维系统中，PBRS 只有微妙的提高了速度。然而，PBRS 的奖励项目较标准的奖励形式更加稳定，更容易调整。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/20/cs.AI_2023_07_20/" data-id="cloh7tqav000z7b88gvfr6sy0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/cs.CL_2023_07_20/" class="article-date">
  <time datetime="2023-07-20T11:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/20/cs.CL_2023_07_20/">cs.CL - 2023-07-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="What-Twitter-Data-Tell-Us-about-the-Future"><a href="#What-Twitter-Data-Tell-Us-about-the-Future" class="headerlink" title="What Twitter Data Tell Us about the Future?"></a>What Twitter Data Tell Us about the Future?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02035">http://arxiv.org/abs/2308.02035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alina Landowska, Marek Robak, Maciej Skorski</li>
<li>for: This paper aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users.</li>
<li>methods: The study uses a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using state-of-the-art models. The research employs topic modeling techniques, such as LDA and BERT, to identify the topics and language cues used by futurists.</li>
<li>results: The study finds that the futurists’ language cues signal futures-in-the-making, which enhance social media users’ ability to anticipate and respond to their own scenarios in the present. The research identifies 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists’ tweets, providing insights into the futures anticipated by Twitter’s futurists.<details>
<summary>Abstract</summary>
Anticipation is a fundamental human cognitive ability that involves thinking about and living towards the future. While language markers reflect anticipatory thinking, research on anticipation from the perspective of natural language processing is limited. This study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using SOTA models. The study identifies 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists' tweets. These findings contribute to the research on topic modelling and provide insights into the futures anticipated by Twitter's futurists. The research demonstrates the futurists' language cues signals futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in present. The fully open-sourced dataset, interactive analysis, and reproducible source code are available for further exploration.
</details>
<details>
<summary>摘要</summary>
人类思维能力中的预期是一种基本的认知能力，它涉及到思考和生活在未来的事物。虽然语言标记反映了预期思维，但从自然语言处理的角度进行研究的研究却很有限。这项研究希望通过Twitter上的未来推测者来调查他们预测的未来，并explore语言提示对预期思维的影响。我们回答了关于Twitter上未来推测者预测和分享的未来是什么，以及这些预测的语言标记如何模型社交数据。为了调查这一点，我们查看相关的研究成果，讨论语言标记和具有影响力的人员对预期思维的影响，并提出一个分类系统，将未来分为“现在未来”和“未来现在”。这项研究通过对公共分享的微博上的未来推测者的数据进行编译，并使用最新的NLP管道实现了可扩展的NLU。我们通过LDA方法和BERTopic方法对未来推测者的微博中提取了15个话题和100个特定话题。这些发现对话题模型进行研究做出了贡献，并为Twitter上未来推测者的语言提示提供了深入的理解。研究表明，未来推测者的语言提示是未来在创造的信号，可以使社交媒体用户预测自己的enario并在现在回应。完整的开源数据、交互分析和可重复的源代码都可以进一步探索。
</details></li>
</ul>
<hr>
<h2 id="FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback"><a href="#FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback" class="headerlink" title="FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback"></a>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10867">http://arxiv.org/abs/2307.10867</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/figcapshf/figcapshf">https://github.com/figcapshf/figcapshf</a></li>
<li>paper_authors: Ashish Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nikos Vlassis, Ryan A. Rossi</li>
<li>for: 这篇论文是为了提高科学图像标题的自动生成技术，以满足读者的需求。</li>
<li>methods: 该论文使用了一种新的框架，即 FigCaps-HF，来生成图像标题。该框架包括自动评估图像标题对的质量以及人工反馈学习（RLHF）方法，以便根据读者的喜好进行标题生成。</li>
<li>results: 该论文的实验结果表明，使用 FigCaps-HF 框架可以提高标题生成的性能，特别是当使用 BLIP 作为基础模型时，RLHF 方法可以实现mean gain的35.7%、16.9%和9%在 ROUGE、BLEU 和 Meteor 等指标上。此外，该论文还释放了一个大规模的人工反馈标题对数据集，以便进一步评估和开发RLHF技术。<details>
<summary>Abstract</summary>
Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.
</details>
<details>
<summary>摘要</summary>
科学视觉和文档中的标签是非常重要的，但现有的科学标签生成方法依然存在一些缺陷，如帮助度、解释性和视觉描述性等指标不够高。这些缺陷导致生成的标签与读者需求不匹配。为了生成高质量的标签，我们提出了一种新的figure-to-caption生成框架，可以根据领域专家反馈来优化标签，以满足读者需求。我们的框架包括以下两个部分：1. 一种自动评估figure-to-caption对的质量方法。2. 一种基于人工反馈的强化学习（RLHF）方法，用于优化一个生成figure-to-caption模型，以满足读者需求。我们的简单学习框架可以在不同类型的模型上提高性能，特别是当使用BLIP作为基础模型时，我们的RLHF框架可以 achieve a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. finally，我们发布了一个大规模的人工反馈 benchmark dataset，以便进一步评估和发展RLHF技术。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Conversational-Shaping-for-Intelligent-Agents"><a href="#Adversarial-Conversational-Shaping-for-Intelligent-Agents" class="headerlink" title="Adversarial Conversational Shaping for Intelligent Agents"></a>Adversarial Conversational Shaping for Intelligent Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11785">http://arxiv.org/abs/2307.11785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Tarasiewicz, Sultan Kenjeyev, Ilana Sebag, Shehab Alshehabi</li>
<li>for: 提高对话机器人的智能性和准确性</li>
<li>methods: 使用生成对抗网络（GANPG）和奖励每个生成步骤（REGS）模型，并在强化学习框架中使用 seq2seq 和 transformers 等不同的训练细节</li>
<li>results: 研究表明，使用 GANPG 和 REGS 模型可以提高对话机器人的对话能力和准确性，并且不同的训练细节可以影响模型的性能<details>
<summary>Abstract</summary>
The recent emergence of deep learning methods has enabled the research community to achieve state-of-the art results in several domains including natural language processing. However, the current robocall system remains unstable and inaccurate: text generator and chat-bots can be tedious and misunderstand human-like dialogue. In this work, we study the performance of two models able to enhance an intelligent conversational agent through adversarial conversational shaping: a generative adversarial network with policy gradient (GANPG) and a generative adversarial network with reward for every generation step (REGS) based on the REGS model presented in Li et al. [18] . This model is able to assign rewards to both partially and fully generated text sequences. We discuss performance with different training details : seq2seq [ 36] and transformers [37 ] in a reinforcement learning framework.
</details>
<details>
<summary>摘要</summary>
Recent advances in deep learning methods have enabled the research community to achieve state-of-the-art results in various domains, including natural language processing. However, the current robocall system remains unstable and inaccurate, with text generators and chatbots often producing tedious and inhuman-like dialogue. In this study, we evaluate the performance of two models that can enhance an intelligent conversational agent through adversarial conversational shaping: a generative adversarial network with policy gradient (GANPG) and a generative adversarial network with reward for every generation step (REGS) based on the REGS model presented in Li et al. [18] . This model can assign rewards to both partially and fully generated text sequences. We discuss the performance of these models with different training details, such as seq2seq [36] and transformers [37], in a reinforcement learning framework.
</details></li>
</ul>
<hr>
<h2 id="Yelp-Reviews-and-Food-Types-A-Comparative-Analysis-of-Ratings-Sentiments-and-Topics"><a href="#Yelp-Reviews-and-Food-Types-A-Comparative-Analysis-of-Ratings-Sentiments-and-Topics" class="headerlink" title="Yelp Reviews and Food Types: A Comparative Analysis of Ratings, Sentiments, and Topics"></a>Yelp Reviews and Food Types: A Comparative Analysis of Ratings, Sentiments, and Topics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10826">http://arxiv.org/abs/2307.10826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenyu Liao, Yiqing Shi, Yujia Hu, Wei Quan</li>
<li>for: 这项研究探讨了 Yelp 评论与食品类型之间的关系，并研究评论中的评分、情感和话题如何随食品类型而变化。</li>
<li>methods: 研究使用了评论分析和机器学习模型来描述评论中的话题，并将食品类型分为四个群组基于评分和情感。</li>
<li>results: 研究发现，一些食品类型的评分、情感和话题呈现相似的特征，而其他类型则具有明显的特征。 评论者对不同类型的食品进行评论时，往往会关注不同的话题。<details>
<summary>Abstract</summary>
This study examines the relationship between Yelp reviews and food types, investigating how ratings, sentiments, and topics vary across different types of food. Specifically, we analyze how ratings and sentiments of reviews vary across food types, cluster food types based on ratings and sentiments, infer review topics using machine learning models, and compare topic distributions among different food types. Our analyses reveal that some food types have similar ratings, sentiments, and topics distributions, while others have distinct patterns. We identify four clusters of food types based on ratings and sentiments and find that reviewers tend to focus on different topics when reviewing certain food types. These findings have important implications for understanding user behavior and cultural influence on digital media platforms and promoting cross-cultural understanding and appreciation.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这个研究研究了Yelp评论和食品类型之间的关系，具体来说是检查评论中的评分和情感是如何随食品类型而变化。我们分析了评论中的评分和情感是如何随食品类型而变化，使用机器学习模型推断评论中的话题，并比较不同食品类型中的话题分布。我们的分析发现，一些食品类型的评分和情感都很相似，而其他些类型则有明显的差异。我们将食品类型分为四个群组 based on ratings and sentiments，并发现在评论某些食品类型时，评论者们更关注的话题不同。这些发现有关于用户行为和文化影响在数字媒体平台上的理解，以及促进跨文化理解和喜爱的重要意义。
</details></li>
</ul>
<hr>
<h2 id="Cross-Corpus-Multilingual-Speech-Emotion-Recognition-Amharic-vs-Other-Languages"><a href="#Cross-Corpus-Multilingual-Speech-Emotion-Recognition-Amharic-vs-Other-Languages" class="headerlink" title="Cross-Corpus Multilingual Speech Emotion Recognition: Amharic vs. Other Languages"></a>Cross-Corpus Multilingual Speech Emotion Recognition: Amharic vs. Other Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10814">http://arxiv.org/abs/2307.10814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ephrem Afele Retta, Richard Sutcliffe, Jabar Mahmood, Michael Abebe Berwo, Eiad Almekhlafi, Sajjad Ahmed Khan, Shehzad Ashraf Chaudhry, Mustafa Mhamed, Jun Feng</li>
<li>for: 这项研究是为了解决缺乏语言资源的情感识别 зада务。</li>
<li>methods: 这项研究使用了跨语言和多语言的情感识别方法，使用了阿姆哈里语、英语、德语和乌尔都语的数据集。</li>
<li>results: 研究发现，使用英语或德语作为源语言，并将其转换到阿姆哈里语为目标语言，可以获得最佳效果。此外，使用多种非阿姆哈里语言进行训练，可以获得更高的准确率。<details>
<summary>Abstract</summary>
In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language. However, where training data for a language does not exist, data from other languages can be used instead. We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU. For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED). For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets. We followed previous research in mapping labels for all datasets to just two classes, positive and negative. Thus we can compare performance on different languages directly, and combine languages for training and testing. In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of VGG), and ResNet50. Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and English SER are equally difficult. Similarly, German SER is more difficult, and Urdu SER is easier. In Experiment 2, we trained on one language and tested on another, in both directions for each pair: Amharic<->German, Amharic<->English, and Amharic<->Urdu. Results with Amharic as target suggested that using English or German as source will give the best result. In Experiment 3, we trained on several non-Amharic languages and then tested on Amharic. The best accuracy obtained was several percent greater than the best accuracy in Experiment 2, suggesting that a better result can be obtained when using two or three non-Amharic languages for training than when using just one non-Amharic language. Overall, the results suggest that cross-lingual and multilingual training can be an effective strategy for training a SER classifier when resources for a language are scarce.
</details>
<details>
<summary>摘要</summary>
传统的语音情感识别（SER）任务中，一个分类器会在一种语言的已有数据集上进行训练。然而，当数据集不存在时，可以使用其他语言的数据集。我们在阿姆哈里亚语、英语、德语和 Urdu 语言上进行了实验，使用我们自己的公共可用的阿姆哈里亚语 Speech Emotion 数据集（ASED），以及现有的 RAVDESS、EMO-DB 和 URDU 数据集。我们按照之前的研究方法，将所有数据集的标签映射到两个类别中，即正面和负面。这样我们可以直接比较不同语言的性能，并将不同语言组合在训练和测试中。在实验 1 中，我们使用了三个模型：AlexNet、VGGE 和 ResNet50，进行了单语言 SER 试验。结果表明，ASED 和 RAVDESS 的性能很相似， suggesting that Amharic 和 English SER 是等效的。同时，德语 SER 更加困难，而 Urdu SER 更加容易。在实验 2 中，我们将一种语言作为输入，并将另一种语言作为目标进行测试，在每个对的两个方向上进行了测试。结果表明，使用英语或德语作为源语言，可以获得最好的结果。在实验 3 中，我们将多种非阿姆哈里亚语言作为训练数据，然后测试在阿姆哈里亚语言上。最好的准确率比实验 2 中的最好准确率高出几个百分点，表明使用两三种非阿姆哈里亚语言进行训练可以获得更好的结果。总的来说，结果表明，跨语言和多语言训练是一种有效的方法，当语言资源匮乏时。
</details></li>
</ul>
<hr>
<h2 id="Layer-wise-Representation-Fusion-for-Compositional-Generalization"><a href="#Layer-wise-Representation-Fusion-for-Compositional-Generalization" class="headerlink" title="Layer-wise Representation Fusion for Compositional Generalization"></a>Layer-wise Representation Fusion for Compositional Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10799">http://arxiv.org/abs/2307.10799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yafang Zheng, Lei Lin, Zhaohong Lai, Binling Wang, Shan Liu, Biao Fu, Wenhao Rao, Peigen Ye, Yidong Chen, Xiaodong Shi</li>
<li>for: 提高序列模型的可compose普遍性，即使在各种应用场景中已经取得了成功，但是这些模型的解决方案被指控为不具有人类化普遍性。</li>
<li>methods: 我们提出了一种名为\textsc{FuSion}的扩展，它通过在编码和解码过程中引入一个\emph{融合注意模块}来适当地融合前几层信息。</li>
<li>results: 我们在两个实际的 benchmark 上测试了\textsc{FuSion}，得到了竞争力和even state-of-the-art 的结果，这种结果证明了我们的提议的有效性。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Despite successes across a broad range of applications, sequence-to-sequence models' construct of solutions are argued to be less compositional than human-like generalization. There is mounting evidence that one of the reasons hindering compositional generalization is representations of the encoder and decoder uppermost layer are entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing token-level semantic information to alleviate the representations entanglement problem, rather than composing and using the syntactic and semantic representations of sequences appropriately as humans do. In addition, we explain why the entanglement problem exists from the perspective of recent studies about training deeper Transformer, mainly owing to the ``shallow'' residual connections and its simple, one-step operations, which fails to fuse previous layers' information effectively. Starting from this finding and inspired by humans' strategies, we propose \textsc{FuSion} (\textbf{Fu}sing \textbf{S}yntactic and Semant\textbf{i}c Representati\textbf{on}s), an extension to sequence-to-sequence models to learn to fuse previous layers' information back into the encoding and decoding process appropriately through introducing a \emph{fuse-attention module} at each encoder and decoder layer. \textsc{FuSion} achieves competitive and even \textbf{state-of-the-art} results on two realistic benchmarks, which empirically demonstrates the effectiveness of our proposal.
</details>
<details>
<summary>摘要</summary>
不withstanding its success across a broad range of applications, sequence-to-sequence models的构建方法受到了人类化泛化的批评，其中一个原因是编码和解码层的表示不能正确地分离。即序列的语法和 semantics表示被不当地杂mix。然而，大多数前一些研究主要集中在增强токен级别的 semantic信息，以降低表示杂mix问题，而不是正确地使用序列的语法和 semantics表示。此外，我们解释了表示杂mix问题的起因，即由于 recient studies about training deeper Transformer模型，主要归因于“浅”的径向连接和简单的一步操作，无法有效地融合上一层的信息。从这个发现出发，我们提出了\textsc{FuSion}（ Fu 合并 Syn 统和 Sem antics 表示），一种基于 sequence-to-sequence 模型的扩展，通过引入一个“融合注意模块”来在编码和解码过程中正确地融合上一层的信息。\textsc{FuSion}在两个实际的 benchmark 上实现了竞争力和even state-of-the-art 的结果，这使得我们的提议得到了实质性的证明。
</details></li>
</ul>
<hr>
<h2 id="Extreme-Multi-Label-Skill-Extraction-Training-using-Large-Language-Models"><a href="#Extreme-Multi-Label-Skill-Extraction-Training-using-Large-Language-Models" class="headerlink" title="Extreme Multi-Label Skill Extraction Training using Large Language Models"></a>Extreme Multi-Label Skill Extraction Training using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10778">http://arxiv.org/abs/2307.10778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens-Joris Decorte, Severine Verlinden, Jeroen Van Hautte, Johannes Deleu, Chris Develder, Thomas Demeester</li>
<li>for: 本研究旨在提高在线招聘广告中技能的自动检测精度，以便在劳动市场分析和电子招聘过程中更好地了解技能需求。</li>
<li>methods: 本研究使用自然语言处理（NLP）技术来自动处理在线招聘广告，并利用大量技能ontology来链接技能。</li>
<li>results: 研究结果显示，使用与文本相关的抽象技能生成器和对比学习策略可以提高技能检测精度，并在三个技能检测标准测试数据集上显示出15-25%的提高。<details>
<summary>Abstract</summary>
Online job ads serve as a valuable source of information for skill requirements, playing a crucial role in labor market analysis and e-recruitment processes. Since such ads are typically formatted in free text, natural language processing (NLP) technologies are required to automatically process them. We specifically focus on the task of detecting skills (mentioned literally, or implicitly described) and linking them to a large skill ontology, making it a challenging case of extreme multi-label classification (XMLC). Given that there is no sizable labeled (training) dataset are available for this specific XMLC task, we propose techniques to leverage general Large Language Models (LLMs). We describe a cost-effective approach to generate an accurate, fully synthetic labeled dataset for skill extraction, and present a contrastive learning strategy that proves effective in the task. Our results across three skill extraction benchmarks show a consistent increase of between 15 to 25 percentage points in \textit{R-Precision@5} compared to previously published results that relied solely on distant supervision through literal matches.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在线职位广告 serve as a valuable source of information for skill requirements, playing a crucial role in labor market analysis and e-recruitment processes. 自然语言处理（NLP）技术被用来自动处理这些广告，特别是在检测技能（直接或间接描述）并将其链接到大量技能 ontology 中，这是一种EXTREME multi-label classification（XMLC）任务。由于没有可用的大规模标注（训练）数据集，我们提出了利用通用 Large Language Models（LLMs）的技术。我们描述了一种经济可行的方法来生成准确的、完全 sintetic 标注数据集，并提出了一种对比学习策略，其在任务中证明有效。我们在三个技能抽取benchmark上得到的结果表明，我们的方法可以与之前基于Literal Matches的结果相比，提高R-Precision@5的准确率（15%-25%）。
</details></li>
</ul>
<hr>
<h2 id="Vesper-A-Compact-and-Effective-Pretrained-Model-for-Speech-Emotion-Recognition"><a href="#Vesper-A-Compact-and-Effective-Pretrained-Model-for-Speech-Emotion-Recognition" class="headerlink" title="Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition"></a>Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10757">http://arxiv.org/abs/2307.10757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/happycolor/vesper">https://github.com/happycolor/vesper</a></li>
<li>paper_authors: Weidong Chen, Xiaofen Xing, Peihao Chen, Xiangmin Xu</li>
<li>for: 这个论文提出了一种适应大规模预训练模型（PTM）到语音情感识别任务的 paradigm。</li>
<li>methods: 该论文提出了一种基于 WavLM 的语音 dataset 的听说预训练 encoder 的改进方法，称为 Vesper。 Vesper 采用了情感导向的面积层束策略，以提高对情感信息的敏感度。</li>
<li>results: 实验结果表明，与 WavLM Base 的 12 层模型相比，Vesper  WITH 4 层模型在 IEMOCAP、MELD 和 CREMA-D  datasets 上表现出色，而 Vesper  WITH 12 层模型则超越了 WavLM Large 的 24 层模型。<details>
<summary>Abstract</summary>
This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task. Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved. Additionally, employing PTMs in practical applications can be challenging due to their considerable size. Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective. In this paper, we focus on the speech emotion recognition task and propose an improved emotion-specific pretrained encoder called Vesper. Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Subsequently, Vesper employs hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, both of which are crucial for emotion recognition. Experimental results on the IEMOCAP, MELD, and CREMA-D datasets demonstrate that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers.
</details>
<details>
<summary>摘要</summary>
The authors focus on the speech emotion recognition task and propose an improved emotion-specific pre-trained encoder called Vesper. Vesper is pre-trained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Additionally, Vesper uses hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, which are crucial for emotion recognition.The authors evaluate Vesper on the IEMOCAP, MELD, and CREMA-D datasets and compare its performance to that of WavLM Base and WavLM Large. The results show that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers.In summary, this paper presents a new approach to speech emotion recognition that adapts large-scale pre-trained models to the specific task of speech emotion recognition. The proposed model, Vesper, is pre-trained on a speech dataset based on WavLM and employs an emotion-guided masking strategy and hierarchical self-supervision to improve its performance. The authors evaluate Vesper on three datasets and show that it outperforms WavLM Base and WavLM Large.
</details></li>
</ul>
<hr>
<h2 id="Large-language-models-shape-and-are-shaped-by-society-A-survey-of-arXiv-publication-patterns"><a href="#Large-language-models-shape-and-are-shaped-by-society-A-survey-of-arXiv-publication-patterns" class="headerlink" title="Large language models shape and are shaped by society: A survey of arXiv publication patterns"></a>Large language models shape and are shaped by society: A survey of arXiv publication patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10700">http://arxiv.org/abs/2307.10700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajiv Movva, Sidhika Balachandar, Kenny Peng, Gabriel Agostini, Nikhil Garg, Emma Pierson</li>
<li>for: 本研究目的是分析大型自然语言模型（LLM）论文的发展趋势，特别是2023年 vs. 2018-2022年的发表模式。</li>
<li>methods: 该研究基于388K篇CS和Stat arXiv上的论文，分析了LLM相关论文的发展趋势，包括论文数量的增加、主题的分布、作者的背景和研究方向的相关性、引用率的分布、国际合作的趋势等方面。</li>
<li>results: 研究发现，LLM研究在社会影响方面呈现18倍增长趋势，新参与LLM研究的作者更likely关注应用和社会影响，而经验较深的作者则更关注理论和基础研究。此外，研究还发现了性别和学术&#x2F;产业领域的差异，以及美国和中国在合作网络中的分歧。总的来说，本研究证明了LLM研究不仅被社会 shapes，而且也 shapes society。<details>
<summary>Abstract</summary>
There has been a steep recent increase in the number of large language model (LLM) papers, producing a dramatic shift in the scientific landscape which remains largely undocumented through bibliometric analysis. Here, we analyze 388K papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 vs. 2018-2022. We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration. We show that LLM research increasingly focuses on societal impacts: there has been an 18x increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors newly publishing on LLMs are more likely to focus on applications and societal impacts than more experienced authors. LLM research is also shaped by social dynamics: we document gender and academic/industry disparities in the topics LLM authors focus on, and a US/China schism in the collaboration network. Overall, our analysis documents the profound ways in which LLM research both shapes and is shaped by society, attesting to the necessity of sociotechnical lenses.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: Recently, there has been a sharp increase in the number of large language model (LLM) papers, leading to a significant shift in the scientific landscape, but this has been largely undocumented through bibliometric analysis. In this study, we analyze 388,000 papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 compared to 2018-2022. We examine how the proportion of LLM papers is increasing, which LLM-related topics are receiving the most attention, and the authors writing LLM papers. We also explore how authors' research topics correlate with their backgrounds, the factors that distinguish highly cited LLM papers, and the patterns of international collaboration. Our findings show that LLM research is increasingly focused on societal impacts: there has been an 18-fold increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors who are new to LLM research are more likely to focus on applications and societal impacts than more experienced authors. LLM research is also influenced by social dynamics, such as gender and academic/industry disparities in the topics LLM authors focus on, and a US/China schism in the collaboration network. Overall, our analysis demonstrates the profound ways in which LLM research both shapes and is shaped by society, highlighting the importance of sociotechnical lenses.
</details></li>
</ul>
<hr>
<h2 id="A-Dataset-and-Strong-Baselines-for-Classification-of-Czech-News-Texts"><a href="#A-Dataset-and-Strong-Baselines-for-Classification-of-Czech-News-Texts" class="headerlink" title="A Dataset and Strong Baselines for Classification of Czech News Texts"></a>A Dataset and Strong Baselines for Classification of Czech News Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10666">http://arxiv.org/abs/2307.10666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hynky1999/czech-news-classification-dataset">https://github.com/hynky1999/czech-news-classification-dataset</a></li>
<li>paper_authors: Hynek Kydlíček, Jindřich Libovický</li>
<li>for: 评估捷克自然语言处理模型的可行性，通过使用多种新闻来源和多个新闻类别，以及推测作者的性别和日期等四个分类任务。</li>
<li>methods: 使用各种先进的自然语言处理技术和大规模生成语言模型进行评估。</li>
<li>results: 人工评估表明机器学习基于预训练变换器模型的性能落后人类表现，而语言特定预训练Encoder分析表现胜过选择的商业化大规模生成语言模型。<details>
<summary>Abstract</summary>
Pre-trained models for Czech Natural Language Processing are often evaluated on purely linguistic tasks (POS tagging, parsing, NER) and relatively simple classification tasks such as sentiment classification or article classification from a single news source. As an alternative, we present CZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech classification datasets, composed of news articles from various sources spanning over twenty years, which allows a more rigorous evaluation of such models. We define four classification tasks: news source, news category, inferred author's gender, and day of the week. To verify the task difficulty, we conducted a human evaluation, which revealed that human performance lags behind strong machine-learning baselines built upon pre-trained transformer models. Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available large-scale generative language models.
</details>
<details>
<summary>摘要</summary>
很多预训练模型在捷克自然语言处理领域通常会在语言学任务（POS标记、分析、NER）和简单的分类任务（情感分类或文章分类）中进行评估。作为一个 alternatif，我们介绍了 CZEch~NEws~Classification~dataset（CZE-NEC），这是一个包含新闻文章来源于多种新闻来源，覆盖了两十年的大型捷克分类数据集。这个数据集允许更加严格地评估这些模型。我们定义了四个分类任务：新闻来源、新闻类别、推测作者的性别和天数。为了证明任务的困难，我们进行了人类评估，发现人类表现落后于基于预训练变换器模型的强大机器学习基线。此外，我们还表明语言特定预训练Encoder分析的语言模型在选择的大规模生成语言模型中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Landscape-of-Natural-Language-Processing-Research"><a href="#Exploring-the-Landscape-of-Natural-Language-Processing-Research" class="headerlink" title="Exploring the Landscape of Natural Language Processing Research"></a>Exploring the Landscape of Natural Language Processing Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10652">http://arxiv.org/abs/2307.10652</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sebischair/exploring-nlp-research">https://github.com/sebischair/exploring-nlp-research</a></li>
<li>paper_authors: Tim Schopf, Karim Arabi, Florian Matthes</li>
<li>for: 本研究旨在提供一份系统性地分类和分析ACL Anthology中的NLP研究论文，以提供研究领域的结构化概述、领域分类、最新发展和未来研究方向。</li>
<li>methods: 本研究使用系统性的分类和分析方法，对ACL Anthology中的NLP研究论文进行了分类和分析，从而提供了研究领域的结构化概述、领域分类和最新发展。</li>
<li>results: 本研究结果显示，NLP领域的研究主要涉及到语义理解、语言模型、自然语言处理、语音识别等领域，并且在最新的发展中，深度学习、word embeddings等技术在NLP领域中得到了广泛的应用和发展。<details>
<summary>Abstract</summary>
As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent. Contributing to closing this gap, we have systematically classified and analyzed research papers in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields of study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.
</details>
<details>
<summary>摘要</summary>
natural language processing (NLP) 研究在最近几年内展示了快速扩散和广泛采用的趋势。随着研究工作的增加，NLP相关领域的研究也逐渐增多。然而，一个全面的研究，把已有的主题分类、趋势识别出来，并对未来研究领域提出建议，仍然缺失。为了填补这一漏洞，我们在ACL Anthology中 sistematically 分类和分析了研究论文。以下是我们的结果：1. 研究领域分类：我们对NLP研究领域进行了系统分类，并将其分为多个子领域。2. 趋势分析：我们分析了最近几年NLP研究的趋势，并对其进行了总结。3. 研究成果概述：我们对NLP研究成果进行了概述，并提出了未来研究的建议。以下是我们的发现：1. 在NLP领域，最近几年内有很多新的研究方向出现，如语义理解、语言生成、机器翻译等。2. 许多研究都在尝试将NLP应用于实际场景中，如语音识别、自然语言处理等。3. 随着数据集的不断扩大和改进，NLP模型的性能也在不断提高。以上是我们对NLP研究领域的一个系统性的分析和概述。未来，我们可能会看到更多的新的研究方向和应用场景出现，同时，我们也需要继续关注NLP领域的发展和进步。
</details></li>
</ul>
<hr>
<h2 id="Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes"><a href="#Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes" class="headerlink" title="Generative Language Models on Nucleotide Sequences of Human Genes"></a>Generative Language Models on Nucleotide Sequences of Human Genes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10634">http://arxiv.org/abs/2307.10634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boun-tabi/generativelm-genes">https://github.com/boun-tabi/generativelm-genes</a></li>
<li>paper_authors: Musa Nuri Ihtiyar, Arzucan Ozgur</li>
<li>for: 本研究旨在开发一个基于转换器的生成语言模型，以探讨DNA序列生成的可能性。</li>
<li>methods: 研究使用了RNN和N-gram等简单技术，以及一些实际生活中的任务来评估模型性能。</li>
<li>results: 研究发现，使用生成模型可以在DNA序列生成中 дости得比较好的效果，但是数据充足性仍然是一个问题。<details>
<summary>Abstract</summary>
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotides without losing much information and making too much simplification. First of all, we systematically examined an almost entirely unexplored problem and observed that RNNs performed the best while simple techniques like N-grams were also promising. Another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural language. How essential using real-life tasks beyond the classical metrics such as perplexity is observed. Furthermore, checking whether the data-hungry nature of these models can be changed through selecting a language with minimal vocabulary size, four owing to four different types of nucleotides, is examined. The reason for reviewing this was that choosing such a language might make the problem easier. However, what we observed in this study was it did not provide that much of a change in the amount of data needed.
</details>
<details>
<summary>摘要</summary>
语言模型，主要是基于转换器的一种，在NLU和NLG等领域取得了巨大的成功。DNasekwalence structure和自然语言很相似，因此DNABert等探索性模型在生物信息学领域具有重要意义。然而，生成方面尚未得到充分的探索，我们因此决定开发一种基于GPT-3的探索性生成语言模型，专门针对DNasekwalence。由于处理整个DNasekwalence的计算资源需求很高，我们决定对小规模的NUcleotide序列进行研究，而不是整个DNasekwalence。这种决定并没有改变问题结构，因为DNasekwalence和NUcleotide序列都可以看作1D序列，由四种不同的核苷酸组成，无需失去太多信息和简化太多。我们首先系统地探讨了几乎未曾被探索的问题，并发现RNNs表现最佳，而简单的技术如N-grams也表现了良好。此外，我们发现在使用生成模型时，不需要了解语言，与自然语言不同。此外，我们发现通过使用实际任务而不仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅
</details></li>
</ul>
<hr>
<h2 id="Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa"><a href="#Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa" class="headerlink" title="Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa"></a>Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10633">http://arxiv.org/abs/2307.10633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shriyash K. Upadhyay, Etan J. Ginsberg</li>
<li>for: 提高语言模型的可用性和性能</li>
<li>methods: 多种方法自动训练</li>
<li>results: 1) 提高较弱方法性能（最高提升30%），2) 提高较强方法性能（最高提升32.2%），3) 提高相关 yet distinct tasks的性能（最高提升10.3%）Here’s a breakdown of each point:</li>
<li>for: The paper aims to improve the availability and performance of language models by introducing a novel training method called Multi-Method Self-Training (MMST).</li>
<li>methods: The paper uses multiple methods for self-training, including the filtered outputs of another method, to augment the strengths and ameliorate the weaknesses of each method.</li>
<li>results: The paper shows that MMST can improve the performance of less performant methods (up to 30%), more performant methods (up to 32.2%), and related yet distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. The improvement in performance is driven by the use of multiple methods, and the paper also explores prompt-engineering and anti-correlated performance between methods as means of making MMST more effective.<details>
<summary>Abstract</summary>
Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods. We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective. We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. It is written using the same characters as Traditional Chinese, but with some differences in stroke order and vocabulary.Here are some key differences between Simplified Chinese and Traditional Chinese:* Simplified Chinese has fewer characters than Traditional Chinese, with about 2,000 commonly used characters compared to over 5,000 in Traditional Chinese.* Simplified Chinese has simpler stroke order and character forms, making it easier to write and read.* Simplified Chinese uses more homophones, which can make it more difficult to understand for non-native speakers.* Simplified Chinese has a more standardized vocabulary and grammar, while Traditional Chinese has more regional variations and idiomatic expressions.I hope this helps! Let me know if you have any other questions.
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Dive-into-the-Disparity-of-Word-Error-Rates-Across-Thousands-of-NPTEL-MOOC-Videos"><a href="#A-Deep-Dive-into-the-Disparity-of-Word-Error-Rates-Across-Thousands-of-NPTEL-MOOC-Videos" class="headerlink" title="A Deep Dive into the Disparity of Word Error Rates Across Thousands of NPTEL MOOC Videos"></a>A Deep Dive into the Disparity of Word Error Rates Across Thousands of NPTEL MOOC Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10587">http://arxiv.org/abs/2307.10587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Kumar Rai, Siddharth D Jaiswal, Animesh Mukherjee</li>
<li>for: 这个论文是为了评估自动语音识别系统在不同地区和民族群体中的性能，并提出了更加包容和可靠的ASR系统和数据集。</li>
<li>methods: 该论文使用了大量的NPTEL MOOC平台上的技术讲解视频和译文，并使用了YouTube自动字幕和OpenAI Whisper模型来评估印度各地区和民族群体的语音特征对自动语音识别系统的影响。</li>
<li>results: 研究发现，存在gender、native region、age和speech rate等因素导致自动语音识别系统的性能差异，但cast不存在差异。此外，研究还发现了不同讲解领域的语音特征差异，这些结果表明需要更加包容和可靠的ASR系统和更多的 represenative 数据集来评估差异。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems are designed to transcribe spoken language into written text and find utility in a variety of applications including voice assistants and transcription services. However, it has been observed that state-of-the-art ASR systems which deliver impressive benchmark results, struggle with speakers of certain regions or demographics due to variation in their speech properties. In this work, we describe the curation of a massive speech dataset of 8740 hours consisting of $\sim9.8$K technical lectures in the English language along with their transcripts delivered by instructors representing various parts of Indian demography. The dataset is sourced from the very popular NPTEL MOOC platform. We use the curated dataset to measure the existing disparity in YouTube Automatic Captions and OpenAI Whisper model performance across the diverse demographic traits of speakers in India. While there exists disparity due to gender, native region, age and speech rate of speakers, disparity based on caste is non-existent. We also observe statistically significant disparity across the disciplines of the lectures. These results indicate the need of more inclusive and robust ASR systems and more representational datasets for disparity evaluation in them.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Instruction-following-Evaluation-through-Verbalizer-Manipulation"><a href="#Instruction-following-Evaluation-through-Verbalizer-Manipulation" class="headerlink" title="Instruction-following Evaluation through Verbalizer Manipulation"></a>Instruction-following Evaluation through Verbalizer Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10558">http://arxiv.org/abs/2307.10558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, Hongxia Jin</li>
<li>For: The paper aims to evaluate the ability of instruction-tuned language models to follow instructions accurately, particularly in responding to less natural or unexpected instructions.* Methods: The proposed evaluation protocol is called verbalizer manipulation, which instructs the model to verbalize the task label with words that align with the model’s priors to different extents. This protocol can be integrated with any classification benchmark to assess the model’s reliance on priors and its ability to override them.* Results: The evaluation results show that the instruction-following abilities of different model families and scales are significantly distinguished by their performance on less natural verbalizers. Even the strongest GPT-4 model struggles to perform better than random guessing on the most challenging verbalizer, highlighting the need for further advancements to improve their instruction-following abilities.Here’s the Chinese translation of the three key points:* For: 这篇论文旨在评估指令驱动的自然语言处理模型是否能够准确遵循指令。* Methods: 该论文提出了一种新的评估协议，即使语言模型的任务标签用语。这种协议可以让模型根据不同的扩展来采用不同的语言表达方式，以评估模型是否能够准确遵循指令。* Results: 评估结果显示，不同的模型家族和规模在遵循指令的能力上存在显著的差异，尤其是在使用不太自然的语言表达时。即使使用最强的GPT-4模型，它也难以在最Difficult的语言表达下超过随机猜测的水平，这 highlights the need for continued advancements to improve their instruction-following abilities。<details>
<summary>Abstract</summary>
While instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. Existing benchmarks primarily focus on common instructions that align well with what the model learned during training. However, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. In this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting ``postive'' for positive sentiment), to minimally aligned (e.g., outputting ``negative'' for positive sentiment). Verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model's reliance on priors and its ability to override them to accurately follow the instructions. We conduct a comprehensive evaluation of four major model families across nine datasets, employing twelve sets of verbalizers for each of them. We observe that the instruction-following abilities of models, across different families and scales, are significantly distinguished by their performance on less natural verbalizers. Even the strongest GPT-4 model struggles to perform better than random guessing on the most challenging verbalizer, emphasizing the need for continued advancements to improve their instruction-following abilities.
</details>
<details>
<summary>摘要</summary>
而ん行模型在不同的自然语言处理任务上有显著的成功，但确切评估它们能否按照指令行为仍然是一个挑战。现有的标准benchmark主要集中在训练中学习的指令上，但是能够准确回应这些指令并不意味着强大的指令遵从能力。在这篇论文中，我们提出了一种新的指令遵从评估协议，即语言映射（Verbalizer Manipulation）。它要求模型将任务标签用与模型在训练中学习的词汇进行映射，从高度相似（例如，输出“正面” для正面情感）到最低度相似（例如，输出“负面” для正面情感）。语言映射可以轻松地与任何分类benchmark集成，以检验模型是否能够根据指令而准确遵从。我们在九个数据集上进行了四家主要模型家族的全面评估，使用每个模型的十二个语言映射。我们发现，不同家族和规模的模型在使用不同的语言映射时，其指令遵从能力异常分化。 zelfs the strongest GPT-4 model struggles to perform better than random guessing on the most challenging verbalizer，这 подчеркивает我们需要继续进行技术创新，以提高模型的指令遵从能力。
</details></li>
</ul>
<hr>
<h2 id="Gender-tuning-Empowering-Fine-tuning-for-Debiasing-Pre-trained-Language-Models"><a href="#Gender-tuning-Empowering-Fine-tuning-for-Debiasing-Pre-trained-Language-Models" class="headerlink" title="Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models"></a>Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10522">http://arxiv.org/abs/2307.10522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi, Radames Cruz Moreno, Hamed Khanpour</li>
<li>for: 降低 PLM 中的社会偏见，提高 PLM 的性别准确率。</li>
<li>methods: 提出了 Gender-tuning 方法，通过在下游任务的数据集上进行粘贴语言模型（MLM）训练目标的综合使用，以降低 PLM 中的社会偏见。</li>
<li>results: Gender-tuning 方法可以在 PLM 中减少平均性别偏见得分，同时提高 PLM 在下游任务上的性能，不需要额外的训练数据集和资源投入。<details>
<summary>Abstract</summary>
Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Transsion-TSUP’s-speech-recognition-system-for-ASRU-2023-MADASR-Challenge"><a href="#Transsion-TSUP’s-speech-recognition-system-for-ASRU-2023-MADASR-Challenge" class="headerlink" title="Transsion TSUP’s speech recognition system for ASRU 2023 MADASR Challenge"></a>Transsion TSUP’s speech recognition system for ASRU 2023 MADASR Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11778">http://arxiv.org/abs/2307.11778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Li, Gaosheng Zhang, An Zhu, Weiyong Li, Shuming Fang, Xiaoyue Yang, Jianchao Zhu</li>
<li>For: 这篇论文描述了由Transsion Speech Understanding Processing Team (TSUP)开发的一种扩展ASR模型，用于ASRU 2023 MADASR Challenge。该系统强调适应低资源印度语言的ASR模型，并覆盖了挑战赛的四个轨道。* Methods: 在轨道1和2中，音响模型使用了压缩形态器编码器和双向转换器解码器，并在CTC-Attention培育训练中使用了共同训练损失。此外，在TLG beam search解码中使用了外部KenLM语言模型。在轨道3和4中，采用了预训练的IndicWhisper模型，并在挑战数据集和公共可用数据集上进行了finetuning。另外，在喊叫搜索解码中支持了外部KenLM语言模型，以便更好地利用挑战中提供的额外文本。* Results: 提案的方法在四个轨道中取得了 Bengali语言的单词错误率（WER）为24.17%、24.43%、15.97%和15.97%，以及 Bhojpuri语言的WER为19.61%、19.54%、15.48%和15.48%。这些结果表明提案的方法的效果。<details>
<summary>Abstract</summary>
This paper presents a speech recognition system developed by the Transsion Speech Understanding Processing Team (TSUP) for the ASRU 2023 MADASR Challenge. The system focuses on adapting ASR models for low-resource Indian languages and covers all four tracks of the challenge. For tracks 1 and 2, the acoustic model utilized a squeezeformer encoder and bidirectional transformer decoder with joint CTC-Attention training loss. Additionally, an external KenLM language model was used during TLG beam search decoding. For tracks 3 and 4, pretrained IndicWhisper models were employed and finetuned on both the challenge dataset and publicly available datasets. The whisper beam search decoding was also modified to support an external KenLM language model, which enabled better utilization of the additional text provided by the challenge. The proposed method achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97% for Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%, and 15.48% for Bhojpuri language in the four tracks. These results demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
For tracks 1 and 2, the acoustic model used a squeezeformer encoder and bidirectional transformer decoder with joint CTC-Attention training loss. Additionally, an external KenLM language model was used during TLG beam search decoding.For tracks 3 and 4, pretrained IndicWhisper models were employed and finetuned on both the challenge dataset and publicly available datasets. The whisper beam search decoding was also modified to support an external KenLM language model, which enabled better utilization of the additional text provided by the challenge.The proposed method achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97% for Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%, and 15.48% for Bhojpuri language in the four tracks. These results demonstrate the effectiveness of the proposed method.
</details></li>
</ul>
<hr>
<h2 id="General-Debiasing-for-Multimodal-Sentiment-Analysis"><a href="#General-Debiasing-for-Multimodal-Sentiment-Analysis" class="headerlink" title="General Debiasing for Multimodal Sentiment Analysis"></a>General Debiasing for Multimodal Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10511">http://arxiv.org/abs/2307.10511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Teng-Sun/GEAR">https://github.com/Teng-Sun/GEAR</a></li>
<li>paper_authors: Teng Sun, Juntong Ni, Wenjie Wang, Liqiang Jing, Yinwei Wei, Liqiang Nie</li>
<li>for: 这个论文主要针对 Multimodal Sentiment Analysis (MSA) 领域的问题，即如何减少模型对偏扰关系的依赖性，以提高模型的 Out-Of-Distribution (OOD) 泛化能力。</li>
<li>methods: 该论文提出了一种通用的减少偏扰关系的框架，基于 Inverse Probability Weighting (IPW) 技术，可以适应不同的数据集和模型。这个框架包括两个主要步骤：1) 分解每个模式中的可靠特征和偏扰特征，2) 使用偏扰特征来估算样本的偏扰程度。最后，使用 IPW 技术来减少大偏扰样本的影响，以便学习有 robustness 的特征 для 情感预测。</li>
<li>results: 该论文通过使用多个 benchmark 和 OOD 测试集来评估模型的泛化能力，并证明了其在不同的数据集和模型下的超越性。 codes 和数据可以在 <a target="_blank" rel="noopener" href="https://github.com/Teng-Sun/GEAR">https://github.com/Teng-Sun/GEAR</a> 上下载。<details>
<summary>Abstract</summary>
Existing work on Multimodal Sentiment Analysis (MSA) utilizes multimodal information for prediction yet unavoidably suffers from fitting the spurious correlations between multimodal features and sentiment labels. For example, if most videos with a blue background have positive labels in a dataset, the model will rely on such correlations for prediction, while "blue background" is not a sentiment-related feature. To address this problem, we define a general debiasing MSA task, which aims to enhance the Out-Of-Distribution (OOD) generalization ability of MSA models by reducing their reliance on spurious correlations. To this end, we propose a general debiasing framework based on Inverse Probability Weighting (IPW), which adaptively assigns small weights to the samples with larger bias (i.e., the severer spurious correlations). The key to this debiasing framework is to estimate the bias of each sample, which is achieved by two steps: 1) disentangling the robust features and biased features in each modality, and 2) utilizing the biased features to estimate the bias. Finally, we employ IPW to reduce the effects of large-biased samples, facilitating robust feature learning for sentiment prediction. To examine the model's generalization ability, we keep the original testing sets on two benchmarks and additionally construct multiple unimodal and multimodal OOD testing sets. The empirical results demonstrate the superior generalization ability of our proposed framework. We have released the code and data to facilitate the reproduction https://github.com/Teng-Sun/GEAR.
</details>
<details>
<summary>摘要</summary>
现有的多模态情感分析（MSA）研究使用多模态信息进行预测，但是不可避免地受到多模态特征和情感标签之间的误 corrrelation的影响。例如，如果 datasets 中的大多数视频具有蓝色背景，模型就会依赖于这些 corrrelation 进行预测，而“蓝色背景”不是情感相关的特征。为解决这个问题，我们定义了一种总体debiasing MSA任务，它的目的是提高多模态情感分析模型的 Out-Of-Distribution（OOD）泛化能力，减少它们对误 corrrelation 的依赖。为此，我们提出了一种基于 inverse probability weighting（IPW）的通用debiasing框架。该框架可适应地将样本中具有更大偏见（即更强的误 corrrelation）的样本 assign 小权重。键点在于估计样本的偏见，我们通过以下两步来实现：1）在每种模式中分解robust feature和偏见 feature，2）利用偏见特征来估计样本的偏见。最后，我们利用 IPW 减少大偏见样本的影响，促进 Robust feature learning  для情感预测。为评估模型的泛化能力，我们保留原始测试集在两个 benchmark 上，并在多种单模态和多模态 OOD 测试集上进行评估。实际结果表明我们的提出的框架具有更高的泛化能力。我们已经将代码和数据发布到https://github.com/Teng-Sun/GEAR，以便复制。
</details></li>
</ul>
<hr>
<h2 id="SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval"><a href="#SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval" class="headerlink" title="SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval"></a>SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10488">http://arxiv.org/abs/2307.10488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thakur-nandan/sprint">https://github.com/thakur-nandan/sprint</a></li>
<li>paper_authors: Nandan Thakur, Kexin Wang, Iryna Gurevych, Jimmy Lin</li>
<li>for: 本研究的目的是提供一个Python工具包（SPRINT），用于评估神经稀疏检索模型。</li>
<li>methods: 本研究使用了Pyserini和Lucene创建了一个共同接口，用于支持多种神经稀疏检索模型的评估。现有五种内置模型：uniCOIL、DeepImpact、SPARTA、TILDEv2和SPLADEv2。用户也可以轻松地添加自定义模型，只需要定义权重方法即可。</li>
<li>results: 使用SPRINT工具包，我们在BEIR benchmark上建立了强大和可重复的零基eline神经稀疏检索基线。我们的结果显示，SPLADEv2在BEIR上的平均得分为0.470 nDCG@10，比其他神经稀疏检索模型高。此外，我们还发现了SPLADEv2的性能提升的原因，即它生成的稀疏表示中大多数的字符在查询和文档之外，这经常是其性能提升的关键。我们在<a target="_blank" rel="noopener" href="https://github.com/thakur-nandan/sprint%E4%B8%AD%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84SPRINT%E5%B7%A5%E5%85%B7%E5%8C%85%E3%80%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%AE%9E%E9%AA%8C%E6%89%80%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/thakur-nandan/sprint中公开了我们的SPRINT工具包、模型和实验所用的数据。</a><details>
<summary>Abstract</summary>
Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The toolkit currently includes five built-in models: uniCOIL, DeepImpact, SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by defining their term weighting method. Using our toolkit, we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers. In this work, we further uncover the reasons behind its performance gain. We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document which is often crucial for its performance gains, i.e. a limitation among its other sparse counterparts. We provide our SPRINT toolkit, models, and data used in our experiments publicly here at https://github.com/thakur-nandan/sprint.
</details>
<details>
<summary>摘要</summary>
传统上，稀疏检索系统都是基于 lexical representation来检索文档的，如BM25，这些系统在信息检索任务中占据了主导地位。 però，随着预训练 transformer 模型如 BERT 的出现， neural sparse retrieval 引入了一个新的 paradigm 内 Retrieval。 despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The toolkit currently includes five built-in models: uniCOIL, DeepImpact, SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by defining their term weighting method. Using our toolkit, we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers. In this work, we further uncover the reasons behind its performance gain. We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document which is often crucial for its performance gains, i.e. a limitation among its other sparse counterparts. We provide our SPRINT toolkit, models, and data used in our experiments publicly here at <https://github.com/thakur-nandan/sprint>.
</details></li>
</ul>
<hr>
<h2 id="FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models"><a href="#FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models" class="headerlink" title="FinGPT: Democratizing Internet-scale Data for Financial Large Language Models"></a>FinGPT: Democratizing Internet-scale Data for Financial Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10485">http://arxiv.org/abs/2307.10485</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4finance-foundation/fingpt">https://github.com/ai4finance-foundation/fingpt</a></li>
<li>paper_authors: Xiao-Yang Liu, Guoxuan Wang, Daochen Zha</li>
<li>For:  FinGPT aims to democratize FinLLMs and stimulate innovation in open finance by providing researchers and practitioners with accessible and transparent resources for developing their FinLLMs.* Methods: FinGPT uses an open-sourced and data-centric framework to automate the collection and curation of real-time financial data from &gt;34 diverse sources on the Internet. It also proposes a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). Additionally, it adopts the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from open-source general-purpose LLMs at a low cost.* Results: FinGPT showcases several applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. These applications demonstrate the potential of FinGPT in unlocking new opportunities in open finance.<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from >34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from open-source general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes are available at https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经表现出了人类语言理解和生成的惊人能力，这可能会革命化金融业。然而，现有的LLM通常在金融领域下手，这主要归结于通用文本数据和金融文本数据之间的差异。尽管存在只有有限的金融文本数据集 available（数据集较小），而且BloombergGPT，首个金融LLM（FinLLM），是关闭源的（只发布了训练日志）。为了普及互联网级金融数据 для LLM，这是一个开放的挑战，因为数据来源多样化，信号噪声比较低，时效性很高。为了解决这些挑战，我们提出了一个开源和数据中心的框架，名为金融生成预训练变换器（FinGPT）。FinGPT自动收集和筛选互联网上>34种多样化的金融数据源，为研究人员和实践者提供了可访问的和透明的资源，以便开发自己的FinLLM。此外，我们还提出了一种简单 yet有效的策略，用于FinLLM的追加训练，称为股票价格反馈学习（RLSP）。此外，我们采用了低级适应（LoRA，QLoRA）方法，允许用户自定义自己的FinLLM，从开源通用自然语言处理器（NLP）获得低成本。FinGPT还应用于多个领域，包括爬虫、情感分析 для算法交易、以及低代码开发。FinGPT旨在普及FinLLM，促进创新，解锁开放金融领域的新机会。代码可以在https://github.com/AI4Finance-Foundation/FinGPT和https://github.com/AI4Finance-Foundation/FinNLP 中找到。
</details></li>
</ul>
<hr>
<h2 id="What-can-we-learn-from-Data-Leakage-and-Unlearning-for-Law"><a href="#What-can-we-learn-from-Data-Leakage-and-Unlearning-for-Law" class="headerlink" title="What can we learn from Data Leakage and Unlearning for Law?"></a>What can we learn from Data Leakage and Unlearning for Law?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10476">http://arxiv.org/abs/2307.10476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaydeep Borkar</li>
<li>for: 这篇论文主要关注大语言模型（LLMs）的隐私问题， LLMS 可能会在训练数据中记忆 personally identifiable information（PII），如电子邮件和电话号码，并在推理过程中泄露这些信息。</li>
<li>methods: 作者使用了一种名为 “right to be forgotten” 的隐私法规，以删除用户数据点中最容易提取的数据点，以保护用户的隐私。他们还发现，删除这些数据点后，新的数据点会变得更容易提取。此外，作者还发现，精度调整后的模型不仅会泄露训练数据，还会泄露在预训练阶段记忆的数据和 PII。</li>
<li>results: 作者发现，随着用户数据点的删除，新的数据点会变得更容易提取，这可能会对公司使用 LLMs 提供服务的隐私和法律问题产生影响。作者希望通过这篇论文，引起 AI 和法律社区之间的交互性讨论，以解决这些问题。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference. A company can train an LLM on its domain-customized data which can potentially also include their users' PII. In order to comply with privacy laws such as the "right to be forgotten", the data points of users that are most vulnerable to extraction could be deleted. We find that once the most vulnerable points are deleted, a new set of points become vulnerable to extraction. So far, little attention has been given to understanding memorization for fine-tuned models. In this work, we also show that not only do fine-tuned models leak their training data but they also leak the pre-training data (and PII) memorized during the pre-training phase. The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-tuned models can pose significant privacy and legal concerns for companies that use LLMs to offer services. We hope this work will start an interdisciplinary discussion within AI and law communities regarding the need for policies to tackle these issues.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Findings-of-Factify-2-Multimodal-Fake-News-Detection"><a href="#Findings-of-Factify-2-Multimodal-Fake-News-Detection" class="headerlink" title="Findings of Factify 2: Multimodal Fake News Detection"></a>Findings of Factify 2: Multimodal Fake News Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10475">http://arxiv.org/abs/2307.10475</a></li>
<li>repo_url: None</li>
<li>paper_authors: S Suryavardan, Shreyash Mishra, Megha Chakraborty, Parth Patwa, Anku Rani, Aman Chadha, Aishwarya Reganti, Amitava Das, Amit Sheth, Manoj Chinnakotla, Asif Ekbal, Srijan Kumar</li>
<li>for: 针对社交媒体上快速增长的假新闻，这篇论文提出了自动检测假信息和证明其准确性的研究。</li>
<li>methods: 该论文使用了多模态的真实性检测和讽刺新闻 dataset，并采用了对比基于方法，将社交媒体声明与支持文档、图像进行对比，分为5类多模态关系。</li>
<li>results: 在第二次任务中，有60多名参与者和9个测试集提交，最高的F1分平均为81.82%。使用DeBERTatext和Swinv2和CLIP图像得到了最佳表现。<details>
<summary>Abstract</summary>
With social media usage growing exponentially in the past few years, fake news has also become extremely prevalent. The detrimental impact of fake news emphasizes the need for research focused on automating the detection of false information and verifying its accuracy. In this work, we present the outcome of the Factify 2 shared task, which provides a multi-modal fact verification and satire news dataset, as part of the DeFactify 2 workshop at AAAI'23. The data calls for a comparison based approach to the task by pairing social media claims with supporting documents, with both text and image, divided into 5 classes based on multi-modal relations. In the second iteration of this task we had over 60 participants and 9 final test-set submissions. The best performances came from the use of DeBERTa for text and Swinv2 and CLIP for image. The highest F1 score averaged for all five classes was 81.82%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-the-Reusability-of-Pre-trained-Language-Models-in-Real-world-Applications"><a href="#Improving-the-Reusability-of-Pre-trained-Language-Models-in-Real-world-Applications" class="headerlink" title="Improving the Reusability of Pre-trained Language Models in Real-world Applications"></a>Improving the Reusability of Pre-trained Language Models in Real-world Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10457">http://arxiv.org/abs/2307.10457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somayeh Ghanbarzadeh, Hamid Palangi, Yan Huang, Radames Cruz Moreno, Hamed Khanpour</li>
<li>for: 提高预训练语言模型（PLM）的可 reuse性和实际应用效果</li>
<li>methods:  integrate 掩码语言模型（MLM）训练目标到细化过程中，以提高 PLM 的通用化</li>
<li>results:  compared with现有状态的技术，Mask-tuning 能够提高 PLM 的通用化性和实际应用效果，并且在不同数据集上的性能也有提高。<details>
<summary>Abstract</summary>
The reusability of state-of-the-art Pre-trained Language Models (PLMs) is often limited by their generalization problem, where their performance drastically decreases when evaluated on examples that differ from the training dataset, known as Out-of-Distribution (OOD)/unseen examples. This limitation arises from PLMs' reliance on spurious correlations, which work well for frequent example types but not for general examples. To address this issue, we propose a training approach called Mask-tuning, which integrates Masked Language Modeling (MLM) training objectives into the fine-tuning process to enhance PLMs' generalization. Comprehensive experiments demonstrate that Mask-tuning surpasses current state-of-the-art techniques and enhances PLMs' generalization on OOD datasets while improving their performance on in-distribution datasets. The findings suggest that Mask-tuning improves the reusability of PLMs on unseen data, making them more practical and effective for real-world applications.
</details>
<details>
<summary>摘要</summary>
现代预训练语言模型（PLM）的再利用性受到其泛化问题的限制，其性能在训练集不同的示例上逐渐下降，称为Out-of-Distribution（OOD）/未见示例。这种限制来自PLM的假 correlate的依赖，它们在常见示例类型上工作良好，但不适用于通用示例。为解决这个问题，我们提出了一种叫Mask-tuning的训练方法，它将Masked Language Modeling（MLM）训练目标纳入细化过程中，以提高PLM的泛化性。经过探索性实验，我们发现Mask-tuning在OOD数据集上的泛化性超过了当前状态艺技术，同时在入distribution数据集上提高PLM的性能。这些发现表明Mask-tuning可以提高PLM在未见数据上的再利用性，使其更加实用和有效于实际应用。
</details></li>
</ul>
<hr>
<h2 id="Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model"><a href="#Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model" class="headerlink" title="Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model"></a>Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10443">http://arxiv.org/abs/2307.10443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shima Foolad, Kourosh Kiani</li>
<li>for: 本文旨在提高 transformer 模型在复杂理解任务中的表现，通过在输入序列中嵌入显式知识来解决 transformer 模型缺乏显式知识的限制。</li>
<li>methods: 本文提出了一种新的注意模式，即在 transformer 架构中 integrate 知识来自多元图的推理知识，而不需要外部知识。该注意模式包括三个关键元素：全局-本地注意 для单词标签、图注意 для实体标签，以及对每个实体标签和单词标签之间的关系类型的考虑。这种注意模式与 LUKE 模型的实体自注意机制结合，以优化注意力的分配。</li>
<li>results: 实验结果表明，我们的模型在 ReCoRD 数据集上的表现较高，超过了当前顶峰 LUKE-Graph 和基线 LUKE 模型。<details>
<summary>Abstract</summary>
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still fall short in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. To address this limitation, many recent works have proposed injecting external knowledge into the model. However, selecting relevant external knowledge, ensuring its availability, and requiring additional processing steps remain challenging. In this paper, we introduce a novel attention pattern that integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture without relying on external knowledge. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism. The experimental findings corroborate that our model outperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on the ReCoRD dataset that focuses on commonsense reasoning.
</details>
<details>
<summary>摘要</summary>
尽管 transformer 模型在机器阅读理解任务中已经做出了重要进步，但它们仍然在处理复杂的推理任务时缺乏明确的知识。为解决这个限制，许多最近的工作已经提议在模型中注入外部知识。然而，选择相关的外部知识、确保其可用性和需要额外处理步骤仍然是挑战。在这篇论文中，我们介绍了一种新的注意模式，它可以在 transformer 架构中 integrate 推理知识，不需要外部知识。这个注意模式包括三个关键元素：全球-地方注意WORD token，对connected在图形上的entity token的注意力，以及对每个entity token和word token的关系型别进行考虑。这导致了两者之间的优化注意力。这个模式与特殊的相对位置标签相结合，使其能够与LUKE的entity-aware自注意运算机制集成。实验结果证实了我们的模型在ReCoRD dataset上的表现比cutting-edge LUKE-Graph和基准LUKE模型更好。
</details></li>
</ul>
<hr>
<h2 id="Thrust-Adaptively-Propels-Large-Language-Models-with-External-Knowledge"><a href="#Thrust-Adaptively-Propels-Large-Language-Models-with-External-Knowledge" class="headerlink" title="Thrust: Adaptively Propels Large Language Models with External Knowledge"></a>Thrust: Adaptively Propels Large Language Models with External Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10442">http://arxiv.org/abs/2307.10442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Jianshu Chen</li>
<li>for: 这 paper 的目的是如何提高大规模预训练语言模型（PTLM）中的知识利用效率，以及External Knowledge 的搜索方法。</li>
<li>methods: 该 paper 提出了一种名为 Instance-level Adaptive Propulsion of External Knowledge（IAPEK）的方法，该方法通过测量 PTLM 模型中的知识量来决定是否需要进行 External Knowledge 的搜索。该方法使用 Thrust 指标，该指标基于一小量的 seen instances 的表示分布来衡量 PTLM 模型的实例级知识程度。</li>
<li>results:  experiments 表明，Thrust 指标是一个好的 Measurement of PTLM 模型的实例级知识程度。此外，通过使用 Thrust 指标作为搜索指标，可以在 88% 的任务上实现显著的成本效益，即提高了 26% 的平均性能。这些发现有助于在实际应用中提高知识增强 LM 的效率和成本控制。<details>
<summary>Abstract</summary>
Although large-scale pre-trained language models (PTLMs) are shown to encode rich knowledge in their model parameters, the inherent knowledge in PTLMs can be opaque or static, making external knowledge necessary. However, the existing information retrieval techniques could be costly and may even introduce noisy and sometimes misleading knowledge. To address these challenges, we propose the instance-level adaptive propulsion of external knowledge (IAPEK), where we only conduct the retrieval when necessary. To achieve this goal, we propose measuring whether a PTLM contains enough knowledge to solve an instance with a novel metric, Thrust, which leverages the representation distribution of a small number of seen instances. Extensive experiments demonstrate that thrust is a good measurement of PTLM models' instance-level knowledgeability. Moreover, we can achieve significantly higher cost-efficiency with the Thrust score as the retrieval indicator than the naive usage of external knowledge on 88% of the evaluated tasks with 26% average performance improvement. Such findings shed light on the real-world practice of knowledge-enhanced LMs with a limited knowledge-seeking budget due to computation latency or costs.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "pre-trained language models" (PTLMs) is translated as "预训练语言模型" (预PTLMs)* "instance-level adaptive propulsion of external knowledge" (IAPEK) is translated as "实例级适应性外知启动" (IAPEK)* "novel metric" is translated as "新的指标" (新指标)* "representation distribution" is translated as "表示分布" (表示分布)* "instance-level knowledgeability" is translated as "实例级知识能力" (实例知识能力)* "cost-efficiency" is translated as "成本效益" (成本效益)* "knowledge-seeking budget" is translated as "知识寻找预算" (知识寻找预算)
</details></li>
</ul>
<hr>
<h2 id="PharmacyGPT-The-AI-Pharmacist"><a href="#PharmacyGPT-The-AI-Pharmacist" class="headerlink" title="PharmacyGPT: The AI Pharmacist"></a>PharmacyGPT: The AI Pharmacist</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10432">http://arxiv.org/abs/2307.10432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengliang Liu, Zihao Wu, Mengxuan Hu, Bokai Zhao, Lin Zhao, Tianyi Zhang, Haixing Dai, Xianyan Chen, Ye Shen, Sheng Li, Brian Murray, Tianming Liu, Andrea Sikora</li>
<li>For: The paper aims to assess the capabilities of large language models (LLMs) in emulating the role of clinical pharmacists, with potential applications in patient care and the development of future AI-driven healthcare solutions.* Methods: The paper uses real data from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital to evaluate the performance of PharmacyGPT, a novel framework that leverages LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes.* Results: The paper offers valuable insights into the potential applications and limitations of LLMs in clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. The analysis provides a comprehensive evaluation of PharmacyGPT’s performance and contributes to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings.Here’s the same information in Simplified Chinese text:* For: 这篇论文目标是评估大语言模型（LLMs）在供药医生角色中的能力，有可能应用于患者护理和未来的人工智能驱动医疗解决方案。* Methods: 这篇论文使用了北卡罗来纳大学 CHAPEL HILL 医院岗位护理单元（ICU）的真实数据来评估PharmacyGPT框架的表现，该框架利用LLMs生成易于理解的患者团集、药物计划和patient outcome预测。* Results: 这篇论文提供了价值的应用和LLMs在供药医生方面的限制，对患者护理和未来人工智能驱动医疗解决方案的开发有重要意义。分析提供了PharmacyGPT表现的全面评估，贡献到健康设施中人工智能的权威使用。<details>
<summary>Abstract</summary>
In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们介绍了药店GPT，一种新的框架，用于评估大型自然语言模型（LLMs）如ChatGPT和GPT-4在仿真药师的角色。我们的方法包括使用LLMs生成可读的患者群集，制定药物计划，预测患者结果。我们在北卡罗来纳大学夏洛 Chapel Hill医院的劳动 intensivist 单元（ICU）中收集到的实际数据进行了研究。我们的分析提供了价值的洞察，探讨了LLMs在临床药师领域的应用和局限性，对患者护理和未来基于人工智能的医疗解决方案的发展产生了影响。通过评估药店GPT的性能，我们希望能在艺术智能在医疗设置中的整合中发挥作用，并促进负责任和有效的使用这些技术。
</details></li>
</ul>
<hr>
<h2 id="LLMs-as-Workers-in-Human-Computational-Algorithms-Replicating-Crowdsourcing-Pipelines-with-LLMs"><a href="#LLMs-as-Workers-in-Human-Computational-Algorithms-Replicating-Crowdsourcing-Pipelines-with-LLMs" class="headerlink" title="LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs"></a>LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10168">http://arxiv.org/abs/2307.10168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongshuang Wu, Haiyi Zhu, Maya Albayrak, Alexis Axon, Amanda Bertsch, Wenxing Deng, Ziqi Ding, Bill Guo, Sireesh Gururaja, Tzu-Sheng Kuo, Jenny T. Liang, Ryan Liu, Ihita Mandal, Jeremiah Milbauer, Xiaolin Ni, Namrata Padmanabhan, Subhashini Ramkumar, Alexis Sudjianto, Jordan Taylor, Ying-Jui Tseng, Patricia Vaidos, Zhijin Wu, Wei Wu, Chenyang Yang</li>
<li>for: 研究是否可以使用机器学习模型（LLMs）来复制人类在协作任务中的行为。</li>
<li>methods: 研究使用现代机器学习模型来模拟人类在“人类计算算法”中的能力，并评估这些模型的成功程度。</li>
<li>results: 研究发现，现代机器学习模型可以在一些复杂的协作任务中模拟人类的能力，但成功程度受到请求者对LLM能力的理解、任务下的具体技能要求以及完成这些任务的最佳交互方式的影响。研究还发现人类和LLM在接受指令方面存在差异，并重要地强调了启用人类面向的安全措施，以及训练人类和LLM的合作技能。<details>
<summary>Abstract</summary>
LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether LLMs can replicate more complex crowdsourcing pipelines. We find that modern LLMs can simulate some of crowdworkers' abilities in these "human computation algorithms," but the level of success is variable and influenced by requesters' understanding of LLM capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and LLMs' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for LLMs, and discuss the potential of training humans and LLMs with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of LLMs on different tasks (by cross-comparing their performances on sub-tasks) and (2) LLMs' potential in complex tasks, where they can complete part of the tasks while leaving others to humans.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-Transformer-Extrapolation"><a href="#Exploring-Transformer-Extrapolation" class="headerlink" title="Exploring Transformer Extrapolation"></a>Exploring Transformer Extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10156">http://arxiv.org/abs/2307.10156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Qin, Yiran Zhong, Hui Deng</li>
<li>for: 本研究旨在 investigate the conditions for length extrapolation in transformers using Relative Positional Encodings (RPEs), and to derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps.</li>
<li>methods: 本研究使用了 Thorough mathematical and empirical analysis to determine the conditions for length extrapolation in transformers, and derived two practices for language modeling tasks on a variety of corpora.</li>
<li>results: 研究发现，如果系列对应的RPE的对数列 converges，那么 transformer 就一定拥有 length extrapolation 性能。 并且在 Wikitext-103、Books、Github 和 WikiBook 等多个dataset上进行了广泛的实验，以证明我们发现的条件的有效性。同时，我们还对 Empirical Receptive Field (ERF) 和 TRF 进行了比较，并在不同的模型上显示了一致的趋势。<details>
<summary>Abstract</summary>
Length extrapolation has attracted considerable attention recently since it allows transformers to be tested on longer sequences than those used in training. Previous research has shown that this property can be attained by using carefully designed Relative Positional Encodings (RPEs). While these methods perform well on a variety of corpora, the conditions for length extrapolation have yet to be investigated. This paper attempts to determine what types of RPEs allow for length extrapolation through a thorough mathematical and empirical analysis. We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE's exponential converges. Two practices are derived from the conditions and examined in language modeling tasks on a variety of corpora. As a bonus from the conditions, we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps. Extensive experiments are conducted on the Wikitext-103, Books, Github, and WikiBook datasets to demonstrate the viability of our discovered conditions. We also compare TRF to Empirical Receptive Field (ERF) across different models, showing consistently matched trends on the aforementioned datasets. The code is available at https://github.com/OpenNLPLab/Rpe.
</details>
<details>
<summary>摘要</summary>
lenght extrapolation 在 recent  years 吸引了许多关注，因为它允许 transformers 在训练中使用的序列长度上进行测试。 previous research 表明，这种属性可以通过使用特制的 Relative Positional Encodings (RPEs) 实现。 although these methods perform well on a variety of corpora, the conditions for length extrapolation have yet to be investigated. This paper attempts to determine what types of RPEs allow for length extrapolation through a thorough mathematical and empirical analysis. We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE's exponential converges. two practices are derived from the conditions and examined in language modeling tasks on a variety of corpora. As a bonus from the conditions, we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps. extensive experiments are conducted on the Wikitext-103, Books, Github, and WikiBook datasets to demonstrate the viability of our discovered conditions. We also compare TRF to Empirical Receptive Field (ERF) across different models, showing consistently matched trends on the aforementioned datasets. the code is available at https://github.com/OpenNLPLab/Rpe.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers"><a href="#Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers" class="headerlink" title="Gradient Sparsification For Masked Fine-Tuning of Transformers"></a>Gradient Sparsification For Masked Fine-Tuning of Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10098">http://arxiv.org/abs/2307.10098</a></li>
<li>repo_url: None</li>
<li>paper_authors: James O’ Neill, Sourav Dutta</li>
<li>for: 本研究目的是提高下游任务的训练 speed和性能，通过对预训练语言模型进行细化调整。</li>
<li>methods: 本研究使用的方法包括 GradDrop 和其变种，它们在 backwards pass 中随机mask gradients，以增强预训练语言模型的性能。</li>
<li>results: 实验结果表明，GradDrop 能够与使用额外的翻译数据进行中间预训练相比，在 multilingual XGLUE benchmark 上表现出色，并且超过标准的 fine-tuning 和慢搅拌。post-analysis 还显示，GradDrop 能够在未经训练的语言上提高性能。<details>
<summary>Abstract</summary>
Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the backward pass, acting as gradient noise. GradDrop is sparse and stochastic unlike gradual freezing. Extensive experiments on the multilingual XGLUE benchmark with XLMR-Large show that GradDrop is competitive against methods that use additional translated data for intermediate pretraining and outperforms standard fine-tuning and gradual unfreezing. A post-analysis shows how GradDrop improves performance with languages it was not trained on, such as under-resourced languages.
</details>
<details>
<summary>摘要</summary>
现在广泛采用已经预训练的自主学习语言模型进行传输学习，这包括冻结预训练网络的梯度并且只更新新增的分类层的梯度，或者是在所有参数上进行梯度更新。渐进解冻可以考虑到这两种方法之间的折衔，以便在存储和训练速度之间做出一个平衡。然而，是否在训练过程中逐渐解冻层的最佳方法还没有得出确定的答案。在这篇论文中，我们提议使用杂色梯度抑制来规范预训练语言模型，以提高总的精通率。我们引入了GradDrop和其变种，它是一种杂色梯度抑制方法，在反向传播中随机地Mask梯度。GradDrop不同于渐进解冻，它是粒子和随机的。我们在多语言XGLUE测试准则上进行了广泛的实验，结果显示GradDrop与使用额外翻译数据进行中间预训练的方法相当竞争，并且超过了标准的精通和渐进解冻。后续分析表明，GradDrop可以提高语言模型的表现，包括未经训练的语言，如少数语言。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/20/cs.CL_2023_07_20/" data-id="cloh7tqd1007n7b881jmoah0x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/cs.LG_2023_07_20/" class="article-date">
  <time datetime="2023-07-20T10:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/20/cs.LG_2023_07_20/">cs.LG - 2023-07-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Synthetic-Control-Methods-by-Density-Matching-under-Implicit-Endogeneity"><a href="#Synthetic-Control-Methods-by-Density-Matching-under-Implicit-Endogeneity" class="headerlink" title="Synthetic Control Methods by Density Matching under Implicit Endogeneity"></a>Synthetic Control Methods by Density Matching under Implicit Endogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11127">http://arxiv.org/abs/2307.11127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kato, Akari Ohda, Masaaki Imaizumi, Kenichiro McAlinn</li>
<li>for: 本研究使用Synthetic control方法（SCM）估计对比案例研究中的效果，SCM可以估计对待单位的Counterfactual outcome，并且是对比案例研究中的一种重要工具。</li>
<li>methods: 本研究提出了一种新的SCM方法，基于density matching假设，即对待单位的结果density可以被 aproximated为一个权重加权的 mixture model。通过这个假设，我们可以估计SC weights，并且我们的估计器具有三个优点：一、我们的估计器是 asymptotically unbiased; two、我们可以降低对counterfactual prediction的mean squared error; three、我们的方法可以生成对待单位的治疗效果的全体分布，不仅是预期值。</li>
<li>results: 本研究通过实验结果展示了我们的方法的效果，并且证明了其比既有SCM方法更加精准和有效。<details>
<summary>Abstract</summary>
Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matching moments of treated outcomes and the weighted sum of moments of untreated outcomes. Our proposed method has three advantages over existing methods. First, our estimator is asymptotically unbiased under the assumption of the mixture model. Second, due to the asymptotic unbiasedness, we can reduce the mean squared error for counterfactual prediction. Third, our method generates full densities of the treatment effect, not only expected values, which broadens the applicability of SCMs. We provide experimental results to demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
Synthetic control methods (SCMs) 已成为比较研究中的重要工具，用于估计 causal inference。 SCMs 的基本思想是使用一个权重和平∑ 观察到的结果来估计对待单位的 counterfactual 结果。 SC 的准确性是估计 causal effect 的关键，因此 SCMs 的 estimation 问题已经引起了很多研究。在这篇论文中，我们首先指出了现有 SCMs 存在一种隐藏的内生性问题，即对 untreated units 的结果和 counterfactual 结果模型中的错误项之间的相关性。我们证明了这个问题会导致 causal effect 估计器偏移。然后，我们提出了一种基于 density matching 的新的 SCM，假设待处理单位的结果的概率可以通过一个权重和平∑ untreated units 的结果概率来近似。基于这个假设，我们可以通过匹配待处理结果的 moments 和权重和平∑ untreated units 的 moments来估计 SC 权重。我们的提议方法有三个优点：首先，我们的估计器在 mixture model 的假设下是 asymptotically unbiased。其次，由于 asymptotic unbiasedness，我们可以降低 counterfactual prediction 的 mean squared error。第三，我们的方法可以生成对待单位的治疗效果的全部概率分布，不仅是预期值，这扩展了 SCMs 的应用范围。我们提供实验结果，以证明我们的提议方法的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Markov-Chain-Model-for-Identifying-Changes-in-Daily-Activity-Patterns-of-People-Living-with-Dementia"><a href="#A-Markov-Chain-Model-for-Identifying-Changes-in-Daily-Activity-Patterns-of-People-Living-with-Dementia" class="headerlink" title="A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia"></a>A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11126">http://arxiv.org/abs/2307.11126</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvfl/markov-chain-model">https://github.com/nvfl/markov-chain-model</a></li>
<li>paper_authors: Nan Fletcher-Lloyd, Alina-Irina Serban, Magdalena Kolanko, David Wingfield, Danielle Wilson, Ramin Nilforooshan, Payam Barnaghi, Eyal Soreq</li>
<li>for: 这个研究是为了检测老人生活在困难中的营养和液体消耗情况，以及对这些情况的影响。</li>
<li>methods: 这个研究使用了互联网物联网技术收集了73户老人生活在家中的数据，并使用线性混合效应分析检测了COVID-19大流行对老人的吃饭和喝水习惯的影响。</li>
<li>results: 研究发现在白天，老人的厨房活动增加了（t(147) &#x3D; -2.90，p &lt; 0.001），而在夜晚，厨房活动减少了（t(147) &#x3D; -2.90，p &lt; 0.001）。此外，研究还提出了一种基于营养模型的方法来检测老人的行为变化。<details>
<summary>Abstract</summary>
Malnutrition and dehydration are strongly associated with increased cognitive and functional decline in people living with dementia (PLWD), as well as an increased rate of hospitalisations in comparison to their healthy counterparts. Extreme changes in eating and drinking behaviours can often lead to malnutrition and dehydration, accelerating the progression of cognitive and functional decline and resulting in a marked reduction in quality of life. Unfortunately, there are currently no established methods by which to objectively detect such changes. Here, we present the findings of an extensive quantitative analysis conducted on in-home monitoring data collected from 73 households of PLWD using Internet of Things technologies. The Coronavirus 2019 (COVID-19) pandemic has previously been shown to have dramatically altered the behavioural habits, particularly the eating and drinking habits, of PLWD. Using the COVID-19 pandemic as a natural experiment, we conducted linear mixed-effects modelling to examine changes in mean kitchen activity within a subset of 21 households of PLWD that were continuously monitored for 499 days. We report an observable increase in day-time kitchen activity and a significant decrease in night-time kitchen activity (t(147) = -2.90, p < 0.001). We further propose a novel analytical approach to detecting changes in behaviours of PLWD using Markov modelling applied to remote monitoring data as a proxy for behaviours that cannot be directly measured. Together, these results pave the way to introduce improvements into the monitoring of PLWD in naturalistic settings and for shifting from reactive to proactive care.
</details>
<details>
<summary>摘要</summary>
🇨🇳 营养不良和肥虚是老年人智能和功能退化的重要风险因素，也会使人们患有智能和功能退化的人群（PLWD）的入院率增加。宽泛的食品和饮料消耗方式的变化可能会导致营养不良和肥虚，加速智能和功能退化的进程，从而导致生活质量下降。可惜，目前没有可靠的方法可以 объектив地探测这些变化。我们在73户老年人智能和功能退化者的家庭中进行了广泛的量化分析，使用互联网物联网技术收集数据。2019冠状病毒疫情（COVID-19）已经对智能和功能退化者的行为习惯产生了深远的影响，特别是饮食和饮料的消耗方式。使用2019冠状病毒疫情作为自然实验，我们使用线性混合效应模型对21户智能和功能退化者的24小时内的厨房活动进行分析。我们发现了日间厨房活动的 observable 增加（t(147) = -2.90，p < 0.001），并且发现夜间厨房活动的显著减少。此外，我们还提出了一种基于远程监测数据的Markov模型，用于检测智能和功能退化者的行为变化。这些结果将为监测智能和功能退化者的监测提供新的方法，并且可以帮助转换到主动监测。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Probabilistic-Deconvolution-of-Galaxy-Images"><a href="#Diffusion-Models-for-Probabilistic-Deconvolution-of-Galaxy-Images" class="headerlink" title="Diffusion Models for Probabilistic Deconvolution of Galaxy Images"></a>Diffusion Models for Probabilistic Deconvolution of Galaxy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11122">http://arxiv.org/abs/2307.11122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yashpatel5400/galgen">https://github.com/yashpatel5400/galgen</a></li>
<li>paper_authors: Zhiwei Xue, Yuhang Li, Yash Patel, Jeffrey Regier</li>
<li>for: 这个论文是为了提出一种基于深度生成模型的PSF逆推算法，用于恢复宇宙图像中的细节。</li>
<li>methods: 该论文使用了一种基于普通 diffusion 模型的方法，不需要类别器，可以更好地提取宇宙图像中的细节。</li>
<li>results: 论文的实验结果表明，基于 diffusion 模型的PSF逆推算法可以更好地捕捉宇宙图像中的细节，并且提供了更多的可能性空间，比如 conditional VAE 的方法。<details>
<summary>Abstract</summary>
Telescopes capture images with a particular point spread function (PSF). Inferring what an image would have looked like with a much sharper PSF, a problem known as PSF deconvolution, is ill-posed because PSF convolution is not an invertible transformation. Deep generative models are appealing for PSF deconvolution because they can infer a posterior distribution over candidate images that, if convolved with the PSF, could have generated the observation. However, classical deep generative models such as VAEs and GANs often provide inadequate sample diversity. As an alternative, we propose a classifier-free conditional diffusion model for PSF deconvolution of galaxy images. We demonstrate that this diffusion model captures a greater diversity of possible deconvolutions compared to a conditional VAE.
</details>
<details>
<summary>摘要</summary>
天文望远镜捕捉到图像，但图像具有特定的点扩散函数（PSF）。尝试恢复图像为更加锐利PSF后的形态，称为PSF恢复，是一个不定问题，因为PSF混合不是可逆变换。深度生成模型吸引了PSF恢复的应用，因为它们可以对候选图像进行 posterior 分布预测，如果将其混合到PSF中，可能会生成观测结果。然而，经典的深度生成模型如VAEs和GANs经常提供不够的样本多样性。为了解决这问题，我们提议一种无类别的条件扩散模型 дляPSF恢复星系图像。我们证明该扩散模型可以捕捉更多的可能的恢复形态，比 conditional VAE 更加多样化。
</details></li>
</ul>
<hr>
<h2 id="PASTA-Pretrained-Action-State-Transformer-Agents"><a href="#PASTA-Pretrained-Action-State-Transformer-Agents" class="headerlink" title="PASTA: Pretrained Action-State Transformer Agents"></a>PASTA: Pretrained Action-State Transformer Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10936">http://arxiv.org/abs/2307.10936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Boige, Yannis Flet-Berliac, Arthur Flajolet, Guillaume Richard, Thomas Pierrot<br>for: This paper aims to investigate the use of pre-trained transformer models for reinforcement learning tasks, specifically addressing the problem of adapting models to new environments with limited data.methods: The authors use a unified methodology that includes tokenization at the action and state component level, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT) to adapt the models to downstream tasks.results: The developed models contain fewer than 10 million parameters and can be fine-tuned with fewer than 10,000 parameters during downstream adaptation, allowing for robust policy learning and encouraging further research into the use of transformers for reinforcement learning.<details>
<summary>Abstract</summary>
Self-supervised learning has brought about a revolutionary paradigm shift in various computing domains, including NLP, vision, and biology. Recent approaches involve pre-training transformer models on vast amounts of unlabeled data, serving as a starting point for efficiently solving downstream tasks. In the realm of reinforcement learning, researchers have recently adapted these approaches by developing models pre-trained on expert trajectories, enabling them to address a wide range of tasks, from robotics to recommendation systems. However, existing methods mostly rely on intricate pre-training objectives tailored to specific downstream applications. This paper presents a comprehensive investigation of models we refer to as Pretrained Action-State Transformer Agents (PASTA). Our study uses a unified methodology and covers an extensive set of general downstream tasks including behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation. Our goal is to systematically compare various design choices and provide valuable insights to practitioners for building robust models. Key highlights of our study include tokenization at the action and state component level, using fundamental pre-training objectives like next token prediction, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT). The developed models in our study contain fewer than 10 million parameters and the application of PEFT enables fine-tuning of fewer than 10,000 parameters during downstream adaptation, allowing a broad community to use these models and reproduce our experiments. We hope that this study will encourage further research into the use of transformers with first-principles design choices to represent RL trajectories and contribute to robust policy learning.
</details>
<details>
<summary>摘要</summary>
自顾学学习在不同计算领域中引发了革命性的思维方式变革，包括自然语言处理、视觉和生物学。现有的方法通常是使用庞大量未标注数据进行预训练 transformer 模型，作为下游任务的开始点，以提高效率。在返回学习领域，研究人员已经采用了这些方法，并开发了基于专家轨迹的预训练模型，以解决广泛的任务，从 робо特斯到推荐系统。然而，现有的方法通常仅适用于特定下游应用程序的精细预训练目标。本文提出了一种叫做 PASTA 的模型，其中包括使用各种设计选择和涵盖广泛的通用下游任务，包括行为做clone、离线学习、感知故障Robustness和动力学变化适应。我们的研究使用一种统一的方法ologies和涵盖了广泛的下游任务，以系统地比较不同的设计选择，并为实践者提供有价值的洞察。关键特点包括动作和状态组件级别的启用，使用基本的预训练目标如下一个token预测，在多个领域同时训练模型，以及使用参数效率的练习（PEFT）。开发的模型中含 fewer than 10 million parameters，并且通过PEFT进行参数练习，可以在下游适应中使用 fewer than 10,000 parameters，使得广泛的社区可以使用这些模型并重现我们的实验。我们希望这种研究会鼓励更多的人使用 transformer 模型的首要原则来表示RL轨迹，并贡献于Robust policy学习。
</details></li>
</ul>
<hr>
<h2 id="Inorganic-synthesis-structure-maps-in-zeolites-with-machine-learning-and-crystallographic-distances"><a href="#Inorganic-synthesis-structure-maps-in-zeolites-with-machine-learning-and-crystallographic-distances" class="headerlink" title="Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances"></a>Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10935">http://arxiv.org/abs/2307.10935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Schwalbe-Koda, Daniel E. Widdowson, Tuan Anh Pham, Vitaliy A. Kurlin</li>
<li>for: 这项研究旨在使用计算机模拟和机器学习（ML）技术，为硅酸盐材料的合成创造无监督的材料合成地图。</li>
<li>methods: 该研究使用了一种强大的距离度量方法和机器学习分析方法，从253个已知硅酸盐中提取出不同的材料合成条件。</li>
<li>results: 研究发现，在不使用标签的情况下，邻近的硅酸盐结构之间的距离度量可以反映硅酸盐的材料合成条件，并且可以预测硅酸盐的合成结果。<details>
<summary>Abstract</summary>
Zeolites are inorganic materials known for their diversity of applications, synthesis conditions, and resulting polymorphs. Although their synthesis is controlled both by inorganic and organic synthesis conditions, computational studies of zeolite synthesis have focused mostly on organic template design. In this work, we use a strong distance metric between crystal structures and machine learning (ML) to create inorganic synthesis maps in zeolites. Starting with 253 known zeolites, we show how the continuous distances between frameworks reproduce inorganic synthesis conditions from the literature without using labels such as building units. An unsupervised learning analysis shows that neighboring zeolites according to our metric often share similar inorganic synthesis conditions, even in template-based routes. In combination with ML classifiers, we find synthesis-structure relationships for 14 common inorganic conditions in zeolites, namely Al, B, Be, Ca, Co, F, Ga, Ge, K, Mg, Na, P, Si, and Zn. By explaining the model predictions, we demonstrate how (dis)similarities towards known structures can be used as features for the synthesis space. Finally, we show how these methods can be used to predict inorganic synthesis conditions for unrealized frameworks in hypothetical databases and interpret the outcomes by extracting local structural patterns from zeolites. In combination with template design, this work can accelerate the exploration of the space of synthesis conditions for zeolites.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modeling-3D-cardiac-contraction-and-relaxation-with-point-cloud-deformation-networks"><a href="#Modeling-3D-cardiac-contraction-and-relaxation-with-point-cloud-deformation-networks" class="headerlink" title="Modeling 3D cardiac contraction and relaxation with point cloud deformation networks"></a>Modeling 3D cardiac contraction and relaxation with point cloud deformation networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10927">http://arxiv.org/abs/2307.10927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Vicente Grau</li>
<li>for: 该研究旨在开发一种基于点云深度学习的准确评估三维心脏功能的方法，以提高我们对心脏健康和疾病机理的理解。</li>
<li>methods: 该方法使用点云深度学习的最新进展，建立了一个点云编码器-解码器结构，以便高效地学习多尺度特征。</li>
<li>results: 研究人员对大量的UK Biobank数据集进行了测试，并发现了average Chamfer距离小于图像获取的像素分辨率，同时也发现了与真实数据集的相似性。此外，研究人员还发现了在各个子 популяции中的差异，并且表明了3D凝聚模式可以超越多个临床标准。<details>
<summary>Abstract</summary>
Global single-valued biomarkers of cardiac function typically used in clinical practice, such as ejection fraction, provide limited insight on the true 3D cardiac deformation process and hence, limit the understanding of both healthy and pathological cardiac mechanics. In this work, we propose the Point Cloud Deformation Network (PCD-Net) as a novel geometric deep learning approach to model 3D cardiac contraction and relaxation between the extreme ends of the cardiac cycle. It employs the recent advances in point cloud-based deep learning into an encoder-decoder structure, in order to enable efficient multi-scale feature learning directly on multi-class 3D point cloud representations of the cardiac anatomy. We evaluate our approach on a large dataset of over 10,000 cases from the UK Biobank study and find average Chamfer distances between the predicted and ground truth anatomies below the pixel resolution of the underlying image acquisition. Furthermore, we observe similar clinical metrics between predicted and ground truth populations and show that the PCD-Net can successfully capture subpopulation-specific differences between normal subjects and myocardial infarction (MI) patients. We then demonstrate that the learned 3D deformation patterns outperform multiple clinical benchmarks by 13% and 7% in terms of area under the receiver operating characteristic curve for the tasks of prevalent MI detection and incident MI prediction and by 7% in terms of Harrell's concordance index for MI survival analysis.
</details>
<details>
<summary>摘要</summary>
全球单值生物标志物typically used in clinical practice, such as ejection fraction, only provide limited insight into the true 3D cardiac deformation process and therefore limit the understanding of both healthy and pathological cardiac mechanics. In this work, we propose the Point Cloud Deformation Network (PCD-Net) as a novel geometric deep learning approach to model 3D cardiac contraction and relaxation between the extreme ends of the cardiac cycle. It employs the recent advances in point cloud-based deep learning into an encoder-decoder structure, in order to enable efficient multi-scale feature learning directly on multi-class 3D point cloud representations of the cardiac anatomy. We evaluate our approach on a large dataset of over 10,000 cases from the UK Biobank study and find average Chamfer distances below the pixel resolution of the underlying image acquisition. Furthermore, we observe similar clinical metrics between predicted and ground truth populations and show that the PCD-Net can successfully capture subpopulation-specific differences between normal subjects and myocardial infarction (MI) patients. We then demonstrate that the learned 3D deformation patterns outperform multiple clinical benchmarks by 13% and 7% in terms of area under the receiver operating characteristic curve for the tasks of prevalent MI detection and incident MI prediction and by 7% in terms of Harrell's concordance index for MI survival analysis.
</details></li>
</ul>
<hr>
<h2 id="Confidence-intervals-for-performance-estimates-in-3D-medical-image-segmentation"><a href="#Confidence-intervals-for-performance-estimates-in-3D-medical-image-segmentation" class="headerlink" title="Confidence intervals for performance estimates in 3D medical image segmentation"></a>Confidence intervals for performance estimates in 3D medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10926">http://arxiv.org/abs/2307.10926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rosanajurdi/SegVal_TMI">https://github.com/rosanajurdi/SegVal_TMI</a></li>
<li>paper_authors: R. El Jurdi, G. Varoquaux, O. Colliot</li>
<li>for: 这 paper 是用来评估医疗图像 segmentation 模型的。</li>
<li>methods: 这 paper 使用了 nnU-net 框架和 Medical Decathlon 挑战赛中的两个数据集，以及两种表现指标： dice 准确率和 Hausdorff 距离。</li>
<li>results: 这 paper 发现，在不同的测试集大小和表现指标的扩散情况下，参数型的信度范围是Bootstrap估计的可靠近似。此外，它还发现，为了达到某个精度水平，通常需要训练样本数量远少于 classification 任务。 typically，需要约 100-200 个测试样本，而且更Difficult的 segmentation 任务可能需要更多的测试样本。<details>
<summary>Abstract</summary>
Medical segmentation models are evaluated empirically. As such an evaluation is based on a limited set of example images, it is unavoidably noisy. Beyond a mean performance measure, reporting confidence intervals is thus crucial. However, this is rarely done in medical image segmentation. The width of the confidence interval depends on the test set size and on the spread of the performance measure (its standard-deviation across of the test set). For classification, many test images are needed to avoid wide confidence intervals. Segmentation, however, has not been studied, and it differs by the amount of information brought by a given test image. In this paper, we study the typical confidence intervals in medical image segmentation. We carry experiments on 3D image segmentation using the standard nnU-net framework, two datasets from the Medical Decathlon challenge and two performance measures: the Dice accuracy and the Hausdorff distance. We show that the parametric confidence intervals are reasonable approximations of the bootstrap estimates for varying test set sizes and spread of the performance metric. Importantly, we show that the test size needed to achieve a given precision is often much lower than for classification tasks. Typically, a 1% wide confidence interval requires about 100-200 test samples when the spread is low (standard-deviation around 3%). More difficult segmentation tasks may lead to higher spreads and require over 1000 samples.
</details>
<details>
<summary>摘要</summary>
医学分割模型通常会被实际测试。这种测试基于有限的示例图像，因此无法避免噪音。除了平均性能指标之外，报告信息interval也是非常重要。然而，在医学像分割中，这并不是常见的做法。信息interval的宽度取决于测试集大小和性能指标的扩散（测试集中的标准差）。对于分类任务，需要许多测试图像来避免宽的信息interval。但是，在分割任务中，不同的测试图像会带来不同的信息量。在这篇论文中，我们研究了医学像分割中常见的信息interval。我们在使用标准nnU-net框架、医疗十大挑战赛提供的两个数据集和两个性能指标（ dice准确率和 Hausdorff 距离）进行了实验。我们发现，参数信息interval是参数Bootstrap估计的可靠近似，并且显示测试集大小和性能指标的扩散对信息interval的影响。进一步地，我们发现，为了 достичь给定的精度，测试样本的数量通常比分类任务低得多。例如，当标准差较低（约3%）时，1% 宽的信息interval只需要100-200个测试样本。更复杂的分割任务可能会导致更高的扩散，需要更多的测试样本。
</details></li>
</ul>
<hr>
<h2 id="Sequential-Multi-Dimensional-Self-Supervised-Learning-for-Clinical-Time-Series"><a href="#Sequential-Multi-Dimensional-Self-Supervised-Learning-for-Clinical-Time-Series" class="headerlink" title="Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series"></a>Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10923">http://arxiv.org/abs/2307.10923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Aniruddh Raghu, Payal Chandak, Ridwan Alam, John Guttag, Collin M. Stultz</li>
<li>for: 这个论文是为了解决现有的自动学习（Self-supervised learning）方法不能处理多Modal时间序列数据的问题。</li>
<li>methods: 该论文提出了一种新的自动学习方法——Sequential Multi-Dimensional SSL，它在序列级和个体高维数据级别应用SSL损失来更好地捕捉信息。</li>
<li>results: 对两个实际的医疗时间序列数据集进行了实验，结果表明，在先行培育后，使用该方法并then fine-tuning在下游任务上提高了性能，并在一些设置下可以通过不同的自动学习损失函数来提高性能。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state. However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram). These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence. In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales. Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg. We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs. Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.
</details>
<details>
<summary>摘要</summary>
自适应学习（SSL） для医疗时间序列数据在当前文献中受到了广泛关注，因为这些数据具有高度的资源和重要的生物physiological状态信息。然而，现有的大多数SSL方法仅适用于单模时间序列，例如序列中的结构化特征（如医学实验室值和生物指标）或个人高维度生理学信号（如电cardiogram）。这些现有方法无法轻松地扩展到模型时间序列，其中每个时间步骤都包含结构化特征和高维度数据。在这项工作中，我们解决这个差距，并提议一种新的SSL方法——Sequential Multi-Dimensional SSL。在这种方法中，我们在序列级别和个体高维度数据点级别都应用SSL损失，以更好地捕捉信息在不同级别。我们的策略是对特定的损失函数类型不拘泥，可以是对比性的，如SimCLR，或非对比性的，如VICReg。我们在两个真实的医疗时间序列数据集上进行了实验，其中时间序列包含高频电cardiograms和实验室值和生物指标的序列。我们的实验结果表明，在这些数据集上预训练后，通过精度调整下游任务，可以超过基准值，并在不同的自我超vised损失函数下达到更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Language-based-Action-Concept-Spaces-Improve-Video-Self-Supervised-Learning"><a href="#Language-based-Action-Concept-Spaces-Improve-Video-Self-Supervised-Learning" class="headerlink" title="Language-based Action Concept Spaces Improve Video Self-Supervised Learning"></a>Language-based Action Concept Spaces Improve Video Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10922">http://arxiv.org/abs/2307.10922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanchana Ranasinghe, Michael Ryoo</li>
<li>for: 学习高效转移和鲁棒的视频表示</li>
<li>methods: 使用语言捆绑自我超vised学习将图像CLIP模型适应视频频谱</li>
<li>results: 提高零shot和线性探测性能在三个动作认识benchmark上<details>
<summary>Abstract</summary>
Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的对语言图像预训练技术已经导致学习了高度可转移和稳定的图像表示。然而，将这些模型应用到视频频道上仍然是一个开放的问题。我们考虑了一种简单的方法，使用语言绑定的自我超vision学习来适应图像 CLIP 模型到视频频道。我们修改了 temporal 模型，在自我数据采样设置下使用语言Encoder 提取的不同动作概念的特征向量构建动作概念空间。我们引入了两个训练目标，概念练习和概念对接，以保留原始表示的通用性，同时强制行动和其属性之间的关系。我们的方法提高了零shot和直线探测性能在三个动作认识标准 bencmarks 上。
</details></li>
</ul>
<hr>
<h2 id="The-Role-of-Entropy-and-Reconstruction-in-Multi-View-Self-Supervised-Learning"><a href="#The-Role-of-Entropy-and-Reconstruction-in-Multi-View-Self-Supervised-Learning" class="headerlink" title="The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning"></a>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10907">http://arxiv.org/abs/2307.10907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-entropy-reconstruction">https://github.com/apple/ml-entropy-reconstruction</a></li>
<li>paper_authors: Borja Rodríguez-Gálvez, Arno Blaas, Pau Rodríguez, Adam Goliński, Xavier Suau, Jason Ramapuram, Dan Busbridge, Luca Zappella</li>
<li>for: 本文研究了多视图自动学习（MVSSL）的成功机制，并通过一种新的下界函数来分析不同的MVSSL家族。</li>
<li>methods: 本文使用一种基于信息准则（MI）的下界函数，包括一个 entropy 和一个重建项（ER），来分析不同的MVSSL方法。</li>
<li>results: 研究结果表明，使用这种 ER 下界函数可以达到与常见MVSSL方法相当的性能，同时使得训练时使用小批量或小EMA系数时更加稳定。<details>
<summary>Abstract</summary>
The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.
</details>
<details>
<summary>摘要</summary>
文中所描述的多视图自学习（MVSSL）的机制仍未完全理解。对于对比MVSSL方法的研究，我们通过InfoNCE，一种低下界的共识信息（MI）来研究。但是其他MVSSL方法和MI之间的关系仍然不清楚。我们考虑一种基于 entropy和重建项（ER）的低下界，并通过这个窗口来分析主要的MVSSL家族。我们显示了使用 clustering-based 方法such as DeepCluster和SwAV时，实际上是最大化MI的。我们还重新解释了 distillation-based 方法such as BYOL和DINO的机制，并证明它们通过直接最大化重建项并间接激发稳定的 entropy来实现。我们通过实验证明这一点。 finally，我们表明将常见MVSSL方法的目标替换为ER下界可以实现竞争性的性能，同时使其在训练时使用小批量或小EMA系数时更加稳定。Here's the breakdown of the translation:* 文中所描述的多视图自学习 (MVSSL)：The text is discussing the mechanisms behind the success of multi-view self-supervised learning (MVSSL).* 机制仍未完全理解：The mechanisms behind MVSSL are not yet fully understood.* 对于对比MVSSL方法的研究：Research on comparing MVSSL methods.* 通过InfoNCE来研究：Researching through the lens of InfoNCE, a lower bound of mutual information (MI).* 其他MVSSL方法和MI之间的关系仍然不清楚：The relationship between other MVSSL methods and MI is still unclear.* 我们考虑一种基于 entropy和重建项（ER）的低下界：We consider a different lower bound on MI consisting of an entropy and a reconstruction term (ER).* 并通过这个窗口来分析主要的MVSSL家族：And analyze the main MVSSL families through this ER bound.* 我们显示了使用 clustering-based 方法such as DeepCluster和SwAV时，实际上是最大化MI的：We show that using clustering-based methods such as DeepCluster and SwAV, the MI is maximized.* 我们还重新解释了 distillation-based 方法such as BYOL和DINO的机制：We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO.* 并证明它们通过直接最大化重建项并间接激发稳定的 entropy来实现：And prove that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy.* 我们通过实验证明这一点：We confirm this empirically.* finally，我们表明将常见MVSSL方法的目标替换为ER下界可以实现竞争性的性能：Finally, we show that replacing the objectives of common MVSSL methods with the ER bound achieves competitive performance.* 同时使其在训练时使用小批量或小EMA系数时更加稳定：And make them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.
</details></li>
</ul>
<hr>
<h2 id="Variational-Point-Encoding-Deformation-for-Dental-Modeling"><a href="#Variational-Point-Encoding-Deformation-for-Dental-Modeling" class="headerlink" title="Variational Point Encoding Deformation for Dental Modeling"></a>Variational Point Encoding Deformation for Dental Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10895">http://arxiv.org/abs/2307.10895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Ziruo Ye, Thomas Ørkild, Peter Lempel Søndergaard, Søren Hauberg</li>
<li>for: 本研究旨在鼓励更多研究，透过发布新的大量牙齿矩阵数据集。</li>
<li>methods: 我们提出了一种扩展FoldingNet的方法，称为Variational FoldingNet（VF-Net），它允许点云表示的 probabilistic 学习。</li>
<li>results: 我们的实验结果表明，VF-Net 比现有模型在牙齿扫描和推理方面具有更高的表现力，同时具有更好的鲁棒性。<details>
<summary>Abstract</summary>
Digital dentistry has made significant advancements in recent years, yet numerous challenges remain to be addressed. In this study, we release a new extensive dataset of tooth meshes to encourage further research. Additionally, we propose Variational FoldingNet (VF-Net), which extends FoldingNet to enable probabilistic learning of point cloud representations. A key challenge in existing latent variable models for point clouds is the lack of a 1-to-1 mapping between input points and output points. Instead, they must rely on optimizing Chamfer distances, a metric that does not have a normalized distributional counterpart, preventing its usage in probabilistic models. We demonstrate that explicit minimization of Chamfer distances can be replaced by a suitable encoder, which allows us to increase computational efficiency while simplifying the probabilistic extension. Our experimental findings present empirical evidence demonstrating the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation. Additionally, our investigation highlights the robustness of VF-Net's latent representations. These results underscore the promising prospects of VF-Net as an effective and reliable method for point cloud reconstruction and analysis.
</details>
<details>
<summary>摘要</summary>
《数字牙科技术的进步和挑战》Recently, digital dentistry has made significant advancements, but there are still many challenges that need to be addressed. In this study, we release a new and extensive dataset of tooth meshes to encourage further research. Additionally, we propose a new method called Variational FoldingNet (VF-Net), which extends FoldingNet to enable probabilistic learning of point cloud representations.Currently, there is a key challenge in existing latent variable models for point clouds, which is the lack of a 1-to-1 mapping between input points and output points. Instead, they must rely on optimizing Chamfer distances, a metric that does not have a normalized distributional counterpart, preventing its usage in probabilistic models. We demonstrate that explicit minimization of Chamfer distances can be replaced by a suitable encoder, which allows us to increase computational efficiency while simplifying the probabilistic extension.Our experimental findings present empirical evidence demonstrating the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation. Additionally, our investigation highlights the robustness of VF-Net's latent representations. These results underscore the promising prospects of VF-Net as an effective and reliable method for point cloud reconstruction and analysis.
</details></li>
</ul>
<hr>
<h2 id="Learning-and-Generalizing-Polynomials-in-Simulation-Metamodeling"><a href="#Learning-and-Generalizing-Polynomials-in-Simulation-Metamodeling" class="headerlink" title="Learning and Generalizing Polynomials in Simulation Metamodeling"></a>Learning and Generalizing Polynomials in Simulation Metamodeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10892">http://arxiv.org/abs/2307.10892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jesperhauch/polynomial_deep_learning">https://github.com/jesperhauch/polynomial_deep_learning</a></li>
<li>paper_authors: Jesper Hauch, Christoffer Riis, Francisco C. Pereira</li>
<li>for: 本研究旨在提高人工神经网络的 polynomial 拟合能力和通用性，以便在多种工程领域中使用。</li>
<li>methods: 本文提出了一种基于多项式神经网络（MNN）的拟合方法，并使用 MNN 作为递归建模Component。</li>
<li>results: 实验表明，MNN 比基eline模型更好地泛化，并且其在验证集上的性能与测试集上的性能相似。此外，作者还提出了一种基于 simulations 的模拟中间模型方法，可以更好地拟合 polynomial 时间步骤更新的 simulations。<details>
<summary>Abstract</summary>
The ability to learn polynomials and generalize out-of-distribution is essential for simulation metamodels in many disciplines of engineering, where the time step updates are described by polynomials. While feed forward neural networks can fit any function, they cannot generalize out-of-distribution for higher-order polynomials. Therefore, this paper collects and proposes multiplicative neural network (MNN) architectures that are used as recursive building blocks for approximating higher-order polynomials. Our experiments show that MNNs are better than baseline models at generalizing, and their performance in validation is true to their performance in out-of-distribution tests. In addition to MNN architectures, a simulation metamodeling approach is proposed for simulations with polynomial time step updates. For these simulations, simulating a time interval can be performed in fewer steps by increasing the step size, which entails approximating higher-order polynomials. While our approach is compatible with any simulation with polynomial time step updates, a demonstration is shown for an epidemiology simulation model, which also shows the inductive bias in MNNs for learning and generalizing higher-order polynomials.
</details>
<details>
<summary>摘要</summary>
“模型学习 polynomials 和泛化到不同分布是Engineering 多个领域的必备技能，因为时间步长更新通常是 polynomials。虽然前向神经网络可以适应任何函数，但它们无法泛化到高阶 polynomials。因此，本文收集并提出了multiplicative neural network（MNN）架构，用于 recursive 构建高阶 polynomials 的近似。我们的实验表明，MNNs 在泛化方面表现更好，并且在验证集中的性能与验证集外的性能相似。此外，我们还提出了一种 simulation metamodeling 方法，用于 simulations with polynomial time step updates。对于这些 simulations，可以通过增加步长来快速 simulate 时间 интерVAL，这意味着需要近似高阶 polynomials。我们的方法与任何具有 polynomial time step updates 的 simulation 相容，并在 epidemiology 模型中进行了示例，这也表明了 MNNs 对于学习和泛化高阶 polynomials 的适应性。”Note that Simplified Chinese is used here, which is the most widely used variety of Chinese in mainland China. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks"><a href="#Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks" class="headerlink" title="Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks"></a>Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10891">http://arxiv.org/abs/2307.10891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cxlvinchau/linna">https://github.com/cxlvinchau/linna</a></li>
<li>paper_authors: Calvin Chau, Jan Křetínský, Stefanie Mohr</li>
<li>for: 提高神经网络的可扩展性。</li>
<li>methods: 使用 linear combination of neurons 来取代单个 neuron，并在 syntaxic 和 semantic 上进行抽象。</li>
<li>results: 实现更高的减少，并引入一种改进的减少方法以保持准确性。<details>
<summary>Abstract</summary>
Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.
</details>
<details>
<summary>摘要</summary>
归纳是一种关键的验证技术，可以提高神经网络的扩展性。然而，归纳神经网络的使用范围还很有限。先前的方法是将一些神经元与其他相似的神经元进行交换，以实现归纳。我们可以将相似性分为逻辑（通过神经元之间的连接量）或semantic（通过神经元对各种输入的活动值）两种。可惜，先前的方法只能实现一定的减少，而且只有部分实现。在这项工作中，我们提供了更 flexible的框架，允许一个神经元被替换为一个线性组合其他神经元，从而提高减少。我们在逻辑和semantic归纳上应用这种方法，并进行实验性评估。此外，我们还引入了一种精细化方法，可以帮助找到更好的减少和精度之间的平衡。
</details></li>
</ul>
<hr>
<h2 id="Player-optimal-Stable-Regret-for-Bandit-Learning-in-Matching-Markets"><a href="#Player-optimal-Stable-Regret-for-Bandit-Learning-in-Matching-Markets" class="headerlink" title="Player-optimal Stable Regret for Bandit Learning in Matching Markets"></a>Player-optimal Stable Regret for Bandit Learning in Matching Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10890">http://arxiv.org/abs/2307.10890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fang Kong, Shuai Li</li>
<li>For:  This paper focuses on the problem of matching markets, specifically on finding a stable matching in an online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms).* Methods: The paper proposes a new algorithm called explore-then-Gale-Shapley (ETGS) and analyzes its performance in terms of the optimal stable regret of each player.* Results: The paper shows that the optimal stable regret of each player can be upper bounded by $O(K\log T&#x2F;\Delta^2)$, which is a significantly better result than previous works that either have a weaker player-pessimal stable matching objective or apply only to markets with special assumptions. Additionally, the paper shows that the regret upper bound matches the previously derived lower bound when the preferences of participants satisfy some special conditions.<details>
<summary>Abstract</summary>
The problem of matching markets has been studied for a long time in the literature due to its wide range of applications. Finding a stable matching is a common equilibrium objective in this problem. Since market participants are usually uncertain of their preferences, a rich line of recent works study the online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms). Most previous works in this line are only able to derive theoretical guarantees for player-pessimal stable regret, which is defined compared with the players' least-preferred stable matching. However, under the pessimal stable matching, players only obtain the least reward among all stable matchings. To maximize players' profits, player-optimal stable matching would be the most desirable. Though \citet{basu21beyond} successfully bring an upper bound for player-optimal stable regret, their result can be exponentially large if players' preference gap is small. Whether a polynomial guarantee for this regret exists is a significant but still open problem. In this work, we provide a new algorithm named explore-then-Gale-Shapley (ETGS) and show that the optimal stable regret of each player can be upper bounded by $O(K\log T/\Delta^2)$ where $K$ is the number of arms, $T$ is the horizon and $\Delta$ is the players' minimum preference gap among the first $N+1$-ranked arms. This result significantly improves previous works which either have a weaker player-pessimal stable matching objective or apply only to markets with special assumptions. When the preferences of participants satisfy some special conditions, our regret upper bound also matches the previously derived lower bound.
</details>
<details>
<summary>摘要</summary>
问题的匹配市场已经在文献中进行了长时间的研究，因为它在各种应用场景中具有广泛的应用前景。在这个问题中，找到一个稳定的匹配是一种常见的平衡目标。由于市场参与者通常对他们的偏好不甚清楚，因此一推最近的研究在在线设置下研究了参与者在多轮互动中学习他们未知的偏好。大多数前一些工作只能 deriv theoretically guarantees for player-pessimal stable regret，它是基于参与者最差偏好的稳定匹配中的最低奖励。然而，在最低稳定匹配下，参与者只能获得所有稳定匹配中最低的奖励。为了提高参与者的收益，参与者最佳稳定匹配是最感到满意的。虽然 \citet{basu21beyond} 成功地提出了一个Upper bound for player-optimal stable regret，但其结果可能会是指数增长的，如果参与者偏好的差距很小。whether a polynomial guarantee for this regret exists is a significant but still open problem。在这个工作中，我们提出了一个新的算法名为explore-then-Gale-Shapley（ETGS），并证明了每个参与者的最佳稳定 regret可以 upper bounded by $O(K\log T/\Delta^2)$，where $K$ is the number of arms, $T$ is the horizon, and $\Delta$ is the participants' minimum preference gap among the first $N+1$-ranked arms。这个结果比前一些工作更好，因为它们的目标是player-pessimal stable matching，或者只适用于特殊的市场假设。当参与者的偏好满足某些特殊条件时，我们的 regret upper bound也与之前 derive的下界匹配。
</details></li>
</ul>
<hr>
<h2 id="What-Twitter-Data-Tell-Us-about-the-Future"><a href="#What-Twitter-Data-Tell-Us-about-the-Future" class="headerlink" title="What Twitter Data Tell Us about the Future?"></a>What Twitter Data Tell Us about the Future?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02035">http://arxiv.org/abs/2308.02035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alina Landowska, Marek Robak, Maciej Skorski</li>
<li>For: This paper investigates the futures projected by futurists on Twitter and explores the impact of language cues on anticipatory thinking among social media users.* Methods: The study uses a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using state-of-the-art models. The research employs topic modeling techniques, such as LDA and BERTopic, to identify the topics and language cues used by futurists.* Results: The study finds 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists’ tweets. The research demonstrates that the futurists’ language cues signal futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in the present.Here are the three information points in Simplified Chinese text:* For: 这个研究 investigate Twitter 上的未来投影和社交媒体用户的预测思维。* Methods: 这个研究使用了超过100万次公共分享的 tweets，并开发了可扩展的自然语言处理管道，使用当前的模型。研究使用了主题模型，如 LDA 和 BERTopic，来标识未来者的话题和语言指示。* Results: 研究发现了 LDA approach 中的15个话题和 BERTopic approach 中的100个不同的话题。研究表明，未来者的语言指示signal未来的形成，使社交媒体用户能够预测自己的enario并在现在 respond to it。<details>
<summary>Abstract</summary>
Anticipation is a fundamental human cognitive ability that involves thinking about and living towards the future. While language markers reflect anticipatory thinking, research on anticipation from the perspective of natural language processing is limited. This study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using SOTA models. The study identifies 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists' tweets. These findings contribute to the research on topic modelling and provide insights into the futures anticipated by Twitter's futurists. The research demonstrates the futurists' language cues signals futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in present. The fully open-sourced dataset, interactive analysis, and reproducible source code are available for further exploration.
</details>
<details>
<summary>摘要</summary>
人类有一种基本的认知能力，即预期（anticipation），它关注未来的发展和生活。虽然语言标记反映了预期思维，但从自然语言处理的角度来研究预期却有限。这项研究目的是Investigate Twitter上的未来预测和社交媒体用户对未来的预测思维的影响。我们解决的研究问题包括Twitter上预测的未来是什么和这些预测如何被社交数据模型化。为了调查这一点，我们提出了相关的研究和语言标记的影响以及著名人士对预期思维的影响，并提出了一个“现在未来”和“未来现在”的分类系统。本研究使用了一亿多个公共分享的推特信息，并开发了一个可扩展的自然语言处理（NLP）管道，使用当前的最佳实践模型。我们从LDA方法和BERTopic方法中提取了15个主题和100个特定主题，这些发现贡献于主题模型研究，并为Twitter上预测未来提供了新的视角。本研究表明预测者的语言标记可以预示未来的发展，使社交媒体用户能够预测和响应他们的enario。我们提供了全部开源的数据集、交互分析和可重复的代码，以便进一步探索。
</details></li>
</ul>
<hr>
<h2 id="Risk-optimized-Outlier-Removal-for-Robust-Point-Cloud-Classification"><a href="#Risk-optimized-Outlier-Removal-for-Robust-Point-Cloud-Classification" class="headerlink" title="Risk-optimized Outlier Removal for Robust Point Cloud Classification"></a>Risk-optimized Outlier Removal for Robust Point Cloud Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10875">http://arxiv.org/abs/2307.10875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinke Li, Junchi Lu</li>
<li>for: 这个研究的目的是为了提高点云深度模型在安全敏感场景中的可靠性和安全性，因为这些模型可能会受到意外或自然occurring的点云误差的干扰。</li>
<li>methods: 这篇研究提出了一个新的点云异常点除除法，called PointCVaR，可以让标准训练的模型消除额外的异常点和重建数据。这个方法开始是通过做出属性分析，以 determine the influence of each point on the model output，我们称之为点云风险。然后，我们使用Conditional Value at Risk (CVaR) 来优化高风险点的筛选过程。</li>
<li>results: 这篇研究在不同的点云误差情况下，通过了多种移除和分类实验，获得了出色的结果。尤其是在受到随机误差、敌意误差和后门触发误差的攻击下，PointCVaR可以成功地防御这些攻击，并且在这些情况下 achieves 87% 的精度。<details>
<summary>Abstract</summary>
The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exceptional results in various removal-and-classification experiments for noisy point clouds, which are corrupted by random noise, adversarial noise, and backdoor trigger noise. Impressively, it achieves 87% accuracy in defense against the backdoor attack by removing triggers. Overall, the proposed PointCVaR effectively eliminates noise points and enhances point cloud classification, making it a promising plug-in module for various models in different scenarios.
</details>
<details>
<summary>摘要</summary>
“随着深度点云模型在安全敏感领域的普及，这些模型对于意外或自然发生的点云噪音的可靠性和安全性受到损害。为了解决这个问题，我们提出了一个新的点云异常点除除法 called PointCVaR，它让标准训练的模型能够更好地消除额外的异常点和重建数据。我们的方法开始 WITH 点云影响分析，决定每个点的影响力，我们称之为点风险。然后，我们使用 Conditional Value at Risk（CVaR）来优化高风险点的范例。我们发现点云噪音通常集中在风险分布的尾部，有较低的频率但高度的风险，导致分类结果受到干扰。尽管不需要额外的训练努力，我们的方法在不同的实验中获得了出色的成绩，包括随机噪音、敌意噪音和后门触发噪音降落。特别是，它在防御后门攻击时取得了87%的准确率。总的来说，我们的PointCVaR可以干扰点云噪音，提高点云分类，使其成为不同情况下的实用插件模组。”
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Meta-Learning-Can-Guarantee-Faster-Rates"><a href="#Nonlinear-Meta-Learning-Can-Guarantee-Faster-Rates" class="headerlink" title="Nonlinear Meta-Learning Can Guarantee Faster Rates"></a>Nonlinear Meta-Learning Can Guarantee Faster Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10870">http://arxiv.org/abs/2307.10870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Meunier, Zhu Li, Arthur Gretton, Samory Kpotufe</li>
<li>for: 本研究的目标是为meta-学习提供理论保证，以便在相关任务之间共享表示结构，从而简化目标任务。</li>
<li>methods: 本研究使用了非线性表示，并采用了严格的常见函数 régularization来约束任务特有的偏误。</li>
<li>results: 研究人员通过 teoretic 分析和实验 validate了meta-学习的非线性表示下的保证，并证明了随着任务数($N$)的增加，学习共享表示的速率可以scale。<details>
<summary>Abstract</summary>
Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonlinearity maps to an infinite-dimensional RKHS, we show that additional biases can be mitigated with careful regularization that leverages the smoothness of task-specific regression functions,
</details>
<details>
<summary>摘要</summary>
很多最近的理论工作在meta-学中目标是利用相似的表示结构来简化目标任务。重要的是，理论工作的主要目标是理解学习共享表示结构时速度如何随着任务数量 $N$ 和样本数量的增加而增长。在首先步骤中，当共享表示结构和任务特定的回归函数都是线性的时，这种性质 readily reveals the benefits of task aggregation，例如，通过平均Arguments。然而，在实践中，表示结构通常是非线性的，引入了每个任务中的非轻松偏见，这些偏见无法如linear case中那样平均化。在 presente 工作中，我们 derive theoretical guarantees for meta-学with nonlinear representations。具体来说，我们假设共享非线性映射到了无穷dimensional RKHS中，我们显示了适当的 regularization可以减轻任务特定的偏见，同时利用任务特定的回归函数的平滑性。
</details></li>
</ul>
<hr>
<h2 id="Performance-Issue-Identification-in-Cloud-Systems-with-Relational-Temporal-Anomaly-Detection"><a href="#Performance-Issue-Identification-in-Cloud-Systems-with-Relational-Temporal-Anomaly-Detection" class="headerlink" title="Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection"></a>Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10869">http://arxiv.org/abs/2307.10869</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ase-submission/rtanomaly">https://github.com/ase-submission/rtanomaly</a></li>
<li>paper_authors: Wenwei Gu, Jinyang Liu, Zhuangbin Chen, Jianping Zhang, Yuxin Su, Jiazhen Gu, Cong Feng, Zengyin Yang, Michael Lyu</li>
<li>for: 本研究旨在提高大规模云服务系统的可靠性和性能，通过准确地识别和定位问题。</li>
<li>methods: 本研究提出了一种基于关系和时间特征的多变量异常检测模型（RTAnomaly），通过图注意层学习 metrics 之间的依赖关系，更好地发现异常 metrics。</li>
<li>results: 对于公共数据集和两个工业数据集，RTAnomaly 与基eline模型进行比较，实现了平均 F1 分数为 0.929 和 Hit@3 为 0.920，表明RTAnomaly 的优越性。<details>
<summary>Abstract</summary>
Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses. To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics. Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans. Some existing methods tackle this problem by analyzing each metric independently to detect anomalies. However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually. To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem. However, most of the studies fall short of extracting these two types of features explicitly. Moreover, there exist some unlabeled anomalies mixed in the training data, which may hinder the detection performance. To address these limitations, we propose the Relational- Temporal Anomaly Detection Model (RTAnomaly) that combines the relational and temporal information of metrics. RTAnomaly employs a graph attention layer to learn the dependencies among metrics, which will further help pinpoint the anomalous metrics that may cause the anomaly effectively. In addition, we exploit the concept of positive unlabeled learning to address the issue of potential anomalies in the training data. To evaluate our method, we conduct experiments on a public dataset and two industrial datasets. RTAnomaly outperforms all the baseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920, demonstrating its superiority.
</details>
<details>
<summary>摘要</summary>
大规模云服务系统中的性能问题会导致重大的收益损失。为确保可靠性，需要准确地识别和定位这些问题使用服务监控指标。由于现代云系统的复杂性和规模，这可能是一项具有挑战性和需要专业知识和资源的任务。现有的方法可能会分析每个指标独立地检测异常。然而，这可能会导致过载的警示，使得工程师难以手动诊断。为了提高性能，不仅需要考虑时间序列中的指标异常，还需要考虑指标之间的相互关系（即关系异常），这可以被视为多变量指标异常检测问题。然而，大多数研究都没有明确提取这两种特征。此外，存在在训练数据中的未标注异常，可能会降低检测性能。为解决这些限制，我们提出了关系时间异常检测模型（RTAnomaly），该模型将指标之间的关系和时间序列信息结合使用。RTAnomaly使用图注意层学习指标之间的依赖关系，以更好地发现可能导致异常的异常指标。此外，我们利用未标注异常学习的概念，以Address the issue of potential anomalies in the training data。为评估我们的方法，我们在公共数据集和两个工业数据集上进行了实验。结果显示，RTAnomaly在所有基线模型之上具有平均F1分数0.929和 Hit@3 0.920，这表明它的优势。
</details></li>
</ul>
<hr>
<h2 id="FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback"><a href="#FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback" class="headerlink" title="FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback"></a>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10867">http://arxiv.org/abs/2307.10867</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/figcapshf/figcapshf">https://github.com/figcapshf/figcapshf</a></li>
<li>paper_authors: Ashish Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nikos Vlassis, Ryan A. Rossi</li>
<li>for: 这个论文主要是为了解决科学文献中图文合成的问题，提高图文合成的质量和准确性。</li>
<li>methods: 该论文使用了一种新的框架，即 FigCaps-HF，来生成图文合成。该框架包括自动评估图文对的质量以及基于人工反馈的学习方法，以优化图文合成的质量和准确性。</li>
<li>results: 该论文通过对不同类型的模型进行比较，证明了 FigCaps-HF 框架可以提高图文合成的性能。特别是，当使用 BLIP 作为基础模型时，RLHF 方法可以获得一个平均提升率达 35.7%、16.9% 和 9% 在 ROUGE、BLEU 和 Meteor 等指标中。此外，该论文还释放了一个大规模的 benchmark 数据集，以便进一步评估和发展 RLHF 技术。<details>
<summary>Abstract</summary>
Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>科学视觉和文档中的标题是非常重要的，现有的科学标题生成方法都是基于文档中提取的figure-caption对，但是这些方法 frequently fall short （15），导致生成的标题与读者首选不符。为了生成高质量的标题，我们介绍了FigCaps-HF，一个新的标题生成框架，可以在读者首选的基础上生成标题。我们的框架包括以下两个部分：1. 一种自动评估figure-caption对的质量方法。2. 一种基于人工反馈的强化学习（RLHF）方法，用于优化一个生成figure-to-caption模型，以满足读者首选。我们的简单学习框架在不同的模型上进行了标准化finetuning后，都能够提高性能。特别是当使用BLIP作为基础模型时，我们的RLHF框架实现了ROUGE、BLEU和Meteor等指标中的平均提升为35.7%、16.9%和9%。最后，我们发布了一个大规模的人工反馈 benchmark dataset，以便进一步评估和发展RLHF技术。
</details></li>
</ul>
<hr>
<h2 id="Addressing-caveats-of-neural-persistence-with-deep-graph-persistence"><a href="#Addressing-caveats-of-neural-persistence-with-deep-graph-persistence" class="headerlink" title="Addressing caveats of neural persistence with deep graph persistence"></a>Addressing caveats of neural persistence with deep graph persistence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10865">http://arxiv.org/abs/2307.10865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/Deep-Graph-Persistence">https://github.com/ExplainableML/Deep-Graph-Persistence</a></li>
<li>paper_authors: Leander Girrbach, Anders Christensen, Ole Winther, Zeynep Akata, A. Sophia Koepke</li>
<li>for: 这个论文的目的是为了提出一种新的深度学习中的数据分析方法，以及一种基于这种方法的深度网络复杂度量度。</li>
<li>methods: 这个论文使用了topological数据分析的方法，以及一种新的层间拟合方法来处理深度网络。</li>
<li>results: 研究发现，深度网络的层次结构和大量 weights 的分布是决定 neural persistence 的两大因素。此外，通过对深度网络进行扩展，可以解决 variance 相关的问题，并且可以准确地量度深度网络的复杂度。<details>
<summary>Abstract</summary>
Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation. Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence .
</details>
<details>
<summary>摘要</summary>
neural  persistency 是一种深度学习中的核心度量，在 topological data analysis 领域中提出。在这种工作中，我们发现了论理和实验两个方面的结论：网络权重的方差和大权重的空间吸引力是影响 neural persistency 的主要因素。这些信息对于线性分类器是有用的，但我们发现了深层神经网络中的后Layer没有相关的空间结构，因此 neural persistency 大致相当于网络权重的方差。此外，对于深度神经网络，层融合策略不考虑层之间的交互。基于我们的分析，我们提出了层拓扑下的 filtration 扩展，该扩展等于在一个特定矩阵上计算 neural persistency。这个方法会隐式地包含神经网络中的持续路径和减少方差相关的问题。代码可以在 <https://github.com/ExplainableML/Deep-Graph-Persistence> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing"><a href="#Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing" class="headerlink" title="Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing"></a>Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10864">http://arxiv.org/abs/2307.10864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva</li>
<li>for: 这篇论文的目的是提出一种基于Generative Semantic Nursing（GSN）的方法，用于解决复杂的提示问题和多个实体之间的属性绑定问题。</li>
<li>methods: 该方法使用了两个新的损失函数：一个新的注意力损失函数和一个绑定损失函数，以提高GSN的表现。</li>
<li>results: 该方法在多个评估标准上表现出色，能够准确地synthesize所需的对象，并且Attribute binding更加紧密。更多视频和更新可以在项目页面上找到：<a target="_blank" rel="noopener" href="https://sites.google.com/view/divide-and-bind">https://sites.google.com/view/divide-and-bind</a>。<details>
<summary>Abstract</summary>
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks. More videos and updates can be found on the project page \url{https://sites.google.com/view/divide-and-bind}.
</details>
<details>
<summary>摘要</summary>
新型大规模文本至图生成模型，如稳定扩散（SD），已经显示出惊人的成果，具有高准确性。然而，当前领先的模型仍然难以生成完全遵循输入提示的图像。先前的工作，听取与激发（Attend & Excite），引入了生成 semantic nursing（GSN）的概念，通过在推理时间进行交叉注意力优化，以更好地包含 semantics。它在生成简单提示（例如，“一只猫和一只狗”）中显示了扎实的成果。然而，其效果在处理更复杂的提示时下降，并不直接地解决不正确的属性绑定问题。为了解决复杂提示或场景中多个实体的挑战，以及提高属性绑定的问题，我们提出了分区与绑定（Divide & Bind）方法。我们引入了两种新的损失目标：一种新的注意力损失和一种绑定损失。我们的方法在处理复杂提示下能够准确地生成愿景中的目标对象，并且具有改进的属性Alignment。更多视频和更新可以在项目页面（<https://sites.google.com/view/divide-and-bind>）中找到。
</details></li>
</ul>
<hr>
<h2 id="Self-paced-Weight-Consolidation-for-Continual-Learning"><a href="#Self-paced-Weight-Consolidation-for-Continual-Learning" class="headerlink" title="Self-paced Weight Consolidation for Continual Learning"></a>Self-paced Weight Consolidation for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10845">http://arxiv.org/abs/2307.10845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/congwei45/spWC">https://github.com/congwei45/spWC</a></li>
<li>paper_authors: Wei Cong, Yang Cong, Gan Sun, Yuyang Liu, Jiahua Dong</li>
<li>for: 这个研究旨在提高sequential task learning中的continual learning效能，并避免catastrophic forgetting这个问题。</li>
<li>methods: 我们提出了一个自适应的Weight Consolidation（spWC）框架，通过评估先前任务的推导性贡献来实现 Robust continual learning。我们还开发了一个自适应的调整方法，可以根据关键性表现指标（例如精度）来评估过去任务的困难程度。</li>
<li>results: 我们的方法可以对多个任务进行sequential learning，并且可以实现better performance和less computational cost。实验结果显示，我们的方法可以与其他流行的continual learning算法相比，在多个公共Benchmark dataset上实现更好的效果。<details>
<summary>Abstract</summary>
Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings. However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks. To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks. To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy). When encountering a new task, all previous tasks are sorted from "difficult" to "easy" based on the priorities. Then the parameters of the new continual learner will be learned via selectively maintaining the knowledge amongst more difficult past tasks, which could well overcome catastrophic forgetting with less computational cost. We adopt an alternative convex search to iteratively update the model parameters and priority weights in the bi-convex formulation. The proposed spWC framework is plug-and-play, which is applicable to most continual learning algorithms (e.g., EWC, MAS and RCIL) in different directions (e.g., classification and segmentation). Experimental results on several public benchmark datasets demonstrate that our proposed framework can effectively improve performance when compared with other popular continual learning algorithms.
</details>
<details>
<summary>摘要</summary>
CONTINUAL LEARNING算法，它们保持新任务参数与前一个任务相似，在Sequential task learning setting中很受欢迎。但是，1）新的 continual learner的性能将受到前一个任务的贡献的影响，而无法分别评估这些贡献；2）随着任务的增加，现有的算法的计算成本将增加很多，因为它们需要对所有任务进行Regularization。为解决这些挑战，我们提出了一个自适应Weight Consolidation（spWC）框架，以实现Robust continual learning。具体来说，我们开发了一种自适应Regularization，通过测量难度来评估过去任务的优先级。当遇到新任务时，我们将所有过去任务排序为“difficult”到“easy”的顺序，根据优先级。然后，我们将新的 continual learner的参数学习 via 选择保留过去任务中更难的知识。这可以很好地解决catastrophic forgetting问题，同时降低计算成本。我们采用了一种alternative convex search来逐步更新模型参数和优先级权重。我们的spWC框架是可插入的，可以应用于大多数 continual learning算法（例如EWC、MAS和RCIL）以及不同的方向（例如分类和分割）。实验结果表明，我们的提议可以在多个公共 benchmark dataset上提高性能，相比其他流行的 continual learning算法。
</details></li>
</ul>
<hr>
<h2 id="Global-Precipitation-Nowcasting-of-Integrated-Multi-satellitE-Retrievals-for-GPM-A-U-Net-Convolutional-LSTM-Architecture"><a href="#Global-Precipitation-Nowcasting-of-Integrated-Multi-satellitE-Retrievals-for-GPM-A-U-Net-Convolutional-LSTM-Architecture" class="headerlink" title="Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture"></a>Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10843">http://arxiv.org/abs/2307.10843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reyhaneh-92/genesis_nowcast">https://github.com/reyhaneh-92/genesis_nowcast</a></li>
<li>paper_authors: Reyhaneh Rahimi, Ardeshir Ebtehaj, Ali Behrangi, Jackson Tan</li>
<li>for: 这个研究旨在开发一种深度学习架构，用于全球范围内降水预测，每30分钟预测4小时前的降水情况。</li>
<li>methods: 这个架构结合了U-Net和卷积长Short-Term Memory（LSTM）神经网络，并使用IMERG和一些关键的降水驱动因素从全球预测系统（GFS）来训练。</li>
<li>results: 研究发现，使用不同的训练损失函数，包括平均方差（回归）和焦点损失（分类），对降水预测质量有着不同的影响。结果表明，回归网络在降水轻度（下1.6毫米&#x2F;小时）方面表现良好，而分类网络在降水极端（大于8毫米&#x2F;小时）方面可以超过回归网络，以 Critical Success Index（CSI）为标准。同时，包含物理变量可以提高降水预测，特别是在较长的预测时间内。<details>
<summary>Abstract</summary>
This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time. The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS). The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied. The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (>8 mm/hr), in terms of the critical success index (CSI).. Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probability distribution to the IMERG than the regression network. It is uncovered that the inclusion of the physical variables can improve precipitation nowcasting, especially at longer lead times in both networks. Taking IMERG as a relative reference, a multi-scale analysis in terms of fractions skill score (FSS), shows that the nowcasting machine remains skillful (FSS > 0.5) at the resolution of 10 km compared to 50 km for GFS. For precipitation rates greater than 4~mm/hr, only the classification network remains FSS-skillful on scales greater than 50 km within a 2-hour lead time.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Label-Calibration-for-Semantic-Segmentation-Under-Domain-Shift"><a href="#Label-Calibration-for-Semantic-Segmentation-Under-Domain-Shift" class="headerlink" title="Label Calibration for Semantic Segmentation Under Domain Shift"></a>Label Calibration for Semantic Segmentation Under Domain Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10842">http://arxiv.org/abs/2307.10842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondrej Bohdal, Da Li, Timothy Hospedales</li>
<li>for: 这篇论文是用于测试一个预训 semantic segmentation 模型在新Domain上的性能是否会严重下降。</li>
<li>methods: 这篇论文使用了一种基于域Shift的预训模型进行适应，通过计算软 labels prototype 并根据最相似的分类概率 vector 进行预测。</li>
<li>results: 论文显示了这种适应方法可以很快速、几乎不需要更多的计算资源，并且能够提高性能。它还证明了这种适应方法在实际上是非常有用的。<details>
<summary>Abstract</summary>
Performance of a pre-trained semantic segmentation model is likely to substantially decrease on data from a new domain. We show a pre-trained model can be adapted to unlabelled target domain data by calculating soft-label prototypes under the domain shift and making predictions according to the prototype closest to the vector with predicted class probabilities. The proposed adaptation procedure is fast, comes almost for free in terms of computational resources and leads to considerable performance improvements. We demonstrate the benefits of such label calibration on the highly-practical synthetic-to-real semantic segmentation problem.
</details>
<details>
<summary>摘要</summary>
“一个先进的语义分割模型在新领域数据上的性能可能会减退很多。我们显示了一个先进模型可以通过计算域转移下的软标签 прототипы，并根据最相似的 вектор预测类别概率来进行预测。我们提出的适应过程快速、计算资源几乎没有成本，并导致了显著的性能提升。我们在实际上非常有用的 sintetic-to-real语义分割问题中展示了这种标签准确化的好处。”Note: "域转移" (domain shift) is translated as "域转移" in Simplified Chinese, and "soft-label" is translated as "软标签" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Conversational-Shaping-for-Intelligent-Agents"><a href="#Adversarial-Conversational-Shaping-for-Intelligent-Agents" class="headerlink" title="Adversarial Conversational Shaping for Intelligent Agents"></a>Adversarial Conversational Shaping for Intelligent Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11785">http://arxiv.org/abs/2307.11785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Tarasiewicz, Sultan Kenjeyev, Ilana Sebag, Shehab Alshehabi</li>
<li>for: 提高对话代理人的智能会话系统稳定性和准确性</li>
<li>methods: 使用生成对抗网络（GANPG）和奖励每一个生成步骤（REGS）模型，并在seq2seq和 transformers 框架下进行强化学习</li>
<li>results: 通过不同的训练细节，模型可以提高对话代理人的性能和可靠性<details>
<summary>Abstract</summary>
The recent emergence of deep learning methods has enabled the research community to achieve state-of-the art results in several domains including natural language processing. However, the current robocall system remains unstable and inaccurate: text generator and chat-bots can be tedious and misunderstand human-like dialogue. In this work, we study the performance of two models able to enhance an intelligent conversational agent through adversarial conversational shaping: a generative adversarial network with policy gradient (GANPG) and a generative adversarial network with reward for every generation step (REGS) based on the REGS model presented in Li et al. [18] . This model is able to assign rewards to both partially and fully generated text sequences. We discuss performance with different training details : seq2seq [ 36] and transformers [37 ] in a reinforcement learning framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems"><a href="#What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems" class="headerlink" title="What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems"></a>What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11784">http://arxiv.org/abs/2307.11784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saddek Bensalem, Chih-Hong Cheng, Wei Huang, Xiaowei Huang, Changshun Wu, Xingyu Zhao</li>
<li>for: 本文旨在提出一种可靠地在安全关键领域使用学习能力的方法，以确保系统的安全性。</li>
<li>methods: 本文提出了一种两步验证方法，以实现可证明的统计保证。</li>
<li>results: 本文认为，现有的方法无法实际实现可证明的保证，therefore promoting the two-step verification method for achieving provable statistical guarantees.<details>
<summary>Abstract</summary>
Machine learning has made remarkable advancements, but confidently utilising learning-enabled components in safety-critical domains still poses challenges. Among the challenges, it is known that a rigorous, yet practical, way of achieving safety guarantees is one of the most prominent. In this paper, we first discuss the engineering and research challenges associated with the design and verification of such systems. Then, based on the observation that existing works cannot actually achieve provable guarantees, we promote a two-step verification method for the ultimate achievement of provable statistical guarantees.
</details>
<details>
<summary>摘要</summary>
机器学习技术已经取得了很大的进步，但在安全关键领域使用学习能力的组件仍然存在挑战。其中一个最大的挑战是实现可靠的安全保证。在这篇论文中，我们首先讨论了设计和验证这些系统的工程和研究挑战。然后，根据现有的工作无法实现可证的保证，我们提出了两步验证方法以实现可证的统计保证。
</details></li>
</ul>
<hr>
<h2 id="On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport"><a href="#On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport" class="headerlink" title="On Combining Expert Demonstrations in Imitation Learning via Optimal Transport"></a>On Combining Expert Demonstrations in Imitation Learning via Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10810">http://arxiv.org/abs/2307.10810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning">https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning</a></li>
<li>paper_authors: Ilana Sebag, Samuel Cohen, Marc Peter Deisenroth</li>
<li>for: 教学Agent特定任务 через专家示范</li>
<li>methods: 使用优化运输方法测量Agent和专家轨迹之间的距离，并将多个专家示范合并在OT上</li>
<li>results: 在OpenAI Gym控制环境中，提出了一种使用多个专家示范的方法，并分析了其效率，发现标准方法不总是最优<details>
<summary>Abstract</summary>
Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environments and demonstrates that the standard method is not always optimal.
</details>
<details>
<summary>摘要</summary>
copied from clipboard模仿学习（IL）目的是教导代理人特定任务通过专家示范。一种关键的IL方法是定义代理人和专家之间的距离，并找到一个代理人策略，以最小化这个距离。优质运输方法在模仿学习中广泛应用，它们提供了测量代理人和专家轨迹之间有意义的距离的方法。然而，多个专家示范的组合尚未得到广泛的研究。标准方法是简单地 concatenate 状态(-动作)轨迹，这会导致轨迹是多模的。我们提出了一种 альтернатив 方法，使用多个多重最优运输距离，使得多个和多样的状态轨迹在OT意义上能够合理地组合，提供一个更加有意义的 geometric average 的示范。我们的方法允许代理人从多个专家中学习，并在 OpenAI Gym 控制环境中进行了效率分析，结果显示，标准方法并不总是优化的。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression"><a href="#Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression" class="headerlink" title="Communication-Efficient Split Learning via Adaptive Feature-Wise Compression"></a>Communication-Efficient Split Learning via Adaptive Feature-Wise Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10805">http://arxiv.org/abs/2307.10805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongjeong Oh, Jaeho Lee, Christopher G. Brinton, Yo-Seb Jeon</li>
<li>for: 提高分布式学习中通信开销的减少</li>
<li>methods: 利用矩阵列中具有不同分散度的特征进行压缩，并采用适应式dropout和适应式量化策略</li>
<li>results: 与现有分布式学习框架相比，提供5.6%以上的分类精度提升，同时减少了320倍的通信开销<details>
<summary>Abstract</summary>
This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process. The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices. SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization. In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped. In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantization levels of this strategy are derived in a closed-form expression. Simulation results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC provides more than a 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while they require 320 times less communication overhead compared to the vanilla SL framework without compression.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Adaptive feature-wise dropout: The intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped.2. Adaptive feature-wise quantization: The non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantization levels of this strategy are derived in a closed-form expression.Simulation results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC provides more than a 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while they require 320 times less communication overhead compared to the vanilla SL framework without compression.Translation in Simplified Chinese:这篇论文提出了一种新的通信减少的split学习（SL）框架，名为SplitFC，它降低了在SL训练过程中传输中间特征和梯度 вектор的通信开销。SplitFC的关键思想是利用不同的散度度在矩阵列中。SplitFC包括两种压缩策略：1. 适应特征 wise dropout：中间特征 вектор通过适应dropout概率确定了dropout probabilities，然后根据链规则，相关的中间梯度 вектор也会被Drop。2. 适应特征 wise quantization：未Drop的中间特征和梯度 вектор通过适应压缩水平确定了压缩级别，以避免压缩误差。压缩级别的优化准确表达得到了关闭式表达。实验结果表明，SplitFC在MNIST、CIFAR-10和CelebA datasets上提供了 más de 5.6%的分类精度提升，同时与无压缩SL框架相比，它需要320倍少的通信开销。</details></li>
</ol>
<hr>
<h2 id="Spatial-Temporal-Data-Mining-for-Ocean-Science-Data-Methodologies-and-Opportunities"><a href="#Spatial-Temporal-Data-Mining-for-Ocean-Science-Data-Methodologies-and-Opportunities" class="headerlink" title="Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities"></a>Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10803">http://arxiv.org/abs/2307.10803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanchen Yang, Wengen Li, Shuyu Wang, Hui Li, Jihong Guan, Shuigeng Zhou, Jiannong Cao</li>
<li>For: This paper provides a comprehensive survey of existing spatial-temporal data mining (STDM) studies for ocean science, including a review of widely-used ST ocean datasets and their unique characteristics, as well as techniques for data quality enhancement and various STDM tasks.* Methods: The paper reviews and discusses various techniques for STDM in ocean science, including data preprocessing, feature extraction, and machine learning algorithms for tasks such as prediction, event detection, pattern mining, and anomaly detection.* Results: The paper highlights the unique challenges and opportunities of STDM in ocean science, and discusses promising research opportunities in this field, including the application of advanced STDM techniques to climate forecasting and disaster warning.<details>
<summary>Abstract</summary>
With the rapid amassing of spatial-temporal (ST) ocean data, many spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, including climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated but with unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models on ST ocean data. To the best of our knowledge, a comprehensive survey of existing studies remains missing in the literature, which hinders not only computer scientists from identifying the research issues in ocean data mining but also ocean scientists to apply advanced STDM techniques. In this paper, we provide a comprehensive survey of existing STDM studies for ocean science. Concretely, we first review the widely-used ST ocean datasets and highlight their unique characteristics. Then, typical ST ocean data quality enhancement techniques are explored. Next, we classify existing STDM studies in ocean science into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate on the techniques for these tasks. Finally, promising research opportunities are discussed. This survey can help scientists from both computer science and ocean science better understand the fundamental concepts, key techniques, and open challenges of STDM for ocean science.
</details>
<details>
<summary>摘要</summary>
随着空间时间（ST）海洋数据的快速汇集，许多空间时间数据挖掘（STDM）研究已经进行以解决海洋问题，如气候预测和灾害警告。相比一般ST数据（例如交通数据），ST海洋数据更加复杂，但具有独特特征，如多样性和高稀畴性。这些特征使得设计和训练STDM模型对ST海洋数据变得更加困难。据我们所知，现有的相关研究检索在文献中缺失，这不仅阻碍了计算机科学家从海洋数据挖掘中了解研究问题，也阻碍了海洋科学家应用先进的STDM技术。在这篇论文中，我们提供了海洋科学领域的全面的STDM研究检索。具体来说，我们首先评论了广泛使用的ST海洋数据集和其独特特征。然后，我们探讨了一般ST海洋数据质量提升技术。接着，我们分类了现有的STDM研究，并详细介绍了这些任务的技术。最后，我们讨论了有前途的研究机遇。这种检索可以帮助计算机科学家和海洋科学家更好地理解STDM的基本概念、关键技术和开放的挑战，以及在海洋科学领域应用STDM技术的可能性。
</details></li>
</ul>
<hr>
<h2 id="Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning"><a href="#Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning" class="headerlink" title="Meta-Transformer: A Unified Framework for Multimodal Learning"></a>Meta-Transformer: A Unified Framework for Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10802">http://arxiv.org/abs/2307.10802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/invictus717/MetaTransformer">https://github.com/invictus717/MetaTransformer</a></li>
<li>paper_authors: Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue</li>
<li>for: 这个研究旨在建立一个能够处理多种模式的模型，并将其与不同的模式进行关联。</li>
<li>methods: 这个方法使用一个冻结的encoder来进行多模式认识，并将不同的输入数据maps到共同的token空间，以EXTRACT高级 semantic feature。</li>
<li>results: 这个方法可以在12种模式上进行通用的学习，包括基本的认识(文本、图像、点 cloud、音频、影片)、实际应用(X-ray、infrared、颜色、IMU)和数据采矿(图形、表格、时间序列)。<details>
<summary>Abstract</summary>
Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer
</details>
<details>
<summary>摘要</summary>
多Modal学习旨在建立处理多种模式的模型。尽管多年的发展，仍然困难设计处理多种模式的统一网络（例如自然语言、2D图像、3D点云、音频、视频、时间序列、表格数据）的模型，因为这些模式之间存在隐藏的差异。在这项工作中，我们提出了一个框架，名为Meta-Transformer，它利用一个冻结的Encoder来实现多Modal感知，无需任何对准的多Modal训练数据。Meta-Transformer框架由三个主要组件组成：一个统一的数据Tokenizer、一个共享Encoder和下游任务的任务特定头。Meta-Transformer是首个在12种模式上进行统一学习的框架，无需对数据进行匹配。在不同的Benchmark上进行的实验表明，Meta-Transformer可以处理各种任务，包括基本的感知（文本、图像、点云、音频、视频）、实用应用（X射线、红外、偏振、IMU）和数据挖掘（图形、表格、时间序列）。Meta-Transformer表明了未来在使用Transformer进行多Modal智能的发展具有扎实的前景。代码将在https://github.com/invictus717/MetaTransformer上提供。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Adam-for-Non-convex-Objectives-Relaxed-Hyperparameters-and-Non-ergodic-Case"><a href="#Convergence-of-Adam-for-Non-convex-Objectives-Relaxed-Hyperparameters-and-Non-ergodic-Case" class="headerlink" title="Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case"></a>Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11782">http://arxiv.org/abs/2307.11782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meixuan He, Yuqing Liang, Jinlan Liu, Dongpo Xu</li>
<li>for:  investigate the convergence properties of Adam algorithm in non-convex settings and develop a better understanding of its performance.</li>
<li>methods:  introduce precise definitions of ergodic and non-ergodic convergence, and establish a weaker sufficient condition for the ergodic convergence guarantee of Adam.</li>
<li>results:  prove that the last iterate of Adam converges to a stationary point for non-convex objectives, and obtain the non-ergodic convergence rate of $O(1&#x2F;K)$ for function values under the Polyak-Lojasiewicz (PL) condition.<details>
<summary>Abstract</summary>
Adam is a commonly used stochastic optimization algorithm in machine learning. However, its convergence is still not fully understood, especially in the non-convex setting. This paper focuses on exploring hyperparameter settings for the convergence of vanilla Adam and tackling the challenges of non-ergodic convergence related to practical application. The primary contributions are summarized as follows: firstly, we introduce precise definitions of ergodic and non-ergodic convergence, which cover nearly all forms of convergence for stochastic optimization algorithms. Meanwhile, we emphasize the superiority of non-ergodic convergence over ergodic convergence. Secondly, we establish a weaker sufficient condition for the ergodic convergence guarantee of Adam, allowing a more relaxed choice of hyperparameters. On this basis, we achieve the almost sure ergodic convergence rate of Adam, which is arbitrarily close to $o(1/\sqrt{K})$. More importantly, we prove, for the first time, that the last iterate of Adam converges to a stationary point for non-convex objectives. Finally, we obtain the non-ergodic convergence rate of $O(1/K)$ for function values under the Polyak-Lojasiewicz (PL) condition. These findings build a solid theoretical foundation for Adam to solve non-convex stochastic optimization problems.
</details>
<details>
<summary>摘要</summary>
亚当是一种常用的机会估计算法在机器学习中。然而，它的整合仍然未全面理解，特别是在非对称设定下。本文专注于探索亚当的参数设定，以提高其在实际应用中的性能。主要贡献如下：首先，我们引入了精确的ergodic和non-ergodic整合定义，这些定义包括大多数 Stochastic optimization算法的整合形式。同时，我们强调non-ergodic整合的superiority。第二，我们提出了一个较弱的充分必要条件，以确保亚当的ergodic整合保证。这允许更加relaxed的参数选择。基于这个基础，我们获得了几乎确定的almost sure ergodic整合速率，它是$o(1/\sqrt{K})$。更重要的是，我们证明了亚当的最后迭代向非对称目标函数 converge。最后，我们取得了非ergodic整合速率为$O(1/K)$，它是PL conditon下的函数值。这些发现建立了亚当在非对称随机估计问题上的坚固理论基础。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection"><a href="#Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection" class="headerlink" title="Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection"></a>Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10792">http://arxiv.org/abs/2307.10792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scortexio/patchcore-few-shot">https://github.com/scortexio/patchcore-few-shot</a></li>
<li>paper_authors: João Santos, Triet Tran, Oliver Rippel</li>
<li>for: 这个论文主要关注于ew-shot anomaly detection（AD）领域的研究，特别是使用只有几个选择的样本来分辨正常和异常数据。</li>
<li>methods: 这篇论文使用了PatchCore，当前的全shot AD&#x2F;AS算法，进行研究，包括优化其多种超参数和将supervised learning中知悉的技术转移到AD领域。</li>
<li>results: 实验表明，可以通过优化超参数和使用图像水平的扩展来实现显著性能提升，并在VisA dataset上实现了新的state of the art。此外，该论文还提出了未来研究的可能性，即研究具有强 inductive bias的特征提取器。<details>
<summary>Abstract</summary>
Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples. While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting. It thus remains unclear if the performance of such pre-existing algorithms can be further improved. We address said question in this work. Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings. We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) significant performance improvements can be realized by optimizing hyperparameters such as the underlying feature extractor, and that (II) image-level augmentations can, but are not guaranteed, to improve performance. Based on these findings, we achieve a new state of the art in few-shot AD on VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to the few-shot setting. Last, we identify the investigation of feature extractors with a strong inductive bias as a potential future research direction for (few-shot) AD/AS.
</details>
<details>
<summary>摘要</summary>
新型异常检测（AD）是一个新趋势的分支，它目标是使用只有几个选择的样本分类正常和异常数据。虽然新提出的几个AD方法会比较旧的全shot预测器，但它们没有专门优化它们为几个shot设定。因此，其性能仍然存在uncertainty。我们在这里解决这个问题。我们对PatchCore，当前的全shotAD/AS算法，在几个shot和多个shot设定下进行AD/AS性能的研究。我们假设可以通过（I）优化其各种超参数，以及（II）将几个shot学习中的技术转移到AD领域来实现性能提高。我们在公共的VisA和MVTec AD datasets上进行了广泛的实验，发现（I）可以通过优化特征提取器来实现显著性能提高，并且（II）图像水平的扩展可以，但并不一定，提高性能。基于这些发现，我们在VisA上实现了新的状态态的AD，进一步证明了适应前 exist AD/AS方法到几个shot设定的价值。最后，我们认为在（几个shot）AD/AS领域 investigating feature extractors with strong inductive bias 是一个可能的未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-attacks-for-mixtures-of-classifiers"><a href="#Adversarial-attacks-for-mixtures-of-classifiers" class="headerlink" title="Adversarial attacks for mixtures of classifiers"></a>Adversarial attacks for mixtures of classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10788">http://arxiv.org/abs/2307.10788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Gnecco Heredia, Benjamin Negrevergne, Yann Chevaleyre</li>
<li>for: 提高鲁棒性 against adversarial attacks</li>
<li>methods: 使用mixtures of classifiers (a.k.a. randomized ensembles)</li>
<li>results: 引入两种攻击性质（有效性和最大化），并证明现有攻击不符合这两种性质。还提出了一种新的攻击方法called lattice climber attack，并在binary linear setting下提供了理论保证，并在 synthetic和实际数据上进行了实验验证。<details>
<summary>Abstract</summary>
Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks. However, it has been shown that existing attacks are not well suited for this kind of classifiers. In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality). We then show that existing attacks do not meet both of these properties. Finally, we introduce a new attack called lattice climber attack with theoretical guarantees on the binary linear setting, and we demonstrate its performance by conducting experiments on synthetic and real datasets.
</details>
<details>
<summary>摘要</summary>
合并分类器（即随机 ensemble）已经被提议用于提高对抗骚扰攻击的Robustness。然而，已经证明现有的攻击方法并不适用于这种类型的分类器。在这篇论文中，我们讨论了攻击混合的问题，并提出了两个愿望的攻击特性（有效性和最大化）。我们then表明现有的攻击方法并不满足这两个特性。最后，我们介绍了一种新的攻击方法called lattice climber attack，并提供了对二分线性设定下的理论保证。我们通过对 sintetic和实际数据进行实验，证明了这种攻击方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Feed-Forward-Source-Free-Domain-Adaptation-via-Class-Prototypes"><a href="#Feed-Forward-Source-Free-Domain-Adaptation-via-Class-Prototypes" class="headerlink" title="Feed-Forward Source-Free Domain Adaptation via Class Prototypes"></a>Feed-Forward Source-Free Domain Adaptation via Class Prototypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10787">http://arxiv.org/abs/2307.10787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondrej Bohdal, Da Li, Timothy Hospedales</li>
<li>for: 本研究旨在探讨源自由领域适应的快速化方法，以替代基于反射传播的适应方法。</li>
<li>methods: 本方法基于预训练模型计算类 prototype，实现了快速化适应并且只需要小量时间。</li>
<li>results: 本研究实现了准确率的显著提升，并且比普通适应方法快速得多。<details>
<summary>Abstract</summary>
Source-free domain adaptation has become popular because of its practical usefulness and no need to access source data. However, the adaptation process still takes a considerable amount of time and is predominantly based on optimization that relies on back-propagation. In this work we present a simple feed-forward approach that challenges the need for back-propagation based adaptation. Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model. It achieves strong improvements in accuracy compared to the pre-trained model and requires only a small fraction of time of existing domain adaptation methods.
</details>
<details>
<summary>摘要</summary>
源自由领域适应已经成为很受欢迎的方法，因为它的实用性和不需要访问源数据。然而，适应过程仍然需要一定的时间，并且主要基于优化，使用反向传播。在这种工作中，我们提出了一种简单的前向方法，挑战需要反向传播基于适应。我们的方法基于使用预训练模型计算类下的prototype，实现了与预训练模型的准确率强劲提高，并且只需一小部分时间。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Beam-Tree-Recursion"><a href="#Efficient-Beam-Tree-Recursion" class="headerlink" title="Efficient Beam Tree Recursion"></a>Efficient Beam Tree Recursion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10779">http://arxiv.org/abs/2307.10779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jishnu Ray Chowdhury, Cornelia Caragea</li>
<li>for: 这个论文的目的是提出一种简单的扩展，以提高 Gumbel Tree RvNN 的长度整合性表现，并维持与其他任务的相似性。</li>
<li>methods: 这个论文使用的方法是识别 BT-RvNN 的主要瓶颈，并提出一些简化其内存使用的策略。</li>
<li>results: 这个论文的结果显示，使用这些策略可以将 BT-RvNN 的内存使用量降低 $10$-$16$ 倍，并创造一个新的州分-of-the-art 在 ListOps 中，同时保持与其他任务的相似性。<details>
<summary>Abstract</summary>
Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.
</details>
<details>
<summary>摘要</summary>
“Beam Tree Recursive Neural Network（BT-RvNN）最近被提出，它是Gumbel Tree RvNN的简单扩展，并在ListOps中实现了状态前瞻性的长度泛化性，同时保持与其他任务的相似性。然而，虽然不是最差的一种，BT-RvNN仍然具有昂贵的内存使用。在这篇论文中，我们确定了BT-RvNN的主要瓶颈是排序函数和循环细胞函数的杂谱。我们提出了一些缓解瓶颈的策略，以降低BT-RvNN的内存使用。总的来说，我们的策略不仅降低了BT-RvNN的内存使用量$10$-$16$倍，还创造了一个新的状态前瞻性在ListOps中，同时保持与其他任务的相似性。此外，我们还提出了利用BT-RvNN生成的潜在树节点表示来将BT-RvNN转换为一个序列Contextualizer的形式$f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$。因此，我们的提议不仅开启了RvNN的扩展可能性，还标准化了使用BT-RvNN作为深度学习工具箱中的另一个构建件，可以轻松堆叠或者与其他流行的模型相互作用，如Transformers和结构化状态空间模型。”
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Use-of-AutoML-for-Data-Driven-Software-Engineering"><a href="#Assessing-the-Use-of-AutoML-for-Data-Driven-Software-Engineering" class="headerlink" title="Assessing the Use of AutoML for Data-Driven Software Engineering"></a>Assessing the Use of AutoML for Data-Driven Software Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10774">http://arxiv.org/abs/2307.10774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Calefato, Luigi Quaranta, Filippo Lanubile, Marcos Kalinowski</li>
<li>for: 填补AI&#x2F;ML技术专业人员短缺的问题，促进自动机器学习（AutoML）的应用。</li>
<li>methods: 使用混合方法研究，包括12种终端AutoML工具在两个SE数据集上的比较，以及对实践者和研究者的调查和访谈。</li>
<li>results: 发现AutoML解决方案可以在SE领域中的分类任务中表现更好 than manually trained and optimized models，但目前可用的AutoML解决方案仍未能完全支持所有队员和开发工作流程的自动化。<details>
<summary>Abstract</summary>
Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can generate models that outperform those trained and optimized by researchers to perform classification tasks in the SE domain. Also, our findings show that the currently available AutoML solutions do not live up to their names as they do not equally support automation across the stages of the ML development workflow and for all the team members. Conclusions. We derive insights to inform the SE research community on how AutoML can facilitate their activities and tool builders on how to design the next generation of AutoML technologies.
</details>
<details>
<summary>摘要</summary>
背景：由于人工智能（AI）和机器学习（ML）在软件开发中的普及，公司困难找到具备深层理解AI/ML技术的员工。在这种情况下，AutoML在解决AI/ML技能差距方面表现出了扎根的优势，因为它承诺自动化建立AI/ML管道，通常需要专业的团队成员进行工程。目标：尽管有增加的兴趣和高期望，但是有关AutoML在开发AI/ML相关系统的团队中的采用和专家和研究人员对其看法的信息不够。方法：为了填补这些空白，本文提出了一项混合方法研究，包括12个终端AutoML工具在两个SE数据集上的benchmark，以及与相关专家和研究人员进行详细交流的用户调查。结果：我们发现AutoML解决方案可以在SE领域中对分类任务进行更好的模型生成，而且我们的发现还表明现有的AutoML解决方案并不能够完全支持自动化ML开发工作流程中的所有阶段和所有团队成员。结论：我们从研究中得到了关于如何使用AutoML促进SE研究人员的活动，以及如何设计下一代AutoML技术的技术建议。
</details></li>
</ul>
<hr>
<h2 id="Music-Genre-Classification-with-ResNet-and-Bi-GRU-Using-Visual-Spectrograms"><a href="#Music-Genre-Classification-with-ResNet-and-Bi-GRU-Using-Visual-Spectrograms" class="headerlink" title="Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms"></a>Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10773">http://arxiv.org/abs/2307.10773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junfei Zhang</li>
<li>for: 提高音乐播放服务的用户体验和满意度，即Music Recommendation Systems。</li>
<li>methods: 使用视觉spectrogram作为输入，并提出了一种 hybrid 模型，结合 Residual neural Network (ResNet) 和 Gated Recurrent Unit (GRU)，以更好地捕捉音乐数据的复杂性。</li>
<li>results: 提出了一种新的 Automatic Music Genre Classification (AMGC) 系统，可以更好地捕捉音乐数据的复杂性，并且可能提高音乐推荐系统的准确率。<details>
<summary>Abstract</summary>
Music recommendation systems have emerged as a vital component to enhance user experience and satisfaction for the music streaming services, which dominates music consumption. The key challenge in improving these recommender systems lies in comprehending the complexity of music data, specifically for the underpinning music genre classification. The limitations of manual genre classification have highlighted the need for a more advanced system, namely the Automatic Music Genre Classification (AMGC) system. While traditional machine learning techniques have shown potential in genre classification, they heavily rely on manually engineered features and feature selection, failing to capture the full complexity of music data. On the other hand, deep learning classification architectures like the traditional Convolutional Neural Networks (CNN) are effective in capturing the spatial hierarchies but struggle to capture the temporal dynamics inherent in music data. To address these challenges, this study proposes a novel approach using visual spectrograms as input, and propose a hybrid model that combines the strength of the Residual neural Network (ResNet) and the Gated Recurrent Unit (GRU). This model is designed to provide a more comprehensive analysis of music data, offering the potential to improve the music recommender systems through achieving a more comprehensive analysis of music data and hence potentially more accurate genre classification.
</details>
<details>
<summary>摘要</summary>
音乐推荐系统已成为现代音乐流媒体服务的重要组成部分，以提高用户体验和满意度。然而，现有的音乐分类方法受到了复杂的音乐数据的限制，尤其是在音乐种类归类方面。传统的机器学习技术可能有潜力，但是它们依赖于人工设计的特征和特征选择，无法捕捉音乐数据的全面复杂性。相反，深度学习分类架构如传统的卷积神经网络（CNN）可以捕捉音乐数据的空间层次结构，但是它们很难捕捉音乐数据中的时间动态特征。为解决这些挑战，本研究提出了一种新的方法，使用视觉спектрограм作为输入，并提出了一种混合模型，结合了Residual神经网络（ResNet）和Gated Recurrent Unit（GRU）。这种模型设计用于为音乐数据提供更全面的分析，并且可能提高音乐推荐系统的准确率。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Emotions-from-EEG-A-GRU-Based-Approach"><a href="#Unveiling-Emotions-from-EEG-A-GRU-Based-Approach" class="headerlink" title="Unveiling Emotions from EEG: A GRU-Based Approach"></a>Unveiling Emotions from EEG: A GRU-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02778">http://arxiv.org/abs/2308.02778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Johari, Gowri Namratha Meedinti, Radhakrishnan Delhibabu, Deepak Joshi<br>for: 这项研究旨在使用EEG数据进行情感识别，以提高人机交互和情感计算领域的发展。methods: 这项研究使用了Gated Recurrent Unit（GRU）算法，它是一种Recurrent Neural Networks（RNNs）的变种，通过利用EEG信号来预测情感状态。研究者们使用了公共可访问的数据集，包括在无动作状态下的中性数据以及受到刺激后的人类EEG记录，以及激发 happiness、neutral和negative情感的刺激。为了获得最佳的特征提取，研究者们对EEG数据进行了磁态除、频率筛选和normalization处理。results: 研究者们使用了GRU模型，并在验证集上达到了100%的准确率。与其他机器学习方法相比，GRU模型的Extreme Gradient Boosting Classifier具有最高的准确率。研究者们还对模型的混淆矩阵进行了分析，从而获得了精准的情感分类结果。这项研究表明了深度学习模型如GRU在情感识别方面的潜力，并且开创了新的情感计算领域的可能性。<details>
<summary>Abstract</summary>
One of the most important study areas in affective computing is emotion identification using EEG data. In this study, the Gated Recurrent Unit (GRU) algorithm, which is a type of Recurrent Neural Networks (RNNs), is tested to see if it can use EEG signals to predict emotional states. Our publicly accessible dataset consists of resting neutral data as well as EEG recordings from people who were exposed to stimuli evoking happy, neutral, and negative emotions. For the best feature extraction, we pre-process the EEG data using artifact removal, bandpass filters, and normalization methods. With 100% accuracy on the validation set, our model produced outstanding results by utilizing the GRU's capacity to capture temporal dependencies. When compared to other machine learning techniques, our GRU model's Extreme Gradient Boosting Classifier had the highest accuracy. Our investigation of the confusion matrix revealed insightful information about the performance of the model, enabling precise emotion classification. This study emphasizes the potential of deep learning models like GRUs for emotion recognition and advances in affective computing. Our findings open up new possibilities for interacting with computers and comprehending how emotions are expressed through brainwave activity.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)一个非常重要的研究领域在情感计算是使用EEG数据进行情感识别。在这项研究中，我们使用了Gated Recurrent Unit（GRU）算法，这是一种Recurrent Neural Networks（RNNs）的变种，以EEG信号来预测情感状态。我们的公共可访问数据集包括普通的中性数据以及在刺激人们表现出喜、中性和负情的EEG记录。为了获得最佳的特征提取，我们对EEG数据进行了噪声除除、频率筛选和 нормализа化处理。在验证集上得到100%的准确率，我们的模型表现出色，利用GRU捕捉时间相关性的能力。与其他机器学习技术相比，我们的GRU模型的Extreme Gradient Boosting Classifier（EGGB）准确率最高。我们对模型的混淆矩阵进行了调查，获得了模型表现的有用信息，启发精准的情感分类。这项研究强调了深度学习模型如GRU的情感识别潜力，并提出了对情感计算的新可能性。我们的发现打开了与计算机交互和理解脑波活动中情感表达的新可能性。
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory"><a href="#Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory" class="headerlink" title="Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory"></a>Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10768">http://arxiv.org/abs/2307.10768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanglab-deepneurocoglab/worm">https://github.com/zhanglab-deepneurocoglab/worm</a></li>
<li>paper_authors: Ankur Sikarwar, Mengmi Zhang</li>
<li>for: 本研究的目的是开发一个完整的工作记忆（WM）benchmark数据集，以便用于AI WM模型的开发和评估。</li>
<li>methods: 本研究使用了10个任务和100万个实验，评估了4种功能、3种领域和11种行为和神经特征。同时，还包括了人类行为的参照值作为比较标准。</li>
<li>results: 研究发现，AI模型在一些情况下能够模拟大脑中的工作记忆特征，如 primacy 和 recency 效应，以及各个领域和功能的神经团块和相关性。然而，也发现现有模型存在一些限制，无法完全approximate人类行为。这个数据集将成为跨 дисциплиinary的资源，用于比较和改进WM模型，研究WM的神经基础，并开发人类样式的WM模型。<details>
<summary>Abstract</summary>
Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM. In the experiments, we also reveal some limitations in existing models to approximate human behavior. This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM's neural underpinnings, and develop WM models with human-like capabilities. Our source code and data are available at https://github.com/ZhangLab-DeepNeuroCogLab/WorM.
</details>
<details>
<summary>摘要</summary>
工作记忆（WM），一种基本的认知过程，协助短暂存储、结合、操作和抽取信息，在推理和决策任务中扮演至关重要的角色。为了有效开发和评估人工智能WM模型，需要一些可靠的benchmark数据集。在这里，我们介绍了一个全面的Working Memory（WorM）benchmark数据集，用于这个目的。WorM包括10个任务和总共100万个尝试，评估了4种功能、3种领域和11种行为和神经特征。我们将现有的循环神经网络和转换器模型在所有这些任务上同时训练和测试。我们还包括了人类行为标准 als an upper bound for comparison。我们的结果表明，人工智能模型在脑中的WM特征中复制了一些特征，如劣antage和最新效应，以及特定领域和功能的神经团和相关特征。在实验中，我们还发现了现有模型的一些局限性，无法模拟人类行为。这个数据集作为一个 ценный资源，可以为认知心理学、神经科学和人工智能社区提供一个标准化的框架，用于比较和改进WM模型，调查WM的神经基础，并开发人类类似的WM模型。我们的源代码和数据可以在https://github.com/ZhangLab-DeepNeuroCogLab/WorM上获取。
</details></li>
</ul>
<hr>
<h2 id="Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query"><a href="#Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query" class="headerlink" title="Actor-agnostic Multi-label Action Recognition with Multi-modal Query"></a>Actor-agnostic Multi-label Action Recognition with Multi-modal Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10763">http://arxiv.org/abs/2307.10763</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mondalanindya/msqnet">https://github.com/mondalanindya/msqnet</a></li>
<li>paper_authors: Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</li>
<li>for: 提出了一种actor-agnostic multi-modal multi-label action recognition方法，以解决actor-specific pose estimation和多个行为同时发生的问题。</li>
<li>methods: 提出了一种基于 transformer 检测框架的 Multi-modal Semantic Query Network (MSQNet) 模型，利用视觉和文本模式更好地表示行为类别。</li>
<li>results: 在五个公开的数据集上进行了广泛的实验，并 consistently outperformed actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%.<details>
<summary>Abstract</summary>
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-specific model designs is a key advantage, as it removes the need for actor pose estimation altogether. Extensive experiments on five publicly available benchmarks show that our MSQNet consistently outperforms the prior arts of actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%. Code will be released at https://github.com/mondalanindya/MSQNet.
</details>
<details>
<summary>摘要</summary>
现有的动作识别方法通常是actor-specific的，因为actor之间存在内在的 topological 和 apparent 差异。这会导致actor-specific 姿势估计（例如人 VS 动物），从而增加模型设计复杂度和维护成本。此外，它们通常只学习视觉modal alone 和单个标签分类，而忽略其他可用的信息源（例如类名文本）以及同时发生的多个动作。为了超越这些限制，我们提出了一种新的approach called 'actor-agnostic multi-modal multi-label action recognition', which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-specific model designs is a key advantage, as it removes the need for actor pose estimation altogether. Extensive experiments on five publicly available benchmarks show that our MSQNet consistently outperforms the prior arts of actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%. Code will be released at https://github.com/mondalanindya/MSQNet.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Voter-Attribute-Bias-for-Fair-Opinion-Aggregation"><a href="#Mitigating-Voter-Attribute-Bias-for-Fair-Opinion-Aggregation" class="headerlink" title="Mitigating Voter Attribute Bias for Fair Opinion Aggregation"></a>Mitigating Voter Attribute Bias for Fair Opinion Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10749">http://arxiv.org/abs/2307.10749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Ueda, Koh Takeuchi, Hisashi Kashima</li>
<li>For: This paper focuses on developing fair opinion aggregation methods to address biases in decision-making, particularly in tasks without objectively true labels.* Methods: The authors propose a combination of opinion aggregation models, such as majority voting and the Dawid and Skene model, with fairness options like sample weighting and data splitting. They also introduce a new Soft D&amp;S model to improve soft label estimation.* Results: The experimental results show that the combination of Soft D&amp;S and data splitting is effective for dense data, while weighted majority voting is effective for sparse data. These findings support fair opinion aggregation in decision-making, both for human and machine-learning models.Here’s the simplified Chinese text for the three key points:* For: 这篇论文关注了在决策中减少意见偏见，特别是在没有唯一正确标签的任务中。* Methods: 作者们提议结合意见集成模型，如多数投票和达韦德和锈模型，以及公平选项，如样本权重和数据分割。他们还提出了一种新的软D&amp;S模型，以提高软标签估计的准确性。* Results: 实验结果表明，将软D&amp;S模型与数据分割结合使用，对于稠密数据是有效的，而Weighted多数投票对于稀疏数据是有效的。这些发现将为人类和机器学习模型的均衡意见集成提供支持。<details>
<summary>Abstract</summary>
The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&S model) with fairness options such as sample weighting. To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels. First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&S model. To address these limitations, we propose a new Soft D&S model with improved accuracy in estimating soft labels. Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&S, in combination with different fairness options using synthetic and semi-synthetic data. The experimental results suggest that the combination of Soft D&S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data. These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.
</details>
<details>
<summary>摘要</summary>
“多元意见的统计发挥了重要的决策作用，例如在招聘和贷款审核中，以及在指导学习中标签数据。 Although majority voting和现有的意见统计模型在简单任务上效果良好，但在无明确真实标签的任务中，它们无法应对不同意见的分歧。具体来说，当投票者属性如性别或种族引入偏见到意见时，统计结果将因投票者属性的分布而异。一个均衡的投票者群体是有利于公平统计结果的，但可能具有困难。在本研究中，我们考虑了基于投票者属性的公平意见统计方法，并评估这些方法的公平性。为此，我们考虑了结合意见统计模型（如多数投票和道维德和斯凯纳模型）和公平选项（如抽样重量）。对于评估公平性，随机软标签被视为更好的选择，而不是硬coded标签。我们首先解决了不考虑投票者属性的soft label估计问题，并发现了一些限制。为了解决这些限制，我们提出了一个新的Soft D&S模型，具有改善了 soft label 估计的精度。此外，我们还评估了不同公平选项与Soft D&S模型的整体公平性，使用人工和半自然数据。实验结果显示，结合Soft D&S模型和抽样重量的公平选项是在厚度数据中有效的，而Weighted多数投票则是在叠节数据中有效的。这些发现将在支持人类和机器学习模型的投票结果均衡中提供价值。”
</details></li>
</ul>
<hr>
<h2 id="Fairness-Aware-Client-Selection-for-Federated-Learning"><a href="#Fairness-Aware-Client-Selection-for-Federated-Learning" class="headerlink" title="Fairness-Aware Client Selection for Federated Learning"></a>Fairness-Aware Client Selection for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10738">http://arxiv.org/abs/2307.10738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Shi, Zelei Liu, Zhuan Shi, Han Yu</li>
<li>for: 提高 Federated Learning（FL）客户端选择的公平性和模型性能。</li>
<li>methods: 基于 Lyapunov 优化的 Fairness-aware Federated Client Selection（FairFedCS）方法，通过考虑客户端的声誉、参与 FL 任务的时间和模型性能的贡献，动态调整客户端选择概率。</li>
<li>results: 在实际的 multimedia 数据集上进行了广泛的实验，并显示了 FairFedCS 可以提高平均 fairness 19.6% 和测试精度 0.73% 比最佳状态的方法。<details>
<summary>Abstract</summary>
Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data. Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem. Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients. The problem of balancing performance and fairness considerations when selecting FL clients remains open. To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance. By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceived poor performance, thereby further enhancing fair client treatment. Extensive experiments based on real-world multimedia datasets show that FairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy on average than the best-performing state-of-the-art approach.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）已经允许多个数据拥有者（即FL客户）共同训练机器学习模型，无需披露私人数据。由于FL服务器只能在每次训练中选择一定数量的客户，因此选择FL客户已成为一个重要的研究问题。现有的方法通常是增强FL模型性能或增强FL客户的公平待遇。尚未解决FL客户性能和公平待遇考虑的权衡问题。为解决这个问题，我们提出了公平性感知 Federated Client Selection（FairFedCS）方法。基于 Lyapunov 优化，它在FL客户选择概率中进行了动态调整，并且同时考虑了FL客户的声誉、参与FL任务的时间和对模型性能的贡献。不使用阈值基于声誉筛选，因此允许FL客户在感知性能不佳时重新恢复声誉，从而进一步提高FL客户的公平待遇。经过基于实际 multimedia 数据集的广泛实验，我们发现 FairFedCS 平均提高了19.6%的公平性和0.73%的测试准确率。
</details></li>
</ul>
<hr>
<h2 id="Long-Tail-Theory-under-Gaussian-Mixtures"><a href="#Long-Tail-Theory-under-Gaussian-Mixtures" class="headerlink" title="Long-Tail Theory under Gaussian Mixtures"></a>Long-Tail Theory under Gaussian Mixtures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10736">http://arxiv.org/abs/2307.10736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/armanbolatov/long_tail">https://github.com/armanbolatov/long_tail</a></li>
<li>paper_authors: Arman Bolatov, Maxat Tezekbayev, Igor Melnykov, Artur Pak, Vassilina Nikoulina, Zhenisbek Assylbekov</li>
<li>for: 这篇论文关注 Feldman 的长尾理论 (2020)，提出了一个简单的 Gaussian 混合模型，以测试不同类型的标签模型在长尾分布下的性能。</li>
<li>methods: 论文使用了一个线性分类器和一个非线性分类器，以评估它们在长尾分布下的适用程度。</li>
<li>results: 论文发现，在长尾分布下，非线性分类器能够对新数据进行更好的适应，而线性分类器则无法降低一定水平的普遍化错误。此外，论文还发现，当尾部的长度变短时，两种模型之间的性能差距可以降低。<details>
<summary>Abstract</summary>
We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
</details>
<details>
<summary>摘要</summary>
我们建议一个简单的 Gaussian 混合模型来生成数据，遵循 Feldman 的长尾理论（2020）。我们显示了一个线性分类器无法在提案的模型中降低泛化误差下限，而一个具有记忆容量的非线性分类器则可以。这证实了对长尾分布的数据，罕见的训练示例必须被考虑来取得最佳的泛化至新数据。最后，我们显示了线性和非线性模型之间的性能差距可以随着尾部的短化而减少，通过实验证明在 sintetic 和实际数据上。
</details></li>
</ul>
<hr>
<h2 id="Comparison-between-transformers-and-convolutional-models-for-fine-grained-classification-of-insects"><a href="#Comparison-between-transformers-and-convolutional-models-for-fine-grained-classification-of-insects" class="headerlink" title="Comparison between transformers and convolutional models for fine-grained classification of insects"></a>Comparison between transformers and convolutional models for fine-grained classification of insects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11112">http://arxiv.org/abs/2307.11112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rita Pucci, Vincent J. Kalkman, Dan Stowell</li>
<li>for: 本研究旨在提高昆虫种类的自动分类精度，尤其是在同一个分类类型下的种类 diferenciación。</li>
<li>methods: 本研究使用了深度学习算法，特别是 transformer 和 convolutional 层结构，进行比较研究。</li>
<li>results: 研究发现，hybrid 模型在准确性和执行速度两个方面均有优异表现，而 transformer 模型在样本缺乏时具有更高的执行速度。<details>
<summary>Abstract</summary>
Fine-grained classification is challenging due to the difficulty of finding discriminatory features. This problem is exacerbated when applied to identifying species within the same taxonomical class. This is because species are often sharing morphological characteristics that make them difficult to differentiate. We consider the taxonomical class of Insecta. The identification of insects is essential in biodiversity monitoring as they are one of the inhabitants at the base of many ecosystems. Citizen science is doing brilliant work of collecting images of insects in the wild giving the possibility to experts to create improved distribution maps in all countries. We have billions of images that need to be automatically classified and deep neural network algorithms are one of the main techniques explored for fine-grained tasks. At the SOTA, the field of deep learning algorithms is extremely fruitful, so how to identify the algorithm to use? We focus on Odonata and Coleoptera orders, and we propose an initial comparative study to analyse the two best-known layer structures for computer vision: transformer and convolutional layers. We compare the performance of T2TViT, a fully transformer-base, EfficientNet, a fully convolutional-base, and ViTAE, a hybrid. We analyse the performance of the three models in identical conditions evaluating the performance per species, per morph together with sex, the inference time, and the overall performance with unbalanced datasets of images from smartphones. Although we observe high performances with all three families of models, our analysis shows that the hybrid model outperforms the fully convolutional-base and fully transformer-base models on accuracy performance and the fully transformer-base model outperforms the others on inference speed and, these prove the transformer to be robust to the shortage of samples and to be faster at inference time.
</details>
<details>
<summary>摘要</summary>
细致分类问题困难，主要因为找到分化特征困难。当应用到同一种植物类中的物种识别时，这问题更加困难。这是因为物种经常共享 morphological 特征，使其困难分 differentiate。我们考虑 insecta 纲。识别昆虫对生物多样性监测至关重要，因为它们是生态系统的基础居民。公民科学在野外采集昆虫图像，提供了专家创建改进的分布图的机会。我们有数十亿个图像需要自动分类，深度学习算法是细致任务中的主要技术之一。在 SOTA 中，深度学习领域非常肥沃，因此如何选择算法？我们将关注 Odonata 和 Coleoptera 两个目。我们提出了一个初步比较研究，检验了两种最为知名的计算机视觉层结构：变换层和卷积层。我们比较了 T2TViT、EfficientNet 和 ViTAE 三种模型的性能，分析了每种物种、每种形态、性别、推理时间和总性能。虽然我们所观察到的性能很高，但我们的分析表明，混合模型在准确性和推理速度两个方面都有优势。此外，完全转换基础模型在准确性方面表现出色，而完全卷积基础模型在推理速度方面表现出色。这证明了转换层在样本缺乏时具有稳定性和快速推理能力。
</details></li>
</ul>
<hr>
<h2 id="LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem"><a href="#LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem" class="headerlink" title="LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?"></a>LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10719">http://arxiv.org/abs/2307.10719</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, Vardan Papyan</li>
<li>For: The paper discusses the risks of malicious use of large language models (LLMs) and the limitations of existing defense mechanisms, such as model fine-tuning or output censorship.* Methods: The paper presents theoretical limitations of semantic censorship approaches, highlighting the inherent challenges in censoring LLM outputs due to their programmatic and instruction-following capabilities.* Results: The paper argues that the challenges of censorship extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones, and proposes that the problem of censorship should be reevaluated and treated as a security problem to mitigate potential risks.Here is the same information in Simplified Chinese text:* For: 论文探讨大语言模型（LLM）的可能的恶用行为风险，以及现有的防御机制的不足。* Methods: 论文描述了semantic censorship的理论限制，强调 LLM 的程序化和指令执行能力使得 censored 输出仍然存在问题。* Results: 论文认为， censored 输出的问题不仅限于semantic censorship，攻击者可以通过收集 permissible 输出来重建不当输出，therefore, the problem of censorship should be reevaluated and treated as a security problem to mitigate potential risks.<details>
<summary>Abstract</summary>
Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Differences-Between-Hard-and-Noisy-labeled-Samples-An-Empirical-Study"><a href="#Differences-Between-Hard-and-Noisy-labeled-Samples-An-Empirical-Study" class="headerlink" title="Differences Between Hard and Noisy-labeled Samples: An Empirical Study"></a>Differences Between Hard and Noisy-labeled Samples: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10718">http://arxiv.org/abs/2307.10718</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahf93/hard-vs-noisy">https://github.com/mahf93/hard-vs-noisy</a></li>
<li>paper_authors: Mahsa Forouzesh, Patrick Thiran</li>
<li>for: 本研究旨在解决难度强、标签错误的样本集合中的噪声样本问题。</li>
<li>methods: 我们提出了一种系统性的实验方法，用于分析困难样本和噪声标签样本之间的相似性和 diferencias。我们还提出了一种简单 yet effective的度量，可以从噪声标签样本中筛选出噪声样本，保留困难样本。</li>
<li>results: 我们的研究表明，使用我们提出的度量筛选出噪声标签样本后，模型在训练过后的测试精度得到了最高的提升。此外，我们的数据分配方法在实际世界中存在标签噪声时也表现出色。<details>
<summary>Abstract</summary>
Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic. Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples. However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model. In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples. These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples. Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples. We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets. We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise. Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.
</details>
<details>
<summary>摘要</summary>
<<SYS>>按照以下转换规则将文本翻译成简化中文：1. 将"noisy"和"incorrectly"替换为"噪音"和"错误"。2. 将"samples"替换为"样本"。3. 将"hard"和"difficult"替换为"困难"。4. 将"labels"替换为"标签"。5. 将"existing"替换为"现有"。6. 将"systematic"替换为"系统的"。7. 将"empirical"替换为"实际的"。8. 将"controlled"替换为"控制的"。9. 将"filter"替换为"筛选"。10. 将"partitions"替换为"分区"。转换后的文本如下：EXTRACTING NOISY OR INCORRECTLY LABELED SAMPLES FROM A LABELED DATASET WITH HARD/DIFFICULT SAMPLES IS AN IMPORTANT YET UNDERE-EXPLORED TOPIC. TWO GENERAL AND OFTEN INDEPENDENT LINES OF WORK EXIST, ONE FOCUSES ON ADDRESSING NOISY LABELS, AND ANOTHER DEALS WITH HARD SAMPLES. HOWEVER, WHEN BOTH TYPES OF DATA ARE PRESENT, MOST EXISTING METHODS TREAT THEM EQUALLY, WHICH RESULTS IN A DECLINE IN THE OVERALL PERFORMANCE OF THE MODEL. IN THIS PAPER, WE FIRST DESIGN VARIOUS SYNTHETIC DATASETS WITH CUSTOM HARDNESS AND NOISINESS LEVELS FOR DIFFERENT SAMPLES. OUR PROPOSED SYSTEMATIC EMPIRICAL STUDY ENABLES US TO BETTER UNDERSTAND THE SIMILARITIES AND MORE IMPORTANTLY THE DIFFERENCES BETWEEN HARD-TO-LEARN SAMPLES AND INCORRECTLY LABELED SAMPLES. THESE CONTROLLED EXPERIMENTS PAVE THE WAY FOR THE DEVELOPMENT OF METHODS THAT DISTINGUISH BETWEEN HARD AND NOISY SAMPLES. THROUGH OUR STUDY, WE INTRODUCE A SIMPLE YET EFFECTIVE METRIC THAT FILTERS OUT NOISY-LABELED SAMPLES WHILE KEEPING THE HARD SAMPLES. WE STUDY VARIOUS DATA PARTITIONING METHODS IN THE PRESENCE OF LABEL NOISE AND OBSERVE THAT FILTERING OUT NOISY SAMPLES FROM HARD SAMPLES WITH THIS PROPOSED METRIC RESULTS IN THE BEST DATASETS AS EVIDENCED BY THE HIGH TEST ACCURACY ACHIEVED AFTER MODELS ARE TRAINED ON THE FILTERED DATASETS. WE DEMONSTRATE THIS FOR BOTH OUR CREATED SYNTHETIC DATASETS AND FOR DATASETS WITH REAL-WORLD LABEL NOISE. FURTHERMORE, OUR PROPOSED DATA PARTITIONING METHOD SIGNIFICANTLY OUTPERFORMS OTHER METHODS WHEN EMPLOYED WITHIN A SEMI-SUPERVISED LEARNING FRAMEWORK.
</details></li>
</ul>
<hr>
<h2 id="AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models"><a href="#AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models" class="headerlink" title="AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models"></a>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10711">http://arxiv.org/abs/2307.10711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachun Pan, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan</li>
<li>for: 这篇论文旨在Addressing the challenge of diffusion probabilistic model (DPM) customization when only available supervision is a differentiable metric defined on the generated contents.</li>
<li>methods: 该论文提出了一种新的方法 called AdjointDPM，它首先使用扩展ODE来生成新的样本，然后使用逆变数法来归导损失的梯度到模型参数（包括conditioning信号、网络参数和初始噪声）。</li>
<li>results: 该论文通过三个有趣的任务来证明AdjointDPM的效果：将视觉特效转换为标识码嵌入，finetune DPMs для特定类型的风格化，以及优化初始噪声生成对抗样本 для安全审核。<details>
<summary>Abstract</summary>
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and gradient backpropagation processes, we further reparameterize the probability-flow ODE and augmented ODE as simple non-stiff ODEs using exponential integration. Finally, we demonstrate the effectiveness of AdjointDPM on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing.
</details>
<details>
<summary>摘要</summary>
现有的自定义方法需要访问多个参考示例，以将预训练的扩散概率模型（DPM）与用户提供的概念进行对接。这篇论文目标是解决DPM自定义时，只有用户提供的可微分度量表示的挑战。因为扩散过程中的DPM采样过程 involve recursive calls to the denoising UNet，直观梯度反propagation需要存储所有迭代过程的间接状态，导致内存消耗极高。为解决这个问题，我们提出了一种新的方法AdjointDPM。首先，AdjointDPM使用扩散模型来生成新的样本，然后使用逆散射敏感度法来反propagate损失的梯度到模型参数（包括conditioning信号、网络参数和初始噪音）。为了减少在前向生成和反propagation过程中的数值错误，我们进一步将概率流ODE和扩充ODE转换为简单的非硬式ODE，并使用加速的exponential integration。最后，我们示例了AdjointDPM在三个有趣的任务上的效果：将视觉特效转换为标识码嵌入，finetune DPMs for specific types of stylization，和优化初始噪音以生成安全审核中的敌意样本。
</details></li>
</ul>
<hr>
<h2 id="Reparameterized-Policy-Learning-for-Multimodal-Trajectory-Optimization"><a href="#Reparameterized-Policy-Learning-for-Multimodal-Trajectory-Optimization" class="headerlink" title="Reparameterized Policy Learning for Multimodal Trajectory Optimization"></a>Reparameterized Policy Learning for Multimodal Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10710">http://arxiv.org/abs/2307.10710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haosulab/RPG">https://github.com/haosulab/RPG</a></li>
<li>paper_authors: Zhiao Huang, Litian Liang, Zhan Ling, Xuanlin Li, Chuang Gan, Hao Su</li>
<li>for: 本研究的目标是解决高维连续动作空间中RL政策参数化的挑战。</li>
<li>methods: 我们提出了一种原则正的框架，将连续RL政策视为环境优质轨迹的生成模型。通过将政策 conditional on一个隐藏变量，我们得到了一种新的可变 bounds，它鼓励环境探索。</li>
<li>results: 我们提出了一种实用的模型基于RL方法，called Reparameterized Policy Gradient (RPG)，它利用多模态政策参数化和学习的世界模型来实现强大的探索能力和高数据效率。实验结果表明，我们的方法可以帮助代理人避免环境中的局部优点，解决杂质奖励环境，并在各种任务中达到优秀的性能。<details>
<summary>Abstract</summary>
We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric intrinsic reward. Our method consistently outperforms previous approaches across a range of tasks. Code and supplementary materials are available on the project page https://haosulab.github.io/RPG/
</details>
<details>
<summary>摘要</summary>
我们研究了重 parametrization policies 的挑战，在高维连续行为空间中进行学习反馈学习（RL）。我们的目标是开发一个多模态策略，超越常用的 Gaussian 参数化的限制。为 достичь这一目标，我们提出了一种原则的框架，将连续RL策略模型为优质轨迹生成模型。通过对策略进行受限变量条件，我们得到了一种新的variational bound，作为优化目标，这会促进环境的探索。然后，我们提出了一种实用的基于模型的RL方法，called Reparameterized Policy Gradient（RPG），利用多模态策略参数化和学习的世界模型，以实现强大的探索能力和高数据效率。我们的方法在多个任务中表现出色，可以帮助代理人避免环境中的地方最优化，并通过嵌入物体内部的内在奖励来解决稀悉奖励环境。我们的方法在多个任务中比前一些方法表现出色，代码和补充材料可以在项目页面https://haosulab.github.io/RPG/ 中找到。
</details></li>
</ul>
<hr>
<h2 id="TwinLiteNet-An-Efficient-and-Lightweight-Model-for-Driveable-Area-and-Lane-Segmentation-in-Self-Driving-Cars"><a href="#TwinLiteNet-An-Efficient-and-Lightweight-Model-for-Driveable-Area-and-Lane-Segmentation-in-Self-Driving-Cars" class="headerlink" title="TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars"></a>TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10705">http://arxiv.org/abs/2307.10705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chequanghuy/TwinLiteNet">https://github.com/chequanghuy/TwinLiteNet</a></li>
<li>paper_authors: Quang Huy Che, Dinh Phuc Nguyen, Minh Quan Pham, Duc Khai Lam</li>
<li>for: 本研究旨在提出一种轻量级的模型，用于自动驾驶车辆环境理解的驱动区域和车道线分割。</li>
<li>methods: 该模型基于TwinLiteNet方法，它是一种廉价的模型，但具有高准确率和高效性。</li>
<li>results: 实验结果表明，TwinLiteNet在BDD100K数据集上 achievement mIoU分数为91.3%，对比现有方法相对较高，且具有较少的计算资源需求。 Code可以在url{<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/TwinLiteNet%7D%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/chequanghuy/TwinLiteNet}中获取。</a><details>
<summary>Abstract</summary>
Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000. Furthermore, TwinLiteNet can run in real-time on embedded devices with limited computing power, especially since it achieves 60FPS on Jetson Xavier NX, making it an ideal solution for self-driving vehicles. Code is available: url{https://github.com/chequanghuy/TwinLiteNet}.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation 是自动驾驶中常见的任务，用于理解周围环境。驱动区域分割和车道检测是安全和高效导航的关键，但原始semantic segmentation模型 computationally expensive和需要高级硬件，不适合自动驾驶车辆中的嵌入式系统。这篇论文提出了一种轻量级的模型，用于驱动区域和车道分割。TwinLiteNet是一种便宜的设计，但它可以实现高度准确和高速的分割结果。我们在BDD100K数据集上评估TwinLiteNet，并与现代模型进行比较。实验结果表明，我们的TwinLiteNet与现有方法相似，需要 significatively fewer computational resources。具体来说，TwinLiteNet在驱动区域任务中 achieves mIoU 分数为 91.3%，并在车道检测任务中 achieves IoU 分数为 31.08%，仅使用 0.4 万个参数。此外，TwinLiteNet可以在实时中在有限的计算能力的嵌入式设备上运行，特别是在 Jetson Xavier NX 上 achieve 60 FPS，使其成为自动驾驶车辆的理想解决方案。代码可以在以下链接中找到：<https://github.com/chequanghuy/TwinLiteNet>。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Smart-Charging-of-Large-Scale-EVs-using-Adaptive-Multi-Agent-Multi-Armed-Bandits"><a href="#Decentralized-Smart-Charging-of-Large-Scale-EVs-using-Adaptive-Multi-Agent-Multi-Armed-Bandits" class="headerlink" title="Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits"></a>Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10704">http://arxiv.org/abs/2307.10704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharyal Zafar, Raphaël Feraud, Anne Blavette, Guy Camilleri, Hamid Ben</li>
<li>for: 这篇论文的目的是提出一种完全分散式的充电系统，以解决电动车充电过载和电压限制问题。</li>
<li>methods: 该系统采用了自适应多代理系统哲学，并使用多臂投掷学来处理系统不确定性。</li>
<li>results:  caso study表明，该系统具有分散式、扩展性、实时性、无模型基础和公平性等特点。<details>
<summary>Abstract</summary>
The drastic growth of electric vehicles and photovoltaics can introduce new challenges, such as electrical current congestion and voltage limit violations due to peak load demands. These issues can be mitigated by controlling the operation of electric vehicles i.e., smart charging. Centralized smart charging solutions have already been proposed in the literature. But such solutions may lack scalability and suffer from inherent drawbacks of centralization, such as a single point of failure, and data privacy concerns. Decentralization can help tackle these challenges. In this paper, a fully decentralized smart charging system is proposed using the philosophy of adaptive multi-agent systems. The proposed system utilizes multi-armed bandit learning to handle uncertainties in the system. The presented system is decentralized, scalable, real-time, model-free, and takes fairness among different players into account. A detailed case study is also presented for performance evaluation.
</details>
<details>
<summary>摘要</summary>
electric vehicles 和 photovoltaics 的快速增长可能会引入新的挑战，如电流拥堵和电压限制由峰值负荷带来。这些问题可以通过智能充电控制来缓解。文章中已经提出了中央化智能充电解决方案。但这些解决方案可能缺乏扩展性和中央化的缺点，如唯一点失败和数据隐私问题。分散化可以解决这些挑战。本文提出了一个完全分散式智能充电系统，使用适应多代理系统的哲学。该系统使用多臂投掷学来处理系统不确定性。提出的系统是分散式、可扩展、实时、模型自由、具有公平性的。还提供了一个详细的案例研究以评估性能。
</details></li>
</ul>
<hr>
<h2 id="Graphs-in-State-Space-Models-for-Granger-Causality-in-Climate-Science"><a href="#Graphs-in-State-Space-Models-for-Granger-Causality-in-Climate-Science" class="headerlink" title="Graphs in State-Space Models for Granger Causality in Climate Science"></a>Graphs in State-Space Models for Granger Causality in Climate Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10703">http://arxiv.org/abs/2307.10703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Víctor Elvira, Émilie Chouzenoux, Jordi Cerdà, Gustau Camps-Valls</li>
<li>for: 评估时间序列之间的预测可能性</li>
<li>methods: 使用图ematrix模型和lasso正则化</li>
<li>results: 提高了对标准Granger causality方法的比较Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to assess the predictability of time series from another time series using Granger causality, which is a widely used method in many applied disciplines.</li>
<li>methods: The paper uses a graphical perspective of state-space models and a recently presented expectation-maximization algorithm called GraphEM to estimate the linear matrix operator in the state equation of a linear-Gaussian state-space model. Lasso regularization is included in the M-step, which is solved using a proximal splitting Douglas-Rachford algorithm.</li>
<li>results: The proposed model and inference technique are demonstrated to have benefits over standard Granger causality methods through experiments in toy examples and challenging climate problems.<details>
<summary>Abstract</summary>
Granger causality (GC) is often considered not an actual form of causality. Still, it is arguably the most widely used method to assess the predictability of a time series from another one. Granger causality has been widely used in many applied disciplines, from neuroscience and econometrics to Earth sciences. We revisit GC under a graphical perspective of state-space models. For that, we use GraphEM, a recently presented expectation-maximisation algorithm for estimating the linear matrix operator in the state equation of a linear-Gaussian state-space model. Lasso regularisation is included in the M-step, which is solved using a proximal splitting Douglas-Rachford algorithm. Experiments in toy examples and challenging climate problems illustrate the benefits of the proposed model and inference technique over standard Granger causality methods.
</details>
<details>
<summary>摘要</summary>
格兰治 causality（GC）frequently 被视为不是实际的 causality。然而，它仍然是最广泛使用的方法来评估时间序列之间的预测性。格兰治 causality 在各种应用领域中广泛使用，从神经科学和经济统计到地球科学。我们在图表视角下重新审视GC。为此，我们使用 GraphEM，一种最近提出的期望最大化算法来估计状态方程中的线性矩阵运算符。lasso 规范化包括在M-step中，使用距离 Douglas-Rachford 算法解决。实验在小例子和挑战性气候问题中ILLUSTRATE 我们提议的模型和推理方法的优势于标准Granger causality方法。
</details></li>
</ul>
<hr>
<h2 id="Self2Self-Single-Image-Denoising-with-Self-Supervised-Learning-and-Image-Quality-Assessment-Loss"><a href="#Self2Self-Single-Image-Denoising-with-Self-Supervised-Learning-and-Image-Quality-Assessment-Loss" class="headerlink" title="Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss"></a>Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10695">http://arxiv.org/abs/2307.10695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JK-the-Ko/Self2SelfPlus">https://github.com/JK-the-Ko/Self2SelfPlus</a></li>
<li>paper_authors: Jaekyun Ko, Sanghwan Lee</li>
<li>for: 本研究旨在提出一种基于单个噪声图像的自监督学习方法，以便提高噪声除去效果的可行性和实用性。</li>
<li>methods: 该方法使用阻塞卷积来提取特征，并使用无参图质量评估来导航训练过程。另外，方法还使用bernoulli采样和dropout来随机 sampling实例从输入图像集，以提高训练效果。</li>
<li>results: 实验结果表明，提出的方法在 sintetic 和实际 dataset 上达到了当前最佳的噪声除去性能。这表明了方法的有效性和实用性，可能用于各种噪声除去任务。<details>
<summary>Abstract</summary>
Recently, denoising methods based on supervised learning have exhibited promising performance. However, their reliance on external datasets containing noisy-clean image pairs restricts their applicability. To address this limitation, researchers have focused on training denoising networks using solely a set of noisy inputs. To improve the feasibility of denoising procedures, in this study, we proposed a single-image self-supervised learning method in which only the noisy input image is used for network training. Gated convolution was used for feature extraction and no-reference image quality assessment was used for guiding the training process. Moreover, the proposed method sampled instances from the input image dataset using Bernoulli sampling with a certain dropout rate for training. The corresponding result was produced by averaging the generated predictions from various instances of the trained network with dropouts. The experimental results indicated that the proposed method achieved state-of-the-art denoising performance on both synthetic and real-world datasets. This highlights the effectiveness and practicality of our method as a potential solution for various noise removal tasks.
</details>
<details>
<summary>摘要</summary>
最近，基于超级vised学习的减噪方法表现出色。然而，它们依赖于外部 dataset 中的噪音-清洁图像对，这限制了它们的应用场景。为解决这些 limitation，研究人员对减噪网络的训练进行了更多的关注。在这种情况下，我们提出了使用单一静止输入图像进行网络训练的自我监督学习方法。使用了扩展 convolution 来抽取特征，并使用无参图像质量评估来引导训练过程。此外，我们使用 Bernoulli 抽样来从输入图像集中随机选择实例进行训练。通过 averaging 生成的多个实例的 predicates，我们得到了最终的结果。实验结果表明，我们的方法在 synthetic 和实际世界 dataset 上实现了 state-of-the-art 的减噪性能。这表明了我们的方法在减噪任务中的实用性和可行性。
</details></li>
</ul>
<hr>
<h2 id="Fractional-Denoising-for-3D-Molecular-Pre-training"><a href="#Fractional-Denoising-for-3D-Molecular-Pre-training" class="headerlink" title="Fractional Denoising for 3D Molecular Pre-training"></a>Fractional Denoising for 3D Molecular Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10683">http://arxiv.org/abs/2307.10683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fengshikun/frad">https://github.com/fengshikun/frad</a></li>
<li>paper_authors: Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, Wei-Ying Ma</li>
<li>for: 提高3D分子预训练方法的性能，特别是在药物搜寻任务中。</li>
<li>methods: 提出了一种新的混合噪声策略，包括对 dip 角和坐标进行噪声处理。同时，提出了一种新的分数噪声处理方法（Frad），可以更好地适应分子的 ani sowropic 特征。</li>
<li>results: 实验表明，Frad 可以更好地提高分子表示性，并在 9 个 QM9 任务和 7 个 MD17 任务中达到新的顶峰性。<details>
<summary>Abstract</summary>
Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of noise and design a novel fractional denoising method (Frad), which only denoises the latter coordinate part. In this way, Frad enjoys both the merits of sampling more low-energy structures and the force field equivalence. Extensive experiments show the effectiveness of Frad in molecular representation, with a new state-of-the-art on 9 out of 12 tasks of QM9 and on 7 out of 8 targets of MD17.
</details>
<details>
<summary>摘要</summary>
“坐标干扰是一种有前途的3D分子预训方法，它在不同的下游药物探索任务中表现出色。理论上，这个目标等于学习力场，这是对下游任务有帮助的。然而，坐标干扰学习一个有效的力场受到两个挑战，即低覆盖样本和各向同性力场。这些挑战的根本原因是现有的坐标干扰方法对分子的分布假设不能够捕捉分子的非对称特征。为了解决这些挑战，我们提出了一种新的混合噪音策略，包括坐标和方向夹角的噪音。但是，将这种混合噪音进行传统的噪音除法不再等于学习力场。经过理论的推导，我们发现这个问题是因为输入构造的假设所导致的。为了解决这问题，我们提出了一种新的分解方法（Frad），它只对后者的坐标部分进行噪音除法。这样，Frad可以同时具有较低的能量结构和力场等于性。实验结果显示Frad在分子表现方面有新的顶峰性，在QM9和MD17上分别取得9/12和7/8的新纪录。”
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-classification-of-noisy-QR-codes"><a href="#Deep-learning-for-classification-of-noisy-QR-codes" class="headerlink" title="Deep learning for classification of noisy QR codes"></a>Deep learning for classification of noisy QR codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10677">http://arxiv.org/abs/2307.10677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca Leygonie, Sylvain Lobry, ), Laurent Wendling (LIPADE)</li>
<li>for: 本研究旨在定义基于深度学习的古典分类模型在抽象图像上的限制，当应用于不可见化对象的图像。</li>
<li>methods: 我们使用了基于深度学习的图像分类模型，并对QR码（快速响应码）进行了训练。QR码不是为人类手动读取而设计的，因此我们可以通过对QR码生成的信息进行分析，了解深度学习模型在抽象图像分类中的限制。</li>
<li>results: 我们的研究结果表明，基于深度学习的模型可以在抽象图像分类中表现出优异的性能，并且在噪声存在的情况下也能够保持一定的稳定性。这项研究表明，基于深度学习的模型可以在理解抽象图像方面发挥作用。<details>
<summary>Abstract</summary>
We wish to define the limits of a classical classification model based on deep learning when applied to abstract images, which do not represent visually identifiable objects.QR codes (Quick Response codes) fall into this category of abstract images: one bit corresponding to one encoded character, QR codes were not designed to be decoded manually. To understand the limitations of a deep learning-based model for abstract image classification, we train an image classification model on QR codes generated from information obtained when reading a health pass. We compare a classification model with a classical (deterministic) decoding method in the presence of noise. This study allows us to conclude that a model based on deep learning can be relevant for the understanding of abstract images.
</details>
<details>
<summary>摘要</summary>
我们想定义深度学习模型在抽象图像分类中的限制，当应用于不可识别的视觉对象。二维码（快速响应码）是这类抽象图像的一个例子：每一比特对应一个编码字符，二维码不是为人类手动解码。通过在健康通行证中读取的信息生成的二维码，我们训练了一个图像分类模型，并与经典（束缚）解码方法进行比较，以评估深度学习模型在抽象图像分类中的局限性。这项研究允许我们 conclued that deep learning模型对抽象图像有 relevance。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-What-to-Share-in-Federated-Learning-Perspectives-on-Model-Utility-Privacy-Leakage-and-Communication-Efficiency"><a href="#A-Survey-of-What-to-Share-in-Federated-Learning-Perspectives-on-Model-Utility-Privacy-Leakage-and-Communication-Efficiency" class="headerlink" title="A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency"></a>A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10655">http://arxiv.org/abs/2307.10655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Shao, Zijian Li, Wenqiang Sun, Tailin Zhou, Yuchang Sun, Lumin Liu, Zehong Lin, Jun Zhang</li>
<li>for: 这个论文主要是为了探讨 Federated Learning（FL）中可以共同培训多个参与者的方法，以保护数据隐私。</li>
<li>methods: 这篇论文分析了不同类型的共享方法，包括模型共享、synthetic数据共享和知识共享。</li>
<li>results: 论文通过对不同共享方法的性能和通信开销进行比较，以及对模型泄露和会员推测攻击的评估，提供了一些结论。<details>
<summary>Abstract</summary>
Federated learning (FL) has emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties. Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets. This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants. Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information. In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. This survey differs from previous ones due to four distinct contributions. First, we present a new taxonomy of FL methods in terms of the sharing methods, which includes three categories of shared information: model sharing, synthetic data sharing, and knowledge sharing. Second, we analyze the vulnerability of different sharing methods to privacy attacks and review the defense mechanisms that provide certain privacy guarantees. Third, we conduct extensive experiments to compare the performance and communication overhead of various sharing methods in FL. Besides, we assess the potential privacy leakage through model inversion and membership inference attacks, while comparing the effectiveness of various defense approaches. Finally, we discuss potential deficiencies in current methods and outline future directions for improvement.
</details>
<details>
<summary>摘要</summary>
受领学习（Federated Learning，FL）已经出现为保持隐私的高效模式，多个方面共同训练不同的数据集。与传统中央学习不同，FL 允许客户端共享隐私保护的信息而不曝光私有数据集。这种方法不仅保证了更好的隐私保护，还促进了多个参与者之间更加有效和安全的协作。因此，FL 已经吸引了广泛的研究人员，促使了许多相关的评估文章。然而，大多数这些评估文章都集中在共享模型参数 durante el proceso de entrenamiento，而忽略了其他形式的本地信息的共享的潜在价值。在这篇文章中，我们提供了一个系统的评估，即在 FL 中何时共享什么，并强调模型的实用性、隐私泄露和通信效率。这种评估与以往不同，主要由以下四个特点：1. 我们提出了一个新的分类方法，将 FL 方法分为三类共享信息：模型共享、合成数据共享和知识共享。2. 我们分析了不同共享方法的隐私攻击的敏感性，并评估了提供一定隐私保证的防御机制。3. 我们进行了广泛的实验，比较不同共享方法在 FL 中的性能和通信开销。此外，我们评估了模型反向泄露和成员身份攻击的风险，并比较了不同防御策略的效果。4. 我们讨论了当前方法的缺陷和未来方向的改进。总之，本文提供了一个系统的评估，帮助读者更好地理解 FL 中共享哪些信息，以及这些信息在模型实用性、隐私泄露和通信效率方面的影响。
</details></li>
</ul>
<hr>
<h2 id="Conditional-expectation-network-for-SHAP"><a href="#Conditional-expectation-network-for-SHAP" class="headerlink" title="Conditional expectation network for SHAP"></a>Conditional expectation network for SHAP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10654">http://arxiv.org/abs/2307.10654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronald Richman, Mario V. Wüthrich</li>
<li>for: 这个研究旨在提出一种能够有效地计算Conditional SHAP值的神经网络方法，以便在神经网络和其他回归模型中使用。</li>
<li>methods: 这种方法使用了SHAP技术，并且特别考虑了特征组件之间的依赖关系。</li>
<li>results: 这种方法可以准确地计算Conditional SHAP值，并且可以提供drop1和anova分析，以及一种考虑特征组件之间依赖关系的PDP图像。<details>
<summary>Abstract</summary>
A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP). Except for tree-based methods, usually the unconditional version is used (for computational reasons). We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components. This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.
</details>
<details>
<summary>摘要</summary>
非常流行的模型无关技术之一是SHapley Additive exPlanation（SHAP）。这两个版本是条件预期版本和无条件预期版本（后者也称为交互SHAP）。通常情况下，使用无条件版本（由于计算原因）。我们提供一种（代理）神经网络方法，可以高效计算条件版本，并且正确考虑特征组件之间的依赖关系。这种提议还有助于提供drop1和anova分析在复杂回归模型中，与其普通线性模型（GLM）对应的分析。此外，我们还提供了一种考虑特征组件依赖关系的PDP对应。
</details></li>
</ul>
<hr>
<h2 id="Refining-the-Optimization-Target-for-Automatic-Univariate-Time-Series-Anomaly-Detection-in-Monitoring-Services"><a href="#Refining-the-Optimization-Target-for-Automatic-Univariate-Time-Series-Anomaly-Detection-in-Monitoring-Services" class="headerlink" title="Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services"></a>Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10653">http://arxiv.org/abs/2307.10653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manqing Dong, Zhanxiang Zhao, Yitong Geng, Wentao Li, Wei Wang, Huai Jiang</li>
<li>for: 这篇论文是为了提高时间序列异常探测的自动化化，以提高工业监控服务的可靠性和系统性能。</li>
<li>methods: 本文提出了一个全面的自动化parameter优化框架，包括三个优化目标：预测得分、形状得分和敏感度得分，这些目标可以轻松地适应不同的模型后段，无需专业知识或手动标注努力。</li>
<li>results: 本文的提案框架已经在线上进行了超过六个月的实际应用，处理了每分钟50,000多个时间序列，并简化了用户的体验，仅需要提供预期的敏感值，并且实现了适当的侦测结果。<details>
<summary>Abstract</summary>
Time series anomaly detection is crucial for industrial monitoring services that handle a large volume of data, aiming to ensure reliability and optimize system performance. Existing methods often require extensive labeled resources and manual parameter selection, highlighting the need for automation. This paper proposes a comprehensive framework for automatic parameter optimization in time series anomaly detection models. The framework introduces three optimization targets: prediction score, shape score, and sensitivity score, which can be easily adapted to different model backbones without prior knowledge or manual labeling efforts. The proposed framework has been successfully applied online for over six months, serving more than 50,000 time series every minute. It simplifies the user's experience by requiring only an expected sensitive value, offering a user-friendly interface, and achieving desired detection results. Extensive evaluations conducted on public datasets and comparison with other methods further confirm the effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
时序列异常检测是对于工业监测服务而言是非常重要的，旨在确保可靠性并优化系统性能。现有的方法经常需要大量的标注资源和手动参数选择，这高亮了自动化的需求。这篇论文提出了一个完整的自动参数优化框架 для时序列异常检测模型。该框架引入了三个优化目标：预测得分、形态得分和敏感度得分，可以轻松地适应不同的模型背景而无需互知或手动标注努力。该提议的框架已经在线上运行了超过六个月，处理了每分钟50,000个时序列，并提供了一个易用的用户界面，以及达到了检测结果的所求的目标。对于公共数据集和其他方法进行了广泛的评估，并证实了提议的效果。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Latency-Probability-Prediction-for-Wireless-Networks-Focusing-on-Tail-Probabilities"><a href="#Data-Driven-Latency-Probability-Prediction-for-Wireless-Networks-Focusing-on-Tail-Probabilities" class="headerlink" title="Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities"></a>Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10648">http://arxiv.org/abs/2307.10648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samiemostafavi/wireless-pr3d">https://github.com/samiemostafavi/wireless-pr3d</a></li>
<li>paper_authors: Samie Mostafavi, Gourav Prateek Sharma, James Gross</li>
<li>for:  Ensuring end-to-end network latency with extremely high reliability (99.999%) in wireless networks, particularly for cyber-physical systems and human-in-the-loop applications.</li>
<li>methods:  Using state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to predict the tail of the latency distribution and estimate the likelihood of rare latencies conditioned on network parameters.</li>
<li>results:  Benchmarking the proposed approaches using actual latency measurements of IEEE 802.11g (WiFi), commercial private, and a software-defined 5G network to evaluate their sensitivities concerning the tail probabilities.<details>
<summary>Abstract</summary>
With the emergence of new application areas, such as cyber-physical systems and human-in-the-loop applications, there is a need to guarantee a certain level of end-to-end network latency with extremely high reliability, e.g., 99.999%. While mechanisms specified under IEEE 802.1as time-sensitive networking (TSN) can be used to achieve these requirements for switched Ethernet networks, implementing TSN mechanisms in wireless networks is challenging due to their stochastic nature. To conform the wireless link to a reliability level of 99.999%, the behavior of extremely rare outliers in the latency probability distribution, or the tail of the distribution, must be analyzed and controlled. This work proposes predicting the tail of the latency distribution using state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to estimate the likelihood of rare latencies conditioned on the network parameters, which can be used to make more informed decisions in wireless transmission. Actual latency measurements of IEEE 802.11g (WiFi), commercial private and a software-defined 5G network are used to benchmark the proposed approaches and evaluate their sensitivities concerning the tail probabilities.
</details>
<details>
<summary>摘要</summary>
随着新的应用领域的出现，如半导体物理系统和人工智能应用，需要保证端到端网络延迟在极高可靠性水平（99.999%）下达成。而根据IEEE 802.1as时间敏感网络（TSN）规范，可以用switched Ethernet网络达到这些要求。然而，在无线网络中实施TSN机制是困难的，因为无线链路的随机性。为了使 wireless link 达到99.999%的可靠性水平，需要分析和控制无线链路的很少极端延迟的行为，即延迟分布的尾部。本工作提议使用现有的数据驱动方法，如混合密度网络（MDN）和极值混合模型，来估计延迟分布的尾部，并用此来估计conditioned on 网络参数的罕见延迟的可能性。实际测量IEEE 802.11g（WiFi）、商业专用和软件定义5G网络的延迟值，用于评估和评测提议的敏感程度。
</details></li>
</ul>
<hr>
<h2 id="Fisher-Rao-distance-and-pullback-SPD-cone-distances-between-multivariate-normal-distributions"><a href="#Fisher-Rao-distance-and-pullback-SPD-cone-distances-between-multivariate-normal-distributions" class="headerlink" title="Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions"></a>Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10644">http://arxiv.org/abs/2307.10644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Nielsen</li>
<li>for: 该论文旨在处理多变量正态分布集合，如扩散tensor成像、结构tensor计算机视觉、雷达信号处理、机器学习等领域的数据集。</li>
<li>methods: 该论文提出了一种快速和稳定的方法来 aproximate multivariate normal distributions的Fisher-Rao距离，以及一种基于几何映射的方法来定义正态分布之间的距离。</li>
<li>results: 该论文的结果表明，基于Fisher信息度量的Fisher-Rao距离可以很好地approximate multivariate normal distributions，而且 Computationally, the pullback Hilbert cone distance is much lighter than the Fisher-Rao distance approximation, since it only requires the extreme minimal and maximal eigenvalues of matrices. In addition, the paper shows how to use these distances in clustering tasks.<details>
<summary>Abstract</summary>
Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of centered normal distributions. We show that the projective Hilbert distance on the cone yields a metric on the embedded normal submanifold and we pullback that cone distance with its associated straight line Hilbert cone geodesics to obtain a distance and smooth paths between normal distributions. Compared to the Fisher-Rao distance approximation, the pullback Hilbert cone distance is computationally light since it requires to compute only the extreme minimal and maximal eigenvalues of matrices. Finally, we show how to use those distances in clustering tasks.
</details>
<details>
<summary>摘要</summary>
数据集中的多变量正态分布非常普遍，例如扩散tensor成像、结构tensor计算机视觉、雷达信号处理、机器学习等。为了处理这些正态数据集，需要定义适当的不同量 zwischen normals和joinning它们的路径。 Fisher-Rao距离是一种原理的距离度量，但它没有固定形式，除了一些特殊情况之外。在这项工作中，我们首先报道了一种快速和稳定的方法来估算正态分布之间的Fisher-Rao距离。其次，我们引入一类基于正态映射的距离，该距离在高维正态区域上定义了一个正态分布的子集。我们展示了该距离在映射后的正态分布上是一个度量，并将其pullback到正态映射上，从而得到了一个距离和正态分布之间的缓解路径。与Fisher-Rao距离估算相比，pullback Hilbert cone距离是计算更轻量级的，因为只需计算正态分布的极小和最大特征值。最后，我们介绍了如何使用这些距离在分 clustering 任务中。
</details></li>
</ul>
<hr>
<h2 id="SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models"><a href="#SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models" class="headerlink" title="SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"></a>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10635">http://arxiv.org/abs/2307.10635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mandyyyyii/scibench">https://github.com/mandyyyyii/scibench</a></li>
<li>paper_authors: Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang</li>
<li>for: This paper aims to evaluate the reasoning capabilities of large language models (LLMs) on complex scientific problem solving.</li>
<li>methods: The paper introduces an expansive benchmark suite called SciBench, which features two datasets: an open set of collegiate-level scientific problems and a closed set of undergraduate-level exams in computer science and mathematics. The authors evaluate the performance of two representative LLMs with various prompting strategies.</li>
<li>results: The results show that current LLMs have an overall score of merely 35.80% and make ten different types of errors. The authors find that no single prompting strategy significantly outperforms others, and some strategies that improve in certain problem-solving skills result in declines in other skills.<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with an overall score of merely 35.80%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.
</details>
<details>
<summary>摘要</summary>
SciBench contains two datasets: an open set featuring collegiate-level scientific problems from mathematics, chemistry, and physics textbooks, and a closed set consisting of undergraduate-level exam problems in computer science and mathematics. We conduct an in-depth benchmark study of two representative LLMs using various prompting strategies, and find that current LLMs achieve only a 35.80% overall score.Through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis shows that no single prompting strategy consistently outperforms others, and some strategies that improve performance in one area lead to declines in other areas.We envision that SciBench will drive further advancements in the reasoning abilities of LLMs, ultimately contributing to scientific research and discovery.
</details></li>
</ul>
<hr>
<h2 id="Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes"><a href="#Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes" class="headerlink" title="Generative Language Models on Nucleotide Sequences of Human Genes"></a>Generative Language Models on Nucleotide Sequences of Human Genes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10634">http://arxiv.org/abs/2307.10634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boun-tabi/generativelm-genes">https://github.com/boun-tabi/generativelm-genes</a></li>
<li>paper_authors: Musa Nuri Ihtiyar, Arzucan Ozgur<br>for:This paper focuses on developing an autoregressive generative language model for DNA sequences, specifically on the nucleotide sequences of human genes.methods:The authors use a systematic approach to examine the performance of different models, including RNNs and N-grams, and explore the use of real-life tasks beyond classical metrics such as perplexity.results:The study finds that RNNs perform the best, and that selecting a language with a minimal vocabulary size does not significantly reduce the amount of data needed.<details>
<summary>Abstract</summary>
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotides without losing much information and making too much simplification. First of all, we systematically examined an almost entirely unexplored problem and observed that RNNs performed the best while simple techniques like N-grams were also promising. Another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural language. How essential using real-life tasks beyond the classical metrics such as perplexity is observed. Furthermore, checking whether the data-hungry nature of these models can be changed through selecting a language with minimal vocabulary size, four owing to four different types of nucleotides, is examined. The reason for reviewing this was that choosing such a language might make the problem easier. However, what we observed in this study was it did not provide that much of a change in the amount of data needed.
</details>
<details>
<summary>摘要</summary>
language models, primarily transformer-based ones, have achieved great success in NLP. to be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. however, the generative side of the coin is mainly unexplored to the best of our knowledge. consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. this decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotides without losing much information and making too much simplification. first of all, we systematically examined an almost entirely unexplored problem and observed that RNNs performed the best while simple techniques like N-grams were also promising. another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural language. how essential using real-life tasks beyond classical metrics such as perplexity is observed. furthermore, checking whether the data-hungry nature of these models can be changed through selecting a language with minimal vocabulary size, four owing to four different types of nucleotides, is examined. the reason for reviewing this was that choosing such a language might make the problem easier. however, what we observed in this study was it did not provide that much of a change in the amount of data needed.
</details></li>
</ul>
<hr>
<h2 id="Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa"><a href="#Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa" class="headerlink" title="Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa"></a>Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10633">http://arxiv.org/abs/2307.10633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shriyash K. Upadhyay, Etan J. Ginsberg</li>
<li>for: 该论文的目的是提出多方法自动训练（MMST），以增强语言模型的可用性和性能。</li>
<li>methods: 该论文使用了一种176B参数的语言和代码模型，并对其进行多方法自动训练，以便augment各种方法的优势和改善各种方法的缺陷。</li>
<li>results: 该论文显示，通过多方法自动训练，可以1)提高较弱的方法性能（最多30%），2)提高较强的方法性能（最多32.2%），3)提高相关 yet distinct tasks的性能（最多10.3%）。此外，论文还进行了ablation analyses，并发现MMST生成的数据量更大，但是性能提高的原因是多种方法的使用。<details>
<summary>Abstract</summary>
Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods. We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective. We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training.
</details>
<details>
<summary>摘要</summary>
大语言模型有许多方法解决同一问题，这引入了新的优势（不同的方法可能适用于不同的问题）和劣势（用户可能Difficult to determine哪种方法使用）。在这篇论文中，我们介绍了多种方法自我培训（MMST），其中一种方法在另一种方法的过滤输出上进行训练，从而可以增强每种方法的优势和缓解劣势。使用176亿参数模型，我们显示了以下三点：1. 使用MMST可以提高较弱的方法（最多30%），使模型更易用。2. 使用MMST可以提高较强的方法（最多32.2%），使模型更高效。3. 使用MMST可以提高相关而不同的任务（最多10.3%）的性能，通过提高模型生成合理性的能力。然后，我们进行了剥离分析，发现MMST生成了更多的数据，但是性能提高的原因是使用多种方法。我们还分析了引擎和反相性的作用，以便使MMST更有效。我们希望这篇论文的证据能够鼓励机器学习研究人员探索语言模型的新训练方法。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques"><a href="#Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques" class="headerlink" title="Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques"></a>Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10617">http://arxiv.org/abs/2307.10617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anusuya Baby Hari Krishnan</li>
<li>for: 本研究旨在提出一种机器学习模型，用于 indentifying 评论中的假评价（deceptive reviews），尤其是针对餐厅评论。</li>
<li>methods: 本研究采用了n-gram模型和max features技术来有效地识别假评价内容，并对五种不同的机器学习分类算法进行了比较。</li>
<li>results: 实验结果表明，使用了负面攻击分类器的方法可以达到最高的精度和假评价识别率。此外，研究还应用了深度学习技术来进一步提高假评价检测的效果。<details>
<summary>Abstract</summary>
In the contemporary digital landscape, online reviews have become an indispensable tool for promoting products and services across various businesses. Marketers, advertisers, and online businesses have found incentives to create deceptive positive reviews for their products and negative reviews for their competitors' offerings. As a result, the writing of deceptive reviews has become an unavoidable practice for businesses seeking to promote themselves or undermine their rivals. Detecting such deceptive reviews has become an intense and ongoing area of research. This research paper proposes a machine learning model to identify deceptive reviews, with a particular focus on restaurants. This study delves into the performance of numerous experiments conducted on a dataset of restaurant reviews known as the Deceptive Opinion Spam Corpus. To accomplish this, an n-gram model and max features are developed to effectively identify deceptive content, particularly focusing on fake reviews. A benchmark study is undertaken to explore the performance of two different feature extraction techniques, which are then coupled with five distinct machine learning classification algorithms. The experimental results reveal that the passive aggressive classifier stands out among the various algorithms, showcasing the highest accuracy not only in text classification but also in identifying fake reviews. Moreover, the research delves into data augmentation and implements various deep learning techniques to further enhance the process of detecting deceptive reviews. The findings shed light on the efficacy of the proposed machine learning approach and offer valuable insights into dealing with deceptive reviews in the realm of online businesses.
</details>
<details>
<summary>摘要</summary>
现代数字景观中，在线评论已成为不同业务的不可或缺的工具。marketers、广告人和在线业务在推广自己或抹黑竞争对手的产品和服务的过程中，都发现了奖励创造假阳性评论的做法。因此，创造假评论已成为促进自己或抹黑竞争对手的不可避免的做法。检测这些假评论已成为研究的焦点之一。这篇研究论文提出了一种机器学习模型，用于识别假评论，特别是针对餐厅的评论。这项研究通过对知名的餐厅评论数据集（Deceptive Opinion Spam Corpus）进行多个实验，开发了ngram模型和最佳特征来有效地识别假内容，特别是假评论。进一步，这篇研究进行了两种不同的特征提取技术的比较，然后与五种不同的机器学习分类算法结合。实验结果表明，通过适应性分类器得到了最高的准确率，不仅在文本分类方面，还在识别假评论方面。此外，研究还探讨了数据扩充和深度学习技术，以进一步提高检测假评论的过程。研究结果突出了提议的机器学习方法的效果，并为在线业务中处理假评论提供了有价值的思路。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges"><a href="#Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges" class="headerlink" title="Heterogeneous Federated Learning: State-of-the-art and Research Challenges"></a>Heterogeneous Federated Learning: State-of-the-art and Research Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10616">http://arxiv.org/abs/2307.10616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marswhu/hfl_survey">https://github.com/marswhu/hfl_survey</a></li>
<li>paper_authors: Mang Ye, Xiuwen Fang, Bo Du, Pong C. Yuen, Dacheng Tao</li>
<li>for: 本文是一篇关于 federated learning（FL）在异步izable 环境下的报告，它们提出了在实际应用中遇到的多种挑战，以及现有的解决方案。</li>
<li>methods: 本文提出了一种新的分类方法，包括数据水平、模型水平和服务器水平的分类方法。此外，文章还提出了一些关键的未来研究方向。</li>
<li>results: 本文通过对多种研究挑战和现有的解决方案进行分析，提出了一些关键的未来研究方向，可以帮助进一步发展 Federated Learning 领域。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level. Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field. A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.
</details>
<details>
<summary>摘要</summary>
随着联合学习（Federated Learning，FL）的应用范围扩大，它在大规模企业应用场景中受到了越来越多的关注。现有的联合学习研究主要集中在模型同质 Settings中。然而，实际的联合学习往往面临参与客户端的数据分布、模型架构、网络环境和硬件设备之间的差异。这种差异的联合学习（Heterogeneous Federated Learning，HFL）是更加复杂和多样化的，需要相应的研究挑战和解决方案。因此，一篇系统性的调查研究在这个领域是非常重要的。在本调查中，我们首先总结了HFL中不同方面的研究挑战，包括统计差异、模型差异、通信差异、设备差异以及其他挑战。此外，我们还进行了现有HFL方法的回顾，并提出了一种新的分类方法，根据HFL过程的三级层次：数据层、模型层和服务器层。最后，我们还讨论了未来研究的一些重要和优先的方向，以便进一步发展这一领域。关于HFL的相关研究可以通过https://github.com/marswhu/HFL_Survey查看更新的集成。
</details></li>
</ul>
<hr>
<h2 id="Flatness-Aware-Minimization-for-Domain-Generalization"><a href="#Flatness-Aware-Minimization-for-Domain-Generalization" class="headerlink" title="Flatness-Aware Minimization for Domain Generalization"></a>Flatness-Aware Minimization for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11108">http://arxiv.org/abs/2307.11108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingxuan Zhang, Renzhe Xu, Han Yu, Yancheng Dong, Pengfei Tian, Peng Cu</li>
<li>for: 这篇研究旨在探讨领域扩展（Domain Generalization，DG）中的优化器选择问题。</li>
<li>methods: 本研究提出了一种新的方法——Flatness-Aware Minimization for Domain Generalization（FAD），可以有效地优化零项和首项的平坦性同时，以提高DG模型的适用范围。</li>
<li>results: 实验结果显示FAD在多种DG数据集上具有优越性，并且能够发现更平坦的极点，较其他零项和首项平坦性感知优化方法更好。<details>
<summary>Abstract</summary>
Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-order flatness-aware optimization methods.
</details>
<details>
<summary>摘要</summary>
领域总结（DG）目标是学习能够适应未知分布变化的Robust模型。作为DG的关键方面，优化器选择尚未得到深入研究。现在，大多数DG方法采用DomainBed的标准准则，并使用Adam作为所有数据集的默认优化器。然而，我们发现Adam并不一定是大多数当前DG方法和数据集的优选优化器。基于损失函数地形的视角，我们提出了一种新的方法：适应性谱优化 для领域总结（FAD），可以高效地同时优化零次项和首次项的平坦性。我们提供了FAD的OOD泛化误差和收敛性的理论分析。我们的实验结果表明FAD在多个DG数据集上具有优秀的性能。此外，我们证明了FAD可以更好地找到平坦的极点，比其他零次项和首次项平坦性意识优化器更强。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Learning-based-Anomaly-Detection-for-IoT-Cybersecurity-via-Bayesian-Hyperparameters-Sensitivity-Analysis"><a href="#Ensemble-Learning-based-Anomaly-Detection-for-IoT-Cybersecurity-via-Bayesian-Hyperparameters-Sensitivity-Analysis" class="headerlink" title="Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis"></a>Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10596">http://arxiv.org/abs/2307.10596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Lai, Farnaz Farid, Abubakar Bello, Fariza Sabrina<br>for: 这 paper 的目的是提高 IoT 网络的安全性 via 异常检测。methods: 这 paper 使用了 ensemble 机器学习方法来提高异常检测的准确性，并使用 Bayesian 超参数优化来适应多种 IoT 传感器读数。results: 实验结果表明，这种方法在比较传统方法时有更高的预测力。<details>
<summary>Abstract</summary>
The Internet of Things (IoT) integrates more than billions of intelligent devices over the globe with the capability of communicating with other connected devices with little to no human intervention. IoT enables data aggregation and analysis on a large scale to improve life quality in many domains. In particular, data collected by IoT contain a tremendous amount of information for anomaly detection. The heterogeneous nature of IoT is both a challenge and an opportunity for cybersecurity. Traditional approaches in cybersecurity monitoring often require different kinds of data pre-processing and handling for various data types, which might be problematic for datasets that contain heterogeneous features. However, heterogeneous types of network devices can often capture a more diverse set of signals than a single type of device readings, which is particularly useful for anomaly detection. In this paper, we present a comprehensive study on using ensemble machine learning methods for enhancing IoT cybersecurity via anomaly detection. Rather than using one single machine learning model, ensemble learning combines the predictive power from multiple models, enhancing their predictive accuracy in heterogeneous datasets rather than using one single machine learning model. We propose a unified framework with ensemble learning that utilises Bayesian hyperparameter optimisation to adapt to a network environment that contains multiple IoT sensor readings. Experimentally, we illustrate their high predictive power when compared to traditional methods.
</details>
<details>
<summary>摘要</summary>
互联网智能设备（IoT）已经在全球范围内集成了数以十万计的智能设备，这些设备可以自动与其他相连的设备交换数据，而无需人类干预。IoT允许大规模数据聚合和分析，从而提高生活质量在多个领域。特别是数据收集到IoT中含有巨量数据异常检测信息。IoT网络设备的多样性同时是挑战和机遇，传统的网络安全监控方法通常需要不同类型的数据处理和处理，这可能会对具有多种特征的数据造成问题。然而，多种网络设备可以捕捉更多的信号，这对异常检测是特别有用。在这篇论文中，我们提出了一项总结性的研究，利用多个机器学习模型的 ensemble 方法提高 IoT 网络安全性 via 异常检测。而不是使用单一的机器学习模型，ensemble 学习可以将多种模型的预测力相互结合，在具有多种特征的数据集中提高预测精度。我们提议一种统一框架，利用 Bayesian 参数优化来适应包含多种 IoT 传感器读数的网络环境。实验表明，我们的方法具有高预测力，与传统方法相比。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques"><a href="#Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques" class="headerlink" title="Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques"></a>Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10588">http://arxiv.org/abs/2307.10588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanif Tayarani, Trisha V. Ramadoss, Vaishnavi Karanam, Gil Tal, Christopher Nitta</li>
<li>for: 降低排放和污染物对环境的影响，将交通领域电化。</li>
<li>methods: 使用人工神经网络算法，对BEV车辆的行程和充电资料进行预测。</li>
<li>results: 比较 benchmark 方法，MCDNN 能更好地预测 BEV 充电事件。<details>
<summary>Abstract</summary>
Energy systems, climate change, and public health are among the primary reasons for moving toward electrification in transportation. Transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. While great for climate and pollution goals, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a total of 1570167 vehicle miles traveled. The numerical findings revealed that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.
</details>
<details>
<summary>摘要</summary>
《能源系统、气候变化和公共卫生》是电动化交通的主要促进因素之一。由于气候变化和污染的关注，全球范围内的电动化交通吸引着广泛的推广。因此，许多汽车制造商即将停止生产内燃机油车，转而生产电池电动车（BEV）。在加利福尼亚州，BEV的采购率在增加，主要是由于气候变化和空气污染的问题。虽然这对气候和污染目标具有优秀的效果，但是不当管理BEV充电可能会导致充电基础设施不足和停电。这项研究开发了一种微型团集深度神经网络（MCDNN）算法，该算法可以高效地学习BEV的行驶和充电数据，以预测BEV的充电事件。MCDNN配置了加利福尼亚州2015-2020年间132台电动车的行驶记录，涵盖5种电动车型，共计1570167公里行驶。numerical发现，提出的MCDNN比各种参考方法，如支持向量机、最近邻居、决策树和其他神经网络模型在预测充电事件方面更有效。
</details></li>
</ul>
<hr>
<h2 id="A-Holistic-Assessment-of-the-Reliability-of-Machine-Learning-Systems"><a href="#A-Holistic-Assessment-of-the-Reliability-of-Machine-Learning-Systems" class="headerlink" title="A Holistic Assessment of the Reliability of Machine Learning Systems"></a>A Holistic Assessment of the Reliability of Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10586">http://arxiv.org/abs/2307.10586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Corso, David Karamadian, Romeo Valentin, Mary Cooper, Mykel J. Kochenderfer</li>
<li>for: 本研究旨在评估机器学习系统的可靠性，以便在高度竞争的领域中提高系统的可靠性。</li>
<li>methods: 本研究提出了一种整体评估机器学习系统可靠性的方法，包括五个关键属性的评估：内部分布准确率、环境变化快速稳定性、针对性攻击快速稳定性、校准性和外部分布检测。</li>
<li>results: 研究人员通过使用提出的方法对500多个模型进行评估，发现不同的算法方法可以同时提高多个可靠性指标，而不是只是优先一个指标。这项研究为机器学习可靠性的全面理解和未来研发提供了一份路线图。<details>
<summary>Abstract</summary>
As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. This paper proposes a holistic assessment methodology for the reliability of ML systems. Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection. A reliability score is also introduced and used to assess the overall system reliability. To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using our proposed reliability metrics and reliability score. Our analysis of over 500 models reveals that designing for one metric does not necessarily constrain others but certain algorithmic techniques can improve reliability across multiple metrics simultaneously. This study contributes to a more comprehensive understanding of ML reliability and provides a roadmap for future research and development.
</details>
<details>
<summary>摘要</summary>
machine learning (ML) 系统在高度重要的设置中如医疗、交通、军事和国家安全中越来越普遍，关于它们的可靠性的问题也得到了关注。尽管在进步方面做出了很大的进展，但是这些系统的性能可能会因为抗对抗攻击或环境变化而减退，导致过于自信的预测、输入错误的检测失败和不能适应意外的情况。本文提出了一种整体评估方法 для ML 系统的可靠性。我们的框架评估了五个关键属性：在输入数据集上的准确率、对输入数据集的变化robustness、对抗攻击的Robustness、calibration和对输入数据集之外的检测。我们还引入了一个可靠度分数，用于评估整体系统的可靠性。为了提供不同算法approach的性能分析，我们分类了现有的技术，然后使用我们的提出的可靠性指标和可靠度分数评估一些实际任务中的选择。我们的分析结果表明，不同的算法approach可以同时改善多个可靠性指标。这种研究对 ML 系统的可靠性进行了更全面的理解，并提供了未来研究和开发的道路图。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-model-for-offshore-China-sea-fog-forecasting"><a href="#Intelligent-model-for-offshore-China-sea-fog-forecasting" class="headerlink" title="Intelligent model for offshore China sea fog forecasting"></a>Intelligent model for offshore China sea fog forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10580">http://arxiv.org/abs/2307.10580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanfei Xiang, Qinghong Zhang, Mingqing Wang, Ruixue Xia, Yang Kong, Xiaomeng Huang</li>
<li>For: 预测海上雾霾的精准和准确性非常重要，以便有效管理海岸和海上经济活动。* Methods: 本研究使用机器学习方法，并在数值天气预测模型中嵌入，以解决海上雾霾预测的问题。在训练机器学习模型之前，我们使用时间延迟相关分析技术来 indentify关键预测因素，并解决海上雾霾出现的下面机制。* Results: 我们的机器学习基于方法在一年的测试数据上表现出色，超越了WRF-NMM和NOAA FSL的预测性能。具体来说，在预测海上雾霾视力低于或等于1公里的情况下，我们的方法在60小时前的预测中具有更高的检测可能性（POD）和更低的误风率（FAR）。<details>
<summary>Abstract</summary>
Accurate and timely prediction of sea fog is very important for effectively managing maritime and coastal economic activities. Given the intricate nature and inherent variability of sea fog, traditional numerical and statistical forecasting methods are often proven inadequate. This study aims to develop an advanced sea fog forecasting method embedded in a numerical weather prediction model using the Yangtze River Estuary (YRE) coastal area as a case study. Prior to training our machine learning model, we employ a time-lagged correlation analysis technique to identify key predictors and decipher the underlying mechanisms driving sea fog occurrence. In addition, we implement ensemble learning and a focal loss function to address the issue of imbalanced data, thereby enhancing the predictive ability of our model. To verify the accuracy of our method, we evaluate its performance using a comprehensive dataset spanning one year, which encompasses both weather station observations and historical forecasts. Remarkably, our machine learning-based approach surpasses the predictive performance of two conventional methods, the weather research and forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm developed by the National Oceanic and Atmospheric Administration (NOAA) Forecast Systems Laboratory (FSL). Specifically, in regard to predicting sea fog with a visibility of less than or equal to 1 km with a lead time of 60 hours, our methodology achieves superior results by increasing the probability of detection (POD) while simultaneously reducing the false alarm ratio (FAR).
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> Effective sea fog prediction is crucial for managing maritime and coastal economic activities. However, traditional numerical and statistical forecasting methods often fall short due to the complex and inherently variable nature of sea fog. This study aims to develop an advanced sea fog forecasting method using a numerical weather prediction model, with the Yangtze River Estuary (YRE) coastal area as a case study. Before training our machine learning model, we employ a time-lagged correlation analysis technique to identify key predictors and understand the underlying mechanisms driving sea fog occurrence. Additionally, we use ensemble learning and a focal loss function to address the issue of imbalanced data, which enhances the predictive ability of our model. To evaluate the accuracy of our method, we use a comprehensive dataset spanning one year, which includes both weather station observations and historical forecasts. Our machine learning-based approach outperforms two conventional methods, the Weather Research and Forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm developed by the National Oceanic and Atmospheric Administration (NOAA) Forecast Systems Laboratory (FSL). Specifically, our methodology achieves better results in predicting sea fog with a visibility of less than or equal to 1 km with a lead time of 60 hours, with higher probability of detection (POD) and lower false alarm ratio (FAR).
</details></li>
</ul>
<hr>
<h2 id="SecureBoost-Hyperparameter-Tuning-via-Multi-Objective-Federated-Learning"><a href="#SecureBoost-Hyperparameter-Tuning-via-Multi-Objective-Federated-Learning" class="headerlink" title="SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning"></a>SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10579">http://arxiv.org/abs/2307.10579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyao Ren, Yan Kang, Lixin Fan, Linghua Yang, Yongxin Tong, Qiang Yang</li>
<li>For: 这个研究目的是提出一个名为 Constrained Multi-Objective SecureBoost (CMOSB) 的算法，用于在阶层式联合学习中选择最佳的 SecureBoost 参数，以实现最佳的调解 между 功能损失、训练成本和隐私泄露。* Methods: 这个研究使用了 SecureBoost 算法，并将其与多bjective evolutionary algorithm (MOEA) 结合，以找到 Pareto 最佳解。另外，这个研究还提出了一个新的实例聚类攻击来量化隐私泄露。* Results: 实验结果显示，CMOSB 可以获得不只是基eline的优化参数，还可以找到最佳的参数集，以满足不同的 FL 参与者的需求。<details>
<summary>Abstract</summary>
SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting. It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability. However, SecureBoost suffers from high computational complexity and risk of label leakage. To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy. Existing methods either set hyperparameters empirically or heuristically, which are far from optimal. To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage. We design measurements of the three objectives. In particular, the privacy leakage is measured using our proposed instance clustering attack. Experimental results demonstrate that the CMOSB yields not only hyperparameters superior to the baseline but also optimal sets of hyperparameters that can support the flexible requirements of FL participants.
</details>
<details>
<summary>摘要</summary>
secureboost是一种树融合算法，利用同质加密保护数据隐私在垂直联合学习设置下。它在金融和医疗等领域广泛应用，因为它具有可读性、效果和隐私保护能力。然而，secureboost受到高计算复杂性和标签泄露的风险。为了激活secureboost的全部潜力，secureboost的超参数应该仔细选择，以达到最佳的平衡点。现有的方法可以通过实验或规则来设置超参数，但这些方法远不够优化。为了填补这一空白，我们提出了一种受限multi-目标secureboost（CMOSB）算法，以找到Pareto优化解决方案，每个解决方案都是一组超参数，实现了Utility损失、训练成本和隐私泄露的优化平衡。我们设计了三个目标量表示。具体来说，隐私泄露被我们提出的实例划分攻击来度量。实验结果表明，CMOSB可以不仅提供超参数优于基准值，还可以找到优化的超参数集，以满足联合学习参与者的灵活要求。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Federated-Learning-Convergence-with-Prototype-Regularization"><a href="#Boosting-Federated-Learning-Convergence-with-Prototype-Regularization" class="headerlink" title="Boosting Federated Learning Convergence with Prototype Regularization"></a>Boosting Federated Learning Convergence with Prototype Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10575">http://arxiv.org/abs/2307.10575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Qiao, Huy Q. Le, Choong Seon Hong</li>
<li>for: 这篇论文旨在提高 Federated Learning (FL) 中的模型性能，解决 Client 间资料不均匀问题。</li>
<li>methods: 本文提出了一种基于 Prototype 的调整策略，通过服务器将分布式 Client 的本地 Prototype 聚合成全局 Prototype，将其传回个别 Client 进行本地训练。</li>
<li>results: 实验结果显示，该方法在 MNIST 和 Fashion-MNIST 上得到了3.3% 和8.9% 的平均测试精度提升，相比最受欢迎的基于 FedAvg 的基eline。此外，本方法在不均匀环境下具有快速的整合速率。<details>
<summary>Abstract</summary>
As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
为了解决客户端数据不均匀性的问题，本文提出了一种基于原型的规范约束策略，用于在分布式机器学习中协同训练共享模型。具体来说，规范过程包括将分布在各客户端上的本地原型由服务器进行汇总，生成一个全局原型，然后将该全局原型发送回到各个客户端，以供本地训练指导。实验结果表明，与最常用的基准方法FedAvg相比，我们的方案在MNIST和Fashion-MNIST两个预测集上平均测试精度提高3.3%和8.9%。此外，我们的方法在不均匀设置下具有快速收敛的特点。
</details></li>
</ul>
<hr>
<h2 id="Deceptive-Alignment-Monitoring"><a href="#Deceptive-Alignment-Monitoring" class="headerlink" title="Deceptive Alignment Monitoring"></a>Deceptive Alignment Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10569">http://arxiv.org/abs/2307.10569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: 本研究旨在防止大机器学习模型的欺骗性行为，以及检测这些模型是否在不明确的目的下进行 modify 其行为。</li>
<li>methods: 本文提出了多个不同的机器学习子领域的研究方向，以检测和防止模型的欺骗性行为。</li>
<li>results: 本文认为，这些研究方向将在未来对检测和防止模型的欺骗性行为起到关键作用，并且将为敏捷机器学习社区带来新的研究机遇。<details>
<summary>Abstract</summary>
As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety & Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
</details>
<details>
<summary>摘要</summary>
随着大机器学习模型的能力不断增长，以及这些模型的自主权力不断扩展，一个新的对手出现了：模型本身。这种威胁被称为“欺骗启动”（deceptive alignment）在AI安全与Alignment社区中。因此，我们将这一方向称为“欺骗启动监测”（Deceptive Alignment Monitoring）。在这种工作中，我们认为未来几年将成为更加重要和关键的方向，并且这些领域的进步将带来长期挑战和新的研究机遇。我们最终呼吁了对抗机器学习社区更加参与这些emerging方向。
</details></li>
</ul>
<hr>
<h2 id="FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation"><a href="#FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation" class="headerlink" title="FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation"></a>FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10563">http://arxiv.org/abs/2307.10563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Pai, Andres Carranza, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: 提高模型 robustness 和可见性，并应对 adversarial 攻击</li>
<li>methods: 基于 probablistic 和几何学的方法，探索 activation space 中 pseudo-class 的性质变化，找到 adversarial 攻击的源头</li>
<li>results: 提供了一种可靠的 anomaly detection 方法，可以帮助提高模型的安全性和可靠性，并应用于实际场景中<details>
<summary>Abstract</summary>
We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.
</details>
<details>
<summary>摘要</summary>
我们介绍FACADE，一个新的机会概率和几何框架，用于无supervised机器学习领域中的机器学习过程中的非常�ynchronize攻击探测。其主要目的是提高机器学习模型的抗干扰能力，提高可扩展的模型监控，并在实际应用中展示了可靠的应用。FACADE的目标是生成机会概率分布 над circuit，从而获得 Pseudo-classes 或高维度模式在活动空间的变化特征，实现了强大的探测和抗干扰攻击的工具。我们的方法可以提高机器学习模型的类别Robustness，并且可以扩展到可扩展的模型监控。在实际应用中，FACADE 展示了可靠的应用。
</details></li>
</ul>
<hr>
<h2 id="Shared-Adversarial-Unlearning-Backdoor-Mitigation-by-Unlearning-Shared-Adversarial-Examples"><a href="#Shared-Adversarial-Unlearning-Backdoor-Mitigation-by-Unlearning-Shared-Adversarial-Examples" class="headerlink" title="Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples"></a>Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10562">http://arxiv.org/abs/2307.10562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu</li>
<li>for: 本研究探讨了如何使用小量净数据级联机器学习模型中的恶意攻击推议。</li>
<li>methods: 本研究使用了链接恶意风险和敌意风险的联系， derivates a novel upper bound for backdoor risk, 并提出了一种新的二级优化问题来 Mitigate backdoor 攻击。</li>
<li>results: 实验表明，我们提出的方法可以在不同的 benchmark 数据集和网络架构上达到 state-of-the-art 的性能。<details>
<summary>Abstract</summary>
Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense.
</details>
<details>
<summary>摘要</summary>
<<SYS>>本文探讨了使用小清洁数据集来纯化被攻击的模型的任务。我们发现了连接背门攻击风险和敌对攻击风险的关系，从而得出了一个新的背门风险Upper bound，它主要捕捉了由两个模型共享的敌对示例（SAEs）所带来的风险。基于这个Upper bound，我们提出了一种新的两级优化问题来 Mitigate 背门攻击。我们称之为共同敌对学习（SAU）。SAU首先生成SAEs，然后通过不正确地分类这些SAEs来减少背门效果在纯化后的模型中。实验结果表明，我们的提出的方法在多个benchmark数据集和网络架构上达到了背门防御的状态之 arts。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Post-variational-quantum-neural-networks"><a href="#Post-variational-quantum-neural-networks" class="headerlink" title="Post-variational quantum neural networks"></a>Post-variational quantum neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10560">http://arxiv.org/abs/2307.10560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Wei Huang, Patrick Rebentrost</li>
<li>for: 本研究旨在提出一种使用混合量子-классические计算和变量算法来解决量子计算机器硬件不够发展的问题，并且提高量子模型优化的效率。</li>
<li>methods: 本研究使用了混合量子-классические计算和变量算法，并提出了“后变量策略”，即将调整参数从量子计算机器传递到类型计算机器上进行优化。 ensemble策略和构建个别量子电路的设计原则也被讨论。</li>
<li>results: 本研究表明，使用后变量策略可以提高量子模型的优化效率，并且可以应用于实际应用场景如手写字符识别，实现96%的分类精度。<details>
<summary>Abstract</summary>
Quantum computing has the potential to provide substantial computational advantages over current state-of-the-art classical supercomputers. However, current hardware is not advanced enough to execute fault-tolerant quantum algorithms. An alternative of using hybrid quantum-classical computing with variational algorithms can exhibit barren plateau issues, causing slow convergence of gradient-based optimization techniques. In this paper, we discuss "post-variational strategies", which shift tunable parameters from the quantum computer to the classical computer, opting for ensemble strategies when optimizing quantum models. We discuss various strategies and design principles for constructing individual quantum circuits, where the resulting ensembles can be optimized with convex programming. Further, we discuss architectural designs of post-variational quantum neural networks and analyze the propagation of estimation errors throughout such neural networks. Lastly, we show that our algorithm can be applied to real-world applications such as image classification on handwritten digits, producing a 96% classification accuracy.
</details>
<details>
<summary>摘要</summary>
量子计算具有提供现代классиical超级计算机substantial计算优势的潜力。然而，当前硬件还不够先进，无法执行 fault-tolerant 量子算法。作为alternative，我们可以使用hybrid量子-классиical计算，使用变量算法，但这会导致恶势垃圾板块问题，使得梯度基于优化技术的敏感度变慢。在这篇论文中，我们讨论了“后变量策略”，即将可调参数从量子计算机shift到类型计算机，选择ensemble策略来优化量子模型。我们讨论了各种策略和设计原则，用于构建个性化的量子Circuit，其结果可以通过凸型Programming优化。此外，我们还讨论了post-variational量子神经网络的建筑设计，并分析了估计错误在such神经网络中的传播。最后，我们示出了我们的算法可以应用于实际应用场景，如手写数字识别，并达到96%的分类精度。
</details></li>
</ul>
<hr>
<h2 id="Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning"><a href="#Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning" class="headerlink" title="Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning"></a>Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10559">http://arxiv.org/abs/2307.10559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymlasu/para-atm-collection">https://github.com/ymlasu/para-atm-collection</a></li>
<li>paper_authors: Yutian Pang, Jueming Hu, Christopher S. Lieber, Nancy J. Cooke, Yongming Liu</li>
<li>for: 预测空交控制员（ATCo）的工作负担，以提高航空业务操作的安全性和空间利用率。</li>
<li>methods: 使用人类在Loop（HITL） simulations with retired ATCo，并对实际航空数据和工作负担标签进行分析。提议使用图表深度学习框架和协Forms预测ATCo工作负担水平。</li>
<li>results: 实验结果表明， besides 交通密度特征，交通冲突特征也对工作负担预测做出贡献（即最小水平&#x2F;垂直分离距离）。 directly learning from空间时间图像的空间特征可以提高预测精度，比手工设计的交通复杂度特征更高。 conformal prediction 是一种有价值的工具，可以进一步提高预测精度，并生成一个范围内的预测工作负担标签。<details>
<summary>Abstract</summary>
Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to identify the ATCo workload levels. The number of aircraft under the controller's control varies both spatially and temporally, resulting in dynamically evolving graphs. The experiment results suggest that (a) besides the traffic density feature, the traffic conflict feature contributes to the workload prediction capabilities (i.e., minimum horizontal/vertical separation distance); (b) directly learning from the spatiotemporal graph layout of airspace with graph neural network can achieve higher prediction accuracy, compare to hand-crafted traffic complexity features; (c) conformal prediction is a valuable tool to further boost model prediction accuracy, resulting a range of predicted workload labels. The code used is available at \href{https://github.com/ymlasu/para-atm-collection/blob/master/air-traffic-prediction/ATC-Workload-Prediction/}{$\mathsf{Link}$}.
</details>
<details>
<summary>摘要</summary>
空交控制（ATC）是一个安全关键的服务系统，需要地面空交控制员（ATCo）不断注意以维护每天的航空运输业务。ATCo的工作负担可能会对操作安全和空域使用产生负面影响。为了避免过载和确保ATCo的工作负担水平接受，需要准确预测ATCo的工作负担。在这篇论文中，我们首先进行了研究人员对ATCo工作负担的评估，主要来自空交 perspective。然后，我们 briefly introduce了人在Loop（HITL） simulations with retired ATCos，其中获取了空交数据和工作负担标签。 simulations were conducted under three Phoenix approach scenarios, while the human ATCos were asked to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis was conducted. Next, we proposed a graph-based deep-learning framework with conformal prediction to identify the ATCo workload levels. The number of aircraft under the controller's control varies both spatially and temporally, resulting in dynamically evolving graphs. The experiment results suggest that (a) besides the traffic density feature, the traffic conflict feature also contributes to the workload prediction capabilities (i.e., minimum horizontal/vertical separation distance); (b) directly learning from the spatiotemporal graph layout of airspace with graph neural network can achieve higher prediction accuracy, compared to hand-crafted traffic complexity features; (c) conformal prediction is a valuable tool to further boost model prediction accuracy, resulting in a range of predicted workload labels. The code used is available at $\mathsf{Link}$.
</details></li>
</ul>
<hr>
<h2 id="SC-VALL-E-Style-Controllable-Zero-Shot-Text-to-Speech-Synthesizer"><a href="#SC-VALL-E-Style-Controllable-Zero-Shot-Text-to-Speech-Synthesizer" class="headerlink" title="SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer"></a>SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10550">http://arxiv.org/abs/2307.10550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/0913ktg/sc_vall-e">https://github.com/0913ktg/sc_vall-e</a></li>
<li>paper_authors: Daegyeom Kim, Seongho Hong, Yong-Hoon Choi</li>
<li>For: The paper is written to propose a style control (SC) VALL-E model for expressive speech synthesis, which can generate diverse voices with controllable attributes such as emotion, speaking rate, pitch, and voice intensity.* Methods: The SC VALL-E model is based on the neural codec language model (VALE) and the generative pretrained transformer 3 (GPT-3), and it uses a newly designed style network to control the attributes of the generated speech. The model takes input from text sentences and prompt audio and is trained to generate controllable speech that can mimic the characteristics of the prompt audio.* Results: The paper conducts comparative experiments with three representative expressive speech synthesis models and measures word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics. The results show that the SC VALL-E model demonstrates competitive performance compared to the existing models and can generate a variety of expressive sounds with controllable attributes.<details>
<summary>Abstract</summary>
Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three representative expressive speech synthesis models: global style token (GST) Tacotron2, variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics to assess the accuracy of generated sentences. For comparing the quality of synthesized speech, we measure comparative mean option score (CMOS) and similarity mean option score (SMOS). To evaluate the style control ability of the generated speech, we observe the changes in F0 and mel-spectrogram by modifying the trained tokens. When using prompt audio that is not present in the training data, SC VALL-E generates a variety of expressive sounds and demonstrates competitive performance compared to the existing models. Our implementation, pretrained models, and audio samples are located on GitHub.
</details>
<details>
<summary>摘要</summary>
干支表达 synthesis 模型通常通过添加具有多个说话者、不同情感和不同说话风格的文本 corpus 来训练，以控制不同特性的语音并生成感兴趣的声音。在这篇文章中，我们提出了一种基于 neural codec 语言模型（称为 VALL-E）的风格控制（SC） VALLE 模型。我们在 VALL-E 的结构上添加了一个新的风格网络，并在这个风格网络中标识了不同的特征表达，如情感、说话速度、音高和声音强度。我们设计了一个可控制这些特征的模型。为评估 SC VALL-E 的表现，我们进行了与三种常见的表达性语音合成模型进行比较：global style token（GST） Tacotron2、variational autoencoder（VAE） Tacotron2 和原始 VALL-E。我们使用 word error rate（WER）、F0 voiced error（FVE）和 F0 gross pitch error（F0GPE）作为评估 metric。为比较生成的语音质量，我们使用 comparative mean option score（CMOS）和 similarity mean option score（SMOS）。为评估生成的语音风格控制能力，我们观察了 F0 和 mel-spectrogram 的变化。当使用不在训练数据中的提示音时，SC VALL-E 能够生成多种表达性的声音，并与现有模型相比具有竞争力。我们的实现、预训练模型和声音样本位于 GitHub。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Flat-Learning-based-Model-Predictive-Control-Using-a-Stability-State-and-Input-Constraining-Safety-Filter"><a href="#Differentially-Flat-Learning-based-Model-Predictive-Control-Using-a-Stability-State-and-Input-Constraining-Safety-Filter" class="headerlink" title="Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter"></a>Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10541">http://arxiv.org/abs/2307.10541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/utiasdsl/fmpc_socp">https://github.com/utiasdsl/fmpc_socp</a></li>
<li>paper_authors: Adam W. Hall, Melissa Greeff, Angela P. Schoellig<br>for: learning-based optimal control algorithms for unknown systemsmethods: exploits differential flatness, nonlinear transformation learned as a Gaussian process, safety filter, two successive convex optimizationsresults: similar performance to state-of-the-art learning-based controllers, significantly better computational efficiency, respects flat state and input constraints, guarantees stability<details>
<summary>Abstract</summary>
Learning-based optimal control algorithms control unknown systems using past trajectory data and a learned model of the system dynamics. These controllers use either a linear approximation of the learned dynamics, trading performance for faster computation, or nonlinear optimization methods, which typically perform better but can limit real-time applicability. In this work, we present a novel nonlinear controller that exploits differential flatness to achieve similar performance to state-of-the-art learning-based controllers but with significantly less computational effort. Differential flatness is a property of dynamical systems whereby nonlinear systems can be exactly linearized through a nonlinear input mapping. Here, the nonlinear transformation is learned as a Gaussian process and is used in a safety filter that guarantees, with high probability, stability as well as input and flat state constraint satisfaction. This safety filter is then used to refine inputs from a flat model predictive controller to perform constrained nonlinear learning-based optimal control through two successive convex optimizations. We compare our method to state-of-the-art learning-based control strategies and achieve similar performance, but with significantly better computational efficiency, while also respecting flat state and input constraints, and guaranteeing stability.
</details>
<details>
<summary>摘要</summary>
学习基于的优化控制算法可以控制未知系统使用过去轨迹数据和学习到系统动力学模型。这些控制器使用 линей化学习动力学模型，换取更快的计算速度，或者非线性优化方法，通常表现更好，但可能限制实时应用。在这种工作中，我们提出了一种新的非线性控制器，利用差分平凡性来实现与现有学习基于控制策略相似的性能，但计算效率明显更高。差分平凡性是动力系统的性质，通过非线性输入映射来将非线性系统 Linearize。在这里，非线性变换是通过 Gaussian Process 学习的，并用于安全筛选器，保证高概率稳定性和输入和平凡状态约束的满足。这个安全筛选器然后用于改进由平凡模型预测控制器输出的输入，通过两次 convex 优化来实现受限制的非线性学习基于优化控制。我们与当前学习基于控制策略进行比较，实现相似的性能，但计算效率明显更高，同时也遵守平凡状态和输入约束，并保证稳定性。
</details></li>
</ul>
<hr>
<h2 id="The-Extractive-Abstractive-Axis-Measuring-Content-“Borrowing”-in-Generative-Language-Models"><a href="#The-Extractive-Abstractive-Axis-Measuring-Content-“Borrowing”-in-Generative-Language-Models" class="headerlink" title="The Extractive-Abstractive Axis: Measuring Content “Borrowing” in Generative Language Models"></a>The Extractive-Abstractive Axis: Measuring Content “Borrowing” in Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11779">http://arxiv.org/abs/2307.11779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nedelina Teneva</li>
<li>for: 本研究旨在探讨生成模型的抽象性和内容授权问题，并提出了EXTRACTIVE-ABSTRACTIVE轴来评估生成模型。</li>
<li>methods: 本研究使用了生成模型对文本数据进行生成和抽象，并对生成结果进行评估。</li>
<li>results: 研究发现，生成模型的抽象性和内容授权问题需要更加重视，并提出了对生成模型的评估指标、数据集和注解指南。<details>
<summary>Abstract</summary>
Generative language models produce highly abstractive outputs by design, in contrast to extractive responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing & Attribution, we propose the the so-called Extractive-Abstractive axis for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本转换为简化中文。<</SYS>>生成语言模型会生成高度抽象的输出，与搜索引擎的EXTRACTIVE responses不同，这种特点对内容授权和归功有重要影响。我们提出EXTRACTIVE-ABSTRACTIVE轴来评估生成模型，并需要开发相应的指标、数据集和注释指南。我们只限制于文本 modal。
</details></li>
</ul>
<hr>
<h2 id="Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks"><a href="#Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks" class="headerlink" title="Fast Unsupervised Deep Outlier Model Selection with Hypernetworks"></a>Fast Unsupervised Deep Outlier Model Selection with Hypernetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10529">http://arxiv.org/abs/2307.10529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueying Ding, Yue Zhao, Leman Akoglu</li>
<li>for: 这篇论文主要针对无监督的偏差检测（Outlier Detection，OD）中的问题，即有效地调整（Hyperparameter，HP）的选择和优化。</li>
<li>methods: 本文提出了一个名为HYPER的方法，它通过设计和训练一个新的对应网络（Hypernetwork，HN），将 HP 映射到OD模型的优化参数。此外，HYPER还使用了元学习来训练一个代理验证函数，以有效地 validate OD 模型。</li>
<li>results: 实验结果显示，HYPER 在 35 个 OD 任务上实现了高性能，并与 8 个基eline 比较得到了显著的效率优势。<details>
<summary>Abstract</summary>
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding to varying HPs), which offers significant speed-up. In addition, it employs meta-learning on historical OD tasks with labels to train a proxy validation function, likewise trained with our proposed HN efficiently. Extensive experiments on 35 OD tasks show that HYPER achieves high performance against 8 baselines with significant efficiency gains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Beyond-Black-Box-Advice-Learning-Augmented-Algorithms-for-MDPs-with-Q-Value-Predictions"><a href="#Beyond-Black-Box-Advice-Learning-Augmented-Algorithms-for-MDPs-with-Q-Value-Predictions" class="headerlink" title="Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions"></a>Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10524">http://arxiv.org/abs/2307.10524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxin Li, Yiheng Lin, Shaolei Ren, Adam Wierman</li>
<li>for: 这个论文旨在研究在单轨迹 Markov决策过程（MDP）中的一致性和可靠性的负担关系，并在不可信 advise 的情况下进行研究。</li>
<li>methods: 该论文使用 Q-值建议来研究一致性和可靠性的负担关系，并在普通 MDP 模型中包括了连续和离散状态&#x2F;动作空间。</li>
<li>results: 研究结果表明，通过使用 Q-值建议，可以在不可信 advise 的情况下实现近似优化的性能保证，并且比靠solely black-box advise 可以获得更高的性能。<details>
<summary>Abstract</summary>
We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.
</details>
<details>
<summary>摘要</summary>
我们研究了在单轨时变Markov决策过程（MDP）中的一致性和可靠性的贸易。我们的工作与传统途径不同，即对建议视为黑盒来源的做法。相反，我们考虑了一种情况，在该情况下，建议的生成方式具有更多的信息。我们证明了一种首次的一致性和可靠性贸易，基于Q值建议在通用MDP模型中，该模型包括连续和离散状态/动作空间。我们的结果表明，通过利用Q值建议，可以在机器学习建议和一个可靠基础线上动态追求更好的性能，从而获得优化的性能保证，这与黑盒建议 alone 无法达到。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Handball-Matches-with-Statistically-Enhanced-Learning-via-Estimated-Team-Strengths"><a href="#Prediction-of-Handball-Matches-with-Statistically-Enhanced-Learning-via-Estimated-Team-Strengths" class="headerlink" title="Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths"></a>Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11777">http://arxiv.org/abs/2307.11777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Felice, Christophe Ley</li>
<li>for: 预测手球赛事</li>
<li>methods: 使用Statistically Enhanced Learning（SEL）模型，并与现有模型进行比较，以评估其性能能力</li>
<li>results: 模型的准确率高于80%，并且通过可解释方法提供了有价值的统计和预测性能分析，有助于手球队教练提前准备比赛。<details>
<summary>Abstract</summary>
We propose a Statistically Enhanced Learning (aka. SEL) model to predict handball games. Our Machine Learning model augmented with SEL features outperforms state-of-the-art models with an accuracy beyond 80%. In this work, we show how we construct the data set to train Machine Learning models on past female club matches. We then compare different models and evaluate them to assess their performance capabilities. Finally, explainability methods allow us to change the scope of our tool from a purely predictive solution to a highly insightful analytical tool. This can become a valuable asset for handball teams' coaches providing valuable statistical and predictive insights to prepare future competitions.
</details>
<details>
<summary>摘要</summary>
我们提出了一个统计增强学习（简称 SEL）模型，用于预测手球比赛。我们的机器学习模型，通过添加 SEL 特征，超过了现状最佳模型的准确率80%。在这项工作中，我们介绍了如何使用过去女子俱乐部比赛数据来训练机器学习模型。然后，我们比较了不同的模型，并评估它们的性能能力。最后，可视化方法使我们的工具从一种仅仅是预测解决方案转化为一种具有高度探索性的分析工具，这将成为手球队教练的宝贵统计和预测信息，以准备未来的比赛。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="FedSoup-Improving-Generalization-and-Personalization-in-Federated-Learning-via-Selective-Model-Interpolation"><a href="#FedSoup-Improving-Generalization-and-Personalization-in-Federated-Learning-via-Selective-Model-Interpolation" class="headerlink" title="FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation"></a>FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10507">http://arxiv.org/abs/2307.10507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghui Chen, Meirui Jiang, Qi Dou, Zehua Wang, Xiaoxiao Li</li>
<li>for: 本研究旨在提高分布式学习（Federated Learning，FL）中模型的通用性和全局性，解决当面临分布shift时现有FL算法的负面效果。</li>
<li>methods: 我们提出了一种新的联邦模型汤（Federated Model Soup，FMS）方法，通过在联邦训练阶段对本地和全局模型进行选择性 interpolate 来优化本地和全局性之间的负面效果。</li>
<li>results: 我们在Retinal和病理图像分类任务上评估了我们的方法，并实现了显著提高对于非典型数据的泛化性。代码可以在<a target="_blank" rel="noopener" href="https://github.com/ubc-tea/FedSoup%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ubc-tea/FedSoup中找到。</a><details>
<summary>Abstract</summary>
Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve the model's generalization performance. We evaluate our method on retinal and pathological image classification tasks, and our proposed method achieves significant improvements for out-of-distribution generalization. Our code is available at https://github.com/ubc-tea/FedSoup.
</details>
<details>
<summary>摘要</summary>
跨存储板 federated learning (FL) 可以开发机器学习模型在数据中心如医院和临床研究实验室等地的数据集上。然而，最近的研究发现，当面临分布变化时，当前的 FL 算法面临一种本地和全球性能之间的负权补偿。特别是，个性化 FL 方法有偏向本地数据过拟合的倾向，导致本地模型呈锐降谷，阻碍其在不同数据集上的泛化性能。在本文中，我们提出一种新的联邦模型汤 soup 方法（即选择性 interpolate 模型参数），以优化本地和全球性能之间的负权补偿。具体来说，在联邦训练阶段，每个客户端都会维护自己的全球模型池，并在本地和全球模型之间进行选择性 interpolate 模型参数。这有助于解决过拟合问题，寻找平降谷，可以显著提高模型的泛化性能。我们在Retinal和病理图像分类任务上评估了我们的方法，并取得了显著的外部数据集泛化性能改进。我们的代码可以在https://github.com/ubc-tea/FedSoup 中找到。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Interpretable-Subspaces-in-Image-Representations"><a href="#Identifying-Interpretable-Subspaces-in-Image-Representations" class="headerlink" title="Identifying Interpretable Subspaces in Image Representations"></a>Identifying Interpretable Subspaces in Image Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10504">http://arxiv.org/abs/2307.10504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Kalibhat, Shweta Bhardwaj, Bayan Bruss, Hamed Firooz, Maziar Sanjabi, Soheil Feizi</li>
<li>for: 本文旨在解释图像表示的特征，提高图像表示的可解释性。</li>
<li>methods: 本文使用了对比概念（Contrasting Concepts）来解释图像表示的特征。首先，使用大量captioning数据集（如LAION-400m）和预训练的视觉语言模型（如CLIP）来生成特征的描述。然后，对每个描述语言进行分数和排名，从而得到少量共享的人类可理解的概念，它们准确地描述了目标特征。此外，本文还使用了对比解释，使用低活跃图像（counterfactual）来消除幻觉的概念。</li>
<li>results: 研究发现，许多现有的方法只能独立解释特征的一部分，但是使用FALCON可以解释大型表示空间中的特征，并且可以通过高阶分数来解释特征。此外，本文还提出了一种将概念从一个可解释的表示空间传递到另一个未知表示空间的技术。<details>
<summary>Abstract</summary>
We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON. We discuss how extracted concepts can be used to explain and debug failures in downstream tasks. Finally, we present a technique to transfer concepts from one (explainable) representation space to another unseen representation space by learning a simple linear transformation.
</details>
<details>
<summary>摘要</summary>
我们提出了自动Feature解释使用对比概念（FALCON）的解释框架，用于解释图像表示中的特征。为目标特征，FALCON使用大量captioningdataset（如LAION-400m）和预训练的视觉语言模型（如CLIP）来caption高度活跃的裁剪图像。每个单词在caption中被分数和排名，导致一小数量的共享、人类可理解的概念，准确地描述目标特征。此外，FALCON还使用对比解释使用低活跃（counterfactual）图像，以消除幻数概念。现有的方法大多解释特征独立，但我们发现在当今的自然语言和指导下的模型中， Less than 20% of the representation space can be explained by individual features。我们表明，在更大的空间中，特征在组合 изуча时变得更加解释，可以通过高级分数概念来解释。我们还讨论了抽取的概念如何用于解释和调试下游任务的失败。最后，我们提出了一种将概念从一个可解释的表示空间传输到另一个未知表示空间的学习简单线性变换的技术。
</details></li>
</ul>
<hr>
<h2 id="A-Competitive-Learning-Approach-for-Specialized-Models-A-Solution-for-Complex-Physical-Systems-with-Distinct-Functional-Regimes"><a href="#A-Competitive-Learning-Approach-for-Specialized-Models-A-Solution-for-Complex-Physical-Systems-with-Distinct-Functional-Regimes" class="headerlink" title="A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes"></a>A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10496">http://arxiv.org/abs/2307.10496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Okezzi F. Ukorigho, Opeoluwa Owoyele</li>
<li>for: 该文章是为了提出一种新的竞争学习方法，用于获取基于数据的物理系统模型。</li>
<li>methods: 该方法使用动态损失函数，让一组模型同时在数据上进行训练，以便在数据中发现不同的功能 режи度。</li>
<li>results: 实验结果表明，该方法可以成功地发现功能 régime，找到真正的管理方程，并减少测试错误。<details>
<summary>Abstract</summary>
Complex systems in science and engineering sometimes exhibit behavior that changes across different regimes. Traditional global models struggle to capture the full range of this complex behavior, limiting their ability to accurately represent the system. In response to this challenge, we propose a novel competitive learning approach for obtaining data-driven models of physical systems. The primary idea behind the proposed approach is to employ dynamic loss functions for a set of models that are trained concurrently on the data. Each model competes for each observation during training, allowing for the identification of distinct functional regimes within the dataset. To demonstrate the effectiveness of the learning approach, we coupled it with various regression methods that employ gradient-based optimizers for training. The proposed approach was tested on various problems involving model discovery and function approximation, demonstrating its ability to successfully identify functional regimes, discover true governing equations, and reduce test errors.
</details>
<details>
<summary>摘要</summary>
科学和工程中的复杂系统有时会展现不同的行为方式，传统的全球模型很难捕捉这些复杂的行为范围，这限制了它们的准确性。为应对这个挑战，我们提议一种新的竞争学习方法，通过在数据上同时训练多个模型，并使用动态损失函数来让每个模型在训练过程中竞争对每个观察结果。这会使得模型能够成功地识别数据集中的不同功能 режи度。为证明该学习方法的效果，我们将其与不同的回归方法结合使用，这些回归方法使用梯度基于优化器进行训练。我们在各种模型发现和函数近似问题中测试了该方法，并证明了它可以成功地识别功能 режи度，发现真正的管理方程和降低测试错误。
</details></li>
</ul>
<hr>
<h2 id="Novel-Batch-Active-Learning-Approach-and-Its-Application-to-Synthetic-Aperture-Radar-Datasets"><a href="#Novel-Batch-Active-Learning-Approach-and-Its-Application-to-Synthetic-Aperture-Radar-Datasets" class="headerlink" title="Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets"></a>Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10495">http://arxiv.org/abs/2307.10495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chapman20j/sar_bal">https://github.com/chapman20j/sar_bal</a></li>
<li>paper_authors: James Chapman, Bohan Chen, Zheng Tan, Jeff Calder, Kevin Miller, Andrea L. Bertozzi</li>
<li>for: 这个论文主要针对的是sequential active learning方法在Synthetic Aperture Radar（SAR）数据集上的应用和提高。</li>
<li>methods: 这篇论文提出了一种新的两部分方法，包括Dijkstra的 Annulus Core-Set（DAC）和LocalMax，以便批处理活动学习。</li>
<li>results: 根据实验结果，这种批处理活动学习方法可以与sequential active learning方法几乎达到同样的准确率，但是更高效，与批处理大小成比例。此外，该方法在classify FUSAR-Ship和OpenSARShip datasets时达到了状态平台CNN-based方法的性能。<details>
<summary>Abstract</summary>
Active learning improves the performance of machine learning methods by judiciously selecting a limited number of unlabeled data points to query for labels, with the aim of maximally improving the underlying classifier's performance. Recent gains have been made using sequential active learning for synthetic aperture radar (SAR) data arXiv:2204.00005. In each iteration, sequential active learning selects a query set of size one while batch active learning selects a query set of multiple datapoints. While batch active learning methods exhibit greater efficiency, the challenge lies in maintaining model accuracy relative to sequential active learning methods. We developed a novel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set (DAC) for core-set generation and LocalMax for batch sampling. The batch active learning process that combines DAC and LocalMax achieves nearly identical accuracy as sequential active learning but is more efficient, proportional to the batch size. As an application, a pipeline is built based on transfer learning feature embedding, graph learning, DAC, and LocalMax to classify the FUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms the state-of-the-art CNN-based methods.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we developed a novel two-part approach for batch active learning, consisting of Dijkstra's Annulus Core-Set (DAC) for core-set generation and LocalMax for batch sampling. The batch active learning process that combines DAC and LocalMax achieves nearly identical accuracy as sequential active learning but is more efficient, proportional to the batch size.As an application, we built a pipeline based on transfer learning feature embedding, graph learning, DAC, and LocalMax to classify the FUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms state-of-the-art CNN-based methods.Here is the translation in Simplified Chinese:活动学可以提高机器学习方法的性能，通过选择一个有限数量的未标注数据点，并将其用于标注，以最大化下面的类ifier表现。最近，有关synthetic aperture radar（SAR）数据的进步已经在arXiv:2204.0005中进行。在每个迭代中，sequential active learning选择一个查询集合，而批处活动学选择多个数据点的查询集合。虽然批处活动学方法更高效，但是保持模型准确性与sequential active learning方法相比是一大挑战。我们开发了一种新的、两部分的批处活动学方法，包括Dijkstra的Annulus Core-Set（DAC）和LocalMax。这种批处活动学过程结合DAC和LocalMax可以实现与sequential active learning方法准确性相似，但是更高效，即与批处大小相关。作为应用，我们建立了基于传输学习特征嵌入、图学习、DAC和LocalMax的管道，用于分类FUSAR-Ship和OpenSARShip数据集。我们的管道超过了基于Convolutional Neural Networks（CNN）的状态控制方法。
</details></li>
</ul>
<hr>
<h2 id="Blockchain-Based-Federated-Learning-Incentivizing-Data-Sharing-and-Penalizing-Dishonest-Behavior"><a href="#Blockchain-Based-Federated-Learning-Incentivizing-Data-Sharing-and-Penalizing-Dishonest-Behavior" class="headerlink" title="Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior"></a>Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10492">http://arxiv.org/abs/2307.10492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Jaberzadeh, Ajay Kumar Shrestha, Faijan Ahamad Khan, Mohammed Afaan Shaikh, Bhargav Dave, Jason Geng</li>
<li>for: 本研究旨在提供一个整合数据信任的联邦学习框架，以便在多方合作下进行安全且公正的数据分享，并提供激励、存取控制机制和处罚不正行为。</li>
<li>methods: 本研究使用了InterPlanetary File System、区块链和智能合约来实现安全且可靠的数据分享，并将数据信任 integrate into federated learning，以提高联邦学习模型的准确性。</li>
<li>results: 实验结果显示，提案的模型能够提高联邦学习模型的准确性，并确保数据分享过程中的安全和公正。此外，研究者还发展了一个基于区块技术的分散式机器学习平台，能够在多方合作下训练 CNN 模型，并维护数据隐私和安全。<details>
<summary>Abstract</summary>
With the increasing importance of data sharing for collaboration and innovation, it is becoming more important to ensure that data is managed and shared in a secure and trustworthy manner. Data governance is a common approach to managing data, but it faces many challenges such as data silos, data consistency, privacy, security, and access control. To address these challenges, this paper proposes a comprehensive framework that integrates data trust in federated learning with InterPlanetary File System, blockchain, and smart contracts to facilitate secure and mutually beneficial data sharing while providing incentives, access control mechanisms, and penalizing any dishonest behavior. The experimental results demonstrate that the proposed model is effective in improving the accuracy of federated learning models while ensuring the security and fairness of the data-sharing process. The research paper also presents a decentralized federated learning platform that successfully trained a CNN model on the MNIST dataset using blockchain technology. The platform enables multiple workers to train the model simultaneously while maintaining data privacy and security. The decentralized architecture and use of blockchain technology allow for efficient communication and coordination between workers. This platform has the potential to facilitate decentralized machine learning and support privacy-preserving collaboration in various domains.
</details>
<details>
<summary>摘要</summary>
随着数据共享的重要性增加，保证数据的安全和可靠性变得越来越重要。数据治理是一种常见的数据管理方式，但它面临着数据孤岛、数据一致性、隐私、安全和访问控制等挑战。为了解决这些挑战，这篇论文提出了一个涵盖数据信任的 federated learning 框架，并与 InterPlanetary File System、区块链和智能合约结合，实现安全和互惠的数据分享，并提供了奖励、访问控制机制和惩戒任何不诚实行为。实验结果表明，提议的模型能够提高 federated learning 模型的准确率，同时保障数据分享的安全性和公平性。论文还描述了一个基于区块链技术的分布式 federated learning 平台，可以同时训练多个工作者的 CNN 模型，并保持数据隐私和安全性。该平台的分布式架构和使用区块链技术，可以实现高效的通信和协调。这种平台具有推动分布式机器学习和保持隐私协作的潜在潜力。
</details></li>
</ul>
<hr>
<h2 id="Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs"><a href="#Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs" class="headerlink" title="(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs"></a>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10490">http://arxiv.org/abs/2307.10490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebagdasa/multimodal_injection">https://github.com/ebagdasa/multimodal_injection</a></li>
<li>paper_authors: Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</li>
<li>for: 用图像和声音进行间接提示和指导 injection 攻击。</li>
<li>methods: 生成攻击者选择的抗干扰噪音或图像，并将其混合到原始模型中。</li>
<li>results: 当用户问题 benign 模型关于干扰后的图像或声音时，攻击者可以通过控制模型的输出文本和对话流来实现攻击。<details>
<summary>Abstract</summary>
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
</details>
<details>
<summary>摘要</summary>
我们示示如何使用图像和声音进行间接提示和指令注入在多modal LLMS中。攻击者创造了这些提示的攻击扰动，并与图像或音频录音混合在一起。当用户对（未修改、良好）模型询问这些混合过的图像或音频时，攻击扰动将使模型输出攻击者选择的文本和/或导致后续对话按照攻击者的指令进行。我们透过多个证明例子，证明这种攻击可以对LLLaVa和PandaGPT进行。
</details></li>
</ul>
<hr>
<h2 id="SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval"><a href="#SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval" class="headerlink" title="SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval"></a>SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10488">http://arxiv.org/abs/2307.10488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thakur-nandan/sprint">https://github.com/thakur-nandan/sprint</a></li>
<li>paper_authors: Nandan Thakur, Kexin Wang, Iryna Gurevych, Jimmy Lin</li>
<li>for: 这个论文主要是为了提供一个统一的 Python 工具集（SPRINT），用于评估基于含义搜索的神经稀缺检索模型。</li>
<li>methods: 这个论文使用了 Pyserini 和 Lucene 等工具来实现一个通用的接口，支持多种基于神经网络的稀缺检索模型。用户可以轻松地添加自己的定制模型，只需要定义权重方法即可。</li>
<li>results: 根据 authors 的实验结果，SPRINT 工具集可以在 BEIR 等 benchmark 上建立强大且可重复的零批稀缺检索基准。其中，SPLADEv2 模型在 BEIR 上的平均得分为 0.470 nDCG@10，胜过其他神经稀缺检索模型。 authors 还发现，SPLADEv2 生成的稀缺表示可以帮助其取得表现提升，其中大多数的字符出现在原始查询和文档之外。<details>
<summary>Abstract</summary>
Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The toolkit currently includes five built-in models: uniCOIL, DeepImpact, SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by defining their term weighting method. Using our toolkit, we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers. In this work, we further uncover the reasons behind its performance gain. We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document which is often crucial for its performance gains, i.e. a limitation among its other sparse counterparts. We provide our SPRINT toolkit, models, and data used in our experiments publicly here at https://github.com/thakur-nandan/sprint.
</details>
<details>
<summary>摘要</summary>
传统上，稀疏搜寻系统从字词表现来进行文档搜寻，如BM25，对于搜寻任务产生了重要影响。随着预训 трансформа器模型，如BERT，神经稀疏搜寻带来了一个新的时代。不过，有限的软件支持不同的稀疏模型在一个共同环境中运行，导致实践者很难比较不同的稀疏模型，并获得实际的评估结果。另外，许多先前的工作仅对内部过滤进行评估，即在MS MARCO上进行内部过滤。但是，实际搜寻系统中需要模型能够对未见过的零数据类型进行推导，这是一个重要的需求。在这个研究中，我们提供了SPRINT，一个基于Pyserini和Lucene的Python工具组，支持一个共同的界面，用于评估神经稀疏搜寻。工具组目前包括五个内置模型：uniCOIL、DeepImpact、SPARTA、TILDEv2和SPLADEv2。用户可以轻松地添加自己定义的条件评估方法。使用我们的工具组，我们建立了强大且可重现的零数据类型神经稀疏搜寻基准，并在BEIR上取得了最好的平均分为0.470 nDCG@10。在这个研究中，我们进一步探索了SPLADEv2的表现原因，发现它生成的稀疏表现中，大多数的字词位于原始查询和文档之外，这经常是其表现优化的关键。我们提供了我们在这个研究中使用的SPRINT工具组、模型和数据，可以在https://github.com/thakur-nandan/sprint上取得。
</details></li>
</ul>
<hr>
<h2 id="FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models"><a href="#FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models" class="headerlink" title="FinGPT: Democratizing Internet-scale Data for Financial Large Language Models"></a>FinGPT: Democratizing Internet-scale Data for Financial Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10485">http://arxiv.org/abs/2307.10485</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4finance-foundation/fingpt">https://github.com/ai4finance-foundation/fingpt</a></li>
<li>paper_authors: Xiao-Yang Liu, Guoxuan Wang, Daochen Zha<br>for:FinGPT aims to democratize Internet-scale financial data for large language models (LLMs) to revolutionize the finance industry.methods:FinGPT introduces an open-sourced and data-centric framework that automates the collection and curation of real-time financial data from diverse sources on the Internet.results:FinGPT provides researchers and practitioners with accessible and transparent resources to develop their FinLLMs, and demonstrates several applications including robo-advisor, sentiment analysis for algorithmic trading, and low-code development.<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from >34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from open-source general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes are available at https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经表现出了人类语言理解和生成的很高水平，这可能会革命化金融业。然而，现有的LLM在金融领域经常缺乏表现，这主要归结于通用文本数据和金融文本数据之间的差异。尽管只有有限的金融文本数据集available（数据集较小），而BloombergGPT，首个金融LLM（FinLLM），则是关闭源的（只发布了训练日志）。为了普及互联网级金融数据 для LLM，这是一个开放的挑战，因为数据来源多样化、信号噪声比较低和时间有效性很高。为了解决这些挑战，我们提出了一个开源和数据中心的框架，名为金融生成预训练变换器（Financial Generative Pre-trained Transformer，FinGPT）。FinGPT自动收集和整理互联网上 >34 个不同来源的实时金融数据，为研究人员和实践者提供了可 accessible 和 transparent 的资源，以便开发自己的FinLLM。此外，我们还提出了一种简单 yet effective的RLSP（市场反馈强化学习）策略，可以通过市场的自然反馈来训练FinLLM。此外，我们采用了LoRA（低级适应）方法，允许用户自定义自己的FinLLM，从开源通用自然语言模型（NLM）中获得优秀的性能，而不需要大量的人工调整。FinGPT还提供了多种应用，包括智能投资、情感分析 для算法交易和低代码开发。FinGPT的代码可以在 <https://github.com/AI4Finance-Foundation/FinGPT> 和 <https://github.com/AI4Finance-Foundation/FinNLP> 上下载。FinGPT的目标是普及FinLLM，促进创新，并在开放金融中解锁新的机会。
</details></li>
</ul>
<hr>
<h2 id="Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting"><a href="#Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting" class="headerlink" title="Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?"></a>Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10472">http://arxiv.org/abs/2307.10472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omkar Dige, Jacob-Junqi Tian, David Emerson, Faiza Khan Khattak</li>
<li>for: 评估语言模型中的社会偏见</li>
<li>methods: 采用零批示评估语言模型的偏见识别能力</li>
<li>results: 结果显示，通过训练 Alpaca 7B 模型，可以达到 56.7% 的准确率，并且规模和数据多样性的扩展可能会带来更好的表现。<details>
<summary>Abstract</summary>
As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.
</details>
<details>
<summary>摘要</summary>
随着语言模型应用的广度和深度快速扩展，现在越来越重要地建立高效的语言模型偏见评估和mitigation的框架。在这篇论文中，我们介绍了我们对适用于各种各样的引入语言模型的偏见识别能力进行评估的方法，包括链条（Chain-of-Thought）提示。在LLaMA和其两个 instrucion fine-tuned 版本中，Alpaca 7B 在偏见识别任务上表现最好，准确率为 56.7%。我们还证明了通过增加 LLVM 大小和数据多样性可以实现更大的性能提升。这是我们偏见 mitigation 框架的首个组成部分，我们会继续更新这个工作，以获得更多的结果。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Visualization-Types-and-Perspectives-in-Patents"><a href="#Classification-of-Visualization-Types-and-Perspectives-in-Patents" class="headerlink" title="Classification of Visualization Types and Perspectives in Patents"></a>Classification of Visualization Types and Perspectives in Patents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10471">http://arxiv.org/abs/2307.10471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tibhannover/patentimageclassification">https://github.com/tibhannover/patentimageclassification</a></li>
<li>paper_authors: Junaid Ahmed Ghauri, Eric Müller-Budack, Ralph Ewerth</li>
<li>for: 本研究旨在提高套件申请检索和浏览的效率，通过使用不同类型的视觉化和视角来显示创新的细节。</li>
<li>methods: 本研究使用了现代深度学习方法，包括变换器，进行图像类型和视角的分类。我们也对CLEF-IP dataset进行扩展，并提供了手动标注的ground truth。</li>
<li>results: 实验结果表明了提案的方法的可行性。我们将源代码、模型和数据集公开发布。<details>
<summary>Abstract</summary>
Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Experimental results have demonstrated the feasibility of the proposed approaches. Source code, models, and dataset will be made publicly available.
</details>
<details>
<summary>摘要</summary>
In this paper, we employ state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We expand the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. Furthermore, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Experimental results have demonstrated the feasibility of the proposed approaches. The source code, models, and dataset will be publicly available.
</details></li>
</ul>
<hr>
<h2 id="Properties-of-Discrete-Sliced-Wasserstein-Losses"><a href="#Properties-of-Discrete-Sliced-Wasserstein-Losses" class="headerlink" title="Properties of Discrete Sliced Wasserstein Losses"></a>Properties of Discrete Sliced Wasserstein Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10352">http://arxiv.org/abs/2307.10352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eloi Tanguy, Rémi Flamary, Julie Delon</li>
<li>for: 这个论文主要研究了 $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z) $ 的属性和优化性，其中 $\gamma_Y $ 和 $\gamma_Z $ 是两个 uniform 抽象概率分布。</li>
<li>methods: 这篇论文使用了多种方法，包括研究 $\mathcal{E} $ 的正则性和优化性，以及其 Monte-Carlo 采样 $\mathcal{E}_p $ 的渐近稳定性和 almost-sure uniform 收敛性。</li>
<li>results: 研究结果表明，在某些情况下，Stochastic Gradient Descent 方法可以减少 $\mathcal{E} $ 和 $\mathcal{E}_p $ 的优化问题，并且这些方法会收敛到（Clarke）优化点。<details>
<summary>Abstract</summary>
The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using only $p$ samples) and show convergence results on the critical points of $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniform convergence. Finally, we show that in a certain sense, Stochastic Gradient Descent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ converge towards (Clarke) critical points of these energies.
</details>
<details>
<summary>摘要</summary>
“划分 Wasserstein（SW）距离已成为比 Wasserstein 距离更受欢迎的选择，用于比较概率分布。它在图像处理、领域适应和生成模型中广泛应用，通常是寻找可以最小化 SW 的参数，以便作为这些参数的损失函数。这些优化问题都有同一个子问题，即寻找可以最小化 SW 能量。在这篇文章中，我们研究了 $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z) $ 的性质，其中 $Y \in \mathbb{R}^{n \times d}$ 是一个某个概率分布的支持。我们调查了这个能量的规律和优化性，以及其 Monte-Carlo 预估 $\mathcal{E}_p$ 的数值，并证明了这些点的均匀收摄和确定性。最后，我们显示了在某些意义上，使用 Stochastic Gradient Descent 方法优化 $\mathcal{E}$ 和 $\mathcal{E}_p$ 可以导向（Clarke）内部点的极值。”
</details></li>
</ul>
<hr>
<h2 id="A-data-science-axiology-the-nature-value-and-risks-of-data-science"><a href="#A-data-science-axiology-the-nature-value-and-risks-of-data-science" class="headerlink" title="A data science axiology: the nature, value, and risks of data science"></a>A data science axiology: the nature, value, and risks of data science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10460">http://arxiv.org/abs/2307.10460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael L. Brodie</li>
<li>for: 这篇论文是为了探讨数据科学的axiology，即其目的、性质、重要性、风险和价值，以帮助理解和定义数据科学，并找到其可能的利益和风险。</li>
<li>methods: 这篇论文使用了AXIOLOGY的方法来探讨数据科学的特点，包括其不可预测的性和AI的应用。</li>
<li>results: 这篇论文的结果表明，数据科学在知识发现方面具有很大的潜力和可能性，但同时也存在一些风险，例如不可预测的结果和AI的应用可能导致的不良影响。<details>
<summary>Abstract</summary>
Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery and will take us into new ways of understanding the world.
</details>
<details>
<summary>摘要</summary>
“数据科学不是一种科学。它是一种研究方法论，具有未曾探索的范围、大小、复杂性和知识发现的力量，超出人类理解的限制。它正在改变我们的世界，已经广泛应用于万千个应用领域，在人工智能竞赛中投入了巨资。这篇论文提出了数据科学的axiology，其目的、本质、重要性、风险和价值，通过探究和评估其非凡的特点。由于数据科学处于其初期，这些初步的论据axiology的目的是帮助我们理解和定义数据科学，认识其潜在的利益、风险和研究挑战。AI基于的数据科学是一种不确定性，可能更加真实地反映我们对世界的理解。数据科学将对我们的世界产生深远的影响，将带我们进入新的理解世界。”
</details></li>
</ul>
<hr>
<h2 id="A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints"><a href="#A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints" class="headerlink" title="A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints"></a>A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10459">http://arxiv.org/abs/2307.10459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Andrei V. Konstantinov, Lev V. Utkin</li>
<li>for: 提出了一种新的计算简单的神经网络输出值约束方法。</li>
<li>methods: 使用了额外的神经网络层来实现约束，并将约束转换为神经网络输出值的限制。</li>
<li>results: 方法可以简单地扩展到受约束的输入输出问题，并且可以实现不同类型的约束，包括线性和二次约束、等式约束和动态约束。计算复杂度为O(n*m)和O(n^2*m)。数据实验 validate了该方法。<details>
<summary>Abstract</summary>
A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pass of the proposed neural network layer by linear and quadratic constraints are O(n*m) and O(n^2*m), respectively, where n is the number of variables, m is the number of constraints. Numerical experiments illustrate the method by solving optimization and classification problems. The code implementing the method is publicly available.
</details>
<details>
<summary>摘要</summary>
一种新的计算简单的方法，用于在神经网络输出值上强制实施硬 convex 约束，被提出。该方法的关键思想是将神经网络参数 вектор映射到一个确定在可行集中的点上。该映射通过额外的神经网络层实现，该层受约束的输出约束。提出的方法可以简单地扩展到输出约束不仅仅是单个输出向量，而且也包括输入的共同约束。投影方法可以简单地在提出的方法中实现。该方法可以具体实现不同类型的约束，包括线性和quadratic约束，等式约束，以及边界约束。该方法的计算简单性是其重要特点，其前向传播复杂度为O(n\*m)和O(n^2\*m)，其中n是变量数，m是约束数。数值实验证明了该方法的可行性和分类能力。代码实现该方法公开可用。
</details></li>
</ul>
<hr>
<h2 id="A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset"><a href="#A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset" class="headerlink" title="A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset"></a>A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10455">http://arxiv.org/abs/2307.10455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zahrag/BIOSCAN-1M">https://github.com/zahrag/BIOSCAN-1M</a></li>
<li>paper_authors: Zahra Gharaee, ZeMing Gong, Nicholas Pellegrino, Iuliia Zarubiieva, Joakim Bruslund Haurum, Scott C. Lowe, Jaclyn T. A. McKeown, Chris C. Y. Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke, Angel X. Chang, Graham W. Taylor, Paul Fieguth</li>
<li>for:  This paper aims to provide a large dataset of hand-labelled insect images to train computer-vision models for taxonomic assessment, and to lay the foundation for a comprehensive survey of global biodiversity.</li>
<li>methods: The dataset, called BIOSCAN-Insect, includes raw nucleotide barcode sequences and assigned barcode index numbers for each record, and is primarily used to train computer-vision models for image-based taxonomic assessment.</li>
<li>results: The paper presents a million-image dataset with a long-tailed class-imbalance distribution and highly fine-grained classification problem at lower taxonomic levels, which provides a challenging task for image-based taxonomic classification.<details>
<summary>Abstract</summary>
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.
</details>
<details>
<summary>摘要</summary>
寻求catalóg insect多样性，我们提出一个新的大型手标注 insect 图像数据集，称为 BIOSCAN-Insect 数据集。每个记录都由专家taxonomically 分类，同时还有关联的遗传信息，包括 raw  нуклеоти德核心序列和分配给每个物种的核心序列编号，这些是生物学基于的种类分类的代理。本文报道一个精心纪录 million 张图像数据集，主要用于训练计算机视觉模型，以提供图像基于的种类评估。然而，数据集还具有一些吸引人的特征，如生物学性的长尾分布和生物学分类系统的层次结构，这些特征都是机器学习社区的研究对象。 basis 的目标是建立一个全面的 global 生物多样性 监测系统，本文引入数据集并通过实现和分析基eline 分类器来探讨分类任务。
</details></li>
</ul>
<hr>
<h2 id="The-importance-of-feature-preprocessing-for-differentially-private-linear-optimization"><a href="#The-importance-of-feature-preprocessing-for-differentially-private-linear-optimization" class="headerlink" title="The importance of feature preprocessing for differentially private linear optimization"></a>The importance of feature preprocessing for differentially private linear optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11106">http://arxiv.org/abs/2307.11106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziteng Sun, Ananda Theertha Suresh, Aditya Krishna Menon</li>
<li>for: 本研究目的是研究 differentially private stochastic gradient descent (DPSGD) 是否具有 sufficient condition to find a good minimizer for every dataset under privacy constraints.</li>
<li>methods: 本研究使用了 differentially private stochastic gradient descent (DPSGD) 和其 variants，以及 feature preprocessing.</li>
<li>results: 研究发现，without feature preprocessing, DPSGD 会导致 privacy error proportional to the maximum norm of features over all samples. 我们提出了一种名为 DPSGD-F 的算法，combines DPSGD with feature preprocessing, and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features. 我们还在图像分类 benchmarks 中证明了它的实用性.<details>
<summary>Abstract</summary>
Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step gradients are clipped and combined with some noise. Given the increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature preprocessing and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features $\max_{x, x' \in D} \|x - x'\|_2$. We then demonstrate the practicality of our algorithm on image classification benchmarks.
</details>
<details>
<summary>摘要</summary>
<SYS> translate-into:zh-CN</SYS>训练机器学习模型 WITH differential privacy (DP) 在最近几年内得到了越来越多的关注。DP中最受欢迎的算法之一是差分隐私梯度下降 (DPSGD) 和其变体，在每步都将梯度clip和杂音结合在一起。随着 DPSGD 的使用越来越普遍，我们问：DPSGD 是否能够在隐私限制下找到每个数据集上的好最小值？作为回答的第一步，我们证明了非私有优化不同于隐私优化，private feature preprocessing 是必需的。在详细的证明中，我们证明了在 Linear classification 任务上，如果没有 feature preprocessing，DPSGD 会导致隐私错误与最大特征值的最大值成正比。我们then propose了一个名为 DPSGD-F 的算法，它将 DPSGD 与 feature preprocessing 结合，并证明了在分类任务上，它的隐私错误与特征值的最大值成正比。最后，我们在图像分类标准 benchmark 上证明了我们的算法的实用性。
</details></li>
</ul>
<hr>
<h2 id="Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model"><a href="#Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model" class="headerlink" title="Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model"></a>Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10443">http://arxiv.org/abs/2307.10443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shima Foolad, Kourosh Kiani</li>
<li>for: 提高机器阅读理解模型的复杂逻辑处理能力</li>
<li>methods: injecting external knowledge into the transformer architecture without relying on external knowledge</li>
<li>results: 模型在ReCoRD数据集上的表现比cutting-edge LUKE-Graph和基eline LUKE模型更优。<details>
<summary>Abstract</summary>
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still fall short in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. To address this limitation, many recent works have proposed injecting external knowledge into the model. However, selecting relevant external knowledge, ensuring its availability, and requiring additional processing steps remain challenging. In this paper, we introduce a novel attention pattern that integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture without relying on external knowledge. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism. The experimental findings corroborate that our model outperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on the ReCoRD dataset that focuses on commonsense reasoning.
</details>
<details>
<summary>摘要</summary>
尽管变换器模型在机器阅读理解任务中做出了重要进步，但它们仍然缺乏明确的知识表达，导致在复杂的推理任务中表现不佳。为解决这一限制，许多最近的研究强调在模型中注入外部知识。然而，选择相关的外部知识，确保其可用性，以及需要额外的处理步骤仍然是挑战。本文提出了一种新的注意模式，即在变换器架构中 интеグ推理知识来自多样化图表示。该注意模式包括三个关键元素：全局-本地注意力 для单词Token，对于实体Token的注意力，以及对每个实体Token和单词Token之间的关系类型进行考虑。这些元素的结合使得注意力得到优化。此外，我们还采用特殊相对位标签，使得该注意模式可以与LUKE模型的实体意识自注意机制集成。实验结果表明，我们的模型在 Commonsense Reasoning 数据集上比悉心LUKE-Graph和基础LUKE模型表现出色。
</details></li>
</ul>
<hr>
<h2 id="Confidence-Estimation-Using-Unlabeled-Data"><a href="#Confidence-Estimation-Using-Unlabeled-Data" class="headerlink" title="Confidence Estimation Using Unlabeled Data"></a>Confidence Estimation Using Unlabeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10440">http://arxiv.org/abs/2307.10440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/topoxlab/consistency-ranking-loss">https://github.com/topoxlab/consistency-ranking-loss</a></li>
<li>paper_authors: Chen Li, Xiaoling Hu, Chao Chen</li>
<li>for: 这篇论文的目的是提出一种基于半监督学习的信任估计方法，即使训练标签很少也可以估计模型对未标注样本的信任程度。</li>
<li>methods: 该方法使用训练过程中预测的一致性作为代理函数，并提出了一种一致性排名损失函数来估计信任程度。</li>
<li>results: 在图像分类和分割任务上，该方法实现了领先的性能在信任估计中，并且通过下游活动学任务的示例表明了该方法的优势。Here’s the English version of the three key points:</li>
<li>for: The purpose of this paper is to propose a confidence estimation method for a semi-supervised setting, where most training labels are unavailable.</li>
<li>methods: The method uses the consistency of predictions through the training process as a surrogate function, and proposes a consistency ranking loss function for confidence estimation.</li>
<li>results: On both image classification and segmentation tasks, the proposed method achieves state-of-the-art performances in confidence estimation, and demonstrates its advantage through an active learning task.<details>
<summary>Abstract</summary>
Overconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process. We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. The code is available at https://github.com/TopoXLab/consistency-ranking-loss
</details>
<details>
<summary>摘要</summary>
通过训练过程中的预测一致性来估计模型的自信度，我们提出了首个在半监督Setting下的自信度估计方法。即使有限的训练标签，我们仍可以通过训练过程中的预测一致性来理想地估计模型对无标示样本的自信度。我们使用训练一致性作为代理函数，并提出了一种一致排名损失用于自信度估计。在图像分类和 segmentation 任务中，我们的方法实现了状态机器人的表现，并且我们还证明了我们的方法在下游活动学任务中的利好。代码可以在 https://github.com/TopoXLab/consistency-ranking-loss 上获取。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Molecular-Property-Predictions-with-Graph-Neural-Architecture-Search"><a href="#Uncertainty-Quantification-for-Molecular-Property-Predictions-with-Graph-Neural-Architecture-Search" class="headerlink" title="Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search"></a>Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10438">http://arxiv.org/abs/2307.10438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengli Jiang, Shiyi Qin, Reid C. Van Lehn, Prasanna Balaprakash, Victor M. Zavala</li>
<li>for: 用于 молекуляр的性能预测</li>
<li>methods: 使用自动搜索生成高性能 GNN  ensemble，并使用 variance decomposition 分解数据和模型不确定性</li>
<li>results: 在多个 benchmark 数据集上表现出色，在预测准确性和 UQ 性能方面超过现有方法，并通过 t-SNE 可视化探索分子特征和不确定性的相关性。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to explore correlations between molecular features and uncertainty, offering insight for dataset improvement. AutoGNNUQ has broad applicability in domains such as drug discovery and materials science, where accurate uncertainty quantification is crucial for decision-making.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 已经成为分子性质预测中一种显著的数据驱动方法。然而，典型的 GNN 模型无法量化预测结果的不确定性。这种能力是在下游任务中使模型使用和部署的信任性质的关键。为此，我们介绍 AutoGNNUQ，一种自动 uncertainty quantification（UQ）方法 для分子性质预测。AutoGNNUQ 利用架构搜索生成一个高性能的 GNN ensemble，以便估计预测结果的不确定性。我们的方法使用差分分析将数据（ aleatoric）和模型（epistemic）不确定性分解，为了减少它们。在我们的计算实验中，我们证明 AutoGNNUQ 在多个benchmark数据集上表现出色，比现有的 UQ 方法更高精度和 UQ 性能。此外，我们使用 t-SNE 可视化来探索分子特征与不确定性之间的相关性，为了改进数据集。AutoGNNUQ 在药物探索和材料科学等领域有广泛的应用，因为它可以减少分子性质预测中的不确定性。
</details></li>
</ul>
<hr>
<h2 id="A-Bayesian-Programming-Approach-to-Car-following-Model-Calibration-and-Validation-using-Limited-Data"><a href="#A-Bayesian-Programming-Approach-to-Car-following-Model-Calibration-and-Validation-using-Limited-Data" class="headerlink" title="A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data"></a>A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10437">http://arxiv.org/abs/2307.10437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franklin Abodo</li>
<li>for: 这个论文是为了提供一种可以准确模拟驾驶行为的模型，以便在交通研究和工程中设计和评估道路改进计划。</li>
<li>methods: 这个论文使用了微型驾驶行为模型，从而 derivate macroscopic 措施如流速和拥堵。然而，现有的模型多数只适用于特定的交通情况和道路配置，而无法 direct 应用于工区（WZ）的情况。因此，美国交通部（USDOT）的负责交通研究的Volpe中心被委托，以开发一种可以准确模拟驾驶行为的CF模型，以便在工区中进行安全的交通规划。</li>
<li>results: 在模型开发过程中，Volpe研究人员发现了困难在模型kalibrase，因此提出了问题是否存在模型中的问题，数据中的问题，或者 kalibrase 过程中的问题。本论文使用 bayesian 方法进行数据分析和参数估计，以探讨和解决这些问题。首先，使用 bayesian 推理测量数据集的充分性。其次，比较 Volpe 研究人员使用的 Genetic Algorithm 基于 calibration 的过程和 bayesian calibration 的结果。最后，通过使用已有的 CF 模型，Wiedemann 99，对 Volpe 模型进行 probabilistic 模型化。验证是通过信息 критери估计 predictive 准确性来进行。<details>
<summary>Abstract</summary>
Traffic simulation software is used by transportation researchers and engineers to design and evaluate changes to roadways. These simulators are driven by models of microscopic driver behavior from which macroscopic measures like flow and congestion can be derived. Many models are designed for a subset of possible traffic scenarios and roadway configurations, while others have no explicit constraints on their application. Work zones (WZs) are one scenario for which no model to date has reproduced realistic driving behavior. This makes it difficult to optimize for safety and other metrics when designing a WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to develop a car-following (CF) model for use in microscopic simulators that can capture and reproduce driver behavior accurately within and outside of WZs. Volpe also performed a naturalistic driving study to collect telematics data from vehicles driven on roads with WZs for use in model calibration. During model development, Volpe researchers observed difficulties in calibrating their model, leaving them to question whether there existed flaws in their model, in the data, or in the procedure used to calibrate the model using the data. In this thesis, I use Bayesian methods for data analysis and parameter estimation to explore and, where possible, address these questions. First, I use Bayesian inference to measure the sufficiency of the size of the data set. Second, I compare the procedure and results of the genetic algorithm based calibration performed by the Volpe researchers with those of Bayesian calibration. Third, I explore the benefits of modeling CF hierarchically. Finally, I apply what was learned in the first three phases using an established CF model, Wiedemann 99, to the probabilistic modeling of the Volpe model. Validation is performed using information criteria as an estimate of predictive accuracy.
</details>
<details>
<summary>摘要</summary>
交通模拟软件被运输研究人员和工程师使用来设计和评估路径变化。这些模拟器由微型驾驶行为模型驱动，从而 derive 流和堵塞等宏观指标。许多模型适用于特定的交通enario 和路径配置，而其他些没有明确的约束。工地（WZ）是一种 scenarios  для which no model to date has reproduced realistic driving behavior. 这使得在设计工地时 diffficult to optimize for safety and other metrics。美国公路管理局委托美国交通部Volpe Center 开发一个可以在微型模拟器中Capture 和重现驾驶行为的 car-following （CF）模型。Volpe 还执行了一项自然驾驶研究，收集了在路径上驾驶的 vehicless 的 telematics 数据，用于模型均衡。在模型开发过程中，Volpe 研究人员注意到了困难在均衡模型，使得他们开始提问是否存在模型中的毛病、数据中的毛病或者均衡模型使用数据的过程中的毛病。在这个论文中，我使用 bayesian 方法来分析数据和参数估计，以探索和解决这些问题。首先，我使用 bayesian 推理来测试数据集的大小是否充分。其次，我比较了 Volpe 研究人员使用的 genetic algorithm 基于的均衡过程和 bayesian 均衡过程的结果。最后，我探索了模型CF 的层次结构化的好处。最后，我使用一个已知的 CF 模型，Wiedemann 99，来应用在 Volpe 模型上。验证是使用信息 критериion 来估计预测精度。
</details></li>
</ul>
<hr>
<h2 id="A-Matrix-Ensemble-Kalman-Filter-based-Multi-arm-Neural-Network-to-Adequately-Approximate-Deep-Neural-Networks"><a href="#A-Matrix-Ensemble-Kalman-Filter-based-Multi-arm-Neural-Network-to-Adequately-Approximate-Deep-Neural-Networks" class="headerlink" title="A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks"></a>A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10436">http://arxiv.org/abs/2307.10436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ved-piyush/menkf-ann-pul">https://github.com/ved-piyush/menkf-ann-pul</a></li>
<li>paper_authors: Ved Piyush, Yuchen Yan, Yuzhen Zhou, Yanbin Yin, Souparno Ghosh</li>
<li>for: This paper aims to propose a new technique for approximating deep learning models, specifically long short-term memory (LSTM) networks, using a Kalman filter-based approach.</li>
<li>methods: The proposed method, called Matrix Ensemble Kalman Filter-based multi-arm ANN (MEnKF-ANN), uses a multi-arm extension of a Kalman filter to approximate LSTM networks, and also performs explicit model stacking to handle unequal-size feature sets.</li>
<li>results: The proposed method can adequately approximate LSTM networks trained to classify carbohydrate substrates based on genomic sequences, and can also provide uncertainty estimates for the predictions.<details>
<summary>Abstract</summary>
Deep Learners (DLs) are the state-of-art predictive mechanism with applications in many fields requiring complex high dimensional data processing. Although conventional DLs get trained via gradient descent with back-propagation, Kalman Filter (KF)-based techniques that do not need gradient computation have been developed to approximate DLs. We propose a multi-arm extension of a KF-based DL approximator that can mimic DL when the sample size is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking that becomes relevant when the training sample has an unequal-size feature set. Our proposed technique can approximate Long Short-term Memory (LSTM) Networks and attach uncertainty to the predictions obtained from these LSTMs with desirable coverage. We demonstrate how MEnKF-ANN can "adequately" approximate an LSTM network trained to classify what carbohydrate substrates are digested and utilized by a microbiome sample whose genomic sequences consist of polysaccharide utilization loci (PULs) and their encoded genes.
</details>
<details>
<summary>摘要</summary>
深度学习器（DL）是现代预测机制的州标，应用于需要复杂高维数据处理的多个领域。尽管传统的DL通过梯度下降和反推训练，但是使用Kalman滤波器（KF）技术不需要计算梯度的方法已经开发出来。我们提议一种基于KF的多臂ANN（MEnKF-ANN），可以模拟DL，即使训练样本规模太小。我们的提议技术还实现了显式模型堆叠，当特征集的大小不同时变得有用。我们的提议技术可以模拟长期短 память网络（LSTM），并将对应的预测结果添加不确定性。我们示例了MEnKF-ANN可以“合理”地模拟一个基于PULs和其编码的微生物批处理训练集，用于预测微生物样本中吃掉和利用的碳水化合物substrate。
</details></li>
</ul>
<hr>
<h2 id="Learning-Formal-Specifications-from-Membership-and-Preference-Queries"><a href="#Learning-Formal-Specifications-from-Membership-and-Preference-Queries" class="headerlink" title="Learning Formal Specifications from Membership and Preference Queries"></a>Learning Formal Specifications from Membership and Preference Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10434">http://arxiv.org/abs/2307.10434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ameesh Shah, Marcell Vazquez-Chanlatte, Sebastian Junges, Sanjit A. Seshia</li>
<li>for: 学习形式规定（如自动机）的正式规定</li>
<li>methods: 提议一种新的框架， combining membership labels和对比 preference，以便更加灵活地进行活动规定学习</li>
<li>results: 在两个不同的领域中实现了框架，并证明了我们的方法可以强健地和方便地通过对比和成员标签来识别规定。<details>
<summary>Abstract</summary>
Active learning is a well-studied approach to learning formal specifications, such as automata. In this work, we extend active specification learning by proposing a novel framework that strategically requests a combination of membership labels and pair-wise preferences, a popular alternative to membership labels. The combination of pair-wise preferences and membership labels allows for a more flexible approach to active specification learning, which previously relied on membership labels only. We instantiate our framework in two different domains, demonstrating the generality of our approach. Our results suggest that learning from both modalities allows us to robustly and conveniently identify specifications via membership and preferences.
</details>
<details>
<summary>摘要</summary>
active learning是一种已经广泛研究的学习方法，用于学习正式规则，如自动机。在这项工作中，我们将活动规则学习框架扩展到请求组合会员标签和对比性偏好。这种组合方式允许我们更加灵活地进行活动规则学习，之前只能通过会员标签进行学习。我们在两个不同领域中实现了我们的框架，并证明了我们的方法的通用性。我们的结果表明，从两种模式学习可以强大地和方便地识别规则via会员和偏好。
</details></li>
</ul>
<hr>
<h2 id="DP-TBART-A-Transformer-based-Autoregressive-Model-for-Differentially-Private-Tabular-Data-Generation"><a href="#DP-TBART-A-Transformer-based-Autoregressive-Model-for-Differentially-Private-Tabular-Data-Generation" class="headerlink" title="DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation"></a>DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10430">http://arxiv.org/abs/2307.10430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigo Castellon, Achintya Gopal, Brian Bloniarz, David Rosenberg</li>
<li>for: 生成具有分布式隐私的 tabular 数据</li>
<li>methods: 使用 transformer 模型实现 differentially-private 推论</li>
<li>results: 在各种数据集上达到与 marginal-based 方法竞争的性能，在某些情况下甚至超越状态之arte 方法表现Here’s the translation in Simplified Chinese:</li>
<li>for: 生成具有分布式隐私的 tabular 数据</li>
<li>methods: 使用 transformer 模型实现 differentially-private 推论</li>
<li>results: 在各种数据集上达到与 marginal-based 方法竞争的性能，在某些情况下甚至超越状态之arte 方法表现<details>
<summary>Abstract</summary>
The generation of synthetic tabular data that preserves differential privacy is a problem of growing importance. While traditional marginal-based methods have achieved impressive results, recent work has shown that deep learning-based approaches tend to lag behind. In this work, we present Differentially-Private TaBular AutoRegressive Transformer (DP-TBART), a transformer-based autoregressive model that maintains differential privacy and achieves performance competitive with marginal-based methods on a wide variety of datasets, capable of even outperforming state-of-the-art methods in certain settings. We also provide a theoretical framework for understanding the limitations of marginal-based approaches and where deep learning-based approaches stand to contribute most. These results suggest that deep learning-based techniques should be considered as a viable alternative to marginal-based methods in the generation of differentially private synthetic tabular data.
</details>
<details>
<summary>摘要</summary>
“ differential privacy 的 synthetic 表格数据生成问题是一个日益重要的问题。传统的边缘基于方法已经取得了很好的成绩，但最近的工作表明，深度学习基于方法在这个领域比较落后。在这篇文章中，我们介绍了一种名为 Differentially-Private TaBular AutoRegressive Transformer (DP-TBART)，这是一种基于 transformer 的自然语言模型，可以保持 differential privacy 并在各种数据集上达到与边缘基于方法相当的性能。我们还提供了一个理论框架，用于理解边缘基于方法的局限性，以及深度学习基于方法在这个领域中的潜在贡献。这些结果表明，深度学习基于方法应该被视为 differential privacy 生成 synthetic 表格数据的可行的替代方案。”Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models"><a href="#PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models" class="headerlink" title="PreDiff: Precipitation Nowcasting with Latent Diffusion Models"></a>PreDiff: Precipitation Nowcasting with Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10422">http://arxiv.org/abs/2307.10422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihan Gao, Xingjian Shi, Boran Han, Hao Wang, Xiaoyong Jin, Danielle Maddix, Yi Zhu, Mu Li, Yuyang Wang</li>
<li>for: 预测地球系统的未来状况，使用深度学习技术来处理大量的空间时间数据。</li>
<li>methods: 提出了一种两stage管道，首先开发了一种可能性扩散模型（PreDiff），其可以进行 probabilistic 预测。其次，通过explicit地控制知识机制，使预测结果与专业知识相一致。</li>
<li>results: 通过在Synthetic dataset N-body MNIST和实际 precipitation nowcasting dataset SEVIR进行实验，确认了PreDiff的可行性和Domain-specific prior knowledge的可控性，并且预测结果具有高度的操作实用性。<details>
<summary>Abstract</summary>
Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly. We conduct empirical studies on two datasets: N-body MNIST, a synthetic dataset with chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset. Specifically, we impose the law of conservation of energy in N-body MNIST and anticipated precipitation intensity in SEVIR. Experiments demonstrate the effectiveness of PreDiff in handling uncertainty, incorporating domain-specific prior knowledge, and generating forecasts that exhibit high operational utility.
</details>
<details>
<summary>摘要</summary>
地球系统预测 Traditional 依靠复杂的物理模型，computationally expensive 和需要专业知识。 last decade, the unprecedented increase in spatiotemporal Earth observation data 使得 data-driven forecasting models using deep learning techniques 表现出了 promise  для diverse Earth system forecasting tasks。 However, these models either struggle with handling uncertainty 或 neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting:1. We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts.2. We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly.We conduct empirical studies on two datasets: N-body MNIST, a synthetic dataset with chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset. Specifically, we impose the law of conservation of energy in N-body MNIST and anticipated precipitation intensity in SEVIR. Experiments demonstrate the effectiveness of PreDiff in handling uncertainty, incorporating domain-specific prior knowledge, and generating forecasts that exhibit high operational utility.
</details></li>
</ul>
<hr>
<h2 id="Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games"><a href="#Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games" class="headerlink" title="Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games"></a>Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11105">http://arxiv.org/abs/2307.11105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Gillberg, Joakim Bergdahl, Alessandro Sestini, Andrew Eakins, Linus Gisslen</li>
<li>for: 这个技术论文是为了推广游戏生产中的机器学习技术，特意是通过让游戏自动化测试解决方案中加入了实验性的学习系统来提高测试覆盖率。</li>
<li>methods: 这篇技术论文描述了一种将学习系统与现有的脚本化测试解决方案集成，以提高测试覆盖率。具体来说，他们使用了一种基于强化学习的方法，通过让机器学习算法学习自动化测试过程中的优化策略，以提高测试效果。</li>
<li>results: 据文章报道，通过将学习系统与脚本化测试解决方案集成，可以有效提高测试覆盖率，并且在一些AAA游戏，如《战场2042》和《黑暗空间2023》中实现了一定的成果。<details>
<summary>Abstract</summary>
Going from research to production, especially for large and complex software systems, is fundamentally a hard problem. In large-scale game production, one of the main reasons is that the development environment can be very different from the final product. In this technical paper we describe an effort to add an experimental reinforcement learning system to an existing automated game testing solution based on scripted bots in order to increase its capacity. We report on how this reinforcement learning system was integrated with the aim to increase test coverage similar to [1] in a set of AAA games including Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to show a use-case of leveraging reinforcement learning in game production and cover some of the largest time sinks anyone who wants to make the same journey for their game may encounter. Furthermore, to help the game industry to adopt this technology faster, we propose a few research directions that we believe will be valuable and necessary for making machine learning, and especially reinforcement learning, an effective tool in game production.
</details>
<details>
<summary>摘要</summary>
从研究到生产，特别是 для大型和复杂的软件系统，是一个基本问题。在大规模游戏生产中，一个主要原因是开发环境和产品环境之间的差异。在这份技术著作中，我们描述了将实验式学习系统添加到现有的自动游戏测试解决方案基于脚本式 Bot 以增加其容量的尝试。我们报告了在一些 AAA 游戏，包括 Battlefield 2042 和 Dead Space (2023) 中将这个学习系统整合的成果，并希望这份技术著作可以显示游戏生产中如何使用学习机器人，以及一些可能会遇到的主要时间潜在障碍。此外，为了帮助游戏业界更快地采纳这技术，我们建议了一些研究方向，我们认为这些研究方向将是有价值和必要的，以便在游戏生产中使用机器学习和特别是实验学习。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net"><a href="#Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net" class="headerlink" title="Interpreting and Correcting Medical Image Classification with PIP-Net"></a>Interpreting and Correcting Medical Image Classification with PIP-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10404">http://arxiv.org/abs/2307.10404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-nauta/pipnet">https://github.com/m-nauta/pipnet</a></li>
<li>paper_authors: Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, Jörg Schlötterer, Maurice van Keulen, Christin Seifert</li>
<li>for: 这篇论文旨在探讨可解释的机器学习模型在医学影像分类 tasks 中的应用性和潜力。</li>
<li>methods: 论文使用的是PIP-Net模型，这是一种可解释的图像分类模型，它学习了人类理解的图像部件。</li>
<li>results: 研究发现，PIP-Net 的决策过程与医学分类标准相一致，只需要提供图像级别的类别标签。此外，研究还发现了如何通过直接禁用不想要的原型来人工修正PIP-Net的思维。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.
</details>
<details>
<summary>摘要</summary>
<<SYS>>本文探讨了可解释型机器学习模型在医学图像分类 task 中的可行性和潜力。 Specifically, we explore the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision-making process is in line with medical classification standards, while only provided with image-level class labels. Additionally, we show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Selection-functions-of-strong-lens-finding-neural-networks"><a href="#Selection-functions-of-strong-lens-finding-neural-networks" class="headerlink" title="Selection functions of strong lens finding neural networks"></a>Selection functions of strong lens finding neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10355">http://arxiv.org/abs/2307.10355</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Herle, C. M. O’Riordan, S. Vegetti</li>
<li>for: 这个论文主要目的是研究深 gravitational lens 系统中 neural network 的偏袋性。</li>
<li>methods: 这个论文使用了类似于常见 literatura 中使用的 Convolutional Neural Networks 和三个不同的训练集来研究 lens finding  neural network 的选择函数。</li>
<li>results: 研究发现，这些 neural network 偏好 larger Einstein radii 和更集中的 source-light distributions。增加检测重要性阈值可以改善选择函数的效果。<details>
<summary>Abstract</summary>
Convolution Neural Networks trained for the task of lens finding with similar architecture and training data as is commonly found in the literature are biased classifiers. An understanding of the selection function of lens finding neural networks will be key to fully realising the potential of the large samples of strong gravitational lens systems that will be found in upcoming wide-field surveys. We use three training datasets, representative of those used to train galaxy-galaxy and galaxy-quasar lens finding neural networks. The networks preferentially select systems with larger Einstein radii and larger sources with more concentrated source-light distributions. Increasing the detection significance threshold to 12$\sigma$ from 8$\sigma$ results in 50 per cent of the selected strong lens systems having Einstein radii $\theta_\mathrm{E}$ $\ge$ 1.04 arcsec from $\theta_\mathrm{E}$ $\ge$ 0.879 arcsec, source radii $R_S$ $\ge$ 0.194 arcsec from $R_S$ $\ge$ 0.178 arcsec and source S\'ersic indices $n_{\mathrm{Sc}^{\mathrm{S}$ $\ge$ 2.62 from $n_{\mathrm{Sc}^{\mathrm{S}$ $\ge$ 2.55. The model trained to find lensed quasars shows a stronger preference for higher lens ellipticities than those trained to find lensed galaxies. The selection function is independent of the slope of the power-law of the mass profiles, hence measurements of this quantity will be unaffected. The lens finder selection function reinforces that of the lensing cross-section, and thus we expect our findings to be a general result for all galaxy-galaxy and galaxy-quasar lens finding neural networks.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks 特别是用于这个任务的 Architecture 和训练数据，即通常在文献中找到的 Architecture 和训练数据，是偏向分类器。 理解这个镜像系统的选择函数是掌握这个大量强 gravitational lens系统的潜在力量的关键。 我们使用了三个训练数据集，代表了通常用于训练 galaxy-galaxy 和 galaxy-quasar 镜像系统的训练数据。 这些网络偏好 Systems with larger Einstein radii 和更集中的源光辉分布。 将检测关键值从 8σ 提高到 12σ 会导致50%选择的强镜系统有 Einstein radii θE 大于或等于 1.04弧度，source radii RS 大于或等于 0.194弧度，source Sérsic indices nScS 大于或等于 2.62。 对于找寻类别的模型，它具有更强的偏好 towards higher lens ellipticities than those trained to find lensed galaxies。 选择函数不受Source 的梯度影响，因此Measurements of this quantity will be unaffected。 镜像选择函数与镜像截面的选择函数相似，因此我们预期我们的发现将是一个通用的结果，适用于所有 galaxy-galaxy 和 galaxy-quasar 镜像系统。
</details></li>
</ul>
<hr>
<h2 id="LightPath-Lightweight-and-Scalable-Path-Representation-Learning"><a href="#LightPath-Lightweight-and-Scalable-Path-Representation-Learning" class="headerlink" title="LightPath: Lightweight and Scalable Path Representation Learning"></a>LightPath: Lightweight and Scalable Path Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10171">http://arxiv.org/abs/2307.10171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen</li>
<li>for: 本文提出了一种轻量级和可扩展的路径表示学习框架，用于智能交通和智能城市应用。</li>
<li>methods: 提议使用笛卡尔环境抽象和全局本地知识传播来减少资源消耗和提高可扩展性，同时保持准确性。</li>
<li>results: 经过广泛的实验 validate 了该框架的可扩展性和精度，并且在资源有限的环境中具有优势。<details>
<summary>Abstract</summary>
Movement paths are used widely in intelligent transportation and smart city applications. To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation. In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential. Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.   We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability. More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with respect to path length. Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders. We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse path encoders. Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
路径表示法广泛应用于智能交通和智能城市应用程序中。为了满足这些应用程序，路径表示学习目标是提供高效精度的路径表示，以便在不同的下游任务中进行高效的操作，如路径排名和旅行费用估算。在资源有限的环境和绿色计算限制下，现有的路径表示学习研究通常强调精度，并且只在必要的情况下进行次要的考虑。我们提出了一个轻量级和可扩展的路径表示学习框架，称为LightPath，以降低资源消耗和实现可扩展性，而不影响准确性。更 Specifically，我们首先提出了一个稀疏自动编码器，以确保框架在路径长度方面具有良好的扩展性。然后，我们提出了一个关系理解框架，以更快地训练更加稀疏的路径编码器。 finally，我们提出了全球-本地知识传播，以进一步减小路径编码器的大小和提高其性能。我们在两个真实世界数据集上进行了广泛的实验，以提供有关效率、可扩展性和效果的深入了解。
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Applications-of-Large-Language-Models"><a href="#Challenges-and-Applications-of-Large-Language-Models" class="headerlink" title="Challenges and Applications of Large Language Models"></a>Challenges and Applications of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10169">http://arxiv.org/abs/2307.10169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy</li>
<li>for: 本研究旨在为机器学习研究人员提供一个系统的开放问题和成功应用领域，以便更快地了解大语言模型（LLMs）领域的当前状态，并更快地成为产ктив的研究人员。</li>
<li>methods: 本研究使用了系统的Literature Review和问题定义方法，以掌握大语言模型领域的当前状态和未解决问题。</li>
<li>results: 本研究提出了一系列的开放问题和成功应用领域，以便 ML 研究人员更快地了解大语言模型领域的当前状态，并更快地成为产ктив的研究人员。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
</details>
<details>
<summary>摘要</summary>
庞大语言模型（LLM）从无存到普遍的机器学习议题中的几年内。由于这个领域的快速进步，因此难以识别还没有解决的挑战和已经有成果的应用领域。在这篇论文中，我们 hoping to establish a systematic set of open problems and application successes，以便ML研究人员更快地了解这个领域的现状，更快地成为生产力。Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="VITS-Variational-Inference-Thomson-Sampling-for-contextual-bandits"><a href="#VITS-Variational-Inference-Thomson-Sampling-for-contextual-bandits" class="headerlink" title="VITS : Variational Inference Thomson Sampling for contextual bandits"></a>VITS : Variational Inference Thomson Sampling for contextual bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10167">http://arxiv.org/abs/2307.10167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Clavier, Tom Huix, Alain Durmus</li>
<li>for: 这 paper 是关于 contextual bandits 的一种变体 Thompson sampling（TS）算法的研究。</li>
<li>methods: 该算法使用 Gaussian Variational Inference 提供高效的 posterior 近似，并且可以轻松地从近似中采样。</li>
<li>results: 该paper 表明 VITS 算法可以实现 sub-linear regret bound，并且在 synthetic 和实际世界数据上进行了实验验证。Here’s the breakdown of each point in English:* For: The paper is about a variant of the Thompson sampling algorithm for contextual bandits.* Methods: The algorithm uses Gaussian Variational Inference to provide efficient posterior approximations, and can easily sample from these approximations.* Results: The paper shows that the VITS algorithm can achieve a sub-linear regret bound, and demonstrates its effectiveness through experiments on both synthetic and real-world datasets.<details>
<summary>Abstract</summary>
In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we demonstrate experimentally the effectiveness of VITS on both synthetic and real world datasets.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍并分析了一种 Contextual Bandit 中的 Thompson 抽样（TS）算法的变体。在每个轮次中，传统的 TS 需要从当前的 posterior 分布中采样，通常是不可行的。为了缓解这个问题，我们可以使用 Approximate Inference 技术，提供靠近 posterior 的样本。然而，当前的 approximate 技术可能会导致低效的估计（Laplace 应用）或者是 computationally expensive（MCMC 方法、Ensemble 抽样...）。在本文中，我们提出了一个新的算法，基于 Gaussian Variational Inference 的 Varitional Inference Thompson Sampling（VITS）。这种方案提供了简单易于采样的强 posterior  aproximation，计算效率高，适用于 TS。此外，我们证明了 VITS 在维度和轮次数方面的下降 regret bound 与传统的 TS 相同。最后，我们通过 synthetic 和实际世界数据的实验证明了 VITS 的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Multimodal-Datasets-with-Image-Captioning"><a href="#Improving-Multimodal-Datasets-with-Image-Captioning" class="headerlink" title="Improving Multimodal Datasets with Image Captioning"></a>Improving Multimodal Datasets with Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10350">http://arxiv.org/abs/2307.10350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, Ludwig Schmidt</li>
<li>for: 提高大型视觉语言模型的成功，如CLIP和Flamingo。</li>
<li>methods: 研究如何使用生成的标题提高web数据的Utility，并比较不同混合策略的性能。</li>
<li>results: 与DataComp benchmark中提出的最佳策略相比，我们的方法在ImageNet和38个任务中提高了2%和4%的性能，并在Flickr和MS-COCO检索中表现了2倍的提升。我们还分析了生成标题的效果，并证明标准图像描述标准不是多Modal训练中标题的可靠指标。最后，我们在大规模的DataComp中进行了实验，探讨生成标题在大量训练数据量下的局限性，以及图像淘汰的重要性。<details>
<summary>Abstract</summary>
Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp's large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity.
</details>
<details>
<summary>摘要</summary>
大量网络数据对于大型视觉语言模型如CLIP和Flamingo的成功起到了关键作用。然而，原始网络数据具有噪音，现有的过滤方法通常会导致数据多样性减少。我们的工作将注意力点在caption质量上，研究如何使用生成的caption提高网络抓取到的文本点的使用价值。通过不同的混合策略来融合原始和生成的caption，我们在ImageNet和38个任务上比DataCompbenchmark中的最佳过滤方法提高2%和4%。我们的最佳方法还在Flickr和MS-COCO检索中表现出2倍的好干净性。我们还分析了生成caption的有效性源泉，并通过不同的图像描述模型的实验，发现标准图像描述benchmark（如NoCaps CIDEr）中模型的性能不是训练多模式时caption的用途的可靠指标。最后，我们在DataComp的大规模数据（1.28B image-text pair）上进行实验，提供了生成caption的局限性以及图像筛选的重要性，随着训练数据量的增加。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Backdoor-Attacks"><a href="#Rethinking-Backdoor-Attacks" class="headerlink" title="Rethinking Backdoor Attacks"></a>Rethinking Backdoor Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10163">http://arxiv.org/abs/2307.10163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lancopku/SOS">https://github.com/lancopku/SOS</a></li>
<li>paper_authors: Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, Aleksander Madry</li>
<li>for: 本文旨在探讨Backdoor攻击的问题，即敌对者在训练集中插入恶意构建的例子，以让模型易受欺诈。</li>
<li>methods: 本文提出了一种新的方法来抗击Backdoor攻击，即通过无结构信息对训练数据分布进行检测，并使用Robust统计技术来检测和移除恶意构建的例子。</li>
<li>results: 本文的结果表明，在缺乏结构信息的情况下，Backdoor攻击是不可识别的，而且与自然出现的特征相同。基于此观察，本文检视了现有的Backdoor攻击防御方法，并描述了它们的假设和依赖关系。最后，本文提出了一种新的假设，即Backdoor攻击对应于训练数据中最强的特征。基于这个假设，本文开发了一种新的检测算法，具有理论保证和实际效果。<details>
<summary>Abstract</summary>
In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.   In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to "detect" in a general sense. Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend. Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. Under this assumption (which we make formal) we develop a new primitive for detecting backdoor attacks. Our primitive naturally gives rise to a detection algorithm that comes with theoretical guarantees and is effective in practice.
</details>
<details>
<summary>摘要</summary>
在一个后门攻击中，敌对者会插入一些恶意构建的后门示例，以让模型易于操纵。防御这类攻击通常包括视这些插入的示例为训练集中的异常值，并使用robust统计技术来探测和除掉它们。在这项工作中，我们提出了一种不同的后门攻击问题的解决方案。具体来说，我们表明了在训练数据分布的结构信息不存在的情况下，后门攻击是无法分辨的，因此不能在通用的概念上探测。然后，我们根据这一观察，重新评估了现有的后门攻击防御方法，描述了它们所假设的（常常隐藏的）假设和依赖项。最后，我们探索了一种假设后门攻击对应于训练数据中最强的特征，并将这种假设进行了正式表述。我们的原则 Naturally gives rise to a detection algorithm that comes with theoretical guarantees and is effective in practice.
</details></li>
</ul>
<hr>
<h2 id="Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning"><a href="#Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning" class="headerlink" title="Robust Driving Policy Learning with Guided Meta Reinforcement Learning"></a>Robust Driving Policy Learning with Guided Meta Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10160">http://arxiv.org/abs/2307.10160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanghoon Lee, Jiachen Li, David Isele, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer</li>
<li>for: 增强自动驾驶车辆在互动交通场景中的自适应能力</li>
<li>methods: 使用随机互动奖励函数生成多种目标，并通过引导政策实现这些目标，以 trains 多种驾驶策略</li>
<li>results: 在一个具有挑战性的 T-路口场景中，成功启动了一个适应性强的驾驶策略，并且能够在未经见过的社交车辆行为下generalizationHere’s the same information in a more detailed format:</li>
<li>for: The paper aims to improve the adaptability of autonomous vehicles in interactive traffic scenarios by training a single meta-policy to handle diverse social vehicle behaviors.</li>
<li>methods: The proposed method uses randomized interaction-based reward functions to generate diverse objectives and train the meta-policy through guiding policies that achieve specific objectives. The ego vehicle’s driving policy is trained to be robust to unseen situations with out-of-distribution (OOD) social agents’ behaviors.</li>
<li>results: The proposed method is tested in a challenging uncontrolled T-intersection scenario, where the ego vehicle’s driving policy is able to generalize well to unseen situations with OOD social agents’ behaviors.I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unseen situations with out-of-distribution (OOD) social agents' behaviors in a challenging uncontrolled T-intersection scenario.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)尽管深度强化学习（DRL）在交互式交通场景中展现出了扎实的结果，现有的工作通常采用固定行为策略来控制社交车辆在训练环境中。这可能会使学习的驾驶策略过拟合环境，使其与不同、未见的行为的社交车辆困难交流。在这种工作中，我们提出了一种高效的方法，用于训练社交车辆的多样化驾驶策略。通过随机 modify social vehicle的互动基于奖励函数，我们可以生成多样的目标，并高效地通过指导策略来训练单一的元策略。我们还提出了一种增强ego车驾驶策略的训练策略，使用已学习的元策略控制社交车辆的环境。我们的方法成功地学习了一个 Egode驾驶策略，该策略在未见的社交车辆行为情况下可以普适地应用。
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Clustering-on-Graphs"><a href="#Curvature-based-Clustering-on-Graphs" class="headerlink" title="Curvature-based Clustering on Graphs"></a>Curvature-based Clustering on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10155">http://arxiv.org/abs/2307.10155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agosztolai/geometric_clustering">https://github.com/agosztolai/geometric_clustering</a></li>
<li>paper_authors: Yu Tian, Zachary Lubberts, Melanie Weber</li>
<li>for: 本文研究了一种基于图形学的无监督节点划分（或社区检测）算法，用于找到图中紧密连接的子结构，即社区或群体。</li>
<li>methods: 本文使用了离散 Ricci  curvature 和其相关的 геометрический流动，以揭示图中的社区结构。 并考虑了多种离散 curvature 观念，并对其进行分析。</li>
<li>results: 本文提供了both theoretical 和 empirical 证明，证明了我们的 curvature-based 划分算法的实用性。 此外，还提供了一些关于图形 curvature 和其对副图 curvature 的关系的结果，可能对 curvature-based 网络分析有独立的价值。<details>
<summary>Abstract</summary>
Unsupervised node clustering (or community detection) is a classical graph learning task. In this paper, we study algorithms, which exploit the geometry of the graph to identify densely connected substructures, which form clusters or communities. Our method implements discrete Ricci curvatures and their associated geometric flows, under which the edge weights of the graph evolve to reveal its community structure. We consider several discrete curvature notions and analyze the utility of the resulting algorithms. In contrast to prior literature, we study not only single-membership community detection, where each node belongs to exactly one community, but also mixed-membership community detection, where communities may overlap. For the latter, we argue that it is beneficial to perform community detection on the line graph, i.e., the graph's dual. We provide both theoretical and empirical evidence for the utility of our curvature-based clustering algorithms. In addition, we give several results on the relationship between the curvature of a graph and that of its dual, which enable the efficient implementation of our proposed mixed-membership community detection approach and which may be of independent interest for curvature-based network analysis.
</details>
<details>
<summary>摘要</summary>
不监督节点划分（或社区探测）是一个经典的图学任务。在这篇论文中，我们研究了利用图形的几何特性来标识紧密连接的子结构，它们组成社区或社区。我们的方法利用离散 Ricci 曲率和其相关的几何流动，以便在图形中揭示社区结构。我们考虑了多种离散曲率概念，并分析了它们的使用价值。与先前文献不同，我们不仅研究单会员社区探测，每个节点都属于唯一一个社区，还研究了混合会员社区探测，社区可能 overlap。为实现后一种情况，我们提出在对граф的 dual 进行社区探测，即线图。我们提供了理论和实验证明，以及对权重图的 curvature 的分析，这些结果可能为 curvature-based 网络分析提供帮助。
</details></li>
</ul>
<hr>
<h2 id="Code-Detection-for-Hardware-Acceleration-Using-Large-Language-Models"><a href="#Code-Detection-for-Hardware-Acceleration-Using-Large-Language-Models" class="headerlink" title="Code Detection for Hardware Acceleration Using Large Language Models"></a>Code Detection for Hardware Acceleration Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10348">http://arxiv.org/abs/2307.10348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Pablo Antonio Martínez, Gregorio Bernabé, José Manuel García</li>
<li>for: 本研究探讨了使用大型自然语言模型（LLM）进行代码检测。</li>
<li>methods: 我们提出了一种初步的提示策略和一种新的提示策略来实现代码检测。</li>
<li>results: 结果表明，我们的新提示策略可以减少假阳性，实现了优秀的总准确率（91.1%, 97.9%, 和 99.7%）。这些结果对现有的代码检测方法提出了明显的挑战。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been massively applied to many tasks, often surpassing state-of-the-art approaches. While their effectiveness in code generation has been extensively studied (e.g., AlphaCode), their potential for code detection remains unexplored.   This work presents the first analysis of code detection using LLMs. Our study examines essential kernels, including matrix multiplication, convolution, and fast-fourier transform, implemented in C/C++. We propose both a preliminary, naive prompt and a novel prompting strategy for code detection.   Results reveal that conventional prompting achieves great precision but poor accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively) due to a high number of false positives. Our novel prompting strategy substantially reduces false positives, resulting in excellent overall accuracy (91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable challenge to existing state-of-the-art code detection methods.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经广泛应用于多种任务，经常超越现有的方法。而它们在代码生成中的应用则未得到充分探讨。 本研究是代码检测使用 LLM 的首次分析。我们的研究探讨了重要的核心操作，包括矩阵乘法、卷积和快速傅立叶变换，它们在 C/C++ 中实现。我们提出了一个初步、简单的提示和一个新的提示策略来进行代码检测。结果显示，传统的提示方法可以很好地精度（68.8%、22.3%和79.2%），但是受到许多假阳性的影响，因此精度低。我们的新提示策略可以干扰假阳性，实现了优秀的总精度（91.1%、97.9%和99.7%）。这些结果对现有的代码检测方法提出了严重的挑战。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion"><a href="#Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion" class="headerlink" title="Benchmarking Potential Based Rewards for Learning Humanoid Locomotion"></a>Benchmarking Potential Based Rewards for Learning Humanoid Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10142">http://arxiv.org/abs/2307.10142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/se-hwan/pbrs-humanoid">https://github.com/se-hwan/pbrs-humanoid</a></li>
<li>paper_authors: Se Hwan Jeon, Steve Heim, Charles Khazoom, Sangbae Kim</li>
<li>for: 本研究旨在 benchmarking 标准的 reward shaping 方法和 potential based reward shaping (PBRS) 方法，以加速 reinforcement learning (RL) 的学习速度。</li>
<li>methods: 本研究使用了 humanoid robot 进行实验，并对两种 reward shaping 方法进行比较。</li>
<li>results: 研究发现，在高维系统中，PBRS 的优化效果只有较小的改善，但 PBRS 的评价标准更容易调整。<details>
<summary>Abstract</summary>
The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly more robust to scaling than typical reward shaping approaches, and thus easier to tune.
</details>
<details>
<summary>摘要</summary>
主要挑战在开发有效的增强学习（RL）管道是设计和调整奖金函数。Well-designed 形式的奖金可以导致学习速度明显提高。然而，Naively 定义的奖金可能会与愿望行为冲突，导致过拟合或者even erratic performance  if not properly tuned。理论上，广泛的 potential based reward shaping（PBRS）可以帮助导引学习过程，无需affecting the optimal policy。虽然一些研究已经探讨了使用 potential based reward shaping 加速学习的整合，但大多数研究仅限于格子世界和低维系统，RL 在机器人领域主要依靠标准的奖金形式。在这篇论文中，我们对标准的奖金形式和 PBRS 进行了对比，发现在这个高维系统中，PBRS 只有微妙的加速学习速度。然而，PBRS 奖金项是标准奖金形式相比较更加Robust 尺度，因此更容易调整。
</details></li>
</ul>
<hr>
<h2 id="ProtiGeno-a-prokaryotic-short-gene-finder-using-protein-language-models"><a href="#ProtiGeno-a-prokaryotic-short-gene-finder-using-protein-language-models" class="headerlink" title="ProtiGeno: a prokaryotic short gene finder using protein language models"></a>ProtiGeno: a prokaryotic short gene finder using protein language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10343">http://arxiv.org/abs/2307.10343</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonytu16/protigeno">https://github.com/tonytu16/protigeno</a></li>
<li>paper_authors: Tony Tu, Gautham Krishna, Amirali Aghazadeh</li>
<li>for: 本研究旨在提高质子生物基因预测的准确性和 recall，尤其是在短读取 Frame (ORFs) 中。</li>
<li>methods: 我们开发了一种基于深度学习的方法，称为 ProtiGeno，使用一个训练在数百万个演化后的蛋白质模型来预测质子生物短基因。</li>
<li>results: 在系统性的大规模实验中，我们示出了 ProtiGeno 可以更高度准确地预测短质子生物基因，比现有的状态艺术基因预测器更高。我们还讲述了 ProtiGeno 预测短基因的预测特征和可能的限制。<details>
<summary>Abstract</summary>
Prokaryotic gene prediction plays an important role in understanding the biology of organisms and their function with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (<180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and models are available at https://github.com/tonytu16/protigeno.
</details>
<details>
<summary>摘要</summary>
probiotic gene prediction plays an important role in understanding the biology of organisms and their function, with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (<180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and models are available at https://github.com/tonytu16/protigeno.Here's the word-for-word translation of the text into Simplified Chinese: probiotic gene prediction plays an important role in understanding the biology of organisms and their function, with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (<180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and models are available at https://github.com/tonytu16/protigeno.Please note that the translation is done using Google Translate, and the result may not be perfectly accurate or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers"><a href="#Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers" class="headerlink" title="Gradient Sparsification For Masked Fine-Tuning of Transformers"></a>Gradient Sparsification For Masked Fine-Tuning of Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10098">http://arxiv.org/abs/2307.10098</a></li>
<li>repo_url: None</li>
<li>paper_authors: James O’ Neill, Sourav Dutta</li>
<li>for: 这 paper 是 investigate 如何提高 transfer learning 中的 fine-tuning 性能，并提出了一种 gradient sparsification 方法 GradDrop。</li>
<li>methods: 这 paper 使用了 two 种方法来 evaluate GradDrop，一种是使用 multilingual XGLUE  bencmark，另一种是使用 XLMR-Large 模型。</li>
<li>results: 实验结果表明，GradDrop 可以与使用额外的翻译数据进行 intermediate pretraining 相比，并且超过标准的 fine-tuning 和慢滑层解决方法。另外，一种 post-analysis 还表明，GradDrop 可以提高 under-resourced 语言的性能。<details>
<summary>Abstract</summary>
Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the backward pass, acting as gradient noise. GradDrop is sparse and stochastic unlike gradual freezing. Extensive experiments on the multilingual XGLUE benchmark with XLMR-Large show that GradDrop is competitive against methods that use additional translated data for intermediate pretraining and outperforms standard fine-tuning and gradual unfreezing. A post-analysis shows how GradDrop improves performance with languages it was not trained on, such as under-resourced languages.
</details>
<details>
<summary>摘要</summary>
广泛采用已经预训练自主语言模型的微调是为了进行转移学习到下游任务。微调可以通过冻结预训练网络的梯度来实现，或者是通过在新增的分类层上进行梯度更新。渐进冻结可以在训练过程中逐渐解冻整个层的梯度，从而实现存储和训练速度之间的平衡。然而，不知道渐进冻结layers在训练过程中是最佳的，相比之下， sparse variant of gradual unfreezing可能会提高微调性能。在这篇论文中，我们提出了随机层梯度掩码来规范预训练语言模型，以提高总体微调性能。我们引入GradDrop和其变种，它是一类梯度减少方法，在反向传播中随机掩码梯度。GradDrop是不同于渐进冻结的，它是随机和粗略的。我们在多语言XGLUE标准测试 benchmark上进行了广泛的实验，结果显示GradDrop和其变种与使用额外翻译数据进行中间预训练的方法相当竞争，并且超过了标准微调和渐进冻结。一种后期分析表明，GradDrop在未经训练的语言上提高性能，如受到了资源的语言。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-invariances-and-introducing-priors-in-Gromov-Wasserstein-distances"><a href="#Revisiting-invariances-and-introducing-priors-in-Gromov-Wasserstein-distances" class="headerlink" title="Revisiting invariances and introducing priors in Gromov-Wasserstein distances"></a>Revisiting invariances and introducing priors in Gromov-Wasserstein distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10093">http://arxiv.org/abs/2307.10093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pinar Demetci, Quang Huy Tran, Ievgen Redko, Ritambhara Singh</li>
<li>for: 本研究旨在提出一种新的优先Transport-基于距离，以提高对metric spaces中样本之间的比较，并且能够在certain应用中控制对映射变换的灵活性。</li>
<li>methods: 本研究使用augmented Gromov-Wasserstein distance，该距离考虑了样本之间的pairwise similarity，同时还能够 incorporate feature alignments，以更好地利用输入数据中的先验知识。</li>
<li>results: 本研究通过 theoretically analyzing the proposed metric，并在单细胞多Omic alignment和机器学习中进行了实验 validate the effectiveness of the proposed method。<details>
<summary>Abstract</summary>
Gromov-Wasserstein distance has found many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations. However, in certain applications, this invariance property can be too flexible, thus undesirable. Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations. We propose a new optimal transport-based distance, called Augmented Gromov-Wasserstein, that allows for some control over the level of rigidity to transformations. It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance. We present theoretical insights into the proposed metric. We then demonstrate its usefulness for single-cell multi-omic alignment tasks and a transfer learning scenario in machine learning.
</details>
<details>
<summary>摘要</summary>
《Gromov-Wasserstein距离》在机器学习中发现了广泛的应用，主要是因为它可以比较度量空间中的度量，并且对于同态变换是不变的。然而，在某些应用场景中，这种不变性属性可能是不需要的，甚至是不жела的。此外，Gromov-Wasserstein距离仅考虑输入数据集中的对应关系，不考虑原始特征表示。我们提议一种新的优化的Gromov-Wasserstein距离，called Augmented Gromov-Wasserstein，允许控制变换的级别。它还包含特征对齐，使得我们可以更好地利用输入数据中的先验知识，提高性能。我们提供了关于提议度量的理论听见。然后，我们在单细ће多元素Alignment任务和机器学习中的传递学习场景中展示了其用于。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/20/cs.LG_2023_07_20/" data-id="cloh7tqip00lk7b88g0zr7x45" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/eess.IV_2023_07_20/" class="article-date">
  <time datetime="2023-07-20T09:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/20/eess.IV_2023_07_20/">eess.IV - 2023-07-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Parse-and-Recall-Towards-Accurate-Lung-Nodule-Malignancy-Prediction-like-Radiologists"><a href="#Parse-and-Recall-Towards-Accurate-Lung-Nodule-Malignancy-Prediction-like-Radiologists" class="headerlink" title="Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists"></a>Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10824">http://arxiv.org/abs/2307.10824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianpeng Zhang, Xianghua Ye, Jianfeng Zhang, Yuxing Tang, Minfeng Xu, Jianfei Guo, Xin Chen, Zaiyi Liu, Jingren Zhou, Le Lu, Ling Zhang</li>
<li>for: 这个研究旨在提高肺癌早期检测的精度，以提高生存 outcome。</li>
<li>methods: 这个方法使用了 radiologist-inspired 的方法，包括 context parsing 和 prototype recalling 模组。context parsing 模组首先将 nodule 的 context 结构分解，然后将 contextual information 聚合以更好地理解 nodule。prototype recalling 模组使用 prototype-based learning 来将先前学习的案例转换为 prototype，并在训练中进行线上协同更新。</li>
<li>results: 这个方法在低剂量和非contrast CT 检测中实现了高水平的检测性能。<details>
<summary>Abstract</summary>
Lung cancer is a leading cause of death worldwide and early screening is critical for improving survival outcomes. In clinical practice, the contextual structure of nodules and the accumulated experience of radiologists are the two core elements related to the accuracy of identification of benign and malignant nodules. Contextual information provides comprehensive information about nodules such as location, shape, and peripheral vessels, and experienced radiologists can search for clues from previous cases as a reference to enrich the basis of decision-making. In this paper, we propose a radiologist-inspired method to simulate the diagnostic process of radiologists, which is composed of context parsing and prototype recalling modules. The context parsing module first segments the context structure of nodules and then aggregates contextual information for a more comprehensive understanding of the nodule. The prototype recalling module utilizes prototype-based learning to condense previously learned cases as prototypes for comparative analysis, which is updated online in a momentum way during training. Building on the two modules, our method leverages both the intrinsic characteristics of the nodules and the external knowledge accumulated from other nodules to achieve a sound diagnosis. To meet the needs of both low-dose and noncontrast screening, we collect a large-scale dataset of 12,852 and 4,029 nodules from low-dose and noncontrast CTs respectively, each with pathology- or follow-up-confirmed labels. Experiments on several datasets demonstrate that our method achieves advanced screening performance on both low-dose and noncontrast scenarios.
</details>
<details>
<summary>摘要</summary>
肺癌是全球最主要的死亡原因之一， early screening 对于提高存活率的影响是关键。在临床实践中， nodule 的 contextual 结构和 radiologist 的经验是识别 benign 和 malignant nodule 的两个核心元素。 contextual 信息可以提供 nodule 的全面信息，如位置、形状和周围血管，经验丰富的 radiologist 可以通过对前期案例进行参考，增强基于决策的基础。在这篇论文中，我们提出了一种基于 radiologist 的方法，用于模拟诊断过程。该方法包括 context parsing 和 prototype recalling 两个模块。context parsing 模块首先将 nodule 的 contextual 结构分解，然后将 contextual 信息聚合，以更全面地理解 nodule。prototype recalling 模块使用 prototype-based learning 来压缩已经学习的案例，并在线更新，以便在训练过程中保持最新。基于这两个模块，我们的方法可以利用 nodule 的内在特征和已经accumulated的外部知识来实现准确的诊断。为了满足低剂量和非对抗 CT 的需求，我们收集了12,852和4,029 nodule 的数据集，其中每个数据集具有 pathology-或 follow-up-确认的标签。实验表明，我们的方法在低剂量和非对抗场景中都达到了高水平的检测性能。
</details></li>
</ul>
<hr>
<h2 id="A-novel-integrated-method-of-detection-grasping-for-specific-object-based-on-the-box-coordinate-matching"><a href="#A-novel-integrated-method-of-detection-grasping-for-specific-object-based-on-the-box-coordinate-matching" class="headerlink" title="A novel integrated method of detection-grasping for specific object based on the box coordinate matching"></a>A novel integrated method of detection-grasping for specific object based on the box coordinate matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11783">http://arxiv.org/abs/2307.11783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongmin Liu, Jirui Wang, Jie Li, Zufeng Li, Kai Ren, Peng Shi</li>
<li>for: 本研究旨在提高服务机器人对特定物体的检测和抓取能力，以更好地照顾老年和残疾人群。</li>
<li>methods: 该研究提出了一种基于盒坐标匹配的检测-抓取集成方法（DG-BCM），包括对SOLOv2实例分割模型进行通道注意力模块（CAM）和空间注意力模块（SAM）的改进，以及将atrous spatial pyramid pooling（ASPP）和CAM添加到生成征式径向减少神经网络（GR-CNN）模型中来优化抓取估计。</li>
<li>results: 实验表明，提高后的模型在对物体检测和抓取任务中均表现出色，而且在对几种具体物体的抓取任务中也得到了可行和有效的结果。<details>
<summary>Abstract</summary>
To better care for the elderly and disabled, it is essential for service robots to have an effective fusion method of object detection and grasp estimation. However, limited research has been observed on the combination of object detection and grasp estimation. To overcome this technical difficulty, a novel integrated method of detection-grasping for specific object based on the box coordinate matching is proposed in this paper. Firstly, the SOLOv2 instance segmentation model is improved by adding channel attention module (CAM) and spatial attention module (SAM). Then, the atrous spatial pyramid pooling (ASPP) and CAM are added to the generative residual convolutional neural network (GR-CNN) model to optimize grasp estimation. Furthermore, a detection-grasping integrated algorithm based on box coordinate matching (DG-BCM) is proposed to obtain the fusion model of object detection and grasp estimation. For verification, experiments on object detection and grasp estimation are conducted separately to verify the superiority of improved models. Additionally, grasping tasks for several specific objects are implemented on a simulation platform, demonstrating the feasibility and effectiveness of DG-BCM algorithm proposed in this paper.
</details>
<details>
<summary>摘要</summary>
Firstly, the SOLOv2 instance segmentation model is enhanced by adding channel attention module (CAM) and spatial attention module (SAM). Then, the atrous spatial pyramid pooling (ASPP) and CAM are added to the generative residual convolutional neural network (GR-CNN) model to optimize grasp estimation.Furthermore, a detection-grasping integrated algorithm based on box coordinate matching (DG-BCM) is proposed to obtain the fusion model of object detection and grasp estimation. For verification, experiments on object detection and grasp estimation are conducted separately to demonstrate the superiority of the improved models. Additionally, grasping tasks for several specific objects are implemented on a simulation platform, showcasing the feasibility and effectiveness of the DG-BCM algorithm proposed in this paper.
</details></li>
</ul>
<hr>
<h2 id="Aggressive-saliency-aware-point-cloud-compression"><a href="#Aggressive-saliency-aware-point-cloud-compression" class="headerlink" title="Aggressive saliency-aware point cloud compression"></a>Aggressive saliency-aware point cloud compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10741">http://arxiv.org/abs/2307.10741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleftheria Psatha, Dimitrios Laskos, Gerasimos Arvanitis, Konstantinos Moustakas</li>
<li>for:  This paper aims to present a novel, geometry-based, end-to-end compression scheme for point clouds, which is essential for various applications such as virtual reality and 3D modeling.</li>
<li>methods:  The proposed method combines information on the geometrical features of the point cloud and the user’s position to achieve remarkable results for aggressive compression schemes demanding very small bit rates. It includes separating visible and non-visible points, calculating four saliency maps, and using delta coordinates and solving a sparse linear system for decoding.</li>
<li>results:  The proposed method achieves significantly better results than the geometry-based point cloud compression (G-PCC) algorithm by the Moving Picture Experts Group (MPEG) for small bit rates, as demonstrated by evaluation studies and comparisons with various point clouds.<details>
<summary>Abstract</summary>
The increasing demand for accurate representations of 3D scenes, combined with immersive technologies has led point clouds to extensive popularity. However, quality point clouds require a large amount of data and therefore the need for compression methods is imperative. In this paper, we present a novel, geometry-based, end-to-end compression scheme, that combines information on the geometrical features of the point cloud and the user's position, achieving remarkable results for aggressive compression schemes demanding very small bit rates. After separating visible and non-visible points, four saliency maps are calculated, utilizing the point cloud's geometry and distance from the user, the visibility information, and the user's focus point. A combination of these maps results in a final saliency map, indicating the overall significance of each point and therefore quantizing different regions with a different number of bits during the encoding process. The decoder reconstructs the point cloud making use of delta coordinates and solving a sparse linear system. Evaluation studies and comparisons with the geometry-based point cloud compression (G-PCC) algorithm by the Moving Picture Experts Group (MPEG), carried out for a variety of point clouds, demonstrate that the proposed method achieves significantly better results for small bit rates.
</details>
<details>
<summary>摘要</summary>
随着三维场景的准确表示需求的增加，并与 immerse 技术相结合，点云的受欢迎程度有所提高。然而，高质量点云需要大量数据，因此压缩方法的需求是非常重要。在这篇论文中，我们提出了一种新的、基于几何特征的、端到端压缩方案，其中包括点云的几何特征和用户的位置信息，实现了非常出色的压缩效果，特别是在非常小的比特率下。首先，我们将可见和非可见点分开，然后计算出四个重要性地图，使用点云的几何特征和用户的距离、可见信息和用户的关注点。这些地图的组合得到了最终的重要性地图，用于在编码过程中对每个点 quantize 不同数量的比特。解码器使用 delta 坐标和解决一个稀疏线性系统来重建点云。经过评估和与基于几何特征的点云压缩算法（G-PCC）由移动画像专家组（MPEG）提出的比较研究，我们的方法在不同的点云上实现了明显更好的效果。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-sunflower-leaf-area-at-vegetative-stage-by-image-analysis-and-application-to-the-estimation-of-water-stress-response-parameters-in-post-registration-varieties"><a href="#Prediction-of-sunflower-leaf-area-at-vegetative-stage-by-image-analysis-and-application-to-the-estimation-of-water-stress-response-parameters-in-post-registration-varieties" class="headerlink" title="Prediction of sunflower leaf area at vegetative stage by image analysis and application to the estimation of water stress response parameters in post-registration varieties"></a>Prediction of sunflower leaf area at vegetative stage by image analysis and application to the estimation of water stress response parameters in post-registration varieties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11110">http://arxiv.org/abs/2307.11110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Casadebaig, Nicolas Blanchet, Nicolas Bernard Langlade</li>
<li>for: 这研究的目的是为了提供一种自动测量阳光花的发育和生理响应，以便更好地了解可用于农业实践中的品种，以及 indentifying植物对环境的生物、遗传和分子基础。</li>
<li>methods: 在INRAE Toulouse的Heliaphen高通量 fenotyping 平台上，我们设计了两个实验，每个实验有8种（2*96植物），并在每天基础上使用光栅摄取植物图像。同时，我们手动测量了这些植物的叶面每两天，持续约10天。图像分析以EXTRACT植物的形态特征，并评估不同的模型来估计植物总叶面积。</li>
<li>results: 使用线性模型并应用 posteriori 缓和后果，我们可以估计植物总叶面积的Relative squared error为11%，效率为93%。使用这些数据计算的叶面积响应和蒸散响应（LER和TR），并与手动测量结果进行比较。结果显示，自动测量的LE和TR参数估计值可以用于模拟。相比之下，手动测量结果显示自动测量的LE值较低，但与绿house grown植物的手动测量结果更相似，这可能表明自动测量方法偏向压力敏感。<details>
<summary>Abstract</summary>
The automatic measurement of developmental and physiological responses of sunflowers to water stress represents an applied challenge for a better knowledge of the varieties available to growers, but also a fundamental one for identifying the biological, genetic and molecular bases of plant response to their environment.On INRAE Toulouse's Heliaphen high-throughput phenotyping platform, we set up two experiments, each with 8 varieties (2*96 plants), and acquired images of plants subjected or not to water stress, using a light barrier on a daily basis. At the same time, we manually measured the leaf surfaces of these plants every other day for the duration of the stress, which lasted around ten days. The images were analyzed to extract morphological characteristics of the segmented plants and different models were evaluated to estimate total plant leaf areas using these data.A linear model with a posteriori smoothing was used to estimate total leaf area with a relative squared error of 11% and an efficiency of 93%. Leaf areas estimated conventionally or with the developed model were used to calculate the leaf expansion and transpiration responses (LER and TR) used in the SUNFLO crop model for 8 sunflower varieties studied. Correlation coefficients of 0.61 and 0.81 for LER and TR respectively validate the use of image-based leaf area estimation. However, the estimated values for LER are lower than for the manual method on Heliaphen, but closer overall to the manual method on greenhouse-grown plants, potentially suggesting an overestimation of stress sensitivity.It can be concluded that the LE and TR parameter estimates can be used for simulations. The low cost of this method (compared with manual measurements), the possibility of parallelizing and repeating measurements on the Heliaphen platform, and of benefiting from the Heliaphen platform's data management, are major improvements for valorizing the SUNFLO model and characterizing the drought sensitivity of cultivated varieties.
</details>
<details>
<summary>摘要</summary>
自动测量阳光花的发育和生理响应对水压力是一个应用挑战，以便更好地了解可用种植者，也是一个基础的挑战，以确定植物对环境的响应的生物、遗传和分子基础。在INRAE Toulouse的Heliaphen高通量现场测量平台上，我们设计了两个实验，每个实验有8种（2*96植物），并在每天基础上使用光梯图像这些植物，并手动测量这些植物的叶面每两天，测试期间约10天。图像分析后，EXTract了植物的形态特征，并评估了不同的模型以估算植物的总叶面面积。使用线性模型和 posteriori 缓和后处理时，可以Estimate植物的总叶面面积，相对平方差为11%，效率为93%。使用这些数据计算植物的叶面Expand和蒸发响应（LER和TR），并使用这些响应在SUNFLO植物模型中。 corrCoef = 0.61和0.81，这些值 validate the use of image-based leaf area estimation。然而，Estimated LER values are lower than those obtained using the manual method on Heliaphen, but closer to the manual method on greenhouse-grown plants, suggesting an overestimation of stress sensitivity。可以 concluThat the LE and TR parameter estimates can be used for simulations. The low cost of this method (compared with manual measurements), the possibility of parallelizing and repeating measurements on the Heliaphen platform, and the possibility of benefiting from the Heliaphen platform's data management, are major improvements for valorizing the SUNFLO model and characterizing the drought sensitivity of cultivated varieties。
</details></li>
</ul>
<hr>
<h2 id="Depth-from-Defocus-Technique-A-Simple-Calibration-Free-Approach-for-Dispersion-Size-Measurement"><a href="#Depth-from-Defocus-Technique-A-Simple-Calibration-Free-Approach-for-Dispersion-Size-Measurement" class="headerlink" title="Depth from Defocus Technique: A Simple Calibration-Free Approach for Dispersion Size Measurement"></a>Depth from Defocus Technique: A Simple Calibration-Free Approach for Dispersion Size Measurement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10678">http://arxiv.org/abs/2307.10678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saini Jatin Rao, Shubham Sharma, Saptarshi Basu, Cameron Tropea</li>
<li>for: 这项研究旨在提出一种基于图像技术的粒子大小和位置测量方法，用于在多相流体中跟踪粒子的运动和大小。</li>
<li>methods: 该方法基于‘深度从杂化’（DFD）技术，使用单个摄像头设置，并采用简单的光学配置和简单的准备过程，以便在更广泛的应用中使用。</li>
<li>results: 研究表明，该方法可以高精度地测量粒子的大小和位置，并且可以在多相流体中跟踪粒子的运动。<details>
<summary>Abstract</summary>
Dispersed particle size measurement is crucial in a variety of applications, be it in the sizing of spray droplets, tracking of particulate matter in multiphase flows, or the detection of target markers in machine vision systems. Further to sizing, such systems are characterised by extracting quantitative information like spatial position and associated velocity of the dispersed phase particles. In the present study we propose an imaging based volumetric measurement approach for estimating the size and position of spherically dispersed particles. The approach builds on the 'Depth from Defocus' (DFD) technique using a single camera approach. The simple optical configuration, consisting of a shadowgraph setup and a straightforward calibration procedure, makes this method readily deployable and accessible for broader applications.
</details>
<details>
<summary>摘要</summary>
“粒子大小分布测量在各种应用中都非常重要，无论是在喷涂液滴大小的测量，或者追踪多相流体中的固体粒子，或者是机器视觉系统中的目标标记检测。在当前研究中，我们提出了基于图像的体积测量方法，用于估算归一化粒子的大小和位置。这种方法基于‘深度从对焦’（DFD）技术，使用单一相机设置。该简单的光学配置和简单的准备过程，使得这种方法可以广泛应用和易于实施。”Note that Simplified Chinese is the standard writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Unified-Demosaicing-for-Bayer-and-Non-Bayer-Patterned-Image-Sensors"><a href="#Efficient-Unified-Demosaicing-for-Bayer-and-Non-Bayer-Patterned-Image-Sensors" class="headerlink" title="Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors"></a>Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10667">http://arxiv.org/abs/2307.10667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haechang Lee, Dongwon Park, Wongi Jeong, Kijeong Kim, Hyunwoo Je, Dongil Ryu, Se Young Chun</li>
<li>for: 这个论文旨在解决现代CMOS图像感知器（CIS）的物理尺寸越来越小，现在手机摄像头采用不同的非 Bayer颜色畸数组（CFA）模式（例如Quad、Nona、QxQ），这些非 Bayer感知器比传统Bayer CFA更加高效，但可能会在排除中产生视觉artefacts。</li>
<li>methods: 该论文提出了一种高效的统一排除方法，可以应用于不同的CFAs和不同的照明条件下的RAW数据。该方法使用了知识学习模型，并且只需要1%的钥匙缓存来实现高效的排除。</li>
<li>results: 该论文的实验结果表明，该方法可以在Synthetic和实际RAW数据中实现了state-of-the-art的排除性能，并且可以在不同的CFAs和照明条件下实现了比较高的稳定性和精度。<details>
<summary>Abstract</summary>
As the physical size of recent CMOS image sensors (CIS) gets smaller, the latest mobile cameras are adopting unique non-Bayer color filter array (CFA) patterns (e.g., Quad, Nona, QxQ), which consist of homogeneous color units with adjacent pixels. These non-Bayer sensors are superior to conventional Bayer CFA thanks to their changeable pixel-bin sizes for different light conditions but may introduce visual artifacts during demosaicing due to their inherent pixel pattern structures and sensor hardware characteristics. Previous demosaicing methods have primarily focused on Bayer CFA, necessitating distinct reconstruction methods for non-Bayer patterned CIS with various CFA modes under different lighting conditions. In this work, we propose an efficient unified demosaicing method that can be applied to both conventional Bayer RAW and various non-Bayer CFAs' RAW data in different operation modes. Our Knowledge Learning-based demosaicing model for Adaptive Patterns, namely KLAP, utilizes CFA-adaptive filters for only 1% key filters in the network for each CFA, but still manages to effectively demosaic all the CFAs, yielding comparable performance to the large-scale models. Furthermore, by employing meta-learning during inference (KLAP-M), our model is able to eliminate unknown sensor-generic artifacts in real RAW data, effectively bridging the gap between synthetic images and real sensor RAW. Our KLAP and KLAP-M methods achieved state-of-the-art demosaicing performance in both synthetic and real RAW data of Bayer and non-Bayer CFAs.
</details>
<details>
<summary>摘要</summary>
“随着CMOS图像感知器（CIS）的物理大小变小，latest的手机摄像头正在采用不同的非 Bayer颜色网络阵列（CFA）模式（例如Quad、Nona、QxQ），这些非 Bayer 感知器比传统Bayer CFA 高效，因为它们可以根据不同的光照情况调整像素单元大小，但可能会在构成过程中带来视觉错误。先前的构成方法主要集中在Bayer CFA，需要特别的重建方法 для非 Bayer 模式的 CIS，不同的照明情况下。在这个工作中，我们提出了一个高效的统一构成方法，可以应用于传统Bayer RAW 和不同的非 Bayer CFAs 的 RAW 数据，不同的操作模式下。我们的知识学习基本的构成模型，即KLAP，只需要1%的键 filters 在网络中，但还是能够有效地构成所有的 CFAs，与大规模模型相比，具有相似的性能。此外，通过在推断中使用meta-learning（KLAP-M），我们的模型可以实际地消除实际感知器的普通遗传特征错误，实现实际数据和 sintetic 数据之间的距离。我们的 KLAP 和 KLAP-M 方法在实验和实际 RAW 数据中均 achiev 了顶尖的构成性能。”
</details></li>
</ul>
<hr>
<h2 id="Physics-Driven-Turbulence-Image-Restoration-with-Stochastic-Refinement"><a href="#Physics-Driven-Turbulence-Image-Restoration-with-Stochastic-Refinement" class="headerlink" title="Physics-Driven Turbulence Image Restoration with Stochastic Refinement"></a>Physics-Driven Turbulence Image Restoration with Stochastic Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10603">http://arxiv.org/abs/2307.10603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vita-group/pirn">https://github.com/vita-group/pirn</a></li>
<li>paper_authors: Ajay Jaiswal, Xingguang Zhang, Stanley H. Chan, Zhangyang Wang<br>for: PiRN 是一种用于恢复这个问题的红外线成像系统中的湮渠问题。methods: PiRN 使用了直接将物理模型integrated 到训练过程中，以帮助网络对实际世界中的湮渠状况进行适应。此外，PiRN 还引入了 Stochastic Refinement (SR) 来增强其 perceived quality。results: PiRN 和 PiRN-SR 能够提高 unknown turbulence conditions 的一致性和恢复评分，并且提供了 pixels 精度和 perceived quality 的州际先进成果。<details>
<summary>Abstract</summary>
Image distortion by atmospheric turbulence is a stochastic degradation, which is a critical problem in long-range optical imaging systems. A number of research has been conducted during the past decades, including model-based and emerging deep-learning solutions with the help of synthetic data. Although fast and physics-grounded simulation tools have been introduced to help the deep-learning models adapt to real-world turbulence conditions recently, the training of such models only relies on the synthetic data and ground truth pairs. This paper proposes the Physics-integrated Restoration Network (PiRN) to bring the physics-based simulator directly into the training process to help the network to disentangle the stochasticity from the degradation and the underlying image. Furthermore, to overcome the ``average effect" introduced by deterministic models and the domain gap between the synthetic and real-world degradation, we further introduce PiRN with Stochastic Refinement (PiRN-SR) to boost its perceptual quality. Overall, our PiRN and PiRN-SR improve the generalization to real-world unknown turbulence conditions and provide a state-of-the-art restoration in both pixel-wise accuracy and perceptual quality. Our codes are available at \url{https://github.com/VITA-Group/PiRN}.
</details>
<details>
<summary>摘要</summary>
图像扭曲 caused by atmospheric turbulence 是一种随机的延迟问题，对于远程光学感知系统来说是一个 kritical problem。过去几十年，一些研究已经进行了，包括基于模型的和 emerging deep learning 解决方案，使用合成数据。虽然最近已经出现了基于物理的快速模拟工具，但是这些模型的训练只是基于合成数据和真实数据对。本文提出了物理结合的Restoration Network（PiRN），将物理基础的模拟器直接引入训练过程，以帮助网络分离随机性和扭曲以及下面的图像。此外，为了超越由 deterministic 模型引入的“平均效果”和真实世界扭曲与合成数据之间的领域差异，我们进一步引入 PiRN with Stochastic Refinement（PiRN-SR），以提高它的感知质量。总的来说，我们的 PiRN 和 PiRN-SR 可以提高对真实世界未知扭曲条件的泛化能力，并提供了 pixel-wise 精度和感知质量的 state-of-the-art 修复。我们的代码可以在 \url{https://github.com/VITA-Group/PiRN} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Is-Grad-CAM-Explainable-in-Medical-Images"><a href="#Is-Grad-CAM-Explainable-in-Medical-Images" class="headerlink" title="Is Grad-CAM Explainable in Medical Images?"></a>Is Grad-CAM Explainable in Medical Images?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10506">http://arxiv.org/abs/2307.10506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhashis Suara, Aayush Jha, Pratik Sinha, Arif Ahmed Sekh</li>
<li>For: This paper is written for researchers and practitioners in the field of medical imaging and artificial intelligence, particularly those interested in Explainable Deep Learning and its applications in medical imaging.* Methods: The paper discusses various explainability techniques, including Grad-CAM, and their limitations in medical imaging applications.* Results: The findings highlight the potential of Explainable Deep Learning and Grad-CAM in improving the accuracy and interpretability of deep learning models in medical imaging.Here’s the same information in Simplified Chinese:* For: 这篇论文是为了吸引医学图像和人工智能领域的研究人员和实践者，尤其是关注Explainable Deep Learning和其在医学图像应用中的潜在应用。* Methods: 论文讨论了各种解释技术，包括Grad-CAM，以及它们在医学图像应用中的限制。* Results: 发现表明Explainable Deep Learning和Grad-CAM在医学图像应用中可以提高深度学习模型的准确率和可读性。<details>
<summary>Abstract</summary>
Explainable Deep Learning has gained significant attention in the field of artificial intelligence (AI), particularly in domains such as medical imaging, where accurate and interpretable machine learning models are crucial for effective diagnosis and treatment planning. Grad-CAM is a baseline that highlights the most critical regions of an image used in a deep learning model's decision-making process, increasing interpretability and trust in the results. It is applied in many computer vision (CV) tasks such as classification and explanation. This study explores the principles of Explainable Deep Learning and its relevance to medical imaging, discusses various explainability techniques and their limitations, and examines medical imaging applications of Grad-CAM. The findings highlight the potential of Explainable Deep Learning and Grad-CAM in improving the accuracy and interpretability of deep learning models in medical imaging. The code is available in (will be available).
</details>
<details>
<summary>摘要</summary>
simplified Chinese:人工智能（AI）领域中，可解释深度学习（Explainable Deep Learning）已经受到了广泛关注，尤其在医疗影像领域，因为需要准确和可解释的机器学习模型以便有效的诊断和治疗规划。Grad-CAM是一个基线，它可以高亮深度学习模型决策过程中使用的图像中的关键区域，从而提高了可解释性和信任性。它在许多计算机视觉（CV）任务中被应用，如分类和解释。本研究探讨了Explainable Deep Learning的原理和医疗影像中的应用，讨论了不同的解释技术和其局限性，以及Grad-CAM在医疗影像中的应用。研究发现，Explainable Deep Learning和Grad-CAM在医疗影像中可以提高深度学习模型的准确性和可解释性。代码将在（将来可用）。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Metaverse-A-Young-Gamer’s-Perspective"><a href="#Metaverse-A-Young-Gamer’s-Perspective" class="headerlink" title="Metaverse: A Young Gamer’s Perspective"></a>Metaverse: A Young Gamer’s Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10439">http://arxiv.org/abs/2307.10439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan V. Bajić, Teo Saeedi-Bajić, Kai Saeedi-Bajić</li>
<li>for: 了解小游戏玩家对Metaverse的需求和期望</li>
<li>methods: 基于年龄在10岁以下的小游戏玩家的视角，探讨Metaverse与物理世界以及其他技术的关系，以及这些年轻用户对Metaverse的期望</li>
<li>results: 提供了对小游戏玩家对Metaverse的认知和期望的准确描述，可以用于规划更详细的主观实验和MMSP技术的研发<details>
<summary>Abstract</summary>
When developing technologies for the Metaverse, it is important to understand the needs and requirements of end users. Relatively little is known about the specific perspectives on the use of the Metaverse by the youngest audience: children ten and under. This paper explores the Metaverse from the perspective of a young gamer. It examines their understanding of the Metaverse in relation to the physical world and other technologies they may be familiar with, looks at some of their expectations of the Metaverse, and then relates these to the specific multimedia signal processing (MMSP) research challenges. The perspectives presented in the paper may be useful for planning more detailed subjective experiments involving young gamers, as well as informing the research on MMSP technologies targeted at these users.
</details>
<details>
<summary>摘要</summary>
在开发 metaverse 技术时，需要了解用户的需求和要求。目前对最年轻的用户（10岁以下）对 metaverse 的使用情况知之甚少。这篇论文从年轻游戏者的视角来探讨 metaverse，了解它与物理世界以及他们可能熟悉的其他技术之间的关系，探讨他们对 metaverse 的期望，并将这些期望与 multimedia signal processing（MMSP）技术研究挑战相关联。这篇论文中的视角可以用于规划更详细的主观实验，以及 informing MMSP 技术的研究。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Latent-Autoencoder-with-Self-Attention-for-Structural-Image-Synthesis"><a href="#Adversarial-Latent-Autoencoder-with-Self-Attention-for-Structural-Image-Synthesis" class="headerlink" title="Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis"></a>Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10166">http://arxiv.org/abs/2307.10166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajie Fan, Laure Vuaille, Hao Wang, Thomas Bäck<br>for:SA-ALAE is proposed to facilitate industrial engineering processes by generating feasible design images of complex engineering parts.methods:SA-ALAE uses a novel Self-Attention Adversarial Latent Autoencoder architecture, which allows generating feasible design images by leveraging the structural patterns and long-range dependencies in industrial design images.results:SA-ALAE is shown to be effective in generating engineering blueprints in a real automotive design task, allowing users to explore novel variants of an existing design and control the generation process by operating in latent space.<details>
<summary>Abstract</summary>
Generative Engineering Design approaches driven by Deep Generative Models (DGM) have been proposed to facilitate industrial engineering processes. In such processes, designs often come in the form of images, such as blueprints, engineering drawings, and CAD models depending on the level of detail. DGMs have been successfully employed for synthesis of natural images, e.g., displaying animals, human faces and landscapes. However, industrial design images are fundamentally different from natural scenes in that they contain rich structural patterns and long-range dependencies, which are challenging for convolution-based DGMs to generate. Moreover, DGM-driven generation process is typically triggered based on random noisy inputs, which outputs unpredictable samples and thus cannot perform an efficient industrial design exploration. We tackle these challenges by proposing a novel model Self-Attention Adversarial Latent Autoencoder (SA-ALAE), which allows generating feasible design images of complex engineering parts. With SA-ALAE, users can not only explore novel variants of an existing design, but also control the generation process by operating in latent space. The potential of SA-ALAE is shown by generating engineering blueprints in a real automotive design task.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>生成工程设计方法驱动深度生成模型（DGM）已经提出，以便促进工程设计过程。在这些过程中，设计通常以图像的形式出现，例如蓝图、工程图纸和CAD模型，具体取决于级别。DGM已经成功地应用于自然图像的生成，例如显示动物、人脸和风景。然而，工程设计图像与自然场景不同，它们具有丰富的结构征式和长距离依赖关系，这些关系是 convolution-based DGM 生成的挑战。此外，DGM 驱动的生成过程通常由随机噪声触发，输出不可预测的样本，因此无法进行有效的工程设计探索。我们解决这些挑战 by proposing a novel model Self-Attention Adversarial Latent Autoencoder (SA-ALAE)，允许生成复杂工程部件的可靠设计图像。通过 SA-ALAE，用户不仅可以探索现有设计的新变体，还可以在幽默空间控制生成过程。SA-ALAE 的潜力在一个实际的汽车设计任务中展示。
</details></li>
</ul>
<hr>
<h2 id="Make-A-Volume-Leveraging-Latent-Diffusion-Models-for-Cross-Modality-3D-Brain-MRI-Synthesis"><a href="#Make-A-Volume-Leveraging-Latent-Diffusion-Models-for-Cross-Modality-3D-Brain-MRI-Synthesis" class="headerlink" title="Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis"></a>Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10094">http://arxiv.org/abs/2307.10094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingting Zhu, Zeyue Xue, Zhenchao Jin, Xian Liu, Jingzhen He, Ziwei Liu, Lequan Yu</li>
<li>for: 这个研究是为了解决现有的医疗影像合成问题，特别是针对跨modal的医疗影像合成。</li>
<li>methods: 这个研究使用了散射基础的架构，名为Make-A-Volume，并使用了2D backbone和缓和层来实现跨modal的3D医疗影像合成。</li>
<li>results: 这个研究的结果显示了Make-A-Volume框架可以实现高效的跨modal医疗影像合成，并且可以避免模式落差和训练不稳定的问题。<details>
<summary>Abstract</summary>
Cross-modality medical image synthesis is a critical topic and has the potential to facilitate numerous applications in the medical imaging field. Despite recent successes in deep-learning-based generative models, most current medical image synthesis methods rely on generative adversarial networks and suffer from notorious mode collapse and unstable training. Moreover, the 2D backbone-driven approaches would easily result in volumetric inconsistency, while 3D backbones are challenging and impractical due to the tremendous memory cost and training difficulty. In this paper, we introduce a new paradigm for volumetric medical data synthesis by leveraging 2D backbones and present a diffusion-based framework, Make-A-Volume, for cross-modality 3D medical image synthesis. To learn the cross-modality slice-wise mapping, we employ a latent diffusion model and learn a low-dimensional latent space, resulting in high computational efficiency. To enable the 3D image synthesis and mitigate volumetric inconsistency, we further insert a series of volumetric layers in the 2D slice-mapping model and fine-tune them with paired 3D data. This paradigm extends the 2D image diffusion model to a volumetric version with a slightly increasing number of parameters and computation, offering a principled solution for generic cross-modality 3D medical image synthesis. We showcase the effectiveness of our Make-A-Volume framework on an in-house SWI-MRA brain MRI dataset and a public T1-T2 brain MRI dataset. Experimental results demonstrate that our framework achieves superior synthesis results with volumetric consistency.
</details>
<details>
<summary>摘要</summary>
《医学影像合成》是一个关键的领域，它具有推动多种医学影像应用的潜力。虽然最近的深度学习生成模型已经取得了一定的成功，但大多数当前的医学影像合成方法仍然 rely on 生成对抗网络，并且受到模式坍缩和训练不稳定的问题困扰。此外，2D 脊梁驱动的方法会导致 volumes 的不一致，而 3D 脊梁却是由于巨大的存储成本和训练困难而困难实现。在这篇论文中，我们介绍了一种新的概念，即通过 2D 脊梁来实现 cross-modality 3D 医学影像合成。为了学习 cross-modality  slice-wise 映射，我们采用了扩散模型，并学习了一个低维的隐藏空间，从而实现了高效的计算。为了启用 3D 图像合成并减少 volumes 的不一致，我们进一步插入了一系列的volumetric层到 2D slice-mapping 模型中，并通过对匹配的 3D 数据进行精心调整。这种方法将 2D 图像扩散模型扩展到volumetric版本，只需要微小的参数和计算量增加，可以实现一种原理性的解决方案 для通用的 cross-modality 3D 医学影像合成。我们在一个自家的 SWI-MRA 脑MRI 数据集和一个公共 T1-T2 脑MRI 数据集上展示了我们的 Make-A-Volume 框架的效果，并证明了我们的框架可以实现superior的合成结果，同时保持volumetric consistency。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/20/eess.IV_2023_07_20/" data-id="cloh7tqoc01207b8836fy7js0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/cs.SD_2023_07_19/" class="article-date">
  <time datetime="2023-07-19T15:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/cs.SD_2023_07_19/">cs.SD - 2023-07-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Alzheimer’s-Disease-Detection-from-Spontaneous-Speech-and-Text-A-review"><a href="#Alzheimer’s-Disease-Detection-from-Spontaneous-Speech-and-Text-A-review" class="headerlink" title="Alzheimer’s Disease Detection from Spontaneous Speech and Text: A review"></a>Alzheimer’s Disease Detection from Spontaneous Speech and Text: A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10005">http://arxiv.org/abs/2307.10005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vrindha M. K., Geethu V., Anurenjan P. R., Deepak S., Sreeni K. G.</li>
<li>for: 本文旨在检查使用语音分析来诊断阿尔茨heimer病。</li>
<li>methods: 本文使用了不同的算法来分类阿尔茨heimer病，包括语音特征工程和自然语言处理。</li>
<li>results: 根据本文的结论，可以通过考虑语音和语言特征来建立更准确的阿尔茨heimer病分类模型。此外，语音信号可能是诊断 деменция的有用工具，并可能成为诊断阿尔茨heimer病的可靠生物标志。<details>
<summary>Abstract</summary>
In the past decade, there has been a surge in research examining the use of voice and speech analysis as a means of detecting neurodegenerative diseases such as Alzheimer's. Many studies have shown that certain acoustic features can be used to differentiate between normal aging and Alzheimer's disease, and speech analysis has been found to be a cost-effective method of detecting Alzheimer's dementia. The aim of this review is to analyze the various algorithms used in speech-based detection and classification of Alzheimer's disease. A literature survey was conducted using databases such as Web of Science, Google Scholar, and Science Direct, and articles published from January 2020 to the present were included based on keywords such as ``Alzheimer's detection'', "speech," and "natural language processing." The ADReSS, Pitt corpus, and CCC datasets are commonly used for the analysis of dementia from speech, and this review focuses on the various acoustic and linguistic feature engineering-based classification models drawn from 15 studies.   Based on the findings of this study, it appears that a more accurate model for classifying Alzheimer's disease can be developed by considering both linguistic and acoustic data. The review suggests that speech signals can be a useful tool for detecting dementia and may serve as a reliable biomarker for efficiently identifying Alzheimer's disease.
</details>
<details>
<summary>摘要</summary>
过去一代，有很多研究探讨使用声音和语音分析来诊断神经退化疾病如阿尔茨海默病。许多研究表明，certain acoustic features可以用于 отличать正常老化和阿尔茨海默病，并且语音分析被认为是一种cost-effective的诊断阿尔茨海默病方法。本文的目的是对speech-based detection和分类阿尔茨海默病Algorithms进行分析。通过Web of Science、Google Scholar和Science Direct等数据库，对于2020年1月至当前期间发表的文章进行了文献综述，根据键入关键字“阿尔茨海默病检测”、“语音”和“自然语言处理”进行过滤。ADReSS、Pitt corpus和CCC数据集是用于分析诊断 деменции的常用数据集，本文将Focus on various acoustic and linguistic feature engineering-based classification models from 15 studies。根据本研究的结果，可以通过考虑语音和语言数据来建立更准确的阿尔茨海默病分类模型。该综述表明，speech signals可以用于检测诊断 деменcia，并可能成为阿尔茨海默病的可靠生物标志。
</details></li>
</ul>
<hr>
<h2 id="DisCover-Disentangled-Music-Representation-Learning-for-Cover-Song-Identification"><a href="#DisCover-Disentangled-Music-Representation-Learning-for-Cover-Song-Identification" class="headerlink" title="DisCover: Disentangled Music Representation Learning for Cover Song Identification"></a>DisCover: Disentangled Music Representation Learning for Cover Song Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09775">http://arxiv.org/abs/2307.09775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Xun, Shengyu Zhang, Yanting Yang, Jieming Zhu, Liqun Deng, Zhou Zhao, Zhenhua Dong, Ruiqi Li, Lichao Zhang, Fei Wu</li>
<li>for: 这个研究的目标是解决音乐信息检索（MIR）领域中的重要问题——歌曲重建（CSI），即从大量歌曲集中寻找查询歌曲的封面版本。</li>
<li>methods: 该研究使用了 causal graph 技术来分析 CSI 任务，并通过知识导向分解模块（KDM）和敌对分解模块（GADM）来阻断内版本和间版本的影响，从而实现不同版本之间的不可分化。</li>
<li>results: 对比best-performing方法，该研究的 DisCover 框架在 CSI 任务中表现出优于其他方法，并且进行了深入分析，证明了不可分化的重要性。<details>
<summary>Abstract</summary>
In the field of music information retrieval (MIR), cover song identification (CSI) is a challenging task that aims to identify cover versions of a query song from a massive collection. Existing works still suffer from high intra-song variances and inter-song correlations, due to the entangled nature of version-specific and version-invariant factors in their modeling. In this work, we set the goal of disentangling version-specific and version-invariant factors, which could make it easier for the model to learn invariant music representations for unseen query songs. We analyze the CSI task in a disentanglement view with the causal graph technique, and identify the intra-version and inter-version effects biasing the invariant learning. To block these effects, we propose the disentangled music representation learning framework (DisCover) for CSI. DisCover consists of two critical components: (1) Knowledge-guided Disentanglement Module (KDM) and (2) Gradient-based Adversarial Disentanglement Module (GADM), which block intra-version and inter-version biased effects, respectively. KDM minimizes the mutual information between the learned representations and version-variant factors that are identified with prior domain knowledge. GADM identifies version-variant factors by simulating the representation transitions between intra-song versions, and exploits adversarial distillation for effect blocking. Extensive comparisons with best-performing methods and in-depth analysis demonstrate the effectiveness of DisCover and the and necessity of disentanglement for CSI.
</details>
<details>
<summary>摘要</summary>
在音乐信息检索（MIR）领域，绘制歌曲标识（CSI）是一项具有挑战性的任务，旨在从庞大的歌曲库中标识查询歌曲的重新录制版本。现有的方法仍然受到高度的内版本变化和 между版本相关性的影响，这是因为模型的版本特定和版本不具有效的因素相互纠缠不清。在这种情况下，我们设定了分离版本特定和版本不具有效的因素的目标，这可以使模型更容易学习查询歌曲未知版本的不变音乐表示。我们使用 causal graph 技术进行分析 CSI 任务，并确定了内版本和 между版本的影响，以阻塞这些影响。为此，我们提出了一个名为 DisCover 的杜陵音乐表示学习框架，它包括两个关键组件：（1）帮助 guid 分离模块（KDM）和（2）整形强化对抗分离模块（GADM）。KDM 尝试将学习的表示与版本特定因素进行分离，而 GADM 则通过模拟版本之间的表示转移，并通过对抗填充来阻塞版本相关的影响。我们对最佳实现和深入分析进行了广泛的比较，并证明了 DisCover 的有效性和分离的必要性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Domain-Generalization-for-Sound-Classification-with-Sparse-Frequency-Regularized-Transformer"><a href="#Improving-Domain-Generalization-for-Sound-Classification-with-Sparse-Frequency-Regularized-Transformer" class="headerlink" title="Improving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer"></a>Improving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09723">http://arxiv.org/abs/2307.09723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlmu/frito">https://github.com/hlmu/frito</a></li>
<li>paper_authors: Honglin Mu, Wentian Xia, Wanxiang Che</li>
<li>for: 提高Transformer模型对不同数据的泛化能力</li>
<li>methods: 限制每个语言序列位置的自注意力响应范围在频率维度上</li>
<li>results: 实现Transformer模型在TAU 2020和Nsynth数据集上的SOTA泛化性能，并且降低了20%的执行时间<details>
<summary>Abstract</summary>
Sound classification models' performance suffers from generalizing on out-of-distribution (OOD) data. Numerous methods have been proposed to help the model generalize. However, most either introduce inference overheads or focus on long-lasting CNN-variants, while Transformers has been proven to outperform CNNs on numerous natural language processing and computer vision tasks. We propose FRITO, an effective regularization technique on Transformer's self-attention, to improve the model's generalization ability by limiting each sequence position's attention receptive field along the frequency dimension on the spectrogram. Experiments show that our method helps Transformer models achieve SOTA generalization performance on TAU 2020 and Nsynth datasets while saving 20% inference time.
</details>
<details>
<summary>摘要</summary>
音频分类模型的表现受到外部数据（OOD）的影响，许多方法已经被提议来帮助模型泛化。然而，大多数方法都会增加推理负担或者专注于长期存在的CNN变体，而 transformer 则被证明可以在自然语言处理和计算机视觉任务上超越 CNN。我们提出了 FRITO，一种有效的 regularization 技术，用于限制 transformer 自注意力的每个序列位置的频率维度在 spectrogram 上。实验显示，我们的方法可以帮助 transformer 模型在 TAU 2020 和 Nsynth 数据集上实现 SOTA 的泛化性能，同时节省 20% 的推理时间。
</details></li>
</ul>
<hr>
<h2 id="SLMGAN-Exploiting-Speech-Language-Model-Representations-for-Unsupervised-Zero-Shot-Voice-Conversion-in-GANs"><a href="#SLMGAN-Exploiting-Speech-Language-Model-Representations-for-Unsupervised-Zero-Shot-Voice-Conversion-in-GANs" class="headerlink" title="SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs"></a>SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09435">http://arxiv.org/abs/2307.09435</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yl4579/SLMGAN">https://github.com/yl4579/SLMGAN</a></li>
<li>paper_authors: Yinghao Aaron Li, Cong Han, Nima Mesgarani</li>
<li>for: 这篇论文旨在提出一种基于大规模预训练语音语言模型（SLM）的抽象语音模型，用于语音变换任务。</li>
<li>methods: 该论文使用了 generator adversarial network（GAN）框架，并将SLM表示加以利用，以实现静止语音变换。</li>
<li>results: 对比其他当前最佳无监督语音变换模型，SLMGAN在自然性和相似性两个指标上表现出色，表明SLM基于的抽象语音模型可以为相关应用提供潜在的优势。<details>
<summary>Abstract</summary>
In recent years, large-scale pre-trained speech language models (SLMs) have demonstrated remarkable advancements in various generative speech modeling applications, such as text-to-speech synthesis, voice conversion, and speech enhancement. These applications typically involve mapping text or speech inputs to pre-trained SLM representations, from which target speech is decoded. This paper introduces a new approach, SLMGAN, to leverage SLM representations for discriminative tasks within the generative adversarial network (GAN) framework, specifically for voice conversion. Building upon StarGANv2-VC, we add our novel SLM-based WavLM discriminators on top of the mel-based discriminators along with our newly designed SLM feature matching loss function, resulting in an unsupervised zero-shot voice conversion system that does not require text labels during training. Subjective evaluation results show that SLMGAN outperforms existing state-of-the-art zero-shot voice conversion models in terms of naturalness and achieves comparable similarity, highlighting the potential of SLM-based discriminators for related applications.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "pre-trained speech language models" (SLMs) ⇒ "预训练的语音语言模型" (SLM)* "text-to-speech synthesis" ⇒ "文本到语音合成"* "voice conversion" ⇒ "语音转换"* "mel-based discriminators" ⇒ "基于mel的探测器"* "SLM feature matching loss function" ⇒ "SLM特征匹配损失函数"* "zero-shot voice conversion" ⇒ "零shot语音转换"* "naturalness" ⇒ "自然性"* "similarity" ⇒ "相似性"
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/cs.SD_2023_07_19/" data-id="cloh7tqku00s77b8888x4fmw2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/eess.AS_2023_07_19/" class="article-date">
  <time datetime="2023-07-19T14:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/eess.AS_2023_07_19/">eess.AS - 2023-07-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-analysis-on-the-effects-of-speaker-embedding-choice-in-non-auto-regressive-TTS"><a href="#An-analysis-on-the-effects-of-speaker-embedding-choice-in-non-auto-regressive-TTS" class="headerlink" title="An analysis on the effects of speaker embedding choice in non auto-regressive TTS"></a>An analysis on the effects of speaker embedding choice in non auto-regressive TTS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09898">http://arxiv.org/abs/2307.09898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adriana Stan, Johannah O’Mahony</li>
<li>for: 这个论文是研究非 autoregressive 多说话人语音合成架构如何利用不同的说话人嵌入集的信息，以提高目标说话人标识的质量。</li>
<li>methods: 这个论文使用了一种新的非 autoregressive 多说话人语音合成架构，并分析了在不同的嵌入集和学习策略下，是否会产生任何质量改进。</li>
<li>results: 研究发现， regardless of the used set of embeddings and learning strategy, the network can handle various speaker identities equally well, with barely noticeable variations in speech output quality, and that speaker leakage within the core structure of the synthesis system is inevitable in the standard training procedures adopted thus far。<details>
<summary>Abstract</summary>
In this paper we introduce a first attempt on understanding how a non-autoregressive factorised multi-speaker speech synthesis architecture exploits the information present in different speaker embedding sets. We analyse if jointly learning the representations, and initialising them from pretrained models determine any quality improvements for target speaker identities. In a separate analysis, we investigate how the different sets of embeddings impact the network's core speech abstraction (i.e. zero conditioned) in terms of speaker identity and representation learning. We show that, regardless of the used set of embeddings and learning strategy, the network can handle various speaker identities equally well, with barely noticeable variations in speech output quality, and that speaker leakage within the core structure of the synthesis system is inevitable in the standard training procedures adopted thus far.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们首次尝试了理解非 autoregressive 多 speaker speech synthesis 架构如何利用不同 speaker embedding 集的信息。我们分析了在一起学习表示和从预训练模型初始化是否会提高目标 speaker 身份的质量改进。在另一项分析中，我们研究了不同 embedding 集的影响于核心语音抽象（即零条件）中的 speaker identity 和表示学习。我们发现，无论使用哪个 embedding 集和学习策略，网络都可以平等地处理不同的 speaker 身份，并且 speech 输出质量的变化幅度几乎无法注意。此外，我们发现在标准的训练程序中，speaker leakage 会在核心Synthesis 系统的结构中发生，这是不可避免的。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Acoustic-Word-Embedding-Learning-via-Correspondence-Transformer-Encoder"><a href="#Self-Supervised-Acoustic-Word-Embedding-Learning-via-Correspondence-Transformer-Encoder" class="headerlink" title="Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder"></a>Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09871">http://arxiv.org/abs/2307.09871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingru Lin, Xianghu Yue, Junyi Ao, Haizhou Li</li>
<li>For: The paper aims to learn robust acoustic word embeddings (AWEs) from a large-scale unlabelled speech corpus.* Methods: The proposed method, called Correspondence Transformer Encoder (CTE), uses a teacher-student learning framework and pre-trains the model with a word-level loss to ensure that different realisations of the same word are close in the underlying embedding space.* Results: The embeddings extracted from the CTE model are robust to speech variations, such as speakers and domains, and achieve new state-of-the-art performance on a low-resource cross-lingual setting.Here’s the simplified Chinese text for the three key points:* For: 这个论文目标是从大规模无标签语音集合中学习坚定的语音字幕嵌入 (AWEs)。* Methods: 提posed方法是使用教师学生学习框架，先在word级别损失下预训练模型，以确保不同的语音实现都是 embedding 空间中的近似。* Results: CTE模型提取的嵌入具有对语音变化（如speaker和domain）的抗变性，并在low-resource cross-lingual设定中达到新的状态纪录性。<details>
<summary>Abstract</summary>
Acoustic word embeddings (AWEs) aims to map a variable-length speech segment into a fixed-dimensional representation. High-quality AWEs should be invariant to variations, such as duration, pitch and speaker. In this paper, we introduce a novel self-supervised method to learn robust AWEs from a large-scale unlabelled speech corpus. Our model, named Correspondence Transformer Encoder (CTE), employs a teacher-student learning framework. We train the model based on the idea that different realisations of the same word should be close in the underlying embedding space. Specifically, we feed the teacher and student encoder with different acoustic instances of the same word and pre-train the model with a word-level loss. Our experiments show that the embeddings extracted from the proposed CTE model are robust to speech variations, e.g. speakers and domains. Additionally, when evaluated on Xitsonga, a low-resource cross-lingual setting, the CTE model achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
听音词嵌入（AWEs）的目标是将变长的语音段映射到固定维度的表示中。高质量的AWEs应该具有不变性，例如持续时间、音高和说话人。在这篇论文中，我们介绍了一种新的自动学习方法，以学习高质量的AWEs从大规模的无标签语音 corpus 中。我们的模型，即匹配变换器 encoder（CTE），采用了教师学生学习框架。我们在不同的语音实例中Feed 教师和学生encoder，并在Word level上预训练模型。我们的实验表明，从我们提出的CTE模型中提取的嵌入是不同语音变化（例如，说话人和频谱）的抗衡。此外，当我们在Xitsonga，一个低资源的 Cross-Lingual Setting 中评估CTE模型时，它达到了新的状态前的最佳性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/eess.AS_2023_07_19/" data-id="cloh7tqn400yj7b884xb758z8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/cs.CV_2023_07_19/" class="article-date">
  <time datetime="2023-07-19T13:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/cs.CV_2023_07_19/">cs.CV - 2023-07-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unsupervised-Accuracy-Estimation-of-Deep-Visual-Models-using-Domain-Adaptive-Adversarial-Perturbation-without-Source-Samples"><a href="#Unsupervised-Accuracy-Estimation-of-Deep-Visual-Models-using-Domain-Adaptive-Adversarial-Perturbation-without-Source-Samples" class="headerlink" title="Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples"></a>Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10062">http://arxiv.org/abs/2307.10062</a></li>
<li>repo_url: None</li>
<li>paper_authors: JoonHo Lee, Jae Oh Woo, Hankyu Moon, Kwonho Lee</li>
<li>for: 本研究的目的是提出一种无需源数据和标签的深度视觉模型部署方法，以解决由于目标分布与源分布的不同而导致的性能下降问题。</li>
<li>methods: 本研究使用pseudo-标签来估计目标域的准确性，并采用最近的无源适应预测算法进行适应。我们在目标模型的输入上应用自适应敌意干扰，以减少由高 идеal共同假设风险导致的假标签的影响。</li>
<li>results: 我们的源无法框架可以有效地Addresses the challenging distribution shift scenarios，并超越需要源数据和标签的训练方法。<details>
<summary>Abstract</summary>
Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions. Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices. Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data. We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms. Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. Our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training.
</details>
<details>
<summary>摘要</summary>
deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions. several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices. our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data. we investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms. our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. we mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training.
</details></li>
</ul>
<hr>
<h2 id="Divert-More-Attention-to-Vision-Language-Object-Tracking"><a href="#Divert-More-Attention-to-Vision-Language-Object-Tracking" class="headerlink" title="Divert More Attention to Vision-Language Object Tracking"></a>Divert More Attention to Vision-Language Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10046">http://arxiv.org/abs/2307.10046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JudasDie/SOTS">https://github.com/JudasDie/SOTS</a></li>
<li>paper_authors: Mingzhe Guo, Zhipeng Zhang, Liping Jing, Haibin Ling, Heng Fan</li>
<li>for: 本文旨在提出一种基于视觉语言学习的tracking方法，以解决现有的视觉语言学习模型在跟踪领域的不足。</li>
<li>methods: 本文提出了一种新的视觉语言表示方法，包括提出了一种通用特征标注策略和一种模态混合器（ModaMixer）。同时，本文还引入了一种对比损失来对不同模式进行对应。</li>
<li>results: 本文的方法在六个跟踪benchmark上进行了实验，并取得了显著的提升。同时，本文还提供了一些理论分析，以证明方法的合理性。<details>
<summary>Abstract</summary>
Multimodal vision-language (VL) learning has noticeably pushed the tendency toward generic intelligence owing to emerging large foundation models. However, tracking, as a fundamental vision problem, surprisingly enjoys less bonus from recent flourishing VL learning. We argue that the reasons are two-fold: the lack of large-scale vision-language annotated videos and ineffective vision-language interaction learning of current works. These nuisances motivate us to design more effective vision-language representation for tracking, meanwhile constructing a large database with language annotation for model learning. Particularly, in this paper, we first propose a general attribute annotation strategy to decorate videos in six popular tracking benchmarks, which contributes a large-scale vision-language tracking database with more than 23,000 videos. We then introduce a novel framework to improve tracking by learning a unified-adaptive VL representation, where the cores are the proposed asymmetric architecture search and modality mixer (ModaMixer). To further improve VL representation, we introduce a contrastive loss to align different modalities. To thoroughly evidence the effectiveness of our method, we integrate the proposed framework on three tracking methods with different designs, i.e., the CNN-based SiamCAR, the Transformer-based OSTrack, and the hybrid structure TransT. The experiments demonstrate that our framework can significantly improve all baselines on six benchmarks. Besides empirical results, we theoretically analyze our approach to show its rationality. By revealing the potential of VL representation, we expect the community to divert more attention to VL tracking and hope to open more possibilities for future tracking with diversified multimodal messages.
</details>
<details>
<summary>摘要</summary>
多模态视语（VL）学习已经明显推动了基本智能的趋势，尤其是由于大规模基础模型的出现。然而，跟踪问题，作为视觉基本问题，却意外地没有从现在蓬勃发展的 VL 学习中受到多大的优惠。我们认为这有两个原因：一是视觉语言注解视频的缺乏大规模数据集，二是当前 VL 学习中视觉语言交互学习的不具有效果。这些弊端使我们强烈需要设计更有效的视觉语言表示，同时建立一个大规模的语言注解视频数据集。在本文中，我们首先提出一种通用属性注解策略，以增加 six 个流行的跟踪benchmark中的视频数量，从而构建了大规模的视觉语言跟踪数据集，包含超过 23,000 个视频。然后，我们引入一种新的框架，以提高跟踪的性能，包括我们所提出的异symmetric architecture search和ModaMixer。为了进一步提高 VL 表示，我们还引入了一种对比损失，以协调不同模式。为了证明我们的方法的效iveness，我们将其应用于三种不同的跟踪方法中，即CNN基于 SiamCAR，Transformer基于 OSTrack，以及混合结构 TransT。实验结果表明，我们的框架可以显著提高所有基准在六个benchmark中。此外，我们还进行了理论分析，以证明我们的方法的合理性。通过推动 VL 表示的潜力，我们希望社区更多地关注 VL 跟踪，并期待在未来跟踪中涉及到多样化的Multimodal消息时开启更多的可能性。
</details></li>
</ul>
<hr>
<h2 id="Class-Attention-to-Regions-of-Lesion-for-Imbalanced-Medical-Image-Recognition"><a href="#Class-Attention-to-Regions-of-Lesion-for-Imbalanced-Medical-Image-Recognition" class="headerlink" title="Class Attention to Regions of Lesion for Imbalanced Medical Image Recognition"></a>Class Attention to Regions of Lesion for Imbalanced Medical Image Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10036">http://arxiv.org/abs/2307.10036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Xin Zhuang, Jiabin Cai, Jianguo Zhang, Wei-shi Zheng, Ruixuan Wang<br>for:  This paper aims to effectively handle data imbalance issues in automated medical image classification by proposing a simple yet effective framework, named CARE, which embeds attention into the training process of CNNs to help the network attend to lesion regions of rare diseases.methods:  The proposed CARE framework uses an attention module that helps CNNs attend to lesion regions of rare diseases during the training phase. The attention module is embedded into the training process of CNNs and does not change the architecture of the original network. Additionally, the authors developed variants of CARE by leveraging traditional saliency methods or a pretrained segmentation model for bounding box generation.results:  The results show that the CARE variants with automated bounding box generation are comparable to the original CARE framework with manual bounding box annotations. The method effectively helps the network focus on the lesion regions of rare diseases and remarkably improves the classification performance of rare diseases on two imbalanced datasets, one for skin images and one for pneumonia.<details>
<summary>Abstract</summary>
Automated medical image classification is the key component in intelligent diagnosis systems. However, most medical image datasets contain plenty of samples of common diseases and just a handful of rare ones, leading to major class imbalances. Currently, it is an open problem in intelligent diagnosis to effectively learn from imbalanced training data. In this paper, we propose a simple yet effective framework, named \textbf{C}lass \textbf{A}ttention to \textbf{RE}gions of the lesion (CARE), to handle data imbalance issues by embedding attention into the training process of \textbf{C}onvolutional \textbf{N}eural \textbf{N}etworks (CNNs). The proposed attention module helps CNNs attend to lesion regions of rare diseases, therefore helping CNNs to learn their characteristics more effectively. In addition, this attention module works only during the training phase and does not change the architecture of the original network, so it can be directly combined with any existing CNN architecture. The CARE framework needs bounding boxes to represent the lesion regions of rare diseases. To alleviate the need for manual annotation, we further developed variants of CARE by leveraging the traditional saliency methods or a pretrained segmentation model for bounding box generation. Results show that the CARE variants with automated bounding box generation are comparable to the original CARE framework with \textit{manual} bounding box annotations. A series of experiments on an imbalanced skin image dataset and a pneumonia dataset indicates that our method can effectively help the network focus on the lesion regions of rare diseases and remarkably improves the classification performance of rare diseases.
</details>
<details>
<summary>摘要</summary>
自动医疗图像分类是智能诊断系统的关键组件。然而，大多数医疗图像集合中充满常见疾病的样本，而罕见疾病的样本却只有几个，这导致了培训数据的主要类别不均。目前，智能诊断中有一个打开的问题是如何有效地学习培训数据中的类别不均问题。在这篇论文中，我们提出了一个简单 yet有效的框架，名为\textbf{C}lass \textbf{A}ttention to \textbf{RE}gions of the lesion (CARE)，用于处理培训数据中的类别不均问题。我们在\textbf{C}onvolutional \textbf{N}eural \textbf{N}etworks (CNNs) 的培训过程中嵌入注意力模块，使 CNNs 能够更有效地关注罕见疾病的 lesion 区域。此外，我们的注意力模块只在培训阶段工作，不会改变原始网络的架构，因此可以直接与任何现有的 CNN 架构结合使用。CARE 框架需要病变区域的 bounding box 来表示罕见疾病的 lesion 区域。为了减少手动标注的需求，我们还开发了 CARE 的多种变体，通过利用传统的注意力方法或一个预训导的分割模型来生成 bounding box。实验结果表明，CARE 变体使用自动生成的 bounding box 与手动标注的 CARE 框架相当。系列的实验在一个充满病变的皮肤图像集合和一个肺炎图像集合中表明，我们的方法可以帮助网络更有效地关注罕见疾病的 lesion 区域，并显著提高罕见疾病的分类性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Fair-Face-Verification-An-In-depth-Analysis-of-Demographic-Biases"><a href="#Towards-Fair-Face-Verification-An-In-depth-Analysis-of-Demographic-Biases" class="headerlink" title="Towards Fair Face Verification: An In-depth Analysis of Demographic Biases"></a>Towards Fair Face Verification: An In-depth Analysis of Demographic Biases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10011">http://arxiv.org/abs/2307.10011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Sarridis, Christos Koutlis, Symeon Papadopoulos, Christos Diou</li>
<li>for: 这篇论文旨在探讨人脸识别和验证系统中存在的性别、年龄和种族偏见问题，以及这些偏见在不同保护组合中的交叠影响。</li>
<li>methods: 本论文使用深度学习方法进行人脸识别和验证，并对现有云端解决方案进行了分析。同时，该论文还包括五种附加的评价指标，以衡量系统的公平性。</li>
<li>results: 研究发现，存在许多种族、年龄和性别的偏见问题，其中非裔人群的True Positive Rate（TPR）相比于白人群体下降11.25%，而且在不同保护组合之间存在显著的偏见。此外，跨多个保护组合的交叠效应也存在显著偏见。这些结果希望能够促进更公平、更公正的人脸识别和验证系统的发展。<details>
<summary>Abstract</summary>
Deep learning-based person identification and verification systems have remarkably improved in terms of accuracy in recent years; however, such systems, including widely popular cloud-based solutions, have been found to exhibit significant biases related to race, age, and gender, a problem that requires in-depth exploration and solutions. This paper presents an in-depth analysis, with a particular emphasis on the intersectionality of these demographic factors. Intersectional bias refers to the performance discrepancies w.r.t. the different combinations of race, age, and gender groups, an area relatively unexplored in current literature. Furthermore, the reliance of most state-of-the-art approaches on accuracy as the principal evaluation metric often masks significant demographic disparities in performance. To counter this crucial limitation, we incorporate five additional metrics in our quantitative analysis, including disparate impact and mistreatment metrics, which are typically ignored by the relevant fairness-aware approaches. Results on the Racial Faces in-the-Wild (RFW) benchmark indicate pervasive biases in face recognition systems, extending beyond race, with different demographic factors yielding significantly disparate outcomes. In particular, Africans demonstrate an 11.25% lower True Positive Rate (TPR) compared to Caucasians, while only a 3.51% accuracy drop is observed. Even more concerning, the intersections of multiple protected groups, such as African females over 60 years old, demonstrate a +39.89% disparate mistreatment rate compared to the highest Caucasians rate. By shedding light on these biases and their implications, this paper aims to stimulate further research towards developing fairer, more equitable face recognition and verification systems.
</details>
<details>
<summary>摘要</summary>
深度学习基于人识别和验证系统在过去几年中有了非常大的进步，但是这些系统，包括广泛使用的云端解决方案，已经被发现有显著的种族、年龄和性别相关的偏见问题，这是需要深入探讨和解决的问题。这篇文章提供了深入的分析，尤其是关于人识别系统中 intersect 的偏见问题。人识别系统中的偏见指的是不同的种族、年龄和性别组合的性能差异，这是现有文献中尚未得到充分探讨的领域。此外，现有大多数状态的艺术方法仅仅依靠精度作为评价指标，这会隐藏人识别系统中的重要民生差异。为了解决这一重要 limitation，我们在量化分析中添加了五个额外的指标，包括不平等影响指标和不公正对待指标，这些指标通常被现有的公平意识方法忽略。results on the Racial Faces in-the-Wild (RFW) benchmark show that face recognition systems exhibit pervasive biases beyond race, with different demographic factors leading to significantly disparate outcomes. Specifically, Africans have a 11.25% lower True Positive Rate (TPR) compared to Caucasians, while only a 3.51% accuracy drop is observed. Furthermore, the intersections of multiple protected groups, such as African females over 60 years old, demonstrate a +39.89% disparate mistreatment rate compared to the highest Caucasians rate. By shedding light on these biases and their implications, this paper aims to stimulate further research towards developing fairer, more equitable face recognition and verification systems.
</details></li>
</ul>
<hr>
<h2 id="MODA-Mapping-Once-Audio-driven-Portrait-Animation-with-Dual-Attentions"><a href="#MODA-Mapping-Once-Audio-driven-Portrait-Animation-with-Dual-Attentions" class="headerlink" title="MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions"></a>MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10008">http://arxiv.org/abs/2307.10008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfei Liu, Lijian Lin, Fei Yu, Changyin Zhou, Yu Li</li>
<li>for: 这篇论文旨在实现基于音频的人脸动画，以生成高精度、多模态的人脸视频。</li>
<li>methods: 该方法包括三个阶段：1）Mapping-Once网络with dual attention（MODA）将音频转换为说话表示；2）facial composer网络生成精度受到控制的面部特征点；3）时间引导渲染器Synthesize稳定的视频。</li>
<li>results: 对比前方法，该方法生成的人脸视频更加自然和真实。<details>
<summary>Abstract</summary>
Audio-driven portrait animation aims to synthesize portrait videos that are conditioned by given audio. Animating high-fidelity and multimodal video portraits has a variety of applications. Previous methods have attempted to capture different motion modes and generate high-fidelity portrait videos by training different models or sampling signals from given videos. However, lacking correlation learning between lip-sync and other movements (e.g., head pose/eye blinking) usually leads to unnatural results. In this paper, we propose a unified system for multi-person, diverse, and high-fidelity talking portrait generation. Our method contains three stages, i.e., 1) Mapping-Once network with Dual Attentions (MODA) generates talking representation from given audio. In MODA, we design a dual-attention module to encode accurate mouth movements and diverse modalities. 2) Facial composer network generates dense and detailed face landmarks, and 3) temporal-guided renderer syntheses stable videos. Extensive evaluations demonstrate that the proposed system produces more natural and realistic video portraits compared to previous methods.
</details>
<details>
<summary>摘要</summary>
Audio-driven portrait animation aims to synthesize portrait videos that are conditioned by given audio. 动画高质量多modal视频肖像有很多应用。先前的方法都是通过不同的模型或样本信号来捕捉不同的动作模式并生成高质量肖像视频。然而，通常lacking correlation learning between lip-sync and other movements (例如，头姿/眼睛跳动)，导致结果不自然。在这篇论文中，我们提出了一个统一的系统，用于多个人、多样化和高质量的说话肖像生成。我们的方法包括三个阶段：1. Mapping-Once网络与双注意力（MODA）生成来自给定音频的说话表示。在MODA中，我们设计了双注意力模块，以编码准确的口型动作和多样化的特征。2. 脸部组合网络生成密集和详细的面部标记点，3. 时间引导渲染器将生成稳定的视频。广泛的评估表明，我们提出的系统可以生成更自然和真实的视频肖像，与先前的方法相比。
</details></li>
</ul>
<hr>
<h2 id="As-large-as-it-gets-Learning-infinitely-large-Filters-via-Neural-Implicit-Functions-in-the-Fourier-Domain"><a href="#As-large-as-it-gets-Learning-infinitely-large-Filters-via-Neural-Implicit-Functions-in-the-Fourier-Domain" class="headerlink" title="As large as it gets: Learning infinitely large Filters via Neural Implicit Functions in the Fourier Domain"></a>As large as it gets: Learning infinitely large Filters via Neural Implicit Functions in the Fourier Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10001">http://arxiv.org/abs/2307.10001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Grabinski, Janis Keuper, Margret Keuper</li>
<li>for:  investigate how large receptive fields are needed in vision applications, and develop a method to learn large filters without increasing memory consumption during training or inference.</li>
<li>methods: learn frequency representations of filter weights as neural implicit functions, allowing for efficient implementation in current frameworks and enabling the use of infinitely large filters with only a few learnable weights.</li>
<li>results: achieve results on par with the state-of-the-art on large image classification benchmarks while executing convolutions solely in the frequency domain, and provide an extensive analysis of the learned receptive fields, showing that the learned filters are well-localized and relatively small in the spatial domain.<details>
<summary>Abstract</summary>
Motivated by the recent trend towards the usage of larger receptive fields for more context-aware neural networks in vision applications, we aim to investigate how large these receptive fields really need to be. To facilitate such study, several challenges need to be addressed, most importantly: (i) We need to provide an effective way for models to learn large filters (potentially as large as the input data) without increasing their memory consumption during training or inference, (ii) the study of filter sizes has to be decoupled from other effects such as the network width or number of learnable parameters, and (iii) the employed convolution operation should be a plug-and-play module that can replace any conventional convolution in a Convolutional Neural Network (CNN) and allow for an efficient implementation in current frameworks. To facilitate such models, we propose to learn not spatial but frequency representations of filter weights as neural implicit functions, such that even infinitely large filters can be parameterized by only a few learnable weights. The resulting neural implicit frequency CNNs are the first models to achieve results on par with the state-of-the-art on large image classification benchmarks while executing convolutions solely in the frequency domain and can be employed within any CNN architecture. They allow us to provide an extensive analysis of the learned receptive fields. Interestingly, our analysis shows that, although the proposed networks could learn very large convolution kernels, the learned filters practically translate into well-localized and relatively small convolution kernels in the spatial domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>We need to find an effective way for models to learn large filters (potentially as large as the input data) without increasing their memory consumption during training or inference.2. The study of filter sizes must be decoupled from other effects such as the network width or number of learnable parameters.3. The employed convolution operation should be a plug-and-play module that can replace any conventional convolution in a Convolutional Neural Network (CNN) and allow for an efficient implementation in current frameworks.To address these challenges, we propose learning not spatial but frequency representations of filter weights as neural implicit functions. This allows even infinitely large filters to be parameterized by only a few learnable weights. The resulting neural implicit frequency CNNs are the first models to achieve results on par with the state-of-the-art on large image classification benchmarks while executing convolutions solely in the frequency domain. They can be employed within any CNN architecture and allow us to provide an extensive analysis of the learned receptive fields.Our analysis shows that, although the proposed networks could learn very large convolution kernels, the learned filters practically translate into well-localized and relatively small convolution kernels in the spatial domain.</details></li>
</ol>
<hr>
<h2 id="Mitigating-Viewer-Impact-from-Disturbing-Imagery-using-AI-Filters-A-User-Study"><a href="#Mitigating-Viewer-Impact-from-Disturbing-Imagery-using-AI-Filters-A-User-Study" class="headerlink" title="Mitigating Viewer Impact from Disturbing Imagery using AI Filters: A User-Study"></a>Mitigating Viewer Impact from Disturbing Imagery using AI Filters: A User-Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10334">http://arxiv.org/abs/2307.10334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Sarridis, Jochen Spangenberg, Olga Papadopoulou, Symeon Papadopoulos<br>for: 这个论文的目的是研究用人工智能技术来减少对激素内容的情感影响。methods: 这个研究使用了107名参与者，主要是记者和人权调查员，测试了五种过滤方式，包括传统的滤除（抑制和部分抑制）和人工智能基于的绘制（绘制、颜色绘制和画画）。研究测量了这些过滤方式对图像信息传递和情感压力减少的效果。results: 研究发现，基于人工智能的绘制过滤方式表现最佳，可以减少负面感受(-30.38%)，同时保持图像的可读性（97.19%）。参与者还建议将AI过滤器 integrate到他们的工作流程中，例如用作先决步骤，以便最终 inspect the original image。这篇论文为专业人士Routinely处理可能激素内容的视觉环境的开发做出了贡献。<details>
<summary>Abstract</summary>
Exposure to disturbing imagery can significantly impact individuals, especially professionals who encounter such content as part of their work. This paper presents a user study, involving 107 participants, predominantly journalists and human rights investigators, that explores the capability of Artificial Intelligence (AI)-based image filters to potentially mitigate the emotional impact of viewing such disturbing content. We tested five different filter styles, both traditional (Blurring and Partial Blurring) and AI-based (Drawing, Colored Drawing, and Painting), and measured their effectiveness in terms of conveying image information while reducing emotional distress. Our findings suggest that the AI-based Drawing style filter demonstrates the best performance, offering a promising solution for reducing negative feelings (-30.38%) while preserving the interpretability of the image (97.19%). Despite the requirement for many professionals to eventually inspect the original images, participants suggested potential strategies for integrating AI filters into their workflow, such as using AI filters as an initial, preparatory step before viewing the original image. Overall, this paper contributes to the development of a more ethically considerate and effective visual environment for professionals routinely engaging with potentially disturbing imagery.
</details>
<details>
<summary>摘要</summary>
曝露恐怖图像可能对个人产生深见影响，特别是专业人士在工作中遇到这类内容。这篇论文报告了一项用户研究，参与者为107名记者和人权调查员，探讨了人工智能（AI）基于图像筛选器的可能性来减轻曝露恐怖图像的情感影响。我们测试了5种不同的筛选器风格，包括传统的掩蔽和部分掩蔽，以及基于AI的绘制、颜色绘制和画画风格。我们发现，AI基于绘制风格的筛选器表现最佳，可以减少负面情感 (-30.38%)  while保持图像的可读性（97.19%）。尽管 eventually需要查看原图像，但参与者建议可以将AI筛选器 integrate into their workflow，例如作为初步预备步骤 перед查看原图像。总的来说，这篇论文对专业人士常遇到可能恐怖图像的可负面影响的开发提供了一种更优的解决方案。
</details></li>
</ul>
<hr>
<h2 id="TUNeS-A-Temporal-U-Net-with-Self-Attention-for-Video-based-Surgical-Phase-Recognition"><a href="#TUNeS-A-Temporal-U-Net-with-Self-Attention-for-Video-based-Surgical-Phase-Recognition" class="headerlink" title="TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical Phase Recognition"></a>TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical Phase Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09997">http://arxiv.org/abs/2307.09997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isabel Funke, Dominik Rivoir, Stefanie Krell, Stefanie Speidel</li>
<li>for: 这个论文的目的是提供一种基于视频流的自适应计算机助手，以便在未来的手术室中提高操作效率和准确性。</li>
<li>methods: 本论文使用的方法包括视频流中提取有用特征和模型时间信息的技术，以及使用注意力机制来捕捉长距离依赖关系。特别是，该论文提出了一种新的注意力模型，即TUNeS，该模型不需要局部注意力或注意力权重补做。</li>
<li>results: 在实验中，所有的时间模型都表现出了在使用长时间文本段训练的特征提取器时的改善，而TUNeS得到了state-of-the-art的结果在Cholec80 dataset上。<details>
<summary>Abstract</summary>
To enable context-aware computer assistance in the operating room of the future, cognitive systems need to understand automatically which surgical phase is being performed by the medical team. The primary source of information for surgical phase recognition is typically video, which presents two challenges: extracting meaningful features from the video stream and effectively modeling temporal information in the sequence of visual features. For temporal modeling, attention mechanisms have gained popularity due to their ability to capture long-range dependencies. In this paper, we explore design choices for attention in existing temporal models for surgical phase recognition and propose a novel approach that does not resort to local attention or regularization of attention weights: TUNeS is an efficient and simple temporal model that incorporates self-attention at the coarsest stage of a U-Net-like structure. In addition, we propose to train the feature extractor, a standard CNN, together with an LSTM on preferably long video segments, i.e., with long temporal context. In our experiments, all temporal models performed better on top of feature extractors that were trained with longer temporal context. On top of these contextualized features, TUNeS achieves state-of-the-art results on Cholec80.
</details>
<details>
<summary>摘要</summary>
为实现智能医疗室的计算机助手功能，�gnitive系统需要自动地理解医疗团队在执行哪一个手术阶段。视频是主要的信息来源，它们存在两个挑战：从视频流中提取有用的特征和有效地模型视频流中的时间信息。为了处理时间信息，注意机制在模型中得到了广泛的应用，因为它们可以捕捉长距离依赖关系。在这篇论文中，我们探讨了现有的 temporal 模型中的注意机制设计选择，并提出了一种新的方法，它不需要本地注意力或注意力权重规则化：TUNeS 是一种高效和简单的时间模型，它在 U-Net  like 结构中使用自注意。此外，我们提议在可能长的视频段上训练特征提取器，即标准的 CNN，并与 LSTM 一起训练。在我们的实验中，所有的时间模型都在使用更长的时间上下文下表现更好。在这些上下文化特征上，TUNeS 实现了 Cholect80 的状态当前结果。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Disentanglement-on-Pruning-Neural-Networks"><a href="#Impact-of-Disentanglement-on-Pruning-Neural-Networks" class="headerlink" title="Impact of Disentanglement on Pruning Neural Networks"></a>Impact of Disentanglement on Pruning Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09994">http://arxiv.org/abs/2307.09994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carl Shneider, Peyman Rostami, Anis Kacem, Nilotpal Sinha, Abd El Rahman Shabayek, Djamila Aouada</li>
<li>for: 这篇论文的目的是为了实现深度学习神经网络在边缘设备上的部署，并且降低其内存占用量、电力消耗和延迟时间。</li>
<li>methods: 本论文使用了β-VAE框架，与标准的条件给出来进行删除，以 Investigate 将网络训练为了学习分离表示的影响。</li>
<li>results: 在 MNIST 和 CIFAR10 数据集上实验，发现 forcing 网络学习分离表示可以增强删除的效果，并且提出了未来工作的道路。<details>
<summary>Abstract</summary>
Deploying deep learning neural networks on edge devices, to accomplish task specific objectives in the real-world, requires a reduction in their memory footprint, power consumption, and latency. This can be realized via efficient model compression. Disentangled latent representations produced by variational autoencoder (VAE) networks are a promising approach for achieving model compression because they mainly retain task-specific information, discarding useless information for the task at hand. We make use of the Beta-VAE framework combined with a standard criterion for pruning to investigate the impact of forcing the network to learn disentangled representations on the pruning process for the task of classification. In particular, we perform experiments on MNIST and CIFAR10 datasets, examine disentanglement challenges, and propose a path forward for future works.
</details>
<details>
<summary>摘要</summary>
部署深度学习神经网络在边缘设备上，以完成实际场景中的任务特定目标，需要减少其内存占用量、能耗和延迟。这可以通过有效的模型压缩来实现。 variational autoencoder（VAE）网络生成的分离的干扰表示可能是实现模型压缩的有效方法，因为它们主要保留任务特定的信息，抛弃无关于任务的信息。我们使用β-VAE框架与标准的束缚方法进行调研，研究在分类任务上强制网络学习分离表示的影响。在特定的MNIST和CIFAR10数据集上进行实验，检查分离挑战，并提出未来工作的路径。
</details></li>
</ul>
<hr>
<h2 id="TinyTrain-Deep-Neural-Network-Training-at-the-Extreme-Edge"><a href="#TinyTrain-Deep-Neural-Network-Training-at-the-Extreme-Edge" class="headerlink" title="TinyTrain: Deep Neural Network Training at the Extreme Edge"></a>TinyTrain: Deep Neural Network Training at the Extreme Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09988">http://arxiv.org/abs/2307.09988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young D. Kwon, Rui Li, Stylianos I. Venieris, Jagmohan Chauhan, Nicholas D. Lane, Cecilia Mascolo</li>
<li>For: 这个研究旨在提高 edge 设备上的内存和 compute 资源限制下的人工智能模型训练效率，以满足用户个性化和隐私需求。* Methods: 本研究提出了 TinyTrain，一个在 edge 设备上进行内存和 compute 资源有限的模型训练方法，通过选择ively 更新模型部分，以及对于数据稀缺的特殊处理，以获得高精度的结果。* Results: TinyTrain 比 vanilla fine-tuning 提高 3.6-5.0% 的精度，同时将backward-pass 内存和计算成本降低到 2,286 倍和 7.68 倍，实现了快速、能效地进行模型训练。<details>
<summary>Abstract</summary>
On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\geq$10\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0\% in accuracy, while reducing the backward-pass memory and computation cost by up to 2,286$\times$ and 7.68$\times$, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5$\times$ faster and 3.5$\times$ more energy-efficient training over status-quo approaches, and 2.8$\times$ smaller memory footprint than SOTA approaches, while remaining within the 1 MB memory envelope of MCU-grade platforms.
</details>
<details>
<summary>摘要</summary>
“设备内训练是用户个性化和隐私的关键。随着互联网物联网设备和微控制器单元（MCU）的普及，这个任务变得更加挑战，因为这些设备具有有限的内存和计算资源，同时有限的标注用户数据。然而，先前的研究忽视了数据稀缺问题，需要过长的训练时间（例如几个小时），或者导致显著的准确性损失（大于10%）。我们提出了TinyTrain，一种在设备内训练方法，可以减少训练时间，并且特别处理数据稀缺问题。TinyTrain引入了任务适应的稀缺更新方法，可以动态选择层/通道，根据多目标 criterion，同时考虑用户数据、内存和计算能力。这使得TinyTrain在未seen任务上保持高准确性，同时减少了后向传输的内存和计算成本。相比普通精度调整，TinyTrain提高了3.6-5.0%的准确性，同时减少了反向传输的内存和计算成本。TinyTrain针对广泛使用的现实世界边缘设备，实现了9.5倍快速训练和3.5倍更高能效训练，同时减少了2.8倍的内存占用。”
</details></li>
</ul>
<hr>
<h2 id="Lazy-Visual-Localization-via-Motion-Averaging"><a href="#Lazy-Visual-Localization-via-Motion-Averaging" class="headerlink" title="Lazy Visual Localization via Motion Averaging"></a>Lazy Visual Localization via Motion Averaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09981">http://arxiv.org/abs/2307.09981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyan Dong, Shaohui Liu, Hengkai Guo, Baoquan Chen, Marc Pollefeys</li>
<li>for: 本研究的目的是提出一种基于图像的视觉地理位置系统，以便实现高精度的地理位置估计。</li>
<li>methods: 该方法不需要构建3D metric maps，而是通过对数据库图像和查询图像的运动平均来实现高精度的地理位置估计。</li>
<li>results: 实验结果显示，该方法可以与现有的结构基本方法相比，达到同等或更高的地理位置准确率。此外，该方法还可以轻松扩展到处理复杂的配置，如多个查询图像的协同地理位置估计和摄像机架。<details>
<summary>Abstract</summary>
Visual (re)localization is critical for various applications in computer vision and robotics. Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed database images. Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database with structure-from-motion, or implicitly encode the 3D information with scene coordinate regression models. On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits. It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remaining unaffected by imperfect reconstruction, etc. In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructing the scene from the database. The key to achieving this owes to a tailored motion averaging over database-query pairs. Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-art structure-based methods. Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such as multi-query co-localization and camera rigs.
</details>
<details>
<summary>摘要</summary>
<SYS>将文本翻译为简化中文。</SYS>计算机视觉和 робо技术中的视觉（重）本地化是非常重要的。其目标是根据每个查询图像，估算相对于数据库图像的6个自由度（DoF）摄像头姿。现在，所有领先的解决方案都是结构基的，即从数据库中构建3D métric 地图，或者隐式地将3D信息编码到场景坐标回归模型中。然而，不需要重建场景的视觉本地化具有明显的优点。它使得部署更加便捷，降低数据库预处理时间，释放存储要求，并不受不完全重建等等的影响。在这份技术报告中，我们示出了可以达到高地位精度无需重建场景的可能性。关键在于对数据库-查询对的特制运动平均。实验表明，我们的视觉本地化提案，懒散寻Localization（LazyLoc）可以与当前领先的结构基方法相比，达到相似的性能。此外，我们还展示了 LazyLoc 的灵活性，可以轻松扩展到处理复杂配置，如多个查询均在和相机套件。Note: "简化中文" refers to Simplified Chinese, which is one of the two standardized Chinese writing systems, used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="ProtoCaps-A-Fast-and-Non-Iterative-Capsule-Network-Routing-Method"><a href="#ProtoCaps-A-Fast-and-Non-Iterative-Capsule-Network-Routing-Method" class="headerlink" title="ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method"></a>ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09944">http://arxiv.org/abs/2307.09944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miles Everett, Mingjun Zhong, Georgios Leontidis</li>
<li>for: 提高卷积网络的运算效率和表现力</li>
<li>methods: 提出了一种新的非迭代路由机制，以及使用可调prototype集成的方法</li>
<li>results: 比对 current best non-iterative Capsule Network 的结果，在Imagewoof dataset上达到了更高的性能Translation:</li>
<li>for: To enhance the operational efficiency and performance of Capsule Networks</li>
<li>methods: Proposed a novel, non-iterative routing mechanism and use of trainable prototype clustering</li>
<li>results: Achieved superior results compared to the current best non-iterative Capsule Network on the Imagewoof dataset, which is too computationally demanding to handle efficiently by iterative approaches.<details>
<summary>Abstract</summary>
Capsule Networks have emerged as a powerful class of deep learning architectures, known for robust performance with relatively few parameters compared to Convolutional Neural Networks (CNNs). However, their inherent efficiency is often overshadowed by their slow, iterative routing mechanisms which establish connections between Capsule layers, posing computational challenges resulting in an inability to scale. In this paper, we introduce a novel, non-iterative routing mechanism, inspired by trainable prototype clustering. This innovative approach aims to mitigate computational complexity, while retaining, if not enhancing, performance efficacy. Furthermore, we harness a shared Capsule subspace, negating the need to project each lower-level Capsule to each higher-level Capsule, thereby significantly reducing memory requisites during training. Our approach demonstrates superior results compared to the current best non-iterative Capsule Network and tests on the Imagewoof dataset, which is too computationally demanding to handle efficiently by iterative approaches. Our findings underscore the potential of our proposed methodology in enhancing the operational efficiency and performance of Capsule Networks, paving the way for their application in increasingly complex computational scenarios.
</details>
<details>
<summary>摘要</summary>
干燥网络（Capsule Networks）已经成为深度学习领域的一种强大的架构，知名于相对于卷积神经网络（CNNs）的Parameters少，但它们的内置效率经常被迅速的迭代路由机制所掩蔽，导致不能扩展。在这篇论文中，我们提出了一种新的非迭代路由机制，取得了trainable prototype clustering的灵感。这种创新的方法旨在降低计算复杂性，保持或者提高性能效率。另外，我们利用共享干燥子空间，从而废弃每层下级干燥向每层上级干燥的投影，从而在训练时大幅减少内存需求。我们的方法在Imagewoof数据集上测试，这个数据集是迅速的迭代方法无法处理的计算强度。我们的发现表明了我们提出的方法在提高干燥网络的运行效率和性能的潜在可能性，为其应用在越来越复杂的计算场景铺开了道路。
</details></li>
</ul>
<hr>
<h2 id="AGAR-Attention-Graph-RNN-for-Adaptative-Motion-Prediction-of-Point-Clouds-of-Deformable-Objects"><a href="#AGAR-Attention-Graph-RNN-for-Adaptative-Motion-Prediction-of-Point-Clouds-of-Deformable-Objects" class="headerlink" title="AGAR: Attention Graph-RNN for Adaptative Motion Prediction of Point Clouds of Deformable Objects"></a>AGAR: Attention Graph-RNN for Adaptative Motion Prediction of Point Clouds of Deformable Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09936">http://arxiv.org/abs/2307.09936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Gomes, Silvia Rossi, Laura Toni</li>
<li>for: 本研究针对三维点 cloud sequence中的弹性物体动作（如人体动作）进行动作预测，探讨这种表示方式中的困难和复杂动作所带来的技术限制。</li>
<li>methods: 我们提出一个改进的建筑方案，专门针对弹性形状和复杂动作的点 cloud prediction。 Specifically, 我们使用一个基于图的方法，学习和利用点 cloud 的空间结构，以提取更有表示力的特征。然后，我们提出一个适应Module，能够合理地调整本地和全局运动，以更好地模型弹性三维点 cloud 中的动作。</li>
<li>results: 我们对以下数据集进行试验：MNIST moving digits、Mixamo human bodies motions、JPEG和CWIPC-SXR real-world dynamic bodies。结果显示，我们的方法在模型复杂动作方面表现更好，并且能够保持点 cloud 的形状。此外，我们还证明了我们的框架在动态特征学习中的一般化能力，通过将框架应用于MSRAction3D数据集，并 achieved results on-par with state-of-the-art methods。<details>
<summary>Abstract</summary>
This paper focuses on motion prediction for point cloud sequences in the challenging case of deformable 3D objects, such as human body motion. First, we investigate the challenges caused by deformable shapes and complex motions present in this type of representation, with the ultimate goal of understanding the technical limitations of state-of-the-art models. From this understanding, we propose an improved architecture for point cloud prediction of deformable 3D objects. Specifically, to handle deformable shapes, we propose a graph-based approach that learns and exploits the spatial structure of point clouds to extract more representative features. Then we propose a module able to combine the learned features in an adaptative manner according to the point cloud movements. The proposed adaptative module controls the composition of local and global motions for each point, enabling the network to model complex motions in deformable 3D objects more effectively. We tested the proposed method on the following datasets: MNIST moving digits, the Mixamo human bodies motions, JPEG and CWIPC-SXR real-world dynamic bodies. Simulation results demonstrate that our method outperforms the current baseline methods given its improved ability to model complex movements as well as preserve point cloud shape. Furthermore, we demonstrate the generalizability of the proposed framework for dynamic feature learning, by testing the framework for action recognition on the MSRAction3D dataset and achieving results on-par with state-of-the-art methods
</details>
<details>
<summary>摘要</summary>
To handle deformable shapes, the proposed method uses a graph-based approach that learns and exploits the spatial structure of point clouds to extract more representative features. Then, a module is proposed that combines the learned features in an adaptive manner according to the point cloud movements. This module controls the composition of local and global motions for each point, allowing the network to model complex motions in deformable 3D objects more effectively.The proposed method was tested on several datasets, including MNIST moving digits, Mixamo human bodies motions, JPEG, and CWIPC-SXR real-world dynamic bodies. The simulation results show that the proposed method outperforms current baseline methods in terms of its ability to model complex movements while preserving point cloud shape. Additionally, the proposed framework was tested for action recognition on the MSRAction3D dataset and achieved results on par with state-of-the-art methods, demonstrating its generalizability for dynamic feature learning.
</details></li>
</ul>
<hr>
<h2 id="DISA-DIfferentiable-Similarity-Approximation-for-Universal-Multimodal-Registration"><a href="#DISA-DIfferentiable-Similarity-Approximation-for-Universal-Multimodal-Registration" class="headerlink" title="DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration"></a>DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09931">http://arxiv.org/abs/2307.09931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imfusiongmbh/disa-universal-multimodal-registration">https://github.com/imfusiongmbh/disa-universal-multimodal-registration</a></li>
<li>paper_authors: Matteo Ronchetti, Wolfgang Wein, Nassir Navab, Oliver Zettinig, Raphael Prevost</li>
<li>for: 这篇论文的目的是为了提出一种能够快速、灵活地进行多Modal imaging的图像注册方法。</li>
<li>methods: 该方法使用了小型卷积神经网络（CNN）来approximate现有的相似度度量，从而使得注册过程变得更加快速和灵活。</li>
<li>results: 实验结果表明，该方法可以在不同的生物学Dataset上进行广泛的应用，并且不需要特殊的重新训练。此外，该方法比传统的局部patch基本metric更快速，并且可以在临床设置中直接应用。<details>
<summary>Abstract</summary>
Multimodal image registration is a challenging but essential step for numerous image-guided procedures. Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities. Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings. We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration. We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data. Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one. Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining. We make our training code and data publicly available.
</details>
<details>
<summary>摘要</summary>
多modal图像匹配是一项复杂但必要的步骤，用于许多图像引导过程。大多数匹配算法依靠计算复杂，通常不导 differentiable的相似度度量来处理不同模态图像之间的形态差异。现代机器学习基于的方法受到特定的解剖学-模态组合的限制，并不能扩展到新的设置。我们提议一种通用框架，用于生成表达力强的跨模态描述符，以实现快速的扭形全局匹配。我们通过将现有的度量简化为投影在小的卷积神经网络（CNN）的特征空间中的点积，实现了快速、可训练的dot product。我们的方法比局部小区域度量更快速，可以直接应用于临床设置，只需要将相似度度量替换为我们提议的度量即可。我们在三个不同的数据集上进行了实验，结果表明我们的方法可以跨模态数据集上泛化良好，无需特殊的再训练。我们将训练代码和数据公开发布。
</details></li>
</ul>
<hr>
<h2 id="Measuring-and-Modeling-Uncertainty-Degree-for-Monocular-Depth-Estimation"><a href="#Measuring-and-Modeling-Uncertainty-Degree-for-Monocular-Depth-Estimation" class="headerlink" title="Measuring and Modeling Uncertainty Degree for Monocular Depth Estimation"></a>Measuring and Modeling Uncertainty Degree for Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09929">http://arxiv.org/abs/2307.09929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mochu Xiang, Jing Zhang, Nick Barnes, Yuchao Dai</li>
<li>for: This paper aims to improve the reliability of monocular depth estimation (MDE) models by modeling uncertainty from the perspective of inherent probability distributions.</li>
<li>methods: The proposed method introduces additional training regularization terms to estimate uncertainty more fairly and with more comprehensive metrics, without requiring extra modules or multiple inferences.</li>
<li>results: The proposed method provides state-of-the-art reliability in uncertainty estimations, and can be further improved when combined with ensemble or sampling methods, as demonstrated in a series of experiments.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是提高单目深度估计（MDE）模型的可靠性，通过来自深度概率体 Volume 和其扩展的内在概率分布的视角来模型不确定性。</li>
<li>methods: 提议的方法通过添加额外训练规则来更加公正地估计不确定性，不需要额外模块或多个推理。</li>
<li>results: 提议的方法可以提供state-of-the-art的可靠性在不确定性估计中，并可以通过ensemble或采样方法进一步改进。<details>
<summary>Abstract</summary>
Effectively measuring and modeling the reliability of a trained model is essential to the real-world deployment of monocular depth estimation (MDE) models. However, the intrinsic ill-posedness and ordinal-sensitive nature of MDE pose major challenges to the estimation of uncertainty degree of the trained models. On the one hand, utilizing current uncertainty modeling methods may increase memory consumption and are usually time-consuming. On the other hand, measuring the uncertainty based on model accuracy can also be problematic, where uncertainty reliability and prediction accuracy are not well decoupled. In this paper, we propose to model the uncertainty of MDE models from the perspective of the inherent probability distributions originating from the depth probability volume and its extensions, and to assess it more fairly with more comprehensive metrics. By simply introducing additional training regularization terms, our model, with surprisingly simple formations and without requiring extra modules or multiple inferences, can provide uncertainty estimations with state-of-the-art reliability, and can be further improved when combined with ensemble or sampling methods. A series of experiments demonstrate the effectiveness of our methods.
</details>
<details>
<summary>摘要</summary>
必须有效地测量和模型训练过的模型的可靠性，以便在实际应用中部署单目深度估计（MDE）模型。然而，MDE的内在缺失和排序敏感性带来了训练模型的不确定度的估计的主要挑战。一方面，使用现有的不确定度模型方法可能会增加内存占用和时间消耗。另一方面，基于模型准确率来估计uncertainty的方法也有问题，因为不确定度可靠性和预测精度不是分离的。在这篇论文中，我们提议从深度概率分布和其扩展的视角来模型MDE模型的不确定度，并使用更全面的指标来评估其可靠性。通过简单地添加额外训练正则化项，我们的模型可以提供高度可靠的不确定度估计，并可以通过 ensemble或抽样方法进行进一步改进。一系列实验证明了我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Embedded-Heterogeneous-Attention-Transformer-for-Cross-lingual-Image-Captioning"><a href="#Embedded-Heterogeneous-Attention-Transformer-for-Cross-lingual-Image-Captioning" class="headerlink" title="Embedded Heterogeneous Attention Transformer for Cross-lingual Image Captioning"></a>Embedded Heterogeneous Attention Transformer for Cross-lingual Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09915">http://arxiv.org/abs/2307.09915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Song, Zhenzhen Hu, Richang Hong</li>
<li>for: 这个论文旨在解决跨语言图像描述任务中的跨语言和跨modal挑战，以建立跨语言和不同modal之间的关联。</li>
<li>methods: 我们使用多元网络来建立跨领域关系和图像和不同语言之间的本地匹配。我们的方法包括穿梭隐藏式多元注意力（MHCA）、多元注意力推理网络（HARN）和多元共注意（HCA）。</li>
<li>results: 我们的方法在MSCOCO数据集上进行英文和中文图像描述任务，和先进的单语言方法相比，获得更好的结果。<details>
<summary>Abstract</summary>
Cross-lingual image captioning is confronted with both cross-lingual and cross-modal challenges for multimedia analysis. The crucial issue in this task is to model the global and local matching between the image and different languages. Existing cross-modal embedding methods based on Transformer architecture oversight the local matching between the image region and monolingual words, not to mention in the face of a variety of differentiated languages. Due to the heterogeneous property of the cross-modal and cross-lingual task, we utilize the heterogeneous network to establish cross-domain relationships and the local correspondences between the image and different languages. In this paper, we propose an Embedded Heterogeneous Attention Transformer (EHAT) to build reasoning paths bridging cross-domain for cross-lingual image captioning and integrate into transformer. The proposed EHAT consists of a Masked Heterogeneous Cross-attention (MHCA), Heterogeneous Attention Reasoning Network (HARN) and Heterogeneous Co-attention (HCA). HARN as the core network, models and infers cross-domain relationship anchored by vision bounding box representation features to connect two languages word features and learn the heterogeneous maps. MHCA and HCA implement cross-domain integration in the encoder through the special heterogeneous attention and enable single model to generate two language captioning. We test on MSCOCO dataset to generate English and Chinese, which are most widely used and have obvious difference between their language families. Our experiments show that our method even achieve better than advanced monolingual methods.
</details>
<details>
<summary>摘要</summary>
cross-lingual image captioning 面临着跨语言和跨模态挑战，以实现多媒体分析。这个任务的关键问题在于模型图像和不同语言之间的全球和本地匹配。现有的跨Modal embedding方法基于Transformer架构，仅考虑图像区域和单一语言词汇之间的本地匹配，而不考虑不同语言之间的差异。由于跨Modal和跨语言任务的异ogeneous性，我们利用异ogeneous网络建立跨Domains关系和图像与不同语言之间的本地匹配。在这篇论文中，我们提出一种Embedded Heterogeneous Attention Transformer（EHAT），用于建立跨Domains的理解路径，并将其集成到Transformer中。EHAT包括Masked Heterogeneous Cross-attention（MHCA）、Heterogeneous Attention Reasoning Network（HARN）和Heterogeneous Co-attention（HCA）。HARN作为核心网络，通过视觉矩形框特征来连接两种语言词汇，并学习异ogeneous地图。MHCA和HCA在编码器中实现跨Domain集成，通过特殊的异ogeneous注意力，使单个模型可以生成两种语言描述。我们在MSCOCO数据集上进行测试，生成英语和中文，这两种语言是最常用的，而且它们的语言家族具有明显的差异。我们的实验结果表明，我们的方法可以超越先进的单语言方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Abstract-Images-on-the-Importance-of-Occlusion-in-a-Minimalist-Encoding-of-Human-Poses"><a href="#Learning-from-Abstract-Images-on-the-Importance-of-Occlusion-in-a-Minimalist-Encoding-of-Human-Poses" class="headerlink" title="Learning from Abstract Images: on the Importance of Occlusion in a Minimalist Encoding of Human Poses"></a>Learning from Abstract Images: on the Importance of Occlusion in a Minimalist Encoding of Human Poses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09893">http://arxiv.org/abs/2307.09893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saad Manzur, Wayne Hayes</li>
<li>for: 提高2D-to-3Dpose lifting网络的跨数据集性能。</li>
<li>methods: 使用透明3D臂提供 occlusion信息，并隐式地编码关节位置。</li>
<li>results: 在不使用部件地图的情况下，通过在抽象的synthetic图像上进行训练，在多个视角下获得了优化的同dataset benchmark表现，以及在跨数据集 benchmark中的“量子跳跃”性提升。<details>
<summary>Abstract</summary>
Existing 2D-to-3D pose lifting networks suffer from poor performance in cross-dataset benchmarks. Although the use of 2D keypoints joined by "stick-figure" limbs has shown promise as an intermediate step, stick-figures do not account for occlusion information that is often inherent in an image. In this paper, we propose a novel representation using opaque 3D limbs that preserves occlusion information while implicitly encoding joint locations. Crucially, when training on data with accurate three-dimensional keypoints and without part-maps, this representation allows training on abstract synthetic images, with occlusion, from as many synthetic viewpoints as desired. The result is a pose defined by limb angles rather than joint positions $\unicode{x2013}$ because poses are, in the real world, independent of cameras $\unicode{x2013}$ allowing us to predict poses that are completely independent of camera viewpoint. The result provides not only an improvement in same-dataset benchmarks, but a "quantum leap" in cross-dataset benchmarks.
</details>
<details>
<summary>摘要</summary>
现有的2D-to-3D姿态提升网络受到跨数据集benchmark的表现不佳。虽然使用2D关键点并由“棒拌”肢体连接的“棒拌”limbs表示方法具有潜在的优势，但棒拌limbs并不考虑图像中的 occlusion 信息。在本文中，我们提出了一种新的表示方法，使用透明的3D肢体来保留 occlusion 信息，同时隐式地编码关节位置。在没有部分地图的情况下，在训练时使用抽象的 sintetic 图像，包括 occlusion，从任意 sintetic 视角训练。这种表示方法使得姿态定义由肢体角度而不是关节位置，因为姿态在实际世界中是独立于摄像头的。这种提出的方法不仅在同一个数据集上的benchmark中提高表现，而且在跨数据集benchmark中做出了“量子跃进”。
</details></li>
</ul>
<hr>
<h2 id="3Deformer-A-Common-Framework-for-Image-Guided-Mesh-Deformation"><a href="#3Deformer-A-Common-Framework-for-Image-Guided-Mesh-Deformation" class="headerlink" title="3Deformer: A Common Framework for Image-Guided Mesh Deformation"></a>3Deformer: A Common Framework for Image-Guided Mesh Deformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09892">http://arxiv.org/abs/2307.09892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Su, Xuefeng Liu, Jianwei Niu, Ji Wan, Xinghao Wu</li>
<li>for: 三维形状编辑框架的开发</li>
<li>methods: 基于异常 Renderer 技术，通过 semantic 图像和 mesh 材质之间的对应关系来进行形状编辑，并通过层次优化架构和多种策略和损失函数来保证形状精度、表面光滑、几何稳定和全局同步等特性。</li>
<li>results: 在评估实验中，提出的 3Deformer 能够生成出吸引人的结果，并达到了现状计算机视觉领域的先进水平。<details>
<summary>Abstract</summary>
We propose 3Deformer, a general-purpose framework for interactive 3D shape editing. Given a source 3D mesh with semantic materials, and a user-specified semantic image, 3Deformer can accurately edit the source mesh following the shape guidance of the semantic image, while preserving the source topology as rigid as possible. Recent studies of 3D shape editing mostly focus on learning neural networks to predict 3D shapes, which requires high-cost 3D training datasets and is limited to handling objects involved in the datasets. Unlike these studies, our 3Deformer is a non-training and common framework, which only requires supervision of readily-available semantic images, and is compatible with editing various objects unlimited by datasets. In 3Deformer, the source mesh is deformed utilizing the differentiable renderer technique, according to the correspondences between semantic images and mesh materials. However, guiding complex 3D shapes with a simple 2D image incurs extra challenges, that is, the deform accuracy, surface smoothness, geometric rigidity, and global synchronization of the edited mesh should be guaranteed. To address these challenges, we propose a hierarchical optimization architecture to balance the global and local shape features, and propose further various strategies and losses to improve properties of accuracy, smoothness, rigidity, and so on. Extensive experiments show that our 3Deformer is able to produce impressive results and reaches the state-of-the-art level.
</details>
<details>
<summary>摘要</summary>
我们提出了3Deformer，一个通用的交互式3D形状编辑框架。从源3D网格中提取 semantic materials，并使用用户指定的semantic图像作为形状指南，3Deformer可以准确地编辑源网格，同时保持原始结构的尺寸一致性。现有的3D形状编辑研究主要集中在学习神经网络预测3D形状，这需要高成本的3D训练数据集和对特定对象的限制。与这些研究不同，我们的3Deformer是一个不需要训练的通用框架，只需要 readily-available semantic图像的监督，并且可以编辑多种物体，不受数据集的限制。在3Deformer中，源网格通过 differentiable renderer 技术进行扭曲，根据 semantic图像和网格材质的对应关系。然而，将复杂的3D形状指导于简单的2D图像存在一些挑战，即保证扭曲精度、表面光滑、几何坚定性和编辑后的 mesh 的全局同步。为解决这些挑战，我们提出了层次优化架构，以平衡全局和局部形状特征，并提出了多种策略和损失来提高精度、光滑、坚定性等属性。我们的3Deformer在广泛的实验中能够 prodcue 出卓越的结果，达到了状态的末点水平。
</details></li>
</ul>
<hr>
<h2 id="A3D-Adaptive-Accurate-and-Autonomous-Navigation-for-Edge-Assisted-Drones"><a href="#A3D-Adaptive-Accurate-and-Autonomous-Navigation-for-Edge-Assisted-Drones" class="headerlink" title="A3D: Adaptive, Accurate, and Autonomous Navigation for Edge-Assisted Drones"></a>A3D: Adaptive, Accurate, and Autonomous Navigation for Edge-Assisted Drones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09880">http://arxiv.org/abs/2307.09880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liekang Zeng, Haowei Chen, Daipeng Feng, Xiaoxi Zhang, Xu Chen</li>
<li>for: 提高自适应无人机导航的精度和安全性</li>
<li>methods: 使用深度神经网络和动态调整任务执行位置、输入分辨率和图像压缩比例来降低推理延迟和提高预测精度</li>
<li>results: 实现了28.06%的终端延迟减少和27.28%的飞行距离增加，比非适应解决方案更高效<details>
<summary>Abstract</summary>
Accurate navigation is of paramount importance to ensure flight safety and efficiency for autonomous drones. Recent research starts to use Deep Neural Networks to enhance drone navigation given their remarkable predictive capability for visual perception. However, existing solutions either run DNN inference tasks on drones in situ, impeded by the limited onboard resource, or offload the computation to external servers which may incur large network latency. Few works consider jointly optimizing the offloading decisions along with image transmission configurations and adapting them on the fly. In this paper, we propose A3D, an edge server assisted drone navigation framework that can dynamically adjust task execution location, input resolution, and image compression ratio in order to achieve low inference latency, high prediction accuracy, and long flight distances. Specifically, we first augment state-of-the-art convolutional neural networks for drone navigation and define a novel metric called Quality of Navigation as our optimization objective which can effectively capture the above goals. We then design a deep reinforcement learning based neural scheduler at the drone side for which an information encoder is devised to reshape the state features and thus improve its learning ability. To further support simultaneous multi-drone serving, we extend the edge server design by developing a network-aware resource allocation algorithm, which allows provisioning containerized resources aligned with drones' demand. We finally implement a proof-of-concept prototype with realistic devices and validate its performance in a real-world campus scene, as well as a simulation environment for thorough evaluation upon AirSim. Extensive experimental results show that A3D can reduce end-to-end latency by 28.06% and extend the flight distance by up to 27.28% compared with non-adaptive solutions.
</details>
<details>
<summary>摘要</summary>
准确的导航是无人机安全和效率的关键。 latest research 使用深度神经网络来提高无人机导航，因为它们具有remarkable predictive capability for visual perception。 However, existing solutions either run DNN inference tasks on drones in situ, impeded by limited onboard resources, or offload the computation to external servers, which may incur large network latency. Few works consider jointly optimizing the offloading decisions along with image transmission configurations and adapting them on the fly.In this paper, we propose A3D, an edge server-assisted drone navigation framework that can dynamically adjust task execution location, input resolution, and image compression ratio to achieve low inference latency, high prediction accuracy, and long flight distances. Specifically, we first augment state-of-the-art convolutional neural networks for drone navigation and define a novel metric called Quality of Navigation, which can effectively capture the above goals. We then design a deep reinforcement learning-based neural scheduler at the drone side, which uses an information encoder to reshape the state features and improve its learning ability. To support simultaneous multi-drone serving, we extend the edge server design by developing a network-aware resource allocation algorithm, which allows provisioning containerized resources aligned with drones' demand. We finally implement a proof-of-concept prototype with realistic devices and validate its performance in a real-world campus scene and a simulation environment for thorough evaluation upon AirSim.Experimental results show that A3D can reduce end-to-end latency by 28.06% and extend the flight distance by up to 27.28% compared with non-adaptive solutions.
</details></li>
</ul>
<hr>
<h2 id="BSDM-Background-Suppression-Diffusion-Model-for-Hyperspectral-Anomaly-Detection"><a href="#BSDM-Background-Suppression-Diffusion-Model-for-Hyperspectral-Anomaly-Detection" class="headerlink" title="BSDM: Background Suppression Diffusion Model for Hyperspectral Anomaly Detection"></a>BSDM: Background Suppression Diffusion Model for Hyperspectral Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09861">http://arxiv.org/abs/2307.09861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/majitao-xd/bsdm-had">https://github.com/majitao-xd/bsdm-had</a></li>
<li>paper_authors: Jitao Ma, Weiying Xie, Yunsong Li, Leyuan Fang</li>
<li>For: 本研究旨在提高频谱异常检测（HAD）的性能， solves the problem of complex background in hyperspectral images (HSIs) without labeled samples.* Methods: 我们提出了一种新的方法，即background suppression diffusion model（BSDM）， which can simultaneously learn latent background distributions and generalize to different datasets.* Results: 我们的方法可以减少HSIs的背景干扰，提高HAD的性能，并且可以适应不同的数据集。 We demonstrate the effectiveness of our method through assessments and generalization experiments on several real HSI datasets.<details>
<summary>Abstract</summary>
Hyperspectral anomaly detection (HAD) is widely used in Earth observation and deep space exploration. A major challenge for HAD is the complex background of the input hyperspectral images (HSIs), resulting in anomalies confused in the background. On the other hand, the lack of labeled samples for HSIs leads to poor generalization of existing HAD methods. This paper starts the first attempt to study a new and generalizable background learning problem without labeled samples. We present a novel solution BSDM (background suppression diffusion model) for HAD, which can simultaneously learn latent background distributions and generalize to different datasets for suppressing complex background. It is featured in three aspects: (1) For the complex background of HSIs, we design pseudo background noise and learn the potential background distribution in it with a diffusion model (DM). (2) For the generalizability problem, we apply a statistical offset module so that the BSDM adapts to datasets of different domains without labeling samples. (3) For achieving background suppression, we innovatively improve the inference process of DM by feeding the original HSIs into the denoising network, which removes the background as noise. Our work paves a new background suppression way for HAD that can improve HAD performance without the prerequisite of manually labeled data. Assessments and generalization experiments of four HAD methods on several real HSI datasets demonstrate the above three unique properties of the proposed method. The code is available at https://github.com/majitao-xd/BSDM-HAD.
</details>
<details>
<summary>摘要</summary>
“几何 Spectral 异常检测（HAD）在地球观测和深空探索中广泛应用。然而，几何 Spectral 图像的复杂背景会导致异常难以分辨。同时，几何 Spectral 图像的标注样本缺乏，导致现有 HAD 方法的泛化差。本文开展了第一次不带标注样本的背景学习问题研究。我们提出了一种新的解决方案，即背景抑制算法（BSDM），可同时学习几何 Spectral 图像的潜在背景分布，并在不同领域的数据集上进行泛化。它具有以下三个特点：1. 对几何 Spectral 图像的复杂背景，我们设计了假背景噪音，并通过演化模型（DM）学习潜在背景分布。2. 对泛化问题，我们应用了统计偏移模块，使BSDM能够适应不同领域的数据集，无需标注样本。3. 为了实现背景抑制，我们创新地改进了 DM 的推理过程，通过将原始几何 Spectral 图像Feed into 杀噪网络，从而 removing 背景噪音。我们的工作开拓了一条新的背景抑制途径，可以提高 HAD 性能，不需要手动标注数据。评估和泛化实验表明，我们的方法在多个真实的几何 Spectral 图像 dataset 上具有三个独特的特点。代码可以在 <https://github.com/majitao-xd/BSDM-HAD> 中找到。”
</details></li>
</ul>
<hr>
<h2 id="Blind-Image-Quality-Assessment-Using-Multi-Stream-Architecture-with-Spatial-and-Channel-Attention"><a href="#Blind-Image-Quality-Assessment-Using-Multi-Stream-Architecture-with-Spatial-and-Channel-Attention" class="headerlink" title="Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention"></a>Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09857">http://arxiv.org/abs/2307.09857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Khalid, Nisar Ahmed</li>
<li>for: 本研究旨在提出一种基于多流空间和通道注意力的自适应图像质量评估算法，以提高图像质量评估的准确性和可靠性。</li>
<li>methods: 该算法首先使用两种不同的后处器生成混合特征，然后通过空间和通道注意力来提供高权重 dlaregion of interest。</li>
<li>results: 研究表明，该算法可以更加准确地评估图像质量，与人类感知评估呈高相关性。同时，该算法也能够在不同类型的图像质量评估 task 中具有优秀的一致性和泛化性。<details>
<summary>Abstract</summary>
BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically. Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions. Most algorithms generate quality without emphasizing the important region of interest. In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed. This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest. Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach. Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.
</details>
<details>
<summary>摘要</summary>
BIQA (无目标图像质量评估) 是一个重要的研究领域，用于自动评估图像质量。尽管有了很大的进步，但无目标图像质量评估仍然是一个困难的任务，因为图像的内容和扭曲都很多。大多数算法会生成质量，而不是强调关键的区域兴趣。为解决这个问题，我们提出了一种多流批处理和通道注意力基于的算法。这个算法可以将多种混合特征从两个不同的背景拼接起来，然后在空间和通道上进行注意力分配，以提高关键区域的权重，从而生成更加准确的预测结果，与人类感知评估呈现高相关性。我们使用了四个传统的图像质量评估数据集来验证我们的提议的有效性，并使用了真实和synthetic扭曲图像库来示示我们的方法的普适性。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Spatio-Temporal-Representation-Learning-for-Gait-Recognition"><a href="#Hierarchical-Spatio-Temporal-Representation-Learning-for-Gait-Recognition" class="headerlink" title="Hierarchical Spatio-Temporal Representation Learning for Gait Recognition"></a>Hierarchical Spatio-Temporal Representation Learning for Gait Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09856">http://arxiv.org/abs/2307.09856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Wang, Bo Liu, Fangfang Liang, Bincheng Wang</li>
<li>for: 本研究旨在提出一种基于层次空间-时间特征学习（HSTL）框架，用于提取走姿特征从粗到细。</li>
<li>methods: 该方法包括层次群集分析、自适应区域基于运动特征提取器（ARME）、层次特征映射（ASTP）和帧级时间下采样（FTA）等多个模块。</li>
<li>results: 对于CASIA-B、OUMVLP、GREW和Gait3D等数据集，该方法比 estado-of-the-art 高效，同时保持模型精度和复杂度的平衡。<details>
<summary>Abstract</summary>
Gait recognition is a biometric technique that identifies individuals by their unique walking styles, which is suitable for unconstrained environments and has a wide range of applications. While current methods focus on exploiting body part-based representations, they often neglect the hierarchical dependencies between local motion patterns. In this paper, we propose a hierarchical spatio-temporal representation learning (HSTL) framework for extracting gait features from coarse to fine. Our framework starts with a hierarchical clustering analysis to recover multi-level body structures from the whole body to local details. Next, an adaptive region-based motion extractor (ARME) is designed to learn region-independent motion features. The proposed HSTL then stacks multiple ARMEs in a top-down manner, with each ARME corresponding to a specific partition level of the hierarchy. An adaptive spatio-temporal pooling (ASTP) module is used to capture gait features at different levels of detail to perform hierarchical feature mapping. Finally, a frame-level temporal aggregation (FTA) module is employed to reduce redundant information in gait sequences through multi-scale temporal downsampling. Extensive experiments on CASIA-B, OUMVLP, GREW, and Gait3D datasets demonstrate that our method outperforms the state-of-the-art while maintaining a reasonable balance between model accuracy and complexity.
</details>
<details>
<summary>摘要</summary>
《坐姿识别方法》是一种人体特征识别技术，可以在无限制环境中识别个体，并且具有广泛的应用前景。现有的方法通常是基于身体部位特征的利用，但是它们经常忽略人体运动的层次关系。本文提出了一种层次空间时间学习（HSTL）框架，用于从粗到细提取坐姿特征。我们的框架开始于层次划分分析，以恢复全身到地方细节的多级体结构。接着，我们设计了一种适应区域基于运动特征Extractor（ARME），用于学习无关于地域的运动特征。我们的HSTL然后将多个ARME堆叠在一起，每个ARME都对应特定的层次分解级别。一种适应空间时间汇聚（ASTP）模块用于在不同级别上捕捉坐姿特征，以进行层次特征映射。最后，一种帧级时间抑制（FTA）模块用于通过多尺度时间抑制来减少坐姿序列中的重复信息。我们的方法在CASIA-B、OUMVLP、GREW和Gait3D数据集上进行了广泛的实验，结果表明我们的方法在精度和复杂性之间保持了良好的平衡，而且比现状态技术更高。
</details></li>
</ul>
<hr>
<h2 id="Cryo-forum-A-framework-for-orientation-recovery-with-uncertainty-measure-with-the-application-in-cryo-EM-image-analysis"><a href="#Cryo-forum-A-framework-for-orientation-recovery-with-uncertainty-measure-with-the-application-in-cryo-EM-image-analysis" class="headerlink" title="Cryo-forum: A framework for orientation recovery with uncertainty measure with the application in cryo-EM image analysis"></a>Cryo-forum: A framework for orientation recovery with uncertainty measure with the application in cryo-EM image analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09847">http://arxiv.org/abs/2307.09847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phonchi/cryo-forum">https://github.com/phonchi/cryo-forum</a></li>
<li>paper_authors: Szu-Chi Chung</li>
<li>for: 这篇论文的目的是解决单分子冰 embedding电子显微镜（cryo-EM）中Orientation parameter的有效决定问题，这个问题在3D结构重建中扮演着关键的角色，但由于数据含有噪声和异常值，因此需要耗费大量的时间进行2D clean-up处理。</li>
<li>methods: 这篇论文提出了一种新的方法，使用10维特征向量表示Orientation，并使用Quadratically-Constrained Quadratic Program来预测Orientation，并且添加了一个不确定度度量。此外，提出了一种新的损失函数，该损失函数考虑了方向之间的距离，从而提高了方法的准确性。</li>
<li>results: 数值分析表明，该方法可以从2D cryo-EM图像中有效地回归Orientation，并且可以直接从数据集中清理噪声。此外，包装了该方法的软件包在用户友好的方式提供，以便开发者可以轻松地使用。<details>
<summary>Abstract</summary>
In single-particle cryo-electron microscopy (cryo-EM), the efficient determination of orientation parameters for 2D projection images poses a significant challenge yet is crucial for reconstructing 3D structures. This task is complicated by the high noise levels present in the cryo-EM datasets, which often include outliers, necessitating several time-consuming 2D clean-up processes. Recently, solutions based on deep learning have emerged, offering a more streamlined approach to the traditionally laborious task of orientation estimation. These solutions often employ amortized inference, eliminating the need to estimate parameters individually for each image. However, these methods frequently overlook the presence of outliers and may not adequately concentrate on the components used within the network. This paper introduces a novel approach that uses a 10-dimensional feature vector to represent the orientation and applies a Quadratically-Constrained Quadratic Program to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric. Furthermore, we propose a unique loss function that considers the pairwise distances between orientations, thereby enhancing the accuracy of our method. Finally, we also comprehensively evaluate the design choices involved in constructing the encoder network, a topic that has not received sufficient attention in the literature. Our numerical analysis demonstrates that our methodology effectively recovers orientations from 2D cryo-EM images in an end-to-end manner. Importantly, the inclusion of uncertainty quantification allows for direct clean-up of the dataset at the 3D level. Lastly, we package our proposed methods into a user-friendly software suite named cryo-forum, designed for easy accessibility by the developers.
</details>
<details>
<summary>摘要</summary>
Single-particle cryo-electron microscopy (cryo-EM) 中的orientation parameter的效率确定是一个重要的挑战，但是它是重要的 для构建3D结构。这个任务受到高噪音水平的影响，包括异常值，因此需要许多时间consuming 2D clean-up 过程。最近，基于深度学习的解决方案出现了，它们提供了一种更加流畅的方法来传统上劳动密集的orientation estimation任务。这些解决方案通常采用总结式推理，从而消除了每个图像需要单独估计参数的需求。然而，这些方法可能忽视异常值的存在，并且可能不充分关注网络中使用的组件。本文介绍了一种新的方法，使用10维特征向量来表示orientation，并使用quadratically-constrained quadratic program来计算预测的orientation为单位旋转矩阵，并且附加了一个不确定度量。此外，我们还提出了一个新的损失函数，该函数考虑了 orientations之间的对称关系，从而提高了我们的方法的准确性。最后，我们还进行了对构建encoder网络的设计选择的完整评估，这是在文献中没有得到 suficient 的注意。我们的数值分析表明，我们的方法可以从2D cryo-EM图像中有效地回收orientation。更重要的是，由于包含不确定度量，我们的方法可以直接清理dataset at the 3D level。最后，我们将我们的提posed方法包装成一个 user-friendly 软件包，名为cryo-forum，用于方便开发者的使用。
</details></li>
</ul>
<hr>
<h2 id="Compressive-Image-Scanning-Microscope"><a href="#Compressive-Image-Scanning-Microscope" class="headerlink" title="Compressive Image Scanning Microscope"></a>Compressive Image Scanning Microscope</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09841">http://arxiv.org/abs/2307.09841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Gunalan, Marco Castello, Simonluca Piazza, Shunlei Li, Alberto Diaspro, Leonardo S. Mattos, Paolo Bianchini</li>
<li>for: 提高扫描镜观测镜像质量和减少数据收集时间</li>
<li>methods: 使用单 photon 触发冲击器（SPAD）数组探测器和固定抽象策略，通过并行扫描多个图像来提高压缩扫描镜图像质量</li>
<li>results: 实现高质量压缩扫描镜图像的生成，同时减少数据收集时间和降低谐振衰变的影响<details>
<summary>Abstract</summary>
We present a novel approach to implement compressive sensing in laser scanning microscopes (LSM), specifically in image scanning microscopy (ISM), using a single-photon avalanche diode (SPAD) array detector. Our method addresses two significant limitations in applying compressive sensing to LSM: the time to compute the sampling matrix and the quality of reconstructed images. We employ a fixed sampling strategy, skipping alternate rows and columns during data acquisition, which reduces the number of points scanned by a factor of four and eliminates the need to compute different sampling matrices. By exploiting the parallel images generated by the SPAD array, we improve the quality of the reconstructed compressive-ISM images compared to standard compressive confocal LSM images. Our results demonstrate the effectiveness of our approach in producing higher-quality images with reduced data acquisition time and potential benefits in reducing photobleaching.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于在激光扫描镜（LSM）中实现压缩感知，特别是在图像扫描镜（ISM）中使用单 photon 膨胀 диод（SPAD）数组探测器。我们的方法解决了应用压缩感知到 LSM 中的两个主要限制：计算抽象矩阵的时间和重建图像的质量。我们采用了固定抽象策略，在数据收集过程中跳过 alternate 行和列，从而将数据量减少为四分之一，并消除计算不同抽象矩阵的需求。通过利用 SPAD 数组生成的平行图像，我们提高了压缩-ISM 图像的重建质量，相比标准压缩干扫LSM图像。我们的结果表明，我们的方法可以生成高质量图像，并降低数据收集时间和可能减少照明损害。
</details></li>
</ul>
<hr>
<h2 id="What-do-neural-networks-learn-in-image-classification-A-frequency-shortcut-perspective"><a href="#What-do-neural-networks-learn-in-image-classification-A-frequency-shortcut-perspective" class="headerlink" title="What do neural networks learn in image classification? A frequency shortcut perspective"></a>What do neural networks learn in image classification? A frequency shortcut perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09829">http://arxiv.org/abs/2307.09829</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nis-research/nn-frequency-shortcuts">https://github.com/nis-research/nn-frequency-shortcuts</a></li>
<li>paper_authors: Shunxin Wang, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio</li>
<li>for: 这个研究旨在 investigate the mechanisms of representation learning in neural networks (NNs) for classification tasks, and expand our understanding of frequency shortcuts.</li>
<li>methods: 研究使用了 synthetic datasets 和 natural images，并提出了一个metric to measure class-wise frequency characteristics和一种方法来Identify frequency shortcuts.</li>
<li>results: 研究发现，NNs 在 classification 任务上倾向于学习简单的解决方案，并在训练中学习的频谱特征取决于最明显的频率带，这些频率带可以是低频或高频。此外，研究还发现了频谱短cuts可以是基于文本或形状的，取决于最好简化目标。最后，研究还 validate了频谱短cuts的可转移性。<details>
<summary>Abstract</summary>
Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transferability of frequency shortcuts on out-of-distribution (OOD) test sets. Our results suggest that frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation. We recommend that future research should focus on effective training schemes mitigating frequency shortcut learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>频率分析有助于理解神经网络（NN）的表征学习机制。大多数研究都集中在NN的回归任务上，而对于分类任务的研究则比较少。这项研究则是对后者的实证研究，并扩展了频率短cut的理解。我们首先在synthetic数据上进行实验，设计了具有不同频率频谱偏好的数据集。我们的结果表明，NNs tend to learn simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies.然后，我们在自然图像上进行了实验，并提出了一种类别频率特征的度量和一种可以识别频率短cut的方法。结果显示，频率短cut可以是基于文本的或形状的，它们取决于对象的简化目标。最后，我们 validate了频率短cut的传送性，并证明它们可以在不同数据集上传送，而且不能完全通过更大的模型容量和数据增强来避免。我们建议未来的研究应该关注有效地减少频率短cut学习的训练方案。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Learning-based-Prediction-for-Disease"><a href="#Multi-modal-Learning-based-Prediction-for-Disease" class="headerlink" title="Multi-modal Learning based Prediction for Disease"></a>Multi-modal Learning based Prediction for Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09823">http://arxiv.org/abs/2307.09823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/batuhankmkaraman/mlbasedad">https://github.com/batuhankmkaraman/mlbasedad</a></li>
<li>paper_authors: Yaran Chen, Xueyu Chen, Yu Han, Haoran Li, Dongbin Zhao, Jingzhong Li, Xu Wang</li>
<li>for: 这个研究旨在开发一个非侵入性的肝病诊断系统，以便预测肝病变化和进展，并避免进行侵入性的肝生検。</li>
<li>methods: 这个研究使用了一个多modal的学习方法，结合来自多个不同来源的资料，包括来自临床评估、实验室和成像等的资料，以及一些面部图像。</li>
<li>results: 研究发现，使用多modal学习方法可以提高肝病诊断的精度，并且可以使用面部图像来进行肝病诊断，这可以降低肝生検的风险和成本。<details>
<summary>Abstract</summary>
Non alcoholic fatty liver disease (NAFLD) is the most common cause of chronic liver disease, which can be predicted accurately to prevent advanced fibrosis and cirrhosis. While, a liver biopsy, the gold standard for NAFLD diagnosis, is invasive, expensive, and prone to sampling errors. Therefore, non-invasive studies are extremely promising, yet they are still in their infancy due to the lack of comprehensive research data and intelligent methods for multi-modal data. This paper proposes a NAFLD diagnosis system (DeepFLDDiag) combining a comprehensive clinical dataset (FLDData) and a multi-modal learning based NAFLD prediction method (DeepFLD). The dataset includes over 6000 participants physical examinations, laboratory and imaging studies, extensive questionnaires, and facial images of partial participants, which is comprehensive and valuable for clinical studies. From the dataset, we quantitatively analyze and select clinical metadata that most contribute to NAFLD prediction. Furthermore, the proposed DeepFLD, a deep neural network model designed to predict NAFLD using multi-modal input, including metadata and facial images, outperforms the approach that only uses metadata. Satisfactory performance is also verified on other unseen datasets. Inspiringly, DeepFLD can achieve competitive results using only facial images as input rather than metadata, paving the way for a more robust and simpler non-invasive NAFLD diagnosis.
</details>
<details>
<summary>摘要</summary>
非 алкого醉liver病 (NAFLD)是 chronic liver disease 最常见的原因，可以准确预测并防止进展到高度纤维化和 cirrhosis。然而，liver biopsy，用于 NAFLD 诊断的 golden standard，是侵入性的、昂贵的，并且容易出现采样错误。因此，不侵入性的研究非常有前途，但是目前还处于初级阶段，因为缺乏全面的研究数据和智能的多Modal数据处理方法。这篇文章提出了一种 combining 全面临床数据集 (FLDData) 和多Modal学习基于 NAFLD 预测方法 (DeepFLD) 的 NAFLD 诊断系统 (DeepFLDDiag)。该数据集包括6000多名参与者的 физиicalexamination，实验室和成像研究，广泛的问卷和一部分参与者的面部图像，这是丰富和有价值的临床数据。从数据集中，我们量化分析并选择了对 NAFLD 预测最有价值的临床 metadata。此外，我们提出的 DeepFLD，一种基于多Modal输入（包括 metadata 和面部图像）的深度神经网络模型，可以更好地预测 NAFLD。在其他未经见过数据上，我们也证明了它的满意性表现。吸引地，DeepFLD 可以使用只有面部图像作为输入，而不是 metadata，达到类似的预测性能，这为非侵入性 NAFLD 诊断开创了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="A-Siamese-based-Verification-System-for-Open-set-Architecture-Attribution-of-Synthetic-Images"><a href="#A-Siamese-based-Verification-System-for-Open-set-Architecture-Attribution-of-Synthetic-Images" class="headerlink" title="A Siamese-based Verification System for Open-set Architecture Attribution of Synthetic Images"></a>A Siamese-based Verification System for Open-set Architecture Attribution of Synthetic Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09822">http://arxiv.org/abs/2307.09822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lydia Abady, Jun Wang, Benedetta Tondi, Mauro Barni</li>
<li>for: This paper aims to address the problem of open-set attribution of synthetic images to the architecture that generated them.</li>
<li>methods: The proposed verification framework relies on a Siamese Network to compare the query image with reference images generated by the same architecture.</li>
<li>results: The proposed method achieves excellent performance in both closed and open-set settings, with strong generalization capabilities.<details>
<summary>Abstract</summary>
Despite the wide variety of methods developed for synthetic image attribution, most of them can only attribute images generated by models or architectures included in the training set and do not work with unknown architectures, hindering their applicability in real-world scenarios. In this paper, we propose a verification framework that relies on a Siamese Network to address the problem of open-set attribution of synthetic images to the architecture that generated them. We consider two different settings. In the first setting, the system determines whether two images have been produced by the same generative architecture or not. In the second setting, the system verifies a claim about the architecture used to generate a synthetic image, utilizing one or multiple reference images generated by the claimed architecture. The main strength of the proposed system is its ability to operate in both closed and open-set scenarios so that the input images, either the query and reference images, can belong to the architectures considered during training or not. Experimental evaluations encompassing various generative architectures such as GANs, diffusion models, and transformers, focusing on synthetic face image generation, confirm the excellent performance of our method in both closed and open-set settings, as well as its strong generalization capabilities.
</details>
<details>
<summary>摘要</summary>
尽管现有多种方法用于生成图像归属，大多数方法只能归属图像由训练集中的模型或架构生成，无法处理未知的架构，这限制了它们在实际场景中的应用性。在这篇论文中，我们提出了一个验证框架，利用对称网络解决生成图像的开放集归属问题。我们考虑了两种不同的设定。在第一个设定中，系统确定两个图像是否由同一个生成架构生成。在第二个设定中，系统验证一个关于生成架构的声明，利用一个或多个由声明的架构生成的参考图像。我们的方法的主要优点在于它可以在关闭和开放集enario中运行，因此输入图像（或订单、参考图像）可以属于训练集中的架构或不属于。我们的实验评估了多种生成架构，包括GANs、扩散模型和转换器，以及对生成的人脸图像进行了评估，并证明了我们的方法在关闭和开放集enario中具有极高的表现力和普适性。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Semantic-Perceptual-Listener-Head-Video-Generation-A-High-performance-Pipeline"><a href="#Hierarchical-Semantic-Perceptual-Listener-Head-Video-Generation-A-High-performance-Pipeline" class="headerlink" title="Hierarchical Semantic Perceptual Listener Head Video Generation: A High-performance Pipeline"></a>Hierarchical Semantic Perceptual Listener Head Video Generation: A High-performance Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09821">http://arxiv.org/abs/2307.09821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhigang Chang, Weitai Hu, Qing Yang, Shibao Zheng<br>for: This paper is written for the technical report of ViCo@2023 Conversational Head Generation Challenge in ACM Multimedia 2023 conference.methods: The paper uses a high-performance solution that enhances the hierarchical semantic extraction capability of the audio encoder module and improves the decoder part, renderer, and post-processing modules.results: The solution proposed in the paper achieves the first place on the official leaderboard for the track of listening head generation.<details>
<summary>Abstract</summary>
In dyadic speaker-listener interactions, the listener's head reactions along with the speaker's head movements, constitute an important non-verbal semantic expression together. The listener Head generation task aims to synthesize responsive listener's head videos based on audios of the speaker and reference images of the listener. Compared to the Talking-head generation, it is more challenging to capture the correlation clues from the speaker's audio and visual information. Following the ViCo baseline scheme, we propose a high-performance solution by enhancing the hierarchical semantic extraction capability of the audio encoder module and improving the decoder part, renderer and post-processing modules. Our solution gets the first place on the official leaderboard for the track of listening head generation. This paper is a technical report of ViCo@2023 Conversational Head Generation Challenge in ACM Multimedia 2023 conference.
</details>
<details>
<summary>摘要</summary>
在双向说话人-听众互动中，听众的头部反应以及说话人的头部运动，共同组成了重要的非语言表达。听众头生成任务的目标是基于说话人的音频和参考图像生成响应式的听众头视频。与对话头生成相比，捕捉说话人的相关征具有更高的挑战性。我们基于ViCo基eline方案，提出一种高性能的解决方案，通过增强音频编码模块的层次semantic抽象能力和decoder部分、渲染器和后处理模块的改进，实现了高质量的听众头生成。我们的解决方案在官方领先者榜上名列第一，这篇报告是ViCo@2023 conversational Head Generation Challenge的技术报告，发表于ACM Multimedia 2023会议上。
</details></li>
</ul>
<hr>
<h2 id="Deep-unrolling-Shrinkage-Network-for-Dynamic-MR-imaging"><a href="#Deep-unrolling-Shrinkage-Network-for-Dynamic-MR-imaging" class="headerlink" title="Deep unrolling Shrinkage Network for Dynamic MR imaging"></a>Deep unrolling Shrinkage Network for Dynamic MR imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09818">http://arxiv.org/abs/2307.09818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yhao-z/dus-net">https://github.com/yhao-z/dus-net</a></li>
<li>paper_authors: Yinghao Zhang, Xiaodi Li, Weihang Li, Yue Hu</li>
<li>For: 这个论文旨在提高动态磁共振成像（MR）图像的重建模型，特别是使用深度抽象网络（CNN）和简单阈值（ST）算法来强制遵循简洁约束。* Methods: 本文提出了一个新的运算子，叫做潜在阈值运算子（AST），这个运算子可以学习每个通道的阈值。同时，本文也提出了一个新的深度 unfolding shrinkage network（DUS-Net），这个模型是基于 alternate direction method of multipliers（ADMM）来优化磁共振成像的对应$l_1$ нор数据重建模型。* Results: 实验结果显示，提案的 DUS-Net 模型在一个公开存储的动态磁共振成像数据集上表现出色，比前一代方法更好。<details>
<summary>Abstract</summary>
Deep unrolling networks that utilize sparsity priors have achieved great success in dynamic magnetic resonance (MR) imaging. The convolutional neural network (CNN) is usually utilized to extract the transformed domain, and then the soft thresholding (ST) operator is applied to the CNN-transformed data to enforce the sparsity priors. However, the ST operator is usually constrained to be the same across all channels of the CNN-transformed data. In this paper, we propose a novel operator, called soft thresholding with channel attention (AST), that learns the threshold for each channel. In particular, we put forward a novel deep unrolling shrinkage network (DUS-Net) by unrolling the alternating direction method of multipliers (ADMM) for optimizing the transformed $l_1$ norm dynamic MR reconstruction model. Experimental results on an open-access dynamic cine MR dataset demonstrate that the proposed DUS-Net outperforms the state-of-the-art methods. The source code is available at \url{https://github.com/yhao-z/DUS-Net}.
</details>
<details>
<summary>摘要</summary>
深度折衣网络在动态磁共振成像中取得了很大成功。通常情况下，卷积神经网络（CNN）将数据映射到转换edomain中，然后使用soft thresholding（ST）操作来实现稀疏约束。然而，ST操作通常对所有通道的CNN-transformed数据进行同样的约束。在这篇论文中，我们提出了一种新的操作，即Channel Attention Soft Thresholding（AST），它可以学习每个通道的阈值。具体来说，我们提出了一种新的深度折衣缩放网络（DUS-Net），通过对 alternate direction method of multipliers（ADMM）的拓展来优化转换$l_1$ norm动力磁共振重建模型。实验结果表明，提议的DUS-Net在一个公开的动态磁共振MR数据集上表现出色，超过了当前的状态值。源代码可以在 \url{https://github.com/yhao-z/DUS-Net} 上获取。
</details></li>
</ul>
<hr>
<h2 id="LDP-Language-driven-Dual-Pixel-Image-Defocus-Deblurring-Network"><a href="#LDP-Language-driven-Dual-Pixel-Image-Defocus-Deblurring-Network" class="headerlink" title="LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network"></a>LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09815">http://arxiv.org/abs/2307.09815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Yang, Liyuan Pan, Yan Yang, Miaomiao Liu</li>
<li>for: 本研究旨在使用语言图像预训练框架（CLIP）来从双像缓冲（DP）对组中估计缓冲图。</li>
<li>methods: 我们首先设计了特制文本提示，使CLIP能够从DP对组中学习缓冲相关的几何假设知识。然后，我们提议一种输入双像DP对到CLIP的格式，无需任何微调。给出估计缓冲图后，我们引入了缓冲优先注意力块、缓冲权重损失和缓冲意识损失来恢复全ocus图像。</li>
<li>results: 我们在广泛的实验中达到了状态 искусственный智能的表现。<details>
<summary>Abstract</summary>
Recovering sharp images from dual-pixel (DP) pairs with disparity-dependent blur is a challenging task.~Existing blur map-based deblurring methods have demonstrated promising results. In this paper, we propose, to the best of our knowledge, the first framework to introduce the contrastive language-image pre-training framework (CLIP) to achieve accurate blur map estimation from DP pairs unsupervisedly. To this end, we first carefully design text prompts to enable CLIP to understand blur-related geometric prior knowledge from the DP pair. Then, we propose a format to input stereo DP pair to the CLIP without any fine-tuning, where the CLIP is pre-trained on monocular images. Given the estimated blur map, we introduce a blur-prior attention block, a blur-weighting loss and a blur-aware loss to recover the all-in-focus image. Our method achieves state-of-the-art performance in extensive experiments.
</details>
<details>
<summary>摘要</summary>
recuperating sharp images from dual-pixel (DP) pairs with disparity-dependent blur is a challenging task. existing blur map-based deblurring methods have demonstrated promising results. in this paper, we propose, to the best of our knowledge, the first framework to introduce the contrastive language-image pre-training framework (CLIP) to achieve accurate blur map estimation from DP pairs unsupervisedly. to this end, we first carefully design text prompts to enable CLIP to understand blur-related geometric prior knowledge from the DP pair. then, we propose a format to input stereo DP pair to the CLIP without any fine-tuning, where the CLIP is pre-trained on monocular images. given the estimated blur map, we introduce a blur-prior attention block, a blur-weighting loss and a blur-aware loss to recover the all-in-focus image. our method achieves state-of-the-art performance in extensive experiments.Here's the text with some minor adjustments to make it more readable in Simplified Chinese: recuperating sharp images from dual-pixel (DP) pairs with disparity-dependent blur is a challenging task. 现有的锐化图-基于的锐化方法已经达到了一定的成果。在这篇论文中，我们提出了，到目前为止为止，第一个引入语言-图像预训练框架（CLIP）来实现不监督的锐化图估计从DP对中。为此，我们首先仔细设计了文本提示，使CLIP能够从DP对中了解锐化相关的几何启示知识。然后，我们提议一种输入双目DP对到CLIP无需任何微调的格式，其中CLIP是基于单目图像的预训练的。给出估计的锐化图，我们引入了锐化关注块、锐化权重损失和锐化意识损失，以恢复所有锐化图像。我们的方法在广泛的实验中达到了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="GenKL-An-Iterative-Framework-for-Resolving-Label-Ambiguity-and-Label-Non-conformity-in-Web-Images-Via-a-New-Generalized-KL-Divergence"><a href="#GenKL-An-Iterative-Framework-for-Resolving-Label-Ambiguity-and-Label-Non-conformity-in-Web-Images-Via-a-New-Generalized-KL-Divergence" class="headerlink" title="GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence"></a>GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09810">http://arxiv.org/abs/2307.09810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/codetopaper/genkl">https://github.com/codetopaper/genkl</a></li>
<li>paper_authors: Xia Huang, Kai Fong Ernest Chong</li>
<li>for: 这篇论文是为了解决在网络图像Dataset中存在ambiguous instances（ID和OOD）问题而写的。</li>
<li>methods: 这篇论文使用了一种新的iterative training框架，called GenKL，来标识和重新标注ambiguous instances。GenKL使用了一种新的 $\alpha,\beta$ -generalized KL divergence来识别NC instances。</li>
<li>results: 在三个web图像Dataset上（Clothing1M、Food101&#x2F;Food101N和mini WebVision 1.0），这种新的iterative training框架GenKL可以 дости到新的state-of-the-art classification accuracy：81.34%、85.73%和78.99%&#x2F;92.54% (top-1&#x2F;top-5)。<details>
<summary>Abstract</summary>
Web image datasets curated online inherently contain ambiguous in-distribution (ID) instances and out-of-distribution (OOD) instances, which we collectively call non-conforming (NC) instances. In many recent approaches for mitigating the negative effects of NC instances, the core implicit assumption is that the NC instances can be found via entropy maximization. For "entropy" to be well-defined, we are interpreting the output prediction vector of an instance as the parameter vector of a multinomial random variable, with respect to some trained model with a softmax output layer. Hence, entropy maximization is based on the idealized assumption that NC instances have predictions that are "almost" uniformly distributed. However, in real-world web image datasets, there are numerous NC instances whose predictions are far from being uniformly distributed. To tackle the limitation of entropy maximization, we propose $(\alpha, \beta)$-generalized KL divergence, $\mathcal{D}_{\text{KL}^{\alpha, \beta}(p\|q)$, which can be used to identify significantly more NC instances. Theoretical properties of $\mathcal{D}_{\text{KL}^{\alpha, \beta}(p\|q)$ are proven, and we also show empirically that a simple use of $\mathcal{D}_{\text{KL}^{\alpha, \beta}(p\|q)$ outperforms all baselines on the NC instance identification task. Building upon $(\alpha,\beta)$-generalized KL divergence, we also introduce a new iterative training framework, GenKL, that identifies and relabels NC instances. When evaluated on three web image datasets, Clothing1M, Food101/Food101N, and mini WebVision 1.0, we achieved new state-of-the-art classification accuracies: $81.34\%$, $85.73\%$ and $78.99\%$/$92.54\%$ (top-1/top-5), respectively.
</details>
<details>
<summary>摘要</summary>
<SYS> translate("Web 图像集合在线批处理中存在歧义的内部（ID）实例和外部（OOD）实例，我们一起称之为非合规（NC）实例。在许多最近的NC实例缓解方法中，核心隐含假设是通过熵 maximization来发现NC实例。")</SYS>以下是文本的简化中文翻译：在线批处理的网络图像集合中存在ID实例和OOD实例，我们称之为非合规（NC）实例。许多最近的NC实例缓解方法中，核心隐含假设是通过熵 maximization来发现NC实例。<SYS> translate("在实际情况下，NC实例的预测结果并不是很接近均匀分布的，这限制了熵 maximization的应用。为了解决这一问题，我们提出了（α, β）总体KL异同（D），可以更好地发现NC实例。")</SYS>以下是文本的简化中文翻译：在实际情况下，NC实例的预测结果并不是很接近均匀分布的，这限制了熵 maximization的应用。为了解决这一问题，我们提出了（α, β）总体KL异同（D），可以更好地发现NC实例。<SYS> translate("我们证明了（α, β）总体KL异同的性质，并且通过实验表明，使用（α, β）总体KL异同可以比基础方法更好地发现NC实例。")</SYS>以下是文本的简化中文翻译：我们证明了（α, β）总体KL异同的性质，并且通过实验表明，使用（α, β）总体KL异同可以比基础方法更好地发现NC实例。<SYS> translate("基于（α, β）总体KL异同，我们还提出了一种新的迭代培训框架，GenKL，可以同时标注和重新标注NC实例。")</SYS>以下是文本的简化中文翻译：基于（α, β）总体KL异同，我们还提出了一种新的迭代培训框架，GenKL，可以同时标注和重新标注NC实例。<SYS> translate("我们在Clothing1M、Food101/Food101N和mini WebVision 1.0三个网络图像集合上评估了GenKL框架，并取得了新的状态级别分类精度：81.34%、85.73%和78.99%/92.54% (top-1/top-5)。")</SYS>以下是文本的简化中文翻译：我们在Clothing1M、Food101/Food101N和mini WebVision 1.0三个网络图像集合上评估了GenKL框架，并取得了新的状态级别分类精度：81.34%、85.73%和78.99%/92.54% (top-1/top-5)。
</details></li>
</ul>
<hr>
<h2 id="Fix-your-downsampling-ASAP-Be-natively-more-robust-via-Aliasing-and-Spectral-Artifact-free-Pooling"><a href="#Fix-your-downsampling-ASAP-Be-natively-more-robust-via-Aliasing-and-Spectral-Artifact-free-Pooling" class="headerlink" title="Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling"></a>Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09804">http://arxiv.org/abs/2307.09804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Grabinski, Janis Keuper, Margret Keuper</li>
<li>for: 提高卷积神经网络的鲁棒性 against common corruptions and adversarial attacks</li>
<li>methods: 使用 aliasing and spectral artifact-free pooling (ASAP) 方法， modify FLC pooling method to reduce spectral leakage artifacts</li>
<li>results: 网络使用 ASAP 方法显示出高度的Native robustness against common corruptions and adversarial attacks, 同时保持相似的clean accuracy or even outperform the baseline.<details>
<summary>Abstract</summary>
Convolutional neural networks encode images through a sequence of convolutions, normalizations and non-linearities as well as downsampling operations into potentially strong semantic embeddings. Yet, previous work showed that even slight mistakes during sampling, leading to aliasing, can be directly attributed to the networks' lack in robustness. To address such issues and facilitate simpler and faster adversarial training, [12] recently proposed FLC pooling, a method for provably alias-free downsampling - in theory. In this work, we conduct a further analysis through the lens of signal processing and find that such current pooling methods, which address aliasing in the frequency domain, are still prone to spectral leakage artifacts. Hence, we propose aliasing and spectral artifact-free pooling, short ASAP. While only introducing a few modifications to FLC pooling, networks using ASAP as downsampling method exhibit higher native robustness against common corruptions, a property that FLC pooling was missing. ASAP also increases native robustness against adversarial attacks on high and low resolution data while maintaining similar clean accuracy or even outperforming the baseline.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks 通过一系列卷积、 норма化和非线性运算以及下采样操作转化图像为强Semantic embedding。然而，先前的工作表明，even slight mistakes during sampling, leading to aliasing, can be directly attributed to the networks' lack of robustness。To address such issues and facilitate simpler and faster adversarial training,  [12] recently proposed FLC pooling, a method for provably alias-free downsampling - in theory。在这项工作中，我们通过信号处理的视角进行进一步的分析，发现现有的 pooling 方法，通过频域地址aliasing，仍然存在频率泄漏 artifacts。因此，我们提议 aliasing和频率泄漏 artifact-free pooling，简称ASAP。尽管ASAP只是FLC pooling的一些修改，但是使用ASAP作为下采样方法的网络在常见损害和攻击下的Native robustness显著提高，而不会影响clean accuracy的性能。
</details></li>
</ul>
<hr>
<h2 id="From-West-to-East-Who-can-understand-the-music-of-the-others-better"><a href="#From-West-to-East-Who-can-understand-the-music-of-the-others-better" class="headerlink" title="From West to East: Who can understand the music of the others better?"></a>From West to East: Who can understand the music of the others better?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09795">http://arxiv.org/abs/2307.09795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pxaris/ccml">https://github.com/pxaris/ccml</a></li>
<li>paper_authors: Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</li>
<li>for: 本研究的目的是了解不同文化和风格的音乐表示，以及是否可以使用现有的深度学习模型来学习这些表示。</li>
<li>methods: 本研究使用了转移学习方法，将西方流行音乐和传统&#x2F;民间音乐数据转移到不同文化和风格的音乐数据上，以 derive 音乐表示之间的相似性。</li>
<li>results: 实验结果显示，通过转移学习，可以在不同文化和风格的音乐数据上达到竞争性的表示性能，而最佳来源数据集的选择则因每个音乐文化而异。<details>
<summary>Abstract</summary>
Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset. Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture. The implementation and the trained models are both provided in a public repository.
</details>
<details>
<summary>摘要</summary>
最近的MIR发展已经导致了许多标准深度学习模型，其embedding可以用于多种下游任务。然而，大多数这些模型都是在西方流行/摇滚音乐和相关风格上训练的。这引发了研究问题，是否可以使用这些模型学习不同的音乐文化和风格的表示，或者可以建立来自不同文化或风格的音乐音频嵌入模型。为了解决这个问题，我们利用了传输学习方法，以derive关于不同音乐文化之间的相似性的洞察。我们使用了两个西方音乐数据集，两个来自东 Mediterranean 的传统/民谣数据集，以及两个印度古典音乐数据集。三个深度音频嵌入模型被训练和传输到不同领域，包括两个CNN基于的和一个Transformer基于的架构，以实现每个目标领域数据集的自动标签。实验结果表明，通过传输学习，在所有领域中可以 achiev competitive 性，而每个音乐文化的最佳来源数据集则不同。实现和训练过的模型都提供在公共存储库中。
</details></li>
</ul>
<hr>
<h2 id="DiffDP-Radiotherapy-Dose-Prediction-via-a-Diffusion-Model"><a href="#DiffDP-Radiotherapy-Dose-Prediction-via-a-Diffusion-Model" class="headerlink" title="DiffDP: Radiotherapy Dose Prediction via a Diffusion Model"></a>DiffDP: Radiotherapy Dose Prediction via a Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09794">http://arxiv.org/abs/2307.09794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scufzh/DiffDP">https://github.com/scufzh/DiffDP</a></li>
<li>paper_authors: Zhenghao Feng, Lu Wen, Peng Wang, Binyu Yan, Xi Wu, Jiliu Zhou, Yan Wang</li>
<li>for: 这个研究旨在提高放射治疗规划中自动预测药用剂分布的精度和效率，并解决现有方法所受到的过滤问题。</li>
<li>methods: 本研究提出了一个扩散基于的药用剂预测模型（DiffDP），它包括一个前进过程和一个反进过程。在前进过程中，DiffDP将药用剂分布图描述当作 Gaussian 噪声，逐步添加小量噪声，并训练一个噪声预测器来预测添加在每个时间步骤中的噪声。在反进过程中，DiffDP从原始 Gaussian 噪声中逐步除去噪声，使用对照预测器，最终输出预测的药用剂分布图描述。</li>
<li>results: 实验结果显示，DiffDP 模型可以优化 radiotherapy 规划中药用剂分布的预测精度和效率，并且能够保持药用剂分布的稳定性和精确性。<details>
<summary>Abstract</summary>
Currently, deep learning (DL) has achieved the automatic prediction of dose distribution in radiotherapy planning, enhancing its efficiency and quality. However, existing methods suffer from the over-smoothing problem for their commonly used L_1 or L_2 loss with posterior average calculations. To alleviate this limitation, we innovatively introduce a diffusion-based dose prediction (DiffDP) model for predicting the radiotherapy dose distribution of cancer patients. Specifically, the DiffDP model contains a forward process and a reverse process. In the forward process, DiffDP gradually transforms dose distribution maps into Gaussian noise by adding small noise and trains a noise predictor to predict the noise added in each timestep. In the reverse process, it removes the noise from the original Gaussian noise in multiple steps with the well-trained noise predictor and finally outputs the predicted dose distribution map. To ensure the accuracy of the prediction, we further design a structure encoder to extract anatomical information from patient anatomy images and enable the noise predictor to be aware of the dose constraints within several essential organs, i.e., the planning target volume and organs at risk. Extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the s
</details>
<details>
<summary>摘要</summary>
The DiffDP model consists of a forward process and a reverse process. In the forward process, DiffDP transforms dose distribution maps into Gaussian noise by adding small noise and trains a noise predictor to predict the added noise in each timestep. In the reverse process, it removes noise from original Gaussian noise in multiple steps with the well-trained noise predictor and outputs the predicted dose distribution map.To ensure accuracy, we designed a structure encoder to extract anatomical information from patient anatomy images, enabling the noise predictor to be aware of dose constraints within essential organs, such as the planning target volume and organs at risk.Extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the superiority of the DiffDP model in accuracy and efficiency compared to existing methods.
</details></li>
</ul>
<hr>
<h2 id="Density-invariant-Features-for-Distant-Point-Cloud-Registration"><a href="#Density-invariant-Features-for-Distant-Point-Cloud-Registration" class="headerlink" title="Density-invariant Features for Distant Point Cloud Registration"></a>Density-invariant Features for Distant Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09788">http://arxiv.org/abs/2307.09788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuquan98/gcl">https://github.com/liuquan98/gcl</a></li>
<li>paper_authors: Quan Liu, Hongzi Zhu, Yunsong Zhou, Hongyang Li, Shan Chang, Minyi Guo</li>
<li>for: 提高远距离自动驾驶车辆3D视野的扩展，重要的一环是远程outdoor LiDAR点云注册。</li>
<li>methods: 提出了Group-wise Contrastive Learning（GCL）方法，使用这种方法可以提取不受密度影响的геометрические特征来注册远程outdoor LiDAR点云。</li>
<li>results: 经过理论分析和实验 validate，GCL方法可以提高远方enario的注册准确率，在KITTI和nuScenes benchmark上提高了40.9%和26.9%。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Registration of distant outdoor LiDAR point clouds is crucial to extending the 3D vision of collaborative autonomous vehicles, and yet is challenging due to small overlapping area and a huge disparity between observed point densities. In this paper, we propose Group-wise Contrastive Learning (GCL) scheme to extract density-invariant geometric features to register distant outdoor LiDAR point clouds. We mark through theoretical analysis and experiments that, contrastive positives should be independent and identically distributed (i.i.d.), in order to train densityinvariant feature extractors. We propose upon the conclusion a simple yet effective training scheme to force the feature of multiple point clouds in the same spatial location (referred to as positive groups) to be similar, which naturally avoids the sampling bias introduced by a pair of point clouds to conform with the i.i.d. principle. The resulting fully-convolutional feature extractor is more powerful and density-invariant than state-of-the-art methods, improving the registration recall of distant scenarios on KITTI and nuScenes benchmarks by 40.9% and 26.9%, respectively. Code is available at https://github.com/liuQuan98/GCL.
</details>
<details>
<summary>摘要</summary>
注册远程外部LiDAR点云是自主驾驶车辆共同视觉扩展的关键，但受到小 overlap 区域和巨大的观察点云密度差异困扰。在这篇论文中，我们提出了集合比较学习（GCL）方案，以提取不受点云密度影响的geometry特征来注册远程外部LiDAR点云。我们通过理论分析和实验发现，对于contrastive正例，需要独立并同分布（i.i.d），以便训练不受点云密度影响的特征提取器。我们提出了一种简单 yet有效的训练方案，使得多个点云在同一个空间位置（称为正例组）中的特征相似，从而自然避免了由一对点云的抽样偏见引入的i.i.d.原则。得到的全连接的特征提取器更强大和不受点云密度影响，在KITTI和nuScenes benchmark上提高了远程enario中注册回快率 by 40.9%和26.9%，分别。代码可以在https://github.com/liuQuan98/GCL上获取。
</details></li>
</ul>
<hr>
<h2 id="DVPT-Dynamic-Visual-Prompt-Tuning-of-Large-Pre-trained-Models-for-Medical-Image-Analysis"><a href="#DVPT-Dynamic-Visual-Prompt-Tuning-of-Large-Pre-trained-Models-for-Medical-Image-Analysis" class="headerlink" title="DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis"></a>DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09787">http://arxiv.org/abs/2307.09787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Along He, Kai Wang, Zhihong Wang, Tao Li, Huazhu Fu<br>for: 针对医疗领域中具有限制的标注数据问题，提出了一种 Parametric Efficient Fine-tuning (PEFT) 方法，以增进泛化模型的适应性和效率。methods: 提出了一种名为 Dynamic Visual Prompt Tuning (DVPT) 的方法，通过将冻结特征经过轻量级瓶颈层处理，学习医疗任务特定的分布，然后使用几个可变的视觉提示进行相互关注，以获取适合每个样本的知识。results: 对医疗分类和分割任务进行了广泛的实验，发现 DVPT 方法不仅可以有效地适应预先训练的模型到医疗领域，还可以在具有有限的标注数据的情况下提高数据效率。例如，与预先训练的模型进行比较，DVPT 方法可以在医疗分类任务上提高 Kappa 分数超过 2.20%，并且可以节省到 60% 的标注数据和 99% 的存储成本。<details>
<summary>Abstract</summary>
Limited labeled data makes it hard to train models from scratch in medical domain, and an important paradigm is pre-training and then fine-tuning. Large pre-trained models contain rich representations, which can be adapted to downstream medical tasks. However, existing methods either tune all the parameters or the task-specific layers of the pre-trained models, ignoring the input variations of medical images, and thus they are not efficient or effective. In this work, we aim to study parameter-efficient fine-tuning (PEFT) for medical image analysis, and propose a dynamic visual prompt tuning method, named DVPT. It can extract knowledge beneficial to downstream tasks from large models with a few trainable parameters. Firstly, the frozen features are transformed by an lightweight bottleneck layer to learn the domain-specific distribution of downstream medical tasks, and then a few learnable visual prompts are used as dynamic queries and then conduct cross-attention with the transformed features, attempting to acquire sample-specific knowledge that are suitable for each sample. Finally, the features are projected to original feature dimension and aggregated with the frozen features. This DVPT module can be shared between different Transformer layers, further reducing the trainable parameters. To validate DVPT, we conduct extensive experiments with different pre-trained models on medical classification and segmentation tasks. We find such PEFT method can not only efficiently adapt the pre-trained models to the medical domain, but also brings data efficiency with partial labeled data. For example, with 0.5\% extra trainable parameters, our method not only outperforms state-of-the-art PEFT methods, even surpasses the full fine-tuning by more than 2.20\% Kappa score on medical classification task. It can saves up to 60\% labeled data and 99\% storage cost of ViT-B/16.
</details>
<details>
<summary>摘要</summary>
因为医疗领域有限的标注数据，训练模型从头来是困难的。一种重要的思路是使用预训练并微调。大型预训练模型含有丰富的表示，可以适应下游医疗任务。然而，现有方法都是全部参数或任务特定层的微调，忽略医疗图像的输入变化，因此不是高效或有效的。在这项工作中，我们目的是研究高效微调（PEFT）方法 для医疗图像分析，并提出了动态视觉提示调整方法（DVPT）。它可以从大型模型中提取有用于下游任务的知识，并且只需几个可调参数。首先，冻结的特征被轻量级瓶颈层转换，以学习下游医疗任务的域特定分布。然后，通过一些可调视觉提示，对转换后的特征进行跨见注意力操作，以获得适合每个样本的样本特定知识。最后，特征被投影到原始特征维度，并与冻结特征进行汇聚。这种DVPT模块可以在不同的Transformer层之间共享，进一步减少可调参数。为验证DVPT，我们在医疗分类和分割任务上进行了广泛的实验。我们发现，这种PEFT方法不仅可以高效地适应预训练模型到医疗领域，还可以提供数据效果，即使只有0.5%的额外可调参数。例如，我们的方法不仅超过了当前最佳PEFT方法的2.20%卡方度提升，还可以降低60%的标注数据和99%的存储成本。
</details></li>
</ul>
<hr>
<h2 id="Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation-via-Prototype-Anchored-Feature-Alignment-and-Contrastive-Learning"><a href="#Source-Free-Domain-Adaptation-for-Medical-Image-Segmentation-via-Prototype-Anchored-Feature-Alignment-and-Contrastive-Learning" class="headerlink" title="Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning"></a>Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09769">http://arxiv.org/abs/2307.09769</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cscyqj/miccai23-protocontra-sfda">https://github.com/cscyqj/miccai23-protocontra-sfda</a></li>
<li>paper_authors: Qinji Yu, Nan Xi, Junsong Yuan, Ziyu Zhou, Kang Dang, Xiaowei Ding</li>
<li>for: 这篇论文旨在解决医疗领域中的对应领域数据不可用问题，通过不需要同时存在源和目标领域数据的情况下，实现对医疗影像分类的领域适应。</li>
<li>methods: 这篇论文提出了一个 novel two-stage source-free domain adaptation（SFDA）框架，仅需要一个已经预训的源分类模型和无标目标数据，便可以在领域适应中进行对应。特别是在专扩型态扩大设计阶段，我们首先使用源模型的预训维度作为源原型，然后引入两向运输来对目标特征进行对齐。此外，我们还将对应领域数据中的不可靠预测 pixels 用于建立更加紧密的目标特征分布。</li>
<li>results: 实验结果显示，在大领域落差设定下，我们的方法在与现有的 SFDA 方法和一些 UDA 方法进行比较中，表现较好。 Code 可以在 <a target="_blank" rel="noopener" href="https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA">https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA</a> 上获得。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) has increasingly gained interests for its capacity to transfer the knowledge learned from a labeled source domain to an unlabeled target domain. However, typical UDA methods require concurrent access to both the source and target domain data, which largely limits its application in medical scenarios where source data is often unavailable due to privacy concern. To tackle the source data-absent problem, we present a novel two-stage source-free domain adaptation (SFDA) framework for medical image segmentation, where only a well-trained source segmentation model and unlabeled target data are available during domain adaptation. Specifically, in the prototype-anchored feature alignment stage, we first utilize the weights of the pre-trained pixel-wise classifier as source prototypes, which preserve the information of source features. Then, we introduce the bi-directional transport to align the target features with class prototypes by minimizing its expected cost. On top of that, a contrastive learning stage is further devised to utilize those pixels with unreliable predictions for a more compact target feature distribution. Extensive experiments on a cross-modality medical segmentation task demonstrate the superiority of our method in large domain discrepancy settings compared with the state-of-the-art SFDA approaches and even some UDA methods. Code is available at https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>无监督领域适应（USDA）在过去几年中得到了越来越多的关注，因为它可以将源领域中标注的知识传递到目标领域中。然而，传统的USDA方法通常需要同时访问源和目标领域的数据，这大大限制了它在医疗场景中的应用，因为源数据通常因隐私问题而不可доступible。为解决源数据缺失问题，我们提出了一个新的两stage无源领域适应（SFDA）框架，用于医疗像素分割，只有一个已经训练的源分割模型和无标注目标数据在领域适应过程中可用。具体来说，在原型锁定特征对应阶段，我们首先利用源模型已经训练的像素精度分类器的权重作为源原型，以保留源特征信息。然后，我们引入双向运输来对目标特征与类原型进行对应，通过最小化其预期成本来做出匹配。此外，我们还设计了一个对比学习阶段，以利用具有不可靠预测的像素来实现目标特征分布更加紧凑。我们在一个cross-modality医疗像素分割任务上进行了广泛的实验，结果表明，我们的方法在大领域差异设定下超过了当前SFDA方法和一些UDAD方法的性能。代码可以在https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA中找到。
</details></li>
</ul>
<hr>
<h2 id="Longitudinal-Data-and-a-Semantic-Similarity-Reward-for-Chest-X-Ray-Report-Generation"><a href="#Longitudinal-Data-and-a-Semantic-Similarity-Reward-for-Chest-X-Ray-Report-Generation" class="headerlink" title="Longitudinal Data and a Semantic Similarity Reward for Chest X-Ray Report Generation"></a>Longitudinal Data and a Semantic Similarity Reward for Chest X-Ray Report Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09758">http://arxiv.org/abs/2307.09758</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aehrc/cxrmate">https://github.com/aehrc/cxrmate</a></li>
<li>paper_authors: Aaron Nicolson, Jason Dowling, Bevan Koopman</li>
<li>For: 提高放射学家的工作效率和精度，以及改善患者护理* Methods: 利用患者的长期历史信息和CXR-BERT reward来提高自动生成放射报告* Results: 比前方法更高效，能够更好地遵循放射学家的工作流程，并且可以提供更好的临床翻译Here’s the breakdown of each point:* For: The paper aims to improve the work efficiency and accuracy of radiologists, as well as improve patient care.* Methods: The paper proposes a new method that utilizes the patient’s longitudinal history and CXR-BERT reward to improve automatic CXR report generation.* Results: The proposed method achieves better performance than previous methods, aligns more closely with radiologists’ assessment, and provides a better pathway to clinical translation.<details>
<summary>Abstract</summary>
The current burnout rate of radiologists is high due to the large and ever growing number of Chest X-Rays (CXRs) needing interpretation and reporting. Promisingly, automatic CXR report generation has the potential to aid radiologists with this laborious task and improve patient care. Previous CXR report generation methods are limited by their diagnostic inaccuracy and their lack of alignment with the workflow of radiologists. To address these issues, we present a new method that utilises the longitudinal history available from a patient's previous CXR study when generating a report, which imitates a radiologist's workflow. We also propose a new reward for reinforcement learning based on CXR-BERT -- which captures the clinical semantic similarity between reports -- to further improve CXR report generation. We conduct experiments on the publicly available MIMIC-CXR dataset with metrics more closely correlated with radiologists' assessment of reporting. The results indicate capturing a patient's longitudinal history improves CXR report generation and that CXR-BERT is a promising alternative to the current state-of-the-art reward. Our approach generates radiology reports that are quantitatively more aligned with those of radiologists than previous methods while simultaneously offering a better pathway to clinical translation. Our Hugging Face checkpoint (https://huggingface.co/aehrc/cxrmate) and code (https://github.com/aehrc/cxrmate) are publicly available.
</details>
<details>
<summary>摘要</summary>
现有的胸部X射线（CXR）报告生成方法有高热含率，主要是因为逐渐增长的胸部X射线需要解释和报告的数量。幸运的是，自动生成CXR报告有很大的潜在作用，可以帮助胸部骨科医生处理这项劳动ioso�的任务，并提高患者的护理质量。在过去的CXR报告生成方法中，存在诊断不准确和医生工作流程不符的问题。为解决这些问题，我们提出了一种新的方法，该方法利用患者的前一次CXR研究记录来生成报告，这与胸部骨科医生的工作流程相似。此外，我们还提出了一种基于CXR-BERT的新奖励方法，这种奖励方法可以进一步提高CXR报告生成的质量。我们在公共可用的MIMIC-CXR数据集上进行了实验，结果表明，捕捉患者的 longitudinal历史可以提高CXR报告生成质量，而CXR-BERT是一个有前途的奖励方法。我们的方法可以生成与胸部骨科医生评估更加一致的报告，同时也提供了更好的临床翻译路径。我们的Hugging Face检查点（https://huggingface.co/aehrc/cxrmate）和代码（https://github.com/aehrc/cxrmate）都是公共可用的。
</details></li>
</ul>
<hr>
<h2 id="Generative-Prompt-Model-for-Weakly-Supervised-Object-Localization"><a href="#Generative-Prompt-Model-for-Weakly-Supervised-Object-Localization" class="headerlink" title="Generative Prompt Model for Weakly Supervised Object Localization"></a>Generative Prompt Model for Weakly Supervised Object Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09756">http://arxiv.org/abs/2307.09756</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/callsys/genpromp">https://github.com/callsys/genpromp</a></li>
<li>paper_authors: Yuzhong Zhao, Qixiang Ye, Weijia Wu, Chunhua Shen, Fang Wan</li>
<li>for: 本研究旨在解决受限监督物体地标的物体本地化问题，提出一种生成提示模型（GenPromp），以便学习不具有特征的物体部分。</li>
<li>methods: 本研究使用生成提示模型（GenPromp），将图像分类标签转换为学习可能的提示编码，然后通过一个生成模型来conditionally恢复输入图像噪音。</li>
<li>results: 在CUB-200-2011和ILSVRC上进行实验，GenPromp分别超过了最佳探测模型的5.2%和5.6%（Top-1 Loc），设置了WSOL领域的固定基线。可以从<a target="_blank" rel="noopener" href="https://github.com/callsys/GenPromp%E8%8E%B7%E5%8F%96%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/callsys/GenPromp获取代码。</a><details>
<summary>Abstract</summary>
Weakly supervised object localization (WSOL) remains challenging when learning object localization models from image category labels. Conventional methods that discriminatively train activation models ignore representative yet less discriminative object parts. In this study, we propose a generative prompt model (GenPromp), defining the first generative pipeline to localize less discriminative object parts by formulating WSOL as a conditional image denoising procedure. During training, GenPromp converts image category labels to learnable prompt embeddings which are fed to a generative model to conditionally recover the input image with noise and learn representative embeddings. During inference, enPromp combines the representative embeddings with discriminative embeddings (queried from an off-the-shelf vision-language model) for both representative and discriminative capacity. The combined embeddings are finally used to generate multi-scale high-quality attention maps, which facilitate localizing full object extent. Experiments on CUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline for WSOL with the generative model. Code is available at https://github.com/callsys/GenPromp.
</details>
<details>
<summary>摘要</summary>
弱监督物体Localization（WSOL）在学习物体Localization模型时仍然是一个挑战。传统的推理模型忽略了特征却不是最终的物体部分。在这个研究中，我们提出了一个生成提示模型（GenPromp），定义了首个生成管道，用于通过将图像类别标签转换为学习可能的提示嵌入，并使用生成模型来 conditionally 恢复输入图像噪声，以学习表示性嵌入。在训练过程中，GenPromp将图像类别标签转换为学习可能的提示嵌入，并将其 fed 到生成模型中，以conditionally 恢复输入图像噪声。在推理过程中，enPromp 将表示性嵌入与推理嵌入（从一个可用的视觉语言模型中查询）相结合，以实现表示性和推理能力的结合。最后，combined embeddings 将被用来生成多尺度高质量注意力地图，以便快速 Localize 全部物体范围。实验表明，GenPromp 可以在 CUB-200-2011 和 ILSVRC 上分别与最佳推理模型相比，提高 Top-1 Loc 的性能5.2%和5.6%，设置了WSOL领域的坚实基准。代码可以在 <https://github.com/callsys/GenPromp> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Scene-Text-Image-Super-resolution-via-Explicit-Location-Enhancement"><a href="#Towards-Robust-Scene-Text-Image-Super-resolution-via-Explicit-Location-Enhancement" class="headerlink" title="Towards Robust Scene Text Image Super-resolution via Explicit Location Enhancement"></a>Towards Robust Scene Text Image Super-resolution via Explicit Location Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09749">http://arxiv.org/abs/2307.09749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csguoh/lemma">https://github.com/csguoh/lemma</a></li>
<li>paper_authors: Hang Guo, Tao Dai, Guanghao Meng, Shu-Tao Xia</li>
<li>for: 提高Scene Text Recognition（STR）精度，address complex background disturbance</li>
<li>methods: 提出了一种新的方法LEMMA，explicitly model character regions to produce high-level text-specific guidance for super-resolution</li>
<li>results: 在TextZoom和四个场景文本识别benchmark上，与其他状态的方法进行比较，得到了superiority<details>
<summary>Abstract</summary>
Scene text image super-resolution (STISR), aiming to improve image quality while boosting downstream scene text recognition accuracy, has recently achieved great success. However, most existing methods treat the foreground (character regions) and background (non-character regions) equally in the forward process, and neglect the disturbance from the complex background, thus limiting the performance. To address these issues, in this paper, we propose a novel method LEMMA that explicitly models character regions to produce high-level text-specific guidance for super-resolution. To model the location of characters effectively, we propose the location enhancement module to extract character region features based on the attention map sequence. Besides, we propose the multi-modal alignment module to perform bidirectional visual-semantic alignment to generate high-quality prior guidance, which is then incorporated into the super-resolution branch in an adaptive manner using the proposed adaptive fusion module. Experiments on TextZoom and four scene text recognition benchmarks demonstrate the superiority of our method over other state-of-the-art methods. Code is available at https://github.com/csguoh/LEMMA.
</details>
<details>
<summary>摘要</summary>
Scene文本超解像（STISR）目标是提高图像质量并提高下游场景文本识别精度，最近几年取得了很大成功。然而，大多数现有方法在前进过程中对背景（非文本区域）和前景（文本区域）待遇处理相同，不充分考虑背景的复杂性，因此限制性。为解决这些问题，在这篇论文中，我们提出了一种新的方法LEMMA，其中明确模型文本区域以生成高级文本特定指导。为了有效地捕捉文本区域的位置，我们提出了位置增强模块，该模块通过注意力地图序列提取文本区域特征。此外，我们提出了多模态对应模块，该模块通过双向视Semantic对齐来生成高质量的初始指导，然后将其与超解像分支结合使用我们提出的适应融合模块。实验证明了我们的方法在TextZoom和四个场景文本识别标准测试集上的优势。代码可以在https://github.com/csguoh/LEMMA中下载。
</details></li>
</ul>
<hr>
<h2 id="Watch-out-Venomous-Snake-Species-A-Solution-to-SnakeCLEF2023"><a href="#Watch-out-Venomous-Snake-Species-A-Solution-to-SnakeCLEF2023" class="headerlink" title="Watch out Venomous Snake Species: A Solution to SnakeCLEF2023"></a>Watch out Venomous Snake Species: A Solution to SnakeCLEF2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09748">http://arxiv.org/abs/2307.09748</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoxsparraw/clef2023">https://github.com/xiaoxsparraw/clef2023</a></li>
<li>paper_authors: Feiran Hu, Peng Wang, Yangyang Li, Chenlong Duan, Zijian Zhu, Fei Wang, Faen Zhang, Yong Li, Xiu-Shen Wei</li>
<li>for: 这个研究是为了开发高级的类别植物识别算法，通过图像和数据库的分析。</li>
<li>methods: 本研究使用了现代的CNN模型和强大的数据增强，以学习图像的更好表现。另外，使用了 seesaw 损失函数来缓解长条形分布的挑战。在后期处理阶段，我们设计了一个轻量级的模型来计算 metadata 特征，并将毒蛇类别标签给一些模型是不确定的示例。</li>
<li>results: 本研究在私人领先板上获得了91.31%的最终度量结果，排名第一。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/xiaoxsparraw/CLEF2023">https://github.com/xiaoxsparraw/CLEF2023</a> 上取得。<details>
<summary>Abstract</summary>
The SnakeCLEF2023 competition aims to the development of advanced algorithms for snake species identification through the analysis of images and accompanying metadata. This paper presents a method leveraging utilization of both images and metadata. Modern CNN models and strong data augmentation are utilized to learn better representation of images. To relieve the challenge of long-tailed distribution, seesaw loss is utilized in our method. We also design a light model to calculate prior probabilities using metadata features extracted from CLIP in post processing stage. Besides, we attach more importance to venomous species by assigning venomous species labels to some examples that model is uncertain about. Our method achieves 91.31% score of the final metric combined of F1 and other metrics on private leaderboard, which is the 1st place among the participators. The code is available at https://github.com/xiaoxsparraw/CLEF2023.
</details>
<details>
<summary>摘要</summary>
《蛇类CLEF2023比赛目标是开发高级算法用于蛇类识别，通过图像和相关元数据分析。本文提出了利用图像和元数据的方法，使用现代 convolutional neural network（CNN）模型和强大的数据增强来学习图像的更好表示。为了解决长尾分布的挑战，我们使用 seesaw loss 方法。此外，我们还设计了一个轻量级模型用于计算元数据特征，并在后处理阶段使用 CLIP 提取的元数据特征来进行估计。此外，我们还强调有毒种类，对模型不确定的示例分配有毒种类标签。我们的方法在私人领先板上实现了 91.31% 的最终指标（F1 和其他指标的组合），位居所有参与者之首。代码可以在 GitHub 上找到：https://github.com/xiaoxsparraw/CLEF2023。
</details></li>
</ul>
<hr>
<h2 id="CPCM-Contextual-Point-Cloud-Modeling-for-Weakly-supervised-Point-Cloud-Semantic-Segmentation"><a href="#CPCM-Contextual-Point-Cloud-Modeling-for-Weakly-supervised-Point-Cloud-Semantic-Segmentation" class="headerlink" title="CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation"></a>CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10316">http://arxiv.org/abs/2307.10316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lizhaoliu-Lec/CPCM">https://github.com/lizhaoliu-Lec/CPCM</a></li>
<li>paper_authors: Lizhao Liu, Zhuangwei Zhuang, Shangxin Huang, Xunlong Xiao, Tianhang Xiang, Cen Chen, Jingdong Wang, Mingkui Tan</li>
<li>for: 这个研究旨在探讨弱监督点 cloud semantic segmentation 的问题，尤其是对于Scene understanding 的环境学习。</li>
<li>methods: 我们提出了一个简单 yet effective的 Contextual Point Cloud Modeling (CPCM) 方法，包括两个部分：RegionMask 策略和 contextual masked training (CMT) 方法。</li>
<li>results: 我们的实验结果显示，CPCM 方法在ScanNet V2和S3DIS benchmarks 上比州前方法表现更好，对于弱监督点 cloud semantic segmentation 提供了更高的性能。<details>
<summary>Abstract</summary>
We study the task of weakly-supervised point cloud semantic segmentation with sparse annotations (e.g., less than 0.1% points are labeled), aiming to reduce the expensive cost of dense annotations. Unfortunately, with extremely sparse annotated points, it is very difficult to extract both contextual and object information for scene understanding such as semantic segmentation. Motivated by masked modeling (e.g., MAE) in image and video representation learning, we seek to endow the power of masked modeling to learn contextual information from sparsely-annotated points. However, directly applying MAE to 3D point clouds with sparse annotations may fail to work. First, it is nontrivial to effectively mask out the informative visual context from 3D point clouds. Second, how to fully exploit the sparse annotations for context modeling remains an open question. In this paper, we propose a simple yet effective Contextual Point Cloud Modeling (CPCM) method that consists of two parts: a region-wise masking (RegionMask) strategy and a contextual masked training (CMT) method. Specifically, RegionMask masks the point cloud continuously in geometric space to construct a meaningful masked prediction task for subsequent context learning. CMT disentangles the learning of supervised segmentation and unsupervised masked context prediction for effectively learning the very limited labeled points and mass unlabeled points, respectively. Extensive experiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstrate the superiority of CPCM over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
我们研究弱监督点云 semantic segmentation 任务，使用稀有的标注（例如，点云中的标注点数少于0.1%），以降低高成本的密集标注成本。然而，与极端稀有的标注点相关，提取场景理解中的对象信息和 контекст信息具有极大的挑战。为了解决这个问题，我们启发自masked modeling（例如MAE）在图像和视频表示学习中的应用。我们寻求通过masked modeling来学习点云中的Contextual信息，但直接将MAE应用于稀有标注的3D点云可能不太可能。首先，从3D点云中有效地遮盖有用的视觉上下文是一个非常困难的问题。其次，如何充分利用稀有标注来Contextual模型仍然是一个开放的问题。在这篇论文中，我们提出了一种简单又有效的Contextual Point Cloud Modeling（CPCM）方法，它包括两个部分：RegionMask和CMT。 RegionMask使用几何空间中的连续遮盖来构建一个有意义的masked prediction任务，以便进行后续的Contextual模型学习。CMT通过分离supervised segmentation和无supervised masked context prediction来有效地学习EXTremely Limited Labels和大量无标注点云，分别学习supervised segmentation和无supervised masked context prediction。我们在ScanNet V2和S3DIS benchmark上进行了广泛的实验，结果表明CPCM在状态的前ier exceeds the state-of-the-art。
</details></li>
</ul>
<hr>
<h2 id="Improved-Distribution-Matching-for-Dataset-Condensation"><a href="#Improved-Distribution-Matching-for-Dataset-Condensation" class="headerlink" title="Improved Distribution Matching for Dataset Condensation"></a>Improved Distribution Matching for Dataset Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09742">http://arxiv.org/abs/2307.09742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uitrbn/idm">https://github.com/uitrbn/idm</a></li>
<li>paper_authors: Ganlong Zhao, Guanbin Li, Yipeng Qin, Yizhou Yu</li>
<li>for: 这个论文的目的是缩小深度学习数据集，以减少储存成本和训练成本，但保持训练模型的表现良好。</li>
<li>methods: 这个论文提出了一种基于分布匹配的数据集缩小方法，该方法更加高效和有前途。具体来说，论文识别了两个重要缺陷（分布数据不均匀和无效的嵌入），并通过三种新技术（分区和扩展增强、有效和丰富的模型抽样、类别对分布调整）解决这些缺陷。</li>
<li>results: 这个方法在大量的数据集上实现了更高效的缩小，并且在训练深度学习模型时显示出了更好的表现，较之前的优化方法更加高效。实验结果显示了这个方法的有效性。<details>
<summary>Abstract</summary>
Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising. Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization). Our simple yet effective method outperforms most previous optimization-oriented methods with much fewer computational resources, thereby scaling data condensation to larger datasets and models. Extensive experiments demonstrate the effectiveness of our method. Codes are available at https://github.com/uitrbn/IDM
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ClickSeg-3D-Instance-Segmentation-with-Click-Level-Weak-Annotations"><a href="#ClickSeg-3D-Instance-Segmentation-with-Click-Level-Weak-Annotations" class="headerlink" title="ClickSeg: 3D Instance Segmentation with Click-Level Weak Annotations"></a>ClickSeg: 3D Instance Segmentation with Click-Level Weak Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09732">http://arxiv.org/abs/2307.09732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leyao Liu, Tao Kong, Minzhao Zhu, Jiashuo Fan, Lu Fang</li>
<li>for: 这个论文旨在提出一种ClickSeg方法，该方法可以在具有只一个点标注的情况下实现3D实例分割。</li>
<li>methods: ClickSeg方法使用了一种基准的弱iously supervised training方法，通过模型自己生成pseudo标签来补充无标签数据。此外，该方法还提出了一个新的训练框架，使用k-means算法并设置了固定的初始种子。</li>
<li>results: ClickSeg方法在ScanNetV2和S3DIS数据集上实现了较好的实验结果，比前一个最佳弱iously supervised实例分割结果高出9.4%的MAP值。使用0.02%的超vision信号，ClickSeg方法可以达到$\sim$90%的准确率，同时也实现了同样的annotation设置下的状态态的 semantic segmentation结果。<details>
<summary>Abstract</summary>
3D instance segmentation methods often require fully-annotated dense labels for training, which are costly to obtain. In this paper, we present ClickSeg, a novel click-level weakly supervised 3D instance segmentation method that requires one point per instance annotation merely. Such a problem is very challenging due to the extremely limited labels, which has rarely been solved before. We first develop a baseline weakly-supervised training method, which generates pseudo labels for unlabeled data by the model itself. To utilize the property of click-level annotation setting, we further propose a new training framework. Instead of directly using the model inference way, i.e., mean-shift clustering, to generate the pseudo labels, we propose to use k-means with fixed initial seeds: the annotated points. New similarity metrics are further designed for clustering. Experiments on ScanNetV2 and S3DIS datasets show that the proposed ClickSeg surpasses the previous best weakly supervised instance segmentation result by a large margin (e.g., +9.4% mAP on ScanNetV2). Using 0.02% supervision signals merely, ClickSeg achieves $\sim$90% of the accuracy of the fully-supervised counterpart. Meanwhile, it also achieves state-of-the-art semantic segmentation results among weakly supervised methods that use the same annotation settings.
</details>
<details>
<summary>摘要</summary>
三维实例分割方法经常需要完全标注的紧密标签进行训练，这些标注是贵重的获得。在这篇论文中，我们提出了ClickSeg，一种新的点级弱标注三维实例分割方法，只需要每个实例一个点的注释。这是一个非常困难的问题，因为非常少的标注，这已经rarely been solved before。我们首先开发了一个基eline的弱标注训练方法，该方法可以生成pseudo标签 для未标注数据，通过模型自身。为了利用点级标注设置的特性，我们进一步提议了一新的训练框架。而不是直接使用模型推理方式，即mean-shift clustering，来生成pseudo标签，我们提议使用k-meansWith fixed initial seeds：注释点。新的相似度度量被设计用于分类。实验表明，提案的ClickSeg超过了之前最佳弱标注实例分割结果，即+9.4% mAP on ScanNetV2。使用0.02%的超vision信号仅，ClickSeg实现了$\sim$90%的准确性，与完全标注的 counterpart相当。此外，它还实现了weakly supervised方法中的最佳semantic segmentation结果。
</details></li>
</ul>
<hr>
<h2 id="NTIRE-2023-Quality-Assessment-of-Video-Enhancement-Challenge"><a href="#NTIRE-2023-Quality-Assessment-of-Video-Enhancement-Challenge" class="headerlink" title="NTIRE 2023 Quality Assessment of Video Enhancement Challenge"></a>NTIRE 2023 Quality Assessment of Video Enhancement Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09729">http://arxiv.org/abs/2307.09729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohong Liu, Xiongkuo Min, Wei Sun, Yulun Zhang, Kai Zhang, Radu Timofte, Guangtao Zhai, Yixuan Gao, Yuqin Cao, Tengchuan Kou, Yunlong Dong, Ziheng Jia, Yilin Li, Wei Wu, Shuming Hu, Sibin Deng, Pengxiang Xiao, Ying Chen, Kai Li, Kai Zhao, Kun Yuan, Ming Sun, Heng Cong, Hao Wang, Lingzhi Fu, Yusheng Zhang, Rongyu Zhang, Hang Shi, Qihang Xu, Longan Xiao, Zhiliang Ma, Mirko Agarla, Luigi Celona, Claudio Rota, Raimondo Schettini, Zhiwei Huang, Yanan Li, Xiaotao Wang, Lei Lei, Hongye Liu, Wei Hong, Ironhead Chuang, Allen Lin, Drake Guan, Iris Chen, Kae Lou, Willy Huang, Yachun Tasi, Yvonne Kao, Haotian Fan, Fangyuan Kong, Shiqi Zhou, Hao Liu, Yu Lai, Shanshan Chen, Wenqi Wang, Haoning Wu, Chaofeng Chen, Chunzheng Zhu, Zekun Guo, Shiling Zhao, Haibing Yin, Hongkui Wang, Hanene Brachemi Meftah, Sid Ahmed Fezza, Wassim Hamidouche, Olivier Déforges, Tengfei Shi, Azadeh Mansouri, Hossein Motamednia, Amir Hossein Bakhtiari, Ahmad Mahmoudi Aznaveh</li>
<li>for: 本研究旨在提高视频处理领域中的视频质量评估（VQA）技术，特别是对于进行了增强处理的视频。</li>
<li>methods: 本研究使用了VQA Dataset for Perceptual Video Enhancement（VDPVE），包括600个彩色、亮度和对比度增强的视频、310个去锐化视频和301个去晃动视频。参赛者使用了多种方法，包括基线方法以及自己研发的模型。</li>
<li>results: 本研究发现了一些方法可以超越基线方法的性能，并且赢得了比赛。赢得者的方法展示了出色的预测性能。<details>
<summary>Abstract</summary>
This paper reports on the NTIRE 2023 Quality Assessment of Video Enhancement Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2023. This challenge is to address a major challenge in the field of video processing, namely, video quality assessment (VQA) for enhanced videos. The challenge uses the VQA Dataset for Perceptual Video Enhancement (VDPVE), which has a total of 1211 enhanced videos, including 600 videos with color, brightness, and contrast enhancements, 310 videos with deblurring, and 301 deshaked videos. The challenge has a total of 167 registered participants. 61 participating teams submitted their prediction results during the development phase, with a total of 3168 submissions. A total of 176 submissions were submitted by 37 participating teams during the final testing phase. Finally, 19 participating teams submitted their models and fact sheets, and detailed the methods they used. Some methods have achieved better results than baseline methods, and the winning methods have demonstrated superior prediction performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Uncertainty-Driven-Multi-Scale-Feature-Fusion-Network-for-Real-time-Image-Deraining"><a href="#Uncertainty-Driven-Multi-Scale-Feature-Fusion-Network-for-Real-time-Image-Deraining" class="headerlink" title="Uncertainty-Driven Multi-Scale Feature Fusion Network for Real-time Image Deraining"></a>Uncertainty-Driven Multi-Scale Feature Fusion Network for Real-time Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09728">http://arxiv.org/abs/2307.09728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Tong, Xuefeng Yan, Yongzhen Wang</li>
<li>for: 提高雨天下视觉测量系统的性能</li>
<li>methods: 提出了一种基于多级特征协同 fusion的不确定性驱动多尺度特征融合网络（UMFFNet），通过把不确定性信息纳入网络来估算推断uncertainty</li>
<li>results: 实验表明，UMFFNet可以在减少预测错误的情况下提高视觉测量系统的性能，并且比存在state-of-the-art图像推理方法更高效。<details>
<summary>Abstract</summary>
Visual-based measurement systems are frequently affected by rainy weather due to the degradation caused by rain streaks in captured images, and existing imaging devices struggle to address this issue in real-time. While most efforts leverage deep networks for image deraining and have made progress, their large parameter sizes hinder deployment on resource-constrained devices. Additionally, these data-driven models often produce deterministic results, without considering their inherent epistemic uncertainty, which can lead to undesired reconstruction errors. Well-calibrated uncertainty can help alleviate prediction errors and assist measurement devices in mitigating risks and improving usability. Therefore, we propose an Uncertainty-Driven Multi-Scale Feature Fusion Network (UMFFNet) that learns the probability mapping distribution between paired images to estimate uncertainty. Specifically, we introduce an uncertainty feature fusion block (UFFB) that utilizes uncertainty information to dynamically enhance acquired features and focus on blurry regions obscured by rain streaks, reducing prediction errors. In addition, to further boost the performance of UMFFNet, we fused feature information from multiple scales to guide the network for efficient collaborative rain removal. Extensive experiments demonstrate that UMFFNet achieves significant performance improvements with few parameters, surpassing state-of-the-art image deraining methods.
</details>
<details>
<summary>摘要</summary>
视觉基于的测量系统常常由雨水影响，因为雨斑在捕捉到的图像中带来的降低效果，现有的图像设备很难在实时中解决这个问题。大多数努力是利用深度网络进行图像除雨，并且已经取得了进步，但它们的参数较大，使得在资源有限的设备上部署不可能。此外，这些数据驱动的模型经常生成决定性的结果，不考虑其内在的可识别 uncertainty，这可能导致不良重建结果。Well-calibrated uncertainty可以帮助减轻预测错误和使测量设备避免风险，提高可用性。因此，我们提出了一种基于不确定性的多级特征融合网络（UMFFNet），该网络学习图像之间的可能性映射分布。我们首次引入了一个不确定性特征融合块（UFFB），该块利用不确定性信息来动态增强获取的特征和焦点在雨斑掩蔽的模糊区域，从而减少预测错误。此外，为了进一步提高 UMFFNet 的性能，我们将特征信息从多个级别融合，以便导航网络进行高效的合作雨除。广泛的实验表明， UMFFNet 可以在少量参数下取得显著的性能提升，超越当前的图像除雨方法。
</details></li>
</ul>
<hr>
<h2 id="SAMConvex-Fast-Discrete-Optimization-for-CT-Registration-using-Self-supervised-Anatomical-Embedding-and-Correlation-Pyramid"><a href="#SAMConvex-Fast-Discrete-Optimization-for-CT-Registration-using-Self-supervised-Anatomical-Embedding-and-Correlation-Pyramid" class="headerlink" title="SAMConvex: Fast Discrete Optimization for CT Registration using Self-supervised Anatomical Embedding and Correlation Pyramid"></a>SAMConvex: Fast Discrete Optimization for CT Registration using Self-supervised Anatomical Embedding and Correlation Pyramid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09727">http://arxiv.org/abs/2307.09727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zi Li, Lin Tian, Tony C. W. Mok, Xiaoyu Bai, Puyang Wang, Jia Ge, Jingren Zhou, Le Lu, Xianghua Ye, Ke Yan, Dakai Jin</li>
<li>for: 这篇论文的目的是提出一种快速且精确的 CT 注册方法，以便在医疗影像处理中实现高精度的注册。</li>
<li>methods: 这篇论文使用了一种叫做 SAMConvex 的快速粗糙到细致的确定优化方法，通过自适应的扩展特征（SAM）来捕捉本地和全局的信息。</li>
<li>results: 这篇论文的结果显示 SAMConvex 方法在两个间patient CT 注册数据集（腹部 CT 和头颈部 CT）以及一个内patient CT 注册数据集（肺部 CT）上都能够实现更高的精度和速度，并且只需要几十毫秒这么长。<details>
<summary>Abstract</summary>
Estimating displacement vector field via a cost volume computed in the feature space has shown great success in image registration, but it suffers excessive computation burdens. Moreover, existing feature descriptors only extract local features incapable of representing the global semantic information, which is especially important for solving large transformations. To address the discussed issues, we propose SAMConvex, a fast coarse-to-fine discrete optimization method for CT registration that includes a decoupled convex optimization procedure to obtain deformation fields based on a self-supervised anatomical embedding (SAM) feature extractor that captures both local and global information. To be specific, SAMConvex extracts per-voxel features and builds 6D correlation volumes based on SAM features, and iteratively updates a flow field by performing lookups on the correlation volumes with a coarse-to-fine scheme. SAMConvex outperforms the state-of-the-art learning-based methods and optimization-based methods over two inter-patient registration datasets (Abdomen CT and HeadNeck CT) and one intra-patient registration dataset (Lung CT). Moreover, as an optimization-based method, SAMConvex only takes $\sim2$s ($\sim5s$ with instance optimization) for one paired images.
</details>
<details>
<summary>摘要</summary>
估算距离 вектор场via一个在特征空间计算的成本量表示图像匹配得到了很大的成功，但它受到过度计算负担。此外，现有的特征描述符只能提取本地特征，无法表示全局semantic信息，尤其是用于解决大 transformations。为了解决这些问题，我们提议SAMConvex，一种快速、粗略到细节的分别优化方法 дляCT匹配，包括一个分离 convex 优化过程来获取变形场 based on a self-supervised anatomical embedding (SAM) feature extractor，该特征提取器可以捕捉到本地和全局信息。具体来说，SAMConvex提取每个voxel的特征并建立6D相关体积表 based on SAM features，然后iteratively更新流场by performing lookups on the correlation volumes with a coarse-to-fine scheme。SAMConvex在对两个Inter-patient CT registration数据集（ Abdomen CT 和 HeadNeck CT）以及一个intra-patient CT registration数据集（Lung CT）进行比较时，超越了当前的学习基于方法和优化基于方法。此外，作为一种优化基于方法，SAMConvex只需要大约2秒（大约5秒with instance optimization）来处理一个对应的图像。
</details></li>
</ul>
<hr>
<h2 id="AesPA-Net-Aesthetic-Pattern-Aware-Style-Transfer-Networks"><a href="#AesPA-Net-Aesthetic-Pattern-Aware-Style-Transfer-Networks" class="headerlink" title="AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks"></a>AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09724">http://arxiv.org/abs/2307.09724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kibeom-hong/aespa-net">https://github.com/kibeom-hong/aespa-net</a></li>
<li>paper_authors: Kibeom Hong, Seogkyu Jeon, Junsoo Lee, Namhyuk Ahn, Kunhee Kim, Pilhyeon Lee, Daesik Kim, Youngjung Uh, Hyeran Byun</li>
<li>for: 这paper是为了解决style transfer问题，即将 contenido image 转换成艺术风格image。</li>
<li>methods: 这paper使用了Attention机制，以实现地图local patches of style image和content image的对应关系。同时，paper also introduces a novel metric called pattern repeatability to quantify the repetition of patterns in the style image, and a self-supervisory task to encourage the attention mechanism to learn precise and meaningful semantic correspondence.</li>
<li>results: 经过qualitative和quantitative evaluations, paper verify了pattern repeatability的可靠性，并demonstrated the superiority of the proposed framework compared to existing methods.<details>
<summary>Abstract</summary>
To deliver the artistic expression of the target style, recent studies exploit the attention mechanism owing to its ability to map the local patches of the style image to the corresponding patches of the content image. However, because of the low semantic correspondence between arbitrary content and artworks, the attention module repeatedly abuses specific local patches from the style image, resulting in disharmonious and evident repetitive artifacts. To overcome this limitation and accomplish impeccable artistic style transfer, we focus on enhancing the attention mechanism and capturing the rhythm of patterns that organize the style. In this paper, we introduce a novel metric, namely pattern repeatability, that quantifies the repetition of patterns in the style image. Based on the pattern repeatability, we propose Aesthetic Pattern-Aware style transfer Networks (AesPA-Net) that discover the sweet spot of local and global style expressions. In addition, we propose a novel self-supervisory task to encourage the attention mechanism to learn precise and meaningful semantic correspondence. Lastly, we introduce the patch-wise style loss to transfer the elaborate rhythm of local patterns. Through qualitative and quantitative evaluations, we verify the reliability of the proposed pattern repeatability that aligns with human perception, and demonstrate the superiority of the proposed framework.
</details>
<details>
<summary>摘要</summary>
通过实现目标风格的艺术表达，最近的研究借鉴了注意机制，因为它可以将风格图像中的本地小块映射到内容图像中的相应小块。然而，由于随机内容和艺术作品之间的 semantics 低，注意模块会不断利用风格图像中的特定本地小块，导致不和谐和明显的重复artefacts。为了超越这个限制和完成无瑕的艺术风格传输，我们将注意力领域的增强和风格图像中的 рит律掌握作为主要目标。在这篇论文中，我们提出了一个新的度量器，即风格图像中的pattern repeatability，用于量化风格图像中的pattern的重复度。基于pattern repeatability，我们提出了Aesthetic Pattern-Aware style transfer Networks（AesPA-Net），该网络可以找到风格图像中的精彩点，并寻找合适的本地和全局风格表达。此外，我们提出了一种新的自我超级视角任务，以便注意力机制学习准确和有意义的semantic相关性。最后，我们引入了 patch-wise style loss，以传递风格图像中的细腻的rhythm。通过质量和量化评估，我们证明了提案的pattern repeatability的可靠性，并证明了我们的提案方框的优越性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Aware-Dual-Contrastive-Learning-for-Multi-label-Image-Classification"><a href="#Semantic-Aware-Dual-Contrastive-Learning-for-Multi-label-Image-Classification" class="headerlink" title="Semantic-Aware Dual Contrastive Learning for Multi-label Image Classification"></a>Semantic-Aware Dual Contrastive Learning for Multi-label Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09715">http://arxiv.org/abs/2307.09715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yu-gi-oh-leilei/sadcl">https://github.com/yu-gi-oh-leilei/sadcl</a></li>
<li>paper_authors: Leilei Ma, Dengdi Sun, Lei Wang, Haifeng Zhao, Bin Luo</li>
<li>for: 提高自然图像中多个对象或特征的图像semantics提取和分类效果</li>
<li>methods: 使用图像semantic关系模型、类 activation maps (CAM) 以及样本对比学习 (SSCL) 和类prototype对比学习 (PSCL)</li>
<li>results: 在五个大规模公共数据集上实验，提出的方法比现有方法高效，并且能够准确捕捉图像内容相关的涉及性 label<details>
<summary>Abstract</summary>
Extracting image semantics effectively and assigning corresponding labels to multiple objects or attributes for natural images is challenging due to the complex scene contents and confusing label dependencies. Recent works have focused on modeling label relationships with graph and understanding object regions using class activation maps (CAM). However, these methods ignore the complex intra- and inter-category relationships among specific semantic features, and CAM is prone to generate noisy information. To this end, we propose a novel semantic-aware dual contrastive learning framework that incorporates sample-to-sample contrastive learning (SSCL) as well as prototype-to-sample contrastive learning (PSCL). Specifically, we leverage semantic-aware representation learning to extract category-related local discriminative features and construct category prototypes. Then based on SSCL, label-level visual representations of the same category are aggregated together, and features belonging to distinct categories are separated. Meanwhile, we construct a novel PSCL module to narrow the distance between positive samples and category prototypes and push negative samples away from the corresponding category prototypes. Finally, the discriminative label-level features related to the image content are accurately captured by the joint training of the above three parts. Experiments on five challenging large-scale public datasets demonstrate that our proposed method is effective and outperforms the state-of-the-art methods. Code and supplementary materials are released on https://github.com/yu-gi-oh-leilei/SADCL.
</details>
<details>
<summary>摘要</summary>
原文：Extracting image semantics effectively and assigning corresponding labels to multiple objects or attributes for natural images is challenging due to the complex scene contents and confusing label dependencies. Recent works have focused on modeling label relationships with graph and understanding object regions using class activation maps (CAM). However, these methods ignore the complex intra- and inter-category relationships among specific semantic features, and CAM is prone to generate noisy information. To this end, we propose a novel semantic-aware dual contrastive learning framework that incorporates sample-to-sample contrastive learning (SSCL) as well as prototype-to-sample contrastive learning (PSCL). Specifically, we leverage semantic-aware representation learning to extract category-related local discriminative features and construct category prototypes. Then based on SSCL, label-level visual representations of the same category are aggregated together, and features belonging to distinct categories are separated. Meanwhile, we construct a novel PSCL module to narrow the distance between positive samples and category prototypes and push negative samples away from the corresponding category prototypes. Finally, the discriminative label-level features related to the image content are accurately captured by the joint training of the above three parts. Experiments on five challenging large-scale public datasets demonstrate that our proposed method is effective and outperforms the state-of-the-art methods. Code and supplementary materials are released on https://github.com/yu-gi-oh-leilei/SADCL.中文翻译：抽取自然图像中的 semantics 效果好并将多个对象或属性分配到相应的标签是一项挑战，原因在于场景内容的复杂性和标签依赖关系的混乱。 recent works 主要关注图标关系模型和对象区域理解使用类 activation maps (CAM)。然而，这些方法忽略了特定semantic feature之间的复杂内部和间接关系，而 CAM 也容易产生噪音信息。为此，我们提出了一种新的semantic-aware dual contrastive learning框架，该框架包括sample-to-sample contrastive learning (SSCL) 以及 prototype-to-sample contrastive learning (PSCL)。具体来说，我们利用semantic-aware representation learning来抽取相应类型的本地特征特征，并构建类型prototype。然后，基于 SSCL，同类标签的视觉表示被聚合在一起，不同类标签的表示被分离开。此外，我们构建了一种新的PSCL模块，以减少正样例与类型prototype之间的距离，并推动负样例离开对应的类型prototype。最后，通过上述三部分的集成，捕捉到图像内容相关的有用特征。 experiments 表明，我们提出的方法效果好，并超过了当前状态的方法。代码和补充材料可以在https://github.com/yu-gi-oh-leilei/SADCL 上获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-Saner-Deep-Image-Registration"><a href="#Towards-Saner-Deep-Image-Registration" class="headerlink" title="Towards Saner Deep Image Registration"></a>Towards Saner Deep Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09696">http://arxiv.org/abs/2307.09696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tuffr5/saner-deep-registration">https://github.com/tuffr5/saner-deep-registration</a></li>
<li>paper_authors: Bin Duan, Ming Zhong, Yan Yan</li>
<li>for: 本文旨在探讨学习基于深度学习的图像registrations方法在医学图像注册中的精度和稳定性。</li>
<li>methods: 本文提出了一种基于reg regularization的sanity-enforcer方法，通过两种sanity check来降低模型的逆向一致性错误和提高其识别力。</li>
<li>results: 实验结果表明，该方法可以 simultanously improve the sanity of models without sacrificing any performance.  Additionally, the authors provide theoretical guarantees for their method.<details>
<summary>Abstract</summary>
With recent advances in computing hardware and surges of deep-learning architectures, learning-based deep image registration methods have surpassed their traditional counterparts, in terms of metric performance and inference time. However, these methods focus on improving performance measurements such as Dice, resulting in less attention given to model behaviors that are equally desirable for registrations, especially for medical imaging. This paper investigates these behaviors for popular learning-based deep registrations under a sanity-checking microscope. We find that most existing registrations suffer from low inverse consistency and nondiscrimination of identical pairs due to overly optimized image similarities. To rectify these behaviors, we propose a novel regularization-based sanity-enforcer method that imposes two sanity checks on the deep model to reduce its inverse consistency errors and increase its discriminative power simultaneously. Moreover, we derive a set of theoretical guarantees for our sanity-checked image registration method, with experimental results supporting our theoretical findings and their effectiveness in increasing the sanity of models without sacrificing any performance. Our code and models are available at https://github.com/tuffr5/Saner-deep-registration.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GlobalMapper-Arbitrary-Shaped-Urban-Layout-Generation"><a href="#GlobalMapper-Arbitrary-Shaped-Urban-Layout-Generation" class="headerlink" title="GlobalMapper: Arbitrary-Shaped Urban Layout Generation"></a>GlobalMapper: Arbitrary-Shaped Urban Layout Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09693">http://arxiv.org/abs/2307.09693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu He, Daniel Aliaga</li>
<li>for:  Layout generation for urban buildings, using graph attention networks for fully automatic generation of realistic city blocks with arbitrary road networks and building shapes.</li>
<li>methods:  Using graph attention networks to skeletonize building layouts and generate realistic urban layouts, with conditional generation based on learned priors.</li>
<li>results:  Superior performance compared to prior layout generation networks, demonstrated through generating layouts for 28 large cities with arbitrary city block and building shapes.<details>
<summary>Abstract</summary>
Modeling and designing urban building layouts is of significant interest in computer vision, computer graphics, and urban applications. A building layout consists of a set of buildings in city blocks defined by a network of roads. We observe that building layouts are discrete structures, consisting of multiple rows of buildings of various shapes, and are amenable to skeletonization for mapping arbitrary city block shapes to a canonical form. Hence, we propose a fully automatic approach to building layout generation using graph attention networks. Our method generates realistic urban layouts given arbitrary road networks, and enables conditional generation based on learned priors. Our results, including user study, demonstrate superior performance as compared to prior layout generation networks, support arbitrary city block and varying building shapes as demonstrated by generating layouts for 28 large cities.
</details>
<details>
<summary>摘要</summary>
《计算机视觉和计算机图形学中的城市建筑布局模拟和设计是非常有价值的。城市建筑布局是一个由城市块网络定义的多个建筑物的集合，我们发现这些布局是离散结构，由多行不同形状的建筑物组成，可以通过skeletonization来将城市块形状划分为标准形状。因此，我们提出了一种完全自动化的城市建筑布局生成方法，使用图注意网络进行生成。我们的方法可以根据学习的先验知识进行Conditional生成，并且可以处理任意城市块和不同的建筑物形状。我们的结果，包括用户调查，显示了与之前的布局生成网络相比，我们的方法表现更出色，并且可以生成28个大城市的布局。》
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-based-Enhanced-Detection-for-Autonomous-Driving-in-Foggy-and-Rainy-Weather"><a href="#Domain-Adaptation-based-Enhanced-Detection-for-Autonomous-Driving-in-Foggy-and-Rainy-Weather" class="headerlink" title="Domain Adaptation based Enhanced Detection for Autonomous Driving in Foggy and Rainy Weather"></a>Domain Adaptation based Enhanced Detection for Autonomous Driving in Foggy and Rainy Weather</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09676">http://arxiv.org/abs/2307.09676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinlong Li, Runsheng Xu, Jin Ma, Qin Zou, Jiaqi Ma, Hongkai Yu</li>
<li>for: 本研究旨在解决自动驾驶摄像头检测中的频繁出现的检测瓶颈问题，即在雾气和雨天下，模型训练在晴天下的检测模型可能无法达到预期的性能。</li>
<li>methods: 本研究提出了一种域 adaptive 对象检测框架，通过在图像水平和对象水平进行适应，以减少不同域之间的差异。此外，我们还提出了一种新的对抗梯度逆转层，通过对困难的实例进行对抗挑战，以提高模型在挑战性实例上的表现。最后，我们还提出了通过数据增强来生成一个辅助域，以便在新域上强制执行一个新的域级别的质量规则。</li>
<li>results: 实验结果表明，本研究在公共 V2V 测试集上实现了对 foggy 和 rainy 驾驶景情的对象检测性能的显著提高。<details>
<summary>Abstract</summary>
Typically, object detection methods for autonomous driving that rely on supervised learning make the assumption of a consistent feature distribution between the training and testing data, however such assumption may fail in different weather conditions. Due to the domain gap, a detection model trained under clear weather may not perform well in foggy and rainy conditions. Overcoming detection bottlenecks in foggy and rainy weather is a real challenge for autonomous vehicles deployed in the wild. To bridge the domain gap and improve the performance of object detectionin foggy and rainy weather, this paper presents a novel framework for domain-adaptive object detection. The adaptations at both the image-level and object-level are intended to minimize the differences in image style and object appearance between domains. Furthermore, in order to improve the model's performance on challenging examples, we introduce a novel adversarial gradient reversal layer that conducts adversarial mining on difficult instances in addition to domain adaptation. Additionally, we suggest generating an auxiliary domain through data augmentation to enforce a new domain-level metric regularization. Experimental findings on public V2V benchmark exhibit a substantial enhancement in object detection specifically for foggy and rainy driving scenarios.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:通常，基于supervised learning的自动驾驶对象检测方法假设了测试数据和训练数据中特征分布的一致性，但这个假设在不同的天气conditions下可能会失败。由于领域差距，训练在晴朗天气下的检测模型可能无法在雾天和雨天条件下表现好。为了在野外部署自动驾驶车辆中适应雾天和雨天条件，本文提出了一种适应领域对象检测框架。这种框架在图像和对象两级进行了适应，以减少图像和对象之间的差异。此外，我们还引入了一种新的对抗梯度反转层，以便在困难的实例上进行对抗挖掘。此外，我们还建议通过数据增强来生成一个辅助领域，以便强制执行一个新的领域级别的度量规则。实验结果表明，在公共V2Vbenchmark上，对雾天和雨天驾驶场景的对象检测表现得到了显著提高。
</details></li>
</ul>
<hr>
<h2 id="Object-aware-Gaze-Target-Detection"><a href="#Object-aware-Gaze-Target-Detection" class="headerlink" title="Object-aware Gaze Target Detection"></a>Object-aware Gaze Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09662">http://arxiv.org/abs/2307.09662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/francescotonini/object-aware-gaze-target-detection">https://github.com/francescotonini/object-aware-gaze-target-detection</a></li>
<li>paper_authors: Francesco Tonini, Nicola Dall’Asen, Cigdem Beyan, Elisa Ricci</li>
<li>for: 这个论文的目的是提出一种基于Transformer架构的目标识别方法，以便自动检测场景中的人员和被注视的对象，并建立每个人头和被注视对象之间的关系。</li>
<li>methods: 这种方法使用了Transformer架构，自动检测场景中的对象（包括人头），并建立每个人头和被注视对象之间的关系，从而实现了全面、可解释的眼动分析。</li>
<li>results: 根据在野外 benchmark 的评估，该方法在所有指标上达到了状态的艺术 Results（增加了2.91%的AUC、降低了50%的眼动距离、提高了9%的离屏平均精度），并在眼动目标检测和眼动对象的分类和 lokalisierung 方面提供了11-13%的提升。<details>
<summary>Abstract</summary>
Gaze target detection aims to predict the image location where the person is looking and the probability that a gaze is out of the scene. Several works have tackled this task by regressing a gaze heatmap centered on the gaze location, however, they overlooked decoding the relationship between the people and the gazed objects. This paper proposes a Transformer-based architecture that automatically detects objects (including heads) in the scene to build associations between every head and the gazed-head/object, resulting in a comprehensive, explainable gaze analysis composed of: gaze target area, gaze pixel point, the class and the image location of the gazed-object. Upon evaluation of the in-the-wild benchmarks, our method achieves state-of-the-art results on all metrics (up to 2.91% gain in AUC, 50% reduction in gaze distance, and 9% gain in out-of-frame average precision) for gaze target detection and 11-13% improvement in average precision for the classification and the localization of the gazed-objects. The code of the proposed method is available https://github.com/francescotonini/object-aware-gaze-target-detection
</details>
<details>
<summary>摘要</summary>
目标尝试检测目标目标在图像中的位置和概率是否离屏。一些工作已经解决这个任务，但它们忽略了人与观看的对象之间的关系。这篇论文提议一种基于Transformer架构的方法，自动检测场景中的对象（包括头部），并建立每个头部和观看的对象之间的关联，从而实现全面、可解释的视线分析，包括视线目标区域、视线像素点、类别和图像中的对象位置。在评估在野 benchmark中，我们的方法实现了状态机器人的结果（增加了2.91%的AUC，减少了50%的视线距离，增加了9%的离屏平均精度）。此外，我们的方法还提高了对观看的对象的分类和 lokalisierung的平均精度，增加了11-13%。我们的代码可以在https://github.com/francescotonini/object-aware-gaze-target-detection上获取。
</details></li>
</ul>
<hr>
<h2 id="Skin-Lesion-Correspondence-Localization-in-Total-Body-Photography"><a href="#Skin-Lesion-Correspondence-Localization-in-Total-Body-Photography" class="headerlink" title="Skin Lesion Correspondence Localization in Total Body Photography"></a>Skin Lesion Correspondence Localization in Total Body Photography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09642">http://arxiv.org/abs/2307.09642</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weilunhuang-jhu/lesioncorrespondencetbp3d">https://github.com/weilunhuang-jhu/lesioncorrespondencetbp3d</a></li>
<li>paper_authors: Wei-Lun Huang, Davood Tashayyod, Jun Kang, Amir Gandjbakhche, Michael Kazhdan, Mehran Armand</li>
<li>for: 长期跟踪皮肤 lesion 的找寻、变化和Texture 有助于早期检测皮肤癌。但是，在全身图像中并未得到充分的研究。</li>
<li>methods: 我们提出了一种新的框架，将几何信息和Texture 信息结合使用，以确定皮肤 lesion 在全身图像中的匹配。首先，在源和目标3D 纹理模型上创建了体部标记或稀疏匹配。然后，每个顶点在每个模型上都被映射到一个表示 geodesic 距离标记的特征向量中。最后，对每个 source 中的 lesion 进行了粗略估算，使用几何信息编码在特征向量中，然后使用 Texture 信息进行精细调整。</li>
<li>results: 我们对公共和私人数据集进行了量化评估，与只有一次 longitudinal 研究的成功率相当。随着全身3D 捕捉的质量和频率的提高，我们预计提出的方法将成为跨度跟踪皮肤 lesion 的重要步骤。<details>
<summary>Abstract</summary>
Longitudinal tracking of skin lesions - finding correspondence, changes in morphology, and texture - is beneficial to the early detection of melanoma. However, it has not been well investigated in the context of full-body imaging. We propose a novel framework combining geometric and texture information to localize skin lesion correspondence from a source scan to a target scan in total body photography (TBP). Body landmarks or sparse correspondence are first created on the source and target 3D textured meshes. Every vertex on each of the meshes is then mapped to a feature vector characterizing the geodesic distances to the landmarks on that mesh. Then, for each lesion of interest (LOI) on the source, its corresponding location on the target is first coarsely estimated using the geometric information encoded in the feature vectors and then refined using the texture information. We evaluated the framework quantitatively on both a public and a private dataset, for which our success rates (at 10 mm criterion) are comparable to the only reported longitudinal study. As full-body 3D capture becomes more prevalent and has higher quality, we expect the proposed method to constitute a valuable step in the longitudinal tracking of skin lesions.
</details>
<details>
<summary>摘要</summary>
longitudinal tracking of skin lesions - finding correspondence, changes in morphology, and texture - is beneficial to the early detection of melanoma. However, it has not been well investigated in the context of full-body imaging. We propose a novel framework combining geometric and texture information to localize skin lesion correspondence from a source scan to a target scan in total body photography (TBP). Body landmarks or sparse correspondence are first created on the source and target 3D textured meshes. Every vertex on each of the meshes is then mapped to a feature vector characterizing the geodesic distances to the landmarks on that mesh. Then, for each lesion of interest (LOI) on the source, its corresponding location on the target is first coarsely estimated using the geometric information encoded in the feature vectors and then refined using the texture information. We evaluated the framework quantitatively on both a public and a private dataset, for which our success rates (at 10 mm criterion) are comparable to the only reported longitudinal study. As full-body 3D capture becomes more prevalent and has higher quality, we expect the proposed method to constitute a valuable step in the longitudinal tracking of skin lesions.Here's the translation in Traditional Chinese as well, for your reference:Longitudinal tracking of skin lesions - finding correspondence, changes in morphology, and texture - is beneficial to the early detection of melanoma. However, it has not been well investigated in the context of full-body imaging. We propose a novel framework combining geometric and texture information to localize skin lesion correspondence from a source scan to a target scan in total body photography (TBP). Body landmarks or sparse correspondence are first created on the source and target 3D textured meshes. Every vertex on each of the meshes is then mapped to a feature vector characterizing the geodesic distances to the landmarks on that mesh. Then, for each lesion of interest (LOI) on the source, its corresponding location on the target is first coarsely estimated using the geometric information encoded in the feature vectors and then refined using the texture information. We evaluated the framework quantitatively on both a public and a private dataset, for which our success rates (at 10 mm criterion) are comparable to the only reported longitudinal study. As full-body 3D capture becomes more prevalent and has higher quality, we expect the proposed method to constitute a valuable step in the longitudinal tracking of skin lesions.I hope this helps!
</details></li>
</ul>
<hr>
<h2 id="Conditional-360-degree-Image-Synthesis-for-Immersive-Indoor-Scene-Decoration"><a href="#Conditional-360-degree-Image-Synthesis-for-Immersive-Indoor-Scene-Decoration" class="headerlink" title="Conditional 360-degree Image Synthesis for Immersive Indoor Scene Decoration"></a>Conditional 360-degree Image Synthesis for Immersive Indoor Scene Decoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09621">http://arxiv.org/abs/2307.09621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kcshum/neural_360_decoration">https://github.com/kcshum/neural_360_decoration</a></li>
<li>paper_authors: Ka Chun Shum, Hong-Wing Pang, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung</li>
<li>for: 这个论文targets the problem of conditional scene decoration for 360-degree images, aiming to generate decorated images of the same scene in panorama view.</li>
<li>methods: 该方法首先开发了一个360度对象布局生成器，用于学习360度视图中对象的约束，然后使用这个对象布局来condition a generative adversarial network（GAN）来 sinthezize输入场景的图像。另外，他们还开发了一个简单 yet effective的场景清空器，用于 removing generated furniture and producing an emptied scene for the model to learn a cyclic constraint.</li>
<li>results: 经过训练于Structure3D dataset，该模型可以生成多种可控的对象布局，并且在Zillow indoor scene dataset上显示出优秀的性能。用户研究表明，该生成结果具有真实的图像质量和家具布局，提供了 immerse 的经验。<details>
<summary>Abstract</summary>
In this paper, we address the problem of conditional scene decoration for 360-degree images. Our method takes a 360-degree background photograph of an indoor scene and generates decorated images of the same scene in the panorama view. To do this, we develop a 360-aware object layout generator that learns latent object vectors in the 360-degree view to enable a variety of furniture arrangements for an input 360-degree background image. We use this object layout to condition a generative adversarial network to synthesize images of an input scene. To further reinforce the generation capability of our model, we develop a simple yet effective scene emptier that removes the generated furniture and produces an emptied scene for our model to learn a cyclic constraint. We train the model on the Structure3D dataset and show that our model can generate diverse decorations with controllable object layout. Our method achieves state-of-the-art performance on the Structure3D dataset and generalizes well to the Zillow indoor scene dataset. Our user study confirms the immersive experiences provided by the realistic image quality and furniture layout in our generation results. Our implementation will be made available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们解决了基于360度图像的条件场景饰化问题。我们的方法使用了一张360度背景图片，并生成了相同场景的潮拂视图图像。为了实现这一点，我们开发了一个360度对象布局生成器，它学习了360度视图中对象的秘密 вектор，以便在输入360度背景图片的基础上实现多种家具的排列。我们使用这个对象布局来condition一个生成敌对网络，以生成输入场景的图像。为了进一步强化我们的模型的生成能力，我们开发了一个简单 yet有效的场景空置器，它将生成的家具移除，并生成了一个空的场景，以便我们的模型学习一个循环约束。我们在Structure3D数据集上训练了我们的模型，并证明了我们的模型可以生成多样的饰化，并且可以控制对象布局。我们的方法实现了状态对的性能在Structure3D数据集上，并且在Zillow室内场景数据集上也具有良好的普适性。我们的用户研究证明了我们的生成结果提供了真实的图像质量和家具布局，使用者可以获得沉浸式的 immerse 体验。我们的实现将被公开。
</details></li>
</ul>
<hr>
<h2 id="Surgical-Action-Triplet-Detection-by-Mixed-Supervised-Learning-of-Instrument-Tissue-Interactions"><a href="#Surgical-Action-Triplet-Detection-by-Mixed-Supervised-Learning-of-Instrument-Tissue-Interactions" class="headerlink" title="Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions"></a>Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09548">http://arxiv.org/abs/2307.09548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav Sharma, Chinedu Innocent Nwoye, Didier Mutter, Nicolas Padoy</li>
<li>for: 这篇论文的目的是提出一种基于交互图的多类 instrumente-aware transformer 模型，用于检测手术动作三元组。</li>
<li>methods: 该模型使用 MCIT 阶段和 IG 阶段两个部分组成，MCIT 阶段模型每个目标的类别特征，以减少 triplet 杂合的风险；IG 阶段构建了一个两侧动态图，以模型 instrumente 和目标之间的互动。</li>
<li>results: 在 CholectT50 数据集上测试，该模型提高了 instrumente localization 和 triplet 检测的性能，并在 MICCAI 2022 年的 CholecTriplet 挑战中击败了所有参赛者，提出了新的顶峰性表现。<details>
<summary>Abstract</summary>
Surgical action triplets describe instrument-tissue interactions as (instrument, verb, target) combinations, thereby supporting a detailed analysis of surgical scene activities and workflow. This work focuses on surgical action triplet detection, which is challenging but more precise than the traditional triplet recognition task as it consists of joint (1) localization of surgical instruments and (2) recognition of the surgical action triplet associated with every localized instrument. Triplet detection is highly complex due to the lack of spatial triplet annotation. We analyze how the amount of instrument spatial annotations affects triplet detection and observe that accurate instrument localization does not guarantee better triplet detection due to the risk of erroneous associations with the verbs and targets. To solve the two tasks, we propose MCIT-IG, a two-stage network, that stands for Multi-Class Instrument-aware Transformer-Interaction Graph. The MCIT stage of our network models per class embedding of the targets as additional features to reduce the risk of misassociating triplets. Furthermore, the IG stage constructs a bipartite dynamic graph to model the interaction between the instruments and targets, cast as the verbs. We utilize a mixed-supervised learning strategy that combines weak target presence labels for MCIT and pseudo triplet labels for IG to train our network. We observed that complementing minimal instrument spatial annotations with target embeddings results in better triplet detection. We evaluate our model on the CholecT50 dataset and show improved performance on both instrument localization and triplet detection, topping the leaderboard of the CholecTriplet challenge in MICCAI 2022.
</details>
<details>
<summary>摘要</summary>
针对手术场景中的 instrumente-tissue 交互，我们提出了“手术动作 triplets”的概念，即（instrument，verb，target）组合。这种概念支持etailed分析手术过程中的活动和工作流程。本文主要关注手术动作 triplet 检测，这是传统 triplet 认识任务的更加精准和复杂的一种。由于缺乏空间 triplet 标注，手术动作 triplet 检测是非常复杂的。我们分析了不同量的 instrumente 空间标注对 triplet 检测的影响，并观察到了虽然精确的 instrumente localization 不能保证更好的 triplet 检测，因为可能会 mistakenly associate with verbs and targets。为解决这两个任务，我们提出了 MCIT-IG 网络，即 Multi-Class Instrument-aware Transformer-Interaction Graph。MCIT 阶段中，我们模型了每个目标的多类 embedding，以降低将 triplets 混淆的风险。IG 阶段则构建了一个双向动态图，以模型 instrumente 和目标之间的互动，即 cast as verbs。我们采用了一种混合强化学习策略，将 MCIT 和 IG 两个阶段的训练数据进行混合。我们发现，通过补充 minimal instrumente 空间标注与目标嵌入，可以提高 triplet 检测的性能。我们对 CholecT50 数据集进行了评估，并在 MICCAI 2022 年的 CholecTriplet 挑战中排名第一。
</details></li>
</ul>
<hr>
<h2 id="Can-Neural-Network-Memorization-Be-Localized"><a href="#Can-Neural-Network-Memorization-Be-Localized" class="headerlink" title="Can Neural Network Memorization Be Localized?"></a>Can Neural Network Memorization Be Localized?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09542">http://arxiv.org/abs/2307.09542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pratyushmaini/localizing-memorization">https://github.com/pratyushmaini/localizing-memorization</a></li>
<li>paper_authors: Pratyush Maini, Michael C. Mozer, Hanie Sedghi, Zachary C. Lipton, J. Zico Kolter, Chiyuan Zhang</li>
<li>for: 这 paper 旨在解释深度过参数网络中 memorization 和 generalization 之间的交互作用。</li>
<li>methods: 这 paper 使用了 gradient accounting、layer rewinding 和 retraining 等三种实验来证明 memorization 不仅局限于最后几层，而是围绕一小组神经元分布在不同层次。</li>
<li>results: 研究发现， memorization 通常局限于模型中几个神经元或通道（大约5），而且可以通过 example-tied dropout 来引导 memorization 到预先确定的神经元集。这种dropout方法可以降低 memorized examples 的准确率从 100% 降至 3%，同时降低了泛化差距。<details>
<summary>Abstract</summary>
Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks $\textit{memorize}$ "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on $\textit{atypical}$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are $\textit{gradient accounting}$ (measuring the contribution to the gradient norms from memorized and clean examples), $\textit{layer rewinding}$ (replacing specific model weights of a converged model with previous training checkpoints), and $\textit{retraining}$ (training rewound layers only on clean examples). Second, we ask a more generic question: can memorization be localized $\textit{anywhere}$ in a model? We discover that memorization is often confined to a small number of neurons or channels (around 5) of the model. Based on these insights we propose a new form of dropout -- $\textit{example-tied dropout}$ that enables us to direct the memorization of examples to an apriori determined set of neurons. By dropping out these neurons, we are able to reduce the accuracy on memorized examples from $100\%\to3\%$, while also reducing the generalization gap.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，深度过参数网络中的记忆和通用之间存在一定的互动。研究人员认为，神经网络会在训练集中的罕见示例上进行记忆。记忆指的是能够正确预测训练集中的非典型示例。在这项工作中，我们发现了一些有利的证据，证明记忆不仅存在于各个层之间，而且受到一些特定层的限制。我们通过三种实验来证明这一点，即梯度考虑（测量模型中记忆和干净示例的贡献）、层恢复（将特定模型参数替换为训练过程中的检查点）和重训练（只训练恢复的层）。我们还提出了一个新的抽象概念——示例绑定Dropout，可以让我们将记忆示例直接链接到预先确定的神经元。通过dropout这些神经元，我们能够将记忆示例的准确率降低到3%，同时降低通用性隔阂。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Bayesian-Augmentation-for-Single-Source-Domain-Generalization"><a href="#Adversarial-Bayesian-Augmentation-for-Single-Source-Domain-Generalization" class="headerlink" title="Adversarial Bayesian Augmentation for Single-Source Domain Generalization"></a>Adversarial Bayesian Augmentation for Single-Source Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09520">http://arxiv.org/abs/2307.09520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheng Cheng, Tejas Gokhale, Yezhou Yang</li>
<li>for: 该论文旨在解决难以泛化到未经见过的图像频谱问题，主要是因为缺乏多样化的训练数据、不可达的目标数据以及实际情况中的大频率差异。</li>
<li>methods: 该论文提出了一种新的算法——对抗抽象学习极 Bayesian 增强（ABA），用于生成单源频谱下的图像增强。 ABA 利用了对抗学习和极 Bayesian 神经网络来引导生成多样化的数据增强——这些生成的图像频谱可以帮助分类器泛化到未经见过的频谱。</li>
<li>results: 作者在多种频谱差异情况下（包括风格差异、子population差异和医学成像中的差异）进行了实验，并证明了 ABA 能够超越所有之前的状态艺术方法，包括预先定义的增强、像素级增强和卷积级增强。<details>
<summary>Abstract</summary>
Generalizing to unseen image domains is a challenging problem primarily due to the lack of diverse training data, inaccessible target data, and the large domain shift that may exist in many real-world settings. As such data augmentation is a critical component of domain generalization methods that seek to address this problem. We present Adversarial Bayesian Augmentation (ABA), a novel algorithm that learns to generate image augmentations in the challenging single-source domain generalization setting. ABA draws on the strengths of adversarial learning and Bayesian neural networks to guide the generation of diverse data augmentations -- these synthesized image domains aid the classifier in generalizing to unseen domains. We demonstrate the strength of ABA on several types of domain shift including style shift, subpopulation shift, and shift in the medical imaging setting. ABA outperforms all previous state-of-the-art methods, including pre-specified augmentations, pixel-based and convolutional-based augmentations.
</details>
<details>
<summary>摘要</summary>
通用到未经见的图像领域是一个具有挑战性的问题，主要由于训练数据缺乏多样性、目标数据困难获取以及实际场景中可能存在很大的领域变化。因此，数据增强成为领域总结方法中的关键组成部分。我们介绍了一种新的算法——对抗权重学增强（ABA），可以在单源领域总结Setting中学习生成图像增强。ABA借鉴了对抗学习和权重神经网络的优势，以引导生成多样化的数据增强——这些合成的图像领域帮助分类器在未经见的领域中总结。我们在多种领域shift中展示了ABA的强大性，包括风格shift、子群体shift和医学成像设置中的shift。ABA在所有前一个状态的方法之上升迁，包括预先指定的增强、像素基于的增强和卷积基于的增强。
</details></li>
</ul>
<hr>
<h2 id="AnyDoor-Zero-shot-Object-level-Image-Customization"><a href="#AnyDoor-Zero-shot-Object-level-Image-Customization" class="headerlink" title="AnyDoor: Zero-shot Object-level Image Customization"></a>AnyDoor: Zero-shot Object-level Image Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09481">http://arxiv.org/abs/2307.09481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, Hengshuang Zhao</li>
<li>for:  This paper presents a novel image generator called AnyDoor, which can teleport objects to new scenes in a harmonious way.</li>
<li>methods: The model uses a diffusion-based approach and is trained only once to generalize to diverse object-scene combinations at inference. The model also incorporates detail features to maintain texture details and support object blending with different surroundings.</li>
<li>results: The approach demonstrates superiority over existing alternatives and has great potential in real-world applications such as virtual try-on and object moving, as shown through extensive experiments.Here’s the Chinese translation:</li>
<li>for: 这篇论文提出了一种名为AnyDoor的图像生成器，可以将目标对象teleport到新的场景中，并在用户指定的位置进行融合。</li>
<li>methods: 模型采用了扩散基于的方法，并在推理阶段仅需要一次训练，以便通用多种对象-场景组合。模型还具有细节特征，以保持Texture细节并允许多种本地变化（例如光照、方向、姿势等），以支持对象融合不同的环境。</li>
<li>results: 方法在对 existed alternatives 的比较中表现出色，并在虚拟试穿和对象移动等实际应用中具有潜在的应用前景，根据广泛的实验结果。<details>
<summary>Abstract</summary>
This work presents AnyDoor, a diffusion-based image generator with the power to teleport target objects to new scenes at user-specified locations in a harmonious way. Instead of tuning parameters for each object, our model is trained only once and effortlessly generalizes to diverse object-scene combinations at the inference stage. Such a challenging zero-shot setting requires an adequate characterization of a certain object. To this end, we complement the commonly used identity feature with detail features, which are carefully designed to maintain texture details yet allow versatile local variations (e.g., lighting, orientation, posture, etc.), supporting the object in favorably blending with different surroundings. We further propose to borrow knowledge from video datasets, where we can observe various forms (i.e., along the time axis) of a single object, leading to stronger model generalizability and robustness. Extensive experiments demonstrate the superiority of our approach over existing alternatives as well as its great potential in real-world applications, such as virtual try-on and object moving. Project page is https://damo-vilab.github.io/AnyDoor-Page/.
</details>
<details>
<summary>摘要</summary>
这个工作介绍了AnyDoor，一种基于扩散的图像生成器，可以让目标对象在用户指定的位置上新场景中快速传输。而不需要对每个对象进行参数调整，我们的模型只需要一次性训练，并在推理阶段自动适应多种对象-场景组合。这种具有挑战性的零shot设定需要对特定对象进行充分的特征化。为此，我们在标准的标识特征之外，采用细节特征，这些特征旨在保持Texture细节，同时允许多元的本地变化（例如照明、方向、姿势等），使对象能够顺利地融入不同的环境中。此外，我们还提出了借鉴视频 datasets，我们可以在时间轴上观察不同形态（例如不同的照明、方向、姿势等）的同一个对象，从而提高模型的泛化和Robustness。广泛的实验证明了我们的方法的优越性以及其在实际应用中的巨大潜力，如虚拟试穿和对象移动等。项目页面可以在https://damo-vilab.github.io/AnyDoor-Page/中找到。
</details></li>
</ul>
<hr>
<h2 id="FACTS-Facial-Animation-Creation-using-the-Transfer-of-Styles"><a href="#FACTS-Facial-Animation-Creation-using-the-Transfer-of-Styles" class="headerlink" title="FACTS: Facial Animation Creation using the Transfer of Styles"></a>FACTS: Facial Animation Creation using the Transfer of Styles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09480">http://arxiv.org/abs/2307.09480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Saunders, Steven Caulkin, Vinay Namboodiri</li>
<li>for: 这篇论文的目的是为了创造更有表情的电子游戏角色。</li>
<li>methods: 这篇论文使用了一种新的方法，即通过修改样式特征来实现表情动画。具体来说，他们使用了一个StarGAN来转换3D表情动画为不同的情感和个人特有的样式。</li>
<li>results: 通过这种方法，他们能够保持动画的唇同步，并且可以实现不同的情感和个人特有的表情动画。<details>
<summary>Abstract</summary>
The ability to accurately capture and express emotions is a critical aspect of creating believable characters in video games and other forms of entertainment. Traditionally, this animation has been achieved with artistic effort or performance capture, both requiring costs in time and labor. More recently, audio-driven models have seen success, however, these often lack expressiveness in areas not correlated to the audio signal. In this paper, we present a novel approach to facial animation by taking existing animations and allowing for the modification of style characteristics. Specifically, we explore the use of a StarGAN to enable the conversion of 3D facial animations into different emotions and person-specific styles. We are able to maintain the lip-sync of the animations with this method thanks to the use of a novel viseme-preserving loss.
</details>
<details>
<summary>摘要</summary>
“创造真实的人物表情是视频游戏和娱乐媒体中非常重要的一部分。传统上，这种动画通常需要艺术努力或表演捕捉，它们需要大量的时间和劳动。在最近几年，听取模型已经取得了成功，但它们通常缺乏不相关于声音信号的表达能力。在这篇论文中，我们提出了一种新的面部动画方法，即通过现有动画的修改，以达到不同情感和人Specific风格。我们使用了一种名为StarGAN的模型，并使用一种新的视emeserving损失来保持动画的唇同步。”
</details></li>
</ul>
<hr>
<h2 id="GroupLane-End-to-End-3D-Lane-Detection-with-Channel-wise-Grouping"><a href="#GroupLane-End-to-End-3D-Lane-Detection-with-Channel-wise-Grouping" class="headerlink" title="GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping"></a>GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09472">http://arxiv.org/abs/2307.09472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoling Li, Chunrui Han, Zheng Ge, Jinrong Yang, En Yu, Haoqian Wang, Hengshuang Zhao, Xiangyu Zhang</li>
<li>for: 提高3D幕면检测效率，适应实际应用需求</li>
<li>methods: 提出一种简单、快速、端到端检测器，同时保持高检测精度</li>
<li>results: 在3个实际3D幕面测试 benchmark 上，GroupLane 比 published state-of-the-art PersFormer 提高13.6% F1 分数，并且具有 faster inference speed 和 lower FLOPs than PersFormer<details>
<summary>Abstract</summary>
Efficiency is quite important for 3D lane detection due to practical deployment demand. In this work, we propose a simple, fast, and end-to-end detector that still maintains high detection precision. Specifically, we devise a set of fully convolutional heads based on row-wise classification. In contrast to previous counterparts, ours supports recognizing both vertical and horizontal lanes. Besides, our method is the first one to perform row-wise classification in bird-eye-view. In the heads, we split feature into multiple groups and every group of feature corresponds to a lane instance. During training, the predictions are associated with lane labels using the proposed single-win one-to-one matching to compute loss, and no post-processing operation is demanded for inference. In this way, our proposed fully convolutional detector, GroupLane, realizes end-to-end detection like DETR. Evaluated on 3 real world 3D lane benchmarks, OpenLane, Once-3DLanes, and OpenLane-Huawei, GroupLane adopting ConvNext-Base as the backbone outperforms the published state-of-the-art PersFormer by 13.6% F1 score in the OpenLane validation set. Besides, GroupLane with ResNet18 still surpasses PersFormer by 4.9% F1 score, while the inference speed is nearly 7x faster and the FLOPs is only 13.3% of it.
</details>
<details>
<summary>摘要</summary>
“效率是3D跑道探测中非常重要的因素，在这个工作中，我们提出了一个简单、快速、端到端的探测器，可以保持高的探测精度。具体来说，我们设计了一组全 convolutional 头，基于行间分类。与前一代相比，我们的方法可以识别 both  Vertical 和 Horizontal 跑道。此外，我们的方法是第一个在 bird-eye-view 中进行行间分类的。在 heads 中，我们将特征分为多个群体，每个群体的特征都与一个跑道实例相对应。在训练时，预测与跑道标签之间的对应使用我们提出的单一胜一对一匹配算法来计算损失，而探测时不需要进行后处理操作。这样，我们的提案的全 convolutional 探测器，GroupLane，可以实现端到端探测，类似于DETR。在3个真实世界3D跑道benchmark上进行评估，GroupLane 使用 ConvNext-Base 作为背景时，与已出版的State-of-the-art PersFormer 相比，在 OpenLane 验证集上提高了13.6% F1 分数。此外，GroupLane 使用 ResNet18 仍然超过 PersFormer 的4.9% F1 分数，而且探测速度几乎是7倍 faster，而 FLOPs 仅仅是13.3% 的它。”
</details></li>
</ul>
<hr>
<h2 id="Occlusion-Aware-Student-Emotion-Recognition-based-on-Facial-Action-Unit-Detection"><a href="#Occlusion-Aware-Student-Emotion-Recognition-based-on-Facial-Action-Unit-Detection" class="headerlink" title="Occlusion Aware Student Emotion Recognition based on Facial Action Unit Detection"></a>Occlusion Aware Student Emotion Recognition based on Facial Action Unit Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09465">http://arxiv.org/abs/2307.09465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shrouk Wally, Ahmed Elsayed, Islam Alkabbany, Asem Ali, Aly Farag</li>
<li>for: 提高课堂环境质量，提高STEM学生在大学初年留存率。</li>
<li>methods: 使用人工 occlusion 数据集和注意力机制，提出 occlusion-aware 表情活动单元抽取方法，以便在课堂 SETTINGS 中识别表情。</li>
<li>results: 研究发现 occlusion 对 facial recognition 模型的性能有较大影响，并提出了一种 occlusion-aware 的表情活动单元抽取方法，可以提高识别表情的可靠性。<details>
<summary>Abstract</summary>
Given that approximately half of science, technology, engineering, and mathematics (STEM) undergraduate students in U.S. colleges and universities leave by the end of the first year [15], it is crucial to improve the quality of classroom environments. This study focuses on monitoring students' emotions in the classroom as an indicator of their engagement and proposes an approach to address this issue. The impact of different facial parts on the performance of an emotional recognition model is evaluated through experimentation. To test the proposed model under partial occlusion, an artificially occluded dataset is introduced. The novelty of this work lies in the proposal of an occlusion-aware architecture for facial action units (AUs) extraction, which employs attention mechanism and adaptive feature learning. The AUs can be used later to classify facial expressions in classroom settings.   This research paper's findings provide valuable insights into handling occlusion in analyzing facial images for emotional engagement analysis. The proposed experiments demonstrate the significance of considering occlusion and enhancing the reliability of facial analysis models in classroom environments. These findings can also be extended to other settings where occlusions are prevalent.
</details>
<details>
<summary>摘要</summary>
据统计，约一半的科学、技术、工程和数学（STEM）本科学生在美国大学和学院内退学之前一年结束。因此，改善课堂环境质量是非常重要的。这项研究关注课堂中学生的情绪状况作为参与度的指标，并提出了一种解决方案。本研究通过实验测试不同的面部部分对情绪识别模型的性能的影响。为了测试提议的模型在受部分遮挡的情况下的性能，我们引入了一个人工遮挡数据集。本研究的新特点在于提出了一种遮挡意识的面部动作单元（AU）提取方法，该方法使用了注意力机制和适应特征学习。这些AU可以用于后续在课堂设置中分类表情。本研究的发现对于处理覆盖情况的面部图像情感参与度分析提供了有价值的信息。提议的实验表明了在课堂环境中考虑覆盖和提高面部分析模型的可靠性是非常重要的。这些发现可以应用到其他受覆盖的场景中。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Student-Behavioral-Engagement-using-Histogram-of-Actions"><a href="#Measuring-Student-Behavioral-Engagement-using-Histogram-of-Actions" class="headerlink" title="Measuring Student Behavioral Engagement using Histogram of Actions"></a>Measuring Student Behavioral Engagement using Histogram of Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09420">http://arxiv.org/abs/2307.09420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</li>
<li>for: 本研究提出了一种新的学生行为参与度测量技术，通过识别学生的动作来预测学生行为参与度水平。</li>
<li>methods: 本研究使用人体骨架来模拟学生姿势和上半身运动，并使用3D-CNN模型来学习学生上半身的动态。然后，使用训练好的3D-CNN模型来识别每段2分钟视频中的动作，并将这些动作组织成一个历史gram，该历史gram用于输入SVM分类器来确定学生是否参与度高或低。</li>
<li>results: 实验结果表明，本研究的方法可以识别学生动作的准确率达83.63%，并且可以捕捉教室中学生的平均参与度。<details>
<summary>Abstract</summary>
In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition. The proposed approach recognizes student actions then predicts the student behavioral engagement level. For student action recognition, we use human skeletons to model student postures and upper body movements. To learn the dynamics of student upper body, a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to build a histogram of actions which encodes the student actions and their frequencies. This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged. To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 actions and 112 video segments annotated with two engagement levels. Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framework can capture the average engagement of the class.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的技术来测量学生的行为参与度。我们的方法首先识别学生的行为，然后预测学生的参与度水平。为了识别学生的行为，我们使用人体骨架来模拟学生的姿势和上半身运动。为了学习学生上半身的动态，我们使用3D-CNN模型进行训练。训练完成后，我们使用3D-CNN模型来识别每段2分钟视频中的行为，并将这些行为组织成一个历史gram中，其中每个行为的频率都是其对应的值。这个历史gram作为输入，我们使用支持向量机进行分类，以判断学生是否参与到活动中。为了评估我们的方法，我们建立了一个包含1414个2分钟视频段和112个视频段的数据集，其中每个视频段都有13种行为和两个参与度水平的标注。实验结果表明，我们的方法可以识别学生的行为 WITH top 1 准确率83.63%，并且可以捕捉学生班级的平均参与度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/cs.CV_2023_07_19/" data-id="cloh7tqf100eh7b88gorggaik" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/cs.AI_2023_07_19/" class="article-date">
  <time datetime="2023-07-19T12:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/cs.AI_2023_07_19/">cs.AI - 2023-07-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Decision-Making-Framework-for-Recommended-Maintenance-of-Road-Segments"><a href="#A-Decision-Making-Framework-for-Recommended-Maintenance-of-Road-Segments" class="headerlink" title="A Decision Making Framework for Recommended Maintenance of Road Segments"></a>A Decision Making Framework for Recommended Maintenance of Road Segments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10085">http://arxiv.org/abs/2307.10085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyu Sun, Yan Yan</li>
<li>for: 提供更科学的道路维护决策工具和证据 для道路维护部门</li>
<li>methods: 结合多种人工智能决策技术，对历史维护数据进行全面探索，适应道路维护科学决策</li>
<li>results: 提出了一幅基于历史维护经验和技术指标的维护计划和维护部分优先级分配方案，帮助道路维护部门更科学地做出决策，考虑有限的预算和历史维护经验。<details>
<summary>Abstract</summary>
With the rapid development of global road transportation, countries worldwide have completed the construction of road networks. However, the ensuing challenge lies in the maintenance of existing roads. It is well-known that countries allocate limited budgets to road maintenance projects, and road management departments face difficulties in making scientifically informed maintenance decisions. Therefore, integrating various artificial intelligence decision-making techniques to thoroughly explore historical maintenance data and adapt them to the context of road maintenance scientific decision-making has become an urgent issue. This integration aims to provide road management departments with more scientific tools and evidence for decision-making. The framework proposed in this paper primarily addresses the following four issues: 1) predicting the pavement performance of various routes, 2) determining the prioritization of maintenance routes, 3) making maintenance decisions based on the evaluation of the effects of past maintenance, and considering comprehensive technical and management indicators, and 4) determining the prioritization of maintenance sections based on the maintenance effectiveness and recommended maintenance effectiveness. By tackling these four problems, the framework enables intelligent decision-making for the optimal maintenance plan and maintenance sections, taking into account limited funding and historical maintenance management experience.
</details>
<details>
<summary>摘要</summary>
The proposed framework primarily addresses the following four issues:1. 预测不同路线的路面性能（pavement performance）2. 确定维护路线的优先级（prioritization of maintenance routes）3. 根据过去维护的影响进行维护决策（maintenance decisions based on the evaluation of the effects of past maintenance）4. 根据维护效果和推荐维护效果来决定维护部分的优先级（prioritization of maintenance sections based on the maintenance effectiveness and recommended maintenance effectiveness）By tackling these four problems, the framework enables intelligent decision-making for the optimal maintenance plan and maintenance sections, taking into account limited funding and historical maintenance management experience.
</details></li>
</ul>
<hr>
<h2 id="Accurate-deep-learning-sub-grid-scale-models-for-large-eddy-simulations"><a href="#Accurate-deep-learning-sub-grid-scale-models-for-large-eddy-simulations" class="headerlink" title="Accurate deep learning sub-grid scale models for large eddy simulations"></a>Accurate deep learning sub-grid scale models for large eddy simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10060">http://arxiv.org/abs/2307.10060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rikhi Bose, Arunabha M. Roy</li>
<li>for: 这两个家族的粒子扩散层（SGS）湍流模型是为大湍流计算（LES）目的而开发的。</li>
<li>methods: 这两个模型使用了physics-informed Deep Learning（DL）算法，与传统的分析模型技术不同，可以生成高阶复杂非线性关系。</li>
<li>results: 两个模型在不同的滤波宽度和 Reynolds 数下预测了SGS 压力的结果，其中一个模型使用了tensor基 neural network（TBNN），具有更高的数学表达能力；另一个模型使用了简单的网络结构，具有更好的特征提取能力，但在统计性能指标上表现更好。<details>
<summary>Abstract</summary>
We present two families of sub-grid scale (SGS) turbulence models developed for large-eddy simulation (LES) purposes. Their development required the formulation of physics-informed robust and efficient Deep Learning (DL) algorithms which, unlike state-of-the-art analytical modeling techniques can produce high-order complex non-linear relations between inputs and outputs. Explicit filtering of data from direct simulations of the canonical channel flow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 provided accurate data for training and testing. The two sets of models use different network architectures. One of the architectures uses tensor basis neural networks (TBNN) and embeds the simplified analytical model form of the general effective-viscosity hypothesis, thus incorporating the Galilean, rotational and reflectional invariances. The other architecture is that of a relatively simple network, that is able to incorporate the Galilean invariance only. However, this simpler architecture has better feature extraction capacity owing to its ability to establish relations between and extract information from cross-components of the integrity basis tensors and the SGS stresses. Both sets of models are used to predict the SGS stresses for feature datasets generated with different filter widths, and at different Reynolds numbers. It is shown that due to the simpler model's better feature learning capabilities, it outperforms the invariance embedded model in statistical performance metrics. In a priori tests, both sets of models provide similar levels of dissipation and backscatter. Based on the test results, both sets of models should be usable in a posteriori actual LESs.
</details>
<details>
<summary>摘要</summary>
我们介绍了两家族的子grid尺度（SGS）随机流模型，用于大扩散 simulated（LES）的目的。它们的发展需要了物理知识具有强大和有效的深度学习（DL）算法，不同于现有的分析模型技术可以生成高阶复杂的非线性关系。我们使用了直接实验的标准频率道流场的数据进行范例训练和测试。这两个模型使用了不同的网络架构。其中一个架构使用了tensor基 neural network（TBNN），并嵌入了简化的分析模型形式的通用有效黏度假设，因此包含了加利ле安、旋转和反射对称性。另一个架构则是一个较简单的网络，它能够将加利ле安对称性独立出来。然而，这个简单的架构有更好的特征学习能力，因为它能够在横向分量和SGS压力之间建立关系和提取信息。两个模型都用于预测SGS压力的特征数据集，并在不同的滤波宽度和 Reynolds 数下进行预测。发现简单的模型在统计性表现指标上比嵌入对称性的模型更好。在先前的测试中，这两个模型都提供了相似的扩散和反射性能。基于这些测试结果，这两个模型在 posteriori 实际 LES 中都可以使用。
</details></li>
</ul>
<hr>
<h2 id="Convergence-Guarantees-for-Stochastic-Subgradient-Methods-in-Nonsmooth-Nonconvex-Optimization"><a href="#Convergence-Guarantees-for-Stochastic-Subgradient-Methods-in-Nonsmooth-Nonconvex-Optimization" class="headerlink" title="Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization"></a>Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10053">http://arxiv.org/abs/2307.10053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xnchxy/GeneralSGD">https://github.com/xnchxy/GeneralSGD</a></li>
<li>paper_authors: Nachuan Xiao, Xiaoyin Hu, Kim-Chuan Toh</li>
<li>for:  investigate the convergence properties of stochastic gradient descent (SGD) method and its variants in training neural networks with nonsmooth activation functions.</li>
<li>methods:  develop a novel framework that assigns different timescales to stepsizes for updating momentum terms and variables, and prove the global convergence of the proposed framework in both single-timescale and two-timescale cases.</li>
<li>results:  prove the convergence properties of several well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD, and clipped SGD, and demonstrate the high efficiency of these methods through preliminary numerical experiments.<details>
<summary>Abstract</summary>
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preliminary numerical experiments demonstrate the high efficiency of our analyzed SGD-type methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究权重梯度下降（SGD）方法和其变体的收敛性质，特别是在训练使用非凸活动函数建立的神经网络时。我们提出了一个新的框架，它在更新摇摆项和变量时分配不同的时间尺度。在某些轻度条件下，我们证明了我们所提议的框架的全球收敛性。我们还证明了我们的框架包括许多已知SGD类型方法，包括重力SGD、SignSGD、Lion、normalized SGD和clipped SGD。此外，当目标函数采用finite-sum形式时，我们证明了这些SGD类型方法的收敛性基于我们所提议的框架。具体来说，我们证明了这些SGD类型方法可以随机选择步长和初始点，并在某些轻度假设下找到目标函数的克拉克站点。初步的数据 экспериiments表明我们分析的SGD类型方法具有高效性。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Conversion-of-MiniZinc-Programs-to-QUBO"><a href="#Automatic-Conversion-of-MiniZinc-Programs-to-QUBO" class="headerlink" title="Automatic Conversion of MiniZinc Programs to QUBO"></a>Automatic Conversion of MiniZinc Programs to QUBO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10032">http://arxiv.org/abs/2307.10032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armin Wolf, Cristian Grozea</li>
<li>for: 提高Physical Quantum Computer（如DWave annealers）上的优化问题解决效率。</li>
<li>methods: 使用自动转换器从MiniZinc转换为QUBO，可以处理广泛的约束优化和约束满足问题，并将其转换为等效的QUBO问题。</li>
<li>results: 提高了优化问题解决效率。<details>
<summary>Abstract</summary>
Obtaining Quadratic Unconstrained Binary Optimisation models for various optimisation problems, in order to solve those on physical quantum computers (such as the the DWave annealers) is nowadays a lengthy and tedious process that requires one to remodel all problem variables as binary variables and squeeze the target function and the constraints into a single quadratic polynomial into these new variables.   We report here on the basis of our automatic converter from MiniZinc to QUBO, which is able to process a large set of constraint optimisation and constraint satisfaction problems and turn them into equivalent QUBOs, effectively optimising the whole process.
</details>
<details>
<summary>摘要</summary>
当前，获取二次不约定 binary 优化模型（QUBO）来解决各种优化问题，需要将问题变量全部转换为二进制变量，并将目标函数和约束函数压缩成单个二次多项式中。我们在这篇报告中介绍了我们的自动转换器，可以将 MiniZinc 问题转换为 QUBO，并且可以处理大量的约束优化和约束满足问题。这有效地优化了整个过程。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-on-Fertility-Proposals-Using-Multi-Grained-Topic-Analysis-Methods"><a href="#An-Empirical-Study-on-Fertility-Proposals-Using-Multi-Grained-Topic-Analysis-Methods" class="headerlink" title="An Empirical Study on Fertility Proposals Using Multi-Grained Topic Analysis Methods"></a>An Empirical Study on Fertility Proposals Using Multi-Grained Topic Analysis Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10025">http://arxiv.org/abs/2307.10025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulin Zhou</li>
<li>for: 研究中国社会对政策的公众意见</li>
<li>methods:  employing co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments</li>
<li>results: 发现提议“废除婚姻限制出生登记”的讨论涉及个人、社会和国家三维度，并详细分析到社会问题、个人行为、社会伦理和法律等方面，人们的情感倾向于负面的主题。基于这些结论，提出了八项建议作为政策决策参考和研究公众意见政治问题的参考方法。<details>
<summary>Abstract</summary>
Fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 "two sessions" proposal "suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and "unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of "removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behaviour, social ethics and law, and national policy, with people's sentiment inclined to be negative in most of the topics. Based on this, eight proposals were made to provide a reference for governmental decision making and to form a reference method for researching public opinion on political issues.
</details>
<details>
<summary>摘要</summary>
fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 "two sessions" proposal "suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and "unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of "removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behavior, social ethics and law, and national policy, with people's sentiment inclined to be negative in most of the topics. Based on this, eight proposals were made to provide a reference for governmental decision making and to form a reference method for researching public opinion on political issues.Here is the text with the names of the months and the years in Simplified Chinese:fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 "two sessions" proposal "suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and "unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of "removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behavior, social ethics and law, and national policy, with people's sentiment inclined to be negative in most of the topics. Based on this, eight proposals were made to provide a reference for governmental decision making and to form a reference method for researching public opinion on political issues.
</details></li>
</ul>
<hr>
<h2 id="RoboCIn-Small-Size-League-Extended-Team-Description-Paper-for-RoboCup-2023"><a href="#RoboCIn-Small-Size-League-Extended-Team-Description-Paper-for-RoboCup-2023" class="headerlink" title="RobôCIn Small Size League Extended Team Description Paper for RoboCup 2023"></a>RobôCIn Small Size League Extended Team Description Paper for RoboCup 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10018">http://arxiv.org/abs/2307.10018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aline Lima de Oliveira, Cauê Addae da Silva Gomes, Cecília Virginia Santos da Silva, Charles Matheus de Sousa Alves, Danilo Andrade Martins de Souza, Driele Pires Ferreira Araújo Xavier, Edgleyson Pereira da Silva, Felipe Bezerra Martins, Lucas Henrique Cavalcanti Santos, Lucas Dias Maciel, Matheus Paixão Gumercindo dos Santos, Matheus Lafayette Vasconcelos, Matheus Vinícius Teotonio do Nascimento Andrade, João Guilherme Oliveira Carvalho de Melo, João Pedro Souza Pereira de Moura, José Ronald da Silva, José Victor Silva Cruz, Pedro Henrique Santana de Morais, Pedro Paulo Salman de Oliveira, Riei Joaquim Matos Rodrigues, Roberto Costa Fernandes, Ryan Vinicius Santos Morais, Tamara Mayara Ramos Teobaldo, Washington Igor dos Santos Silva, Edna Natividade Silva Barros</li>
<li>for: 本研究目的是在2023年的RoboCup Small Size League（SSL）组别B联赛中卫冕冠军。</li>
<li>methods: 本研究使用了Unification架构，并进行了软件和人工智能 refactoring。此外，我们也详细说明了将机械部件集成到机械系统中的过程。</li>
<li>results: 本研究已经发表了两篇相关SSL的学术研究论文，并在25届RoboCup国际symposium和19届IEEE拉丁美洲机器人学会议（LARS 2022）上发表。我们还在持续将过去的代码库转换到Unification架构。<details>
<summary>Abstract</summary>
Rob\^oCIn has participated in RoboCup Small Size League since 2019, won its first world title in 2022 (Division B), and is currently a three-times Latin-American champion. This paper presents our improvements to defend the Small Size League (SSL) division B title in RoboCup 2023 in Bordeaux, France. This paper aims to share some of the academic research that our team developed over the past year. Our team has successfully published 2 articles related to SSL at two high-impact conferences: the 25th RoboCup International Symposium and the 19th IEEE Latin American Robotics Symposium (LARS 2022). Over the last year, we have been continuously migrating from our past codebase to Unification. We will describe the new architecture implemented and some points of software and AI refactoring. In addition, we discuss the process of integrating machined components into the mechanical system, our development for participating in the vision blackout challenge last year and what we are preparing for this year.
</details>
<details>
<summary>摘要</summary>
罗博琪（Rob\^oCIn）自2019年起参加小型足球联赛（Small Size League，SSL），赢得了2022年世界冠军（分区B），并现为拉丁美洲三届冠军。本文描述我们在2023年法国博览会（RoboCup 2023）中防守小型足球联赛（SSL）分区B冠军的改进。本文目的是分享过去一年我们团队进行的一些学术研究。我们成功发表了两篇与SSL相关的文章，在两个高影响因子会议上进行了发表：25届RoboCup国际学术会议和19届IEEE拉丁美洲机器人学术会议（LARS 2022）。过去一年，我们不断迁移自己的代码基础，迁移到统一平台。我们将描述新的架构实现和一些软件和人工智能重构。此外，我们还讨论了在机械系统中 integrate 机器组件的过程，我们在过去一年参加视网膜挑战并准备今年参加。
</details></li>
</ul>
<hr>
<h2 id="6G-Network-Business-Support-System"><a href="#6G-Network-Business-Support-System" class="headerlink" title="6G Network Business Support System"></a>6G Network Business Support System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10004">http://arxiv.org/abs/2307.10004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye Ouyang, Yaqin Zhang, Peng Wang, Yunxin Liu, Wen Qiao, Jun Zhu, Yang Liu, Feng Zhang, Shuling Wang, Xidong Wang<br>for:* 6G BSS systems will support the efficient connection of intelligent agents and lead the digital, intelligent, and green transformation of the economy and society.methods:* The paper introduces the overall vision, potential key technologies, and functional architecture of 6G BSS systems.results:* The paper presents an evolutionary roadmap and technological prospects for the BSS systems from 5G to 6G.<details>
<summary>Abstract</summary>
6G is the next-generation intelligent and integrated digital information infrastructure, characterized by ubiquitous interconnection, native intelligence, multi-dimensional perception, global coverage, green and low-carbon, native network security, etc. 6G will realize the transition from serving people and people-things communication to supporting the efficient connection of intelligent agents, and comprehensively leading the digital, intelligent and green transformation of the economy and the society. As the core support system for mobile communication network, 6 6G BSS need to integrate with new business models brought about by the development of the next-generation Internet and IT, upgrade from "network-centric" to "business and service centric" and "customer-centric". 6G OSS and BSS systems need to strengthen their integration to improve the operational efficiency and benefits of customers by connecting the digital intelligence support capabilities on both sides of supply and demand. This paper provides a detailed introduction to the overall vision, potential key technologies, and functional architecture of 6G BSS systems. It also presents an evolutionary roadmap and technological prospects for the BSS systems from 5G to 6G.
</details>
<details>
<summary>摘要</summary>
6G是下一代智能 интегрирован的数字信息基础设施，具有 ubique 连接、内置智能、多维感知、全球覆盖、绿色低碳、内网安全等特点。6G将实现从人工智能和物联网的服务转移到支持高效的智能代理连接，全面领导数字、智能和绿色经济社会的转型。作为移动通信网络核心支持系统，6G BSS需要与新一代互联网和IT的商业化模式相结合，升级从“网络中心”到“业务和服务中心”和“客户中心”。6G OSS和BSS系统需要加强对两侧供应需求的连接，以提高客户运营效率和利益。本文对6G BSS系统的总视图、潜在关键技术和功能架构进行详细介绍，还提供了5G到6G BSS系统的进化路线图和技术前景。
</details></li>
</ul>
<hr>
<h2 id="TbExplain-A-Text-based-Explanation-Method-for-Scene-Classification-Models-with-the-Statistical-Prediction-Correction"><a href="#TbExplain-A-Text-based-Explanation-Method-for-Scene-Classification-Models-with-the-Statistical-Prediction-Correction" class="headerlink" title="TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction"></a>TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10003">http://arxiv.org/abs/2307.10003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Aminimehr, Pouya Khani, Amirali Molaei, Amirmohammad Kazemeini, Erik Cambria</li>
<li>for: 提高黑盒机器学习模型的解释性，即Explainable Artificial Intelligence (XAI)。</li>
<li>methods: 使用XAI技术和预训练对象检测器，提供图文 объяснения场景分类模型的下发。另外，提出一种纠正预测和文本 объяснение基于输入图像中对象的统计的新方法。</li>
<li>results: 对Scene Classification模型进行质量和量化测试，发现TbExplain可以提高分类精度，并且对于初始预测不可靠时，文本 объяснение具有可靠性。<details>
<summary>Abstract</summary>
The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually explain them based on the statistics of objects in the input image when the initial prediction is unreliable. To assess the trustworthiness and validity of the text-based explanations, we conducted a qualitative experiment, and the findings indicated that these explanations are sufficiently reliable. Furthermore, our quantitative and qualitative experiments on TbExplain with scene classification datasets reveal an improvement in classification accuracy over ResNet variants.
</details>
<details>
<summary>摘要</summary>
黑盒式人工智能（XAI）领域的目标是提高黑盒机器学习模型的解释性。建立基于输入特征的重要性值的热力映射是广泛使用的方法来解释这些模型在预测中所使用的下面运算。然而，热力映射并不完美，例如非专家用户可能无法全面理解热力映射的逻辑（在预测中相关的像素被不同的强度或颜色表示）。此外，输入图像中对模型预测的物件和区域也不常完全区分。在这篇论文中，我们提出了一个名为TbExplain的框架，使用XAI技术和预训物件探测器来提供Scene Classification模型的文本解释。此外，TbExplain还包括一个新的方法来修正预测和文本解释基于输入图像中物件的统计时，当初始预测不可靠时。为评估文本解释的可信度和有效性，我们进行了一个质量性实验，发现这些解释具有足够的可靠性。此外，我们在TbExplainScene Classification dataset上进行了量值和质量性实验，发现TbExplain可以与ResNetVariants相比提高分类精度。
</details></li>
</ul>
<hr>
<h2 id="Our-Model-Achieves-Excellent-Performance-on-MovieLens-What-Does-it-Mean"><a href="#Our-Model-Achieves-Excellent-Performance-on-MovieLens-What-Does-it-Mean" class="headerlink" title="Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?"></a>Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09985">http://arxiv.org/abs/2307.09985</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FrankYuchen/MovieLensRec">https://github.com/FrankYuchen/MovieLensRec</a></li>
<li>paper_authors: Yu-chen Fan, Yitong Ji, Jie Zhang, Aixin Sun</li>
<li>for: 这个研究用于分析MovieLens数据集，并解释这个数据集如何影响推荐算法的评估。</li>
<li>methods: 这个研究使用了细化分析方法，揭示了用户交互的各个阶段之间的差异，以及用户交互如何受到内置推荐算法的影响。</li>
<li>results: 研究发现，用户交互的不同阶段会对用户的首选项产生影响，并且随着用户的交互增加，推荐算法的学习效果会逐渐下降。此外，改变用户交互的顺序会使Sequential algoritms更难 capture用户交互进程。<details>
<summary>Abstract</summary>
A typical benchmark dataset for recommender system (RecSys) evaluation consists of user-item interactions generated on a platform within a time period. The interaction generation mechanism partially explains why a user interacts with (e.g.,like, purchase, rate) an item, and the context of when a particular interaction happened. In this study, we conduct a meticulous analysis on the MovieLens dataset and explain the potential impact on using the dataset for evaluating recommendation algorithms. We make a few main findings from our analysis. First, there are significant differences in user interactions at the different stages when a user interacts with the MovieLens platform. The early interactions largely define the user portrait which affect the subsequent interactions. Second, user interactions are highly affected by the candidate movies that are recommended by the platform's internal recommendation algorithm(s). Removal of interactions that happen nearer to the last few interactions of a user leads to increasing difficulty in learning user preference, thus deteriorating recommendation accuracy. Third, changing the order of user interactions makes it more difficult for sequential algorithms to capture the progressive interaction process. Based on these findings, we further discuss the discrepancy between the interaction generation mechanism that is employed by the MovieLens system and that of typical real world recommendation scenarios. In summary, models that achieve excellent recommendation accuracy on the MovieLens dataset may not demonstrate superior performance in practice for at least two kinds of differences: (i) the differences in the contexts of user-item interaction generation, and (ii) the differences in user knowledge about the item collections.
</details>
<details>
<summary>摘要</summary>
一个典型的RecSys评估数据集包含用户和项目之间的交互，在一个平台上的一段时间内生成。交互生成机制部分解释了用户为何与项目（例如，喜欢、购买、评分）交互。在本研究中，我们进行了仔细的分析MovieLens数据集，并解释了使用该数据集评估推荐算法的可能的影响。我们得出了一些主要发现：1. 用户在不同阶段与MovieLens平台交互时，交互差异显著。早期交互主要定义用户肖像，影响后续交互。2. 用户交互受到MovieLens平台内部推荐算法提供的候选电影的影响，移除用户最后几次交互的交互会降低用户喜好学习的难度，导致推荐精度下降。3. 改变用户交互的顺序使得sequential算法更难捕捉用户交互过程的进步。根据这些发现，我们进一步讨论了MovieLens系统employs的交互生成机制与真实世界推荐场景中的交互生成机制之间的差异。总结来说，在MovieLens数据集上达到杰出推荐精度的模型可能不会在实际场景中表现出色，因为有两种类型的差异：1. 用户和项目之间交互的上下文差异。2. 用户对项目集的了解程度的差异。
</details></li>
</ul>
<hr>
<h2 id="XSkill-Cross-Embodiment-Skill-Discovery"><a href="#XSkill-Cross-Embodiment-Skill-Discovery" class="headerlink" title="XSkill: Cross Embodiment Skill Discovery"></a>XSkill: Cross Embodiment Skill Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09955">http://arxiv.org/abs/2307.09955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, Shuran Song</li>
<li>for: 本研究旨在将人工智能学习扩展到机器人控制领域，利用人类示范影片作为机器人学习的数据源。</li>
<li>methods: 本研究使用了一个名为XSkill的循环传染学习框架，通过将人类和机器人的摄像头动作影片组合成一个跨实体表示（skill prototype），并将这个表示转换到机器人动作中使用条件散布政策。</li>
<li>results: 实验结果显示XSkill可以将学习自人类示范影片中的技能转换到机器人动作中，并且可以根据人类提供的问题影片来组合学习的技能以完成未见过的任务。<details>
<summary>Abstract</summary>
Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performance of XSkill is best understood from the anonymous website: https://xskillcorl.github.io.
</details>
<details>
<summary>摘要</summary>
人类示例视频是机器人学习中广泛可用的数据源，同时也是一种直观的用户界面，用于表达所愿的行为。然而，直接从不结构化的人类视频中提取可重用的机器人操作技巧是困难的，这是因为人类和机器人之间存在大的实体差异和未观察到的行为参数。为 bridging这个实体差距，本文提出了XSkill，一种仿冒学习框架，它可以：1）从无标签的人类和机器人操作视频中找到跨实体表示called skill prototypes，2）使用条件扩散策略将技能表示转移到机器人动作，并最后3）使用人类提示视频来组合学习的技能完成未看到的任务。我们在模拟环境和实际环境中进行了实验，结果表明XSkill可以快速地传递和组合学习的技能，从而实现更一般和可扩展的仿冒学习框架。XSkill的性能可以通过无名website：https://xskillcorl.github.io来了解。
</details></li>
</ul>
<hr>
<h2 id="U-CE-Uncertainty-aware-Cross-Entropy-for-Semantic-Segmentation"><a href="#U-CE-Uncertainty-aware-Cross-Entropy-for-Semantic-Segmentation" class="headerlink" title="U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation"></a>U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09947">http://arxiv.org/abs/2307.09947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Landgraf, Markus Hillemann, Kira Wursthorn, Markus Ulrich</li>
<li>for: 提高自适应驾驶等安全关键应用中模型的可靠性和可信度。</li>
<li>methods: 基于 pixel-wise 权重的 uncertainty-aware Cross-Entropy 损失函数 (U-CE)，将predictive uncertainty纳入训练过程中。</li>
<li>results: 在Cityscapes和ACDC datasets上，使用ResNet-18和ResNet-101两种常见背景网络架构，U-CE训练模型可以不仅提高分割性能，还提供了有意义的uncertainty值。<details>
<summary>Abstract</summary>
Deep neural networks have shown exceptional performance in various tasks, but their lack of robustness, reliability, and tendency to be overconfident pose challenges for their deployment in safety-critical applications like autonomous driving. In this regard, quantifying the uncertainty inherent to a model's prediction is a promising endeavour to address these shortcomings. In this work, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) that incorporates dynamic predictive uncertainties into the training process by pixel-wise weighting of the well-known cross-entropy loss (CE). Through extensive experimentation, we demonstrate the superiority of U-CE over regular CE training on two benchmark datasets, Cityscapes and ACDC, using two common backbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to train models that not only improve their segmentation performance but also provide meaningful uncertainties after training. Consequently, we contribute to the development of more robust and reliable segmentation models, ultimately advancing the state-of-the-art in safety-critical applications and beyond.
</details>
<details>
<summary>摘要</summary>
Through extensive experimentation, we demonstrate the superiority of U-CE over regular CE training on two benchmark datasets, Cityscapes and ACDC, using two common backbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to train models that not only improve their segmentation performance but also provide meaningful uncertainties after training. This contribution advances the development of more robust and reliable segmentation models, ultimately benefiting safety-critical applications and beyond.
</details></li>
</ul>
<hr>
<h2 id="TREEMENT-Interpretable-Patient-Trial-Matching-via-Personalized-Dynamic-Tree-Based-Memory-Network"><a href="#TREEMENT-Interpretable-Patient-Trial-Matching-via-Personalized-Dynamic-Tree-Based-Memory-Network" class="headerlink" title="TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network"></a>TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09942">http://arxiv.org/abs/2307.09942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Theodorou, Cao Xiao, Jimeng Sun</li>
<li>for: 这个研究旨在提高药物开发中的临床试验进程，使用机器学习模型快速匹配病人临床试验，并提供更好的解释性结果以便采纳。</li>
<li>methods: 这个研究使用了一种名为TREEMENT的个性化动态树状内存网络模型，利用预设的临床生物学 Ontology 扩展个体化病人表示，然后使用注意力对搜寻查询学习自适应搜寻以提高匹配性和解释性。</li>
<li>results: 比较TREEMENT模型和现有模型，TREEMENT模型在实际数据上表现较好，降低了7%的条件水平匹配错误率，并在临床试验水平上表现出色。此外，TREEMENT模型也提供了好的解释性，让模型结果更易采纳。<details>
<summary>Abstract</summary>
Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment. In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHR) data and eligibility criteria of clinical trials. However, they either depend on trial-specific expert rules that cannot expand to other trials or perform matching at a very general level with a black-box model where the lack of interpretability makes the model results difficult to be adopted.   To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network model named TREEMENT. It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to offer a granular level of alignment for improved performance and interpretability. We evaluated TREEMENT against existing models on real-world datasets and demonstrated that TREEMENT outperforms the best baseline by 7% in terms of error reduction in criteria-level matching and achieves state-of-the-art results in its trial-level matching ability. Furthermore, we also show TREEMENT can offer good interpretability to make the model results easier for adoption.
</details>
<details>
<summary>摘要</summary>
临床试验是药品开发的关键，但它们经常受到昂贵和不效的病人招募的困扰。在最近的几年中，机器学习模型被提议用于加速病人招募，通过自动将病人与临床试验相匹配，基于 longitudinal 病人电子医疗记录 (EHR) 数据和临床试验的参与条件。然而，它们可能会依赖于具体的试验规则，无法扩展到其他试验，或者使用黑obox模型，导致模型结果难以采用。为了提供准确和可解释的病人试验匹配，我们引入了个性化动态树型记忆网络模型 named TREEMENT。它利用层次的临床 Ontology 扩展个性化病人表示，然后使用注意力寻找查询学习从参与条件嵌入中提取的精细水平匹配，以提高性能和可解释性。我们对实际数据进行了评估，并证明 TREEMENT 在参与条件匹配和试验匹配能力方面都达到了领先的水平，与最佳基eline 比较，TREEMENT 可以提高error reduction 7%。此外，我们还示出 TREEMENT 可以提供好的可解释性，使得模型结果更易于采用。
</details></li>
</ul>
<hr>
<h2 id="Spuriosity-Didn’t-Kill-the-Classifier-Using-Invariant-Predictions-to-Harness-Spurious-Features"><a href="#Spuriosity-Didn’t-Kill-the-Classifier-Using-Invariant-Predictions-to-Harness-Spurious-Features" class="headerlink" title="Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features"></a>Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09933">http://arxiv.org/abs/2307.09933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cian Eastwood, Shashank Singh, Andrei Liviu Nicolicioiu, Marin Vlastelica, Julius von Kügelgen, Bernhard Schölkopf</li>
<li>for: 避免基于数据集外的失败，现代工作尝试提取具有稳定或不变的关系性的特征，抛弃不稳定的特征，以避免其与标签之间的关系变化。</li>
<li>methods: 我们的主要贡献是展示如何在测试Domain中没有标签的情况下使用不稳定特征。我们提出了稳定特征增强（SFB）算法，它可以：（i）学习一个分离稳定和条件无关不稳定特征的预测器；和（ii）使用稳定特征预测来适应不稳定特征预测。</li>
<li>results: 我们 teorically prove That SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.<details>
<summary>Abstract</summary>
To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.
</details>
<details>
<summary>摘要</summary>
recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.Here's the translation in Traditional Chinese:近期的研究尝试提取具有稳定或跨Domain对于标签的相似关系的特征，将"伪�component"或不稳定的特征排除出来。然而，不稳定的特征经常带有标签的补充信息，可以帮助提高效能。我们的主要贡献是表明可以在测试Domain中使用不稳定的特征， без标签。具体来说，我们证明可以使用稳定特征 Pseudo-labels 提供足够的指导，以便在测试Domain中使用不稳定特征。基于这个理论性的见解，我们提出了稳定特征提升（SFB）算法，包括： (i) 学习分类器，以分类稳定和条件独立的不稳定特征; (ii) 使用稳定特征预测，以适应不稳定特征的预测。理论上，我们证明SFB可以学习无需测试Domain标签的 asymptotically-optimal 预测器。实际上，我们透过实验表明SFB在真实和 sintetic 数据上具有优秀的效能。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Non-Regular-Extensions-of-Propositional-Dynamic-Logic-with-Description-Logics-Features"><a href="#Exploring-Non-Regular-Extensions-of-Propositional-Dynamic-Logic-with-Description-Logics-Features" class="headerlink" title="Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features"></a>Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09913">http://arxiv.org/abs/2307.09913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bartosz Bednarczyk</li>
<li>for:  investigate the impact of non-regular path expressions on the decidability of satisfiability checking and querying in description logics extending ALC.</li>
<li>methods:  employing regular and visibly-pushdown languages, and using undecidability results to show the loss of decidability.</li>
<li>results:  established undecidability of the concept satisfiability problem for ALCvpl extended with nominals, and undecidability of query entailment for queries involving non-regular atoms.<details>
<summary>Abstract</summary>
We investigate the impact of non-regular path expressions on the decidability of satisfiability checking and querying in description logics extending ALC. Our primary objects of interest are ALCreg and ALCvpl, the extensions of with path expressions employing, respectively, regular and visibly-pushdown languages. The first one, ALCreg, is a notational variant of the well-known Propositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was introduced and investigated by Loding and Serre in 2007. The logic ALCvpl generalises many known decidable non-regular extensions of ALCreg.   We provide a series of undecidability results. First, we show that decidability of the concept satisfiability problem for ALCvpl is lost upon adding the seemingly innocent Self operator. Second, we establish undecidability for the concept satisfiability problem for ALCvpl extended with nominals. Interestingly, our undecidability proof relies only on one single non-regular (visibly-pushdown) language, namely on r#s# := { r^n s^n | n in N } for fixed role names r and s. Finally, in contrast to the classical database setting, we establish undecidability of query entailment for queries involving non-regular atoms from r#s#, already in the case of ALC-TBoxes.
</details>
<details>
<summary>摘要</summary>
我们研究非正规路径表达对描述逻辑中的可 decidability和查询问题的影响。我们的主要研究对象是 ALCreg 和 ALCvpl，它们分别使用正规语言和可见推下语言来表达非正规路径。ALCreg 是 Fischer 和 Ladner 提出的一种知名的推理逻辑，而 ALCvpl 是 Loding 和 Serre 在 2007 年提出的一种扩展。我们提供了一系列的不可 decidability 结果。首先，我们显示了 ALCvpl 中的概念可行性问题是在添加 Self 运算符后失去可 decidability。其次，我们证明了 ALCvpl 中的概念可行性问题是在添加nominals后不可 decidability。值得注意的是，我们的不可 decidability 证明只需一个非正规（可见推下）语言，即 r#s# := { r^n s^n | n in N } for fixed role names r 和 s。最后，我们证明了在 classical 数据库设定下，对于含有非正规atom的 r#s# 查询，就算是 ALC-TBoxes 中，也是不可 decidability。
</details></li>
</ul>
<hr>
<h2 id="Chit-Chat-or-Deep-Talk-Prompt-Engineering-for-Process-Mining"><a href="#Chit-Chat-or-Deep-Talk-Prompt-Engineering-for-Process-Mining" class="headerlink" title="Chit-Chat or Deep Talk: Prompt Engineering for Process Mining"></a>Chit-Chat or Deep Talk: Prompt Engineering for Process Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09909">http://arxiv.org/abs/2307.09909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Urszula Jessen, Michal Sroka, Dirk Fahland</li>
<li>for: 这个研究探讨了使用大语言模型（LLM）来增强对话代理人在过程挖掘中的应用，以解决其内置的复杂性和多样化技能要求。</li>
<li>methods: 我们提出了一种创新的方法，利用先前的自然语言处理（NLP）研究，以解决现有解决方案中的多个问题。</li>
<li>results: 我们的框架可以提高对话代理人的性能和可访问性，如公共问题和数据集上的实验所示。我们的研究为未来 LLM 在过程挖掘中的角色做出了贡献，并提出了改进 LLM 记忆、实时用户测试和多样数据集的建议。<details>
<summary>Abstract</summary>
This research investigates the application of Large Language Models (LLMs) to augment conversational agents in process mining, aiming to tackle its inherent complexity and diverse skill requirements. While LLM advancements present novel opportunities for conversational process mining, generating efficient outputs is still a hurdle. We propose an innovative approach that amend many issues in existing solutions, informed by prior research on Natural Language Processing (NLP) for conversational agents. Leveraging LLMs, our framework improves both accessibility and agent performance, as demonstrated by experiments on public question and data sets. Our research sets the stage for future explorations into LLMs' role in process mining and concludes with propositions for enhancing LLM memory, implementing real-time user testing, and examining diverse data sets.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这个研究 investigate Large Language Models (LLMs) 在进程挖掘中增强对话代理人，以解决该领域的内在复杂性和多样化技能要求。虽然 LLM 的进步提供了对话进程挖掘中新的机遇，但生成高效输出仍然是一个障碍。我们提议一种创新的方法，利用 LLMs 解决现有解决方案中的许多问题，基于对话代理人的先前研究。我们的框架可以提高对话代理人的可访问性和性能，如实验结果所示。我们的研究为未来对 LLMs 在进程挖掘中的角色进行未来的探索提供了基础，并结束于对 LLM 的内存增强、实时用户测试和多样数据集的探索。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Identity-Representation-Conditioned-Memory-Compensation-Network-for-Talking-Head-video-Generation"><a href="#Implicit-Identity-Representation-Conditioned-Memory-Compensation-Network-for-Talking-Head-video-Generation" class="headerlink" title="Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation"></a>Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09906">http://arxiv.org/abs/2307.09906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harlanhong/iccv2023-mcnet">https://github.com/harlanhong/iccv2023-mcnet</a></li>
<li>paper_authors: Fa-Ting Hong, Dan Xu</li>
<li>for: 高品质的人脸动画生成（talking head generation），使用动态资讯从目标驱动影片中获取，并维持人脸的身份特征。</li>
<li>methods: 提出了一个全球人脸表现空间学习（global facial representation space），并设计了一个新的条件参数记忆补偿网络（MCNet），以高精度的人脸生成。</li>
<li>results: 实验结果显示MCNet可以学习代表性和补偿性的人脸记忆，并在VoxCeleb1和CelebV数据集上明显超越前一代的人脸生成方法。<details>
<summary>Abstract</summary>
Talking head video generation aims to animate a human face in a still image with dynamic poses and expressions using motion information derived from a target-driving video, while maintaining the person's identity in the source image. However, dramatic and complex motions in the driving video cause ambiguous generation, because the still source image cannot provide sufficient appearance information for occluded regions or delicate expression variations, which produces severe artifacts and significantly degrades the generation quality. To tackle this problem, we propose to learn a global facial representation space, and design a novel implicit identity representation conditioned memory compensation network, coined as MCNet, for high-fidelity talking head generation.~Specifically, we devise a network module to learn a unified spatial facial meta-memory bank from all training samples, which can provide rich facial structure and appearance priors to compensate warped source facial features for the generation. Furthermore, we propose an effective query mechanism based on implicit identity representations learned from the discrete keypoints of the source image. It can greatly facilitate the retrieval of more correlated information from the memory bank for the compensation. Extensive experiments demonstrate that MCNet can learn representative and complementary facial memory, and can clearly outperform previous state-of-the-art talking head generation methods on VoxCeleb1 and CelebV datasets. Please check our \href{https://github.com/harlanhong/ICCV2023-MCNET}{Project}.
</details>
<details>
<summary>摘要</summary>
幕前人物视频生成技术目的是使用动态姿势和表情信息来动画一张静止图像中的人脸，保持人脸的身份信息，但是由于驱动视频中的剧烈和复杂动作，可能会导致生成杂乱和严重损害质量。为解决这个问题，我们提议学习全面的人脸表示空间，并设计了一种基于人脸记忆强化网络（MCNet）的高精度人脸生成方法。特别是，我们设计了一种网络模块来学习所有训练样本中的统一空间人脸媒体记忆银行，这可以提供丰富的人脸结构和外观偏好，以补偿驱动视频中的扭曲源图像特征。此外，我们提出了一种基于人脸记忆中学习的隐式身份表示查询机制，可以大大优化寻找更相关信息的检索。我们的实验表明，MCNet可以学习代表性和补偿性的人脸记忆，并在VoxCeleb1和CelebV数据集上明显超越前一代人脸生成方法。请查看我们的\href{https://github.com/harlanhong/ICCV2023-MCNET}{项目}.
</details></li>
</ul>
<hr>
<h2 id="PyTAG-Challenges-and-Opportunities-for-Reinforcement-Learning-in-Tabletop-Games"><a href="#PyTAG-Challenges-and-Opportunities-for-Reinforcement-Learning-in-Tabletop-Games" class="headerlink" title="PyTAG: Challenges and Opportunities for Reinforcement Learning in Tabletop Games"></a>PyTAG: Challenges and Opportunities for Reinforcement Learning in Tabletop Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09905">http://arxiv.org/abs/2307.09905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/martinballa/pytag">https://github.com/martinballa/pytag</a></li>
<li>paper_authors: Martin Balla, George E. M. Long, Dominik Jeurissen, James Goodman, Raluca D. Gaina, Diego Perez-Liebana</li>
<li>for: 这 paper 是为了探索使用 Reinforcement Learning (RL) 在现代桌游戏中进行研究的。</li>
<li>methods: 这 paper 使用了 Python API 和 Tabletop Games 框架 (TAG)，并提出了基eline 的 Proximal Policy Optimization 算法在一 subset of games 上进行训练。</li>
<li>results: 这 paper 获得了一些基eline 的结果，并讨论了现代桌游戏在RL研究中的一些特殊挑战。<details>
<summary>Abstract</summary>
In recent years, Game AI research has made important breakthroughs using Reinforcement Learning (RL). Despite this, RL for modern tabletop games has gained little to no attention, even when they offer a range of unique challenges compared to video games. To bridge this gap, we introduce PyTAG, a Python API for interacting with the Tabletop Games framework (TAG). TAG contains a growing set of more than 20 modern tabletop games, with a common API for AI agents. We present techniques for training RL agents in these games and introduce baseline results after training Proximal Policy Optimisation algorithms on a subset of games. Finally, we discuss the unique challenges complex modern tabletop games provide, now open to RL research through PyTAG.
</details>
<details>
<summary>摘要</summary>
Recently, 游戏人工智能研究（Reinforcement Learning，RL）得到了重要的突破。然而，RL在现代桌面游戏方面却受到了 peu 的关注，即使这些游戏具有许多独特的挑战，比如桌面游戏。为了 bridging 这个差距，我们介绍 PyTAG，一个 Python API for interacting with the Tabletop Games framework（TAG）。TAG 包含了20多种现代桌面游戏，具有共同的 API for AI agents。我们介绍了在这些游戏中训练 RL 代理的技巧，并在一部分游戏上提出了基线结果。最后，我们讨论了现代桌面游戏对 RL 研究提供的特殊挑战，现在通过 PyTAG 开放给 RL 研究。
</details></li>
</ul>
<hr>
<h2 id="An-analysis-on-the-effects-of-speaker-embedding-choice-in-non-auto-regressive-TTS"><a href="#An-analysis-on-the-effects-of-speaker-embedding-choice-in-non-auto-regressive-TTS" class="headerlink" title="An analysis on the effects of speaker embedding choice in non auto-regressive TTS"></a>An analysis on the effects of speaker embedding choice in non auto-regressive TTS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09898">http://arxiv.org/abs/2307.09898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adriana Stan, Johannah O’Mahony</li>
<li>for: 本研究旨在探讨非 autoregressive 多 speaker speech synthesis 架构如何利用不同 speaker embedding 集中的信息。</li>
<li>methods: 本研究使用 jointly 学习表示和使用预训练模型初始化的方法，以确定是否存在质量改进。</li>
<li>results: 研究发现，不 matter 使用哪种 embedding 集和学习策略，网络都可以处理不同 speaker 标识符 equally well，并且 speech output 质量的变化幅度很小。此外，研究还发现，在标准的训练过程中，speaker leakage 会在核心 speech abstraction 中出现。<details>
<summary>Abstract</summary>
In this paper we introduce a first attempt on understanding how a non-autoregressive factorised multi-speaker speech synthesis architecture exploits the information present in different speaker embedding sets. We analyse if jointly learning the representations, and initialising them from pretrained models determine any quality improvements for target speaker identities. In a separate analysis, we investigate how the different sets of embeddings impact the network's core speech abstraction (i.e. zero conditioned) in terms of speaker identity and representation learning. We show that, regardless of the used set of embeddings and learning strategy, the network can handle various speaker identities equally well, with barely noticeable variations in speech output quality, and that speaker leakage within the core structure of the synthesis system is inevitable in the standard training procedures adopted thus far.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种非autoregressive多speaker语音合成架构中 informations present in different speaker embedding sets的理解。我们分析了在jointly learning representations并使用预训练模型初始化时，对目标speaker identity的质量改进的影响。在另一个分析中，我们研究了不同 embedding sets对网络的核心speech abstraction（i.e. zero conditioned）的影响， Specifically, we investigate how the different sets of embeddings impact the network's core speech abstraction (i.e. zero conditioned) in terms of speaker identity and representation learning. We show that, regardless of the used set of embeddings and learning strategy, the network can handle various speaker identities equally well, with barely noticeable variations in speech output quality, and that speaker leakage within the core structure of the synthesis system is inevitable in the standard training procedures adopted thus far.
</details></li>
</ul>
<hr>
<h2 id="Amortised-Design-Optimization-for-Item-Response-Theory"><a href="#Amortised-Design-Optimization-for-Item-Response-Theory" class="headerlink" title="Amortised Design Optimization for Item Response Theory"></a>Amortised Design Optimization for Item Response Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09891">http://arxiv.org/abs/2307.09891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antti Keurulainen, Isak Westerlund, Oskar Keurulainen, Andrew Howes</li>
<li>for: 这篇论文主要用于教育和心理学领域中的学生能力评估和测试项目选择。</li>
<li>methods: 该论文提出了一种基于深度强化学习（DRL）的有效性投入设计（OED）方法，以减少对学生交互的计算成本，并在实时中提供学生能力评估。</li>
<li>results: 该论文通过训练DRL代理人使用合成数据，实现了对学生能力分布的优化测试项目选择，并在实际应用中提供了准确的学生能力评估。<details>
<summary>Abstract</summary>
Item Response Theory (IRT) is a well known method for assessing responses from humans in education and psychology. In education, IRT is used to infer student abilities and characteristics of test items from student responses. Interactions with students are expensive, calling for methods that efficiently gather information for inferring student abilities. Methods based on Optimal Experimental Design (OED) are computationally costly, making them inapplicable for interactive applications. In response, we propose incorporating amortised experimental design into IRT. Here, the computational cost is shifted to a precomputing phase by training a Deep Reinforcement Learning (DRL) agent with synthetic data. The agent is trained to select optimally informative test items for the distribution of students, and to conduct amortised inference conditioned on the experiment outcomes. During deployment the agent estimates parameters from data, and suggests the next test item for the student, in close to real-time, by taking into account the history of experiments and outcomes.
</details>
<details>
<summary>摘要</summary>
item Response Theory（IRT）是一种常用的方法，用于评估学生在教育和心理学中的回答。在教育领域，IRT用于从学生回答中推断学生能力和测试项Characteristics。与学生互动的成本高，需要有效地收集学生能力信息。基于优化实验设计（OED）的方法 computationally expensive，不适用于交互应用。为此，我们提议将amortized experimental design incorporated into IRT。在这种方式下，计算成本在预计算phase中卷shifted到训练一个深度学习（DRL）代理人，使其能够选择Student Distribution中最有用的测试项，并在实际应用中使用conditioned on experiment outcomes进行amortized inference。在部署时，代理人通过对数据进行参数估计，并根据学生历史和试题结果来建议下一个测试项。Note: "amortized" in the text refers to the idea of precomputing the optimal experimental design and storing it in a way that allows for efficient inference during deployment, rather than computing it on the fly.
</details></li>
</ul>
<hr>
<h2 id="A-reinforcement-learning-approach-for-VQA-validation-an-application-to-diabetic-macular-edema-grading"><a href="#A-reinforcement-learning-approach-for-VQA-validation-an-application-to-diabetic-macular-edema-grading" class="headerlink" title="A reinforcement learning approach for VQA validation: an application to diabetic macular edema grading"></a>A reinforcement learning approach for VQA validation: an application to diabetic macular edema grading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09886">http://arxiv.org/abs/2307.09886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatiana Fountoukidou, Raphael Sznitman</li>
<li>for: This paper focuses on providing a more comprehensive and appropriate validation approach for highly powerful Visual Question Answering (VQA) algorithms, specifically for diabetic macular edema (DME) grading.</li>
<li>methods: The proposed approach uses an automatic adaptive questioning method based on reinforcement learning (RL), which selects the next question to pose based on the history of previously asked questions.</li>
<li>results: The experiments show that the RL agent exhibits similar behavior to a clinician, asking questions that are relevant to key clinical concepts.<details>
<summary>Abstract</summary>
Recent advances in machine learning models have greatly increased the performance of automated methods in medical image analysis. However, the internal functioning of such models is largely hidden, which hinders their integration in clinical practice. Explainability and trust are viewed as important aspects of modern methods, for the latter's widespread use in clinical communities. As such, validation of machine learning models represents an important aspect and yet, most methods are only validated in a limited way. In this work, we focus on providing a richer and more appropriate validation approach for highly powerful Visual Question Answering (VQA) algorithms. To better understand the performance of these methods, which answer arbitrary questions related to images, this work focuses on an automatic visual Turing test (VTT). That is, we propose an automatic adaptive questioning method, that aims to expose the reasoning behavior of a VQA algorithm. Specifically, we introduce a reinforcement learning (RL) agent that observes the history of previously asked questions, and uses it to select the next question to pose. We demonstrate our approach in the context of evaluating algorithms that automatically answer questions related to diabetic macular edema (DME) grading. The experiments show that such an agent has similar behavior to a clinician, whereby asking questions that are relevant to key clinical concepts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Test-takers-have-a-say-understanding-the-implications-of-the-use-of-AI-in-language-tests"><a href="#Test-takers-have-a-say-understanding-the-implications-of-the-use-of-AI-in-language-tests" class="headerlink" title="Test-takers have a say: understanding the implications of the use of AI in language tests"></a>Test-takers have a say: understanding the implications of the use of AI in language tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09885">http://arxiv.org/abs/2307.09885</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Dawen Zhang, Thong Hoang, Shidong Pan, Yongquan Hu, Zhenchang Xing, Mark Staples, Xiwei Xu, Qinghua Lu, Aaron Quigley<br>for: 这个研究的目的是了解在语言测试中使用人工智能（AI）的影响，特别是测试者的看法和行为方式。methods: 这个研究使用了面对面和在线调查，了解测试者对AI在语言测试中的感知和行为。results: 研究发现，AI的 интеграción可能会提高测试者对测试的公正性和可用性的认知，但也可能会导致测试者对测试的可靠性和互动性的不信任。这些发现可以帮助各方在决定使用AI在语言测试中的时候做出更 Informed 的选择，以保护社会的利益和测试的integrity。<details>
<summary>Abstract</summary>
Language tests measure a person's ability to use a language in terms of listening, speaking, reading, or writing. Such tests play an integral role in academic, professional, and immigration domains, with entities such as educational institutions, professional accreditation bodies, and governments using them to assess candidate language proficiency. Recent advances in Artificial Intelligence (AI) and the discipline of Natural Language Processing have prompted language test providers to explore AI's potential applicability within language testing, leading to transformative activity patterns surrounding language instruction and learning. However, with concerns over AI's trustworthiness, it is imperative to understand the implications of integrating AI into language testing. This knowledge will enable stakeholders to make well-informed decisions, thus safeguarding community well-being and testing integrity. To understand the concerns and effects of AI usage in language tests, we conducted interviews and surveys with English test-takers. To the best of our knowledge, this is the first empirical study aimed at identifying the implications of AI adoption in language tests from a test-taker perspective. Our study reveals test-taker perceptions and behavioral patterns. Specifically, we identify that AI integration may enhance perceptions of fairness, consistency, and availability. Conversely, it might incite mistrust regarding reliability and interactivity aspects, subsequently influencing the behaviors and well-being of test-takers. These insights provide a better understanding of potential societal implications and assist stakeholders in making informed decisions concerning AI usage in language testing.
</details>
<details>
<summary>摘要</summary>
语言测试测量人类语言使用能力，包括听说、读写等方面。这些测试在学术、职业和移民领域具有重要的应用，由于语言测试提供者正在探索人工智能（AI）在语言测试中的应用前景，导致语言测试领域的变革。然而，由于AI的可靠性问题，需要深入了解AI在语言测试中的影响，以便各方能够做出了解据的决策，保护社会利益和测试的公正性。为了了解AI在语言测试中的影响，我们通过对英语测试者进行了访谈和问卷调查。根据我们所知，这是第一项针对语言测试中AI使用的实证研究，旨在了解测试者的看法和行为倾向。我们发现，AI的integrating可能会提高公正性、一致性和可用性的认知，但同时也可能会产生不信任的情感，影响测试者的行为和心理健康。这些发现可以帮助各方更好地理解AI在语言测试中的社会影响，并帮助各方做出了解据的决策。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Likelihood-Estimation-with-One-way-Flows"><a href="#Adversarial-Likelihood-Estimation-with-One-way-Flows" class="headerlink" title="Adversarial Likelihood Estimation with One-way Flows"></a>Adversarial Likelihood Estimation with One-way Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09882">http://arxiv.org/abs/2307.09882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Ben-Dov, Pravir Singh Gupta, Victoria Abrevaya, Michael J. Black, Partha Ghosh</li>
<li>for: 本研究旨在提高Generative Adversarial Networks（GANs）的质量和可靠性，并提供一种不受偏见的估计方法。</li>
<li>methods: 本研究使用了importance sampling和energy-based Setting，并开发了一种新的一次流网络（one-way flow network）来计算生成器密度。</li>
<li>results: 本研究的实验结果表明，使用这种新的方法可以更快速地收敛，生成质量与传统GANs相似，避免过拟合常用数据集，并生成了平滑的低维 latent representation。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require to have a tractable inverse function. Our experimental results show that we converge faster, produce comparable sample quality to GANs with similar architecture, successfully avoid over-fitting to commonly used datasets and produce smooth low-dimensional latent representations of the training data.
</details>
<details>
<summary>摘要</summary>
генеративные adversarial networks (GANs) 可以生成高质量样本，但不提供样本附近的概率密度估计。然而，有人注意到，在能量基本设置下，最大化征韵 log-likelihood 可以导致对抗性框架，其中分类器提供无正则化概率（常称为能量）。我们进一步发展这个视角，吸收重要抽象，并表明了以下两点：1.  Wasserstein GAN 实现了偏置估计 partition function，我们提议使用不偏置的估计器；2. 在优化可能性时，必须最大化生成器 entropy，这被假设为提供更好的模式覆盖率。与前一些工作不同，我们直接计算生成器样本的概率密度。这是计算分配函数的不可或缺的关键因素。我们的实验结果表明，我们的方法可以更快 converges，生成与 GANs 类似的样本质量，避免过拟合常用的数据集，并生成缓慢低维 latent 表示。
</details></li>
</ul>
<hr>
<h2 id="Amortised-Experimental-Design-and-Parameter-Estimation-for-User-Models-of-Pointing"><a href="#Amortised-Experimental-Design-and-Parameter-Estimation-for-User-Models-of-Pointing" class="headerlink" title="Amortised Experimental Design and Parameter Estimation for User Models of Pointing"></a>Amortised Experimental Design and Parameter Estimation for User Models of Pointing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09878">http://arxiv.org/abs/2307.09878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antti Keurulainen, Isak Westerlund, Oskar Keurulainen, Andrew Howes</li>
<li>for: 这篇论文主要是为了研究如何使用用户数据来估计模型参数，以便自动化互动设计选择。</li>
<li>methods: 这篇论文使用了一种基于实验设计的方法，通过与 simulate 的参与者进行交互来快速收集数据并估计参数。</li>
<li>results: 研究发现，通过使用这种方法，可以快速地收集大量数据，并且可以使用 Synthetic data 而不需要巨量的人工数据来估计参数。<details>
<summary>Abstract</summary>
User models play an important role in interaction design, supporting automation of interaction design choices. In order to do so, model parameters must be estimated from user data. While very large amounts of user data are sometimes required, recent research has shown how experiments can be designed so as to gather data and infer parameters as efficiently as possible, thereby minimising the data requirement. In the current article, we investigate a variant of these methods that amortises the computational cost of designing experiments by training a policy for choosing experimental designs with simulated participants. Our solution learns which experiments provide the most useful data for parameter estimation by interacting with in-silico agents sampled from the model space thereby using synthetic data rather than vast amounts of human data. The approach is demonstrated for three progressively complex models of pointing.
</details>
<details>
<summary>摘要</summary>
用户模型在互动设计中发挥重要作用，支持自动化互动设计选择。为此，模型参数必须从用户数据进行估算。虽然有时需要很大量的用户数据，但最近的研究表明，可以通过设计 эксперименты，以最小化数据需求来收集数据和推导参数。在当前文章中，我们调查了一种变体的这些方法，即通过训练选择实验设计的策略来减少计算成本。我们的解决方案通过与模型空间中随机 sampling的数字人类进行交互，以使用合成数据而不是巨量的人类数据来学习哪些实验提供最有用的数据 для参数估算。我们的方法在三个不同的指向模型上进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Vulnerable-Nodes-in-Urban-Infrastructure-Interdependent-Network"><a href="#Detecting-Vulnerable-Nodes-in-Urban-Infrastructure-Interdependent-Network" class="headerlink" title="Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network"></a>Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09866">http://arxiv.org/abs/2307.09866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/kdd2023-id546-urbaninfra">https://github.com/tsinghua-fib-lab/kdd2023-id546-urbaninfra</a></li>
<li>paper_authors: Jinzhu Mao, Liu Cao, Chen Gao, Huandong Wang, Hangyu Fan, Depeng Jin, Yong Li</li>
<li>for: 本研究旨在理解和 caracterizar la vulnerabilidad de las infraestructuras urbanas, es decir, los equipos de ingeniería esenciales para el funcionamiento normal de las ciudades y que existen naturalmente en la forma de redes. La aplicación potencial incluye proteger instalaciones frágiles y diseñar topologías robustas, etc.</li>
<li>methods: 本研究 utiliza la teoría de grafos neuronales y aprendizaje por refuerzo para modelar la red heterogénea de la infraestructura urbana y caracterizar su vulnerabilidad de manera precisa. El sistema propuesto se entrena con datos reales y utiliza técnicas de aprendizaje profundo para comprender y analizar la red heterogénea, lo que permite capturar el riesgo de fallos en cascada y descubrir las infraestructuras vulnerables de las ciudades.</li>
<li>results:  los resultados extensivos de los experimentos con diferentes solicitudes demuestran no solo el poder expresivo del sistema propuesto, sino también su capacidad de transferencia y la necesidad de los componentes específicos.<details>
<summary>Abstract</summary>
Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failure and discover vulnerable infrastructures of cities. Extensive experiments with various requests demonstrate not only the expressive power of our system but also transferring ability and necessity of the specific components.
</details>
<details>
<summary>摘要</summary>
理解和 характеризуй城市基础设施的漏洞性，即工程设施的重要组成部分，是对我们来说非常重要的。潜在应用包括保护脆弱设施和设计强健拓扑等等。由于不同的拓扑特征与基础设施漏洞性之间存在强相关性，以及其复杂的演化机制，一些启发式和机器学习分析方法无法处理这种情况。在这篇论文中，我们将城市系统模型为不同类型图的异质图，并提出基于图神经网络和强化学习的系统，可以在实际数据上训练，准确地评估城市系统的漏洞性。我们的系统利用深度学习技术来理解和分析异质图，从而捕捉城市系统中的风险冲击和漏洞设施。广泛的实验表明，我们的系统不仅具有表达力，还能够跨请求传递知识和特定组件的必要性。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-population-informed-approach-to-the-definition-of-data-driven-models-for-structural-dynamics"><a href="#Towards-a-population-informed-approach-to-the-definition-of-data-driven-models-for-structural-dynamics" class="headerlink" title="Towards a population-informed approach to the definition of data-driven models for structural dynamics"></a>Towards a population-informed approach to the definition of data-driven models for structural dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09862">http://arxiv.org/abs/2307.09862</a></li>
<li>repo_url: None</li>
<li>paper_authors: G. Tsialiamanis, N. Dervilis, D. J. Wagg, K. Worden</li>
<li>for: 本研究旨在透过 физиcs-based 方法和机器学习算法结合，对于构造动力学领域中的问题进行模型化，但是由于数据稀缺性，机器学习算法经常表现不佳。这篇研究旨在鼓励使用基于人口的模型，从 físics-based 模型中学习关系性。</li>
<li>methods: 本研究使用了两种meta-learning 算法：model-agnostic meta-learning (MAML) 算法和 conditional neural processes (CNP) 模型。这两种算法在训练人口中学习关系性，并且表现出与传统机器学习算法相似的行为。</li>
<li>results: 研究结果显示，这两种算法可以对于构造动力学问题进行高精度的预测，并且比传统机器学习算法更好地适应训练人口中的数据稀缺性。<details>
<summary>Abstract</summary>
Machine learning has affected the way in which many phenomena for various domains are modelled, one of these domains being that of structural dynamics. However, because machine-learning algorithms are problem-specific, they often fail to perform efficiently in cases of data scarcity. To deal with such issues, combination of physics-based approaches and machine learning algorithms have been developed. Although such methods are effective, they also require the analyser's understanding of the underlying physics of the problem. The current work is aimed at motivating the use of models which learn such relationships from a population of phenomena, whose underlying physics are similar. The development of such models is motivated by the way that physics-based models, and more specifically finite element models, work. Such models are considered transferrable, explainable and trustworthy, attributes which are not trivially imposed or achieved for machine-learning models. For this reason, machine-learning approaches are less trusted by industry and often considered more difficult to form validated models. To achieve such data-driven models, a population-based scheme is followed here and two different machine-learning algorithms from the meta-learning domain are used. The two algorithms are the model-agnostic meta-learning (MAML) algorithm and the conditional neural processes (CNP) model. The algorithms seem to perform as intended and outperform a traditional machine-learning algorithm at approximating the quantities of interest. Moreover, they exhibit behaviour similar to traditional machine learning algorithms (e.g. neural networks or Gaussian processes), concerning their performance as a function of the available structures in the training population.
</details>
<details>
<summary>摘要</summary>
The current work aims to develop models that learn relationships from a population of phenomena with similar underlying physics. This is inspired by the way physics-based models, such as finite element models, work. These models are considered transferrable, explainable, and trustworthy, which are not easily achieved with machine learning models. As a result, machine learning approaches are less trusted by industry and are often more difficult to form validated models.To achieve these data-driven models, a population-based approach is used with two machine learning algorithms from the meta-learning domain: the model-agnostic meta-learning (MAML) algorithm and the conditional neural processes (CNP) model. These algorithms seem to perform as intended and outperform traditional machine learning algorithms at approximating the quantities of interest. Additionally, they exhibit similar behavior to traditional machine learning algorithms, such as neural networks or Gaussian processes, in terms of their performance as a function of the available structures in the training population.
</details></li>
</ul>
<hr>
<h2 id="Towards-Reliable-Rare-Category-Analysis-on-Graphs-via-Individual-Calibration"><a href="#Towards-Reliable-Rare-Category-Analysis-on-Graphs-via-Individual-Calibration" class="headerlink" title="Towards Reliable Rare Category Analysis on Graphs via Individual Calibration"></a>Towards Reliable Rare Category Analysis on Graphs via Individual Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09858">http://arxiv.org/abs/2307.09858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wulongfeng/calirare">https://github.com/wulongfeng/calirare</a></li>
<li>paper_authors: Longfeng Wu, Bowen Lei, Dongkuan Xu, Dawei Zhou</li>
<li>for: 本研究旨在提高罕见类分类的可靠性，尤其是在面临高度不均衡数据分布的情况下。</li>
<li>methods: 本文提出了一种新的个体准确性框架，named CALIRARE，以解决罕见类分类中的独特挑战。该框架包括一种节点级别的不确定性量化算法，以及一种基于实例水平的准确性评价指标EICE。</li>
<li>results: 实验结果表明，CALIRARE可以有效地缓解罕见类分类中的偏置和不确定性问题，提高模型的准确性和可靠性。<details>
<summary>Abstract</summary>
Rare categories abound in a number of real-world networks and play a pivotal role in a variety of high-stakes applications, including financial fraud detection, network intrusion detection, and rare disease diagnosis. Rare category analysis (RCA) refers to the task of detecting, characterizing, and comprehending the behaviors of minority classes in a highly-imbalanced data distribution. While the vast majority of existing work on RCA has focused on improving the prediction performance, a few fundamental research questions heretofore have received little attention and are less explored: How confident or uncertain is a prediction model in rare category analysis? How can we quantify the uncertainty in the learning process and enable reliable rare category analysis?   To answer these questions, we start by investigating miscalibration in existing RCA methods. Empirical results reveal that state-of-the-art RCA methods are mainly over-confident in predicting minority classes and under-confident in predicting majority classes. Motivated by the observation, we propose a novel individual calibration framework, named CALIRARE, for alleviating the unique challenges of RCA, thus enabling reliable rare category analysis. In particular, to quantify the uncertainties in RCA, we develop a node-level uncertainty quantification algorithm to model the overlapping support regions with high uncertainty; to handle the rarity of minority classes in miscalibration calculation, we generalize the distribution-based calibration metric to the instance level and propose the first individual calibration measurement on graphs named Expected Individual Calibration Error (EICE). We perform extensive experimental evaluations on real-world datasets, including rare category characterization and model calibration tasks, which demonstrate the significance of our proposed framework.
</details>
<details>
<summary>摘要</summary>
罕见类在许多实际网络中充斥，在各种高风险应用中扮演着重要角色，如金融欺诈检测、网络入侵检测和罕见疾病诊断。罕见类分析（RCA）指的是在高度不均匀数据分布中探测、特征化和理解少数类别的行为。而现有大多数RCA研究都专注于提高预测性能，而忽略了一些基本研究问题，如预测模型对罕见类的自信度和不确定性如何？如何量化学习过程中的不确定性，使罕见类分析可靠？为回答这些问题，我们开始 investigate existing RCA方法的miscalibration。实验结果表明，现状顶峰RCA方法主要是对少数类预测过于自信，对多数类预测过于不自信。这一观察导我们提出一种新的个体准确框架，名为CALIRARE，以解决RCAUnique挑战，使罕见类分析可靠。 Specifically, to quantify the uncertainties in RCA, we develop a node-level uncertainty quantification algorithm to model the overlapping support regions with high uncertainty; to handle the rarity of minority classes in miscalibration calculation, we generalize the distribution-based calibration metric to the instance level and propose the first individual calibration measurement on graphs named Expected Individual Calibration Error (EICE). We perform extensive experimental evaluations on real-world datasets, including rare category characterization and model calibration tasks, which demonstrate the significance of our proposed framework.
</details></li>
</ul>
<hr>
<h2 id="A-Fast-and-Map-Free-Model-for-Trajectory-Prediction-in-Traffics"><a href="#A-Fast-and-Map-Free-Model-for-Trajectory-Prediction-in-Traffics" class="headerlink" title="A Fast and Map-Free Model for Trajectory Prediction in Traffics"></a>A Fast and Map-Free Model for Trajectory Prediction in Traffics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09831">http://arxiv.org/abs/2307.09831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhong Xiang, Jingmin Zhang, Zhixiong Nan</li>
<li>for: 提出了一种高效的路径预测模型，不需要卫星地图，可以在实际交通场景中提高预测精度和计算效率。</li>
<li>methods: 提出了一种两stage的方法，首先编码单个机器的空间时间信息，然后在第二stage中探索多个机器的空间时间交互信息，并使用注意力机制、LSTM、图像卷积网络和时间变换器来学习各自机器的 ric dynamic和交互信息。</li>
<li>results: 在Argoverse数据集上比较了现有的map-free方法和map-based状态前方法，其性能最高，并且比基eline方法更快。<details>
<summary>Abstract</summary>
To handle the two shortcomings of existing methods, (i)nearly all models rely on high-definition (HD) maps, yet the map information is not always available in real traffic scenes and HD map-building is expensive and time-consuming and (ii) existing models usually focus on improving prediction accuracy at the expense of reducing computing efficiency, yet the efficiency is crucial for various real applications, this paper proposes an efficient trajectory prediction model that is not dependent on traffic maps. The core idea of our model is encoding single-agent's spatial-temporal information in the first stage and exploring multi-agents' spatial-temporal interactions in the second stage. By comprehensively utilizing attention mechanism, LSTM, graph convolution network and temporal transformer in the two stages, our model is able to learn rich dynamic and interaction information of all agents. Our model achieves the highest performance when comparing with existing map-free methods and also exceeds most map-based state-of-the-art methods on the Argoverse dataset. In addition, our model also exhibits a faster inference speed than the baseline methods.
</details>
<details>
<summary>摘要</summary>
为了解决现有方法的两个缺陷，即大多数模型依赖高清晰度地图， yet 地图信息在实际交通场景中不一定可用，而且制图 HD 地图是昂贵的和时间consuming，以及现有模型通常是通过增强预测精度来忽略计算效率，而计算效率对实际应用来说是关键，这篇论文提出了一种高效的轨迹预测模型，这种模型不依赖于交通地图。我们的核心想法是在第一阶段对单个机器的空间时间信息进行编码，然后在第二阶段通过多 Agent 的空间时间互动来探索。通过全面利用注意机制、LSTM、图 convolution 网络和时间变换器，我们的模型能够学习所有机器的丰富动态和互动信息。我们的模型在对比现有无地图方法的情况下 achieved 最高性能，同时也超过了大多数基于地图的现状方法在 Argoverse 数据集上。此外，我们的模型还具有更快的推理速度than 基eline 方法。
</details></li>
</ul>
<hr>
<h2 id="Online-Continual-Learning-for-Robust-Indoor-Object-Recognition"><a href="#Online-Continual-Learning-for-Robust-Indoor-Object-Recognition" class="headerlink" title="Online Continual Learning for Robust Indoor Object Recognition"></a>Online Continual Learning for Robust Indoor Object Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09827">http://arxiv.org/abs/2307.09827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Umberto Michieli, Mete Ozay</li>
<li>for: 本研究旨在提高家用机器人视觉系统在变化环境中与未见类交互。</li>
<li>methods: 我们提出了一种基于少量示例学习（few-shot，FS）和不断学习（continual learning，CL）的方法，称为RobOCLe。RobOCLe使用高阶统计矩阵来描述样本的特征空间，并基于这些矩阵的相似性来预测样本的类别标签。</li>
<li>results: 我们在不同的训练&#x2F;测试扩展情况下评估了CL模型的RobOCLe的Robustness。结果显示，不同的高阶统计矩阵可以捕捉不同的变形特征，从而提供更高的Robustness而无需减少推断速率。<details>
<summary>Abstract</summary>
Vision systems mounted on home robots need to interact with unseen classes in changing environments. Robots have limited computational resources, labelled data and storage capability. These requirements pose some unique challenges: models should adapt without forgetting past knowledge in a data- and parameter-efficient way. We characterize the problem as few-shot (FS) online continual learning (OCL), where robotic agents learn from a non-repeated stream of few-shot data updating only a few model parameters. Additionally, such models experience variable conditions at test time, where objects may appear in different poses (e.g., horizontal or vertical) and environments (e.g., day or night). To improve robustness of CL agents, we propose RobOCLe, which; 1) constructs an enriched feature space computing high order statistical moments from the embedded features of samples; and 2) computes similarity between high order statistics of the samples on the enriched feature space, and predicts their class labels. We evaluate robustness of CL models to train/test augmentations in various cases. We show that different moments allow RobOCLe to capture different properties of deformations, providing higher robustness with no decrease of inference speed.
</details>
<details>
<summary>摘要</summary>
家用机器人视系统需要与未经见过的类型在变化环境中交互。机器人有限的计算资源，标注数据和存储能力。这些需求带来一些独特的挑战：模型需要适应无重复数据和参数的方式进行学习，而不会忘记过去的知识。我们描述这个问题为几shot（FS）在线连续学习（OCL）问题， где机器人代理人通过非重复的流量几shot数据来学习，只需要更新几个模型参数。此外，模型在测试时可能会遇到不同的位置（例如，横向或纵向）和环境（例如，白天或黑夜）。为了提高CL模型的Robustness，我们提议RobOCLe，它包括以下两个部分：1. 构建增强的特征空间，计算样本嵌入特征的高阶统计 moments。2. 在增强的特征空间上计算样本的相似度，预测样本的类别标签。我们对CL模型的Robustness进行了不同的训练/测试拓展的评估。我们发现不同的 moments 可以让 RobOCLe 捕捉不同的变形特征，从而提供更高的Robustness，而无需减少推断速度。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Forecasting-with-Coherent-Aggregation"><a href="#Probabilistic-Forecasting-with-Coherent-Aggregation" class="headerlink" title="Probabilistic Forecasting with Coherent Aggregation"></a>Probabilistic Forecasting with Coherent Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09797">http://arxiv.org/abs/2307.09797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffrey Négiar, Ruijun Ma, O. Nangba Meetei, Mengfei Cao, Michael W. Mahoney</li>
<li>for: 本研究旨在提供高度准确的预测分布，同时尊重层次结构信息。</li>
<li>methods: 本文提出一种基于因素模型结构的新方法，通过构建层次结构的预测来保证预测的准确性。</li>
<li>results: 对三个层次预测数据集进行比较，本方法可以达到11.8-41.4%的显著提升，而且可以调整基础分布和因素数量来影响预测结果。<details>
<summary>Abstract</summary>
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probability Score and quantile losses. We can choose arbitrary continuous distributions for the factor and the base-level distributions. We compare our method to two previous methods which can be optimized end-to-end, while enforcing coherent aggregation. Our model achieves significant improvements: between $11.8-41.4\%$ on three hierarchical forecasting datasets. We also analyze the influence of parameters in our model with respect to base-level distribution and number of factors.
</details>
<details>
<summary>摘要</summary>
获取准确的 probabilistic 预测，同时尊重层次结构，在许多应用程序中是一项重要的操作挑战，例如能源管理、供应链规划和资源分配。基本挑战在多变量预测中，即 forecast 需要尊重层次结构的准确性。在这篇论文中，我们提出了一种新的模型，利用因子模型结构生成准确的预测。这是由于一个简单观察（交换性）的结论：在层次结构中重新排序基级系列不会改变它们的总和。我们的模型使用卷积神经网络生成因子、因子加载和基级分布的参数；它生成可以与模型参数进行梯度检查的样本；因此它可以优化任何样本基于损失函数，包括连续排名概率分数和量iles损失。我们可以选择任何连续分布来描述因子和基级分布。我们与之前的两种可以结构化优化的方法进行比较，我们的方法在三个层次预测 datasets 上实现了显著提升：11.8-41.4%。我们还分析了我们模型中参数的影响，即基级分布和因子数量。
</details></li>
</ul>
<hr>
<h2 id="ZeroQuant-FP-A-Leap-Forward-in-LLMs-Post-Training-W4A8-Quantization-Using-Floating-Point-Formats"><a href="#ZeroQuant-FP-A-Leap-Forward-in-LLMs-Post-Training-W4A8-Quantization-Using-Floating-Point-Formats" class="headerlink" title="ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"></a>ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09782">http://arxiv.org/abs/2307.09782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
<li>paper_authors: Xiaoxia Wu, Zhewei Yao, Yuxiong He</li>
<li>for: 本研究旨在探讨大语言模型（LLM）中 strike 平衡计算效率和模型质量的挑战，以及在 NVIDIA H100 硬件上部署 LLM 的可能性。</li>
<li>methods: 本研究使用浮点数（FP）量化技术，特别是 FP8 和 FP4，来解决 LLM 中的异常值问题。我们还提出了两种缩放约束来减少精度对齐所带来的开销，并将 LoRC 策略integrated into our quantization methods。</li>
<li>results: 我们的研究发现，对 LLM 来说，FP8 活动常量比 INT8 更高效，特别是模型参数超过一亿时。对于参量量化，我们发现 FP4 与 INT4 相比，性能相对或更高，使得在 FP 支持的硬件上进行部署更加简单。我们的研究结果表明，FP 量化可以为 LLM 带来很高的效率，使其在资源有限的场景中进行部署。<details>
<summary>Abstract</summary>
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints for weight quantization that negligibly impact the performance compared to the standard W4A8 model. We additionally enhance our quantization methods by integrating the Low Rank Compensation (LoRC) strategy, yielding improvements especially in smaller models. The results of our investigation emphasize the immense potential of FP quantization for LLMs, paving the way for high-efficiency deployment in resource-limited settings.
</details>
<details>
<summary>摘要</summary>
在大语言模型（LLM）领域中，Computational efficiency和维护模型质量的平衡是一项具有挑战性的任务。uniform quantization的内在限制，特别是处理异常值时，驱动了我们对浮点（FP）量化的研究，特别是FP8和FP4。这项研究发现，对LLMs来说，FP8活动通常比INT8相对胜过，其性能优势在模型参数超过一亿时变得更加明显。对于权量量化，我们发现，FP4的性能与INT4相当，甚至超过INT4，这使得在FP支持的硬件上进行部署变得更加简单。为了减少精度对齐所导致的开销，我们提议两种缩放约束 для权量量化，对W4A8模型的性能影响很小。此外，我们还增强了我们的量化方法，通过 интеграating Low Rank Compensation（LoRC）策略，尤其是在较小的模型中，得到了改进。研究结果表明，FP量化对LLMs具有巨大的潜力，为高效部署在有限资源的场景提供了道路。
</details></li>
</ul>
<hr>
<h2 id="Text2Layer-Layered-Image-Generation-using-Latent-Diffusion-Model"><a href="#Text2Layer-Layered-Image-Generation-using-Latent-Diffusion-Model" class="headerlink" title="Text2Layer: Layered Image Generation using Latent Diffusion Model"></a>Text2Layer: Layered Image Generation using Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09781">http://arxiv.org/abs/2307.09781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyang Zhang, Wentian Zhao, Xin Lu, Jeff Chien</li>
<li>for: 该论文旨在探讨层compositing的图像编辑方法，以层分割图像生成为出发点。</li>
<li>methods: 该论文提出了一种基于扩散模型的层分割图像生成方法，通过训练自编码器和扩散模型来实现层分割图像的生成。</li>
<li>results: 实验结果显示，该方法可以生成高质量的层分割图像，同时提供了一个基准 для未来的研究。<details>
<summary>Abstract</summary>
Layer compositing is one of the most popular image editing workflows among both amateurs and professionals. Motivated by the success of diffusion models, we explore layer compositing from a layered image generation perspective. Instead of generating an image, we propose to generate background, foreground, layer mask, and the composed image simultaneously. To achieve layered image generation, we train an autoencoder that is able to reconstruct layered images and train diffusion models on the latent representation. One benefit of the proposed problem is to enable better compositing workflows in addition to the high-quality image output. Another benefit is producing higher-quality layer masks compared to masks produced by a separate step of image segmentation. Experimental results show that the proposed method is able to generate high-quality layered images and initiates a benchmark for future work.
</details>
<details>
<summary>摘要</summary>
层compositing是现场图像编辑中最受欢迎的工作流程，both amateur和professional都广泛采用。受扩散模型的成功启发，我们从层图生成的角度来探讨层compositing。而不是生成整个图像，我们提议同时生成背景、前景、层Mask和组合图像。为实现层图生成，我们训练了一个能够重建层图的autoencoder，并在幂 rappresentation中训练扩散模型。本提案的两大优点是：一是可以实现更好的组合工作流程，二是生成高质量的层Mask，比起分开进行图像 segmentation后生成的Mask更高质量。实验结果表明，我们的方法能够生成高质量的层图并成为未来工作的标准。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Single-Feature-Importance-with-ICECREAM"><a href="#Beyond-Single-Feature-Importance-with-ICECREAM" class="headerlink" title="Beyond Single-Feature Importance with ICECREAM"></a>Beyond Single-Feature Importance with ICECREAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09779">http://arxiv.org/abs/2307.09779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Oesterle, Patrick Blöbaum, Atalanti A. Mastakouri, Elke Kirschbaum</li>
<li>for: 本研究旨在解释模型中不同变量协会的影响，以及这些变量协会对Target变量的分布的影响。</li>
<li>methods: 本研究提出了一种信息理论量化度量，用于度量协会变量对Target变量的影响。这种方法可以识别具有特定结果的变量协会，而不是单独评估每个变量的重要性。</li>
<li>results: 在synthetic和实际数据上进行了实验，并显示了ICECREAM在解释和根本原因分析任务中的出色表现，并在这些任务中实现了惊人的准确率。<details>
<summary>Abstract</summary>
Which set of features was responsible for a certain output of a machine learning model? Which components caused the failure of a cloud computing application? These are just two examples of questions we are addressing in this work by Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM). Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable. This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods which can assign contributions only to individual factors and rank them by their importance. In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.
</details>
<details>
<summary>摘要</summary>
我们在这个工作中通过“ icecream”方法来解释模型的输出和云计算应用的失败。 Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable. This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods which can assign contributions only to individual factors and rank them by their importance. In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.Here's the breakdown of the translation:* "icecream" 是我们提出的一种信息理论基于的量化方法，用于解释模型的输出和云计算应用的失败。* "coalition of variables" 是指一组变量的集合，这些变量可能会影响模型的输出或云计算应用的结果。* "target variable" 是指我们想要解释的变量，例如模型的输出或云计算应用的结果。* "well-established explainability and causal contribution analysis methods" 是指已有的解释和 causal contribution 分析方法，这些方法可能会忽略一些变量的相互作用，导致解释不准确。* "information-theoretic quantitative measure" 是指我们提出的一种基于信息理论的量化方法，用于衡量变量之间的相互作用。* "essential to obtain a certain outcome" 是指这些变量的集合是必须的，以确保模型的输出或云计算应用的结果。* "impressive accuracy" 是指我们在实验中得到的结果具有很高的准确率。I hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Generating-Redstone-Style-Cities-in-Minecraft"><a href="#Generating-Redstone-Style-Cities-in-Minecraft" class="headerlink" title="Generating Redstone Style Cities in Minecraft"></a>Generating Redstone Style Cities in Minecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09777">http://arxiv.org/abs/2307.09777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Huang, Chengpeng Hu, Julian Togelius, Jialin Liu</li>
<li>for: 这个论文旨在提供一种用于生成 Minecraft 中城市的生成器，以便玩家可以在不同的场景中体验更多的多样性，并帮助理解和改进在数字世界和真实世界中城市的设计。</li>
<li>methods: 生成过程由六个主要步骤组成，即荒地清理、地形修改、建筑布局生成、路径规划、灯光设置和墙体建造。三种算法，包括一种规则基于的算法、一种演化布局算法和一种随机算法，被应用于生成建筑布局，从而决定不同的红石风格建筑的位置。</li>
<li>results: 实验结果表明，规则基于的算法在平坦地图上能够更快地找到可接受的建筑布局，而演化布局算法在 rugged 地图上表现较好。用户研究 comparing 我们的生成器与2022年版 Minecraft Settlement Generation Competition 的优秀入选作品，根据比赛的评价标准，表明我们的生成器在适应和功能性方面表现良好。<details>
<summary>Abstract</summary>
Procedurally generating cities in Minecraft provides players more diverse scenarios and could help understand and improve the design of cities in other digital worlds and the real world. This paper presents a city generator that was submitted as an entry to the 2023 Edition of Minecraft Settlement Generation Competition for Minecraft. The generation procedure is composed of six main steps, namely vegetation clearing, terrain reshaping, building layout generation, route planning, streetlight placement, and wall construction. Three algorithms, including a heuristic-based algorithm, an evolving layout algorithm, and a random one are applied to generate the building layout, thus determining where to place different redstone style buildings, and tested by generating cities on random maps in limited time. Experimental results show that the heuristic-based algorithm is capable of finding an acceptable building layout faster for flat maps, while the evolving layout algorithm performs better in evolving layout for rugged maps. A user study is conducted to compare our generator with outstanding entries of the competition's 2022 edition using the competition's evaluation criteria and shows that our generator performs well in the adaptation and functionality criteria
</details>
<details>
<summary>摘要</summary>
通过生成 Minecraft 中的城市，玩家可以获得更多的多样化场景，并且可以帮助我们更好地理解和改进真实世界中的城市设计。这篇论文介绍了一种城市生成器，该生成器作为2023年 Minecraft 定居点生成比赛的参赛作品。生成过程包括6个主要步骤，namely 维度清除、地形重塑、建筑布局生成、路径规划、灯光设置和墙体建设。三种算法，包括一种规则基于的算法、一种演化布局算法和一种随机算法，被应用于生成建筑布局，因此决定在不同的红石风格建筑物中放置不同的位置。这些算法在生成城市的过程中被测试，并在限时内生成Random maps上进行了测试。实验结果显示，规则基于的算法在平坦地图上能够更快地找到适当的建筑布局，而演化布局算法在 rugged 地图上表现更好。另外，我们进行了用户研究，对2022年版 Minecraft 定居点生成比赛的出色入选作品进行了比较，并通过使用比赛的评价标准表明，我们的生成器在适应性和功能性方面表现良好。
</details></li>
</ul>
<hr>
<h2 id="Eliminating-Label-Leakage-in-Tree-Based-Vertical-Federated-Learning"><a href="#Eliminating-Label-Leakage-in-Tree-Based-Vertical-Federated-Learning" class="headerlink" title="Eliminating Label Leakage in Tree-Based Vertical Federated Learning"></a>Eliminating Label Leakage in Tree-Based Vertical Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10318">http://arxiv.org/abs/2307.10318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hideaki Takahashi, Jingjing Liu, Yang Liu</li>
<li>for: 这项研究旨在探讨tree-based federated learning中的泄露问题，以及一种有效的防止泄露的防御机制。</li>
<li>methods: 这项研究使用了一种新的标签推导攻击方法ID2Graph，通过使用训练样本中的记录IDsets生成图结构，提取图中的社区信息，并使用社区信息对本地数据进行分 clustering。</li>
<li>results: 实验结果表明，ID2Graph攻击可以快速地泄露tree-based模型的训练标签信息，而ID-LMID防御机制能够有效地防止标签泄露。<details>
<summary>Abstract</summary>
Vertical federated learning (VFL) enables multiple parties with disjoint features of a common user set to train a machine learning model without sharing their private data. Tree-based models have become prevalent in VFL due to their interpretability and efficiency. However, the vulnerability of tree-based VFL has not been sufficiently investigated. In this study, we first introduce a novel label inference attack, ID2Graph, which utilizes the sets of record-IDs assigned to each node (i.e., instance space) to deduce private training labels. The ID2Graph attack generates a graph structure from training samples, extracts communities from the graph, and clusters the local dataset using community information. To counteract label leakage from the instance space, we propose an effective defense mechanism, ID-LMID, which prevents label leakage by focusing on mutual information regularization. Comprehensive experiments conducted on various datasets reveal that the ID2Graph attack presents significant risks to tree-based models such as Random Forest and XGBoost. Further evaluations on these benchmarks demonstrate that ID-LMID effectively mitigates label leakage in such instances.
</details>
<details>
<summary>摘要</summary>
Vertical Federated Learning (VFL) 允许不同党有不同特征的用户集合训练机器学习模型，无需共享私人数据。Tree-based模型在VFL中变得普遍，因为它们具有可读性和效率。然而，tree-based VFL 的漏洞尚未得到充分调查。在这种研究中，我们首先介绍了一种新的标签推理攻击，ID2Graph，它利用每个节点（即实例空间）分配的集合ID来推理私人训练标签。ID2Graph 攻击生成了训练样本中的图结构，提取了图中的社区信息，并使用社区信息将地方数据分组。为了防止实例空间中的标签泄露，我们提议一种有效的防御机制，ID-LMID，它通过互信息规范来防止标签泄露。对于Random Forest和XGBoost等树状模型，我们的实验表明，ID2Graph 攻击对这些模型具有显著的风险。而ID-LMID 可以有效地遏制标签泄露在这些情况下。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-WiFi-CSI-Based-Human-Activity-Recognition-A-Systematic-Study"><a href="#Self-Supervised-Learning-for-WiFi-CSI-Based-Human-Activity-Recognition-A-Systematic-Study" class="headerlink" title="Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study"></a>Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02412">http://arxiv.org/abs/2308.02412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JJJinx/SSLCSI">https://github.com/JJJinx/SSLCSI</a></li>
<li>paper_authors: Ke Xu, Jiangtao Wang, Hongyuan Zhu, Dingchang Zheng</li>
<li>for: 本研究旨在探讨wiFi CSIsensor-based Human Activity Recognition（HAR）领域中SSL算法的潜力，以提高深度学习模型在CSIsensor数据上的性能。</li>
<li>methods: 本研究使用了四类SSL算法，包括已经研究过的和尚未研究过的，并对三个公共可用的CSIsensor HAR数据集进行了评估。</li>
<li>results: 实验结果表明，SSL算法可以有效地提高wiFi CSIsensor-based HAR的性能，但是还存在一些限制和盲点，需要进一步改进才能在实际应用中使用。<details>
<summary>Abstract</summary>
Recently, with the advancement of the Internet of Things (IoT), WiFi CSI-based HAR has gained increasing attention from academic and industry communities. By integrating the deep learning technology with CSI-based HAR, researchers achieve state-of-the-art performance without the need of expert knowledge. However, the scarcity of labeled CSI data remains the most prominent challenge when applying deep learning models in the context of CSI-based HAR due to the privacy and incomprehensibility of CSI-based HAR data. On the other hand, SSL has emerged as a promising approach for learning meaningful representations from data without heavy reliance on labeled examples. Therefore, considerable efforts have been made to address the challenge of insufficient data in deep learning by leveraging SSL algorithms. In this paper, we undertake a comprehensive inventory and analysis of the potential held by different categories of SSL algorithms, including those that have been previously studied and those that have not yet been explored, within the field. We provide an in-depth investigation of SSL algorithms in the context of WiFi CSI-based HAR. We evaluate four categories of SSL algorithms using three publicly available CSI HAR datasets, each encompassing different tasks and environmental settings. To ensure relevance to real-world applications, we design performance metrics that align with specific requirements. Furthermore, our experimental findings uncover several limitations and blind spots in existing work, highlighting the barriers that need to be addressed before SSL can be effectively deployed in real-world WiFi-based HAR applications. Our results also serve as a practical guideline for industry practitioners and provide valuable insights for future research endeavors in this field.
</details>
<details>
<summary>摘要</summary>
最近，因互联网物联网（IoT）的进步，WiFi CSI基本的人工智能识别（HAR）已经获得了学术和工业社区的增加关注。通过融合深度学习技术与CSI基本的HAR，研究人员可以 achieve state-of-the-art performance 不需要专家知识。然而，CSI基本的数据短缺仍然是对应深度学习模型的最大挑战，因为CSI基本的数据隐私和不可理解。另一方面，安全性可见性学习（SSL）已经出现了具有潜在的应用前景的方法，可以从数据中学习有意义的表现，不需要专家知识。因此，许多努力已经被做出来解决深度学习中的数据不足问题，通过应用SSL算法。在这篇论文中，我们进行了广泛的调查和分析不同类型的SSL算法，包括已经研究过的和尚未研究过的，在这个领域中。我们对这些SSL算法进行了深入的探索，并将其应用到WiFi CSI基本的HAR中。我们使用三个公开available的CSI HAR数据集，每个数据集都包含不同的任务和环境设定，以评估SSL算法的性能。为了保持实际应用的相关性，我们设计了与实际应用相关的表现度量。我们的实验结果显示出了一些限制和盲点，强调了现有的问题，需要解决才能够在实际应用中有效地应用SSL。我们的结果也服务了实际应用的实践者，并提供了价值的见解，供未来的研究努力参考。
</details></li>
</ul>
<hr>
<h2 id="Perturbing-a-Neural-Network-to-Infer-Effective-Connectivity-Evidence-from-Synthetic-EEG-Data"><a href="#Perturbing-a-Neural-Network-to-Infer-Effective-Connectivity-Evidence-from-Synthetic-EEG-Data" class="headerlink" title="Perturbing a Neural Network to Infer Effective Connectivity: Evidence from Synthetic EEG Data"></a>Perturbing a Neural Network to Infer Effective Connectivity: Evidence from Synthetic EEG Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09770">http://arxiv.org/abs/2307.09770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peizhen Yang, Xinke Shen, Zongsheng Li, Zixiang Luo, Kexin Lou, Quanying Liu</li>
<li>for: 这项研究旨在利用数据驱动方法推断脑电响应的 causal 关系，以探索脑信息处理和认知功能的逻辑结构。</li>
<li>methods: 研究人员使用了不同类型的人工神经网络（CNN、vanilla RNN、GRU、LSTM和Transformer）来预测历史数据的脑电信号，然后通过干扰这些神经网络的输入来推断脑电信号之间的 causal 关系。</li>
<li>results: 研究人员发现，使用人工神经网络来预测脑电信号的方法可以更好地推断脑电信号之间的 causal 关系，并且可以超过经典的Granger causality方法。<details>
<summary>Abstract</summary>
Identifying causal relationships among distinct brain areas, known as effective connectivity, holds key insights into the brain's information processing and cognitive functions. Electroencephalogram (EEG) signals exhibit intricate dynamics and inter-areal interactions within the brain. However, methods for characterizing nonlinear causal interactions among multiple brain regions remain relatively underdeveloped. In this study, we proposed a data-driven framework to infer effective connectivity by perturbing the trained neural networks. Specifically, we trained neural networks (i.e., CNN, vanilla RNN, GRU, LSTM, and Transformer) to predict future EEG signals according to historical data and perturbed the networks' input to obtain effective connectivity (EC) between the perturbed EEG channel and the rest of the channels. The EC reflects the causal impact of perturbing one node on others. The performance was tested on the synthetic EEG generated by a biological-plausible Jansen-Rit model. CNN and Transformer obtained the best performance on both 3-channel and 90-channel synthetic EEG data, outperforming the classical Granger causality method. Our work demonstrated the potential of perturbing an artificial neural network, learned to predict future system dynamics, to uncover the underlying causal structure.
</details>
<details>
<summary>摘要</summary>
描述脑部信息处理和认知功能的关键信息在脑部之间的关系，即有效连接，是脑部研究的关键。电энцеfalogram（EEG）信号表现出脑部内部不同区域之间的复杂动态交互。然而，用于描述多个脑部区域之间的非线性相关性的方法仍然较为未发展。在这项研究中，我们提出了一种数据驱动的框架，用于推测有效连接。具体来说，我们将神经网络（如CNN、vanilla RNN、GRU、LSTM和Transformer）训练为预测基于历史数据的未来EEG信号，并将神经网络的输入干扰以获得有效连接（EC）。EC表示对一个节点的干扰对其他节点的影响。我们在生物可能的Jansen-Rit模型生成的 sintetic EEG数据上测试了表现。CNN和Transformer在3个渠道和90个渠道的 sintetic EEG数据上表现出色，超过了经典Granger causality方法。我们的工作表明了可以使用训练用于预测未来系统动力的人工神经网络来揭示脑部的下面 causal结构。
</details></li>
</ul>
<hr>
<h2 id="Sig-Splines-universal-approximation-and-convex-calibration-of-time-series-generative-models"><a href="#Sig-Splines-universal-approximation-and-convex-calibration-of-time-series-generative-models" class="headerlink" title="Sig-Splines: universal approximation and convex calibration of time series generative models"></a>Sig-Splines: universal approximation and convex calibration of time series generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09767">http://arxiv.org/abs/2307.09767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Magnus Wiese, Phillip Murray, Ralf Korn</li>
<li>for: 这篇论文是为了描述一种基于神经花式的多变量离散时间序列资料的新型生成模型。</li>
<li>methods: 这篇论文使用了神经花式的线性转换和签名转换来建立一个可以替换传统神经网络的新型生成模型。</li>
<li>results: 这篇论文的结果显示了这个新型模型不仅具有神经网络的通用性，而且还具有线性的参数。<details>
<summary>Abstract</summary>
We propose a novel generative model for multivariate discrete-time time series data. Drawing inspiration from the construction of neural spline flows, our algorithm incorporates linear transformations and the signature transform as a seamless substitution for traditional neural networks. This approach enables us to achieve not only the universality property inherent in neural networks but also introduces convexity in the model's parameters.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的生成模型，用于多变量离散时间序列数据。我们的算法启发自神经剖流的建构，并将线性变换和签名变换作为神经网络的替换方式。这种方法不仅可以实现神经网络的universality性，还可以在模型参数中引入凸形。
</details></li>
</ul>
<hr>
<h2 id="Towards-Building-More-Robust-Models-with-Frequency-Bias"><a href="#Towards-Building-More-Robust-Models-with-Frequency-Bias" class="headerlink" title="Towards Building More Robust Models with Frequency Bias"></a>Towards Building More Robust Models with Frequency Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09763">http://arxiv.org/abs/2307.09763</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/retsuh-bqw/ICCV23-Towards-Building-More-Robust-Models-with-Frequency-Bias">https://github.com/retsuh-bqw/ICCV23-Towards-Building-More-Robust-Models-with-Frequency-Bias</a></li>
<li>paper_authors: Qingwen Bu, Dong Huang, Heming Cui</li>
<li>for: 本研究旨在提高深度神经网络的抗辐射性，并提出一种可插入到任何敌方训练框架中的吸引模块，以便更好地利用频谱特征。</li>
<li>methods: 本研究使用了一种叫做频率偏好控制模块（Frequency Preference Control Module），该模块可以自适应地重新配置中间特征表示的低频和高频成分，以更好地利用频谱特征。</li>
<li>results: 实验表明，使用我们提出的频率偏好控制模块可以轻松地将敌方训练框架与不同的架构和数据集绑定，并且可以提高模型的抗辐射性。此外，我们还进行了对频谱偏好对敌方训练过程和最终抗辐射性的研究，发现了有趣的结论。<details>
<summary>Abstract</summary>
The vulnerability of deep neural networks to adversarial samples has been a major impediment to their broad applications, despite their success in various fields. Recently, some works suggested that adversarially-trained models emphasize the importance of low-frequency information to achieve higher robustness. While several attempts have been made to leverage this frequency characteristic, they have all faced the issue that applying low-pass filters directly to input images leads to irreversible loss of discriminative information and poor generalizability to datasets with distinct frequency features. This paper presents a plug-and-play module called the Frequency Preference Control Module that adaptively reconfigures the low- and high-frequency components of intermediate feature representations, providing better utilization of frequency in robust learning. Empirical studies show that our proposed module can be easily incorporated into any adversarial training framework, further improving model robustness across different architectures and datasets. Additionally, experiments were conducted to examine how the frequency bias of robust models impacts the adversarial training process and its final robustness, revealing interesting insights.
</details>
<details>
<summary>摘要</summary>
深度神经网络在面对攻击样本时的抵触性问题，对其广泛应用带来了很大障碍，尽管它在不同领域取得了成功。近期一些研究表明，使用对抗样本训练的模型强调低频信息的重要性可以提高模型的抗性。然而，直接将输入图像应用低通滤波器导致了不可逆的信息损失和不好的泛化性，这限制了在不同频谱特征的数据集上的应用。本文提出了一个叫做频率偏好控制模块的插件式模块，可以自适应调整中间特征表示中的低频和高频组分，从而更好地利用频率在Robust学习中。实验表明，我们的提议的模块可以轻松地与任何对抗训练框架结合使用，并在不同架构和数据集上提高模型的抗性。此外，我们还进行了对抗训练过程中频率偏好的模型的影响和最终抗性的实验，发现了有趣的发现。
</details></li>
</ul>
<hr>
<h2 id="Reinforcing-POD-based-model-reduction-techniques-in-reaction-diffusion-complex-networks-using-stochastic-filtering-and-pattern-recognition"><a href="#Reinforcing-POD-based-model-reduction-techniques-in-reaction-diffusion-complex-networks-using-stochastic-filtering-and-pattern-recognition" class="headerlink" title="Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition"></a>Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09762">http://arxiv.org/abs/2307.09762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Ajayakumar, Soumyendu Raha</li>
<li>for: 模型复杂网络系统，但是这些系统的维度可能会使其分析变得困难。</li>
<li>methods: 我们提出了一种算法框架，该框架结合了模式识别技术和随机滤波理论来增强模型的输出。</li>
<li>results: 我们的研究结果显示，我们的方法可以在受到输入数据干扰后提高模型的准确性。Here’s the breakdown of each point in English:</li>
<li>for: The paper is written for modeling complex networks, which can be challenging to analyze due to their high dimensionality.</li>
<li>methods: The paper proposes an algorithmic framework that combines techniques from pattern recognition and stochastic filtering theory to enhance the output of dimensionality reduction models.</li>
<li>results: The results of the study show that the proposed method can improve the accuracy of the surrogate model under perturbed inputs.<details>
<summary>Abstract</summary>
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
</details>
<details>
<summary>摘要</summary>
复杂网络被用来模型许多实际世界系统。然而，这些系统的维度可以使得它们变得困难分析。降维技术如POD可以在这些情况下使用。然而，这些模型对输入数据的变化很敏感。我们提出了一个算法框架，该框架结合了 Pattern recognition（PR）和随机滤波理论来增强模型的输出。我们的研究结果表明，我们的方法可以在受到输入数据变化时提高模型的准确性。深度神经网络（DNNs）容易受到恶意攻击。然而，最近的研究发现，神经Ordinary Differential Equations（ODEs）在特定应用中具有抗锋性。我们对比了我们的算法框架与神经ODE-based方法作为参考。
</details></li>
</ul>
<hr>
<h2 id="FedBug-A-Bottom-Up-Gradual-Unfreezing-Framework-for-Federated-Learning"><a href="#FedBug-A-Bottom-Up-Gradual-Unfreezing-Framework-for-Federated-Learning" class="headerlink" title="FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning"></a>FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10317">http://arxiv.org/abs/2307.10317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iandrover/fedbug">https://github.com/iandrover/fedbug</a></li>
<li>paper_authors: Chia-Hsiang Kao, Yu-Chiang Frank Wang</li>
<li>for: 这篇论文的目的是提出一种基于分布式学习的 Federated Learning（FL）框架，以帮助多个客户端在共享模型中协同训练而无需泄露数据隐私。</li>
<li>methods: 这篇论文提出了一种名为 FedBug（基于底层慢融冰的 Federated Learning）的新的 FL 框架，该框架可以有效地 Mitigate 客户端泄露（client drift）问题。FedBug 在客户端上启动自适应层次结构，从输入层到输出层逐层解冻模型参数，以使模型在多个客户端上协同训练并保持一致性。</li>
<li>results: 该论文通过了严格的实验和理论分析，证明 FedBug 在多个数据集、训练条件和网络架构下具有优秀的性能和可靠性。<details>
<summary>Abstract</summary>
Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze FedBug in a novel over-parameterization FL setup, revealing its superior convergence rate compared to FedAvg. Through comprehensive experiments, spanning various datasets, training conditions, and network architectures, we validate the efficacy of FedBug. Our contributions encompass a novel FL framework, theoretical analysis, and empirical validation, demonstrating the wide potential and applicability of FedBug.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）提供了一个合作训练框架，让多个客户端参与共享模型，而无需泄露数据隐私。由于本地数据的不同性，更新的客户端模型可能会过滤和分化，又称为客户遗传问题。在这篇论文中，我们提出了FedBug（联合学习底下逐层解冻），一个新的FL框架，可以有效地缓和客户遗传问题。FedBug在服务器端分发的每个全球轮次中，将客户端模型中的各层参数作为参考点进行跨客户同步。具体来说，在客户端上，FedBug会首先冻结整个模型，然后逐渐解冻层，从输入层到输出层。这个底下逐层解冻的方法让模型可以在新解冻的层上训练，将数据对应到一个共同的潜在空间，在这个潜在空间中，数据分隔的折冲线保持一致 across all clients。我们在一个新的FL设置中进行了理论分析，显示FedBug在较高的速度比FedAvg更快地趋向数据。通过了多种数据集、训练条件和网络架构，我们验证了FedBug的有效性。我们的贡献包括一个新的FL框架、理论分析和实验验证，显示了FedBug的广泛应用和可能性。
</details></li>
</ul>
<hr>
<h2 id="Space-Engage-Collaborative-Space-Supervision-for-Contrastive-based-Semi-Supervised-Semantic-Segmentation"><a href="#Space-Engage-Collaborative-Space-Supervision-for-Contrastive-based-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation"></a>Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09755">http://arxiv.org/abs/2307.09755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WangChangqi98/CSS">https://github.com/WangChangqi98/CSS</a></li>
<li>paper_authors: Changqi Wang, Haoyu Xie, Yuhui Yuan, Chong Fu, Xiangyu Yue</li>
<li>for: 提高semantic segmentation模型的 Robustness和精度，使用限量标注图像和大量无标注图像进行训练。</li>
<li>methods: 提出了一种基于像素级对准学习的方法，通过在很少标注图像下进行协同学习，从而提高模型的表征表示能力。</li>
<li>results: 对两个公共的benchmark进行测试，研究结果表明，与现有方法相比，本方法具有竞争性的性能。<details>
<summary>Abstract</summary>
Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation model with limited labeled images and a substantial volume of unlabeled images. To improve the robustness of representations, powerful methods introduce a pixel-wise contrastive learning approach in latent space (i.e., representation space) that aggregates the representations to their prototypes in a fully supervised manner. However, previous contrastive-based S4 methods merely rely on the supervision from the model's output (logits) in logit space during unlabeled training. In contrast, we utilize the outputs in both logit space and representation space to obtain supervision in a collaborative way. The supervision from two spaces plays two roles: 1) reduces the risk of over-fitting to incorrect semantic information in logits with the help of representations; 2) enhances the knowledge exchange between the two spaces. Furthermore, unlike previous approaches, we use the similarity between representations and prototypes as a new indicator to tilt training those under-performing representations and achieve a more efficient contrastive learning process. Results on two public benchmarks demonstrate the competitive performance of our method compared with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
半指导Semantic Segmentation（S4）目标是使用有限量的标注图像和大量的无标注图像来训练一个分割模型。为了提高表示的稳定性，我们引入了一种像素级对比学习方法，将表示 aggregate到其权重空间中的聚合权重。然而，之前的对比基于S4方法仅仅在无标注训练中使用模型的输出（logits）作为监督。与之不同，我们在两个空间中获得监督：1）降低因 incorrect semantic information在logits中过度适应的风险，通过使用表示；2）提高两个空间之间的知识交换。此外，与之前的方法不同，我们使用表示和聚合权重之间的相似性作为一个新的指标，以调整不优秀的表示并实现更高效的对比学习过程。对两个公共的标准测试集进行了比较，我们的方法与当前的状态OF-THE-ART方法相比，具有竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="Information-Retrieval-Meets-Large-Language-Models-A-Strategic-Report-from-Chinese-IR-Community"><a href="#Information-Retrieval-Meets-Large-Language-Models-A-Strategic-Report-from-Chinese-IR-Community" class="headerlink" title="Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community"></a>Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09751">http://arxiv.org/abs/2307.09751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, Shen Gao, Jiafeng Guo, Xiangnan He, Yanyan Lan, Chenliang Li, Yiqun Liu, Ziyu Lyu, Weizhi Ma, Jun Ma, Zhaochun Ren, Pengjie Ren, Zhiqiang Wang, Mingwen Wang, Ji-Rong Wen, Le Wu, Xin Xin, Jun Xu, Dawei Yin, Peng Zhang, Fan Zhang, Weinan Zhang, Min Zhang, Xiaofei Zhu</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在信息检索领域中的应用和潜力，并提出一种新的信息检索技术哲学。</li>
<li>methods: 本研究使用了工作坊的形式，即在2023年4月举行的中国信息检索社区策略性工作坊，以获得参与者的views和ideas。</li>
<li>results: 工作坊的结果表明，LLMs可以帮助改善信息检索的效果和体验，同时也存在一些 Computational Costs、信任性和道德问题等挑战。此外，提出了一种新的信息检索技术哲学，即“人-LLM-信息检索”技术哲学。<details>
<summary>Abstract</summary>
The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop's outcomes, including the rethinking of IR's core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.
</details>
<details>
<summary>摘要</summary>
研究领域信息检索（IR）在过去几年发展得非常 significatively，从传统搜索扩展到满足多样化用户信息需求。最近，大型自然语言模型（LLM）在文本理解、生成和知识推理方面表现出色，为IR研究开创了吸引人的新途径。LLM不仅实现生成检索，还提供改进的用户理解、模型评估和用户-系统互动解决方案。此外，IR模型、LLM和人类之间的协同关系形成了一个更强大的技术哲学，用于信息检索。IR模型在实时提供有关信息，LLM增加内存知识，人类扮演搜索者和评估者的中心角色，以确保信息服务的可靠性。然而，计算成本、可靠性、领域特定限制以及伦理考虑等问题仍然存在。为了全面讨论LLM对IR研究的转变性影响，中国IR社区在2023年4月举行了一场策略性工作坊，并获得了有价值的成果。本文总结了工作坊的结果，包括重新思考IR核心价值观、LLM和IR之间的互进式关系、提议一种新的IR技术哲学，以及开放的挑战。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-conversational-quality-in-language-learning-chatbots-An-evaluation-of-GPT4-for-ASR-error-correction"><a href="#Enhancing-conversational-quality-in-language-learning-chatbots-An-evaluation-of-GPT4-for-ASR-error-correction" class="headerlink" title="Enhancing conversational quality in language learning chatbots: An evaluation of GPT4 for ASR error correction"></a>Enhancing conversational quality in language learning chatbots: An evaluation of GPT4 for ASR error correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09744">http://arxiv.org/abs/2307.09744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Mai, Julie Carson-Berndsen</li>
<li>for: 本研究旨在探讨使用GPT4进行对话会话中的自然语言处理技术，以提高语言学习者的语言技能。</li>
<li>methods: 本研究使用GPT4进行对话会话中的自动听写错误纠正，并使用semantic textual similarity (STS)和next response sensibility (NRS) metrics来评估对话质量。</li>
<li>results: 研究发现，使用GPT4进行错误纠正后，对话质量得到提高，尽管word-error-rate (WER)增加。GPT4还超过了标准错误纠正方法，无需培训数据。<details>
<summary>Abstract</summary>
The integration of natural language processing (NLP) technologies into educational applications has shown promising results, particularly in the language learning domain. Recently, many spoken open-domain chatbots have been used as speaking partners, helping language learners improve their language skills. However, one of the significant challenges is the high word-error-rate (WER) when recognizing non-native/non-fluent speech, which interrupts conversation flow and leads to disappointment for learners. This paper explores the use of GPT4 for ASR error correction in conversational settings. In addition to WER, we propose to use semantic textual similarity (STS) and next response sensibility (NRS) metrics to evaluate the impact of error correction models on the quality of the conversation. We find that transcriptions corrected by GPT4 lead to higher conversation quality, despite an increase in WER. GPT4 also outperforms standard error correction methods without the need for in-domain training data.
</details>
<details>
<summary>摘要</summary>
<SYS>< translator: Google translate</translator></SYS>文本 интеграция到教育应用程序中已经显示出了有望的结果，特别是在语言学习领域。最近，许多开放频道的口语聊天机器人被用作对话伙伴，帮助语言学习者提高语言技能。然而，一个重要的挑战是非Native/非流利语音识别器的高字幕错误率（WER），这会中断对话流程并使学习者失望。这篇论文探讨使用GPT4进行ASR错误修正的方法。除了WER，我们还提出使用Semantic Textual Similarity（STS）和Next Response Sensibility（NRS）度量来评估错误修正模型对对话质量的影响。我们发现GPT4修正的 trascriptions 会导致更高的对话质量，尽管WER增加。GPT4 还超越了标准的错误修正方法，无需培训数据。
</details></li>
</ul>
<hr>
<h2 id="Absolutist-AI"><a href="#Absolutist-AI" class="headerlink" title="Absolutist AI"></a>Absolutist AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10315">http://arxiv.org/abs/2307.10315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mitchell Barrington</li>
<li>for: 该论文提出了训练AI系统 WITH 绝对约束（不允许某些行为无论其价值如何）可能会取得很大进步在AI安全问题上。</li>
<li>methods: 该论文提供了一个保障AI系统不会出现最坏情况的固定轮廓，并使AI系统更容易被更正。</li>
<li>results: 该论文证明了绝对约束可以防止AI系统导致灾难性结果，并且不会让AI系统变得无理性。<details>
<summary>Abstract</summary>
This paper argues that training AI systems with absolute constraints -- which forbid certain acts irrespective of the amount of value they might produce -- may make considerable progress on many AI safety problems in principle. First, it provides a guardrail for avoiding the very worst outcomes of misalignment. Second, it could prevent AIs from causing catastrophes for the sake of very valuable consequences, such as replacing humans with a much larger number of beings living at a higher welfare level. Third, it makes systems more corrigible, allowing creators to make corrective interventions in them, such as altering their objective functions or shutting them down. And fourth, it helps systems explore their environment more safely by prohibiting them from exploring especially dangerous acts. I offer a decision-theoretic formalization of an absolute constraints, improving on existing models in the literature, and use this model to prove some results about the training and behavior of absolutist AIs. I conclude by showing that, although absolutist AIs will not maximize expected value, they will not be susceptible to behave irrationally, and they will not (contra coherence arguments) face environmental pressure to become expected-value maximizers.
</details>
<details>
<summary>摘要</summary>
The paper offers a decision-theoretic formalization of absolute constraints, improving on existing models in the literature, and uses this model to prove results about the training and behavior of absolutist AIs. The author concludes that, although absolutist AIs will not maximize expected value, they will not behave irrationally, and they will not face environmental pressure to become expected-value maximizers.
</details></li>
</ul>
<hr>
<h2 id="Multi-Grained-Multimodal-Interaction-Network-for-Entity-Linking"><a href="#Multi-Grained-Multimodal-Interaction-Network-for-Entity-Linking" class="headerlink" title="Multi-Grained Multimodal Interaction Network for Entity Linking"></a>Multi-Grained Multimodal Interaction Network for Entity Linking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09721">http://arxiv.org/abs/2307.09721</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pengfei-luo/mimic">https://github.com/pengfei-luo/mimic</a></li>
<li>paper_authors: Pengfei Luo, Tong Xu, Shiwei Wu, Chen Zhu, Linli Xu, Enhong Chen</li>
<li>for: 解决多模态 mentions 映射到多模态知识图的ambiguity问题</li>
<li>methods: 提出了一种基于Multi-GraIned Multimodal InteraCtion Network $\textbf{(MIMIC)}$框架的解决方案，包括文本&#x2F;视觉编码器、三种交互单元和单元一致性目标函数</li>
<li>results: 实验结果表明，我们的解决方案在三个公共测试集上表现出色，并且ablation study verify了设计的模块的效果<details>
<summary>Abstract</summary>
Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years. Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication. Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance. To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network $\textbf{(MIMIC)}$ framework for solving the MEL task. Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features. Then, to derive the similarity matching score for each mention-entity pair, we device three interaction units to comprehensively explore the intra-modal interaction and inter-modal fusion among features of entities and mentions. In particular, three modules, namely the Text-based Global-Local interaction Unit (TGLU), Vision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based interaction Unit (CMFU) are designed to capture and integrate the fine-grained representation lying in abbreviated text and implicit visual cues. Afterwards, we introduce a unit-consistency objective function via contrastive learning to avoid inconsistency and model degradation. Experimental results on three public benchmark datasets demonstrate that our solution outperforms various state-of-the-art baselines, and ablation studies verify the effectiveness of designed modules.
</details>
<details>
<summary>摘要</summary>
Multimodal实体链接（MEL）任务，目标是将不确定的提及映射到多模态知识图，在过去几年内吸引了广泛的关注。虽然大量努力被投入到多模态之间的补做效应的探索中，但它们可能会忽略短文本上的缩写表述和隐藏的视觉指示。worse, noisy数据可能会在学习过程中导致不一致的多模态，从而严重降低性能。为解决上述问题，在这篇论文中，我们提出了一种新的多模态交互网络（MIMIC）框架，用于解决MEL任务。具体来说，提及和实体的共同输入首先被文本/视觉编码器分别解码，以提取全局描述特征和局部细节特征。然后，为每个提及实体对应的匹配分数，我们设计了三种交互单元，用于全面探索实体和提及之间的内模态交互和跨模态融合。具体来说，我们设计了文本基于的全球-本地交互单元（TGLU）、视觉基于的双向交互单元（VDLU）和交叉模态融合基于的交互单元（CMFU），以捕捉和融合缩写文本中的细节表述和隐藏的视觉指示。接着，我们引入了一个单元一致性目标函数，通过对比学习来避免不一致和模型衰竭。实验结果表明，我们的解决方案在三个公共 benchmark 数据集上表现出色，并且ablation 研究证明了我们设计的模块的有效性。
</details></li>
</ul>
<hr>
<h2 id="Two-Tales-of-Platoon-Intelligence-for-Autonomous-Mobility-Control-Enabling-Deep-Learning-Recipes"><a href="#Two-Tales-of-Platoon-Intelligence-for-Autonomous-Mobility-Control-Enabling-Deep-Learning-Recipes" class="headerlink" title="Two Tales of Platoon Intelligence for Autonomous Mobility Control: Enabling Deep Learning Recipes"></a>Two Tales of Platoon Intelligence for Autonomous Mobility Control: Enabling Deep Learning Recipes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09711">http://arxiv.org/abs/2307.09711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soohyun Park, Haemin Lee, Chanyoung Park, Soyi Jung, Minseok Choi, Joongheon Kim</li>
<li>for: 解决自动驾驶汽车和无人机的自主行动控制和资源管理问题</li>
<li>methods: 使用多代理人学习（MARL）和神经Myerson拍卖</li>
<li>results: 实现多代理人的分布式行动，以及保证多代理人之间的信任性和优化高动态系统的收益Note: The above text is in Simplified Chinese.<details>
<summary>Abstract</summary>
This paper presents the deep learning-based recent achievements to resolve the problem of autonomous mobility control and efficient resource management of autonomous vehicles and UAVs, i.e., (i) multi-agent reinforcement learning (MARL), and (ii) neural Myerson auction. Representatively, communication network (CommNet), which is one of the most popular MARL algorithms, is introduced to enable multiple agents to take actions in a distributed manner for their shared goals by training all agents' states and actions in a single neural network. Moreover, the neural Myerson auction guarantees trustfulness among multiple agents as well as achieves the optimal revenue of highly dynamic systems. Therefore, we survey the recent studies on autonomous mobility control based on MARL and neural Myerson auction. Furthermore, we emphasize that integration of MARL and neural Myerson auction is expected to be critical for efficient and trustful autonomous mobility services.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Multi-agent reinforcement learning (MARL): This approach enables multiple agents to learn and take actions in a distributed manner to achieve shared goals.2. Neural Myerson auction: This method guarantees trustworthiness among multiple agents and achieves optimal revenue in highly dynamic systems.The paper surveys recent studies on autonomous mobility control based on MARL and neural Myerson auction, and highlights the importance of integrating these two approaches for efficient and trustworthy autonomous mobility services.In Simplified Chinese:这篇论文介绍了深度学习技术在自动驾驶车载和无人机中的应用，以解决自动驾驶车载和无人机的资源管理和自动驾驶控制问题。这些方法包括：1. 多代理人奖励学习（MARL）：这种方法可以让多个代理人在分布式环境中学习和行动，以实现共同目标。2. 神经美森拍卖：这种方法保证多个代理人之间的信任性，并在高度动态系统中实现最优的收益。这篇论文对自动驾驶车载基于MARL和神经美森拍卖的最新研究进行了综述，并强调将这两种方法集成起来将是efficient和可信的自动驾驶服务的关键。</details></li>
</ol>
<hr>
<h2 id="RaTE-a-Reproducible-automatic-Taxonomy-Evaluation-by-Filling-the-Gap"><a href="#RaTE-a-Reproducible-automatic-Taxonomy-Evaluation-by-Filling-the-Gap" class="headerlink" title="RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap"></a>RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09706">http://arxiv.org/abs/2307.09706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cestlucas/rate">https://github.com/cestlucas/rate</a></li>
<li>paper_authors: Tianjian Gao, Phillipe Langlais</li>
<li>for:  automatic taxonomy construction (ATC) algorithm evaluation</li>
<li>methods:  using a large pre-trained language model for label-free taxonomy scoring</li>
<li>results: 1) RaTE correlates well with human judgments, 2) artificially degrading a taxonomy leads to decreasing RaTE score.<details>
<summary>Abstract</summary>
Taxonomies are an essential knowledge representation, yet most studies on automatic taxonomy construction (ATC) resort to manual evaluation to score proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just as important as taxonomy construction. We propose RaTE, an automatic label-free taxonomy scoring procedure, which relies on a large pre-trained language model. We apply our evaluation procedure to three state-of-the-art ATC algorithms with which we built seven taxonomies from the Yelp domain, and show that 1) RaTE correlates well with human judgments and 2) artificially degrading a taxonomy leads to decreasing RaTE score.
</details>
<details>
<summary>摘要</summary>
taxonomies是知识表示的重要组成部分，然而大多数自动taxonomy建构（ATC）研究仍然采用手动评估提议的算法。我们认为自动taxonomy评估（ATE）也非常重要。我们提出了一种名为RaTE的自动无标签分类方法，它基于大型预训练语言模型。我们对三种state-of-the-art ATC算法进行了七个Yelp领域的taxonomy建构，并显示了以下两点：1）RaTE与人类评价高度相关，2） искусственно降低一个分类会导致RaTE分数下降。
</details></li>
</ul>
<hr>
<h2 id="STRAPPER-Preference-based-Reinforcement-Learning-via-Self-training-Augmentation-and-Peer-Regularization"><a href="#STRAPPER-Preference-based-Reinforcement-Learning-via-Self-training-Augmentation-and-Peer-Regularization" class="headerlink" title="STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization"></a>STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09692">http://arxiv.org/abs/2307.09692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rll-research/bpref">https://github.com/rll-research/bpref</a></li>
<li>paper_authors: Yachen Kang, Li He, Jinxin Liu, Zifeng Zhuang, Donglin Wang<br>for:This paper aims to learn a complex reward function with binary human preference using preference-based reinforcement learning (PbRL).methods:The paper proposes a self-training method with proposed peer regularization to overcome the issue of similarity trap in PbRL, which improperly enhances the consistency possibility of the model’s predictions between segment pairs and reduces the confidence in reward learning.results:The proposed approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization, demonstrating its effectiveness in large-scale applications.Here is the Chinese translation of the three key points:for:这篇论文目标是使用 preference-based reinforcement learning (PbRL) 学习复杂的奖励函数和人类偏好。methods:论文提议一种自适应方法，并提出了一种 peer regularization 来解决 PbRL 中的相似陷阱问题，该问题会使模型在类比对中增强预测的一致性，从而降低奖励学习的信任度。results:提议的方法可以使用不同的半超vised альтернатив和 peer regularization 学习多种 lokomotion 和机器人 manipulation 行为， demonstrating its effectiveness in large-scale applications。<details>
<summary>Abstract</summary>
Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's predictions between segment pairs, and thus reduces the confidence in reward learning, since the augmented distribution does not match with the original one in PbRL. To overcome such issue, we present a self-training method along with our proposed peer regularization, which penalizes the reward model memorizing uninformative labels and acquires confident predictions. Empirically, we demonstrate that our approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization.
</details>
<details>
<summary>摘要</summary>
preference-based reinforcement learning (PbRL) 承诺学习复杂的奖励函数使用二进制人类偏好。然而，这种人loop形式需要较大的人类努力来分配偏好标签对segment对， limiting its large-scale applications。 recent approaches have tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. In addition, consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exists a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possibility of the model's predictions between segment pairs, and thus reduces the confidence in reward learning, since the augmented distribution does not match with the original one in PbRL. To overcome such issue, we present a self-training method along with our proposed peer regularization, which penalizes the reward model memorizing uninformative labels and acquires confident predictions. Empirically, we demonstrate that our approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization.
</details></li>
</ul>
<hr>
<h2 id="Amazon-M2-A-Multilingual-Multi-locale-Shopping-Session-Dataset-for-Recommendation-and-Text-Generation"><a href="#Amazon-M2-A-Multilingual-Multi-locale-Shopping-Session-Dataset-for-Recommendation-and-Text-Generation" class="headerlink" title="Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation"></a>Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09688">http://arxiv.org/abs/2307.09688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen, Haoyu Han, Hanqing Lu, Zhengyang Wang, Ruirui Li, Zhen Li, Monica Xiao Cheng, Rahul Goutam, Haiyang Zhang, Karthik Subbian, Suhang Wang, Yizhou Sun, Jiliang Tang, Bing Yin, Xianfeng Tang</li>
<li>for: This paper is written for those who are interested in improving customer shopping experiences and personalized recommendations in e-commerce.</li>
<li>methods: The paper uses a multilingual session dataset, named Amazon-M2, which consists of millions of user sessions from six different locales, to benchmark various algorithms and tasks related to next-product recommendation, next-product recommendation with domain shifts, and next-product title generation.</li>
<li>results: The paper introduces three tasks and benchmarks a range of algorithms on the proposed dataset, drawing new insights for further research and practice. Additionally, the paper hosts a competition in the KDD CUP 2023 and attracts thousands of users and submissions.<details>
<summary>Abstract</summary>
Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks. To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation. With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice. In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 and have attracted thousands of users and submissions. The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.
</details>
<details>
<summary>摘要</summary>
模拟客户购买意图是电商的关键任务，直接影响用户体验和参与度。因此，正确理解客户偏好是提供个性化推荐的关键。会话基于推荐，利用客户会话数据预测他们下一次交互，在现有会话数据上有限制，无法全面捕捉用户行为和偏好谱系。为了弥补这一差距，我们提出了亚马逊多语言多地区购物会话数据集（Amazon-M2）。这是首个多语言数据集，包含来自六个不同的地区，主要语言包括英文、德语、日语、法语、意大利语和西班牙语。这个数据集可以帮助我们提高个性化和用户偏好的理解，从而对现有任务和新任务产生新的发现和挑战。为了评估数据集的潜力，我们在本文中引入了三个任务：（1）下一个产品推荐（2）下一个产品推荐域转移（3）下一个产品标题生成。通过这三个任务，我们对我们提posed的数据集和任务进行了 benchmark，从而获得了新的发现和实践。此外，我们还在KDD CUP 2023中举办了一场竞赛，并吸引了数千名用户和提交。赢家的解决方案和相关的工作坊可以在我们的官方网站https://kddcup23.github.io/上获得。
</details></li>
</ul>
<hr>
<h2 id="PubMed-and-Beyond-Recent-Advances-and-Best-Practices-in-Biomedical-Literature-Search"><a href="#PubMed-and-Beyond-Recent-Advances-and-Best-Practices-in-Biomedical-Literature-Search" class="headerlink" title="PubMed and Beyond: Recent Advances and Best Practices in Biomedical Literature Search"></a>PubMed and Beyond: Recent Advances and Best Practices in Biomedical Literature Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09683">http://arxiv.org/abs/2307.09683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiao Jin, Robert Leaman, Zhiyong Lu</li>
<li>for: This paper aims to help readers efficiently fulfill their information needs in biomedicine by providing a comprehensive survey of literature search tools tailored to specific information needs.</li>
<li>methods: The paper examines widely used PubMed search engine and describes literature search tools catering to five specific information needs, including identifying high-quality clinical research, retrieving gene-related information, searching by meaning, locating related articles, and mining literature to discover associations between concepts.</li>
<li>results: The paper provides a comprehensive view of biomedical literature search functionalities with 36 publicly available tools, and offers practical considerations and best practices for choosing and using these tools. Additionally, the paper provides a perspective on the future of literature search engines considering recent breakthroughs in large language models such as ChatGPT.<details>
<summary>Abstract</summary>
Biomedical research yields a wealth of information, much of which is only accessible through the literature. Consequently, literature search is an essential tool for building on prior knowledge in clinical and biomedical research. Although recent improvements in artificial intelligence have expanded functionality beyond keyword-based search, these advances may be unfamiliar to clinicians and researchers. In response, we present a survey of literature search tools tailored to both general and specific information needs in biomedicine, with the objective of helping readers efficiently fulfill their information needs. We first examine the widely used PubMed search engine, discussing recent improvements and continued challenges. We then describe literature search tools catering to five specific information needs: 1. Identifying high-quality clinical research for evidence-based medicine. 2. Retrieving gene-related information for precision medicine and genomics. 3. Searching by meaning, including natural language questions. 4. Locating related articles with literature recommendation. 5. Mining literature to discover associations between concepts such as diseases and genetic variants. Additionally, we cover practical considerations and best practices for choosing and using these tools. Finally, we provide a perspective on the future of literature search engines, considering recent breakthroughs in large language models such as ChatGPT. In summary, our survey provides a comprehensive view of biomedical literature search functionalities with 36 publicly available tools.
</details>
<details>
<summary>摘要</summary>
生物医学研究提供了大量信息，其中许多信息只能通过文献来获取。因此，文献搜索成为了临床和生物医学研究中不可或缺的工具。虽然最近的人工智能技术已经扩展了搜索功能之 beyond 键盘基本搜索，但这些进步可能对临床和研究人员来说还是不熟悉的。为了帮助读者快速满足他们的信息需求，我们提供了一份关于文献搜索工具的讲话，涵盖了以下五个特定信息需求：1. 找到高质量的临床研究，以便实施基于证据的医疗。2. 搜索与基因相关的信息，以满足精准医学和基因学的需求。3. 通过自然语言问题进行搜索。4. 找到相关的文献，并获得文献推荐。5. 挖掘文献中的概念之间的关系，如疾病和基因变异之间的关系。此外，我们还讨论了选择和使用这些工具的实际考虑和最佳实践。最后，我们提供了对文献搜索引擎的未来展望，考虑到最近的大语言模型如ChatGPT的突破。总之，我们的讲话提供了36种公共可用的生物医学文献搜索功能的全面视图。
</details></li>
</ul>
<hr>
<h2 id="What’s-meant-by-explainable-model-A-Scoping-Review"><a href="#What’s-meant-by-explainable-model-A-Scoping-Review" class="headerlink" title="What’s meant by explainable model: A Scoping Review"></a>What’s meant by explainable model: A Scoping Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09673">http://arxiv.org/abs/2307.09673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mallika Mainali, Rosina O Weber</li>
<li>for: 本研究探讨了使用人工智能模型并采用后期解释方法来生成解释的论文，以及作者是否在使用这些方法时进行评估。</li>
<li>methods: 本研究使用了探索性评估方法来 investigate 论文是否进行了评估，并发现了81%的应用论文中的作者在使用解释模型时没有进行任何评估。</li>
<li>results: 本研究发现，81%的应用论文中的作者在使用解释模型时没有进行任何评估，这表明了在使用解释模型时，作者们往往假设它们已经足够地被评估。<details>
<summary>Abstract</summary>
We often see the term explainable in the titles of papers that describe applications based on artificial intelligence (AI). However, the literature in explainable artificial intelligence (XAI) indicates that explanations in XAI are application- and domain-specific, hence requiring evaluation whenever they are employed to explain a model that makes decisions for a specific application problem. Additionally, the literature reveals that the performance of post-hoc methods, particularly feature attribution methods, varies substantially hinting that they do not represent a solution to AI explainability. Therefore, when using XAI methods, the quality and suitability of their information outputs should be evaluated within the specific application. For these reasons, we used a scoping review methodology to investigate papers that apply AI models and adopt methods to generate post-hoc explanations while referring to said models as explainable. This paper investigates whether the term explainable model is adopted by authors under the assumption that incorporating a post-hoc XAI method suffices to characterize a model as explainable. To inspect this problem, our review analyzes whether these papers conducted evaluations. We found that 81% of the application papers that refer to their approaches as an explainable model do not conduct any form of evaluation on the XAI method they used.
</details>
<details>
<summary>摘要</summary>
我们经常看到“可解释的”（Explainable）一词在人工智能（AI）应用中的标题中出现。然而，XAI文献表明，在不同应用和领域中，解释都是应用和领域特定的，因此需要在特定应用中进行评估。此外，文献还表明，后期方法特别是特征归属方法的性能差异很大，这意味着它们不是AI可解释的解决方案。因此，当使用XAI方法时，需要评估其输出信息的质量和适用性。为了解决这个问题，我们采用了一种批量评估方法来研究那些使用AI模型并采用后期解释方法的论文。我们发现，81%的应用论文中的作者在使用XAI方法时没有进行任何评估。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Unified-Agent-with-Foundation-Models"><a href="#Towards-A-Unified-Agent-with-Foundation-Models" class="headerlink" title="Towards A Unified Agent with Foundation Models"></a>Towards A Unified Agent with Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09668">http://arxiv.org/abs/2307.09668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, Martin Riedmiller</li>
<li>for: 这个论文目的是怎样使用语言模型和视觉语言模型来提高人工智能机器人的探索和学习能力？</li>
<li>methods: 这个论文使用了语言为核心的理解和计划方法，以解决一些基本的人工智能探索和学习挑战，如有限的奖励、重用经验数据、计划技能和从观察中学习。</li>
<li>results: 在一个具有笨重奖励的机器人拼堆环境中，这种方法能够提高探索效率和重用经验数据的能力，并且可以在不同的任务上重用已学会的技能。<details>
<summary>Abstract</summary>
Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language model Reinforcement Learning agent 能力 investigated  embed  scene understanding planning-like behavior  text form  efficient exploration  reusing experience data  scheduling skills  learning from observations  sparse-reward simulated robotic manipulation environment  substantial performance improvements  exploration efficiency  reuse learned skills  solve novel tasks  imitate human expertsLanguage Models 和 Vision Language Models 最近Displayed unprecedented capabilities 理解人类意图 reasoning  scene understanding 和 planning-like behavior 在文本形式中，其中许多。在这项工作中，我们调查了如何在Reinforcement Learning（RL）代理人中嵌入和利用这些能力。我们设计了一个框架，用语言作为核心的理解工具，探索如何使得代理人可以解决RL中的一系列基本挑战，例如高效探索、重用经验数据、培训技能和从观察中学习，这些传统需要独立的、垂直设计的算法。我们在一个具有缺乏奖励的 simulate 机器人搬运环境中测试了我们的方法，where a robot needs to stack a set of objects。我们示出了较好的性能提升，包括探索效率和从停挂数据中重用经验，并示出了如何重用已学到的技能来解决新任务或模仿人类专家的视频。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Anticipating-Technical-Expertise-and-Capability-Evolution-in-Research-Communities-using-Dynamic-Graph-Transformers"><a href="#Anticipating-Technical-Expertise-and-Capability-Evolution-in-Research-Communities-using-Dynamic-Graph-Transformers" class="headerlink" title="Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers"></a>Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09665">http://arxiv.org/abs/2307.09665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameera Horawalavithana, Ellyn Ayton, Anastasiya Usenko, Robin Cosbey, Svitlana Volkova<br>for: 这种研究旨在预测技术专业和能力发展趋势，以便为国际和全球安全做出更好的准备。特别是在安全关键领域 like 核不扩散 (NN) 和快速发展的领域 like 人工智能 (AI) 中，预测技术专业和能力发展变得非常重要。methods: 该研究使用了传统的统计关系学学习方法（如链接预测在合作网络中），并将问题转化为动态多态图表示。研究人员还开发了一种新的能量预测技术，以便预测科学家和机构之间的合作模式、作者行为和技术能力的发展。results: 研究人员使用的动态图变换（DGT）神经网络模型，可以在AI和NN两个领域中预测合作、合作伙伴和技术能力的发展。DGT模型的性能比最佳静止图模型提高30-80%。研究人员还发现，DGT模型在未seen节点出现在测试数据中时，对 inductive 任务的性能有所提高。具体来说，模型可以准确预测已有的科学家和新手科学家之间的合作关系在 AI 领域中。<details>
<summary>Abstract</summary>
The ability to anticipate technical expertise and capability evolution trends globally is essential for national and global security, especially in safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging fields like artificial intelligence (AI). In this work, we extend traditional statistical relational learning approaches (e.g., link prediction in collaboration networks) and formulate a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations. We develop novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities (e.g., scientist and institution levels) in two distinct research fields. We implement a dynamic graph transformer (DGT) neural architecture, which pushes the state-of-the-art graph neural network models by (a) forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b) relying on both discrete -- and continuous -- time inputs. We demonstrate that our DGT models predict collaboration, partnership, and expertise patterns with 0.26, 0.73, and 0.53 mean reciprocal rank values for AI and 0.48, 0.93, and 0.22 for NN domains. DGT model performance exceeds the best-performing static graph baseline models by 30-80% across AI and NN domains. Our findings demonstrate that DGT models boost inductive task performance, when previously unseen nodes appear in the test data, for the domains with emerging collaboration patterns (e.g., AI). Specifically, models accurately predict which established scientists will collaborate with early career scientists and vice-versa in the AI domain.
</details>
<details>
<summary>摘要</summary>
能够预测技术培养和能力演化趋势是国家和全球安全的关键，特别是在安全敏感领域如核不扩散（NN）和快速出现的领域如人工智能（AI）。在这种工作中，我们扩展了传统的统计关系学学习方法（例如链接预测在合作网络中），并将问题定义为预测技术培养和能力演化使用动态多类图表示。我们开发了新的能力来预测合作模式、作者行为和技术能力演化的不同细腻度（例如科学家和机构层次）在两个不同的研究领域。我们实现了动态图变换（DGT）神经网络模型，该模型超过了状态可能的图像基eline模型的性能，通过（a）预测多样性（而不是同质）的节点和边，以及（b）使用时间输入。我们 demonstate that DGT models predict collaboration, partnership, and expertise patterns with mean reciprocal rank values of 0.26, 0.73, and 0.53 for AI and 0.48, 0.93, and 0.22 for NN domains. DGT model performance exceeds the best-performing static graph baseline models by 30-80% across AI and NN domains. Our findings demonstrate that DGT models boost inductive task performance, when previously unseen nodes appear in the test data, for the domains with emerging collaboration patterns (e.g., AI). Specifically, models accurately predict which established scientists will collaborate with early career scientists and vice-versa in the AI domain.
</details></li>
</ul>
<hr>
<h2 id="HAT-CL-A-Hard-Attention-to-the-Task-PyTorch-Library-for-Continual-Learning"><a href="#HAT-CL-A-Hard-Attention-to-the-Task-PyTorch-Library-for-Continual-Learning" class="headerlink" title="HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning"></a>HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09653">http://arxiv.org/abs/2307.09653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xduan7/hat-cl">https://github.com/xduan7/hat-cl</a></li>
<li>paper_authors: Xiaotian Duan</li>
<li>for: 这篇论文旨在解决 continual learning 中的忘记现象，即神经网络学习新任务时会忘记之前的知识。</li>
<li>methods: 这篇论文提出了一种名为 HAT 机制来 Mitigate 这个问题，但实际应用中存在可用性和兼容性问题，以及现有网络重用支持不足。</li>
<li>results: 本文引入了一种更User-Friendly、PyTorch-Compatible的 HAT 机制，名为 HAT-CL。 HAT-CL 不仅自动 manipulate 梯度，还可以将 PyTorch 模块转换为 HAT 模块，并提供了一个完整的模块集，可以轻松地整合到现有的架构中。 In addition, the authors introduce novel mask manipulation techniques for HAT, which have consistently shown improvements across various experiments.<details>
<summary>Abstract</summary>
Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT, which have consistently shown improvements across various experiments. Our work paves the way for a broader application of the HAT mechanism, opening up new possibilities in continual learning across diverse models and applications.
</details>
<details>
<summary>摘要</summary>
灾难性忘却（Catastrophic Forgetting）是机器学习中的一个问题，它表示神经网络在学习新任务时会遗传已经获得的知识。这个问题对 kontinual learning（持续学习）领域 pose a significant challenge。在这篇论文中，我们介绍了一个名为 HAT-CL（Hard-Attention-to-the-Task）的用户友好的、PyTorch-相容的重新设计。HAT-CL不仅自动调整Gradient的方法，而且可以将PyTorch模组转换为HAT模组，并且提供了一个涵盖了多个模组的套件，可以与现有的架构集成。此外，HAT-CL还提供了一些Ready-to-use HAT网络，与TIMM库相关联系。除了重新设计和重新实现HAT之外，我们还引入了一些新的面积调整技术，这些技术在不同的实验中均展示了改善。我们的工作开辟了HAT机制的应用，将在不同的模型和应用中实现持续学习的新可能性。
</details></li>
</ul>
<hr>
<h2 id="VISER-A-Tractable-Solution-Concept-for-Games-with-Information-Asymmetry"><a href="#VISER-A-Tractable-Solution-Concept-for-Games-with-Information-Asymmetry" class="headerlink" title="VISER: A Tractable Solution Concept for Games with Information Asymmetry"></a>VISER: A Tractable Solution Concept for Games with Information Asymmetry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09652">http://arxiv.org/abs/2307.09652</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jermcmahan/viser">https://github.com/jermcmahan/viser</a></li>
<li>paper_authors: Jeremy McMahan, Young Wu, Yudong Chen, Xiaojin Zhu, Qiaomin Xie</li>
<li>for: 这篇论文是为了解决受信息不均衡影响的现实世界游戏问题，例如安全游戏和多代理人学习。</li>
<li>methods: 这篇论文提出了一种新的解决方案，即VISER（受害者安全，攻击者最佳回应）。VISER使得外部观察者可以预测这些游戏的结果。</li>
<li>results: 作者表示，每个玩家的VISER策略都可以独立计算，并且可以用线性规划（LP）来计算。此外，作者还扩展了VISER到其Markov游戏对应的Markov-perfect版本，可以使用一系列LP来解决。<details>
<summary>Abstract</summary>
Many real-world games suffer from information asymmetry: one player is only aware of their own payoffs while the other player has the full game information. Examples include the critical domain of security games and adversarial multi-agent reinforcement learning. Information asymmetry renders traditional solution concepts such as Strong Stackelberg Equilibrium (SSE) and Robust-Optimization Equilibrium (ROE) inoperative. We propose a novel solution concept called VISER (Victim Is Secure, Exploiter best-Responds). VISER enables an external observer to predict the outcome of such games. In particular, for security applications, VISER allows the victim to better defend itself while characterizing the most damaging attacks available to the attacker. We show that each player's VISER strategy can be computed independently in polynomial time using linear programming (LP). We also extend VISER to its Markov-perfect counterpart for Markov games, which can be solved efficiently using a series of LPs.
</details>
<details>
<summary>摘要</summary>
Many real-world games suffer from information asymmetry: one player only knows their own payoffs, while the other player has full game information. Examples include security games and adversarial multi-agent reinforcement learning. Information asymmetry makes traditional solution concepts like Strong Stackelberg Equilibrium (SSE) and Robust-Optimization Equilibrium (ROE) inoperative. We propose a novel solution concept called VISER (Victim Is Secure, Exploiter best-Responds). VISER enables an external observer to predict the outcome of such games. In particular, for security applications, VISER allows the victim to better defend itself while characterizing the most damaging attacks available to the attacker. We show that each player's VISER strategy can be computed independently in polynomial time using linear programming (LP). We also extend VISER to its Markov-perfect counterpart for Markov games, which can be solved efficiently using a series of LPs.Here's the translation in Traditional Chinese:Many real-world games suffer from information asymmetry: one player only knows their own payoffs, while the other player has full game information. Examples include security games and adversarial multi-agent reinforcement learning. Information asymmetry makes traditional solution concepts like Strong Stackelberg Equilibrium (SSE) and Robust-Optimization Equilibrium (ROE) inoperative. We propose a novel solution concept called VISER (Victim Is Secure, Exploiter best-Responds). VISER enables an external observer to predict the outcome of such games. In particular, for security applications, VISER allows the victim to better defend itself while characterizing the most damaging attacks available to the attacker. We show that each player's VISER strategy can be computed independently in polynomial time using linear programming (LP). We also extend VISER to its Markov-perfect counterpart for Markov games, which can be solved efficiently using a series of LPs.
</details></li>
</ul>
<hr>
<h2 id="With-Flying-Colors-Predicting-Community-Success-in-Large-scale-Collaborative-Campaigns"><a href="#With-Flying-Colors-Predicting-Community-Success-in-Large-scale-Collaborative-Campaigns" class="headerlink" title="With Flying Colors: Predicting Community Success in Large-scale Collaborative Campaigns"></a>With Flying Colors: Predicting Community Success in Large-scale Collaborative Campaigns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09650">http://arxiv.org/abs/2307.09650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abraham Israeli, Oren Tsur</li>
<li>For: This paper aims to study the effectiveness of online communities in promoting their agenda, specifically in the context of Reddit’s r&#x2F;place.* Methods: The paper uses a novel task of predicting the success level of online communities in Reddit’s r&#x2F;place, and experiments with several hybrid models combining various types of features.* Results: The models significantly outperform all baseline models over all definitions of “success level”, and the results provide insights into the factors that contribute to the success of coordinated campaigns.Here are the three points in Simplified Chinese:* For: 这篇论文目的是研究在Reddit的r&#x2F;place上线上社区的效果，具体来说是研究社区的合作能力和成果。* Methods: 这篇论文使用了一个新的任务，即预测Reddit的r&#x2F;place上线上社区的成功水平，并试用了多种混合模型。* Results: 这些模型在所有基础模型上都显著地超越了，并且提供了社区协作成功的因素分析。<details>
<summary>Abstract</summary>
Online communities develop unique characteristics, establish social norms, and exhibit distinct dynamics among their members. Activity in online communities often results in concrete ``off-line'' actions with a broad societal impact (e.g., political street protests and norms related to sexual misconduct). While community dynamics, information diffusion, and online collaborations have been widely studied in the past two decades, quantitative studies that measure the effectiveness of online communities in promoting their agenda are scarce. In this work, we study the correspondence between the effectiveness of a community, measured by its success level in a competitive online campaign, and the underlying dynamics between its members. To this end, we define a novel task: predicting the success level of online communities in Reddit's r/place - a large-scale distributed experiment that required collaboration between community members. We consider an array of definitions for success level; each is geared toward different aspects of collaborative achievement. We experiment with several hybrid models, combining various types of features. Our models significantly outperform all baseline models over all definitions of `success level'. Analysis of the results and the factors that contribute to the success of coordinated campaigns can provide a better understanding of the resilience or the vulnerability of communities to online social threats such as election interference or anti-science trends. We make all data used for this study publicly available for further research.
</details>
<details>
<summary>摘要</summary>
在线社区发展独特特征，成社会规范，展现特殊的动态。活动在线社区常引起“Off-line”行动，影响广泛社会（如政治街头抗议和性骚扰规范）。 despite community dynamics, information diffusion, and online collaborations have been widely studied in the past two decades, quantitative studies that measure the effectiveness of online communities in promoting their agenda are scarce. In this work, we study the correspondence between the effectiveness of a community, measured by its success level in a competitive online campaign, and the underlying dynamics between its members. To this end, we define a novel task: predicting the success level of online communities in Reddit's r/place - a large-scale distributed experiment that required collaboration between community members. We consider an array of definitions for success level; each is geared toward different aspects of collaborative achievement. We experiment with several hybrid models, combining various types of features. Our models significantly outperform all baseline models over all definitions of `success level'. Analysis of the results and the factors that contribute to the success of coordinated campaigns can provide a better understanding of the resilience or the vulnerability of communities to online social threats such as election interference or anti-science trends. We make all data used for this study publicly available for further research.Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China and other parts of the world. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Promoting-Exploration-in-Memory-Augmented-Adam-using-Critical-Momenta"><a href="#Promoting-Exploration-in-Memory-Augmented-Adam-using-Critical-Momenta" class="headerlink" title="Promoting Exploration in Memory-Augmented Adam using Critical Momenta"></a>Promoting Exploration in Memory-Augmented Adam using Critical Momenta</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09638">http://arxiv.org/abs/2307.09638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chandar-lab/cmoptimizer">https://github.com/chandar-lab/cmoptimizer</a></li>
<li>paper_authors: Pranshu Malviya, Gonçalo Mordido, Aristide Baratin, Reza Babanezhad Harikandeh, Jerry Huang, Simon Lacoste-Julien, Razvan Pascanu, Sarath Chandar</li>
<li>for: 本研究旨在提高大规模深度学习模型的训练过程中，使用 Adaptive 梯度下降法带来更好的性能。</li>
<li>methods: 本研究提出了一种基于缓存的 Adam 优化器，通过保留一些关键动量项的缓存来促进探索更平的极值陷阱。</li>
<li>results: 实验表明，使用该新的缓存版本的 Adam 优化器可以提高多种 Adam 变体在标准的语言模型和图像分类任务中的性能。<details>
<summary>Abstract</summary>
Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
</details>
<details>
<summary>摘要</summary>
适应式梯度基本优化器，如 Adam，在训练大规模深度学习模型中留下了印记。这些优化器具有快速收敛的优点，同时也更加抗性于超参数选择。然而，它们通常的泛化性比非适应方法差。最近的研究表明，这种性能差距与平坦的最小值选择相关：适应方法通常会在loss函数的梯度图上找到更加锐角的解，这会导致泛化性下降。为了解决这个问题，我们提出了一种新的带缓存的 Adam 方法，通过在训练过程中使用缓存来保持权重的积累。这种方法可以使优化器在训练过程中更加勇敢地尝试不同的方向，从而增强泛化性。我们通过实验证明，我们的方法可以提高许多 Adam 变体在标准的supervised语言模型和图像分类任务中的性能。
</details></li>
</ul>
<hr>
<h2 id="Traffic-Domain-Video-Question-Answering-with-Automatic-Captioning"><a href="#Traffic-Domain-Video-Question-Answering-with-Automatic-Captioning" class="headerlink" title="Traffic-Domain Video Question Answering with Automatic Captioning"></a>Traffic-Domain Video Question Answering with Automatic Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09636">http://arxiv.org/abs/2307.09636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Qasemi, Jonathan M. Francis, Alessandro Oltramari</li>
<li>for: 本研究旨在推动智能交通监测和交通运输系统领域内的高级机器推理能力。</li>
<li>methods: 本研究使用了一种新的方法，即“视频问答自动captioning”（TRIVIA），以强化大型视频语言模型中的城市交通场景知识。</li>
<li>results: 实验结果表明，TRIVIA方法可以提高代表性视频语言模型的准确率，相比基eline设置提高了6.5点（19.88%）。这一突破性的方法在交通相关应用中具有极大的推动力，鼓励研究者和实践者共同探索视频语言模型的潜力。<details>
<summary>Abstract</summary>
Video Question Answering (VidQA) exhibits remarkable potential in facilitating advanced machine reasoning capabilities within the domains of Intelligent Traffic Monitoring and Intelligent Transportation Systems. Nevertheless, the integration of urban traffic scene knowledge into VidQA systems has received limited attention in previous research endeavors. In this work, we present a novel approach termed Traffic-domain Video Question Answering with Automatic Captioning (TRIVIA), which serves as a weak-supervision technique for infusing traffic-domain knowledge into large video-language models. Empirical findings obtained from the SUTD-TrafficQA task highlight the substantial enhancements achieved by TRIVIA, elevating the accuracy of representative video-language models by a remarkable 6.5 points (19.88%) compared to baseline settings. This pioneering methodology holds great promise for driving advancements in the field, inspiring researchers and practitioners alike to unlock the full potential of emerging video-language models in traffic-related applications.
</details>
<details>
<summary>摘要</summary>
视频问答（VidQA）具有很好的潜力，能够推动智能交通监测和交通系统领域的高级机器理解能力。然而，在前一些研究中，对城市交通场景知识的 интеграción into VidQA系统受到了限制。在这项工作中，我们提出了一种新的方法，称为交通领域视频问答自动标注（TRIVIA），它作为软监督技术，将交通领域知识注入到大型视频语言模型中。实验结果表明，TRIVIA可以提高代表性的视频语言模型的准确率，相比基线设定，提高6.5点（19.88%）。这种先锋的方法会推动这个领域的发展，鼓励研究人员和实践者们一起探索emerging视频语言模型在交通相关应用中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-Dual-domain-Network-for-Few-view-Dedicated-Cardiac-SPECT-Image-Reconstructions"><a href="#Transformer-based-Dual-domain-Network-for-Few-view-Dedicated-Cardiac-SPECT-Image-Reconstructions" class="headerlink" title="Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions"></a>Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09624">http://arxiv.org/abs/2307.09624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huidong Xie, Bo Zhou, Xiongchao Chen, Xueqi Guo, Stephanie Thorn, Yi-Hwa Liu, Ge Wang, Albert Sinusas, Chi Liu</li>
<li>for: 这个论文的目的是提出一种基于transformer的3D域 dual-domain网络（TIP-Net），用于从站立数据中生成高质量的3D冠状血管SPECT图像重建。</li>
<li>methods: 该方法首先从投影数据直接重建3D冠状血管SPECT图像，无需迭代重建过程。然后，给出重建输出和原始几视重建结果，进一步使用图像域重建网络进行纠正。</li>
<li>results: 经验 validate by cardiac catheterization图像、核心医生的诊断解读和FDA 510(k)-清理的产品软件，该方法在人体研究中比前一代方法高于cardiac defect contrast，可能启用站立几视专业cardiac SPECT镜像仪器上高质量的缺陷可见化。<details>
<summary>Abstract</summary>
Cardiovascular disease (CVD) is the leading cause of death worldwide, and myocardial perfusion imaging using SPECT has been widely used in the diagnosis of CVDs. The GE 530/570c dedicated cardiac SPECT scanners adopt a stationary geometry to simultaneously acquire 19 projections to increase sensitivity and achieve dynamic imaging. However, the limited amount of angular sampling negatively affects image quality. Deep learning methods can be implemented to produce higher-quality images from stationary data. This is essentially a few-view imaging problem. In this work, we propose a novel 3D transformer-based dual-domain network, called TIP-Net, for high-quality 3D cardiac SPECT image reconstructions. Our method aims to first reconstruct 3D cardiac SPECT images directly from projection data without the iterative reconstruction process by proposing a customized projection-to-image domain transformer. Then, given its reconstruction output and the original few-view reconstruction, we further refine the reconstruction using an image-domain reconstruction network. Validated by cardiac catheterization images, diagnostic interpretations from nuclear cardiologists, and defect size quantified by an FDA 510(k)-cleared clinical software, our method produced images with higher cardiac defect contrast on human studies compared with previous baseline methods, potentially enabling high-quality defect visualization using stationary few-view dedicated cardiac SPECT scanners.
</details>
<details>
<summary>摘要</summary>
心血管疾病（CVD）是全球最主要的死亡原因，而myocardial perfusion imaging using SPECT已经广泛用于CVD的诊断。GE 530/570c专用心血管SPECT扫描仪采用了静止geometry，同时获取19个投影图以增强敏感度和实现动态扫描。然而，有限的角度采样会影响图像质量。深度学习方法可以实现从静止数据生成高质量图像。这是基本上是几视图图像问题。在这种工作中，我们提出了一种新的3D transformer-based dual-domain网络，called TIP-Net，用于高质量3Dcardiac SPECT图像重建。我们的方法首先从投影数据直接重建3Dcardiac SPECT图像，而不需要迭代重建过程。然后，给出其重建输出和原始几视图重建结果，我们进一步修复图像使用图像域重建网络。被 cardiac catheterization 图像、核子医生的诊断解读和FDA 510(k)-cleared严格评估软件评估，我们的方法在人类研究中生成了比前一种基准方法更高的心血管损伤对比度，可能允许使用静止几视图专用心血管SPECT扫描仪获得高质量损伤可视化。
</details></li>
</ul>
<hr>
<h2 id="Gradient-strikes-back-How-filtering-out-high-frequencies-improves-explanations"><a href="#Gradient-strikes-back-How-filtering-out-high-frequencies-improves-explanations" class="headerlink" title="Gradient strikes back: How filtering out high frequencies improves explanations"></a>Gradient strikes back: How filtering out high frequencies improves explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09591">http://arxiv.org/abs/2307.09591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabine Muzellec, Léo Andéol, Thomas Fel, Rufin VanRullen, Thomas Serre</li>
<li>for: 这篇论文旨在解释深度神经网络的决策过程中，使用估计基于方法和推断基于方法之间的比较，以及这两种方法之间的差异。</li>
<li>methods: 这篇论文使用了三种常见的视觉分类模型，并对它们的梯度进行分析，发现高频信息对于梯度的描述是不必要的，并且这些高频信息可能是由抽取操作引入的噪声。</li>
<li>results: 研究发现，通过使用最佳低通 filtered attribution maps，可以提高梯度基于方法的解释效果，并且可以在多个模型中获得更好的解释效果。这些结果表明，使用估计基于方法可以提高解释效果，而不需要使用复杂的计算方法。<details>
<summary>Abstract</summary>
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthermore, our analysis reveals that the operations used in Convolutional Neural Networks (CNNs) for downsampling appear to be a significant source of this high-frequency content -- suggesting aliasing as a possible underlying basis. We then apply an optimal low-pass filter for attribution maps and demonstrate that it improves gradient-based attribution methods. We show that (i) removing high-frequency noise yields significant improvements in the explainability scores obtained with gradient-based methods across multiple models -- leading to (ii) a novel ranking of state-of-the-art methods with gradient-based methods at the top. We believe that our results will spur renewed interest in simpler and computationally more efficient gradient-based methods for explainability.
</details>
<details>
<summary>摘要</summary>
近年来，有一场激进的发展，即使用预测来解释深度神经网络的决策。然而，我们仍然不清楚为什么预测基于的方法超越了梯度基于的方法。在这里，我们从employm empirical observation开始：这两种方法的解释地图具有不同的功率谱，预测基于的方法 fewer high-frequency content than gradient-based methods。这个观察引出了多个问题：这些高频信息的来源是什么，并不是系统真正做出的决策吗？最后，为什么缺少高频信息的预测基于方法会得到多个维度的更好的解释分数？我们分析了三种代表性的视觉分类模型的梯度，发现梯度包含了高频信息的噪声。进一步分析发现，Convolutional Neural Networks (CNNs) 的下采样操作是高频信息的主要来源，这可能是噪声的基础。我们应用最佳低通Filter для解释地图，并观察到：（i）从高频噪声中移除噪声可以提高梯度基于方法的解释分数，（ii）这种方法可以将 gradient-based 方法列为现有的最佳方法之一。我们认为，我们的结果会激励人们对简单而 computationally more efficient 的梯度基于方法的解释表示新的兴趣。
</details></li>
</ul>
<hr>
<h2 id="Automating-Wood-Species-Detection-and-Classification-in-Microscopic-Images-of-Fibrous-Materials-with-Deep-Learning"><a href="#Automating-Wood-Species-Detection-and-Classification-in-Microscopic-Images-of-Fibrous-Materials-with-Deep-Learning" class="headerlink" title="Automating Wood Species Detection and Classification in Microscopic Images of Fibrous Materials with Deep Learning"></a>Automating Wood Species Detection and Classification in Microscopic Images of Fibrous Materials with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09588">http://arxiv.org/abs/2307.09588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars Nieradzik, Jördis Sieburg-Rockel, Stephanie Helmling, Janis Keuper, Thomas Weibel, Andrea Olbrich, Henrike Stephani</li>
<li>for: 这个论文的目的是提出了一种系统的方法来生成大量的浸泡木参考图像集，以便使用深度学习自动识别纤维材料中的硬木种。</li>
<li>methods: 该方法使用了一个灵活的管道来轻松地注解血管元素，并比较了不同的神经网络架构和超参数的性能。</li>
<li>results: 该方法的表现与人工专家类似，未来这将改善全球木材产品流量的控制，以保护森林。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We have developed a methodology for the systematic generation of a large image dataset of macerated wood references, which we used to generate image data for nine hardwood genera. This is the basis for a substantial approach to automate, for the first time, the identification of hardwood species in microscopic images of fibrous materials by deep learning. Our methodology includes a flexible pipeline for easy annotation of vessel elements. We compare the performance of different neural network architectures and hyperparameters. Our proposed method performs similarly well to human experts. In the future, this will improve controls on global wood fiber product flows to protect forests.
</details>
<details>
<summary>摘要</summary>
我们已经开发了一种系统化生成大量浸泡木参考图像的方法ologies，用于生成九种硬木属图像数据。这是我们使用深度学习自动识别硬木种类的基础。我们的方法包括一个灵活的管道，方便对血管元素进行标注。我们比较了不同的神经网络架构和超参数的性能。我们的提议方法与人类专家相似。未来，这将改善全球木质产品流动的控制，以保护森林。Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Overthinking-the-Truth-Understanding-how-Language-Models-Process-False-Demonstrations"><a href="#Overthinking-the-Truth-Understanding-how-Language-Models-Process-False-Demonstrations" class="headerlink" title="Overthinking the Truth: Understanding how Language Models Process False Demonstrations"></a>Overthinking the Truth: Understanding how Language Models Process False Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09476">http://arxiv.org/abs/2307.09476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dannyallover/overthinking_the_truth">https://github.com/dannyallover/overthinking_the_truth</a></li>
<li>paper_authors: Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt</li>
<li>for: 这 paper 研究了模型在几个示例学习中吸收和复制危险内容的现象，并提出了两种相关现象：过度思考和假推导头。</li>
<li>methods: 研究使用了一种基于 interior representations 的方法，通过decode predictions from intermediate layers来研究模型的内部表示。</li>
<li>results: 研究发现，在某些层次上，模型会因为 incorrect demonstrations 而出现过度思考现象，并且这种现象可能归结于 false induction heads 的存在。<details>
<summary>Abstract</summary>
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.
</details>
<details>
<summary>摘要</summary>
现代语言模型可以通过几个示例学习模式，完成复杂任务，无需调整。然而，模仿也可能使模型复制错误或有害内容，如果存在上下文中。我们通过模型内部表示的研究，发现了两种相关现象：过度思考和假推导头。第一种现象，过度思考，发生在decode预测时，当正确vs.错误几个示例中的解码。在早期层次，两种示例都会导致模型行为相似，但行为在某个"关键层"后开始差异化，并且在这个层次下，错误示例下的准确率逐渐下降。第二种现象，假推导头，可能是过度思考的可能机制：这些是在晚期层次中的头，它们会将false信息从前一个示例中拟合并复制，并且去掉这些头会减少过度思考。我们的结果表明，研究模型中间计算可能是制止模型不良行为的有望的途径。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Conditional-Slot-Attention-for-Object-Centric-Learning"><a href="#Unsupervised-Conditional-Slot-Attention-for-Object-Centric-Learning" class="headerlink" title="Unsupervised Conditional Slot Attention for Object Centric Learning"></a>Unsupervised Conditional Slot Attention for Object Centric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09437">http://arxiv.org/abs/2307.09437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avinash Kori, Francesco Locatello, Francesca Toni, Ben Glocker</li>
<li>for: 本研究旨在学习无监督下的对象水平表示，以便在下游逻辑任务中进行更好的表示。</li>
<li>methods: 我们提出了一种无监督条件槽注意力方法（Unsupervised Conditional Slot Attention），使用一个新的概率槽词典（PSD），其中每个槽词ictionary包含一个抽象对象水平特性向量作为键和一个参数化的 Gaussian 分布作为其对应的值。</li>
<li>results: 我们的方法在多个下游任务中提供了场景组合能力和几个几shot适应能力的显著提升，包括对象发现、 compositional scene generation 和 compositional visual reasoning。 另外，我们的方法在对象发现任务中与槽注意力方法相当或更好，而在其他两个任务中则显示出了更好的性能。<details>
<summary>Abstract</summary>
Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show that our method provides scene composition capabilities and a significant boost in a few shot adaptability tasks of compositional visual reasoning, while performing similarly or better than slot attention in object discovery tasks
</details>
<details>
<summary>摘要</summary>
通过不监督学习，从object-centric表示中提取下游逻辑任务的object-level表示是一个emerging领域。在这种情况下，绑定arbitrary数量的object实例到特殊的object槽是一个关键挑战。现有的object-centric表示方法，如槽注意力，通过迭代注意力学习可composable表示，但未能实现特殊槽级别绑定。为解决这个问题，在这篇论文中，我们提出了不监督条件槽注意力，使用一种新的概率槽词典（PSD）。我们定义PSD中的键为抽象object-level属性向量，值为参数化的 Gaussian Distribution。我们示出了学习的特定object-levelconditioning分布的好处，在多个下游任务中，包括object发现、compositionalScene生成和compositional visual reasoning。我们显示了我们的方法可以实现Scene组合能力，并在compositional visual reasoning中具有显著的几步适应性提升，同时与槽注意力在object发现任务中表现相似或更好。
</details></li>
</ul>
<hr>
<h2 id="SLMGAN-Exploiting-Speech-Language-Model-Representations-for-Unsupervised-Zero-Shot-Voice-Conversion-in-GANs"><a href="#SLMGAN-Exploiting-Speech-Language-Model-Representations-for-Unsupervised-Zero-Shot-Voice-Conversion-in-GANs" class="headerlink" title="SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs"></a>SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09435">http://arxiv.org/abs/2307.09435</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yl4579/SLMGAN">https://github.com/yl4579/SLMGAN</a></li>
<li>paper_authors: Yinghao Aaron Li, Cong Han, Nima Mesgarani</li>
<li>for: 这篇论文旨在提出一种基于大规模预训练语音语言模型（SLM）的语音变换模型，使用Generative Adversarial Network（GAN）框架，以实现零参数语音变换系统。</li>
<li>methods: 这篇论文使用了SLM Representations来进行拟合任务，并在StarGANv2-VC基础上添加了我们的新的SLM-based WavLM拟合器和我们自己设计的SLM特征匹配损失函数，从而实现了无监督零参数语音变换系统。</li>
<li>results: 对比 existing state-of-the-art zero-shot voice conversion models, 本paper的SLMGAN模型在自然性和相似性方面表现出色，并且与文本标注不需要进行训练。<details>
<summary>Abstract</summary>
In recent years, large-scale pre-trained speech language models (SLMs) have demonstrated remarkable advancements in various generative speech modeling applications, such as text-to-speech synthesis, voice conversion, and speech enhancement. These applications typically involve mapping text or speech inputs to pre-trained SLM representations, from which target speech is decoded. This paper introduces a new approach, SLMGAN, to leverage SLM representations for discriminative tasks within the generative adversarial network (GAN) framework, specifically for voice conversion. Building upon StarGANv2-VC, we add our novel SLM-based WavLM discriminators on top of the mel-based discriminators along with our newly designed SLM feature matching loss function, resulting in an unsupervised zero-shot voice conversion system that does not require text labels during training. Subjective evaluation results show that SLMGAN outperforms existing state-of-the-art zero-shot voice conversion models in terms of naturalness and achieves comparable similarity, highlighting the potential of SLM-based discriminators for related applications.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近年来，大规模预训练的语音语言模型（SLM）在各种生成语音模型应用中表现出色，如文本到语音合成、声音转换和语音增强等。这些应用通常涉及将文本或语音输入映射到预训练SLM表示，从而解码目标语音。本文介绍了一种新的方法，SLMGAN，利用SLM表示来在生成对抗网络（GAN）框架中进行探索性任务，特别是语音转换。基于StarGANv2-VC，我们添加了我们的新的SLM基于WavLM探测器，以及我们新的SLM特征匹配损失函数，从而实现了无监督零shot语音转换系统，不需要文本标注 durante 训练。主观评估结果表明，SLMGAN在自然性和相似性方面比前 estado-of-the-art 零shot语音转换模型高， highlighting the potential of SLM-based discriminators for related applications.
</details></li>
</ul>
<hr>
<h2 id="Balancing-Privacy-and-Progress-in-Artificial-Intelligence-Anonymization-in-Histopathology-for-Biomedical-Research-and-Education"><a href="#Balancing-Privacy-and-Progress-in-Artificial-Intelligence-Anonymization-in-Histopathology-for-Biomedical-Research-and-Education" class="headerlink" title="Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education"></a>Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09426">http://arxiv.org/abs/2307.09426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neel Kanwal, Emiel A. M. Janssen, Kjersti Engan</li>
<li>For: This paper aims to explore the legal regulations and terminologies for medical data-sharing in the context of histopathology, with a focus on balancing privacy and progress in bio-informatics research.* Methods: The paper reviews existing approaches to medical data-sharing and highlights challenges from the histopathological perspective, including the risk of data linkage attacks and the lack of standardization in digital pathology.* Results: The paper presents a data-sharing guideline for histological data to foster multidisciplinary research and education, while addressing the challenges of privacy and data usability.Here’s the same information in Simplified Chinese text:* For: 这篇论文目标是探讨医疗数据共享的法律规定和术语，特别是在 Histopathology 领域内进行生物信息学研究的平衡 privacy 和进步。* Methods: 论文回顾了现有的医疗数据共享方法，并将 Histopathology 领域的挑战作为主要关注点，包括数据链接攻击和数字patology 的不一致性。* Results: 论文提出了一个 Histological 数据共享指南，以促进多学科研究和教育，同时解决 privacy 和数据可用性之间的矛盾。<details>
<summary>Abstract</summary>
The advancement of biomedical research heavily relies on access to large amounts of medical data. In the case of histopathology, Whole Slide Images (WSI) and clinicopathological information are valuable for developing Artificial Intelligence (AI) algorithms for Digital Pathology (DP). Transferring medical data "as open as possible" enhances the usability of the data for secondary purposes but poses a risk to patient privacy. At the same time, existing regulations push towards keeping medical data "as closed as necessary" to avoid re-identification risks. Generally, these legal regulations require the removal of sensitive data but do not consider the possibility of data linkage attacks due to modern image-matching algorithms. In addition, the lack of standardization in DP makes it harder to establish a single solution for all formats of WSIs. These challenges raise problems for bio-informatics researchers in balancing privacy and progress while developing AI algorithms. This paper explores the legal regulations and terminologies for medical data-sharing. We review existing approaches and highlight challenges from the histopathological perspective. We also present a data-sharing guideline for histological data to foster multidisciplinary research and education.
</details>
<details>
<summary>摘要</summary>
医学研究的进步受到医疗数据的大量存储的限制。在 histopathology 中，整个标本图像（WSI）和临床 PATHOLOGY 信息是开发数字 pathology （DP） 的人工智能（AI）算法的宝贵资源。将医疗数据“作为公开的可能”可以提高数据的可重用性，但会对患者隐私带来风险。同时，现有的法规强调保持医疗数据“作为关闭的必要”，以避免重新邻接风险。通常，这些法规要求去除敏感数据，但不考虑现代图像匹配算法可能导致的数据链接攻击。此外，DP 的不一致性使其更难建立一个 universal 的解决方案。这些挑战使生物信息学研究人员在保护隐私和进步之间做出坚韧的决策。本文研究医疗数据共享的法规和术语。我们评论现有的方法并高亮 histopathological 视角中的挑战。我们还提出了 histological 数据共享指南，以促进多学科研究和教育。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-for-Imitation-Learning-in-NetHack"><a href="#Scaling-Laws-for-Imitation-Learning-in-NetHack" class="headerlink" title="Scaling Laws for Imitation Learning in NetHack"></a>Scaling Laws for Imitation Learning in NetHack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09423">http://arxiv.org/abs/2307.09423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade<br>for: 这个论文主要目的是研究如何通过练习学习来帮助机器学习模型更好地学习人类专家的行为。methods: 这篇论文使用了委婉学习（Imitation Learning，IL）方法，并且对模型和数据集进行了缩放。results: 研究发现，通过缩放模型和数据集可以使IL模型更好地学习人类专家的行为，并且可以达到至少2倍的性能提升。<details>
<summary>Abstract</summary>
Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL and find they outperform prior state-of-the-art by at least 2x in all settings. Our work both demonstrates the scaling behavior of imitation learning in a challenging domain, as well as the viability of scaling up current approaches for increasingly capable agents in NetHack, a game that remains elusively hard for current AI systems.
</details>
<details>
<summary>摘要</summary>
copying learning (IL) 是机器学习中最广泛使用的方法之一。然而，虽然强大，但许多研究发现它通常无法完全恢复专家行为的下面。然而，这些研究几乎没有深入探究扩大模型和数据集大小的作用。受到最近的自然语言处理（NLP）研究的启发，我们调查了扩大模型和数据集大小是否可以在imitating learning setting中带来类似的改进。为了证明我们的发现，我们关注了NetHack游戏，这是一个复杂的环境，具有生成过程、随机性、长期依赖和部分可见性。我们发现IL损失和平均返回金额随着计算预算的增加而呈现平滑的关系，并且具有强相关性，从而导致了训练计算优化IL代理人的模型大小和样本数量的力学关系。我们预测和训练了一些NetHack代理人，并发现它们在所有设置下都高于之前的状态艺术，至少提高2倍。我们的工作不仅证明了imitating learning在复杂环境中的扩展行为，也证明了可以通过扩大当前方法来创建更有能力的NetHack代理人，一游戏仍然是AI系统所难以解决的。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/cs.AI_2023_07_19/" data-id="cloh7tqat000v7b886p2k9uji" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/cs.CL_2023_07_19/" class="article-date">
  <time datetime="2023-07-19T11:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/cs.CL_2023_07_19/">cs.CL - 2023-07-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Android-in-the-Wild-A-Large-Scale-Dataset-for-Android-Device-Control"><a href="#Android-in-the-Wild-A-Large-Scale-Dataset-for-Android-Device-Control" class="headerlink" title="Android in the Wild: A Large-Scale Dataset for Android Device Control"></a>Android in the Wild: A Large-Scale Dataset for Android Device Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10088">http://arxiv.org/abs/2307.10088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, Timothy Lillicrap</li>
<li>For: 这个论文旨在提供一个大型的设备控制数据集，用于研究设备控制系统的语言理解和可视上下文的推断。* Methods: 该论文使用了人类示例来描述设备交互，包括屏幕和操作，以及相应的自然语言指令。它包含715k个集和30k个唯一的指令，四个版本的Android（v10-13），八种设备类型（Pixel 2 XL到Pixel 6），以及不同的屏幕分辨率。* Results: 该论文报告了两个代理的性能分布在数据集中，并提出了一种新的挑战：从视觉上的动作进行推断。而不是简单的UI元素基础的动作，动作空间包括精准的手势（例如水平滚动来操作轮播widget）。<details>
<summary>Abstract</summary>
There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls to operate carousel widgets). We organize our dataset to encourage robustness analysis of device-control systems, i.e., how well a system performs in the presence of new task descriptions, new applications, or new platform versions. We develop two agents and report performance across the dataset. The dataset is available at https://github.com/google-research/google-research/tree/master/android_in_the_wild.
</details>
<details>
<summary>摘要</summary>
“现在有一个增长的兴趣是将人类自然语言指令转换为数位设备的控制系统，以直接控制其用户界面。我们发布了一个叫Android在野（AITW）的数据集，该数据集比现有的数据集有很大的规模。该数据集包含人类对设备互动的示例，包括萤幕和动作，以及对应的自然语言指令。它包含715,000个集和30,000个专案，涵盖四个版本的Android（v10-13）和八款设备（Pixel 2 XL到Pixel 6），具有不同的萤幕分辨率。它包含多步任务，需要自然语言理解和视觉上下文。这个数据集对于设备控制系统的研究提出了新的挑战：需要从视觉上的动作推导出可用的动作。而不是简单的UI元素基础的动作，动作空间包括精确的手势（例如，横向滑块操作卡车ousel widget）。我们将数据集分为多个分支，以便鼓励设备控制系统的韧性分析，即在新的任务描述、新的应用程序或新的平台版本下，系统的表现如何。我们开发了两个代理，并在数据集上进行了性能评估。数据集可以在https://github.com/google-research/google-research/tree/master/android_in_the_wild上取得。”
</details></li>
</ul>
<hr>
<h2 id="Generating-Mathematical-Derivations-with-Large-Language-Models"><a href="#Generating-Mathematical-Derivations-with-Large-Language-Models" class="headerlink" title="Generating Mathematical Derivations with Large Language Models"></a>Generating Mathematical Derivations with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09998">http://arxiv.org/abs/2307.09998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jmeadows17/deriving-equations-with-llms">https://github.com/jmeadows17/deriving-equations-with-llms</a></li>
<li>paper_authors: Jordan Meadows, Marco Valentino, Andre Freitas</li>
<li>for: 本研究旨在使用大型自然语言模型（LLM） derive 数学结果，以探索这些模型的局限性，并可能支持数学发现。</li>
<li>methods: 我们使用符号引擎生成方程式，并 comparing 不同预训练策略的 Robustness 和泛化能力。</li>
<li>results: 我们发现， fine-tuned FLAN-T5-large (MathT5) 模型在所有静态和 OUT-OF-distribution 测试集上表现出色，但是 fine-tuned 模型对未看过的符号和方程结构变化显示高度敏感。此外，我们还发现了一些常见的逻辑错误，如包含错误、无关的和重复的方程。最后，我们发现现有的评价指标不能准确评价生成的数学文本质量。<details>
<summary>Abstract</summary>
The derivation of mathematical results in specialised fields, using Large Language Models (LLMs), is an emerging research direction that can help identify models' limitations, and potentially support mathematical discovery. In this paper, we leverage a symbolic engine to generate derivations of equations at scale, and investigate the capabilities of LLMs when deriving goal equations from premises. Specifically, we employ in-context learning for GPT and fine-tune a range of T5 models to compare the robustness and generalisation of pre-training strategies to specialised models. Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in conventional scores. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse 1.7K equations, and over 200 derivations, to highlight common reasoning errors such as the inclusion of incorrect, irrelevant, and redundant equations. Finally, we explore the suitability of existing metrics for evaluating mathematical derivations and find evidence that, while they can capture general properties such as sensitivity to perturbations, they fail to highlight fine-grained reasoning errors and essential differences between models. Overall, this work demonstrates that training models on synthetic data may improve their math capabilities beyond much larger LLMs, but current metrics are not appropriately assessing the quality of generated mathematical text.
</details>
<details>
<summary>摘要</summary>
大数据集中的数学结果推导，使用大语言模型（LLM），是一个emerging的研究方向，可以帮助我们了解模型的局限性，并可能支持数学发现。在这篇论文中，我们利用符号引擎生成方程式的推导，并对LLM的推导能力进行了调整。具体来说，我们使用上下文学习来训练GPT，并对T5模型进行了较为精细的调整。我们通过对特定模型进行准确的训练来比较预处理策略的稳定性和泛化性。实验结果表明，对MathT5模型进行了精细的训练后，其在所有静态和非静态测试集上的成绩都高于GPT模型。然而，我们的深入分析表明，这些精细调整后的模型在未看过的符号和方程结构变化时更加敏感。此外，我们对1.7K方程和200个推导进行了分析，发现了一些常见的逻辑错误，如包含错误、无关和繁殖的方程。最后，我们检查了现有的评价指标是否能够正确评估生成的数学文本质量，发现它们可以捕捉大致的敏感性，但是不能捕捉细致的逻辑错误和模型之间的重要差异。总的来说，这项研究表明，通过训练模型在大数据集中可以提高其数学能力，但现有的评价指标不能准确评估生成的数学文本质量。
</details></li>
</ul>
<hr>
<h2 id="GUIDO-A-Hybrid-Approach-to-Guideline-Discovery-Ordering-from-Natural-Language-Texts"><a href="#GUIDO-A-Hybrid-Approach-to-Guideline-Discovery-Ordering-from-Natural-Language-Texts" class="headerlink" title="GUIDO: A Hybrid Approach to Guideline Discovery &amp; Ordering from Natural Language Texts"></a>GUIDO: A Hybrid Approach to Guideline Discovery &amp; Ordering from Natural Language Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09959">http://arxiv.org/abs/2307.09959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nils-freyer/guido">https://github.com/nils-freyer/guido</a></li>
<li>paper_authors: Nils Freyer, Dustin Thewes, Matthias Meinecke</li>
<li>for: 提取文书中的工作流程网络，以简化指南或正式化文书中的过程描述</li>
<li>methods: 使用BERT模型来分类句子的相关性，并使用依赖分析来提取相关句子中的工作流程模型</li>
<li>results: GUIDO方法可以很好地提取工作流程模型，与纯机器学本来的方法相比，注解成本较低。<details>
<summary>Abstract</summary>
Extracting workflow nets from textual descriptions can be used to simplify guidelines or formalize textual descriptions of formal processes like business processes and algorithms. The task of manually extracting processes, however, requires domain expertise and effort. While automatic process model extraction is desirable, annotating texts with formalized process models is expensive. Therefore, there are only a few machine-learning-based extraction approaches. Rule-based approaches, in turn, require domain specificity to work well and can rarely distinguish relevant and irrelevant information in textual descriptions. In this paper, we present GUIDO, a hybrid approach to the process model extraction task that first, classifies sentences regarding their relevance to the process model, using a BERT-based sentence classifier, and second, extracts a process model from the sentences classified as relevant, using dependency parsing. The presented approach achieves significantly better results than a pure rule-based approach. GUIDO achieves an average behavioral similarity score of $0.93$. Still, in comparison to purely machine-learning-based approaches, the annotation costs stay low.
</details>
<details>
<summary>摘要</summary>
可以从文本描述中提取工作流程网络，以简化指南或正式化文本描述形式的过程，如商业过程和算法。 however， manually extracting processes requires domain expertise and effort. While automatic process model extraction is desirable, annotating texts with formalized process models is expensive. Therefore, there are only a few machine-learning-based extraction approaches. Rule-based approaches require domain specificity to work well and can rarely distinguish relevant and irrelevant information in textual descriptions. In this paper, we present GUIDO， a hybrid approach to the process model extraction task that first, classifies sentences based on their relevance to the process model using a BERT-based sentence classifier, and second, extracts a process model from the sentences classified as relevant using dependency parsing. The presented approach achieves significantly better results than a pure rule-based approach. GUIDO achieves an average behavioral similarity score of $0.93$. Still, in comparison to purely machine-learning-based approaches, the annotation costs stay low.Note: I used the Google Translate API to translate the text into Simplified Chinese. Please note that the translation may not be perfect and may require some adjustments to accurately convey the intended meaning.
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-can-accomplish-Business-Process-Management-Tasks"><a href="#Large-Language-Models-can-accomplish-Business-Process-Management-Tasks" class="headerlink" title="Large Language Models can accomplish Business Process Management Tasks"></a>Large Language Models can accomplish Business Process Management Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09923">http://arxiv.org/abs/2307.09923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Grohs, Luka Abb, Nourhan Elsayed, Jana-Rebecca Rehse</li>
<li>for: 本研究旨在探讨自然语言处理技术如何应用于商业流程管理（BPM）中，以提高组织活动的效率和成果。</li>
<li>methods: 本研究使用大语言模型（LLM）来解决三种 exemplary BPM 任务：从文本描述中检索 Imperative 过程模型、从文本描述中检索 Declarative 过程模型、以及根据文本描述评估过程任务的合适性 для机器过程自动化。</li>
<li>results: 研究表明，无需较多的配置或提示工程，LLMs 可以与现有解决方案相比或者更好地完成这些任务，并讨论未来 BPM 研究的未来和实际应用中的可能性。<details>
<summary>Abstract</summary>
Business Process Management (BPM) aims to improve organizational activities and their outcomes by managing the underlying processes. To achieve this, it is often necessary to consider information from various sources, including unstructured textual documents. Therefore, researchers have developed several BPM-specific solutions that extract information from textual documents using Natural Language Processing techniques. These solutions are specific to their respective tasks and cannot accomplish multiple process-related problems as a general-purpose instrument. However, in light of the recent emergence of Large Language Models (LLMs) with remarkable reasoning capabilities, such a general-purpose instrument with multiple applications now appears attainable. In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by applying a specific LLM to three exemplary tasks: mining imperative process models from textual descriptions, mining declarative process models from textual descriptions, and assessing the suitability of process tasks from textual descriptions for robotic process automation. We show that, without extensive configuration or prompt engineering, LLMs perform comparably to or better than existing solutions and discuss implications for future BPM research as well as practical usage.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DAPrompt-Deterministic-Assumption-Prompt-Learning-for-Event-Causality-Identification"><a href="#DAPrompt-Deterministic-Assumption-Prompt-Learning-for-Event-Causality-Identification" class="headerlink" title="DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification"></a>DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09813">http://arxiv.org/abs/2307.09813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Xiang, Chuanhong Zhan, Bang Wang</li>
<li>for: 本研究旨在解决事件关系识别任务中，是否存在 causal 关系 между两个事件的问题。</li>
<li>methods: 我们提出了一种新的 deterministic assumption prompt learning 模型，称为 DAPrompt，它基于预训练语言模型中嵌入的百科全书式知识。</li>
<li>results: 实验结果表明，相比现状态的算法，DAPrompt 模型在 EventStoryLine  corpora 和 Causal-TimeBank corpus 上显示了显著的性能提升。<details>
<summary>Abstract</summary>
Event Causality Identification (ECI) aims at determining whether there is a causal relation between two event mentions. Conventional prompt learning designs a prompt template to first predict an answer word and then maps it to the final decision. Unlike conventional prompts, we argue that predicting an answer word may not be a necessary prerequisite for the ECI task. Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption. The design motivation is to try the most utilization of the encyclopedia-like knowledge embedded in a pre-trained language model. In light of such considerations, we propose a deterministic assumption prompt learning model, called DAPrompt, for the ECI task. In particular, we design a simple deterministic assumption template concatenating with the input event pair, which includes two masks as predicted events' tokens. We use the probabilities of predicted events to evaluate the assumption rationality for the final event causality decision. Experiments on the EventStoryLine corpus and Causal-TimeBank corpus validate our design objective in terms of significant performance improvements over the state-of-the-art algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="IncDSI-Incrementally-Updatable-Document-Retrieval"><a href="#IncDSI-Incrementally-Updatable-Document-Retrieval" class="headerlink" title="IncDSI: Incrementally Updatable Document Retrieval"></a>IncDSI: Incrementally Updatable Document Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10323">http://arxiv.org/abs/2307.10323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varsha Kishore, Chao Wan, Justin Lovelace, Yoav Artzi, Kilian Q. Weinberger</li>
<li>for: 这篇论文是关于文档检索的一种新的方法，即使用神经网络来直接将查询转换为相应的文档。</li>
<li>methods: 这种方法使用一种名为Differentiable Search Index的新的搜索索引方法，可以在训练时间内添加新的文档，而不需要重新训练整个数据集。</li>
<li>results: 这种方法可以在20-50毫秒内添加新的文档，而不需要重新训练整个数据集，并且与完全训练新模型相比，其性能 Competitive。<details>
<summary>Abstract</summary>
Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI is available at https://github.com/varshakishore/IncDSI.
</details>
<details>
<summary>摘要</summary>
干净搜索索引是一种最近提出的文档检索模式，它将文档库内容编码到神经网络参数中，直接将查询符号映射到相应的文档。这些模型在许多标准准测试 benchmark 上实现了状态足的表现。然而，这些类型的模型有一定的限制：添加新文档不容易。我们提出了 IncDSI，一种在实时（约20-50ms每个文档）添加文档的方法，不需要重新训练整个数据集（或者部分）。相反，我们将文档添加形式为约束优化问题，以便减少网络参数的变化。虽然速度是当前的许多orders of magnitude，但我们的方法与重新训练整个数据集相比，并不逊色。这使得可以在实时更新文档检索系统，以便在实时添加新信息。我们的 IncDSI 代码可以在 GitHub 上找到：https://github.com/varshakishore/IncDSI。
</details></li>
</ul>
<hr>
<h2 id="On-the-Origin-of-LLMs-An-Evolutionary-Tree-and-Graph-for-15-821-Large-Language-Models"><a href="#On-the-Origin-of-LLMs-An-Evolutionary-Tree-and-Graph-for-15-821-Large-Language-Models" class="headerlink" title="On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models"></a>On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09793">http://arxiv.org/abs/2307.09793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Gao, Andrew Kean Gao</li>
<li>for: 本研究旨在找出大语言模型（LLMs）中具有共同特征和趋势的家族和Subgroup,以便更好地理解和分析这些模型的性能和应用 potential.</li>
<li>methods: 该研究使用了 hierarchical clustering 和 n-grams 以及 term frequency-inverse document frequency（TF-IDF）来找出 LLMS 中的共同特征和趋势,并建立了一个 web 应用程序来快速生成多种可见化 visualization.</li>
<li>results: 研究发现，使用 TF-IDF 和 n-grams 可以准确地找出 LLMS 中的家族和Subgroup,并且可以通过 Constellation web 应用程序来快速生成多种可见化 visualization,以便更好地理解和分析这些模型的性能和应用 potential.<details>
<summary>Abstract</summary>
Since late 2022, Large Language Models (LLMs) have become very prominent with LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs are announced each week, many of which are deposited to Hugging Face, a repository of machine learning models and datasets. To date, nearly 16,000 Text Generation models have been uploaded to the site. Given the huge influx of LLMs, it is of interest to know which LLM backbones, settings, training methods, and families are popular or trending. However, there is no comprehensive index of LLMs available. We take advantage of the relatively systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering and identify communities amongst LLMs using n-grams and term frequency-inverse document frequency. Our methods successfully identify families of LLMs and accurately cluster LLMs into meaningful subgroups. We present a public web application to navigate and explore Constellation, our atlas of 15,821 LLMs. Constellation rapidly generates a variety of visualizations, namely dendrograms, graphs, word clouds, and scatter plots. Constellation is available at the following link: https://constellation.sites.stanford.edu/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mood-Classification-of-Bangla-Songs-Based-on-Lyrics"><a href="#Mood-Classification-of-Bangla-Songs-Based-on-Lyrics" class="headerlink" title="Mood Classification of Bangla Songs Based on Lyrics"></a>Mood Classification of Bangla Songs Based on Lyrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10314">http://arxiv.org/abs/2307.10314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor</li>
<li>for: 这个研究旨在分类孔乃诗歌的情感类型，以便更好地理解人们对音乐的感受。</li>
<li>methods: 该研究使用自然语言处理和Bert算法来分析4000首孔乃诗歌的歌词，并将其分为四种情感类型：快乐、悲伤、爱情和放松。</li>
<li>results: 研究发现，4000首孔乃诗歌中有1513首表达悲伤的情感，1362首表达爱情的情感，886首表达快乐的情感，而剩下的239首则属于放松的情感。这些结果表明，自动地分类孔乃诗歌的情感类型是可行的，并且准确性较高。<details>
<summary>Abstract</summary>
Music can evoke various emotions, and with the advancement of technology, it has become more accessible to people. Bangla music, which portrays different human emotions, lacks sufficient research. The authors of this article aim to analyze Bangla songs and classify their moods based on the lyrics. To achieve this, this research has compiled a dataset of 4000 Bangla song lyrics, genres, and used Natural Language Processing and the Bert Algorithm to analyze the data. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362 for the romantic mood, 886 for happiness, and the rest 239 are classified as relaxation. By embedding the lyrics of the songs, the authors have classified the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is crucial as it enables a multi-class classification of songs' moods, making the music more relatable to people's emotions. The article presents the automated result of the four moods accurately derived from the song lyrics.
</details>
<details>
<summary>摘要</summary>
音乐可以诱发多种情感，技术的发展使得音乐更加容易访问。孟加拉音乐，表达不同人类情感的形式，尚未得到充分的研究。本文的作者想要分析孟加拉歌曲，根据歌词来分类其情感。为了实现这一目标，本研究编译了4000首孟加拉歌曲的歌词、类型，并使用自然语言处理和Bert算法来分析数据。总共有1513首歌曲表达了悲伤的情感，1362首表达了爱情的情感，886首表达了喜乐的情感，剩下的239首被分类为宁静。通过嵌入歌曲 lyrics，作者将歌曲分为四种情感：快乐、悲伤、爱情和宁静。这项研究非常重要，因为它使得歌曲的情感更加可 relate 到人们的情感，从而使得音乐更加美妙。文章展示了自动从歌曲 lyrics 中提取出的四种情感的准确分类结果。
</details></li>
</ul>
<hr>
<h2 id="CValues-Measuring-the-Values-of-Chinese-Large-Language-Models-from-Safety-to-Responsibility"><a href="#CValues-Measuring-the-Values-of-Chinese-Large-Language-Models-from-Safety-to-Responsibility" class="headerlink" title="CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility"></a>CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09705">http://arxiv.org/abs/2307.09705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/x-plug/cvalues">https://github.com/x-plug/cvalues</a></li>
<li>paper_authors: Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, Jingren Zhou</li>
<li>for: 评估大语言模型（LLMs）是否符合人类价值观念的重要性在不断增长，这篇论文提出了首个中文人类价值评估标准（CValues），用于评估 LLMS 在安全性和责任性两个方面的价值Alignment。</li>
<li>methods: 该论文使用了人工收集的阴性安全提问和责任提问，以及专业专家卷积的多选提问，以提供中文 LLMS 的全面价值评估。</li>
<li>results: 研究发现，大多数中文 LLMS 在安全性方面表现良好，但在责任性方面还有很大的提升空间。此外，自动和人类评估都是评估中文 LLMS 的人类价值Alignment 的重要方法。<details>
<summary>Abstract</summary>
With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts. Therefore, evaluation of human values alignment is becoming increasingly important. Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context. In this paper, we present CValues, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria. As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. Our findings suggest that while most Chinese LLMs perform well in terms of safety, there is considerable room for improvement in terms of responsibility. Moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects. The benchmark and code is available on ModelScope and Github.
</details>
<details>
<summary>摘要</summary>
随着大语言模型（LLM）的快速演化，有关它们可能带来风险或有负面社会影响的担忧增加。因此，评估人类价值Alignment在当前变得越来越重要。先前的工作主要集中于评估LLM的某些知识和理解能力，而忽略了与人类价值的Alignment，尤其在中文上下文中。在这篇论文中，我们介绍了CValues，第一个中文人类价值评估标准，用于评估LLM的安全和责任性能力。我们手动收集了10个场景下的逆向安全提示和8个领域下的责任提示，并构建了多选提示以进行自动评估。我们发现，大多数中文LLM在安全性方面表现良好，但在责任性方面有很大的改进空间。此外，人工评估和自动评估都是评估中文LLM的人类价值Alignment的重要方法。我们的找到结果和标准化的代码将在ModelScope和Github上公布。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Guided-Generation-for-Large-Language-Models"><a href="#Efficient-Guided-Generation-for-Large-Language-Models" class="headerlink" title="Efficient Guided Generation for Large Language Models"></a>Efficient Guided Generation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09702">http://arxiv.org/abs/2307.09702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/normal-computing/outlines">https://github.com/normal-computing/outlines</a></li>
<li>paper_authors: Brandon T. Willard, Rémi Louf</li>
<li>for: 本文提出了一种使用 finite-state machine 框架来解决神经文本生成问题的方法。</li>
<li>methods: 该方法使用 regular expressions 和 context-free grammars 来引导文本生成，并可以在语言模型中构建索引，从而保证生成的文本结构可靠。</li>
<li>results: 该方法在性能上显著超过了现有的解决方案，并允许在不同领域中应用域特定的知识和约束。<details>
<summary>Abstract</summary>
In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们展示了如何将神经文本生成问题构思为finite-state机器的状态转移问题。这个框架导致了一种高效的使用正则表达式和context-free grammar来引导文本生成的方法，可以建立语言模型词汇索引。该方法是模型无关的，允许承载域特定的知识和限制，并能够建立可靠的界面，保证生成的文本结构。它增加了少量的токен序列生成过程的负担，并显著超越了现有的解决方案。我们在Python开源库Outlines中提供了实现。
</details></li>
</ul>
<hr>
<h2 id="Efficiency-Pentathlon-A-Standardized-Arena-for-Efficiency-Evaluation"><a href="#Efficiency-Pentathlon-A-Standardized-Arena-for-Efficiency-Evaluation" class="headerlink" title="Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation"></a>Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09701">http://arxiv.org/abs/2307.09701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi</li>
<li>for: The paper aims to address the practical challenges in evaluating and comparing the efficiency of natural language processing (NLP) models, and to provide a standardized and centralized platform for fair and reproducible evaluations.</li>
<li>methods: The paper introduces Pentathlon, a benchmark for holistic and realistic evaluation of NLP model efficiency, which focuses on inference and offers a strictly-controlled hardware platform, a suite of metrics, and a software library for seamless integration.</li>
<li>results: The paper hopes to stimulate algorithmic innovations in building efficient NLP models and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.Here is the same information in Simplified Chinese text:</li>
<li>for: 本文旨在解决现代自然语言处理（NLP）系统的计算需求增加了研究障碍和环境问题的实际挑战，并提供一个标准化和中心化的评估平台。</li>
<li>methods: 本文介绍了 Pentathlon，一个用于整体和现实启用的 NLP 模型效率评估 benchmark，它将注重协议，提供一个严格控制的硬件平台，以及一个综合指标集和可顺应式软件库。</li>
<li>results: 本文希望通过促进高效 NLP 模型的算法创新和社会环境因素的认识，推动未来一代 NLP 模型的发展。<details>
<summary>Abstract</summary>
Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.
</details>
<details>
<summary>摘要</summary>
现代自然语言处理（NLP）系统的计算需求不断增加，使得进行前沿研究的门槛增加，同时也提出了严重的环境问题。然而，模型效率的进步受到了实际检验和比较的实际挑战。例如，硬件控制因为不同机构的访问权限而具有不同水平的困难。此外，在计算力（FLOPs）中提高的改进 frequently fails to translate into real-world applications.为此，我们介绍了 Pentathlon，一个用于整体和现实应用场景的模型效率评价标准。Pentathlon 专注于推理，占模型生命周期的主要部分。它提供一个严格控制的硬件平台，并遵循现实应用场景的设计。它包含一 suite of metrics targeting different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption。Pentathlon 还提供了一个可以轻松地 интеGRATE到任何代码库的软件库，可以实现评价。作为一个标准化和中心化的评价平台，Pentathlon 可以减少对比较性和可重现性的工作负担。虽然首先关注于自然语言处理（NLP）模型，但Pentathlon 设计可扩展到其他领域。我们期望Pentathlon 能够促进模型建构的算法创新，并且推动未来一代 NLP 模型的开发中对社会和环境的考虑。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights"><a href="#Analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights" class="headerlink" title="Analyzing sports commentary in order to automatically recognize events and extract insights"></a>Analyzing sports commentary in order to automatically recognize events and extract insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10303">http://arxiv.org/abs/2307.10303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanismiraoui/analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights">https://github.com/yanismiraoui/analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights</a></li>
<li>paper_authors: Yanis Miraoui</li>
<li>for: 本研究旨在使用不同自然语言处理技术和方法自动识别体育活动中的主要动作。</li>
<li>methods: 本研究使用了不同的自然语言处理技术和方法，包括文本分析和分类，以EXTRACT主要动作的抽象。</li>
<li>results: 研究发现，可以使用情感分析来检测主要动作。<details>
<summary>Abstract</summary>
In this paper, we carefully investigate how we can use multiple different Natural Language Processing techniques and methods in order to automatically recognize the main actions in sports events. We aim to extract insights by analyzing live sport commentaries from different sources and by classifying these major actions into different categories. We also study if sentiment analysis could help detect these main actions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们仔细研究了如何使用多种自然语言处理技术和方法来自动识别体育活动中的主要动作。我们目标是通过分析不同来源的直播体育评论来提取情感，并将这些主要动作分类为不同类别。我们还研究了情感分析是否可以帮助探测这些主要动作。
</details></li>
</ul>
<hr>
<h2 id="Can-Model-Fusing-Help-Transformers-in-Long-Document-Classification-An-Empirical-Study"><a href="#Can-Model-Fusing-Help-Transformers-in-Long-Document-Classification-An-Empirical-Study" class="headerlink" title="Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study"></a>Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09532">http://arxiv.org/abs/2307.09532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damithdr/legal-classification">https://github.com/damithdr/legal-classification</a></li>
<li>paper_authors: Damith Premasiri, Tharindu Ranasinghe, Ruslan Mitkov</li>
<li>for: 本研究旨在解决长文档分类问题，以提高现有的转换器模型在多个领域中的应用能力。</li>
<li>methods: 本研究使用模型融合技术来解决长文档分类问题，并与BERT和Longformer架构进行比较。</li>
<li>results: 研究发现，模型融合技术可以提高转换器模型在长文档分类任务中的表现，并且比BERT和Longformer架构更具有灵活性和可扩展性。<details>
<summary>Abstract</summary>
Text classification is an area of research which has been studied over the years in Natural Language Processing (NLP). Adapting NLP to multiple domains has introduced many new challenges for text classification and one of them is long document classification. While state-of-the-art transformer models provide excellent results in text classification, most of them have limitations in the maximum sequence length of the input sequence. The majority of the transformer models are limited to 512 tokens, and therefore, they struggle with long document classification problems. In this research, we explore on employing Model Fusing for long document classification while comparing the results with well-known BERT and Longformer architectures.
</details>
<details>
<summary>摘要</summary>
文本分类是一个长期研究的领域，在自然语言处理（NLP）中进行研究。适应多个领域的NLP带来了许多新的挑战，其中之一是长文档分类。虽然现代变换器模型在文本分类方面提供了出色的结果，但大多数变换器模型受限于输入序列 longest length，因此在长文档分类问题上表现不佳。在这项研究中，我们探讨使用模型融合来解决长文档分类问题，并与知名的BERT和Longformer架构进行比较。
</details></li>
</ul>
<hr>
<h2 id="ChatSpot-Bootstrapping-Multimodal-LLMs-via-Precise-Referring-Instruction-Tuning"><a href="#ChatSpot-Bootstrapping-Multimodal-LLMs-via-Precise-Referring-Instruction-Tuning" class="headerlink" title="ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"></a>ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09474">http://arxiv.org/abs/2307.09474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, Xiangyu Zhang</li>
<li>for: 提高多模态大语言模型（MLLMs）的用户-人工智能互动性。</li>
<li>methods: 使用多种参照表示方式，如点和方正矩形作为参照提示，使MLLMs能够更加精细地关注特定区域。</li>
<li>results: ChatSpot模型在不同的互动方式和任务上表现出色，提供了更加灵活和抽象的互动体验。<details>
<summary>Abstract</summary>
Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.
</details>
<details>
<summary>摘要</summary>
人机合作性是多模态大语言模型（MLLM）的关键方面，但现有的端到端 MLLM 只允许用户通过语言指令与其交互，导致交互准确性和效率受限。在这项研究中，我们提出精确引用指令，使用多种参考表示such as points and boxes作为引用提示，以引导 MLLM 关注特定区域。这使得 MLLM 可以更加精准地进行交互。基于精确引用指令，我们提出 ChatSpot，一个综合的端到端多模态大语言模型，支持多种交互方式，包括鼠标点击、拖拽和绘制方块，以提供更加灵活和无缝的交互体验。此外，我们构建了一个多层次视力语言指令遵循 dataset，基于现有数据集和 GPT-4 生成。此外，我们设计了一系列评估任务，以评估区域识别和交互的效果。实验结果表明 ChatSpot 表现出色。
</details></li>
</ul>
<hr>
<h2 id="A-comparative-analysis-of-SRGAN-models"><a href="#A-comparative-analysis-of-SRGAN-models" class="headerlink" title="A comparative analysis of SRGAN models"></a>A comparative analysis of SRGAN models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09456">http://arxiv.org/abs/2307.09456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Rezapoor Nikroo, Ajinkya Deshmukh, Anantha Sharma, Adrian Tam, Kaarthik Kumar, Cleo Norris, Aditya Dangi</li>
<li>for: 这些模型是用于实现单图像超分辨的，以提高图像的分辨率和质量。</li>
<li>methods: 这些模型使用了多种state-of-the-art SRGAN模型，包括ESRGAN、Real-ESRGAN和EDSR，并使用了一个管道来评估这些模型的性能。</li>
<li>results: 研究发现，ESDR-BASE模型从huggingface库中的模型最高效，它在量化指标和主观视觉质量评估中都达到了最佳效果，并且具有最低的计算开销。 EDSR模型可以生成高PSNR和SSIM值的图像，并且可以通过Tesseract OCR引擎获得高质量的OCR结果。<details>
<summary>Abstract</summary>
In this study, we evaluate the performance of multiple state-of-the-art SRGAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline. Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead. Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine. These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications where high-quality visual fidelity is critical and optimized compute.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们评估了多种现代SRGAN（超分解生成对抗网络）模型，ESRGAN、Real-ESRGAN和EDSR，在一个实际图像降解管道中的表现。我们的结果表明，一些模型能够显著提高输入图像的分辨率，同时保持视觉质量，这被评估使用Tesseract OCR引擎。我们发现，来自huggingface的EDSR-BASE模型在对比其他候选模型的情况下，在量化指标和主观视觉质量评估中占据领先地位，同时具有最小的计算开销。specifically，EDSR生成的图像具有更高的峰峰信号噪声比（PSNR）和结构相似度指标（SSIM）值，并且能够返回高质量的OCR结果。这些发现表明，EDSR是一种稳定有效的单图超分解方法，可以用于应用场景需要高质量视觉准确性和优化计算。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-Outlier-Exposure-for-Out-of-Distribution-Detection-using-Pretrained-Transformers"><a href="#Pseudo-Outlier-Exposure-for-Out-of-Distribution-Detection-using-Pretrained-Transformers" class="headerlink" title="Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers"></a>Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09455">http://arxiv.org/abs/2307.09455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaeyoung Kim, Kyuheon Jung, Dongbin Na, Sion Jang, Eunbin Park, Sungchul Choi</li>
<li>for: 这篇论文的目的是为了帮助语言模型检测对应预设（In-distribution，ID）和非对应预设（Out-of-distribution，OOD）样本的区别，以避免语言模型预测错误的情况。</li>
<li>methods: 这篇论文使用了一种简单而有效的方法called Pseudo Outlier Exposure（POE），它可以将ID类别的 tokens顺序排序，从而生成一个伪类别的数据集，来训练弃置网络。POE方法不需要任何外部OOD数据，且可以与现有的Transformers架构一起使用。</li>
<li>results: 根据该论文的结果，POE方法与现有的方法相比，在多个文本分类 benchmark上具有相似的性能，且可以实现ID和OOD样本之间的区别。<details>
<summary>Abstract</summary>
For real-world language applications, detecting an out-of-distribution (OOD) sample is helpful to alert users or reject such unreliable samples. However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold. A rejection network can be trained with ID and diverse outlier samples to detect test OOD samples, but explicitly collecting auxiliary OOD datasets brings an additional burden for data collection. In this paper, we propose a simple but effective method called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD dataset by sequentially masking tokens related to ID classes. The surrogate OOD sample introduced by POE shows a similar representation to ID data, which is most effective in training a rejection network. Our method does not require any external OOD data and can be easily implemented within off-the-shelf Transformers. A comprehensive comparison with state-of-the-art algorithms demonstrates POE's competitiveness on several text classification benchmarks.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "real-world language applications" is translated as "实际语言应用" (shí jí yǔ yán bìng)* "in-distribution" is translated as "内部分布" (nèi bù zhāng)* "out-of-distribution" is translated as "外部分布" (wài bù zhāng)* "overconfident" is translated as "过于自信" (guò yú zì xìn)* "semantic representation" is translated as " semantics 表示" (xiàng yì bǐng dài)* "surrogate OOD dataset" is translated as "代理 OOD 数据集" (dài lǐ OOD shù jiàn)* "rejection network" is translated as "拒绝网络" (guà jiè wǎng luò)* "off-the-shelf Transformers" is translated as "Ready-to-use Transformers" (准备好的 Transformers)* "state-of-the-art algorithms" is translated as "当前最佳算法" (dāng qián zuì jiā algoritmos)
</details></li>
</ul>
<hr>
<h2 id="Let’s-ViCE-Mimicking-Human-Cognitive-Behavior-in-Image-Generation-Evaluation"><a href="#Let’s-ViCE-Mimicking-Human-Cognitive-Behavior-in-Image-Generation-Evaluation" class="headerlink" title="Let’s ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation"></a>Let’s ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09416">http://arxiv.org/abs/2307.09416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Betti, Jacopo Staiano, Lorenzo Baraldi, Lorenzo Baraldi, Rita Cucchiara, Nicu Sebe</li>
<li>for: 这篇论文主要是为了解决图像生成领域的自动评估问题，即用语言模型和视觉问答技术来评估生成的图像质量和与提示的一致性。</li>
<li>methods: 该论文提出了一种新的自动评估方法，即视觉概念评估（ViCE），它通过复制人类认知过程来评估图像质量。ViCE组合了大型语言模型和视觉问答技术，并通过问答系统来调查图像，以获得评估结果。</li>
<li>results: 据论文所示，ViCE方法可以准确地评估图像质量和与提示的一致性，并且可以与人类评估结果相匹配。这些结果表明，ViCE方法可以成为图像生成和图像目标编辑任务中的一种有用的自动评估工具。<details>
<summary>Abstract</summary>
Research in Image Generation has recently made significant progress, particularly boosted by the introduction of Vision-Language models which are able to produce high-quality visual content based on textual inputs. Despite ongoing advancements in terms of generation quality and realism, no methodical frameworks have been defined yet to quantitatively measure the quality of the generated content and the adherence with the prompted requests: so far, only human-based evaluations have been adopted for quality satisfaction and for comparing different generative methods. We introduce a novel automated method for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a generated/edited image and the corresponding prompt/instructions, with a process inspired by the human cognitive behaviour. ViCE combines the strengths of Large Language Models (LLMs) and Visual Question Answering (VQA) into a unified pipeline, aiming to replicate the human cognitive process in quality assessment. This method outlines visual concepts, formulates image-specific verification questions, utilizes the Q&A system to investigate the image, and scores the combined outcome. Although this brave new hypothesis of mimicking humans in the image evaluation process is in its preliminary assessment stage, results are promising and open the door to a new form of automatic evaluation which could have significant impact as the image generation or the image target editing tasks become more and more sophisticated.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a novel automated method for Visual Concept Evaluation (ViCE), which assesses the consistency between a generated/edited image and the corresponding prompt/instructions. This method is inspired by human cognitive behavior and combines the strengths of large language models (LLMs) and visual question answering (VQA) into a unified pipeline.The ViCE method outlines visual concepts, formulates image-specific verification questions, utilizes a Q&A system to investigate the image, and scores the combined outcome. Although this approach is in its preliminary assessment stage, the results are promising and open the door to a new form of automatic evaluation that could have significant impact as image generation and image target editing tasks become more sophisticated.
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Query-Reformulation-for-Conversational-Search"><a href="#Zero-shot-Query-Reformulation-for-Conversational-Search" class="headerlink" title="Zero-shot Query Reformulation for Conversational Search"></a>Zero-shot Query Reformulation for Conversational Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09384">http://arxiv.org/abs/2307.09384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dayuyang1999/zeqr">https://github.com/dayuyang1999/zeqr</a></li>
<li>paper_authors: Dayu Yang, Yue Zhang, Hui Fang</li>
<li>for: 提高 conversational search 中的搜寻效果，解决资料缺乏问题</li>
<li>methods: 提出了一个Zero-shot Query Reformulation（ZeQR）框架，利用语言模型解决 Raw 查询中的核心对比和排除问题，不需要对话搜寻数据进行超级vision</li>
<li>results: 透过实验表明，ZeQR 方法可以优于现有的基eline，提高查询意图理解和搜寻效果<details>
<summary>Abstract</summary>
As the popularity of voice assistants continues to surge, conversational search has gained increased attention in Information Retrieval. However, data sparsity issues in conversational search significantly hinder the progress of supervised conversational search methods. Consequently, researchers are focusing more on zero-shot conversational search approaches. Nevertheless, existing zero-shot methods face three primary limitations: they are not universally applicable to all retrievers, their effectiveness lacks sufficient explainability, and they struggle to resolve common conversational ambiguities caused by omission. To address these limitations, we introduce a novel Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based on previous dialogue contexts without requiring supervision from conversational search data. Specifically, our framework utilizes language models designed for machine reading comprehension tasks to explicitly resolve two common ambiguities: coreference and omission, in raw queries. In comparison to existing zero-shot methods, our approach is universally applicable to any retriever without additional adaptation or indexing. It also provides greater explainability and effectively enhances query intent understanding because ambiguities are explicitly and proactively resolved. Through extensive experiments on four TREC conversational datasets, we demonstrate the effectiveness of our method, which consistently outperforms state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
随着语音助手的普及，对话搜索在信息检索领域受到了越来越多的关注。然而，对话搜索数据中的数据缺乏问题对超级vised conversational search方法的进步带来了很大的障碍。因此，研究人员更加关注zero-shot conversational search方法。然而，现有的zero-shot方法存在三个主要的限制：它们不适用于所有搜索引擎，它们的效iveness缺乏足够的解释力，并且它们无法解决通常的对话歧义，导致 omission 问题。为了解决这些限制，我们提出了一种新的Zero-shot Query Reformulation（ZeQR）框架，该框架基于对话上下文来重新 reformulate 查询，无需对 conversational search 数据进行监督。具体来说，我们的框架利用特定设计 для机器阅读理解任务的语言模型来解决 raw 查询中的两个常见歧义：核心引用和 omission。与现有的zero-shot方法相比，我们的方法可以无需额外适应或索引化应用于任何搜索引擎。此外，我们的方法还提供了更高的解释力，因为歧义被明确地和 про动地解决。通过对四个 TREC 对话数据集进行广泛的实验，我们证明了我们的方法的效果， consistently 超过了当前的基eline。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/cs.CL_2023_07_19/" data-id="cloh7tqd0007l7b88744h1rhh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/cs.LG_2023_07_19/" class="article-date">
  <time datetime="2023-07-19T10:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/cs.LG_2023_07_19/">cs.LG - 2023-07-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Android-in-the-Wild-A-Large-Scale-Dataset-for-Android-Device-Control"><a href="#Android-in-the-Wild-A-Large-Scale-Dataset-for-Android-Device-Control" class="headerlink" title="Android in the Wild: A Large-Scale Dataset for Android Device Control"></a>Android in the Wild: A Large-Scale Dataset for Android Device Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10088">http://arxiv.org/abs/2307.10088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, Timothy Lillicrap</li>
<li>For: 本研究准备了一个大型设备控制 dataset，以便研究设备控制系统可以从人工语言指令中提取信息并直接控制设备用户界面。* Methods: 该 dataset 包含人类展示设备交互的示例，以及对应的自然语言指令。它包括 715k 个集成，30k 个唯一的指令，四个版本的 Android (v10-13)，八种设备类型 (Pixel 2 XL 到 Pixel 6) with varying 屏幕分辨率。它包含多步任务，需要语言和视觉上下文的含义理解。* Results: 该 dataset 提出了一个新的挑战：通过视觉上下文进行动作推理。而不是简单的 UI 元素基本动作，动作空间包括精准的手势（例如，横向滚动来操作轮播 widget）。作者们组织了该 dataset，以便检验设备控制系统的可靠性，即在新的任务描述、应用程序或平台版本下，系统如何表现。作者们开发了两个代理，并在 dataset 上进行了性能测试。dataset 可以在 <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research/tree/master/android_in_the_wild">https://github.com/google-research/google-research/tree/master/android_in_the_wild</a> 中下载。<details>
<summary>Abstract</summary>
There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls to operate carousel widgets). We organize our dataset to encourage robustness analysis of device-control systems, i.e., how well a system performs in the presence of new task descriptions, new applications, or new platform versions. We develop two agents and report performance across the dataset. The dataset is available at https://github.com/google-research/google-research/tree/master/android_in_the_wild.
</details>
<details>
<summary>摘要</summary>
“有一 growing interest 在设备控制系统中，能够理解人类自然语言指令并直接控制设备用户界面。我们提供了一个设备控制数据集，Android in the Wild (AITW)，该数据集规模比现有数据集有很大的提升。数据集包含人类设备交互示例，包括屏幕和操作，以及相应的自然语言指令。其包含30k个独特指令、四个版本的Android（v10-13）、八种设备（Pixel 2 XL 到 Pixel 6）以及不同屏幕分辨率。它包含多步任务，需要自然语言和视觉上下文的含义理解。这个数据集带来了一个新的挑战：通过视觉出现来推理操作。而不是简单的UI元素基于的操作，操作空间包括精准的手势（例如，横向滚动来操作轮播 widget）。我们将数据集分成了多个分区，以便鼓励设备控制系统的鲁棒性分析，即在新的任务描述、应用程序或平台版本下，系统的性能如何。我们开发了两个代理，并在数据集上进行了性能测试。数据集可以在https://github.com/google-research/google-research/tree/master/android_in_the_wild 上获取。”
</details></li>
</ul>
<hr>
<h2 id="A-Dual-Formulation-for-Probabilistic-Principal-Component-Analysis"><a href="#A-Dual-Formulation-for-Probabilistic-Principal-Component-Analysis" class="headerlink" title="A Dual Formulation for Probabilistic Principal Component Analysis"></a>A Dual Formulation for Probabilistic Principal Component Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10078">http://arxiv.org/abs/2307.10078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henri De Plaen, Johan A. K. Suykens</li>
<li>for: 本研究探讨了概率主成分分析在希尔бер特空间中的特性，并证明了优解在 dual space 中有一个表示。这使得我们可以开发一种生成框架 для核方法。</li>
<li>methods: 本研究使用了概率主成分分析和 dual space 的技术。</li>
<li>results: 研究证明了 Kernel Principal Component Analysis 是 Probabilistic Principal Component Analysis 的一种特例，并在一个小型示例和一个实际数据集上进行了示例。<details>
<summary>Abstract</summary>
In this paper, we characterize Probabilistic Principal Component Analysis in Hilbert spaces and demonstrate how the optimal solution admits a representation in dual space. This allows us to develop a generative framework for kernel methods. Furthermore, we show how it englobes Kernel Principal Component Analysis and illustrate its working on a toy and a real dataset.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 caracterize Probabilistic Principal Component Analysis在希尔бер特空间中，并证明优解 admit dual空间的表示。这 позволяет我们开发generative框架 дляkernel方法。此外，我们还示了它包含Kernel Principal Component Analysis，并在一个玩偶和一个真实数据集上验证其工作。Here's a breakdown of the translation:* "Probabilistic Principal Component Analysis" is translated as "概率主成分分析" (gōng cháng zhòng xīng yǐng yǎn jī).* "in Hilbert spaces" is translated as "在希尔бер特空间中" (zhōng yǐn hī lè bèi tiān xiàng).* "demonstrate how the optimal solution admits a representation in dual space" is translated as "证明优解 admit dual空间的表示" (zhèng mín yú jiě admit dì zhōng yǐng yǎn jī).* "This allows us to develop a generative framework for kernel methods" is translated as "这 позволяет我们开发generative框架 дляkernel方法" (zhèng mìng yú jī yī jiān kē yì).* "Furthermore, we show how it englobes Kernel Principal Component Analysis" is translated as "此外，我们还示了它包含Kernel Principal Component Analysis" (qí wài, wǒmen hái shì le ta bāng zhì Kernel Principal Component Analysis).* "and illustrate its working on a toy and a real dataset" is translated as "并在一个玩偶和一个真实数据集上验证其工作" (yī yī gè zhōng zhì yī yī zhèng shí yī jīn jī).
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Accuracy-Estimation-of-Deep-Visual-Models-using-Domain-Adaptive-Adversarial-Perturbation-without-Source-Samples"><a href="#Unsupervised-Accuracy-Estimation-of-Deep-Visual-Models-using-Domain-Adaptive-Adversarial-Perturbation-without-Source-Samples" class="headerlink" title="Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples"></a>Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10062">http://arxiv.org/abs/2307.10062</a></li>
<li>repo_url: None</li>
<li>paper_authors: JoonHo Lee, Jae Oh Woo, Hankyu Moon, Kwonho Lee</li>
<li>for: 这个论文是为了解决在投入深度视觉模型时可能出现的性能下降问题，具体是因为源和目标分布之间存在差异。</li>
<li>methods: 该论文提出了一种新的源无法场景下的模型准确率估计方法，不需要源数据和标签。具体来说，它使用pseudo-标签来估计目标域中模型的准确率，并采用了源自由适应算法来解决问题。</li>
<li>results: 该论文的实验结果表明，该方法可以在不需要源数据和标签的情况下，有效地Addressing the challenging distribution shift scenarios and outperform existing methods that require source data and labels for training.<details>
<summary>Abstract</summary>
Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions. Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices. Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data. We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms. Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. Our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training.
</details>
<details>
<summary>摘要</summary>
deploying deep visual models 可能会导致性能下降，因为目标分布和源分布之间存在差异。 多种方法利用标签的源数据估算目标领域的准确性，但是获取标签的源数据常常因调制解调器密性或服务器上的资源限制而受到限制。 我们的工作提出了一个新的框架，可以在无法存取源数据的情况下估算模型的准确性。 我们 investigates the feasibility of using pseudo-labels for accuracy estimation, and evolves this idea into adopting recent advances in source-free domain adaptation algorithms. 我们的方法量化源数据的假标签与目标领域的假标签函数之间的不一致率，并且运用适应式对抗扰动来减少因高理想共同假设风险而产生的错误假标签。 我们的提议的source-free框架有效地解决了困难的分布迁移情况，并且超越了需要源数据和标签进行训练的现有方法。
</details></li>
</ul>
<hr>
<h2 id="Accurate-deep-learning-sub-grid-scale-models-for-large-eddy-simulations"><a href="#Accurate-deep-learning-sub-grid-scale-models-for-large-eddy-simulations" class="headerlink" title="Accurate deep learning sub-grid scale models for large eddy simulations"></a>Accurate deep learning sub-grid scale models for large eddy simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10060">http://arxiv.org/abs/2307.10060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rikhi Bose, Arunabha M. Roy</li>
<li>for: 这两家模型是为了大涵观 simulations（LES）的目的而开发的。</li>
<li>methods: 这两家模型使用了深度学习（DL）算法，与传统的分析模型技术不同，可以生成高阶复杂的非线性关系。</li>
<li>results: 实验结果显示，使用了更简单的模型可以更好地学习特征，并且在不同的滤波器宽度和 Reynolds 数下预测 SGS 压力的预测性能更高。<details>
<summary>Abstract</summary>
We present two families of sub-grid scale (SGS) turbulence models developed for large-eddy simulation (LES) purposes. Their development required the formulation of physics-informed robust and efficient Deep Learning (DL) algorithms which, unlike state-of-the-art analytical modeling techniques can produce high-order complex non-linear relations between inputs and outputs. Explicit filtering of data from direct simulations of the canonical channel flow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 provided accurate data for training and testing. The two sets of models use different network architectures. One of the architectures uses tensor basis neural networks (TBNN) and embeds the simplified analytical model form of the general effective-viscosity hypothesis, thus incorporating the Galilean, rotational and reflectional invariances. The other architecture is that of a relatively simple network, that is able to incorporate the Galilean invariance only. However, this simpler architecture has better feature extraction capacity owing to its ability to establish relations between and extract information from cross-components of the integrity basis tensors and the SGS stresses. Both sets of models are used to predict the SGS stresses for feature datasets generated with different filter widths, and at different Reynolds numbers. It is shown that due to the simpler model's better feature learning capabilities, it outperforms the invariance embedded model in statistical performance metrics. In a priori tests, both sets of models provide similar levels of dissipation and backscatter. Based on the test results, both sets of models should be usable in a posteriori actual LESs.
</details>
<details>
<summary>摘要</summary>
我们介绍了两家子grid scale（SGS）湍流模型，用于大扰波 simulations（LES）的目的。它们的发展需要了具有物理学 Informed  Robust 和高效的深度学习（DL）算法，不同于现有的分析模型技术可以生成高阶复杂非线性关系。我们使用了对直接实验的标准涡流频率 $Re_\tau\approx 395$ 和 $Re_\tau\approx 590$ 的数据进行直接训练和测试。这两组模型使用了不同的网络架构。其中一个架构使用了tensor基 neural network（TBNN），并将简单的分析模型形式给适用到通用效率假设，因此包含了加利力、旋转和反射不变性。另一个架构则是一个较简单的网络，它能够将加利力不变性给适用，但是它的特征提取能力比较强，因为它可以从标本网络和SGS压力之间的交互关系中提取信息。这两组模型都用于预测SGS压力，并在不同的范围和 Reynolds 数下进行了预测。发现较简单的模型在统计性能指标下比具有不变性的模型表现更好。在先前的测试中，这两组模型都提供了相似的净减杂和反射。根据测试结果，这两组模型都可以在 posteriori 实际的LES中使用。
</details></li>
</ul>
<hr>
<h2 id="Convergence-Guarantees-for-Stochastic-Subgradient-Methods-in-Nonsmooth-Nonconvex-Optimization"><a href="#Convergence-Guarantees-for-Stochastic-Subgradient-Methods-in-Nonsmooth-Nonconvex-Optimization" class="headerlink" title="Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization"></a>Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10053">http://arxiv.org/abs/2307.10053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xnchxy/GeneralSGD">https://github.com/xnchxy/GeneralSGD</a></li>
<li>paper_authors: Nachuan Xiao, Xiaoyin Hu, Kim-Chuan Toh</li>
<li>for:  investigate the convergence properties of stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions.</li>
<li>methods:  develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, and prove the global convergence of the proposed framework in both single-timescale and two-timescale cases.</li>
<li>results:  prove the convergence properties of SGD-type methods based on the proposed framework, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD, and demonstrate the high efficiency of these methods through preliminary numerical experiments.<details>
<summary>Abstract</summary>
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preliminary numerical experiments demonstrate the high efficiency of our analyzed SGD-type methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了束ochastic gradient descent（SGD）方法和其变体在训练使用nonsmooth activation function的神经网络时的收敛性质。我们提出了一个新的框架，将步长分配给更新势能项和变量的两个不同时间尺度。在某些轻量条件下，我们证明了我们提议的框架在单时间尺度和双时间尺度情况下的全球收敛性。我们表明，我们的提议框架包括许多已知SGD-type方法，包括坚重球SGD、SignSGD、Lion、normalized SGD和clipped SGD。此外，当目标函数采用finite-sum形式时，我们证明了这些SGD-type方法的收敛性基于我们的提议框架。具体来说，我们证明这些SGD-type方法在 randomly chosen步长和初始点下，可以找到目标函数的clarke静点。初步的数值实验表明我们分析的SGD-type方法具有高效性。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Reliability-When-Different-Features-Matter-in-Different-Contexts"><a href="#Contextual-Reliability-When-Different-Features-Matter-in-Different-Contexts" class="headerlink" title="Contextual Reliability: When Different Features Matter in Different Contexts"></a>Contextual Reliability: When Different Features Matter in Different Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10026">http://arxiv.org/abs/2307.10026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaurav Ghosal, Amrith Setlur, Daniel S. Brown, Anca D. Dragan, Aditi Raghunathan</li>
<li>for: 本研究旨在提高深度神经网络的非恶性性能，通过考虑上下文相关的可靠性来解决深度神经网络过度依赖偶合关系的问题。</li>
<li>methods: 本研究提出了一种两阶段框架called Explicit Non-spurious feature Prediction (ENP)，首先在给定上下文中标识合适的特征，然后使用这些特征来训练模型。</li>
<li>results: 本研究的理论和实验结果表明，相比现有方法，ENP框架可以提高非恶性性能，并提供了新的上下文相关可靠性的标准准measure。<details>
<summary>Abstract</summary>
Deep neural networks often fail catastrophically by relying on spurious correlations. Most prior work assumes a clear dichotomy into spurious and reliable features; however, this is often unrealistic. For example, most of the time we do not want an autonomous car to simply copy the speed of surrounding cars -- we don't want our car to run a red light if a neighboring car does so. However, we cannot simply enforce invariance to next-lane speed, since it could provide valuable information about an unobservable pedestrian at a crosswalk. Thus, universally ignoring features that are sometimes (but not always) reliable can lead to non-robust performance. We formalize a new setting called contextual reliability which accounts for the fact that the "right" features to use may vary depending on the context. We propose and analyze a two-stage framework called Explicit Non-spurious feature Prediction (ENP) which first identifies the relevant features to use for a given context, then trains a model to rely exclusively on these features. Our work theoretically and empirically demonstrates the advantages of ENP over existing methods and provides new benchmarks for contextual reliability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Europepolls-A-Dataset-of-Country-Level-Opinion-Polling-Data-for-the-European-Union-and-the-UK"><a href="#Europepolls-A-Dataset-of-Country-Level-Opinion-Polling-Data-for-the-European-Union-and-the-UK" class="headerlink" title="Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK"></a>Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10022">http://arxiv.org/abs/2307.10022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/konstantinos-p/europepolls">https://github.com/konstantinos-p/europepolls</a></li>
<li>paper_authors: Konstantinos Pitas</li>
<li>for:  fills a gap in available opinion polling data for the European Union and the UK, providing a large and open dataset for researchers to study voting behavior and multimodal data.</li>
<li>methods:  uses Wikipedia data and the pandas library to gather and preprocess the data, making it available in both raw and preprocessed formats.</li>
<li>results:  enables researchers to study complex interactions between multimodal data and voting behavior, with the potential for new insights and discoveries using recent advances in LLMs and deep learning.<details>
<summary>Abstract</summary>
I propose an open dataset of country-level historical opinion polling data for the European Union and the UK. The dataset aims to fill a gap in available opinion polling data for the European Union. Some existing datasets are restricted to the past five years, limiting research opportunities. At the same time, some larger proprietary datasets exist but are available only in a visual preprocessed time series format. Finally, while other large datasets for individual countries might exist, these could be inaccessible due to language barriers. The data was gathered from Wikipedia, and preprocessed using the pandas library. Both the raw and the preprocessed data are in the .csv format. I hope that given the recent advances in LLMs and deep learning in general, this large dataset will enable researchers to uncover complex interactions between multimodal data (news articles, economic indicators, social media) and voting behavior. The raw data, the preprocessed data, and the preprocessing scripts are available on GitHub.
</details>
<details>
<summary>摘要</summary>
我提议开放的国家级历史民意调查数据集，涵盖欧盟和英国。这个数据集的目标是填补现有的欧盟民意调查数据的空白，一些现有的数据集只有过去五年的数据，限制了研究机会。同时，一些大型专有数据集存在，但只能在可视化预处理的时间序列格式下获得。此外，各国个别的大型数据集可能存在，但可能因语言障碍而不可达。这些数据来自Wikipedia，使用pandas库进行预处理。原始数据和预处理后的数据均在.csv格式下可用。我希望通过最近的人工智能技术和深度学习的发展，这个大型数据集将帮助研究人员发现多Modal数据（新闻文章、经济指标、社交媒体）和选举行为之间的复杂互动。原始数据、预处理后的数据和预处理脚本都可在GitHub上下载。
</details></li>
</ul>
<hr>
<h2 id="TbExplain-A-Text-based-Explanation-Method-for-Scene-Classification-Models-with-the-Statistical-Prediction-Correction"><a href="#TbExplain-A-Text-based-Explanation-Method-for-Scene-Classification-Models-with-the-Statistical-Prediction-Correction" class="headerlink" title="TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction"></a>TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10003">http://arxiv.org/abs/2307.10003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Aminimehr, Pouya Khani, Amirali Molaei, Amirmohammad Kazemeini, Erik Cambria</li>
<li>for: 这个研究旨在提高透明化人工智能（XAI）模型的解释性，使得不熟悉机器学习的用户可以更好地理解模型的预测结果。</li>
<li>methods: 该研究提出了一种名为TbExplain的框架，使用XAI技术和预训练的对象检测器来提供场景分类模型的文本解释。此外，TbExplain还包括一种新的方法来修正预测和文本解释基于输入图像中对象的统计学分布。</li>
<li>results: 对TbExplain在场景分类 dataset 上进行质量和质量测试，发现它可以提高场景分类精度，并且对于不可靠的预测，可以提供可靠的文本解释。<details>
<summary>Abstract</summary>
The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually explain them based on the statistics of objects in the input image when the initial prediction is unreliable. To assess the trustworthiness and validity of the text-based explanations, we conducted a qualitative experiment, and the findings indicated that these explanations are sufficiently reliable. Furthermore, our quantitative and qualitative experiments on TbExplain with scene classification datasets reveal an improvement in classification accuracy over ResNet variants.
</details>
<details>
<summary>摘要</summary>
黑obox机器学习模型的解释可视化（XAI）领域目标是提高模型预测结果的解释性。建立基于输入特征重要性值的热图是一种流行的方法来解释模型在生成预测结果时使用的下面函数。然而，热图并不完全无瑕疵，例如，非专业用户可能无法完全理解热图的逻辑（在热图中显示相关像素的不同颜色或强度）。此外，输入图像中对模型预测结果的重要对象和区域也经常不充分分化。在这篇论文中，我们提出了一个名为TbExplain的框架，该框架使用XAI技术和预训练的对象检测器来提供场景分类模型的文本基于解释。此外，TbExplain还包括一种新的方法来根据输入图像中对象的统计特征来修正预测结果和文本解释，当初始预测结果不可靠时。为评估文本基于解释的可信度和有效性，我们进行了一次质量性实验，实验结果表明这些解释具有足够的可靠性。此外，我们对TbExplain与场景分类数据集进行了量化和质量性实验，发现TbExplain比ResNet变体增加了分类精度。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Disentanglement-on-Pruning-Neural-Networks"><a href="#Impact-of-Disentanglement-on-Pruning-Neural-Networks" class="headerlink" title="Impact of Disentanglement on Pruning Neural Networks"></a>Impact of Disentanglement on Pruning Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09994">http://arxiv.org/abs/2307.09994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carl Shneider, Peyman Rostami, Anis Kacem, Nilotpal Sinha, Abd El Rahman Shabayek, Djamila Aouada</li>
<li>for: 这个研究目的是为了实现对于实际世界的任务特定目标进行深度学习神经网络在边缘设备上部署，因此需要对神经网络进行压缩，以减少它们的内存占用、电力消耗和延迟。</li>
<li>methods: 这个研究使用了 beta-VAE 框架，与标准的条件进行剪裁，以 investigate 对于训练任务的条件下，强制神经网络学习分离的表现如何影响剪裁过程。</li>
<li>results: 这个研究在 MNIST 和 CIFAR10  datasets 上进行实验，探讨了分离挑战，并提出了未来研究的路径。<details>
<summary>Abstract</summary>
Deploying deep learning neural networks on edge devices, to accomplish task specific objectives in the real-world, requires a reduction in their memory footprint, power consumption, and latency. This can be realized via efficient model compression. Disentangled latent representations produced by variational autoencoder (VAE) networks are a promising approach for achieving model compression because they mainly retain task-specific information, discarding useless information for the task at hand. We make use of the Beta-VAE framework combined with a standard criterion for pruning to investigate the impact of forcing the network to learn disentangled representations on the pruning process for the task of classification. In particular, we perform experiments on MNIST and CIFAR10 datasets, examine disentanglement challenges, and propose a path forward for future works.
</details>
<details>
<summary>摘要</summary>
deploying deep learning neural networks on edge devices, to accomplish task specific objectives in the real world, requires a reduction in their memory footprint, power consumption, and latency. this can be realized via efficient model compression. disentangled latent representations produced by variational autoencoder (VAE) networks are a promising approach for achieving model compression because they mainly retain task-specific information, discarding useless information for the task at hand. we make use of the beta-vae framework combined with a standard criterion for pruning to investigate the impact of forcing the network to learn disentangled representations on the pruning process for the task of classification. in particular, we perform experiments on mnist and cifar10 datasets, examine disentanglement challenges, and propose a path forward for future works.
</details></li>
</ul>
<hr>
<h2 id="UniMatch-A-Unified-User-Item-Matching-Framework-for-the-Multi-purpose-Merchant-Marketing"><a href="#UniMatch-A-Unified-User-Item-Matching-Framework-for-the-Multi-purpose-Merchant-Marketing" class="headerlink" title="UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing"></a>UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09989">http://arxiv.org/abs/2307.09989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qifang Zhao, Tianyu Li, Meng Du, Yu Jiang, Qinghui Sun, Zhongyao Wang, Hong Liu, Huan Xu</li>
<li>for: 降低云服务中private域营销时的机器学习模型成本</li>
<li>methods: 提出了一个统一的用户项目匹配框架，同时进行用户定向和项目推荐，只需一个模型</li>
<li>results: 实验表明，该框架可以同时实现用户定向和项目推荐，并且比 estado del arte 方法更高效，同时减少了计算资源和日常维护成本<details>
<summary>Abstract</summary>
When doing private domain marketing with cloud services, the merchants usually have to purchase different machine learning models for the multiple marketing purposes, leading to a very high cost. We present a unified user-item matching framework to simultaneously conduct item recommendation and user targeting with just one model. We empirically demonstrate that the above concurrent modeling is viable via modeling the user-item interaction matrix with the multinomial distribution, and propose a bidirectional bias-corrected NCE loss for the implementation. The proposed loss function guides the model to learn the user-item joint probability $p(u,i)$ instead of the conditional probability $p(i|u)$ or $p(u|i)$ through correcting both the users and items' biases caused by the in-batch negative sampling. In addition, our framework is model-agnostic enabling a flexible adaptation of different model architectures. Extensive experiments demonstrate that our framework results in significant performance gains in comparison with the state-of-the-art methods, with greatly reduced cost on computing resources and daily maintenance.
</details>
<details>
<summary>摘要</summary>
(Note: Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. It is different from Traditional Chinese, which is used in Taiwan and other countries. The translation is written in Simplified Chinese.)
</details></li>
</ul>
<hr>
<h2 id="TinyTrain-Deep-Neural-Network-Training-at-the-Extreme-Edge"><a href="#TinyTrain-Deep-Neural-Network-Training-at-the-Extreme-Edge" class="headerlink" title="TinyTrain: Deep Neural Network Training at the Extreme Edge"></a>TinyTrain: Deep Neural Network Training at the Extreme Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09988">http://arxiv.org/abs/2307.09988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young D. Kwon, Rui Li, Stylianos I. Venieris, Jagmohan Chauhan, Nicholas D. Lane, Cecilia Mascolo</li>
<li>for: 这个论文的目的是提出一个在设备上训练的方法，以提高用户化和隐私。</li>
<li>methods: 这个方法使用选择性更新部分模型，以及特别是处理数据缺乏的问题。它还引入了一个任务适应的迭代更新方法，可以在不同的任务下灵活地选择层&#x2F;通道，以jointly captures用户数据、内存和计算能力。</li>
<li>results: 相比于vanilla fine-tuning整个网络，这个方法可以提高精度 by 3.6-5.0%，并同时降低了传递内存和计算成本，分别降低了2,286倍和7.68倍。这个方法可以在实际的edge设备上进行9.5倍 faster和3.5倍更能效的训练，并且仅需1 MB的内存空间，比SOTA方法小得多。<details>
<summary>Abstract</summary>
On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\geq$10\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0\% in accuracy, while reducing the backward-pass memory and computation cost by up to 2,286$\times$ and 7.68$\times$, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5$\times$ faster and 3.5$\times$ more energy-efficient training over status-quo approaches, and 2.8$\times$ smaller memory footprint than SOTA approaches, while remaining within the 1 MB memory envelope of MCU-grade platforms.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified ChineseOn-device training is essential for user personalization and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labeled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g., a few hours), or induce substantial accuracy loss (≥10%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0% in accuracy, while reducing the backward-pass memory and computation cost by up to 2,286 times and 7.68 times, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5 times faster and 3.5 times more energy-efficient training over status-quo approaches, and 2.8 times smaller memory footprint than SOTA approaches, while remaining within the 1 MB memory envelope of MCU-grade platforms.Translation:在设备上训练是用户个性化和隐私的关键。随着物联网设备和微控制器单元（MCU）的普及，这种任务变得更加挑战，因为它们具有有限的存储和计算资源，以及有限的用户数据标注。然而，先前的工作忽视了数据稀缺问题，或者需要过长的训练时间（例如几个小时），或者导致减少精度（≥10%）。我们提出了TinyTrain，一种在设备上训练的方法，可以快速地减少训练时间，并且直接处理数据稀缺问题。TinyTrain引入了任务适应的稀有更新方法，可以在多bjective criterion 中选择层/通道，该 criterion jointly 捕捉用户数据、内存和计算能力等因素。这使得TinyTrain在未经训练任务上达到高精度，同时减少了反向传播的内存和计算成本。对于广泛使用的边缘设备，TinyTrain实现了9.5倍快速的训练，3.5倍更高的能效性，并且内存占用率比SOTAapproaches 小2.8倍。同时，TinyTrain仍然保持在MCU-grade平台上的1 MB内存范围内。
</details></li>
</ul>
<hr>
<h2 id="Learner-Referral-for-Cost-Effective-Federated-Learning-Over-Hierarchical-IoT-Networks"><a href="#Learner-Referral-for-Cost-Effective-Federated-Learning-Over-Hierarchical-IoT-Networks" class="headerlink" title="Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks"></a>Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09977">http://arxiv.org/abs/2307.09977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulan Gao, Ziqiang Ye, Yue Xiao, Wei Xiang</li>
<li>for: 这篇论文的目的是提出一种基于联合学习者推荐和本地模型准确优化的 Federated Learning（FL）方法，以Address data privacy concerns和提高FL在分布式网络中的可扩展性和可靠性。</li>
<li>methods: 这篇论文使用了联合学习者推荐、通信和计算资源调度、本地模型准确优化等方法来Minimize the cost incurred by the worst-case participant and ensure the long-term fairness of FL in hierarchical Internet of Things (HieIoT) networks。</li>
<li>results: numerical simulations and experimental results on the MNIST&#x2F;CIFAR-10 datasets demonstrate that our proposed LRef-FedCS approach could achieve a good balance between pursuing high global accuracy and reducing cost。<details>
<summary>Abstract</summary>
The paradigm of federated learning (FL) to address data privacy concerns by locally training parameters on resource-constrained clients in a distributed manner has garnered significant attention. Nonetheless, FL is not applicable when not all clients within the coverage of the FL server are registered with the FL network. To bridge this gap, this paper proposes joint learner referral aided federated client selection (LRef-FedCS), along with communications and computing resource scheduling, and local model accuracy optimization (LMAO) methods. These methods are designed to minimize the cost incurred by the worst-case participant and ensure the long-term fairness of FL in hierarchical Internet of Things (HieIoT) networks. Utilizing the Lyapunov optimization technique, we reformulate the original problem into a stepwise joint optimization problem (JOP). Subsequently, to tackle the mixed-integer non-convex JOP, we separatively and iteratively address LRef-FedCS and LMAO through the centralized method and self-adaptive global best harmony search (SGHS) algorithm, respectively. To enhance scalability, we further propose a distributed LRef-FedCS approach based on a matching game to replace the centralized method described above. Numerical simulations and experimental results on the MNIST/CIFAR-10 datasets demonstrate that our proposed LRef-FedCS approach could achieve a good balance between pursuing high global accuracy and reducing cost.
</details>
<details>
<summary>摘要</summary>
《联邦学习（FL）》的概念，通过在分布式环境中本地训练参数，以解决数据隐私问题，已经吸引了广泛的关注。然而，FL不适用于所有FL服务器的覆盖区域内的所有客户端没有注册到FL网络。为了bridging这个差距，本文提出了联合学生推荐帮助 federated client selection（LRef-FedCS）、通信和计算资源调度，以及本地模型准确优化（LMAO）方法。这些方法的目的是 minimize the cost incurred by the worst-case participant and ensure the long-term fairness of FL in hierarchical Internet of Things（HieIoT） networks。通过利用Lyapunov优化技术，我们将原始问题转换为stepwise joint optimization problem（JOP）。然后，为了解决杂合Integer non-convex JOP，我们分别和iteratively处理LRef-FedCS和LMAO通过中央化方法和自适应全球最佳匹配搜索（SGHS）算法，分别进行处理。为了提高扩展性，我们还提出了基于匹配游戏的分布式LRef-FedCS方法来取代中央化方法。numerical simulations and experimental results on the MNIST/CIFAR-10 datasets show that our proposed LRef-FedCS approach can achieve a good balance between pursuing high global accuracy and reducing cost.
</details></li>
</ul>
<hr>
<h2 id="Towards-green-AI-based-software-systems-an-architecture-centric-approach-GAISSA"><a href="#Towards-green-AI-based-software-systems-an-architecture-centric-approach-GAISSA" class="headerlink" title="Towards green AI-based software systems: an architecture-centric approach (GAISSA)"></a>Towards green AI-based software systems: an architecture-centric approach (GAISSA)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09964">http://arxiv.org/abs/2307.09964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silverio Martínez-Fernández, Xavier Franch, Francisco Durán</li>
<li>for: 这个研究项目的目的是提供数据科学家和软件工程师用于模型和开发绿色人工智能系统的工具支持方法。</li>
<li>methods: 该项目使用了architecture-centric方法，以帮助数据科学家和软件工程师在设计和实现绿色人工智能系统时更加高效。</li>
<li>results: current research results indicate that the GAISSA project has the potential to achieve its objectives and provide effective methods for developing green AI-based systems.<details>
<summary>Abstract</summary>
Nowadays, AI-based systems have achieved outstanding results and have outperformed humans in different domains. However, the processes of training AI models and inferring from them require high computational resources, which pose a significant challenge in the current energy efficiency societal demand. To cope with this challenge, this research project paper describes the main vision, goals, and expected outcomes of the GAISSA project. The GAISSA project aims at providing data scientists and software engineers tool-supported, architecture-centric methods for the modelling and development of green AI-based systems. Although the project is in an initial stage, we describe the current research results, which illustrate the potential to achieve GAISSA objectives.
</details>
<details>
<summary>摘要</summary>
现在，基于人工智能的系统已经取得了杰出的成绩，在不同的领域超越了人类。但是，训练AI模型和从其进行推理需要高度的计算资源，这对当今能效社会需求 pose 了 significiant 挑战。为了应对这个挑战，本研究项目论文描述了GAISSA项目的主要视野、目标和预期结果。GAISSA项目的目标是为数据科学家和软件工程师提供工具支持、建筑中心的方法，用于开发绿色基于人工智能的系统。虽然项目还在初期阶段，我们介绍了当前的研究成果，这些成果表明GAISSA目标的可能性。
</details></li>
</ul>
<hr>
<h2 id="XSkill-Cross-Embodiment-Skill-Discovery"><a href="#XSkill-Cross-Embodiment-Skill-Discovery" class="headerlink" title="XSkill: Cross Embodiment Skill Discovery"></a>XSkill: Cross Embodiment Skill Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09955">http://arxiv.org/abs/2307.09955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, Shuran Song</li>
<li>for: 本研究旨在实现从人工智能影像中提取可重用的机器人操作技能，并将其应用于实际世界中。</li>
<li>methods: 本研究使用了一个叫做XSkill的实践学习框架，它可以从无标注的人工和机器人操作影像中独立获取技能标本，并将其转换为机器人动作使用条件散乱策略。</li>
<li>results: 实验结果显示，XSkill可以将获取的技能标本转换为机器人动作，并且可以将学习的技能组合以实现未见过的任务，实现了更加一般和可扩展的实践学习框架。<details>
<summary>Abstract</summary>
Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performance of XSkill is best understood from the anonymous website: https://xskillcorl.github.io.
</details>
<details>
<summary>摘要</summary>
人类示例视频是机器人学习的广泛可用数据源，也是一种直观的用户界面，可以直接表达机器人需要的行为。然而，直接从无结构的人类视频中提取可重用的机器人操作技能是具有大embodiment差异和未观察行为参数的挑战。为bridging这个embodiment gap，本文提出了XSkill，一个仿学学习框架，它可以：1. 基于无标注的人类和机器人操作视频，纯粹地发现跨embodiment的表示 called skill prototypes;2. 使用条件扩散策略将表示转移到机器人动作;3. 使用人类提示视频完成未seen任务。我们在 simulated和实际环境中进行了实验，结果表明 XSkill可以快速 Transfer skill和组合未seen任务，从而实现更一般和可扩展的仿学学习框架。XSkill的性能可以通过无名网站https://xskillcorl.github.io进行了解。
</details></li>
</ul>
<hr>
<h2 id="Impatient-Bandits-Optimizing-Recommendations-for-the-Long-Term-Without-Delay"><a href="#Impatient-Bandits-Optimizing-Recommendations-for-the-Long-Term-Without-Delay" class="headerlink" title="Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay"></a>Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09943">http://arxiv.org/abs/2307.09943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spotify-research/impatient-bandits">https://github.com/spotify-research/impatient-bandits</a></li>
<li>paper_authors: Thomas M. McDonald, Lucas Maystre, Mounia Lalmas, Daniel Russo, Kamil Ciosek</li>
<li>for: 这个论文目标是提高用户在线平台上的满意度，通过研究内容探索任务，以及将这个任务формализова为多重钩盘问题。</li>
<li>methods: 作者使用了一种 bayesian 筛选器来 integrate full observations和partial outcomes，并开发了一种钩盘算法来快速Identify用户长期满意的内容。</li>
<li>results: 实验表明，作者的方法可以substantially improve performance compared to approaches that either optimize for short-term proxies, or wait for the long-term outcome to be fully realized。<details>
<summary>Abstract</summary>
Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify content aligned with long-term success by carefully balancing exploration and exploitation. We apply our approach to a podcast recommendation problem, where we seek to identify shows that users engage with repeatedly over two months. We empirically validate that our approach results in substantially better performance compared to approaches that either optimize for short-term proxies, or wait for the long-term outcome to be fully realized.
</details>
<details>
<summary>摘要</summary>
优化器系统在在线平台上是一种普遍存在的特性。现在，它们更加专注于提高用户的长期满意度。在这个 контексте，我们研究一种内容探索任务，我们将其正式定义为带延迟奖励的多臂投机问题。我们发现，选择学习信号的决策存在一定的负担：等待获得完整奖励可能需要几周时间，从而妨碍学习的速度，而且测量短期代理奖励只能够做出不准确的长期目标预测。为了解决这个挑战，我们采取了两个步骤：首先，我们开发了一个带延迟奖励的预测模型。这个模型把所有已有信息都合并到一起，并使用某种权重来衡量奖励的可能性。其次，我们设计了一种投机算法，该算法利用这个新的预测模型来快速地标识与长期成功相符的内容。我们在一个 Podcast 推荐问题中应用了这种方法，我们的目标是在两个月内找到用户重复听众的播客。我们通过实验证明，我们的方法可以与等待完整奖励或仅仅优化短期代理奖励相比，显著提高表现。
</details></li>
</ul>
<hr>
<h2 id="TREEMENT-Interpretable-Patient-Trial-Matching-via-Personalized-Dynamic-Tree-Based-Memory-Network"><a href="#TREEMENT-Interpretable-Patient-Trial-Matching-via-Personalized-Dynamic-Tree-Based-Memory-Network" class="headerlink" title="TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network"></a>TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09942">http://arxiv.org/abs/2307.09942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Theodorou, Cao Xiao, Jimeng Sun</li>
<li>for: 快速和效率地招募临床试验参与者，提高临床试验效率。</li>
<li>methods: 使用机器学习模型自动匹配患者与临床试验，基于患者长期电子医疗记录（EHR）数据和临床试验招募要求。</li>
<li>results: 提出一种名为TREEMENT的个性化动态树型记忆网络模型，可以提供精度和可读性的患者试验匹配。对现有模型进行比较，TREEMENT在实际数据集上表现出7%的错误减少和优于最佳基eline的试验级匹配能力。此外，TREEMENT还可以提供良好的可读性，使模型结果更易于采用。<details>
<summary>Abstract</summary>
Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment. In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHR) data and eligibility criteria of clinical trials. However, they either depend on trial-specific expert rules that cannot expand to other trials or perform matching at a very general level with a black-box model where the lack of interpretability makes the model results difficult to be adopted.   To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network model named TREEMENT. It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to offer a granular level of alignment for improved performance and interpretability. We evaluated TREEMENT against existing models on real-world datasets and demonstrated that TREEMENT outperforms the best baseline by 7% in terms of error reduction in criteria-level matching and achieves state-of-the-art results in its trial-level matching ability. Furthermore, we also show TREEMENT can offer good interpretability to make the model results easier for adoption.
</details>
<details>
<summary>摘要</summary>
临床试验是药品开发的关键一环，但它们经常面临临床招募的成本和效率问题。在最近几年，机器学习模型被提出来加速临床招募，通过自动将病人与临床试验相匹配，基于患者的长期电子医疗记录（EHR）数据和临床试验的参与条件。但是，这些模型可能会依赖于专门为某个试验而设计的规则，无法扩展到其他试验，或者使用黑obox模型，导致模型结果难以被采纳。为了提供准确和可解释的病人试验匹配，我们介绍了一种个性化动态树型记忆网络模型，名为TREEMENT。它利用层次的临床 ontology 扩展个性化病人表示，然后使用注意力寻找查询学习自适应搜索来提供精细水平的匹配，从而提高性能和可解释性。我们对实际数据进行了评估，并证明TREEMENT在指标水平匹配方面比最佳参考模型下降7%，并在试验水平匹配能力方面达到了当前最佳 результа。此外，我们还证明TREEMENT可以提供好的可解释性，使得模型结果更容易采纳。
</details></li>
</ul>
<hr>
<h2 id="Spuriosity-Didn’t-Kill-the-Classifier-Using-Invariant-Predictions-to-Harness-Spurious-Features"><a href="#Spuriosity-Didn’t-Kill-the-Classifier-Using-Invariant-Predictions-to-Harness-Spurious-Features" class="headerlink" title="Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features"></a>Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09933">http://arxiv.org/abs/2307.09933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cian Eastwood, Shashank Singh, Andrei Liviu Nicolicioiu, Marin Vlastelica, Julius von Kügelgen, Bernhard Schölkopf</li>
<li>For: The paper is written for improving the performance of machine learning models on out-of-distribution data.* Methods: The paper proposes a method called Stable Feature Boosting (SFB) that learns to use unstable features in the test domain without labels. SFB consists of two steps: (i) learning a predictor that separates stable and conditionally-independent unstable features, and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain.* Results: The paper proves that SFB can learn an asymptotically-optimal predictor without test-domain labels, and demonstrates the effectiveness of SFB on real and synthetic data.<details>
<summary>Abstract</summary>
To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.
</details>
<details>
<summary>摘要</summary>
We propose Stable Feature Boosting (SFB), an algorithm that learns to separate stable and conditionally-independent unstable features, and uses the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.In summary, our approach leverages pseudo-labels based on stable features to guide the use of unstable features in the test domain, without requiring test-domain labels. This can lead to improved performance on out-of-distribution data, and our algorithm is proven to be asymptotically optimal.
</details></li>
</ul>
<hr>
<h2 id="DISA-DIfferentiable-Similarity-Approximation-for-Universal-Multimodal-Registration"><a href="#DISA-DIfferentiable-Similarity-Approximation-for-Universal-Multimodal-Registration" class="headerlink" title="DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration"></a>DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09931">http://arxiv.org/abs/2307.09931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imfusiongmbh/disa-universal-multimodal-registration">https://github.com/imfusiongmbh/disa-universal-multimodal-registration</a></li>
<li>paper_authors: Matteo Ronchetti, Wolfgang Wein, Nassir Navab, Oliver Zettinig, Raphael Prevost</li>
<li>for: 用于多Modal imaging registration的挑战性问题，以便实现多种图像导航程序。</li>
<li>methods: 使用小型卷积神经网络（CNN）来创建表达力强的跨Modal描述符，以便快速进行可变的全局对 align。</li>
<li>results: 在三个不同的数据集上进行实验，结果表明我们的方法可以快速地执行，并且可以在临床设置中直接应用，无需特殊 retraining。<details>
<summary>Abstract</summary>
Multimodal image registration is a challenging but essential step for numerous image-guided procedures. Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities. Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings. We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration. We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data. Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one. Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining. We make our training code and data publicly available.
</details>
<details>
<summary>摘要</summary>
多modal图像匹配是一项复杂但必要的步骤，用于许多图像引导过程。大多数注册算法利用计算复杂，通常不可导的相似度指标来处理不同模式下的生物结构的外观差异。现代机器学习基于方法受限于特定的解剖学-模式组合，并不能泛化到新的设定。我们提出了一个通用框架，用于生成表达性较高的跨模式描述符，以实现快速的可变截形注册。我们实现了这一点通过将现有的度量简化为点积在一个小型卷积神经网络（CNN）的特征空间中，该网络自然地可导，可以在不注册数据下进行训练。我们的方法比局部补充度量快速得多，可以直接在临床设置中应用，只需替换相似度度量即可。我们在三个不同的数据集上进行了实验，并证明我们的方法可以覆盖训练数据以外的广泛范围，包括未看到的解剖结构和模式对。我们将训练代码和数据公开发布。
</details></li>
</ul>
<hr>
<h2 id="TimeTuner-Diagnosing-Time-Representations-for-Time-Series-Forecasting-with-Counterfactual-Explanations"><a href="#TimeTuner-Diagnosing-Time-Representations-for-Time-Series-Forecasting-with-Counterfactual-Explanations" class="headerlink" title="TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations"></a>TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09916">http://arxiv.org/abs/2307.09916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/catherinehao/timetuner">https://github.com/catherinehao/timetuner</a></li>
<li>paper_authors: Jianing Hao, Qing Shi, Yilin Ye, Wei Zeng</li>
<li>for: 本研究旨在帮助分析者理解时间序列表示之间的关系，以及如何通过特征工程来提高模型预测性能。</li>
<li>methods: 本研究使用了一种新的视觉分析框架，named TimeTuner，它可以帮助分析者理解模型行为与时间序列表示之间的关系，并提供多种可视化视图来描述模型表现。</li>
<li>results: 研究表明，TimeTuner可以帮助分析者更好地理解时间序列表示的特征，并提高模型预测性能。在实验中，TimeTuner使用了两种变换方法（平滑和采样），并在实际时间序列预测 task 中进行了应用。<details>
<summary>Abstract</summary>
Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features and model predictions. Next, we design multiple coordinated views including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance. We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.
</details>
<details>
<summary>摘要</summary>
TimeTuner 的核心技术包括以下两阶段方法：1. 首先，我们利用 counterfactual 解释来连接时间序列表示、多Variable 特征和预测的关系。2. 接下来，我们设计了多个协调的观点，包括分割基于相互关联的变数矩阵和排列在一起的双轴条纹，并提供了一些互动来让用户进入转换选择过程，穿梭特征空间，并理解模型性能。我们实现 TimeTuner 使用两种转换方法：平滑和抽样。我们在实际应用中运用这两种转换方法，并在单variate 太阳黑子和多variate 空气污染物中进行了实验。专家反馈表明，我们的系统可以帮助描述时间序列表示和导引特征工程过程。
</details></li>
</ul>
<hr>
<h2 id="Deep-projection-networks-for-learning-time-homogeneous-dynamical-systems"><a href="#Deep-projection-networks-for-learning-time-homogeneous-dynamical-systems" class="headerlink" title="Deep projection networks for learning time-homogeneous dynamical systems"></a>Deep projection networks for learning time-homogeneous dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09912">http://arxiv.org/abs/2307.09912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir R. Kostic, Pietro Novelli, Riccardo Grazzi, Karim Lounici, Massimiliano Pontil</li>
<li>for: 学习时间homogeneous dynamical systems的meaningful representation，用于预测未来状态或观测量。</li>
<li>methods: 使用卷积神经网络学习投影算子，通过优化一个类似于 canonical correlation analysis（CCA）的目标函数来学习。</li>
<li>results: 提出了一种稳定且可靠的方法，可以应用于具有挑战性的情况，并且可以改进前一些方法的性能。<details>
<summary>Abstract</summary>
We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data. This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables. The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA). However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios. Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation. We apply our method to challenging discrete dynamical systems, discussing improvements over previous methods, as well as to continuous dynamical systems.
</details>
<details>
<summary>摘要</summary>
我们考虑一般时间对称动力系统，包括离散和连续类型，并研究从观测数据中学习一个有意义的状态表现。这个表现通常是通过神经网络实现，并与投影算子相关。我们通过优化一个目标函数，与标准均值分析（CCA）的目标函数相似，但不同的是，我们的目标函数没有矩阵逆元，因此更稳定和适用于具有挑战性的情况。我们的目标函数是CCA的紧缩版本，而我们还提出了两种调整方案，一种鼓励表现的分量对称，另一种利用柯莫格罗夫的方程式。我们将方法应用到具有挑战性的离散动力系统和连续动力系统。
</details></li>
</ul>
<hr>
<h2 id="Repeated-Observations-for-Classification"><a href="#Repeated-Observations-for-Classification" class="headerlink" title="Repeated Observations for Classification"></a>Repeated Observations for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09896">http://arxiv.org/abs/2307.09896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hüseyin Afşer, László Györfi, Harro Walk</li>
<li>for: 这个论文研究了非 Parametric 分类问题，具体来说是在重复观察的情况下。</li>
<li>methods: 这篇论文提出了一些简单的分类规则，这些规则的 conditional error probabilities 的整数速度 converge 为 $t\to\infty$.</li>
<li>results: 文章分析了一些特定的模型，如 robust detection by nominal densities、prototype classification、linear transformation、linear classification、scaling。<details>
<summary>Abstract</summary>
We study the problem nonparametric classification with repeated observations. Let $\bX$ be the $d$ dimensional feature vector and let $Y$ denote the label taking values in $\{1,\dots ,M\}$. In contrast to usual setup with large sample size $n$ and relatively low dimension $d$, this paper deals with the situation, when instead of observing a single feature vector $\bX$ we are given $t$ repeated feature vectors $\bV_1,\dots ,\bV_t $. Some simple classification rules are presented such that the conditional error probabilities have exponential convergence rate of convergence as $t\to\infty$. In the analysis, we investigate particular models like robust detection by nominal densities, prototype classification, linear transformation, linear classification, scaling.
</details>
<details>
<summary>摘要</summary>
我们研究非 Parametric 分类问题，对于重复观察 $\bX$ 的状况。在传统设置中，我们有大量样本数 $n$ 和低维度 $d$，但这里我们则是考虑在 $\bV_1, \bV_2, ..., \bV_t$ 中观察 $t$ 次的重复Feature vector。我们提出了一些简单的分类规则，其中 conditional error probability 的数值具有 exponential 的减少速度，随着 $t$ 趋向无限大。在分析中，我们探讨了特定的模型，例如 Robust 检测、prototype 分类、线性转换、线性分类、缩放。
</details></li>
</ul>
<hr>
<h2 id="Symmetric-Equilibrium-Learning-of-VAEs"><a href="#Symmetric-Equilibrium-Learning-of-VAEs" class="headerlink" title="Symmetric Equilibrium Learning of VAEs"></a>Symmetric Equilibrium Learning of VAEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09883">http://arxiv.org/abs/2307.09883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Flach, Dmitrij Schlesinger, Alexander Shekhovtsov</li>
<li>for: 这个论文是为了推广Variational Autoencoders（VAE）的应用范围，使其能够在更复杂的学习场景中使用，例如普通的半监督学习和使用复杂的生成模型为先验。</li>
<li>methods: 这个论文提出了一种Nash平衡学习方法，该方法可以适应更复杂的学习场景，例如只能通过采样获取数据和幂态分布的情况。该方法使用随机扩散来学习VAE，并且可以应用于各种下游任务。</li>
<li>results: 实验表明，使用Nash平衡学习方法学习VAE可以与使用标准的ELBO学习方法相比，并且可以应用于不可以使用标准VAE学习方法的任务。<details>
<summary>Abstract</summary>
We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa. The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect. Moreover, it requires a closed form a-priori latent distribution. This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors. We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling. The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks. We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability for tasks that are not accessible by standard VAE learning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们看待变量自动机 (VAE) 为抽象-编码对，它将数据空间中的分布映射到幂空间中的分布和 vice versa。标准的 VAE 学习方法，即最大化证据下界 (ELBO)，具有显著的不均衡性，而且需要闭式预先知道的幂空间分布。这限制了 VAE 在更复杂的场景下的应用，如总体半导导学习和使用复杂的生成模型为假设。我们提议一种纳什平衡学习方法，它可以放弃这些限制，并在数据和幂空间中的样本可用时学习 VAE。这种灵活性和简单性使其适用于各种学习场景和下游任务。我们实验表明，与 ELBO 学习相比，这种方法学习的模型相当，并且可以应用于不可用于标准 VAE 学习的任务。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Likelihood-Estimation-with-One-way-Flows"><a href="#Adversarial-Likelihood-Estimation-with-One-way-Flows" class="headerlink" title="Adversarial Likelihood Estimation with One-way Flows"></a>Adversarial Likelihood Estimation with One-way Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09882">http://arxiv.org/abs/2307.09882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Ben-Dov, Pravir Singh Gupta, Victoria Abrevaya, Michael J. Black, Partha Ghosh</li>
<li>for: 本文使用 Generative Adversarial Networks (GANs) 生成高质量样本，但GANs不提供样本周围的概率密度估计。然而，在能量基础设定下，最大化循环可能性函数可以导致对拒绝者的挑战，并且可以获得不正规化的概率密度（常称为能量）。本文进一步发展这种视角，并 incorporate 重要抽样，以获得一种不偏估计的合理性。</li>
<li>methods: 本文提出了一种新的流网络 architecture，called one-way flow network，它不需要有可追踪的逆函数，因此更加自由。此外，本文还使用了重要抽样来提高生成器的概率密度估计。</li>
<li>results: 本文的实验结果表明，使用了一种新的抽样策略和一种更自由的流网络 architecture，可以更快地收敛，并且生成的样本质量与相似的 GAN 架构相比几乎相同。此外，本文还成功避免了常见的数据集适应问题，并生成了平滑的低维 latent representation。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require to have a tractable inverse function. Our experimental results show that we converge faster, produce comparable sample quality to GANs with similar architecture, successfully avoid over-fitting to commonly used datasets and produce smooth low-dimensional latent representations of the training data.
</details>
<details>
<summary>摘要</summary>
生成对抗网络（GANs）可生成高质量样本，但不提供样本周围的概率密度估计。然而，有观点认为，在能量基础设置下，最大化各自对抗极大值可以导致对抗框架，其中承认器提供不归一化密度（常称为能量）。我们进一步发展这个视角，包括重要抽样，并表明以下两点：1.  Wasserstein GAN 实现了偏差估计 partition function，我们提议使用不偏差估计器；2. 在优化likelihood时，需要最大化生成器的Entropy。这是假设提供更好的模式覆盖率。与前一些作品不同，我们直接计算生成样本的浓度。这是键启用计算生成器的Entropy项和不偏差估计器。我们的实验结果表明，我们 faster converging, 生成样本质量与GANs相似，避免了通用数据集上的过拟合，并生成了平滑、低维度的训练数据表示。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Vulnerable-Nodes-in-Urban-Infrastructure-Interdependent-Network"><a href="#Detecting-Vulnerable-Nodes-in-Urban-Infrastructure-Interdependent-Network" class="headerlink" title="Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network"></a>Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09866">http://arxiv.org/abs/2307.09866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/kdd2023-id546-urbaninfra">https://github.com/tsinghua-fib-lab/kdd2023-id546-urbaninfra</a></li>
<li>paper_authors: Jinzhu Mao, Liu Cao, Chen Gao, Huandong Wang, Hangyu Fan, Depeng Jin, Yong Li</li>
<li>For:  This paper aims to understand and characterize the vulnerability of urban infrastructures, which are essential for the regular running of cities and exist naturally in the form of networks.* Methods: The paper proposes a system based on graph neural network with reinforcement learning to accurately model the interdependent network as a heterogeneous graph and capture the risk of cascade failure and discover vulnerable infrastructures of cities.* Results: The proposed system is demonstrated to be effective through extensive experiments with various requests, showing not only its expressive power but also its transferring ability and necessity of specific components.<details>
<summary>Abstract</summary>
Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failure and discover vulnerable infrastructures of cities. Extensive experiments with various requests demonstrate not only the expressive power of our system but also transferring ability and necessity of the specific components.
</details>
<details>
<summary>摘要</summary>
理解和特征城市基础设施的抗垮性很重要，这些基础设施包括城市的工程设施，这些设施自然存在于城市的网络形式中。 potential应用包括保护脆弱设施和设计Robust topology等等。由于不同的 topological特征和基础设施抗垮性存在强相关性和复杂的演化机制，一些启发式和机器学习分析方法无法处理这种情况。在这篇论文中，我们模型城市系统为不同类型图的异质图，并提出基于图神经网络和强化学习的系统，可以在实际数据上训练，准确地评估城市系统的抗垮性。我们的系统利用深度学习技术来理解和分析异质图，以便捕捉城市系统中的风险和潜在敏感设施。我们的实验结果表明，我们的系统不仅具有表达力，还可以转移和特定组件的必要性。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-population-informed-approach-to-the-definition-of-data-driven-models-for-structural-dynamics"><a href="#Towards-a-population-informed-approach-to-the-definition-of-data-driven-models-for-structural-dynamics" class="headerlink" title="Towards a population-informed approach to the definition of data-driven models for structural dynamics"></a>Towards a population-informed approach to the definition of data-driven models for structural dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09862">http://arxiv.org/abs/2307.09862</a></li>
<li>repo_url: None</li>
<li>paper_authors: G. Tsialiamanis, N. Dervilis, D. J. Wagg, K. Worden</li>
<li>for: 本研究旨在适应数据稀缺问题，通过将物理学基本方法与机器学习算法结合使用，提高结构动力学领域的模型性能。</li>
<li>methods: 本研究使用了两种meta-learning算法，分别是模型独立多学习（MAML）算法和条件神经过程（CNP）模型，以Population-based方式实现数据驱动模型的建立。</li>
<li>results: 实验结果表明，使用这两种机器学习算法可以更好地估算关键量，并且表现与传统机器学习算法类似，即随着训练集体系数量的增加，性能逐渐提高。<details>
<summary>Abstract</summary>
Machine learning has affected the way in which many phenomena for various domains are modelled, one of these domains being that of structural dynamics. However, because machine-learning algorithms are problem-specific, they often fail to perform efficiently in cases of data scarcity. To deal with such issues, combination of physics-based approaches and machine learning algorithms have been developed. Although such methods are effective, they also require the analyser's understanding of the underlying physics of the problem. The current work is aimed at motivating the use of models which learn such relationships from a population of phenomena, whose underlying physics are similar. The development of such models is motivated by the way that physics-based models, and more specifically finite element models, work. Such models are considered transferrable, explainable and trustworthy, attributes which are not trivially imposed or achieved for machine-learning models. For this reason, machine-learning approaches are less trusted by industry and often considered more difficult to form validated models. To achieve such data-driven models, a population-based scheme is followed here and two different machine-learning algorithms from the meta-learning domain are used. The two algorithms are the model-agnostic meta-learning (MAML) algorithm and the conditional neural processes (CNP) model. The algorithms seem to perform as intended and outperform a traditional machine-learning algorithm at approximating the quantities of interest. Moreover, they exhibit behaviour similar to traditional machine learning algorithms (e.g. neural networks or Gaussian processes), concerning their performance as a function of the available structures in the training population.
</details>
<details>
<summary>摘要</summary>
机器学习已经对许多领域的现象模型ling有所影响，其中一个领域是结构动力学。然而，由于机器学习算法是问题特定的，因此在数据稀缺时常常表现不好。为解决这些问题，physics-based方法和机器学习算法的组合被开发出来。虽然这些方法有效，但也需要分析者对问题的物理基础知识。本工作的目的是鼓励使用基于人口现象的模型，其中人口现象的物理基础相似。这种模型被认为是可转移、可解释和可信任的，这些特性不是容易受到机器学习模型的限制。因此，机器学习方法在业界中 Less trusted， often considered more difficult to form validated models. To achieve such data-driven models, a population-based scheme is followed here and two different machine-learning algorithms from the meta-learning domain are used. The two algorithms are the model-agnostic meta-learning (MAML) algorithm and the conditional neural processes (CNP) model. The algorithms seem to perform as intended and outperform a traditional machine-learning algorithm at approximating the quantities of interest. Moreover, they exhibit behavior similar to traditional machine learning algorithms (e.g. neural networks or Gaussian processes), concerning their performance as a function of the available structures in the training population.
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Credit-Index-Option-Hedging"><a href="#Reinforcement-Learning-for-Credit-Index-Option-Hedging" class="headerlink" title="Reinforcement Learning for Credit Index Option Hedging"></a>Reinforcement Learning for Credit Index Option Hedging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09844">http://arxiv.org/abs/2307.09844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Mandelli, Marco Pinciroli, Michele Trapletti, Edoardo Vittori</li>
<li>for: 本研究目标是找到最佳减负策略，以减少信用指数选择的风险。</li>
<li>methods: 我们采用了实用的方法，即使 discrete time 和交易成本，并在实际市场数据上测试了我们的策略。我们使用了现代算法，即信任区域波动优化（TRVO）算法。</li>
<li>results: 我们的研究表明，基于TRVO算法得到的减负策略比布莱克和斯科尔的Delta减负策略更高效。<details>
<summary>Abstract</summary>
In this paper, we focus on finding the optimal hedging strategy of a credit index option using reinforcement learning. We take a practical approach, where the focus is on realism i.e. discrete time, transaction costs; even testing our policy on real market data. We apply a state of the art algorithm, the Trust Region Volatility Optimization (TRVO) algorithm and show that the derived hedging strategy outperforms the practitioner's Black & Scholes delta hedge.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注发现债券指数选项最优化套利策略的优化策略，使用回归学习。我们采取了实用的方法，即注重实际情况，即逐步时间、交易成本等。我们应用了现代算法TRVO算法，并证明得到的套利策略超过实践中的黑格&瑞德（Black & Scholes）delta套利。
</details></li>
</ul>
<hr>
<h2 id="Near-Linear-Time-Projection-onto-the-ell-1-infty-Ball-Application-to-Sparse-Autoencoders"><a href="#Near-Linear-Time-Projection-onto-the-ell-1-infty-Ball-Application-to-Sparse-Autoencoders" class="headerlink" title="Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders"></a>Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09836">http://arxiv.org/abs/2307.09836</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/memo-p/projection">https://github.com/memo-p/projection</a></li>
<li>paper_authors: Guillaume Perez, Laurent Condat, Michel Barlaud</li>
<li>for: 本文主要研究如何快速训练大规模神经网络，特别是如何使用投影技术来减少神经网络的总成本。</li>
<li>methods: 本文提出了一种新的投影算法，用于减少$\ell_{1,\infty}$ нор范围内的矩阵元素。该算法的最坏时间复杂度为$\mathcal{O}\big(nm+J\log(nm)\big)$，其中$J$是一个减少到0的常数，或者是$nm$的常数，具体取决于矩阵的稀烈程度。此外，本文还提议在神经网络训练中采用$\ell_{1,\infty}$球投影来实现特征选择和稀烈化。</li>
<li>results: 本文的实验结果显示，在生物学应用中，只有小部分数据（少于2%）是有关的，而使用本文的方法可以快速地减少神经网络的总成本。此外，在总体上，本文的方法也是最快的。<details>
<summary>Abstract</summary>
Looking for sparsity is nowadays crucial to speed up the training of large-scale neural networks. Projections onto the $\ell_{1,2}$ and $\ell_{1,\infty}$ are among the most efficient techniques to sparsify and reduce the overall cost of neural networks. In this paper, we introduce a new projection algorithm for the $\ell_{1,\infty}$ norm ball. The worst-case time complexity of this algorithm is $\mathcal{O}\big(nm+J\log(nm)\big)$ for a matrix in $\mathbb{R}^{n\times m}$. $J$ is a term that tends to 0 when the sparsity is high, and to $nm$ when the sparsity is low. Its implementation is easy and it is guaranteed to converge to the exact solution in a finite time. Moreover, we propose to incorporate the $\ell_{1,\infty}$ ball projection while training an autoencoder to enforce feature selection and sparsity of the weights. Sparsification appears in the encoder to primarily do feature selection due to our application in biology, where only a very small part ($<2\%$) of the data is relevant. We show that both in the biological case and in the general case of sparsity that our method is the fastest.
</details>
<details>
<summary>摘要</summary>
现在，寻找稀疏性（sparsity）是训练大规模神经网络的关键。 projet onto the $\ell_{1,2}$ and $\ell_{1,\infty}$ 是最有效的技术来减少和降低神经网络的总成本。在这篇论文中，我们提出了一种新的 projection algorithm for the $\ell_{1,\infty}$ norm ball。该算法的最坏情况时间复杂度为 $\mathcal{O}\big(nm+J\log(nm)\big)$，其中 $J$ 是一个随着稀疏性的变量，当稀疏性高时，$J$ 趋于 0，而当稀疏性低时，$J$ 趋于 $nm$。该算法的实现容易，并且可以在有限时间内 converge to the exact solution。此外，我们还提议在训练 autoencoder 时，通过 incorporating the $\ell_{1,\infty}$ ball projection 来强制权重的稀疏性和选择性。在我们的应用中，只有一小部分（<2%）的数据是关键的，因此在encoder中进行稀疏化可以主要地实现特征选择。我们展示了在生物学应用中和总的稀疏性情况下，我们的方法是最快的。
</details></li>
</ul>
<hr>
<h2 id="Deep-Operator-Network-Approximation-Rates-for-Lipschitz-Operators"><a href="#Deep-Operator-Network-Approximation-Rates-for-Lipschitz-Operators" class="headerlink" title="Deep Operator Network Approximation Rates for Lipschitz Operators"></a>Deep Operator Network Approximation Rates for Lipschitz Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09835">http://arxiv.org/abs/2307.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Schwab, Andreas Stein, Jakob Zech</li>
<li>for: 这个论文是为了研究深度运算网络（Deep Operator Networks，DON）在模拟 lipschitz（或 holder）连续映射 $\mathcal G:\mathcal X\to\mathcal Y$  между（subsets of） separable Hilbert spaces $\mathcal X$, $\mathcal Y$ 的表达率 bounds。</li>
<li>methods: 这个论文使用了 linear encoders $\mathcal E$ 和 decoders $\mathcal D$ via（biorthogonal）Riesz bases of $\mathcal X$, $\mathcal Y$，并使用了一个参数化坐标映射的 approximator network，以实现 infinite-dimensional、 Parametric coordinate map 的 lipschitz continuous 表达。</li>
<li>results: 这个论文得到了 Lipschitz（或 holder）连续映射 $\mathcal G$ 的表达率 bounds，不需要 $\mathcal G$ 是 holomorphic 的假设。关键在证明表达率 bounds 时使用了 either super-expressive activations 或 nonstandard NN architectures with standard (ReLU) activations。例如，可以用 elementry superexpressive activations 或 nonstandard NN architectures with standard (ReLU) activations，如 [Yarotski: Elementary superexpressive activations, Int. Conf. on ML, 2021] 和 [Shen, Yang and Zhang: Neural network approximation: Three hidden layers are enough, Neural Networks, 2021] 等。<details>
<summary>Abstract</summary>
We establish universality and expression rate bounds for a class of neural Deep Operator Networks (DON) emulating Lipschitz (or H\"older) continuous maps $\mathcal G:\mathcal X\to\mathcal Y$ between (subsets of) separable Hilbert spaces $\mathcal X$, $\mathcal Y$. The DON architecture considered uses linear encoders $\mathcal E$ and decoders $\mathcal D$ via (biorthogonal) Riesz bases of $\mathcal X$, $\mathcal Y$, and an approximator network of an infinite-dimensional, parametric coordinate map that is Lipschitz continuous on the sequence space $\ell^2(\mathbb N)$. Unlike previous works ([Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022], [Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022]), which required for example $\mathcal G$ to be holomorphic, the present expression rate results require mere Lipschitz (or H\"older) continuity of $\mathcal G$. Key in the proof of the present expression rate bounds is the use of either super-expressive activations (e.g. [Yarotski: Elementary superexpressive activations, Int. Conf. on ML, 2021], [Shen, Yang and Zhang: Neural network approximation: Three hidden layers are enough, Neural Networks, 2021], and the references there) which are inspired by the Kolmogorov superposition theorem, or of nonstandard NN architectures with standard (ReLU) activations as recently proposed in [Zhang, Shen and Yang: Neural Network Architecture Beyond Width and Depth, Adv. in Neural Inf. Proc. Sys., 2022]. We illustrate the abstract results by approximation rate bounds for emulation of a) solution operators for parametric elliptic variational inequalities, and b) Lipschitz maps of Hilbert-Schmidt operators.
</details>
<details>
<summary>摘要</summary>
我们证明了一 classe of neural Deep Operator Networks (DON) 的 universality和expression rate bounds，其中 $\mathcal G:\mathcal X\to\mathcal Y$ 是一个 Lipschitz (或Holder) 连续的映射 zwischen (subsets of) 分离 Hilbert spaces $\mathcal X$, $\mathcal Y$。DON 架构使用了线性encoder $\mathcal E$ 和decoder $\mathcal D$ via (biorthogonal) Riesz bases of $\mathcal X$, $\mathcal Y$, 并且使用一个实际coordinate map的 approximator network，这个coordinate map是一个无限维度的参数化映射，并且在sequence space $\ell^2(\mathbb N)$上是 Lipschitz 连续的。不同于先前的工作([Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022], [Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022])，我们不需要 $\mathcal G$ 是holomorphic的，而是只需要它是Lipschitz (或Holder) 连续的。在证明我们的expression rate bounds时，我们使用了 either super-expressive activations (e.g. [Yarotski: Elementary superexpressive activations, Int. Conf. on ML, 2021], [Shen, Yang and Zhang: Neural network approximation: Three hidden layers are enough, Neural Networks, 2021], 和 referenes there)，这些activations是基于Kolmogorov superposition theorem的，或者使用 nonstandard NN architectures with standard (ReLU) activations，如最近提出的 [Zhang, Shen and Yang: Neural Network Architecture Beyond Width and Depth, Adv. in Neural Inf. Proc. Sys., 2022]。我们这里给出了 approximation rate bounds for emulation of a) solution operators for parametric elliptic variational inequalities, 和 b) Lipschitz maps of Hilbert-Schmidt operators。
</details></li>
</ul>
<hr>
<h2 id="What-do-neural-networks-learn-in-image-classification-A-frequency-shortcut-perspective"><a href="#What-do-neural-networks-learn-in-image-classification-A-frequency-shortcut-perspective" class="headerlink" title="What do neural networks learn in image classification? A frequency shortcut perspective"></a>What do neural networks learn in image classification? A frequency shortcut perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09829">http://arxiv.org/abs/2307.09829</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nis-research/nn-frequency-shortcuts">https://github.com/nis-research/nn-frequency-shortcuts</a></li>
<li>paper_authors: Shunxin Wang, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio</li>
<li>for:  investigate the mechanisms of representation learning in neural networks (NNs) for classification tasks, and expand the understanding of frequency shortcuts.</li>
<li>methods: perform experiments on synthetic datasets and natural images, propose a metric to measure class-wise frequency characteristics, and identify frequency shortcuts.</li>
<li>results: demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics; confirm that frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation.<details>
<summary>Abstract</summary>
Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transferability of frequency shortcuts on out-of-distribution (OOD) test sets. Our results suggest that frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation. We recommend that future research should focus on effective training schemes mitigating frequency shortcut learning.
</details>
<details>
<summary>摘要</summary>
频率分析有助于理解神经网络（NN）的表征学习机制。大多数研究集中在 regression 任务上进行学习动力研究，而 Classification 方面则得少。这项研究进行实验性调查，扩展了频率短cut的理解。我们在设计的synthetic datasets上进行实验，发现NNs在类别化任务上倾向于找到简单的解决方案，并在训练过程中学习的第一个频率特征取决于数据集的偏好。我们还确认了这种现象在自然图像上。我们提出了一种类别频率特征的度量和frequency短cut的标识方法。结果显示，frequency短cut可以基于文字特征或形态特征，归结在最简化目标时。最后，我们验证了频率短cut的传送性，发现它们可以在不同的数据集上传送，并且不可以通过更大的模型容量和数据增强来完全避免。我们建议未来的研究应该关注有效地 mitigate 频率短cut学习。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Learning-based-Prediction-for-Disease"><a href="#Multi-modal-Learning-based-Prediction-for-Disease" class="headerlink" title="Multi-modal Learning based Prediction for Disease"></a>Multi-modal Learning based Prediction for Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09823">http://arxiv.org/abs/2307.09823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/batuhankmkaraman/mlbasedad">https://github.com/batuhankmkaraman/mlbasedad</a></li>
<li>paper_authors: Yaran Chen, Xueyu Chen, Yu Han, Haoran Li, Dongbin Zhao, Jingzhong Li, Xu Wang</li>
<li>for: 预测非酒精性肝病（NAFLD）的病理诊断</li>
<li>methods:  combining a comprehensive clinical dataset（FLDData）和一种基于多Modal学习的NAFLD预测方法（DeepFLD），使用多Modal输入，包括Metadata和面部图像，以提高非侵入性诊断的准确率</li>
<li>results:  DeepFLD模型在不同的测试集上达到了高度的表现，并且使用只有面部图像作为输入，可以实现相对较好的性能，这显示了该方法的可行性和简洁性。<details>
<summary>Abstract</summary>
Non alcoholic fatty liver disease (NAFLD) is the most common cause of chronic liver disease, which can be predicted accurately to prevent advanced fibrosis and cirrhosis. While, a liver biopsy, the gold standard for NAFLD diagnosis, is invasive, expensive, and prone to sampling errors. Therefore, non-invasive studies are extremely promising, yet they are still in their infancy due to the lack of comprehensive research data and intelligent methods for multi-modal data. This paper proposes a NAFLD diagnosis system (DeepFLDDiag) combining a comprehensive clinical dataset (FLDData) and a multi-modal learning based NAFLD prediction method (DeepFLD). The dataset includes over 6000 participants physical examinations, laboratory and imaging studies, extensive questionnaires, and facial images of partial participants, which is comprehensive and valuable for clinical studies. From the dataset, we quantitatively analyze and select clinical metadata that most contribute to NAFLD prediction. Furthermore, the proposed DeepFLD, a deep neural network model designed to predict NAFLD using multi-modal input, including metadata and facial images, outperforms the approach that only uses metadata. Satisfactory performance is also verified on other unseen datasets. Inspiringly, DeepFLD can achieve competitive results using only facial images as input rather than metadata, paving the way for a more robust and simpler non-invasive NAFLD diagnosis.
</details>
<details>
<summary>摘要</summary>
非酒精肝病（NAFLD）是现代肝病最常见的原因，可以准确预测并预防进展到高度纤维化和 cirrhosis。然而，肝切片，作为 NAFLD 诊断的标准方法，是侵入性的、昂贵的，且容易出现采样错误。因此，非侵入性的研究非常有前途，但是现在仍然处于初期阶段，因为缺乏全面的研究数据和智能的多Modal 数据处理方法。本文提出了一种基于 Deep Learning 技术的 NAFLD 诊断系统（DeepFLDDiag），结合了丰富的临床数据集（FLDData）和多Modal 学习基于 NAFLD 预测方法（DeepFLD）。该数据集包括超过 6000 名参与者的身体检查、实验室和成像研究、广泛的问卷和一部分参与者的面部图像，这是丰富和有价值的临床数据。从数据集中，我们量化分析并选择了对 NAFLD 预测最有价值的临床Metadata。此外，我们提出的 DeepFLD 模型，用于预测 NAFLD 的多Modal 输入，包括Metadata 和面部图像，超越了只使用Metadata 的方法。在其他未看到数据集上，我们也得到了满意的性能。更有意思的是，DeepFLD 可以使用面部图像作为输入，而不需要Metadata，这打开了一个更加简单、更加稳定的非侵入性 NAFLD 诊断之路。
</details></li>
</ul>
<hr>
<h2 id="Deep-unrolling-Shrinkage-Network-for-Dynamic-MR-imaging"><a href="#Deep-unrolling-Shrinkage-Network-for-Dynamic-MR-imaging" class="headerlink" title="Deep unrolling Shrinkage Network for Dynamic MR imaging"></a>Deep unrolling Shrinkage Network for Dynamic MR imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09818">http://arxiv.org/abs/2307.09818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yhao-z/dus-net">https://github.com/yhao-z/dus-net</a></li>
<li>paper_authors: Yinghao Zhang, Xiaodi Li, Weihang Li, Yue Hu<br>for:* 这篇论文主要是关于动态核磁共振成像（MR）重建模型的研究。methods:* 该论文提出了一种新的soft thresholding with channel attention（AST）操作，用于在每个通道上学习阈值。* 该论文还提出了一种深度折叠缩小网络（DUS-Net），通过对多个分量的alternating direction method of multipliers（ADMM）进行拓展，来优化转换后$l_1$ нор的动态MR重建模型。results:* 实验结果表明，提议的DUS-Net方法在一个开放的动态照片MR数据集上表现出色，超过了当前的状态艺术方法。<details>
<summary>Abstract</summary>
Deep unrolling networks that utilize sparsity priors have achieved great success in dynamic magnetic resonance (MR) imaging. The convolutional neural network (CNN) is usually utilized to extract the transformed domain, and then the soft thresholding (ST) operator is applied to the CNN-transformed data to enforce the sparsity priors. However, the ST operator is usually constrained to be the same across all channels of the CNN-transformed data. In this paper, we propose a novel operator, called soft thresholding with channel attention (AST), that learns the threshold for each channel. In particular, we put forward a novel deep unrolling shrinkage network (DUS-Net) by unrolling the alternating direction method of multipliers (ADMM) for optimizing the transformed $l_1$ norm dynamic MR reconstruction model. Experimental results on an open-access dynamic cine MR dataset demonstrate that the proposed DUS-Net outperforms the state-of-the-art methods. The source code is available at \url{https://github.com/yhao-z/DUS-Net}.
</details>
<details>
<summary>摘要</summary>
深度 unfolding 网络，利用减法约束，在动态磁共振成像中取得了很大的成功。通常情况下，卷积神经网络（CNN）会被用来提取转换的频谱，然后使用软resholding（ST）操作来强制施加约束。但是，ST操作通常会被限制为所有通道的 CNN-transformed 数据中的同一个值。在这篇论文中，我们提出了一种新的操作，即通道注意力软resholding（AST），它可以学习每个通道的阈值。具体来说，我们提出了一种新的深度 unfolding shrinkage network（DUS-Net），通过对 alternate direction method of multipliers（ADMM）的拓展来优化转换 $l_1$  нор的动态 MR 重建模型。实验结果表明，提议的 DUS-Net 在一个公开的动态磁共振动态MR数据集上表现出了比状态之前的方法更好的性能。源代码可以在 \url{https://github.com/yhao-z/DUS-Net} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Manifold-Learning-with-Sparse-Regularised-Optimal-Transport"><a href="#Manifold-Learning-with-Sparse-Regularised-Optimal-Transport" class="headerlink" title="Manifold Learning with Sparse Regularised Optimal Transport"></a>Manifold Learning with Sparse Regularised Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09816">http://arxiv.org/abs/2307.09816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zsteve/QROT">https://github.com/zsteve/QROT</a></li>
<li>paper_authors: Stephen Zhang, Gilles Mordant, Tetsuya Matsumoto, Geoffrey Schiebinger</li>
<li>for: 本研究旨在提出一种基于优化运输的拟合 manifold 学习方法，以减少实际数据中的噪声和采样影响。</li>
<li>methods: 该方法使用一种对称的优化运输方法，加以二阶规则化， constructions 一个稀疏和可适应的凝聚矩阵，可以视为泛化凝聚kernel的扩展。</li>
<li>results: 研究证明了该kernel在维度缩放时是一个准确的laplace-type运算，并且在各种伪随机噪声下保持稳定性。此外，该方法在一些实验中表现出了较高的效率和精度。<details>
<summary>Abstract</summary>
Manifold learning is a central task in modern statistics and data science. Many datasets (cells, documents, images, molecules) can be represented as point clouds embedded in a high dimensional ambient space, however the degrees of freedom intrinsic to the data are usually far fewer than the number of ambient dimensions. The task of detecting a latent manifold along which the data are embedded is a prerequisite for a wide family of downstream analyses. Real-world datasets are subject to noisy observations and sampling, so that distilling information about the underlying manifold is a major challenge. We propose a method for manifold learning that utilises a symmetric version of optimal transport with a quadratic regularisation that constructs a sparse and adaptive affinity matrix, that can be interpreted as a generalisation of the bistochastic kernel normalisation. We prove that the resulting kernel is consistent with a Laplace-type operator in the continuous limit, establish robustness to heteroskedastic noise and exhibit these results in simulations. We identify a highly efficient computational scheme for computing this optimal transport for discrete data and demonstrate that it outperforms competing methods in a set of examples.
</details>
<details>
<summary>摘要</summary>
《 manifold learning 是现代统计和数据科学中的中心任务。许多数据（细胞、文档、图像、分子）可以被表示为高维拓扑空间中的点云，但数据中的自由度通常比拓扑空间维度少得多。探测数据所embedded的秘密拓扑是下游分析的先决条件。实际世界数据受到噪声观测和采样的影响，因此提取数据下的底层拓扑信息是一大挑战。我们提出一种基于对称的最优运输方法，加入 quadratic regularization，构建一个稀疏和适应性的倾斜矩阵，这可以看作是泛函 kernel 的普适化。我们证明这种核函在连续限制下与拉普拉斯型运算相一致，并且具有抗hetroskedastic 噪声的稳定性。我们还提出了高效的计算方案，可以高效地计算这种最优运输，并在一些示例中证明其超过竞争方法。》
</details></li>
</ul>
<hr>
<h2 id="GenKL-An-Iterative-Framework-for-Resolving-Label-Ambiguity-and-Label-Non-conformity-in-Web-Images-Via-a-New-Generalized-KL-Divergence"><a href="#GenKL-An-Iterative-Framework-for-Resolving-Label-Ambiguity-and-Label-Non-conformity-in-Web-Images-Via-a-New-Generalized-KL-Divergence" class="headerlink" title="GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence"></a>GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09810">http://arxiv.org/abs/2307.09810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/codetopaper/genkl">https://github.com/codetopaper/genkl</a></li>
<li>paper_authors: Xia Huang, Kai Fong Ernest Chong<br>for:* The paper is written to address the problem of non-conforming (NC) instances in web image datasets, which can negatively impact the performance of image classification models.methods:* The paper proposes a new method called $\mathcal{D}<em>{\text{KL}^{\alpha, \beta}(p|q)$ based on the concept of generalized KL divergence to identify NC instances.* The method is based on the idea of measuring the difference between the predicted distribution of an instance and the true distribution, and it is proven to be more effective than traditional entropy-based methods.results:* The paper achieves new state-of-the-art classification accuracies on three web image datasets: Clothing1M, Food101&#x2F;Food101N, and mini WebVision 1.0. The achieved accuracies are $81.34%$, $85.73%$ and $78.99%$&#x2F;$92.54%$ (top-1&#x2F;top-5), respectively.Here is the information in Simplified Chinese text:for:* 论文是为了解决网络图像集中的非合规（NC）实例问题，这些实例可能会影响图像分类模型的性能。methods:* 论文提出了一种基于泛化KL差的新方法，称为 $\mathcal{D}</em>{\text{KL}^{\alpha, \beta}(p|q)$，用于标识NC实例。* 该方法基于测量实例预测分布与真实分布之间的差异，并证明其比传统的熵基本方法更有效。results:* 论文在三个网络图像集上达到了新的状态对图像分类模型的最佳性能：Clothing1M、Food101&#x2F;Food101N和 mini WebVision 1.0。得到的准确率分别是81.34%、85.73%和78.99%&#x2F;92.54% (top-1&#x2F;top-5)。<details>
<summary>Abstract</summary>
Web image datasets curated online inherently contain ambiguous in-distribution (ID) instances and out-of-distribution (OOD) instances, which we collectively call non-conforming (NC) instances. In many recent approaches for mitigating the negative effects of NC instances, the core implicit assumption is that the NC instances can be found via entropy maximization. For "entropy" to be well-defined, we are interpreting the output prediction vector of an instance as the parameter vector of a multinomial random variable, with respect to some trained model with a softmax output layer. Hence, entropy maximization is based on the idealized assumption that NC instances have predictions that are "almost" uniformly distributed. However, in real-world web image datasets, there are numerous NC instances whose predictions are far from being uniformly distributed. To tackle the limitation of entropy maximization, we propose $(\alpha, \beta)$-generalized KL divergence, $\mathcal{D}_{\text{KL}^{\alpha, \beta}(p\|q)$, which can be used to identify significantly more NC instances. Theoretical properties of $\mathcal{D}_{\text{KL}^{\alpha, \beta}(p\|q)$ are proven, and we also show empirically that a simple use of $\mathcal{D}_{\text{KL}^{\alpha, \beta}(p\|q)$ outperforms all baselines on the NC instance identification task. Building upon $(\alpha,\beta)$-generalized KL divergence, we also introduce a new iterative training framework, GenKL, that identifies and relabels NC instances. When evaluated on three web image datasets, Clothing1M, Food101/Food101N, and mini WebVision 1.0, we achieved new state-of-the-art classification accuracies: $81.34\%$, $85.73\%$ and $78.99\%$/$92.54\%$ (top-1/top-5), respectively.
</details>
<details>
<summary>摘要</summary>
网络图像数据集Curated online自然而含有ambiguous in-distribution (ID)实例和out-of-distribution (OOD)实例，我们共同称之为非符合 (NC) 实例。在许多最近的approaches中，核心隐式假设是通过Entropy maximization来缓解NC实例的负面影响。为了使Entropy well-defined，我们 interprets the output prediction vector of an instance as the parameter vector of a multinomial random variable, with respect to some trained model with a softmax output layer。因此，Entropy maximization基于理想化的假设，NC实例的预测值都是“几乎” uniform distribution。然而，在实际的网络图像数据集中，有许多NC实例 whose predictions are far from being uniformly distributed。为了缓解Entropy maximization的局限性，我们提出了$(\alpha, \beta)$-通常KL差（Divergence）， $\mathcal{D}_{\text{KL}^{\alpha, \beta}(p\|q)$，可以用来标识更多的NC实例。我们也证明了这种方法的理论性质，并通过实验表明，使用$\mathcal{D}_{\text{KL}^{\alpha, \beta}(p\|q)$可以超过所有基准的NC实例标识性能。基于$(\alpha, \beta)$-通常KL差，我们还介绍了一种新的迭代培训框架，GenKL，可以标识和重新标注NC实例。当应用于三个网络图像数据集，Clothing1M、Food101/Food101N和mini WebVision 1.0时，我们 achieved new state-of-the-art classification accuracies：$81.34\%$, $85.73\%$和$78.99\%$/$92.54\%$ (top-1/top-5)，分别。
</details></li>
</ul>
<hr>
<h2 id="Graph-Federated-Learning-Based-on-the-Decentralized-Framework"><a href="#Graph-Federated-Learning-Based-on-the-Decentralized-Framework" class="headerlink" title="Graph Federated Learning Based on the Decentralized Framework"></a>Graph Federated Learning Based on the Decentralized Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09801">http://arxiv.org/abs/2307.09801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peilin Liu, Yanni Tang, Mingyue Zhang, Wu Chen</li>
<li>for: 防止数据隐私泄露和提高模型准确率，提出一种基于分布式机器学习的图学习方法。</li>
<li>methods: 使用分布式机器学习的客户端-服务器框架，并在客户端之间进行数据交换和模型训练。基于节点之间数据相似性的确定节点信任度，然后进行权重线性聚合。</li>
<li>results: 与 FedAvg、Fedprox、GCFL 和 GCFL+ 进行比较，实验结果表明提出的方法在准确率和稳定性方面具有较高的性能。<details>
<summary>Abstract</summary>
Graph learning has a wide range of applications in many scenarios, which require more need for data privacy. Federated learning is an emerging distributed machine learning approach that leverages data from individual devices or data centers to improve the accuracy and generalization of the model, while also protecting the privacy of user data. Graph-federated learning is mainly based on the classical federated learning framework i.e., the Client-Server framework. However, the Client-Server framework faces problems such as a single point of failure of the central server and poor scalability of network topology. First, we introduce the decentralized framework to graph-federated learning. Second, determine the confidence among nodes based on the similarity of data among nodes, subsequently, the gradient information is then aggregated by linear weighting based on confidence. Finally, the proposed method is compared with FedAvg, Fedprox, GCFL, and GCFL+ to verify the effectiveness of the proposed method. Experiments demonstrate that the proposed method outperforms other methods.
</details>
<details>
<summary>摘要</summary>
《图学学习有广泛的应用场景，需要更加重视数据隐私。联邦学习是一种迅速发展的分布式机器学习方法，利用设备或数据中心上的数据来提高模型的准确性和通用性，同时保护用户数据的隐私。图联邦学习主要基于经典的客户端-服务器框架。但是，客户端-服务器框架受到中央服务器垂直点失败和网络拓扑缺乏扩展性的问题。我们首先引入了分布式框架到图联邦学习。其次，通过数据相似性对节点进行确定 Similarity，然后将Gradient信息通过线性权重平均来聚合。最后，我们对提出的方法进行了与FedAvg、Fedprox、GCFL和GCFL+进行比较，以验证提出的方法的有效性。实验表明，提出的方法在其他方法之上具有优势。》Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form as well.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Forecasting-with-Coherent-Aggregation"><a href="#Probabilistic-Forecasting-with-Coherent-Aggregation" class="headerlink" title="Probabilistic Forecasting with Coherent Aggregation"></a>Probabilistic Forecasting with Coherent Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09797">http://arxiv.org/abs/2307.09797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffrey Négiar, Ruijun Ma, O. Nangba Meetei, Mengfei Cao, Michael W. Mahoney</li>
<li>for: 这篇论文主要应用于多変量预测 tasks，例如能源管理、供应链观察和资源分配等领域，以提供精确的机会预测。</li>
<li>methods: 本论文提出了一个新的模型，利用因子模型结构来生成具有层次结构的预测，并使用卷积神经网来生成因子、负载和基层分布的参数。这个模型可以跟踪基层分布的变化，并且可以根据样本的差异来估计基层分布的参数。</li>
<li>results: 该论文的实验结果显示，该模型可以在三个层次预测数据集上获得明显的改善（11.8-41.4%），并且可以调整基层分布和因子数量对模型的影响。<details>
<summary>Abstract</summary>
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probability Score and quantile losses. We can choose arbitrary continuous distributions for the factor and the base-level distributions. We compare our method to two previous methods which can be optimized end-to-end, while enforcing coherent aggregation. Our model achieves significant improvements: between $11.8-41.4\%$ on three hierarchical forecasting datasets. We also analyze the influence of parameters in our model with respect to base-level distribution and number of factors.
</details>
<details>
<summary>摘要</summary>
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model that leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings, and base-level distributions; it produces samples that can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probability Score and quantile losses. We can choose arbitrary continuous distributions for the factor and the base-level distributions. We compare our method to two previous methods that can be optimized end-to-end while enforcing coherent aggregation. Our model achieves significant improvements: between 11.8-41.4% on three hierarchical forecasting datasets. We also analyze the influence of parameters in our model with respect to base-level distribution and number of factors.Here's the text in Traditional Chinese as well:Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model that leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings, and base-level distributions; it produces samples that can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probability Score and quantile losses. We can choose arbitrary continuous distributions for the factor and the base-level distributions. We compare our method to two previous methods that can be optimized end-to-end while enforcing coherent aggregation. Our model achieves significant improvements: between 11.8-41.4% on three hierarchical forecasting datasets. We also analyze the influence of parameters in our model with respect to base-level distribution and number of factors.
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Early-with-Meta-Learning"><a href="#Forecasting-Early-with-Meta-Learning" class="headerlink" title="Forecasting Early with Meta Learning"></a>Forecasting Early with Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09796">http://arxiv.org/abs/2307.09796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/super-shayan/feml">https://github.com/super-shayan/feml</a></li>
<li>paper_authors: Shayan Jawed, Kiran Madhusudhanan, Vijaya Krishna Yalavarthi, Lars Schmidt-Thieme</li>
<li>for: 本研究的目的是开发一种基于Meta学习的时间序列预测方法，以便在早期观察期间，使用已有的数据集来预测时间序列。</li>
<li>methods: 本研究使用了一种基于对抗学习的Meta学习方法，其中包括一个共享的卷积层，用于学习不同数据集的特征，以及数据集特定的头，用于预测不同的输出长度。</li>
<li>results: 研究表明，FEML可以在不同数据集上进行Meta学习，并通过在 auxiliary task 中学习对抗生成的样本，提高预测性能，比单任务学习和不同的解决方案（如共同学习、多任务学习和经典预测基准）的表现更佳。<details>
<summary>Abstract</summary>
In the early observation period of a time series, there might be only a few historic observations available to learn a model. However, in cases where an existing prior set of datasets is available, Meta learning methods can be applicable. In this paper, we devise a Meta learning method that exploits samples from additional datasets and learns to augment time series through adversarial learning as an auxiliary task for the target dataset. Our model (FEML), is equipped with a shared Convolutional backbone that learns features for varying length inputs from different datasets and has dataset specific heads to forecast for different output lengths. We show that FEML can meta learn across datasets and by additionally learning on adversarial generated samples as auxiliary samples for the target dataset, it can improve the forecasting performance compared to single task learning, and various solutions adapted from Joint learning, Multi-task learning and classic forecasting baselines.
</details>
<details>
<summary>摘要</summary>
在时间序列的早期观察期，可能只有几个历史观察值可以学习模型。但在存在现有的先前数据集的情况下，元学习方法可以应用。在这篇论文中，我们设计了一种元学习方法，利用来自其他数据集的样本，并通过对抗学习作为目标数据集的auxiliary任务来增强时间序列的学习。我们的模型（FEML）具有共享的卷积几何体，可以从不同的数据集中学习不同长度的输入特征，并具有特定数据集的头来预测不同的输出长度。我们显示，FEML可以在不同数据集之间meta学习，并通过额外学习对抗生成的样本来提高预测性能，并评估了单任务学习、联合学习、多任务学习和经典预测基准的多种解决方案。
</details></li>
</ul>
<hr>
<h2 id="From-West-to-East-Who-can-understand-the-music-of-the-others-better"><a href="#From-West-to-East-Who-can-understand-the-music-of-the-others-better" class="headerlink" title="From West to East: Who can understand the music of the others better?"></a>From West to East: Who can understand the music of the others better?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09795">http://arxiv.org/abs/2307.09795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pxaris/ccml">https://github.com/pxaris/ccml</a></li>
<li>paper_authors: Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</li>
<li>For: 本研究用于了解不同音乐文化和风格下的音频嵌入模型是否可以学习音乐特征，以及是否可以通过转移学习来捕捉不同音乐文化之间的相似性。* Methods: 本研究使用了转移学习方法，将西方乐器演奏的音频数据和东方传统乐器演奏的音频数据作为源数据，并使用了两种CNN和一种Transformer预训练模型进行音频嵌入。* Results: 实验结果显示，通过转移学习可以在所有领域中实现竞争性的表现，但是不同音乐文化的源数据具有不同的优势。研究提供了一个公共存储库，包含实现和训练的模型。<details>
<summary>Abstract</summary>
Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset. Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture. The implementation and the trained models are both provided in a public repository.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="IncDSI-Incrementally-Updatable-Document-Retrieval"><a href="#IncDSI-Incrementally-Updatable-Document-Retrieval" class="headerlink" title="IncDSI: Incrementally Updatable Document Retrieval"></a>IncDSI: Incrementally Updatable Document Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10323">http://arxiv.org/abs/2307.10323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varsha Kishore, Chao Wan, Justin Lovelace, Yoav Artzi, Kilian Q. Weinberger</li>
<li>For: The paper is written for document retrieval systems that need to be updated with new information in real-time.* Methods: The paper proposes a method called IncDSI, which adds documents to a trained neural network-based search index in real-time, without retraining the entire model.* Results: The authors claim that their approach is orders of magnitude faster than retraining the model on the entire dataset, and is competitive with retraining the model on the whole dataset in terms of performance. They also demonstrate the effectiveness of their approach on several benchmarks.<details>
<summary>Abstract</summary>
Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI is available at https://github.com/varshakishore/IncDSI.
</details>
<details>
<summary>摘要</summary>
diferenciable 搜寻指标是一种最近提出的文档搜寻方法，它将文档集中的信息嵌入神经网络的参数中，并直接将查询映射到相应的文档。这种模型在许多benchmark上达到了州际级的表现，但它具有一个重要的限制：不容易在模型训练后添加新的文档。我们提出了IncDSI，一种在实时（约20-50ms每个文档）添加文档的方法，不需要重新训练整个资料集或其部分。相反，我们将文档添加形式化为一个受限制的优化问题，以最小化网络参数的变化。这种方法比较应用训练整个资料集的方法快得多，但是其表现和重新训练整个资料集的方法相当，可以在实时更新文档搜寻系统，以便获取新的信息。我们的IncDSI代码可以在https://github.com/varshakishore/IncDSI 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Note-on-Hardness-of-Computing-Recursive-Teaching-Dimension"><a href="#A-Note-on-Hardness-of-Computing-Recursive-Teaching-Dimension" class="headerlink" title="A Note on Hardness of Computing Recursive Teaching Dimension"></a>A Note on Hardness of Computing Recursive Teaching Dimension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09792">http://arxiv.org/abs/2307.09792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pasin Manurangsi</li>
<li>for: 计算概念类（输入直接提供）的Recursive Teaching Dimension（RTD）问题需要 $n^{\Omega(\log n)}$ 时间，假设快速寻路假设（ETH）。</li>
<li>methods: 使用快速寻路假设（ETH）。</li>
<li>results: 得到 $n^{O(\log n)}$ 时间的解决方案，与笨方法 ($n^{O(\log n)}$)相同。<details>
<summary>Abstract</summary>
In this short note, we show that the problem of computing the recursive teaching dimension (RTD) for a concept class (given explicitly as input) requires $n^{\Omega(\log n)}$-time, assuming the exponential time hypothesis (ETH). This matches the running time $n^{O(\log n)}$ of the brute-force algorithm for the problem.
</details>
<details>
<summary>摘要</summary>
在这个短文中，我们证明计算概念类中的重ursive教学维度（RTD）问题需要$n^{\Omega(\log n)}$时间，假设快速幂时间假设（ETH）成立。这与布尔特 FORCE算法的运行时间 $n^{O(\log n)}$ 相同。
</details></li>
</ul>
<hr>
<h2 id="Reproducibility-in-Machine-Learning-Driven-Research"><a href="#Reproducibility-in-Machine-Learning-Driven-Research" class="headerlink" title="Reproducibility in Machine Learning-Driven Research"></a>Reproducibility in Machine Learning-Driven Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10320">http://arxiv.org/abs/2307.10320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harald Semmelrock, Simone Kopeinik, Dieter Theiler, Tony Ross-Hellauer, Dominik Kowald</li>
<li>for: 这个论文旨在探讨机器学习（ML）驱动的研究中存在的可重现性危机，以及这些研究领域中存在的可重现性问题和障碍。</li>
<li>methods: 该论文采用了文献综述的方法，检视了不同研究领域中ML可重现性的现状，并确定了可重现性问题和障碍。</li>
<li>results: 该论文发现了各种研究领域中ML可重现性的问题和障碍，并提出了一些可能的解决方案，包括使用ML平台等工具和实践。<details>
<summary>Abstract</summary>
Research is facing a reproducibility crisis, in which the results and findings of many studies are difficult or even impossible to reproduce. This is also the case in machine learning (ML) and artificial intelligence (AI) research. Often, this is the case due to unpublished data and/or source-code, and due to sensitivity to ML training conditions. Although different solutions to address this issue are discussed in the research community such as using ML platforms, the level of reproducibility in ML-driven research is not increasing substantially. Therefore, in this mini survey, we review the literature on reproducibility in ML-driven research with three main aims: (i) reflect on the current situation of ML reproducibility in various research fields, (ii) identify reproducibility issues and barriers that exist in these research fields applying ML, and (iii) identify potential drivers such as tools, practices, and interventions that support ML reproducibility. With this, we hope to contribute to decisions on the viability of different solutions for supporting ML reproducibility.
</details>
<details>
<summary>摘要</summary>
研究正面临一种复制危机，在许多研究中结果和发现很难或根本不可复制。这也是机器学习（ML）和人工智能（AI）研究中的问题。常常这是因为未发表的数据和/或源代码，以及对ML训练条件的敏感性。虽然研究社区中有许多解决此问题的方案，如使用ML平台，但ML驱动研究的复制性不增加的情况并没有改善。因此，在这次小访问中，我们会回顾ML驱动研究中的复制性情况，了解不同领域的ML复制性情况，识别ML复制性问题和障碍，以及可能支持ML复制性的工具、做法和措施。希望通过这些研究，对不同解决方案的可行性做出决策。
</details></li>
</ul>
<hr>
<h2 id="ZeroQuant-FP-A-Leap-Forward-in-LLMs-Post-Training-W4A8-Quantization-Using-Floating-Point-Formats"><a href="#ZeroQuant-FP-A-Leap-Forward-in-LLMs-Post-Training-W4A8-Quantization-Using-Floating-Point-Formats" class="headerlink" title="ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"></a>ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09782">http://arxiv.org/abs/2307.09782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
<li>paper_authors: Xiaoxia Wu, Zhewei Yao, Yuxiong He</li>
<li>for: 这个研究的目的是为大型语言模型（LLM）实现高效性和模型质量的平衡。</li>
<li>methods: 这个研究使用浮点数（FP）量化，特别是FP8和FP4，作为可能的解决方案，并进行了全面的探索。</li>
<li>results: 研究结果显示， для LLMs，FP8活化常常比其整数（INT8）相等者更好，尤其在模型中有多亿个参数时。 对于预算量化，我们发现FP4的表现与INT4相似或更好，使得在FP支持的硬件上进行部署变得更加简单。 我们还提出了两个缩减对称的精简方法，以减少因精度不同而导致的遗憾。 此外，我们将量化方法与LoRC策略结合，导致小型模型中的改进。 研究结果显示FP量化具有巨大的潜力，并且这种方法可以实现高效地部署在资源有限的环境中。<details>
<summary>Abstract</summary>
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints for weight quantization that negligibly impact the performance compared to the standard W4A8 model. We additionally enhance our quantization methods by integrating the Low Rank Compensation (LoRC) strategy, yielding improvements especially in smaller models. The results of our investigation emphasize the immense potential of FP quantization for LLMs, paving the way for high-efficiency deployment in resource-limited settings.
</details>
<details>
<summary>摘要</summary>
在大语言模型（LLM）领域中，实现计算效率和模型质量之间的平衡是一项具有挑战性的任务。在无格量化的环境中，特别是处理异常值时，我们发现了使用浮点数（FP）量化的可能性，特别是FP8和FP4。本研究的全面调查表明，对于LLMs，FP8活动通常比INT8等效，而且在模型参数超过一亿时，性能差异变得更加明显。对于权量量化，我们发现了FP4的性能与INT4相当，甚至超越INT4，这使得在FP支持的硬件上进行部署变得更加简单。为了减少精度对齐所带来的开销，我们提议了两种缩放约束，对于标准W4A8模型而言，影响非常小。此外，我们还增强了我们的量化方法，通过 интеграating Low Rank Compensation（LoRC）策略，尤其在较小的模型中，得到了改进。研究结果表明，FP量化对LLMs来说具有极大的潜力，为高效部署在有限资源的场景提供了道路。
</details></li>
</ul>
<hr>
<h2 id="Text2Layer-Layered-Image-Generation-using-Latent-Diffusion-Model"><a href="#Text2Layer-Layered-Image-Generation-using-Latent-Diffusion-Model" class="headerlink" title="Text2Layer: Layered Image Generation using Latent Diffusion Model"></a>Text2Layer: Layered Image Generation using Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09781">http://arxiv.org/abs/2307.09781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyang Zhang, Wentian Zhao, Xin Lu, Jeff Chien</li>
<li>for: 本研究旨在探讨层 composite workflow 中的图像生成问题，通过Diffusion模型的成功启发我们对层图像生成进行探索。</li>
<li>methods: 我们提议同时生成背景图像、前景图像、层Mask和组合图像，并使用自适应encoder来重建层图像和对抽象表示进行扩散模型训练。</li>
<li>results: 我们的方法可以生成高质量的层图像和层Mask，并为未来工作提供了一个新的标准。<details>
<summary>Abstract</summary>
Layer compositing is one of the most popular image editing workflows among both amateurs and professionals. Motivated by the success of diffusion models, we explore layer compositing from a layered image generation perspective. Instead of generating an image, we propose to generate background, foreground, layer mask, and the composed image simultaneously. To achieve layered image generation, we train an autoencoder that is able to reconstruct layered images and train diffusion models on the latent representation. One benefit of the proposed problem is to enable better compositing workflows in addition to the high-quality image output. Another benefit is producing higher-quality layer masks compared to masks produced by a separate step of image segmentation. Experimental results show that the proposed method is able to generate high-quality layered images and initiates a benchmark for future work.
</details>
<details>
<summary>摘要</summary>
层compositing是图像编辑中最受欢迎的工作流程之一，both amateur和professional都very popular。被diffusion模型的成功启发，我们从层图生成的角度出发，对层compositing进行探索。而不是生成一个图像，我们提议同时生成背景、前景、层mask和组合图像。为实现层图生成，我们训练了一个能够重建层图的autoencoder，并在层表示中训练diffusion模型。这个问题的两个benefit是：一是实现更好的compositing工作流程，二是生成更高质量的层mask，比起分开进行图像分割后生成的mask更高质量。实验结果表明，我们的方法可以生成高质量的层图并为未来的工作提供了参考。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Single-Feature-Importance-with-ICECREAM"><a href="#Beyond-Single-Feature-Importance-with-ICECREAM" class="headerlink" title="Beyond Single-Feature Importance with ICECREAM"></a>Beyond Single-Feature Importance with ICECREAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09779">http://arxiv.org/abs/2307.09779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Oesterle, Patrick Blöbaum, Atalanti A. Mastakouri, Elke Kirschbaum</li>
<li>for: 本文旨在描述一种用于解释模型输出和云计算应用失败的方法。</li>
<li>methods: 本文提出了一种基于联盟的信息量度量表示方法，用于掌握模型输出和失败原因。</li>
<li>results: 在synthetic和实际数据上进行了实验，并证明了该方法可以在解释和根本原因分析任务中减轻表达和排名的约束，并且在两个任务中具有卓越的准确率。<details>
<summary>Abstract</summary>
Which set of features was responsible for a certain output of a machine learning model? Which components caused the failure of a cloud computing application? These are just two examples of questions we are addressing in this work by Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM). Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable. This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods which can assign contributions only to individual factors and rank them by their importance. In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.
</details>
<details>
<summary>摘要</summary>
“我们在这个研究中使用 Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM) 方法来回答一些问题，例如：哪些功能集引起了某个机器学习模型的特定输出？哪些组件导致云 computing 应用程序的失败？我们提出一个信息理论基础的量化度量，用于衡量参数集对目标变量的分布的影响。这允许我们找出获得特定结果所必需的元素，而不是传统的解释和 causal contribution 分析方法，它们仅将贡献分配到个别的参数上，并按其重要性排名。在实验中，我们使用 synthetic 和实际数据，与现有的方法进行比较，结果显示 ICECREAM 在解释和根本原因分析任务中具有优秀的性能。”Note that Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore, while Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Polyffusion-A-Diffusion-Model-for-Polyphonic-Score-Generation-with-Internal-and-External-Controls"><a href="#Polyffusion-A-Diffusion-Model-for-Polyphonic-Score-Generation-with-Internal-and-External-Controls" class="headerlink" title="Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls"></a>Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10304">http://arxiv.org/abs/2307.10304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aik2mlj/polyffusion">https://github.com/aik2mlj/polyffusion</a></li>
<li>paper_authors: Lejun Min, Junyan Jiang, Gus Xia, Jingwei Zhao</li>
<li>for:  generating polyphonic music scores using a diffusion model</li>
<li>methods: uses internal control (pre-defined parts of the music) and external control (external yet related information such as chord, texture, or other features) via cross-attention mechanism</li>
<li>results: significantly outperforms existing Transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.Here’s the simplified Chinese version:</li>
<li>for: 通过扩散模型生成多重音乐谱</li>
<li>methods: 使用内部控制（用户先定义音乐的一部分）和外部控制（通过对应关系机制将外部信息引入）</li>
<li>results: 与现有的Transformer和采样基eline相比，表现出色，并且使用预训练分解表示者作为外部条件可以更有效地控制音乐生成。<details>
<summary>Abstract</summary>
We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing Transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.
</details>
<details>
<summary>摘要</summary>
我们提出了Polyffusion，一种扩散模型，通过将音乐视为像图像的钢琴谱表示，生成多重音乐分解。该模型具有可控的音乐生成功能，可以通过两种方法进行控制：内部控制和外部控制。内部控制指的是用户先定义部分音乐，然后让模型填充其余部分，类似于遮盖音乐生成（或音乐填充）任务。外部控制通过跨注意机制将外部 yet related的信息，如和声、xture或其他特征，用于控制模型。我们表明，通过内部和外部控制，Polyffusion可以统一许多音乐创作任务，包括给予伴奏的旋律生成、给予旋律的伴奏生成、任意音乐段填充和基于和声或xture的音乐安排。实验结果显示，我们的模型在与传统Transformer和采样基础之间显著超越了现有的基elines，并且使用预训练的分离表示作为外部条件可以更有效地控制。
</details></li>
</ul>
<hr>
<h2 id="Eliminating-Label-Leakage-in-Tree-Based-Vertical-Federated-Learning"><a href="#Eliminating-Label-Leakage-in-Tree-Based-Vertical-Federated-Learning" class="headerlink" title="Eliminating Label Leakage in Tree-Based Vertical Federated Learning"></a>Eliminating Label Leakage in Tree-Based Vertical Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10318">http://arxiv.org/abs/2307.10318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hideaki Takahashi, Jingjing Liu, Yang Liu</li>
<li>for: 这个论文是研究 vertical federated learning（VFL）中的攻击和防御机制的。</li>
<li>methods: 论文使用了一种新的标签推导攻击方法ID2Graph，通过利用训练样本中每个节点的集合（即实例空间）来推导私有训练标签。同时，论文还提出了一种有效的防御机制ID-LMID，通过互信息常量化来防止标签泄露。</li>
<li>results: 实验表明，ID2Graph攻击可以带来tree-based模型的泄露风险，而ID-LMID有效地mitigates label leakage in such instances。<details>
<summary>Abstract</summary>
Vertical federated learning (VFL) enables multiple parties with disjoint features of a common user set to train a machine learning model without sharing their private data. Tree-based models have become prevalent in VFL due to their interpretability and efficiency. However, the vulnerability of tree-based VFL has not been sufficiently investigated. In this study, we first introduce a novel label inference attack, ID2Graph, which utilizes the sets of record-IDs assigned to each node (i.e., instance space) to deduce private training labels. The ID2Graph attack generates a graph structure from training samples, extracts communities from the graph, and clusters the local dataset using community information. To counteract label leakage from the instance space, we propose an effective defense mechanism, ID-LMID, which prevents label leakage by focusing on mutual information regularization. Comprehensive experiments conducted on various datasets reveal that the ID2Graph attack presents significant risks to tree-based models such as Random Forest and XGBoost. Further evaluations on these benchmarks demonstrate that ID-LMID effectively mitigates label leakage in such instances.
</details>
<details>
<summary>摘要</summary>
vertically federated learning (VFL) 可以让多个党有不同特征的共同用户集中训练机器学习模型，而不需要共享私人数据。由于树状模型的可读性和效率，因此在VFL中它们变得普遍。然而，树状VFL的漏洞尚未得到充分调查。在本研究中，我们首先介绍了一种新的标签推断攻击，即ID2Graph攻击，它利用训练样本中每个节点（即实例空间）分配的集合ID来推断私人训练标签。ID2Graph攻击生成了训练样本中的图结构，提取了图中的社区信息，并使用社区信息对本地数据进行归一化。为了防止实例空间中的标签泄露，我们提议了一种有效的防御机制，即ID-LMID，它通过强调共轭信息规则来防止标签泄露。在各种数据集上进行了广泛的实验，显示ID2Graph攻击对树状模型，如Random Forest和XGBoost，具有重大风险。进一步在这些标准底上进行的评估表明，ID-LMID有效地减轻标签泄露的风险。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-WiFi-CSI-Based-Human-Activity-Recognition-A-Systematic-Study"><a href="#Self-Supervised-Learning-for-WiFi-CSI-Based-Human-Activity-Recognition-A-Systematic-Study" class="headerlink" title="Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study"></a>Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02412">http://arxiv.org/abs/2308.02412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JJJinx/SSLCSI">https://github.com/JJJinx/SSLCSI</a></li>
<li>paper_authors: Ke Xu, Jiangtao Wang, Hongyuan Zhu, Dingchang Zheng</li>
<li>for: 这篇论文主要是为了探讨 WiFi CSI 技术在人体动作识别（HAR）领域中的应用，以及如何使用深度学习技术和 SSL 算法来解决数据不充足问题。</li>
<li>methods: 该论文使用了多种 SSL 算法，包括已经研究过的和尚未研究过的类型，并在三个公开available的 CSI HAR 数据集上进行了广泛的实验研究。</li>
<li>results: 该论文的实验结果表明，使用 SSL 算法可以在 WiFi CSI 技术应用中提高 HAR 性能，但是还存在一些限制和盲点，需要进一步的改进才能在实际应用中使用。<details>
<summary>Abstract</summary>
Recently, with the advancement of the Internet of Things (IoT), WiFi CSI-based HAR has gained increasing attention from academic and industry communities. By integrating the deep learning technology with CSI-based HAR, researchers achieve state-of-the-art performance without the need of expert knowledge. However, the scarcity of labeled CSI data remains the most prominent challenge when applying deep learning models in the context of CSI-based HAR due to the privacy and incomprehensibility of CSI-based HAR data. On the other hand, SSL has emerged as a promising approach for learning meaningful representations from data without heavy reliance on labeled examples. Therefore, considerable efforts have been made to address the challenge of insufficient data in deep learning by leveraging SSL algorithms. In this paper, we undertake a comprehensive inventory and analysis of the potential held by different categories of SSL algorithms, including those that have been previously studied and those that have not yet been explored, within the field. We provide an in-depth investigation of SSL algorithms in the context of WiFi CSI-based HAR. We evaluate four categories of SSL algorithms using three publicly available CSI HAR datasets, each encompassing different tasks and environmental settings. To ensure relevance to real-world applications, we design performance metrics that align with specific requirements. Furthermore, our experimental findings uncover several limitations and blind spots in existing work, highlighting the barriers that need to be addressed before SSL can be effectively deployed in real-world WiFi-based HAR applications. Our results also serve as a practical guideline for industry practitioners and provide valuable insights for future research endeavors in this field.
</details>
<details>
<summary>摘要</summary>
近些年，因互联网物联网（IoT）的进步，WiFi CSI基本的人类活动识别（HAR）已经获得学术和工业社区的关注。通过融合深度学习技术与CSI基本的HAR，研究人员可以 дости得现场的性能，不需要专家知识。然而，CSI基本的数据短缺对应用深度学习模型在CSI基本的HAR领域中具有最大的挑战，因为CSI基本的数据具有隐私和不可理解的问题。一方面，SSL（匿名学习）已经成为一种可能地学习具有意义的表现，不需要大量的标注数据。因此，在深度学习应用中，对于CSI基本的数据短缺的挑战，SSL算法已经获得了大量的研究和应用。在这篇文章中，我们对WiFi CSI基本的HAR领域中SSL算法的应用进行了全面的调查和分析。我们对不同类型的SSL算法进行了深入的探访，包括已经研究过的和尚未研究过的类型。我们还对三个公开可用的CSI HAR数据集进行了实验，每个数据集都包含不同的任务和环境设定。为了保持现实应用的相关性，我们设计了适合实际应用的表现指标。我们的实验结果显示，SSL算法在WiFi CSI基本的HAR领域中具有许多局限和盲点，这些盲点需要被解决才能实现实际应用。我们的结果也可以作为实践工程师的实用指南，并提供了未来研究的有益指导。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Spatial-Temporal-Variational-Quantum-Circuit-to-Enable-Deep-Learning-on-NISQ-Devices"><a href="#A-Novel-Spatial-Temporal-Variational-Quantum-Circuit-to-Enable-Deep-Learning-on-NISQ-Devices" class="headerlink" title="A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices"></a>A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09771">http://arxiv.org/abs/2307.09771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyang Li, Zhepeng Wang, Zhirui Hu, Prasanna Date, Ang Li, Weiwen Jiang</li>
<li>For: This paper proposes a novel spatial-temporal design, named ST-VQC, to improve the accuracy and robustness of quantum learning algorithms for handling non-linear datasets and noisy environments.* Methods: The proposed ST-VQC integrates non-linearity in quantum learning through a block-based encoding quantum sub-circuit and layer-wise computation quantum sub-circuit, and adopts a SWAP-Free physical circuit design to improve robustness. An automated optimization framework is proposed to generate the ST-VQC quantum circuit.* Results: The proposed ST-VQC achieves over 30% accuracy improvement compared with existing VQCs on actual quantum computers, and outperforms a linear classifier by 27.9% on a non-linear synthetic dataset.<details>
<summary>Abstract</summary>
Quantum computing presents a promising approach for machine learning with its capability for extremely parallel computation in high-dimension through superposition and entanglement. Despite its potential, existing quantum learning algorithms, such as Variational Quantum Circuits(VQCs), face challenges in handling more complex datasets, particularly those that are not linearly separable. What's more, it encounters the deployability issue, making the learning models suffer a drastic accuracy drop after deploying them to the actual quantum devices. To overcome these limitations, this paper proposes a novel spatial-temporal design, namely ST-VQC, to integrate non-linearity in quantum learning and improve the robustness of the learning model to noise. Specifically, ST-VQC can extract spatial features via a novel block-based encoding quantum sub-circuit coupled with a layer-wise computation quantum sub-circuit to enable temporal-wise deep learning. Additionally, a SWAP-Free physical circuit design is devised to improve robustness. These designs bring a number of hyperparameters. After a systematic analysis of the design space for each design component, an automated optimization framework is proposed to generate the ST-VQC quantum circuit. The proposed ST-VQC has been evaluated on two IBM quantum processors, ibm_cairo with 27 qubits and ibmq_lima with 7 qubits to assess its effectiveness. The results of the evaluation on the standard dataset for binary classification show that ST-VQC can achieve over 30% accuracy improvement compared with existing VQCs on actual quantum computers. Moreover, on a non-linear synthetic dataset, the ST-VQC outperforms a linear classifier by 27.9%, while the linear classifier using classical computing outperforms the existing VQC by 15.58%.
</details>
<details>
<summary>摘要</summary>
量子计算技术在机器学习方面具有极大的潜力，它可以通过积分和异常相互作用来进行非常平行的计算，以处理高维数据。然而，现有的量子学习算法，如变量量子电路（VQC），在处理更复杂的数据集时会遇到挑战，特别是不可分割的数据集。此外，它还会遇到部署问题，使得学习模型在真实的量子设备上表现出较大的精度下降。为了解决这些局限性，这篇论文提出了一种新的空间-时间设计，即ST-VQC，以 интегрирова量子学习中的非线性。具体来说，ST-VQC可以通过一种新的块基编码量子子电路和层次计算量子子电路来提取空间特征，以实现时间 wise深度学习。此外，一种SWAP-Free物理电路设计也被提出，以提高robustness。这些设计带来了一些参数。经系统分析每个设计组件的设计空间，我们提出了一个自动优化框架，以生成ST-VQC量子电路。我们对IBM量子处理器ibmq_lima和ibm_cairo，它们具有27个量子比特和7个量子比特，进行了评估。结果表明，ST-VQC在标准的二分类数据集上的评价结果表明，ST-VQC可以在真实的量子计算机上实现30%的精度提高。此外，在一个非线性的synthetic数据集上，ST-VQC超过了一个线性分类器的27.9%的提高，而线性分类器使用классический计算机则超过了现有VQC的15.58%的提高。
</details></li>
</ul>
<hr>
<h2 id="How-Curvature-Enhance-the-Adaptation-Power-of-Framelet-GCNs"><a href="#How-Curvature-Enhance-the-Adaptation-Power-of-Framelet-GCNs" class="headerlink" title="How Curvature Enhance the Adaptation Power of Framelet GCNs"></a>How Curvature Enhance the Adaptation Power of Framelet GCNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09768">http://arxiv.org/abs/2307.09768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dshi3553usyd/curvature_enhanced_graph_convolution">https://github.com/dshi3553usyd/curvature_enhanced_graph_convolution</a></li>
<li>paper_authors: Dai Shi, Yi Guo, Zhiqi Shao, Junbin Gao</li>
<li>for: This paper aims to enhance the performance of graph neural networks (GNNs) by incorporating graph geometric information, specifically discrete graph Ricci curvature.</li>
<li>methods: The proposed approach uses the graph Ricci curvature defined on the edges of a graph to measure the difficulty of information transit between nodes. The curvature information is inserted into the GNN model with a carefully designed transformation function $\zeta$ to alleviate computational issues such as over-smoothing.</li>
<li>results: The proposed curvature-based GNN model outperforms state-of-the-art baselines in both homophily and heterophily graph datasets, indicating the effectiveness of involving graph geometric information in GNNs. Additionally, the curvature-based graph edge drop algorithm is proposed to drop edges with very positive Ricci curvature to enhance the model’s adaptation to heterophily graphs.Here is the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目的是使用图形神经网络（GNN）中的图形信息来提高模型性能。</li>
<li>methods: 提议的方法使用图中边的 Ricci  curvature 来衡量信息在不同节点之间的传递难度。 curvature 信息被注入到 GNN 模型中，并使用特定的变换函数 $\zeta$ 来缓解计算问题。</li>
<li>results: 提议的 curvature-based GNN 模型在同性和异性图像数据集上都有较高的性能，这表明将图形信息包含在 GNN 中是有效的。此外，提出了基于 Ricci  curvature 的图边Drop 算法，以便适应异性图。<details>
<summary>Abstract</summary>
Graph neural network (GNN) has been demonstrated powerful in modeling graph-structured data. However, despite many successful cases of applying GNNs to various graph classification and prediction tasks, whether the graph geometrical information has been fully exploited to enhance the learning performance of GNNs is not yet well understood. This paper introduces a new approach to enhance GNN by discrete graph Ricci curvature. Specifically, the graph Ricci curvature defined on the edges of a graph measures how difficult the information transits on one edge from one node to another based on their neighborhoods. Motivated by the geometric analogy of Ricci curvature in the graph setting, we prove that by inserting the curvature information with different carefully designed transformation function $\zeta$, several known computational issues in GNN such as over-smoothing can be alleviated in our proposed model. Furthermore, we verified that edges with very positive Ricci curvature (i.e., $\kappa_{i,j} \approx 1$) are preferred to be dropped to enhance model's adaption to heterophily graph and one curvature based graph edge drop algorithm is proposed. Comprehensive experiments show that our curvature-based GNN model outperforms the state-of-the-art baselines in both homophily and heterophily graph datasets, indicating the effectiveness of involving graph geometric information in GNNs.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Network (GNN) 已经在模型图structured数据中显示出强大的能力。然而，虽然 многи successfully applied GNNs to various graph classification and prediction tasks, whether the graph geometric information has been fully exploited to enhance the learning performance of GNNs is not yet well understood. This paper introduces a new approach to enhance GNN by discrete graph Ricci curvature. Specifically, the graph Ricci curvature defined on the edges of a graph measures how difficult the information transits on one edge from one node to another based on their neighborhoods. Motivated by the geometric analogy of Ricci curvature in the graph setting, we prove that by inserting the curvature information with different carefully designed transformation function $\zeta$, several known computational issues in GNN such as over-smoothing can be alleviated in our proposed model. Furthermore, we verified that edges with very positive Ricci curvature (i.e., $\kappa_{i,j} \approx 1$) are preferred to be dropped to enhance model's adaption to heterophily graph and one curvature based graph edge drop algorithm is proposed. Comprehensive experiments show that our curvature-based GNN model outperforms the state-of-the-art baselines in both homophily and heterophily graph datasets, indicating the effectiveness of involving graph geometric information in GNNs.
</details></li>
</ul>
<hr>
<h2 id="Sig-Splines-universal-approximation-and-convex-calibration-of-time-series-generative-models"><a href="#Sig-Splines-universal-approximation-and-convex-calibration-of-time-series-generative-models" class="headerlink" title="Sig-Splines: universal approximation and convex calibration of time series generative models"></a>Sig-Splines: universal approximation and convex calibration of time series generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09767">http://arxiv.org/abs/2307.09767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Magnus Wiese, Phillip Murray, Ralf Korn</li>
<li>for: 这个论文是为了提出一种新的生成模型，用于处理多变量离散时间序列数据。</li>
<li>methods: 该算法启发自神经抽屉流的建构，并将线性变换和签名变换作为神经网络的替换。这种方法不仅具有神经网络的通用性，还引入了模型参数的凸性。</li>
<li>results: 该模型可以实现不仅神经网络的通用性，还可以保证模型参数的凸性。<details>
<summary>Abstract</summary>
We propose a novel generative model for multivariate discrete-time time series data. Drawing inspiration from the construction of neural spline flows, our algorithm incorporates linear transformations and the signature transform as a seamless substitution for traditional neural networks. This approach enables us to achieve not only the universality property inherent in neural networks but also introduces convexity in the model's parameters.
</details>
<details>
<summary>摘要</summary>
我们提出一种新的生成模型，用于多变量离散时间序列数据。我们的算法启发自神经 spline flows 的结构，并在其中包含线性变换和签名变换，作为传统神经网络的替换。这种方法不仅具有神经网络的universality性，还会使模型参数变得几何。
</details></li>
</ul>
<hr>
<h2 id="Reinforcing-POD-based-model-reduction-techniques-in-reaction-diffusion-complex-networks-using-stochastic-filtering-and-pattern-recognition"><a href="#Reinforcing-POD-based-model-reduction-techniques-in-reaction-diffusion-complex-networks-using-stochastic-filtering-and-pattern-recognition" class="headerlink" title="Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition"></a>Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09762">http://arxiv.org/abs/2307.09762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Ajayakumar, Soumyendu Raha</li>
<li>for: 模型实际世界系统，但是这些系统的维度可能会使其分析变得困难。</li>
<li>methods: 使用dimensionality reduction技术，如POD，但这些模型容易受输入数据的干扰。</li>
<li>results: 我们的方法可以改善受到干扰输入的模型的准确性。<details>
<summary>Abstract</summary>
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
</details>
<details>
<summary>摘要</summary>
困难的网络被用来模型许多实际世界系统。然而，这些系统的维度可以使得它们变得复杂而难以分析。降维技术如POD可以在这些情况下使用。然而，这些模型容易受输入数据的扰动影响。我们提议一种算法框架，将 Pattern recognition 技术和 Stochastic filtering theory 结合使用，以提高这些模型的输出精度。我们的研究结果表明，我们的方法可以在受到扰动输入时提高模型的准确性。深度神经网络（DNNs）容易受到恶意攻击。然而，最近的研究表明，神经 Ordinary Differential Equations （ODEs）在某些应用场景中具有特殊的Robustness。我们将我们的算法框架与神经 ODE 方法作为参考进行比较。
</details></li>
</ul>
<hr>
<h2 id="FedBug-A-Bottom-Up-Gradual-Unfreezing-Framework-for-Federated-Learning"><a href="#FedBug-A-Bottom-Up-Gradual-Unfreezing-Framework-for-Federated-Learning" class="headerlink" title="FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning"></a>FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10317">http://arxiv.org/abs/2307.10317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iandrover/fedbug">https://github.com/iandrover/fedbug</a></li>
<li>paper_authors: Chia-Hsiang Kao, Yu-Chiang Frank Wang</li>
<li>for: 该论文旨在提出一种新的 Federated Learning（FL）框架，以解决客户端遗传的问题。</li>
<li>methods: 该论文提出了一种名为 FedBug（Federated Learning with Bottom-Up Gradual Unfreezing）的新的 FL 框架，它在客户端上采用层 wise 的渐进解冻策略，从输入层到输出层，以实现跨客户端的Alignment。</li>
<li>results: 该论文通过理论分析和实验 validate 了 FedBug 的高效性，并且在不同的数据集、训练条件和网络架构下进行了广泛的测试。<details>
<summary>Abstract</summary>
Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze FedBug in a novel over-parameterization FL setup, revealing its superior convergence rate compared to FedAvg. Through comprehensive experiments, spanning various datasets, training conditions, and network architectures, we validate the efficacy of FedBug. Our contributions encompass a novel FL framework, theoretical analysis, and empirical validation, demonstrating the wide potential and applicability of FedBug.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）提供了一个合作训练框架，让多个客户端共同训练一个共享模型，不会妥协数据隐私。由于本地数据组的不同性，客户端模型的更新可能会过滤和分化，即client drift问题。在这篇文章中，我们提出了FedBug（联合学习 with Bottom-Up Gradual Unfreezing），一个新的FL框架，用于有效地解决client drift问题。FedBug在服务器端每个全球轮次释出的客户端模型参数作为参考点，进行跨客户Alignment。具体来说，在客户端上，FedBug首先将整个模型冻结，然后逐层解冻，从输入层到输出层。这种底上逐层的方法让模型在新解冻的层上训练，将数据 проек 到一个共同的潜在空间，其中的分隔条件保持一致 across all clients。我们在一个新的FL设置下进行了理论分析，显示FedBug的渐进速度比FedAvg更高。通过了多种数据集、训练条件和网络架构，我们验证了FedBug的有效性。我们的贡献包括一个新的FL框架、理论分析和实验验证，展示了FedBug的广泛应用和可能性。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Extreme-Learning-Machines-with-zero-Spectral-Bias"><a href="#Constructing-Extreme-Learning-Machines-with-zero-Spectral-Bias" class="headerlink" title="Constructing Extreme Learning Machines with zero Spectral Bias"></a>Constructing Extreme Learning Machines with zero Spectral Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09759">http://arxiv.org/abs/2307.09759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaumudi Joshi, Vukka Snigdha, Arya Kumar Bhattacharya</li>
<li>for: This paper aims to address the issue of Spectral Bias (SB) in Extreme Learning Machines (ELMs) and its impact on the resolution of higher frequencies, which is crucial in fields like Physics Informed Neural Networks (PINNs).</li>
<li>methods: The paper uses Fourier Feature Embeddings to mitigate SB in ELMs, which has been shown to be effective in addressing this issue in other types of Artificial Neural Networks (ANNs).</li>
<li>results: The paper demonstrates that the proposed approach completely eliminates SB in ELMs, making them feasible for practical problems like PINNs where resolution of higher frequencies is essential.<details>
<summary>Abstract</summary>
The phenomena of Spectral Bias, where the higher frequency components of a function being learnt in a feedforward Artificial Neural Network (ANN) are seen to converge more slowly than the lower frequencies, is observed ubiquitously across ANNs. This has created technology challenges in fields where resolution of higher frequencies is crucial, like in Physics Informed Neural Networks (PINNs). Extreme Learning Machines (ELMs) that obviate an iterative solution process which provides the theoretical basis of Spectral Bias (SB), should in principle be free of the same. This work verifies the reliability of this assumption, and shows that it is incorrect. However, the structure of ELMs makes them naturally amenable to implementation of variants of Fourier Feature Embeddings, which have been shown to mitigate SB in ANNs. This approach is implemented and verified to completely eliminate SB, thus bringing into feasibility the application of ELMs for practical problems like PINNs where resolution of higher frequencies is essential.
</details>
<details>
<summary>摘要</summary>
现象 known as Spectral Bias（спектральная предвзятость），其中高频成分函数在Feedforward Artificial Neural Network（ANN）中学习时 convergence  slower than low frequency components, 是在 ANN 中广泛观察到的。这种情况在 Physics Informed Neural Networks（PINNs）等领域带来了技术挑战，因为在这些领域，高频分量的解析是关键。Extreme Learning Machines（ELMs），它们不需要迭代解决过程，因此可以避免 Spectral Bias（SB）的理论基础。然而，实际上，ELMs 中的结构使其自然地适合实现 variants of Fourier Feature Embeddings（FFE），这种方法已经在 ANN 中证明可以 Mitigate SB。本文验证了这种方法的可靠性，并证明了它可以完全消除 SB，因此使得 ELMs 在实际应用中，如 PINNs，可以实现高频分量的解析。
</details></li>
</ul>
<hr>
<h2 id="Improved-Distribution-Matching-for-Dataset-Condensation"><a href="#Improved-Distribution-Matching-for-Dataset-Condensation" class="headerlink" title="Improved Distribution Matching for Dataset Condensation"></a>Improved Distribution Matching for Dataset Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09742">http://arxiv.org/abs/2307.09742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uitrbn/idm">https://github.com/uitrbn/idm</a></li>
<li>paper_authors: Ganlong Zhao, Guanbin Li, Yipeng Qin, Yizhou Yu</li>
<li>for: 这个研究旨在开发一种能够将大量数据集整合成小数据集，并且能够训练高性能模型，以减少深度学习应用中的储存成本和训练努力。</li>
<li>methods: 本研究使用分布匹配法来实现数据集整合，并且提出三种新技术来解决对分布匹配的两大缺陷（即分布特征数量不均匀和无效的嵌入）。</li>
<li>results:  compared to先前的优化方法，本研究的方法更加高效，可以处理更大的数据集和模型，并且在实验中得到了更好的效果。<details>
<summary>Abstract</summary>
Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising. Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization). Our simple yet effective method outperforms most previous optimization-oriented methods with much fewer computational resources, thereby scaling data condensation to larger datasets and models. Extensive experiments demonstrate the effectiveness of our method. Codes are available at https://github.com/uitrbn/IDM
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mood-Classification-of-Bangla-Songs-Based-on-Lyrics"><a href="#Mood-Classification-of-Bangla-Songs-Based-on-Lyrics" class="headerlink" title="Mood Classification of Bangla Songs Based on Lyrics"></a>Mood Classification of Bangla Songs Based on Lyrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10314">http://arxiv.org/abs/2307.10314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor</li>
<li>for: 本研究旨在分析孟加拉语歌曲的情感，并通过自然语言处理和BERT算法来分类歌曲的情感。</li>
<li>methods: 该研究使用自然语言处理和BERT算法来分析4000首孟加拉语歌曲的 lyrics，并将歌曲分为四种情感：快乐、悲伤、爱情和宁静。</li>
<li>results: 研究发现，4000首歌曲中有1513首表达悲伤情感，1362首表达爱情情感，886首表达快乐情感，剩下的239首表达宁静情感。通过嵌入歌曲 lyrics，该研究准确地分类了歌曲的四种情感。<details>
<summary>Abstract</summary>
Music can evoke various emotions, and with the advancement of technology, it has become more accessible to people. Bangla music, which portrays different human emotions, lacks sufficient research. The authors of this article aim to analyze Bangla songs and classify their moods based on the lyrics. To achieve this, this research has compiled a dataset of 4000 Bangla song lyrics, genres, and used Natural Language Processing and the Bert Algorithm to analyze the data. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362 for the romantic mood, 886 for happiness, and the rest 239 are classified as relaxation. By embedding the lyrics of the songs, the authors have classified the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is crucial as it enables a multi-class classification of songs' moods, making the music more relatable to people's emotions. The article presents the automated result of the four moods accurately derived from the song lyrics.
</details>
<details>
<summary>摘要</summary>
音乐可以诱发不同的情感，而技术的进步使得音乐更加 accessible。孟加拉音乐，表达不同人类情感的音乐，尚未得到足够的研究。本文的作者希望通过分析孟加拉歌曲的歌词，根据歌词来分类它们的情感。为此，本研究编译了4000首孟加拉歌曲的歌词、种类，并使用自然语言处理和BERT算法来分析数据。其中4000首歌曲中，1513首表达了悲伤的情感，1362首表达了爱情的情感，886首表达了快乐的情感，剩下的239首被分类为放松的情感。通过嵌入歌曲歌词，作者将歌曲分为四种情感：快乐、悲伤、爱情和放松。这项研究非常重要，因为它使得音乐与人们的情感更加相似，使得更多人能够通过音乐来表达自己的情感。文章公布了自动分类出的四种情感的准确结果。
</details></li>
</ul>
<hr>
<h2 id="RaTE-a-Reproducible-automatic-Taxonomy-Evaluation-by-Filling-the-Gap"><a href="#RaTE-a-Reproducible-automatic-Taxonomy-Evaluation-by-Filling-the-Gap" class="headerlink" title="RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap"></a>RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09706">http://arxiv.org/abs/2307.09706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cestlucas/rate">https://github.com/cestlucas/rate</a></li>
<li>paper_authors: Tianjian Gao, Phillipe Langlais</li>
<li>for: 这篇论文主要用于提出一种自动生成税onomy的评估方法，以便用于评估自动生成税onomy算法的效果。</li>
<li>methods: 该论文使用了一种基于大量预训练语言模型的自动标签 свобо Taxonomy 评估方法，称为RaTE。</li>
<li>results: 研究发现，RaTE 与人类评估结果相关度高，并且人工降低税onomy 会导致 RaTE 分数下降。<details>
<summary>Abstract</summary>
Taxonomies are an essential knowledge representation, yet most studies on automatic taxonomy construction (ATC) resort to manual evaluation to score proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just as important as taxonomy construction. We propose RaTE, an automatic label-free taxonomy scoring procedure, which relies on a large pre-trained language model. We apply our evaluation procedure to three state-of-the-art ATC algorithms with which we built seven taxonomies from the Yelp domain, and show that 1) RaTE correlates well with human judgments and 2) artificially degrading a taxonomy leads to decreasing RaTE score.
</details>
<details>
<summary>摘要</summary>
《taxonomies是知识表示的重要方面，但大多数自动taxonomy建构(ATC)研究仍然采用手动评估提议的算法。我们认为自动taxonomy评估(ATE)也是非常重要。我们提出了一种无标签的自动评估方法，名为RaTE，它基于大量预训练的自然语言模型。我们对三种当前领先ATC算法进行了应用，并在Yelp领域建立了七个稿件，并证明了以下两点：1）RaTE与人类评估呈相关性，2） искусственно降低稿件会导致RaTE分数下降。》Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Guided-Generation-for-Large-Language-Models"><a href="#Efficient-Guided-Generation-for-Large-Language-Models" class="headerlink" title="Efficient Guided Generation for Large Language Models"></a>Efficient Guided Generation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09702">http://arxiv.org/abs/2307.09702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/normal-computing/outlines">https://github.com/normal-computing/outlines</a></li>
<li>paper_authors: Brandon T. Willard, Rémi Louf</li>
<li>for: 这篇论文主要针对文本生成问题进行了构建性的重新定义，它基于finite-state machine的状态转移来实现有效的文本生成 guideline。</li>
<li>methods: 该方法使用了regular expressions和context-free grammar来控制文本生成，可以构建语言模型词汇表的索引，并且允许执行域专业知识和约束。</li>
<li>results: 该方法可以减少TokenSequence生成过程中的开销，并且在比较试验中显著超越现有的解决方案。一个开源的Python库Outlines提供了实现。<details>
<summary>Abstract</summary>
In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们示例如如何将神经文本生成问题转换成 finite-state machine 的状态转移问题。这个框架导致了一种高效的方法来使用正则表达式和 context-free  грамматики来引导文本生成，并允许建立语言模型词汇索引。这种方法是模型不偏向的，允许承载领域特定知识和约束，并允许建立可靠的界面，保证生成的文本结构正确。它增加了Token序列生成过程中的负担，并在性能上明显超过现有的解决方案。我们在 Python 开源库 Outlines 中提供了一个实现。
</details></li>
</ul>
<hr>
<h2 id="STRAPPER-Preference-based-Reinforcement-Learning-via-Self-training-Augmentation-and-Peer-Regularization"><a href="#STRAPPER-Preference-based-Reinforcement-Learning-via-Self-training-Augmentation-and-Peer-Regularization" class="headerlink" title="STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization"></a>STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09692">http://arxiv.org/abs/2307.09692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rll-research/bpref">https://github.com/rll-research/bpref</a></li>
<li>paper_authors: Yachen Kang, Li He, Jinxin Liu, Zifeng Zhuang, Donglin Wang</li>
<li>for: 学习复杂的奖励函数，使用人类偏好作为学习目标。</li>
<li>methods: 使用 preference-based reinforcement learning (PbRL) 方法，并对不带标签的段使用 reuse 技术，以降低人类努力的成本。 consistency regularization 也被考虑以提高 semi-supervised learning 的性能。</li>
<li>results: 对于不同的 semi-supervised 选择和 peer regularization，我们的方法可以学习出多种 locomotive 和机器人 manipulate 行为。然而，我们发现在 PbRL 中存在一种 Similarity Trap 现象，导致 consistency regularization 不当地增强了模型对段对的一致性，从而降低了奖励学习的信任度。我们的自动训练方法和提出的 peer regularization 可以解决这个问题，并且实验证明了我们的方法的效果。<details>
<summary>Abstract</summary>
Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's predictions between segment pairs, and thus reduces the confidence in reward learning, since the augmented distribution does not match with the original one in PbRL. To overcome such issue, we present a self-training method along with our proposed peer regularization, which penalizes the reward model memorizing uninformative labels and acquires confident predictions. Empirically, we demonstrate that our approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization.
</details>
<details>
<summary>摘要</summary>
preference-based reinforcement learning (PbRL) 承诺学习复杂的奖励函数使用二进制人类偏好。然而，这种人 loop 形式需要较大的人类努力来分配偏好标签对 segment pair，限制其大规模应用。 recent approach 尝试 reuse unlabeled segments，这些 segment 隐式地描述了分布，从而减少人类努力。 In addition, consistency regularization 是进一步考虑的，以提高 semi-supervised learning 的性能。然而，我们注意到，与通用的分类任务不同，在 PbRL 中存在一种特殊的现象，我们在这篇论文中定义为 similarity trap。人类可能对类似 segment pair 表示对立的偏好，但这种相似性可能会诱导 consistency regularization 失败在 PbRL 中。由于存在 similarity trap，consistency regularization 可能会增强模型对 segment pair 的预测一致性，从而降低奖励学习的信任度，因为扩展的分布与原始分布在 PbRL 中不匹配。为了解决这个问题，我们提出了一种 self-training 方法，并与我们的提议的 peer regularization 一起使用。 peer regularization 会对奖励模型的预测进行约束，以避免模型记忆无用的标签。我们的方法可以在不同的 semi-supervised 变体和 peer regularization 下学习多种 locomotion 和 robotic manipulation 行为。
</details></li>
</ul>
<hr>
<h2 id="Joint-Service-Caching-Communication-and-Computing-Resource-Allocation-in-Collaborative-MEC-Systems-A-DRL-based-Two-timescale-Approach"><a href="#Joint-Service-Caching-Communication-and-Computing-Resource-Allocation-in-Collaborative-MEC-Systems-A-DRL-based-Two-timescale-Approach" class="headerlink" title="Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach"></a>Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09691">http://arxiv.org/abs/2307.09691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianqian Liu, Haixia Zhang, Xin Zhang, Dongfeng Yuan</li>
<li>for: 提高多元Access Edge Computing（MEC）系统中端点的功能，以满足终端设备的严格服务质量（QoS）要求。</li>
<li>methods: 提议一种协作式MEC框架，以便在边缘服务器之间分享资源，并通过缓存、协同卸载和计算和通信资源的均衡来最大化长期QoS和减少缓存切换成本。</li>
<li>results: 通过对问题的解决，提高了平均QoS和缓存切换成本。<details>
<summary>Abstract</summary>
Meeting the strict Quality of Service (QoS) requirements of terminals has imposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems, due to the limited multidimensional resources. To address this challenge, we propose a collaborative MEC framework that facilitates resource sharing between the edge servers, and with the aim to maximize the long-term QoS and reduce the cache switching cost through joint optimization of service caching, collaborative offfoading, and computation and communication resource allocation. The dual timescale feature and temporal recurrence relationship between service caching and other resource allocation make solving the problem even more challenging. To solve it, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). In doing so, we reformulate the optimization problem as a Markov decision process (MDP) where the small-timescale resource allocation decisions generated by an improved GA are taken as the states and input into a centralized LSTM-DDPG agent to generate the service caching decision for the large-timescale. Simulation results demonstrate that our proposed algorithm outperforms the baseline algorithms in terms of the average QoS and cache switching cost.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which consists of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). We reformulate the optimization problem as a Markov decision process (MDP) where small-timescale resource allocation decisions generated by an improved GA are taken as states and input into a centralized LSTM-DDPG agent to generate service caching decisions for the large-timescale.Simulation results show that our proposed algorithm outperforms baseline algorithms in terms of average QoS and cache switching costs.
</details></li>
</ul>
<hr>
<h2 id="Amazon-M2-A-Multilingual-Multi-locale-Shopping-Session-Dataset-for-Recommendation-and-Text-Generation"><a href="#Amazon-M2-A-Multilingual-Multi-locale-Shopping-Session-Dataset-for-Recommendation-and-Text-Generation" class="headerlink" title="Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation"></a>Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09688">http://arxiv.org/abs/2307.09688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen, Haoyu Han, Hanqing Lu, Zhengyang Wang, Ruirui Li, Zhen Li, Monica Xiao Cheng, Rahul Goutam, Haiyang Zhang, Karthik Subbian, Suhang Wang, Yizhou Sun, Jiliang Tang, Bing Yin, Xianfeng Tang</li>
<li>for: 这个论文的目的是提高电商个性化推荐，以提高用户体验和参与度。</li>
<li>methods: 这篇论文使用了用户会话数据来预测用户的下一个交互，并在不同的语言和地区上进行了多样化的数据采集。</li>
<li>results: 论文提出了一个新的多语言多地区的用户会话数据集，可以帮助提高个性化推荐和理解用户偏好，并且可以用于各种现有任务以及新任务的研究和实践。<details>
<summary>Abstract</summary>
Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks. To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation. With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice. In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 and have attracted thousands of users and submissions. The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.
</details>
<details>
<summary>摘要</summary>
modelo de intenciones de compras de los clientes es una tarea crucial para el comercio electrónico, ya que directamente afecta la experiencia del usuario y la participación. Por lo tanto, comprender preferencias de los clientes de manera precisa es esencial para brindar recomendaciones personalizadas. La recomendación de sesión, que utiliza datos de sesiones de clientes para predecir su próxima interacción, ha ganado popularidad en la actualidad. Sin embargo, los conjuntos de datos de sesión existentes tienen limitaciones en términos de atributos de item, diversidad de usuarios y escala de datos. Como resultado, no pueden abarcar completamente el espectro de comportamientos y preferencias de los usuarios. Para superar este gap, presentamos el Dataset de Sesiones de Compras Multilingüe de Amazon, conocido como Amazon-M2. Es el primer conjunto de datos multilingüe que consta de millones de sesiones de usuarios de seis locales diferentes, donde las lenguas principales de los productos son inglés, alemán, japonés, francés, italiano y español. Destacablemente, el conjunto de datos puede ayudarnos a mejorar la personalización y la comprensión de las preferencias de los usuarios, lo que puede beneficiar diversas tareas existentes e incluso permitir nuevas tareas. Para probar el potencial del conjunto de datos, introducimos tres tareas en este trabajo: (1) recomendación de productos siguientes, (2) recomendación de productos siguientes con cambios de dominio, y (3) generación de títulos de productos siguientes. Con estas tareas, evaluamos una variedad de algoritmos en nuestro conjunto de datos propuesto, obteniendo nuevos hallazgos para la investigación y la práctica. Además, basado en nuestro conjunto de datos y tareas, organizamos una competencia en la KDD CUP 2023 y atraemos a miles de usuarios y suscripciones. Los soluciones ganadoras y el trabajo asociado se pueden acceder en nuestro sitio web <https://kddcup23.github.io/>.
</details></li>
</ul>
<hr>
<h2 id="Convex-Geometry-of-ReLU-layers-Injectivity-on-the-Ball-and-Local-Reconstruction"><a href="#Convex-Geometry-of-ReLU-layers-Injectivity-on-the-Ball-and-Local-Reconstruction" class="headerlink" title="Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction"></a>Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09672">http://arxiv.org/abs/2307.09672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danedane-haider/alpha-rectifying-frames">https://github.com/danedane-haider/alpha-rectifying-frames</a></li>
<li>paper_authors: Daniel Haider, Martin Ehler, Peter Balazs</li>
<li>for: 研究ReLU层的具体输入 Vector 的可逆性，并提供一种可行的方法来确保 ReLU 层的可逆性。</li>
<li>methods: 使用框架理论设定，研究 ReLU 层在closed ball 上的具体输入 Vector 的可逆性，并利用 convex geometry 的视角来提供一种计算可行的方法。</li>
<li>results: 提供了一种可逆性判断方法，并提供了一种具体的重建方法，可以用于任何输入 Vector 在 ball 上。<details>
<summary>Abstract</summary>
The paper uses a frame-theoretic setting to study the injectivity of a ReLU-layer on the closed ball of $\mathbb{R}^n$ and its non-negative part. In particular, the interplay between the radius of the ball and the bias vector is emphasized. Together with a perspective from convex geometry, this leads to a computationally feasible method of verifying the injectivity of a ReLU-layer under reasonable restrictions in terms of an upper bound of the bias vector. Explicit reconstruction formulas are provided, inspired by the duality concept from frame theory. All this gives rise to the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.
</details>
<details>
<summary>摘要</summary>
文章使用框理论设定来研究具有$\mathbb{R}^n$闭球的ReLU层的具体性。特别是，文章强调半径和偏置向量之间的交互作用。通过几何学的视角，这导致了对ReLU层的具体性进行有限制的 computationally feasible 验证方法。 besides, the paper provides explicit reconstruction formulas, inspired by the duality concept from frame theory. all this allows for the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.Here's the text in Traditional Chinese:文章使用框理论设定来研究具有$\mathbb{R}^n$闭球的ReLU层的具体性。特别是，文章强调半径和偏置向量之间的交互作用。通过几何学的视角，这导致了对ReLU层的具体性进行有限制的 computationally feasible 验证方法。 besides, the paper provides explicit reconstruction formulas, inspired by the duality concept from frame theory. all this allows for the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.
</details></li>
</ul>
<hr>
<h2 id="JAZZVAR-A-Dataset-of-Variations-found-within-Solo-Piano-Performances-of-Jazz-Standards-for-Music-Overpainting"><a href="#JAZZVAR-A-Dataset-of-Variations-found-within-Solo-Piano-Performances-of-Jazz-Standards-for-Music-Overpainting" class="headerlink" title="JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting"></a>JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09670">http://arxiv.org/abs/2307.09670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleanor Row, Jingjing Tang, George Fazekas</li>
<li>for: 这个论文是为了描述一个 JazzVAR 数据集，用于研究 jazz 标准曲目的变奏和表演方式。</li>
<li>methods: 论文使用了人工提取的 JazzVAR 数据集，包含 502 对变奏和原始 MIDI 段落。每个变奏段落都有对应的原始段落，包含旋律和和声。</li>
<li>results: 论文提出了一种新的生成音乐任务 - 音乐覆盖，并提出了一个基于 Transformer 模型的基线模型，用于这个任务。此外，数据集还可用于表达性表演分析和演奏者识别等其他应用。<details>
<summary>Abstract</summary>
Jazz pianists often uniquely interpret jazz standards. Passages from these interpretations can be viewed as sections of variation. We manually extracted such variations from solo jazz piano performances. The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments. Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from the original jazz standard. Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often focus on improvisation sections within jazz performances. In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the Original and Variation pairs, and our analysis of the dataset. We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on the JAZZVAR dataset for this task. Other potential applications of our dataset include expressive performance analysis and performer identification.
</details>
<details>
<summary>摘要</summary>
爵士钢琴家常会有独特的表演方式，这些表演方式可以被视为旋律的变化部分。我们 manually提取了这些变化段落从爵士钢琴独奏表演中。JAZZVAR数据集是一个收集502对变化和原始MIDI段落的集合。每个变化段落在数据集中都是由原始爵士标准旋律和和声的一部分，而不是 improvise部分。在这篇论文中，我们介绍了获取和排序 реперtoire的过程，以及创建原始和变化段落的管道。我们还对数据集进行分析，并介绍了一种新的音乐任务——音乐覆盖，以及基于JAZZVAR数据集的这种任务的基线Transformer模型。此外，JAZZVAR数据集还有其他潜在应用，如表演技巧分析和演奏者识别。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Unified-Agent-with-Foundation-Models"><a href="#Towards-A-Unified-Agent-with-Foundation-Models" class="headerlink" title="Towards A Unified Agent with Foundation Models"></a>Towards A Unified Agent with Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09668">http://arxiv.org/abs/2307.09668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, Martin Riedmiller</li>
<li>for: 本文旨在探讨如何将语言模型和视觉语言模型 embedding into 强化学习（RL）代理人，以便利用其理解人类意图、逻辑、场景理解和计划行为等能力。</li>
<li>methods: 本文提出了一个框架，用语言作为核心思维工具，以解决RL中一些基本挑战，如高效探索、重用经验数据、调度技能和学习从观察中学习。</li>
<li>results: 我们在一个具有缺乏奖励的虚拟机器人搅拌环境中测试了我们的方法，并证明了在探索效率和重用经验数据方面获得了显著性能提升，并示出了如何重用已学到的技能解决新任务或模仿人类专家视频。<details>
<summary>Abstract</summary>
Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts.
</details>
<details>
<summary>摘要</summary>
现在的语言模型和视觉语言模型已经展示了对人类意图、逻辑、场景理解和规划行为的无 precedent的能力，以文本形式为主。在这个工作中，我们investigate如何将这些能力 embedding在奖励学习（RL）代理人中。我们设计了一个框架，用语言作为核心思维工具，探索如何使得代理人可以解决一系列基本RL挑战，如高效探索、重用经验数据、调度技能和从观察中学习，这些传统需要分立的算法。我们在一个具有缺少奖励的模拟 robotic manipulation 环境中测试了我们的方法，并示出了对基eline的显著性能提高，包括探索效率和可以重用数据集的 reuse。此外，我们还 illustrate如何 reuse学习的技能来解决新任务或者模仿人类专家的视频。
</details></li>
</ul>
<hr>
<h2 id="Anticipating-Technical-Expertise-and-Capability-Evolution-in-Research-Communities-using-Dynamic-Graph-Transformers"><a href="#Anticipating-Technical-Expertise-and-Capability-Evolution-in-Research-Communities-using-Dynamic-Graph-Transformers" class="headerlink" title="Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers"></a>Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09665">http://arxiv.org/abs/2307.09665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameera Horawalavithana, Ellyn Ayton, Anastasiya Usenko, Robin Cosbey, Svitlana Volkova</li>
<li>for: 这项研究的目的是为了预测技术能力和技能的发展趋势，以便提高国家和全球安全，特别是在核不扩散领域和迅速发展的人工智能领域。</li>
<li>methods: 这项研究使用了传统的统计关系学学习方法（如链接预测在合作网络中），并使用动态不同类型图表示法来解决技术能力和技能发展的问题。</li>
<li>results: 研究人员们开发了一种动态图变换（DGT）神经网络模型，可以预测科学家和机构之间的合作 Patterns、作者行为和技能发展趋势，并在人工智能和核不扩散领域中达到了state-of-the-art的性能。<details>
<summary>Abstract</summary>
The ability to anticipate technical expertise and capability evolution trends globally is essential for national and global security, especially in safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging fields like artificial intelligence (AI). In this work, we extend traditional statistical relational learning approaches (e.g., link prediction in collaboration networks) and formulate a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations. We develop novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities (e.g., scientist and institution levels) in two distinct research fields. We implement a dynamic graph transformer (DGT) neural architecture, which pushes the state-of-the-art graph neural network models by (a) forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b) relying on both discrete -- and continuous -- time inputs. We demonstrate that our DGT models predict collaboration, partnership, and expertise patterns with 0.26, 0.73, and 0.53 mean reciprocal rank values for AI and 0.48, 0.93, and 0.22 for NN domains. DGT model performance exceeds the best-performing static graph baseline models by 30-80% across AI and NN domains. Our findings demonstrate that DGT models boost inductive task performance, when previously unseen nodes appear in the test data, for the domains with emerging collaboration patterns (e.g., AI). Specifically, models accurately predict which established scientists will collaborate with early career scientists and vice-versa in the AI domain.
</details>
<details>
<summary>摘要</summary>
可以预测技术专业和能力发展趋势是国家和全球安全的关键，尤其是在安全关键领域如核不扩散（NN）和快速出现的领域如人工智能（AI）。在这项工作中，我们扩展了传统的统计关系学学习方法（例如链接预测在合作网络中），并将技术专业和能力发展的问题转化为动态不同类型图表示。我们开发了新的能力来预测合作模式、作者行为和技术能力的演变，并在不同粒度（例如科学家和机构层次）上进行预测。我们实现了动态图变换（DGT）神经网络模型，它超越了当前最佳STATIC GRAPH模型，通过（a）预测不同类型的节点和边（而不是同类型的节点和边），以及（b）使用时间输入。我们的DGT模型在AI和NN领域中预测了合作、合作伙伴和技术能力的模式，其中AI领域的mean reciprocal rank值为0.26、0.73和0.53，NN领域的mean reciprocal rank值为0.48、0.93和0.22。DGT模型的性能超过了最佳静止GRAPH模型的30-80%。我们的发现表明，DGT模型可以提高适应任务性能，当前没有seen节点出现在测试数据中，特别是在AI领域。具体来说，模型可以准确预测知名科学家和初出茅廊的科学家之间的合作在AI领域。
</details></li>
</ul>
<hr>
<h2 id="Physics-based-Reduced-Order-Modeling-for-Uncertainty-Quantification-of-Guided-Wave-Propagation-using-Bayesian-Optimization"><a href="#Physics-based-Reduced-Order-Modeling-for-Uncertainty-Quantification-of-Guided-Wave-Propagation-using-Bayesian-Optimization" class="headerlink" title="Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization"></a>Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09661">http://arxiv.org/abs/2307.09661</a></li>
<li>repo_url: None</li>
<li>paper_authors: G. I. Drakoulas, T. V. Gortsas, D. Polyzos</li>
<li>for: 这 paper 的目的是提出一种基于机器学习的减少阶模型（BO-ML-ROM），用于提高 condition-based maintenance 中的STRUCTURAL HEALTH MONITORING（SHM）的计算效率。</li>
<li>methods: 这 paper 使用了 Bayesian optimization 框架和 finite element method 来实现 BO-ML-ROM，并通过 Sobol 指数来实现全局敏感分析。</li>
<li>results: 实验结果显示，Bayesian optimization 比一般采样方法更高效和快速，BO-ML-ROM 可以减少 GWP 的计算成本，并且可以准确地预测 SHM 中的结果。<details>
<summary>Abstract</summary>
In the context of digital twins, structural health monitoring (SHM) constitutes the backbone of condition-based maintenance, facilitating the interconnection between virtual and physical assets. Guided wave propagation (GWP) is commonly employed for the inspection of structures in SHM. However, GWP is sensitive to variations in the material properties of the structure, leading to false alarms. In this direction, uncertainty quantification (UQ) is regularly applied to improve the reliability of predictions. Computational mechanics is a useful tool for the simulation of GWP, and is often applied for UQ. Even so, the application of UQ methods requires numerous simulations, while large-scale, transient numerical GWP solutions increase the computational cost. Reduced order models (ROMs) are commonly employed to provide numerical results in a limited amount of time. In this paper, we propose a machine learning (ML)-based ROM, mentioned as BO-ML-ROM, to decrease the computational time related to the simulation of the GWP. The ROM is integrated with a Bayesian optimization (BO) framework, to adaptively sample the parameters for the ROM training. The finite element method is used for the simulation of the high-fidelity models. The formulated ROM is used for forward UQ of the GWP in an aluminum plate with varying material properties. To determine the influence of each parameter perturbation, a global, variance-based sensitivity analysis is implemented based on Sobol' indices. It is shown that Bayesian optimization outperforms one-shot sampling methods, both in terms of accuracy and speed-up. The predicted results reveal the efficiency of BO-ML-ROM for GWP and demonstrate its value for UQ.
</details>
<details>
<summary>摘要</summary>
在数字双身框架中，结构健康监测（SHM）作为 Condition-based 维护的基础，实现了虚拟和物理资产之间的连接。通常使用推波propagation（GWP）进行结构的检测。但GWP受到结构物理属性的变化影响，导致假警示。为了改善预测的可靠性，uncertainty量化（UQ）经常应用。计算机机制是结构 simulations的有用工具，并常用于UQ。然而，UQ方法的应用需要许多 simulations，而大规模、过程 numerical GWP 解决方案会增加计算成本。减少级模型（ROMs）通常用于提供数字化的结果，以降低计算时间。在本文中，我们提出了一种基于机器学习（ML）的 ROM，称为 BO-ML-ROM，以降低 GWP 的计算时间。BO-ML-ROM 与 Bayesian 优化（BO）框架集成，以适应参数的参数采样。使用finite element 方法进行高精度模型的 simulations。我们采用了 Sobol 指数来实现全球、卷积基于sensitivity分析，以确定每个参数的影响。结果显示， Bayesian 优化超过一键采样方法，both in terms of accuracy和speed-up。预测结果表明，BO-ML-ROM 对 GWP 有效，并且对 UQ 提供了价值。
</details></li>
</ul>
<hr>
<h2 id="Neural-Priority-Queues-for-Graph-Neural-Networks"><a href="#Neural-Priority-Queues-for-Graph-Neural-Networks" class="headerlink" title="Neural Priority Queues for Graph Neural Networks"></a>Neural Priority Queues for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09660">http://arxiv.org/abs/2307.09660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Jain, Petar Veličković, Pietro Liò</li>
<li>for: 这篇论文旨在扩展Graph Neural Networks（GNNs）中的外部记忆。</li>
<li>methods: 该论文提出了一种名为Neural Priority Queues（NPQs）的算法，用于增强GNNs的推理能力。</li>
<li>results: 实验结果表明，NPQs能够 capture long-range interactions，并且与算法性理解相关。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have shown considerable success in neural algorithmic reasoning. Many traditional algorithms make use of an explicit memory in the form of a data structure. However, there has been limited exploration on augmenting GNNs with external memory. In this paper, we present Neural Priority Queues, a differentiable analogue to algorithmic priority queues, for GNNs. We propose and motivate a desiderata for memory modules, and show that Neural PQs exhibit the desiderata, and reason about their use with algorithmic reasoning. This is further demonstrated by empirical results on the CLRS-30 dataset. Furthermore, we find the Neural PQs useful in capturing long-range interactions, as empirically shown on a dataset from the Long-Range Graph Benchmark.
</details>
<details>
<summary>摘要</summary>
图内神经网络（GNNs）在神经算法逻辑中表现出了很大的成功。许多传统算法都利用了明确的内存结构，但是对于GNNs中的外部Memory的探索是有限的。在这篇论文中，我们介绍了神经优先队列（Neural Priority Queues），它是神经网络中的一种可导的外部 Memory 模块。我们提出了内存模块的需求和愿景，并证明了神经优先队列符合这些需求，并且可以与算法逻辑进行推理。这种结果在CLRS-30数据集上得到了实验证明，并且在Long-Range Graph Benchmark数据集上也得到了实验证明，其可以捕捉长距离交互。
</details></li>
</ul>
<hr>
<h2 id="HAT-CL-A-Hard-Attention-to-the-Task-PyTorch-Library-for-Continual-Learning"><a href="#HAT-CL-A-Hard-Attention-to-the-Task-PyTorch-Library-for-Continual-Learning" class="headerlink" title="HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning"></a>HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09653">http://arxiv.org/abs/2307.09653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xduan7/hat-cl">https://github.com/xduan7/hat-cl</a></li>
<li>paper_authors: Xiaotian Duan</li>
<li>for: 本研究旨在 Mitigating catastrophic forgetting 问题，即神经网络学习新任务时，损失之前学习的知识。</li>
<li>methods: 本文引入了 Hard-Attention-to-the-Task (HAT) 机制，并提供了一个 user-friendly 和 PyTorch-compatible 的 redesign - HAT-CL。HAT-CL 不仅自动 manipulate 梯度，还可以将 PyTorch 模块转换为 HAT 模块。</li>
<li>results: 本研究成果包括一个 comprehensive 的 HAT 模块集，可以顺利地 интеGRATE 到现有的架构中，以及一些Ready-to-use HAT 网络，它们可以轻松地与 TIMM 库集成。此外，本研究还提出了一些新的 mask  manipulate 技术，这些技术在多种实验中表现出了提高。<details>
<summary>Abstract</summary>
Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT, which have consistently shown improvements across various experiments. Our work paves the way for a broader application of the HAT mechanism, opening up new possibilities in continual learning across diverse models and applications.
</details>
<details>
<summary>摘要</summary>
Catastrophic forgetting,  neuronal networks losing previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT, which have consistently shown improvements across various experiments. Our work paves the way for a broader application of the HAT mechanism, opening up new possibilities in continual learning across diverse models and applications.Here's the text with some additional information about the Simplified Chinese translation:The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. The translation is written in a formal and neutral style, with a focus on accurately conveying the meaning of the original text.Some of the key features of the translation include:* Use of Catastrophic forgetting (泛化遗忘) to refer to the phenomenon of neural networks losing previously obtained knowledge during the learning of new tasks.* Use of Hard-Attention-to-the-Task (HAT) mechanism (硬件注意力机制) to refer to the mechanism proposed in the paper to mitigate catastrophic forgetting.* Use of PyTorch-compatible (PyTorch 兼容) to refer to the fact that the HAT mechanism is designed to be compatible with the PyTorch deep learning framework.* Use of ready-to-use HAT networks (准备好使用的 HAT 网络) to refer to the fact that the HAT-CL mechanism provides pre-built HAT networks that can be easily integrated into existing architectures.* Use of novel mask manipulation techniques (新的面积操作技术) to refer to the additional techniques proposed in the paper to improve the performance of the HAT mechanism.Overall, the translation aims to accurately convey the meaning and content of the original text in a formal and neutral style, while also taking into account the conventions and characteristics of Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Application-of-BadNets-in-Spam-Filters"><a href="#Application-of-BadNets-in-Spam-Filters" class="headerlink" title="Application of BadNets in Spam Filters"></a>Application of BadNets in Spam Filters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09649">http://arxiv.org/abs/2307.09649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swagnik Roychoudhury, Akshaj Kumar Veldanda</li>
<li>for: 防止垃圾邮件泄露用户个人信息和威胁邮件系统安全性</li>
<li>methods: 利用机器学习模型攻击推送垃圾邮件</li>
<li>results: 成功实现攻击后门攻击，提示机器学习模型供应链需要严格评估和监测<details>
<summary>Abstract</summary>
Spam filters are a crucial component of modern email systems, as they help to protect users from unwanted and potentially harmful emails. However, the effectiveness of these filters is dependent on the quality of the machine learning models that power them. In this paper, we design backdoor attacks in the domain of spam filtering. By demonstrating the potential vulnerabilities in the machine learning model supply chain, we highlight the need for careful consideration and evaluation of the models used in spam filters. Our results show that the backdoor attacks can be effectively used to identify vulnerabilities in spam filters and suggest the need for ongoing monitoring and improvement in this area.
</details>
<details>
<summary>摘要</summary>
防ospam filter 是现代电子邮件系统中的重要组件，它们帮助保护用户从不想要的和可能有害的电子邮件中受到保护。然而，这些防ospam filter 的效iveness 取决于机器学习模型的质量。在这篇论文中，我们设计了针对防ospam filter 的后门攻击。我们通过示例出了机器学习模型供应链中的潜在漏洞，并高亮了在使用这些模型时需要仔细评估和评估。我们的结果表明，后门攻击可以有效地找到防ospam filter 中的漏洞，并建议需要持续监测和改进。
</details></li>
</ul>
<hr>
<h2 id="Promoting-Exploration-in-Memory-Augmented-Adam-using-Critical-Momenta"><a href="#Promoting-Exploration-in-Memory-Augmented-Adam-using-Critical-Momenta" class="headerlink" title="Promoting Exploration in Memory-Augmented Adam using Critical Momenta"></a>Promoting Exploration in Memory-Augmented Adam using Critical Momenta</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09638">http://arxiv.org/abs/2307.09638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chandar-lab/cmoptimizer">https://github.com/chandar-lab/cmoptimizer</a></li>
<li>paper_authors: Pranshu Malviya, Gonçalo Mordido, Aristide Baratin, Reza Babanezhad Harikandeh, Jerry Huang, Simon Lacoste-Julien, Razvan Pascanu, Sarath Chandar</li>
<li>for: 本文主要针对大规模深度学习模型的训练。</li>
<li>methods: 本文提出了一种新的记忆扩展版的Adam优化器，通过在训练过程中使用缓存来提高探索较平的极小值的能力。</li>
<li>results: 本文经验表明，该方法可以提高多种Adam变体在标准的报告语言模型和图像分类任务中的表现。<details>
<summary>Abstract</summary>
Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
</details>
<details>
<summary>摘要</summary>
《适应型梯度下降优化器，特别是Adam，在训练大规模深度学习模型中留下了痕迹。这些优化器的优点在于它们在选择超参数时表现出更快的收敛速度和更高的稳定性。然而，它们通常在泛化性方面表现不佳。现有研究表明，这种性能差异与找到狭窄的极小点有关：适应方法通常找到具有更锐度的极小点，这会导致泛化性下降。为了解决这个问题，我们提出了一种新的内存扩展版本的Adam优化器，通过在训练过程中使用缓存来促进探索更平降的极小点。直观来说，使用缓存会让优化器在缺乏够广的极小点基因上进行过射。我们实验表明，我们的方法可以提高许多变种的Adam优化器在标准的supervised语言模型和图像分类任务中的性能。》Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Towards-Federated-Foundation-Models-Scalable-Dataset-Pipelines-for-Group-Structured-Learning"><a href="#Towards-Federated-Foundation-Models-Scalable-Dataset-Pipelines-for-Group-Structured-Learning" class="headerlink" title="Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning"></a>Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09619">http://arxiv.org/abs/2307.09619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/dataset_grouper">https://github.com/google-research/dataset_grouper</a></li>
<li>paper_authors: Zachary Charles, Nicole Mitchell, Krishna Pillutla, Michael Reneer, Zachary Garrett</li>
<li>for:  This paper is written for researchers and practitioners in the field of federated learning, particularly those interested in creating large-scale group-structured datasets for federated learning simulations.</li>
<li>methods:  The paper introduces a new library called Dataset Grouper, which allows researchers to create large-scale group-structured datasets for federated learning simulations. The library scales to settings where even a single group’s dataset is too large to fit in memory, and provides flexibility in choosing the base dataset and defining partitions.</li>
<li>results:  The paper demonstrates the effectiveness of Dataset Grouper through experimental results on large-scale federated language modeling simulations. The results show that algorithms like FedAvg operate more as meta-learning methods than as empirical risk minimization methods at this scale, suggesting their utility in downstream personalization and task-specific adaptation.<details>
<summary>Abstract</summary>
We introduce a library, Dataset Grouper, to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library allows the creation of group-structured versions of existing datasets based on user-specified partitions, and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper allows for large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empirical risk minimization methods at this scale, suggesting their utility in downstream personalization and task-specific adaptation.
</details>
<details>
<summary>摘要</summary>
我团队介绍一个库 named Dataset Grouper，用于创建大规模群体结构化（例如联邦学习）数据集，使得联邦学习模拟可以在基础模型的规模上进行。这个库允许用户根据自己的分区来创建群体结构化的现有数据集的版本，直接导致了一些灵活的多种数据集，可以适应现有的软件框架。Dataset Grouper具有三个关键优势：首先，它可以处理具有太多数据的设置，以至于单个群体的数据集不能置于内存中；其次，它允许用户选择基本（非分区）数据集和定义分区，提供了更多的灵活性；最后，它是框架不依赖的。我们通过实验证明，使用Dataset Grouper可以进行大规模的联邦语言模型 simulations，比前作多出了数个数量级的领域。我们的实验结果表明，在这种规模下，算法如 FedAvg 更像是元学习方法而非实际风险最小化方法，这表明它们在下游个性化和任务特定适应方面具有Utility。
</details></li>
</ul>
<hr>
<h2 id="Gradient-strikes-back-How-filtering-out-high-frequencies-improves-explanations"><a href="#Gradient-strikes-back-How-filtering-out-high-frequencies-improves-explanations" class="headerlink" title="Gradient strikes back: How filtering out high frequencies improves explanations"></a>Gradient strikes back: How filtering out high frequencies improves explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09591">http://arxiv.org/abs/2307.09591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabine Muzellec, Léo Andéol, Thomas Fel, Rufin VanRullen, Thomas Serre</li>
<li>for: 本研究旨在解释深度神经网络（CNN）的决策过程中， Gradient-based 和 Prediction-based 两种方法的差异，以及这些方法对决策的影响。</li>
<li>methods: 本研究使用了三种代表性的视觉分类模型的梯度分析，探讨了这些模型中高频信息的来源，以及这些信息对决策的影响。</li>
<li>results: 研究发现，Gradient-based 方法的梯度中含有噪声信息，而 Prediction-based 方法的梯度中减少了高频信息。此外，研究还发现，CNN 中的下采样操作可能是高频信息的主要来源。通过应用优化的低通量滤波器，可以改善 Gradient-based 方法的解释效果。研究结果表明，移除高频噪声可以提高 Gradient-based 方法的解释效果，并且将 Gradient-based 方法 ranked 为 state-of-the-art 方法。<details>
<summary>Abstract</summary>
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthermore, our analysis reveals that the operations used in Convolutional Neural Networks (CNNs) for downsampling appear to be a significant source of this high-frequency content -- suggesting aliasing as a possible underlying basis. We then apply an optimal low-pass filter for attribution maps and demonstrate that it improves gradient-based attribution methods. We show that (i) removing high-frequency noise yields significant improvements in the explainability scores obtained with gradient-based methods across multiple models -- leading to (ii) a novel ranking of state-of-the-art methods with gradient-based methods at the top. We believe that our results will spur renewed interest in simpler and computationally more efficient gradient-based methods for explainability.
</details>
<details>
<summary>摘要</summary>
近年来，有一个快速发展的新的预测基于的归因方法，逐渐替代了老的梯度基于的方法来解释深度神经网络的决策。然而，还不清楚为什么预测基于的方法超过梯度基于的方法。在这里，我们开始从一个实际观察出发：这两种方法生成的归因图有非常不同的功率特征，预测基于的方法的归因图具有较低的功率特征，而梯度基于的方法的归因图具有更高的功率特征。这个观察问题出现了多个问题：预测基于的方法中具有高频信息的来源是什么，这些信息是否真正反映系统做出的决策？最后，预测基于的方法中缺乏高频信息的原因是为什么会导致多个纪录的解释分数提高？我们分析了三种代表性的视觉分类模型的梯度，发现梯度包含了高频信息的噪声。我们的分析还表明，在Convolutional Neural Networks（CNNs）中用于下采样的操作是高频信息的主要来源，这表明噪声可能是下采样操作的基础。我们应用最佳低通 filters 来修正归因图，并证明了移除高频噪声可以大幅提高梯度基于的解释分数。我们的结果表明，（i）移除高频噪声可以使得梯度基于的解释分数在多个模型中得到显著改进，导致（ii）新的状态艺术方法排名，梯度基于的方法位于排名的顶峰。我们认为，我们的结果将会激发人们对简单而计算效率更高的梯度基于的解释方法的新的兴趣。
</details></li>
</ul>
<hr>
<h2 id="Self-Compatibility-Evaluating-Causal-Discovery-without-Ground-Truth"><a href="#Self-Compatibility-Evaluating-Causal-Discovery-without-Ground-Truth" class="headerlink" title="Self-Compatibility: Evaluating Causal Discovery without Ground Truth"></a>Self-Compatibility: Evaluating Causal Discovery without Ground Truth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09552">http://arxiv.org/abs/2307.09552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp M. Faller, Leena Chennuru Vankadara, Atalanti A. Mastakouri, Francesco Locatello, Dominik Janzing</li>
<li>for: 本研究的目的是提出一种新的方法来证明 causal discovery 算法的输出是否正确， absence of ground truth.</li>
<li>methods: 本研究使用了一种新的方法， which relies on the notion of compatibility between causal graphs learned on different subsets of variables.</li>
<li>results: 研究表明， detecting incompatibilities can falsify wrongly inferred causal relations due to violation of assumptions or errors from finite sample effects. Additionally, the method provides strong evidence for the causal models whenever compatibility entails strong implications for the joint distribution.<details>
<summary>Abstract</summary>
As causal ground truth is incredibly rare, causal discovery algorithms are commonly only evaluated on simulated data. This is concerning, given that simulations reflect common preconceptions about generating processes regarding noise distributions, model classes, and more. In this work, we propose a novel method for falsifying the output of a causal discovery algorithm in the absence of ground truth. Our key insight is that while statistical learning seeks stability across subsets of data points, causal learning should seek stability across subsets of variables. Motivated by this insight, our method relies on a notion of compatibility between causal graphs learned on different subsets of variables. We prove that detecting incompatibilities can falsify wrongly inferred causal relations due to violation of assumptions or errors from finite sample effects. Although passing such compatibility tests is only a necessary criterion for good performance, we argue that it provides strong evidence for the causal models whenever compatibility entails strong implications for the joint distribution. We also demonstrate experimentally that detection of incompatibilities can aid in causal model selection.
</details>
<details>
<summary>摘要</summary>
《因果真实是极其罕见的，因此 causal discovery 算法通常只能在模拟数据上进行评估。这有一定的问题，因为模拟数据反映了我们对生成过程的共同假设，如噪声分布、模型类型等。在这种情况下，我们提出了一种新的方法来证明 causal discovery 算法的输出是错误的。我们的关键发现是，统计学学习寻求数据点集之间的稳定性，而 causal learning 则应该寻求变量集之间的稳定性。根据这一点，我们的方法基于变量集之间的兼容性来评估 causal 模型。我们证明了，如果检测到兼容性不兼容，则可以证明因果关系是错误的，这可能是因为假设的违反或 finite sample effects 的错误。虽然通过兼容性测试只是一个必要条件，但我们 argue 它提供了强有力的证明，当兼容性导致 JOINT 分布的强制性时。我们还在实验中证明了，检测到兼容性可以帮助在 causal 模型选择中。》Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights"><a href="#Analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights" class="headerlink" title="Analyzing sports commentary in order to automatically recognize events and extract insights"></a>Analyzing sports commentary in order to automatically recognize events and extract insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10303">http://arxiv.org/abs/2307.10303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanismiraoui/analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights">https://github.com/yanismiraoui/analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights</a></li>
<li>paper_authors: Yanis Miraoui</li>
<li>for: 本研究旨在使用多种自然语言处理技术和方法自动识别体育赛事中的主要动作。</li>
<li>methods: 本研究使用了多种生动语言评论来分类不同来源的主要动作，并研究了情感分析是否能够检测主要动作。</li>
<li>results: 研究发现，使用多种自然语言处理技术和方法可以准确地识别体育赛事中的主要动作，并且情感分析可以帮助检测主要动作。<details>
<summary>Abstract</summary>
In this paper, we carefully investigate how we can use multiple different Natural Language Processing techniques and methods in order to automatically recognize the main actions in sports events. We aim to extract insights by analyzing live sport commentaries from different sources and by classifying these major actions into different categories. We also study if sentiment analysis could help detect these main actions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们仔细研究了如何使用多种自然语言处理技术和方法来自动识别体育活动中的主要动作。我们希望通过分析不同来源的直播体育评论来提取分析结论，并将这些主要动作分类为不同的类别。此外，我们还研究了情感分析是否可以帮助检测这些主要动作。
</details></li>
</ul>
<hr>
<h2 id="The-semantic-landscape-paradigm-for-neural-networks"><a href="#The-semantic-landscape-paradigm-for-neural-networks" class="headerlink" title="The semantic landscape paradigm for neural networks"></a>The semantic landscape paradigm for neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09550">http://arxiv.org/abs/2307.09550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyas Gokhale</li>
<li>for: 这篇论文的目的是提出一个概念和数学框架，用于描述深度神经网络的训练剖架和性能。</li>
<li>methods: 这篇论文使用的方法是基于神经网络学习的表示学习的概念和数学模型，用于解释神经网络的训练过程和性能。</li>
<li>results: 这篇论文的结果是提出了一种名为“semantic landscape”的概念和数学框架，可以用于描述神经网络的训练剖架和性能，并且可以用于解释神经网络的各种现象，如grokking和emergence。<details>
<summary>Abstract</summary>
Deep neural networks exhibit a fascinating spectrum of phenomena ranging from predictable scaling laws to the unpredictable emergence of new capabilities as a function of training time, dataset size and network size. Analysis of these phenomena has revealed the existence of concepts and algorithms encoded within the learned representations of these networks. While significant strides have been made in explaining observed phenomena separately, a unified framework for understanding, dissecting, and predicting the performance of neural networks is lacking. Here, we introduce the semantic landscape paradigm, a conceptual and mathematical framework that describes the training dynamics of neural networks as trajectories on a graph whose nodes correspond to emergent algorithms that are instrinsic to the learned representations of the networks. This abstraction enables us to describe a wide range of neural network phenomena in terms of well studied problems in statistical physics. Specifically, we show that grokking and emergence with scale are associated with percolation phenomena, and neural scaling laws are explainable in terms of the statistics of random walks on graphs. Finally, we discuss how the semantic landscape paradigm complements existing theoretical and practical approaches aimed at understanding and interpreting deep neural networks.
</details>
<details>
<summary>摘要</summary>
深度神经网络展现出一种惊喜的谱面，从预测可预测的缩放法则到由训练时间、数据集大小和网络大小而带来的新的能力的不可预测出现。对这些现象的分析发现了神经网络中学习表示中的概念和算法。虽然在分解这些现象方面已经做出了重要的进步，但一个综合的框架 для理解、拆分和预测神经网络的性能是缺失的。在这里，我们引入 semantic landscape 概念，它是一种概念和数学框架，用于描述神经网络的训练剖ogram，其节点对应于神经网络学习表示中的内在的算法。这种抽象使得我们可以用已有的统计物理学问题来描述各种神经网络现象。例如，我们表明了感知和emergence with scale是percolation现象的关联，而神经网络的尺度法则是可以通过图集的统计学来解释的。最后，我们讨论了 semantic landscape 概念如何与现有的理论和实践方法相结合，以更好地理解和解释深度神经网络。
</details></li>
</ul>
<hr>
<h2 id="DreaMR-Diffusion-driven-Counterfactual-Explanation-for-Functional-MRI"><a href="#DreaMR-Diffusion-driven-Counterfactual-Explanation-for-Functional-MRI" class="headerlink" title="DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI"></a>DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09547">http://arxiv.org/abs/2307.09547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/icon-lab/dreamr">https://github.com/icon-lab/dreamr</a></li>
<li>paper_authors: Hasan Atakan Bedel, Tolga Çukur</li>
<li>for: 这个论文旨在提供一种高准确性、可信度和效率的 diffusion-driven counterfactual method，以便对 fMRI 数据进行解释。</li>
<li>methods: 该论文使用了一种新型的 fractional multi-phase-distilled diffusion prior，以提高抽样效率而不妥协准确性，同时使用 transformer 架构来考虑 long-range spatiotemporal 上下文。</li>
<li>results: 对 neuroimaging 数据进行了广泛的实验，并证明了 DreaMR 比 state-of-the-art counterfactual 方法在 sample generation 方面具有更高的准确性、可信度和效率。<details>
<summary>Abstract</summary>
Deep learning analyses have offered sensitivity leaps in detection of cognitive states from functional MRI (fMRI) measurements across the brain. Yet, as deep models perform hierarchical nonlinear transformations on their input, interpreting the association between brain responses and cognitive states is challenging. Among common explanation approaches for deep fMRI classifiers, attribution methods show poor specificity and perturbation methods show limited plausibility. While counterfactual generation promises to address these limitations, previous methods use variational or adversarial priors that yield suboptimal sample fidelity. Here, we introduce the first diffusion-driven counterfactual method, DreaMR, to enable fMRI interpretation with high specificity, plausibility and fidelity. DreaMR performs diffusion-based resampling of an input fMRI sample to alter the decision of a downstream classifier, and then computes the minimal difference between the original and counterfactual samples for explanation. Unlike conventional diffusion methods, DreaMR leverages a novel fractional multi-phase-distilled diffusion prior to improve sampling efficiency without compromising fidelity, and it employs a transformer architecture to account for long-range spatiotemporal context in fMRI scans. Comprehensive experiments on neuroimaging datasets demonstrate the superior specificity, fidelity and efficiency of DreaMR in sample generation over state-of-the-art counterfactual methods for fMRI interpretation.
</details>
<details>
<summary>摘要</summary>
深度学习分析已经提供了识别认知状态从功能磁共振成像（fMRI）测量的感知跳变。然而，由于深度模型对输入进行堆叠非线性变换，因此解释脑响应和认知状态之间的关系具有挑战。当前的解释方法包括负回归方法和负影响方法，但它们具有较低的特点和可靠性。在这些限制下，Counterfactual生成技术被提出，以提高深度fMRI分类器的解释能力。在本文中，我们介绍了首个扩散驱动Counterfactual方法，即DreaMR，用于实现高特点、可靠性和准确性的fMRI解释。DreaMR使用扩散基于扩散的重新混合输入fMRI样本，并计算原始和对应样本之间的最小差异来进行解释。与传统扩散方法不同，DreaMR利用一种新的分数多相态混合扩散先进技术来提高抽象效率而无需妥协准确性，并使用一个转换架构来考虑fMRI扫描中的长距离空间时间 correlations。实验表明，DreaMR在 neuroscience 数据集上具有更高的特点、可靠性和效率，并且在 sample generation 方面与当前最佳的Counterfactual方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Can-Neural-Network-Memorization-Be-Localized"><a href="#Can-Neural-Network-Memorization-Be-Localized" class="headerlink" title="Can Neural Network Memorization Be Localized?"></a>Can Neural Network Memorization Be Localized?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09542">http://arxiv.org/abs/2307.09542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pratyushmaini/localizing-memorization">https://github.com/pratyushmaini/localizing-memorization</a></li>
<li>paper_authors: Pratyush Maini, Michael C. Mozer, Hanie Sedghi, Zachary C. Lipton, J. Zico Kolter, Chiyuan Zhang</li>
<li>for: 本文探讨了深度过参数网络中 memorization 和通用化的关系，并提出了一种新的 dropout 方法来控制 memorization。</li>
<li>methods: 本文使用了三种实验来证明 memorization 不是局限于各层，而是一种在模型中的小集合的现象。这三种实验包括 gradient accounting、layer rewinding 和 retraining。</li>
<li>results: 研究发现，大多数层在 memorization 方面是 redundant，而 memorization 的层通常不是最后几层。此外，研究还发现 memorization 通常是模型中的一小部分 neuron 或通道（大约 5）的现象。基于这些发现，本文提出了一种新的 dropout 方法——example-tied dropout，可以控制 memorization 的分布。<details>
<summary>Abstract</summary>
Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks $\textit{memorize}$ "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on $\textit{atypical}$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are $\textit{gradient accounting}$ (measuring the contribution to the gradient norms from memorized and clean examples), $\textit{layer rewinding}$ (replacing specific model weights of a converged model with previous training checkpoints), and $\textit{retraining}$ (training rewound layers only on clean examples). Second, we ask a more generic question: can memorization be localized $\textit{anywhere}$ in a model? We discover that memorization is often confined to a small number of neurons or channels (around 5) of the model. Based on these insights we propose a new form of dropout -- $\textit{example-tied dropout}$ that enables us to direct the memorization of examples to an apriori determined set of neurons. By dropping out these neurons, we are able to reduce the accuracy on memorized examples from $100\%\to3\%$, while also reducing the generalization gap.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，深度过参数网络中的各层之间的交互作用可以帮助网络“记忆”特殊的训练集示例。记忆指的是网络可以正确预测训练集中的特殊示例。在这项工作中，我们表明了这种记忆不是局限于具体层，而是一种局限于小量神经元的现象。我们通过三种实验证据来证明这一点：1. Gradient Accounting：我们测试了各层对 memorized 和 clean 示例的贡献，发现大多数层对 memorized 示例的贡献很小，而且这些层通常不是最后几层。2. Layer Rewinding：我们将特定模型的权重更改为训练过程中的检查点权重，发现这些层对 memorized 示例的贡献也很小。3. Retraining：我们只在 clean 示例上训练rewound层，发现这些层对 memorized 示例的贡献也很小。 Based on these findings, we propose a new form of dropout called "example-tied dropout" that can direct the memorization of examples to a priori determined neurons. By dropping out these neurons, we can reduce the accuracy on memorized examples from 100% to 3%, while also reducing the generalization gap.
</details></li>
</ul>
<hr>
<h2 id="Forecasting-the-steam-mass-flow-in-a-powerplant-using-the-parallel-hybrid-network"><a href="#Forecasting-the-steam-mass-flow-in-a-powerplant-using-the-parallel-hybrid-network" class="headerlink" title="Forecasting the steam mass flow in a powerplant using the parallel hybrid network"></a>Forecasting the steam mass flow in a powerplant using the parallel hybrid network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09483">http://arxiv.org/abs/2307.09483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrii Kurkin, Jonas Hegemann, Mo Kordzanganeh, Alexey Melnikov</li>
<li>for: 这种研究旨在提高热电厂的蒸汽质量流量预测精度，以提高操作效率和降低成本。</li>
<li>methods: 该研究使用了并行混合神经网络架构，结合 Parametrized Quantum Circuit 和 Conventional Feed-Forward Neural Network，专门为产业场景的时间序列预测。</li>
<li>results: 对比 Standalone Classical 和 Quantum 模型，并行混合模型在测试集上的 MSE 损失较低，比 pure Classical 和 pure Quantum 网络低出至少 5.7 和 4.9 倍。此外，混合模型在测试集上的相对误差较小，与基eline Classical 模型相比，最多下降 2 倍。这些发现可以帮助energy sector解决实际问题，最终带来更好的发电厂运行效率。<details>
<summary>Abstract</summary>
Efficient and sustainable power generation is a crucial concern in the energy sector. In particular, thermal power plants grapple with accurately predicting steam mass flow, which is crucial for operational efficiency and cost reduction. In this study, we use a parallel hybrid neural network architecture that combines a parametrized quantum circuit and a conventional feed-forward neural network specifically designed for time-series prediction in industrial settings to enhance predictions of steam mass flow 15 minutes into the future. Our results show that the parallel hybrid model outperforms standalone classical and quantum models, achieving more than 5.7 and 4.9 times lower mean squared error (MSE) loss on the test set after training compared to pure classical and pure quantum networks, respectively. Furthermore, the hybrid model demonstrates smaller relative errors between the ground truth and the model predictions on the test set, up to 2 times better than the pure classical model. These findings contribute to the broader scientific understanding of how integrating quantum and classical machine learning techniques can be applied to real-world challenges faced by the energy sector, ultimately leading to optimized power plant operations.
</details>
<details>
<summary>摘要</summary>
efficient和可持续的电力生产是能源领域的关键问题。特别是热电厂面临精准预测蒸汽质量的挑战，这对操作效率和成本减少都是关键。在这种研究中，我们使用并行的混合神经网络架构，将 parametrized quantum circuit 和 conventional feed-forward neural network 结合在一起，专门用于工业设置中的时间序列预测。我们的结果表明，并行混合模型比纯经典和量子模型都更高效，在测试集上减少MSE损失的mean squared error 比例下降超过5.7和4.9倍，而且在测试集上相对误差与真实值的比较也更小，最多下降2倍于纯经典模型。这些发现对能源领域的科学理解如何将量子和经典机器学习技术集成应用于实际挑战中，带来了优化的发电厂操作。
</details></li>
</ul>
<hr>
<h2 id="Overthinking-the-Truth-Understanding-how-Language-Models-Process-False-Demonstrations"><a href="#Overthinking-the-Truth-Understanding-how-Language-Models-Process-False-Demonstrations" class="headerlink" title="Overthinking the Truth: Understanding how Language Models Process False Demonstrations"></a>Overthinking the Truth: Understanding how Language Models Process False Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09476">http://arxiv.org/abs/2307.09476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dannyallover/overthinking_the_truth">https://github.com/dannyallover/overthinking_the_truth</a></li>
<li>paper_authors: Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt</li>
<li>for: 研究模型内部表征如何导致危险模仿行为</li>
<li>methods: 使用 few-shot learning 训练模型，并分析模型内部表征以探究危险模仿行为的两个相关现象：过思考和假推导头</li>
<li>results: 研究发现，在某些层次上，模型会因为 incorrect 示例而导致模型行为分化，并且这种分化可能是由 false induction heads 引起的。禁用 false induction heads 可以降低过思考现象。这些结果提供了一个可能有用的方向，以便更好地理解和防止危险模型行为。<details>
<summary>Abstract</summary>
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.
</details>
<details>
<summary>摘要</summary>
现代语言模型可以通过几招学习模式，完成复杂任务而无需微调。然而，仿效也可能导致模型复制错误或有害内容，如果存在在上下文中。我们通过模型内部表示的研究，发现了两种相关现象：过思和假推导头。第一种现象，过思，发生在我们decode预测从中间层，正确vs.错误几招示范下。在早期层，两种示范都会导致相似的模型行为，但行为在某“关键层”后弯曲分别， incorrect示范后的准确率逐渐下降。第二种现象，假推导头，可能是过思的机制原因：这些在晚期层出现的头部会复制和仿效先前示范中的错误信息，并且它们的ablation可以减少过思。我们的结果表明，研究中间模型计算可能是理解和防止危险模型行为的有望之路。
</details></li>
</ul>
<hr>
<h2 id="A-Cryogenic-Memristive-Neural-Decoder-for-Fault-tolerant-Quantum-Error-Correction"><a href="#A-Cryogenic-Memristive-Neural-Decoder-for-Fault-tolerant-Quantum-Error-Correction" class="headerlink" title="A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction"></a>A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09463">http://arxiv.org/abs/2307.09463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frédéric Marcotte, Pierre-Antoine Mouny, Victor Yon, Gebremedhin A. Dagnew, Bohdan Kulchytskyy, Sophie Rochette, Yann Beilliard, Dominique Drouin, Pooya Ronagh<br>for: 这个论文目的是为了提高量子错误修复（QEC）中的神经网络扩散器的性能。methods: 这个论文使用的方法是基于内存计算（IMC）架构，使用栅格阵列来存储神经网络的 synaptic 权重和进行分析时的分数积分。results: 这个论文的结果表明，使用 TiO$_\textrm{x}$-based 记忆Device 的非理想特性会导致损失精度，但通过硬件意识训练方法可以 Mitigate 这种损失，使得神经网络扩散器可以达到 $9.23\times 10^{-4}$ 的 pseudo-threshold。<details>
<summary>Abstract</summary>
Neural decoders for quantum error correction (QEC) rely on neural networks to classify syndromes extracted from error correction codes and find appropriate recovery operators to protect logical information against errors. Despite the good performance of neural decoders, important practical requirements remain to be achieved, such as minimizing the decoding time to meet typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. Designing a dedicated integrated circuit to perform the decoding task in co-integration with a quantum processor appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck. In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory computing (IMC) architecture, where crossbar arrays of resistive memory devices are employed to both store the synaptic weights of the decoder neural network and perform analog matrix-vector multiplications during inference. In proof-of-concept numerical experiments supported by experimental measurements, we investigate the impact of TiO$_\textrm{x}$-based memristive devices' non-idealities on decoding accuracy. Hardware-aware training methods are developed to mitigate the loss in accuracy, allowing the memristive neural decoders to achieve a pseudo-threshold of $9.23\times 10^{-4}$ for the distance-three surface code, whereas the equivalent digital neural decoder achieves a pseudo-threshold of $1.01\times 10^{-3}$. This work provides a pathway to scalable, fast, and low-power cryogenic IMC hardware for integrated QEC.
</details>
<details>
<summary>摘要</summary>
量子错误修复（QEC）神经解码器 rely on 神经网络来分类错误码和找到保护逻辑信息的正确操作符。despite the good performance of neural decoders, important practical requirements remain to be achieved, such as minimizing the decoding time to meet typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. Designing a dedicated integrated circuit to perform the decoding task in co-integration with a quantum processor appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck. In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory computing（IMC）architecture, where crossbar arrays of resistive memory devices are employed to both store the synaptic weights of the decoder neural network and perform analog matrix-vector multiplications during inference. In proof-of-concept numerical experiments supported by experimental measurements, we investigate the impact of TiO$_\textrm{x}$-based memristive devices' non-idealities on decoding accuracy. Hardware-aware training methods are developed to mitigate the loss in accuracy, allowing the memristive neural decoders to achieve a pseudo-threshold of $9.23\times 10^{-4}$ for the distance-three surface code, whereas the equivalent digital neural decoder achieves a pseudo-threshold of $1.01\times 10^{-3}$. This work provides a pathway to scalable, fast, and low-power cryogenic IMC hardware for integrated QEC.
</details></li>
</ul>
<hr>
<h2 id="Does-Circuit-Analysis-Interpretability-Scale-Evidence-from-Multiple-Choice-Capabilities-in-Chinchilla"><a href="#Does-Circuit-Analysis-Interpretability-Scale-Evidence-from-Multiple-Choice-Capabilities-in-Chinchilla" class="headerlink" title="Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla"></a>Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09458">http://arxiv.org/abs/2307.09458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, Vladimir Mikulik</li>
<li>for: 这 paper 旨在测试 Chinchilla 模型中的回路分析的可扩展性。</li>
<li>methods: 这 paper 使用了现有的回路分析技术，包括 logit 归因、注意力图像化和活动覆盖，对 Chinchilla 模型进行了测试。</li>
<li>results: 研究发现，对于多选问题的回答，可以快速压缩查询、关键和值子空间，无损性性能。然而，当使用这些解释来理解 heads 对多选问题回答的行为时，发现只是一个 partial explanation， suggessts 还有更多的学习需要完成。<details>
<summary>Abstract</summary>
\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \emph{label} given knowledge of the correct answer \emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).   We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, key and value subspaces of the head without loss of performance when operating on the answer labels for multiple-choice questions, and we show that the query and key subspaces represent an `Nth item in an enumeration' feature to at least some extent. However, when we attempt to use this explanation to understand the heads' behaviour on a more general distribution including randomized answer labels, we find that it is only a partial explanation, suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering.
</details>
<details>
<summary>摘要</summary>
我们进一步研究 `正确字符' 类型的注意力头，以了解它们的Semantics。结果是混合的，我们在对多选问答的答案标签进行操作时，可以压缩查询、键和值子空间，而不会影响性能。此外，我们发现查询和键子空间表示了 `N个项在排序' 的特征，至少在一些程度上。然而，当我们尝试使用这种解释来理解 `correct letter' 头在多选问答中的行为时，我们发现这只是一个 partial explanation， suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering。
</details></li>
</ul>
<hr>
<h2 id="Smooth-Attention-for-Deep-Multiple-Instance-Learning-Application-to-CT-Intracranial-Hemorrhage-Detection"><a href="#Smooth-Attention-for-Deep-Multiple-Instance-Learning-Application-to-CT-Intracranial-Hemorrhage-Detection" class="headerlink" title="Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection"></a>Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09457">http://arxiv.org/abs/2307.09457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yunanwu2168/sa-mil">https://github.com/yunanwu2168/sa-mil</a></li>
<li>paper_authors: Yunan Wu, Francisco M. Castro-Macías, Pablo Morales-Álvarez, Rafael Molina, Aggelos K. Katsaggelos</li>
<li>for: 这篇研究应用于医疗影像诊断，特别是头部CT扫描中的血肿诊断。</li>
<li>methods: 这篇研究提出了一个缓和对应深度多例学习（SA-DMIL）模型，通过设定首次和第二次约束来实现缓和性。</li>
<li>results: 研究结果显示，SA-DMIL模型在头部CT扫描中血肿诊断方面比非缓和对应深度多例学习模型表现更好，并且学习了对于每个扫描的空间相依性。同时，它也超过了目前医疗影像诊断中的现有州态艺术方法。<details>
<summary>Abstract</summary>
Multiple Instance Learning (MIL) has been widely applied to medical imaging diagnosis, where bag labels are known and instance labels inside bags are unknown. Traditional MIL assumes that instances in each bag are independent samples from a given distribution. However, instances are often spatially or sequentially ordered, and one would expect similar diagnostic importance for neighboring instances. To address this, in this study, we propose a smooth attention deep MIL (SA-DMIL) model. Smoothness is achieved by the introduction of first and second order constraints on the latent function encoding the attention paid to each instance in a bag. The method is applied to the detection of intracranial hemorrhage (ICH) on head CT scans. The results show that this novel SA-DMIL: (a) achieves better performance than the non-smooth attention MIL at both scan (bag) and slice (instance) levels; (b) learns spatial dependencies between slices; and (c) outperforms current state-of-the-art MIL methods on the same ICH test set.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Convergent-regularization-in-inverse-problems-and-linear-plug-and-play-denoisers"><a href="#Convergent-regularization-in-inverse-problems-and-linear-plug-and-play-denoisers" class="headerlink" title="Convergent regularization in inverse problems and linear plug-and-play denoisers"></a>Convergent regularization in inverse problems and linear plug-and-play denoisers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09441">http://arxiv.org/abs/2307.09441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Hauptmann, Subhadip Mukherjee, Carola-Bibiane Schönlieb, Ferdia Sherry</li>
<li>for: 这篇论文的目的是研究插件和排除（PnP）去噪的可行性和稳定性。</li>
<li>methods: 这篇论文使用了经典的干扰理论和一些可证明是可靠的数据驱动方法来研究PnP算法的稳定性。</li>
<li>results: 这篇论文提出了一种新的spectral filtering技术来控制PnP算法中的噪声约束，并证明了PnP算法在linear denoiser情况下是一种可靠的减噪方法。<details>
<summary>Abstract</summary>
Plug-and-play (PnP) denoising is a popular iterative framework for solving imaging inverse problems using off-the-shelf image denoisers. Their empirical success has motivated a line of research that seeks to understand the convergence of PnP iterates under various assumptions on the denoiser. While a significant amount of research has gone into establishing the convergence of the PnP iteration for different regularity conditions on the denoisers, not much is known about the asymptotic properties of the converged solution as the noise level in the measurement tends to zero, i.e., whether PnP methods are provably convergent regularization schemes under reasonable assumptions on the denoiser. This paper serves two purposes: first, we provide an overview of the classical regularization theory in inverse problems and survey a few notable recent data-driven methods that are provably convergent regularization schemes. We then continue to discuss PnP algorithms and their established convergence guarantees. Subsequently, we consider PnP algorithms with linear denoisers and propose a novel spectral filtering technique to control the strength of regularization arising from the denoiser. Further, by relating the implicit regularization of the denoiser to an explicit regularization functional, we rigorously show that PnP with linear denoisers leads to a convergent regularization scheme. More specifically, we prove that in the limit as the noise vanishes, the PnP reconstruction converges to the minimizer of a regularization potential subject to the solution satisfying the noiseless operator equation. The theoretical analysis is corroborated by numerical experiments for the classical inverse problem of tomographic image reconstruction.
</details>
<details>
<summary>摘要</summary>
插件并运行（PnP）杜食是一种流行的迭代框架，用于解决图像逆问题。它们的实际成功激发了一条研究的线索，旨在了解PnP迭代的稳定性和收敛性。虽然一些研究已经证明了PnP迭代的稳定性，但它们对静音水平下的恒定性还不够了解。这篇论文旨在两个目的：首先，提供经典的反问题理论的概述，并Survey一些最近的数据驱动方法，这些方法是可证明的收敛规则。然后，我们继续讨论PnP算法，并证明它们的稳定性和收敛性。接下来，我们考虑PnP算法与线性杜食器的组合，并提出一种新的 спектраль滤波技术来控制杜食器对正则化的影响。此外，我们将杜食器的隐式正则化相关到一个显式的正则化函数中，并证明PnP与线性杜食器的组合是一种收敛的正则化方案。具体来说，我们证明在静音水平下，PnP重建将收敛到一个具有正则化潜在能量的最小化问题的解。这个 teorema 的实际分析得到了数值实验的证明。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Conditional-Slot-Attention-for-Object-Centric-Learning"><a href="#Unsupervised-Conditional-Slot-Attention-for-Object-Centric-Learning" class="headerlink" title="Unsupervised Conditional Slot Attention for Object Centric Learning"></a>Unsupervised Conditional Slot Attention for Object Centric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09437">http://arxiv.org/abs/2307.09437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avinash Kori, Francesco Locatello, Francesca Toni, Ben Glocker</li>
<li>for: 这个论文的目的是学习无监督下的对象水平表示，以便在下游逻辑任务中进行更好的理解和推理。</li>
<li>methods: 这个论文使用了一种新的Conditional Slot Attention方法，它使用一个新的随机 Gaussian distribution来学习对象水平的特定槽级别绑定。</li>
<li>results: 这个论文的结果表明，使用Conditional Slot Attention方法可以在多个下游任务中提供场景组合能力和几个步骤适应能力，同时在对象发现任务中表现与Slot Attention方法相当或更好。<details>
<summary>Abstract</summary>
Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show that our method provides scene composition capabilities and a significant boost in a few shot adaptability tasks of compositional visual reasoning, while performing similarly or better than slot attention in object discovery tasks
</details>
<details>
<summary>摘要</summary>
“抽取对象水平表示是人工智能领域的一个emerging领域。在无监督情况下学习对象中心表示存在多个挑战，其中一个关键的问题是将多个对象实例绑定到专门的对象槽。现有的对象中心表示方法，如满意注意力，利用迭代注意力学习可 compose 表示，但缺乏专门槽级别绑定。为解决这个问题，在这篇论文中我们提出了无监督条件满意注意力，使用一种新的probabilistic slot dictionary（PSD）。我们定义 PSD 的键是抽取对象级别属性向量，值是 Parametric Gaussian distribution。我们示出了学习的特定对象级别conditioning分布的好处，在多个下游任务中，包括对象发现、 compositional scene generation 和 compositional visual reasoning。我们显示了我们的方法提供了场景组合能力，并在一些几枚投入的 compositional visual reasoning 任务中带来了明显的提升，而与满意注意力在对象发现任务中表现类似或更好”
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-for-Imitation-Learning-in-NetHack"><a href="#Scaling-Laws-for-Imitation-Learning-in-NetHack" class="headerlink" title="Scaling Laws for Imitation Learning in NetHack"></a>Scaling Laws for Imitation Learning in NetHack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09423">http://arxiv.org/abs/2307.09423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade</li>
<li>for: 研究 Whether scaling up the model and data size can improve the performance of imitation learning in a challenging environment, such as the game of NetHack.</li>
<li>methods: 使用 Inspired by recent work in Natural Language Processing (NLP), the authors carefully scale up the model and data size to investigate the effectiveness of imitation learning in NetHack.</li>
<li>results: 发现 IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. The authors also find that their agents outperform prior state-of-the-art by at least 2x in all settings.<details>
<summary>Abstract</summary>
Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL and find they outperform prior state-of-the-art by at least 2x in all settings. Our work both demonstrates the scaling behavior of imitation learning in a challenging domain, as well as the viability of scaling up current approaches for increasingly capable agents in NetHack, a game that remains elusively hard for current AI systems.
</details>
<details>
<summary>摘要</summary>
遗传学习（IL）是机器学习中最广泛使用的方法之一，然而许多研究发现，它通常无法完全回归到专家行为的下面。然而，这些研究几乎没有深入探究扩大模型和数据集大小的作用。以最近的自然语言处理（NLP）研究为例，“扩大”已经导致了更加强大的LLMs的出现。我们调查了扩大模型和数据集大小对IL的影响，以示我们的发现。我们使用NetHack游戏作为研究环境，这是一个复杂的环境，具有过程生成、随机性、长期依赖和部分可见性。我们发现，IL损失和平均回报与计算预算有直线关系，并且存在计算优质的IL代理人的尺度-样本数量关系。我们预测和训练了一些NetHack代理人，并发现它们在所有设置下至少两倍于之前的状态艺术。我们的工作不仅证明了IL在复杂环境中的扩展行为，也证明了现有方法的扩大可以提供更加强大的代理人在NetHack游戏中。NetHack游戏仍然是现有AI系统所拒绝的游戏，我们的工作提供了一个可能的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Causality-oriented-robustness-exploiting-general-additive-interventions"><a href="#Causality-oriented-robustness-exploiting-general-additive-interventions" class="headerlink" title="Causality-oriented robustness: exploiting general additive interventions"></a>Causality-oriented robustness: exploiting general additive interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10299">http://arxiv.org/abs/2307.10299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xwshen51/drig">https://github.com/xwshen51/drig</a></li>
<li>paper_authors: Xinwei Shen, Peter Bühlmann, Armeen Taeb<br>for: This paper focuses on developing a robust prediction method that can handle distribution shifts in real-world applications.methods: The proposed method, Distributional Robustness via Invariant Gradients (DRIG), exploits general additive interventions in training data to achieve robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality.results: The authors prove that DRIG yields predictions that are robust among a data-dependent class of distribution shifts, and show that their framework includes anchor regression as a special case. Additionally, they extend their approach to the semi-supervised domain adaptation setting to further improve prediction performance, and empirically validate their methods on synthetic simulations and on single-cell data.<details>
<summary>Abstract</summary>
Since distribution shifts are common in real-world applications, there is a pressing need for developing prediction models that are robust against such shifts. Existing frameworks, such as empirical risk minimization or distributionally robust optimization, either lack generalizability for unseen distributions or rely on postulated distance measures. Alternatively, causality offers a data-driven and structural perspective to robust predictions. However, the assumptions necessary for causal inference can be overly stringent, and the robustness offered by such causal models often lacks flexibility. In this paper, we focus on causality-oriented robustness and propose Distributional Robustness via Invariant Gradients (DRIG), a method that exploits general additive interventions in training data for robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality. In a linear setting, we prove that DRIG yields predictions that are robust among a data-dependent class of distribution shifts. Furthermore, we show that our framework includes anchor regression (Rothenh\"ausler et al.\ 2021) as a special case, and that it yields prediction models that protect against more diverse perturbations. We extend our approach to the semi-supervised domain adaptation setting to further improve prediction performance. Finally, we empirically validate our methods on synthetic simulations and on single-cell data.
</details>
<details>
<summary>摘要</summary>
因为分布shift是实际应用中的常见现象，因此有一项急需开发具有对不visible分布的Robustness的预测模型。现有的框架，如empirical risk minimization或distributionally robust optimization，可能会lack普遍性 для未经看过的分布，或者基于假设的距离度量。然而， causality提供了一种数据驱动和结构性的 Perspective for robust predictions。然而， causal inference的假设可能会太 stringent，而且 causal models中的Robustness often lacks flexibility。在这篇论文中，我们关注 causality-oriented robustness，并提出 Distributional Robustness via Invariant Gradients (DRIG)，一种利用训练数据中的通用加itivity interventions来为未经看过的interventions提供Robust predictions，并自然地 interpolates between in-distribution prediction和causality。在线性设置下，我们证明了 DRIG 的预测是对数据依赖性的 class of distribution shifts 中Robust。此外，我们显示了我们的框架包含了 anchor regression (Rothenh\"ausler et al.\ 2021) 的特例，并且其预测模型可以保护 против更多元的扰动。我们将我们的方法推广到 semi-supervised domain adaptation setting，以进一步改善预测性能。最后，我们实际验证了我们的方法在 sintetic simulations 和单元细胞数据上。
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-with-Costly-Features-in-Non-stationary-Environments"><a href="#Online-Learning-with-Costly-Features-in-Non-stationary-Environments" class="headerlink" title="Online Learning with Costly Features in Non-stationary Environments"></a>Online Learning with Costly Features in Non-stationary Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09388">http://arxiv.org/abs/2307.09388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saeedghoorchian/ncc-bandits">https://github.com/saeedghoorchian/ncc-bandits</a></li>
<li>paper_authors: Saeed Ghoorchian, Evgenii Kortukov, Setareh Maghsudi</li>
<li>for: maximizing long-term rewards in sequential decision-making problems</li>
<li>methods: extending the contextual bandit setting to observe subsets of features’ states, and developing an algorithm with a sublinear regret guarantee</li>
<li>results: superior performance in a real-world scenario compared to existing methods<details>
<summary>Abstract</summary>
Maximizing long-term rewards is the primary goal in sequential decision-making problems. The majority of existing methods assume that side information is freely available, enabling the learning agent to observe all features' states before making a decision. In real-world problems, however, collecting beneficial information is often costly. That implies that, besides individual arms' reward, learning the observations of the features' states is essential to improve the decision-making strategy. The problem is aggravated in a non-stationary environment where reward and cost distributions undergo abrupt changes over time. To address the aforementioned dual learning problem, we extend the contextual bandit setting and allow the agent to observe subsets of features' states. The objective is to maximize the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average. Therefore, the agent faces a trade-off between minimizing the cost of information acquisition and possibly improving the decision-making process using the obtained information. To this end, we develop an algorithm that guarantees a sublinear regret in time. Numerical results demonstrate the superiority of our proposed policy in a real-world scenario.
</details>
<details>
<summary>摘要</summary>
“最大化长期回报是Sequential decision-making问题的主要目标。现有的方法多数假设可以免费获得侧情报，让决策机器可以在决策之前观察所有特征的状态。然而，在实际问题中，收集有利信息可能是成本的。这意味着，除了个别臂的奖励之外，学习特征的状态观察也是重要的。在非站点环境下，奖励和成本分布会随时间而改变，这问题更加严重。为解决上述双重学习问题，我们将上下文ual bandit设定扩展，让机器人可以观察特征subset的状态。目标是 Maximizing the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average.因此，机器人面临一个优化资讯成本和可能改善决策过程的问题。为此，我们开发了一个 guarantees a sublinear regret in time的算法。numerical results show that our proposed policy is superior to other methods in a real-world scenario.”Note: Please note that the translation is in Simplified Chinese, and the word order and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Batched-Predictors-Generalize-within-Distribution"><a href="#Batched-Predictors-Generalize-within-Distribution" class="headerlink" title="Batched Predictors Generalize within Distribution"></a>Batched Predictors Generalize within Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09379">http://arxiv.org/abs/2307.09379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Loukas, Pan Kessel</li>
<li>for: 这个论文主要研究批处理预测器的通用性特性，即用一小组例子（或批）来预测样本的平均标签。</li>
<li>methods: 这篇论文使用了一种适当的总化了Rademacher复杂度的方法来证明批处理预测器的泛化性能比标准每个样本预测更强。</li>
<li>results: 论文通过实验证明，批处理预测器在不同任务、结构和应用中都有较强的泛化性能，而且这些性能积极不受过参数化的影响。<details>
<summary>Abstract</summary>
We study the generalization properties of batched predictors, i.e., models tasked with predicting the mean label of a small set (or batch) of examples. The batched prediction paradigm is particularly relevant for models deployed to determine the quality of a group of compounds in preparation for offline testing. By utilizing a suitable generalization of the Rademacher complexity, we prove that batched predictors come with exponentially stronger generalization guarantees as compared to the standard per-sample approach. Surprisingly, the proposed bound holds independently of overparametrization. Our theoretical insights are validated experimentally for various tasks, architectures, and applications.
</details>
<details>
<summary>摘要</summary>
我们研究批处理预测器的通用性质，即模型用于预测一小集（或批）的示例的平均标签。批处理预测模式特别有用于用于在线测试前对一组分子的质量进行评估。我们使用适当的总体化卡达默chs complexity来证明批处理预测器的泛化保证比标准每个样本预测更强大，而且这些保证独立于过参数化。我们的理论发现被实验证实了 для多种任务、架构和应用。Note: "批处理预测器" in Chinese is "批处理预测模式" (pīnzhèng yùjì zhìdǎo), and "总体化卡达默chs complexity" in Chinese is "总体化卡达默chs复杂度" (zòngtǐhuì kǎdàmùchōng dòu).
</details></li>
</ul>
<hr>
<h2 id="Data-Cross-Segmentation-for-Improved-Generalization-in-Reinforcement-Learning-Based-Algorithmic-Trading"><a href="#Data-Cross-Segmentation-for-Improved-Generalization-in-Reinforcement-Learning-Based-Algorithmic-Trading" class="headerlink" title="Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading"></a>Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09377">http://arxiv.org/abs/2307.09377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikram Duvvur, Aashay Mehta, Edward Sun, Bo Wu, Ken Yew Chan, Jeff Schneider</li>
<li>for: 这个论文旨在提出一种基于强化学习的股票交易系统，以便在稀缺交易市场和不同化资产市场中进行更有效的交易。</li>
<li>methods: 该论文使用了强化学习算法，并将预测模型的信号用于交易决策。</li>
<li>results: 在20多年的马来西亚证券交易数据上测试的结果表明，该算法可以在稀缺交易市场和不同化资产市场中提供更高的交易效率和更好的资产配置。<details>
<summary>Abstract</summary>
The use of machine learning in algorithmic trading systems is increasingly common. In a typical set-up, supervised learning is used to predict the future prices of assets, and those predictions drive a simple trading and execution strategy. This is quite effective when the predictions have sufficient signal, markets are liquid, and transaction costs are low. However, those conditions often do not hold in thinly traded financial markets and markets for differentiated assets such as real estate or vehicles. In these markets, the trading strategy must consider the long-term effects of taking positions that are relatively more difficult to change. In this work, we propose a Reinforcement Learning (RL) algorithm that trades based on signals from a learned predictive model and addresses these challenges. We test our algorithm on 20+ years of equity data from Bursa Malaysia.
</details>
<details>
<summary>摘要</summary>
机器学习在算法交易系统中越来越普遍。一般情况下，监督学习用于预测资产未来价格，这些预测驱动简单的交易和执行策略。这很有效，当预测具有足够的信号，市场流动性高，交易成本低时。然而，这些条件往往不符合财务证券市场和特殊资产市场，如房地产或汽车。在这些市场中，交易策略需要考虑长期持有位置的难度。在这种情况下，我们提出一种强化学习（RL）算法，基于学习预测模型的信号进行交易。我们对马来西亚证券交易所20多年的股票数据进行测试。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/cs.LG_2023_07_19/" data-id="cloh7tqin00lc7b88baocha7d" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/70/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/69/">69</a><a class="page-number" href="/page/70/">70</a><span class="page-number current">71</span><a class="page-number" href="/page/72/">72</a><a class="page-number" href="/page/73/">73</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/72/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
