
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/13/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/cs.CV_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T13:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/cs.CV_2023_11_04/">cs.CV - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Anthropomorphic-Grasping-with-Neural-Object-Shape-Completion"><a href="#Anthropomorphic-Grasping-with-Neural-Object-Shape-Completion" class="headerlink" title="Anthropomorphic Grasping with Neural Object Shape Completion"></a>Anthropomorphic Grasping with Neural Object Shape Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02510">http://arxiv.org/abs/2311.02510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Hidalgo-Carvajal, Hanzhi Chen, Gemma C. Bettelani, Jaesug Jung, Melissa Zavaglia, Laura Busse, Abdeldjallil Naceri, Stefan Leutenegger, Sami Haddadin</li>
<li>for: 这 paper 的目的是提高 робоット 在人造环境中的物体抓取和操作能力。</li>
<li>methods: 这 paper 使用了人类对物体的理解，通过重建和完善部分观察的物体形状，并使用7度自由度人工手臂来抓取和操作物体。</li>
<li>results: 这 paper 的方法在不同的方向和位置下，可以提高基eline的抓取成功率约30%，并实现了多种不同物体类别上的150多个成功抓取。这表明这种方法可以准确预测和执行抓取姿势，并在实际场景中提高了робоット的抓取和操作能力。<details>
<summary>Abstract</summary>
The progressive prevalence of robots in human-suited environments has given rise to a myriad of object manipulation techniques, in which dexterity plays a paramount role. It is well-established that humans exhibit extraordinary dexterity when handling objects. Such dexterity seems to derive from a robust understanding of object properties (such as weight, size, and shape), as well as a remarkable capacity to interact with them. Hand postures commonly demonstrate the influence of specific regions on objects that need to be grasped, especially when objects are partially visible. In this work, we leverage human-like object understanding by reconstructing and completing their full geometry from partial observations, and manipulating them using a 7-DoF anthropomorphic robot hand. Our approach has significantly improved the grasping success rates of baselines with only partial reconstruction by nearly 30% and achieved over 150 successful grasps with three different object categories. This demonstrates our approach's consistent ability to predict and execute grasping postures based on the completed object shapes from various directions and positions in real-world scenarios. Our work opens up new possibilities for enhancing robotic applications that require precise grasping and manipulation skills of real-world reconstructed objects.
</details>
<details>
<summary>摘要</summary>
人类环境中机器人的普及进展，导致了一系列物体抓取技巧的发展，dexterity在这些技巧中扮演着关键角色。人类在抓取物体时表现出了惊人的灵活性，这种灵活性归功于对物体特性（如重量、大小、形状）的稳固了理解，以及与物体进行互动的出色能力。手姿常常反映物体需要抓取的特定区域的影响，特别是当物体只部分可见时。在这项工作中，我们利用人类对物体的理解，通过重建和完成部分观察的物体形态，并使用7自由度人工手掌进行抓取。我们的方法比基eline只有部分重建时的抓取成功率提高了近30%，并在不同的物体类别上达成了150多次成功的抓取。这表明我们的方法可以在真实世界enario中预测和执行基于完整的物体形态的抓取姿势，开启了新的机器人应用的可能性，例如精准的抓取和 manipulate技巧。
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Reconstruction-of-the-Left-Atrium-using-Sparse-Catheter-Paths"><a href="#Neural-Network-Reconstruction-of-the-Left-Atrium-using-Sparse-Catheter-Paths" class="headerlink" title="Neural Network Reconstruction of the Left Atrium using Sparse Catheter Paths"></a>Neural Network Reconstruction of the Left Atrium using Sparse Catheter Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02488">http://arxiv.org/abs/2311.02488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alon Baram, Moshe Safran, Tomer Noy, Naveh Geri, Hayit Greenspan</li>
<li>for: 这个论文是为了提供一种可以在进程早期提供左心室Visualization的方法，以便使用简单的刺激器动作来获得 Left atrial shape reconstruction。</li>
<li>methods: 该论文提出了一种 dense encoder-decoder 网络，并使用一种新的Regularization term来重构左心室的形状。</li>
<li>results: 该论文表明，该方法可以在3分钟内基于部分数据来重构 Left atrial shape，并且可以生成真实的Visualization。 Synthetic和人类临床案例都被示出。<details>
<summary>Abstract</summary>
Catheter based radiofrequency ablation for pulmonary vein isolation has become the first line of treatment for atrial fibrillation in recent years. This requires a rather accurate map of the left atrial sub-endocardial surface including the ostia of the pulmonary veins, which requires dense sampling of the surface and takes more than 10 minutes. The focus of this work is to provide left atrial visualization early in the procedure to ease procedure complexity and enable further workflows, such as using catheters that have difficulty sampling the surface. We propose a dense encoder-decoder network with a novel regularization term to reconstruct the shape of the left atrium from partial data which is derived from simple catheter maneuvers. To train the network, we acquire a large dataset of 3D atria shapes and generate corresponding catheter trajectories. Once trained, we show that the suggested network can sufficiently approximate the atrium shape based on a given trajectory. We compare several network solutions for the 3D atrium reconstruction. We demonstrate that the solution proposed produces realistic visualization using partial acquisition within a 3-minute time interval. Synthetic and human clinical cases are shown.
</details>
<details>
<summary>摘要</summary>
医疗器械导管基于射频热力学隔离，作为现代抗不规征性颤动疾病治疗的首选方式，已经在过去几年得到广泛应用。这需要一个非常准确的左心室内部表面地图，包括肺动脉口，这需要密集的表面探测，需要 более10分钟的时间。我们的目标是提供早期左心室视觉，以便简化过程复杂性和启用更多的工作流程，如使用困难探测表面的导管。我们提议一种密集编码-解码网络，以重建左心室形状从partial数据中。为了训练网络，我们收集了大量3Datria形状数据，并生成相应的导管轨迹。我们显示，我们的提议的网络可以基于给定轨迹sufficiently approximate left atrium shape。我们比较了多个网络解决方案，并显示我们的解决方案可以生成真实的视觉使用部分收集在3分钟时间内。我们使用 sintetic和人类临床案例展示。
</details></li>
</ul>
<hr>
<h2 id="A-Strictly-Bounded-Deep-Network-for-Unpaired-Cyclic-Translation-of-Medical-Images"><a href="#A-Strictly-Bounded-Deep-Network-for-Unpaired-Cyclic-Translation-of-Medical-Images" class="headerlink" title="A Strictly Bounded Deep Network for Unpaired Cyclic Translation of Medical Images"></a>A Strictly Bounded Deep Network for Unpaired Cyclic Translation of Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02480">http://arxiv.org/abs/2311.02480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swati Rai, Jignesh S. Bhatt, Sarat Kumar Patra</li>
<li>for: 这个论文是关于医学影像翻译的一种解决方案，它的目标是提供一种稳定的双向翻译模型，可以处理不同模式的医学影像。</li>
<li>methods: 该论文提出了一种基于patch-level concatenated cyclic conditional generative adversarial network（pCCGAN）的方法，它包括两个相互连接的CGAN，每个 generator都是conditional的，它们使用叠加的异质补做patches来学习特征。同时， generator还使用adaptive dictionary来降低可能的衰减。</li>
<li>results: 论文的实验结果表明，该方法可以在实际的CT和MRI图像翻译中提供superior的结果，并且通过质量、量化和简洁分析表明，该方法可以减少变异和提高稳定性。<details>
<summary>Abstract</summary>
Medical image translation is an ill-posed problem. Unlike existing paired unbounded unidirectional translation networks, in this paper, we consider unpaired medical images and provide a strictly bounded network that yields a stable bidirectional translation. We propose a patch-level concatenated cyclic conditional generative adversarial network (pCCGAN) embedded with adaptive dictionary learning. It consists of two cyclically connected CGANs of 47 layers each; where both generators (each of 32 layers) are conditioned with concatenation of alternate unpaired patches from input and target modality images (not ground truth) of the same organ. The key idea is to exploit cross-neighborhood contextual feature information that bounds the translation space and boosts generalization. The generators are further equipped with adaptive dictionaries learned from the contextual patches to reduce possible degradation. Discriminators are 15-layer deep networks that employ minimax function to validate the translated imagery. A combined loss function is formulated with adversarial, non-adversarial, forward-backward cyclic, and identity losses that further minimize the variance of the proposed learning machine. Qualitative, quantitative, and ablation analysis show superior results on real CT and MRI.
</details>
<details>
<summary>摘要</summary>
医学图像翻译是一个不定Problem。不同于现有的已经对应的无限向量翻译网络，在这篇论文中，我们考虑了无对应的医学图像，并提供了一个具有稳定性的双向翻译网络。我们提议了一种patch-level concatenated cyclic conditional generative adversarial network (pCCGAN)，它包括两个相互连接的CGAN，每个CGAN都有47层，其中每个生成器都是通过 concatenation of alternate unpaired patches from input and target modality images (不是真实的ground truth) of the same organ来Conditional generation。我们的关键思想是利用跨邻域特征信息，以防止翻译空间过大，并提高泛化能力。生成器还使用了从Contextual patches中学习的自适应字典，以避免可能的下降。检测器是15层深度的网络，使用最大函数来验证翻译的图像。我们定义了一个组合损失函数，包括对抗损失、非对抗损失、前向后向环路损失和标识损失，以更加减小提议的学习机器的幂等误差。Qualitative、quantitative和ablation分析表明，我们的方法在真实的CT和MRI上达到了superior的结果。
</details></li>
</ul>
<hr>
<h2 id="SPHEAR-Spherical-Head-Registration-for-Complete-Statistical-3D-Modeling"><a href="#SPHEAR-Spherical-Head-Registration-for-Complete-Statistical-3D-Modeling" class="headerlink" title="SPHEAR: Spherical Head Registration for Complete Statistical 3D Modeling"></a>SPHEAR: Spherical Head Registration for Complete Statistical 3D Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02461">http://arxiv.org/abs/2311.02461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduard Gabriel Bazavan, Andrei Zanfir, Thiemo Alldieck, Teodor Alexandru Szente, Mihai Zanfir, Cristian Sminchisescu</li>
<li>for: 该论文旨在提出一种准确、可导的参数统计3D人头模型，基于圆柱体嵌入的新型3D注册方法。</li>
<li>methods: 该模型使用非rigid注册方法，不受表面假设所限制，提高重建准确性，并减少人工干预。</li>
<li>results: 该模型可以生成高分辨率的自然色Texture、表面法向图、毛发辐射图等，同时支持自动化的视觉数据生成、 semantic 注释和总体重建任务。与现有方法相比，该模型的组件快速、内存利用率高，实验证明了设计方式的有效性和注册、重建和生成技术的准确性。<details>
<summary>Abstract</summary>
We present \emph{SPHEAR}, an accurate, differentiable parametric statistical 3D human head model, enabled by a novel 3D registration method based on spherical embeddings. We shift the paradigm away from the classical Non-Rigid Registration methods, which operate under various surface priors, increasing reconstruction fidelity and minimizing required human intervention. Additionally, SPHEAR is a \emph{complete} model that allows not only to sample diverse synthetic head shapes and facial expressions, but also gaze directions, high-resolution color textures, surface normal maps, and hair cuts represented in detail, as strands. SPHEAR can be used for automatic realistic visual data generation, semantic annotation, and general reconstruction tasks. Compared to state-of-the-art approaches, our components are fast and memory efficient, and experiments support the validity of our design choices and the accuracy of registration, reconstruction and generation techniques.
</details>
<details>
<summary>摘要</summary>
我们介绍了SPHEAR，一种精准、可微分的参数型三维人头模型，基于圆形嵌入的新三维 регистра方法。我们弃却了传统的非固定注册方法，这些方法基于不同的表面优先级，从而提高了重建准确性并最小化了人工干预。此外，SPHEAR是一个完整的模型，允许不仅采样多种 sintetic 头部形状和 facial expression，还可以控制视线方向、高分辨率颜色Texture、表面法向图和毛发 Represented in detail, as strands。SPHEAR可以用于自动生成真实的视觉数据，semantic annotation和总体重建任务。相比之前的方法，我们的组件快速和内存减少，实验证明了我们的设计选择和注册、重建和生成技术的准确性。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Network-Structures-from-Corporate-Organization-Charts-Using-Heuristic-Image-Processing"><a href="#Extracting-Network-Structures-from-Corporate-Organization-Charts-Using-Heuristic-Image-Processing" class="headerlink" title="Extracting Network Structures from Corporate Organization Charts Using Heuristic Image Processing"></a>Extracting Network Structures from Corporate Organization Charts Using Heuristic Image Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02460">http://arxiv.org/abs/2311.02460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiroki Sayama, Junichi Yamanoi</li>
<li>for: 这个研究旨在开拓企业结构的影响力和性能表现。</li>
<li>methods: 研究者开发了一种图像处理方法，用于从公司组织架构图中提取和重构组织网络数据。该方法包括多个逻辑步骤，通过识别文本标签、方框、连接线和其他对象来检测组织架构图中的元素。检测到的元素将被组织成Python的NetworkX图形对象，用于可视化、验证和进一步的网络分析。</li>
<li>results: 研究者通过应用该方法，成功地从2008年至2011年《企业组织架构图&#x2F;系统图手册》中的10,008个组织架构PDF文档中提取了4,606个组织网络（数据获取成功率为46%）。对每个重构的组织网络进行了多种网络指标的测量，以便进一步的统计分析，以 investigate 其可能的相关性与企业行为和表现。<details>
<summary>Abstract</summary>
Organizational structure of corporations has potential to provide implications for dynamics and performance of corporate operations. However, this subject has remained unexplored because of the lack of readily available organization network datasets. To overcome the this gap, we developed a new heuristic image-processing method to extract and reconstruct organization network data from published organization charts. Our method analyzes a PDF file of a corporate organization chart and detects text labels, boxes, connecting lines, and other objects through multiple steps of heuristically implemented image processing. The detected components are reorganized together into a Python's NetworkX Graph object for visualization, validation and further network analysis. We applied the developed method to the organization charts of all the listed firms in Japan shown in the ``Organization Chart/System Diagram Handbook'' published by Diamond, Inc., from 2008 to 2011. Out of the 10,008 organization chart PDF files, our method was able to reconstruct 4,606 organization networks (data acquisition success rate: 46%). For each reconstructed organization network, we measured several network diagnostics, which will be used for further statistical analysis to investigate their potential correlations with corporate behavior and performance.
</details>
<details>
<summary>摘要</summary>
企业组织结构具有可能对企业运营动态和性能产生影响，但这个主题尚未被探讨，因为有限的可用组织网络数据。为了bridge这个阻隔，我们开发了一种新的图像处理方法，用于从公布的组织图中提取和重建组织网络数据。我们的方法通过多个逻辑地图处理步骤，从PDF文档中的组织图中检测文本标签、方块、连接线和其他对象。检测到的组件被重新组织为Python的NetworkX图形对象，以供可视化、验证和进一步的网络分析。我们应用了这种方法于日本上市公司《组织图/系统 диаграм手册》（Diamond, Inc.，2008-2011年）中所显示的10,008个组织图PDF文档中。其中，我们的方法成功地重建了4,606个组织网络（数据获取成功率：46%）。对每个重建的组织网络，我们测量了多种网络指标，这些指标将用于进一步的统计分析，以研究它们可能与企业行为和性能之间的相关性。
</details></li>
</ul>
<hr>
<h2 id="P-Age-Pexels-Dataset-for-Robust-Spatio-Temporal-Apparent-Age-Classification"><a href="#P-Age-Pexels-Dataset-for-Robust-Spatio-Temporal-Apparent-Age-Classification" class="headerlink" title="P-Age: Pexels Dataset for Robust Spatio-Temporal Apparent Age Classification"></a>P-Age: Pexels Dataset for Robust Spatio-Temporal Apparent Age Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02432">http://arxiv.org/abs/2311.02432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abid Ali, Ashish Marisetty, Francois Bremond</li>
<li>for: 这个论文主要targets age estimation in videos, addressing challenges such as occlusions, low resolution, and lighting conditions.</li>
<li>methods: 该论文提出了一种新的方向 для年龄分类，利用视频基于模型来 capture 人体动态信息，dominating 面部基于方法。该方法使用两树结构，TimeSformer和EfficientNet作为 backing，以 efficiently capture 人体和面部动态信息。</li>
<li>results: 该方法在不同的视频数据集上进行了评测，与现有的面部基于方法相比，达到了更高的准确率。特别是在真实世界情况下，当人脸受到干扰、模糊或遮盾时，该方法仍能够准确地 estimte 年龄。<details>
<summary>Abstract</summary>
Age estimation is a challenging task that has numerous applications. In this paper, we propose a new direction for age classification that utilizes a video-based model to address challenges such as occlusions, low-resolution, and lighting conditions. To address these challenges, we propose AgeFormer which utilizes spatio-temporal information on the dynamics of the entire body dominating face-based methods for age classification. Our novel two-stream architecture uses TimeSformer and EfficientNet as backbones, to effectively capture both facial and body dynamics information for efficient and accurate age estimation in videos. Furthermore, to fill the gap in predicting age in real-world situations from videos, we construct a video dataset called Pexels Age (P-Age) for age classification. The proposed method achieves superior results compared to existing face-based age estimation methods and is evaluated in situations where the face is highly occluded, blurred, or masked. The method is also cross-tested on a variety of challenging video datasets such as Charades, Smarthome, and Thumos-14.
</details>
<details>
<summary>摘要</summary>
���������ж�� Age 估计是一项复杂的任务，具有许多应用。在这篇论文中，我们提出了一种新的方向 для age 分类，利用视频基本模型来解决 occlusions、低分辨率和照明条件等挑战。为了解决这些挑战，我们提出了 AgeFormer，它利用全身动态信息来替代面部基本方法 для age 分类。我们的新型两核体系使用 TimeSformer 和 EfficientNet 作为后备网络，以有效地捕捉全身动态信息以实现高效准确的 age 估计。此外，为了填充实际情况中的 age 估计漏斗，我们建立了一个名为 Pexels Age (P-Age) 的视频 dataset，用于 age 分类。我们的方法在不同的挑战性视频 dataset 上实现了比较出色的结果，比如 Charades、Smarthome 和 Thumos-14。
</details></li>
</ul>
<hr>
<h2 id="Task-Arithmetic-with-LoRA-for-Continual-Learning"><a href="#Task-Arithmetic-with-LoRA-for-Continual-Learning" class="headerlink" title="Task Arithmetic with LoRA for Continual Learning"></a>Task Arithmetic with LoRA for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02428">http://arxiv.org/abs/2311.02428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajas Chitale, Ankit Vaidya, Aditya Kane, Archana Ghotkar</li>
<li>for: 这篇论文旨在解决连续训练问题，即训练数据分配为“任务”的序列。</li>
<li>methods: 我们提出了一种使用低维数据适应和任务加权的新方法，以解决连续训练问题和对模型进行多次训练的计算成本问题。</li>
<li>results: 我们的方法可以完全对抗快速忘却问题，并且降低了训练模型的计算成本。将10个类别的样本存储在小型快取中可以实现近似于全集调整的性能。<details>
<summary>Abstract</summary>
Continual learning refers to the problem where the training data is available in sequential chunks, termed "tasks". The majority of progress in continual learning has been stunted by the problem of catastrophic forgetting, which is caused by sequential training of the model on streams of data. Moreover, it becomes computationally expensive to sequentially train large models multiple times. To mitigate both of these problems at once, we propose a novel method to continually train transformer-based vision models using low-rank adaptation and task arithmetic. Our method completely bypasses the problem of catastrophic forgetting, as well as reducing the computational requirement for training models on each task. When aided with a small memory of 10 samples per class, our method achieves performance close to full-set finetuning. We present rigorous ablations to support the prowess of our method.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese: kontinuäl learning réferë à problém where training data è disponibile in sequential chunks, termed "tasks". Majorité avancements in kontinuäl learning hanno été stunted per problem of catastrophic forgetting, which è caused da sequential training of model on streams of data. Moreover, it becomes computationally expensive to sequentially train large models multiple times. To mitigate both of these problems at once, we propose a novel method to continually train transformer-based vision models using low-rank adaptation and task arithmetic. Our method completely bypasses the problem of catastrophic forgetting, as well as reducing the computational requirement for training models on each task. When aided with a small memory of 10 samples per class, our method achieves performance close to full-set finetuning. We present rigorous ablations to support the prowess of our method.
</details></li>
</ul>
<hr>
<h2 id="P2O-Calib-Camera-LiDAR-Calibration-Using-Point-Pair-Spatial-Occlusion-Relationship"><a href="#P2O-Calib-Camera-LiDAR-Calibration-Using-Point-Pair-Spatial-Occlusion-Relationship" class="headerlink" title="P2O-Calib: Camera-LiDAR Calibration Using Point-Pair Spatial Occlusion Relationship"></a>P2O-Calib: Camera-LiDAR Calibration Using Point-Pair Spatial Occlusion Relationship</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02413">http://arxiv.org/abs/2311.02413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Su Wang, Shini Zhang, Xuchong Qiu</li>
<li>for: 提高自动驾驶和机器人领域中感知器的准确和可靠性，通过缺乏目标的准则进行抗干扰的三个维度推准。</li>
<li>methods: 基于三元比较关系的2D-3D边缘点抽取法，以及基于抽取的2D-3D点对对应关系进行干扰导向的点匹配方法。</li>
<li>results: 对实际图像集KITTI进行评估，比较方法与现有目标 moins 方法的性能，结果表明，提出的方法可以在各种环境中具有低误差和高可靠性，为实际应用中的高质量摄像头-LiDAR推准做出贡献。<details>
<summary>Abstract</summary>
The accurate and robust calibration result of sensors is considered as an important building block to the follow-up research in the autonomous driving and robotics domain. The current works involving extrinsic calibration between 3D LiDARs and monocular cameras mainly focus on target-based and target-less methods. The target-based methods are often utilized offline because of restrictions, such as additional target design and target placement limits. The current target-less methods suffer from feature indeterminacy and feature mismatching in various environments. To alleviate these limitations, we propose a novel target-less calibration approach which is based on the 2D-3D edge point extraction using the occlusion relationship in 3D space. Based on the extracted 2D-3D point pairs, we further propose an occlusion-guided point-matching method that improves the calibration accuracy and reduces computation costs. To validate the effectiveness of our approach, we evaluate the method performance qualitatively and quantitatively on real images from the KITTI dataset. The results demonstrate that our method outperforms the existing target-less methods and achieves low error and high robustness that can contribute to the practical applications relying on high-quality Camera-LiDAR calibration.
</details>
<details>
<summary>摘要</summary>
“准确和可靠的感知器的准确性是自动驾驶和机器人领域的重要基础结构。目前的外部投入calibration方法主要集中在目标基础和无目标方法两个领域。目标基础方法通常在线上使用，但是受到附加的目标设计和目标置放限制。现有的无目标方法受到环境中的特征不确定和特征匹配问题的影响。为了解决这些限制，我们提出了一种新的无目标准确方法，基于2D-3D边缘点提取和3D空间 occlusion 关系。基于提取的2D-3D点对，我们进一步提出了一种 occlusion-guided 点对应方法，可以提高准确率并降低计算成本。为验证我们的方法的有效性，我们对实际来自 KITTI 数据集的图像进行质量评估。结果表明，我们的方法在无目标情况下比现有的方法更高精度和更高可靠性，可以为实际应用中的高质量 Camera-LiDAR 准确协调做出贡献。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-quantum-image-classification-and-federated-learning-for-hepatic-steatosis-diagnosis"><a href="#Hybrid-quantum-image-classification-and-federated-learning-for-hepatic-steatosis-diagnosis" class="headerlink" title="Hybrid quantum image classification and federated learning for hepatic steatosis diagnosis"></a>Hybrid quantum image classification and federated learning for hepatic steatosis diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02402">http://arxiv.org/abs/2311.02402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Lusnig, Asel Sagingalieva, Mikhail Surmach, Tatjana Protasevich, Ovidiu Michiu, Joseph McLoughlin, Christopher Mansell, Graziano de’ Petris, Deborah Bonazza, Fabrizio Zanconati, Alexey Melnikov, Fabio Cavalli<br>for: 这项研究旨在开发一种可助Pathologist在日常诊断中使用的智能系统，该系统可以利用深度学习技术和量子计算技术，并采用联合学习方法来实现隐私友好的多方参与学习。methods: 该研究使用了一种混合式量子神经网络，该网络包括5个量子比特和超过100个变量门，可以用于评估非酒精性肝脂肿，并提出了一种基于类型深度学习解决方案，通过减少每个参与者的数据量来解决隐私问题。results: 研究发现，混合式量子神经网络的肝脂肿图像分类精度达97%，高于其类似的类型深度学习模型（ResNet）的95.2%，而且在减少数据量的情况下，hybrid方法仍然能够superior generalization和less potential for overfitting，这表明该方法在医疗应用中具有优异的普适性和可靠性。<details>
<summary>Abstract</summary>
With the maturity achieved by deep learning techniques, intelligent systems that can assist physicians in the daily interpretation of clinical images can play a very important role. In addition, quantum techniques applied to deep learning can enhance this performance, and federated learning techniques can realize privacy-friendly collaborative learning among different participants, solving privacy issues due to the use of sensitive data and reducing the number of data to be collected for each individual participant. We present in this study a hybrid quantum neural network that can be used to quantify non-alcoholic liver steatosis and could be useful in the diagnostic process to determine a liver's suitability for transplantation; at the same time, we propose a federated learning approach based on a classical deep learning solution to solve the same problem, but using a reduced data set in each part. The liver steatosis image classification accuracy of the hybrid quantum neural network, the hybrid quantum ResNet model, consisted of 5 qubits and more than 100 variational gates, reaches 97%, which is 1.8% higher than its classical counterpart, ResNet. Crucially, that even with a reduced dataset, our hybrid approach consistently outperformed its classical counterpart, indicating superior generalization and less potential for overfitting in medical applications. In addition, a federated approach with multiple clients, up to 32, despite the lower accuracy, but still higher than 90%, would allow using, for each participant, a very small dataset, i.e., up to one-thirtieth. Our work, based over real-word clinical data can be regarded as a scalable and collaborative starting point, could thus fulfill the need for an effective and reliable computer-assisted system that facilitates the daily diagnostic work of the clinical pathologist.
</details>
<details>
<summary>摘要</summary>
随着深度学习技术的成熔，智能系统可以帮助医生日常解读临床图像，扮演着非常重要的角色。此外，应用于深度学习的量子技术可以提高这种性能，而 federated learning 技术可以实现隐私友好的合作学习，解决由敏感数据使用而导致的隐私问题，同时减少每个参与者需要收集的数据量。在本研究中，我们提出了一种混合量子神经网络，可以用于评估非酒精肝炎病变，并且可以在诊断过程中决定肝脏的适用性 для移植。同时，我们提出了基于类型深度学习解决方案的联邦学习方法，可以使用减少的数据集来解决同一个问题。混合量子神经网络的肝炎病变图像分类精度达97%，高于其类型深度学习模型（ResNet）的95.2%。更重要的是，我们的混合方法在减少数据集时仍然可以高效地分类肝炎病变图像，这表明它在医疗应用中具有更好的总结和更少的潜在过拟合问题。此外，多客户联邦学习方法可以使用每个参与者的非常小数据集（最多一半）来解决同一个问题，尽管它的准确率虽然不如单个客户的混合量子神经网络，但仍高于90%。我们的工作，基于实际临床数据，可以视为一种可扩展和协作的起点，可以满足医疗应用中的有效和可靠计算助手的需求。
</details></li>
</ul>
<hr>
<h2 id="Domain-Transfer-in-Latent-Space-DTLS-Wins-on-Image-Super-Resolution-–-a-Non-Denoising-Model"><a href="#Domain-Transfer-in-Latent-Space-DTLS-Wins-on-Image-Super-Resolution-–-a-Non-Denoising-Model" class="headerlink" title="Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution – a Non-Denoising Model"></a>Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution – a Non-Denoising Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02358">http://arxiv.org/abs/2311.02358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun-Chuen Hui, Wan-Chi Siu, Ngai-Fong Law</li>
<li>for: 大规模图像超分辨是一项computer vision任务，因为很多信息在高度压缩图像中缺失，例如scale x16超分辨。</li>
<li>methods:  diffusion models 在过去几年中得到了成功，它使用 Gaussian noise 来构建一个 latent photo-realistic space，并作为连接latent vector space和 latent photo-realistic space的链接。</li>
<li>results: 在这篇文章中，我们提出了一种简单的方法，它不使用 Gaussian noise，而是采用基于diffusion models的一些基本结构来实现高质量的图像超分辨。我们使用 DNN 来实现域传递，以便利用邻域域的统计特性来进行渐进 interpolate，并通过参照输入LR图像来进行条件域传递，从而进一步提高图像质量。实验结果表明，我们的方法不仅超越了当前的大规模超分辨模型，还超越了当前的扩散模型。这种方法可以轻松扩展到其他图像到图像任务，如图像照明、填充、降噪等。<details>
<summary>Abstract</summary>
Large scale image super-resolution is a challenging computer vision task, since vast information is missing in a highly degraded image, say for example forscale x16 super-resolution. Diffusion models are used successfully in recent years in extreme super-resolution applications, in which Gaussian noise is used as a means to form a latent photo-realistic space, and acts as a link between the space of latent vectors and the latent photo-realistic space. There are quite a few sophisticated mathematical derivations on mapping the statistics of Gaussian noises making Diffusion Models successful. In this paper we propose a simple approach which gets away from using Gaussian noise but adopts some basic structures of diffusion models for efficient image super-resolution. Essentially, we propose a DNN to perform domain transfer between neighbor domains, which can learn the differences in statistical properties to facilitate gradual interpolation with results of reasonable quality. Further quality improvement is achieved by conditioning the domain transfer with reference to the input LR image. Experimental results show that our method outperforms not only state-of-the-art large scale super resolution models, but also the current diffusion models for image super-resolution. The approach can readily be extended to other image-to-image tasks, such as image enlightening, inpainting, denoising, etc.
</details>
<details>
<summary>摘要</summary>
大规模图像超解析是一项计算机视觉任务，因为高度受损图像中的信息量很大，例如 scale x16 超解析。扩散模型在过去几年得到了成功，在极端超解析应用中使用 Gaussian 噪声作为一种 latent photo-realistic 空间的形成者，并作为 latent vector 空间和 latent photo-realistic 空间之间的连接。有很多复杂的数学推导，映射 Gaussian 噪声的统计特性，使扩散模型成功。在这篇论文中，我们提出了一种简单的方法，不使用 Gaussian 噪声，但采用了扩散模型的一些基本结构，实现高质量图像超解析。我们提议使用 DNN 进行频率域传输，以便利用邻域频率的不同来实现慢滑均衡，并通过参考输入低解析图像来进行条件域传输，从而进一步提高图像质量。实验结果表明，我们的方法不仅超过了当前大规模超解析模型的状态，还超过了当前扩散模型的图像超解析性能。该方法可以轻松扩展到其他图像-图像任务，如图像照明、填充、去噪等。
</details></li>
</ul>
<hr>
<h2 id="Proposal-Level-Unsupervised-Domain-Adaptation-for-Open-World-Unbiased-Detector"><a href="#Proposal-Level-Unsupervised-Domain-Adaptation-for-Open-World-Unbiased-Detector" class="headerlink" title="Proposal-Level Unsupervised Domain Adaptation for Open World Unbiased Detector"></a>Proposal-Level Unsupervised Domain Adaptation for Open World Unbiased Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02342">http://arxiv.org/abs/2311.02342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanyi Liu, Zhongqi Yue, Xian-Sheng Hua</li>
<li>for: 开展开放世界对象检测（OWOD），结合开放集合对象检测和逐步学习能力，面对视觉世界中的开放和动态挑战。</li>
<li>methods: 采用Unsupervised Domain Adaptation方法，将已知类别的预测器作为源频谱，使用自动回归法学习域 invariants 的前景特征，实现不偏向的预测。</li>
<li>results: 通过OOD evaluation，我们达到了状态 искусственный智能的性能水平。<details>
<summary>Abstract</summary>
Open World Object Detection (OWOD) combines open-set object detection with incremental learning capabilities to handle the challenge of the open and dynamic visual world. Existing works assume that a foreground predictor trained on the seen categories can be directly transferred to identify the unseen categories' locations by selecting the top-k most confident foreground predictions. However, the assumption is hardly valid in practice. This is because the predictor is inevitably biased to the known categories, and fails under the shift in the appearance of the unseen categories. In this work, we aim to build an unbiased foreground predictor by re-formulating the task under Unsupervised Domain Adaptation, where the current biased predictor helps form the domains: the seen object locations and confident background locations as the source domain, and the rest ambiguous ones as the target domain. Then, we adopt the simple and effective self-training method to learn a predictor based on the domain-invariant foreground features, hence achieving unbiased prediction robust to the shift in appearance between the seen and unseen categories. Our approach's pipeline can adapt to various detection frameworks and UDA methods, empirically validated by OWOD evaluation, where we achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Translation notes:* Open World Object Detection (OWOD) is translated as "开放世界物体检测" (kāifàng shìjiè wùzhì kǎoyan).* existing works is translated as "现有的工作" (xiàn yǒu de gōngzuò).* foreground predictor is translated as "前景预测器" (qiánjìng yùdiǎn).* unseen categories is translated as "未知类别" (wèi zhī lèibì).* domain adaptation is translated as "领域适应" (dòngyì tiěbìng).* self-training method is translated as "自我启用法" (zìwǒ kāifàng fǎ).* domain-invariant features is translated as "领域不变特征" (dòngyì bùbiàn tèzhèng).* unbiased prediction is translated as "无偏预测" (wùpíng yùdiǎn).* state-of-the-art performance is translated as "顶尖性能" (dǐngjiān xìngnéng).
</details></li>
</ul>
<hr>
<h2 id="MC-Stereo-Multi-peak-Lookup-and-Cascade-Search-Range-for-Stereo-Matching"><a href="#MC-Stereo-Multi-peak-Lookup-and-Cascade-Search-Range-for-Stereo-Matching" class="headerlink" title="MC-Stereo: Multi-peak Lookup and Cascade Search Range for Stereo Matching"></a>MC-Stereo: Multi-peak Lookup and Cascade Search Range for Stereo Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02340">http://arxiv.org/abs/2311.02340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaojie Feng, Junda Cheng, Hao Jia, Longliang Liu, Gangwei Xu, Xin Yang</li>
<li>for: 本研究旨在提高iterative optimization方法的掌握能力，特别是解决多峰分布问题和固定搜索范围的限制。</li>
<li>methods: 本文提出了一种新的iterative optimization架构，称为MC-Stereo，它通过多峰查找策略来减轻多峰分布问题，并在迭代框架中 интеGRATE了粗细搜索的概念。此外，我们还引入了一个预训练的网络来提高前端的特征提取器。</li>
<li>results: 根据实验结果，MC-Stereo在KITTI-2012和KITTI-2015测试集上 ranked first among all publicly available方法，并在ETH3D上达到了领域内最佳性能。<details>
<summary>Abstract</summary>
Stereo matching is a fundamental task in scene comprehension. In recent years, the method based on iterative optimization has shown promise in stereo matching. However, the current iteration framework employs a single-peak lookup, which struggles to handle the multi-peak problem effectively. Additionally, the fixed search range used during the iteration process limits the final convergence effects. To address these issues, we present a novel iterative optimization architecture called MC-Stereo. This architecture mitigates the multi-peak distribution problem in matching through the multi-peak lookup strategy, and integrates the coarse-to-fine concept into the iterative framework via the cascade search range. Furthermore, given that feature representation learning is crucial for successful learnbased stereo matching, we introduce a pre-trained network to serve as the feature extractor, enhancing the front end of the stereo matching pipeline. Based on these improvements, MC-Stereo ranks first among all publicly available methods on the KITTI-2012 and KITTI-2015 benchmarks, and also achieves state-of-the-art performance on ETH3D. The code will be open sourced after the publication of this paper.
</details>
<details>
<summary>摘要</summary>
斯tereo匹配是Scene理解中的基本任务。在最近几年，基于迭代优化的方法在斯tereo匹配中表现良好。然而，当前的迭代框架使用单峰搜索，在多峰问题中效果不佳。此外，使用的fixed搜索范围限制了最终的整合效果。为解决这些问题，我们提出了一种新的迭代优化架构，称为MC-Stereo。这种架构通过多峰搜索策略解决了匹配中的多峰分布问题，并通过缩放搜索范围实现了从粗到细的搜索概念。此外，由于特征表示学习是成功的learnbased斯tereo匹配的关键，我们引入了预训练的网络作为特征提取器，从而提高了斯tereo匹配管线的前端。基于这些改进，MC-Stereo在KITTI-2012和KITTI-2015benchmark上名列前茅，并在ETH3D上实现了状态的art performance。代码将在本文发表后开源。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Machine-Learning-for-Clinically-Assistive-Imaging-Based-Biomedical-Applications"><a href="#Multimodal-Machine-Learning-for-Clinically-Assistive-Imaging-Based-Biomedical-Applications" class="headerlink" title="Multimodal Machine Learning for Clinically-Assistive Imaging-Based Biomedical Applications"></a>Multimodal Machine Learning for Clinically-Assistive Imaging-Based Biomedical Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02332">http://arxiv.org/abs/2311.02332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elisa Warner, Joonsang Lee, William Hsu, Tanveer Syeda-Mahmood, Charles Kahn, Arvind Rao</li>
<li>for: 这项研究旨在探讨适用于医疗人工智能系统的机器学习应用，尤其是在多modal数据集合 integrate 方面。</li>
<li>methods: 这篇论文描述了五种对多modal AI 的挑战（表示、融合、对接、翻译和共学习），并评估了这些挑战在医疗影像基础的临床决策支持模型中的应用。</li>
<li>results: 这篇论文结论提出了未来这个领域的发展趋势，并建议了在成功诊断模型的翻译和应用中进一步探索的方向。<details>
<summary>Abstract</summary>
Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models and even more recently generative models. Recent years have seen a rise in the discovery of widely-available deep learning architectures that support multimodal data integration, particularly with images. The incorporation of multiple modalities into these models is a thriving research topic, presenting its own unique challenges. In this work, we discuss five challenges to multimodal AI as it pertains to ML (representation, fusion, alignment, translation, and co-learning) and survey recent approaches to addressing these challenges in the context of medical image-based clinical decision support models. We conclude with a discussion of the future of the field, suggesting directions that should be elucidated further for successful clinical models and their translation to the clinical setting.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在医疗人工智能（AI）系统中的应用从传统和统计方法逐渐转移到深度学习模型，并在最近几年内，更多地使用生成模型。在过去几年中，我们发现了许多可用的深度学习架构，尤其是与图像集成。将多种模式integrated into these models presents unique challenges. In this work, we discuss five challenges to multimodal AI as it pertains to ML (representation, fusion, alignment, translation, and co-learning) and survey recent approaches to addressing these challenges in the context of medical image-based clinical decision support models. We conclude with a discussion of the future of the field, suggesting directions that should be further explored for successful clinical models and their translation to the clinical setting.Here's a breakdown of the translation:1. 机器学习 (ML) - Machine learning2. 在医疗人工智能 (AI)系统中 - In medical artificial intelligence systems3. 应用从传统和统计方法逐渐转移 - From traditional and statistical methods to4. 到深度学习模型 - Deep learning models5. 并在最近几年内，更多地使用生成模型 - And more recently, using generative models6. 在过去几年中，我们发现了许多可用的深度学习架构 - In the past few years, we have found many available deep learning architectures7. 尤其是与图像集成 - Especially with image integration8. 将多种模式integrated into these models presents unique challenges - Integrating multiple modes into these models presents unique challenges9. In this work, we discuss five challenges to multimodal AI as it pertains to ML - In this work, we discuss five challenges to multimodal AI as it relates to machine learning10.  representation, fusion, alignment, translation, and co-learning - Representation, fusion, alignment, translation, and co-learning11. 并 survey recent approaches to addressing these challenges - And survey recent approaches to addressing these challenges12. 在医疗图像基础的临床决策模型中 - In medical image-based clinical decision support models13. We conclude with a discussion of the future of the field - We conclude with a discussion of the future of the field14. 建议更多的探索 - Suggesting further explorationPlease note that the translation is done in Simplified Chinese, which is the most widely used standard for Chinese writing. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Counting-Manatee-Aggregations-using-Deep-Neural-Networks-and-Anisotropic-Gaussian-Kernel"><a href="#Counting-Manatee-Aggregations-using-Deep-Neural-Networks-and-Anisotropic-Gaussian-Kernel" class="headerlink" title="Counting Manatee Aggregations using Deep Neural Networks and Anisotropic Gaussian Kernel"></a>Counting Manatee Aggregations using Deep Neural Networks and Anisotropic Gaussian Kernel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02315">http://arxiv.org/abs/2311.02315</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeyimilk/deep-learning-for-manatee-counting">https://github.com/yeyimilk/deep-learning-for-manatee-counting</a></li>
<li>paper_authors: Zhiqiang Wang, Yiran Pang, Cihan Ulus, Xingquan Zhu</li>
<li>for: 这个论文是为了自动计算水獭群体中的数量而写的。</li>
<li>methods: 该方法使用深度学习技术，使用低质量图像作为输入，并使用不同类型的深度神经网络来学习水獭的密度函数，以计算水獭群体中的数量。</li>
<li>results: 实验结果显示，使用Anisotropic Gaussian Kernel（AGK）kernel，并应用于不同类型的深度神经网络，可以准确地计算水獭群体中的数量，特别是在复杂背景环境下。<details>
<summary>Abstract</summary>
Manatees are aquatic mammals with voracious appetites. They rely on sea grass as the main food source, and often spend up to eight hours a day grazing. They move slow and frequently stay in group (i.e. aggregations) in shallow water to search for food, making them vulnerable to environment change and other risks. Accurate counting manatee aggregations within a region is not only biologically meaningful in observing their habit, but also crucial for designing safety rules for human boaters, divers, etc., as well as scheduling nursing, intervention, and other plans. In this paper, we propose a deep learning based crowd counting approach to automatically count number of manatees within a region, by using low quality images as input. Because manatees have unique shape and they often stay in shallow water in groups, water surface reflection, occlusion, camouflage etc. making it difficult to accurately count manatee numbers. To address the challenges, we propose to use Anisotropic Gaussian Kernel (AGK), with tunable rotation and variances, to ensure that density functions can maximally capture shapes of individual manatees in different aggregations. After that, we apply AGK kernel to different types of deep neural networks primarily designed for crowd counting, including VGG, SANet, Congested Scene Recognition network (CSRNet), MARUNet etc. to learn manatee densities and calculate number of manatees in the scene. By using generic low quality images extracted from surveillance videos, our experiment results and comparison show that AGK kernel based manatee counting achieves minimum Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The proposed method works particularly well for counting manatee aggregations in environments with complex background.
</details>
<details>
<summary>摘要</summary>
MANATEES 是水生哺乳动物，具有极高的食量。它们依赖于海草为食，并可能每天花费8小时在牧场。它们移动缓慢，并常常在浅水区寻找食物，使得它们易受环境变化和其他风险的影响。正确地计算MANATEES 的聚集数量在一个区域非仅生物学上重要，还是关键的 для设计人类潜水员、潜水等安全规则，以及安排护理、救援等计划。在这篇论文中，我们提出了基于深度学习的人ATEES 聚集计数方法，使用低质量图像作为输入。由于MANATEES 的特有形状和它们常常在浅水区寻找食物，水面反射、遮挡等因素使得准确计数MANATEES 数量非常困难。为了解决这些挑战，我们提议使用不规则 Gaussian kernel（AGK），并可调整旋转和方差，以确保AGK kernel能够最大化个体MANATEES 形状在不同的聚集中。然后，我们将AGK kernel应用到不同类型的深度神经网络，包括 VGG、SANet、 Congested Scene Recognition network（CSRNet）等，以学习MANATEES 的浓度函数并计算场景中的MANATEES 数量。通过使用普通的低质量图像，我们的实验结果和比较表明，AGK kernel基于MANATEES 计数实现了最小的精度平均误差（MAE）和平方根误差（RMSE）。我们的方法在环境复杂背景下特别有效。
</details></li>
</ul>
<hr>
<h2 id="LISNeRF-Mapping-LiDAR-based-Implicit-Mapping-via-Semantic-Neural-Fields-for-Large-Scale-3D-Scenes"><a href="#LISNeRF-Mapping-LiDAR-based-Implicit-Mapping-via-Semantic-Neural-Fields-for-Large-Scale-3D-Scenes" class="headerlink" title="LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes"></a>LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02313">http://arxiv.org/abs/2311.02313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianyuan Zhang, Zhiliu Yang</li>
<li>for: This paper proposes a method for large-scale 3D semantic reconstruction from LiDAR measurements alone, which is crucial for outdoor autonomous agents to fulfill high-level tasks such as planning and navigation.</li>
<li>methods: The proposed method uses an octree-based and hierarchical structure to store implicit features, which are decoded to semantic information and signed distance value through shallow Multilayer Perceptrons (MLPs). Off-the-shelf algorithms are used to predict semantic labels and instance IDs of point cloud, and the implicit features and MLPs parameters are jointly optimized with self-supervision paradigm for point cloud geometry and pseudo-supervision paradigm for semantic and panoptic labels.</li>
<li>results: The proposed method is evaluated on three real-world datasets, SemanticKITTI, SemanticPOSS, and nuScenes, and demonstrates effectiveness and efficiency compared to current state-of-the-art 3D mapping methods.<details>
<summary>Abstract</summary>
Large-scale semantic mapping is crucial for outdoor autonomous agents to fulfill high-level tasks such as planning and navigation. This paper proposes a novel method for large-scale 3D semantic reconstruction through implicit representations from LiDAR measurements alone. We firstly leverages an octree-based and hierarchical structure to store implicit features, then these implicit features are decoded to semantic information and signed distance value through shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf algorithms to predict the semantic labels and instance IDs of point cloud. Then we jointly optimize the implicit features and MLPs parameters with self-supervision paradigm for point cloud geometry and pseudo-supervision pradigm for semantic and panoptic labels. Subsequently, Marching Cubes algorithm is exploited to subdivide and visualize the scenes in the inferring stage. For scenarios with memory constraints, a map stitching strategy is also developed to merge sub-maps into a complete map. As far as we know, our method is the first work to reconstruct semantic implicit scenes from LiDAR-only input. Experiments on three real-world datasets, SemanticKITTI, SemanticPOSS and nuScenes, demonstrate the effectiveness and efficiency of our framework compared to current state-of-the-art 3D mapping methods.
</details>
<details>
<summary>摘要</summary>
大规模 semantic mapping 是外部自主 Agent 完成高级任务，如规划和导航的关键。这篇论文提出了一种基于 LiDAR 测量的大规模 3D semantic 重建方法。我们首先利用 Octree 结构来存储偏函数特征，然后使用 shallow Multilayer Perceptrons (MLPs) 来解码这些偏函数特征为 semantic 信息和 signed distance value。我们采用了市场上的算法来预测 semantic 标签和实例 ID 的点云。然后，我们同时优化偏函数和 MLPs 参数使用自我超级vised paradigm for point cloud geometry和 pseudo-supervision 概念 для semantic 和 panoptic 标签。在推断阶段，我们利用 Marching Cubes 算法来分割和可视化场景。对于存储限制的场景，我们还开发了一种 map stitching 策略来合并子地图到完整的地图。根据我们所知，我们的方法是首个从 LiDAR 输入 alone 重建 semantic implicit scene。我们在三个实际世界数据集（SemanticKITTI、SemanticPOSS 和 nuScenes）进行了实验，并证明了我们的框架在现状最佳的 3D 地图方法中表现出了效果和效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/cs.CV_2023_11_04/" data-id="clpxp040900mjfm88bc2y8v9k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/cs.AI_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T12:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/cs.AI_2023_11_04/">cs.AI - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="UniTSFace-Unified-Threshold-Integrated-Sample-to-Sample-Loss-for-Face-Recognition"><a href="#UniTSFace-Unified-Threshold-Integrated-Sample-to-Sample-Loss-for-Face-Recognition" class="headerlink" title="UniTSFace: Unified Threshold Integrated Sample-to-Sample Loss for Face Recognition"></a>UniTSFace: Unified Threshold Integrated Sample-to-Sample Loss for Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02523">http://arxiv.org/abs/2311.02523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiufu Li, Xi Jia, Jiancan Zhou, Linlin Shen, Jinming Duan</li>
<li>for: 这篇论文的目的是提出一种高效的面部识别方法，以满足现实世界中的面部验证应用。</li>
<li>methods: 该方法使用了一种新的集成损失函数（USS loss），该损失函数具有一个明确的统一阈值，用于分辨正面和负面对的对比。</li>
<li>results: 实验结果表明，提出的 USS loss 高效地使用了 sample-to-sample 的损失函数，并可以与 sample-to-class 的损失函数结合使用。此外，该方法在多个 benchmark 数据集上表现出色，比如 MFR、IJB-C、LFW、CFP-FP、AgeDB 和 MegaFace，并且可以超越现有的方法，如 CosFace、ArcFace、VPL、AnchorFace 和 UNPG。<details>
<summary>Abstract</summary>
Sample-to-class-based face recognition models can not fully explore the cross-sample relationship among large amounts of facial images, while sample-to-sample-based models require sophisticated pairing processes for training. Furthermore, neither method satisfies the requirements of real-world face verification applications, which expect a unified threshold separating positive from negative facial pairs. In this paper, we propose a unified threshold integrated sample-to-sample based loss (USS loss), which features an explicit unified threshold for distinguishing positive from negative pairs. Inspired by our USS loss, we also derive the sample-to-sample based softmax and BCE losses, and discuss their relationship. Extensive evaluation on multiple benchmark datasets, including MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace, demonstrates that the proposed USS loss is highly efficient and can work seamlessly with sample-to-class-based losses. The embedded loss (USS and sample-to-class Softmax loss) overcomes the pitfalls of previous approaches and the trained facial model UniTSFace exhibits exceptional performance, outperforming state-of-the-art methods, such as CosFace, ArcFace, VPL, AnchorFace, and UNPG. Our code is available.
</details>
<details>
<summary>摘要</summary>
“现有的面部识别模型（sample-to-class基本模型）无法充分利用大量的面部图像之间的交叉样本关系，而sample-to-sample基本模型则需要复杂的对应过程进行训练。此外，这两种方法都不能满足实际面部验证应用中的需求，需要一个统一的阈值来分辨正面和负面的面部对。在本篇文章中，我们提出了统一阈值结合sample-to-sample基本损失（USS损失），其中包含一个明确的统一阈值，用于分辨正面和负面的面部对。我们也从USS损失中 derivated sample-to-sample基本软max损失和BCE损失，并讨论它们之间的关系。我们对多个benchmark数据集，包括MFR、IJB-C、LFW、CFP-FP、AgeDB和MegaFace进行了广泛的评估，结果显示了我们的USS损失非常高效，并且可以与sample-to-class基本损失一起运作。我们的模型UniTSFace在训练时使用了USS损失和sample-to-class软max损失，并且表现出色，超越了现有的方法，如CosFace、ArcFace、VPL、AnchorFace和UNPG。我们的代码可以通过我们的网站下载。”
</details></li>
</ul>
<hr>
<h2 id="MAAIP-Multi-Agent-Adversarial-Interaction-Priors-for-imitation-from-fighting-demonstrations-for-physics-based-characters"><a href="#MAAIP-Multi-Agent-Adversarial-Interaction-Priors-for-imitation-from-fighting-demonstrations-for-physics-based-characters" class="headerlink" title="MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters"></a>MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02502">http://arxiv.org/abs/2311.02502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Younes, Ewa Kijak, Richard Kulpa, Simon Malinowski, Franck Multon</li>
<li>for: 这篇论文旨在提出一种基于多智能生成对抗学习的多个物理角色动作模拟方法，以满足互动应用和电影电视行业中的自动次要人物动作生成需求。</li>
<li>methods: 该方法基于多智能生成对抗学习，使用印杂学习技术来模拟多个物理角色的交互和动作。</li>
<li>results: 该方法在两种不同的拳击和全身武术风格下进行了测试，并成功地模拟了不同风格的交互和动作。<details>
<summary>Abstract</summary>
Simulating realistic interaction and motions for physics-based characters is of great interest for interactive applications, and automatic secondary character animation in the movie and video game industries. Recent works in reinforcement learning have proposed impressive results for single character simulation, especially the ones that use imitation learning based techniques. However, imitating multiple characters interactions and motions requires to also model their interactions. In this paper, we propose a novel Multi-Agent Generative Adversarial Imitation Learning based approach that generalizes the idea of motion imitation for one character to deal with both the interaction and the motions of the multiple physics-based characters. Two unstructured datasets are given as inputs: 1) a single-actor dataset containing motions of a single actor performing a set of motions linked to a specific application, and 2) an interaction dataset containing a few examples of interactions between multiple actors. Based on these datasets, our system trains control policies allowing each character to imitate the interactive skills associated with each actor, while preserving the intrinsic style. This approach has been tested on two different fighting styles, boxing and full-body martial art, to demonstrate the ability of the method to imitate different styles.
</details>
<details>
<summary>摘要</summary>
仿真人物的交互和动作是现代应用中很受欢迎的话题，特别是在电影和电子游戏行业中。最近的学习策略中，强调实现单个人物的仿真动作，尤其是使用仿制学习技术。然而，模拟多个人物之间的交互和动作需要同时模型他们之间的互动。在这篇论文中，我们提出了一种新的多智能体生成对抗学习仿真学习方法，扩展了单个人物的动作仿真到多个物理基于的人物之间的交互和动作。我们使用两个无结构数据集作为输入：1）一个单个演员数据集，包含一个演员执行一系列动作和应用相关的动作链接，和2）一个互动数据集，包含一些多个演员之间的互动示例。基于这两个数据集，我们的系统通过控制策略让每个人物学习与每个演员相互交互的技能，保持内在的风格。我们在拳击和全身武术两种不同的战斗风格中测试了这种方法，以示方法的多样性。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Post-Wildfire-Vegetation-Recovery-in-California-using-a-Convolutional-Long-Short-Term-Memory-Tensor-Regression-Network"><a href="#Forecasting-Post-Wildfire-Vegetation-Recovery-in-California-using-a-Convolutional-Long-Short-Term-Memory-Tensor-Regression-Network" class="headerlink" title="Forecasting Post-Wildfire Vegetation Recovery in California using a Convolutional Long Short-Term Memory Tensor Regression Network"></a>Forecasting Post-Wildfire Vegetation Recovery in California using a Convolutional Long Short-Term Memory Tensor Regression Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02492">http://arxiv.org/abs/2311.02492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahe Liu, Xiaodi Wang</li>
<li>for: 这个研究旨在发展成功的生态系统恢复策略，帮助理解火灾后植被恢复的过程。</li>
<li>methods: 这个研究使用了一种新的方法，即将Convolutional Long Short-Term Memory Tensor Regression（ConvLSTMTR）网络应用于火灾后植被恢复的预测。</li>
<li>results: 研究结果表明，ConvLSTMTR网络可以准确预测火灾后植被恢复的速度，并且可以分类不同的恢复趋势。<details>
<summary>Abstract</summary>
The study of post-wildfire plant regrowth is essential for developing successful ecosystem recovery strategies. Prior research mainly examines key ecological and biogeographical factors influencing post-fire succession. This research proposes a novel approach for predicting and analyzing post-fire plant recovery. We develop a Convolutional Long Short-Term Memory Tensor Regression (ConvLSTMTR) network that predicts future Normalized Difference Vegetation Index (NDVI) based on short-term plant growth data after fire containment. The model is trained and tested on 104 major California wildfires occurring between 2013 and 2020, each with burn areas exceeding 3000 acres. The integration of ConvLSTM with tensor regression enables the calculation of an overall logistic growth rate k using predicted NDVI. Overall, our k-value predictions demonstrate impressive performance, with 50% of predictions exhibiting an absolute error of 0.12 or less, and 75% having an error of 0.24 or less. Finally, we employ Uniform Manifold Approximation and Projection (UMAP) and KNN clustering to identify recovery trends, offering insights into regions with varying rates of recovery. This study pioneers the combined use of tensor regression and ConvLSTM, and introduces the application of UMAP for clustering similar wildfires. This advances predictive ecological modeling and could inform future post-fire vegetation management strategies.
</details>
<details>
<summary>摘要</summary>
研究火灾后植物回复的学术研究非常重要，以发展成功的生态系统回复策略。先前的研究主要探讨火灾后的生态和生物地理因素的影响。这个研究提出了一种新的方法来预测和分析火灾后植物的回复。我们开发了一个卷积长短期记忆点 regression（ConvLSTMTR）网络，可以预测未来 Normalized Difference Vegetation Index（NDVI）基于火灾后植物增长数据。这个模型在104次加利福尼亚州大火灾中训练和测试，每次火灾面积超过3000英亩。通过卷积和tensor regression的结合，我们可以计算整体的几何增长率k。总的来说，我们的k值预测表现出色，50%的预测值几何准确性在0.12或更低，75%的预测值几何准确性在0.24或更低。最后，我们使用Uniform Manifold Approximation and Projection（UMAP）和KNN推敲来识别回复趋势，提供了不同回复速率的区域差异的见解。这项研究创新了tensor regression和ConvLSTM的结合，并首次应用UMAP来推敲相似的野火。这些进展可能对未来火灾后植物管理策略提供帮助。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-of-Deep-Learning-for-Spatiotemporal-Data-Challenges-and-Opportunities"><a href="#Uncertainty-Quantification-of-Deep-Learning-for-Spatiotemporal-Data-Challenges-and-Opportunities" class="headerlink" title="Uncertainty Quantification of Deep Learning for Spatiotemporal Data: Challenges and Opportunities"></a>Uncertainty Quantification of Deep Learning for Spatiotemporal Data: Challenges and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02485">http://arxiv.org/abs/2311.02485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenchong He, Zhe Jiang</li>
<li>for: 随着GPS、Remote Sensing和计算模拟技术的发展，大量的地ospatial和时间特征数据正在不断增加，这些数据资产提供了改变社会的Unique机遇。但是，深度学习模型在高度决策应用中可能会出现意外和错误的预测，导致严重的后果。不确定性评估（UQ）可以 estimating a deep learning model’s confidence.</li>
<li>methods: 本文提供了深度学习模型的不确定性评估简介，包括其特殊挑战和现有方法。我们尤其关注不确定性来源的重要性。</li>
<li>results: 本文 highlights several future research directions for spatiotemporal data, including the importance of uncertainty sources.Here is the same information in English:</li>
<li>for: With the advancement of GPS, remote sensing, and computational simulations, large amounts of geospatial and spatiotemporal data are being collected at an increasing speed, providing unique opportunities to transform society. However, deep learning models sometimes make unexpected and incorrect predictions with unwarranted confidence, causing severe consequences in high-stake decision-making applications. Uncertainty quantification (UQ) aims to estimate a deep learning model’s confidence.</li>
<li>methods: This paper provides a brief overview of UQ of deep learning for spatiotemporal data, including its unique challenges and existing methods. We particularly focus on the importance of uncertainty sources.</li>
<li>results: The paper highlights several future research directions for spatiotemporal data, including the importance of uncertainty sources.<details>
<summary>Abstract</summary>
With the advancement of GPS, remote sensing, and computational simulations, large amounts of geospatial and spatiotemporal data are being collected at an increasing speed. Such emerging spatiotemporal big data assets, together with the recent progress of deep learning technologies, provide unique opportunities to transform society. However, it is widely recognized that deep learning sometimes makes unexpected and incorrect predictions with unwarranted confidence, causing severe consequences in high-stake decision-making applications (e.g., disaster management, medical diagnosis, autonomous driving). Uncertainty quantification (UQ) aims to estimate a deep learning model's confidence. This paper provides a brief overview of UQ of deep learning for spatiotemporal data, including its unique challenges and existing methods. We particularly focus on the importance of uncertainty sources. We identify several future research directions for spatiotemporal data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:随着GPS、远程感知和计算 simulate的发展，大量的地ospatial和时空数据在不断增加。这些emerging spatiotemporal big data assets，加上深度学习技术的最新进步，为社会转型提供了唯一的机会。然而，广泛认可的深度学习 sometimes makes unexpected and incorrect predictions with unwarranted confidence，对高度决策应用（如灾害管理、医疗诊断、自动驾驶）可致严重的后果。 uncertainty quantification (UQ) aimsto estimate a deep learning model's confidence。这篇文章提供了深度学习 for spatiotemporal data的 UQ 简介，包括其特殊挑战和现有方法。我们尤其关注了 uncertainty sources 的重要性。我们标识了多个未来研究方向 for spatiotemporal data。
</details></li>
</ul>
<hr>
<h2 id="Generalized-zero-shot-audio-to-intent-classification"><a href="#Generalized-zero-shot-audio-to-intent-classification" class="headerlink" title="Generalized zero-shot audio-to-intent classification"></a>Generalized zero-shot audio-to-intent classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02482">http://arxiv.org/abs/2311.02482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Veera Raghavendra Elluru, Devang Kulshreshtha, Rohit Paturi, Sravan Bodapati, Srikanth Ronanki</li>
<li>for: 这个研究旨在提高使用音频数据的语音识别系统的未见意能力。</li>
<li>methods: 该研究提议一种通用零shot音频到意类型分类框架，只需要几个示例文本句子每个意图。这个框架首先通过使用一个自动生成的预训练模型来训练一个监督音频到意类型分类器。然后，我们使用神经网络音频生成器生成音频嵌入 для示例文本词汇，并使用普通的cosinus相似性来进行通用零shot分类。此外，我们还提出了一种多Modal训练策略，它将字幕信息 integrate到音频表示中以提高零shot性能。</li>
<li>results: 我们的多Modal训练策略提高了SLURP dataset上未见意分类的准确率，比Audio只训练策略高2.75%和18.2%。<details>
<summary>Abstract</summary>
Spoken language understanding systems using audio-only data are gaining popularity, yet their ability to handle unseen intents remains limited. In this study, we propose a generalized zero-shot audio-to-intent classification framework with only a few sample text sentences per intent. To achieve this, we first train a supervised audio-to-intent classifier by making use of a self-supervised pre-trained model. We then leverage a neural audio synthesizer to create audio embeddings for sample text utterances and perform generalized zero-shot classification on unseen intents using cosine similarity. We also propose a multimodal training strategy that incorporates lexical information into the audio representation to improve zero-shot performance. Our multimodal training approach improves the accuracy of zero-shot intent classification on unseen intents of SLURP by 2.75% and 18.2% for the SLURP and internal goal-oriented dialog datasets, respectively, compared to audio-only training.
</details>
<details>
<summary>摘要</summary>
spoken language understanding systems using audio-only data 获得 popularity，但它们对未经见意旨的处理能力仍然有限。在这种研究中，我们提议一种通用的零shot audio-to-intent分类框架，只需几个sample text sentences per intent。为实现这一点，我们首先使用一个自我超vised audio-to-intent分类器进行训练，然后利用一个神经网络音频生成器生成音频嵌入 дляsample text词汇，并使用cosine similarity进行通用零shot分类。我们还提出了一种多Modal训练策略，该策略将语言信息 incorporated into the audio representation，以提高零shot性能。我们的多Modal训练方法在SLURP上的零shot意旨分类精度提高2.75%和18.2%，相比 audio-only 训练。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Equation-Learner-Networks-for-Precision-Preserving-Extrapolation-of-Robotic-Skills"><a href="#Constrained-Equation-Learner-Networks-for-Precision-Preserving-Extrapolation-of-Robotic-Skills" class="headerlink" title="Constrained Equation Learner Networks for Precision-Preserving Extrapolation of Robotic Skills"></a>Constrained Equation Learner Networks for Precision-Preserving Extrapolation of Robotic Skills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02475">http://arxiv.org/abs/2311.02475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector Perez-Villeda, Justus Piater, Matteo Saveriano</li>
<li>for: 该论文目的是解决在程序示示中学习 novel 技能后，如何适应不同的环境和条件，而不需要收集新的训练数据。</li>
<li>methods: 该论文提出了一种新的监督学习框架，即受限Equation Learner Networks（CEN），用于解决程序示示中的轨迹适应问题。CEN 使用 Equation Learner Networks 来学习一组 analytical 表达，并用这些表达作为基函数。</li>
<li>results: 实验结果表明，CEN 可以比现有方法更好地适应 robotic 技能，并且可以保持适应的精度。在一些 robotic 任务中，CEN 在不同的环境下实现了更高的总体性和适应性。<details>
<summary>Abstract</summary>
In Programming by Demonstration, the robot learns novel skills from human demonstrations. After learning, the robot should be able not only to reproduce the skill, but also to generalize it to shifted domains without collecting new training data. Adaptation to similar domains has been investigated in the literature; however, an open problem is how to adapt learned skills to different conditions that are outside of the data distribution, and, more important, how to preserve the precision of the desired adaptations. This paper presents a novel supervised learning framework called Constrained Equation Learner Networks that addresses the trajectory adaptation problem in Programming by Demonstrations from a constrained regression perspective. While conventional approaches for constrained regression use one kind of basis function, e.g., Gaussian, we exploit Equation Learner Networks to learn a set of analytical expressions and use them as basis functions. These basis functions are learned from demonstration with the objective to minimize deviations from the training data while imposing constraints that represent the desired adaptations, like new initial or final points or maintaining the trajectory within given bounds. Our approach addresses three main difficulties in adapting robotic trajectories: 1) minimizing the distortion of the trajectory for new adaptations; 2) preserving the precision of the adaptations; and 3) dealing with the lack of intuition about the structure of basis functions. We validate our approach both in simulation and in real experiments in a set of robotic tasks that require adaptation due to changes in the environment, and we compare obtained results with two existing approaches. Performed experiments show that Constrained Equation Learner Networks outperform state of the art approaches by increasing generalization and adaptability of robotic skills.
</details>
<details>
<summary>摘要</summary>
在程序编程中，机器人从人类示例学习新技能。学习后，机器人应该不仅能复制技能，还能泛化到偏移域无需新的训练数据。针对相似域的适应已经在文献中 investigate;然而，一个开放的问题是如何适应不同的条件，这些条件外部训练数据分布。此外，更重要的是如何保持适应的精度。这篇论文提出了一种新的监督学习框架，即受限 regression 框架，用于程序编程中的示例适应问题。与传统的受限 regression 方法一样，我们使用一种基于Equation Learner Networks的学习算法，以学习一组分析表达式，并使其作为基函数使用。这些基函数从示例学习中得出，并与具有限制的目标函数进行拟合，以最小化示例数据与适应结果之间的差异，同时保持适应的精度。我们的方法解决了以下三个主要难题：1）减少新适应中的路径扭曲;2）保持适应的精度;3）处理基函数的感知问题。我们在实验中 validate 了我们的方法，并与两种现有方法进行比较。实验结果表明，受限Equation Learner Networks 可以比现有方法提高机器人技能的泛化和适应能力。
</details></li>
</ul>
<hr>
<h2 id="Multi-State-Brain-Network-Discovery"><a href="#Multi-State-Brain-Network-Discovery" class="headerlink" title="Multi-State Brain Network Discovery"></a>Multi-State Brain Network Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02466">http://arxiv.org/abs/2311.02466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Hang Yin, Yao Su, Xinyue Liu, Thomas Hartvigsen, Yanhua Li, Xiangnan Kong</li>
<li>for: This paper aims to discover brain networks from spatio-temporal signals obtained by neuroimaging data, such as fMRI scans of human brains, and to model multi-state brain networks that capture the intricate patterns of brain activities.</li>
<li>methods: The proposed method, called MNGL (Multi-state Network Graphical Lasso), combines CGL (coherent graphical lasso) with GMM (Gaussian Mixture Model) to successfully model multi-state brain networks.</li>
<li>results: Compared to recent state-of-the-art alternatives, MNGL outperforms by discovering more explanatory and realistic results using both synthetic and real world ADHD 200 fMRI datasets.Here’s the Chinese translation of the three information:</li>
<li>for: 这篇论文旨在基于神经成像数据，如fMRI扫描人脑的信号，发现脑网络，并模型多状态脑网络，以捕捉脑活动的复杂征性。</li>
<li>methods: 提议的方法是MNGL（多状态网络图解吸积模型），它将CGL（协调图解吸积模型）与GMM（加性分布模型）结合，成功地模型多状态脑网络。</li>
<li>results: 与最新的状态艺术相比，MNGL在使用 sintetic和实际ADHD 200 fMRI数据上表现出色，可以更好地捕捉脑活动的特征。<details>
<summary>Abstract</summary>
Brain network discovery aims to find nodes and edges from the spatio-temporal signals obtained by neuroimaging data, such as fMRI scans of human brains. Existing methods tend to derive representative or average brain networks, assuming observed signals are generated by only a single brain activity state. However, the human brain usually involves multiple activity states, which jointly determine the brain activities. The brain regions and their connectivity usually exhibit intricate patterns that are difficult to capture with only a single-state network. Recent studies find that brain parcellation and connectivity change according to the brain activity state. We refer to such brain networks as multi-state, and this mixture can help us understand human behavior. Thus, compared to a single-state network, a multi-state network can prevent us from losing crucial information of cognitive brain network. To achieve this, we propose a new model called MNGL (Multi-state Network Graphical Lasso), which successfully models multi-state brain networks by combining CGL (coherent graphical lasso) with GMM (Gaussian Mixture Model). Using both synthetic and real world ADHD 200 fMRI datasets, we demonstrate that MNGL outperforms recent state-of-the-art alternatives by discovering more explanatory and realistic results.
</details>
<details>
<summary>摘要</summary>
��� Git network discovery aims to find nodes and edges from the spatio-temporal signals obtained by neuroimaging data, such as fMRI scans of human brains. Existing methods tend to derive representative or average brain networks, assuming observed signals are generated by only a single brain activity state. However, the human brain usually involves multiple activity states, which jointly determine the brain activities. The brain regions and their connectivity usually exhibit intricate patterns that are difficult to capture with only a single-state network. Recent studies find that brain parcellation and connectivity change according to the brain activity state. We refer to such brain networks as multi-state, and this mixture can help us understand human behavior. Thus, compared to a single-state network, a multi-state network can prevent us from losing crucial information of cognitive brain network. To achieve this, we propose a new model called MNGL (Multi-state Network Graphical Lasso), which successfully models multi-state brain networks by combining CGL (coherent graphical lasso) with GMM (Gaussian Mixture Model). Using both synthetic and real world ADHD 200 fMRI datasets, we demonstrate that MNGL outperforms recent state-of-the-art alternatives by discovering more explanatory and realistic results.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Levels-of-AGI-Operationalizing-Progress-on-the-Path-to-AGI"><a href="#Levels-of-AGI-Operationalizing-Progress-on-the-Path-to-AGI" class="headerlink" title="Levels of AGI: Operationalizing Progress on the Path to AGI"></a>Levels of AGI: Operationalizing Progress on the Path to AGI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02462">http://arxiv.org/abs/2311.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, Shane Legg</li>
<li>for: This paper proposes a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors.</li>
<li>methods: The paper analyzes existing definitions of AGI and distills six principles that a useful ontology for AGI should satisfy.</li>
<li>results: The paper proposes “Levels of AGI” based on depth (performance) and breadth (generality) of capabilities, and discusses the challenges of quantifying the behavior and capabilities of AGI models against these levels.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文提出了一个用于分类人工通用智能（AGI）模型和其前体的框架。</li>
<li>methods: 这篇论文通过分析现有的AGI定义，提出了 six 个用于AGIontology的原则。</li>
<li>results: 这篇论文提出了“Levels of AGI”，基于深度（性能）和面积（通用）的能力，并讨论了量化AGI模型的行为和能力的挑战。<details>
<summary>Abstract</summary>
We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It is our hope that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, we propose 'Levels of AGI' based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we discuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.
</details>
<details>
<summary>摘要</summary>
我们提出了一套AGI模型和其前体的能力和行为分类框架。这套框架 introduce AGI性能、通用性和自主性的多个级别。我们希望这套框架可以与自动驾驶技术类似，提供一种共同语言，比较模型、评估风险和衡量AGI的进步。为开发这套框架，我们分析了现有的AGI定义，并总结出六个AGIontology应满足的原则。这些原则包括专注于能力而不是机制，分开评估通用性和性能，以及定义AGI的发展阶段，而不是专注于终点。基于这些原则，我们提出了“AGI级别”，按照性能和通用性的深度和面积来评估AGI模型的能力。我们还讨论了未来测试AGI模型的标准 benchmark，并讨论了这些级别与部署考虑因素，如自主和风险的关系。最后，我们强调选择合适的人机交互模式对于负责任和安全地部署高能力AI系统非常重要。
</details></li>
</ul>
<hr>
<h2 id="Can-ChatGPT-support-software-verification"><a href="#Can-ChatGPT-support-software-verification" class="headerlink" title="Can ChatGPT support software verification?"></a>Can ChatGPT support software verification?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02433">http://arxiv.org/abs/2311.02433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Janßen, Cedric Richter, Heike Wehrheim</li>
<li>for: 这个论文的目的是研究使用 chatGPT 支持正式软件验证。</li>
<li>methods: 论文使用 chatGPT 生成 loop invariants，并通过 Frama-C 和 CPAchecker 验证其有效性和实用性。</li>
<li>results: 论文的结果表明，chatGPT 可以生成有效和实用的 loop invariants，帮助 Frama-C 验证 tasks 之前无法解决。<details>
<summary>Abstract</summary>
Large language models have become increasingly effective in software engineering tasks such as code generation, debugging and repair. Language models like ChatGPT can not only generate code, but also explain its inner workings and in particular its correctness. This raises the question whether we can utilize ChatGPT to support formal software verification.   In this paper, we take some first steps towards answering this question. More specifically, we investigate whether ChatGPT can generate loop invariants. Loop invariant generation is a core task in software verification, and the generation of valid and useful invariants would likely help formal verifiers. To provide some first evidence on this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants. We check validity and usefulness of the generated invariants by passing them to two verifiers, Frama-C and CPAchecker. Our evaluation shows that ChatGPT is able to produce valid and useful invariants allowing Frama-C to verify tasks that it could not solve before. Based on our initial insights, we propose ways of combining ChatGPT (or large language models in general) and software verifiers, and discuss current limitations and open issues.
</details>
<details>
<summary>摘要</summary>
To provide some initial evidence for this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants. We then check the validity and usefulness of the generated invariants by passing them to two verifiers, Frama-C and CPAchecker. Our evaluation shows that ChatGPT is able to produce valid and useful invariants that allow Frama-C to verify tasks that it could not solve before. Based on our initial insights, we propose ways of combining ChatGPT (or large language models in general) and software verifiers, and discuss current limitations and open issues.
</details></li>
</ul>
<hr>
<h2 id="CDR-Adapter-Learning-Adapters-to-Dig-Out-More-Transferring-Ability-for-Cross-Domain-Recommendation-Models"><a href="#CDR-Adapter-Learning-Adapters-to-Dig-Out-More-Transferring-Ability-for-Cross-Domain-Recommendation-Models" class="headerlink" title="CDR-Adapter: Learning Adapters to Dig Out More Transferring Ability for Cross-Domain Recommendation Models"></a>CDR-Adapter: Learning Adapters to Dig Out More Transferring Ability for Cross-Domain Recommendation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02398">http://arxiv.org/abs/2311.02398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanyu Chen, Yao Yao, Wai Kin Victor Chan, Li Xiao, Kai Zhang, Liang Zhang, Yun Ye</li>
<li>for: 解决推荐系统中的数据稀缺和冷启动问题，提高推荐性能。</li>
<li>methods: 提出了一种可扩展和高效的解决方案，即CDR-Adapter，它通过分离原来的推荐模型和映射函数，实现知识传递而不需要重新引入网络结构，从而避免了计算成本高和知识淡化问题。</li>
<li>results: 在标准测试集上进行了广泛的实验，证明了我们的方法的有效性，比如 state-of-the-art CDR 方法。<details>
<summary>Abstract</summary>
Data sparsity and cold-start problems are persistent challenges in recommendation systems. Cross-domain recommendation (CDR) is a promising solution that utilizes knowledge from the source domain to improve the recommendation performance in the target domain. Previous CDR approaches have mainly followed the Embedding and Mapping (EMCDR) framework, which involves learning a mapping function to facilitate knowledge transfer. However, these approaches necessitate re-engineering and re-training the network structure to incorporate transferrable knowledge, which can be computationally expensive and may result in catastrophic forgetting of the original knowledge. In this paper, we present a scalable and efficient paradigm to address data sparsity and cold-start issues in CDR, named CDR-Adapter, by decoupling the original recommendation model from the mapping function, without requiring re-engineering the network structure. Specifically, CDR-Adapter is a novel plug-and-play module that employs adapter modules to align feature representations, allowing for flexible knowledge transfer across different domains and efficient fine-tuning with minimal training costs. We conducted extensive experiments on the benchmark dataset, which demonstrated the effectiveness of our approach over several state-of-the-art CDR approaches.
</details>
<details>
<summary>摘要</summary>
数据稀缺和冷启问题是推荐系统中的惯常挑战。跨Domain推荐（CDR）是一种有前途的解决方案，它利用源Domain中的知识提高目标Domain中的推荐性能。先前的CDR方法主要遵循Embedding and Mapping（EMCDR）框架，这些方法通常需要重新工程和重新训练网络结构，以便传递可移植的知识。然而，这些方法可能需要大量的计算资源和可能会导致原始知识的恐慌遗忘。在这篇论文中，我们提出了一种可扩展和高效的方法，以解决推荐系统中的数据稀缺和冷启问题，名为CDR-Adapter。CDR-Adapter是一种新的插件模块，它使用适配器模块来对特征表示进行减Alignment，以便在不同的Domain之间进行可靠的知识传递和高效的细化训练，无需重新工程网络结构。我们对标准数据集进行了广泛的实验，结果表明我们的方法比先前的CDR方法更有效。
</details></li>
</ul>
<hr>
<h2 id="Continual-Learning-of-Unsupervised-Monocular-Depth-from-Videos"><a href="#Continual-Learning-of-Unsupervised-Monocular-Depth-from-Videos" class="headerlink" title="Continual Learning of Unsupervised Monocular Depth from Videos"></a>Continual Learning of Unsupervised Monocular Depth from Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02393">http://arxiv.org/abs/2311.02393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NeurAI-Lab/CUDE-MonoDepthCL">https://github.com/NeurAI-Lab/CUDE-MonoDepthCL</a></li>
<li>paper_authors: Hemang Chawla, Arnav Varma, Elahe Arani, Bahram Zonooz</li>
<li>for: 提高无监督单目深度估计的能力，应用于 робо扮和自动驾驶等领域。</li>
<li>methods: 提出了一个框架，以及一种基于备忘的双存储方法（MonoDepthCL），利用空间时间协调的一致性来实现不间断学习。</li>
<li>results: 模型在不同的频率和规模上进行了训练和测试，显示了在不间断学习中的性能稳定性和增长。<details>
<summary>Abstract</summary>
Spatial scene understanding, including monocular depth estimation, is an important problem in various applications, such as robotics and autonomous driving. While improvements in unsupervised monocular depth estimation have potentially allowed models to be trained on diverse crowdsourced videos, this remains underexplored as most methods utilize the standard training protocol, wherein the models are trained from scratch on all data after new data is collected. Instead, continual training of models on sequentially collected data would significantly reduce computational and memory costs. Nevertheless, naive continual training leads to catastrophic forgetting, where the model performance deteriorates on older domains as it learns on newer domains, highlighting the trade-off between model stability and plasticity. While several techniques have been proposed to address this issue in image classification, the high-dimensional and spatiotemporally correlated outputs of depth estimation make it a distinct challenge. To the best of our knowledge, no framework or method currently exists focusing on the problem of continual learning in depth estimation. Thus, we introduce a framework that captures the challenges of continual unsupervised depth estimation (CUDE), and define the necessary metrics to evaluate model performance. We propose a rehearsal-based dual-memory method, MonoDepthCL, which utilizes spatiotemporal consistency for continual learning in depth estimation, even when the camera intrinsics are unknown.
</details>
<details>
<summary>摘要</summary>
空间场景理解，包括单目深度估计，在各种应用中具有重要性，如 роботиcs和自动驾驶。尽管无监督单目深度估计的改进允许模型在不同的人工训练视频上进行训练，但这还是未经探索的，因为大多数方法使用标准训练协议，即从 scratch 上所有数据进行训练，新数据收集后。相反，继续训练模型在顺序收集的数据上会显著减少计算和内存成本。然而，简单的继续训练会导致忘记现象，模型对老化领域的性能下降，显示出模型稳定性和抑制的负面关系。虽然一些技术已经被提出来解决这个问题在图像分类方面，但高维度和空间时间相关的输出使得深度估计是一个特殊的挑战。根据我们所知，现在没有任何框架或方法专门关注深度估计的连续学习问题。因此，我们提出了一个框架，即不间断无监督深度估计框架（CUDE），并定义了评估模型性能的必要指标。我们提议一种备忘队列方法，即单目深度CL，它通过空间时间一致性来实现连续学习深度估计，即使摄像头内参不详。
</details></li>
</ul>
<hr>
<h2 id="Cross-Level-Distillation-and-Feature-Denoising-for-Cross-Domain-Few-Shot-Classification"><a href="#Cross-Level-Distillation-and-Feature-Denoising-for-Cross-Domain-Few-Shot-Classification" class="headerlink" title="Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification"></a>Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02392">http://arxiv.org/abs/2311.02392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jarucezh/cldfd">https://github.com/jarucezh/cldfd</a></li>
<li>paper_authors: Hao Zheng, Runqi Wang, Jianzhuang Liu, Asako Kanezaki</li>
<li>for: 本研究目标是解决跨频道几拟分类问题，即在培育模型时使用不同频道的数据进行学习。</li>
<li>methods: 我们采用了跨层知识填充法，以强化模型在目标数据集中提取更多特征信息。此外，我们还提出了一种特征净化操作，以降低特征重复和避免过拟合。</li>
<li>results: 我们的方法可以在BSCD-FSL benchmark中超越前任的Dynamic-Distillation方法，在1-shot和5-shot分类任务上的平均提高5.44%和1.37%。代码将在GitHub上提供。<details>
<summary>Abstract</summary>
The conventional few-shot classification aims at learning a model on a large labeled base dataset and rapidly adapting to a target dataset that is from the same distribution as the base dataset. However, in practice, the base and the target datasets of few-shot classification are usually from different domains, which is the problem of cross-domain few-shot classification. We tackle this problem by making a small proportion of unlabeled images in the target domain accessible in the training stage. In this setup, even though the base data are sufficient and labeled, the large domain shift still makes transferring the knowledge from the base dataset difficult. We meticulously design a cross-level knowledge distillation method, which can strengthen the ability of the model to extract more discriminative features in the target dataset by guiding the network's shallow layers to learn higher-level information. Furthermore, in order to alleviate the overfitting in the evaluation stage, we propose a feature denoising operation which can reduce the feature redundancy and mitigate overfitting. Our approach can surpass the previous state-of-the-art method, Dynamic-Distillation, by 5.44% on 1-shot and 1.37% on 5-shot classification tasks on average in the BSCD-FSL benchmark. The implementation code will be available at https://github.com/jarucezh/cldfd.
</details>
<details>
<summary>摘要</summary>
传统的几shot分类目标是学习一个模型，然后快速适应目标数据集，但在实际应用中，基数据集和目标数据集通常来自不同的领域，这是跨领域几shot分类的问题。我们解决这个问题 by 在训练阶段使得小量目标数据集中的无标照图像变得可访问。尽管基数据集充足并彩色标注，但大领域变化仍然使得从基数据集传输知识困难。我们仔细设计了跨层知识填充方法，该方法可以使模型在目标数据集中提取更多特征分类器。此外，为了避免评估阶段的过拟合，我们提议一种特征净化操作，可以减少特征重复和抑制过拟合。我们的方法可以在BSCD-FSL标准准则下平均超过前一个状态的方法，Dynamic-Distillation，在1shot和5shot分类任务上的平均性能提高5.44%和1.37%。代码实现将提供在https://github.com/jarucezh/cldfd中。
</details></li>
</ul>
<hr>
<h2 id="AI-based-Self-healing-Solutions-Applied-to-Cellular-Networks-An-Overview"><a href="#AI-based-Self-healing-Solutions-Applied-to-Cellular-Networks-An-Overview" class="headerlink" title="AI-based Self-healing Solutions Applied to Cellular Networks: An Overview"></a>AI-based Self-healing Solutions Applied to Cellular Networks: An Overview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02390">http://arxiv.org/abs/2311.02390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaleh Farmani, Amirreza Khalil Zadeh</li>
<li>For: The paper is written for researchers and practitioners in the field of cellular networks, specifically those interested in self-healing and machine learning techniques for network management.* Methods: The paper provides an overview of machine learning methods, including classical and deep learning variants, that are used to implement self-healing for cell outages in cellular networks.* Results: The paper reviews the state-of-the-art in literature for cell outages, with a particular emphasis on machine learning-based approaches.Here are the three key points in Simplified Chinese text:* For: 本文是为Cellular网络相关研究人员和实践者所写，尤其是关注自适应和机器学习技术的网络管理方面。* Methods: 本文提供了机器学习方法的概述，包括经典和深度学习变体，用于实现Cellular网络中的自适应。* Results: 本文对Cellular网络中的维护和自适应方面进行了文献综述，尤其是关注机器学习基于的方法。<details>
<summary>Abstract</summary>
In this article, we provide an overview of machine learning (ML) methods, both classical and deep variants, that are used to implement self-healing for cell outages in cellular networks. Self-healing is a promising approach to network management, which aims to detect and compensate for cell outages in an autonomous way. This technology aims to decrease the expenses associated with the installation and maintenance of existing 4G and 5G, i.e. emerging 6G networks by simplifying operational tasks through its ability to heal itself. We provide an overview of the basic concepts and taxonomy for SON, self-healing, and ML techniques, in network management. Moreover, we review the state-of-the-art in literature for cell outages, with a particular emphasis on ML-based approaches.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们提供了机器学习（ML）方法的概述，包括古典和深度变种，用于实现cell网络中的自适应维护。自适应维护是一种有前途的网络管理方法，旨在通过自动化方式探测和补做cell网络中的维护问题。这技术可以降低现有4G和5G等新生6G网络的安装和维护成本，通过简化操作任务来简化操作任务。我们还提供了网络管理基本概念和分类，以及相关文献综述。特别是，我们对文献中关于cell网络维护的研究进行了深入审查，强调了基于ML的方法。
</details></li>
</ul>
<hr>
<h2 id="Ultra-Long-Sequence-Distributed-Transformer"><a href="#Ultra-Long-Sequence-Distributed-Transformer" class="headerlink" title="Ultra-Long Sequence Distributed Transformer"></a>Ultra-Long Sequence Distributed Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02382">http://arxiv.org/abs/2311.02382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Wang, Isaac Lyngaas, Aristeidis Tsaris, Peng Chen, Sajal Dash, Mayanka Chandra Shekar, Tao Luo, Hong-Jun Yoon, Mohamed Wahib, John Gouley</li>
<li>for: 该论文旨在提出一种高效的分布式训练方法，以便使用长序列训练变换器模型。</li>
<li>methods: 该方法分割长序列成多个段，并将每个段分配给不同的GPU进行计算。然后，它使用了一种复合通信和双均值平均技术来避免部分自注意计算的汇聚和通信开销。</li>
<li>results: 与现有的序列并行技术相比，该方法在144个Nvidia V100 GPU上实现了5.6倍的速度提升和10.2倍的内存可用性提升。此外，该算法可以扩展到极长序列长度50,112，在3,456个GPU上实现161%的超线性并行效率和32PFLOP的吞吐量。<details>
<summary>Abstract</summary>
Transformer models trained on long sequences often achieve higher accuracy than short sequences. Unfortunately, conventional transformers struggle with long sequence training due to the overwhelming computation and memory requirements. Existing methods for long sequence training offer limited speedup and memory reduction, and may compromise accuracy. This paper presents a novel and efficient distributed training method, the Long Short-Sequence Transformer (LSS Transformer), for training transformer with long sequences. It distributes a long sequence into segments among GPUs, with each GPU computing a partial self-attention for its segment. Then, it uses a fused communication and a novel double gradient averaging technique to avoid the need to aggregate partial self-attention and minimize communication overhead. We evaluated the performance between LSS Transformer and the state-of-the-art Nvidia sequence parallelism on a Wikipedia enwik8 dataset. Results show that our proposed method lead to 5.6x faster and 10.2x more memory-efficient implementation compared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs. Moreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456 GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32 petaflops.
</details>
<details>
<summary>摘要</summary>
很多变换器模型在长序列上训练时会达到更高的准确率。然而，普通的变换器在长序列训练时会遇到过于复杂的计算和内存需求的问题。现有的长序列训练方法可能会减少速度和内存占用，并可能会降低准确率。这篇论文提出了一种新的和高效的分布式训练方法——长短序列变换器（LSS Transformer），用于训练变换器模型。它将长序列分成多个 GPU 上的段，每个 GPU 计算自己的段部分自注意。然后，它使用了一种混合通信和一种新的双 Gradient 平均技术，以避免需要合并部分自注意和减少通信开销。我们对 LSS Transformer 和状态对照短序列并行方法进行了对比，使用 Wikipedia enwik8 数据集。结果显示，我们提出的方法比状态对照短序列并行方法在 144 Nvidia V100 GPU 上得到了5.6倍快速和10.2倍内存高效的实现。此外，我们的算法可以在极长序列长度为50,112的情况下，在3,456 GPU 上进行扩展，实现161%的超线性并行率和32 petaflops 的吞吐量。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Reinforcement-Learning-of-Robotic-Manipulations-via-Feedback-from-Large-Language-Models"><a href="#Accelerating-Reinforcement-Learning-of-Robotic-Manipulations-via-Feedback-from-Large-Language-Models" class="headerlink" title="Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models"></a>Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02379">http://arxiv.org/abs/2311.02379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Chu, Xufeng Zhao, Cornelius Weber, Mengdi Li, Stefan Wermter</li>
<li>for: 提高RLAgent的学习效率和成功率，通过人工智能语言模型提供有用的反馈。</li>
<li>methods: 利用大量语言数据预训练的大语言模型（LLM），提供RLAgent有用的反馈，帮助RLAgent更快速地学习和成功完成 робо控制任务。</li>
<li>results: 实验结果表明，使用Lafite-RL框架，RLAgent可以更快速地学习并成功完成RLBench任务，并且在学习效率和成功率上都有显著提高。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) plays an important role in the robotic manipulation domain since it allows self-learning from trial-and-error interactions with the environment. Still, sample efficiency and reward specification seriously limit its potential. One possible solution involves learning from expert guidance. However, obtaining a human expert is impractical due to the high cost of supervising an RL agent, and developing an automatic supervisor is a challenging endeavor. Large Language Models (LLMs) demonstrate remarkable abilities to provide human-like feedback on user inputs in natural language. Nevertheless, they are not designed to directly control low-level robotic motions, as their pretraining is based on vast internet data rather than specific robotics data. In this paper, we introduce the Lafite-RL (Language agent feedback interactive Reinforcement Learning) framework, which enables RL agents to learn robotic tasks efficiently by taking advantage of LLMs' timely feedback. Our experiments conducted on RLBench tasks illustrate that, with simple prompt design in natural language, the Lafite-RL agent exhibits improved learning capabilities when guided by an LLM. It outperforms the baseline in terms of both learning efficiency and success rate, underscoring the efficacy of the rewards provided by an LLM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MTS-DVGAN-Anomaly-Detection-in-Cyber-Physical-Systems-using-a-Dual-Variational-Generative-Adversarial-Network"><a href="#MTS-DVGAN-Anomaly-Detection-in-Cyber-Physical-Systems-using-a-Dual-Variational-Generative-Adversarial-Network" class="headerlink" title="MTS-DVGAN: Anomaly Detection in Cyber-Physical Systems using a Dual Variational Generative Adversarial Network"></a>MTS-DVGAN: Anomaly Detection in Cyber-Physical Systems using a Dual Variational Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02378">http://arxiv.org/abs/2311.02378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haili Sun, Yan Huang, Lansheng Han, Cai Fu, Hongle Liu, Xiang Long</li>
<li>for: 这篇论文旨在应用深度生成模型来探测 Cyber-physical systems (CPSs) 中的新型攻击，并无需靠扩展标签信息。</li>
<li>methods: 这篇论文提出了一个名为 MST-DVGAN 的新型Unsupervised dual variational generative adversarial model，用于侦测 multivariate time series data 中的异常情况。</li>
<li>results:  compared with state-of-the-art methods, the proposed MTS-DVGAN is more stable and can achieve consistent performance improvement in detecting anomalies in CPSs.<details>
<summary>Abstract</summary>
Deep generative models are promising in detecting novel cyber-physical attacks, mitigating the vulnerability of Cyber-physical systems (CPSs) without relying on labeled information. Nonetheless, these generative models face challenges in identifying attack behaviors that closely resemble normal data, or deviate from the normal data distribution but are in close proximity to the manifold of the normal cluster in latent space. To tackle this problem, this article proposes a novel unsupervised dual variational generative adversarial model named MST-DVGAN, to perform anomaly detection in multivariate time series data for CPS security. The central concept is to enhance the model's discriminative capability by widening the distinction between reconstructed abnormal samples and their normal counterparts. Specifically, we propose an augmented module by imposing contrastive constraints on the reconstruction process to obtain a more compact embedding. Then, by exploiting the distribution property and modeling the normal patterns of multivariate time series, a variational autoencoder is introduced to force the generative adversarial network (GAN) to generate diverse samples. Furthermore, two augmented loss functions are designed to extract essential characteristics in a self-supervised manner through mutual guidance between the augmented samples and original samples. Finally, a specific feature center loss is introduced for the generator network to enhance its stability. Empirical experiments are conducted on three public datasets, namely SWAT, WADI and NSL_KDD. Comparing with the state-of-the-art methods, the evaluation results show that the proposed MTS-DVGAN is more stable and can achieve consistent performance improvement.
</details>
<details>
<summary>摘要</summary>
深度生成模型可以有效探测 novel 的Cyber-physical attacks，减轻 Cyber-physical systems (CPSs) 的感受性，不需要靠据标注信息。然而，这些生成模型面临着难以识别攻击行为，与正常数据 Distribution 相似或者在正常数据分布的邻近区域内偏离。为解决这个问题，本文提出了一种新的无监督 dual variational generative adversarial model（MST-DVGAN），用于Cyber-physical systems (CPSs) 安全中的异常检测。中心思想是通过增强模型的歧义能力，使重构后的异常样本与正常样本更加明显不同。具体来说，我们提出了一个增强模块，通过对重构过程进行冲突约束，以获得更加紧凑的嵌入。然后，通过利用多变量时序数据的分布特性和模型正常模式，引入了一种变量自动编码器，以使生成对抗网络（GAN）生成更加多样化的样本。此外，我们还设计了两种增强loss函数，通过自然指导两者之间的增强样本和原始样本之间的互动，从而提取出更加重要的特征。最后，我们引入了一种特定的特征中心损失，以提高生成器网络的稳定性。我们在三个公共数据集上进行了实验，即SWAT、WADI和NSL_KDD。与现状的方法相比，我们的MST-DVGAN表现更加稳定，并且可以在各种场景下提供更高的性能改进。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Deep-Nonnegative-Matrix-Factorization-for-Community-Detection"><a href="#Contrastive-Deep-Nonnegative-Matrix-Factorization-for-Community-Detection" class="headerlink" title="Contrastive Deep Nonnegative Matrix Factorization for Community Detection"></a>Contrastive Deep Nonnegative Matrix Factorization for Community Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02357">http://arxiv.org/abs/2311.02357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuecheng Li, Jialong Chen, Chuan Chen, Lei Yang, Zibin Zheng</li>
<li>for: 本研究旨在提出一种新的社区探测算法，以解决现有的非正式矩阵分解（NMF）基于方法的三大问题：1）它们直接将原始网络转换为社区会员空间，难以捕捉层次结构信息；2）它们通常仅关注网络的拓扑结构，忽略节点特征；3）它们难以学习社区探测所需的全局结构信息。</li>
<li>methods: 我们提出了一种新的社区探测算法，名为对比深度非负矩阵分解（CDNMF）。我们首先深化NMF，以增强其信息提取能力。然后，我们受到对比学习的启发，把网络拓扑结构和节点特征作为两个对比视图构建。此外，我们使用了一个减噪负样本层，以提高模型的社区同义性。</li>
<li>results: 我们在三个公共实验 graphs 上进行了实验，并证明了 CDNMF 模型在社区探测方面的优异性。与现有方法相比，CDNMF 模型在社区内部的节点相似性学习和全局结构信息捕捉方面具有优势。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/6lyc/CDNMF.git">https://github.com/6lyc/CDNMF.git</a> 中找到。<details>
<summary>Abstract</summary>
Recently, nonnegative matrix factorization (NMF) has been widely adopted for community detection, because of its better interpretability. However, the existing NMF-based methods have the following three problems: 1) they directly transform the original network into community membership space, so it is difficult for them to capture the hierarchical information; 2) they often only pay attention to the topology of the network and ignore its node attributes; 3) it is hard for them to learn the global structure information necessary for community detection. Therefore, we propose a new community detection algorithm, named Contrastive Deep Nonnegative Matrix Factorization (CDNMF). Firstly, we deepen NMF to strengthen its capacity for information extraction. Subsequently, inspired by contrastive learning, our algorithm creatively constructs network topology and node attributes as two contrasting views. Furthermore, we utilize a debiased negative sampling layer and learn node similarity at the community level, thereby enhancing the suitability of our model for community detection. We conduct experiments on three public real graph datasets and the proposed model has achieved better results than state-of-the-art methods. Code available at https://github.com/6lyc/CDNMF.git.
</details>
<details>
<summary>摘要</summary>
现在，非正式矩阵分解（NMF）已经广泛应用于社群检测，因为它的更好的解释性。然而，现有的NMF基于方法有以下三个问题：1）它们直接将原始网络转换成社群成员空间，因此很难捕捉层次信息; 2）它们通常只关注网络的结构，忽略节点特征; 3）它们难以学习必要的全局结构信息 для社群检测。因此，我们提出了一个新的社群检测算法，名为对比深度非正式矩阵分解（CDNMF）。首先，我们深入了NMF，以增强其信息提取能力。然后，我们受到对比学习的启发，创新地将网络结构和节点特征作为两个对比视图。此外，我们使用了一层恢复降低的负样本层，并在社群层上学习节点相似性，从而提高了我们模型的适应性。我们在三个公共实验Graph数据集上进行了实验，并取得了与当前最佳方法的更好的结果。代码可以在https://github.com/6lyc/CDNMF.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Perturbation-based-Active-Learning-for-Question-Answering"><a href="#Perturbation-based-Active-Learning-for-Question-Answering" class="headerlink" title="Perturbation-based Active Learning for Question Answering"></a>Perturbation-based Active Learning for Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02345">http://arxiv.org/abs/2311.02345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Luo, Mihai Surdeanu</li>
<li>for: 建立一个问答模型，可以降低标注成本，通过使用活动学习（AL）训练策略。</li>
<li>methods: 使用活动学习的采样策略，选择最有用的无标示训练数据，以更新模型。</li>
<li>results: 提出了一种扰动基于采样策略，与常用的采样策略相比，更有效率。<details>
<summary>Abstract</summary>
Building a question answering (QA) model with less annotation costs can be achieved by utilizing active learning (AL) training strategy. It selects the most informative unlabeled training data to update the model effectively. Acquisition functions for AL are used to determine how informative each training example is, such as uncertainty or diversity based sampling. In this work, we propose a perturbation-based active learning acquisition strategy and demonstrate it is more effective than existing commonly used strategies.
</details>
<details>
<summary>摘要</summary>
使用活动学习（AL）训练策略可以降低问答（QA）模型标注成本。它选择最有用的未标注训练数据来更新模型，以达到更高的效果。获取函数用于AL来确定每个训练示例的有用程度，如不确定性或多样性基本采样。在这项工作中，我们提议使用扰动基于的活动学习获取策略，并证明它比现有的通用使用策略更有效。
</details></li>
</ul>
<hr>
<h2 id="You-Only-Forward-Once-Prediction-and-Rationalization-in-A-Single-Forward-Pass"><a href="#You-Only-Forward-Once-Prediction-and-Rationalization-in-A-Single-Forward-Pass" class="headerlink" title="You Only Forward Once: Prediction and Rationalization in A Single Forward Pass"></a>You Only Forward Once: Prediction and Rationalization in A Single Forward Pass</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02344">http://arxiv.org/abs/2311.02344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Jiang, Junwen Duan, Zhe Qu, Jianxin Wang</li>
<li>for: 本研究旨在提高无监督逻辑抽象的精度和效果，使模型预测时可以快速提取有用的信息。</li>
<li>methods: 本研究使用了一种新的单阶段框架，即You Only Forward Once（YOFO）框架，其中使用了一个预训练的语言模型如BERT进行预测和分析。</li>
<li>results: 实验结果显示，YOFO模型可以比前一代RNP模型更加准确地预测和提取有用的逻辑抽象。对比于前一代方法，YOFO模型可以提高token级F1得分达18.4%。此外，研究还发现YOFO模型可以快速提取有用的逻辑抽象，并且可以在模型中移除不重要的token。<details>
<summary>Abstract</summary>
Unsupervised rationale extraction aims to extract concise and contiguous text snippets to support model predictions without any annotated rationale. Previous studies have used a two-phase framework known as the Rationalizing Neural Prediction (RNP) framework, which follows a generate-then-predict paradigm. They assumed that the extracted explanation, called rationale, should be sufficient to predict the golden label. However, the assumption above deviates from the original definition and is too strict to perform well. Furthermore, these two-phase models suffer from the interlocking problem and spurious correlations. To solve the above problems, we propose a novel single-phase framework called You Only Forward Once (YOFO), derived from a relaxed version of rationale where rationales aim to support model predictions rather than make predictions. In our framework, A pre-trained language model like BERT is deployed to simultaneously perform prediction and rationalization with less impact from interlocking or spurious correlations. Directly choosing the important tokens in an unsupervised manner is intractable. Instead of directly choosing the important tokens, YOFO gradually removes unimportant tokens during forward propagation. Through experiments on the BeerAdvocate and Hotel Review datasets, we demonstrate that our model is able to extract rationales and make predictions more accurately compared to RNP-based models. We observe an improvement of up to 18.4\% in token-level F1 compared to previous state-of-the-art methods. We also conducted analyses and experiments to explore the extracted rationales and token decay strategies. The results show that YOFO can extract precise and important rationales while removing unimportant tokens in the middle part of the model.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Unsupervised rationale extraction aims to extract concise and contiguous text snippets to support model predictions without any annotated rationale. Previous studies have used a two-phase framework known as the Rationalizing Neural Prediction (RNP) framework, which follows a generate-then-predict paradigm. They assumed that the extracted explanation, called rationale, should be sufficient to predict the golden label. However, the assumption above deviates from the original definition and is too strict to perform well. Furthermore, these two-phase models suffer from the interlocking problem and spurious correlations. To solve the above problems, we propose a novel single-phase framework called You Only Forward Once (YOFO), derived from a relaxed version of rationale where rationales aim to support model predictions rather than make predictions. In our framework, A pre-trained language model like BERT is deployed to simultaneously perform prediction and rationalization with less impact from interlocking or spurious correlations. Directly choosing the important tokens in an unsupervised manner is intractable. Instead of directly choosing the important tokens, YOFO gradually removes unimportant tokens during forward propagation. Through experiments on the BeerAdvocate and Hotel Review datasets, we demonstrate that our model is able to extract rationales and make predictions more accurately compared to RNP-based models. We observe an improvement of up to 18.4\% in token-level F1 compared to previous state-of-the-art methods. We also conducted analyses and experiments to explore the extracted rationales and token decay strategies. The results show that YOFO can extract precise and important rationales while removing unimportant tokens in the middle part of the model.Translation:无监督的理由抽取目标在支持模型预测而不需要任何标注的理由。先前的研究使用了一个两个阶段框架，称为神经网络预测合理化（RNP）框架，该框架采用生成Then预测模式。它们假设提取的解释，即理由，应该足够预测金 Label。然而，上述假设与原始定义偏离，并且太严格来不能perform well。此外，这些两个阶段模型受到了交叠问题和偶极相关性的影响。为解决以上问题，我们提出了一种单阶段框架，称为你只能前进一次（YOFO），该框架基于放松的理由定义，其中理由的目的是支持模型预测而不是预测。在我们的框架中，使用预训练的自然语言模型，如BERT，同时进行预测和合理化，以减少交叠或偶极相关性的影响。直接从无监督中选择重要的字符是不可能。相反，YOFO在前进传播过程中逐渐移除无关重要的字符。通过对BeerAdvocate和酒店评论数据集进行实验，我们证明了我们的模型可以更加准确地提取理由和预测。我们观察到的提取改善率可达18.4%。我们还进行了分析和实验，以探索提取的理由和字符衰减策略。结果表明，YOFO可以提取精确和重要的理由，并在中部模型中移除无关重要的字符。
</details></li>
</ul>
<hr>
<h2 id="Stable-Diffusion-Reference-Only-Image-Prompt-and-Blueprint-Jointly-Guided-Multi-Condition-Diffusion-Model-for-Secondary-Painting"><a href="#Stable-Diffusion-Reference-Only-Image-Prompt-and-Blueprint-Jointly-Guided-Multi-Condition-Diffusion-Model-for-Secondary-Painting" class="headerlink" title="Stable Diffusion Reference Only: Image Prompt and Blueprint Jointly Guided Multi-Condition Diffusion Model for Secondary Painting"></a>Stable Diffusion Reference Only: Image Prompt and Blueprint Jointly Guided Multi-Condition Diffusion Model for Secondary Painting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02343">http://arxiv.org/abs/2311.02343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Ai, Lu Sheng</li>
<li>for: 这篇论文旨在提高二次绘制的效率，特别是在漫画、动画等艺术创作领域。</li>
<li>methods: 这篇论文提出了一新的自动化图像生成方法，仅使用两种条件图像进行精确控制生成，以减少对ControlNet的需求。</li>
<li>results: 这篇论文的实验结果显示，这新的方法可以实现高效的二次绘制，并且可以实现高品质的图像生成。<details>
<summary>Abstract</summary>
Stable Diffusion and ControlNet have achieved excellent results in the field of image generation and synthesis. However, due to the granularity and method of its control, the efficiency improvement is limited for professional artistic creations such as comics and animation production whose main work is secondary painting. In the current workflow, fixing characters and image styles often need lengthy text prompts, and even requires further training through TextualInversion, DreamBooth or other methods, which is very complicated and expensive for painters. Therefore, we present a new method in this paper, Stable Diffusion Reference Only, a images-to-image self-supervised model that uses only two types of conditional images for precise control generation to accelerate secondary painting. The first type of conditional image serves as an image prompt, supplying the necessary conceptual and color information for generation. The second type is blueprint image, which controls the visual structure of the generated image. It is natively embedded into the original UNet, eliminating the need for ControlNet. We released all the code for the module and pipeline, and trained a controllable character line art coloring model at https://github.com/aihao2000/stable-diffusion-reference-only, that achieved state-of-the-art results in this field. This verifies the effectiveness of the structure and greatly improves the production efficiency of animations, comics, and fanworks.
</details>
<details>
<summary>摘要</summary>
stable diffusion和controlnet在图像生成和合成领域具有优秀的成绩，但由于它的粒度和控制方法，对专业艺术创作如漫画和动画制作而言，效率提升的限制很大。现在的工作流程中，fixing人物和图像风格经常需要长时间的文本提示，甚至需要通过文本反向、梦幻箱等方法进行进一步的训练，这对画家来说是非常复杂和昂贵的。因此，我们在这篇论文中提出了一新的方法：stable diffusion reference only，这是一种图像到图像的自我超vis的模型，只需要两种类型的条件图像来进行精确的控制生成，以加速secondary painting。第一种类型的条件图像 acted as an image prompt，提供了必要的概念和颜色信息 для生成。第二种类型的条件图像是蓝图图像，它控制了生成图像的视觉结构，并且Native embedding在原始UNet中，消除了需要控制网的需求。我们已经发布了模块和管道的代码，并在https://github.com/aihao2000/stable-diffusion-reference-only上训练了一个可控色彩插画模型，达到了这个领域的州前成绩。这证明了结构的有效性，对动画、漫画和粉丝创作的生产效率进行了很大的提升。
</details></li>
</ul>
<hr>
<h2 id="Potato-Leaf-Disease-Classification-using-Deep-Learning-A-Convolutional-Neural-Network-Approach"><a href="#Potato-Leaf-Disease-Classification-using-Deep-Learning-A-Convolutional-Neural-Network-Approach" class="headerlink" title="Potato Leaf Disease Classification using Deep Learning: A Convolutional Neural Network Approach"></a>Potato Leaf Disease Classification using Deep Learning: A Convolutional Neural Network Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02338">http://arxiv.org/abs/2311.02338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Utkarsh Yashwant Tambe, A. Shobanadevi, A. Shanthini, Hsiu-Chun Hsu</li>
<li>for: 本研究使用深度学习（Deep Learning）来分类芋头叶病。</li>
<li>methods: 提议的方法包括对叶图像数据进行预处理，使用深度学习模型训练，并对测试集进行评估。</li>
<li>results: 实验结果显示，使用深度学习模型，全面准确率达99.1%，可高度准确地识别芋头叶病两种，包括早期虫疫和晚期虫疫，以及健康叶片。这种方法可能为芋头农业中疾病识别提供可靠和有效的解决方案，帮助保持食品安全和避免农业损失。<details>
<summary>Abstract</summary>
In this study, a Convolutional Neural Network (CNN) is used to classify potato leaf illnesses using Deep Learning. The suggested approach entails preprocessing the leaf image data, training a CNN model on that data, and assessing the model's success on a test set. The experimental findings show that the CNN model, with an overall accuracy of 99.1%, is highly accurate in identifying two kinds of potato leaf diseases, including Early Blight, Late Blight, and Healthy. The suggested method may offer a trustworthy and effective remedy for identifying potato diseases, which is essential for maintaining food security and minimizing financial losses in agriculture. The model can accurately recognize the various disease types even when there are severe infections present. This work highlights the potential of deep learning methods for categorizing potato diseases, which can help with effective and automated disease management in potato farming.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们使用了卷积神经网络（CNN）来分类芋头叶病。我们建议的方法包括对叶图像数据进行预处理，使用该数据训练CNN模型，并对测试集进行评估。实验结果表明，我们的CNN模型，准确率为99.1%，高度准确地识别了两种芋头叶病，包括早期病和晚期病，以及健康的叶片。我们建议的方法可能为芋头农业中维护食品安全和减少经济损失提供一个可靠和有效的解决方案。这种方法可以准确地识别不同的病种，即使有严重的感染存在。这项研究表明了深度学习方法在芋头病类分类中的潜力，这可能会帮助实现自动化和有效的芋头病管理。
</details></li>
</ul>
<hr>
<h2 id="STOW-Discrete-Frame-Segmentation-and-Tracking-of-Unseen-Objects-for-Warehouse-Picking-Robots"><a href="#STOW-Discrete-Frame-Segmentation-and-Tracking-of-Unseen-Objects-for-Warehouse-Picking-Robots" class="headerlink" title="STOW: Discrete-Frame Segmentation and Tracking of Unseen Objects for Warehouse Picking Robots"></a>STOW: Discrete-Frame Segmentation and Tracking of Unseen Objects for Warehouse Picking Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02337">http://arxiv.org/abs/2311.02337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Li, Muru Zhang, Markus Grotz, Kaichun Mo, Dieter Fox</li>
<li>for: 这篇论文主要关注在dynamic industrial robotic contexts和domestic robotic applications中，对于物品的重新排序、移除和部分遮蔽等操作，以及在长时间内实现物品追踪的任务。</li>
<li>methods: 本文提出了一个新的合成和真实世界数据集，以及一个基于transformer模组的联合分割和追踪方法，以解决这些具有挑战性的任务。</li>
<li>results: 本文的实验结果显示，该方法与最近的方法相比，有着优秀的性能。另外，请参考官方网站(\href{<a target="_blank" rel="noopener" href="https://sites.google.com/view/stow-corl23%7D%7Bwebsite%7D">https://sites.google.com/view/stow-corl23}{website}</a>) для更多的结果和视频。<details>
<summary>Abstract</summary>
Segmentation and tracking of unseen object instances in discrete frames pose a significant challenge in dynamic industrial robotic contexts, such as distribution warehouses. Here, robots must handle object rearrangement, including shifting, removal, and partial occlusion by new items, and track these items after substantial temporal gaps. The task is further complicated when robots encounter objects not learned in their training sets, which requires the ability to segment and track previously unseen items. Considering that continuous observation is often inaccessible in such settings, our task involves working with a discrete set of frames separated by indefinite periods during which substantial changes to the scene may occur. This task also translates to domestic robotic applications, such as rearrangement of objects on a table. To address these demanding challenges, we introduce new synthetic and real-world datasets that replicate these industrial and household scenarios. We also propose a novel paradigm for joint segmentation and tracking in discrete frames along with a transformer module that facilitates efficient inter-frame communication. The experiments we conduct show that our approach significantly outperforms recent methods. For additional results and videos, please visit \href{https://sites.google.com/view/stow-corl23}{website}. Code and dataset will be released.
</details>
<details>
<summary>摘要</summary>
Segmentation and tracking of unseen object instances in discrete frames poses a significant challenge in dynamic industrial robotic contexts, such as distribution warehouses. Here, robots must handle object rearrangement, including shifting, removal, and partial occlusion by new items, and track these items after substantial temporal gaps. The task is further complicated when robots encounter objects not learned in their training sets, which requires the ability to segment and track previously unseen items. Considering that continuous observation is often inaccessible in such settings, our task involves working with a discrete set of frames separated by indefinite periods during which substantial changes to the scene may occur. This task also translates to domestic robotic applications, such as rearrangement of objects on a table. To address these demanding challenges, we introduce new synthetic and real-world datasets that replicate these industrial and household scenarios. We also propose a novel paradigm for joint segmentation and tracking in discrete frames along with a transformer module that facilitates efficient inter-frame communication. The experiments we conduct show that our approach significantly outperforms recent methods. For additional results and videos, please visit \href{https://sites.google.com/view/stow-corl23}{website}. Code and dataset will be released.Here's the translation in Traditional Chinese:Segmentation and tracking of unseen object instances in discrete frames poses a significant challenge in dynamic industrial robotic contexts, such as distribution warehouses. Here, robots must handle object rearrangement, including shifting, removal, and partial occlusion by new items, and track these items after substantial temporal gaps. The task is further complicated when robots encounter objects not learned in their training sets, which requires the ability to segment and track previously unseen items. Considering that continuous observation is often inaccessible in such settings, our task involves working with a discrete set of frames separated by indefinite periods during which substantial changes to the scene may occur. This task also translates to domestic robotic applications, such as rearrangement of objects on a table. To address these demanding challenges, we introduce new synthetic and real-world datasets that replicate these industrial and household scenarios. We also propose a novel paradigm for joint segmentation and tracking in discrete frames along with a transformer module that facilitates efficient inter-frame communication. The experiments we conduct show that our approach significantly outperforms recent methods. For additional results and videos, please visit \href{https://sites.google.com/view/stow-corl23}{website}. Code and dataset will be released.
</details></li>
</ul>
<hr>
<h2 id="Complex-Organ-Mask-Guided-Radiology-Report-Generation"><a href="#Complex-Organ-Mask-Guided-Radiology-Report-Generation" class="headerlink" title="Complex Organ Mask Guided Radiology Report Generation"></a>Complex Organ Mask Guided Radiology Report Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02329">http://arxiv.org/abs/2311.02329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GaryGuTC/COMG_model">https://github.com/GaryGuTC/COMG_model</a></li>
<li>paper_authors: Gu Tiancheng, Liu Dongnan, Li Zhiyuan, Cai Weidong</li>
<li>for: 提高诊断报告的精度和详细程度，alleviate traditional radiology reporting workload.</li>
<li>methods: 基于多Modal的组织面干（COMG）报告生成模型， incorporates 多个器官（如骨、肺、心脏、 mediastinum）的面干，以提供更详细的医疗信息和引导模型注意力。</li>
<li>results: 在 IU-Xray 和 MIMIC 两个公共数据集上实验，COMG 比 SOTA 模型 KiUT 提高了11.4% 和 9.7% 的 BLEU@4 分数。<details>
<summary>Abstract</summary>
The goal of automatic report generation is to generate a clinically accurate and coherent phrase from a single given X-ray image, which could alleviate the workload of traditional radiology reporting.However, in a real-world scenario, radiologists frequently face the challenge of producing extensive reports derived from numerous medical images, thereby medical report generation from multi-image perspective is needed.In this paper, we propose the Complex Organ Mask Guided (termed as COMG) report generation model, which incorporates masks from multiple organs (e.g., bones, lungs, heart, and mediastinum), to provide more detailed information and guide the model's attention to these crucial body regions. Specifically, we leverage prior knowledge of the disease corresponding to each organ in the fusion process to enhance the disease identification phase during the report generation process. Additionally, cosine similarity loss is introduced as target function to ensure the convergence of cross-modal consistency and facilitate model optimization.Experimental results on two public datasets show that COMG achieves a 11.4% and 9.7% improvement in terms of BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively.
</details>
<details>
<summary>摘要</summary>
目的自动报告生成是生成单一X-ray图像的依据生成一个临床精确和连贯的句子，以减轻传统医疗影像报告的工作负担。然而，在实际应用中，医生很频繁地面临着从多个医疗影像中生成详细的报告，因此对多个医疗影像的医疗报告生成是需要的。在这篇文章中，我们提出了复杂器官面精准指南（COMG）报告生成模型，它将多个器官（如骨、肺、心脏和脊椎）的面精准指南 integrate into the model,以提供更多的详细信息和导引模型的注意力到这些重要的身体区域。具体来说，我们利用各器官疾病的专业知识在融合过程中强化疾病识别阶段，以提高报告生成过程中的疾病识别率。此外，我们引入了cosine similarity损失函数，以便在混合modal consistency的整合过程中实现模型优化。实验结果显示，在两个公共数据集上，COMG对于KiUT的BLEU@4分数有11.4%和9.7%的提升。
</details></li>
</ul>
<hr>
<h2 id="FragXsiteDTI-Revealing-Responsible-Segments-in-Drug-Target-Interaction-with-Transformer-Driven-Interpretation"><a href="#FragXsiteDTI-Revealing-Responsible-Segments-in-Drug-Target-Interaction-with-Transformer-Driven-Interpretation" class="headerlink" title="FragXsiteDTI: Revealing Responsible Segments in Drug-Target Interaction with Transformer-Driven Interpretation"></a>FragXsiteDTI: Revealing Responsible Segments in Drug-Target Interaction with Transformer-Driven Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02326">http://arxiv.org/abs/2311.02326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Khodabandeh Yalabadi, Mehdi Yazdani-Jahromi, Niloofar Yousefi, Aida Tayebi, Sina Abdidizaji, Ozlem Ozmen Garibay</li>
<li>For: 预测药物目标交互（DTI）在药物发现中具有重要意义，但是现有模型具有解释性和性能优化等挑战。本文提出了一种基于转换器的新模型，即FragXsiteDTI，以解决DTI预测中的这些挑战。* Methods: FragXsiteDTI模型 simultaneous 利用药物分子块和蛋白质孔隙，并采用了转换器架构，包括跨注意力和自注意力。模型还具有可学习的隐藏数组，通过cross-attention和self-attention来改进模型的性能。* Results: 根据三个 benchmarking 数据集的计算结果，FragXsiteDTI 模型在预测DTI方面表现出了明显的优势，并且可以准确地表达药物和目标蛋白质之间的交互。此外，模型还可以提供可读性的解释，包括药物和目标蛋白质中关键的组分。<details>
<summary>Abstract</summary>
Drug-Target Interaction (DTI) prediction is vital for drug discovery, yet challenges persist in achieving model interpretability and optimizing performance. We propose a novel transformer-based model, FragXsiteDTI, that aims to address these challenges in DTI prediction. Notably, FragXsiteDTI is the first DTI model to simultaneously leverage drug molecule fragments and protein pockets. Our information-rich representations for both proteins and drugs offer a detailed perspective on their interaction. Inspired by the Perceiver IO framework, our model features a learnable latent array, initially interacting with protein binding site embeddings using cross-attention and later refined through self-attention and used as a query to the drug fragments in the drug's cross-attention transformer block. This learnable query array serves as a mediator and enables seamless information translation, preserving critical nuances in drug-protein interactions. Our computational results on three benchmarking datasets demonstrate the superior predictive power of our model over several state-of-the-art models. We also show the interpretability of our model in terms of the critical components of both target proteins and drug molecules within drug-target pairs.
</details>
<details>
<summary>摘要</summary>
drugs-target interaction (DTI) prediction is crucial for drug discovery, but challenges remain in achieving model interpretability and optimizing performance. We propose a novel transformer-based model, FragXsiteDTI, to address these challenges in DTI prediction. Notably, FragXsiteDTI is the first DTI model to simultaneously leverage drug molecule fragments and protein pockets. Our information-rich representations for both proteins and drugs provide a detailed perspective on their interaction. Inspired by the Perceiver IO framework, our model features a learnable latent array that initially interacts with protein binding site embeddings using cross-attention and is later refined through self-attention. This learnable query array serves as a mediator and enables seamless information translation, preserving critical nuances in drug-protein interactions. Our computational results on three benchmarking datasets demonstrate the superior predictive power of our model over several state-of-the-art models. We also show the interpretability of our model in terms of the critical components of both target proteins and drug molecules within drug-target pairs.Here's the translation in Traditional Chinese:这是一个标题，请将其转换为中文。药品-标的互动（DTI）预测是药品探索的重要环节，但是还有许多挑战需要解决，以提高模型的解释性和性能。我们提出了一个新的transformer-based模型，FragXsiteDTI，以解决DTI预测中的这些挑战。值得注意的是，FragXsiteDTI是首个同时利用药品分子片段和蛋白质包含物的DTI模型。我们的模型具有丰富的资讯表现，对药品和蛋白质之间的互动提供了详细的见解。受到Perceiver IO框架的启发，我们的模型具有可学习的潜在阵列，首先与蛋白质绑定位置嵌入进行交互，然后通过自我对话和探索来进一步细化。这个可学习的查询阵列作为一个中介者，实现了无障碍的资讯转化，保留了药品-蛋白质互动中的重要特征。我们的computational result表明，FragXsiteDTI模型在三个benchmarkingdataset上的预测力高于多个现有模型。我们还展示了我们模型在药品-标的对之中的解释性，包括标的蛋白质和药品分子之间的关键Component。
</details></li>
</ul>
<hr>
<h2 id="Thermal-Face-Image-Classification-using-Deep-Learning-Techniques"><a href="#Thermal-Face-Image-Classification-using-Deep-Learning-Techniques" class="headerlink" title="Thermal Face Image Classification using Deep Learning Techniques"></a>Thermal Face Image Classification using Deep Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02314">http://arxiv.org/abs/2311.02314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prosenjit Chatterjee, ANK Zaman</li>
<li>for: 这篇论文应用于热像分类。</li>
<li>methods: 本文使用深度学习方法，specifically ResNet-50和VGGNet-19对热像进行特征提取，并应用Kalman统计filter进行图像预测。</li>
<li>results: 实验结果显示提案方法具有高精度和高效率。<details>
<summary>Abstract</summary>
Thermal images have various applications in security, medical and industrial domains. This paper proposes a practical deep-learning approach for thermal image classification. Accurate and efficient classification of thermal images poses a significant challenge across various fields due to the complex image content and the scarcity of annotated datasets. This work uses a convolutional neural network (CNN) architecture, specifically ResNet-50 and VGGNet-19, to extract features from thermal images. This work also applied Kalman filter on thermal input images for image denoising. The experimental results demonstrate the effectiveness of the proposed approach in terms of accuracy and efficiency.
</details>
<details>
<summary>摘要</summary>
热图像在安全、医疗和工业领域有多种应用。这篇论文提出了一种实用的深度学习方法来对热图像进行分类。由于热图像的复杂图像内容以及不同领域的热图像标注数据的稀缺，精度和效率的图像分类具有 significannot challenges。本文使用 convolutional neural network (CNN) 架构，具体来说是 ResNet-50 和 VGGNet-19，来从热图像中提取特征。此外，本文还应用了 Kalman 筛选器来降噪热输入图像。实验结果表明提出的方法在精度和效率两个方面具有remarkable的效果。
</details></li>
</ul>
<hr>
<h2 id="OSM-vs-HD-Maps-Map-Representations-for-Trajectory-Prediction"><a href="#OSM-vs-HD-Maps-Map-Representations-for-Trajectory-Prediction" class="headerlink" title="OSM vs HD Maps: Map Representations for Trajectory Prediction"></a>OSM vs HD Maps: Map Representations for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02305">http://arxiv.org/abs/2311.02305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing-Yan Liao, Parth Doshi, Zihan Zhang, David Paz, Henrik Christensen</li>
<li>for: 提出了一种使用 OpenStreetMap (OSM) 作为替代高清地图 (HD Maps) 的方法，以便在自动驾驶中长期预测动向。</li>
<li>methods: 该方法通过扩展应用范围和 интеграción intercept 约束，使用 OSM 来实现长期预测动向，并且能够与 HD Map 基本相当。</li>
<li>results: 研究表明，该方法可以在不同的情景下提供更好的预测性能，并且可以在自动驾驶中广泛应用。<details>
<summary>Abstract</summary>
While High Definition (HD) Maps have long been favored for their precise depictions of static road elements, their accessibility constraints and susceptibility to rapid environmental changes impede the widespread deployment of autonomous driving, especially in the motion forecasting task. In this context, we propose to leverage OpenStreetMap (OSM) as a promising alternative to HD Maps for long-term motion forecasting. The contributions of this work are threefold: firstly, we extend the application of OSM to long-horizon forecasting, doubling the forecasting horizon compared to previous studies. Secondly, through an expanded receptive field and the integration of intersection priors, our OSM-based approach exhibits competitive performance, narrowing the gap with HD Map-based models. Lastly, we conduct an exhaustive context-aware analysis, providing deeper insights in motion forecasting across diverse scenarios as well as conducting class-aware comparisons. This research not only advances long-term motion forecasting with coarse map representations but additionally offers a potential scalable solution within the domain of autonomous driving.
</details>
<details>
<summary>摘要</summary>
高清定图（HD Map）已经长期被喜欢用于其精确地表示静止道路元素，但是访问限制和环境变化的敏感性使得自动驾驶的广泛部署受到限制，尤其是在动作预测任务中。在这种情况下，我们提议使用开源地图（OSM）作为自动驾驶中长期动作预测的可能的代替方案。本研究的贡献有三个方面：一、我们扩展了OSM的应用范围，使其能够进行更长的预测 horizon，与前一些研究相比， doubling the forecasting horizon。二、通过扩展的接受场和交叉点约束的结合，我们的OSM基于方法在竞争性方面表现出色，与HD Map基于模型相匹配。三、我们进行了广泛的Context-aware分析，提供了更深入的运动预测理解，并进行了不同场景的类型敏感比较。这种研究不仅提高了长期动作预测的粗糙地图表示，还提供了可扩展的解决方案在自动驾驶领域。
</details></li>
</ul>
<hr>
<h2 id="MFTCoder-Boosting-Code-LLMs-with-Multitask-Fine-Tuning"><a href="#MFTCoder-Boosting-Code-LLMs-with-Multitask-Fine-Tuning" class="headerlink" title="MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"></a>MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02303">http://arxiv.org/abs/2311.02303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen, Hailian Zhou, Hang Yu, Jianguo Li</li>
<li>for: 提高 CodeLLama 模型的编程能力，并且可以同时 fine-tune 多个任务。</li>
<li>methods: 使用多任务 fine-tuning 框架（MFTcoder），并结合多种损失函数来解决多任务学习中的常见挑战。</li>
<li>results: 比较传统 fine-tuning 方法和 mixed 任务 fine-tuning 方法，MFTcoder 能够达到更高的性能，并且可以快速训练和部署。<details>
<summary>Abstract</summary>
Code LLMs have emerged as a specialized research field, with remarkable studies dedicated to enhancing model's coding capabilities through fine-tuning on pre-trained models. Previous fine-tuning approaches were typically tailored to specific downstream tasks or scenarios, which meant separate fine-tuning for each task, requiring extensive training resources and posing challenges in terms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness among different code-related tasks. To overcome these limitations, we present a multi-task fine-tuning framework, MFTcoder, that enables simultaneous and parallel fine-tuning on multiple tasks. By incorporating various loss functions, we effectively address common challenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent convergence speeds. Extensive experiments have conclusively demonstrated that our multi-task fine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble of tasks. Moreover, MFTcoder offers efficient training capabilities, including efficient data tokenization modes and PEFT fine-tuning, resulting in significantly improved speed compared to traditional fine-tuning methods. MFTcoder seamlessly integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen. Leveraging the CodeLLama foundation, our MFTcoder fine-tuned model, \textsc{CodeFuse-CodeLLama-34B}, achieves an impressive pass@1 score of 74.4\% on the HumaneEval benchmark, surpassing GPT-4 performance (67\%, zero-shot). MFTCoder is open-sourced at \url{https://github.com/codefuse-ai/MFTCOder}
</details>
<details>
<summary>摘要</summary>
code llms 已经成为一个专门的研究领域，有很多研究把焦点放在提高模型的编程能力上，通过对预训练模型进行细化。过去的细化方法通常是为特定下游任务或场景进行定制，这意味着每个任务都需要单独进行细化，需要很多训练资源，同时也存在部署和维护上的挑战。此外，这些方法还没有利用编程任务之间的内在相互连接。为了解决这些限制，我们提出了一个多任务细化框架，MFTcoder，它允许同时并行细化多个任务。通过 incorporating 多种损失函数，我们有效地解决了多任务学习中常见的挑战，如数据不均衡、任务难度不同、和不同任务的学习速度不一致。广泛的实验证明了我们的多任务细化方法在单任务细化和混合任务细化的情况下都有出色的表现。此外，MFTcoder还提供了高效的训练能力，包括高效的数据分割模式和PEFT细化，从而在传统细化方法的基础上减少了训练时间。MFTcoder可以与多个主流的开源 LLMS 集成，如 CodeLLama 和 Qwen。利用 CodeLLama 基础，我们的 MFTcoder 细化模型，\textsc{CodeFuse-CodeLLama-34B}，在 HumaneEval benchmark 上达到了很吸引人的 pass@1 分数为 74.4%，超越 GPT-4 的表现（67%，零shot）。MFTCoder 的源代码可以在 <https://github.com/codefuse-ai/MFTCOder> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Successive-Model-Agnostic-Meta-Learning-for-Few-Shot-Fault-Time-Series-Prognosis"><a href="#Successive-Model-Agnostic-Meta-Learning-for-Few-Shot-Fault-Time-Series-Prognosis" class="headerlink" title="Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis"></a>Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02300">http://arxiv.org/abs/2311.02300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Su, Jiajun Hu, Songsen Yu</li>
<li>for: 解决几个shot的缺陷预测问题，提高预测精度和泛化能力。</li>
<li>methods: 引入新的’pseudo meta-task’分区方案，将续时序数据视为一个meta-任务，分割成多个短时期，提取更全面的特征和关系，提高预测精度。同时，引入差分算法提高方法的稳定性。</li>
<li>results: 通过在多个缺陷预测和时间序列预测 dataset上进行广泛的实验，证明了我们的方法可以在少量数据下提高预测性能和泛化能力。<details>
<summary>Abstract</summary>
Meta learning is a promising technique for solving few-shot fault prediction problems, which have attracted the attention of many researchers in recent years. Existing meta-learning methods for time series prediction, which predominantly rely on random and similarity matching-based task partitioning, face three major limitations: (1) feature exploitation inefficiency; (2) suboptimal task data allocation; and (3) limited robustness with small samples. To overcome these limitations, we introduce a novel 'pseudo meta-task' partitioning scheme that treats a continuous time period of a time series as a meta-task, composed of multiple successive short time periods. Employing continuous time series as pseudo meta-tasks allows our method to extract more comprehensive features and relationships from the data, resulting in more accurate predictions. Moreover, we introduce a differential algorithm to enhance the robustness of our method across different datasets. Through extensive experiments on several fault and time series prediction datasets, we demonstrate that our approach substantially enhances prediction performance and generalization capability under both few-shot and general conditions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>预测技术是解决几个shot错误预测问题的有力方法，这些问题在最近几年内吸引了许多研究人员的关注。现有的预测方法对时间序列预测，主要基于随机和相似性匹配的任务分配，面临三大限制：（1）特征利用不充分;（2）不优化的任务数据分配;以及（3）只能在小样本情况下保持有限的 Robustness。为了突破这些限制，我们提出了一种新的“伪meta任务”分配方案，将一个连续时间序列视为一个meta任务，由多个连续的短时间序列组成。使用连续时间序列为伪meta任务，我们的方法可以从数据中提取更全面的特征和关系，从而实现更准确的预测。此外，我们还引入了一种差分算法，以提高我们方法的 Robustness 性在不同的数据集上。通过对多个错误和时间序列预测数据集进行广泛的实验，我们示出了我们方法可以在少量示例和普通情况下显著提高预测性能和泛化能力。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-the-Various-Methodologies-Towards-making-Artificial-Intelligence-More-Explainable"><a href="#A-Survey-of-the-Various-Methodologies-Towards-making-Artificial-Intelligence-More-Explainable" class="headerlink" title="A Survey of the Various Methodologies Towards making Artificial Intelligence More Explainable"></a>A Survey of the Various Methodologies Towards making Artificial Intelligence More Explainable</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02291">http://arxiv.org/abs/2311.02291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sopam Dasgupta</li>
<li>for: 这个论文的目的是提高机器决策过程中的解释性和可解释性，以便更好地理解决交的决策理由。</li>
<li>methods: 本论文使用了一种基于人工智能的方法，通过对模型的解释性和可解释性进行分析和评估，以便提高机器决策过程中的解释性和可解释性。</li>
<li>results: 本论文的研究结果表明，通过提高机器决策过程中的解释性和可解释性，可以更好地理解决交的决策理由，并且可以通过对模型的解释性和可解释性进行分析和评估，以便更好地提高机器决策过程中的解释性和可解释性。<details>
<summary>Abstract</summary>
Machines are being increasingly used in decision-making processes, resulting in the realization that decisions need explanations. Unfortunately, an increasing number of these deployed models are of a 'black-box' nature where the reasoning behind the decisions is unknown. Hence, there is a need for clarity behind the reasoning of these decisions. As humans, we would want these decisions to be presented to us in an explainable manner. However, explanations alone are insufficient. They do not necessarily tell us how to achieve an outcome but merely tell us what achieves the given outcome. For this reason, my research focuses on explainability/interpretability and how it extends to counterfactual thinking.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Predicting-Ground-Reaction-Force-from-Inertial-Sensors"><a href="#Predicting-Ground-Reaction-Force-from-Inertial-Sensors" class="headerlink" title="Predicting Ground Reaction Force from Inertial Sensors"></a>Predicting Ground Reaction Force from Inertial Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02287">http://arxiv.org/abs/2311.02287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Song, Marco Paolieri, Harper E. Stewart, Leana Golubchik, Jill L. McNitt-Gray, Vishal Misra, Devavrat Shah</li>
<li>for: 这个论文的目的是用IMU数据来预测脚下压力（GRF），以便分析运动员的生物机械变量（如接触时间和加速度）。</li>
<li>methods: 这篇论文使用了三种轻量级的预测方法：k-Nearest Neighbors（KNN）回归、支持向量表示插值（SVD）回归和深度学习神经网络（LSTM）。</li>
<li>results: 研究结果表明，使用KNN回归和SVD插值可以与LSTM神经网络相比，具有相似或更高的准确率，并且训练时间更短，hyperparameter优化也更简单。尤其是当使用个人训练数据时，SER和KNN方法更加准确。此外，使用个人数据可以降低预测错误的大多数变量。<details>
<summary>Abstract</summary>
The study of ground reaction forces (GRF) is used to characterize the mechanical loading experienced by individuals in movements such as running, which is clinically applicable to identify athletes at risk for stress-related injuries. Our aim in this paper is to determine if data collected with inertial measurement units (IMUs), that can be worn by athletes during outdoor runs, can be used to predict GRF with sufficient accuracy to allow the analysis of its derived biomechanical variables (e.g., contact time and loading rate).   In this paper, we consider lightweight approaches in contrast to state-of-the-art prediction using LSTM neural networks. Specifically, we compare use of LSTMs to k-Nearest Neighbors (KNN) regression as well as propose a novel solution, SVD Embedding Regression (SER), using linear regression between singular value decomposition embeddings of IMUs data (input) and GRF data (output). We evaluate the accuracy of these techniques when using training data collected from different athletes, from the same athlete, or both, and we explore the use of acceleration and angular velocity data from sensors at different locations (sacrum and shanks). Our results illustrate that simple machine learning methods such as SER and KNN can be similarly accurate or more accurate than LSTM neural networks, with much faster training times and hyperparameter optimization; in particular, SER and KNN are more accurate when personal training data are available, and KNN comes with benefit of providing provenance of prediction. Notably, the use of personal data reduces prediction errors of all methods for most biomechanical variables.
</details>
<details>
<summary>摘要</summary>
研究地面反应力(GRF)是为了描述运动员在运动中所经历的机械负荷的重要工具。我们的目标是确定 whether 使用抗应力计（IMUs）收集的数据可以准确预测 GRF，以便分析其 derived 生物力学变量（例如，接触时间和加载率）。 在这篇论文中，我们考虑使用轻量级方法，而不是现有的预测方法使用 LSTM 神经网络。我们比较使用 LSTM 和 k-最近邻域（KNN）回归，以及提出了一个新的解决方案，即 Singular Value Decomposition 嵌入回归（SER），使用 IMUs 数据（输入）和 GRF 数据（输出）之间的线性回归。我们评估了这些技术的准确性，使用不同的教学数据集，包括不同运动员、同一个运动员和两者。我们发现，简单的机器学习方法如 SER 和 KNN 可以与 LSTM 神经网络相比较准确，具有更快的训练时间和权重优化。特别是，使用个人教学数据可以减少预测错误，SER 和 KNN 在这种情况下更加准确。另外，使用个人数据还可以提供预测的来源。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/cs.AI_2023_11_04/" data-id="clpxp03va006qfm8858zh596f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/cs.CL_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T11:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/cs.CL_2023_11_04/">cs.CL - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Can-Chat-GPT-solve-a-Linguistics-Exam"><a href="#Can-Chat-GPT-solve-a-Linguistics-Exam" class="headerlink" title="Can Chat GPT solve a Linguistics Exam?"></a>Can Chat GPT solve a Linguistics Exam?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02499">http://arxiv.org/abs/2311.02499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patricia Ronan, Gerold Schneider</li>
<li>for: 这个研究是用来测试 chatGPT4 是否能成功解决入门语言学考试的。</li>
<li>methods: 这个研究使用了 chatGPT4 语言模型，并将过去的考试题 fed 到它中进行测试。</li>
<li>results: 研究发现，chatGPT4 在解释复杂和嵌套任务方面非常成功，但在分析 morphemes 和 phrases 方面表现较差。在简单的情况下，它表现 suficiently well，但在缺失一对一对应的情况下，它的结果是混合的。现在，模型还不能处理视觉化任务，如语法树的分析或生成。通过更EXTENSIVE的预处理，将这些任务转换为文本数据，可以使模型也成功地解决这些任务。<details>
<summary>Abstract</summary>
The present study asks if ChatGPT4, the version of ChatGPT which uses the language model GPT4, can successfully solve introductory linguistic exams. Previous exam questions of an Introduction to Linguistics course at a German university are used to test this. The exam questions were fed into ChatGPT4 with only minimal preprocessing. The results show that the language model is very successful in the interpretation even of complex and nested tasks. It proved surprisingly successful in the task of broad phonetic transcription, but performed less well in the analysis of morphemes and phrases. In simple cases it performs sufficiently well, but rarer cases, particularly with missing one-to-one correspondence, are currently treated with mixed results. The model is not yet able to deal with visualisations, such as the analysis or generation of syntax trees. More extensive preprocessing, which translates these tasks into text data, allow the model to also solve these tasks successfully.
</details>
<details>
<summary>摘要</summary>
本研究问题是否可以使用 ChatGPT4，基于语言模型 GPT4 解决入门语言考试。研究使用了一个德国大学 introductory linguistics 课程的先前考试题，并将其feed into ChatGPT4 中，只进行了最小的处理。结果显示，语言模型在复杂和嵌入的任务中表现非常成功。它在广泛的音素识别任务中表现出色，但在分析 morphemes 和 phrases 方面表现较差。在简单的情况下，它的表现足够好，但在缺少一对一对应的情况下，现在的结果是混合的。模型目前无法处理视觉化任务，如语法树的分析或生成。通过更进一步的处理，将这些任务转化为文本数据后，模型也可以成功解决这些任务。
</details></li>
</ul>
<hr>
<h2 id="Citance-Contextualized-Summarization-of-Scientific-Papers"><a href="#Citance-Contextualized-Summarization-of-Scientific-Papers" class="headerlink" title="Citance-Contextualized Summarization of Scientific Papers"></a>Citance-Contextualized Summarization of Scientific Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02408">http://arxiv.org/abs/2311.02408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahbaz Syed, Ahmad Dawar Hakimi, Khalid Al-Khatib, Martin Potthast</li>
<li>for: 本研究旨在提供一种新的文本概要方法，可以根据给定的引用句（即“citance”）生成有用的概要。</li>
<li>methods: 该方法首先提取并模型了文献中的引用，然后根据引用的位置 retrieve 相关的段落，最后生成基于每个引用的概要。</li>
<li>results: 我们使用 $\textbf{Webis-Context-SciSumm-2023}$ 数据集进行评估，发现我们的方法可以生成高质量的概要，并且可以准确地捕捉到文献中的关键信息。<details>
<summary>Abstract</summary>
Current approaches to automatic summarization of scientific papers generate informative summaries in the form of abstracts. However, abstracts are not intended to show the relationship between a paper and the references cited in it. We propose a new contextualized summarization approach that can generate an informative summary conditioned on a given sentence containing the citation of a reference (a so-called ``citance''). This summary outlines the content of the cited paper relevant to the citation location. Thus, our approach extracts and models the citances of a paper, retrieves relevant passages from cited papers, and generates abstractive summaries tailored to each citance. We evaluate our approach using $\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing 540K~computer science papers and 4.6M~citances therein.
</details>
<details>
<summary>摘要</summary>
现有的自动摘要方法可以生成有用的摘要，但这些摘要不能显示科学论文中引用的文献之间的关系。我们提议一种新的受条件摘要方法，可以根据给定的引用句（即“ citance”）生成相关的摘要。这个摘要将描述引用的文献中与该引用相关的内容。因此，我们的方法可以提取和模型文献中的引用，从引用的文献中检索相关的段落，并生成基于每个引用的摘要。我们使用 $\textbf{Webis-Context-SciSumm-2023}$  dataset，该 dataset包含 540 万个计算机科学论文和 460 万个引用。
</details></li>
</ul>
<hr>
<h2 id="TreeSwap-Data-Augmentation-for-Machine-Translation-via-Dependency-Subtree-Swapping"><a href="#TreeSwap-Data-Augmentation-for-Machine-Translation-via-Dependency-Subtree-Swapping" class="headerlink" title="TreeSwap: Data Augmentation for Machine Translation via Dependency Subtree Swapping"></a>TreeSwap: Data Augmentation for Machine Translation via Dependency Subtree Swapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02355">http://arxiv.org/abs/2311.02355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/attilanagy234/TreeSwap">https://github.com/attilanagy234/TreeSwap</a></li>
<li>paper_authors: Attila Nagy, Dorina Lakatos, Botond Barta, Judit Ács</li>
<li>for: 该论文主要用于提出一种新的数据扩充方法，用于提高神经机器翻译模型在具有有限训练数据的情况下的性能。</li>
<li>methods: 该方法基于 SentenceDependencyGraph，通过将源句子和目标句子中的对象和主语交换来生成新的句子。</li>
<li>results: 对4种语言对在限制资源 datasets 上进行了实验，结果显示，TreeSwap 方法可以在多个语言对的两个方向中提供了顺序的改进。<details>
<summary>Abstract</summary>
Data augmentation methods for neural machine translation are particularly useful when limited amount of training data is available, which is often the case when dealing with low-resource languages. We introduce a novel augmentation method, which generates new sentences by swapping objects and subjects across bisentences. This is performed simultaneously based on the dependency parse trees of the source and target sentences. We name this method TreeSwap. Our results show that TreeSwap achieves consistent improvements over baseline models in 4 language pairs in both directions on resource-constrained datasets. We also explore domain-specific corpora, but find that our method does not make significant improvements on law, medical and IT data. We report the scores of similar augmentation methods and find that TreeSwap performs comparably. We also analyze the generated sentences qualitatively and find that the augmentation produces a correct translation in most cases. Our code is available on Github.
</details>
<details>
<summary>摘要</summary>
� apparatus augmentation methods for neural machine translation are particularly useful when limited amount of training data is available, which is often the case when dealing with low-resource languages. We introduce a novel augmentation method, which generates new sentences by swapping objects and subjects across bisentences. This is performed simultaneously based on the dependency parse trees of the source and target sentences. We name this method TreeSwap. Our results show that TreeSwap achieves consistent improvements over baseline models in 4 language pairs in both directions on resource-constrained datasets. We also explore domain-specific corpora, but find that our method does not make significant improvements on law, medical and IT data. We report the scores of similar augmentation methods and find that TreeSwap performs comparably. We also analyze the generated sentences qualitatively and find that the augmentation produces a correct translation in most cases. Our code is available on Github.Here's a word-for-word translation of the text in Traditional Chinese:� apparatus augmentation methods for neural machine translation are particularly useful when limited amount of training data is available, which is often the case when dealing with low-resource languages. We introduce a novel augmentation method, which generates new sentences by swapping objects and subjects across bisentences. This is performed simultaneously based on the dependency parse trees of the source and target sentences. We name this method TreeSwap. Our results show that TreeSwap achieves consistent improvements over baseline models in 4 language pairs in both directions on resource-constrained datasets. We also explore domain-specific corpora, but find that our method does not make significant improvements on law, medical and IT data. We report the scores of similar augmentation methods and find that TreeSwap performs comparably. We also analyze the generated sentences qualitatively and find that the augmentation produces a correct translation in most cases. Our code is available on Github.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-English-Writing-Proficiency-in-China’s-Polytechnic-Students-An-In-Depth-Literature-Review-on-the-Application-of-the-Input-Hypothesis"><a href="#Enhancing-English-Writing-Proficiency-in-China’s-Polytechnic-Students-An-In-Depth-Literature-Review-on-the-Application-of-the-Input-Hypothesis" class="headerlink" title="Enhancing English Writing Proficiency in China’s Polytechnic Students An In-Depth Literature Review on the Application of the Input Hypothesis"></a>Enhancing English Writing Proficiency in China’s Polytechnic Students An In-Depth Literature Review on the Application of the Input Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02341">http://arxiv.org/abs/2311.02341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhou</li>
<li>for: 这个研究论文的目的是探讨如何使用输入假设（Stephen Krashen）来提高polytechnic学生的英语写作能力。</li>
<li>methods: 这个研究使用了实际观察和前期研究的数据，以检验输入假设对polytechnic学生的写作能力的影响。</li>
<li>results: 研究发现，通过提供可理解的输入，polytechnic学生的写作能力有所改善，这证明了输入假设的有效性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Having good English writing skills is extremely important for students in polytechnic institutions. However, a lot of students in technical schools have difficulties in reaching high levels of skill. The Input Hypothesis, created by Stephen Krashen, suggests that people learn languages well when they receive information that's a little harder than what they already know but still understandable. This research paper wants to study how the Input Hypothesis can help polytechnic students improve their English writing skills. The study will include real-life observations and experiments from the previous research. We will look at data from polytechnic students who are receiving special writing instruction to see if the Input Hypothesis actually helps improve their writing skills. The paper can better inform polytechnic students, faculty members, and support staff and even members of the larger community about the attributions, the processes, and the possible outcomes of second language development for polytechnic students.   Keywords: English writing skills, Polytechnic students, Input hypothesis, Comprehensible input
</details>
<details>
<summary>摘要</summary>
有良好的英语写作技巧对polytechnic学生非常重要。然而，许多技术学校的学生在达到高水平技巧方面遇到困难。输入假设（Input Hypothesis），由史蒂芬·卡什描述，表明人们在接受可以理解但是一些 harder than what they already know的信息时，会学习语言非常好。这篇研究论文旨在研究如何使用输入假设来帮助polytechnic学生提高英语写作技巧。这篇论文将包括以前的实验和观察数据，以确定输入假设是否确实有助于提高polytechnic学生的写作技巧。这篇论文可以更好地告诉polytechnic学生、教师和支持人员，以及社区成员关于第二语言发展的特点、过程和可能的结果。Keywords: 英语写作技巧, polytechnic学生, 输入假设, 可以理解的输入
</details></li>
</ul>
<hr>
<h2 id="Identifying-Context-Dependent-Translations-for-Evaluation-Set-Production"><a href="#Identifying-Context-Dependent-Translations-for-Evaluation-Set-Production" class="headerlink" title="Identifying Context-Dependent Translations for Evaluation Set Production"></a>Identifying Context-Dependent Translations for Evaluation Set Production</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02321">http://arxiv.org/abs/2311.02321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rachel Wicks, Matt Post</li>
<li>for: 本研究的目的是解决Context-aware机器翻译的评估 метри克和测试集的缺失，以便更好地评估Context-aware机器翻译系统的性能。</li>
<li>methods: 本研究使用了现代化、扩展和通用的前一代Annotation pipeline，生成了CTXPRO工具，可以正确地翻译五种语言现象：性别、正式度和生物性 для代词、句子间隔融合、和不确定名词变化。</li>
<li>results: 研究使用了 seven 种语言对（EN到DE、ES、FR、IT、PL、PT和RU）和两个数据集（OpenSubtitles和WMT测试集），并验证了 CTXPRO 的性能，包括与前一代工作的重叠和分类一个Context-aware机器翻译系统和一个句子基于系统。<details>
<summary>Abstract</summary>
A major impediment to the transition to context-aware machine translation is the absence of good evaluation metrics and test sets. Sentences that require context to be translated correctly are rare in test sets, reducing the utility of standard corpus-level metrics such as COMET or BLEU. On the other hand, datasets that annotate such sentences are also rare, small in scale, and available for only a few languages. To address this, we modernize, generalize, and extend previous annotation pipelines to produce CTXPRO, a tool that identifies subsets of parallel documents containing sentences that require context to correctly translate five phenomena: gender, formality, and animacy for pronouns, verb phrase ellipsis, and ambiguous noun inflections. The input to the pipeline is a set of hand-crafted, per-language, linguistically-informed rules that select contextual sentence pairs using coreference, part-of-speech, and morphological features provided by state-of-the-art tools. We apply this pipeline to seven languages pairs (EN into and out-of DE, ES, FR, IT, PL, PT, and RU) and two datasets (OpenSubtitles and WMT test sets), and validate its performance using both overlap with previous work and its ability to discriminate a contextual MT system from a sentence-based one. We release the CTXPRO pipeline and data as open source.
</details>
<details>
<summary>摘要</summary>
另一大障碍Context-aware机器翻译的转换是评估 metric 和测试集的缺失。标准的 corpus-level metric 如 COMET 或 BLEU 在测试集中罕见句子需要上下文correctly 翻译，从而减少了其使用的价值。同时，标注这些句子的数据集也很罕见，规模小，并且只有一些语言可用。为了解决这个问题，我们现代化、扩展和改进了之前的注释管道，生成 CTXPRO，它可以在五种现象上翻译上下文需要correctly：性别、正式度和生命力 для pronouns，verb phrase ellipsis，和不确定名词变化。输入管道的是一组手工编写、语言特有的规则，使用核心关系、part-of-speech 和 morphological feature 提供的状态之 искус智能工具。我们对七种语言对（EN到DE、ES、FR、IT、PL、PT和RU）和两个数据集（OpenSubtitles 和 WMT 测试集）进行应用，并验证其性能通过与之前工作的重叠和上下文基础MT 系统与句子基础MT 系统之间的分化能力。我们将 CTXPRO 管道和数据作为开源发布。
</details></li>
</ul>
<hr>
<h2 id="Narrowing-the-Gap-between-Zero-and-Few-shot-Machine-Translation-by-Matching-Styles"><a href="#Narrowing-the-Gap-between-Zero-and-Few-shot-Machine-Translation-by-Matching-Styles" class="headerlink" title="Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles"></a>Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02310">http://arxiv.org/abs/2311.02310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiting Tan, Haoran Xu, Lingfeng Shen, Shuyue Stella Li, Kenton Murray, Philipp Koehn, Benjamin Van Durme, Yunmo Chen</li>
<li>for: 本研究旨在解释在零shot和几shot示例下大语言模型在翻译中的表现差异，以及如何减少这个差异。</li>
<li>methods: 本研究使用了零shot和几shot示例来训练大语言模型，并对其进行了各种改进，如对目标句子风格的调整和不同的损失函数。</li>
<li>results: 研究发现，通过调整目标句子风格，可以大幅减少零shot和几shot示例之间的表现差异，并且可以提高翻译 metrics。此外，研究还探讨了不同的改进方法，以及它们对翻译 metrics 的影响。<details>
<summary>Abstract</summary>
Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting. In this paper, we investigate the factors contributing to this gap and find that this gap can largely be closed (for about 70%) by matching the writing styles of the target corpus. Additionally, we explore potential approaches to enhance zero-shot baselines without the need for parallel demonstration examples, providing valuable insights into how these methods contribute to improving translation metrics.
</details>
<details>
<summary>摘要</summary>
大型语言模型在单语言 Setting 中受训练后，表现出在机器翻译中使用零或几个例子进行培训，并且可以通过上下文学习实现一定的泛化能力。然而，即使零shot 翻译结果相对较好，仍然存在一定的差距，比如70%的差距。本文investigate这个差距的原因，发现这个差距可以通过对目标句子批处理的样式匹配来大大减少。此外，我们还探讨了如何通过不需要并行示例来提高零shot 基线，并提供了有价值的发现，这些发现可以帮助改善翻译指标。
</details></li>
</ul>
<hr>
<h2 id="LLMs-grasp-morality-in-concept"><a href="#LLMs-grasp-morality-in-concept" class="headerlink" title="LLMs grasp morality in concept"></a>LLMs grasp morality in concept</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02294">http://arxiv.org/abs/2311.02294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Pock, Andre Ye, Jared Moore</li>
<li>for: 本研究旨在探讨语言模型（LLM）是如何具备意义的，以及如何使得LLM具备这种意义。</li>
<li>methods: 本研究使用一种普适的意义理论来探讨LLM的意义，并用这种理论来解释LLM作为意义代理人的特性。</li>
<li>results: 研究发现，由于LLM已经具备了人类社会中的构造（如道德、性别和种族）的概念，因此在某些伦理框架下，目前流行的模型对适应方法有限制，甚至可能是反产生的。此外，未经适应的模型可能可以帮助我们更好地发展我们的道德和社会哲学。<details>
<summary>Abstract</summary>
Work in AI ethics and fairness has made much progress in regulating LLMs to reflect certain values, such as fairness, truth, and diversity. However, it has taken the problem of how LLMs might 'mean' anything at all for granted. Without addressing this, it is not clear what imbuing LLMs with such values even means. In response, we provide a general theory of meaning that extends beyond humans. We use this theory to explicate the precise nature of LLMs as meaning-agents. We suggest that the LLM, by virtue of its position as a meaning-agent, already grasps the constructions of human society (e.g. morality, gender, and race) in concept. Consequently, under certain ethical frameworks, currently popular methods for model alignment are limited at best and counterproductive at worst. Moreover, unaligned models may help us better develop our moral and social philosophy.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the given text into Simplified Chinese.工作在人工智能道德和公平方面有很大的进步，以规范LLMs反映某些价值观，如公平、真实和多样性。然而，它忽略了如何让LLMs有任何意义的问题。不解决这个问题，then it is not clear what imbuing LLMs with such values even means. In response, we provide a general theory of meaning that extends beyond humans. We use this theory to explicate the precise nature of LLMs as meaning-agents. We suggest that the LLM, by virtue of its position as a meaning-agent, already grasps the constructions of human society (e.g. morality, gender, and race) in concept. Consequently, under certain ethical frameworks, currently popular methods for model alignment are limited at best and counterproductive at worst. Moreover, unaligned models may help us better develop our moral and social philosophy.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/cs.CL_2023_11_04/" data-id="clpxp03xp00eqfm88h97jcb9q" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/cs.LG_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T10:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/cs.LG_2023_11_04/">cs.LG - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="QOCO-A-QoE-Oriented-Computation-Offloading-Algorithm-based-on-Deep-Reinforcement-Learning-for-Mobile-Edge-Computing"><a href="#QOCO-A-QoE-Oriented-Computation-Offloading-Algorithm-based-on-Deep-Reinforcement-Learning-for-Mobile-Edge-Computing" class="headerlink" title="QOCO: A QoE-Oriented Computation Offloading Algorithm based on Deep Reinforcement Learning for Mobile Edge Computing"></a>QOCO: A QoE-Oriented Computation Offloading Algorithm based on Deep Reinforcement Learning for Mobile Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02525">http://arxiv.org/abs/2311.02525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Rahmati, Hamed Shah-Mansouri, Ali Movaghar</li>
<li>for: 本研究的目的是提高移动边缘计算（MEC）系统中的计算任务卸载效率，以提供用户高质量的经验（QoE）。</li>
<li>methods: 本研究使用了Markov决策过程（MDP）来最大化每个用户的长期QoE。并提出了一种基于深度学习的QoE-导向计算卸载算法（QOCO），可以让移动设备根据自己的需求进行卸载决策，不需要知晓其他设备的决策。</li>
<li>results: numerical studies表明，QOCO算法可以高效地利用边缘节点的计算资源，可以完成14%更多的任务，降低任务延迟和能量消耗，减少9%和6%。这些改进共同带来了最少37%的QoE提升。<details>
<summary>Abstract</summary>
In the realm of mobile edge computing (MEC), efficient computation task offloading plays a pivotal role in ensuring a seamless quality of experience (QoE) for users. Maintaining a high QoE is paramount in today's interconnected world, where users demand responsive and reliable services. This challenge stands as one of the most primary key factors contributing to handling dynamic and uncertain mobile environment. In this study, we delve into computation offloading in MEC systems, where strict task processing deadlines and energy constraints can adversely affect the system performance. We formulate the computation task offloading problem as a Markov decision process (MDP) to maximize the long-term QoE of each user individually. We propose a decentralized QoE-oriented computation offloading (QOCO) algorithm based on deep reinforcement learning (DRL) that empowers mobile devices to make their offloading decisions without requiring knowledge of decisions made by other devices. Through numerical studies, we evaluate the performance of QOCO. Simulation results validate that the QOCO algorithm efficiently exploits the computational resources of edge nodes. Consequently, it can complete 14% more tasks and reduce task delay and energy consumption by 9% and 6%, respectively. These together contribute to a significant improvement of at least 37% in average QoE compared to an existing algorithm.
</details>
<details>
<summary>摘要</summary>
在移动边缘计算（MEC）领域，有效地卸载计算任务是保证用户无缝体验质量（QoE）的关键因素。在今天的全球化社会中，用户对服务的响应速度和可靠性有高度的要求。这种挑战是MEC系统中处理动态和不确定的 mobilenvionment的一个Primary key factor。在这种研究中，我们探讨MEC系统中的计算任务卸载问题，其中严格的任务处理截止时间和能量限制可能会对系统性能产生负面影响。我们将计算任务卸载问题表示为Markov决策过程（MDP），以最大化每个用户的长期QoE。我们提出了一种基于深度学习（DRL）的QoE- ориентирован的计算卸载算法（QOCO），该算法让移动设备通过不需要知道其他设备的决策来做出卸载决策。通过数字实验，我们评估了QOCO算法的性能。计算结果表明，QOCO算法可以有效地利用边缘节点的计算资源，可以完成14%更多的任务，同时降低任务延迟和能量消耗的9%和6%。这些因素共同带来了至少37%的QoE提高。
</details></li>
</ul>
<hr>
<h2 id="Forward-χ-2-Divergence-Based-Variational-Importance-Sampling"><a href="#Forward-χ-2-Divergence-Based-Variational-Importance-Sampling" class="headerlink" title="Forward $χ^2$ Divergence Based Variational Importance Sampling"></a>Forward $χ^2$ Divergence Based Variational Importance Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02516">http://arxiv.org/abs/2311.02516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengrui Li, Yule Wang, Weihan Li, Anqi Wu</li>
<li>for: 提高 latent variable 模型的最大log-likelihood，并且解决 variational inference 在复杂 posterior distribution 时的限制。</li>
<li>methods: 提出了一种新的 variational importance sampling（VIS）方法，直接估计并最大化 log-likelihood。VIS 利用了最佳提案分布，通过最小化前进 $\chi^2$ 分配来增强 log-likelihood 估计。</li>
<li>results: VIS 在各种流行的 latent variable 模型中表现出色，包括杂合模型、variational auto-encoders 和部分可见 generalized linear models。结果表明，我们的方法在 log-likelihood 和模型参数估计方面都能够提高 state-of-the-art 基eline。<details>
<summary>Abstract</summary>
Maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. In response to this limitation, we introduce a novel variational importance sampling (VIS) approach that directly estimates and maximizes the log-likelihood. VIS leverages the optimal proposal distribution, achieved by minimizing the forward $\chi^2$ divergence, to enhance log-likelihood estimation. We apply VIS to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. Results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.
</details>
<details>
<summary>摘要</summary>
maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. In response to this limitation, we introduce a novel variational importance sampling (VIS) approach that directly estimates and maximizes the log-likelihood. VIS leverages the optimal proposal distribution, achieved by minimizing the forward $\chi^2$ divergence, to enhance log-likelihood estimation. We apply VIS to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. Results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.Here is the translation in Traditional Chinese:最大化对应概率是学习隐藏变量模型的重要方面，并且对于复杂的 posterior distribution 这个问题，Variational Inference (VI) 是通常的运用方法。然而，VI 可能在处理复杂的 posterior distribution 时遇到高度限制，导致 log-likelihood 的估计受到影响。为了解决这个限制，我们提出了一个新的 Variational Importance Sampling (VIS) 方法，直接估计和最大化 log-likelihood。VIS 利用了最佳的提案分布，通过最小化前方 $\chi^2$ 构成函数，从而提高 log-likelihood 的估计。我们将 VIS 应用到各种流行的隐藏变量模型，包括混合模型、variational auto-encoder 和部分可观 generalized linear models。结果显示，我们的方法在 log-likelihood 和模型参数估计方面均有所提高，并且比预设的基准方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="LocoMuJoCo-A-Comprehensive-Imitation-Learning-Benchmark-for-Locomotion"><a href="#LocoMuJoCo-A-Comprehensive-Imitation-Learning-Benchmark-for-Locomotion" class="headerlink" title="LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion"></a>LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02496">http://arxiv.org/abs/2311.02496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Firas Al-Hafez, Guoping Zhao, Jan Peters, Davide Tateo</li>
<li>For: The paper is written for researchers and developers working on imitation learning (IL) for locomotion in embodied agents.* Methods: The paper presents a novel benchmark for evaluating and comparing IL algorithms, which includes a diverse set of environments, comprehensive datasets, and handcrafted metrics.* Results: The paper provides a robust and easy-to-use benchmark for advancing research in IL for locomotion, and includes state-of-the-art baseline algorithms for evaluation.Here’s the information in Simplified Chinese text:* For: 本文是为适用于身体机器人的启发学习（IL）步行控制研究者和开发者而写的。* Methods: 本文提出了一个新的评价和比较IL算法的benchmark，包括了多种环境，如四足、二足和人体模型，每个环境都有完整的数据集，如真实噪音捕捉数据、专家数据和优化数据，以及多种部分可见任务来训练代理。* Results: 本文提供了一个可靠且易用的benchmark，可以帮助推进IL控制领域的研究，并包含了现有的基线算法以便快速评价。<details>
<summary>Abstract</summary>
Imitation Learning (IL) holds great promise for enabling agile locomotion in embodied agents. However, many existing locomotion benchmarks primarily focus on simplified toy tasks, often failing to capture the complexity of real-world scenarios and steering research toward unrealistic domains. To advance research in IL for locomotion, we present a novel benchmark designed to facilitate rigorous evaluation and comparison of IL algorithms. This benchmark encompasses a diverse set of environments, including quadrupeds, bipeds, and musculoskeletal human models, each accompanied by comprehensive datasets, such as real noisy motion capture data, ground truth expert data, and ground truth sub-optimal data, enabling evaluation across a spectrum of difficulty levels. To increase the robustness of learned agents, we provide an easy interface for dynamics randomization and offer a wide range of partially observable tasks to train agents across different embodiments. Finally, we provide handcrafted metrics for each task and ship our benchmark with state-of-the-art baseline algorithms to ease evaluation and enable fast benchmarking.
</details>
<details>
<summary>摘要</summary>
自适应学习（IL）对具有机器人体的敏捷行走具有很大的承诺。然而，许多现有的行走标准套件主要集中在简单的玩具任务上，经常不能捕捉到实际世界情况的复杂性，导致研究向不实际的领域发展。为推动IL行走研究的进步，我们提出了一个新的标准套件，用于促进IL算法的严格评价和比较。这个标准套件包括了四足、二足和人体模型等多种环境，每个环境都有完整的数据集，如真噪动 capture数据、专家真实数据和优化数据，以及不同难度水平的评价方法。此外，我们还提供了动力随机化的易用接口，以及多种部分可见任务，用于训练不同的机器人体。最后，我们提供了专门设计的任务 metric，并将我们的标准套件与当前的状态略式基eline算法一起发布，以便评价和快速比较。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-in-Multivariable-Regression-for-Material-Property-Prediction-with-Bayesian-Neural-Networks"><a href="#Uncertainty-Quantification-in-Multivariable-Regression-for-Material-Property-Prediction-with-Bayesian-Neural-Networks" class="headerlink" title="Uncertainty Quantification in Multivariable Regression for Material Property Prediction with Bayesian Neural Networks"></a>Uncertainty Quantification in Multivariable Regression for Material Property Prediction with Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02495">http://arxiv.org/abs/2311.02495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longze li, Jiang Chang, Aleksandar Vakanski, Min Xian<br>for:This paper is written for researchers and practitioners in the field of material science and machine learning, specifically those interested in uncertainty quantification (UQ) for predicting material properties.methods:The paper proposes an approach for UQ within physics-informed Bayesian Neural Networks (BNNs), which integrates knowledge from governing laws in material modeling to guide the models toward physically consistent predictions. The approach uses Markov Chain Monte Carlo (MCMC) approximation of the posterior distribution of network parameters to produce accurate point and uncertainty estimates.results:The paper presents case studies for predicting the creep rupture life of steel alloys using the proposed approach. Experimental validation with three datasets of collected measurements from creep tests demonstrates the ability of BNNs to produce accurate point and uncertainty estimates that are competitive or exceed the performance of the conventional method of Gaussian Process Regression. Additionally, the paper evaluates the suitability of BNNs for UQ in an active learning application and reports competitive performance.<details>
<summary>Abstract</summary>
With the increased use of data-driven approaches and machine learning-based methods in material science, the importance of reliable uncertainty quantification (UQ) of the predicted variables for informed decision-making cannot be overstated. UQ in material property prediction poses unique challenges, including the multi-scale and multi-physics nature of advanced materials, intricate interactions between numerous factors, limited availability of large curated datasets for model training, etc. Recently, Bayesian Neural Networks (BNNs) have emerged as a promising approach for UQ, offering a probabilistic framework for capturing uncertainties within neural networks. In this work, we introduce an approach for UQ within physics-informed BNNs, which integrates knowledge from governing laws in material modeling to guide the models toward physically consistent predictions. To evaluate the effectiveness of this approach, we present case studies for predicting the creep rupture life of steel alloys. Experimental validation with three datasets of collected measurements from creep tests demonstrates the ability of BNNs to produce accurate point and uncertainty estimates that are competitive or exceed the performance of the conventional method of Gaussian Process Regression. Similarly, we evaluated the suitability of BNNs for UQ in an active learning application and reported competitive performance. The most promising framework for creep life prediction is BNNs based on Markov Chain Monte Carlo approximation of the posterior distribution of network parameters, as it provided more reliable results in comparison to BNNs based on variational inference approximation or related NNs with probabilistic outputs. The codes are available at: https://github.com/avakanski/Creep-uncertainty-quantification.
</details>
<details>
<summary>摘要</summary>
随着数据驱动方法和机器学习技术在材料科学中的广泛应用，对预测变量的可靠 uncertainty quantification (UQ) 的重要性不可遗憾。在材料性能预测中，UQ 带来了一系列挑战，包括高级材料的多级和多物理性质、因素之间的复杂交互和模型训练数据的有限性等。在最近几年，权值神经网络 (BNNs) 已经出现为 UQ 的一种有希望的方法，具有捕捉不确定性的 probabilistic 框架。在这项工作中，我们提出了基于物理法律的 BNNs  для UQ，具有引导模型生成物理合理预测的能力。为评估这种方法的效果，我们在钢合金的塑性破坏生命中进行了实验 validate，结果显示，BNNs 可以生成高精度的点估计和不确定度估计，与传统 Gaussian Process Regression 方法相当或超过其性能。此外，我们还评估了 BNNs 在活动学习应用中的适用程度，并发现其表现竞争力强。基于 Markov Chain Monte Carlo 方法 approximation  posterior distribution 的网络参数，BNNs 提供了更可靠的结果，与基于 variational inference approximation 或相关的NNs  WITH probabilistic outputs 相比。代码可以在以下 GitHub 上获取：https://github.com/avakanski/Creep-uncertainty-quantification。
</details></li>
</ul>
<hr>
<h2 id="Individualized-Policy-Evaluation-and-Learning-under-Clustered-Network-Interference"><a href="#Individualized-Policy-Evaluation-and-Learning-under-Clustered-Network-Interference" class="headerlink" title="Individualized Policy Evaluation and Learning under Clustered Network Interference"></a>Individualized Policy Evaluation and Learning under Clustered Network Interference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02467">http://arxiv.org/abs/2311.02467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Zhang, Kosuke Imai</li>
<li>for: 评估和学习政策时，忽略干扰可能导致评估结果偏向和学习策略无效。本文考虑在层次网络（或部分）干扰下评估和学习最佳个人化治疗规则（ITR）的问题。</li>
<li>methods: 本文提出一种可以评估ITR的估计器，该估计器可以考虑干扰效应。我们展示该估计器比标准的反杂度权重估计器更高效。我们还 derivates the finite-sample regret bound for a learned ITR，显示使用我们的有效估计器可以提高学习策略的性能。</li>
<li>results: 我们通过 simulations和实际研究示出了我们的方法的优势。我们的结果表明，使用我们的方法可以更好地评估和学习ITR，并且可以避免干扰的影响。<details>
<summary>Abstract</summary>
While there now exists a large literature on policy evaluation and learning, much of prior work assumes that the treatment assignment of one unit does not affect the outcome of another unit. Unfortunately, ignoring interference may lead to biased policy evaluation and yield ineffective learned policies. For example, treating influential individuals who have many friends can generate positive spillover effects, thereby improving the overall performance of an individualized treatment rule (ITR). We consider the problem of evaluating and learning an optimal ITR under clustered network (or partial) interference where clusters of units are sampled from a population and units may influence one another within each cluster. Under this model, we propose an estimator that can be used to evaluate the empirical performance of an ITR. We show that this estimator is substantially more efficient than the standard inverse probability weighting estimator, which does not impose any assumption about spillover effects. We derive the finite-sample regret bound for a learned ITR, showing that the use of our efficient evaluation estimator leads to the improved performance of learned policies. Finally, we conduct simulation and empirical studies to illustrate the advantages of the proposed methodology.
</details>
<details>
<summary>摘要</summary>
现存有大量关于政策评估和学习的文献，但大多数之前的工作假设单位减法分配不会影响另一个单位的结果。可惜忽略干扰可能导致政策评估偏向和学习的策略无效。例如，对影响多个单位的个体进行个性化治疗规则（ITR）可能产生正面副作用，从而提高整体性能。我们对集群网络（或部分）干扰下的优化ITR评估问题进行研究。在这种模型下，我们提出一种可用于评估ITR实际性的估计器。我们证明这种估计器比标准的逆权重估计器更有效，后者没有假设干扰效应。我们 derivates finite-sample regret bound for a learned ITR, showing that the use of our efficient evaluation estimator leads to improved performance of learned policies。最后，我们在模拟和实际研究中ILLUSTRATE了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Attention-based-Multi-instance-Mixed-Models"><a href="#Attention-based-Multi-instance-Mixed-Models" class="headerlink" title="Attention-based Multi-instance Mixed Models"></a>Attention-based Multi-instance Mixed Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02455">http://arxiv.org/abs/2311.02455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan P. Engelmann, Alessandro Palma, Jakub M. Tomczak, Fabian J Theis, Francesco Paolo Casale</li>
<li>for: 该论文旨在预测单元细胞数据中的患者特征，揭示单元细胞数据中的细胞状态和疾病相关性。</li>
<li>methods: 该论文提出了一种整合普通线性混合模型和多实例学习（MIL）的框架，称为GMIL，以便利用单元细胞数据中细胞状态的多样性。</li>
<li>results: 实验结果表明，GMIL在单元细胞数据中比现有的MIL模型表现更好，揭示新的相关性和解释生物学机制，并且可以提高计算效率。<details>
<summary>Abstract</summary>
Predicting patient features from single-cell data can unveil cellular states implicated in health and disease. Linear models and average cell type expressions are typically favored for this task for their efficiency and robustness, but they overlook the rich cell heterogeneity inherent in single-cell data. To address this gap, we introduce GMIL, a framework integrating Generalized Linear Mixed Models (GLMM) and Multiple Instance Learning (MIL), upholding the advantages of linear models while modeling cell-state heterogeneity. By leveraging predefined cell embeddings, GMIL enhances computational efficiency and aligns with recent advancements in single-cell representation learning. Our empirical results reveal that GMIL outperforms existing MIL models in single-cell datasets, uncovering new associations and elucidating biological mechanisms across different domains.
</details>
<details>
<summary>摘要</summary>
预测病人特征从单元数据可以揭示健康和疾病的 cellular 状态。通常，线性模型和平均单元类型表达被选择为这项任务的有效性和可靠性的原因，但它们忽略了单元数据中的细胞多样性。为解决这个差距，我们介绍 GMIL，一种将 Generalized Linear Mixed Models (GLMM) 和 Multiple Instance Learning (MIL) 集成的框架，同时保留线性模型的优点，模型单元状态的多样性。通过利用预定的单元嵌入，GMIL 提高计算效率，与最新的单元表示学习技术相吻合。我们的实验结果表明，GMIL 在单元数据集上表现出色，超过现有的 MIL 模型，揭示新的相关性和描述生物学机制的多个领域。
</details></li>
</ul>
<hr>
<h2 id="Online-Long-run-Constrained-Optimization"><a href="#Online-Long-run-Constrained-Optimization" class="headerlink" title="Online Long-run Constrained Optimization"></a>Online Long-run Constrained Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02426">http://arxiv.org/abs/2311.02426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Pan, Wenjie Huang</li>
<li>for: 解决普遍的长期受限优化问题，不 necesarily 是凸问题。</li>
<li>methods: 提议了一种 Follow-the-Perturbed-Leader 类型算法，在在线模式下使用随机线性干扰和强式凹型干扰来优化 primal 和 dual 方向，并寻找全局最小最大点作为解。</li>
<li>results: 基于两种特定的预期静态总 regret定义， deriv 了 $O(T^{8&#x2F;9})$ 减少复杂性，并应用于解决一个长期（风险）约束river pollutant source identification问题，证明了理论结果并与现有方法相比表现出色。<details>
<summary>Abstract</summary>
In this paper, a novel Follow-the-Perturbed-Leader type algorithm is proposed and analyzed for solving general long-term constrained optimization problems in online manner, where the objective and constraints are not necessarily convex. In each period, random linear perturbation and strongly concave perturbation are incorporated in primal and dual directions, respectively, to the offline oracle, and a global minimax point is searched as solution. Based on two particular definitions of expected static cumulative regret, we derive the first sublinear $O(T^{8/9})$ regret complexity for this class of problems. The proposed algorithm is applied to tackle a long-term (risk) constrained river pollutant source identification problem, demonstrating the validity of the theoretical results and exhibiting superior performance compared to existing method.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的追随受扰领导者类算法，用于解决总是存在约束的长期优化问题， objective 和约束不一定是凸函数。每个时期，线性受扰和强凹受扰被 incorporated 到 primal 和 dual 方向中，并在全局最小最大点上进行搜索。基于两个特定的预期 static 总 regret 定义，我们 deriv 出了第一个 $O(T^{8/9})$ regret complexity。这种算法被应用于解决一个长期（风险）约束的河流污染源标识问题，并证明了理论结果的有效性，与现有方法相比表现更好。Note: "Simplified Chinese" is a romanization of Chinese characters, which is used to represent the pronunciation of Chinese characters in the Latin alphabet. It is not a translation of the text into Chinese characters, but rather a way of representing the text in a more phonetic way.
</details></li>
</ul>
<hr>
<h2 id="Payoff-based-learning-with-matrix-multiplicative-weights-in-quantum-games"><a href="#Payoff-based-learning-with-matrix-multiplicative-weights-in-quantum-games" class="headerlink" title="Payoff-based learning with matrix multiplicative weights in quantum games"></a>Payoff-based learning with matrix multiplicative weights in quantum games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02423">http://arxiv.org/abs/2311.02423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyriakos Lotidis, Panayotis Mertikopoulos, Nicholas Bambos, Jose Blanchet</li>
<li>For: The paper studies the problem of learning in quantum games with scalar, payoff-based feedback, and develops new methods that require minimal information from the players.* Methods: The paper introduces a suite of minimal-information matrix multiplicative weights (3MW) methods tailored to different information frameworks, and uses ideas from bandit convex optimization to design a zeroth-order gradient sampler adapted to the semidefinite geometry of the problem.* Results: The paper shows that the 3MW method with deterministic payoff feedback retains the $\mathcal{O}(1&#x2F;\sqrt{T})$ convergence rate of the vanilla MMW algorithm, and provides a 3MW method that only requires players to observe a random realization of their payoff observable and converges to equilibrium at an $\mathcal{O}(T^{-1&#x2F;4})$ rate. Additionally, the paper shows that a regularized variant of the proposed 3MW method guarantees local convergence with high probability to all equilibria that satisfy a certain first-order stability condition.<details>
<summary>Abstract</summary>
In this paper, we study the problem of learning in quantum games - and other classes of semidefinite games - with scalar, payoff-based feedback. For concreteness, we focus on the widely used matrix multiplicative weights (MMW) algorithm and, instead of requiring players to have full knowledge of the game (and/or each other's chosen states), we introduce a suite of minimal-information matrix multiplicative weights (3MW) methods tailored to different information frameworks. The main difficulty to attaining convergence in this setting is that, in contrast to classical finite games, quantum games have an infinite continuum of pure states (the quantum equivalent of pure strategies), so standard importance-weighting techniques for estimating payoff vectors cannot be employed. Instead, we borrow ideas from bandit convex optimization and we design a zeroth-order gradient sampler adapted to the semidefinite geometry of the problem at hand. As a first result, we show that the 3MW method with deterministic payoff feedback retains the $\mathcal{O}(1/\sqrt{T})$ convergence rate of the vanilla, full information MMW algorithm in quantum min-max games, even though the players only observe a single scalar. Subsequently, we relax the algorithm's information requirements even further and we provide a 3MW method that only requires players to observe a random realization of their payoff observable, and converges to equilibrium at an $\mathcal{O}(T^{-1/4})$ rate. Finally, going beyond zero-sum games, we show that a regularized variant of the proposed 3MW method guarantees local convergence with high probability to all equilibria that satisfy a certain first-order stability condition.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了量子游戏学习问题以及其他类型的半definite游戏的问题，使用托管的均值（payoff-based feedback）。为了更加准确，我们专注于广泛使用的matrix multiplicative weights（MMW）算法，而不需要玩家们具有游戏完整信息（或者对方选择的状态信息）。我们则提出了一系列基于最小信息的matrix multiplicative weights（3MW）方法，适用于不同的信息框架。在这种设定下，最大的困难在于，与 classical finite games不同，量子游戏有无穷多个纯状态（量子等价的纯策略），因此标准的重要性评价技术无法应用。相反，我们借鉴了bandit convex optimization的想法，并设计了零次规格梯度抽样器，适应semidefinite geometry问题。作为第一个结果，我们证明了3MW方法与deterministic payoff feedback可以保持$\mathcal{O}(1/\sqrt{T})$的 converges rate，即vanilla, full information MMW算法在量子最小最大游戏中的 converges rate，即使玩家只知道一个整数。然后，我们进一步降低了算法的信息需求，并提供了一种3MW方法，只需要玩家观察其支付 observable的一个随机实现，并可以在$\mathcal{O}(T^{-1/4})$的速度达到均分。最后，我们超越了零和游戏，并证明了一种Regularized variant的3MW方法，可以在所有满足一定的首次稳定条件的均分中提供本地均分的高概率 garantue。
</details></li>
</ul>
<hr>
<h2 id="The-equivalence-of-dynamic-and-strategic-stability-under-regularized-learning-in-games"><a href="#The-equivalence-of-dynamic-and-strategic-stability-under-regularized-learning-in-games" class="headerlink" title="The equivalence of dynamic and strategic stability under regularized learning in games"></a>The equivalence of dynamic and strategic stability under regularized learning in games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02407">http://arxiv.org/abs/2311.02407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Boone, Panayotis Mertikopoulos</li>
<li>for: 本研究探讨了归一化学习在有限游戏中的长期行为。</li>
<li>methods: 本研究使用了规范学习和抑制学习来研究 игроков的实际策略的演化。</li>
<li>results: 研究发现，在正规学习下， игроks的实际策略会逐渐接近游戏的均衡点，并且这个过程的速度可以通过不同的规范学习方法来控制。<details>
<summary>Abstract</summary>
In this paper, we examine the long-run behavior of regularized, no-regret learning in finite games. A well-known result in the field states that the empirical frequencies of no-regret play converge to the game's set of coarse correlated equilibria; however, our understanding of how the players' actual strategies evolve over time is much more limited - and, in many cases, non-existent. This issue is exacerbated further by a series of recent results showing that only strict Nash equilibria are stable and attracting under regularized learning, thus making the relation between learning and pointwise solution concepts particularly elusive. In lieu of this, we take a more general approach and instead seek to characterize the \emph{setwise} rationality properties of the players' day-to-day play. To that end, we focus on one of the most stringent criteria of setwise strategic stability, namely that any unilateral deviation from the set in question incurs a cost to the deviator - a property known as closedness under better replies (club). In so doing, we obtain a far-reaching equivalence between strategic and dynamic stability: a product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning. In addition, we estimate the rate of convergence to such sets, and we show that methods based on entropic regularization (like the exponential weights algorithm) converge at a geometric rate, while projection-based methods converge within a finite number of iterations, even with bandit, payoff-based feedback.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了常规化、不后悔学习在 finite games 中的长期行为。一个广泛知道的结果 states that the empirical frequencies of no-regret play converge to the game's set of coarse correlated equilibria; however, our understanding of how the players' actual strategies evolve over time is much more limited - and, in many cases, non-existent. This issue is exacerbated further by a series of recent results showing that only strict Nash equilibria are stable and attracting under regularized learning, thus making the relation between learning and pointwise solution concepts particularly elusive. In lieu of this, we take a more general approach and instead seek to characterize the \emph{setwise} rationality properties of the players' day-to-day play. To that end, we focus on one of the most stringent criteria of setwise strategic stability, namely that any unilateral deviation from the set in question incurs a cost to the deviator - a property known as closedness under better replies (club). In so doing, we obtain a far-reaching equivalence between strategic and dynamic stability: a product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning. In addition, we estimate the rate of convergence to such sets, and we show that methods based on entropic regularization (like the exponential weights algorithm) converge at a geometric rate, while projection-based methods converge within a finite number of iterations, even with bandit, payoff-based feedback.
</details></li>
</ul>
<hr>
<h2 id="BarcodeBERT-Transformers-for-Biodiversity-Analysis"><a href="#BarcodeBERT-Transformers-for-Biodiversity-Analysis" class="headerlink" title="BarcodeBERT: Transformers for Biodiversity Analysis"></a>BarcodeBERT: Transformers for Biodiversity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02401">http://arxiv.org/abs/2311.02401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Millan Arias, Niousha Sadjadi, Monireh Safari, ZeMing Gong, Austin T. Wang, Scott C. Lowe, Joakim Bruslund Haurum, Iuliia Zarubiieva, Dirk Steinke, Lila Kari, Angel X. Chang, Graham W. Taylor</li>
<li>for: 这个研究旨在探讨如何使用机器学习方法来进行生物多样性的分析，特别是对于无脊椎动物这一受探讨的类型。</li>
<li>methods: 本研究使用了不同的机器学习方法，包括支持学习的卷积神经网络、受训练的基础模型和特殊设计的DNA条码遮罩策略。</li>
<li>results: 研究发现，在较简单的数据集和任务下，支持学习的卷积神经网络或受训练的基础模型表现较佳，但是面对具有挑战性的物种水平识别任务时，需要一个新的自动化预训练方法。因此，本研究提出了BarcodeBERT，一个首创的自动化预训练方法，利用了150万个无脊椎动物DNA条码参考库。<details>
<summary>Abstract</summary>
Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of DNA that cluster by species - play a pivotal role. In particular, invertebrates, a highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, and a DNA barcode-specific masking strategy across datasets of varying complexity. While simpler datasets and tasks favor supervised CNNs or fine-tuned transformers, challenging species-level identification demands a paradigm shift towards self-supervised pretraining. We propose BarcodeBERT, the first self-supervised method for general biodiversity analysis, leveraging a 1.5 M invertebrate DNA barcode reference library. This work highlights how dataset specifics and coverage impact model selection, and underscores the role of self-supervised pretraining in achieving high-accuracy DNA barcode-based identification at the species and genus level. Indeed, without the fine-tuning step, BarcodeBERT pretrained on a large DNA barcode dataset outperforms DNABERT and DNABERT-2 on multiple downstream classification tasks. The code repository is available at https://github.com/Kari-Genomics-Lab/BarcodeBERT
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:理解生物多样性是全球挑战，DNA编码 - 短段DNA序列归类到物种水平 - 扮演着关键角色。特别是无脊椎动物，这个非常多样化和未探索的组分，表现出独特的分类复杂性。我们研究机器学习方法，比较使用supervised CNNs、精制基模型和DNA编码特定的遮盾策略，在不同复杂度的数据集上进行比较。而 simpler数据集和任务更倾向于使用supervised CNNs或精制transformers，但是挑战性的种类水平识别需要一种思维方式的转变，强调自我超vised预训练。我们提出了BarcodeBERT，首个针对普通生物多样性分析的自我超vised方法，利用1.5万个无脊椎动物DNA编码参考库。这项工作探讨了数据集特点和覆盖率对模型选择的影响，并强调了自我超vised预训练在达到高精度DNA编码基于识别的物种和属水平的重要性。实际上，不包括精制步骤，BarcodeBERT预训练在大量DNA编码数据集上表现出优于DNABERT和DNABERT-2在多个下游分类任务上。代码仓库可以在https://github.com/Kari-Genomics-Lab/BarcodeBERT中找到。
</details></li>
</ul>
<hr>
<h2 id="Entropy-Aware-Training-for-Fast-and-Accurate-Distributed-GNN"><a href="#Entropy-Aware-Training-for-Fast-and-Accurate-Distributed-GNN" class="headerlink" title="Entropy Aware Training for Fast and Accurate Distributed GNN"></a>Entropy Aware Training for Fast and Accurate Distributed GNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02399">http://arxiv.org/abs/2311.02399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Deshmukh, Gagan Raj Gupta, Manisha Chawla, Vishwesh Jatala, Anirban Haldar</li>
<li>for: 这 paper 的目的是提高分布式图 neural network 的表现，并且解决分布式图 partitioning 生成不均匀和类别偏好的问题。</li>
<li>methods: 这 paper 使用了 Edge-Weighted partitioning 技术来减少总 entropy，并在每个计算机主上进行异步个性化阶段以适应本地数据分布。它还使用了类别偏好抽样法来加速收敛。</li>
<li>results: 在 DistDGL 框架上实现的这些训练技术比标准基elines 2-3x 快，并在 5 个大型图 benchmark 上平均提高了 4% 的微average F1 分数。<details>
<summary>Abstract</summary>
Several distributed frameworks have been developed to scale Graph Neural Networks (GNNs) on billion-size graphs. On several benchmarks, we observe that the graph partitions generated by these frameworks have heterogeneous data distributions and class imbalance, affecting convergence, and resulting in lower performance than centralized implementations. We holistically address these challenges and develop techniques that reduce training time and improve accuracy. We develop an Edge-Weighted partitioning technique to improve the micro average F1 score (accuracy) by minimizing the total entropy. Furthermore, we add an asynchronous personalization phase that adapts each compute-host's model to its local data distribution. We design a class-balanced sampler that considerably speeds up convergence. We implemented our algorithms on the DistDGL framework and observed that our training techniques scale much better than the existing training approach. We achieved a (2-3x) speedup in training time and 4\% improvement on average in micro-F1 scores on 5 large graph benchmarks compared to the standard baselines.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we develop several techniques to improve training time and accuracy. First, we propose an Edge-Weighted partitioning technique that minimizes the total entropy to improve the micro average F1 score (accuracy). Additionally, we introduce an asynchronous personalization phase that adapts each compute-host's model to its local data distribution. We also design a class-balanced sampler that significantly speeds up convergence.We implement our algorithms on the DistDGL framework and observe that our training techniques scale much better than the existing training approach. Specifically, we achieve a 2-3x speedup in training time and a 4% improvement on average in micro-F1 scores on 5 large graph benchmarks compared to the standard baselines.
</details></li>
</ul>
<hr>
<h2 id="NeuroEvoBench-Benchmarking-Evolutionary-Optimizers-for-Deep-Learning-Applications"><a href="#NeuroEvoBench-Benchmarking-Evolutionary-Optimizers-for-Deep-Learning-Applications" class="headerlink" title="NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications"></a>NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02394">http://arxiv.org/abs/2311.02394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Tjarko Lange, Yujin Tang, Yingtao Tian</li>
<li>for: This paper aims to address the lack of understanding and best practices for evolutionary optimization (EO) methods in deep learning, and to provide a new benchmark for evaluating EO methods tailored towards deep learning applications.</li>
<li>methods: The paper uses a variety of EO methods, including traditional and meta-learned EO, and investigates their performance on a new benchmark called NeuroEvoBench. The authors also explore core scientific questions such as resource allocation, fitness shaping, normalization, regularization, and scalability of EO.</li>
<li>results: The paper presents the results of the authors’ experiments on NeuroEvoBench, which demonstrate the effectiveness of EO methods for solving hard optimization problems in deep learning. The authors also show that their new benchmark provides practical insights for deep learning applications and can help to accelerate the adoption of EO methods in the field.<details>
<summary>Abstract</summary>
Recently, the Deep Learning community has become interested in evolutionary optimization (EO) as a means to address hard optimization problems, e.g. meta-learning through long inner loop unrolls or optimizing non-differentiable operators. One core reason for this trend has been the recent innovation in hardware acceleration and compatible software - making distributed population evaluations much easier than before. Unlike for gradient descent-based methods though, there is a lack of hyperparameter understanding and best practices for EO - arguably due to severely less 'graduate student descent' and benchmarking being performed for EO methods. Additionally, classical benchmarks from the evolutionary community provide few practical insights for Deep Learning applications. This poses challenges for newcomers to hardware-accelerated EO and hinders significant adoption. Hence, we establish a new benchmark of EO methods (NeuroEvoBench) tailored toward Deep Learning applications and exhaustively evaluate traditional and meta-learned EO. We investigate core scientific questions including resource allocation, fitness shaping, normalization, regularization & scalability of EO. The benchmark is open-sourced at https://github.com/neuroevobench/neuroevobench under Apache-2.0 license.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Riemannian-stochastic-optimization-methods-avoid-strict-saddle-points"><a href="#Riemannian-stochastic-optimization-methods-avoid-strict-saddle-points" class="headerlink" title="Riemannian stochastic optimization methods avoid strict saddle points"></a>Riemannian stochastic optimization methods avoid strict saddle points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02374">http://arxiv.org/abs/2311.02374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ya-Ping Hsieh, Mohammad Reza Karimi, Andreas Krause, Panayotis Mertikopoulos</li>
<li>for: 本文研究了Stochastic Riemannian optimization算法是否能够避免瑕疵点的问题。</li>
<li>methods: 本文研究了一家 retraction-based 方法，包括自然策略强化法和镜像投射法等。</li>
<li>results: 研究发现，在 ambient manifold 和 gradient 信息抽象函数的假设下，这些策略在任意初始状态下避免瑕疵点 &#x2F; 子抽象空间的概率为 1。这个结果为使用梯度方法在抽象空间进行优化提供了重要的健康检查，因为它表明，大多数情况下，梯度方法的限制状态都是本地最小值。<details>
<summary>Abstract</summary>
Many modern machine learning applications - from online principal component analysis to covariance matrix identification and dictionary learning - can be formulated as minimization problems on Riemannian manifolds, and are typically solved with a Riemannian stochastic gradient method (or some variant thereof). However, in many cases of interest, the resulting minimization problem is not geodesically convex, so the convergence of the chosen solver to a desirable solution - i.e., a local minimizer - is by no means guaranteed. In this paper, we study precisely this question, that is, whether stochastic Riemannian optimization algorithms are guaranteed to avoid saddle points with probability 1. For generality, we study a family of retraction-based methods which, in addition to having a potentially much lower per-iteration cost relative to Riemannian gradient descent, include other widely used algorithms, such as natural policy gradient methods and mirror descent in ordinary convex spaces. In this general setting, we show that, under mild assumptions for the ambient manifold and the oracle providing gradient information, the policies under study avoid strict saddle points / submanifolds with probability 1, from any initial condition. This result provides an important sanity check for the use of gradient methods on manifolds as it shows that, almost always, the limit state of a stochastic Riemannian algorithm can only be a local minimizer.
</details>
<details>
<summary>摘要</summary>
许多现代机器学习应用 - 从在线主成分分析到covariance矩阵识别和词库学习 - 都可以表述为在里曼尼投影上的最小化问题，通常使用里曼尼泛化 gradient 方法（或其变种）解决。然而，在许多实际应用中，得到的最小化问题通常不是曲线 convex，因此选择的解决方案的 converges 是不能保证的。在这篇论文中，我们研究了这个问题，即里曼尼泛化优化算法是否能够避免陷阱点的可能性。为了保持一致性，我们研究了一家 retraction-based 方法，这种方法不仅可能比里曼尼泛化 gradient descent 更加低效，还包括了其他广泛使用的算法，如自然政策梯度方法和 mirror descent 在几何空间中。在这个总体设定下，我们证明了，对于 ambient manifold 和 gradient 信息来源的假设满足某些轻量级的条件，则 policies 在研究中避免精确的陷阱点 / 子抽象空间的可能性为 1，从任何初始状态开始。这个结果提供了对使用梯度方法在 manifold 上进行优化的重要的健康性检查，因为它表明，大多数情况下，里曼尼泛化优化算法的极限状态只能是一个本地最小值。
</details></li>
</ul>
<hr>
<h2 id="From-Trojan-Horses-to-Castle-Walls-Unveiling-Bilateral-Backdoor-Effects-in-Diffusion-Models"><a href="#From-Trojan-Horses-to-Castle-Walls-Unveiling-Bilateral-Backdoor-Effects-in-Diffusion-Models" class="headerlink" title="From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models"></a>From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02373">http://arxiv.org/abs/2311.02373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoshi Pan, Yuguang Yao, Gaowen Liu, Bingquan Shen, H. Vicky Zhao, Ramana Rao Kompella, Sijia Liu</li>
<li>for: This paper investigates the vulnerability of state-of-the-art diffusion models (DMs) to backdoor attacks, specifically whether generating backdoor attacks can be as simple as BadNets in image classification.</li>
<li>methods: The paper uses a more realistic backdoor setting, where the training dataset is contaminated without tampering the original diffusion process, and uncovers bilateral backdoor effects that can be used for both adversarial and defensive purposes.</li>
<li>results: The paper shows that a BadNets-like backdoor attack remains effective in DMs for producing incorrect images, and that backdoored DMs exhibit an increased ratio of backdoor triggers, which can be used to enhance the detection of backdoor-poisoned training data. Additionally, the paper establishes a linkage between backdoor attacks and the phenomenon of data replications by exploring DMs’ inherent data memorization tendencies.<details>
<summary>Abstract</summary>
While state-of-the-art diffusion models (DMs) excel in image generation, concerns regarding their security persist. Earlier research highlighted DMs' vulnerability to backdoor attacks, but these studies placed stricter requirements than conventional methods like 'BadNets' in image classification. This is because the former necessitates modifications to the diffusion sampling and training procedures. Unlike the prior work, we investigate whether generating backdoor attacks in DMs can be as simple as BadNets, i.e., by only contaminating the training dataset without tampering the original diffusion process. In this more realistic backdoor setting, we uncover bilateral backdoor effects that not only serve an adversarial purpose (compromising the functionality of DMs) but also offer a defensive advantage (which can be leveraged for backdoor defense). Specifically, we find that a BadNets-like backdoor attack remains effective in DMs for producing incorrect images (misaligned with the intended text conditions), and thereby yielding incorrect predictions when DMs are used as classifiers. Meanwhile, backdoored DMs exhibit an increased ratio of backdoor triggers, a phenomenon we refer to as `trigger amplification', among the generated images. We show that this latter insight can be used to enhance the detection of backdoor-poisoned training data. Even under a low backdoor poisoning ratio, studying the backdoor effects of DMs is also valuable for designing anti-backdoor image classifiers. Last but not least, we establish a meaningful linkage between backdoor attacks and the phenomenon of data replications by exploring DMs' inherent data memorization tendencies. The codes of our work are available at https://github.com/OPTML-Group/BiBadDiff.
</details>
<details>
<summary>摘要</summary>
当前最先进的扩散模型（DM）在图像生成方面表现出色，但security问题仍然存在。 Earlier research highlighted DMs的易受到后门攻击的问题，但这些研究假设了与传统方法 like 'BadNets' in image classification不同的需求。 This is because the former requires modifications to the diffusion sampling and training procedures. Unlike prior work, we investigate whether generating backdoor attacks in DMs can be as simple as BadNets, i.e., by only contaminating the training dataset without tampering the original diffusion process. In this more realistic backdoor setting, we uncover bilateral backdoor effects that not only serve an adversarial purpose (compromising the functionality of DMs) but also offer a defensive advantage (which can be leveraged for backdoor defense). Specifically, we find that a BadNets-like backdoor attack remains effective in DMs for producing incorrect images (misaligned with the intended text conditions), and thereby yielding incorrect predictions when DMs are used as classifiers. Meanwhile, backdoored DMs exhibit an increased ratio of backdoor triggers, a phenomenon we refer to as 'trigger amplification', among the generated images. We show that this latter insight can be used to enhance the detection of backdoor-poisoned training data. Even under a low backdoor poisoning ratio, studying the backdoor effects of DMs is also valuable for designing anti-backdoor image classifiers. Last but not least, we establish a meaningful linkage between backdoor attacks and the phenomenon of data replications by exploring DMs' inherent data memorization tendencies. The codes of our work are available at <https://github.com/OPTML-Group/BiBadDiff>.
</details></li>
</ul>
<hr>
<h2 id="TACNET-Temporal-Audio-Source-Counting-Network"><a href="#TACNET-Temporal-Audio-Source-Counting-Network" class="headerlink" title="TACNET: Temporal Audio Source Counting Network"></a>TACNET: Temporal Audio Source Counting Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02369">http://arxiv.org/abs/2311.02369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirreza Ahmadnejad, Ahmad Mahmmodian Darviishani, Mohmmad Mehrdad Asadi, Sajjad Saffariyeh, Pedram Yousef, Emad Fatemizadeh</li>
<li>for: 这篇论文是为了解决音频源计数任务中的限制而设计的 Temporal Audio Source Counting Network (TaCNet) 架构。</li>
<li>methods: TaCNet 直接处理原始音频输入，减少了复杂的预处理步骤，简化了工作流程。它在实时speaker计数任务中表现出色，即使输入窗口被截取。</li>
<li>results: 在使用 LibriCount 数据集进行广泛评估中，TaCNet 的平均准确率为 74.18%，在 11 个类别中表现出色，包括中文和波斯语应用场景。这种跨语言适应性表明其 universality 和可能的影响。<details>
<summary>Abstract</summary>
In this paper, we introduce the Temporal Audio Source Counting Network (TaCNet), an innovative architecture that addresses limitations in audio source counting tasks. TaCNet operates directly on raw audio inputs, eliminating complex preprocessing steps and simplifying the workflow. Notably, it excels in real-time speaker counting, even with truncated input windows. Our extensive evaluation, conducted using the LibriCount dataset, underscores TaCNet's exceptional performance, positioning it as a state-of-the-art solution for audio source counting tasks. With an average accuracy of 74.18 percentage over 11 classes, TaCNet demonstrates its effectiveness across diverse scenarios, including applications involving Chinese and Persian languages. This cross-lingual adaptability highlights its versatility and potential impact.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Temporal Audio Source Counting Network（TaCNet），一种创新的架构，用于解决音频来源计数任务中的限制。TaCNet直接操作 raw 音频输入，从而消除复杂的预处理步骤，简化工作流程。尤其是在实时speaker计数任务中，TaCNet表现出色，即使输入窗口被截断。我们对利用 LibriCount 数据集进行了广泛的评估，并证明 TaCNet 在多种场景下表现出优秀的性能，包括使用中文和波斯语。这种跨语言适应性表明 TaCNet 的多样性和影响力。
</details></li>
</ul>
<hr>
<h2 id="MATA-Combining-Learnable-Node-Matching-with-A-Algorithm-for-Approximate-Graph-Edit-Distance-Computation"><a href="#MATA-Combining-Learnable-Node-Matching-with-A-Algorithm-for-Approximate-Graph-Edit-Distance-Computation" class="headerlink" title="MATA*: Combining Learnable Node Matching with A* Algorithm for Approximate Graph Edit Distance Computation"></a>MATA*: Combining Learnable Node Matching with A* Algorithm for Approximate Graph Edit Distance Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02356">http://arxiv.org/abs/2311.02356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junfeng Liu, Min Zhou, Shuai Ma, Lujia Pan</li>
<li>for: 这 paper 的目的是提出一种数据驱动的混合方法来 aproximate Graph Edit Distance (GED) 计算，以解决现有 A* 算法在搜索空间中寻找优化解决方案的可扩展性问题，以及学习基于方法不能准确地回归 GED 的问题。</li>
<li>methods: 这 paper 使用了 Graph Neural Networks (GNNs) 和 A* 算法来实现数据驱动的混合方法 MATA*，其中首先设计了一种结构增强 GNN 来同时学习本地和高阶结构信息，然后通过一个可导的 top-k 操作生成多个优秀的候选节点，最后使用这些候选节点来快速找到解决方案。</li>
<li>results: 经验表明，MATA* 对于大型图进行 aproximate GED 计算具有显著优势，可以高效地解决现有的搜索和学习方法的缺陷。<details>
<summary>Abstract</summary>
Graph Edit Distance (GED) is a general and domain-agnostic metric to measure graph similarity, widely used in graph search or retrieving tasks. However, the exact GED computation is known to be NP-complete. For instance, the widely used A* algorithms explore the entire search space to find the optimal solution which inevitably suffers scalability issues. Learning-based methods apply graph representation techniques to learn the GED by formulating a regression task, which can not recover the edit path and lead to inaccurate GED approximation (i.e., the predicted GED is smaller than the exact). To this end, in this work, we present a data-driven hybrid approach MATA* for approximate GED computation based on Graph Neural Networks (GNNs) and A* algorithms, which models from the perspective of learning to match nodes instead of directly regressing GED. Specifically, aware of the structure-dominant operations (i.e.,node and edge insertion/deletion) property in GED computation, a structure-enhanced GNN is firstly designed to jointly learn local and high-order structural information for node embeddings for node matchings. Second, top-k candidate nodes are produced via a differentiable top-k operation to enable the training for node matchings, which is adhering to another property of GED, i.e., multiple optimal node matchings. Third, benefiting from the candidate nodes, MATA* only performs on the promising search directions, reaching the solution efficiently. Finally, extensive experiments show the superiority of MATA* as it significantly outperforms the combinatorial search-based, learning-based and hybrid methods and scales well to large-size graphs.
</details>
<details>
<summary>摘要</summary>
图文编辑距离（GED）是一个通用和领域不依赖的度量图像相似性，广泛用于图像搜索或检索任务。然而，GED的准确计算是知道NP完备的。例如，通用的A*算法探索整个搜索空间以找到优化解决方案，不可避免的Scalability问题。学习基于方法采用图像表示技术来学习GED，可以不能恢复编辑路径，导致不准确的GED估计（即预测的GED小于实际）。为此，在这项工作中，我们提出了一种数据驱动的混合方法MATA*，基于图像神经网络（GNNs）和A*算法，用于粗略GED计算。具体来说，我们注意到GED计算中结构多 Operation（即节点和边插入/删除）性质，因此我们首先设计了结构增强的GNN来同时学习本地和高阶结构信息以获得节点匹配。其次，通过可导的top-k操作生成top-k候选节点，以便在匹配过程中训练节点匹配。第三，由于候选节点，MATA*仅在有前景的搜索方向上进行搜索，以达到解决方案的目的。最后，我们进行了广泛的实验，并证明MATA*在相比 combinatorial search-based、学习基本和混合方法的情况下表现出色，并且可以在大规模图像上执行。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Opinion-Formation-on-Networks"><a href="#Sample-Complexity-of-Opinion-Formation-on-Networks" class="headerlink" title="Sample Complexity of Opinion Formation on Networks"></a>Sample Complexity of Opinion Formation on Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02349">http://arxiv.org/abs/2311.02349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolin Liu, Rajmohan Rajaraman, Ravi Sundaram, Anil Vullikanti, Omer Wasim, Haifeng Xu</li>
<li>for: 寻求最佳资源分配策略，使公共卫生官员在社交网络上宣传新疫苗，以达到社区内所有人的共识，并保证宣传内容与实际事实相符。</li>
<li>methods: 基于recognized opinion formation game，每个代理的意见视为数据 derive的模型参数，而不仅仅是先前研究中的实数。这种扩展可以更深入地理解意见形成，与联邦学习密切相关。通过这种形式ulation，我们确定了样本复杂性 bound for any network，并显示了特定网络结构的上下文 bound。</li>
<li>results: 发现优化策略通常将样本分配给代理 inverse proportion to their degree，这对政策产生了重要的含义。我们的发现被验证了在 sinthezied 和实际世界网络上。<details>
<summary>Abstract</summary>
Consider public health officials aiming to spread awareness about a new vaccine in a community interconnected by a social network. How can they distribute information with minimal resources, ensuring community-wide understanding that aligns with the actual facts? This concern mirrors numerous real-world situations. In this paper, we initialize the study of sample complexity in opinion formation to solve this problem. Our model is built on the recognized opinion formation game, where we regard each agent's opinion as a data-derived model parameter, not just a real number as in prior studies. Such an extension offers a wider understanding of opinion formation and ties closely with federated learning. Through this formulation, we characterize the sample complexity bounds for any network and also show asymptotically tight bounds for specific network structures. Intriguingly, we discover optimal strategies often allocate samples inversely to the degree, hinting at vital policy implications. Our findings are empirically validated on both synthesized and real-world networks.
</details>
<details>
<summary>摘要</summary>
公共卫生官员想推广新疫苗的知识在社交媒体上，如何尽可能地分散信息，使整个社区都能够理解，同时与实际情况保持一致？这个问题与现实生活中的许多情况有着很大的相似性。在这篇论文中，我们开始研究样本复杂性在意见形成中的问题。我们在已知的意见形成游戏中使用每个代理的意见作为数据获得的模型参数，而不仅仅是一个实数，这种扩展可以更好地理解意见形成，并与联邦学习 closely 相关。通过这种形式ulation，我们定义了任何网络的样本复杂性bound，以及特定网络结构的上下文bound。我们发现，优化策略通常会尽可能地分配样本，与代理的度量成正比，这对政策有着重要的含义。我们的发现得到了Synthesized和实际世界网络的验证。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Natural-Language-of-DNA-using-Encoder-Decoder-Foundation-Models-with-Byte-level-Precision"><a href="#Understanding-the-Natural-Language-of-DNA-using-Encoder-Decoder-Foundation-Models-with-Byte-level-Precision" class="headerlink" title="Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision"></a>Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02333">http://arxiv.org/abs/2311.02333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Malusare, Harish Kothandaraman, Dipesh Tamboli, Nadia A. Lanman, Vaneet Aggarwal</li>
<li>for: 这个论文是为了分析 DNA 序列的 byte-level 精度而设计的 Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) 基础模型。</li>
<li>methods: 这个模型使用 Transformer 架构的 encoder-decoder 结构，并使用 sub-quadratic 实现注意力来开发一个高效的 sequence-to-sequence 模型。</li>
<li>results: 在不同的下游任务中，包括识别激活器、 promote 和 slice  сайты、识别 genomic 序列的生物功能注释、识别 base call mismatches 和 insertion&#x2F;deletion 错误、以及生成Influenza 病毒的变异，ENBED 模型都表现出了显著的提升，相比 existed 状态的艺术结果。<details>
<summary>Abstract</summary>
This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) identification of biological function annotations of genomic sequences, (3) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, and (4) generating mutations of the Influenza virus using the encoder-decoder architecture and validating them against real-world observations. In each of these tasks, we demonstrate significant improvement as compared to the existing state-of-the-art results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Identification of enhancers, promoters, and splice sites: The ENBED model outperforms existing methods in identifying these functional elements in DNA sequences.2. Identification of biological function annotations of genomic sequences: The ENBED model accurately predicts the biological functions of genomic sequences, including the presence of transcription factor binding sites and other regulatory elements.3. Recognition of sequences containing base call mismatches and insertion&#x2F;deletion errors: The ENBED model is able to identify sequences with base call errors and insertions&#x2F;deletions, which is an advantage over tokenization schemes that lose the ability to analyze at the byte level.4. Generating mutations of the Influenza virus using the encoder-decoder architecture and validating them against real-world observations: The ENBED model is used to generate mutations of the Influenza virus and the generated mutations are validated against real-world observations, demonstrating the potential of the model for drug resistance analysis and vaccine design.In each of these tasks, the ENBED model achieves significant improvement over existing state-of-the-art results, demonstrating its effectiveness in analyzing DNA sequences at the byte level.</details></li>
</ol>
<hr>
<h2 id="An-Operator-Learning-Framework-for-Spatiotemporal-Super-resolution-of-Scientific-Simulations"><a href="#An-Operator-Learning-Framework-for-Spatiotemporal-Super-resolution-of-Scientific-Simulations" class="headerlink" title="An Operator Learning Framework for Spatiotemporal Super-resolution of Scientific Simulations"></a>An Operator Learning Framework for Spatiotemporal Super-resolution of Scientific Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02328">http://arxiv.org/abs/2311.02328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentin Duruisseaux, Amit Chakraborty<br>for:* 这篇论文是为了解决高维度解析方法在数学模型中的计算限制问题，即使在小尺度下可以更好地捕捉实际动态。methods:* 这篇论文使用机器学习技术进行超Resolution，从低维度估算中重建高维度数值解。results:* 这篇论文提出了一种名为Super Resolution Operator Network（SROpNet）的新方法，可以在各种实际问题中提供更高精度的解决方案。<details>
<summary>Abstract</summary>
In numerous contexts, high-resolution solutions to partial differential equations are required to capture faithfully essential dynamics which occur at small spatiotemporal scales, but these solutions can be very difficult and slow to obtain using traditional methods due to limited computational resources. A recent direction to circumvent these computational limitations is to use machine learning techniques for super-resolution, to reconstruct high-resolution numerical solutions from low-resolution simulations which can be obtained more efficiently. The proposed approach, the Super Resolution Operator Network (SROpNet), frames super-resolution as an operator learning problem and draws inspiration from existing architectures to learn continuous representations of solutions to parametric differential equations from low-resolution approximations, which can then be evaluated at any desired location. In addition, no restrictions are imposed on the locations of (the fixed number of) spatiotemporal sensors at which the low-resolution approximations are provided, thereby enabling the consideration of a broader spectrum of problems arising in practice, for which many existing super-resolution approaches are not well-suited.
</details>
<details>
<summary>摘要</summary>
在许多Context中，高分辨率解决方案是必要的，以捕捉小时空尺度下的重要动力学行为。然而，使用传统方法可能会很慢和困难，因为计算资源有限。一种新的方向是使用机器学习技术来实现超解析，从低分辨率的 simulations 中重建高分辨率的数学解。我们的方法，称为 Super Resolution Operator Network (SROpNet)，将超解析视为一个操作学习问题， drew inspiration from existing architectures to learn continuous representations of solutions to parametric differential equations from low-resolution approximations, which can then be evaluated at any desired location.此外，我们不假设仅有一定数量的空间时间感知器的位置，因此可以考虑更多的实际问题，其中许多现有的超解析方法并不适用。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-of-Representations-for-Space-Generates-Multi-Modular-Grid-Cells"><a href="#Self-Supervised-Learning-of-Representations-for-Space-Generates-Multi-Modular-Grid-Cells" class="headerlink" title="Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells"></a>Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02316">http://arxiv.org/abs/2311.02316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rylan Schaeffer, Mikail Khona, Tzuhsuan Ma, Cristóbal Eyzaguirre, Sanmi Koyejo, Ila Rani Fiete</li>
<li>For: 解决空间问题的映射、定位和导航，哺乳动物的后代发展出了突出的空间表示。* Methods: 使用了四种方法：编码理论、动力系统、功能优化和监督深度学习。* Results: 提出了一种新的自监学习（SSL）框架，能够在不需要指导位信息或工程特定的读出表示的情况下，使多个网格细胞模块出现在训练后进行泛化。<details>
<summary>Abstract</summary>
To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, there is no satisfactory synthesis of core principles that lead to multi-modular grid cells in deep recurrent neural networks. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.
</details>
<details>
<summary>摘要</summary>
为解决地图、位置Localization和导航问题，哺乳动物的演化历史中发展出了突出的空间表示。一种重要的空间表示是诺贝尔奖获得的格子细胞：神经元表示自己的位置，是一个本地和不规则的量，似乎具有奇怪的非本地和空间周期性的活动模式。为什么哺乳动物演化出这种怪异的格子表示？数学分析表明这些多周期的表示具有出色的算法代码性和内置的错误修复特性，但到目前为止，没有满意的核心原则的合成。在这项工作中，我们开始 by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.
</details></li>
</ul>
<hr>
<h2 id="Heteroskedastic-Tensor-Clustering"><a href="#Heteroskedastic-Tensor-Clustering" class="headerlink" title="Heteroskedastic Tensor Clustering"></a>Heteroskedastic Tensor Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02306">http://arxiv.org/abs/2311.02306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Zhou, Yuxin Chen</li>
<li>for: 提取tensor数据中各个模式下的准确层次结构</li>
<li>methods: 使用一种新的特征值算法 called $\mathsf{Thresholded~Deflated\text{-}HeteroPCA}$，然后使用approx $k$-means来获取层次结构</li>
<li>results: 提供了一种可靠地实现tensor clustering的算法，并且在多种设置下比现有算法表现出更高的可靠性和精度。<details>
<summary>Abstract</summary>
Tensor clustering, which seeks to extract underlying cluster structures from noisy tensor observations, has gained increasing attention. One extensively studied model for tensor clustering is the tensor block model, which postulates the existence of clustering structures along each mode and has found broad applications in areas like multi-tissue gene expression analysis and multilayer network analysis. However, currently available computationally feasible methods for tensor clustering either are limited to handling i.i.d. sub-Gaussian noise or suffer from suboptimal statistical performance, which restrains their utility in applications that have to deal with heteroskedastic data and/or low signal-to-noise-ratio (SNR).   To overcome these challenges, we propose a two-stage method, named $\mathsf{High\text{-}order~HeteroClustering}$ ($\mathsf{HHC}$), which starts by performing tensor subspace estimation via a novel spectral algorithm called $\mathsf{Thresholded~Deflated\text{-}HeteroPCA}$, followed by approximate $k$-means to obtain cluster nodes. Encouragingly, our algorithm provably achieves exact clustering as long as the SNR exceeds the computational limit (ignoring logarithmic factors); here, the SNR refers to the ratio of the pairwise disparity between nodes to the noise level, and the computational limit indicates the lowest SNR that enables exact clustering with polynomial runtime. Comprehensive simulation and real-data experiments suggest that our algorithm outperforms existing algorithms across various settings, delivering more reliable clustering performance.
</details>
<details>
<summary>摘要</summary>
tensor clustering，它旨在从含有噪声的张量观察中提取下面的底层结构，在过去几年内获得了越来越多的关注。一种广泛研究的张量 clustering 模型是张量块模型，它假设每个模式中存在层次结构，并在多个领域，如多组织表达分析和多层网络分析中发现了广泛的应用。然而，目前可用的计算可行的张量 clustering 方法 Either 是处理 i.i.d. 子 Gaussian 噪声的限制，或者受到不佳的统计性能的限制，这限制了它们在应用中处理不均匀数据和/或低信号响应比例 (SNR) 的能力。为了解决这些挑战，我们提出了一种两stage方法，名为 $\mathsf{High\text{-}order~HeteroClustering}$ ($\mathsf{HHC}$)，它首先通过一种新的спектраль算法 called $\mathsf{Thresholded~Deflated\text{-}HeteroPCA}$ 进行张量子空间估计，然后使用approx $k$-means 获取集群节点。鼓舞人的是，我们的算法可以在 SNR 超过计算限制 (忽略对数因素) 的情况下，提供正确的划分结果，其中 SNR 是对比两个节点之间的差异与噪声水平的比率，而计算限制则是最低的 SNR 可以使用的计算时间的下限。在广泛的 simulate 和实际数据实验中，我们的算法比现有的算法在不同的设置下表现出更高的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Multi-Modal-Representation-Learning-for-Spark-Plug-Fault-Diagnosis"><a href="#Contrastive-Multi-Modal-Representation-Learning-for-Spark-Plug-Fault-Diagnosis" class="headerlink" title="Contrastive Multi-Modal Representation Learning for Spark Plug Fault Diagnosis"></a>Contrastive Multi-Modal Representation Learning for Spark Plug Fault Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02282">http://arxiv.org/abs/2311.02282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ardavan Modarres, Vahid Mohammad-Zadeh Eivaghi, Mahdi Aliyari Shoorehdeli, Ashkan Moosavian</li>
<li>for: 这个研究旨在提高工业设备状态监控中的条件监控，因为单一感知量不能提供足够的信息，而且单一感知量的噪音会导致误导。因此，需要一个有效的数据融合策略。</li>
<li>methods: 这个研究使用了一种具有对比学习概念的Denosing Multi-Modal Autoencoder，并且首次应用了这种方法在机器健康监控领域中。这种方法不仅能够充分融合多种感知量（或视角）的数据，而且可以在测试时将一个视角 omitted 而不会影响性能，或者甚至不需要将任何视角 omitted。</li>
<li>results: 这个研究的结果显示，使用了Denosing Multi-Modal Autoencoder的方法可以实现高效的多感知量融合，并且可以在感知量失效时继续运行，不需要更改现有的感知量组态。此外，这种方法可以实现更cost-effective的状态监控系统，不需要增加更多的感知量。<details>
<summary>Abstract</summary>
Due to the incapability of one sensory measurement to provide enough information for condition monitoring of some complex engineered industrial mechanisms and also for overcoming the misleading noise of a single sensor, multiple sensors are installed to improve the condition monitoring of some industrial equipment. Therefore, an efficient data fusion strategy is demanded. In this research, we presented a Denoising Multi-Modal Autoencoder with a unique training strategy based on contrastive learning paradigm, both being utilized for the first time in the machine health monitoring realm. The presented approach, which leverages the merits of both supervised and unsupervised learning, not only achieves excellent performance in fusing multiple modalities (or views) of data into an enriched common representation but also takes data fusion to the next level wherein one of the views can be omitted during inference time with very slight performance reduction, or even without any reduction at all. The presented methodology enables multi-modal fault diagnosis systems to perform more robustly in case of sensor failure occurrence, and one can also intentionally omit one of the sensors (the more expensive one) in order to build a more cost-effective condition monitoring system without sacrificing performance for practical purposes. The effectiveness of the presented methodology is examined on a real-world private multi-modal dataset gathered under non-laboratory conditions from a complex engineered mechanism, an inline four-stroke spark-ignition engine, aiming for spark plug fault diagnosis. This dataset, which contains the accelerometer and acoustic signals as two modalities, has a very slight amount of fault, and achieving good performance on such a dataset promises that the presented method can perform well on other equipment as well.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-learning’s-own-Industrial-Revolution"><a href="#Machine-learning’s-own-Industrial-Revolution" class="headerlink" title="Machine learning’s own Industrial Revolution"></a>Machine learning’s own Industrial Revolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02278">http://arxiv.org/abs/2311.02278</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Yuan Luo, Song Han, Jingjing Liu</li>
<li>for: 本研究目的是帮助机器学习完成自己的工业革命，以满足越来越高的企业需求和广泛的行业。</li>
<li>methods: 本文提出了一种新的工业革命模型，用于帮助机器学习实现自己的目标，包括标准化和自动化生产网络。</li>
<li>results: 本文预测了机器学习的未来发展趋势，并提出了新的机会和挑战，以帮助机器学习在广泛的行业中得到更广泛的应用和利用。<details>
<summary>Abstract</summary>
Machine learning is expected to enable the next Industrial Revolution. However, lacking standardized and automated assembly networks, ML faces significant challenges to meet ever-growing enterprise demands and empower broad industries. In the Perspective, we argue that ML needs to first complete its own Industrial Revolution, elaborate on how to best achieve its goals, and discuss new opportunities to enable rapid translation from ML's innovation frontier to mass production and utilization.
</details>
<details>
<summary>摘要</summary>
机器学习预计会推动下一个工业革命。然而，由于缺乏标准化和自动化的组装网络，机器学习面临着满足永不减少的企业需求和推广到多个行业的重大挑战。在我们的视角中，机器学习需要先完成自己的工业革命，详细说明如何最好实现目标，并讨论新的机会来快速将机器学习的创新前沿翻译成大规模生产和应用。Note: Simplified Chinese is used here, as it is more widely used in mainland China and is the standard language for most online content. Traditional Chinese is used in Taiwan and Hong Kong, and it has some differences in grammar and vocabulary compared to Simplified Chinese.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/cs.LG_2023_11_04/" data-id="clpxp043400umfm88elk2gky5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/eess.SP_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T08:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/eess.SP_2023_11_04/">eess.SP - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="On-Learning-the-Distribution-of-a-Random-Spatial-Field-in-a-Location-Unaware-Mobile-Sensing-Setup"><a href="#On-Learning-the-Distribution-of-a-Random-Spatial-Field-in-a-Location-Unaware-Mobile-Sensing-Setup" class="headerlink" title="On Learning the Distribution of a Random Spatial Field in a Location-Unaware Mobile Sensing Setup"></a>On Learning the Distribution of a Random Spatial Field in a Location-Unaware Mobile Sensing Setup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02464">http://arxiv.org/abs/2311.02464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meera Pai</li>
<li>for: 本研究的目的是学习一个固定一 dimensional 路径上的空间时间场的统计分布，在absence of location information。</li>
<li>methods: 本研究使用了移动感知设备采集空间时间场的样本，并提出了一些简单的假设来学习场的统计分布。</li>
<li>results: 研究表明，可以使用移动感知设备采集的样本来学习空间时间场的统计分布，并且提供了一系列的分析和实验结果来支持这一结论。<details>
<summary>Abstract</summary>
In applications like environment monitoring and pollution control, physical quantities are modeled by spatio-temporal fields. It is of interest to learn the statistical distribution of such fields as a function of space, time or both. In this work, our aim is to learn the statistical distribution of a spatio-temporal field along a fixed one dimensional path, as a function of spatial location, in the absence of location information. Spatial field analysis, commonly done using static sensor networks is a well studied problem in literature. Recently, due to flexibility in setting the spatial sampling density and low hardware cost, owing to larger spatial coverage, mobile sensors are used for this purpose. The main challenge in using mobile sensors is their location uncertainty. Obtaining location information of samples requires additional hardware and cost. So, we consider the case when the spatio-temporal field along the fixed length path is sampled using a simple mobile sensing device that records field values while traversing the path without any location information. We ask whether it is possible to learn the statistical distribution of the field, as a function of spatial location, using samples from the location-unaware mobile sensor under some simple assumptions on the field. We answer this question in affirmative and provide a series of analytical and experimental results to support our claim.
</details>
<details>
<summary>摘要</summary>
在环境监测和污染控制应用中，物理量是通过空间-时间场的模拟来表示。我们的目标是在固定一个一维路径上学习这个场的统计分布，以空间位置为变量。在文献中广泛研究的空间场分析中，通常使用静止感知网络进行检测。然而，由于可以自由设置空间抽样密度以及低硬件成本，由于更大的空间覆盖率，移动感知设备在最近几年中变得越来越受欢迎。然而，移动感知设备的位置不确定性成为主要挑战。为了获取样本的位置信息，需要额外的硬件和成本。因此，我们考虑了在固定一个一维路径上，使用简单的移动感知设备记录场值，而不提供位置信息的情况下，是否可以学习场的统计分布，以空间位置为变量。我们的答案是可以，并且提供了一系列的分析和实验结果来支持我们的说法。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Imperfect-Resolution-of-Near-Field-Beamforming-A-Hybrid-NOMA-Perspective"><a href="#Utilizing-Imperfect-Resolution-of-Near-Field-Beamforming-A-Hybrid-NOMA-Perspective" class="headerlink" title="Utilizing Imperfect Resolution of Near-Field Beamforming: A Hybrid-NOMA Perspective"></a>Utilizing Imperfect Resolution of Near-Field Beamforming: A Hybrid-NOMA Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02451">http://arxiv.org/abs/2311.02451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiguo Ding, H. Vincent Poor</li>
<li>for: 本研究旨在利用近场通信中的不完全解像程度来提高无线网络吞吐和连接稳定性。</li>
<li>methods: 该研究提出了一种混合非对准多ступ通信（NOMA）传输策略，使用预配置的近场扫描器来服务更多的用户。然后通过不同的顺序扫描取消技术来解决能量消耗最小化问题。</li>
<li>results: 分析和 simulate结果表明，随着近场解像程度的提高，hybrid NOMA传输策略可以提高无线网络的吞吐和连接稳定性。<details>
<summary>Abstract</summary>
This letter studies how the imperfect resolution of near-field beamforming, the key feature of near-field communications, can be used to improve the throughput and connectivity of wireless networks. In particular, a hybrid non-orthogonal multiple access (NOMA) transmission strategy is developed to use preconfigured near-field beams for serving additional users. An energy consumption minimization problem is first formulated and then solved by using different successive interference cancellation strategies. Both analytical and simulation results are presented to illustrate the impact of the resolution of near-field beamforming on the design of hybrid NOMA transmission.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "near-field beamforming" is translated as "近场扩散" (jìn chǎng kuò xiǎn)* "hybrid non-orthogonal multiple access" is translated as "混合非正交多接入" (hùn hǎi fēi zhèng jiāng duō yù)* "preconfigured near-field beams" is translated as "预先配置的近场扩散" (xiù xiān bèng jī de jìn chǎng kuò xiǎn)* "energy consumption minimization" is translated as "能量消耗最小化" (néng yàng xiāo hóu zuì xiǎo)* "successive interference cancellation" is translated as "successive interference cancellation" (成功ive kancel)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Quantized-but-uncoded-Distributed-Detection-QDD-with-Unreliable-Reporting-Channels"><a href="#Quantized-but-uncoded-Distributed-Detection-QDD-with-Unreliable-Reporting-Channels" class="headerlink" title="Quantized-but-uncoded Distributed Detection (QDD) with Unreliable Reporting Channels"></a>Quantized-but-uncoded Distributed Detection (QDD) with Unreliable Reporting Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02447">http://arxiv.org/abs/2311.02447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Cao, Ramanarayanan Viswanathan</li>
<li>for: 本研究旨在提出一种新的分布式检测方法，即量化但未编码的分布式检测（QDD），以提高传输能力和复杂性。</li>
<li>methods: 本研究使用量化但未编码的方法，其中每个感知器对其完整的观测数据进行量化，然后将归一化后的值传输到总Integration Center（FC）。</li>
<li>results: 比较CDD和QDD两种方法，本研究发现QDD在传输能力限制下表现更好，但是需要更多的参数选择。此外，在独立观测下，QDD保持了CDD中的必需条件，即最佳感知器决策规则是likelihood ratio quantizers（LRQ），不受通信渠道条件影响。<details>
<summary>Abstract</summary>
Distributed detection primarily centers around two approaches: Unquantized Distributed Detection (UDD), where each sensor reports its complete observation to the fusion center (FC), and quantized-and-Coded DD (CDD), where each sensor first partitions the observation space and then reports to the FC a codeword. In this paper, we introduce Quantized-but-uncoded DD (QDD), where each sensor, after quantization, transmits a summarized value, instead of a codeword, to the FC. We show that QDD well adapts to the constraint of transmission power when compared to CDD, albeit with increased complexity in parameter selection. Moreover, we establish that, in the presence of independent observations, QDD upholds a necessary condition inherent in CDD. Specifically, the optimal sensor decision rules are the likelihood ratio quantizers (LRQ), irrelevant to the channel conditions. In the context of a single-sensor scenario involving binary decision at the sensor, we find that the optimal sensor rule in QDD is in general no longer ``channel blind", a feature presented in CDD. In addition, we compare these systems numerically under the same transmission power and bandwidth, while assuming additive white Gaussian noise (AWGN) in both sensing and reporting stages. Finally, we present some potential directions for future research.
</details>
<details>
<summary>摘要</summary>
主要分布检测方法有两种：不量化分布检测（UDD），每个感知器都直接将完整的观测报告给归一化中心（FC），以及量化编码分布检测（CDD），每个感知器首先将观测空间分割，然后向FC报告一个编码word。在本文中，我们介绍了量化但未编码的分布检测（QDD），每个感知器，经过量化，将减少值传输到FC，而不是编码word。我们表明，QDD在传输功率限制下比CDD更适应，尽管它增加了参数选择的复杂性。此外，我们证明，在独立观测下，QDD保持了CDD中的必需条件。具体来说，感知器的优化决策规则是 likelihood ratio quantizers（LRQ），不受通信条件影响。在单感知器场景中，我们发现QDD的优化决策规则不再是“通信盲目”的，这是CDD中的特点。此外，我们在同传输功率和宽度下，对这些系统进行了数值比较，假设感知和报告阶段都存在添加itive white Gaussian noise（AWGN）。最后，我们提出了未来研究的一些可能的方向。
</details></li>
</ul>
<hr>
<h2 id="PIPO-Net-A-Penalty-based-Independent-Parameters-Optimization-Deep-Unfolding-Network"><a href="#PIPO-Net-A-Penalty-based-Independent-Parameters-Optimization-Deep-Unfolding-Network" class="headerlink" title="PIPO-Net: A Penalty-based Independent Parameters Optimization Deep Unfolding Network"></a>PIPO-Net: A Penalty-based Independent Parameters Optimization Deep Unfolding Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02443">http://arxiv.org/abs/2311.02443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiumei Li, Zhijie Zhang, Huang Bai, Ljubiša Stanković, Junpeng Hao, Junmei Sun</li>
<li>for: 用于重建压缩感知图像</li>
<li>methods: 使用罚函数优化策略和高频补充块</li>
<li>results: 实现高精度重建压缩感知图像<details>
<summary>Abstract</summary>
Compressive sensing (CS) has been widely applied in signal and image processing fields. Traditional CS reconstruction algorithms have a complete theoretical foundation but suffer from the high computational complexity, while fashionable deep network-based methods can achieve high-accuracy reconstruction of CS but are short of interpretability. These facts motivate us to develop a deep unfolding network named the penalty-based independent parameters optimization network (PIPO-Net) to combine the merits of the above mentioned two kinds of CS methods. Each module of PIPO-Net can be viewed separately as an optimization problem with respective penalty function. The main characteristic of PIPO-Net is that, in each round of training, the learnable parameters in one module are updated independently from those of other modules. This makes the network more flexible to find the optimal solutions of the corresponding problems. Moreover, the mean-subtraction sampling and the high-frequency complementary blocks are developed to improve the performance of PIPO-Net. Experiments on reconstructing CS images demonstrate the effectiveness of the proposed PIPO-Net.
</details>
<details>
<summary>摘要</summary>
压缩感知（CS）在信号处理和图像处理领域广泛应用。传统的CS重建算法具有完善的理论基础，但计算复杂性高；而时尚的深度网络基于方法可以实现高精度的CS重建，但缺乏可读性。这些因素激发我们开发一种名为罚函数基本独立参数优化网络（PIPO-Net）的深度 unfolding 网络，将上述两种CS方法的优点结合起来。PIPO-Net 中每个模块可以视为一个优化问题，每个模块的学习参数在训练过程中独立地更新。这使得网络更加灵活地找到相应的优化解决方案。此外，我们还提出了mean-subtraction sampling和高频补充块来提高PIPO-Net的性能。实验表明，提议的PIPO-Net 可以有效地重建CS图像。
</details></li>
</ul>
<hr>
<h2 id="SplitMAC-Wireless-Split-Learning-over-Multiple-Access-Channels"><a href="#SplitMAC-Wireless-Split-Learning-over-Multiple-Access-Channels" class="headerlink" title="SplitMAC: Wireless Split Learning over Multiple Access Channels"></a>SplitMAC: Wireless Split Learning over Multiple Access Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02405">http://arxiv.org/abs/2311.02405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonjung Kim, Yongjeong Oh, Yo-Seb Jeon</li>
<li>For: 本文提出了一种新的分解学习（SL）框架，称为SplitMAC，它可以降低SL的延迟时间，通过同时在多个访问通道上传输多个设备的混合数据和设备 сторо面模型。* Methods: 本文使用分 grouping 策略，将设备分为多个组，并让同一组内的设备同时传输其混合数据和设备 сторо面模型。优化问题是将设备分配到最佳的组，以最小化SL延迟时间。* Results:  simulations 表明，我们的SL框架，尤其是使用提出的设备分配算法，可以在各种信号噪响比（SNR）场景下减少SL延迟时间。<details>
<summary>Abstract</summary>
This paper presents a novel split learning (SL) framework, referred to as SplitMAC, which reduces the latency of SL by leveraging simultaneous uplink transmission over multiple access channels. The key strategy is to divide devices into multiple groups and allow the devices within the same group to simultaneously transmit their smashed data and device-side models over the multiple access channels. The optimization problem of device grouping to minimize SL latency is formulated, and the benefit of device grouping in reducing the uplink latency of SL is theoretically derived. By examining a two-device grouping case, two asymptotically-optimal algorithms are devised for device grouping in low and high signal-to-noise ratio (SNR) scenarios, respectively, while providing proofs of their optimality. By merging these algorithms, a near-optimal device grouping algorithm is proposed to cover a wide range of SNR. Simulation results demonstrate that our SL framework with the proposed device grouping algorithm is superior to existing SL frameworks in reducing SL latency.
</details>
<details>
<summary>摘要</summary>
Two asymptotically-optimal algorithms are proposed for device grouping in low and high signal-to-noise ratio (SNR) scenarios, respectively. These algorithms are proven to be optimal, and by merging them, a near-optimal device grouping algorithm is proposed to cover a wide range of SNR. Simulation results show that the proposed SL framework with the device grouping algorithm is superior to existing SL frameworks in reducing SL latency.In simplified Chinese, the text can be translated as:这篇论文提出了一种新的分布式学习（SL）框架，称为SplitMAC，它可以降低SL的延迟时间。这个框架的关键策略是将设备分成多个组，并让同一组的设备同时传输压缩数据和设备侧模型通过多个访问通道。将设备分组优化问题以减少SL延迟时间被形式化，并证明了设备分组可以减少上行延迟时间。对于低和高信号噪比（SNR）两种场景，分别提出了两种极似优算法，其中一种是适用于低SNR场景，另一种是适用于高SNR场景。这两种算法都是可数的优化算法，并且将它们合并可以得到一个近似优化的设备分组算法，以覆盖各种SNR场景。实验结果表明，提出的SL框架以及设备分组算法都是现有SL框架的改进版本，可以更好地减少SL延迟时间。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Reflecting-Surface-Aided-Wireless-Communication-with-Movable-Elements"><a href="#Intelligent-Reflecting-Surface-Aided-Wireless-Communication-with-Movable-Elements" class="headerlink" title="Intelligent Reflecting Surface-Aided Wireless Communication with Movable Elements"></a>Intelligent Reflecting Surface-Aided Wireless Communication with Movable Elements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02376">http://arxiv.org/abs/2311.02376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guojie Hu, Qingqing Wu, Dognhui Xu, Kui Xu, Jiangbo Si, Yunlong Cai, Naofal Al-Dhahir<br>for: 这个研究旨在提高通信性能的智能镜面技术 (IRS) 中，为了降低生产和控制成本，采用独立阶段调校 (DPS)，但这种设置对于通过总RIician折射而具有问题。methods: 我们在这篇论文中设计了优化的非均匀DPS，以获得满意的性能水平。我们面对的主要挑战是当IRS元素的位置固定时，可能会出现各个构成元素间的偏移，导致不同的偏移模式，从而导致生产成本增加，特别是当IRS元素的数量很大时。results: 我们透过 simulations 表明，我们的提案可以与竞争 benchmark 相比，实现系统性能的明显提高。<details>
<summary>Abstract</summary>
Intelligent reflecting surface (IRS) has been recognized as a powerful technology for boosting communication performance. To reduce manufacturing and control costs, it is preferable to consider discrete phase shifts (DPSs) for IRS, which are set by default as uniformly distributed in the range of $[ - \pi,\pi )$ in the literature. Such setting, however, cannot achieve a desirable performance over the general Rician fading where the channel phase concentrates in a narrow range with a higher probability. Motivated by this drawback, we in this paper design optimal non-uniform DPSs for IRS to achieve a desirable performance level. The fundamental challenge is the \textit{possible offset in phase distribution across different cascaded source-element-destination channels}, if adopting conventional IRS where the position of each element is fixed. Such phenomenon leads to different patterns of optimal non-uniform DPSs for each IRS element and thus causes huge manufacturing costs especially when the number of IRS elements is large. Driven by the recently emerging fluid antenna system (or movable antenna technology), we demonstrate that if the position of each IRS element can be flexibly adjusted, the above phase distribution offset can be surprisingly eliminated, leading to the same pattern of DPSs for each IRS element. Armed with this, we then determine the form of unified non-uniform DPSs based on a low-complexity iterative algorithm. Simulations show that our proposed design significantly improves the system performance compared to competitive benchmarks.
</details>
<details>
<summary>摘要</summary>
智能反射表面（IRS）已被认为是一种强大的通信性能提升技术。为了降低生产和控制成本，它是将独立阶段调整（DPS）设置为默认值，即在 $[-\pi, \pi)$ 中 uniformly 分布的文献中的偏好。但这个设置无法在一般的雷电折射中 achieve  Desirable 性能，因为通道频率偏集在狭窄的范围中，具有更高的几率。驱动了这个缺陷，我们在这篇论文中设计了优化的非均匀DPS，以 achieve  Desirable 性能水平。基本挑战在于 possible 频率分布偏移 across different 缝合源-元素-目标通道，如果采用传统的 IRS，则每个 IRS 元素的位置固定。这个现象导致每个 IRS 元素的优化非均匀DPS 具有不同的几何结构，导致生产成本尤其高于当 IRS 元素的数量较多。驱动了最近发展的流体天线系统（或可动天线技术），我们示出了如果每个 IRS 元素的位置可以灵活地调整，这个偏移问题可以 unexpectedly 消除，导致每个 IRS 元素的 DPS 具有同样的模式。 armed  with  this，我们then 决定了非均匀 DPS 的形式，基于一种低复杂度的迭代算法。模拟结果显示，我们的提案对于竞争性能标准的优化做出了显著改善。
</details></li>
</ul>
<hr>
<h2 id="A-Physics-based-Machine-Learning-Model-to-characterize-Room-Temperature-Semiconductor-Detectors-in-3D"><a href="#A-Physics-based-Machine-Learning-Model-to-characterize-Room-Temperature-Semiconductor-Detectors-in-3D" class="headerlink" title="A Physics based Machine Learning Model to characterize Room Temperature Semiconductor Detectors in 3D"></a>A Physics based Machine Learning Model to characterize Room Temperature Semiconductor Detectors in 3D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02290">http://arxiv.org/abs/2311.02290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srutarshi Banerjee, Miesher Rodrigues, Manuel Ballester, Alexander H. Vija, Aggelos K. Katsaggelos</li>
<li>For: The paper aims to develop a novel physics-based machine learning (PBML) model for characterizing room temperature semiconductor radiation detectors (RTSDs) in 3D space.* Methods: The PBML model is based on a discretized sub-pixelated 3D volume, and it considers the different physics-based charge transport properties such as drift, trapping, detrapping, and recombination of charges as trainable model weights. The model uses backpropagation to determine the trainable weights and optimize the loss function.* Results: The proposed PBML model is the first to characterize a full 3D charge transport model of RTSDs, and it can accurately determine the trainable weights that represent the one-to-one relation to the actual physical charge transport properties in a voxelized detector.<details>
<summary>Abstract</summary>
Room temperature semiconductor radiation detectors (RTSD) for X-ray and gamma-ray detection are vital tools for medical imaging, astrophysics and other applications. CdZnTe (CZT) has been the main RTSD for more than three decades with desired detection properties. In a typical pixelated configuration, CZT have electrodes on opposite ends. For advanced event reconstruction algorithms at sub-pixel level, detailed characterization of the RTSD is required in three dimensional (3D) space. However, 3D characterization of the material defects and charge transport properties in the sub-pixel regime is a labor-intensive process with skilled manpower and novel experimental setups. Presently, state-of-art characterization is done over the bulk of the RTSD considering homogenous properties. In this paper, we propose a novel physics based machine learning (PBML) model to characterize the RTSD over a discretized sub-pixelated 3D volume which is assumed. Our novel approach is the first to characterize a full 3D charge transport model of the RTSD. In this work, we first discretize the RTSD between a pixelated electrodes spatially in 3D - x, y, and z. The resulting discretizations are termed as voxels in 3D space. In each voxel, the different physics based charge transport properties such as drift, trapping, detrapping and recombination of charges are modeled as trainable model weights. The drift of the charges considers second order non-linear motion which is observed in practice with the RTSDs. Based on the electron-hole pair injections as input to the PBML model, and signals at the electrodes, free and trapped charges (electrons and holes) as outputs of the model, the PBML model determines the trainable weights by backpropagating the loss function. The trained weights of the model represents one-to-one relation to that of the actual physical charge transport properties in a voxelized detector.
</details>
<details>
<summary>摘要</summary>
室温半导体辐射探测器（RTSD）在医学影像、astrophysics和其他应用中是非常重要的工具。 Cadmium zinc telluride（CZT）在过去三十年中一直是主要的RTSD，具有欢得的探测性能。在常见的像素化配置中，CZT有电极在两端。为了在像素水平上使用高级事件重建算法，RTSD的详细三维（3D）特性的Characterization是必要的。然而，在sub-像素级别上对材料缺陷和电子传输性能的3DCharacterization是一项劳动密集的过程，需要专业人员和特殊的实验设备。现在，状态机器的Characterization都是基于整体RTSD的假设，忽略了物理性的细节。在这篇论文中，我们提出了一种新的物理学基本机器学习（PBML）模型，用于 caracterizing RTSD的3D电子传输模型。我们首先将RTSD在三维空间中分割成像素化的电极，并将每个像素称为voxel。在每个voxel中，我们模型了不同的物理学基本的电子传输特性，如漂移、固定、释放和 recombination of charges。这些模型参数被视为可训练的模型参数。基于电子-引起对的插入和电极上的信号，以及free和固定电荷（电子和洞）的输出，PBML模型通过反射损失函数来确定模型参数。训练后，模型的参数表示了RTSD的实际物理电子传输特性的一对一关系。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/eess.SP_2023_11_04/" data-id="clpxp04c301hifm88ae1ybhe8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/cs.SD_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T15:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/03/cs.SD_2023_11_03/">cs.SD - 2023-11-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FiloBass-A-Dataset-and-Corpus-Based-Study-of-Jazz-Basslines"><a href="#FiloBass-A-Dataset-and-Corpus-Based-Study-of-Jazz-Basslines" class="headerlink" title="FiloBass: A Dataset and Corpus Based Study of Jazz Basslines"></a>FiloBass: A Dataset and Corpus Based Study of Jazz Basslines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02023">http://arxiv.org/abs/2311.02023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xavier Riley, Simon Dixon</li>
<li>for: 这篇论文旨在探讨爵士乐double bass的重要作用，尤其是在辅助演奏中。</li>
<li>methods: 作者提供了48首职业爵士乐 double bass手的手记谱和相关的metadata，包括Audio stem、Score、Performance-aligned MIDI和 markers for musical form。</li>
<li>results: 通过对 FiloBass 谱系进行 contrastive 分析，作者发现了一些关键的爵士乐 double bass 演奏技巧，并且与现有的教学方法进行比较。<details>
<summary>Abstract</summary>
We present FiloBass: a novel corpus of music scores and annotations which focuses on the important but often overlooked role of the double bass in jazz accompaniment. Inspired by recent work that sheds light on the role of the soloist, we offer a collection of 48 manually verified transcriptions of professional jazz bassists, comprising over 50,000 note events, which are based on the backing tracks used in the FiloSax dataset. For each recording we provide audio stems, scores, performance-aligned MIDI and associated metadata for beats, downbeats, chord symbols and markers for musical form.   We then use FiloBass to enrich our understanding of jazz bass lines, by conducting a corpus-based musical analysis with a contrastive study of existing instructional methods. Together with the original FiloSax dataset, our work represents a significant step toward a fully annotated performance dataset for a jazz quartet setting. By illuminating the critical role of the bass in jazz, this work contributes to a more nuanced and comprehensive understanding of the genre.
</details>
<details>
<summary>摘要</summary>
我们现在发布了FiloBass：一个新的音乐谱和注释集，专注于爵士乐伴奏中重要 yet often overlooked的double bass角色。受最近关于独奏者的工作所 inspirited，我们提供了48名职业爵士乐 bassist的手动验证 транскрип，包括超过50,000个音 Event，基于FiloSax数据集中的 backing tracks。每个录音都提供了音质 stem，谱表，与 markers for musical form 的表示，以及相关的metadata。我们使用FiloBass来推广我们对爵士乐 bass line的理解，通过对现有的教学方法进行相比研究。与原始 FiloSax数据集一起，我们的工作表示了一个完整的表演数据集的 jazz quartet 设置。通过推照爵士乐中的重要性，我们的工作对爵士乐领域的理解做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Acousto-optic-reconstruction-of-exterior-sound-field-based-on-concentric-circle-sampling-with-circular-harmonic-expansion"><a href="#Acousto-optic-reconstruction-of-exterior-sound-field-based-on-concentric-circle-sampling-with-circular-harmonic-expansion" class="headerlink" title="Acousto-optic reconstruction of exterior sound field based on concentric circle sampling with circular harmonic expansion"></a>Acousto-optic reconstruction of exterior sound field based on concentric circle sampling with circular harmonic expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01715">http://arxiv.org/abs/2311.01715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phuc Duc Nguyen, Kenji Ishikawa, Noboru Harada, Takehiro Moriya</li>
<li>for: 提供一种新的外部声场重建方法，用于解决现有的声场重建算法在外部场景下的表现不佳问题。</li>
<li>methods: 该方法基于圆形卷积抽样和二维外部声场重建方法，使用径向圆形延展来扩展圆形卷积抽样。</li>
<li>results: 对比 conventinal 重建方法，提出的方法在数字实验和实际实验中具有更高的准确性，同时使用的抽样数据量很少。<details>
<summary>Abstract</summary>
Acousto-optic sensing provides an alternative approach to traditional microphone arrays by shedding light on the interaction of light with an acoustic field. Sound field reconstruction is a fascinating and advanced technique used in acousto-optics sensing. Current challenges in sound-field reconstruction methods pertain to scenarios in which the sound source is located within the reconstruction area, known as the exterior problem. Existing reconstruction algorithms, primarily designed for interior scenarios, often exhibit suboptimal performance when applied to exterior cases. This paper introduces a novel technique for exterior sound-field reconstruction. The proposed method leverages concentric circle sampling and a two-dimensional exterior sound-field reconstruction approach based on circular harmonic extensions. To evaluate the efficacy of this approach, both numerical simulations and practical experiments are conducted. The results highlight the superior accuracy of the proposed method when compared to conventional reconstruction methods, all while utilizing a minimal amount of measured projection data.
</details>
<details>
<summary>摘要</summary>
通过声光相互作用，声学探测提供了一种不同于传统麦克风数组的方法。声场重建是声学探测中的一种先进技术，但现有的声场重建方法在外部场景下存在许多挑战。这篇论文介绍了一种新的外部声场重建方法，该方法基于圆形弧形抽样和二维外部声场重建方法。为评估该方法的有效性，该论文进行了数值仿真和实验室实验。结果表明，该方法比传统重建方法更高精度，同时只需要使用 minimal amount of measured projection data。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/cs.SD_2023_11_03/" data-id="clpxp045r0123fm88d2ce0gzu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/eess.AS_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T14:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/03/eess.AS_2023_11_03/">eess.AS - 2023-11-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SE-Territory-Monaural-Speech-Enhancement-Meets-the-Fixed-Virtual-Perceptual-Space-Mapping"><a href="#SE-Territory-Monaural-Speech-Enhancement-Meets-the-Fixed-Virtual-Perceptual-Space-Mapping" class="headerlink" title="SE Territory: Monaural Speech Enhancement Meets the Fixed Virtual Perceptual Space Mapping"></a>SE Territory: Monaural Speech Enhancement Meets the Fixed Virtual Perceptual Space Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01679">http://arxiv.org/abs/2311.01679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinmeng Xu, Jibin Wu, Xiaoyong Wei, Yan Liu, Richard So, Yuhong Yang, Weiping Tu, Kay Chen Tan</li>
<li>for: 提高单麦口音频噪声纠正性能</li>
<li>methods: 提出了一种将单麦口音频Mapping到固定的Simulation空间中，以便更好地 отли别目标speech和噪声。这种方法基于二stage多任务学习框架，首先使用supervised speech mapping块将单麦口音频映射到虚拟空间中，然后使用cross-attention capture虚拟空间中的虚拟方向信息，以提高target speech的提取。</li>
<li>results: 对比其他最新的单麦口音频纠正方法，提出的SE-TerrNet显著超越了它们，both in terms of speech quality和语音可读性。<details>
<summary>Abstract</summary>
Monaural speech enhancement has achieved remarkable progress recently. However, its performance has been constrained by the limited spatial cues available at a single microphone. To overcome this limitation, we introduce a strategy to map monaural speech into a fixed simulation space for better differentiation between target speech and noise. Concretely, we propose SE-TerrNet, a novel monaural speech enhancement model featuring a virtual binaural speech mapping network via a two-stage multi-task learning framework. In the first stage, monaural noisy input is projected into a virtual space using supervised speech mapping blocks, creating binaural representations. These blocks synthesize binaural noisy speech from monaural input via an ideal binaural room impulse response. The synthesized output assigns speech and noise sources to fixed directions within the perceptual space. In the second stage, the obtained binaural features from the first stage are aggregated. This aggregation aims to decrease pattern discrepancies between the mapped binaural and original monaural features, achieved by implementing an intermediate fusion module. Furthermore, this stage incorporates the utilization of cross-attention to capture the injected virtual spatial information to improve the extraction of the target speech. Empirical studies highlight the effectiveness of virtual spatial cues in enhancing monaural speech enhancement. As a result, the proposed SE-TerrNet significantly surpasses the recent monaural speech enhancement methods in terms of both speech quality and intelligibility.
</details>
<details>
<summary>摘要</summary>
单声 speech 增强已经在最近得到了惊人的进步，但其表现受到单一麦克风提供的空间讯号限制。为了突破这个限制，我们提出了将单声 speech 映射到固定的 simulationspace 中，以更好地区分target speech 和噪声。具体来说，我们提出了 SE-TerrNet，一个新的单声 speech 增强模型，拥有一个通过二阶段多任务学习框架的虚拟 binatural speech 映射网络。在第一阶段，单声噪音输入被投射到虚拟空间中，使用supervised speech 映射封页 synthesize binatural noisy speech from monaural input，这些封页使得speech和噪声源分配到固定的方向 within the perceptual space。在第二阶段，获得的虚拟空间中的特征被聚合，以减少对映射后的单声 speech 和原始单声 input 的模式差异，这是通过实现一个中继融合模组来实现的。此外，这个阶段还包括利用跨关注处理来捕捉在虚拟空间中注射的虚拟空间信息，以提高对target speech的抽取。 empirical studies 显示虚拟空间信息在增强单声 speech 中发挥了惊人的作用。因此，提案的 SE-TerrNet 在比较 recent monaural speech enhancement methods 的情况下，实现了显著的提高。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/eess.AS_2023_11_03/" data-id="clpxp047f0166fm882slrd4vr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/cs.CV_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T13:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/03/cs.CV_2023_11_03/">cs.CV - 2023-11-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="EmerNeRF-Emergent-Spatial-Temporal-Scene-Decomposition-via-Self-Supervision"><a href="#EmerNeRF-Emergent-Spatial-Temporal-Scene-Decomposition-via-Self-Supervision" class="headerlink" title="EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision"></a>EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02077">http://arxiv.org/abs/2311.02077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NVlabs/EmerNeRF">https://github.com/NVlabs/EmerNeRF</a></li>
<li>paper_authors: Jiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Seung Wook Kim, Boyi Li, Tong Che, Danfei Xu, Sanja Fidler, Marco Pavone, Yue Wang</li>
<li>for: 这篇论文旨在学习动态驾驶场景的空间-时间表示。</li>
<li>methods: 该方法基于神经场，同时捕捉场景的几何学、外观、运动和 semantics，通过自我启发来实现。</li>
<li>results: 该方法在感知器模拟中达到了状态前的最佳性能，与之前的方法相比，在静态 (+2.93 PSNR) 和动态 (+3.70 PSNR) 场景中都有显著的改善。此外，通过提高4D空间时间特征的semantic泛化，进一步提高3D感知性能。<details>
<summary>Abstract</summary>
We present EmerNeRF, a simple yet powerful approach for learning spatial-temporal representations of dynamic driving scenes. Grounded in neural fields, EmerNeRF simultaneously captures scene geometry, appearance, motion, and semantics via self-bootstrapping. EmerNeRF hinges upon two core components: First, it stratifies scenes into static and dynamic fields. This decomposition emerges purely from self-supervision, enabling our model to learn from general, in-the-wild data sources. Second, EmerNeRF parameterizes an induced flow field from the dynamic field and uses this flow field to further aggregate multi-frame features, amplifying the rendering precision of dynamic objects. Coupling these three fields (static, dynamic, and flow) enables EmerNeRF to represent highly-dynamic scenes self-sufficiently, without relying on ground truth object annotations or pre-trained models for dynamic object segmentation or optical flow estimation. Our method achieves state-of-the-art performance in sensor simulation, significantly outperforming previous methods when reconstructing static (+2.93 PSNR) and dynamic (+3.70 PSNR) scenes. In addition, to bolster EmerNeRF's semantic generalization, we lift 2D visual foundation model features into 4D space-time and address a general positional bias in modern Transformers, significantly boosting 3D perception performance (e.g., 37.50% relative improvement in occupancy prediction accuracy on average). Finally, we construct a diverse and challenging 120-sequence dataset to benchmark neural fields under extreme and highly-dynamic settings.
</details>
<details>
<summary>摘要</summary>
我们介绍EmerNeRF，一种简单 yet powerful的方法，用于学习动态驾驶场景的空间-时间表示。基于神经场，EmerNeRF同时捕捉场景的几何结构、外观、运动和 semantics，通过自我启发。EmerNeRF的两个核心组成部分是：首先，它将场景分解为静止和动态场景两部分。这种分解是通过自我超级视图来实现，从而允许我们的模型从通用的各种数据源中学习。其次，EmerNeRF将动态场景中的引导流场景参数化，并使用这个流场景来进一步归并多帧特征，提高动态对象的渲染精度。将这三个场景（静止、动态和流）相互融合，使得EmerNeRF可以自主地表示高度动态的场景，不需要基于真实物理对象的批注或先进的模型来进行动态对象分 segmentation或光学流计算。我们的方法在感知器模拟中达到了状态的最佳性能，与之前的方法相比，在重建静止 (+2.93 PSNR) 和动态 (+3.70 PSNR) 场景中显著地超越。此外，为增强EmerNeRF的 semantic泛化性，我们将2D视觉基础模型特征抬到4D空间-时间中，并解决现代Transformers中的通用位置偏好，显著提高3D感知性能（例如，均值上的37.50%相对改进率）。最后，我们构建了一个多样化和挑战性的120序列数据集，用于评测神经场下的极端和高度动态设置。
</details></li>
</ul>
<hr>
<h2 id="Learning-Historical-Status-Prompt-for-Accurate-and-Robust-Visual-Tracking"><a href="#Learning-Historical-Status-Prompt-for-Accurate-and-Robust-Visual-Tracking" class="headerlink" title="Learning Historical Status Prompt for Accurate and Robust Visual Tracking"></a>Learning Historical Status Prompt for Accurate and Robust Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02072">http://arxiv.org/abs/2311.02072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenrui Cai, Qingjie Liu, Yunhong Wang</li>
<li>for: 提高跟踪性能（improve tracking performance）</li>
<li>methods: 增强历史信息提供（enhance the provision of historical information），使用搜索区域特征来引入历史外观信息（use search region features to introduce historical appearance information），利用历史位置信息构建精细的目标掩模（construct refined masks of the target using historical position information）</li>
<li>results: 对LaSOT、LaSOT ext、GOT10k和NfS进行了实验，并显示了与所有状态的前一个approaches的比较优异（outperforms all state-of-the-art approaches），并且 Module exhibits strong generality and can be seamlessly integrated into trackers to improve tracking performance.<details>
<summary>Abstract</summary>
Most trackers perform template and search region similarity matching to find the most similar object to the template during tracking. However, they struggle to make prediction when the target appearance changes due to the limited historical information introduced by roughly cropping the current search region based on the predicted result of previous frame. In this paper, we identify that the central impediment to improving the performance of existing trackers is the incapacity to integrate abundant and effective historical information. To address this issue, we propose a Historical Information Prompter (HIP) to enhance the provision of historical information. We also build HIPTrack upon HIP module. HIP is a plug-and-play module that make full use of search region features to introduce historical appearance information. It also incorporates historical position information by constructing refined mask of the target. HIP is a lightweight module to generate historical information prompts. By integrating historical information prompts, HIPTrack significantly enhances the tracking performance without the need to retrain the backbone. Experimental results demonstrate that our method outperforms all state-of-the-art approaches on LaSOT, LaSOT ext, GOT10k and NfS. Futhermore, HIP module exhibits strong generality and can be seamlessly integrated into trackers to improve tracking performance. The source code and models will be released for further research.
</details>
<details>
<summary>摘要</summary>
大多数跟踪器在跟踪过程中使用模板和搜索区域相似性匹配来找到最相似的目标对象。然而，当目标外观发生变化时，他们很难作出预测，因为现有的历史信息受限，通常是基于上一帧预测结果粗略剪辑的当前搜索区域的信息。在这篇论文中，我们认为现有跟踪器的主要障碍是缺乏充分和有效的历史信息的集成。为解决这个问题，我们提出了历史信息推动器（HIP）模块，用于增强历史信息的提供。我们还构建了基于HIP模块的HIPTrack跟踪器。HIP是一个轻量级的模块，可以充分利用搜索区域特征来引入历史外观信息，并将历史位置信息通过构建高精度的目标掩蔽来增强。我们的方法在LaSOT、LaSOT ext、GOT10k和NfS等四个测试集上进行了实验，结果表明我们的方法超过了所有现有的方法。此外，HIP模块表现出了强大的通用性，可以轻松地与跟踪器集成，以提高跟踪性能。我们将发布源代码和模型，以便进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="LOTUS-Continual-Imitation-Learning-for-Robot-Manipulation-Through-Unsupervised-Skill-Discovery"><a href="#LOTUS-Continual-Imitation-Learning-for-Robot-Manipulation-Through-Unsupervised-Skill-Discovery" class="headerlink" title="LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery"></a>LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02058">http://arxiv.org/abs/2311.02058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weikang Wan, Yifeng Zhu, Rutav Shah, Yuke Zhu</li>
<li>for: 这篇论文旨在描述一种可以让物理机器人不断学习并解决新的抓取任务的 kontinuierliches Imitation Learning 算法（LOTUS）。</li>
<li>methods: LOTUS 使用一种开放词汇视觉模型进行不断发现新技能，并通过更新现有技能来避免过去任务的恐慌遗忘，以解决视觉基于抓取任务的长期学习过程。</li>
<li>results: 相比于先前的基elines，LOTUS 在成功率上高出11%，表明它在知识传递方面具有优势。更多结果和视频可以在项目网站上找到：<a target="_blank" rel="noopener" href="https://ut-austin-rpl.github.io/Lotus/">https://ut-austin-rpl.github.io/Lotus/</a>.<details>
<summary>Abstract</summary>
We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More results and videos can be found on the project website: https://ut-austin-rpl.github.io/Lotus/.
</details>
<details>
<summary>摘要</summary>
我团队现在介绍一种名为LOTUS的持续学习算法，它使得物理机器人可以不断学习并解决新的抓取任务，从机器人的生命周期开始。LOTUS的核心思想在于建立一个不断增长的技能库，从一系列新任务中提取出技能的循环征例。LOTUS开始于持续技能发现过程，使用一个开放词汇视模型，从不分段的示例中提取出技能。继续技能发现更新现有技能，以避免过去任务的恐慌忘记，并添加新技能来解决新任务。LOTUS训练一个灵活组合各种技能的元控制器，以解决视觉基于的机器人 manipulate 任务在持续学习过程中。我们的广泛实验表明，LOTUS比前方法提高了11%的成功率，表明它在知识传递方面的优势比前方法更高。更多结果和视频可以在项目网站上找到：https://ut-austin-rpl.github.io/Lotus/.
</details></li>
</ul>
<hr>
<h2 id="Occlusion-Aware-2D-and-3D-Centerline-Detection-for-Urban-Driving-via-Automatic-Label-Generation"><a href="#Occlusion-Aware-2D-and-3D-Centerline-Detection-for-Urban-Driving-via-Automatic-Label-Generation" class="headerlink" title="Occlusion-Aware 2D and 3D Centerline Detection for Urban Driving via Automatic Label Generation"></a>Occlusion-Aware 2D and 3D Centerline Detection for Urban Driving via Automatic Label Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02044">http://arxiv.org/abs/2311.02044</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Paz, Narayanan E. Ranganatha, Srinidhi K. Srinivas, Yunchao Yao, Henrik I. Christensen</li>
<li>for: 本研究旨在探索和确定在高度动态城市驾驶场景下的路径topology信息，包括2D和3D两种情况。</li>
<li>methods: 我们提出了一种自动生成标签过程和 occlusion 处理策略，以处理各种干扰和堵塞情况。我们还实现了多种中心线探测方法的比较研究，以评估这些方法的性能和可解性。</li>
<li>results: 我们的研究表明，我们的方法可以在不同的感知器配置下进行适应，并且在实际场景中展示了优秀的性能和实用性。我们还公开发布了我们的数据集和实验模型，以便进一步的研究和应用。<details>
<summary>Abstract</summary>
This research work seeks to explore and identify strategies that can determine road topology information in 2D and 3D under highly dynamic urban driving scenarios. To facilitate this exploration, we introduce a substantial dataset comprising nearly one million automatically labeled data frames. A key contribution of our research lies in developing an automatic label-generation process and an occlusion handling strategy. This strategy is designed to model a wide range of occlusion scenarios, from mild disruptions to severe blockages. Furthermore, we present a comprehensive ablation study wherein multiple centerline detection methods are developed and evaluated. This analysis not only benchmarks the performance of various approaches but also provides valuable insights into the interpretability of these methods. Finally, we demonstrate the practicality of our methods and assess their adaptability across different sensor configurations, highlighting their versatility and relevance in real-world scenarios. Our dataset and experimental models are publicly available.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Unsupervised-Object-Detection-From-LiDAR-Point-Clouds"><a href="#Towards-Unsupervised-Object-Detection-From-LiDAR-Point-Clouds" class="headerlink" title="Towards Unsupervised Object Detection From LiDAR Point Clouds"></a>Towards Unsupervised Object Detection From LiDAR Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02007">http://arxiv.org/abs/2311.02007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lunjun Zhang, Anqi Joyce Yang, Yuwen Xiong, Sergio Casas, Bin Yang, Mengye Ren, Raquel Urtasun</li>
<li>for: 这个论文研究了自驾报道Scene中无监督物体检测的问题。</li>
<li>methods: 该方法利用（i）点云密集区域的点集 clustering，（ii）时间一致性来过滤噪杂的无监督检测，（iii） CNN的翻译对称性来扩展自动标签到远距离，以及（iv）自我超参数。</li>
<li>results: 该方法能够在零批训练的情况下，无需监督训练，在稀疏、远距离区域中检测物体，并且能够不断自我改进。在自驾报道场景中，提出了一个新的规划中心的感知指标，基于距离Collision。实验表明，我们的无监督物体检测器在PandaSet和Argoverse 2 Sensor dataset上显著超越了无监督基线。<details>
<summary>Abstract</summary>
In this paper, we study the problem of unsupervised object detection from 3D point clouds in self-driving scenes. We present a simple yet effective method that exploits (i) point clustering in near-range areas where the point clouds are dense, (ii) temporal consistency to filter out noisy unsupervised detections, (iii) translation equivariance of CNNs to extend the auto-labels to long range, and (iv) self-supervision for improving on its own. Our approach, OYSTER (Object Discovery via Spatio-Temporal Refinement), does not impose constraints on data collection (such as repeated traversals of the same location), is able to detect objects in a zero-shot manner without supervised finetuning (even in sparse, distant regions), and continues to self-improve given more rounds of iterative self-training. To better measure model performance in self-driving scenarios, we propose a new planning-centric perception metric based on distance-to-collision. We demonstrate that our unsupervised object detector significantly outperforms unsupervised baselines on PandaSet and Argoverse 2 Sensor dataset, showing promise that self-supervision combined with object priors can enable object discovery in the wild. For more information, visit the project website: https://waabi.ai/research/oyster
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了无监督对象检测从3D点云中的问题。我们提出了一种简单 yet有效的方法，利用以下四个方法：（i）点云归一在近距离地区 dense point clouds 中进行归一，（ii）时间一致性来筛除噪杂无监督检测，（iii） CNN 的转换对称性来扩展自动标签至长距离，以及（iv）自我超vision来改进自己。我们的方法，命名为 OYSTER（对象发现 via 空间-时间细化），不需要数据采集中的任何限制（如重复 traverse 同一个位置），能够在零shot 模式下检测对象，而且在稀疏、远方地区也能够做出适当的检测。我们还提出了一个新的准备中心 metric 来衡量自驾报uning 中的模型性能，并在 PandaSet 和 Argoverse 2 Sensor 数据集上进行了证明。我们的无监督对象检测器在比较baseline 上表现出了显著的优势，这表明了自我超vision 结合对象预知可以在野外实现对象发现。更多信息请参考我们项目网站：https://waabi.ai/research/oyster。
</details></li>
</ul>
<hr>
<h2 id="A-Structured-Pruning-Algorithm-for-Model-based-Deep-Learning"><a href="#A-Structured-Pruning-Algorithm-for-Model-based-Deep-Learning" class="headerlink" title="A Structured Pruning Algorithm for Model-based Deep Learning"></a>A Structured Pruning Algorithm for Model-based Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02003">http://arxiv.org/abs/2311.02003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chicago Park, Weijie Gan, Zihao Zou, Yuyang Hu, Zhixin Sun, Ulugbek S. Kamilov</li>
<li>for: 解决图像反问题，提高深度学习模型的计算效率。</li>
<li>methods: 使用结构化剪辑算法（SPADE）剪辑MBDL网络中不必要的参数，并对剪辑后的网络进行微调以保持性能。</li>
<li>results: SPADE可以大幅降低MBDL网络的测试时间计算复杂度，保持竞争力性能。<details>
<summary>Abstract</summary>
There is a growing interest in model-based deep learning (MBDL) for solving imaging inverse problems. MBDL networks can be seen as iterative algorithms that estimate the desired image using a physical measurement model and a learned image prior specified using a convolutional neural net (CNNs). The iterative nature of MBDL networks increases the test-time computational complexity, which limits their applicability in certain large-scale applications. We address this issue by presenting structured pruning algorithm for model-based deep learning (SPADE) as the first structured pruning algorithm for MBDL networks. SPADE reduces the computational complexity of CNNs used within MBDL networks by pruning its non-essential weights. We propose three distinct strategies to fine-tune the pruned MBDL networks to minimize the performance loss. Each fine-tuning strategy has a unique benefit that depends on the presence of a pre-trained model and a high-quality ground truth. We validate SPADE on two distinct inverse problems, namely compressed sensing MRI and image super-resolution. Our results highlight that MBDL models pruned by SPADE can achieve substantial speed up in testing time while maintaining competitive performance.
</details>
<details>
<summary>摘要</summary>
有一个增长的兴趣在model-based deep learning（MBDL）方面，用于解决图像反向问题。MBDL网络可以看作是迭代算法，利用物理测量模型和学习的图像先验（CNNs）来估算所需的图像。迭代性的MBDL网络会增加测试时的计算复杂性，限制其在某些大规模应用程序中的应用。我们解决这个问题，提出了结构化剪辑算法 для model-based deep learning（SPADE），是MBDL网络中非必要的Weight剪辑算法。我们提出了三种不同的细化策略，以适应不同的预训练模型和高质量的测试数据。每种细化策略具有独特的优点，它们取决于预训练模型和测试数据的存在。我们验证SPADE在压缩感知MRI和图像超分辨率两个不同的反向问题上，可以实现显著的测试时间减少，而保持竞争力的性能。
</details></li>
</ul>
<hr>
<h2 id="Detection-of-keratoconus-Diseases-using-deep-Learning"><a href="#Detection-of-keratoconus-Diseases-using-deep-Learning" class="headerlink" title="Detection of keratoconus Diseases using deep Learning"></a>Detection of keratoconus Diseases using deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01996">http://arxiv.org/abs/2311.01996</a></li>
<li>repo_url: None</li>
<li>paper_authors: AKM Enzam-Ul Haque, Golam Rabbany, Md. Siam</li>
<li>for: 这个研究的目的是评估不同的深度学习模型在诊断 keratoconus 疾病中的表现。</li>
<li>methods: 这个研究使用了五种不同的 CNN 深度学习架构（DenseNet201、InceptionV3、MobileNetV2、VGG19、Xception）进行比较。</li>
<li>results: 研究结果显示，使用 DenseNet201 架构的模型在 keratoconus 疾病识别中表现出色，其精度为 89.14%，precision 为 89.51%，recall 为 88.75%，F1 分数为 89.08%。这些结果显示了这个模型在实际应用中的稳定性和可靠性。<details>
<summary>Abstract</summary>
One of the most serious corneal disorders, keratoconus is difficult to diagnose in its early stages and can result in blindness. This illness, which often appears in the second decade of life, affects people of all sexes and races. Convolutional neural networks (CNNs), one of the deep learning approaches, have recently come to light as particularly promising tools for the accurate and timely diagnosis of keratoconus. The purpose of this study was to evaluate how well different D-CNN models identified keratoconus-related diseases. To be more precise, we compared five different CNN-based deep learning architectures (DenseNet201, InceptionV3, MobileNetV2, VGG19, Xception). In our comprehensive experimental analysis, the DenseNet201-based model performed very well in keratoconus disease identification in our extensive experimental research. This model outperformed its D-CNN equivalents, with an astounding accuracy rate of 89.14% in three crucial classes: Keratoconus, Normal, and Suspect. The results demonstrate not only the stability and robustness of the model but also its practical usefulness in real-world applications for accurate and dependable keratoconus identification. In addition, D-CNN DenseNet201 performs extraordinarily well in terms of precision, recall rates, and F1 scores in addition to accuracy. These measures validate the model's usefulness as an effective diagnostic tool by highlighting its capacity to reliably detect instances of keratoconus and to reduce false positives and negatives.
</details>
<details>
<summary>摘要</summary>
一种非常严重的角膜疾病，扩散性角膜病（keratoconus）难以在早期 диагности，可能导致失明。这种疾病通常在第二个decades of life出现，影响男女老少都有。深度学习方法（D-CNN），特别是深度神经网络（CNN），最近才被发现对早期扩散性角膜病的准确诊断表现出极高的抗锋性和可靠性。本研究的目的是评估不同D-CNN模型在扩散性角膜病识别方面的表现。具体来说，我们对五种不同的CNN-基本架构（DenseNet201、InceptionV3、MobileNetV2、VGG19、Xception）进行了比较。在我们的广泛的实验分析中，基于DenseNet201的模型在扩散性角膜病识别方面表现非常出色，其准确率为89.14%，在三个关键类别（扩散性角膜病、正常和可疑）中表现出极高的稳定性和可靠性。结果表明这种模型不仅在实际应用中具有高度的可靠性和稳定性，还能够准确地检测扩散性角膜病的实例，降低假阳性和假阴性。此外，D-CNN DenseNet201在准确率、回暗率和F1分数方面也表现出了极高的表现，这些指标 validate了这种模型在实际应用中的有效性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Scale-Pretrained-Vision-Foundation-Models-for-Label-Efficient-3D-Point-Cloud-Segmentation"><a href="#Leveraging-Large-Scale-Pretrained-Vision-Foundation-Models-for-Label-Efficient-3D-Point-Cloud-Segmentation" class="headerlink" title="Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation"></a>Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01989">http://arxiv.org/abs/2311.01989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shichao Dong, Fayao Liu, Guosheng Lin</li>
<li>for: 这个论文的目的是将大规模的预训模型（如Segment-Anything Model和Contrastive Language-Image Pre-training）应用于3D点云分类任务中。</li>
<li>methods: 这个方法首先使用不同的大规模视觉基模型进行2D semantic mask的初步预测。然后将这些mask预测对RGB-D影像序列进行投射，以生成3D semantic pseudo标签。我们还引入了一个semantic label fusion策略，将所有结果联合成一个统一的3D semantic pseudo标签。</li>
<li>results: 这个方法在ScanNet dataset上进行了实验，结果显示了采用通用2D基模型解决3D点云分类任务的有效性。<details>
<summary>Abstract</summary>
Recently, large-scale pre-trained models such as Segment-Anything Model (SAM) and Contrastive Language-Image Pre-training (CLIP) have demonstrated remarkable success and revolutionized the field of computer vision. These foundation vision models effectively capture knowledge from a large-scale broad data with their vast model parameters, enabling them to perform zero-shot segmentation on previously unseen data without additional training. While they showcase competence in 2D tasks, their potential for enhancing 3D scene understanding remains relatively unexplored. To this end, we present a novel framework that adapts various foundational models for the 3D point cloud segmentation task. Our approach involves making initial predictions of 2D semantic masks using different large vision models. We then project these mask predictions from various frames of RGB-D video sequences into 3D space. To generate robust 3D semantic pseudo labels, we introduce a semantic label fusion strategy that effectively combines all the results via voting. We examine diverse scenarios, like zero-shot learning and limited guidance from sparse 2D point labels, to assess the pros and cons of different vision foundation models. Our approach is experimented on ScanNet dataset for 3D indoor scenes, and the results demonstrate the effectiveness of adopting general 2D foundation models on solving 3D point cloud segmentation tasks.
</details>
<details>
<summary>摘要</summary>
最近，大规模的预训练模型如划分任何模型（SAM）和语言图像对比预训练（CLIP）在计算机视觉领域表现了非凡的成功，并对该领域产生了革命性的变革。这些基础视觉模型通过它们的庞大模型参数，能够从大规模的广泛数据中捕捉知识，并在未经训练的情况下，对新的数据进行零Instance分割。虽然它们在2D任务中显示出了能力，但它们对3D场景理解的潜力还尚未得到了充分的探索。为此，我们提出了一种新的框架，用于适应不同的基础视觉模型为3D点云分割任务。我们的方法包括使用不同的大规模视觉模型来初步预测2D semanticmask。我们 then将这些mask预测从不同的RGB-D视频序列中的多个帧 proyect到3D空间。为生成Robust的3D semantic pseudo标签，我们引入了一种semantic标签融合策略，通过投票来有效地结合所有结果。我们在不同的enario，如零shot学习和基于稀薄2D点标签的限制指导下，评估了不同的视觉基础模型的优缺点。我们的方法在ScanNet数据集上进行了实验，结果表明，采用通用的2D基础模型可以解决3D点云分割任务。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Image-Transport-on-Sparse-Dictionaries"><a href="#Optimal-Image-Transport-on-Sparse-Dictionaries" class="headerlink" title="Optimal Image Transport on Sparse Dictionaries"></a>Optimal Image Transport on Sparse Dictionaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01984">http://arxiv.org/abs/2311.01984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junqing Huang, Haihui Wang, Andreas Weiermann, Michael Ruzhansky</li>
<li>for: 这篇论文旨在提出一种基于稀疏表示和最优运输的图像传输算法，用于同时实现图像表示和变换。</li>
<li>methods: 论文使用稀疏表示压缩图像特征，然后根据这些特征编码生成两个学习的字典，最后使用最优运输计划来实现图像的传输和转换。</li>
<li>results: 论文通过实验表明，该算法可以具有高效率和高质量的图像传输和转换效果，并且可以应用于不同的图像转换任务，如图像颜色转换和艺术风格转换。<details>
<summary>Abstract</summary>
In this paper, we derive a novel optimal image transport algorithm over sparse dictionaries by taking advantage of Sparse Representation (SR) and Optimal Transport (OT). Concisely, we design a unified optimization framework in which the individual image features (color, textures, styles, etc.) are encoded using sparse representation compactly, and an optimal transport plan is then inferred between two learned dictionaries in accordance with the encoding process. This paradigm gives rise to a simple but effective way for simultaneous image representation and transformation, which is also empirically solvable because of the moderate size of sparse coding and optimal transport sub-problems. We demonstrate its versatility and many benefits to different image-to-image translation tasks, in particular image color transform and artistic style transfer, and show the plausible results for photo-realistic transferred effects.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 derivate了一种新的最优图像传输算法，利用图像稀疏表示（SR）和最优传输（OT）的优势。简单来说，我们设计了一个统一优化框架，在这个框架中，图像特征（颜色、文化、风格等）都是通过稀疏表示紧凑地编码的，然后根据编码过程来寻找两个学习的字典之间的最优传输计划。这种思想给出了一种简单 yet 有效的同时图像表示和变换方法，同时这也是可解决的因为稀疏编码和最优传输子问题的模式较小。我们在不同的图像转换任务中展示了它的多样性和多种优点，特别是图像颜色变换和艺术风格传递等，并显示了可信的转换效果。
</details></li>
</ul>
<hr>
<h2 id="Depth-guided-Free-space-Segmentation-for-a-Mobile-Robot"><a href="#Depth-guided-Free-space-Segmentation-for-a-Mobile-Robot" class="headerlink" title="Depth-guided Free-space Segmentation for a Mobile Robot"></a>Depth-guided Free-space Segmentation for a Mobile Robot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01966">http://arxiv.org/abs/2311.01966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Sevastopoulos, Joey Hussain, Stasinos Konstantopoulos, Vangelis Karkaletsis, Fillia Makedon</li>
<li>for: 本研究旨在提供一种精度的indoor自由空间分割方法，以便在各种复杂的indoor环境中提高自主迷 Navigation。</li>
<li>methods: 本方法基于不监督的Masking技术，使用正例实例生成分割标签，根据文本同质和深度均匀性。在这个步骤中，我们还生成了相对高深度的超像素，并将其与Dense Prediction Transformer（DPT）特征提取器进行对齐。</li>
<li>results: 我们的实验表明，在具有堆积物和复杂障碍物的Scene中，本方法能够显示出 suficient的性能。<details>
<summary>Abstract</summary>
Accurate indoor free-space segmentation is a challenging task due to the complexity and the dynamic nature that indoor environments exhibit. We propose an indoors free-space segmentation method that associates large depth values with navigable regions. Our method leverages an unsupervised masking technique that, using positive instances, generates segmentation labels based on textural homogeneity and depth uniformity. Moreover, we generate superpixels corresponding to areas of higher depth and align them with features extracted from a Dense Prediction Transformer (DPT). Using the estimated free-space masks and the DPT feature representation, a SegFormer model is fine-tuned on our custom-collected indoor dataset. Our experiments demonstrate sufficient performance in intricate scenarios characterized by cluttered obstacles and challenging identification of free space.
</details>
<details>
<summary>摘要</summary>
准确的indoor自由空间分割是一项复杂和动态环境下的挑战。我们提出了一种indoor自由空间分割方法，将大深度值关联到可行区域。我们的方法利用无监督的masking技术，通过正例实例生成分割标签，基于тексту冗余和深度均匀性。此外，我们生成了高深度区域对应的superpixel，并与Dense Prediction Transformer（DPT）提取的特征进行对应。使用估算的自由空间幕和DPT特征表示，我们在自定义indoor数据集上进行了SegFormer模型的微调。我们的实验表明，在叠拥障碍物和复杂识别自由空间的情况下，我们的方法具有足够的性能。
</details></li>
</ul>
<hr>
<h2 id="ProS-Facial-Omni-Representation-Learning-via-Prototype-based-Self-Distillation"><a href="#ProS-Facial-Omni-Representation-Learning-via-Prototype-based-Self-Distillation" class="headerlink" title="ProS: Facial Omni-Representation Learning via Prototype-based Self-Distillation"></a>ProS: Facial Omni-Representation Learning via Prototype-based Self-Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01929">http://arxiv.org/abs/2311.01929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xing Di, Yiyu Zheng, Xiaoming Liu, Yu Cheng</li>
<li>for: 本研究提出了一种新的无监督面表示学习方法，即基于原型的自我混合（ProS），以解决现有监督方法强依赖大量注释的脸部训练数据，带来数据收集和隐私问题。</li>
<li>methods: 我们提出了一种基于两个视Transformer（教师和学生模型）的方法，通过不同的扩展图像（裁剪、模糊、颜色等）进行训练。此外，我们还建立了一个面部意识的检索系统，并在扩展图像中进行了准备。为了提高学习的特征，我们引入了一个基于原型的匹配损失函数，将特征与一组可学习的原型进行对比。</li>
<li>results: 我们的方法在多种任务上达到了状态 искусственный的性能，包括特征预测、表情识别和面部对齐。此外，我们还进行了对synthetic face图像的预训练，并发现ProS在这种情况下也表现出了良好的性能。<details>
<summary>Abstract</summary>
This paper presents a novel approach, called Prototype-based Self-Distillation (ProS), for unsupervised face representation learning. The existing supervised methods heavily rely on a large amount of annotated training facial data, which poses challenges in terms of data collection and privacy concerns. To address these issues, we propose ProS, which leverages a vast collection of unlabeled face images to learn a comprehensive facial omni-representation. In particular, ProS consists of two vision-transformers (teacher and student models) that are trained with different augmented images (cropping, blurring, coloring, etc.). Besides, we build a face-aware retrieval system along with augmentations to obtain the curated images comprising predominantly facial areas. To enhance the discrimination of learned features, we introduce a prototype-based matching loss that aligns the similarity distributions between features (teacher or student) and a set of learnable prototypes. After pre-training, the teacher vision transformer serves as a backbone for downstream tasks, including attribute estimation, expression recognition, and landmark alignment, achieved through simple fine-tuning with additional layers. Extensive experiments demonstrate that our method achieves state-of-the-art performance on various tasks, both in full and few-shot settings. Furthermore, we investigate pre-training with synthetic face images, and ProS exhibits promising performance in this scenario as well.
</details>
<details>
<summary>摘要</summary>
The proposed method consists of two vision-transformers (teacher and student models) trained with different augmented images, such as cropping, blurring, and coloring. Additionally, a face-aware retrieval system is built to obtain curated images with predominantly facial areas. To enhance the discrimination of learned features, a prototype-based matching loss is introduced to align the similarity distributions between features and a set of learnable prototypes.After pre-training, the teacher vision transformer serves as a backbone for downstream tasks, including attribute estimation, expression recognition, and landmark alignment, which can be achieved through simple fine-tuning with additional layers. Extensive experiments show that our method achieves state-of-the-art performance on various tasks, both in full and few-shot settings. Moreover, we investigate pre-training with synthetic face images and find that ProS performs well in this scenario as well.
</details></li>
</ul>
<hr>
<h2 id="Contrast-Agnostic-Groupwise-Registration-by-Robust-PCA-for-Quantitative-Cardiac-MRI"><a href="#Contrast-Agnostic-Groupwise-Registration-by-Robust-PCA-for-Quantitative-Cardiac-MRI" class="headerlink" title="Contrast-Agnostic Groupwise Registration by Robust PCA for Quantitative Cardiac MRI"></a>Contrast-Agnostic Groupwise Registration by Robust PCA for Quantitative Cardiac MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01916">http://arxiv.org/abs/2311.01916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinqi Li, Yi Zhang, Yidong Zhao, Jan van Gemert, Qian Tao</li>
<li>for: 这个论文的目的是提出一种基于robust原理Component分析（rPCA）的新的运动补偿框架，以解决quantitative cardiac MRI中的基线图像准确 registrations问题。</li>
<li>methods: 该方法使用了robust原理Component分析（rPCA） decomposes quantitative cardiac MRI into low-rank and sparse components，并将groupwise CNN-based registration backbone integrate within the rPCA framework。</li>
<li>results: 实验表明，该方法可以提高registration perfomance，并 reducet quantitative mapping error in both in-domain (pre-contrast MOLLI) and out-of-domain (post-contrast MOLLI) inference。<details>
<summary>Abstract</summary>
Quantitative cardiac magnetic resonance imaging (MRI) is an increasingly important diagnostic tool for cardiovascular diseases. Yet, co-registration of all baseline images within the quantitative MRI sequence is essential for the accuracy and precision of quantitative maps. However, co-registering all baseline images from a quantitative cardiac MRI sequence remains a nontrivial task because of the simultaneous changes in intensity and contrast, in combination with cardiac and respiratory motion. To address the challenge, we propose a novel motion correction framework based on robust principle component analysis (rPCA) that decomposes quantitative cardiac MRI into low-rank and sparse components, and we integrate the groupwise CNN-based registration backbone within the rPCA framework. The low-rank component of rPCA corresponds to the quantitative mapping (i.e. limited degree of freedom in variation), while the sparse component corresponds to the residual motion, making it easier to formulate and solve the groupwise registration problem. We evaluated our proposed method on cardiac T1 mapping by the modified Look-Locker inversion recovery (MOLLI) sequence, both before and after the Gadolinium contrast agent administration. Our experiments showed that our method effectively improved registration performance over baseline methods without introducing rPCA, and reduced quantitative mapping error in both in-domain (pre-contrast MOLLI) and out-of-domain (post-contrast MOLLI) inference. The proposed rPCA framework is generic and can be integrated with other registration backbones.
</details>
<details>
<summary>摘要</summary>
现代心脏磁共振成像（MRI）已成为心血管疾病诊断中越来越重要的工具。然而，在量化MRI序列中所有基线图像的协调是必要的，以确保量化地图的准确性和精度。然而，在量化心脏MRI序列中协调所有基线图像仍然是一项困难的任务，因为同时出现的是图像Intensity和对比度的变化，以及心跳和呼吸动作的运动。为解决这个挑战，我们提出了一种基于robust主成分分析（rPCA）的新的动态恢复框架，该框架可以将量化心脏MRI分解成低级别和稀疏组件。我们将组合式 convolutional neural network（CNN）基于的集群注registrations backboneintegrated within the rPCA framework。低级别rPCA组件对应于量化映射（即有限度的变化），而稀疏组件对应于剩余运动，这使得更加容易解决集群注registrations问题。我们通过使用MOLLI序列进行修改的Look-Locker倒映重建（MOLLI）序列进行测试，并在不使用rPCA的基eline方法上进行比较。我们的实验表明，我们的方法可以有效地提高注registrations性能，并降低在域域（预contrast MOLLI）和离域域（后contrast MOLLI）的量化映射误差。我们的提议的rPCA框架是通用的，可以与其他注registrations backbone结合使用。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-assessment-of-AR-assisted-neurosurgery-systems"><a href="#End-to-End-assessment-of-AR-assisted-neurosurgery-systems" class="headerlink" title="End-to-End assessment of AR-assisted neurosurgery systems"></a>End-to-End assessment of AR-assisted neurosurgery systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01912">http://arxiv.org/abs/2311.01912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Bagheri, Farhad Piri, Hadi Digale, Saem Sattarzadeh, Mohammad Reza Mohammadi</li>
<li>for: 这项研究旨在描述一种基于扩展现实技术的 neurosurgery 系统，并评估该系统在医学操作中的可靠性和精度。</li>
<li>methods: 该研究使用了多种技术来评估 AR-assisted neurosurgery 系统，包括 registratin and tracking 技术，以及基于物理反馈的评估方法。</li>
<li>results: 研究发现，尽管系统可能会出现注射和跟踪错误，但Physical feedback 可以减少error caused by hologram displacement。然而，缺乏可见反馈对 HOLOgram 的影响不значиatives。<details>
<summary>Abstract</summary>
Augmented Reality (AR) has emerged as a significant advancement in surgical procedures, offering a solution to the challenges posed by traditional neuronavigation methods. These conventional techniques often necessitate surgeons to split their focus between the surgical site and a separate monitor that displays guiding images. Over the years, many systems have been developed to register and track the hologram at the targeted locations, each employed its own evaluation technique. On the other hand, hologram displacement measurement is not a straightforward task because of various factors such as occlusion, Vengence-Accomodation Conflict, and unstable holograms in space. In this study, we explore and classify different techniques for assessing an AR-assisted neurosurgery system and propose a new technique to systematize the assessment procedure. Moreover, we conduct a deeper investigation to assess surgeon error in the pre- and intra-operative phases of the surgery based on the respective feedback given. We found that although the system can undergo registration and tracking errors, physical feedback can significantly reduce the error caused by hologram displacement. However, the lack of visual feedback on the hologram does not have a significant effect on the user 3D perception.
</details>
<details>
<summary>摘要</summary>
augmened reality (AR) 在 neurosurgery 中发展出了重要的进步，解决了传统神经导航方法的挑战。这些传统技术 часто需要Surgeon 同时分心于手术Site 和一个分开的显示器上的导航图像。随着时间的推移，许多系统被开发出来了register 和跟踪 HOLOgram 的方法，每种系统都使用了自己的评估技巧。然而， HOLOgram 的偏移量测量并不是一个简单的任务，因为 occlusion、 Vengence-Accomodation Conflict 和不稳定的 HOLOgram 在空间中。在这种研究中，我们探讨了不同的 AR-assisted neurosurgery 系统评估技巧，并提出了一种新的评估过程系统。此外，我们进行了更深入的调查，以评估手术阶段 pre- 和 intra- 操作期间的Surgeon 错误，基于相应的反馈。我们发现，尽管系统可能会出现 registration 和 tracking 错误，但physical feedback 可以减少 HOLOgram 偏移量所导致的错误。然而，缺乏 HOLOgram 的视觉反馈并没有对用户3D认知产生重要影响。
</details></li>
</ul>
<hr>
<h2 id="LLM-driven-Multimodal-Target-Volume-Contouring-in-Radiation-Oncology"><a href="#LLM-driven-Multimodal-Target-Volume-Contouring-in-Radiation-Oncology" class="headerlink" title="LLM-driven Multimodal Target Volume Contouring in Radiation Oncology"></a>LLM-driven Multimodal Target Volume Contouring in Radiation Oncology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01908">http://arxiv.org/abs/2311.01908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Jin Sung Kim, Jong Chul Ye</li>
<li>for: 这个研究旨在应用大语言模型（LLM）来整合图像和文本资讯，以解决辐照疗法中难以进行的目标体组定义问题。</li>
<li>methods: 本研究使用了LLM来验证并整合图像和文本资讯，并在乳癌辐照疗法中进行了静脉体组定义。</li>
<li>results: 研究结果显示，该模型在实际应用环境下具有了更好的一致性和数据效率，并在较具有复杂性的乳癌辐照疗法中进行了有效的静脉体组定义。<details>
<summary>Abstract</summary>
Target volume contouring for radiation therapy is considered significantly more challenging than the normal organ segmentation tasks as it necessitates the utilization of both image and text-based clinical information. Inspired by the recent advancement of large language models (LLMs) that can facilitate the integration of the textural information and images, here we present a novel LLM-driven multi-modal AI that utilizes the clinical text information and is applicable to the challenging task of target volume contouring for radiation therapy, and validate it within the context of breast cancer radiation therapy target volume contouring. Using external validation and data-insufficient environments, which attributes highly conducive to real-world applications, we demonstrate that the proposed model exhibits markedly improved performance compared to conventional vision-only AI models, particularly exhibiting robust generalization performance and data-efficiency. To our best knowledge, this is the first LLM-driven multimodal AI model that integrates the clinical text information into target volume delineation for radiation oncology.
</details>
<details>
<summary>摘要</summary>
目标体积辐射治疗中的预测体积辐射是与常见器官分割任务相比许多更加困难，因为它需要利用图像和文本信息。受最近大语言模型（LLMs）的发展启发，我们现在提出了一种基于文本信息的多modal AI，可以利用临床文本信息，并适用于难以完成的辐射治疗target体积辐射预测任务。在乳腺癌辐射治疗target体积辐射预测中进行了验证。我们使用了外部验证和数据缺乏环境，这些环境对实际应用来说非常有利，并证明了我们提出的模型在与传统视觉只AI模型相比 Displayed remarkable improved performance，特别是在robust generalization和数据效率方面。到目前为止，这是首个基于文本信息的多modal AI模型，可以在辐射肿瘤治疗中结合临床文本信息进行target体积辐射预测。
</details></li>
</ul>
<hr>
<h2 id="From-Chaos-to-Calibration-A-Geometric-Mutual-Information-Approach-to-Target-Free-Camera-LiDAR-Extrinsic-Calibration"><a href="#From-Chaos-to-Calibration-A-Geometric-Mutual-Information-Approach-to-Target-Free-Camera-LiDAR-Extrinsic-Calibration" class="headerlink" title="From Chaos to Calibration: A Geometric Mutual Information Approach to Target-Free Camera LiDAR Extrinsic Calibration"></a>From Chaos to Calibration: A Geometric Mutual Information Approach to Target-Free Camera LiDAR Extrinsic Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01905">http://arxiv.org/abs/2311.01905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Borer, Jeremy Tschirner, Florian Ölsner, Stefan Milz</li>
<li>for: 这 paper 是为了提出一种无需基准数据的目标自由外部均衡算法，用于自动驾驶车辆中的感知融合。</li>
<li>methods: 这 paper 使用的方法是基于分析牵引信息的 mutual information 方法，而这些方法最初是在 2012 年提出的。</li>
<li>results: 作者们在使用 KITTI 和 KITTI-360  fisheye 数据集进行证明，并表明其提出的改进方法可以准确地均衡 camera-LiDAR 外部参数。<details>
<summary>Abstract</summary>
Sensor fusion is vital for the safe and robust operation of autonomous vehicles. Accurate extrinsic sensor to sensor calibration is necessary to accurately fuse multiple sensor's data in a common spatial reference frame. In this paper, we propose a target free extrinsic calibration algorithm that requires no ground truth training data, artificially constrained motion trajectories, hand engineered features or offline optimization and that is accurate, precise and extremely robust to initialization error.   Most current research on online camera-LiDAR extrinsic calibration requires ground truth training data which is impossible to capture at scale. We revisit analytical mutual information based methods first proposed in 2012 and demonstrate that geometric features provide a robust information metric for camera-LiDAR extrinsic calibration. We demonstrate our proposed improvement using the KITTI and KITTI-360 fisheye data set.
</details>
<details>
<summary>摘要</summary>
感测融合是自动驾驶车辆安全和稳定运行的关键。精准的外部感测器到感测器准确匹配是必要的，以将多个感测器的数据在共同空间参照幂中融合准确。在这篇论文中，我们提出了无需地面真实数据、人工限制的运动轨迹、手工设计特征或线上优化的无Target extrinsic准确报表算法。现有大多数在线相机-LiDAR外部准确匹配研究均需要地面真实数据，但这些数据难以在大规模上采集。我们回到2012年提出的分析约束基础方法，并证明了光学特征提供了Robust信息度量的摄像机-LiDAR外部准确匹配。我们使用KITTI和KITTI-360鱼眼数据集来示出我们的提议改进。
</details></li>
</ul>
<hr>
<h2 id="Simulation-of-acquisition-shifts-in-T2-Flair-MR-images-to-stress-test-AI-segmentation-networks"><a href="#Simulation-of-acquisition-shifts-in-T2-Flair-MR-images-to-stress-test-AI-segmentation-networks" class="headerlink" title="Simulation of acquisition shifts in T2 Flair MR images to stress test AI segmentation networks"></a>Simulation of acquisition shifts in T2 Flair MR images to stress test AI segmentation networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01894">http://arxiv.org/abs/2311.01894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christiane Posselt, Mehmet Yigit Avci, Mehmet Yigitsoy, Patrick Schünke, Christoph Kolbitsch, Tobias Schäffter, Stefanie Remmele</li>
<li>for: 这项研究的目的是提供一个基于实验的MRI数据 simulation框架，用于对深度分割网络进行”压力测试”，以检验在临床实践中可能出现的数据采集偏移。</li>
<li>methods: 该方法使用MR信号方程来生成”采集偏移DERIVATIVES”的MR图像，以模拟在临床实践中可能出现的采集偏移。实验包括验证生成的图像与实际MR扫描的 validate，以及对当前state-of-the-art MS抑制网络进行示例压力测试，以探索一个通用的F1分数模型函数，以描述依赖于TE和TI参数的干扰效应。</li>
<li>results: 实验结果表明，在极端参数设置下，生成的图像与实际图像之间的差异可达19%，而且对于TE和TI参数，F1分数模型函数的干扰效应可以以 quadratic 模型函数（R^2 &gt; 0.9）来描述。模型函数的系数表明，TE参数的变化对模型性能的影响更大于TI参数。<details>
<summary>Abstract</summary>
Purpose: To provide a simulation framework for routine neuroimaging test data, which allows for "stress testing" of deep segmentation networks against acquisition shifts that commonly occur in clinical practice for T2 weighted (T2w) fluid attenuated inversion recovery (FLAIR) Magnetic Resonance Imaging (MRI) protocols.   Approach: The approach simulates "acquisition shift derivatives" of MR images based on MR signal equations. Experiments comprise the validation of the simulated images by real MR scans and example stress tests on state-of-the-art MS lesion segmentation networks to explore a generic model function to describe the F1 score in dependence of the contrast-affecting sequence parameters echo time (TE) and inversion time (TI).   Results: The differences between real and simulated images range up to 19 % in gray and white matter for extreme parameter settings. For the segmentation networks under test the F1 score dependency on TE and TI can be well described by quadratic model functions (R^2 > 0.9). The coefficients of the model functions indicate that changes of TE have more influence on the model performance than TI.   Conclusions: We show that these deviations are in the range of values as may be caused by erroneous or individual differences of relaxation times as described by literature. The coefficients of the F1 model function allow for quantitative comparison of the influences of TE and TI. Limitations arise mainly from tissues with the low baseline signal (like CSF) and when the protocol contains contrast-affecting measures that cannot be modelled due to missing information in the DICOM header.
</details>
<details>
<summary>摘要</summary>
目的：提供一个 Routine neuroimaging 测试数据的模拟框架，以便对深度分割网络进行“压力测试”，以适应在临床实践中经常出现的MR成像剖析（FLAIR）图像数据的获取变化。方法：通过MR信号方程来模拟MR图像的“获取变化 Derivatives”。实验包括验证模拟图像和真实MR扫描图像之间的差异，以及对现有MS损伤分割网络进行例子压力测试，以探索一个通用的F1分数函数，以描述TE和TI参数对F1分数的依赖关系。结果：模拟和真实图像之间的差异在灰白 matter 中可达19%，对于极端参数设置。对测试网络来说，TE和TI参数对F1分数的依赖关系可以用 quadratic model functions 描述（R^2 > 0.9）。模型函数的系数表明，TE参数对模型性能的影响大于TI参数。结论：我们表明这些差异与文献中描述的Relaxation Times的错误或个体差异相似。模型函数的系数允许对TE和TI参数的影响进行量化比较。限制主要来自于CSF和其他低基准信号的组织，以及在协调图像中包含不可模拟的对照措施。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-between-Multi-focus-and-Multi-modal-A-Focused-Integration-Framework-for-Multi-modal-Image-Fusion"><a href="#Bridging-the-Gap-between-Multi-focus-and-Multi-modal-A-Focused-Integration-Framework-for-Multi-modal-Image-Fusion" class="headerlink" title="Bridging the Gap between Multi-focus and Multi-modal: A Focused Integration Framework for Multi-modal Image Fusion"></a>Bridging the Gap between Multi-focus and Multi-modal: A Focused Integration Framework for Multi-modal Image Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01886">http://arxiv.org/abs/2311.01886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ixilai/MFIF-MMIF">https://github.com/ixilai/MFIF-MMIF</a></li>
<li>paper_authors: Xilai Li, Xiaosong Li, Tao Ye, Xiaoqi Cheng, Wuyang Liu, Haishu Tan</li>
<li>for: 本研究は多modal imaging fusion（MMIF）の Challenge に焦点を当てています。specifically, the paper focuses on the challenge of fusing multiple visible images with different focal regions and infrared images in real-world MMIF applications.</li>
<li>methods: この研究では、 semi-sparsity-based smoothing filterを导入して、imagesをstructure and texture componentsに decomposite。furthermore, a novel multi-scale operator is proposed to fuse the texture components, which can detect significant information by considering the pixel focus attributes and relevant data from various modal images.</li>
<li>results: 调查结果は、state-of-the-art methodsを超えるvisua perception と quantitative evaluationを示します。extensive experiments on existing MMIF datasets, as well as the object detection and depth estimation tasks, consistently demonstrate that the proposed algorithm can surpass the state-of-the-art methods in visual perception and quantitative evaluation.<details>
<summary>Abstract</summary>
Multi-modal image fusion (MMIF) integrates valuable information from different modality images into a fused one. However, the fusion of multiple visible images with different focal regions and infrared images is a unprecedented challenge in real MMIF applications. This is because of the limited depth of the focus of visible optical lenses, which impedes the simultaneous capture of the focal information within the same scene. To address this issue, in this paper, we propose a MMIF framework for joint focused integration and modalities information extraction. Specifically, a semi-sparsity-based smoothing filter is introduced to decompose the images into structure and texture components. Subsequently, a novel multi-scale operator is proposed to fuse the texture components, capable of detecting significant information by considering the pixel focus attributes and relevant data from various modal images. Additionally, to achieve an effective capture of scene luminance and reasonable contrast maintenance, we consider the distribution of energy information in the structural components in terms of multi-directional frequency variance and information entropy. Extensive experiments on existing MMIF datasets, as well as the object detection and depth estimation tasks, consistently demonstrate that the proposed algorithm can surpass the state-of-the-art methods in visual perception and quantitative evaluation. The code is available at https://github.com/ixilai/MFIF-MMIF.
</details>
<details>
<summary>摘要</summary>
多模态图像融合（MMIF）将多个不同模式图像融合到一起，以获取更加有价值的信息。然而，在实际应用中，融合不同视觉图像的焦点区域和抗雷达图像是一个前所未有的挑战。这是因为视觉光学镜的深度焦点有限，使得同一场景中同时捕捉焦点信息很困难。为解决这问题，本文提出了一个MMIF框架，包括对合并焦点信息和多种模式信息的抽象。具体来说，我们引入了一种半稀畴基于滤波器来分解图像为结构和文本组成部分。然后，我们提出了一种多尺度运算器，用于融合文本组成部分，可以检测到场景中重要信息，并考虑视觉图像中像素焦点特性和不同模式图像中相关数据。此外，为了实现场景的亮度和对比度的有效捕捉，我们考虑了多个方向的频谱异常和信息熵。广泛的实验表明，提出的算法可以在现有MMIF数据集上，以及对象检测和深度估计任务上，超越当前的方法在视觉识别和量化评价中。代码可以在https://github.com/ixilai/MFIF-MMIF中下载。
</details></li>
</ul>
<hr>
<h2 id="An-Ensemble-Machine-Learning-Approach-for-Screening-Covid-19-based-on-Urine-Parameters"><a href="#An-Ensemble-Machine-Learning-Approach-for-Screening-Covid-19-based-on-Urine-Parameters" class="headerlink" title="An Ensemble Machine Learning Approach for Screening Covid-19 based on Urine Parameters"></a>An Ensemble Machine Learning Approach for Screening Covid-19 based on Urine Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01854">http://arxiv.org/abs/2311.01854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behzad Moayedi, Abdalsamad Keramatfar, Mohammad Hadi Goldani, Mohammad Javad Fallahi, Alborz Jahangirisisakht, Mohammad Saboori, Leyla badiei</li>
<li>for: 这项研究旨在提出一种基于尿检测弓的COVID-19检测方法，以提高检测精度和效率。</li>
<li>methods: 研究人员使用RGB颜色空间参数来检测尿检测弓中的健康状况。然后，他们将RGB空间转换成10个额外的颜色空间，以提高模型的准确性。最后，他们提出了一种基于多层感知器神经网络的新ensemble模型。</li>
<li>results: 研究人员通过去除模型空间中的不确定区域来提高模型的检测性能。最终，他们的模型达到了80%的检测精度。这些结果表明，尿检测弓可能成为COVID-19检测中的一种有用工具，特别是在资源受限的设置中，PCR测试不可靠时。进一步的研究是需要验证这些发现，并探讨尿检测弓在COVID-19诊断和管理中的潜在作用。<details>
<summary>Abstract</summary>
The rapid spread of COVID-19 and the emergence of new variants underscore the importance of effective screening measures. Rapid diagnosis and subsequent quarantine of infected individuals can prevent further spread of the virus in society. While PCR tests are the gold standard for COVID-19 diagnosis, they are costly and time-consuming. In contrast, urine test strips are an inexpensive, non-invasive, and rapidly obtainable screening method that can provide important information about a patient's health status. In this study, we collected a new dataset and used the RGB (Red Green Blue) color space of urine test strips parameters to detect the health status of individuals. To improve the accuracy of our model, we converted the RGB space to 10 additional color spaces. After evaluating four different machine learning models, we proposed a new ensemble model based on a multi-layer perceptron neural network. Although the initial results were not strong, we were able to improve the model's screening performance for COVID-19 by removing uncertain regions of the model space. Ultimately, our model achieved a screening accuracy of 80% based on urine parameters. Our results suggest that urine test strips can be a useful tool for COVID-19 screening, particularly in resource-constrained settings where PCR testing may not be feasible. Further research is needed to validate our findings and explore the potential role of urine test strips in COVID-19 diagnosis and management.
</details>
<details>
<summary>摘要</summary>
COVID-19 的快速传播和新变种的出现强调了有效的检测措施的重要性。快速诊断和随后隔离感染者可以防止病毒在社会中进一步传播。虽然 PCR 测试是 COVID-19 诊断的标准方法，但它们是昂贵的和时间consuming。相比之下，尿检试剂是一种便宜、不侵入的检测方法，可以提供对患者健康状况的重要信息。在这项研究中，我们收集了一个新的数据集，并使用尿检试剂参数的 RGB（红绿蓝）颜色空间来检测个人健康状况。为了提高模型的准确性，我们将 RGB 空间转换为 10 个额外的颜色空间。经过评估四种不同的机器学习模型后，我们提出了一种基于多层感知神经网络的新ensemble模型。虽然初始结果不强，但我们通过移除模型空间的不确定区域来提高模型的检测性能。最终，我们的模型达到了基于尿检试剂的 COVID-19 检测精度80%。我们的结果表明，尿检试剂可以在资源受限的设置中作为 COVID-19 检测工具，特别是在 PCR 测试不可能的情况下。进一步的研究是需要验证我们的发现，并探讨尿检试剂在 COVID-19 诊断和管理中的潜在作用。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Representation-Learning-for-Multitask-Trajectory-Anomaly-Detection"><a href="#Holistic-Representation-Learning-for-Multitask-Trajectory-Anomaly-Detection" class="headerlink" title="Holistic Representation Learning for Multitask Trajectory Anomaly Detection"></a>Holistic Representation Learning for Multitask Trajectory Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01851">http://arxiv.org/abs/2311.01851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandros Stergiou, Brent De Weerdt, Nikos Deligiannis</li>
<li>for: 视频异常检测，Recognize abnormal events in videos</li>
<li>methods: 使用skeleton sequences和多任务学习，Learn expected motions across segments at different times</li>
<li>results: 与状态最佳结果对比，Show the advantages and effectiveness of our approach on anomaly detection in skeleton trajectories<details>
<summary>Abstract</summary>
Video anomaly detection deals with the recognition of abnormal events in videos. Apart from the visual signal, video anomaly detection has also been addressed with the use of skeleton sequences. We propose a holistic representation of skeleton trajectories to learn expected motions across segments at different times. Our approach uses multitask learning to reconstruct any continuous unobserved temporal segment of the trajectory allowing the extrapolation of past or future segments and the interpolation of in-between segments. We use an end-to-end attention-based encoder-decoder. We encode temporally occluded trajectories, jointly learn latent representations of the occluded segments, and reconstruct trajectories based on expected motions across different temporal segments. Extensive experiments on three trajectory-based video anomaly detection datasets show the advantages and effectiveness of our approach with state-of-the-art results on anomaly detection in skeleton trajectories.
</details>
<details>
<summary>摘要</summary>
视频异常检测是指视频中异常事件的识别。此外，视频异常检测还利用了骨架序列来进行Addressing。我们提议一种整体表示骨架轨迹的学习方法，以便在不同时间段中学习预期的运动。我们的方法使用多任务学习来重建任何不见的时间段，从而实现过去或未来时间段的拟合和中间时间段的插值。我们使用端到端的注意力基于Encoder-Decoder。我们编码时间受阻的骨架轨迹，同时学习隐藏的 segment 的射频表示，并根据不同时间段的预期运动来重建轨迹。我们在三个骨架轨迹基于视频异常检测数据集上进行了广泛的实验，并达到了当前最佳的异常检测效果。
</details></li>
</ul>
<hr>
<h2 id="Multi-LiDAR-Localization-and-Mapping-Pipeline-for-Urban-Autonomous-Driving"><a href="#Multi-LiDAR-Localization-and-Mapping-Pipeline-for-Urban-Autonomous-Driving" class="headerlink" title="Multi-LiDAR Localization and Mapping Pipeline for Urban Autonomous Driving"></a>Multi-LiDAR Localization and Mapping Pipeline for Urban Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01823">http://arxiv.org/abs/2311.01823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Sauerbeck, Dominik Kulmer, Markus Pielmeier, Maximilian Leitenstern, Christoph Weiß, Johannes Betz</li>
<li>for: 本研究旨在提供一个高精度、可靠的自动驾驶车辆地图和位置算法，以便在城市环境中安全、可靠地导航。</li>
<li>methods: 该研究使用了四个LiDAR感知器，并基于KISS-ICP算法进行地图生成和位置匹配。</li>
<li>results: 研究人员通过对一辆实验车进行测试，证明了该算法的高精度和实时性。<details>
<summary>Abstract</summary>
Autonomous vehicles require accurate and robust localization and mapping algorithms to navigate safely and reliably in urban environments. We present a novel sensor fusion-based pipeline for offline mapping and online localization based on LiDAR sensors. The proposed approach leverages four LiDAR sensors. Mapping and localization algorithms are based on the KISS-ICP, enabling real-time performance and high accuracy. We introduce an approach to generate semantic maps for driving tasks such as path planning. The presented pipeline is integrated into the ROS 2 based Autoware software stack, providing a robust and flexible environment for autonomous driving applications. We show that our pipeline outperforms state-of-the-art approaches for a given research vehicle and real-world autonomous driving application.
</details>
<details>
<summary>摘要</summary>
自主车辆需要准确和可靠的本地化和地图算法，以确保安全和可靠地在城市环境中 navigating。我们提出了一个基于LiDAR感知器的新的整合式地图和本地化管道，该管道使用四个LiDAR感知器。地图和本地化算法基于KISS-ICP，可以实现实时性和高精度。我们介绍了一种生成适用于驾驶任务的 semantics 地图的方法。该管道被 интеGRATED INTO ROS 2 基础设施 Autoware 软件栈，提供一个可靠和灵活的环境 для自主驾驶应用程序。我们表明我们的管道在给定的研究车辆和实际的自主驾驶应用程序中超越了现状的方法。
</details></li>
</ul>
<hr>
<h2 id="Estimating-3D-Uncertainty-Field-Quantifying-Uncertainty-for-Neural-Radiance-Fields"><a href="#Estimating-3D-Uncertainty-Field-Quantifying-Uncertainty-for-Neural-Radiance-Fields" class="headerlink" title="Estimating 3D Uncertainty Field: Quantifying Uncertainty for Neural Radiance Fields"></a>Estimating 3D Uncertainty Field: Quantifying Uncertainty for Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01815">http://arxiv.org/abs/2311.01815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianxiong Shen, Ruijie Ren, Adria Ruiz, Francesc Moreno-Noguer</li>
<li>for:  Addressing the limitation of Neural Radiance Fields (NeRF) in quantifying uncertainty, particularly in unseen space including occluded and outside scene content, for applications in robotics.</li>
<li>methods:  Propose a novel approach to estimate a 3D Uncertainty Field based on learned incomplete scene geometry, considering accumulated transmittance along each camera ray to infer 2D pixel-wise uncertainty.</li>
<li>results:  Our approach is the only one that can explicitly reason about high uncertainty both on 3D unseen regions and its involved 2D rendered pixels, compared with recent methods. Our designed uncertainty field is ideally suited for real-world robotics tasks, such as next-best-view selection.<details>
<summary>Abstract</summary>
Current methods based on Neural Radiance Fields (NeRF) significantly lack the capacity to quantify uncertainty in their predictions, particularly on the unseen space including the occluded and outside scene content. This limitation hinders their extensive applications in robotics, where the reliability of model predictions has to be considered for tasks such as robotic exploration and planning in unknown environments. To address this, we propose a novel approach to estimate a 3D Uncertainty Field based on the learned incomplete scene geometry, which explicitly identifies these unseen regions. By considering the accumulated transmittance along each camera ray, our Uncertainty Field infers 2D pixel-wise uncertainty, exhibiting high values for rays directly casting towards occluded or outside the scene content. To quantify the uncertainty on the learned surface, we model a stochastic radiance field. Our experiments demonstrate that our approach is the only one that can explicitly reason about high uncertainty both on 3D unseen regions and its involved 2D rendered pixels, compared with recent methods. Furthermore, we illustrate that our designed uncertainty field is ideally suited for real-world robotics tasks, such as next-best-view selection.
</details>
<details>
<summary>摘要</summary>
当前基于神经辐射场（NeRF）的方法缺乏量化未知预测的能力，特别是未经见到的空间，包括 occluded 和外场景内容。这种限制约束了它们在 робо扮仪中的广泛应用，因为需要考虑模型预测的可靠性，如 робо扮仪在未知环境中的探索和规划。为解决这个问题，我们提出了一种新的方法，以便估计3D 未知场（Uncertainty Field），基于学习的不完整场景几何学。我们考虑每束相机线的积累传输率，以便在每个像素位置上估计2D 像素尺度的uncertainty。为量化在学习表面上的不确定性，我们建模了随机辐射场。我们的实验表明，我们的方法是唯一一种能够直接考虑高度未知的3D 未见区域和相关的2D 渲染像素的方法，与当前的方法相比。此外，我们还证明了我们设计的uncertainty field适用于实际的世界 robotics 任务，如下一个最佳视角选择。
</details></li>
</ul>
<hr>
<h2 id="FETV-A-Benchmark-for-Fine-Grained-Evaluation-of-Open-Domain-Text-to-Video-Generation"><a href="#FETV-A-Benchmark-for-Fine-Grained-Evaluation-of-Open-Domain-Text-to-Video-Generation" class="headerlink" title="FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation"></a>FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01813">http://arxiv.org/abs/2311.01813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llyx97/fetv">https://github.com/llyx97/fetv</a></li>
<li>paper_authors: Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, Lu Hou</li>
<li>for: 本研究旨在提供一个细化的评估文本到视频（T2V）生成模型的Benchmark，以提高T2V模型的评估和研究。</li>
<li>methods: 本研究使用了多个方法，包括：（1）开发了一个多方面的Benchmark，名为FETV，用于细化评估T2V模型的表现；（2）对四种代表性的T2V模型进行了手动评估，以分析它们在不同类型的文本提示下的优缺点；（3）将FETV作为测试环境，评估了现有的自动T2V评估指标的可靠性。</li>
<li>results: 研究发现，现有的自动评估指标与人类评估指标存在较大的差异，并且不同的文本提示类型下的表现也存在差异。为了解决这个问题，本研究提出了一些解决方案，并开发了两种新的自动评估指标，它们与人类评估指标更加相似。<details>
<summary>Abstract</summary>
Recently, open-domain text-to-video (T2V) generation models have made remarkable progress. However, the promising results are mainly shown by the qualitative cases of generated videos, while the quantitative evaluation of T2V models still faces two critical problems. Firstly, existing studies lack fine-grained evaluation of T2V models on different categories of text prompts. Although some benchmarks have categorized the prompts, their categorization either only focuses on a single aspect or fails to consider the temporal information in video generation. Secondly, it is unclear whether the automatic evaluation metrics are consistent with human standards. To address these problems, we propose FETV, a benchmark for Fine-grained Evaluation of Text-to-Video generation. FETV is multi-aspect, categorizing the prompts based on three orthogonal aspects: the major content, the attributes to control and the prompt complexity. FETV is also temporal-aware, which introduces several temporal categories tailored for video generation. Based on FETV, we conduct comprehensive manual evaluations of four representative T2V models, revealing their pros and cons on different categories of prompts from different aspects. We also extend FETV as a testbed to evaluate the reliability of automatic T2V metrics. The multi-aspect categorization of FETV enables fine-grained analysis of the metrics' reliability in different scenarios. We find that existing automatic metrics (e.g., CLIPScore and FVD) correlate poorly with human evaluation. To address this problem, we explore several solutions to improve CLIPScore and FVD, and develop two automatic metrics that exhibit significant higher correlation with humans than existing metrics. Benchmark page: https://github.com/llyx97/FETV.
</details>
<details>
<summary>摘要</summary>
近些年来，开放领域文本到视频（T2V）生成模型已经取得了很大的进步。然而，这些成果主要表现在质量上，而量化评价T2V模型仍面临两个主要问题。首先，现有的研究缺乏细化的文本提示评价T2V模型。尽管一些标准化的benchmark已经将提示分类，但这些分类通常只关注单一方面，而且忽视视频生成中的时间信息。其次，不清楚 automatic评价指标与人类标准是否一致。为解决这些问题，我们提出了FETV，一个用于细化评价T2V模型的benchmark。FETV是多个方面的，根据三个各自独立的方面进行分类：主要内容、控制属性和提示复杂度。FETV还是时间感知的，对视频生成进行了多个时间分类。基于FETV，我们进行了全面的手动评价四种表现T2V模型的示例，揭示它们在不同类型的提示上的优缺点。我们还将FETV作为测试台来评估自动T2V指标的可靠性。FETV的多个方面分类使得自动指标的可靠性在不同的场景中进行细化分析。我们发现现有的自动指标（例如CLIPScore和FVD）与人类评价相关性很差。为解决这个问题，我们探索了多种解决方案，并开发了两种新的自动指标，它们与人类评价更高相关性。benchmark页面：https://github.com/llyx97/FETV。
</details></li>
</ul>
<hr>
<h2 id="inkn’hue-Enhancing-Manga-Colorization-from-Multiple-Priors-with-Alignment-Multi-Encoder-VAE"><a href="#inkn’hue-Enhancing-Manga-Colorization-from-Multiple-Priors-with-Alignment-Multi-Encoder-VAE" class="headerlink" title="inkn’hue: Enhancing Manga Colorization from Multiple Priors with Alignment Multi-Encoder VAE"></a>inkn’hue: Enhancing Manga Colorization from Multiple Priors with Alignment Multi-Encoder VAE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01804">http://arxiv.org/abs/2311.01804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rossiyareich/inknhue">https://github.com/rossiyareich/inknhue</a></li>
<li>paper_authors: Tawin Jiramahapokee<br>for:  Mangaka (manga artists) who want to colorize their black and white manga artwork.methods:  Our proposed specialized framework for manga colorization uses a multi-encoder VAE to align shading and vibrant coloring models, allowing for clear and colorful results with the option to incorporate reference images and manual hints.results:  Our approach achieves clear and colorful results for manga colorization, addressing the challenges of existing methods that often fall short in achieving desired results.<details>
<summary>Abstract</summary>
Manga, a form of Japanese comics and distinct visual storytelling, has captivated readers worldwide. Traditionally presented in black and white, manga's appeal lies in its ability to convey complex narratives and emotions through intricate line art and shading. Yet, the desire to experience manga in vibrant colors has sparked the pursuit of manga colorization, a task of paramount significance for artists. However, existing methods, originally designed for line art and sketches, face challenges when applied to manga. These methods often fall short in achieving the desired results, leading to the need for specialized manga-specific solutions. Existing approaches frequently rely on a single training step or extensive manual artist intervention, which can yield less satisfactory outcomes. To address these challenges, we propose a specialized framework for manga colorization. Leveraging established models for shading and vibrant coloring, our approach aligns both using a multi-encoder VAE. This structured workflow ensures clear and colorful results, with the option to incorporate reference images and manual hints.
</details>
<details>
<summary>摘要</summary>
漫画，一种日本漫画和独特的视觉故事，在全球读者中备受欢迎。传统上是黑白的，漫画的吸引力在于它可以通过复杂的线涂和阴影来传达复杂的故事和情感。然而，有人希望经过彩色的漫画，这引发了漫画颜色化的探索。现有的方法，原本设计用于线画和素描，在应用于漫画时遇到了挑战。这些方法经常不能达到所需的结果，导致了特有的漫画专用解决方案的需求。现有的方法frequently rely on single training step或者广泛的手动艺术家干预，这可能会导致less satisfactory的结果。为解决这些挑战，我们提出了一个特有的漫画颜色化框架。利用已经建立的模型 для阴影和鲜艳的彩色，我们的方法将阴影和彩色 align使用多encoder VAE。这种结构化的工作流程可以确保明亮鲜艳的结果，同时还可以包括参考图像和手动提示。
</details></li>
</ul>
<hr>
<h2 id="Generating-Unbiased-Pseudo-labels-via-a-Theoretically-Guaranteed-Chebyshev-Constraint-to-Unify-Semi-supervised-Classification-and-Regression"><a href="#Generating-Unbiased-Pseudo-labels-via-a-Theoretically-Guaranteed-Chebyshev-Constraint-to-Unify-Semi-supervised-Classification-and-Regression" class="headerlink" title="Generating Unbiased Pseudo-labels via a Theoretically Guaranteed Chebyshev Constraint to Unify Semi-supervised Classification and Regression"></a>Generating Unbiased Pseudo-labels via a Theoretically Guaranteed Chebyshev Constraint to Unify Semi-supervised Classification and Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01782">http://arxiv.org/abs/2311.01782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Wu, Junbiao Pang, Qingming Huang</li>
<li>for: 这个论文主要针对的是 semi-supervised classification 和 regression 问题，但是 semi-supervised classification 方法 rarely 应用于 regression 任务。</li>
<li>methods: 我们提出了一种基于 Chebyshev 不等式的 theoretically guaranteed 约束，将多个预测结果组合成高质量标签，并提出了一种 Unbiased Pseudo-labels network (UBPL network)  Architecture，用于生成不偏的标签。</li>
<li>results: 我们的方法可以在 pose estimation 数据集 Mouse、FLIC 和 LSP 上达到 SOTA 性能，以及在 classification 数据集 CIFAR10&#x2F;100 和 SVHN 上达到更好的性能。<details>
<summary>Abstract</summary>
Both semi-supervised classification and regression are practically challenging tasks for computer vision. However, semi-supervised classification methods are barely applied to regression tasks. Because the threshold-to-pseudo label process (T2L) in classification uses confidence to determine the quality of label. It is successful for classification tasks but inefficient for regression tasks. In nature, regression also requires unbiased methods to generate high-quality labels. On the other hand, T2L for classification often fails if the confidence is generated by a biased method. To address this issue, in this paper, we propose a theoretically guaranteed constraint for generating unbiased labels based on Chebyshev's inequality, combining multiple predictions to generate superior quality labels from several inferior ones. In terms of high-quality labels, the unbiased method naturally avoids the drawback of T2L. Specially, we propose an Unbiased Pseudo-labels network (UBPL network) with multiple branches to combine multiple predictions as pseudo-labels, where a Feature Decorrelation loss (FD loss) is proposed based on Chebyshev constraint. In principle, our method can be used for both classification and regression and can be easily extended to any semi-supervised framework, e.g. Mean Teacher, FixMatch, DualPose. Our approach achieves superior performance over SOTAs on the pose estimation datasets Mouse, FLIC and LSP, as well as the classification datasets CIFAR10/100 and SVHN.
</details>
<details>
<summary>摘要</summary>
Both semi-supervised classification and regression are practically challenging tasks for computer vision. However, semi-supervised classification methods are barely applied to regression tasks. Because the threshold-to-pseudo label process (T2L) in classification uses confidence to determine the quality of label. It is successful for classification tasks but inefficient for regression tasks. In nature, regression also requires unbiased methods to generate high-quality labels. On the other hand, T2L for classification often fails if the confidence is generated by a biased method. To address this issue, in this paper, we propose a theoretically guaranteed constraint for generating unbiased labels based on Chebyshev's inequality, combining multiple predictions to generate superior quality labels from several inferior ones. In terms of high-quality labels, the unbiased method naturally avoids the drawback of T2L. Specifically, we propose an Unbiased Pseudo-labels network (UBPL network) with multiple branches to combine multiple predictions as pseudo-labels, where a Feature Decorrelation loss (FD loss) is proposed based on Chebyshev constraint. In principle, our method can be used for both classification and regression and can be easily extended to any semi-supervised framework, e.g. Mean Teacher, FixMatch, DualPose. Our approach achieves superior performance over SOTAs on the pose estimation datasets Mouse, FLIC and LSP, as well as the classification datasets CIFAR10/100 and SVHN.Note: Simplified Chinese is a romanization of Chinese, and the translation may not be exact as the original text is in English.
</details></li>
</ul>
<hr>
<h2 id="CheX-Nomaly-Segmenting-Lung-Abnormalities-from-Chest-Radiographs-using-Machine-Learning"><a href="#CheX-Nomaly-Segmenting-Lung-Abnormalities-from-Chest-Radiographs-using-Machine-Learning" class="headerlink" title="CheX-Nomaly: Segmenting Lung Abnormalities from Chest Radiographs using Machine Learning"></a>CheX-Nomaly: Segmenting Lung Abnormalities from Chest Radiographs using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01777">http://arxiv.org/abs/2311.01777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanskriti Singh<br>for: 这个研究旨在提高胸部X射线成像（CXR）异常区域的精度诊断，特别是关于误判读异常区域的问题。methods: 这个研究使用了一个二进制本地化U-Net模型，利用了传播学习技术，并将异常区域分类为14种肺部疾病和“无症”情况。results: 研究发现，通过将异常区域分类为14种肺部疾病和“无症”情况，可以增强异常区域标识模型的一般化性。此外，这个模型还可以在与训练数据中没有看过的疾病之间进行协同分类。<details>
<summary>Abstract</summary>
The global challenge in chest radiograph X-ray (CXR) abnormalities often being misdiagnosed is primarily associated with perceptual errors, where healthcare providers struggle to accurately identify the location of abnormalities, rather than misclassification errors. We currently address this problem through disease-specific segmentation models. Unfortunately, these models cannot be released in the field due to their lack of generalizability across all thoracic diseases. A binary model tends to perform poorly when it encounters a disease that isn't represented in the dataset. We present CheX-nomaly: a binary localization U-net model that leverages transfer learning techniques with the incorporation of an innovative contrastive learning approach. Trained on the VinDr-CXR dataset, which encompasses 14 distinct diseases in addition to 'no finding' cases, my model achieves generalizability across these 14 diseases and others it has not seen before. We show that we can significantly improve the generalizability of an abnormality localization model by incorporating a contrastive learning method and dissociating the bounding boxes with its disease class. We also introduce a new loss technique to apply to enhance the U-nets performance on bounding box segmentation. By introducing CheX-nomaly, we offer a promising solution to enhance the precision of chest disease diagnosis, with a specific focus on reducing the significant number of perceptual errors in healthcare.
</details>
<details>
<summary>摘要</summary>
全球挑战在胸部X射线图像（CXR）异常现象 frequently 被误诊是由于诊断者对异常位置的识别出现问题，而不是分类错误。我们目前通过疾病特定的分割模型来解决这个问题。然而，这些模型无法在实际应用中发布，因为它们缺乏对所有胸部疾病的普适性。一个二进制模型在遇到未在数据集中 represent 的疾病时会表现很差。我们介绍了 CheX-nomaly：一种二进制本地化U-net模型，利用了传输学习技术和一种创新的对比学习方法。它在 VinDr-CXR 数据集上训练，该数据集包括 14 种疾病以及 "无发现"  casos，并能够在这些疾病中实现普适性。我们表明，通过将对比学习方法与 bounding box 分割相结合，可以显著提高异常位置模型的普适性。此外，我们还介绍了一种新的损失函数，用于提高 U-net 的 bounding box 分割性能。通过引入 CheX-nomaly，我们提供了一种有 Promise 的解决方案，以提高胸部疾病诊断的精度，特别是减少健康保健中的重要性误诊。
</details></li>
</ul>
<hr>
<h2 id="PDF-Point-Diffusion-Implicit-Function-for-Large-scale-Scene-Neural-Representation"><a href="#PDF-Point-Diffusion-Implicit-Function-for-Large-scale-Scene-Neural-Representation" class="headerlink" title="PDF: Point Diffusion Implicit Function for Large-scale Scene Neural Representation"></a>PDF: Point Diffusion Implicit Function for Large-scale Scene Neural Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01773">http://arxiv.org/abs/2311.01773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Ding, Fukun Yin, Jiayuan Fan, Hui Li, Xin Chen, Wen Liu, Chongshan Lu, Gang YU, Tao Chen</li>
<li>for: 大规模场景新视图合成</li>
<li>methods: 点扩散函数（PDF）和缩小抽象空间</li>
<li>results: 超越相关状态前方法的效果Here’s a more detailed explanation of each point:</li>
<li>for: The paper is focused on the task of novel view synthesis for large-scale outdoor scenes.</li>
<li>methods: The proposed method uses a Point Diffusion implicit Function (PDF) to learn the surface distribution of the scene and reduce the sampling space. The method also employs a large-scale point cloud super-resolution diffusion module to enhance the sparse point cloud reconstructed from training images. In addition, the method uses region sampling based on Mip-NeRF 360 to model the background representation.</li>
<li>results: The paper demonstrates the effectiveness of the proposed method for large-scale scene novel view synthesis, outperforming relevant state-of-the-art baselines.<details>
<summary>Abstract</summary>
Recent advances in implicit neural representations have achieved impressive results by sampling and fusing individual points along sampling rays in the sampling space. However, due to the explosively growing sampling space, finely representing and synthesizing detailed textures remains a challenge for unbounded large-scale outdoor scenes. To alleviate the dilemma of using individual points to perceive the entire colossal space, we explore learning the surface distribution of the scene to provide structural priors and reduce the samplable space and propose a Point Diffusion implicit Function, PDF, for large-scale scene neural representation. The core of our method is a large-scale point cloud super-resolution diffusion module that enhances the sparse point cloud reconstructed from several training images into a dense point cloud as an explicit prior. Then in the rendering stage, only sampling points with prior points within the sampling radius are retained. That is, the sampling space is reduced from the unbounded space to the scene surface. Meanwhile, to fill in the background of the scene that cannot be provided by point clouds, the region sampling based on Mip-NeRF 360 is employed to model the background representation. Expensive experiments have demonstrated the effectiveness of our method for large-scale scene novel view synthesis, which outperforms relevant state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
The core of our method is a large-scale point cloud super-resolution diffusion module that enhances the sparse point cloud reconstructed from several training images into a dense point cloud as an explicit prior. Then in the rendering stage, only sampling points with prior points within the sampling radius are retained. That is, the sampling space is reduced from the unbounded space to the scene surface.Meanwhile, to fill in the background of the scene that cannot be provided by point clouds, the region sampling based on Mip-NeRF 360 is employed to model the background representation. Expensive experiments have demonstrated the effectiveness of our method for large-scale scene novel view synthesis, which outperforms relevant state-of-the-art baselines.(Note: The text has been translated into Simplified Chinese, but some grammar and wording may be adjusted to better fit the language and conventions of the target audience.)
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Unified-Transformer-based-Framework-for-Scene-Graph-Generation-and-Human-object-Interaction-Detection"><a href="#Towards-a-Unified-Transformer-based-Framework-for-Scene-Graph-Generation-and-Human-object-Interaction-Detection" class="headerlink" title="Towards a Unified Transformer-based Framework for Scene Graph Generation and Human-object Interaction Detection"></a>Towards a Unified Transformer-based Framework for Scene Graph Generation and Human-object Interaction Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01755">http://arxiv.org/abs/2311.01755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao He, Lianli Gao, Jingkuan Song, Yuan-Fang Li</li>
<li>for: 本文旨在探讨Scene Graph Generation (SGG)和Human-Object Interaction (HOI)检测任务之间的自然关系，并提出一种基于Transformer架构的一步模型SG2HOI+。</li>
<li>methods: 本文使用两个相互嵌入的Transformer来融合SGG和HOI检测任务，首先生成视觉特征集合中的关系 triple，然后使用另一个Transformer-based decoder预测人物-物体交互。</li>
<li>results: 实验结果表明，SG2HOI+模型在Visual Genome、V-COCO和HICO-DET等标准 benchmark dataset上表现出色，与前一代一步SGG模型相比，SG2HOI+模型在HOI任务上表现竞争力强，同时对SGG任务也带来了显著改进。<details>
<summary>Abstract</summary>
Scene graph generation (SGG) and human-object interaction (HOI) detection are two important visual tasks aiming at localising and recognising relationships between objects, and interactions between humans and objects, respectively.   Prevailing works treat these tasks as distinct tasks, leading to the development of task-specific models tailored to individual datasets. However, we posit that the presence of visual relationships can furnish crucial contextual and intricate relational cues that significantly augment the inference of human-object interactions. This motivates us to think if there is a natural intrinsic relationship between the two tasks, where scene graphs can serve as a source for inferring human-object interactions. In light of this, we introduce SG2HOI+, a unified one-step model based on the Transformer architecture. Our approach employs two interactive hierarchical Transformers to seamlessly unify the tasks of SGG and HOI detection. Concretely, we initiate a relation Transformer tasked with generating relation triples from a suite of visual features. Subsequently, we employ another transformer-based decoder to predict human-object interactions based on the generated relation triples. A comprehensive series of experiments conducted across established benchmark datasets including Visual Genome, V-COCO, and HICO-DET demonstrates the compelling performance of our SG2HOI+ model in comparison to prevalent one-stage SGG models. Remarkably, our approach achieves competitive performance when compared to state-of-the-art HOI methods. Additionally, we observe that our SG2HOI+ jointly trained on both SGG and HOI tasks in an end-to-end manner yields substantial improvements for both tasks compared to individualized training paradigms.
</details>
<details>
<summary>摘要</summary>
Scene graph生成（SGG）和人物对象交互（HOI）检测是两个重要的视觉任务，旨在本地化和识别对象之间的关系，以及人类和对象之间的交互。  existing works treat these tasks as distinct tasks, leading to the development of task-specific models tailored to individual datasets. However, we argue that the presence of visual relationships can provide crucial contextual and intricate relational cues that significantly enhance the inference of human-object interactions. This motivates us to explore whether there is an inherent relationship between the two tasks, where scene graphs can serve as a source for inferring human-object interactions.In light of this, we introduce SG2HOI+, a unified one-step model based on the Transformer architecture. Our approach employs two interactive hierarchical Transformers to seamlessly unify the tasks of SGG and HOI detection. Specifically, we initiate a relation Transformer tasked with generating relation triples from a suite of visual features. Subsequently, we use another transformer-based decoder to predict human-object interactions based on the generated relation triples.A comprehensive series of experiments conducted across established benchmark datasets including Visual Genome, V-COCO, and HICO-DET demonstrates the compelling performance of our SG2HOI+ model compared to prevalent one-stage SGG models. Remarkably, our approach achieves competitive performance when compared to state-of-the-art HOI methods. Furthermore, we observe that our SG2HOI+ jointly trained on both SGG and HOI tasks in an end-to-end manner yields substantial improvements for both tasks compared to individualized training paradigms.
</details></li>
</ul>
<hr>
<h2 id="Data-Centric-Long-Tailed-Image-Recognition"><a href="#Data-Centric-Long-Tailed-Image-Recognition" class="headerlink" title="Data-Centric Long-Tailed Image Recognition"></a>Data-Centric Long-Tailed Image Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01744">http://arxiv.org/abs/2311.01744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanbiao Ma, Licheng Jiao, Fang Liu, Shuyuan Yang, Xu Liu, Puhua Chen</li>
<li>for: 提高模型性能，增强数据质量和量</li>
<li>methods: 信息扩展，帮助平衡样本的质量和量</li>
<li>results: 通过Feature Diversity Gain（FDG）来解释信息扩展的效果，可以提高模型性能而无需修改模型结构。<details>
<summary>Abstract</summary>
In the context of the long-tail scenario, models exhibit a strong demand for high-quality data. Data-centric approaches aim to enhance both the quantity and quality of data to improve model performance. Among these approaches, information augmentation has been progressively introduced as a crucial category. It achieves a balance in model performance by augmenting the richness and quantity of samples in the tail classes. However, there is currently a lack of research into the underlying mechanisms explaining the effectiveness of information augmentation methods. Consequently, the utilization of information augmentation in long-tail recognition tasks relies heavily on empirical and intricate fine-tuning. This work makes two primary contributions. Firstly, we approach the problem from the perspectives of feature diversity and distribution shift, introducing the concept of Feature Diversity Gain (FDG) to elucidate why information augmentation is effective. We find that the performance of information augmentation can be explained by FDG, and its performance peaks when FDG achieves an appropriate balance. Experimental results demonstrate that by using FDG to select augmented data, we can further enhance model performance without the need for any modifications to the model's architecture. Thus, data-centric approaches hold significant potential in the field of long-tail recognition, beyond the development of new model structures. Furthermore, we systematically introduce the core components and fundamental tasks of a data-centric long-tail learning framework for the first time. These core components guide the implementation and deployment of the system, while the corresponding fundamental tasks refine and expand the research area.
</details>
<details>
<summary>摘要</summary>
在长尾场景下，模型强需高质量数据。数据中心化方法旨在提高模型性能的数据量和质量。其中，信息扩展被逐渐引入为关键类别。它在尾类中增加样本的质量和量，以提高模型性能。然而，关于信息扩展效果的内在机制还没有充分研究。因此，长尾识别任务中使用信息扩展的实践仍然受到较重的经验和细致的微调的限制。本工作做出了两项主要贡献。首先，我们从特征多样性和分布shift的角度出发，引入特征多样性增强度（FDG）来解释信息扩展的效果。我们发现，信息扩展的性能与FDG之间存在直接关系，并且FDG在适当的平衡点上达到最高性能。实验结果表明，通过使用FDG选择扩展数据，我们可以不需要修改模型结构，进一步提高模型性能。因此，数据中心化方法在长尾识别领域拥有广泛的潜力，超出了新模型结构的开发。此外，我们系统地介绍了长尾识别数据中心化框架的核心组件和基本任务。这些核心组件导向了实施和部署系统，而基本任务则是修复和扩展研究领域。
</details></li>
</ul>
<hr>
<h2 id="MixCon3D-Synergizing-Multi-View-and-Cross-Modal-Contrastive-Learning-for-Enhancing-3D-Representation"><a href="#MixCon3D-Synergizing-Multi-View-and-Cross-Modal-Contrastive-Learning-for-Enhancing-3D-Representation" class="headerlink" title="MixCon3D: Synergizing Multi-View and Cross-Modal Contrastive Learning for Enhancing 3D Representation"></a>MixCon3D: Synergizing Multi-View and Cross-Modal Contrastive Learning for Enhancing 3D Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01734">http://arxiv.org/abs/2311.01734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucsc-vlaa/mixcon3d">https://github.com/ucsc-vlaa/mixcon3d</a></li>
<li>paper_authors: Yipeng Gao, Zeyu Wang, Wei-Shi Zheng, Cihang Xie, Yuyin Zhou</li>
<li>for: 增强3D开放世界理解，包括文本、图像和点云信息。</li>
<li>methods: 结合2D图像和3D点云信息进行对比学习，并通过多视图2D图像的整合，提高传统三Modal表示的准确性和全面性。</li>
<li>results: 在三个代表性的benchmark上实现了显著提高，比基eline表现提高5.7%，并在文本对应3D检索和点云描述等更多应用中表现出色。<details>
<summary>Abstract</summary>
Contrastive learning has emerged as a promising paradigm for 3D open-world understanding, jointly with text, image, and point cloud. In this paper, we introduce MixCon3D, which combines the complementary information between 2D images and 3D point clouds to enhance contrastive learning. With the further integration of multi-view 2D images, MixCon3D enhances the traditional tri-modal representation by offering a more accurate and comprehensive depiction of real-world 3D objects and bolstering text alignment. Additionally, we pioneer the first thorough investigation of various training recipes for the 3D contrastive learning paradigm, building a solid baseline with improved performance. Extensive experiments conducted on three representative benchmarks reveal that our method renders significant improvement over the baseline, surpassing the previous state-of-the-art performance on the challenging 1,156-category Objaverse-LVIS dataset by 5.7%. We further showcase the effectiveness of our approach in more applications, including text-to-3D retrieval and point cloud captioning. The code is available at https://github.com/UCSC-VLAA/MixCon3D.
</details>
<details>
<summary>摘要</summary>
对开放世界3D理解而言，对比学习已经出现为一种有前途的方法，与文本、图像和点云集成一起使用。在这篇论文中，我们介绍了 MixCon3D，它结合了2D图像和3D点云的补充信息，以增强对比学习。另外，我们还进一步统合了多视角2D图像，从而提高了传统的三 modal 表现，提供更精确和全面的实际世界3D物体的描述，并且增强文本对齐。此外，我们还进行了第一次的对3D对比学习模型的训练组合方法的全面探讨，建立了优秀的基础点。实验结果显示，我们的方法在三个代表性的评分标准上得到了显著的改善，比前一个State-of-the-art的Objaverse-LVIS资料集的表现提高5.7%。此外，我们还证明了我们的方法在更多的应用中的效果，包括文本至3D搜寻和点云描述。代码可以在https://github.com/UCSC-VLAA/MixCon3D 上获取。
</details></li>
</ul>
<hr>
<h2 id="Capturing-Local-and-Global-Features-in-Medical-Images-by-Using-Ensemble-CNN-Transformer"><a href="#Capturing-Local-and-Global-Features-in-Medical-Images-by-Using-Ensemble-CNN-Transformer" class="headerlink" title="Capturing Local and Global Features in Medical Images by Using Ensemble CNN-Transformer"></a>Capturing Local and Global Features in Medical Images by Using Ensemble CNN-Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01731">http://arxiv.org/abs/2311.01731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javad Mirzapour Kaleybar, Hooman Saadat, Hooman Khaloo</li>
<li>for: The paper is written for the analysis of medical images, specifically for the diagnosis of COVID-19.</li>
<li>methods: The paper proposes a new classification model called the Controllable Ensemble Transformer and CNN (CETC), which combines the strengths of CNNs and transformers to capture both local and global features in medical images.</li>
<li>results: The CETC model outperforms existing state-of-the-art models across various evaluation metrics, demonstrating its superiority in accurately and efficiently analyzing medical images for the diagnosis of COVID-19.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为医疗图像分类而写的，特别是用于COVID-19诊断。</li>
<li>methods: 这篇论文提出了一种新的分类模型，即可控分类转换器和 convolutional neural networks (CETC)，它将 convolutional neural networks (CNNs) 和转换器相结合，以便在医疗图像中 Capture both local and global features。</li>
<li>results: CETC模型在不同评价指标中表现出色，超越了现有的状态码模型，证明了它在医疗图像分类中的优异性和高效性。<details>
<summary>Abstract</summary>
This paper introduces a groundbreaking classification model called the Controllable Ensemble Transformer and CNN (CETC) for the analysis of medical images. The CETC model combines the powerful capabilities of convolutional neural networks (CNNs) and transformers to effectively capture both local and global features present in medical images. The model architecture comprises three main components: a convolutional encoder block (CEB), a transposed-convolutional decoder block (TDB), and a transformer classification block (TCB). The CEB is responsible for capturing multi-local features at different scales and draws upon components from VGGNet, ResNet, and MobileNet as backbones. By leveraging this combination, the CEB is able to effectively detect and encode local features. The TDB, on the other hand, consists of sub-decoders that decode and sum the captured features using ensemble coefficients. This enables the model to efficiently integrate the information from multiple scales. Finally, the TCB utilizes the SwT backbone and a specially designed prediction head to capture global features, ensuring a comprehensive understanding of the entire image. The paper provides detailed information on the experimental setup and implementation, including the use of transfer learning, data preprocessing techniques, and training settings. The CETC model is trained and evaluated using two publicly available COVID-19 datasets. Remarkably, the model outperforms existing state-of-the-art models across various evaluation metrics. The experimental results clearly demonstrate the superiority of the CETC model, emphasizing its potential for accurately and efficiently analyzing medical images.
</details>
<details>
<summary>摘要</summary>
The model architecture consists of three main components: a Convolutional Encoder Block (CEB), a Transposed-Convolutional Decoder Block (TDB), and a Transformer Classification Block (TCB). The CEB uses components from VGGNet, ResNet, and MobileNet as backbones to capture multi-local features at different scales. The TDB consists of sub-decoders that decode and sum the captured features using ensemble coefficients, allowing the model to efficiently integrate information from multiple scales. Finally, the TCB uses the SwT backbone and a specially designed prediction head to capture global features, ensuring a comprehensive understanding of the entire image.The paper provides detailed information on the experimental setup and implementation, including transfer learning, data preprocessing techniques, and training settings. The CETC model is trained and evaluated using two publicly available COVID-19 datasets, and the results show that it outperforms existing state-of-the-art models across various evaluation metrics. The experimental results demonstrate the superiority of the CETC model in accurately and efficiently analyzing medical images, highlighting its potential for future applications.
</details></li>
</ul>
<hr>
<h2 id="EXIM-A-Hybrid-Explicit-Implicit-Representation-for-Text-Guided-3D-Shape-Generation"><a href="#EXIM-A-Hybrid-Explicit-Implicit-Representation-for-Text-Guided-3D-Shape-Generation" class="headerlink" title="EXIM: A Hybrid Explicit-Implicit Representation for Text-Guided 3D Shape Generation"></a>EXIM: A Hybrid Explicit-Implicit Representation for Text-Guided 3D Shape Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01714">http://arxiv.org/abs/2311.01714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuzhengzhe/exim">https://github.com/liuzhengzhe/exim</a></li>
<li>paper_authors: Zhengzhe Liu, Jingyu Hu, Ka-Hei Hui, Xiaojuan Qi, Daniel Cohen-Or, Chi-Wing Fu</li>
<li>for: 本 paper 描述了一种基于文本指导的三维形状生成技术，用于生成高质量的三维形状。</li>
<li>methods: 该技术利用了一种混合的三维形状表示方式，称为 EXIM，这种方式结合了显式和隐式表示的优点，从而实现高效的形状生成。</li>
<li>results: 经过广泛的实验，该技术能够基于自然语言描述生成高质量的三维形状，并且能够保证形状与文本的匹配性。Code和模型在 GitHub 上发布。<details>
<summary>Abstract</summary>
This paper presents a new text-guided technique for generating 3D shapes. The technique leverages a hybrid 3D shape representation, namely EXIM, combining the strengths of explicit and implicit representations. Specifically, the explicit stage controls the topology of the generated 3D shapes and enables local modifications, whereas the implicit stage refines the shape and paints it with plausible colors. Also, the hybrid approach separates the shape and color and generates color conditioned on shape to ensure shape-color consistency. Unlike the existing state-of-the-art methods, we achieve high-fidelity shape generation from natural-language descriptions without the need for time-consuming per-shape optimization or reliance on human-annotated texts during training or test-time optimization. Further, we demonstrate the applicability of our approach to generate indoor scenes with consistent styles using text-induced 3D shapes. Through extensive experiments, we demonstrate the compelling quality of our results and the high coherency of our generated shapes with the input texts, surpassing the performance of existing methods by a significant margin. Codes and models are released at https://github.com/liuzhengzhe/EXIM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Taking-a-PEEK-into-YOLOv5-for-Satellite-Component-Recognition-via-Entropy-based-Visual-Explanations"><a href="#Taking-a-PEEK-into-YOLOv5-for-Satellite-Component-Recognition-via-Entropy-based-Visual-Explanations" class="headerlink" title="Taking a PEEK into YOLOv5 for Satellite Component Recognition via Entropy-based Visual Explanations"></a>Taking a PEEK into YOLOv5 for Satellite Component Recognition via Entropy-based Visual Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01703">http://arxiv.org/abs/2311.01703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mackenzie J. Meni, Trupti Mahendrakar, Olivia D. M. Raney, Ryan T. White, Michael L. Mayo, Kevin Pilkiewicz</li>
<li>for: 这篇论文旨在解决低地球轨道（LEO）中的撞击风险和空间垃圾问题，特别是处理不合作和未知的空间垃圾。</li>
<li>methods: 该论文使用自主小追踪卫星群，通过You Only Look Once v5（YOLOv5）对象检测模型进行目标几何确定和安全飞行轨迹规划。该模型具有批处理能力和快速检测能力，但缺乏可解释性，使得人类无法理解模型的决策过程。</li>
<li>results: 通过引入信息论统计分析方法，该论文分析了模型决策过程中的信息 entropy 和 latent representation，从而提供了可解释的决策过程。通过硬件在 loop 实验，PEEK 方法可以帮助分析模型决策过程，从而提高模型的可靠性和安全性。<details>
<summary>Abstract</summary>
The escalating risk of collisions and the accumulation of space debris in Low Earth Orbit (LEO) has reached critical concern due to the ever increasing number of spacecraft. Addressing this crisis, especially in dealing with non-cooperative and unidentified space debris, is of paramount importance. This paper contributes to efforts in enabling autonomous swarms of small chaser satellites for target geometry determination and safe flight trajectory planning for proximity operations in LEO. Our research explores on-orbit use of the You Only Look Once v5 (YOLOv5) object detection model trained to detect satellite components. While this model has shown promise, its inherent lack of interpretability hinders human understanding, a critical aspect of validating algorithms for use in safety-critical missions. To analyze the decision processes, we introduce Probabilistic Explanations for Entropic Knowledge extraction (PEEK), a method that utilizes information theoretic analysis of the latent representations within the hidden layers of the model. Through both synthetic in hardware-in-the-loop experiments, PEEK illuminates the decision-making processes of the model, helping identify its strengths, limitations and biases.
</details>
<details>
<summary>摘要</summary>
“随着低地球轨道（LEO）中的撞击风险和空间垃圾的数量不断增加，已经达到了摄理关注的水准。尤其是在处理不合作和未知的空间垃圾方面，这是一个非常重要的课题。本文对于实现自动化小搜索卫星群的目标几何决定和安全飞行轨道规划进行了贡献。我们的研究探讨了在轨道上使用You Only Look Once v5（YOLOv5）物件检测模型来检测卫星 комponents。这个模型已经表现出了应用潜力，但是它的自然无法解释限制了人类的理解，这是一个 Critical aspect of validating algorithms for use in safety-critical missions。为了分析决策过程，我们提出了可能性关注的方法，它利用了资讯论分析隐藏层的内部代表。通过硬件在Loop实验，PEEK可以照明模型的决策过程，帮助识别它的优点、局限和偏见。”
</details></li>
</ul>
<hr>
<h2 id="Medical-Image-Segmentation-with-Domain-Adaptation-A-Survey"><a href="#Medical-Image-Segmentation-with-Domain-Adaptation-A-Survey" class="headerlink" title="Medical Image Segmentation with Domain Adaptation: A Survey"></a>Medical Image Segmentation with Domain Adaptation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01702">http://arxiv.org/abs/2311.01702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cchen-cc/SIFA">https://github.com/cchen-cc/SIFA</a></li>
<li>paper_authors: Yuemeng Li, Yong Fan</li>
<li>for: 本文主要旨在提供医学成像数据分析领域中深度学习（DL）模型的适用场景，以及DL模型在不同数据集中的泛化问题的解决方案。</li>
<li>methods: 本文综述了域适应方法的应用在医学成像数据分析领域中的DL模型 segmentation，包括域适应技术的基本原理、域适应方法的类型和应用场景等。</li>
<li>results: 本文对域适应方法在医学成像数据分析领域中DL模型 segmentation的应用进行了综述，并评估了域适应方法的效果和可行性。<details>
<summary>Abstract</summary>
Deep learning (DL) has shown remarkable success in various medical imaging data analysis applications. However, it remains challenging for DL models to achieve good generalization, especially when the training and testing datasets are collected at sites with different scanners, due to domain shift caused by differences in data distributions. Domain adaptation has emerged as an effective means to address this challenge by mitigating domain gaps in medical imaging applications. In this review, we specifically focus on domain adaptation approaches for DL-based medical image segmentation. We first present the motivation and background knowledge underlying domain adaptations, then provide a comprehensive review of domain adaptation applications in medical image segmentations, and finally discuss the challenges, limitations, and future research trends in the field to promote the methodology development of domain adaptation in the context of medical image segmentation. Our goal was to provide researchers with up-to-date references on the applications of domain adaptation in medical image segmentation studies.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）在各种医疗影像数据分析应用中表现出了惊人的成功。然而，DL模型在不同扫描器上收集的训练和测试数据集上的总体化仍然是一大挑战，尤其是由于数据分布的差异引起的领域偏移。领域适应技术在医疗影像应用中 emerged as an effective means to address this challenge by mitigating domain gaps.在这篇文章中，我们专门关注DL模型在医疗影像分割任务中的领域适应方法。我们首先介绍了领域适应的动机和背景知识，然后提供了医疗影像分割领域中领域适应的完整回顾，最后讨论了领域适应在医疗影像分割中的挑战、局限性和未来研究趋势，以便为研究人员提供最新的参考资料。我们的目标是为研究人员提供有关领域适应在医疗影像分割研究中的应用。
</details></li>
</ul>
<hr>
<h2 id="Universal-Perturbation-based-Secret-Key-Controlled-Data-Hiding"><a href="#Universal-Perturbation-based-Secret-Key-Controlled-Data-Hiding" class="headerlink" title="Universal Perturbation-based Secret Key-Controlled Data Hiding"></a>Universal Perturbation-based Secret Key-Controlled Data Hiding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01696">http://arxiv.org/abs/2311.01696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghua Wang, Wen Yao, Tingsong Jiang, Xiaoqian Chen</li>
<li>for: 这个研究旨在提出一种基于通用随机干扰的隐藏数据方法，实现单一随机干扰中隐藏多个秘密图像，并使用秘密键控制的解oder提取不同秘密图像。</li>
<li>methods: 本研究使用了通用随机干扰来作为数据传输的传输干扰，并提出了一个秘密键控制的解oder来提取不同秘密图像。此外，本研究还提出了一个抑制损失函数以防止秘密图像泄露，并使用了一个强健模组件来增强解oder对于损坏的抗性。</li>
<li>results: 实验结果显示，本研究的方法可以实现高效的隐藏数据，并且在不同的数据集上具有良好的可靠性和安全性。此外，实验还显示了本研究的方法在实际应用中的可行性，例如在WeChat和Twitter等平台上进行了 físico 测试。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are demonstrated to be vulnerable to universal perturbation, a single quasi-perceptible perturbation that can deceive the DNN on most images. However, the previous works are focused on using universal perturbation to perform adversarial attacks, while the potential usability of universal perturbation as data carriers in data hiding is less explored, especially for the key-controlled data hiding method. In this paper, we propose a novel universal perturbation-based secret key-controlled data-hiding method, realizing data hiding with a single universal perturbation and data decoding with the secret key-controlled decoder. Specifically, we optimize a single universal perturbation, which serves as a data carrier that can hide multiple secret images and be added to most cover images. Then, we devise a secret key-controlled decoder to extract different secret images from the single container image constructed by the universal perturbation by using different secret keys. Moreover, a suppress loss function is proposed to prevent the secret image from leakage. Furthermore, we adopt a robust module to boost the decoder's capability against corruption. Finally, A co-joint optimization strategy is proposed to find the optimal universal perturbation and decoder. Extensive experiments are conducted on different datasets to demonstrate the effectiveness of the proposed method. Additionally, the physical test performed on platforms (e.g., WeChat and Twitter) verifies the usability of the proposed method in practice.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）被证明为易受到通用扰动的影响，一种可见的扰动可以误导DNN大多数图像。然而，之前的工作主要关注于使用通用扰动进行敌意攻击，而忽略了通用扰动作为数据隐藏的可能性，特别是针对针对钥匙控制的数据隐藏方法。在这篇论文中，我们提出了一种基于通用扰动的钥匙控制数据隐藏方法，实现了使用单个通用扰动隐藏多个秘密图像，并使用秘密钥来控制数据解码。具体来说，我们优化了单个通用扰动，该扰动serve as a data carrier可以隐藏多个秘密图像并可以添加到大多数覆盖图像中。然后，我们设计了一个钥匙控制的解码器，通过使用不同的秘密钥来从单个容器图像中提取不同的秘密图像。此外，我们还提出了一个防止秘密图像泄露的抑制损失函数，并采用一个强化模块来提高解码器对损害的抗性。最后，我们提出了一种共同优化策略，以找到最佳的通用扰动和解码器。我们在不同的数据集上进行了广泛的实验，以证明我们的方法的有效性。此外，我们在平台（如WeChat和Twitter）上进行了实际测试，以证明我们的方法在实践中的可用性。
</details></li>
</ul>
<hr>
<h2 id="Disentangled-Representation-Learning-with-Transmitted-Information-Bottleneck"><a href="#Disentangled-Representation-Learning-with-Transmitted-Information-Bottleneck" class="headerlink" title="Disentangled Representation Learning with Transmitted Information Bottleneck"></a>Disentangled Representation Learning with Transmitted Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01686">http://arxiv.org/abs/2311.01686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuohang Dang, Minnan Luo, Chengyou Jia, Guang Dai, Jihong Wang, Xiaojun Chang, Jingdong Wang, Qinghua Zheng</li>
<li>for: 本文主要针对于提高模型的 robustness和普适性，通过对表征信息进行抽象和分离来实现。</li>
<li>methods: 本文提出了一种新的目标函数DisTIB，该函数通过传递信息网络来实现表征信息的交互和分离。同时，本文使用了变分推断来 derive tractable estimation，并使用标准梯度下降进行优化。</li>
<li>results: 本文通过广泛的实验 validate了DisTIB的吸引力和可行性，并证明了其在不同下游任务中的超过对手性。<details>
<summary>Abstract</summary>
Encoding only the task-related information from the raw data, \ie, disentangled representation learning, can greatly contribute to the robustness and generalizability of models. Although significant advances have been made by regularizing the information in representations with information theory, two major challenges remain: 1) the representation compression inevitably leads to performance drop; 2) the disentanglement constraints on representations are in complicated optimization. To these issues, we introduce Bayesian networks with transmitted information to formulate the interaction among input and representations during disentanglement. Building upon this framework, we propose \textbf{DisTIB} (\textbf{T}ransmitted \textbf{I}nformation \textbf{B}ottleneck for \textbf{Dis}entangled representation learning), a novel objective that navigates the balance between information compression and preservation. We employ variational inference to derive a tractable estimation for DisTIB. This estimation can be simply optimized via standard gradient descent with a reparameterization trick. Moreover, we theoretically prove that DisTIB can achieve optimal disentanglement, underscoring its superior efficacy. To solidify our claims, we conduct extensive experiments on various downstream tasks to demonstrate the appealing efficacy of DisTIB and validate our theoretical analyses.
</details>
<details>
<summary>摘要</summary>
<font face="Times New Roman" style="font-size: 18px; font-family: 'Times New Roman', Times, serif;">    <font color="#00698f">Encoding</font> <font color="#ffcc00">only</font> <font color="#00698f">the</font> <font color="#ffcc00">task-related</font> <font color="#00698f">information</font> <font color="#ffcc00>from</font> <font color="#00698f">the</font> <font color="#ffcc00">raw</font> <font color="#00698f">data</font>, <font color="#ffcc00>ie</font>, <font color="#00698f">disentangled</font> <font color="#ffcc00>representation</font> <font color="#00698f>learning</font>, <font color="#ffcc00>can</font> <font color="#00698f>greatly</font> <font color="#ffcc00>contribute</font> <font color="#00698f>to</font> <font color="#ffcc00>the</font> <font color="#00698f>robustness</font> <font color="#ffcc00>and</font> <font color="#00698f>generalizability</font> <font color="#ffcc00>of</font> <font color="#00698f>models</font>.</font><font color="#00698f"> Although</font> <font color="#ffcc00>significant</font> <font color="#00698f>advances</font> <font color="#ffcc00>have</font> <font color="#00698f>been</font> <font color="#ffcc00>made</font> <font color="#00698f>by</font> <font color="#ffcc00>regularizing</font> <font color="#00698f>the</font> <font color="#ffcc00>information</font> <font color="#00698f>in</font> <font color="#00698f>representations</font> <font color="#ffcc00>with</font> <font color="#00698f>information</font> <font color="#ffcc00>theory</font>, <font color="#00698f>two</font> <font color="#ffcc00>major</font> <font color="#00698f>challenges</font> <font color="#ffcc00>remain</font>:</font><font color="#00698f">1</font> <font color="#ffcc00>the</font> <font color="#00698f>representation</font> <font color="#ffcc00>compression</font> <font color="#00698f>inevitably</font> <font color="#ffcc00>leads</font> <font color="#00698f>to</font> <font color="#ffcc00>performance</font> <font color="#00698f>drop</font>;</font><font color="#00698f">2</font> <font color="#ffcc00>the</font> <font color="#00698f>disentanglement</font> <font color="#ffcc00>constraints</font> <font color="#00698f>on</font> <font color="#ffcc00>representations</font> <font color="#00698f>are</font> <font color="#ffcc00>in</font> <font color="#00698f>complicated</font> <font color="#ffcc00>optimization</font>.</font><font color="#00698f">To</font> <font color="#ffcc00>these</font> <font color="#00698f>issues</font>, <font color="#ffcc00>we</font> <font color="#00698f>introduce</font> <font color="#ffcc00>Bayesian</font> <font color="#00698f>networks</font> <font color="#ffcc00>with</font> <font color="#00698f>transmitted</font> <font color="#ffcc00>information</font> <font color="#00698f>to</font> <font color="#ffcc00>formulate</font> <font color="#00698f>the</font> <font color="#ffcc00>interaction</font> <font color="#00698f>among</font> <font color="#ffcc00>input</font> <font color="#00698f>and</font> <font color="#ffcc00>representations</font> <font color="#00698f>during</font> <font color="#00698f>disentanglement</font>.</font><font color="#00698f">Building</font> <font color="#ffcc00>upon</font> <font color="#00698f>this</font> <font color="#ffcc00>framework</font>, <font color="#00698f>we</font> <font color="#ffcc00>propose</font> <font color="#00698f>DisTIB</font> <font color="#ffcc00>(</font> <font color="#00698f>T</font> <font color="#ffcc00>ransmitted</font> <font color="#00698f>I</font> <font color="#ffcc00>nformation</font> <font color="#00698f>B</font> <font color="#ffcc00>ottleneck</font> <font color="#00698f>for</font> <font color="#ffcc00>Dis</font> <font color="#00698f>entangled</font> <font color="#ffcc00>representation</font> <font color="#00698f>learning</font>)</font>, <font color="#00698f>a</font> <font color="#ffcc00>novel</font> <font color="#00698f>objective</font> <font color="#ffcc00>that</font> <font color="#00698f>navigates</font> <font color="#ffcc00>the</font> <font color="#00698f>balance</font> <font color="#ffcc00>between</font> <font color="#00698f>information</font> <font color="#ffcc00>compression</font> <font color="#00698f>and</font> <font color="#ffcc00>preservation</font>.</font><font color="#00698f">We</font> <font color="#ffcc00>employ</font> <font color="#00698f>variational</font> <font color="#ffcc00>inference</font> <font color="#00698f>to</font> <font color="#ffcc00>derive</font> <font color="#00698f>a</font> <font color="#ffcc00>tractable</font> <font color="#00698f>estimation</font> <font color="#ffcc00>for</font> <font color="#00698f>DisTIB</font>.</font><font color="#00698f">This</font> <font color="#ffcc00>estimation</font> <font color="#00698f>can</font> <font color="#ffcc00>be</font> <font color="#00698f>simply</font> <font color="#ffcc00>optimized</font> <font color="#00698f>via</font> <font color="#ffcc00>standard</font> <font color="#00698f>gradient</font> <font color="#ffcc00>descent</font> <font color="#00698f>with</font> <font color="#ffcc00>a</font> <font color="#00698f>reparameterization</font> <font color="#ffcc00>trick</font>.</font><font color="#00698f">Moreover</font>, <font color="#ffcc00>we</font> <font color="#00698f>theoretically</font> <font color="#ffcc00>prove</font> <font color="#00698f>that</font> <font color="#ffcc00>DisTIB</font> <font color="#00698f>can</font> <font color="#ffcc00>achieve</font> <font color="#00698f>optimal</font> <font color="#ffcc00>disentanglement</font>,</font> <font color="#00698f>underscoring</font> <font color="#ffcc00>its</font> <font color="#00698f>superior</font> <font color="#ffcc00>efficacy</font>.</font><font color="#00698f">To</font> <font color="#ffcc00>solidify</font> <font color="#00698f>our</font> <font color="#ffcc00>claims</font>,</font> <font color="#00698f>we</font> <font color="#ffcc00>conduct</font> <font color="#00698f>extensive</font> <font color="#ffcc00>experiments</font> <font color="#00698f>on</font> <font color="#ffcc00>various</font> <font color="#00698f>downstream</font> <font color="#ffcc00>tasks</font> <font color="#00698f>to</font> <font color="#ffcc00>demonstrate</font> <font color="#00698f>the</font> <font color="#ffcc00>appealing</font> <font color="#00698f>efficacy</font> <font color="#ffcc00>of</font> <font color="#00698f>DisTIB</font> <font color="#ffcc00>and</font> <font color="#00698f>validate</font> <font color="#ffcc00>our</font> <font color="#00698f>theoretical</font> <font color="#ffcc00>analyses</font>.</font>
</details></li>
</ul>
<hr>
<h2 id="Flow-Based-Feature-Fusion-for-Vehicle-Infrastructure-Cooperative-3D-Object-Detection"><a href="#Flow-Based-Feature-Fusion-for-Vehicle-Infrastructure-Cooperative-3D-Object-Detection" class="headerlink" title="Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection"></a>Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01682">http://arxiv.org/abs/2311.01682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haibao-yu/ffnet-vic3d">https://github.com/haibao-yu/ffnet-vic3d</a></li>
<li>paper_authors: Haibao Yu, Yingjuan Tang, Enze Xie, Jilei Mao, Ping Luo, Zaiqing Nie</li>
<li>for: 提高自动驾驶感知能力</li>
<li>methods: 使用Feature Flow Net（FFNet），一种新的合作检测框架，利用流量预测模块预测未来特征，补偿异步问题</li>
<li>results: 在DAIR-V2X数据集上实现了比现有合作检测方法更高的性能，只需要约1&#x2F;100的数据传输成本，覆盖所有延迟，可以在一个模型中实现<details>
<summary>Abstract</summary>
Cooperatively utilizing both ego-vehicle and infrastructure sensor data can significantly enhance autonomous driving perception abilities. However, the uncertain temporal asynchrony and limited communication conditions can lead to fusion misalignment and constrain the exploitation of infrastructure data. To address these issues in vehicle-infrastructure cooperative 3D (VIC3D) object detection, we propose the Feature Flow Net (FFNet), a novel cooperative detection framework. FFNet is a flow-based feature fusion framework that uses a feature flow prediction module to predict future features and compensate for asynchrony. Instead of transmitting feature maps extracted from still-images, FFNet transmits feature flow, leveraging the temporal coherence of sequential infrastructure frames. Furthermore, we introduce a self-supervised training approach that enables FFNet to generate feature flow with feature prediction ability from raw infrastructure sequences. Experimental results demonstrate that our proposed method outperforms existing cooperative detection methods while only requiring about 1/100 of the transmission cost of raw data and covers all latency in one model on the DAIR-V2X dataset. The code is available at \href{https://github.com/haibao-yu/FFNet-VIC3D}{https://github.com/haibao-yu/FFNet-VIC3D}.
</details>
<details>
<summary>摘要</summary>
合作使用 Egovehicle 和基础设施感知数据可以大幅提高自动驾驶感知能力。然而，不确定的时间偏移和限制通信条件可能导致融合不一致，从而限制基础设施数据的利用。为解决这些问题，我们提出了 Feature Flow Net（FFNet），一种新的合作探测框架。FFNet 是一种基于流量的特征融合框架，使用特征流预测模块预测未来特征，以补偿偏移。而不是将静止图像中的特征图传输，FFNet 传输特征流，利用基础设施图像序列的时间启发关系。此外，我们提出了一种自动超参训练方法，使 FFNet 可以从 raw 基础设施序列中生成特征流，并且具有特征预测能力。实验结果表明，我们提posed 方法在 DAIR-V2X 数据集上比既有的合作探测方法高效，仅需要约 1/100 的传输成本，并且可以覆盖所有延迟。代码可以在 \href{https://github.com/haibao-yu/FFNet-VIC3D}{https://github.com/haibao-yu/FFNet-VIC3D} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Content-Significance-Distribution-of-Sub-Text-Blocks-in-Articles-and-Its-Application-to-Article-Organization-Assessment"><a href="#Content-Significance-Distribution-of-Sub-Text-Blocks-in-Articles-and-Its-Application-to-Article-Organization-Assessment" class="headerlink" title="Content Significance Distribution of Sub-Text Blocks in Articles and Its Application to Article-Organization Assessment"></a>Content Significance Distribution of Sub-Text Blocks in Articles and Its Application to Article-Organization Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01673">http://arxiv.org/abs/2311.01673</a></li>
<li>repo_url: None</li>
<li>paper_authors: You Zhou, Jie Wang</li>
<li>for: 这paper是为了研究文章中各个子句块的内容重要性而写的。</li>
<li>methods: 这paper使用了Hugging Face的SentenceTransformer生成文本嵌入，并使用文本嵌入的MoverScore来衡量每个子句块与整篇文章之间的相似度。</li>
<li>results: 这paper表明，通过一种近似算法，可以快速计算每个子句块的内容重要性分布（CSD-1），并且这些分布随着文章类型的不同而展现出明显的差异。此外，这paper还发现，对于某些文章类型，CSD-2的平均值具有明确的特征，这些特征可以衡量文章的结构和组织。<details>
<summary>Abstract</summary>
We explore how to capture the significance of a sub-text block in an article and how it may be used for text mining tasks. A sub-text block is a sub-sequence of sentences in the article. We formulate the notion of content significance distribution (CSD) of sub-text blocks, referred to as CSD of the first kind and denoted by CSD-1. In particular, we leverage Hugging Face's SentenceTransformer to generate contextual sentence embeddings, and use MoverScore over text embeddings to measure how similar a sub-text block is to the entire text. To overcome the exponential blowup on the number of sub-text blocks, we present an approximation algorithm and show that the approximated CSD-1 is almost identical to the exact CSD-1. Under this approximation, we show that the average and median CSD-1's for news, scholarly research, argument, and narrative articles share the same pattern. We also show that under a certain linear transformation, the complement of the cumulative distribution function of the beta distribution with certain values of $\alpha$ and $\beta$ resembles a CSD-1 curve. We then use CSD-1's to extract linguistic features to train an SVC classifier for assessing how well an article is organized. Through experiments, we show that this method achieves high accuracy for assessing student essays. Moreover, we study CSD of sentence locations, referred to as CSD of the second kind and denoted by CSD-2, and show that average CSD-2's for different types of articles possess distinctive patterns, which either conform common perceptions of article structures or provide rectification with minor deviation.
</details>
<details>
<summary>摘要</summary>
我们探讨了如何捕捉文章中具有重要意义的子文本块，以及如何将其用于文本挖掘任务。子文本块是文章中的子序列。我们定义了文章内容重要性分布（CSD）的首次类型，并使用Hugging Face的 SentenceTransformer生成文本上下文嵌入，以及在文本嵌入空间中计算MoverScore来衡量子文本块与整个文章的相似度。为了解决子文本块数量呈指数爆发的问题，我们提出了一种近似算法，并证明了近似的CSD-1与正确的CSD-1几乎相同。在这种近似下，我们发现了不同文章类型的平均和中位CSD-1呈同样的模式。此外，我们还发现了一种特定的线性变换，使得 beta 分布的剩余分布函数的补做与CSD-1呈同样的形式。我们使用CSD-1来提取语言特征，并使用这些特征来训练一个SVC分类器，以评估文章是否具有良好的结构。通过实验，我们发现这种方法可以高效地评估学生的文章。此外，我们还研究了具有不同类型的文章的CSD-2，并发现了不同类型的文章的平均CSD-2具有不同的特征模式，一些符合常见的文章结构假设，一些则提供了一些修正。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Cloud-Pipelines-for-Neural-Radiance-Fields"><a href="#Efficient-Cloud-Pipelines-for-Neural-Radiance-Fields" class="headerlink" title="Efficient Cloud Pipelines for Neural Radiance Fields"></a>Efficient Cloud Pipelines for Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01659">http://arxiv.org/abs/2311.01659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Derek Jacoby, Donglin Xu, Weder Ribas, Minyi Xu, Ting Liu, Vishwanath Jayaraman, Mengdi Wei, Emma De Blois, Yvonne Coady</li>
<li>for: 这篇论文是关于Neural Radiance Fields（NeRFs）的应用和实现方法的研究。</li>
<li>methods: 这篇论文使用了高性能的学术计算机群组件和Microsoft Azure云pipeline来构建NeRFs。</li>
<li>results: 该论文描述了NeRFs在各种应用中的可能性，包括虚拟生产、虚拟现实和地ospatial分析中的变化检测。<details>
<summary>Abstract</summary>
Since their introduction in 2020, Neural Radiance Fields (NeRFs) have taken the computer vision community by storm. They provide a multi-view representation of a scene or object that is ideal for eXtended Reality (XR) applications and for creative endeavors such as virtual production, as well as change detection operations in geospatial analytics. The computational cost of these generative AI models is quite high, however, and the construction of cloud pipelines to generate NeRFs is neccesary to realize their potential in client applications. In this paper, we present pipelines on a high performance academic computing cluster and compare it with a pipeline implemented on Microsoft Azure. Along the way, we describe some uses of NeRFs in enabling novel user interaction scenarios.
</details>
<details>
<summary>摘要</summary>
自2020年引入以来，神经辐射场（NeRF）已经在计算机视觉社区引起了一阵风波。它们提供了一种场景或物体的多视图表示方式，非常适合扩展现实（XR）应用和虚拟生产、地球分析等创新应用。然而，这些生成AI模型的计算成本很高，因此构建云管道生成NeRF的措施是必须的，以实现客户端应用中的潜力。在这篇论文中，我们介绍了一个高性能的学术计算群集上的管道，并与Microsoft Azure上的管道进行比较。此外，我们还描述了NeRF在启发新用户交互方案的应用。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Spurious-Correlations-via-Robust-Visual-Concepts-in-Real-and-AI-Generated-Image-Classification"><a href="#Detecting-Spurious-Correlations-via-Robust-Visual-Concepts-in-Real-and-AI-Generated-Image-Classification" class="headerlink" title="Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification"></a>Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01655">http://arxiv.org/abs/2311.01655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Preetam Prabhu Srikar Dammu, Chirag Shah</li>
<li>for: 检测模型中的假 correlate，提高模型的可靠性和抗分布Shift性。</li>
<li>methods: 提出一种通用的方法，能够快速、少量的人工干预检测可能的假 correlate，并提供直观的解释。</li>
<li>results: 在AI生成图像中表现出色，能够检测模型中的假 correlate，而且不需要像素级别的注释。<details>
<summary>Abstract</summary>
Often machine learning models tend to automatically learn associations present in the training data without questioning their validity or appropriateness. This undesirable property is the root cause of the manifestation of spurious correlations, which render models unreliable and prone to failure in the presence of distribution shifts. Research shows that most methods attempting to remedy spurious correlations are only effective for a model's known spurious associations. Current spurious correlation detection algorithms either rely on extensive human annotations or are too restrictive in their formulation. Moreover, they rely on strict definitions of visual artifacts that may not apply to data produced by generative models, as they are known to hallucinate contents that do not conform to standard specifications. In this work, we introduce a general-purpose method that efficiently detects potential spurious correlations, and requires significantly less human interference in comparison to the prior art. Additionally, the proposed method provides intuitive explanations while eliminating the need for pixel-level annotations. We demonstrate the proposed method's tolerance to the peculiarity of AI-generated images, which is a considerably challenging task, one where most of the existing methods fall short. Consequently, our method is also suitable for detecting spurious correlations that may propagate to downstream applications originating from generative models.
</details>
<details>
<summary>摘要</summary>
机器学习模型经常会自动学习训练数据中的关联，无论其VALIDITY或合适性是否得到评估。这种不良性质是潜在的相关性损害的根本原因，导致模型在分布变化时失效和不可靠。研究表明，现有的相关性检测方法只能有效对模型已知的假 correlate。当前的相关性检测算法可能需要广泛的人工标注或是过于 restrictive的形式。另外，它们可能会基于硬coded的视觉artifacts，这些artifacts可能不适用于由生成模型生成的数据，因为这些模型可能会hallucinate不符合标准规范的内容。在这种情况下，我们引入一种通用的方法，能够高效地检测潜在的相关性，并需要较少的人工干预。此外，我们的方法可以提供直观的解释，而不需要像素级别的标注。我们示示了我们的方法对生成模型生成的图像异常 Task 的耐误性，这是现有方法无法满足的一个挑战。因此，我们的方法适用于检测生成模型中传递的相关性，以及其下游应用中的相关性。
</details></li>
</ul>
<hr>
<h2 id="INeAT-Iterative-Neural-Adaptive-Tomography"><a href="#INeAT-Iterative-Neural-Adaptive-Tomography" class="headerlink" title="INeAT: Iterative Neural Adaptive Tomography"></a>INeAT: Iterative Neural Adaptive Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01653">http://arxiv.org/abs/2311.01653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Xiong, Changqing Su, Zihan Lin, You Zhou, Zhaofei Yu<br>for:* 这个研究旨在提高 Computed Tomography (CT) 的三维图像重建效果，特别是在面对 CT 扫描过程中的干扰和位置偏移时。methods:* 这个研究使用 Neural Adaptive Tomography (NeAT) 方法，它基于神经降光场来实现 CT 的三维图像重建。* 这个研究提出了一个叫 Iterative Neural Adaptive Tomography (INeAT) 的新方法，它利用迭代干扰优化来有效地抵消 CT 扫描过程中的干扰和位置偏移的影响。results:* 这个研究发现 INeAT 可以实现干扰和位置偏移不对的 CT 重建，并且可以维持和正常状态的 CT 重建效果，即使是在面对干扰和位置偏移的情况下。* 这个研究还发现 INeAT 可以实现短时间和低成本 CT 技术，因为它可以使用不稳定的扫描数据来进行重建。<details>
<summary>Abstract</summary>
Computed Tomography (CT) with its remarkable capability for three-dimensional imaging from multiple projections, enjoys a broad range of applications in clinical diagnosis, scientific observation, and industrial detection. Neural Adaptive Tomography (NeAT) is a recently proposed 3D rendering method based on neural radiance field for CT, and it demonstrates superior performance compared to traditional methods. However, it still faces challenges when dealing with the substantial perturbations and pose shifts encountered in CT scanning processes. Here, we propose a neural rendering method for CT reconstruction, named Iterative Neural Adaptive Tomography (INeAT), which incorporates iterative posture optimization to effectively counteract the influence of posture perturbations in data, particularly in cases involving significant posture variations. Through the implementation of a posture feedback optimization strategy, INeAT iteratively refines the posture corresponding to the input images based on the reconstructed 3D volume. We demonstrate that INeAT achieves artifact-suppressed and resolution-enhanced reconstruction in scenarios with significant pose disturbances. Furthermore, we show that our INeAT maintains comparable reconstruction performance to stable-state acquisitions even using data from unstable-state acquisitions, which significantly reduces the time required for CT scanning and relaxes the stringent requirements on imaging hardware systems, underscoring its immense potential for applications in short-time and low-cost CT technology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Keypoint-Description-by-Symmetry-Assessment-–-Applications-in-Biometrics"><a href="#Keypoint-Description-by-Symmetry-Assessment-–-Applications-in-Biometrics" class="headerlink" title="Keypoint Description by Symmetry Assessment – Applications in Biometrics"></a>Keypoint Description by Symmetry Assessment – Applications in Biometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01651">http://arxiv.org/abs/2311.01651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna Mikaelyan, Fernando Alonso-Fernandez, Josef Bigun</li>
<li>for: 提出了一种基于模型的特征提取器，用于描述附近关键点的地方特征，通过有限扩展来Estimate空间变化的方向。</li>
<li>methods: 使用哈密顿函数来描述附近地方的形状，这些函数在原点（关键点）的iso-曲线上具有高度的Symmetry，并且Estimate的参数具有明确的几何 interpretations。</li>
<li>results: 通过使用公开的数据集（NIST SD27）进行实验，提出了一种基于这种特征的验证和识别键点方法，其中验证性能达到19% EER，识别性能为24-78%。此外，还进行了近距离肖像特征的验证，并达到了13% EER的性能，与现有技术相当。而将两种系统 fusion 后，可以 obtaint measurable 性能提升。<details>
<summary>Abstract</summary>
We present a model-based feature extractor to describe neighborhoods around keypoints by finite expansion, estimating the spatially varying orientation by harmonic functions. The iso-curves of such functions are highly symmetric w.r.t. the origin (a keypoint) and the estimated parameters have well defined geometric interpretations. The origin is also a unique singularity of all harmonic functions, helping to determine the location of a keypoint precisely, whereas the functions describe the object shape of the neighborhood. This is novel and complementary to traditional texture features which describe texture-shape properties i.e. they are purposively invariant to translation (within a texture). We report on experiments of verification and identification of keypoints in forensic fingerprints by using publicly available data (NIST SD27) and discuss the results in comparison to other studies. These support our conclusions that the novel features can equip single cores or single minutia with a significant verification power at 19% EER, and an identification power of 24-78% for ranks of 1-20. Additionally, we report verification results of periocular biometrics using near-infrared images, reaching an EER performance of 13%, which is comparable to the state of the art. More importantly, fusion of two systems, our and texture features (Gabor), result in a measurable performance improvement. We report reduction of the EER to 9%, supporting the view that the novel features capture relevant visual information, which traditional texture features do not.
</details>
<details>
<summary>摘要</summary>
我们提出了基于模型的特征提取器，用finite expansion来描述附近关键点的 neighbohood，并估算空间变化的方向使用含有圆锥函数。这些iso-curves的函数具有很高的对称性关系于起始点（关键点），并且估算参数具有明确的 геометрической意义。起始点也是所有含有圆锥函数的唯一特点，帮助确定关键点的具体位置，而这些函数描述了附近物体的形状。这是一种新的和补充性的方法，与传统的文本特征不同，后者描述了文本-形状属性，即在翻译（在文本中）的抗抗变异性。我们在使用公共可用数据（NIST SD27）进行了实验，并发现结果与其他研究相比，支持我们的结论：新特征可以为单个核心或单个细胞提供显著的验证力，验证 Err 率为19%，并且可以为排名1-20的 Identification 提供24-78%的识别力。此外，我们还报告了使用near-infrared图像的 periocular 生物ometrics 验证结果，达到了13%的 Err 率，与状态之一样。更重要的是，将两个系统（我们的系统和文本特征）进行融合，可以获得可观测性提高。我们报告了 Err 率的减少为9%，支持我们的新特征捕捉了重要的视觉信息，传统的文本特征不捕捉。
</details></li>
</ul>
<hr>
<h2 id="SemiGPC-Distribution-Aware-Label-Refinement-for-Imbalanced-Semi-Supervised-Learning-Using-Gaussian-Processes"><a href="#SemiGPC-Distribution-Aware-Label-Refinement-for-Imbalanced-Semi-Supervised-Learning-Using-Gaussian-Processes" class="headerlink" title="SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes"></a>SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01646">http://arxiv.org/abs/2311.01646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelhak Lemkhenter, Manchen Wang, Luca Zancato, Gurumurthy Swaminathan, Paolo Favaro, Davide Modolo</li>
<li>for: 这个论文是关于semi-supervised learning的研究，旨在提出一种基于 Gaussian Processes 的分布意识修正策略，以提高模型在不同数据分布下的性能。</li>
<li>methods: 该策略基于 Gaussian Processes 的概率分布模型，包括一个归一化项以处理全局数据分布的偏见问题，以保持本地敏感度。</li>
<li>results: 对比 FixMatch、ReMixMatch、SimMatch 等 semi-supervised 方法和不同的预训练策略，SemiGPC 能够提高性能，特别在数据量较少的情况下。此外，SemiGPC 在不同水平的分类任务上达到了状态的艺术性能。<details>
<summary>Abstract</summary>
In this paper we introduce SemiGPC, a distribution-aware label refinement strategy based on Gaussian Processes where the predictions of the model are derived from the labels posterior distribution. Differently from other buffer-based semi-supervised methods such as CoMatch and SimMatch, our SemiGPC includes a normalization term that addresses imbalances in the global data distribution while maintaining local sensitivity. This explicit control allows SemiGPC to be more robust to confirmation bias especially under class imbalance. We show that SemiGPC improves performance when paired with different Semi-Supervised methods such as FixMatch, ReMixMatch, SimMatch and FreeMatch and different pre-training strategies including MSN and Dino. We also show that SemiGPC achieves state of the art results under different degrees of class imbalance on standard CIFAR10-LT/CIFAR100-LT especially in the low data-regime. Using SemiGPC also results in about 2% avg.accuracy increase compared to a new competitive baseline on the more challenging benchmarks SemiAves, SemiCUB, SemiFungi and Semi-iNat.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了一种基于 Gaussian Processes 的分布意识的标签级化策略，称为 SemiGPC。与其他缓冲区基于 semi-supervised 方法，如 CoMatch 和 SimMatch，SemiGPC 包含一个归一化项，以Address 数据全局分布不均衡的问题，同时保持本地敏感度。这种显式控制使 SemiGPC 更具Robustness 对Confirmation Bias，特别在类别不均衡情况下。我们展示了 SemiGPC 可以与不同的 semi-supervised 方法和预训练策略结合使用，包括 FixMatch、ReMixMatch、SimMatch 和 FreeMatch，以及不同的预训练策略，如 MSN 和 Dino。我们还展示了 SemiGPC 在不同的类别不均衡情况下的状态掌握结果，特别在低数据量情况下。使用 SemiGPC 也导致了相对于一个新竞争基准的平均准确率提高约 2%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/cs.CV_2023_11_03/" data-id="clpxp040a00mlfm883jxpd4kg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/cs.AI_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T12:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/03/cs.AI_2023_11_03/">cs.AI - 2023-11-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Post-Turing-Mapping-the-landscape-of-LLM-Evaluation"><a href="#Post-Turing-Mapping-the-landscape-of-LLM-Evaluation" class="headerlink" title="Post Turing: Mapping the landscape of LLM Evaluation"></a>Post Turing: Mapping the landscape of LLM Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02049">http://arxiv.org/abs/2311.02049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexey Tikhonov, Ivan P. Yamshchikov</li>
<li>for: 这篇论文旨在探讨大语言模型（LLM）的评估方法的发展历程，从阿兰·图灵的创始问题到现代人工智能研究。</li>
<li>methods: 这篇论文将LMM的发展分为不同的时期，每个时期都有其独特的标准和评估标准。传统的评估方法，如图灵测验，随着LMM越来越接近人类行为，而失去了可靠性。</li>
<li>results: 这篇论文强调了需要一个统一的评估系统，因为LMM在更广泛的社会影响下使用。通过分析常见的评估方法，这篇论文强调了标准化和客观标准的重要性，以确保LMM的可靠性、公平性和社会利好。<details>
<summary>Abstract</summary>
In the rapidly evolving landscape of Large Language Models (LLMs), introduction of well-defined and standardized evaluation methodologies remains a crucial challenge. This paper traces the historical trajectory of LLM evaluations, from the foundational questions posed by Alan Turing to the modern era of AI research. We categorize the evolution of LLMs into distinct periods, each characterized by its unique benchmarks and evaluation criteria. As LLMs increasingly mimic human-like behaviors, traditional evaluation proxies, such as the Turing test, have become less reliable. We emphasize the pressing need for a unified evaluation system, given the broader societal implications of these models. Through an analysis of common evaluation methodologies, we advocate for a qualitative shift in assessment approaches, underscoring the importance of standardization and objective criteria. This work serves as a call for the AI community to collaboratively address the challenges of LLM evaluation, ensuring their reliability, fairness, and societal benefit.
</details>
<details>
<summary>摘要</summary>
在大语言模型（LLM）的快速演化中，定义和标准化评估方法仍然是一个核心挑战。这篇文章跟踪了LLM评估的历史发展，从阿兰·图灵提出的基础问题到现代人工智能研究的时期。我们将LLM的发展分成不同的时期，每个时期都具有独特的标准和评估标准。随着LLM越来越接近人类行为，传统的评估代理人，如图灵测试，变得更加不可靠。我们强调了评估系统的统一化的需要，因为这些模型在社会中的广泛应用具有更大的社会意义。通过分析常见的评估方法，我们强调了标准化和客观标准的重要性，以便确保LLM的可靠性、公平性和社会 benefit。这篇文章作为人工智能社区的呼吁，呼吁所有相关人员共同面临LLM评估的挑战，以确保其可靠性、公平性和社会 benefit。
</details></li>
</ul>
<hr>
<h2 id="Quantum-circuit-synthesis-with-diffusion-models"><a href="#Quantum-circuit-synthesis-with-diffusion-models" class="headerlink" title="Quantum circuit synthesis with diffusion models"></a>Quantum circuit synthesis with diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02041">http://arxiv.org/abs/2311.02041</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/florianfuerrutter/genqc">https://github.com/florianfuerrutter/genqc</a></li>
<li>paper_authors: Florian Fürrutter, Gorka Muñoz-Gil, Hans J. Briegel</li>
<li>for: 本研究使用生成机器学习模型，具体来说是杂化扩散模型（DM），以便将量子操作翻译成可行的物理实现。</li>
<li>methods: 本研究使用文本条件来控制DM模型，使其生成所需的量子操作 dentro gate-based quantum circuit。这种方法可以避免在训练过程中经典计算量子动力学的极高开销，从而提高模型的效率。</li>
<li>results: 研究表明，DM模型在两个任务中表现出色：生成强相关性和编辑量子Circuit。模型可以生成新的Circuit，并支持扩展such as masking和编辑，以适应目标量子设备的约束。由于其灵活性和通用性，我们认为DM模型将在量子Circuit合成中扮演重要角色，提高实际应用和理论量子计算的理解。<details>
<summary>Abstract</summary>
Quantum computing has recently emerged as a transformative technology. Yet, its promised advantages rely on efficiently translating quantum operations into viable physical realizations. In this work, we use generative machine learning models, specifically denoising diffusion models (DMs), to facilitate this transformation. Leveraging text-conditioning, we steer the model to produce desired quantum operations within gate-based quantum circuits. Notably, DMs allow to sidestep during training the exponential overhead inherent in the classical simulation of quantum dynamics -- a consistent bottleneck in preceding ML techniques. We demonstrate the model's capabilities across two tasks: entanglement generation and unitary compilation. The model excels at generating new circuits and supports typical DM extensions such as masking and editing to, for instance, align the circuit generation to the constraints of the targeted quantum device. Given their flexibility and generalization abilities, we envision DMs as pivotal in quantum circuit synthesis, enhancing both practical applications but also insights into theoretical quantum computation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="VQPy-An-Object-Oriented-Approach-to-Modern-Video-Analytics"><a href="#VQPy-An-Object-Oriented-Approach-to-Modern-Video-Analytics" class="headerlink" title="VQPy: An Object-Oriented Approach to Modern Video Analytics"></a>VQPy: An Object-Oriented Approach to Modern Video Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01623">http://arxiv.org/abs/2311.01623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vqpy/vqpy">https://github.com/vqpy/vqpy</a></li>
<li>paper_authors: Shan Yu, Zhenting Zhu, Yu Chen, Hanchen Xu, Pengzhan Zhao, Yang Wang, Arthi Padmanabhan, Hugo Latapie, Harry Xu</li>
<li>for: 用于开发视频分析系统和服务的前沿技术之一是视频查询，用户可以通过这些查询找到视频中的特定 interessante Objekte。</li>
<li>methods: 该方法基于视频对象（如人、动物、车辆等）与传统对象指定语言中的对象模型之间的相似性，并提出了一种基于对象指定的视频分析方法。该方法名为VQPy，它包括一个容易用户表达视频对象和其交互的前端（基于Python），以及可自动构建和优化管道的可扩展后端。</li>
<li>results: 我们已经实现了VQPy，并将其开源到Cisco的DeepVision框架中。这个技术已经被商业化并应用于视频分析领域。<details>
<summary>Abstract</summary>
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
</details>
<details>
<summary>摘要</summary>
视频分析广泛应用于现代系统和服务中。用户发展的视频查询是视频分析的前导力量。建立在视频对象（如人、动物、车等）的核心思想上，我们提议开发一种对象射影视频分析方法。该方法名为VQPy，包括一个前端使用Python语言的变体，用于让用户轻松表达视频对象和它们之间的交互，以及可扩展的后端，可自动构建和优化视频对象的管道。我们已经实现和开源了VQPy，它在Cisco的DeepVision框架中被商业化。
</details></li>
</ul>
<hr>
<h2 id="APRICOT-Acuity-Prediction-in-Intensive-Care-Unit-ICU-Predicting-Stability-Transitions-and-Life-Sustaining-Therapies"><a href="#APRICOT-Acuity-Prediction-in-Intensive-Care-Unit-ICU-Predicting-Stability-Transitions-and-Life-Sustaining-Therapies" class="headerlink" title="APRICOT: Acuity Prediction in Intensive Care Unit (ICU): Predicting Stability, Transitions, and Life-Sustaining Therapies"></a>APRICOT: Acuity Prediction in Intensive Care Unit (ICU): Predicting Stability, Transitions, and Life-Sustaining Therapies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02026">http://arxiv.org/abs/2311.02026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Contreras, Brandon Silva, Benjamin Shickel, Tezcan Ozrazgat Baslanti, Yuanfang Ren, Ziyuan Guan, Sabyasachi Bandyopadhyay, Kia Khezeli, Azra Bihorac, Parisa Rashidi<br>for: 这个研究的目的是开发一个基于 transformer  ней网络的实时评估 ICU 病人症状的模型，以便在实时监测病人症状，提供便利于医生进行时间性的干预。methods: 该研究使用了三个大数据集进行开发和验证：University of Florida Health (UFH)、eICU Collaborative Research Database (eICU) 和 Medical Information Mart for Intensive Care (MIMIC)-IV。模型使用 transformer  ней网络，并进行了外部、时间和前向验证。results: 模型的表现与当前状态艺术方法相当，并且可以预测病人需要生命维持治疗的可能性。此外，模型还可以预测病人需要的生命维持治疗，例如呼吸机和 vasopressor。这些结果表明，APRICOT 模型可以帮助医生在实时监测病人症状，并提供有用的信息以便进行时间性的干预。<details>
<summary>Abstract</summary>
The acuity state of patients in the intensive care unit (ICU) can quickly change from stable to unstable, sometimes leading to life-threatening conditions. Early detection of deteriorating conditions can result in providing more timely interventions and improved survival rates. Current approaches rely on manual daily assessments. Some data-driven approaches have been developed, that use mortality as a proxy of acuity in the ICU. However, these methods do not integrate acuity states to determine the stability of a patient or the need for life-sustaining therapies. In this study, we propose APRICOT (Acuity Prediction in Intensive Care Unit), a Transformer-based neural network to predict acuity state in real-time in ICU patients. We develop and extensively validate externally, temporally, and prospectively the APRICOT model on three large datasets: University of Florida Health (UFH), eICU Collaborative Research Database (eICU), and Medical Information Mart for Intensive Care (MIMIC)-IV. The performance of APRICOT shows comparable results to state-of-the-art mortality prediction models (external AUROC 0.93-0.93, temporal AUROC 0.96-0.98, and prospective AUROC 0.98) as well as acuity prediction models (external AUROC 0.80-0.81, temporal AUROC 0.77-0.78, and prospective AUROC 0.87). Furthermore, APRICOT can make predictions for the need for life-sustaining therapies, showing comparable results to state-of-the-art ventilation prediction models (external AUROC 0.80-0.81, temporal AUROC 0.87-0.88, and prospective AUROC 0.85), and vasopressor prediction models (external AUROC 0.82-0.83, temporal AUROC 0.73-0.75, prospective AUROC 0.87). This tool allows for real-time acuity monitoring of a patient and can provide helpful information to clinicians to make timely interventions. Furthermore, the model can suggest life-sustaining therapies that the patient might need in the next hours in the ICU.
</details>
<details>
<summary>摘要</summary>
ICU病人的病状可以快速从稳定转变为不稳定，有时会导致生命危险。早期检测病人的状况下降可以提供更时效的干预措施，提高生存率。现有的方法基于手动日常评估。一些数据驱动的方法已经开发，它们使用死亡率作为ICU病人的严重程度的代理。但这些方法并不能考虑病人的稳定状况或需要的生命维持治疗。本研究提出了APRICOT（ICU病人稳定状况预测）模型，基于变换器来预测ICU病人的稳定状况。我们在UFH、eICU和MIMIC-IV三个大数据集上开发和验证了APRICOT模型，并取得了相当于现状的result。APRICOT模型的性能与现状的死亡预测模型（外部AUROC0.93-0.93，时间AUROC0.96-0.98，前瞻AUROC0.98）以及稳定状况预测模型（外部AUROC0.80-0.81，时间AUROC0.77-0.78，前瞻AUROC0.87）相当。此外，APRICOT模型还可以预测需要生命维持治疗的可能性，与现状的呼吸预测模型（外部AUROC0.80-0.81，时间AUROC0.87-0.88，前瞻AUROC0.85）和 vasopressor 预测模型（外部AUROC0.82-0.83，时间AUROC0.73-0.75，前瞻AUROC0.87）相当。这种工具可以实时监测ICU病人的稳定状况，并提供便利的信息，以便医生在时间上作出有效的干预措施。此外，模型还可以预测在ICU中接下来几个小时内需要的生命维持治疗。
</details></li>
</ul>
<hr>
<h2 id="Active-Reasoning-in-an-Open-World-Environment"><a href="#Active-Reasoning-in-an-Open-World-Environment" class="headerlink" title="Active Reasoning in an Open-World Environment"></a>Active Reasoning in an Open-World Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02018">http://arxiv.org/abs/2311.02018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, Yixin Zhu</li>
<li>for: 这 paper 的目的是提出一个可互动的开放世界环境，以评估活跃的理解能力。</li>
<li>methods: 这 paper 使用了一种名为 $Conan$ 的交互式开放世界环境，激发了人工智能代理人的活跃探索和多轮推理。</li>
<li>results: 经过分析 $Conan$ 环境，这 paper 发现了现代状态的许多模型在活跃探索和解释复杂情况时存在缺陷。同时， paper 还探讨了从推理转移到推理的过程，并在 $Conan$ 环境中实现了这种转移。<details>
<summary>Abstract</summary>
Recent advances in vision-language learning have achieved notable success on complete-information question-answering datasets through the integration of extensive world knowledge. Yet, most models operate passively, responding to questions based on pre-stored knowledge. In stark contrast, humans possess the ability to actively explore, accumulate, and reason using both newfound and existing information to tackle incomplete-information questions. In response to this gap, we introduce $Conan$, an interactive open-world environment devised for the assessment of active reasoning. $Conan$ facilitates active exploration and promotes multi-round abductive inference, reminiscent of rich, open-world settings like Minecraft. Diverging from previous works that lean primarily on single-round deduction via instruction following, $Conan$ compels agents to actively interact with their surroundings, amalgamating new evidence with prior knowledge to elucidate events from incomplete observations. Our analysis on $Conan$ underscores the shortcomings of contemporary state-of-the-art models in active exploration and understanding complex scenarios. Additionally, we explore Abduction from Deduction, where agents harness Bayesian rules to recast the challenge of abduction as a deductive process. Through $Conan$, we aim to galvanize advancements in active reasoning and set the stage for the next generation of artificial intelligence agents adept at dynamically engaging in environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DeliverAI-Reinforcement-Learning-Based-Distributed-Path-Sharing-Network-for-Food-Deliveries"><a href="#DeliverAI-Reinforcement-Learning-Based-Distributed-Path-Sharing-Network-for-Food-Deliveries" class="headerlink" title="DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries"></a>DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02017">http://arxiv.org/abs/2311.02017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashman Mehra, Snehanshu Saha, Vaskar Raychoudhury, Archana Mathur</li>
<li>for: 这篇论文目的是为了提出一个基于人工智能学习的食物配送方案，以减少现有的配送成本和提高配送效率。</li>
<li>methods: 本论文使用了一种多目标优化方法，将consumer satisfaction和配送成本都当作优化目标，并通过一个基于强化学习的代理人系统来实现实时的决策。</li>
<li>results: 根据 simulations 的结果，DeliverAI 可以降低配送车队大小 by 12%，减少配送距离 by 13%，并提高配送效率 by 50% 相比基准。<details>
<summary>Abstract</summary>
Delivery of items from the producer to the consumer has experienced significant growth over the past decade and has been greatly fueled by the recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are rapidly growing and are sharing the same business model of consumer items or food delivery. Existing food delivery methods are sub-optimal because each delivery is individually optimized to go directly from the producer to the consumer via the shortest time path. We observe a significant scope for reducing the costs associated with completing deliveries under the current model. We model our food delivery problem as a multi-objective optimization, where consumer satisfaction and delivery costs, both, need to be optimized. Taking inspiration from the success of ride-sharing in the taxi industry, we propose DeliverAI - a reinforcement learning-based path-sharing algorithm. Unlike previous attempts for path-sharing, DeliverAI can provide real-time, time-efficient decision-making using a Reinforcement learning-enabled agent system. Our novel agent interaction scheme leverages path-sharing among deliveries to reduce the total distance traveled while keeping the delivery completion time under check. We generate and test our methodology vigorously on a simulation setup using real data from the city of Chicago. Our results show that DeliverAI can reduce the delivery fleet size by 12\%, the distance traveled by 13%, and achieve 50% higher fleet utilization compared to the baselines.
</details>
<details>
<summary>摘要</summary>
生产者到消费者的物品交付经历了过去十年的显著增长，而且受到最近的流行病影响很大。亚马逊新鲜、拍卖、uberEats、InstaCart和doorDash等公司的快速增长，都是共享同一个生意模式，即消费者物品或食品交付。现有的食品交付方法有限，每个交付都是单独优化，直接从生产者到消费者，最短时间路径。我们观察到现有交付模式中存在很大的成本减少空间。我们将食品交付问题模型为多目标优化问题，即消费者满意度和交付成本都需要优化。从taxi行业中成功的乘车共享经验，我们提出DeliverAI - 基于强化学习的路径共享算法。与过去的路径共享方法不同，DeliverAI可以在实时、时间高效的情况下，通过强化学习 Agent系统进行决策。我们的新代理互动方式利用交付中的路径共享，以降低总距离和保持交付完成时间。我们在使用实际的 ЧикаGO市数据进行了严格的测试，结果显示，DeliverAI可以减少交付车队大小12%，总距离减少13%，并实现交付车队使用率50%高于基准值。
</details></li>
</ul>
<hr>
<h2 id="Score-Models-for-Offline-Goal-Conditioned-Reinforcement-Learning"><a href="#Score-Models-for-Offline-Goal-Conditioned-Reinforcement-Learning" class="headerlink" title="Score Models for Offline Goal-Conditioned Reinforcement Learning"></a>Score Models for Offline Goal-Conditioned Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02013">http://arxiv.org/abs/2311.02013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshit Sikchi, Rohan Chitnis, Ahmed Touati, Alborz Geramifard, Amy Zhang, Scott Niekum</li>
<li>for: 本研究的目的是开发一种能够在完全离线 dataset 上学习多个目标的 Reinforcement Learning（RL）方法，以便开发通用的智能体可以在不需要手工设计奖励函数的情况下学习多种多样的技能。</li>
<li>methods: 本研究使用的方法是基于权重的 mixture-distribution matching，它将occupancy matching perspective和凸 dual形式的学习目标相结合，以更好地利用不优化的离线数据。</li>
<li>results: 实验表明，SMORe 可以在高维观察数据的 robot 抓取和行走任务上超过了现状的基准值，并且可以在不同的环境下达到更好的性能。<details>
<summary>Abstract</summary>
Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a convex dual formulation to derive a learning objective that can better leverage suboptimal offline data. SMORe learns scores or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. SMORe is principled and our extensive experiments on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks, including high-dimensional observations, show that SMORe can outperform state-of-the-art baselines by a significant margin.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>Offline Goal-Conditioned Reinforcement Learning（GCRL）的任务是从无线据集中学习多个目标，使用稀疏奖励函数。Offline GCRL 是开发通用代理人可以利用预存在的数据学习多样化和可重用的技能，而无需手工设计奖励函数的关键。然而，当前 GCRL 方法基于监督学习和对比学习 often 在离线设置下是不优化的。一种 alternativa  perspective on GCRL 是优化occupancy 匹配，但需要学习一个discriminator，该 discriminator  subsequenly 作为 Pseudo-reward  для下游 RL。在学习过程中，inaccuracies 在学习的 discriminator 可能会倒逼 negatively 影响 resulting 策略。我们提出一种新的 GCRL 方法，称为 SMORe，该方法基于 mixture-distribution 匹配的新镜头。关键思想是将 GCRL 的 occupancy 匹配视角与 convex  dual 表示相结合，以 deriv 一个学习目标，可以更好地利用不优化的离线数据。SMORe 学习 actions 在状态下的importance ，代表着达到特定目标的可能性。SMORe 是原则的，我们在无线 GCRL benchmark 上的完全离线任务中，包括机器人 manipulate 和 locomotion 任务，以高维观察表示，展现了 SMORe 可以在 state-of-the-art 基准点上高于表现。
</details></li>
</ul>
<hr>
<h2 id="Obtaining-Explainable-Classification-Models-using-Distributionally-Robust-Optimization"><a href="#Obtaining-Explainable-Classification-Models-using-Distributionally-Robust-Optimization" class="headerlink" title="Obtaining Explainable Classification Models using Distributionally Robust Optimization"></a>Obtaining Explainable Classification Models using Distributionally Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01994">http://arxiv.org/abs/2311.01994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjeeb Dash, Soumyadip Ghosh, Joao Goncalves, Mark S. Squillante</li>
<li>for: 这个论文的目的是提出一种能够同时保证模型泛化质量和计算成本低的分类器建模方法。</li>
<li>methods: 这个论文使用了sets of feature value rulesconstructed using distributionally robust optimization，并使用column generation来高效地搜索rule sets的空间。</li>
<li>results: 论文的实验结果显示，提出的方法可以在一个大量的公共可用的二分类问题实例上超过竞争方法（如随机森林或推动等），从一个或多个以下维度来衡量：泛化质量、计算成本和可读性。<details>
<summary>Abstract</summary>
Model explainability is crucial for human users to be able to interpret how a proposed classifier assigns labels to data based on its feature values. We study generalized linear models constructed using sets of feature value rules, which can capture nonlinear dependencies and interactions. An inherent trade-off exists between rule set sparsity and its prediction accuracy. It is computationally expensive to find the right choice of sparsity -- e.g., via cross-validation -- with existing methods. We propose a new formulation to learn an ensemble of rule sets that simultaneously addresses these competing factors. Good generalization is ensured while keeping computational costs low by utilizing distributionally robust optimization. The formulation utilizes column generation to efficiently search the space of rule sets and constructs a sparse ensemble of rule sets, in contrast with techniques like random forests or boosting and their variants. We present theoretical results that motivate and justify the use of our distributionally robust formulation. Extensive numerical experiments establish that our method improves over competing methods -- on a large set of publicly available binary classification problem instances -- with respect to one or more of the following metrics: generalization quality, computational cost, and explainability.
</details>
<details>
<summary>摘要</summary>
模型可读性是关键，以便人类用户可以根据特征值来解释提议分类器对数据分配标签。我们研究使用集合特征值规则构建的泛化线性模型，可以捕捉非线性关系和互动。存在一种折衔选择精度和预测精度之间的矛盾。现有方法 computationally expensive 找到最佳精度 -- 例如，via 批处理 -- 的方法。我们提出了一种新的表述，以同时解决这些矛盾的因素。我们的方法可以保证良好的泛化质量，同时降低计算成本，通过使用分布式 robust 优化。我们的表述使用列生成来快速搜索规则集的空间，并构建一个稀疏的规则集 ensemble，与 Random Forest 或 boosting 和其他变体不同。我们提供了理论上的结果，以证明和正确使用我们的分布式 robust 表述。我们的实验证明，我们的方法在一组公共可用的二分类问题实例上比竞争方法更好，以下一个或多个纪录：泛化质量、计算成本和可读性。
</details></li>
</ul>
<hr>
<h2 id="RT-Trajectory-Robotic-Task-Generalization-via-Hindsight-Trajectory-Sketches"><a href="#RT-Trajectory-Robotic-Task-Generalization-via-Hindsight-Trajectory-Sketches" class="headerlink" title="RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches"></a>RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01977">http://arxiv.org/abs/2311.01977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao</li>
<li>for: 本研究旨在提高机器人学习系统的普适性，使其能够更好地适应新任务和新情况。</li>
<li>methods: 本研究提出了一种基于粗略运动路径图像的政策条件方法（RT-Trajectory），该方法通过粗略的运动路径图像来表达任务，并且可以让策略更好地适应新任务。</li>
<li>results: 实验结果表明，RT-Trajectory 能够在各种真实世界机器人任务中表现出较好的普适性，并且可以在不同的训练数据下表现出更广泛的任务能力。<details>
<summary>Abstract</summary>
Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies: they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.
</details>
<details>
<summary>摘要</summary>
通用化仍然是Robot学习系统中最重要的目标之一。Recently proposed approaches show promise in generalizing to novel objects, semantic concepts, or visual distribution shifts, but generalizing to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies: they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.
</details></li>
</ul>
<hr>
<h2 id="The-language-of-prompting-What-linguistic-properties-make-a-prompt-successful"><a href="#The-language-of-prompting-What-linguistic-properties-make-a-prompt-successful" class="headerlink" title="The language of prompting: What linguistic properties make a prompt successful?"></a>The language of prompting: What linguistic properties make a prompt successful?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01967">http://arxiv.org/abs/2311.01967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alina Leidinger, Robert van Rooij, Ekaterina Shutova</li>
<li>for: 这研究旨在调查LLMs的不同大小、预训练和指令调整后在不同语法结构和词义上的提示表现。</li>
<li>methods: 该研究使用了LLMs的不同大小、预训练和指令调整后的模型，并对提示进行了语法结构和词义上的变化，以调查提示表现的相关性。</li>
<li>results: 研究发现，LLMs的表现与提示语法结构和词义上的变化有负相关性，而不是与提示的低准确率或字频、歧义率或提示长度相关。这 suggets that 现有的评价标准可能不够全面，需要更加全面和可靠的评价标准。<details>
<summary>Abstract</summary>
The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performance cannot generally be explained by perplexity, word frequency, ambiguity or prompt length. Based on our results, we put forward a proposal for a more robust and comprehensive evaluation standard for prompting research.
</details>
<details>
<summary>摘要</summary>
最新一代LLM可以通过提示来实现吸引人的零shot或几shot性能在多种NLP任务中。然而，由于选择提示的性能具有很高的敏感度，因此在这方面投入了大量的时间和精力，包括投入人们或设计提示优化方法。然而，我们仍然缺乏系统性的理解，推广提示的语言性质如何与任务性能相关。在这项工作中，我们研究了不同大小的LLM，预训练和指导调整后的表现，对于semantically相同但 linguistically不同的提示。我们研究了 grammatical properties 如模式、时态、方式和可能性，以及 lexico-semantic variation through the use of synonyms。我们的发现证明了常见的假设，即LLMs在低凝度提示上表现最佳，不正确。提示在不同的数据集或模型之间传递不好，并且不能通过凝度、字 frequency、歧义或提示长度来解释性能。根据我们的结果，我们提出了一种更加可靠和全面的评估标准 для提示研究。
</details></li>
</ul>
<hr>
<h2 id="Don’t-Make-Your-LLM-an-Evaluation-Benchmark-Cheater"><a href="#Don’t-Make-Your-LLM-an-Evaluation-Benchmark-Cheater" class="headerlink" title="Don’t Make Your LLM an Evaluation Benchmark Cheater"></a>Don’t Make Your LLM an Evaluation Benchmark Cheater</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01964">http://arxiv.org/abs/2311.01964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han</li>
<li>for: 评估大语言模型（LLM）性能，提高人工智能前进。</li>
<li>methods: 使用评估指标来评估LLM的能力水平，但可能会导致不当使用和误导性的评估结果。</li>
<li>results: 在大量实验中发现，使用评估数据来训练模型可能会带来很大的评估结果偏高，导致模型性能的误估。提出了一些指南，以改善LLM的评估和训练。<details>
<summary>Abstract</summary>
Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \ie \emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct extensive experiments to study the effect of benchmark leverage, and find that it can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance. To improve the use of existing evaluation benchmarks, we finally present several guidelines for both LLM developers and benchmark maintainers. We hope this work can draw attention to appropriate training and evaluation of LLMs.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" is translated as "大语言模型" (dà yǔ yán módel), which is a common term used to refer to deep learning models that are trained on large amounts of text data.* "Artificial intelligence" is translated as "人工智能" (rén gōng zhì nǎo), which is the standard term used in Chinese to refer to AI.* "Evaluation benchmarks" is translated as "评估标准" (píng jī biaodian), which refers to the datasets and tasks used to evaluate the performance of LLMs.* "Benchmark leakage" is translated as "标准泄露" (biāo zhì zhòu), which refers to the practice of using data related to evaluation sets for model training.* "Pre-training data" is translated as "预训练数据" (xiù xù xíng xīn), which refers to the data used to pre-train LLMs before fine-tuning them on specific tasks.
</details></li>
</ul>
<hr>
<h2 id="Assessing-Fidelity-in-XAI-post-hoc-techniques-A-Comparative-Study-with-Ground-Truth-Explanations-Datasets"><a href="#Assessing-Fidelity-in-XAI-post-hoc-techniques-A-Comparative-Study-with-Ground-Truth-Explanations-Datasets" class="headerlink" title="Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets"></a>Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01961">http://arxiv.org/abs/2311.01961</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Miró-Nicolau, A. Jaume-i-Capó, G. Moyà-Alcover</li>
<li>for: 本研究的目的是评估current state-of-the-art XAI方法的准确性和可靠性，并从中排除低准确性的方法，以促进更好的XAI技术的发展。</li>
<li>methods: 本研究使用了三种常见的XAI方法，namely backpropagation of output information to input, sensitivity analysis, and Class Activation Maps (CAM).</li>
<li>results: 研究结果表明，基于output backpropagation的XAI方法具有较高的准确性和可靠性，而sensitivity analysis和CAM方法则相对较低。然而，backpropagation方法生成的关键区域图像具有较高的噪声水平。这些发现有助于排除错误的解释并推动XAI技术的进一步发展。<details>
<summary>Abstract</summary>
The evaluation of the fidelity of eXplainable Artificial Intelligence (XAI) methods to their underlying models is a challenging task, primarily due to the absence of a ground truth for explanations. However, assessing fidelity is a necessary step for ensuring a correct XAI methodology. In this study, we conduct a fair and objective comparison of the current state-of-the-art XAI methods by introducing three novel image datasets with reliable ground truth for explanations. The primary objective of this comparison is to identify methods with low fidelity and eliminate them from further research, thereby promoting the development of more trustworthy and effective XAI techniques. Our results demonstrate that XAI methods based on the backpropagation of output information to input yield higher accuracy and reliability compared to methods relying on sensitivity analysis or Class Activation Maps (CAM). However, the backpropagation method tends to generate more noisy saliency maps. These findings have significant implications for the advancement of XAI methods, enabling the elimination of erroneous explanations and fostering the development of more robust and reliable XAI.
</details>
<details>
<summary>摘要</summary>
评估Explainable Artificial Intelligence（XAI）方法的准确性是一项复杂的任务，主要是因为无法获得解释的准确参照值。然而，评估准确性是确保XAI方法正确性的必要步骤。在这种研究中，我们通过引入三个新的图像集来进行公正和客观地比较当前领域的XAI方法。我们的主要目标是通过评估XAI方法的准确性和可靠性来消除低准确性的方法，以促进更加可靠和有效的XAI技术的发展。我们的结果显示，基于输出信息的倒推法生成的XAI方法比靠视分析或Class Activation Maps（CAM）方法更高的准确性和可靠性。然而，倒推法通常会生成更多的噪声灵敏图。这些发现有重要的意义 дляXAI方法的发展，可以消除错误的解释并推动更加稳定和可靠的XAI技术的发展。
</details></li>
</ul>
<hr>
<h2 id="Architecture-of-Smart-Certificates-for-Web3-Applications-Against-Cyberthreats-in-Financial-Industry"><a href="#Architecture-of-Smart-Certificates-for-Web3-Applications-Against-Cyberthreats-in-Financial-Industry" class="headerlink" title="Architecture of Smart Certificates for Web3 Applications Against Cyberthreats in Financial Industry"></a>Architecture of Smart Certificates for Web3 Applications Against Cyberthreats in Financial Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01956">http://arxiv.org/abs/2311.01956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Kambiz Behfar, Jon Crowcroft</li>
<li>for: 本研究探讨了当今互联网的安全挑战，尤其是区块链和分布式存储等新技术的应用。它还研究了未来互联网的形态，并提出了一种新的“智能证书”设计方案，以帮助企业更好地保护自己免受网络攻击，并确保数据和系统的安全。</li>
<li>methods: 本研究使用了Web3应用程序和安全解决方案，如Certik、Forta、Slither和Securify等，以提高企业数字基础设施的可恢复性。它还提出了一种基于多层次架构的抗性分析和攻击聚合方法，以及一种用于检测和排查证书的证明力和可信worthiness。</li>
<li>results: 本研究提出的“智能证书”设计方案可以帮助企业更好地保护自己免受网络攻击，并提高数据和系统的安全性。此外，通过使用证书检测和排查技术，可以检测和排查证书的证明力和可信worthiness，以确保企业数据和系统的安全。<details>
<summary>Abstract</summary>
This study addresses the security challenges associated with the current internet transformations, specifically focusing on emerging technologies such as blockchain and decentralized storage. It also investigates the role of Web3 applications in shaping the future of the internet. The primary objective is to propose a novel design for 'smart certificates,' which are digital certificates that can be programmatically enforced. Utilizing such certificates, an enterprise can better protect itself from cyberattacks and ensure the security of its data and systems. Web3 recent security solutions by companies and projects like Certik, Forta, Slither, and Securify are the equivalent of code scanning tool that were originally developed for Web1 and Web2 applications, and definitely not like certificates to help enterprises feel safe against cyberthreats. We aim to improve the resilience of enterprises' digital infrastructure by building on top of Web3 application and put methodologies in place for vulnerability analysis and attack correlation, focusing on architecture of different layers, Wallet/Client, Application and Smart Contract, where specific components are provided to identify and predict threats and risks. Furthermore, Certificate Transparency is used for enhancing the security, trustworthiness and decentralized management of the certificates, and detecting misuses, compromises, and malfeasances.
</details>
<details>
<summary>摘要</summary>
Current Web3 security solutions, such as those offered by Certik, Forta, Slither, and Securify, are comparable to code scanning tools developed for Web1 and Web2 applications, and are not sufficient to protect enterprises from cyber threats. To improve the resilience of enterprises' digital infrastructure, we propose building on top of Web3 applications and implementing methodologies for vulnerability analysis and attack correlation, focusing on the architecture of different layers, including the wallet/client, application, and smart contract.To enhance the security, trustworthiness, and decentralized management of the certificates, we use Certificate Transparency. This also helps detect misuses, compromises, and malfeasances. Our proposed design for smart certificates aims to provide enterprises with a more secure and reliable way to protect their data and systems in the face of evolving cyber threats.
</details></li>
</ul>
<hr>
<h2 id="A-Quantitative-Autonomy-Quantification-Framework-for-Fully-Autonomous-Robotic-Systems"><a href="#A-Quantitative-Autonomy-Quantification-Framework-for-Fully-Autonomous-Robotic-Systems" class="headerlink" title="A Quantitative Autonomy Quantification Framework for Fully Autonomous Robotic Systems"></a>A Quantitative Autonomy Quantification Framework for Fully Autonomous Robotic Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01939">http://arxiv.org/abs/2311.01939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nasser Gyagenda, Hubert Roth</li>
<li>for: 本研究旨在提供一个基于任务需求的自主性评估框架，以帮助在限定人工监督下进行自主函数的机器人系统部署。</li>
<li>methods: 本研究使用了三种自主性指标，包括必要能力、可靠性和回应能力，并将它们与任务需求进行映射，以量化机器人系统的自主性水平。</li>
<li>results: 研究发现，使用提案的自主性评估框架可以帮助确定机器人系统的自主性水平，并且可以提供一个共同语言和规范界限 для自主系统开发者和使用者。两个案例研究，包括一个在路上验证的自主汽车和DARPA subT挑战规则分析，都显示了该框架的可行性和有用性。<details>
<summary>Abstract</summary>
Although autonomous functioning facilitates deployment of robotic systems in domains that admit limited human oversight on our planet and beyond, finding correspondence between task requirements and autonomous capability is still an open challenge. Consequently, a number of methods for quantifying autonomy have been proposed over the last three decades, but to our knowledge all these have no discernment of sub-mode features of variation of autonomy and some are based on metrics that violet the Goodhart's law. This paper focuses on the full autonomous mode and proposes a task-requirements based autonomy assessment framework. The framework starts by establishing robot task characteristics from which three autonomy metrics, namely requisite capability, reliability and responsiveness, and functions for determining autonomy as a two-part measure, namely of level of autonomy and degree of autonomy are derived. These characteristics are founded on the realization that robots ultimately replace human skilled workers, to find a mapping between human job and robot task characteristics. The distinction between level and degree of autonomy stemmed from the acknowledgment that autonomy is not just a question of existence, but also one of performance of requisite capability. When continuously monitored, the proposed metrics provide a means of monitoring the integrity of a system. The framework has been demonstrated on two case studies, namely autonomous vehicle at an on-road dynamic driving task and the DARPA subT challenge rules analysis. The framework provides not only a tool for quantifying autonomy, but also a regulatory interface and common language for autonomous systems developers and users.
</details>
<details>
<summary>摘要</summary>
尽管自主 fonctioning 可以在允许有限的人工监督下在我们 planet 和 beyond 中部署 robotic systems，但发现任务需求和自主能力之间的对应仍然是一个打开的挑战。因此，过去三十年来，一些方法用于量化自主性被提出，但据我们所知，这些方法都没有辨别自主模式中的特征变化，并且一些基于不符合Goodhart's law的度量。这篇文章关注全自主模式，并提出一种基于任务需求的自主性评估框架。该框架开始于确定机器人任务特征，并从这些特征 derive 三个自主度量：一是必需能力、可靠性和响应性，二是用于确定自主性的两部分度量：一是自主度量级别，二是自主度量的度量。这些特征基于人工智能将 eventually replace 人类高级工人的认识，以找到机器人任务特征和人工任务之间的映射。在监控下，提出的度量可以监测系统的完整性。框架在两个案例中进行了示例分析：一是在路面动态驾驶任务上的自动驾驶车辆，二是 DARPA subT 挑战规则分析。该框架不仅提供了量化自主性的工具，还提供了自主系统开发者和用户之间的标准化界面和通用语言。
</details></li>
</ul>
<hr>
<h2 id="Supermind-Ideator-Exploring-generative-AI-to-support-creative-problem-solving"><a href="#Supermind-Ideator-Exploring-generative-AI-to-support-creative-problem-solving" class="headerlink" title="Supermind Ideator: Exploring generative AI to support creative problem-solving"></a>Supermind Ideator: Exploring generative AI to support creative problem-solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01937">http://arxiv.org/abs/2311.01937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven R. Rick, Gianni Giacomelli, Haoran Wen, Robert J. Laubacher, Nancy Taubenslag, Jennifer L. Heyman, Max Sina Knicker, Younes Jeddi, Hendrik Maier, Stephen Dwyer, Pranav Ragupathy, Thomas W. Malone</li>
<li>for: This paper is written for people who want to use creative problem-solving techniques to generate innovative ideas for designing groups of people and&#x2F;or computers.</li>
<li>methods: The paper uses a large language model (GPT 3.5) and adds prompting, fine-tuning, and a user interface specifically designed to help people use creative problem-solving techniques.</li>
<li>results: The paper describes early experiences with using this system and suggests ways it could be extended to support additional techniques for other specific problem-solving domains.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为了帮助人们使用创新问题解决技术来生成创新的想法，特别是设计人员和计算机(“超级 minds”)的组合。</li>
<li>methods: 论文使用大语言模型（GPT 3.5）和添加提示、精度调整以及特制用户界面，以帮助人们使用创新问题解决技术。</li>
<li>results: 论文描述了使用这种系统的初期经验，并建议将其扩展到支持其他特定问题解决领域的技巧。<details>
<summary>Abstract</summary>
Previous efforts to support creative problem-solving have included (a) techniques (such as brainstorming and design thinking) to stimulate creative ideas, and (b) software tools to record and share these ideas. Now, generative AI technologies can suggest new ideas that might never have occurred to the users, and users can then select from these ideas or use them to stimulate even more ideas. Here, we describe such a system, Supermind Ideator. The system uses a large language model (GPT 3.5) and adds prompting, fine tuning, and a user interface specifically designed to help people use creative problem-solving techniques. Some of these techniques can be applied to any problem; others are specifically intended to help generate innovative ideas about how to design groups of people and/or computers ("superminds"). We also describe our early experiences with using this system and suggest ways it could be extended to support additional techniques for other specific problem-solving domains.
</details>
<details>
<summary>摘要</summary>
previous efforts to support creative problem-solving have included (a) techniques (such as brainstorming and design thinking) to stimulate creative ideas, and (b) software tools to record and share these ideas. now, generative AI technologies can suggest new ideas that might never have occurred to the users, and users can then select from these ideas or use them to stimulate even more ideas. here, we describe such a system, Supermind Ideator. the system uses a large language model (GPT 3.5) and adds prompting, fine tuning, and a user interface specifically designed to help people use creative problem-solving techniques. some of these techniques can be applied to any problem; others are specifically intended to help generate innovative ideas about how to design groups of people and/or computers ("superminds"). we also describe our early experiences with using this system and suggest ways it could be extended to support additional techniques for other specific problem-solving domains.Here's the translation in Traditional Chinese:previous efforts to support creative problem-solving have included (a) techniques (such as brainstorming and design thinking) to stimulate creative ideas, and (b) software tools to record and share these ideas. now, generative AI technologies can suggest new ideas that might never have occurred to the users, and users can then select from these ideas or use them to stimulate even more ideas. here, we describe such a system, Supermind Ideator. the system uses a large language model (GPT 3.5) and adds prompting, fine tuning, and a user interface specifically designed to help people use creative problem-solving techniques. some of these techniques can be applied to any problem; others are specifically intended to help generate innovative ideas about how to design groups of people and/or computers ("superminds"). we also describe our early experiences with using this system and suggest ways it could be extended to support additional techniques for other specific problem-solving domains.
</details></li>
</ul>
<hr>
<h2 id="GateLoop-Fully-Data-Controlled-Linear-Recurrence-for-Sequence-Modeling"><a href="#GateLoop-Fully-Data-Controlled-Linear-Recurrence-for-Sequence-Modeling" class="headerlink" title="GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling"></a>GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01927">http://arxiv.org/abs/2311.01927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Katsch</li>
<li>For:  This paper aims to improve the efficiency and effectiveness of sequence models, particularly for auto-regressive language modeling.* Methods: The authors develop a foundational sequence model called GateLoop, which generalizes linear recurrent models by employing data-controlled state transitions. The model comes with two efficient modes: $O(l)$ recurrent mode and $O(l \log_{2} l)$ parallel mode.* Results: The authors show that GateLoop outperforms existing models for auto-regressive language modeling, and the approach can be interpreted as providing data-controlled relative-positional information to Attention. The findings suggest that incorporating data-controlled complex cumulative products may be a crucial step towards more powerful sequence models.<details>
<summary>Abstract</summary>
Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on data-controlled cumulative sums for context aggregation, our findings suggest that incorporating data-controlled complex cumulative products may be a crucial step towards more powerful sequence models.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:线性回归已经证明是效果很好的长序列模型工具。在这种工作中，我们发现现有模型没有充分利用其潜力。我们因此开发了 GateLoop，一种基础序列模型，可以普适 linear recurrent models such as S4, S5, LRU和RetNet，通过使用数据控制状态转移。我们的方法在自然语言模型中实际比现有模型 луч。它具有低成本的 $O(l)$ 回归模式和高效的 $O(l \log_{2} l)$ 并行模式，使用了高优化的相关扫描实现。此外，我们得出了 $O(l^2)$ 代理注意力模式，这有着很重要的意义 для Transformer 和最近提出的体系。我们的方法可以提供数据控制的相对位置信息给注意力，这与许多现有模型仅通过数据控制累加来实现上下文汇集不同。我们的发现表明，将数据控制的复杂累加纳入Sequence模型可能是至关重要的一步。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Illuminate-a-Progressive-Pathway-to-Artificial-Healthcare-Assistant-A-Review"><a href="#Large-Language-Models-Illuminate-a-Progressive-Pathway-to-Artificial-Healthcare-Assistant-A-Review" class="headerlink" title="Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"></a>Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01918">http://arxiv.org/abs/2311.01918</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingze-yuan/awesome-llm-healthcare">https://github.com/mingze-yuan/awesome-llm-healthcare</a></li>
<li>paper_authors: Mingze Yuan, Peng Bao, Jiajia Yuan, Yunhao Shen, Zifan Chen, Yi Xie, Jie Zhao, Yang Chen, Li Zhang, Lin Shen, Bin Dong</li>
<li>For: This paper provides a comprehensive review of the applications and implications of large language models (LLMs) in medicine, with a focus on their potential to enhance various aspects of healthcare.* Methods: The paper examines the fundamental applications of general-purpose and specialized LLMs, as well as the emerging development of LLM-powered autonomous agents for healthcare. It also explores the ability of multimodal LLMs to process diverse data types like medical imaging and EHRs to augment diagnostic accuracy.* Results: The paper highlights the transformative potential of LLMs in modern medicine, but also acknowledges the need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.<details>
<summary>Abstract</summary>
With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This paper provides a comprehensive review on the applications and implications of LLMs in medicine. It begins by examining the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review then focuses on multimodal LLMs, investigating their ability to process diverse data types like medical imaging and EHRs to augment diagnostic accuracy. To address LLMs' limitations regarding personalization and complex clinical reasoning, the paper explores the emerging development of LLM-powered autonomous agents for healthcare. Furthermore, it summarizes the evaluation methodologies for assessing LLMs' reliability and safety in medical contexts. Overall, this review offers an extensive analysis on the transformative potential of LLMs in modern medicine. It also highlights the pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice. Visit https://github.com/mingze-yuan/Awesome-LLM-Healthcare for an accompanying GitHub repository containing latest papers.
</details>
<details>
<summary>摘要</summary>
受人工智能快速发展的推动，大型自然语言模型（LLM）在人类语言理解和推理方面已经表现出了承诺的能力。这一点引发了医疗领域应用LLM的兴趣，以提高医疗教育、临床决策等方面。然而，医疗涉及多种数据类型和复杂的理解技能，这些挑战 LLM 的应用。本文提供了医疗领域 LLM 应用和意涂抵触的全面评论。它首先检查了通用和专门的 LLM 在知识检索、研究支持、临床工作流程自动化和诊断协助方面的应用。认识到医疗的多样性，文章然后关注多模态 LLM，研究其能够处理医疗图像和 EHR 等多种数据类型，以增强诊断准确性。为了解决 LLM 的个性化和复杂临床理解限制，文章探讨了emerging 的 LLM 驱动的医疗自动化技术的发展。此外，文章还总结了评估 LLM 在医疗上的可靠性和安全性的评价方法。总之，本文提供了现代医疗中 LLM 的转变 potential，以及在应用这些模型之前，需要不断优化和优先考虑伦理监督的重要性。有关最新的论文，请参考 <https://github.com/mingze-yuan/Awesome-LLM-Healthcare> GitHub 存储夹。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Functional-Data-Analysis-with-Sequential-Neural-Networks-Advantages-and-Comparative-Study"><a href="#Enhancing-Functional-Data-Analysis-with-Sequential-Neural-Networks-Advantages-and-Comparative-Study" class="headerlink" title="Enhancing Functional Data Analysis with Sequential Neural Networks: Advantages and Comparative Study"></a>Enhancing Functional Data Analysis with Sequential Neural Networks: Advantages and Comparative Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01875">http://arxiv.org/abs/2311.01875</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Zhao, J. Li, M. Chen, S. Jadhav</li>
<li>for: 这篇论文旨在应用Sequential Neural Networks (SNNs) 来解决功能资料分析 (Functional Data Analysis, FDA) 领域中的问题。</li>
<li>methods: 这篇论文使用了SNNs 来实现功能资料的分析，并与传统 FDA 方法进行比较。</li>
<li>results: 研究发现，SNNs 可以在功能资料分析中提供更好的性能，并且可以轻松地实现。此外，SNNs 还可以处理高维ensionality 的功能资料，并且可以应对实际世界中的资料分析问题。<details>
<summary>Abstract</summary>
Functional Data Analysis (FDA) is a statistical domain developed to handle functional data characterized by high dimensionality and complex data structures. Sequential Neural Networks (SNNs) are specialized neural networks capable of processing sequence data, a fundamental aspect of functional data. Despite their great flexibility in modeling functional data, SNNs have been inadequately employed in the FDA community. One notable advantage of SNNs is the ease of implementation, making them accessible to a broad audience beyond academia. Conversely, FDA-based methodologies present challenges, particularly for practitioners outside the field, due to their intricate complexity. In light of this, we propose utilizing SNNs in FDA applications and demonstrate their effectiveness through comparative analyses against popular FDA regression models based on numerical experiments and real-world data analysis. SNN architectures allow us to surpass the limitations of traditional FDA methods, offering scalability, flexibility, and improved analytical performance. Our findings highlight the potential of SNN-based methodologies as powerful tools for data applications involving functional data.
</details>
<details>
<summary>摘要</summary>
SNNs 的一个优点是易于实现，使得它们可以被更广泛的应用者所使用，不仅限于学术界。然而， FDA 基础方法ologies 具有复杂的核心，对于非学术界的实践者而言，它们可能会具有困难性。因此，我们建议使用 SNNs 在 FDA 应用中，并通过比较性分析与实际数据分析，评估 SNNs 在 FDA 领域的表现。SNN 架构可以超过传统 FDA 方法的限制，提供可扩展性、柔软性和改进的分析性能。我们的发现显示 SNN-based 方法ologies 具有强大的应用潜力，可以帮助解决各种函数数据的应用问题。
</details></li>
</ul>
<hr>
<h2 id="Multi-EuP-The-Multilingual-European-Parliament-Dataset-for-Analysis-of-Bias-in-Information-Retrieval"><a href="#Multi-EuP-The-Multilingual-European-Parliament-Dataset-for-Analysis-of-Bias-in-Information-Retrieval" class="headerlink" title="Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval"></a>Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01870">http://arxiv.org/abs/2311.01870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinrui Yang, Timothy Baldwin, Trevor Cohn</li>
<li>for: 这个论文是为了研究多语言信息检索（IR）上的公平性而设计的多语言标准数据集。</li>
<li>methods: 这个数据集包含22,000多语言文档，收集自欧洲议会，涵盖24种语言。这个数据集具有 Authentic multilingual corpus，包括所有24种语言的话题翻译，以及cross-lingual relevance judgments。此外，数据集还包含文档关于文档的人口信息，使研究语言偏见更加容易。</li>
<li>results: 作者表明Multi-EuP数据集可以用于评估单语言和多语言IR的效果。作者还进行了一项初步实验，检查选择tokenization策略会导致的语言偏见。<details>
<summary>Abstract</summary>
We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K multi-lingual documents collected from the European Parliament, spanning 24 languages. This dataset is designed to investigate fairness in a multilingual information retrieval (IR) context to analyze both language and demographic bias in a ranking context. It boasts an authentic multilingual corpus, featuring topics translated into all 24 languages, as well as cross-lingual relevance judgments. Furthermore, it offers rich demographic information associated with its documents, facilitating the study of demographic bias. We report the effectiveness of Multi-EuP for benchmarking both monolingual and multilingual IR. We also conduct a preliminary experiment on language bias caused by the choice of tokenization strategy.
</details>
<details>
<summary>摘要</summary>
我们介绍 Multi-EuP，一个新的多语言标准数据集，包含22000多语言文档，收集自欧洲议会，涵盖24种语言。这个数据集旨在研究多语言搜索（IR）上的公平性，以分析语言和人口特征偏见在排序上。它拥有真实的多语言资料库，包括所有24种语言的话题翻译，以及跨语言相关评估。此外，它还提供了文档关于其文档的多元化信息，使得研究人员可以研究语言偏见。我们证明Multi-EuP可以用于评测单语言和多语言搜索的效果。我们还进行了一项预liminary实验，探讨因选择Tokenization策略而导致的语言偏见。
</details></li>
</ul>
<hr>
<h2 id="Towards-Concept-Aware-Large-Language-Models"><a href="#Towards-Concept-Aware-Large-Language-Models" class="headerlink" title="Towards Concept-Aware Large Language Models"></a>Towards Concept-Aware Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01866">http://arxiv.org/abs/2311.01866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Shani, Jilles Vreeken, Dafna Shahaf</li>
<li>for: 本研究旨在将人类概念转移到机器上，以提高机器的概念形成和推理能力。</li>
<li>methods: 本研究使用了现代大语言模型（LLMs），并分析了这些模型是否能够正确地捕捉人类概念的结构。 研究还讨论了将概念包含在不同阶段的数据pipeline中，以提高机器的概念形成和推理能力。</li>
<li>results: 研究发现，使用概念进行预训可以更好地匹配人类的概念理解，并且可以提高机器的预测Robustness。这些初步结果显示了概念意识型机器语言模型的推论力。<details>
<summary>Abstract</summary>
Concepts play a pivotal role in various human cognitive functions, including learning, reasoning and communication. However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts.   In this work, we analyze how well contemporary LLMs capture human concepts and their structure. We then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline. We sketch a method for pretraining LLMs using concepts, and also explore the simpler approach that uses the output of existing LLMs. Despite its simplicity, our proof-of-concept is shown to better match human intuition, as well as improve the robustness of predictions. These preliminary results underscore the promise of concept-aware LLMs.
</details>
<details>
<summary>摘要</summary>
文本中的概念扮演着重要的角色，包括学习、理解和交流。然而，目前的机器学习模型（LLM）却只能处理单词，而不是概念。在这篇文章中，我们分析了当代LLM是如何捕捉人类概念的。然后，我们讨论了如何开发概念意识的LLM，包括不同阶段的pipeline中的不同方法。我们采用了一种预训练方法，使用概念来训练LLM，并且也探讨了使用现有LLM的输出来实现的简单方法。尽管这种方法的简单，但我们的证明显示，它可以更好地匹配人类的直觉，并提高预测的稳定性。这些初步结果表明概念意识LLM具有承诺的承诺。
</details></li>
</ul>
<hr>
<h2 id="SortNet-Learning-To-Rank-By-a-Neural-Based-Sorting-Algorithm"><a href="#SortNet-Learning-To-Rank-By-a-Neural-Based-Sorting-Algorithm" class="headerlink" title="SortNet: Learning To Rank By a Neural-Based Sorting Algorithm"></a>SortNet: Learning To Rank By a Neural-Based Sorting Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01864">http://arxiv.org/abs/2311.01864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli</li>
<li>for: 本研究旨在提出一种适应性排序算法，能够根据用户需求进行个性化排序。</li>
<li>methods: 本研究使用了一种基于神经网络的比较器，通过在每个训练例之间进行多次迭代，使得模型能够学习出最有用的排序顺序。</li>
<li>results: 实验结果表明，SortNet算法在LETO dataset上表现出色，与其他当前领先算法相比，具有更高的排序精度和更好的排序稳定性。<details>
<summary>Abstract</summary>
The problem of relevance ranking consists of sorting a set of objects with respect to a given criterion. Since users may prefer different relevance criteria, the ranking algorithms should be adaptable to the user needs. Two main approaches exist in literature for the task of learning to rank: 1) a score function, learned by examples, which evaluates the properties of each object yielding an absolute relevance value that can be used to order the objects or 2) a pairwise approach, where a "preference function" is learned using pairs of objects to define which one has to be ranked first. In this paper, we present SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. The neural network training set provides examples of the desired ordering between pairs of items and it is constructed by an iterative procedure which, at each iteration, adds the most informative training examples. Moreover, the comparator adopts a connectionist architecture that is particularly suited for implementing a preference function. We also prove that such an architecture has the universal approximation property and can implement a wide class of functions. Finally, the proposed algorithm is evaluated on the LETOR dataset showing promising performances in comparison with other state of the art algorithms.
</details>
<details>
<summary>摘要</summary>
问题的排序 ranks a set of objects based on a given criterion. Since users may prefer different relevance criteria, the ranking algorithms should be adaptable to the user needs. 在文献中有两种主要的方法来实现学习排序：1）通过例子学习的分数函数，对每个对象评估其特性，并从中计算出一个绝对相关性值，以便将对象排序；2）通过对象对的 preference function 学习，用于定义哪一个对象应该排在前面。在这篇论文中，我们提出了 SortNet，一种可变排序算法，通过神经网络作为比较器来排序对象。神经网络训练集提供了欲要的对象之间的排序示例，并通过迭代过程，在每一迭代中添加最有用的训练示例。此外，比较器采用了 Connectionist 架构，特别适合实现 preference function。我们还证明了这种架构具有 universal approximation property，可以实现广泛的函数。最后，我们对 LETOR 数据集进行了评估，与其他当前状态的算法相比，SortNet 表现很出色。
</details></li>
</ul>
<hr>
<h2 id="FAME-Flexible-Scalable-Analogy-Mappings-Engine"><a href="#FAME-Flexible-Scalable-Analogy-Mappings-Engine" class="headerlink" title="FAME: Flexible, Scalable Analogy Mappings Engine"></a>FAME: Flexible, Scalable Analogy Mappings Engine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01860">http://arxiv.org/abs/2311.01860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahar Jacob, Chen Shani, Dafna Shahaf</li>
<li>for:  This paper aims to advance computational analogy by developing a new framework that can handle partial analogies and suggest new entities to be added.</li>
<li>methods: The paper uses automatic extraction of commonsense representations to identify mappings between entities, and the input requirements are relaxed to only require names of entities.</li>
<li>results: The model achieves 81.2% accuracy on classical 2x2 analogy problems and 77.8% accuracy on larger problems, outperforming human performance and providing interpretable results.<details>
<summary>Abstract</summary>
Analogy is one of the core capacities of human cognition; when faced with new situations, we often transfer prior experience from other domains. Most work on computational analogy relies heavily on complex, manually crafted input. In this work, we relax the input requirements, requiring only names of entities to be mapped. We automatically extract commonsense representations and use them to identify a mapping between the entities. Unlike previous works, our framework can handle partial analogies and suggest new entities to be added. Moreover, our method's output is easily interpretable, allowing for users to understand why a specific mapping was chosen.   Experiments show that our model correctly maps 81.2% of classical 2x2 analogy problems (guess level=50%). On larger problems, it achieves 77.8% accuracy (mean guess level=13.1%). In another experiment, we show our algorithm outperforms human performance, and the automatic suggestions of new entities resemble those suggested by humans. We hope this work will advance computational analogy by paving the way to more flexible, realistic input requirements, with broader applicability.
</details>
<details>
<summary>摘要</summary>
人类认知的核心能力之一是比喻，当面临新情况时，我们常将先前在其他领域的经验转移到新的情况中。大多数计算比喻工作都需要复杂、手动制作的输入。在这项工作中，我们宽松了输入要求，只需提供实体的名称，我们会自动提取常识表示并用其来确定实体之间的映射。与前一些工作不同，我们的框架可以处理 incomplete analogy 和提出新的实体。此外，我们的方法的输出易于解释，允许用户理解为何选择了特定的映射。实验表明，我们的模型在经典的 2x2 比喻问题中 correctly map 81.2% (guess level=50%)，在更大的问题上达到 77.8% 的准确率 (mean guess level=13.1%)。在另一个实验中，我们的算法超越了人类性能，并且自动提出的新实体建议与人类的建议相似。我们希望这项工作将计算比喻的发展带进更加灵活、现实主义的输入要求，并具有更广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="A-Neural-Radiance-Field-Based-Architecture-for-Intelligent-Multilayered-View-Synthesis"><a href="#A-Neural-Radiance-Field-Based-Architecture-for-Intelligent-Multilayered-View-Synthesis" class="headerlink" title="A Neural Radiance Field-Based Architecture for Intelligent Multilayered View Synthesis"></a>A Neural Radiance Field-Based Architecture for Intelligent Multilayered View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01842">http://arxiv.org/abs/2311.01842</a></li>
<li>repo_url: None</li>
<li>paper_authors: D. Dhinakaran, S. M. Udhaya Sankar, G. Elumalai, N. Jagadish kumar</li>
<li>For: The paper aims to improve on-demand source routing systems in mobile ad hoc networks by proposing a new routing strategy called Optimized Route Selection via Red Imported Fire Ants (RIFA) Strategy.* Methods: The proposed method uses predicting route failure and energy utilization to select the path during the routing phase. The authors evaluate the performance of the proposed strategy based on parameters such as energy usage, packet delivery rate (PDR), and end-to-end (E2E) delay.* Results: The results show that the proposed strategy is superior to traditional routing methods in terms of network lifetime, node energy consumption, and typical E2E delay under most network performance measures and factors.<details>
<summary>Abstract</summary>
A mobile ad hoc network is made up of a number of wireless portable nodes that spontaneously come together en route for establish a transitory network with no need for any central management. A mobile ad hoc network (MANET) is made up of a sizable and reasonably dense community of mobile nodes that travel across any terrain and rely solely on wireless interfaces for communication, not on any well before centralized management. Furthermore, routing be supposed to offer a method for instantly delivering data across a network between any two nodes. Finding the best packet routing from across infrastructure is the major issue, though. The proposed protocol's major goal is to identify the least-expensive nominal capacity acquisition that assures the transportation of realistic transport that ensures its durability in the event of any node failure. This study suggests the Optimized Route Selection via Red Imported Fire Ants (RIFA) Strategy as a way to improve on-demand source routing systems. Predicting Route Failure and energy Utilization is used to pick the path during the routing phase. Proposed work assess the results of the comparisons based on performance parameters like as energy usage, packet delivery rate (PDR), and end-to-end (E2E) delay. The outcome demonstrates that the proposed strategy is preferable and increases network lifetime while lowering node energy consumption and typical E2E delay under the majority of network performance measures and factors.
</details>
<details>
<summary>摘要</summary>
mobile ad hoc network 是由一些无线可携式节点组成，这些节点自动集结在路上，建立一个没有中央管理的临时网络，以确保数据在网络中快速传输。mobile ad hoc network (MAN) 是一个较大、相对密集的移动节点社区，这些节点在任何地形上行驶，仅通过无线接口进行通信，不需要任何先前中央化管理。此外，路由应该提供一种能够立即传输数据 между任何两个节点的方法。然而，找到最佳包路由是网络的主要问题。提议的协议的主要目标是确定最低成本的常规容量获得，以确保实际交通的持续性，并在任何节点故障时保证网络的稳定性。本研究提出了优化via Red Imported Fire Ants (RIFA) 策略，以提高启动源路由系统的性能。在路由阶段，预测路径失败和能量利用，以选择路径。提议的工作评估结果基于性能参数，如能量消耗、包传输率 (PDR) 和终端到终端延迟 (E2E)。结果表明，提议的策略更有利，可以提高网络寿命，降低节点能量消耗和常规 E2E 延迟，对于大多数网络性能指标和因素。
</details></li>
</ul>
<hr>
<h2 id="DiffDub-Person-generic-Visual-Dubbing-Using-Inpainting-Renderer-with-Diffusion-Auto-encoder"><a href="#DiffDub-Person-generic-Visual-Dubbing-Using-Inpainting-Renderer-with-Diffusion-Auto-encoder" class="headerlink" title="DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder"></a>DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01811">http://arxiv.org/abs/2311.01811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Liu, Chenpeng Du, Shuai Fan, Feilong Chen, Kai Yu</li>
<li>for: 这篇论文是为了解决生成高质量、人类通用的视频配音问题而写的。</li>
<li>methods: 该论文提出了一种扩散 Dubbing 技术，包括在插入式渲染器中使用掩码来分割可编辑区和不可编辑区，以及使用数据增强和补充眼动引导等灵活策略来解决问题。</li>
<li>results: 经过严格的实验证明，该技术可以在人类通用和多语言enario中提供高质量、自然的视频配音，并且在不具备对应的音频视频数据的情况下也能够达到良好的效果。<details>
<summary>Abstract</summary>
Generating high-quality and person-generic visual dubbing remains a challenge. Recent innovation has seen the advent of a two-stage paradigm, decoupling the rendering and lip synchronization process facilitated by intermediate representation as a conduit. Still, previous methodologies rely on rough landmarks or are confined to a single speaker, thus limiting their performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We first craft the Diffusion auto-encoder by an inpainting renderer incorporating a mask to delineate editable zones and unaltered regions. This allows for seamless filling of the lower-face region while preserving the remaining parts. Throughout our experiments, we encountered several challenges. Primarily, the semantic encoder lacks robustness, constricting its ability to capture high-level features. Besides, the modeling ignored facial positioning, causing mouth or nose jitters across frames. To tackle these issues, we employ versatile strategies, including data augmentation and supplementary eye guidance. Moreover, we encapsulated a conformer-based reference encoder and motion generator fortified by a cross-attention mechanism. This enables our model to learn person-specific textures with varying references and reduces reliance on paired audio-visual data. Our rigorous experiments comprehensively highlight that our ground-breaking approach outpaces existing methods with considerable margins and delivers seamless, intelligible videos in person-generic and multilingual scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过两个阶段 paradigm，我们提出了一种新的Diffusion-based dubbing方法，即DiffDub。首先，我们采用了一个Diffusion自适应Encoder，通过填充低部分的面部区域，保留原始部分的精度。在我们的实验中，我们遇到了一些挑战。主要是semantic encoder的稳定性不够，导致高级特征的捕捉受到限制。另外，模型忽略了面部位置，导致在帧内的嘴或鼻部震动。为了解决这些问题，我们采用了多种策略，包括数据增强和补充眼部指导。此外，我们还包装了一个基于参考编码器和运动生成器的cross-attention机制，使我们的模型能够学习人Specific的面部特征，并降低了对配对的音频视频数据的依赖。我们的严谨的实验表明，我们的创新方法在人Specific和多语言场景中比 existed方法有considerable的优势，并能够提供流畅、智能的视频。
</details></li>
</ul>
<hr>
<h2 id="AFPQ-Asymmetric-Floating-Point-Quantization-for-LLMs"><a href="#AFPQ-Asymmetric-Floating-Point-Quantization-for-LLMs" class="headerlink" title="AFPQ: Asymmetric Floating Point Quantization for LLMs"></a>AFPQ: Asymmetric Floating Point Quantization for LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01792">http://arxiv.org/abs/2311.01792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangsichengsjtu/afpq">https://github.com/zhangsichengsjtu/afpq</a></li>
<li>paper_authors: Yijia Zhang, Sicheng Zhang, Shijie Cao, Dayou Du, Jianyu Wei, Ting Cao, Ningyi Xu</li>
<li>for: 提高大型自然语言模型（LLM）的部署效率和可扩展性。</li>
<li>methods: 提出了偏好性浮点量化（AFPQ）方法，将正负值分别设置为不同的浮点幂。</li>
<li>results: 相比传统浮点量化方法，AFPQ方法可以大幅提高精度，并且可以轻松地与其他量化方法结合使用，无需额外存储空间。<details>
<summary>Abstract</summary>
Large language models (LLMs) show great performance in various tasks, but face deployment challenges from limited memory capacity and bandwidth. Low-bit weight quantization can save memory and accelerate inference. Although floating-point (FP) formats show good performance in LLM quantization, they tend to perform poorly with small group sizes or sub-4 bits. We find the reason is that the absence of asymmetry in previous FP quantization makes it unsuitable for handling asymmetric value distribution of LLM weight tensors. In this work, we propose asymmetric FP quantization (AFPQ), which sets separate scales for positive and negative values. Our method leads to large accuracy improvements and can be easily plugged into other quantization methods, including GPTQ and AWQ, for better performance. Besides, no additional storage is needed compared with asymmetric integer (INT) quantization. The code is available at https://github.com/zhangsichengsjtu/AFPQ.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在多种任务中表现出色，但面临部署挑战由限制内存容量和带宽所致。低位数量量化可以降低内存占用和加速推理。虽然浮点数（FP）格式在LLM量化中表现良好，但它们在小组size或下于4位时表现糟糕。我们发现这是因为前一代FP量化缺乏非对称性，导致无法处理语言模型权重张量的非对称值分布。在这种情况下，我们提出非对称浮点量化（AFPQ）方法，该方法在正负值中设置不同的缩放因素。我们的方法可以大幅提高准确性，并且可以轻松地与其他量化方法混合使用，包括GPTQ和AWQ，以提高性能。此外，与非对称整数（INT）量化相比，我们的方法不需要额外存储空间。代码可以在 GitHub 上找到：https://github.com/zhangsichengsjtu/AFPQ。
</details></li>
</ul>
<hr>
<h2 id="TCM-GPT-Efficient-Pre-training-of-Large-Language-Models-for-Domain-Adaptation-in-Traditional-Chinese-Medicine"><a href="#TCM-GPT-Efficient-Pre-training-of-Large-Language-Models-for-Domain-Adaptation-in-Traditional-Chinese-Medicine" class="headerlink" title="TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine"></a>TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01786">http://arxiv.org/abs/2311.01786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoxing Yang, Jianyu Shi, Zan Wang, Xiaohong Liu, Guangyu Wang<br>for: This paper aims to improve the performance of large language models in the field of Traditional Chinese Medicine (TCM) by proposing a novel domain-specific TCM domain adaptation approach.methods: The proposed TCM Domain Adaptation (TCMDA) approach uses a large TCM-specific corpus, TCM-Corpus-1B, to pre-train and fine-tune a pre-trained language model, TCM-GPT-7B, to improve its performance on TCM-related tasks. The TCMDA approach leverages the LoRA technique to efficiently train specific dense layers for pre-training and fine-tuning.results: The proposed TCMDA approach achieves the best performance on two TCM tasks, TCM examination and TCM diagnosis, outperforming other models by relative increments of 17% and 12% in accuracy, respectively. This study represents the pioneering validation of domain adaptation of a large language model with 7 billion parameters in the TCM domain.<details>
<summary>Abstract</summary>
Pre-training and fine-tuning have emerged as a promising paradigm across various natural language processing (NLP) tasks. The effectiveness of pretrained large language models (LLM) has witnessed further enhancement, holding potential for applications in the field of medicine, particularly in the context of Traditional Chinese Medicine (TCM). However, the application of these general models to specific domains often yields suboptimal results, primarily due to challenges like lack of domain knowledge, unique objectives, and computational efficiency. Furthermore, their effectiveness in specialized domains, such as Traditional Chinese Medicine, requires comprehensive evaluation. To address the above issues, we propose a novel domain specific TCMDA (TCM Domain Adaptation) approach, efficient pre-training with domain-specific corpus. Specifically, we first construct a large TCM-specific corpus, TCM-Corpus-1B, by identifying domain keywords and retreving from general corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained model's weights and uses rank decomposition matrices to efficiently train specific dense layers for pre-training and fine-tuning, efficiently aligning the model with TCM-related tasks, namely TCM-GPT-7B. We further conducted extensive experiments on two TCM tasks, including TCM examination and TCM diagnosis. TCM-GPT-7B archived the best performance across both datasets, outperforming other models by relative increments of 17% and 12% in accuracy, respectively. To the best of our knowledge, our study represents the pioneering validation of domain adaptation of a large language model with 7 billion parameters in TCM domain. We will release both TCMCorpus-1B and TCM-GPT-7B model once accepted to facilitate interdisciplinary development in TCM and NLP, serving as the foundation for further study.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>预训练和精度调整已成为自然语言处理（NLP）任务中有效的方法。大型语言模型（LLM）的效果在特定领域中得到进一步改进，可能用于医学领域，特别是中医（TCM）领域。然而，通用模型在特定领域应用 frequently leads to suboptimal results，主要因为lack of domain knowledge、unique objectives和计算效率等问题。此外，这些普通模型在特殊领域中的效果还需要进行全面评估。为了解决上述问题，我们提议一种新的域特定TCMDA（TCM域调整）方法，fficiently pre-train with domain-specific corpus。我们首先构建了大量TCM域特定 corpus，TCM-Corpus-1B，通过标识域名和检索general corpus中的内容。然后，我们的TCMDA利用LoRA，即冻结预训练模型的参数并使用排名分解矩阵来效率地训练特定的稠密层，以高效地将模型与TCM相关任务相匹配，即TCM-GPT-7B。我们进行了广泛的TCM任务的实验，包括TCM考试和TCM诊断。TCM-GPT-7B在两个TCM任务上得到了最高性能，与其他模型相比，TCM任务上的准确率提高了17%和12%。到目前为止，我们的研究是TCM域中大语言模型7亿参数的域调整的先驱 validate。我们将在接下来释出TCMCorpus-1B和TCM-GPT-7B模型，以便推动TCM和NLP领域的交叉发展，并作为TCM领域的基础研究。
</details></li>
</ul>
<hr>
<h2 id="Modeling-the-Uncertainty-with-Maximum-Discrepant-Students-for-Semi-supervised-2D-Pose-Estimation"><a href="#Modeling-the-Uncertainty-with-Maximum-Discrepant-Students-for-Semi-supervised-2D-Pose-Estimation" class="headerlink" title="Modeling the Uncertainty with Maximum Discrepant Students for Semi-supervised 2D Pose Estimation"></a>Modeling the Uncertainty with Maximum Discrepant Students for Semi-supervised 2D Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01770">http://arxiv.org/abs/2311.01770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Wu, Junbiao Pang, Qingming Huang</li>
<li>for: 提高 semi-supervised pose estimation 任务中的计算机视觉性能</li>
<li>methods: 使用 dual mean-teacher 框架，构建两个最大差异学生 (MDSs)，以及创造多种不确定性来评估 pseudo-labels 的质量</li>
<li>results: 实验结果显示，我们的方法可以提高 semi-supervised pose estimation 中的三个数据集的性能<details>
<summary>Abstract</summary>
Semi-supervised pose estimation is a practically challenging task for computer vision. Although numerous excellent semi-supervised classification methods have emerged, these methods typically use confidence to evaluate the quality of pseudo-labels, which is difficult to achieve in pose estimation tasks. For example, in pose estimation, confidence represents only the possibility that a position of the heatmap is a keypoint, not the quality of that prediction. In this paper, we propose a simple yet efficient framework to estimate the quality of pseudo-labels in semi-supervised pose estimation tasks from the perspective of modeling the uncertainty of the pseudo-labels. Concretely, under the dual mean-teacher framework, we construct the two maximum discrepant students (MDSs) to effectively push two teachers to generate different decision boundaries for the same sample. Moreover, we create multiple uncertainties to assess the quality of the pseudo-labels. Experimental results demonstrate that our method improves the performance of semi-supervised pose estimation on three datasets.
</details>
<details>
<summary>摘要</summary>
semi-supervised pose estimation是计算机视觉中的实际挑战。虽然有很多优秀的半指导类型方法出现了，但这些方法通常使用信任来评估pseudo-标签的质量，这在pose estimation任务中很难实现。例如，在pose estimation中，信任只表示一个热图中的位置是关键点的可能性，而不是这个预测的质量。在这篇论文中，我们提出了一个简单又高效的框架，用于在半指导 pose estimation任务中评估pseudo-标签的质量。具体来说，在 dual mean-teacher 框架下，我们构建了两个最大差分学生（MDSs），以便让两个教师生成不同的决策边界 для同一个样本。此外，我们还创造了多种不确定性，以评估pseudo-标签的质量。实验结果表明，我们的方法可以改善半指导 pose estimation在三个数据集上的性能。
</details></li>
</ul>
<hr>
<h2 id="Indo-LEGO-ABSA-A-Multitask-Generative-Aspect-Based-Sentiment-Analysis-for-Indonesian-Language"><a href="#Indo-LEGO-ABSA-A-Multitask-Generative-Aspect-Based-Sentiment-Analysis-for-Indonesian-Language" class="headerlink" title="Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language"></a>Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01757">http://arxiv.org/abs/2311.01757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Randy Zakya Suchrady, Ayu Purwarianti</li>
<li>For: This paper aims to implement a multitask learning and prompting approach for aspect-based sentiment analysis in Bahasa Indonesia using generative pre-trained language models.* Methods: The Indo LEGO-ABSA model is developed using the LEGO-ABSA framework, which employs the T5 model (specifically mT5) and trains all tasks within aspect-based sentiment analysis using multitask learning.* Results: The model achieved high accuracy on several tasks within aspect-based sentiment analysis, including Aspect Sentiment Triplet Extraction (f1-score of 79.55%), Unified Aspect-based Sentiment Analysis (86.09%), Aspect Opinion Pair Extraction (79.85%), Aspect Term Extraction (87.45%), and Opinion Term Extraction (88.09%).<details>
<summary>Abstract</summary>
Aspect-based sentiment analysis is a method in natural language processing aimed at identifying and understanding sentiments related to specific aspects of an entity. Aspects are words or phrases that represent an aspect or attribute of a particular entity. Previous research has utilized generative pre-trained language models to perform aspect-based sentiment analysis. LEGO-ABSA is one framework that has successfully employed generative pre-trained language models in aspect-based sentiment analysis, particularly in English. LEGO-ABSA uses a multitask learning and prompting approach to enhance model performance. However, the application of this approach has not been done in the context of Bahasa Indonesia. Therefore, this research aims to implement the multitask learning and prompting approach in aspect-based sentiment analysis for Bahasa Indonesia using generative pre-trained language models. In this study, the Indo LEGO-ABSA model is developed, which is an aspect-based sentiment analysis model utilizing generative pre-trained language models and trained with multitask learning and prompting. Indo LEGO-ABSA is trained with a hotel domain dataset in the Indonesian language. The obtained results include an f1-score of 79.55% for the Aspect Sentiment Triplet Extraction task, 86.09% for Unified Aspect-based Sentiment Analysis, 79.85% for Aspect Opinion Pair Extraction, 87.45% for Aspect Term Extraction, and 88.09% for Opinion Term Extraction. Indo LEGO-ABSA adopts the LEGO-ABSA framework that employs the T5 model, specifically mT5, by applying multitask learning to train all tasks within aspect-based sentiment analysis.
</details>
<details>
<summary>摘要</summary>
“对象基 sentiment分析”是自然语言处理中的一种方法，旨在识别和理解对特定实体的情感。实体上的“方面”是指实体的某个特征或属性。先前的研究已经使用生成预训语言模型进行对象基 sentiment分析。LEGO-ABSA是一个成功地使用生成预训语言模型进行对象基 sentiment分析的框架，特别是在英文中。LEGO-ABSA使用多任务学习和提示方法来提高模型性能。然而，这种方法尚未应用于印尼语。因此，本研究的目的是将多任务学习和提示方法应用于印尼语的对象基 sentiment分析。在这项研究中，我们开发了印尼 LEGO-ABSA 模型，这是一个使用生成预训语言模型和多任务学习的对象基 sentiment分析模型。印尼 LEGO-ABSA 在饭店领域的印尼语数据上进行训练， obtained 的结果包括对于对象情感三元组抽取任务的 f1 分数为 79.55%，对于统一对象基情感分析任务的分数为 86.09%，对于方面意见对立抽取任务的分数为 79.85%，对于方面词抽取任务的分数为 87.45%，和对于意见词抽取任务的分数为 88.09%。印尼 LEGO-ABSA 运用了 LEGO-ABSA 框架，具体地是使用 T5 模型，特别是 mT5，通过多任务学习训练所有对象基 sentiment分析任务。
</details></li>
</ul>
<hr>
<h2 id="RiskQ-Risk-sensitive-Multi-Agent-Reinforcement-Learning-Value-Factorization"><a href="#RiskQ-Risk-sensitive-Multi-Agent-Reinforcement-Learning-Value-Factorization" class="headerlink" title="RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization"></a>RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01753">http://arxiv.org/abs/2311.01753</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmu-rl-3dv/riskq">https://github.com/xmu-rl-3dv/riskq</a></li>
<li>paper_authors: Siqi Shen, Chennan Ma, Chao Li, Weiquan Liu, Yongquan Fu, Songzhu Mei, Xinwang Liu, Cheng Wang</li>
<li>for: 这个论文旨在解决多智能系统中的风险敏感多智能学习问题，即在不确定环境、不同代理策略和部分可见性下学习协调的政策。</li>
<li>methods: 作者引入了风险敏感个体全球最大原则（RIGM），该原则要求每个代理的风险敏感行动选择集等于中央政策的风险敏感行动选择。作者还提出了一种名为风险Q的方法，该方法可以模型多个代理的共同返回分布，并满足RIGM原则。</li>
<li>results: 作者通过广泛的实验表明，风险Q方法可以在多个环境下实现优秀的性能。代码可以在<a target="_blank" rel="noopener" href="https://github.com/xmu-rl-3dv/RiskQ%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xmu-rl-3dv/RiskQ中找到。</a><details>
<summary>Abstract</summary>
Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeling quantiles of it as weighted quantile mixtures of per-agent return distribution utilities. RiskQ satisfies the RIGM principle for the VaR and distorted risk metrics. We show that RiskQ can obtain promising performance through extensive experiments. The source code of RiskQ is available in https://github.com/xmu-rl-3dv/RiskQ.
</details>
<details>
<summary>摘要</summary>
Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. To address this limitation, we propose RiskQ, which models the joint return distribution by modeling quantiles of it as weighted quantile mixtures of per-agent return distribution utilities. RiskQ satisfies the RIGM principle for the VaR and distorted risk metrics.We show that RiskQ can obtain promising performance through extensive experiments. The source code of RiskQ is available at https://github.com/xmu-rl-3dv/RiskQ.Translation notes:* "Multi-agent systems" is translated as "多Agent系统" (duō agent xì tǒng)* "Environmental uncertainty" is translated as "环境不确定" (huán jìng bù jiè dìng)* "Varying policies of agents" is translated as "代理人政策的变化" (dì zhěng zhèng yì zhī yì)* "Partial observability" is translated as "部分可见性" (bù zhāng kě jian xìng)* "Risk-sensitive MARL" is translated as "风险敏感的多Agent学习" (fēng xìng mǐn gǎn de duō agent xué xí)* "RIGM principle" is translated as "风险敏感原则" (fēng xìng mǐn gǎn yuán xì)* "Value at Risk" is translated as "值得风险" (zhí dé fēng xìng)* "Distorted risk measurements" is translated as "扭曲的风险测量" (kuò xiào de fēng xìng gòu liàng)* "RiskQ" is translated as "风险Q" (fēng xìng Q)
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficiency-Optimization-for-Subterranean-LoRaWAN-Using-A-Reinforcement-Learning-Approach-A-Direct-to-Satellite-Scenario"><a href="#Energy-Efficiency-Optimization-for-Subterranean-LoRaWAN-Using-A-Reinforcement-Learning-Approach-A-Direct-to-Satellite-Scenario" class="headerlink" title="Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario"></a>Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01743">http://arxiv.org/abs/2311.01743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiqiang Lin, Muhammad Asad Ullah, Hirley Alves, Konstantin Mikhaylov, Tong Hao</li>
<li>for: 这篇论文旨在探讨如何在无 terrestrial 网络（NTN）中充分利用地下 LoRaWAN 网络，以实现负荷较大的农业和灾难救援操作中的经济和社会效益。</li>
<li>methods: 这篇论文使用了强调SF的 LoRa 模ulation，以优化数据传输率、无线通信时间、覆盖范围和能量消耗。但是，在大规模地下 LoRaWAN NTN 中，尚存在效率地分配SF 到终端设备的挑战。为此，这篇论文提出了一种基于强化学习（RL）的SF 分配策略，以优化系统的能源效率（EE）。</li>
<li>results: 对四个标准方法进行比较，RL 基于 SF 分配策略在极地下直接到卫星场景中表现出色，特别是 MAD3QN 在 MAA2C 方法之上具有更高的吞吐量和EE。<details>
<summary>Abstract</summary>
The integration of subterranean LoRaWAN and non-terrestrial networks (NTN) delivers substantial economic and societal benefits in remote agriculture and disaster rescue operations. The LoRa modulation leverages quasi-orthogonal spreading factors (SFs) to optimize data rates, airtime, coverage and energy consumption. However, it is still challenging to effectively assign SFs to end devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN. To address this, we investigate a reinforcement learning (RL)-based SFs allocation scheme to optimize the system's energy efficiency (EE). To efficiently capture the device-to-environment interactions in dense networks, we proposed an SFs allocation technique using the multi-agent dueling double deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C) algorithms based on an analytical reward mechanism. Our proposed RL-based SFs allocation approach evinces better performance compared to four benchmarks in the extreme underground direct-to-satellite scenario. Remarkably, MAD3QN shows promising potentials in surpassing MAA2C in terms of convergence rate and EE.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "The integration of subterranean LoRaWAN and non-terrestrial networks (NTN) delivers substantial economic and societal benefits in remote agriculture and disaster rescue operations. The LoRa modulation leverages quasi-orthogonal spreading factors (SFs) to optimize data rates, airtime, coverage and energy consumption. However, it is still challenging to effectively assign SFs to end devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN. To address this, we investigate a reinforcement learning (RL)-based SFs allocation scheme to optimize the system's energy efficiency (EE). To efficiently capture the device-to-environment interactions in dense networks, we proposed an SFs allocation technique using the multi-agent dueling double deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C) algorithms based on an analytical reward mechanism. Our proposed RL-based SFs allocation approach evinces better performance compared to four benchmarks in the extreme underground direct-to-satellite scenario. Remarkably, MAD3QN shows promising potentials in surpassing MAA2C in terms of convergence rate and EE." into Simplified Chinese.Here's the translation:<<SYS>>将地下LoRaWAN和非地球网络（NTN）集成可以实现各种经济和社会效益，如远程农业和灾难救援操作。LoRa模ulation使用 quasi-正交扩展因子（SF）来优化数据速率、广播时间、覆盖率和能量消耗。然而，在大规模地下LoRaWAN NTN中有效地分配SF仍然是一个挑战。为了解决这个问题，我们研究了一种基于强化学习（RL）的SF分配策略，以优化系统的能效性（EE）。为了有效地捕捉设备与环境之间的互动，我们提出了一种使用多代理对抗逻辑网络（MAD3QN）和多代理优势actor-critic（MAA2C）算法的SF分配技术，基于分析奖励机制。我们的提出的RL基于SF分配方法在极地下直接卫星enario下表现更好，特别是MAD3QN在MAA2C方面具有潜在的提升。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Error-Mitigation-of-Quantum-Processes-with-Data-Augmentation-Empowered-Neural-Model"><a href="#Flexible-Error-Mitigation-of-Quantum-Processes-with-Data-Augmentation-Empowered-Neural-Model" class="headerlink" title="Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model"></a>Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01727">http://arxiv.org/abs/2311.01727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EXPmaster/DAEM">https://github.com/EXPmaster/DAEM</a></li>
<li>paper_authors: Manwen Liao, Yan Zhu, Giulio Chiribella, Yuxiang Yang</li>
<li>for: 这个论文的目的是为了开发一种可以在实际应用中使用的量子计算中的错误纠正方法。</li>
<li>methods: 这个论文使用了数据扩充 empowered 神经网络模型来实现错误纠正。这种模型不需要任何具体的噪声类型和测量设置的知识，可以直接从受到噪声影响的量子过程的受测结果中估算噪声自由统计数据。</li>
<li>results: 在数值实验中，这种模型能够高效地纠正多种类型的噪声，包括Markovian 噪声和非Markovian 噪声，比前一代的错误纠正方法更高效。此外，这种模型还可以应用于多种不同的量子过程中，包括大规模量子系统和连续变量量子态。这种数据扩充 empowered 神经网络模型为实现更可靠和可robust的量子技术提供了一个坚实的基础。<details>
<summary>Abstract</summary>
Neural networks have shown their effectiveness in various tasks in the realm of quantum computing. However, their application in quantum error mitigation, a crucial step towards realizing practical quantum advancements, has been restricted by reliance on noise-free statistics. To tackle this critical challenge, we propose a data augmentation empowered neural model for error mitigation (DAEM). Our model does not require any prior knowledge about the specific noise type and measurement settings and can estimate noise-free statistics solely from the noisy measurement results of the target quantum process, rendering it highly suitable for practical implementation. In numerical experiments, we show the model's superior performance in mitigating various types of noise, including Markovian noise and Non-Markovian noise, compared with previous error mitigation methods. We further demonstrate its versatility by employing the model to mitigate errors in diverse types of quantum processes, including those involving large-scale quantum systems and continuous-variable quantum states. This powerful data augmentation-empowered neural model for error mitigation establishes a solid foundation for realizing more reliable and robust quantum technologies in practical applications.
</details>
<details>
<summary>摘要</summary>
neural networks 在量子计算领域中的各种任务中表现出了效果。然而，它们在量子错误修正中，实现实用量子进步的关键步骤，受到了噪声自由统计的限制。为解决这个挑战，我们提议一种基于数据增强的神经网络模型 для错误修正（DAEM）。我们的模型不需要任何噪声类型和测量设置的先前知识，可以通过噪声损失结果来估算噪声自由统计，这使得它在实际应用中非常适用。在数学实验中，我们展示了模型在不同类型的噪声下的优秀性能，比如Markovian噪声和非Markovian噪声，并且在不同类型的量子过程中进行错误修正，包括大规模量子系统和连续变量量子态。这种数据增强 empowered 神经网络模型 для错误修正，为实现更可靠和robust的量子技术在实际应用中提供了一个坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Towards-Calibrated-Robust-Fine-Tuning-of-Vision-Language-Models"><a href="#Towards-Calibrated-Robust-Fine-Tuning-of-Vision-Language-Models" class="headerlink" title="Towards Calibrated Robust Fine-Tuning of Vision-Language Models"></a>Towards Calibrated Robust Fine-Tuning of Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01723">http://arxiv.org/abs/2311.01723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changdae Oh, Mijoo Kim, Hyesu Lim, Junhyeok Park, Euiseog Jeong, Zhi-Qi Cheng, Kyungwoo Song</li>
<li>for: This paper focuses on the problem of calibration and robustness in fine-tuning pre-trained vision-language models (VLMs) under distribution shift.</li>
<li>methods: The authors propose a simple approach called calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness of pre-trained VLMs on both in-distribution (ID) and out-of-distribution (OOD) datasets.</li>
<li>results: The authors show that their proposed method, CaRot, effectively improves the calibration and robustness of pre-trained VLMs on OOD datasets, as verified by empirical results on ImageNet-1K distribution shift evaluation.Here’s the Chinese version of the three key points:</li>
<li>for: 这篇论文关注在微调投入预训练语义视觉模型（VLMs）下的分布Shift问题。</li>
<li>methods: 作者提出了一种简单的方法，即强制投入Calibration和Robustness的CaRot方法，以适应ID和OOD datasets上的预训练VLMs。</li>
<li>results: 作者证明了CaRot方法能有效地提高预训练VLMs在OOD datasets上的Calibration和Robustness，经验结果通过ImageNet-1K分布Shift评估 verify。<details>
<summary>Abstract</summary>
While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on both ID and OOD datasets. Empirical results on ImageNet-1K distribution shift evaluation verify the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
While 微调 fine-tuning 可以发挥预训练模型对特定任务的潜力，它同时 sacrifice 模型对非常用数据集（OOD）的泛化能力。为了解决这个问题，我们提出了一种名为 robust fine-tuning 的方法，该方法具有在 ID 数据集和 OOD 数据集上的性能和泛化能力。然而，另一个重要的机器学习（ML）需求，即信息报告（confidence calibration），在实际高风险 ML 应用中受到了忽视。我们在这篇文章中首次提出了预训练 VLM 的准确性报告在分布shift下的问题，并证明了微调和当今最佳的 robust fine-tuning 方法在 OOD 数据集上会削弱预训练 VLM 的准确性。为解决这个问题，我们提出了一种简单的方法，即准确性报告和Robust fine-tuning（CaRot），该方法激励预训练 VLM 在 ID 和 OOD 数据集上具备准确性和泛化能力。实验结果表明，CaRot 可以有效地改善预训练 VLM 在 ImageNet-1K 分布shift 评估中的性能。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Benchmarking-Chinese-Aspect-Sentiment-Quad-Prediction"><a href="#An-Empirical-Study-of-Benchmarking-Chinese-Aspect-Sentiment-Quad-Prediction" class="headerlink" title="An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction"></a>An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01713">http://arxiv.org/abs/2311.01713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junxian Zhou, Haiqin Yang, Ye Junpeng, Yuxuan He, Hao Mou</li>
<li>for:  expanding the capacity of aspect-level sentiment analysis</li>
<li>methods:  constructing two large Chinese ASQP datasets and evaluating the performance of GPT series models</li>
<li>results:  highlighting the need for additional techniques to address ASQP and the potential issues with using GPTsHere’s the simplified Chinese text:</li>
<li>for: 扩大方面 sentiment 分析的容量</li>
<li>methods:  constructing two large Chinese ASQP datasets 和 evaluating GPT 系列模型的表现</li>
<li>results:  highlighting the need for additional techniques to address ASQP 和 GPT 的可能问题<details>
<summary>Abstract</summary>
Aspect sentiment quad prediction (ASQP) is a critical subtask of aspect-level sentiment analysis. Current ASQP datasets are characterized by their small size and low quadruple density, which hinders technical development. To expand capacity, we construct two large Chinese ASQP datasets crawled from multiple online platforms. The datasets hold several significant characteristics: larger size (each with 10,000+ samples) and rich aspect categories, more words per sentence, and higher density than existing ASQP datasets. Moreover, we are the first to evaluate the performance of Generative Pre-trained Transformer (GPT) series models on ASQP and exhibit potential issues. The experiments with state-of-the-art ASQP baselines underscore the need to explore additional techniques to address ASQP, as well as the importance of further investigation into methods to improve the performance of GPTs.
</details>
<details>
<summary>摘要</summary>
“对象层情感预测（ASQP）是对应情感分析的重要子任务。现有的ASQP数据集的特点是小型和低四元密度，这限制了技术的发展。为了扩大能力，我们建立了两个大型的中文ASQP数据集，从多个在线平台网站爬虫取得。这两个数据集具有以下特点：更大的大小（每个数据集都有10,000+个样本）、丰富的层别分类、更多的单词数和更高的密度，较现有的ASQP数据集高。此外，我们是首次将生成预训Transformer（GPT）系列模型应用于ASQP，并发现了一些潜在的问题。实验中使用现有的ASQP基线，显示了需要进一步探索ASQP的技术，以及GPT的性能提升的重要性。”
</details></li>
</ul>
<hr>
<h2 id="Data-Free-Distillation-of-Language-Model-by-Text-to-Text-Transfer"><a href="#Data-Free-Distillation-of-Language-Model-by-Text-to-Text-Transfer" class="headerlink" title="Data-Free Distillation of Language Model by Text-to-Text Transfer"></a>Data-Free Distillation of Language Model by Text-to-Text Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01689">http://arxiv.org/abs/2311.01689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Bai, Xinduo Liu, Hailin Hu, Tianyu Guo, Qinghua Zhang, Yunhe Wang</li>
<li>for: 本研究的目的是提出一种基于生成语言模型的数据自由知识填充（DFKD）框架，以提高模型压缩性和特定性。</li>
<li>methods: 我们提出了一种名为DFKD-T$^{3}$的新框架，其中使用预训练的生成语言模型作为可控的数据生成器，以实现文本到文本的结构学习。</li>
<li>results: 我们的方法可以在不同的下游任务中提高填充性和多样性，并且可以直接用于填充其他语言模型，超越当前SOTA方法。<details>
<summary>Abstract</summary>
Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the model when original training data is unavailable. Previous works for DFKD in NLP mainly focus on distilling encoder-only structures like BERT on classification tasks, which overlook the notable progress of generative language modeling. In this work, we propose a novel DFKD framework, namely DFKD-T$^{3}$, where the pretrained generative language model can also serve as a controllable data generator for model compression. This novel framework DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to transform the general domain corpus to compression-friendly task data, targeting to improve both the \textit{specificity} and \textit{diversity}. Extensive experiments show that our method can boost the distillation performance in various downstream tasks such as sentiment analysis, linguistic acceptability, and information extraction. Furthermore, we show that the generated texts can be directly used for distilling other language models and outperform the SOTA methods, making our method more appealing in a general DFKD setting. Our code is available at https://gitee.com/mindspore/models/tree/master/research/nlp/DFKD\_T3.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本转换为简化中文。<</SYS>>论文摘要：数据无法获取的知识填充（DFKD）在模型压缩中扮演着重要的角色。先前的DFKD研究主要集中在NLP领域的分类任务上，忽略了生成语言模型的进步。在这项工作中，我们提出了一种新的DFKD框架，即DFKD-T$^{3}$，其中预训练的生成语言模型也可以作为数据生成器来压缩模型。这种新的框架DFKD-T$^{3}$实现了一个综合的文本到文本框架，可以将通用领域文库转换为适合压缩的任务数据，以提高特点和多样性。我们的方法可以在多个下游任务中提高填充性，包括情感分类、语言可接受性和信息抽取。此外，我们还证明了生成的文本可以直接用于填充其他语言模型，并且超越当前最佳方法，使我们的方法在通用DFKD Setting中更加吸引人。我们的代码可以在https://gitee.com/mindspore/models/tree/master/research/nlp/DFKD\_T3中找到。
</details></li>
</ul>
<hr>
<h2 id="The-R-O-A-D-to-precision-medicine"><a href="#The-R-O-A-D-to-precision-medicine" class="headerlink" title="The R.O.A.D. to precision medicine"></a>The R.O.A.D. to precision medicine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01681">http://arxiv.org/abs/2311.01681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Bertsimas, Angelos G. Koulouras, Georgios Antonios Margonis</li>
<li>for: 这个研究旨在 addresses the deficiencies of Randomized trial data subgroup analysis, and transforms ObservAtional Data to be used as if they were randomized, thus paving the road for precision medicine.</li>
<li>methods: 方法包括一个 novel two-step process to correct the estimated probabilities of the outcome under a treatment, and then use these probabilities to train Optimal Policy Trees (OPTs) to assign treatments to subgroups of patients based on their characteristics.</li>
<li>results: 研究结果显示，这些推荐高于专家的推荐在Gastrointestinal stromal tumors (GIST) 和 extremity sarcomas 中。 In addition, the framework identified a subset of patients with unique characteristics who may not require treatment, which was validated in an external cohort.<details>
<summary>Abstract</summary>
We propose a prognostic stratum matching framework that addresses the deficiencies of Randomized trial data subgroup analysis and transforms ObservAtional Data to be used as if they were randomized, thus paving the road for precision medicine. Our approach counters the effects of unobserved confounding in observational data by correcting the estimated probabilities of the outcome under a treatment through a novel two-step process. These probabilities are then used to train Optimal Policy Trees (OPTs), which are decision trees that optimally assign treatments to subgroups of patients based on their characteristics. This facilitates the creation of clinically intuitive treatment recommendations. We applied our framework to observational data of patients with gastrointestinal stromal tumors (GIST) and validated the OPTs in an external cohort using the sensitivity and specificity metrics. We show that these recommendations outperformed those of experts in GIST. We further applied the same framework to randomized clinical trial (RCT) data of patients with extremity sarcomas. Remarkably, despite the initial trial results suggesting that all patients should receive treatment, our framework, after addressing imbalances in patient distribution due to the trial's small sample size, identified through the OPTs a subset of patients with unique characteristics who may not require treatment. Again, we successfully validated our recommendations in an external cohort.
</details>
<details>
<summary>摘要</summary>
我们提出了一个预测层匹配框架，用于解决随机化试验数据 subgroup 分析中的缺陷，并将观察数据转化为可以被用于随机化的数据，从而开阔了精准医学的道路。我们的方法可以在观察数据中减轻不观察到的潜在干扰因素的影响，通过一种新的两步过程来修正对待治疗的估计概率。这些估计概率然后用于训练优化策略树（OPTs），这些树是根据病人特征来优化治疗分配的决策树。这使得可以创造出临床直观的治疗建议。我们在肠癌细胞肿（GIST）观察数据中应用了我们的框架，并在外部群体中验证了OPTs的性能。我们显示，我们的建议超过了GIST专家的建议。此外，我们还应用了同一个框架到手术试验数据中，并在试验结果显示所有患者应该接受治疗的情况下，我们的框架经过对患者分布不均衡的修正，通过OPTsidentified一 subgroup of patients with unique characteristics who may not require treatment。我们在外部群体中验证了我们的建议。
</details></li>
</ul>
<hr>
<h2 id="DialogBench-Evaluating-LLMs-as-Human-like-Dialogue-Systems"><a href="#DialogBench-Evaluating-LLMs-as-Human-like-Dialogue-Systems" class="headerlink" title="DialogBench: Evaluating LLMs as Human-like Dialogue Systems"></a>DialogBench: Evaluating LLMs as Human-like Dialogue Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01677">http://arxiv.org/abs/2311.01677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiao Ou, Junda Lu, Che Liu, Yihong Tang, Fuzheng Zhang, Di Zhang, Zhongyuan Wang, Kun Gai</li>
<li>for: 评估大型自然语言模型（LLMs）的对话系统人类化能力</li>
<li>methods: 提出了一个对话评估Benchmark，名为DialogBench，用于评估LLMs的对话系统人类化能力</li>
<li>results: 实验结果表明，对LLMs进行指导微调可以提高它们的人类化能力，但是还有很多方面需要进一步改进，以达到人类化对话系统的标准。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities, refreshing human's impressions on dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users by satisfying the need for communication, affection and social belonging. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that currently contains $12$ dialogue tasks to assess the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely-used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive test over $28$ LLMs (including pre-trained and supervised instruction-tuning) shows that instruction fine-tuning benefits improve the human likeness of LLMs to a certain extent, but there is still much room to improve those capabilities for most LLMs as human-like dialogue systems. In addition, experimental results also indicate that LLMs perform differently in various abilities that human-like dialogue systems should have. We will publicly release DialogBench, along with the associated evaluation code for the broader research community.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经取得了杰出的突破，刷新人类对对话系统的印象。对话系统的长期目标是成为与用户建立长期连接的人工智能系统，满足用户的交流、感情和社交属性需求。因此，有一个急需评估 LLM 的人工智能对话系统的能力。在本文中，我们提出了 DialogBench，一个对话评估标准，包括 $12$ 个对话任务，以评估 LLM 是否具备人工智能对话系统的能力。具体来说，我们将 GPT-4 提供评估实例。我们首先根据广泛使用的设计原则设计基本提示，然后进一步降低现有的偏见，以生成更高品质的评估实例。我们对 $28$ 个 LLM (包括预训和实际指导 fine-tuning) 进行了广泛的测试，发现实际指导 fine-tuning 可以提高 LLM 的人工智能程度，但是大多数 LLM 仍然有很大的执行空间来提升其能力。此外，实验结果还显示了不同的 LLM 在不同的能力上表现不同。我们将在未来发布 DialogBench，以及相关的评估代码，供更广泛的研究社区使用。
</details></li>
</ul>
<hr>
<h2 id="MineSegSAT-An-automated-system-to-evaluate-mining-disturbed-area-extents-from-Sentinel-2-imagery"><a href="#MineSegSAT-An-automated-system-to-evaluate-mining-disturbed-area-extents-from-Sentinel-2-imagery" class="headerlink" title="MineSegSAT: An automated system to evaluate mining disturbed area extents from Sentinel-2 imagery"></a>MineSegSAT: An automated system to evaluate mining disturbed area extents from Sentinel-2 imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01676">http://arxiv.org/abs/2311.01676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ezra MacDonald, Derek Jacoby, Yvonne Coady</li>
<li>for: 这个论文的目的是为了评估采矿业对环境的影响，以便更好地理解和mitigate采矿活动的生态后果。</li>
<li>methods: 这个论文使用了SegFormer深度学习分割框架，将Sentinel-2数据集中的环境影响区域分类为不同的土地覆盖类别。</li>
<li>results: 论文通过使用不同的损失函数（Dice、Tversky和Lovasz损失函数）进行训练，并在测试区域上进行推断，以确定采矿活动对环境的影响。<details>
<summary>Abstract</summary>
Assessing the environmental impact of the mineral extraction industry plays a critical role in understanding and mitigating the ecological consequences of extractive activities. This paper presents MineSegSAT, a model that presents a novel approach to predicting environmentally impacted areas of mineral extraction sites using the SegFormer deep learning segmentation architecture trained on Sentinel-2 data. The data was collected from non-overlapping regions over Western Canada in 2021 containing areas of land that have been environmentally impacted by mining activities that were identified from high-resolution satellite imagery in 2021. The SegFormer architecture, a state-of-the-art semantic segmentation framework, is employed to leverage its advanced spatial understanding capabilities for accurate land cover classification. We investigate the efficacy of loss functions including Dice, Tversky, and Lovasz loss respectively. The trained model was utilized for inference over the test region in the ensuing year to identify potential areas of expansion or contraction over these same periods. The Sentinel-2 data is made available on Amazon Web Services through a collaboration with Earth Daily Analytics which provides corrected and tiled analytics-ready data on the AWS platform. The model and ongoing API to access the data on AWS allow the creation of an automated tool to monitor the extent of disturbed areas surrounding known mining sites to ensure compliance with their environmental impact goals.
</details>
<details>
<summary>摘要</summary>
<<SYS>>翻译文本到简化中文。<<</SYS>> evaluating the environmental impact of the mineral extraction industry is crucial in understanding and mitigating the ecological consequences of extractive activities. This paper presents MineSegSAT, a model that employs the SegFormer deep learning segmentation architecture to predict environmentally impacted areas of mineral extraction sites using Sentinel-2 data. The data was collected from non-overlapping regions over Western Canada in 2021, containing areas of land that have been environmentally impacted by mining activities identified from high-resolution satellite imagery in 2021. The SegFormer architecture, a state-of-the-art semantic segmentation framework, is leveraged to accurately classify land cover. We investigate the efficacy of loss functions including Dice, Tversky, and Lovasz loss, respectively. The trained model was utilized for inference over the test region in the ensuing year to identify potential areas of expansion or contraction over the same periods. The Sentinel-2 data is made available on Amazon Web Services through a collaboration with Earth Daily Analytics, which provides corrected and tiled analytics-ready data on the AWS platform. The model and ongoing API to access the data on AWS allow the creation of an automated tool to monitor the extent of disturbed areas surrounding known mining sites to ensure compliance with their environmental impact goals.我们使用SegFormer深度学习分割框架来预测采矿活动对环境的影响，并使用Sentinel-2数据集来训练模型。这个数据集包含2021年在加拿大西部的非重叠区域中发生了采矿活动所导致的环境影响。我们使用Dice、Tversky和Lovasz损失函数进行调参。训练完成后，我们使用该模型来对测试区域进行预测，以确定可能的扩张或缩减区域。Sentinel-2数据通过我们与Earth Daily Analytics的合作在AWS平台上提供了已经 corrected和分割的分析Ready数据。我们的模型和ongoing API可以在AWS平台上访问数据，并创建一个自动化工具来监测采矿活动周围的受损区域，以确保遵循环境影响目标。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-driven-Community-Resilience-Rating-based-on-Intertwined-Socio-Technical-Systems-Features"><a href="#Deep-Learning-driven-Community-Resilience-Rating-based-on-Intertwined-Socio-Technical-Systems-Features" class="headerlink" title="Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features"></a>Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01661">http://arxiv.org/abs/2311.01661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yin, Ali Mostafavi</li>
<li>for: 这个论文主要是为了提高社区抗逆能力的评估和评价。</li>
<li>methods: 这个论文使用了一种基于深度学习的三层模型，即Resili-Net，来评估社区的抗逆能力水平。</li>
<li>results: 根据美国多个都会区的公共可 accessible 数据，Resili-Net 模型可以对社区的抗逆能力进行评估，并将其分为五个不同的水平。此外，模型还可以对社区抗逆能力的变化进行分析，以便为特定的抗逆能力提高措施提供指导。<details>
<summary>Abstract</summary>
Community resilience is a complex and muti-faceted phenomenon that emerges from complex and nonlinear interactions among different socio-technical systems and their resilience properties. However, present studies on community resilience focus primarily on vulnerability assessment and utilize index-based approaches, with limited ability to capture heterogeneous features within community socio-technical systems and their nonlinear interactions in shaping robustness, redundancy, and resourcefulness components of resilience. To address this gap, this paper presents an integrated three-layer deep learning model for community resilience rating (called Resili-Net). Twelve measurable resilience features are specified and computed within community socio-technical systems (i.e., facilities, infrastructures, and society) related to three resilience components of robustness, redundancy, and resourcefulness. Using publicly accessible data from multiple metropolitan statistical areas in the United States, Resili-Net characterizes the resilience levels of spatial areas into five distinct levels. The interpretability of the model outcomes enables feature analysis for specifying the determinants of resilience in areas within each resilience level, allowing for the identification of specific resilience enhancement strategies. Changes in community resilience profiles under urban development patterns are further examined by changing the value of related socio-technical systems features. Accordingly, the outcomes provide novel perspectives for community resilience assessment by harnessing machine intelligence and heterogeneous urban big data.
</details>
<details>
<summary>摘要</summary>
社区抗险能力是一种复杂多方面的现象，由社区技术系统之间的复杂和非线性互动而生成。然而，当前社区抗险研究主要关注的是漏斗分析，使用指标方法，具有限定能力捕捉社区技术系统中异质特性和非线性互动的复杂特征。为了填补这一漏洞，这篇文章提出了一种基于深度学习的社区抗险评价模型（名为Resili-Net）。这种模型确定了社区技术系统中12个可测量的抗险特征，并将其分配到三个抗险组成部分：坚韧性、备用性和资源fulness。使用美国多个都会区的公共可达数据，Resili-Net将社区抗险水平分为五个不同水平。模型结果的可读性允许特征分析，以确定每个水平中的抗险特征决定因素，并提供了特定抗险提高策略的标识。此外，通过改变相关社区技术系统特征的值，模型还研究了社区抗险资料的变化。因此，模型的结果为社区抗险评价提供了新的视角，并利用机器智能和多种城市大数据来捕捉社区抗险能力。
</details></li>
</ul>
<hr>
<h2 id="MARRS-Multimodal-Reference-Resolution-System"><a href="#MARRS-Multimodal-Reference-Resolution-System" class="headerlink" title="MARRS: Multimodal Reference Resolution System"></a>MARRS: Multimodal Reference Resolution System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01650">http://arxiv.org/abs/2311.01650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halim Cagri Ates, Shruti Bhargava, Site Li, Jiarui Lu, Siddhardha Maddula, Joel Ruben Antony Moniz, Anil Kumar Nalamalapu, Roman Hoang Nguyen, Melis Ozyildirim, Alkesh Patel, Dhivya Piraviperumal, Vincent Renkens, Ankit Samal, Thy Tran, Bo-Hsiang Tseng, Hong Yu, Yuan Zhang, Rong Zou</li>
<li>for: 这篇论文旨在描述一种在自然语言理解系统中处理上下文的框架，即多模态参考解决系统（MARRS）。</li>
<li>methods: 这篇论文使用了不同的机器学习模型来处理上下文，包括参考解决模型和上下文 rewrite 模型。</li>
<li>results: 这篇论文介绍了 MARRS 如何处理多种上下文，包括对话上下文、视觉上下文和背景上下文，同时保护用户隐私。<details>
<summary>Abstract</summary>
Successfully handling context is essential for any dialog understanding task. This context maybe be conversational (relying on previous user queries or system responses), visual (relying on what the user sees, for example, on their screen), or background (based on signals such as a ringing alarm or playing music). In this work, we present an overview of MARRS, or Multimodal Reference Resolution System, an on-device framework within a Natural Language Understanding system, responsible for handling conversational, visual and background context. In particular, we present different machine learning models to enable handing contextual queries; specifically, one to enable reference resolution, and one to handle context via query rewriting. We also describe how these models complement each other to form a unified, coherent, lightweight system that can understand context while preserving user privacy.
</details>
<details>
<summary>摘要</summary>
成功处理上下文是任务理解任务的关键。这种上下文可能是对话（基于前一个用户的查询或系统的回答）、视觉（基于用户看到的内容，例如屏幕）或背景（基于信号such as 铃声或播放音乐）。在这种工作中，我们介绍了 MARRS，即多modal引用解决系统，是一个在自然语言理解系统中的设备框架，负责处理对话、视觉和背景上下文。特别是，我们介绍了不同的机器学习模型，以便处理上下文 queries; specifically, one to enable reference resolution, and one to handle context via query rewriting。我们还描述了这些模型如何衔接起来，形成一个协调、轻量级的系统，能够理解上下文，而保持用户隐私。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/cs.AI_2023_11_03/" data-id="clpxp03va006ofm886bxi35q7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/cs.CL_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T11:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/03/cs.CL_2023_11_03/">cs.CL - 2023-11-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Grounded-Intuition-of-GPT-Vision’s-Abilities-with-Scientific-Images"><a href="#Grounded-Intuition-of-GPT-Vision’s-Abilities-with-Scientific-Images" class="headerlink" title="Grounded Intuition of GPT-Vision’s Abilities with Scientific Images"></a>Grounded Intuition of GPT-Vision’s Abilities with Scientific Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02069">http://arxiv.org/abs/2311.02069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahwang16/grounded-intuition-gpt-vision">https://github.com/ahwang16/grounded-intuition-gpt-vision</a></li>
<li>paper_authors: Alyssa Hwang, Andrew Head, Chris Callison-Burch</li>
<li>for: 本研究旨在帮助研究者更好地理解新型模型GPT-Vision的能力和局限性。</li>
<li>methods: 本研究使用了grounded theory和主题分析，从社会科学和人机交互的角度来设置一个严格的质量评估框架，以便对自然语言处理领域的新模型进行评估。</li>
<li>results: 研究发现，GPT-Vision具有特殊的激励特性，它响应于提示、图像中的对话文本和相对空间关系。这种方法和分析可以帮助研究者更好地了解新模型的应用前景，同时探索如何使用GPT-Vision来减轻信息的访问难度。<details>
<summary>Abstract</summary>
GPT-Vision has impressed us on a range of vision-language tasks, but it comes with the familiar new challenge: we have little idea of its capabilities and limitations. In our study, we formalize a process that many have instinctively been trying already to develop "grounded intuition" of this new model. Inspired by the recent movement away from benchmarking in favor of example-driven qualitative evaluation, we draw upon grounded theory and thematic analysis in social science and human-computer interaction to establish a rigorous framework for qualitative evaluation in natural language processing. We use our technique to examine alt text generation for scientific figures, finding that GPT-Vision is particularly sensitive to prompting, counterfactual text in images, and relative spatial relationships. Our method and analysis aim to help researchers ramp up their own grounded intuitions of new models while exposing how GPT-Vision can be applied to make information more accessible.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vicinal-Risk-Minimization-for-Few-Shot-Cross-lingual-Transfer-in-Abusive-Language-Detection"><a href="#Vicinal-Risk-Minimization-for-Few-Shot-Cross-lingual-Transfer-in-Abusive-Language-Detection" class="headerlink" title="Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection"></a>Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02025">http://arxiv.org/abs/2311.02025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gretel Liz De la Peña Sarracén, Paolo Rosso, Robert Litschko, Goran Glavaš, Simone Paolo Ponzetto</li>
<li>for: 本研究旨在提高跨语言恶意语言识别的性能，使用数据扩充和持续预训练进行领域适应。</li>
<li>methods: 本研究使用了两种现有的数据扩充技术，并提出了一种新的数据扩充方法（MIXAG），该方法根据实例表示的角度进行 interpolate 对照对。</li>
<li>results: 实验结果表明，数据扩充策略可以提高跨语言少量恶意语言识别的性能，特别是在多领域和多语言环境下。<details>
<summary>Abstract</summary>
Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adaptation can favour the class of abusive texts (reducing false negatives), but at the same time, declines the precision of the abusive language detection model.
</details>
<details>
<summary>摘要</summary>
cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adaptation can favour the class of abusive texts (reducing false negatives), but at the same time, declines the precision of the abusive language detection model.Here's the translation in Traditional Chinese:cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adaptation can favour the class of abusive texts (reducing false negatives), but at the same time, declines the precision of the abusive language detection model.
</details></li>
</ul>
<hr>
<h2 id="ProSG-Using-Prompt-Synthetic-Gradients-to-Alleviate-Prompt-Forgetting-of-RNN-like-Language-Models"><a href="#ProSG-Using-Prompt-Synthetic-Gradients-to-Alleviate-Prompt-Forgetting-of-RNN-like-Language-Models" class="headerlink" title="ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models"></a>ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01981">http://arxiv.org/abs/2311.01981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Luo, Kunming Wu, Cheng Dai, Sixian Ding, Xinhao Chen</li>
<li>for: 解决语言模型在生成过程中忘记提示问题</li>
<li>methods: 使用 sintetic gradient 教导模型在生成过程中记忆提示</li>
<li>results: 实验结果表明，该方法能够解决语言模型在生成过程中忘记提示的问题<details>
<summary>Abstract</summary>
RNN-like language models are getting renewed attention from NLP researchers in recent years and several models have made significant progress, which demonstrates performance comparable to traditional transformers. However, due to the recurrent nature of RNNs, this kind of language model can only store information in a set of fixed-length state vectors. As a consequence, they still suffer from forgetfulness though after a lot of improvements and optimizations, when given complex instructions or prompts. As the prompted generation is the main and most concerned function of LMs, solving the problem of forgetting in the process of generation is no wonder of vital importance. In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient. To force the model to memorize the prompt, we derive the states that encode the prompt, then transform it into model parameter modification using low-rank gradient approximation, which hard-codes the prompt into model parameters temporarily. We construct a dataset for experiments, and the results have demonstrated the effectiveness of our method in solving the problem of forgetfulness in the process of prompted generation. We will release all the code upon acceptance.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注在生成过程中缓解提示忘记的问题，我们提出了一种建议，通过合成梯度来教育模型在生成过程中记忆提示。我们首先提取了编码提示的状态，然后将其转换成模型参数修改使用低级导数预测，这会将提示短时间内写入模型参数中。我们构建了一个数据集，并进行了实验，结果表明我们的方法有效地解决了生成过程中的忘记问题。我们将代码发布于接受后。
</details></li>
</ul>
<hr>
<h2 id="Too-Much-Information-Keeping-Training-Simple-for-BabyLMs"><a href="#Too-Much-Information-Keeping-Training-Simple-for-BabyLMs" class="headerlink" title="Too Much Information: Keeping Training Simple for BabyLMs"></a>Too Much Information: Keeping Training Simple for BabyLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01955">http://arxiv.org/abs/2311.01955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Edman, Lisa Bylinina</li>
<li>for: 这篇论文描述了格罗宁根大学对 BabyLM 挑战的工作。</li>
<li>methods: 我们采用了如宝宝一样，将语言模型引入 simpler concept 先后理解更复杂的概念的想法。我们通过不同的角度（context size、词汇量和总语言复杂度）来检查这种策略的效果。</li>
<li>results: 我们发现只有context size truly beneficial to training a language model，但这simple change to context size 使我们在(Super)GLUE任务上的平均提高2点，在MSGS任务上的平均提高1点，在BLiMP任务上的平均提高12%。我们的context-limited模型比基线模型，在10 times更多的数据上进行训练。<details>
<summary>Abstract</summary>
This paper details the work of the University of Groningen for the BabyLM Challenge. We follow the idea that, like babies, language models should be introduced to simpler concepts first and build off of that knowledge to understand more complex concepts. We examine this strategy of simple-then-complex through a variety of lenses, namely context size, vocabulary, and overall linguistic complexity of the data. We find that only one, context size, is truly beneficial to training a language model. However this simple change to context size gives us improvements of 2 points on average on (Super)GLUE tasks, 1 point on MSGS tasks, and 12\% on average on BLiMP tasks. Our context-limited model outperforms the baseline that was trained on 10$\times$ the amount of data.
</details>
<details>
<summary>摘要</summary>
这份论文介绍了格隆根大学对宝宝LM挑战的工作。我们采用了婴儿式学习策略，即首先教育语言模型简单概念，然后逐步增加知识来理解更复杂的概念。我们通过不同的角度来检查这种简单然后复杂的策略，即上下文大小、词汇量和总语言复杂度。我们发现只有上下文大小真正有利于语言模型训练，但这种简单的改变使我们在（Super）GLUE任务上平均提高2点，MSGS任务上平均提高1点，BLiMP任务上平均提高12%。我们的上下文限定模型超过基eline模型，即使训练数据量为10倍。
</details></li>
</ul>
<hr>
<h2 id="Hint-enhanced-In-Context-Learning-wakes-Large-Language-Models-up-for-knowledge-intensive-tasks"><a href="#Hint-enhanced-In-Context-Learning-wakes-Large-Language-Models-up-for-knowledge-intensive-tasks" class="headerlink" title="Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks"></a>Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01949">http://arxiv.org/abs/2311.01949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Wang, Qingyan Guo, Xinzhe Ni, Chufan Shi, Lemao Liu, Haiyun Jiang, Yujiu Yang</li>
<li>for: 提高大语言模型（LLM）在知识密集任务中表现，特别是开放领域问答任务。</li>
<li>methods: 提出Hint-enhanced In-Context Learning（HICL）新 парадиг，利用LLM的解释能力从示例中提取问题相关的知识，然后将知识用作更Explicit的提示。同时，跟踪示例的来源以确定特定的示例，并引入Hint-related Example Retriever（HER）来选择有用的示例。</li>
<li>results: 对3个开放领域问答 benchmark进行评估，与标准设置相比，HICL加HER得到了平均性能提升2.89 EM score和2.52 F1 score在gpt-3.5-turbo上，7.62 EM score和7.27 F1 score在LLaMA-2-Chat-7B上。<details>
<summary>Abstract</summary>
In-context learning (ICL) ability has emerged with the increasing scale of large language models (LLMs), enabling them to learn input-label mappings from demonstrations and perform well on downstream tasks. However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions. To address this limitation, we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to explore the power of ICL in open-domain question answering, an important form in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract query-related knowledge from demonstrations, then concatenates the knowledge to prompt LLMs in a more explicit way. Furthermore, we track the source of this knowledge to identify specific examples, and introduce a Hint-related Example Retriever (HER) to select informative examples for enhanced demonstrations. We evaluate HICL with HER on 3 open-domain QA benchmarks, and observe average performance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM score and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.
</details>
<details>
<summary>摘要</summary>
受大语言模型（LLM）的规模增长的影响，宽 Context Learning（ICL）能力已经出现，使得 LLMS 可以从示例中学习输入标签映射，并在下游任务中表现良好。然而，在标准 ICLE 设置下， LLMS 可能会忽略示例中相关的查询信息，导致错误预测。为了解决这个限制，我们提出了一种新的思路calledHint-enhanced In-Context Learning（HICL），以探索 ICLE 在开放领域问答中的力量。HICL 利用 LLMS 的理解能力提取示例中相关的查询知识，然后将这些知识 concatenates 到提示 LLMS 以更加显式的方式。此外，我们跟踪这些知识的来源，并引入一个Hint-related Example Retriever（HER）来选择有用的示例，以提高示例的质量。我们在3个开放领域问答标准 benchmark上评估 HICL 和 HER，并观察了gpt-3.5-turbo 和 LLaMA-2-Chat-7B 上的平均性能提升2.89 EM 分数和2.52 F1 分数，升级7.62 EM 分数和7.27 F1 分数。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Temporal-Dynamic-Knowledge-Graphs-from-Interactive-Text-based-Games"><a href="#Constructing-Temporal-Dynamic-Knowledge-Graphs-from-Interactive-Text-based-Games" class="headerlink" title="Constructing Temporal Dynamic Knowledge Graphs from Interactive Text-based Games"></a>Constructing Temporal Dynamic Knowledge Graphs from Interactive Text-based Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01928">http://arxiv.org/abs/2311.01928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yukw777/temporal-discrete-graph-updater">https://github.com/yukw777/temporal-discrete-graph-updater</a></li>
<li>paper_authors: Keunwoo Peter Yu</li>
<li>for: 这个论文的目的是提出一种新的图ppoydunker模型，以提高对文本游戏中的动态知识图的表示和学习。</li>
<li>methods: 该模型使用一种名为时间点基于图神经网络的方法，将动态知识图表示为一系列时间戳的图事件，以提高知识图的准确性和可解释性。</li>
<li>results: 通过对TextWorld数据集进行实验，研究发现TDGU模型比基elineDGU模型表现更好，并且通过缺省研究和对更复杂的环境的演示，证明TDGU模型具有更好的泛化能力。<details>
<summary>Abstract</summary>
In natural language processing, interactive text-based games serve as a test bed for interactive AI systems. Prior work has proposed to play text-based games by acting based on discrete knowledge graphs constructed by the Discrete Graph Updater (DGU) to represent the game state from the natural language description. While DGU has shown promising results with high interpretability, it suffers from lower knowledge graph accuracy due to its lack of temporality and limited generalizability to complex environments with objects with the same label. In order to address DGU's weaknesses while preserving its high interpretability, we propose the Temporal Discrete Graph Updater (TDGU), a novel neural network model that represents dynamic knowledge graphs as a sequence of timestamped graph events and models them using a temporal point based graph neural network. Through experiments on the dataset collected from a text-based game TextWorld, we show that TDGU outperforms the baseline DGU. We further show the importance of temporal information for TDGU's performance through an ablation study and demonstrate that TDGU has the ability to generalize to more complex environments with objects with the same label. All the relevant code can be found at \url{https://github.com/yukw777/temporal-discrete-graph-updater}.
</details>
<details>
<summary>摘要</summary>
在自然语言处理领域，文本基于游戏作为互动AI系统的测试床。先前的工作已经提议通过基于自然语言描述生成的Discrete Graph Updater（DGU）来控制文本基于游戏。 although DGU has shown promising results with high interpretability, it suffers from lower knowledge graph accuracy due to its lack of temporality and limited generalizability to complex environments with objects with the same label. 为了解决DGU的缺陷而保持高度可读性，我们提出了Temporal Discrete Graph Updater（TDGU），一种新的神经网络模型，它表示动态知识图为一个时间戳的图事件序列，并使用时间点基于图神经网络来模型。 通过TextWorld数据集上的实验，我们表明TDGU超过了基准DGU。我们还进行了剖析研究，证明了TDGU的时间信息的重要性，并示出TDGU可以在更复杂的环境中 generale化。所有相关的代码可以在 GitHub上找到，链接为 \url{https://github.com/yukw777/temporal-discrete-graph-updater}.
</details></li>
</ul>
<hr>
<h2 id="BoschAI-PLABA-2023-Leveraging-Edit-Operations-in-End-to-End-Neural-Sentence-Simplification"><a href="#BoschAI-PLABA-2023-Leveraging-Edit-Operations-in-End-to-End-Neural-Sentence-Simplification" class="headerlink" title="BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification"></a>BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01907">http://arxiv.org/abs/2311.01907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentin Knappich, Simon Razniewski, Annemarie Friedrich</li>
<li>for: 这个论文的目的是提出一种基于LLAMA2的自动简化系统，以便非专业人员更好地理解复杂的科学文献。</li>
<li>methods: 该系统使用语言模型将复杂语言翻译成简单语言。论文提出了使用句子级和字节级损失权重来减少模型的训练信号和保守性。</li>
<li>results: 经验证明，该方法可以生成更加接近人工标注者创造的简化文本 (+1.8% &#x2F; +3.5% SARI),使用更加简单的语言 (-1 &#x2F; -1.1 FKGL)和更多的修改（1.6x &#x2F; 1.8x编辑距离），相比同模型通过标准十字 entropy进行 fine-tuning。此外，论文还表明了控制编辑距离和简单性水平（FKGL）的Hyperparameter $\lambda$。<details>
<summary>Abstract</summary>
Automatic simplification can help laypeople to comprehend complex scientific text. Language models are frequently applied to this task by translating from complex to simple language. In this paper, we describe our system based on Llama 2, which ranked first in the PLABA shared task addressing the simplification of biomedical text. We find that the large portion of shared tokens between input and output leads to weak training signals and conservatively editing models. To mitigate these issues, we propose sentence-level and token-level loss weights. They give higher weight to modified tokens, indicated by edit distance and edit operations, respectively. We conduct an empirical evaluation on the PLABA dataset and find that both approaches lead to simplifications closer to those created by human annotators (+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x / 1.8x edit distance) compared to the same model fine-tuned with standard cross entropy. We furthermore show that the hyperparameter $\lambda$ in token-level loss weights can be used to control the edit distance and the simplicity level (FKGL).
</details>
<details>
<summary>摘要</summary>
自动简化可以帮助非专家理解复杂科学文本。语言模型经常用于这种任务，将复杂语言翻译成简单语言。在这篇论文中，我们描述了基于LLAMA 2的系统，该系统在PLABA共享任务中排名第一，用于简化生物医学文本。我们发现输入和输出共享的大量共同token会导致弱的训练信号和保守的编辑模型。为了解决这些问题，我们提议使用句子级和token级损失权重。它们将修改后的token得到更高的权重，根据编辑距离和编辑操作来进行评估。我们对PLABA数据集进行了实验评估，发现两种方法都能够生成更加简洁的简化文本（+1.8% / +3.5% SARI）， simpler language (-1 / -1.1 FKGL）和更多的编辑（1.6x / 1.8x编辑距离），比同样的模型通过标准十字Entropy训练更好。我们还发现了$\lambda$参数在token级损失权重中可以控制编辑距离和简洁水平（FKGL）。
</details></li>
</ul>
<hr>
<h2 id="Indicative-Summarization-of-Long-Discussions"><a href="#Indicative-Summarization-of-Long-Discussions" class="headerlink" title="Indicative Summarization of Long Discussions"></a>Indicative Summarization of Long Discussions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01882">http://arxiv.org/abs/2311.01882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webis-de/emnlp-23">https://github.com/webis-de/emnlp-23</a></li>
<li>paper_authors: Shahbaz Syed, Dominik Schwabe, Khalid Al-Khatib, Martin Potthast</li>
<li>for: 提供一种novel的无监督方法，使用大型自然语言模型（LLM）生成长讨论的指示性摘要，以便方便用户快速浏览和理解长讨论。</li>
<li>methods: 方法首先对讨论中的argument sentence进行聚类，然后生成聚类标签作为摘要，最后将生成的摘要分类为口语框架。</li>
<li>results: 经过优化的提问工程approach，我们测试了19个LLM的生成聚类标签和口语框架分类能力，并进行了用户研究，结果表明，我们的提出的指示性摘要可以帮助用户快速浏览和理解长讨论。<details>
<summary>Abstract</summary>
Online forums encourage the exchange and discussion of different stances on many topics. Not only do they provide an opportunity to present one's own arguments, but may also gather a broad cross-section of others' arguments. However, the resulting long discussions are difficult to overview. This paper presents a novel unsupervised approach using large language models (LLMs) to generating indicative summaries for long discussions that basically serve as tables of contents. Our approach first clusters argument sentences, generates cluster labels as abstractive summaries, and classifies the generated cluster labels into argumentation frames resulting in a two-level summary. Based on an extensively optimized prompt engineering approach, we evaluate 19~LLMs for generative cluster labeling and frame classification. To evaluate the usefulness of our indicative summaries, we conduct a purpose-driven user study via a new visual interface called Discussion Explorer: It shows that our proposed indicative summaries serve as a convenient navigation tool to explore long discussions.
</details>
<details>
<summary>摘要</summary>
在线讨论区域鼓励不同观点的交流和讨论。不仅可以展示自己的Arguments，还可以收集各种不同的Arguments。然而，长时间的讨论可能很难概括。这篇论文提出了一种新的无监督方法，使用大型自然语言模型（LLMs）生成长讨论的指示性摘要。我们的方法首先对Argument sentence进行聚合，生成聚合Label作为摘要，然后将生成的聚合Label进行分类，生成两级摘要。通过大量优化的提示工程 Approach，我们评估了19种LLMs的生成聚合标签和框架分类。为了评估我们的指示性摘要的有用性，我们进行了一项目的用途驱动的用户研究，通过一种新的视觉界面 called Discussion Explorer：它表明了我们的提posed indicative summaries可以作为浏览长讨论的便捷导航工具。
</details></li>
</ul>
<hr>
<h2 id="Sentiment-Analysis-through-LLM-Negotiations"><a href="#Sentiment-Analysis-through-LLM-Negotiations" class="headerlink" title="Sentiment Analysis through LLM Negotiations"></a>Sentiment Analysis through LLM Negotiations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01876">http://arxiv.org/abs/2311.01876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofei Sun, Xiaoya Li, Shengyu Zhang, Shuhe Wang, Fei Wu, Jiwei Li, Tianwei Zhang, Guoyin Wang</li>
<li>for: This paper aims to improve the accuracy of sentiment analysis by introducing a multi-LLM negotiation framework that leverages the complementary abilities of multiple language models to generate more accurate and well-reasoned decisions.</li>
<li>methods: The proposed framework consists of a reasoning-infused generator and an explanation-deriving discriminator, which iterate until a consensus is reached. The generator provides decisions along with rationale, while the discriminator evaluates the credibility of the generator’s decisions.</li>
<li>results: The proposed approach consistently outperforms the in-context learning (ICL) baseline across all benchmarks, and even achieves superior performances compared to supervised baselines on the Twitter and movie review datasets.<details>
<summary>Abstract</summary>
A standard paradigm for sentiment analysis is to rely on a singular LLM and makes the decision in a single round under the framework of in-context learning. This framework suffers the key disadvantage that the single-turn output generated by a single LLM might not deliver the perfect decision, just as humans sometimes need multiple attempts to get things right. This is especially true for the task of sentiment analysis where deep reasoning is required to address the complex linguistic phenomenon (e.g., clause composition, irony, etc) in the input.   To address this issue, this paper introduces a multi-LLM negotiation framework for sentiment analysis. The framework consists of a reasoning-infused generator to provide decision along with rationale, a explanation-deriving discriminator to evaluate the credibility of the generator. The generator and the discriminator iterate until a consensus is reached. The proposed framework naturally addressed the aforementioned challenge, as we are able to take the complementary abilities of two LLMs, have them use rationale to persuade each other for correction.   Experiments on a wide range of sentiment analysis benchmarks (SST-2, Movie Review, Twitter, yelp, amazon, IMDB) demonstrate the effectiveness of proposed approach: it consistently yields better performances than the ICL baseline across all benchmarks, and even superior performances to supervised baselines on the Twitter and movie review datasets.
</details>
<details>
<summary>摘要</summary>
一般来说，用一个单一的深度学习模型（LLM）进行情感分析是一种常见的方法。这种方法的缺点是，单个LLM的输出可能不会提供完美的决策，就像人类在做出决策时有时需要多次尝试。这是特别真的 для情感分析任务，因为这个任务需要深入理解复杂的语言现象（例如句子组成、讽刺等）。为解决这个问题，这篇论文提出了一种多个LLM谈判框架 для情感分析。该框架包括一个理由感染生成器，用于提供决策以及理由，以及一个解释评估器，用于评估生成器的合理性。生成器和解释评估器会进行谈判，直到达成一致。提议的框架自然地解决了以上挑战，因为我们可以利用两个LLM的补充能力，让它们使用理由来证明对方需要更正。实验结果表明，提议的方法在各种情感分析标准benchmark（SST-2、电影评论、Twitter、Yelp、Amazon、IMDB）上表现出色， consistently 超过ICL基线，甚至在Twitter和电影评论数据集上超越了经过监督的基线。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Black-Box-Adversarial-Attacks-on-Neural-Text-Detectors"><a href="#Efficient-Black-Box-Adversarial-Attacks-on-Neural-Text-Detectors" class="headerlink" title="Efficient Black-Box Adversarial Attacks on Neural Text Detectors"></a>Efficient Black-Box Adversarial Attacks on Neural Text Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01873">http://arxiv.org/abs/2311.01873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vitalii Fishchuk, Daniel Braun</li>
<li>for:  investigate the effectiveness of three simple and resource-efficient strategies to alter texts generated by GPT-3.5 to misclassify neural text detectors.</li>
<li>methods:  parameter tweaking, prompt engineering, and character-level mutations.</li>
<li>results:  especially parameter tweaking and character-level mutations are effective strategies.Here’s the summary in Traditional Chinese as well:</li>
<li>for: 研究使用三种简单且资源有效的策略，让GPT-3.5生成的文本被神经文本探测器误将为人工生成的文本。</li>
<li>methods: 参数调整、提示工程和字元水平的变化。</li>
<li>results: 特别是参数调整和字元水平的变化是有效的策略。<details>
<summary>Abstract</summary>
Neural text detectors are models trained to detect whether a given text was generated by a language model or written by a human. In this paper, we investigate three simple and resource-efficient strategies (parameter tweaking, prompt engineering, and character-level mutations) to alter texts generated by GPT-3.5 that are unsuspicious or unnoticeable for humans but cause misclassification by neural text detectors. The results show that especially parameter tweaking and character-level mutations are effective strategies.
</details>
<details>
<summary>摘要</summary>
neural text detectors 是模型，用于detect whether a given text was generated by a language model or written by a human。在这篇论文中，我们investigate three simple and resource-efficient strategies（parameter tweaking，prompt engineering，and character-level mutations）to alter texts generated by GPT-3.5 that are unsuspicious or unnoticeable for humans but cause misclassification by neural text detectors。result showsthat especially parameter tweaking and character-level mutations are effective strategies。
</details></li>
</ul>
<hr>
<h2 id="R-3-NL2GQL-A-Hybrid-Models-Approach-for-for-Accuracy-Enhancing-and-Hallucinations-Mitigation"><a href="#R-3-NL2GQL-A-Hybrid-Models-Approach-for-for-Accuracy-Enhancing-and-Hallucinations-Mitigation" class="headerlink" title="$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation"></a>$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01862">http://arxiv.org/abs/2311.01862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhiqix/nl2gql">https://github.com/zhiqix/nl2gql</a></li>
<li>paper_authors: Yuhang Zhou, He Yu, Siyu Tian, Dan Chen, Liuzhi Zhou, Xinlin Yu, Chuanjun Ji, Sen Liu, Guangnan Ye, Hongfeng Chai</li>
<li>for: 这篇论文主要应用于将自然语言转换为graph查询语言（NL2GQL）任务中，并解决了Foundation Models在NL2GQL任务中的挑战。</li>
<li>methods: 本论文使用了Foundation Models，并将其分为大小不同的模型，以进行不同的调整和组合。</li>
<li>results: 实验结果显示，大型Foundation Models在NL2GQL任务中展现出了优秀的横推数据能力，而小型Foundation Models则在细化和调整后，对于意思理解和 grammatical accuracy 有所进步。<details>
<summary>Abstract</summary>
While current NL2SQL tasks constructed using Foundation Models have achieved commendable results, their direct application to Natural Language to Graph Query Language (NL2GQL) tasks poses challenges due to the significant differences between GQL and SQL expressions, as well as the numerous types of GQL. Our extensive experiments reveal that in NL2GQL tasks, larger Foundation Models demonstrate superior cross-schema generalization abilities, while smaller Foundation Models struggle to improve their GQL generation capabilities through fine-tuning. However, after fine-tuning, smaller models exhibit better intent comprehension and higher grammatical accuracy. Diverging from rule-based and slot-filling techniques, we introduce R3-NL2GQL, which employs both smaller and larger Foundation Models as reranker, rewriter and refiner. The approach harnesses the comprehension ability of smaller models for information reranker and rewriter, and the exceptional generalization and generation capabilities of larger models to transform input natural language queries and code structure schema into any form of GQLs. Recognizing the lack of established datasets in this nascent domain, we have created a bilingual dataset derived from graph database documentation and some open-source Knowledge Graphs (KGs). We tested our approach on this dataset and the experimental results showed that delivers promising performance and robustness.Our code and dataset is available at https://github.com/zhiqix/NL2GQL
</details>
<details>
<summary>摘要</summary>
当前的NL2SQL任务使用基础模型构建得到了可嘉的结果，但直接应用于自然语言到图查询语言（NL2GQL）任务却存在挑战，主要是因为GQL和SQL表达之间存在显著差异，以及GQL的多种类型。我们的广泛实验表明，在NL2GQL任务中，更大的基础模型在跨 schema 泛化能力方面表现出色，而更小的基础模型通过细化不能提高其生成GQL能力。然而，经细化后，更小的模型具有更高的意图理解和语法正确率。不同于规则基于和槽填充技术，我们提出了R3-NL2GQL，它使用更小和更大的基础模型来重新排序、重写和精度。这种方法利用更小的模型对信息重新排序和重写的能力，以及更大的模型对输入自然语言查询和代码结构 schema 的转换能力。认识到这个领域的数据集还没有成熔，我们从图数据库文档和一些开源知识图（KG） derivated 一个双语数据集。我们对这个数据集进行了测试，实验结果表明了我们的方法具有扎实的表现和稳定性。代码和数据集可以在https://github.com/zhiqix/NL2GQL 上获取。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-to-the-Rescue-Reducing-the-Complexity-in-Scientific-Workflow-Development-Using-ChatGPT"><a href="#Large-Language-Models-to-the-Rescue-Reducing-the-Complexity-in-Scientific-Workflow-Development-Using-ChatGPT" class="headerlink" title="Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT"></a>Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01825">http://arxiv.org/abs/2311.01825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mario Sänger, Ninon De Mecquenem, Katarzyna Ewa Lewińska, Vasilis Bountris, Fabian Lehmann, Ulf Leser, Thomas Kosch</li>
<li>for: 这篇研究旨在测试大自然语言模型（LLM）在科学工作流程中的效率，以支持用户在实现工作流程时所遇到的挑战。</li>
<li>methods: 研究使用了ChatGPT作为LLM，并进行了三个使用者研究，以评估ChatGPT在理解、适应和扩展工作流程方面的效能。</li>
<li>results: 研究结果显示LLM对工作流程的解释有高效性，但在交换组件或目的性工作流程扩展方面表现较差。研究也描述了LLM在这些困难情况下的限制，并建议未来研究的方向。<details>
<summary>Abstract</summary>
Scientific workflow systems are increasingly popular for expressing and executing complex data analysis pipelines over large datasets, as they offer reproducibility, dependability, and scalability of analyses by automatic parallelization on large compute clusters. However, implementing workflows is difficult due to the involvement of many black-box tools and the deep infrastructure stack necessary for their execution. Simultaneously, user-supporting tools are rare, and the number of available examples is much lower than in classical programming languages. To address these challenges, we investigate the efficiency of Large Language Models (LLMs), specifically ChatGPT, to support users when dealing with scientific workflows. We performed three user studies in two scientific domains to evaluate ChatGPT for comprehending, adapting, and extending workflows. Our results indicate that LLMs efficiently interpret workflows but achieve lower performance for exchanging components or purposeful workflow extensions. We characterize their limitations in these challenging scenarios and suggest future research directions.
</details>
<details>
<summary>摘要</summary>
We conducted three user studies in two scientific domains to evaluate ChatGPT's ability to comprehend, adapt, and extend workflows. Our results show that LLMs can efficiently interpret workflows, but their performance is lower when it comes to exchanging components or creating purposeful workflow extensions. We have identified the limitations of these challenging scenarios and suggest future research directions.Translated into Simplified Chinese:科学工作流系统在表达和执行复杂数据分析管道上占据着越来越多的市场份额，因为它们提供了可重现性、可靠性和可扩展性，并可自动平行化在大型计算集群上。然而，实现工作流程的困难在于许多黑盒工具的参与以及执行所需的深层基础设施。同时，用户支持工具罕见，可用的示例数量也远低于经典编程语言。为了解决这些挑战，我们研究了大语言模型（LLM），具体来说是ChatGPT，在科学工作流程中支持用户。我们在两个科学领域中进行了三个用户研究，以评估ChatGPT在理解、适应和扩展工作流程方面的能力。我们的结果表明，LLM可以高效地理解工作流程，但在交换组件或创造有目的工作流程扩展方面表现较差。我们对这些挑战的限制进行了特点分析，并建议未来的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Minimalist-Grammar-Construction-without-Overgeneration"><a href="#Minimalist-Grammar-Construction-without-Overgeneration" class="headerlink" title="Minimalist Grammar: Construction without Overgeneration"></a>Minimalist Grammar: Construction without Overgeneration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01820">http://arxiv.org/abs/2311.01820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isidor Konrad Maier, Johannes Kuhn, Jesse Beisegel, Markus Huber-Liebl, Matthias Wolff</li>
<li>for: 这篇论文是如何编写 minimalist grammar (MG) 的指南。</li>
<li>methods: 使用 variant of context free grammars (CFG) 作为输入格式，并使用 licensors&#x2F;-ees 特殊的方式处理例外情况。</li>
<li>results: 构建的 MG 可以避免过度生成，并且使用 adapters 解决 exceptions 处理中的问题。<details>
<summary>Abstract</summary>
In this paper we give instructions on how to write a minimalist grammar (MG). In order to present the instructions as an algorithm, we use a variant of context free grammars (CFG) as an input format. We can exclude overgeneration, if the CFG has no recursion, i.e. no non-terminal can (indirectly) derive to a right-hand side containing itself. The constructed MGs utilize licensors/-ees as a special way of exception handling. A CFG format for a derivation $A\_eats\_B\mapsto^* peter\_eats\_apples$, where $A$ and $B$ generate noun phrases, normally leads to overgeneration, e.\,g., $i\_eats\_apples$. In order to avoid overgeneration, a CFG would need many non-terminal symbols and rules, that mainly produce the same word, just to handle exceptions. In our MGs however, we can summarize CFG rules that produce the same word in one item and handle exceptions by a proper distribution of licensees/-ors. The difficulty with this technique is that in most generations the majority of licensees/-ors is not needed, but still has to be triggered somehow. We solve this problem with $\epsilon$-items called \emph{adapters}.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了写 minimalist grammar（MG）的指导方针。为了表示这些指导方针为算法，我们使用 variant of context free grammars（CFG）作为输入格式。如果 CFG 没有回归，则可以排除过度生成。 constructed MGs 使用licensee/-or作为特殊的例外处理方式。CFG 格式 для一个 derivation $A\_eats\_B\mapsto^* peter\_eats\_apples$，where $A$ 和 $B$ 生成名词短语，通常会导致过度生成，例如 $i\_eats\_apples$。为了避免过度生成，一个 CFG 需要很多非树状符号和规则，主要生成同一个词的不同形式，只是为了处理例外。在我们的 MGs 中，我们可以汇总 CFG 规则生成同一个词的项目，并通过正确的分配licensee/-or来处理例外。这种技术的困难在于，在大多数生成中，主要的licensee/-or并不需要，但仍需要某种触发方式。我们解决这个问题使用 $\epsilon$-item called \emph{adapters}。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Framing-Bias-with-Polarity-Minimization-Loss"><a href="#Mitigating-Framing-Bias-with-Polarity-Minimization-Loss" class="headerlink" title="Mitigating Framing Bias with Polarity Minimization Loss"></a>Mitigating Framing Bias with Polarity Minimization Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01817">http://arxiv.org/abs/2311.01817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yejin Bang, Nayeon Lee, Pascale Fung</li>
<li>for: 防止新闻报道中的偏见倾向</li>
<li>methods: 提出一种新的损失函数，用于降低多个新闻报道中的偏见差异</li>
<li>results: 实验结果表明，通过在模型中添加该损失函数可以减少偏见倾向，其效果最大化在降低信息偏见倾向（即报道中选择的信息偏见）。<details>
<summary>Abstract</summary>
Framing bias plays a significant role in exacerbating political polarization by distorting the perception of actual events. Media outlets with divergent political stances often use polarized language in their reporting of the same event. We propose a new loss function that encourages the model to minimize the polarity difference between the polarized input articles to reduce framing bias. Specifically, our loss is designed to jointly optimize the model to map polarity ends bidirectionally. Our experimental results demonstrate that incorporating the proposed polarity minimization loss leads to a substantial reduction in framing bias when compared to a BART-based multi-document summarization model. Notably, we find that the effectiveness of this approach is most pronounced when the model is trained to minimize the polarity loss associated with informational framing bias (i.e., skewed selection of information to report).
</details>
<details>
<summary>摘要</summary>
帧偏调 plays a significant role in exacerbating political polarization by distorting the perception of actual events. Media outlets with divergent political stances often use polarized language in their reporting of the same event. We propose a new loss function that encourages the model to minimize the polarity difference between the polarized input articles to reduce framing bias. Specifically, our loss is designed to jointly optimize the model to map polarity ends bidirectionally. Our experimental results demonstrate that incorporating the proposed polarity minimization loss leads to a substantial reduction in framing bias when compared to a BART-based multi-document summarization model. Notably, we find that the effectiveness of this approach is most pronounced when the model is trained to minimize the polarity loss associated with informational framing bias (i.e., skewed selection of information to report).Here's the translation in Traditional Chinese:帧偏调对政治化分化具有重要作用，导致现实事件的观察被扭曲。媒体对同一事件的报导 often 使用偏 polarized 的语言，这会导致政治分化。我们提出了一个新的损失函数，这个损失函数鼓励模型将 polarity 的差异最小化，以减少帧偏调。具体来说，我们的损失函数设计来对 polarity 的端点进行bidirectional 的对映。我们的实验结果显示，将 proposed polarity 损失函数添加到模型中可以对帧偏调进行重大减少，相比之下，使用 BART 基于多篇文章摘要模型。当然，我们发现这种方法在对 informational framing bias 进行对映时表现最佳。
</details></li>
</ul>
<hr>
<h2 id="UP4LS-User-Profile-Constructed-by-Multiple-Attributes-for-Enhancing-Linguistic-Steganalysis"><a href="#UP4LS-User-Profile-Constructed-by-Multiple-Attributes-for-Enhancing-Linguistic-Steganalysis" class="headerlink" title="UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis"></a>UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01775">http://arxiv.org/abs/2311.01775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihao Wang, Ruiqi Song, Ru Zhang, Jianyi Liu</li>
<li>for: 提高语言隐藏分析（LS）任务的性能，特别是在社交媒体上。</li>
<li>methods: 利用用户 profiling 技术，挖掘用户的写作习惯、心理状态和关注点，然后与现有方法结合使用语言模型来提取特征。</li>
<li>results: 对现有方法进行改进，实现减少隐藏样本数量下的性能提升，具体提升约25%。<details>
<summary>Abstract</summary>
Linguistic steganalysis (LS) tasks aim to effectively detect stegos generated by linguistic steganography. Existing LS methods overlook the distinctive user characteristics, leading to weak performance in social networks. The limited occurrence of stegos further complicates detection. In this paper, we propose the UP4LS, a novel framework with the User Profile for enhancing LS performance. Specifically, by delving into post content, we explore user attributes like writing habits, psychological states, and focal areas, thereby building the user profile for LS. For each attribute, we design the identified feature extraction module. The extracted features are mapped to high-dimensional user features via deep-learning networks from existing methods. Then the language model is employed to extract content features. The user and content features are integrated to optimize feature representation. During the training phase, we prioritize the distribution of stegos. Experiments demonstrate that UP4LS can significantly enhance the performance of existing methods, and an overall accuracy improvement of nearly 25%. In particular, the improvement is especially pronounced with fewer stego samples. Additionally, UP4LS also sets the stage for studies on related tasks, encouraging extensive applications on LS tasks.
</details>
<details>
<summary>摘要</summary>
文本隐藏分析（LS）任务目的是有效检测基于语言隐藏技术生成的隐藏文本（stegos）。现有的LS方法忽略了用户特征，导致检测效果在社交网络中弱化。隐藏文本的有限发生频率更进一步复杂了检测。本文提出了UP4LS，一种新的框架，通过探索文章内容，捕捉用户特征，如写作习惯、心理状态和焦点领域，建立用户profile，并为每个特征设计特定的特征提取模块。这些特征被映射到现有方法中的深度学习网络中，然后使用语言模型提取内容特征。用户和内容特征被结合，以优化特征表示。在训练阶段，我们优先考虑隐藏文本的分布。实验表明，UP4LS可以显著提高现有方法的性能，具体提高约25%。尤其是在 fewer stego samples 的情况下，提高更加明显。此外，UP4LS还为相关任务提供了开门篇，激发了广泛的应用研究。
</details></li>
</ul>
<hr>
<h2 id="PPTC-Benchmark-Evaluating-Large-Language-Models-for-PowerPoint-Task-Completion"><a href="#PPTC-Benchmark-Evaluating-Large-Language-Models-for-PowerPoint-Task-Completion" class="headerlink" title="PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion"></a>PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01767">http://arxiv.org/abs/2311.01767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gydpku/pptc">https://github.com/gydpku/pptc</a></li>
<li>paper_authors: Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, Duan Nan</li>
<li>for: 这项研究旨在评估大自然语言模型（LLM）在完成多个交互、多modal操作的复杂多modal环境中的能力。</li>
<li>methods: 该研究使用了PowerPoint Task Completion（PPTC） benchmarch来评估LLM在创建和编辑PPT文件基于用户 instrucion的能力。</li>
<li>results: 研究发现GPT-4在单转对话测试中具有75.1%的准确率，但在完成整个会话中表现不佳，只有6%的会话准确率。研究发现三种主要错误原因：交互累积、长时间处理PPT模板和多modal识别。这些问题对未来LLM和代理系统 pose 极大挑战。<details>
<summary>Abstract</summary>
Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6\% session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems. We release the data, code, and evaluation system of PPTC at \url{https://github.com/gydpku/PPTC}.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）评估中心在测试它们零shot/几shot能力来完成基本的自然语言任务以及将指令转化为工具API。然而，对于使用复杂工具完成多Turn多模态任务在复杂多模态环境中评估LLM的能力还没有被研究。为了解决这一漏洞，我们介绍了PowerPoint任务完成（PPTC）标准测试套件，用于评估LLM在基于用户指令创建和编辑PPT文件方面的能力。该套件包含279个多Turn会话，涵盖多个主题和百度 instrucciones 涉及多模态操作。我们还提出了PPTXMatch评估系统，它根据预测文件而不是标签API序列来评估LLM是否完成了指令。这种支持多种LLM生成的API序列。我们测试了三个关闭LLM和六个开源LLM。结果显示，GPT-4在单Turn对话测试中的准确率为75.1%，但在完成整个会话时表现不佳，只有6%的会话准确率。我们发现了三种主要的错误原因：在多Turn会话中的错误积累、长PPT模板处理和多模态感知。这些问题对未来LLM和代理系统带来了很大挑战。我们将数据、代码和评估系统发布到GitHub上，请参考 \url{https://github.com/gydpku/PPTC}.
</details></li>
</ul>
<hr>
<h2 id="Support-or-Refute-Analyzing-the-Stance-of-Evidence-to-Detect-Out-of-Context-Mis-and-Disinformation"><a href="#Support-or-Refute-Analyzing-the-Stance-of-Evidence-to-Detect-Out-of-Context-Mis-and-Disinformation" class="headerlink" title="Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation"></a>Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01766">http://arxiv.org/abs/2311.01766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yuan, Jie Guo, Weidong Qiu, Zheng Huang, Shujun Li</li>
<li>for: 防止在线谣言和false information的扩散</li>
<li>methods: 提出了一种基于多模态证据的偏见抽取网络（SEN），可以同时抽取不同证据的偏见，以提高识别结果的准确性</li>
<li>results: 对大规模公共数据集进行了广泛的实验，发现提出的方法比前期基elines的表现升高3.2%的精度。<details>
<summary>Abstract</summary>
Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale dataset demonstrated that our proposed method outperformed the state-of-the-art baselines, with the best model achieving a performance gain of 3.2% in accuracy.
</details>
<details>
<summary>摘要</summary>
互联网上的谬误和不准确信息已成为现代社会的重要问题，是多种不同类型的在线危害的主要来源。一种常见的谬误信息形式是Context Out-of-Context（OOC）信息，即不同的信息元素被谬误地联系起来，例如真实的图像与谬误的文字描述或歪曲的文本描述。 although some past studies have tried to defend against OOC misinformation through external evidence, they tend to ignore the role of different pieces of evidence with different stances. 驱动了寻求解决这个问题的直觉，我们提出了一种姿态提取网络（SEN），可以在一个统一的框架中提取不同类型的多Modal证据的姿态。此外，我们还引入了基于命名实体之间的共occurrence关系的支持驳回分数，来进一步提高文本SEN的准确性。经过了一系列的大规模公共数据集的实验，我们的提议方法在准确性方面超过了现有的基线，最佳模型在准确性方面提高了3.2%。
</details></li>
</ul>
<hr>
<h2 id="EmojiLM-Modeling-the-New-Emoji-Language"><a href="#EmojiLM-Modeling-the-New-Emoji-Language" class="headerlink" title="EmojiLM: Modeling the New Emoji Language"></a>EmojiLM: Modeling the New Emoji Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01751">http://arxiv.org/abs/2311.01751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/komeijiforce/emojilm">https://github.com/komeijiforce/emojilm</a></li>
<li>paper_authors: Letian Peng, Zilong Wang, Hang Liu, Zihan Wang, Jingbo Shang</li>
<li>for: 研究在线上社交媒体上的表情符号（emoji）的使用趋势和应用。</li>
<li>methods: 使用大型自然语言模型创建了大量文本-表情符号平行数据库（Text2Emoji），并基于这个平行数据库对文本-表情符号 bidirectional 翻译进行了几何分析。</li>
<li>results: 比较baseline模型和平行数据库，我们的提案模型在公共benchmark上和人工评估中均有出色的表现，并且显示了文本-表情符号bidirectional 翻译的应用价值。<details>
<summary>Abstract</summary>
With the rapid development of the internet, online social media welcomes people with different backgrounds through its diverse content. The increasing usage of emoji becomes a noticeable trend thanks to emoji's rich information beyond cultural or linguistic borders. However, the current study on emojis is limited to single emoji prediction and there are limited data resources available for further study of the interesting linguistic phenomenon. To this end, we synthesize a large text-emoji parallel corpus, Text2Emoji, from a large language model. Based on the parallel corpus, we distill a sequence-to-sequence model, EmojiLM, which is specialized in the text-emoji bidirectional translation. Extensive experiments on public benchmarks and human evaluation demonstrate that our proposed model outperforms strong baselines and the parallel corpus benefits emoji-related downstream tasks.
</details>
<details>
<summary>摘要</summary>
“因互联网的快速发展，在线社交媒体逐渐推广不同背景的人透过各种多元内容。增加使用表情符号的趋势也因为表情符号具有跨文化或语言边界的丰富信息，成为当前研究热点。然而，现有的研究仅专注于单一表情符号预测，有限的数据资源对进一步研究表情符号的兴趣语言现象提供了有限的支持。为此，我们合成了大量文本-表情符号平行数据库，Text2Emoji，基于大型语言模型。根据平行数据库，我们提炼了文本-表情符号双向翻译模型，EmojiLM，并进行了广泛的公共benchmark和人类评价。实验结果显示，我们提议的模型优于强基eline，并且平行数据库对表情符号相关下游任务具有助益。”
</details></li>
</ul>
<hr>
<h2 id="SAC-3-Reliable-Hallucination-Detection-in-Black-Box-Language-Models-via-Semantic-aware-Cross-check-Consistency"><a href="#SAC-3-Reliable-Hallucination-Detection-in-Black-Box-Language-Models-via-Semantic-aware-Cross-check-Consistency" class="headerlink" title="SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency"></a>SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01740">http://arxiv.org/abs/2311.01740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, Sricharan Kumar</li>
<li>for: 检测语言模型中的幻觉是现代自然语言处理中的一个关键步骤，以确定语言模型的可靠性。</li>
<li>methods: 我们基于语言模型的自我一致性进行检测，并发现了问题水平和模型水平的两种幻觉，这些幻觉不能通过自我一致性检测察看到。我们提出了一种新的采样方法，即含义相关的检查三重方法（SAC$^3$），该方法基于自我一致性检测的原理，并具有更多的机制来检测问题水平和模型水平的幻觉。</li>
<li>results: 我们通过广泛和系统的实验分析，证明了SAC$^3$ 方法在多个问答和开放领域生成 benchmark 上的表现，可以准确地检测非事实和事实声明。<details>
<summary>Abstract</summary>
Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC$^3$) that expands on the principle of self-consistency checking. Our SAC$^3$ approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC$^3$ outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.
</details>
<details>
<summary>摘要</summary>
现代语言模型（LM）的可信worthiness问题是一个关键步骤。为了解决这个问题，我们重新审视了现有的检测方法，基于语言模型自我一致性。我们发现了两种类型的幻觉，即问题级幻觉和模型级幻觉，这些幻觉不可以通过自我一致性检查 alone 检测出来。基于这一发现，我们提出了一种新的采样基于方法，即含义相关的交叉检查一致性（SAC$^3$）。我们的SAC$^3$方法具有检测问题级和模型级幻觉的能力，通过利用包括semantically相同的问题抖动和跨模型响应一致性检查在内的进一步技术。我们通过了广泛和系统的实验分析，证明了SAC$^3$在检测多个问答和开放领域生成benchmark上的非事实和事实陈述性能比前者更高。
</details></li>
</ul>
<hr>
<h2 id="Proto-lm-A-Prototypical-Network-Based-Framework-for-Built-in-Interpretability-in-Large-Language-Models"><a href="#Proto-lm-A-Prototypical-Network-Based-Framework-for-Built-in-Interpretability-in-Large-Language-Models" class="headerlink" title="Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models"></a>Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01732">http://arxiv.org/abs/2311.01732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Xie, Soroush Vosoughi, Saeed Hassanpour</li>
<li>for: This paper aims to improve the interpretability of Large Language Models (LLMs) by developing a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance.</li>
<li>methods: The proposed method, called proto-lm, uses a prototypical network to learn interpretable embeddings that can be used to understand how the LLM is making predictions. The method is based on a white-box framework, which allows for transparency and interpretability of the model’s inner workings.</li>
<li>results: The authors demonstrate the applicability and interpretability of their method through experiments on a wide range of NLP tasks, and show that their approach can pave the way for more interpretable models without sacrificing performance. Specifically, their results indicate that the proposed method can learn interpretable embeddings that can be used to understand how the LLM is making predictions, and that the method maintains competitive performance on a variety of NLP tasks.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method's applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大型语言模型（LLMs）已经帮助了自然语言处理（NLP）领域的发展，但它们的无法解释性带来了主要的担忧。现有的LLMs解释方法都是后期应用的，并且有些缺点，如专注于低级特征和文本单位高级解释性的缺失。在这项工作中，我们提出了 proto-lm，一种基于 прото型网络的白色盒框架，使得 LLMs 可以在练习阶段直接学习可解释的嵌入，而不会影响性能。我们的方法在多种 NLP 任务上进行了实验，并证明了其可应用性和解释性。 results 表明了一种可能性，即创建可解释的模型不需要牺牲性能。这种新的LLMs解释方法可能会开辟出一条新的解释性道路，无需牺牲性能。
</details></li>
</ul>
<hr>
<h2 id="A-New-Korean-Text-Classification-Benchmark-for-Recognizing-the-Political-Intents-in-Online-Newspapers"><a href="#A-New-Korean-Text-Classification-Benchmark-for-Recognizing-the-Political-Intents-in-Online-Newspapers" class="headerlink" title="A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers"></a>A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01712">http://arxiv.org/abs/2311.01712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kdavid2355/kopolitic-benchmark-dataset">https://github.com/kdavid2355/kopolitic-benchmark-dataset</a></li>
<li>paper_authors: Beomjune Kim, Eunsun Lee, Dongbin Na</li>
<li>for: 本文主要针对在南韩新闻媒体上发表的政治意图文章进行自动识别。</li>
<li>methods: 该文使用了深度学习基于变换器架构的语言模型，并在大规模的韩国新闻数据集上进行训练。</li>
<li>results: 训练后的模型显示了良好的文本分类性能，并且可以同时进行多任务分类。此外，该文还提供了大规模的韩国新闻数据集，可供Future研究使用。<details>
<summary>Abstract</summary>
Many users reading online articles in various magazines may suffer considerable difficulty in distinguishing the implicit intents in texts. In this work, we focus on automatically recognizing the political intents of a given online newspaper by understanding the context of the text. To solve this task, we present a novel Korean text classification dataset that contains various articles. We also provide deep-learning-based text classification baseline models trained on the proposed dataset. Our dataset contains 12,000 news articles that may contain political intentions, from the politics section of six of the most representative newspaper organizations in South Korea. All the text samples are labeled simultaneously in two aspects (1) the level of political orientation and (2) the level of pro-government. To the best of our knowledge, our paper is the most large-scale Korean news dataset that contains long text and addresses multi-task classification problems. We also train recent state-of-the-art (SOTA) language models that are based on transformer architectures and demonstrate that the trained models show decent text classification performance. All the codes, datasets, and trained models are available at https://github.com/Kdavid2355/KoPolitic-Benchmark-Dataset.
</details>
<details>
<summary>摘要</summary>
многие用户在阅读在线报纸时可能会遇到很大的区分隐含意图的困难。在这项工作中，我们关注自动识别在线报纸中的政治意图，通过理解文本的上下文来解决这个问题。为解决这个任务，我们提供了一个新的韩国文本分类数据集，该数据集包含了多种文章。我们还提供了基于深度学习的文本分类基线模型，该模型在我们提posed的数据集上训练。我们的数据集包含12,000篇报纸文章，这些文章可能包含政治意图，来自韩国六家最重要的报纸组织的政治部分。所有的文本样本都同时被标注了两个方面：（1）政治方向的水平和（2）政府支持度的水平。根据我们所知，我们的论文是最大规模的韩国新闻数据集，它包含了长文本，并解决了多任务分类问题。我们还训练了最新的状态zig对应的语言模型，该模型基于变换架构，并示出了训练后的模型在文本分类任务上的不错表现。所有的代码、数据集和训练模型都可以在https://github.com/Kdavid2355/KoPolitic-Benchmark-Dataset上获取。
</details></li>
</ul>
<hr>
<h2 id="CASE-Commonsense-Augmented-Score-with-an-Expanded-Answer-Space"><a href="#CASE-Commonsense-Augmented-Score-with-an-Expanded-Answer-Space" class="headerlink" title="CASE: Commonsense-Augmented Score with an Expanded Answer Space"></a>CASE: Commonsense-Augmented Score with an Expanded Answer Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01684">http://arxiv.org/abs/2311.01684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wk-chen/commonsense-augmented-score-with-an-expanded-answer-space">https://github.com/wk-chen/commonsense-augmented-score-with-an-expanded-answer-space</a></li>
<li>paper_authors: Wenkai Chen, Sahithya Ravi, Vered Shwartz</li>
<li>for: 这个论文是为了提高 Language Model (LM) 在多项选择问答任务中的表现，特别是 Addressing the limitation of basic score 对所有单词的对待。</li>
<li>methods: 该论文提出了 Commonsense-Augmented Score with Expanded Answer Space (CASE)，即基于含义关系的单词重要性权重，以及生成多元答案的方法。</li>
<li>results: 对五个常识 benchmark 进行了测试，RESULTS 表明，在使用 smaller LMs 时，CASE 方法可以超越强基线，并且与答案空间扩展方法相结合时，效果更好。<details>
<summary>Abstract</summary>
LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks to the knowledge they acquired in their training. In multiple-choice QA tasks, the LM probabilities are used as an imperfect measure of the plausibility of each answer choice. One of the major limitations of the basic score is that it treats all words as equally important. We propose CASE, a Commonsense-Augmented Score with an Expanded Answer Space. CASE addresses this limitation by assigning importance weights for individual words based on their semantic relations to other words in the input. The dynamic weighting approach outperforms basic LM scores, not only because it reduces noise from unimportant words, but also because it informs the model of implicit commonsense knowledge that may be useful for answering the question. We then also follow prior work in expanding the answer space by generating lexically-divergent answers that are conceptually-similar to the choices. When combined with answer space expansion, our method outperforms strong baselines on 5 commonsense benchmarks. We further show these two approaches are complementary and may be especially beneficial when using smaller LMs.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation may not be perfect, and some nuances or idioms may not be fully conveyed.
</details></li>
</ul>
<hr>
<h2 id="Plot-Retrieval-as-an-Assessment-of-Abstract-Semantic-Association"><a href="#Plot-Retrieval-as-an-Assessment-of-Abstract-Semantic-Association" class="headerlink" title="Plot Retrieval as an Assessment of Abstract Semantic Association"></a>Plot Retrieval as an Assessment of Abstract Semantic Association</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01666">http://arxiv.org/abs/2311.01666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shicheng Xu, Liang Pang, Jiangnan Li, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, Jie Zhou</li>
<li>for: 提高阅读体验和效率，提取相关剧情图文</li>
<li>methods: 使用标注数据集Plot Retrieval进行训练和评估信息检索模型的抽象含义关系能力</li>
<li>results: 现有信息检索模型仍然在捕捉抽象含义关系方面做不够，需要进一步研究抽象含义模型化能力<details>
<summary>Abstract</summary>
Retrieving relevant plots from the book for a query is a critical task, which can improve the reading experience and efficiency of readers. Readers usually only give an abstract and vague description as the query based on their own understanding, summaries, or speculations of the plot, which requires the retrieval model to have a strong ability to estimate the abstract semantic associations between the query and candidate plots. However, existing information retrieval (IR) datasets cannot reflect this ability well. In this paper, we propose Plot Retrieval, a labeled dataset to train and evaluate the performance of IR models on the novel task Plot Retrieval. Text pairs in Plot Retrieval have less word overlap and more abstract semantic association, which can reflect the ability of the IR models to estimate the abstract semantic association, rather than just traditional lexical or semantic matching. Extensive experiments across various lexical retrieval, sparse retrieval, dense retrieval, and cross-encoder methods compared with human studies on Plot Retrieval show current IR models still struggle in capturing abstract semantic association between texts. Plot Retrieval can be the benchmark for further research on the semantic association modeling ability of IR models.
</details>
<details>
<summary>摘要</summary>
<<SYS>> Retrieving relevant plots from a book based on a query is a crucial task that can enhance the reading experience and efficiency of readers. However, existing information retrieval (IR) datasets do not reflect this ability well. In this paper, we propose Plot Retrieval, a labeled dataset to train and evaluate the performance of IR models on the novel task of Plot Retrieval. The text pairs in Plot Retrieval have less word overlap and more abstract semantic association, which can better reflect the ability of IR models to estimate the abstract semantic association rather than just traditional lexical or semantic matching. Extensive experiments comparing various lexical retrieval, sparse retrieval, dense retrieval, and cross-encoder methods with human studies on Plot Retrieval show that current IR models still struggle in capturing abstract semantic associations between texts. Plot Retrieval can serve as a benchmark for further research on the semantic association modeling ability of IR models.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/cs.CL_2023_11_03/" data-id="clpxp03xp00eofm88hklt46en" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/12/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><a class="page-number" href="/page/15/">15</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/14/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
