
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/34/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/eess.IV_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T09:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/eess.IV_2023_10_06/">eess.IV - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Plug-and-Play-Image-Registration-Network"><a href="#A-Plug-and-Play-Image-Registration-Network" class="headerlink" title="A Plug-and-Play Image Registration Network"></a>A Plug-and-Play Image Registration Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04297">http://arxiv.org/abs/2310.04297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhao Hu, Weijie Gan, Zhixin Sun, Hongyu An, Ulugbek S. Kamilov<br>for: 这个研究旨在开发一个基于深度学习的形变影像注册（DIR）方法，以提高生物医学影像注册的精度和效率。methods: 这个方法基于一个条件enced Convolutional Neural Network（CNN）来估计两个输入影像之间的注册场，并透过将CNN检测器”嵌入”到一个迭代法中，以增强注册的稳定性和准确性。results: 我们的方法在OASIS和CANDI dataset上的数据显示，能够 дости得生物医学影像注册的州度顶峰性能。<details>
<summary>Abstract</summary>
Deformable image registration (DIR) is an active research topic in biomedical imaging. There is a growing interest in developing DIR methods based on deep learning (DL). A traditional DL approach to DIR is based on training a convolutional neural network (CNN) to estimate the registration field between two input images. While conceptually simple, this approach comes with a limitation that it exclusively relies on a pre-trained CNN without explicitly enforcing fidelity between the registered image and the reference. We present plug-and-play image registration network (PIRATE) as a new DIR method that addresses this issue by integrating an explicit data-fidelity penalty and a CNN prior. PIRATE pre-trains a CNN denoiser on the registration field and "plugs" it into an iterative method as a regularizer. We additionally present PIRATE+ that fine-tunes the CNN prior in PIRATE using deep equilibrium models (DEQ). PIRATE+ interprets the fixed-point iteration of PIRATE as a network with effectively infinite layers and then trains the resulting network end-to-end, enabling it to learn more task-specific information and boosting its performance. Our numerical results on OASIS and CANDI datasets show that our methods achieve state-of-the-art performance on DIR.
</details>
<details>
<summary>摘要</summary>
扭形图像registratio (DIR) 是生物医学成像领域的活跃研究领域。随着深度学习 (DL) 的发展，DIR 方法也在不断地演化。传统的 DL 方法是通过训练一个卷积神经网络 (CNN) 来估计两个输入图像之间的 registrtion 场。然而，这种方法存在一个限制，即完全依赖于预训练的 CNN，而不是直接强制图像注册和参考图像之间的准确性。我们提出了一种新的 DIR 方法，即插件和游戏图像注册网络 (PIRATE)，它通过结合显式数据准确性罚和 CNN 先验来解决这个问题。PIRATE 先训练了一个 CNN 减噪器在注册场景中，然后将其作为 PIRATE 的规则进行插入。此外，我们还提出了 PIRATE+，它在 PIRATE 中使用深度平衡模型 (DEQ) 进行 fine-tuning，从而使 PIRATE 能够更好地学习任务特有的信息，提高其性能。我们的数字结果表明，我们的方法在 OASIS 和 CANDI 数据集上实现了 DIR 领域的状态能力。
</details></li>
</ul>
<hr>
<h2 id="Towards-Non-contact-3D-Ultrasound-for-Wrist-Imaging"><a href="#Towards-Non-contact-3D-Ultrasound-for-Wrist-Imaging" class="headerlink" title="Towards Non-contact 3D Ultrasound for Wrist Imaging"></a>Towards Non-contact 3D Ultrasound for Wrist Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04296">http://arxiv.org/abs/2310.04296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antony Jerald, A. N. Madhavanunni, Gayathri Malamal, Mahesh Raveendranatha Panicker<br>for:The paper aims to develop a novel approach for non-contact freehand 3D ultrasound imaging with minimal complexity added to existing point of care ultrasound (POCUS) systems.methods:The proposed approach uses a mechanical track for non-contact ultrasound scanning, which restricts the probe motion to a linear plane and simplifies the acquisition and 3D reconstruction process. A pipeline for US 3D volume reconstruction using an US research platform and a GPU-based edge device is developed.results:The proposed approach is demonstrated through ex-vivo and in-vivo experiments, showing its efficacy in providing accurate 3D US imaging with adjustable field of view capability, non-contact design, and low cost of deployment without significantly altering the existing setup.Please note that the above information is in Simplified Chinese:for: 这篇论文的目的是开发一种新的非接触自由手3D超声成像方法，以便在现有的点检超声系统（POCUS）上增加了最小复杂度。methods: 该提议使用机械轨迹进行非接触超声扫描，这限制了探针的运动范围为直线平面，以简化获取和3D重建过程。一个基于超声研究平台和GPU的边缘设备上的3D超声卷积管道也被开发。results: 该提议的效果通过外科和生物实验显示，能够提供精度的3D超声成像，并且具有可调Field of view功能、非接触设计和低成本部署，不会对现有设置产生显著影响。<details>
<summary>Abstract</summary>
Objective: The objective of this work is an attempt towards non-contact freehand 3D ultrasound imaging with minimal complexity added to the existing point of care ultrasound (POCUS) systems. Methods: This study proposes a novel approach of using a mechanical track for non-contact ultrasound (US) scanning. The approach thus restricts the probe motion to a linear plane, to simplify the acquisition and 3D reconstruction process. A pipeline for US 3D volume reconstruction employing an US research platform and a GPU-based edge device is developed. Results: The efficacy of the proposed approach is demonstrated through ex-vivo and in-vivo experiments. Conclusion: The proposed approach with the adjustable field of view capability, non-contact design, and low cost of deployment without significantly altering the existing setup would open doors for up gradation of traditional systems to a wide range of 3D US imaging applications. Significance: Ultrasound (US) imaging is a popular clinical imaging modality for the point-of-care bedside imaging, particularly of the wrist/knee in the pediatric population due to its non-invasive and radiation free nature. However, the limited views of tissue structures obtained with 2D US in such scenarios make the diagnosis challenging. To overcome this, 3D US imaging which uses 2D US images and their orientation/position to reconstruct 3D volumes was developed. The accurate position estimation of the US probe at low cost has always stood as a challenging task in 3D reconstruction. Additionally, US imaging involves contact, which causes difficulty to pediatric subjects while monitoring live fractures or open wounds. Towards overcoming these challenges, a novel framework is attempted in this work.
</details>
<details>
<summary>摘要</summary>
Methods: This study proposes a new approach using a mechanical track for non-contact US scanning, which simplifies the acquisition and 3D reconstruction process by restricting the probe motion to a linear plane. A pipeline for US 3D volume reconstruction using an US research platform and a GPU-based edge device is developed.Results: The effectiveness of the proposed approach is demonstrated through ex-vivo and in-vivo experiments.Conclusion: The proposed approach with adjustable field of view capability, non-contact design, and low cost of deployment without significantly altering the existing setup would expand the applications of 3D US imaging in a wide range of clinical scenarios, particularly in pediatric populations.Significance: Traditional 2D US imaging has limited views of tissue structures, making diagnosis challenging. 3D US imaging overcomes this limitation by using 2D US images and their orientation/position to reconstruct 3D volumes. However, accurate probe position estimation at low cost has been a long-standing challenge in 3D reconstruction. Additionally, contact-based US imaging can be difficult for pediatric subjects, particularly when monitoring live fractures or open wounds. This novel framework addresses these challenges and has the potential to upgrade traditional systems for a wide range of 3D US imaging applications.In Simplified Chinese:目标：本研究的目标是开发一种新的、非接触、低成本的ultrasound（US）图像三维重建方法，以便在现有的点检查ultrasound（POCUS）系统上进行最小化的修改。方法：这种研究提议使用机械轨迹来实现非接触US扫描，这将简化获取和三维重建过程，并且只有在 linear 平面上进行探针运动。在US研究平台和GPU基于的边缘设备上开发了US三维图像重建的管道。结果：经过对外部和内部实验，效果表明了该方法的可行性。结论：该方法可以提供可调适的视场、非接触设计和低成本实施，不会对现有设置进行重大改变。这将扩展3D US图像 reconstruction在各种临床应用中的可能性，特别是在儿童人口中。重要性：传统的2D US图像有限制的视场，从而使诊断变得困难。3D US图像使用2D US图像和其orientation/position来重建3DVolume，从而突破这些限制。然而，在3D重建中准确地计算探针位置的低成本问题一直是一个挑战。此外，基于接触的US图像扫描可能会对儿童人口产生困难，特别是在监测活动骨折或开放性伤口时。这种新的框架可以解决这些挑战，并且有可能升级传统系统，以扩展3D US图像重建的应用范围。
</details></li>
</ul>
<hr>
<h2 id="Hessian-based-Similarity-Metric-for-Multimodal-Medical-Image-Registration"><a href="#Hessian-based-Similarity-Metric-for-Multimodal-Medical-Image-Registration" class="headerlink" title="Hessian-based Similarity Metric for Multimodal Medical Image Registration"></a>Hessian-based Similarity Metric for Multimodal Medical Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04009">http://arxiv.org/abs/2310.04009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Eskandari, Houssem-Eddine Gueziri, D. Louis Collins</li>
<li>for: 这个论文主要是为了提出一种新的医学影像匹配算法，用于衡量不同医学影像模式之间的相似性。</li>
<li>methods: 该论文使用了一种基于幂函数的方法，通过研究两个完美匹配的图像板块之间的偏微分关系，来量化它们之间的相似性。</li>
<li>results: 该论文通过实验表明，该新的相似性度量可以快速和精度地衡量不同医学影像模式之间的相似性，并且可以快速和精度地进行医学影像匹配。<details>
<summary>Abstract</summary>
One of the fundamental elements of both traditional and certain deep learning medical image registration algorithms is measuring the similarity/dissimilarity between two images. In this work, we propose an analytical solution for measuring similarity between two different medical image modalities based on the Hessian of their intensities. First, assuming a functional dependence between the intensities of two perfectly corresponding patches, we investigate how their Hessians relate to each other. Secondly, we suggest a closed-form expression to quantify the deviation from this relationship, given arbitrary pairs of image patches. We propose a geometrical interpretation of the new similarity metric and an efficient implementation for registration. We demonstrate the robustness of the metric to intensity nonuniformities using synthetic bias fields. By integrating the new metric in an affine registration framework, we evaluate its performance for MRI and ultrasound registration in the context of image-guided neurosurgery using target registration error and computation time.
</details>
<details>
<summary>摘要</summary>
一种基本元素 OF both traditional and certain deep learning医疗图像注册算法是测量两个图像之间的相似性/不同性。在这项工作中，我们提出了一个分析解决方案，用于测量两种不同医疗图像模式之间的相似性，基于图像强度的赫西安关系。首先，我们假设两个完美匹配的图像块之间存在函数依赖关系，然后我们研究了这两个赫西安之间的关系。其次，我们提出了一个具有closed-form表达式，用于衡量这种关系的偏差，给出任意两个图像块的对应。我们提出了一种几何解释这个新的相似度标准和一种高效的实现方式，并且在affine注册框架中集成了这个标准。我们通过使用模拟的扭曲场来证明metric的稳定性，并且在MRI和ultrasound注册中进行了image-guided neurosurgery的应用，通过target registration error和计算时间来评估metric的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/eess.IV_2023_10_06/" data-id="clp88dc5y01baob880o6fav40" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/eess.SP_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T08:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/eess.SP_2023_10_06/">eess.SP - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Learning-Based-Active-Spatial-Channel-Gain-Prediction-Using-a-Swarm-of-Unmanned-Aerial-Vehicles"><a href="#Deep-Learning-Based-Active-Spatial-Channel-Gain-Prediction-Using-a-Swarm-of-Unmanned-Aerial-Vehicles" class="headerlink" title="Deep Learning Based Active Spatial Channel Gain Prediction Using a Swarm of Unmanned Aerial Vehicles"></a>Deep Learning Based Active Spatial Channel Gain Prediction Using a Swarm of Unmanned Aerial Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04547">http://arxiv.org/abs/2310.04547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enes Krijestorac, Danijela Cabric</li>
<li>for: 预测无线通道增强（CG）在空间中的预测是许多重要无线网络设计问题的必需工具。本文开发了采用环境特定特征，即建筑地图和CG测量，以实现高精度预测的预测方法。</li>
<li>methods: 我们提出了两种活动预测方法，即基于深度学习（DL）和 Kriging  interpolación。第一种方法不依赖发送器位置，并利用3D地图补做不精确的预测。我们使用DL来 incorporate 3D maps into prediction和 reinforcement learning for optimal path planning for UAVs based on DL prediction。第二种方法基于 Kriging interpolación，需要知道发送器位置，而且不能使用3D地图。我们在一个基于射线追踪的通道模拟器中训练和评估两种提议的方法。</li>
<li>results: 我们通过 simulated experiments demonstrate the importance of active prediction compared to prediction based on randomly collected measurements of channel gain。另外，我们还表明使用 DL 和 3D maps，可以在不知道发送器位置的情况下实现高精度预测。 finally，我们还证明了在使用多个 UAVs 采集测量时，协调的路径规划对于活动预测具有重要的重要性。<details>
<summary>Abstract</summary>
Prediction of wireless channel gain (CG) across space is a necessary tool for many important wireless network design problems. In this paper, we develop prediction methods that use environment-specific features, namely building maps and CG measurements, to achieve high prediction accuracy. We assume that measurements are collected using a swarm of coordinated unmanned aerial vehicles (UAVs). We develop novel active prediction approaches which consist of both methods for UAV path planning for optimal measurement collection and methods for prediction of CG across space based on the collected measurements. We propose two active prediction approaches based on deep learning (DL) and Kriging interpolation. The first approach does not rely on the location of the transmitter and utilizes 3D maps to compensate for the lack of it. We utilize DL to incorporate 3D maps into prediction and reinforcement learning for optimal path planning for the UAVs based on DL prediction. The second active prediction approach is based on Kriging interpolation, which requires known transmitter location and cannot utilize 3D maps. We train and evaluate the two proposed approaches in a ray-tracing-based channel simulator. Using simulations, we demonstrate the importance of active prediction compared to prediction based on randomly collected measurements of channel gain. Furthermore, we show that using DL and 3D maps, we can achieve high prediction accuracy even without knowing the transmitter location. We also demonstrate the importance of coordinated path planning for active prediction when using multiple UAVs compared to UAVs collecting measurements independently in a greedy manner.
</details>
<details>
<summary>摘要</summary>
<<SYS>>按照以下准则进行简化中文翻译：1. 使用标准中文翻译词汇和 grammar2. 尽可能简化语句结构和表达3. 保留原文的意思和主题Prediction of wireless channel gain (CG) across space is a crucial tool for many important wireless network design problems. In this paper, we develop prediction methods that use environment-specific features, namely building maps and CG measurements, to achieve high prediction accuracy. We assume that measurements are collected using a swarm of coordinated unmanned aerial vehicles (UAVs). We develop novel active prediction approaches that consist of both methods for UAV path planning for optimal measurement collection and methods for prediction of CG across space based on the collected measurements. We propose two active prediction approaches based on deep learning (DL) and Kriging interpolation. The first approach does not rely on the location of the transmitter and utilizes 3D maps to compensate for the lack of it. We utilize DL to incorporate 3D maps into prediction and reinforcement learning for optimal path planning for the UAVs based on DL prediction. The second active prediction approach is based on Kriging interpolation, which requires known transmitter location and cannot utilize 3D maps. We train and evaluate the two proposed approaches in a ray-tracing-based channel simulator. Using simulations, we demonstrate the importance of active prediction compared to prediction based on randomly collected measurements of channel gain. Furthermore, we show that using DL and 3D maps, we can achieve high prediction accuracy even without knowing the transmitter location. We also demonstrate the importance of coordinated path planning for active prediction when using multiple UAVs compared to UAVs collecting measurements independently in a greedy manner.Translated text:预测无线通道增强（CG）在空间是许多重要无线网络设计问题中的必需工具。在这篇论文中，我们开发了预测方法，使用环境特定特征，namely building maps和CG测量，以实现高预测精度。我们假设测量是通过一群协调的无人飞行器（UAVs）进行收集。我们开发了两种活动预测方法，它们分别基于深度学习（DL）和 Kriging  interpolate。第一种方法不依赖发送器位置，并利用3D地图补偿发送器位置的缺失。我们利用 DL 将3D地图 incorporated into prediction，并通过强化学习对 UAVs 的路径规划进行优化，基于 DL 预测。第二种方法基于 Kriging interpolate，它需要知道发送器位置，而且无法使用3D地图。我们在一个基于投影法的通道模拟器中训练和评估了两种提议的方法。使用仿真，我们表明了活动预测比随机收集通道增强的预测更重要。此外，我们还表明了使用 DL 和3D地图，我们可以在不知道发送器位置的情况下实现高预测精度。我们还 demonstarted 多个 UAVs 协调的路径规划对活动预测的重要性。
</details></li>
</ul>
<hr>
<h2 id="Evolution-of-High-Throughput-Satellite-Systems-Vision-Requirements-and-Key-Technologies"><a href="#Evolution-of-High-Throughput-Satellite-Systems-Vision-Requirements-and-Key-Technologies" class="headerlink" title="Evolution of High Throughput Satellite Systems: Vision, Requirements, and Key Technologies"></a>Evolution of High Throughput Satellite Systems: Vision, Requirements, and Key Technologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04389">http://arxiv.org/abs/2310.04389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olfa Ben Yahia, Zineb Garroussi, Olivier Bélanger, Brunilde Sansò, Jean-François Frigon, Stéphane Martel, Antoine Lesage-Landry, Gunes Karabulut Kurt</li>
<li>For: The paper provides a comprehensive state-of-the-art of high throughput satellite (HTS) systems and envisions the next generation of extremely high-throughput satellite (EHTS) systems.* Methods: The paper discusses various techniques such as beamforming, advanced modulation techniques, reconfigurable phased array technologies, and electronically steerable antennas that are being used to improve the performance of HTS systems.* Results: The paper provides a vision for future EHTS systems that will maximize spectrum reuse and data rates, and flexibly steer capacity to satisfy user demand. Additionally, the paper introduces a novel architecture for future regenerative payloads and summarizes the challenges imposed by this architecture.<details>
<summary>Abstract</summary>
High throughput satellites (HTS), with their digital payload technology, are expected to play a key role as enablers of the upcoming 6G networks. HTS are mainly designed to provide higher data rates and capacities. Fueled by technological advancements including beamforming, advanced modulation techniques, reconfigurable phased array technologies, and electronically steerable antennas, HTS have emerged as a fundamental component for future network generation. This paper offers a comprehensive state-of-the-art of HTS systems, with a focus on standardization, patents, channel multiple access techniques, routing, load balancing, and the role of software-defined networking (SDN). In addition, we provide a vision for next-satellite systems that we named as extremely-HTS (EHTS) toward autonomous satellites supported by the main requirements and key technologies expected for these systems. The EHTS system will be designed such that it maximizes spectrum reuse and data rates, and flexibly steers the capacity to satisfy user demand. We introduce a novel architecture for future regenerative payloads while summarizing the challenges imposed by this architecture.
</details>
<details>
<summary>摘要</summary>
高通信率卫星（HTS）预计将扮演6G网络的关键激活器。HTS主要用于提供更高的数据速率和容量。驱动技术的进步，包括射频扫描、高级调制技术、可编程相位阵列技术和电子扫描天线，使HTS成为未来网络代表性的组件。本文提供了HTS系统的全面状态艺术，强调标准化、套件、通道多访问技术、路由、负荷均衡和软件定义网络（SDN）的角色。此外，我们还提出了下一代卫星系统，我们称之为“极高通信率卫星”（EHTS），该系统将具备自主卫星的主要需求和关键技术。EHTS系统将实现spectrum reuse和数据速率的最大化，并可以自动调整容量来满足用户需求。我们还介绍了未来复合 payload 的新架构，并总结了这种架构带来的挑战。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Backpressure-Routing-Using-Wireless-Link-Features"><a href="#Enhanced-Backpressure-Routing-Using-Wireless-Link-Features" class="headerlink" title="Enhanced Backpressure Routing Using Wireless Link Features"></a>Enhanced Backpressure Routing Using Wireless Link Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04364">http://arxiv.org/abs/2310.04364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongyuan Zhao, Gunjan Verma, Ananthram Swami, Santiago Segarra</li>
<li>for: 提高 wireless multi-hop 网络中分布式Routing和Scheduling的效率和延迟</li>
<li>methods: 使用 Biased BP 和短路寻址机制，不增加每次时间步骤的信号量 overhead</li>
<li>results: 提出了优化积分偏好、保持偏好在移动环境下、以及 incorporating sojourn time awareness into biased BP 等三个长期挑战，并通过分析和实验证明其效果。<details>
<summary>Abstract</summary>
Backpressure (BP) routing is a well-established framework for distributed routing and scheduling in wireless multi-hop networks. However, the basic BP scheme suffers from poor end-to-end delay due to the drawbacks of slow startup, random walk, and the last packet problem. Biased BP with shortest path awareness can address the first two drawbacks, and sojourn time-based backlog metrics were proposed for the last packet problem. Furthermore, these BP variations require no additional signaling overhead in each time step compared to the basic BP. In this work, we further address three long-standing challenges associated with the aforementioned low-cost BP variations, including optimal scaling of the biases, bias maintenance under mobility, and incorporating sojourn time awareness into biased BP. Our analysis and experimental results show that proper scaling of biases can be achieved with the help of common link features, which can effectively reduce end-to-end delay of BP by mitigating the random walk of packets under low-to-medium traffic, including the last packet scenario. In addition, our low-overhead bias maintenance scheme is shown to be effective under mobility, and our bio-inspired sojourn time-aware backlog metric is demonstrated to be more efficient and effective for the last packet problem than existing approaches when incorporated into biased BP.
</details>
<details>
<summary>摘要</summary>
背压路由（BP）是无线多项网络中分布路由和排程的一个成熟框架。然而，基本BP方案受到终端到终端延迟的问题，包括启动时间较慢、随机漫步和最后一个包问题。偏好BP可以解决首两个问题，而且可以使用游历时间-基础的伙伴度量来解决最后一个包问题。此外，这些BP变化不需要每个时间步骤中额外的讯号过程。在这个工作中，我们进一步解决了这些低成本BP变化的三个长期挑战，包括对偏好的优化维护、在移动环境中维护偏好以及将游历时间意识到偏好BP中。我们的分析和实验结果显示，正确地对偏好进行缩小可以使用通用链接特征来减少BP对终端的延迟，包括最后一个包enario。此外，我们的低负载维护方案在移动环境中是有效的，并且将游历时间意识到偏好BP中的方法比较高效和有效。
</details></li>
</ul>
<hr>
<h2 id="A-physics-informed-generative-model-for-passive-radio-frequency-sensing"><a href="#A-physics-informed-generative-model-for-passive-radio-frequency-sensing" class="headerlink" title="A physics-informed generative model for passive radio-frequency sensing"></a>A physics-informed generative model for passive radio-frequency sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04173">http://arxiv.org/abs/2310.04173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefano Savazzi, Federica Fieramosca, Sanaz Kianoush, Vittorio Rampa, Michele D’amico</li>
<li>for: 研究人员使用电romagnetic (EM) 体模型来预测无线设备附近的电磁波强度，并且应用于通信和位置测定等问题。</li>
<li>methods: 使用physics-informed生成神经网络 (GNN) 模型，将电磁波体diffraction方法 incorporated into variational autoencoder (VAE) 技术，以便模拟&#x2F;重建缺失的样本或学习受物理法则约束的数据分布。</li>
<li>results: 与传统diffraction-based EM body工具相比，提出的 EM-informed生成模型能够更好地预测真实的电磁波强度，并且在实际测量数据上验证了其有效性。<details>
<summary>Abstract</summary>
Electromagnetic (EM) body models predict the impact of human presence and motions on the Radio-Frequency (RF) stray radiation received by wireless devices nearby. These wireless devices may be co-located members of a Wireless Local Area Network (WLAN) or even cellular devices connected with a Wide Area Network (WAN). Despite their accuracy, EM models are time-consuming methods which prevent their adoption in strict real-time computational imaging problems and Bayesian estimation, such as passive localization, RF tomography, and holography. Physics-informed Generative Neural Network (GNN) models have recently attracted a lot of attention thanks to their potential to reproduce a process by incorporating relevant physical laws and constraints. Thus, GNNs can be used to simulate/reconstruct missing samples, or learn physics-informed data distributions. The paper discusses a Variational Auto-Encoder (VAE) technique and its adaptations to incorporate a relevant EM body diffraction method with applications to passive RF sensing and localization/tracking. The proposed EM-informed generative model is verified against classical diffraction-based EM body tools and validated on real RF measurements. Applications are also introduced and discussed.
</details>
<details>
<summary>摘要</summary>
电磁体（EM）模型预测人员存在和运动对附近无线设备接收的 радио频偏振（RF）杂谱的影响。这些无线设备可能是分布在同一个地方的无线本地网络（WLAN）成员或者连接到宽带网络（WAN）的无线设备。尽管它们的准确性很高，但EM模型是时间consuming的方法，这阻碍了它们在严格的实时计算图像问题和 bayesian估计中的采用。物理学 Informed Generative Neural Network（GNN）模型在最近吸引了很多关注，因为它们可以通过包含相关的物理法律和约束来重现一个过程。因此，GNN可以用来 simulate/重construct缺失的样本，或者学习物理学 Informed 数据分布。文章介绍了一种 Variational Auto-Encoder（VAE）技术和其修改，以包含相关的EM体 diffraction 方法，并应用于无线RF感知和定位/跟踪。提出的EM-informed生成模型被证明了 классиical diffraction-based EM体工具和实际RF测量。应用也是介绍和讨论的。
</details></li>
</ul>
<hr>
<h2 id="Physics-assisted-machine-learning-for-THz-spectroscopy-sensing-moisture-on-plant-leaves"><a href="#Physics-assisted-machine-learning-for-THz-spectroscopy-sensing-moisture-on-plant-leaves" class="headerlink" title="Physics-assisted machine learning for THz spectroscopy: sensing moisture on plant leaves"></a>Physics-assisted machine learning for THz spectroscopy: sensing moisture on plant leaves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04056">http://arxiv.org/abs/2310.04056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milan Koumans, Daan Meulendijks, Haiko Middeljans, Djero Peeters, Jacob C. Douma, Dook van Mechelen</li>
<li>for: 这个论文旨在用机器学习技术提高 THz 时间域спектроскопи亮度，以实现实用应用。</li>
<li>methods: 该论文使用了决策树和卷积神经网络等机器学习技术，基于光物理学知识进行辅助。</li>
<li>results: 研究人员通过对 12,000 个水pattern 的 THz 时间域数据进行分析，提出了关于决定水Pattern 的重要发现，并证明了这些模型在不同的测试集上的普适性。<details>
<summary>Abstract</summary>
Signal processing techniques are of vital importance to bring THz spectroscopy to a maturity level to reach practical applications. In this work, we illustrate the use of machine learning techniques for THz time-domain spectroscopy assisted by domain knowledge based on light-matter interactions. We aim at the potential agriculture application to determine the amount of free water on plant leaves, so-called leaf wetness. This quantity is important for understanding and predicting plant diseases that need leaf wetness for disease development. The overall transmission of a moist plant leaf for 12,000 distinct water patterns was experimentally acquired using THz time-domain spectroscopy. We report on key insights of applying decision trees and convolutional neural networks to the data using physics-motivated choices. Eventually, we discuss the generalizability of these models to determine leaf wetness after testing them on cases with increasing deviations from the training set.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Signal processing techniques are of vital importance to bring THz spectroscopy to a maturity level to reach practical applications. In this work, we illustrate the use of machine learning techniques for THz time-domain spectroscopy assisted by domain knowledge based on light-matter interactions. We aim at the potential agriculture application to determine the amount of free water on plant leaves, so-called leaf wetness. This quantity is important for understanding and predicting plant diseases that need leaf wetness for disease development. The overall transmission of a moist plant leaf for 12,000 distinct water patterns was experimentally acquired using THz time-domain spectroscopy. We report on key insights of applying decision trees and convolutional neural networks to the data using physics-motivated choices. Eventually, we discuss the generalizability of these models to determine leaf wetness after testing them on cases with increasing deviations from the training set." into Simplified Chinese.中文简体版：信号处理技术对于 THz спектроскопия的成熔度具有核心重要性，以实现实用应用。本工作介绍了基于光物理相互作用的机器学习技术在 THz 时域спектроскопии中的应用。我们target了农业应用，通过测量植物叶子上的自由水量，也称为叶质湿度。这个量对于理解和预测植物疾病非常重要，疾病发展需要叶质湿度。我们通过 THz 时域спектроскопии实验获得了12,000个不同水平的叶质湿度数据。我们使用决策树和卷积神经网络对数据进行分析，并根据物理原理进行选择。最后，我们讨论了这些模型在不同于训练集的情况下的泛化性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/eess.SP_2023_10_06/" data-id="clp88dc7q01fhob8827woe6r8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/05/cs.SD_2023_10_05/" class="article-date">
  <time datetime="2023-10-05T15:00:00.000Z" itemprop="datePublished">2023-10-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/05/cs.SD_2023_10_05/">cs.SD - 2023-10-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="EFFUSE-Efficient-Self-Supervised-Feature-Fusion-for-E2E-ASR-in-Multilingual-and-Low-Resource-Scenarios"><a href="#EFFUSE-Efficient-Self-Supervised-Feature-Fusion-for-E2E-ASR-in-Multilingual-and-Low-Resource-Scenarios" class="headerlink" title="EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Multilingual and Low Resource Scenarios"></a>EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Multilingual and Low Resource Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03938">http://arxiv.org/abs/2310.03938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tejes Srivastava, Jiatong Shi, William Chen, Shinji Watanabe</li>
<li>for: 提高多语言语音识别 task 的性能</li>
<li>methods: 使用 SSL 模型进行预测，并将多个 SSL 模型的特征进行预测</li>
<li>results: 提高了 ML-SUPERB  benchmarck 中的平均 SUPERB 分数，并且减少了模型参数大小和执行时间<details>
<summary>Abstract</summary>
Self-Supervised Learning (SSL) models have demonstrated exceptional performance in various speech tasks, particularly in low-resource and multilingual domains. Recent works show that fusing SSL models could achieve superior performance compared to using one SSL model. However, fusion models have increased model parameter size, leading to longer inference times. In this paper, we propose a novel approach of predicting other SSL models' features from a single SSL model, resulting in a light-weight framework with competitive performance. Our experiments show that SSL feature prediction models outperform individual SSL models in multilingual speech recognition tasks. The leading prediction model achieves an average SUPERB score increase of 135.4 in ML-SUPERB benchmarks. Moreover, our proposed framework offers an efficient solution, as it reduces the resulting model parameter size and inference times compared to previous fusion models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Challenges-and-Insights-Exploring-3D-Spatial-Features-and-Complex-Networks-on-the-MISP-Dataset"><a href="#Challenges-and-Insights-Exploring-3D-Spatial-Features-and-Complex-Networks-on-the-MISP-Dataset" class="headerlink" title="Challenges and Insights: Exploring 3D Spatial Features and Complex Networks on the MISP Dataset"></a>Challenges and Insights: Exploring 3D Spatial Features and Complex Networks on the MISP Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03901">http://arxiv.org/abs/2310.03901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Shao</li>
<li>for: 本研究旨在探讨多通道多人说话识别问题中，如干扰声、延迟和 overlap 等问题，以及如何通过 Contextual cues 来分离目标说话人的speech。</li>
<li>methods: 本研究使用了3D spatial feature，具体来说是通过计算目标说话人的位势信息来提高识别率。</li>
<li>results: 研究发现，通过使用3D spatial feature，可以减少或完全消除中间处理步骤，从而提高识别率。此外，对 MISP 数据集的扩展和模型的验证也表明了该方法的可行性和有效性。<details>
<summary>Abstract</summary>
Multi-channel multi-talker speech recognition presents formidable challenges in the realm of speech processing, marked by issues such as background noise, reverberation, and overlapping speech. Overcoming these complexities requires leveraging contextual cues to separate target speech from a cacophonous mix, enabling accurate recognition. Among these cues, the 3D spatial feature has emerged as a cutting-edge solution, particularly when equipped with spatial information about the target speaker. Its exceptional ability to discern the target speaker within mixed audio, often rendering intermediate processing redundant, paves the way for the direct training of "All-in-one" ASR models. These models have demonstrated commendable performance on both simulated and real-world data. In this paper, we extend this approach to the MISP dataset to further validate its efficacy. We delve into the challenges encountered and insights gained when applying 3D spatial features to MISP, while also exploring preliminary experiments involving the replacement of these features with more complex input and models.
</details>
<details>
<summary>摘要</summary>
多通道多发言人语音识别面临多种复杂性，如背景噪音、反射和 overlap 的问题。为了解决这些复杂性，需要利用上下文ual cue 分离目标语音从杂乱的混音中，以实现准确的识别。在这些上下文ual cue 中，3D 空间特征已经成为一种前导的解决方案，特别当配备空间信息目标说话人时。它的出色能力在杂乱的音频中分离目标说话人，经常使得中间处理redundant，从而降低了ASR模型的训练复杂性。在这篇论文中，我们将这种方法应用到MISP数据集，以进一步验证其效果。我们将详细介绍在应用3D 空间特征时遇到的挑战和获得的洞察，同时也将展开一些将这些特征更换为更复杂的输入和模型的先期实验。
</details></li>
</ul>
<hr>
<h2 id="Audio-Event-Relational-Graph-Representation-Learning-for-Acoustic-Scene-Classification"><a href="#Audio-Event-Relational-Graph-Representation-Learning-for-Acoustic-Scene-Classification" class="headerlink" title="Audio Event-Relational Graph Representation Learning for Acoustic Scene Classification"></a>Audio Event-Relational Graph Representation Learning for Acoustic Scene Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03889">http://arxiv.org/abs/2310.03889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Chuang Yu, Wenwu Wang, Dick Botteldooren</li>
<li>for: 本研究旨在揭示实际生活中各种听觉场景与语音事件关系图中的Semantic embedding的关系。</li>
<li>methods: 本研究提出了一种事件关系图表示学习（ERGL）框架，用于实现听觉场景分类，同时清晰地表明分类所用的cue。在事件关系图中，每个事件的嵌入被视为节点，而每对节点之间的关系cue被描述为多维边特征。</li>
<li>results: 在一个真实的听觉场景分类 dataset上，提出的ERGL方法实现了与限制数量的语音事件嵌入相关的高度竞争性表现。结果表明可以通过语音事件关系图来识别多样化的听觉场景。可见化的语音事件关系图 Representation可以在这里（<a target="_blank" rel="noopener" href="https://github.com/Yuanbo2020/ERGL%EF%BC%89">https://github.com/Yuanbo2020/ERGL）</a> obtener。<details>
<summary>Abstract</summary>
Most deep learning-based acoustic scene classification (ASC) approaches identify scenes based on acoustic features converted from audio clips containing mixed information entangled by polyphonic audio events (AEs). However, these approaches have difficulties in explaining what cues they use to identify scenes. This paper conducts the first study on disclosing the relationship between real-life acoustic scenes and semantic embeddings from the most relevant AEs. Specifically, we propose an event-relational graph representation learning (ERGL) framework for ASC to classify scenes, and simultaneously answer clearly and straightly which cues are used in classifying. In the event-relational graph, embeddings of each event are treated as nodes, while relationship cues derived from each pair of nodes are described by multi-dimensional edge features. Experiments on a real-life ASC dataset show that the proposed ERGL achieves competitive performance on ASC by learning embeddings of only a limited number of AEs. The results show the feasibility of recognizing diverse acoustic scenes based on the audio event-relational graph. Visualizations of graph representations learned by ERGL are available here (https://github.com/Yuanbo2020/ERGL).
</details>
<details>
<summary>摘要</summary>
大多数深度学习基于的声音场景分类（ASC）方法都是基于声音特征，将混合多种声音事件（AEs）转换为声音特征。然而，这些方法往往难以解释它们如何标识场景。本文提出了第一个研究声音场景与 Semantic Embeddings 之间的关系的研究，以及一种Event-Relational Graph Representation Learning（ERGL）框架，用于分类场景。在事件关系图中，每个事件的嵌入被视为节点，而每对节点之间的关系cue被描述为多维边feature。实验表明，提出的ERGL可以在一个有限数量的AEs上达到竞争力的ASC性能。结果表明，可以通过声音事件关系图来识别多样化的声音场景。可以在这里查看Visualization of ERGL学习的图表（https://github.com/Yuanbo2020/ERGL）。
</details></li>
</ul>
<hr>
<h2 id="Securing-Voice-Biometrics-One-Shot-Learning-Approach-for-Audio-Deepfake-Detection"><a href="#Securing-Voice-Biometrics-One-Shot-Learning-Approach-for-Audio-Deepfake-Detection" class="headerlink" title="Securing Voice Biometrics: One-Shot Learning Approach for Audio Deepfake Detection"></a>Securing Voice Biometrics: One-Shot Learning Approach for Audio Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03856">http://arxiv.org/abs/2310.03856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awais Khan, Khalid Mahmood Malik</li>
<li>for: 防止冒饵攻击 voice biometrics 系统，尤其是运用 audio deepfakes 进行逻辑存取攻击。</li>
<li>methods: 使用 one-shot learning 和 Metric Learning 技术探测和识别不同统计分布的 Synthetic 攻击，并使用有效的 спектраль特征集抽出有价的时间嵌入。</li>
<li>results: 在 ASVspoof 2019 逻辑存取（LA）数据集上评估了 Quick-SpoofNet 的表现，并在不同的 deepfake 攻击下进行了测试。实验结果显示 Quick-SpoofNet 能够具有高度的攻击探测率和优化的一致性。<details>
<summary>Abstract</summary>
The Automatic Speaker Verification (ASV) system is vulnerable to fraudulent activities using audio deepfakes, also known as logical-access voice spoofing attacks. These deepfakes pose a concerning threat to voice biometrics due to recent advancements in generative AI and speech synthesis technologies. While several deep learning models for speech synthesis detection have been developed, most of them show poor generalizability, especially when the attacks have different statistical distributions from the ones seen. Therefore, this paper presents Quick-SpoofNet, an approach for detecting both seen and unseen synthetic attacks in the ASV system using one-shot learning and metric learning techniques. By using the effective spectral feature set, the proposed method extracts compact and representative temporal embeddings from the voice samples and utilizes metric learning and triplet loss to assess the similarity index and distinguish different embeddings. The system effectively clusters similar speech embeddings, classifying bona fide speeches as the target class and identifying other clusters as spoofing attacks. The proposed system is evaluated using the ASVspoof 2019 logical access (LA) dataset and tested against unseen deepfake attacks from the ASVspoof 2021 dataset. Additionally, its generalization ability towards unseen bona fide speech is assessed using speech data from the VSDC dataset.
</details>
<details>
<summary>摘要</summary>
“自动话语识别（ASV）系统面临伪造活动的威胁，包括语音深圳攻击（Deepfake）。这些深圳攻击对话语音识别器具有潜在的威胁，因为近年来的生成AI和语音合成技术得到了进步。虽然许多深度学习模型用于语音合成检测已经发展出来，但大多数它们在不同的统计分布下显示出差。因此，这篇文章提出了快速攻击网络（Quick-SpoofNet），用于检测ASV系统中见到和未见到的合成攻击。这个方法使用有效的спектраль特征集，将声音样本中的时间特征提取出来，并使用度量学习和三重损失来评估相似性指数。系统可以划分相似的声音嵌入，将真正的话语识别为目标类别，并识别其他嵌入为伪造攻击。这个系统在ASVspoof 2019逻辑存取（LA）数据集上进行评估，并对未见到的深圳攻击进行测试。此外，它的普遍能力也被评估使用VSDC数据集上的话语数据。”
</details></li>
</ul>
<hr>
<h2 id="Speaker-localization-using-direct-path-dominance-test-based-on-sound-field-directivity"><a href="#Speaker-localization-using-direct-path-dominance-test-based-on-sound-field-directivity" class="headerlink" title="Speaker localization using direct path dominance test based on sound field directivity"></a>Speaker localization using direct path dominance test based on sound field directivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03688">http://arxiv.org/abs/2310.03688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boaz Rafaely, Koby Alhaiany</li>
<li>for: 这项研究的目的是开发一种robust to reverberation的DOA估计方法。</li>
<li>methods: 该方法基于时域频域分布的直接路径占据性测试，但不需要频率缓和矩阵分解。</li>
<li>results: 对比之前的方法，提议的方法在噪声和泛音条件下保持了相似的Robustness，并且计算效率高于原方法四倍。<details>
<summary>Abstract</summary>
Estimation of the direction-of-arrival (DoA) of a speaker in a room is important in many audio signal processing applications. Environments with reverberation that masks the DoA information are particularly challenging. Recently, a DoA estimation method that is robust to reverberation has been developed. This method identifies time-frequency bins dominated by the contribution from the direct path, which carries the correct DoA information. However, its implementation is computationally demanding as it requires frequency smoothing to overcome the effect of coherent early reflections and matrix decomposition to apply the direct-path dominance (DPD) test. In this work, a novel computationally-efficient alternative to the DPD test is proposed, based on the directivity measure for sensor arrays, which requires neither frequency smoothing nor matrix decomposition, and which has been reformulated for sound field directivity with spherical microphone arrays. The paper presents the proposed method and a comparison to previous methods under a range of reverberation and noise conditions. Result demonstrate that the proposed method shows comparable performance to the original method in terms of robustness to reverberation and noise, and is about four times more computationally efficient for the given experiment.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate( Estimation of the direction-of-arrival (DoA) of a speaker in a room is important in many audio signal processing applications. Environments with reverberation that masks the DoA information are particularly challenging. Recently, a DoA estimation method that is robust to reverberation has been developed. This method identifies time-frequency bins dominated by the contribution from the direct path, which carries the correct DoA information. However, its implementation is computationally demanding as it requires frequency smoothing to overcome the effect of coherent early reflections and matrix decomposition to apply the direct-path dominance (DPD) test. In this work, a novel computationally-efficient alternative to the DPD test is proposed, based on the directivity measure for sensor arrays, which requires neither frequency smoothing nor matrix decomposition, and which has been reformulated for sound field directivity with spherical microphone arrays. The paper presents the proposed method and a comparison to previous methods under a range of reverberation and noise conditions. Result demonstrate that the proposed method shows comparable performance to the original method in terms of robustness to reverberation and noise, and is about four times more computationally efficient for the given experiment. )中文简体版：<<SYS>>音频信号处理应用中，确定发声者的方向来（DoA）在房间中非常重要。尤其是在听到延迟响应的环境中，DoA信息会被遮盖。最近，一种可以在延迟响应的环境中具有高Robustness的DoA估算方法已经被开发出来。这种方法可以在时域频域中标识由直接路径提供的DoA信息的占据率。然而，它的实现具有计算挺大的问题，需要频率平滑以超越协同早期反射的效果，并且需要矩阵分解来应用直通性测试。在这项工作中，一种新的计算高效的代替方法被提出，基于探测阵列的直接性度，不需要频率平滑也不需要矩阵分解。这种方法的实现可以在圆形 Mikrofon 阵列上进行 reformulation。文章介绍了该方法，并对之前的方法进行比较，包括各种噪声和延迟的条件下的性能。结果表明，该方法在robustness和计算效率方面与原始方法相似，且计算效率高于原始方法四倍。
</details></li>
</ul>
<hr>
<h2 id="Performance-and-energy-balance-a-comprehensive-study-of-state-of-the-art-sound-event-detection-systems"><a href="#Performance-and-energy-balance-a-comprehensive-study-of-state-of-the-art-sound-event-detection-systems" class="headerlink" title="Performance and energy balance: a comprehensive study of state-of-the-art sound event detection systems"></a>Performance and energy balance: a comprehensive study of state-of-the-art sound event detection systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03455">http://arxiv.org/abs/2310.03455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ronfrancesca/sed_carbon_footprint">https://github.com/ronfrancesca/sed_carbon_footprint</a></li>
<li>paper_authors: Francesca Ronchini, Romain Serizel</li>
<li>for: 这篇研究旨在探讨深度学习系统中增加复杂性和能耗问题的趋势，以及这些系统对环境的影响。</li>
<li>methods: 本研究使用了过去两年的探测和分类响应挑战 зада务的提交作为基础，进行比较和详细分析。</li>
<li>results: 研究发现，过去两年中深度学习系统的复杂性和能耗问题有所增加，并且这些系统对环境的影响也逐渐增加。<details>
<summary>Abstract</summary>
In recent years, deep learning systems have shown a concerning trend toward increased complexity and higher energy consumption. As researchers in this domain and organizers of one of the Detection and Classification of Acoustic Scenes and Events challenges tasks, we recognize the importance of addressing the environmental impact of data-driven SED systems. In this paper, we propose an analysis focused on SED systems based on the challenge submissions. This includes a comparison across the past two years and a detailed analysis of this year's SED systems. Through this research, we aim to explore how the SED systems are evolving every year in relation to their energy efficiency implications.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习系统在复杂性和能耗方面表现出了担忧的趋势。作为这个领域的研究人员和挑战任务组织者之一，我们认为对数据驱动的SED系统环境影响的问题非常重要。在这篇论文中，我们提出了基于挑战提交的SED系统分析。包括过去两年的比较和本年SED系统的详细分析。通过这些研究，我们希望探讨每年SED系统在能效环境方面的发展趋势。
</details></li>
</ul>
<hr>
<h2 id="VaSAB-The-variable-size-adaptive-information-bottleneck-for-disentanglement-on-speech-and-singing-voice"><a href="#VaSAB-The-variable-size-adaptive-information-bottleneck-for-disentanglement-on-speech-and-singing-voice" class="headerlink" title="VaSAB: The variable size adaptive information bottleneck for disentanglement on speech and singing voice"></a>VaSAB: The variable size adaptive information bottleneck for disentanglement on speech and singing voice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03444">http://arxiv.org/abs/2310.03444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederik Bous, Axel Roebel</li>
<li>for: voice transformation, disentanglement of F0 parameter</li>
<li>methods: dropout-based information bottleneck auto-encoder, adaptive bottleneck size</li>
<li>results: improved disentanglement of F0 parameter for both speech and singing voice, improved synthesis quality, universal voice model for both speech and singing voice<details>
<summary>Abstract</summary>
The information bottleneck auto-encoder is a tool for disentanglement commonly used for voice transformation. The successful disentanglement relies on the right choice of bottleneck size. Previous bottleneck auto-encoders created the bottleneck by the dimension of the latent space or through vector quantization and had no means to change the bottleneck size of a specific model. As the bottleneck removes information from the disentangled representation, the choice of bottleneck size is a trade-off between disentanglement and synthesis quality. We propose to build the information bottleneck using dropout which allows us to change the bottleneck through the dropout rate and investigate adapting the bottleneck size depending on the context. We experimentally explore into using the adaptive bottleneck for pitch transformation and demonstrate that the adaptive bottleneck leads to improved disentanglement of the F0 parameter for both, speech and singing voice leading to improved synthesis quality. Using the variable bottleneck size, we were able to achieve disentanglement for singing voice including extremely high pitches and create a universal voice model, that works on both speech and singing voice with improved synthesis quality.
</details>
<details>
<summary>摘要</summary>
信息瓶颈自适应Encoder是一种常用的分解工具，通常用于音频变换。成功的分解取决于瓶颈大小的选择。过去的瓶颈自适应Encoder通过缺省空间维度或VECTOR量化来创建瓶颈，而无法改变特定模型中的瓶颈大小。因为瓶颈从分解表示中移除信息，因此瓶颈大小的选择是一种负担很大的负担，即分解和生成质量之间的权衡。我们提议使用dropout来构建信息瓶颈，这allow us可以通过dropout率来改变瓶颈大小，并且在不同的上下文中进行调整。我们通过实验explore使用可变瓶颈大小来进行音高变换，并证明可变瓶颈可以提高F0参数的分解，并且对于语音和歌唱voice都可以提高生成质量。使用可变瓶颈大小，我们可以实现对歌唱voice的分解，包括极高的音高，并创建一个通用的语音模型，可以在语音和歌唱voice上进行改进的生成。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/05/cs.SD_2023_10_05/" data-id="clp88dc1r010job886vge36i5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/05/eess.AS_2023_10_05/" class="article-date">
  <time datetime="2023-10-05T14:00:00.000Z" itemprop="datePublished">2023-10-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/05/eess.AS_2023_10_05/">eess.AS - 2023-10-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Latent-Filling-Latent-Space-Data-Augmentation-for-Zero-shot-Speech-Synthesis"><a href="#Latent-Filling-Latent-Space-Data-Augmentation-for-Zero-shot-Speech-Synthesis" class="headerlink" title="Latent Filling: Latent Space Data Augmentation for Zero-shot Speech Synthesis"></a>Latent Filling: Latent Space Data Augmentation for Zero-shot Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03538">http://arxiv.org/abs/2310.03538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jae-Sung Bae, Joun Yeop Lee, Ji-Hyun Lee, Seongkyu Mun, Taehwa Kang, Hoon-Young Cho, Chanwoo Kim</li>
<li>for: 提高 zero-shot text-to-speech（ZS-TTS）系统的性能</li>
<li>methods: 使用简单而有效的幽合空间数据增强方法（Latent Filling，LF），在ZS-TTS系统的 speaker embedding 空间中进行数据增强</li>
<li>results: LF 能够提高 speaker 相似性，同时保持 speech 质量<details>
<summary>Abstract</summary>
Previous works in zero-shot text-to-speech (ZS-TTS) have attempted to enhance its systems by enlarging the training data through crowd-sourcing or augmenting existing speech data. However, the use of low-quality data has led to a decline in the overall system performance. To avoid such degradation, instead of directly augmenting the input data, we propose a latent filling (LF) method that adopts simple but effective latent space data augmentation in the speaker embedding space of the ZS-TTS system. By incorporating a consistency loss, LF can be seamlessly integrated into existing ZS-TTS systems without the need for additional training stages. Experimental results show that LF significantly improves speaker similarity while preserving speech quality.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is a standardized form of Chinese that uses shorter words and sentences, and is often used in informal writing and online communication. The translation above uses Simplified Chinese characters and grammar.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/05/eess.AS_2023_10_05/" data-id="clp88dc3f014mob88gxh54n9c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/05/cs.CV_2023_10_05/" class="article-date">
  <time datetime="2023-10-05T13:00:00.000Z" itemprop="datePublished">2023-10-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/05/cs.CV_2023_10_05/">cs.CV - 2023-10-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Diffusion-Models-as-Masked-Audio-Video-Learners"><a href="#Diffusion-Models-as-Masked-Audio-Video-Learners" class="headerlink" title="Diffusion Models as Masked Audio-Video Learners"></a>Diffusion Models as Masked Audio-Video Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03937">http://arxiv.org/abs/2310.03937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elvis Nunez, Yanzi Jin, Mohammad Rastegari, Sachin Mehta, Maxwell Horton</li>
<li>for: 这篇论文是为了探讨采用散射模型与MAViL架构的协同运作，以实现更好的音频-视频表现。</li>
<li>methods: 这篇论文使用了Masked Audio-Video Learners（MAViL）架构，结合了对比学习和对应预测，将音频spectrogram和视频帧组合在一起，并且使用散射模型对应视频帧。</li>
<li>results: 这篇论文的结果显示，通过将散射模型与MAViL架构结合，可以实现32%的预训操作数量（FLOPS）和18%的预训时间（wall clock time）的减少，并且不会对音频类别 зада对的表现造成影响。<details>
<summary>Abstract</summary>
Over the past several years, the synchronization between audio and visual signals has been leveraged to learn richer audio-visual representations. Aided by the large availability of unlabeled videos, many unsupervised training frameworks have demonstrated impressive results in various downstream audio and video tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a state-of-the-art audio-video pre-training framework. MAViL couples contrastive learning with masked autoencoding to jointly reconstruct audio spectrograms and video frames by fusing information from both modalities. In this paper, we study the potential synergy between diffusion models and MAViL, seeking to derive mutual benefits from these two frameworks. The incorporation of diffusion into MAViL, combined with various training efficiency methodologies that include the utilization of a masking ratio curriculum and adaptive batch sizing, results in a notable 32% reduction in pre-training Floating-Point Operations (FLOPS) and an 18% decrease in pre-training wall clock time. Crucially, this enhanced efficiency does not compromise the model's performance in downstream audio-classification tasks when compared to MAViL's performance.
</details>
<details>
<summary>摘要</summary>
在过去几年，听视同步学习被利用来学习更加 ricah 的听视信号表示。由于大量的无标视频数据的可用性，许多无监督训练框架在各种下游听视任务中表现出色。最近，Masked Audio-Video Learners（MAViL）在听视预训练框架中崛起为state-of-the-art。MAViL将对听视信号和视频帧进行对比学习和压缩编码，通过两种Modalities的信息融合来重建听视spectrogram和视频帧。在这篇文章中，我们研究了diffusion模型和MAViL之间的可能的共识，以寻找这两个框架之间的互惠关系。通过将diffusion incorporated into MAViL，并结合多种训练效率技术，包括使用masquerade ratio curriculum和adaptive batch sizing，我们得到了一个明显的32%的预训练FLOPS减少和18%的预训练wall clock time减少。幸运的是，这些提高的效率不会影响模型在下游听视分类任务中的性能，与MAViL的性能相比。
</details></li>
</ul>
<hr>
<h2 id="Open-Fusion-Real-time-Open-Vocabulary-3D-Mapping-and-Queryable-Scene-Representation"><a href="#Open-Fusion-Real-time-Open-Vocabulary-3D-Mapping-and-Queryable-Scene-Representation" class="headerlink" title="Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation"></a>Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03923">http://arxiv.org/abs/2310.03923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UARK-AICV/OpenFusion">https://github.com/UARK-AICV/OpenFusion</a></li>
<li>paper_authors: Kashu Yamazaki, Taisei Hanyu, Khoa Vo, Thang Pham, Minh Tran, Gianfranco Doretto, Anh Nguyen, Ngan Le</li>
<li>for: 这篇论文旨在提出一种实时开放词汇3D环境地图创建和可询问场景表示方法，使用RGB-D数据。</li>
<li>methods: 该方法利用预训练的视觉语言基础模型（VLFM）进行开放集semantic解决，并使用TSDF快速生成3D场景重建。</li>
<li>results: 对于ScanNet数据集，Open-Fusion的表现明显超过了领先的零shot方法，并且可以实现无需额外3D训练的注释自由3D分割。此外，Open-Fusion通过结合区域基于VLFM和TSDF的优化hungarian特征匹配机制，实现了实时3D场景理解，包括物体概念和开放世界 semantics。Here’s the English version of the three key information points for reference:</li>
<li>for: This paper proposes a real-time open-vocabulary 3D environmental mapping method using RGB-D data, which is groundbreaking.</li>
<li>methods: The method utilizes a pre-trained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction.</li>
<li>results: The results on the ScanNet dataset demonstrate that Open-Fusion outperforms leading zero-shot methods and can achieve annotation-free 3D segmentation without additional 3D training. Additionally, Open-Fusion seamlessly combines the strengths of region-based VLFM and TSDF, enabling real-time 3D scene comprehension that includes object concepts and open-world semantics.<details>
<summary>Abstract</summary>
Precise 3D environmental mapping is pivotal in robotics. Existing methods often rely on predefined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, a groundbreaking approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pre-trained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D segmentation for open-vocabulary without necessitating additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: https://uark-aicv.github.io/OpenFusion
</details>
<details>
<summary>摘要</summary>
precisions 3D 环境地图是 robotics 中关键的。现有方法 oftentimes rely on predefined concepts during training or are time-consuming when generating semantic maps。这篇文章 introduce Open-Fusion， a groundbreaking approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data。Open-Fusion 利用 pre-trained vision-language foundation model (VLFM) for open-set semantic comprehension，并使用 Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction。通过 leveraging VLFM，我们可以 extract region-based embeddings and their associated confidence maps。这些是 then integrated with 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism。 worth noting that Open-Fusion delivers outstanding annotation-free 3D segmentation for open-vocabulary without requiring additional 3D training。 benchmark tests on ScanNet dataset against leading zero-shot methods highlight Open-Fusion's superiority。 Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics。readers can view demos on our project page: https://uark-aicv.github.io/OpenFusion
</details></li>
</ul>
<hr>
<h2 id="Coloring-Deep-CNN-Layers-with-Activation-Hue-Loss"><a href="#Coloring-Deep-CNN-Layers-with-Activation-Hue-Loss" class="headerlink" title="Coloring Deep CNN Layers with Activation Hue Loss"></a>Coloring Deep CNN Layers with Activation Hue Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03911">http://arxiv.org/abs/2310.03911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis-François Bouchard, Mohsen Ben Lazreg, Matthew Toews</li>
<li>for: 这篇论文旨在提出一种新的深度卷积神经网络（CNN）活动空间模型，即“活动色彩”，以优化模型进行更有效的学习。</li>
<li>methods: 该论文使用了一种基于 nearest neighbor indexing of activation vectors 的方法，发现类信息强度集中于某个角度 $\theta$ 处， both in $(x,y)$ 图像平面和多通道活动空间。 论文还提出了一种使用活动色彩标签的偏置项来补偿标准一采样损失。</li>
<li>results: 论文通过在多种分类任务上从零开始训练，包括 ImageNet，发现了一些有限的改进。<details>
<summary>Abstract</summary>
This paper proposes a novel hue-like angular parameter to model the structure of deep convolutional neural network (CNN) activation space, referred to as the {\em activation hue}, for the purpose of regularizing models for more effective learning. The activation hue generalizes the notion of color hue angle in standard 3-channel RGB intensity space to $N$-channel activation space. A series of observations based on nearest neighbor indexing of activation vectors with pre-trained networks indicate that class-informative activations are concentrated about an angle $\theta$ in both the $(x,y)$ image plane and in multi-channel activation space. A regularization term in the form of hue-like angular $\theta$ labels is proposed to complement standard one-hot loss. Training from scratch using combined one-hot + activation hue loss improves classification performance modestly for a wide variety of classification tasks, including ImageNet.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种新的深度 convolutional neural network (CNN) 正则化方法，基于一种新的概念——“活动色”。活动色是一种模型深度 CNN 的活动空间结构的方法，它基于标准的3通道 RGB 强度空间中的色度角的想法。作者们提议使用一种类似于色度角的angular标签来补充标准的一个零一个Hot损失函数，并表明这种方法可以提高各种分类任务的分类性能，包括 ImageNet。
</details></li>
</ul>
<hr>
<h2 id="TWICE-Dataset-Digital-Twin-of-Test-Scenarios-in-a-Controlled-Environment"><a href="#TWICE-Dataset-Digital-Twin-of-Test-Scenarios-in-a-Controlled-Environment" class="headerlink" title="TWICE Dataset: Digital Twin of Test Scenarios in a Controlled Environment"></a>TWICE Dataset: Digital Twin of Test Scenarios in a Controlled Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03895">http://arxiv.org/abs/2310.03895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Novicki Neto, Fabio Reway, Yuri Poledna, Maikol Funk Drechsler, Eduardo Parente Ribeiro, Werner Huber, Christian Icking</li>
<li>for: 本研究是为了提供一个实际测试车道和实验室中重现的恶势力天气下自动驾驶车辆安全可靠运行的数据集。</li>
<li>methods: 该数据集包括了摄像头、雷达、LiDAR、自转仪和GPS数据，并在恶势力天气条件下进行了多种测试场景，包括雨天、夜间和雪天等。</li>
<li>results: 该数据集包含了超过2小时的录制数据，总量超过280GB，因此是自动驾驶领域研究人员测试和改进算法的宝贵资源，同时也可以探索 simulation-to-reality  gap。数据集可以在以下地址下载：<a target="_blank" rel="noopener" href="https://twicedataset.github.io/site/">https://twicedataset.github.io/site/</a><details>
<summary>Abstract</summary>
Ensuring the safe and reliable operation of autonomous vehicles under adverse weather remains a significant challenge. To address this, we have developed a comprehensive dataset composed of sensor data acquired in a real test track and reproduced in the laboratory for the same test scenarios. The provided dataset includes camera, radar, LiDAR, inertial measurement unit (IMU), and GPS data recorded under adverse weather conditions (rainy, night-time, and snowy conditions). We recorded test scenarios using objects of interest such as car, cyclist, truck and pedestrian -- some of which are inspired by EURONCAP (European New Car Assessment Programme). The sensor data generated in the laboratory is acquired by the execution of simulation-based tests in hardware-in-the-loop environment with the digital twin of each real test scenario. The dataset contains more than 2 hours of recording, which totals more than 280GB of data. Therefore, it is a valuable resource for researchers in the field of autonomous vehicles to test and improve their algorithms in adverse weather conditions, as well as explore the simulation-to-reality gap. The dataset is available for download at: https://twicedataset.github.io/site/
</details>
<details>
<summary>摘要</summary>
Ensuring the safe and reliable operation of autonomous vehicles under adverse weather remains a significant challenge. To address this, we have developed a comprehensive dataset composed of sensor data acquired in a real test track and reproduced in the laboratory for the same test scenarios. The provided dataset includes camera, radar, LiDAR, inertial measurement unit (IMU), and GPS data recorded under adverse weather conditions (rainy, night-time, and snowy conditions). We recorded test scenarios using objects of interest such as car, cyclist, truck and pedestrian -- some of which are inspired by EURONCAP (European New Car Assessment Programme). The sensor data generated in the laboratory is acquired by the execution of simulation-based tests in hardware-in-the-loop environment with the digital twin of each real test scenario. The dataset contains more than 2 hours of recording, which totals more than 280GB of data. Therefore, it is a valuable resource for researchers in the field of autonomous vehicles to test and improve their algorithms in adverse weather conditions, as well as explore the simulation-to-reality gap. The dataset is available for download at: https://twicedataset.github.io/site/Here's the word-for-word translation of the text into Simplified Chinese: Ensuring the safe and reliable operation of autonomous vehicles under adverse weather remains a significant challenge. To address this, we have developed a comprehensive dataset composed of sensor data acquired in a real test track and reproduced in the laboratory for the same test scenarios. The provided dataset includes camera, radar, LiDAR, inertial measurement unit (IMU), and GPS data recorded under adverse weather conditions (rainy, night-time, and snowy conditions). We recorded test scenarios using objects of interest such as car, cyclist, truck and pedestrian -- some of which are inspired by EURONCAP (European New Car Assessment Programme). The sensor data generated in the laboratory is acquired by the execution of simulation-based tests in hardware-in-the-loop environment with the digital twin of each real test scenario. The dataset contains more than 2 hours of recording, which totals more than 280GB of data. Therefore, it is a valuable resource for researchers in the field of autonomous vehicles to test and improve their algorithms in adverse weather conditions, as well as explore the simulation-to-reality gap. The dataset is available for download at: https://twicedataset.github.io/site/
</details></li>
</ul>
<hr>
<h2 id="Characterizing-the-Features-of-Mitotic-Figures-Using-a-Conditional-Diffusion-Probabilistic-Model"><a href="#Characterizing-the-Features-of-Mitotic-Figures-Using-a-Conditional-Diffusion-Probabilistic-Model" class="headerlink" title="Characterizing the Features of Mitotic Figures Using a Conditional Diffusion Probabilistic Model"></a>Characterizing the Features of Mitotic Figures Using a Conditional Diffusion Probabilistic Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03893">http://arxiv.org/abs/2310.03893</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cagladbahadir/dpm-for-mitotic-figures">https://github.com/cagladbahadir/dpm-for-mitotic-figures</a></li>
<li>paper_authors: Cagla Deniz Bahadir, Benjamin Liechty, David J. Pisapia, Mert R. Sabuncu</li>
<li>for: 本研究旨在描述mitosis标注的不确定性和分类任务的人类可读性特征。</li>
<li>methods: 我们使用泛化噪声模型生成相同核心变换为mitosis标注的Synthetic图像序列，以便识别不同的核心特征，如细胞核粒体粗糙度、核心密度、核心异常和核心与细胞体的对比度。</li>
<li>results: 我们的方法可以帮助病理学家更好地理解和解释决策所需的特征。<details>
<summary>Abstract</summary>
Mitotic figure detection in histology images is a hard-to-define, yet clinically significant task, where labels are generated with pathologist interpretations and where there is no ``gold-standard'' independent ground-truth. However, it is well-established that these interpretation based labels are often unreliable, in part, due to differences in expertise levels and human subjectivity. In this paper, our goal is to shed light on the inherent uncertainty of mitosis labels and characterize the mitotic figure classification task in a human interpretable manner. We train a probabilistic diffusion model to synthesize patches of cell nuclei for a given mitosis label condition. Using this model, we can then generate a sequence of synthetic images that correspond to the same nucleus transitioning into the mitotic state. This allows us to identify different image features associated with mitosis, such as cytoplasm granularity, nuclear density, nuclear irregularity and high contrast between the nucleus and the cell body. Our approach offers a new tool for pathologists to interpret and communicate the features driving the decision to recognize a mitotic figure.
</details>
<details>
<summary>摘要</summary>
In this paper, we aim to shed light on the inherent uncertainty of mitosis labels and characterize the mitotic figure classification task in a human-interpretable manner. We train a probabilistic diffusion model to synthesize patches of cell nuclei for a given mitosis label condition. By using this model, we can generate a sequence of synthetic images that correspond to the same nucleus transitioning into the mitotic state. This allows us to identify different image features associated with mitosis, such as cytoplasm granularity, nuclear density, nuclear irregularity, and high contrast between the nucleus and the cell body.Our approach offers a new tool for pathologists to interpret and communicate the features driving the decision to recognize a mitotic figure.
</details></li>
</ul>
<hr>
<h2 id="FNOSeg3D-Resolution-Robust-3D-Image-Segmentation-with-Fourier-Neural-Operator"><a href="#FNOSeg3D-Resolution-Robust-3D-Image-Segmentation-with-Fourier-Neural-Operator" class="headerlink" title="FNOSeg3D: Resolution-Robust 3D Image Segmentation with Fourier Neural Operator"></a>FNOSeg3D: Resolution-Robust 3D Image Segmentation with Fourier Neural Operator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03872">http://arxiv.org/abs/2310.03872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/multimodal-3d-image-segmentation">https://github.com/ibm/multimodal-3d-image-segmentation</a></li>
<li>paper_authors: Ken C. L. Wong, Hongzhi Wang, Tanveer Syeda-Mahmood</li>
<li>for: 这个论文主要是为了解决深度学习在医疗三维静止图像分割中的计算复杂性问题，通过使用下采样的图像进行训练，但是这会导致模型在原始分辨率下的准确性下降。</li>
<li>methods: 这个论文提出了一种基于干扰 нейронOperator（FNO）的3D分割模型，该模型具有零批量超分辨率和全局响应场的特点。 authors 通过减少参数量和加入循环连接和深度监督来改进FNO，从而实现了具有少量参数和高精度的FNOSeg3D模型。</li>
<li>results: 对于BraTS’19数据集，FNOSeg3D模型在不同的训练图像分辨率下表现出了较好的鲁棒性，与其他测试模型相比，它的参数量少于1%。<details>
<summary>Abstract</summary>
Due to the computational complexity of 3D medical image segmentation, training with downsampled images is a common remedy for out-of-memory errors in deep learning. Nevertheless, as standard spatial convolution is sensitive to variations in image resolution, the accuracy of a convolutional neural network trained with downsampled images can be suboptimal when applied on the original resolution. To address this limitation, we introduce FNOSeg3D, a 3D segmentation model robust to training image resolution based on the Fourier neural operator (FNO). The FNO is a deep learning framework for learning mappings between functions in partial differential equations, which has the appealing properties of zero-shot super-resolution and global receptive field. We improve the FNO by reducing its parameter requirement and enhancing its learning capability through residual connections and deep supervision, and these result in our FNOSeg3D model which is parameter efficient and resolution robust. When tested on the BraTS'19 dataset, it achieved superior robustness to training image resolution than other tested models with less than 1% of their model parameters.
</details>
<details>
<summary>摘要</summary>
The FNO is a deep learning framework for learning mappings between functions in partial differential equations, which has the advantages of zero-shot super-resolution and global receptive field. We improve the FNO by reducing its parameter requirement and enhancing its learning capability through residual connections and deep supervision, resulting in our FNOSeg3D model. This model is parameter efficient and resolution robust, achieving superior performance compared to other models with less than 1% of their parameters when tested on the BraTS'19 dataset.
</details></li>
</ul>
<hr>
<h2 id="Consistency-Regularization-Improves-Placenta-Segmentation-in-Fetal-EPI-MRI-Time-Series"><a href="#Consistency-Regularization-Improves-Placenta-Segmentation-in-Fetal-EPI-MRI-Time-Series" class="headerlink" title="Consistency Regularization Improves Placenta Segmentation in Fetal EPI MRI Time Series"></a>Consistency Regularization Improves Placenta Segmentation in Fetal EPI MRI Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03870">http://arxiv.org/abs/2310.03870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/firstmover/cr-seg">https://github.com/firstmover/cr-seg</a></li>
<li>paper_authors: Yingcheng Liu, Neerav Karani, Neel Dey, S. Mazdak Abulnaga, Junshen Xu, P. Ellen Grant, Esra Abaci Turk, Polina Golland</li>
<li>for: 这个论文旨在提高围绕胎动成像的胎盘三维自动分割精度，以便提高先天医疗的诊断和治疗。</li>
<li>methods: 该论文提出了一种有效的半监督学习方法，使用了一种协调正则化损失函数来提高三维胎盘分割的精度。</li>
<li>results: 实验结果表明，该方法可以提高总分割精度，并且对于异常样本和困难样本表现更好。此外，该方法还可以提高时间序列中的预测准确性，这将有助于更 preciselly 计算胎盘生物标志物。<details>
<summary>Abstract</summary>
The placenta plays a crucial role in fetal development. Automated 3D placenta segmentation from fetal EPI MRI holds promise for advancing prenatal care. This paper proposes an effective semi-supervised learning method for improving placenta segmentation in fetal EPI MRI time series. We employ consistency regularization loss that promotes consistency under spatial transformation of the same image and temporal consistency across nearby images in a time series. The experimental results show that the method improves the overall segmentation accuracy and provides better performance for outliers and hard samples. The evaluation also indicates that our method improves the temporal coherency of the prediction, which could lead to more accurate computation of temporal placental biomarkers. This work contributes to the study of the placenta and prenatal clinical decision-making. Code is available at https://github.com/firstmover/cr-seg.
</details>
<details>
<summary>摘要</summary>
placenta 在胎儿发展中扮演重要角色。自动化3D placenta分割从胎儿EPI MRI序列图像中提取placenta信息可能提高产前照管。本文提出一种有效的半指导学习方法，用于提高胎儿EPI MRI时序序列中placenta分割精度。我们使用一种协调常量损失函数，以便在同一张图像中的不同位置和邻近图像时序序列中保持一致性。实验结果表明，该方法可以提高总的分割精度，并且对于异常样本和困难样本表现更好。评估还表明，我们的方法可以提高计算时间序列placental生物标志的准确性。这种研究对于胎儿和产前诊断具有重要意义。代码可以在https://github.com/firstmover/cr-seg上获取。
</details></li>
</ul>
<hr>
<h2 id="OpenIncrement-A-Unified-Framework-for-Open-Set-Recognition-and-Deep-Class-Incremental-Learning"><a href="#OpenIncrement-A-Unified-Framework-for-Open-Set-Recognition-and-Deep-Class-Incremental-Learning" class="headerlink" title="OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning"></a>OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03848">http://arxiv.org/abs/2310.03848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gawainxu/openincremen">https://github.com/gawainxu/openincremen</a></li>
<li>paper_authors: Jiawen Xu, Claas Grohnfeldt, Odej Kao</li>
<li>for: 这篇论文是用于探讨深度增量学习的问题，特别是当 Novel samples 是预先识别的时候， neural network 的重训可能会导致错误的预测。</li>
<li>methods: 本文提出了一个基于 open set recognition 的深度类别增量学习框架，并将类别增量学习的特征整合到了距离基本 open set recognition。</li>
<li>results: 实验结果显示，我们的方法在比较于现有的增量学习技术的情况下，表现出了更好的性能，并且在 open set recognition 方面比基于方法表现更好。<details>
<summary>Abstract</summary>
In most works on deep incremental learning research, it is assumed that novel samples are pre-identified for neural network retraining. However, practical deep classifiers often misidentify these samples, leading to erroneous predictions. Such misclassifications can degrade model performance. Techniques like open set recognition offer a means to detect these novel samples, representing a significant area in the machine learning domain.   In this paper, we introduce a deep class-incremental learning framework integrated with open set recognition. Our approach refines class-incrementally learned features to adapt them for distance-based open set recognition. Experimental results validate that our method outperforms state-of-the-art incremental learning techniques and exhibits superior performance in open set recognition compared to baseline methods.
</details>
<details>
<summary>摘要</summary>
多数深度增量学习研究中假设新样本已经被预先标识，以供神经网络重新训练。然而，实际的深度分类器经常错误地分类这些样本，导致错误预测。这种错误分类可能会降低模型性能。开集识别技术提供了检测这些新样本的方式，这是机器学习领域的一个重要领域。在这篇论文中，我们介绍了一种集成了开集识别的深度分类器增量学习框架。我们的方法可以将增量学习得到的特征进行修正，以适应距离基于开集识别。实验结果表明，我们的方法比状态数据增量学习技术更高效，并在基准方法比较下表现出超越性。
</details></li>
</ul>
<hr>
<h2 id="Less-is-More-On-the-Feature-Redundancy-of-Pretrained-Models-When-Transferring-to-Few-shot-Tasks"><a href="#Less-is-More-On-the-Feature-Redundancy-of-Pretrained-Models-When-Transferring-to-Few-shot-Tasks" class="headerlink" title="Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks"></a>Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03843">http://arxiv.org/abs/2310.03843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Luo, Difan Zou, Lianli Gao, Zenglin Xu, Jingkuan Song</li>
<li>For: 这篇论文的目的是研究如何将预训练模型转移到下游任务中，以便在几 shot 的情况下提高模型的性能。* Methods: 这篇论文使用了线性探测方法，即在预训练模型中固化特征后，使用特征来训练一个线性分类器。然而，在下游数据中存在差距，因此这篇论文提出了问题：预训练特征中的哪些维度是有用的？* Results: 研究发现，在几 shot 的情况下，预训练特征可以很 redundant，即使只使用1%的最重要维度，也可以恢复使用全个表示的性能。此外，研究还发现，这种 redundancy 在几 shot 的情况下是非常明显的，而且随着数据量的增加，这种 redundancy 逐渐消失。这种现象的理论理解和解释，以及如何通过软掩码来解决这种问题，都是这篇论文的重要贡献。<details>
<summary>Abstract</summary>
Transferring a pretrained model to a downstream task can be as easy as conducting linear probing with target data, that is, training a linear classifier upon frozen features extracted from the pretrained model. As there may exist significant gaps between pretraining and downstream datasets, one may ask whether all dimensions of the pretrained features are useful for a given downstream task. We show that, for linear probing, the pretrained features can be extremely redundant when the downstream data is scarce, or few-shot. For some cases such as 5-way 1-shot tasks, using only 1\% of the most important feature dimensions is able to recover the performance achieved by using the full representation. Interestingly, most dimensions are redundant only under few-shot settings and gradually become useful when the number of shots increases, suggesting that feature redundancy may be the key to characterizing the "few-shot" nature of few-shot transfer problems. We give a theoretical understanding of this phenomenon and show how dimensions with high variance and small distance between class centroids can serve as confounding factors that severely disturb classification results under few-shot settings. As an attempt at solving this problem, we find that the redundant features are difficult to identify accurately with a small number of training samples, but we can instead adjust feature magnitude with a soft mask based on estimated feature importance. We show that this method can generally improve few-shot transfer performance across various pretrained models and downstream datasets.
</details>
<details>
<summary>摘要</summary>
传播预训模型到下游任务可以非常简单，只需要在目标数据上进行线性探测，即在预训模型中冻结特征后，训练一个线性分类器。由于预训和下游数据集之间可能存在很大差距，因此我们可能会问到，预训特征中的所有维度都是下游任务中有用吗。我们发现，在线性探测情况下，预训特征可以非常重复，特别是在scarce或少shot情况下。例如，在5种类1个shot任务中，只使用1%的最重要特征维度可以恢复使用全表示性能。 Interestingly, most dimensions are redundant only under few-shot settings and gradually become useful when the number of shots increases, suggesting that feature redundancy may be the key to characterizing the "few-shot" nature of few-shot transfer problems. We give a theoretical understanding of this phenomenon and show how dimensions with high variance and small distance between class centroids can serve as confounding factors that severely disturb classification results under few-shot settings. As an attempt at solving this problem, we find that the redundant features are difficult to identify accurately with a small number of training samples, but we can instead adjust feature magnitude with a soft mask based on estimated feature importance. We show that this method can generally improve few-shot transfer performance across various pretrained models and downstream datasets.
</details></li>
</ul>
<hr>
<h2 id="HartleyMHA-Self-Attention-in-Frequency-Domain-for-Resolution-Robust-and-Parameter-Efficient-3D-Image-Segmentation"><a href="#HartleyMHA-Self-Attention-in-Frequency-Domain-for-Resolution-Robust-and-Parameter-Efficient-3D-Image-Segmentation" class="headerlink" title="HartleyMHA: Self-Attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation"></a>HartleyMHA: Self-Attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04466">http://arxiv.org/abs/2310.04466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/multimodal-3d-image-segmentation">https://github.com/ibm/multimodal-3d-image-segmentation</a></li>
<li>paper_authors: Ken C. L. Wong, Hongzhi Wang, Tanveer Syeda-Mahmood</li>
<li>for: 提高3D图像分割中自动注意力模型的训练缓存效率，避免训练图像大小过小导致的准确性下降。</li>
<li>methods: 基于FNO框架，使用 Hartley transform 和共享参数来减少模型大小，并在频域中应用自注意力以提高表达能力和效率。</li>
<li>results: 在BraTS’19数据集上测试， HartleyMHA 模型可以与其他模型相比，在训练图像大小不同情况下保持超过99%的准确性，而且具有训练缓存效率的优势。<details>
<summary>Abstract</summary>
With the introduction of Transformers, different attention-based models have been proposed for image segmentation with promising results. Although self-attention allows capturing of long-range dependencies, it suffers from a quadratic complexity in the image size especially in 3D. To avoid the out-of-memory error during training, input size reduction is usually required for 3D segmentation, but the accuracy can be suboptimal when the trained models are applied on the original image size. To address this limitation, inspired by the Fourier neural operator (FNO), we introduce the HartleyMHA model which is robust to training image resolution with efficient self-attention. FNO is a deep learning framework for learning mappings between functions in partial differential equations, which has the appealing properties of zero-shot super-resolution and global receptive field. We modify the FNO by using the Hartley transform with shared parameters to reduce the model size by orders of magnitude, and this allows us to further apply self-attention in the frequency domain for more expressive high-order feature combination with improved efficiency. When tested on the BraTS'19 dataset, it achieved superior robustness to training image resolution than other tested models with less than 1% of their model parameters.
</details>
<details>
<summary>摘要</summary>
“受到变换器引入后，不同的注意力基于模型在图像分割方面提出了出色的结果。虽然自注意力允许捕捉长距离依赖关系，但它在图像大小上具有二次复杂性，特别是在3D分割中。为了避免训练过程中的内存出错，通常需要降低输入图像大小，但在原始图像大小上应用已经训练的模型时，准确率可能会受到限制。为解决这个局限性，我们引入了HartleyMHA模型，它具有高效的自注意力和鲁棒性。FNO是一种深度学习框架，用于学习函数partial differential equations中的映射，它具有透明的特性，如零shot超解像和全球响应区。我们通过使用Hartley变换和共享参数来降低模型大小，从而使得自注意力在频域中进行更加有表达力的高阶特征组合，并且提高了效率。在BraTS'19数据集上测试时，它与其他测试模型相比，具有更好的训练图像分辨率鲁棒性，仅占其模型参数的0.1%。”
</details></li>
</ul>
<hr>
<h2 id="Integrating-Audio-Visual-Features-for-Multimodal-Deepfake-Detection"><a href="#Integrating-Audio-Visual-Features-for-Multimodal-Deepfake-Detection" class="headerlink" title="Integrating Audio-Visual Features for Multimodal Deepfake Detection"></a>Integrating Audio-Visual Features for Multimodal Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03827">http://arxiv.org/abs/2310.03827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sneha Muppalla, Shan Jia, Siwei Lyu</li>
<li>for: 本研究旨在提出一种音视频基于的deepfake检测方法，以提高对多模态检测的精度。</li>
<li>methods: 本方法结合细致的deepfake标识与二分类算法，将样本分为四类，并对带内和跨域测试进行提升。</li>
<li>results: 实验结果表明，该方法在多模态检测中显著提高了检测精度，并在带内和跨域测试中具有优异表现。<details>
<summary>Abstract</summary>
Deepfakes are AI-generated media in which an image or video has been digitally modified. The advancements made in deepfake technology have led to privacy and security issues. Most deepfake detection techniques rely on the detection of a single modality. Existing methods for audio-visual detection do not always surpass that of the analysis based on single modalities. Therefore, this paper proposes an audio-visual-based method for deepfake detection, which integrates fine-grained deepfake identification with binary classification. We categorize the samples into four types by combining labels specific to each single modality. This method enhances the detection under intra-domain and cross-domain testing.
</details>
<details>
<summary>摘要</summary>
深圳技术是由人工智能修改的图像或视频。随着深圳技术的发展， privacy和安全问题得到了关注。大多数深圳检测技术都是基于单一模式的检测。现有的音频视频检测方法不总能超越单个模式的分析。因此，本文提出了一种音频视频基于的深圳检测方法，该方法将细致的深圳标识与二分类结合。我们将样本分为四类，通过将每个单模式的标签结合。这种方法可以在同频和交叉频测试中提高检测精度。
</details></li>
</ul>
<hr>
<h2 id="WLST-Weak-Labels-Guided-Self-training-for-Weakly-supervised-Domain-Adaptation-on-3D-Object-Detection"><a href="#WLST-Weak-Labels-Guided-Self-training-for-Weakly-supervised-Domain-Adaptation-on-3D-Object-Detection" class="headerlink" title="WLST: Weak Labels Guided Self-training for Weakly-supervised Domain Adaptation on 3D Object Detection"></a>WLST: Weak Labels Guided Self-training for Weakly-supervised Domain Adaptation on 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03821">http://arxiv.org/abs/2310.03821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacky121298/WLST">https://github.com/jacky121298/WLST</a></li>
<li>paper_authors: Tsung-Lin Tsou, Tsung-Han Wu, Winston H. Hsu</li>
<li>for: 提高3D物体检测中的领域适应性，特别是在无目标标注的情况下。</li>
<li>methods: 提出了一种通用的弱标签导向自教学框架（WLST），利用自动标签器生成3D假标签，以提高目标频谱的训练过程。</li>
<li>results: 经验证明，我们的WLST框架可以提高3D物体检测中的领域适应性，并且在所有评价任务上表现出色，超过了之前的状态作法。<details>
<summary>Abstract</summary>
In the field of domain adaptation (DA) on 3D object detection, most of the work is dedicated to unsupervised domain adaptation (UDA). Yet, without any target annotations, the performance gap between the UDA approaches and the fully-supervised approach is still noticeable, which is impractical for real-world applications. On the other hand, weakly-supervised domain adaptation (WDA) is an underexplored yet practical task that only requires few labeling effort on the target domain. To improve the DA performance in a cost-effective way, we propose a general weak labels guided self-training framework, WLST, designed for WDA on 3D object detection. By incorporating autolabeler, which can generate 3D pseudo labels from 2D bounding boxes, into the existing self-training pipeline, our method is able to generate more robust and consistent pseudo labels that would benefit the training process on the target domain. Extensive experiments demonstrate the effectiveness, robustness, and detector-agnosticism of our WLST framework. Notably, it outperforms previous state-of-the-art methods on all evaluation tasks.
</details>
<details>
<summary>摘要</summary>
在三维 объек目检测领域中的领域适应（DA）中，大多数工作集中在无监督领域适应（UDA）上。然而，无法获得目标域标注，DA方法与完全监督方法之间的性能差距仍然存在，这对实际应用来说是不实际的。相反，弱监督领域适应（WDA）是一个未得到充分发掘的 yet practical task，只需要少量的标注努力在目标域上。为了提高DA性能，我们提出了一个通用的弱标签指导自学习框架，WLST，设计为WDA在三维对象检测中进行。通过将自动标签器，可以生成3Dpseudo标签从2D bounding box，加入现有的自学习管道中，我们的方法可以生成更加稳定和一致的pseudo标签，这将对目标域训练过程中帮助提高DA性能。广泛的实验表明我们的WLST框架的有效性、稳定性和检测器免疫性。特别是，它超过了之前的状态 искусственный方法在所有评估任务上。
</details></li>
</ul>
<hr>
<h2 id="ContactGen-Generative-Contact-Modeling-for-Grasp-Generation"><a href="#ContactGen-Generative-Contact-Modeling-for-Grasp-Generation" class="headerlink" title="ContactGen: Generative Contact Modeling for Grasp Generation"></a>ContactGen: Generative Contact Modeling for Grasp Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03740">http://arxiv.org/abs/2310.03740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevenlsw/contactgen">https://github.com/stevenlsw/contactgen</a></li>
<li>paper_authors: Shaowei Liu, Yang Zhou, Jimei Yang, Saurabh Gupta, Shenlong Wang</li>
<li>for: 这个论文旨在提出一种基于物体中心的接触表示方法，以便更好地模型手部与物体之间的交互。</li>
<li>methods: 该方法包括三个组件：接触地图显示接触位置，部分地图表示接触手部，方向地图表示每个部分中的接触方向。给定输入物体，我们提出了一种 conditional generative 模型，以便预测接触地图并采用模型基于优化来预测多种具有多样性和几何可能性的抓取。</li>
<li>results: 实验结果表明，我们的方法可以生成高精度和多样性的人类抓取，并且适用于各种物体。项目页面：<a target="_blank" rel="noopener" href="https://stevenlsw.github.io/contactgen/">https://stevenlsw.github.io/contactgen/</a><details>
<summary>Abstract</summary>
This paper presents a novel object-centric contact representation ContactGen for hand-object interaction. The ContactGen comprises three components: a contact map indicates the contact location, a part map represents the contact hand part, and a direction map tells the contact direction within each part. Given an input object, we propose a conditional generative model to predict ContactGen and adopt model-based optimization to predict diverse and geometrically feasible grasps. Experimental results demonstrate our method can generate high-fidelity and diverse human grasps for various objects. Project page: https://stevenlsw.github.io/contactgen/
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的物体呈现中心的接触表示方法ContactGen，用于手对象交互。ContactGen包括三个组成部分：接触地图显示接触位置，手部地图表示接触手部，以及每个部分的方向地图。给定输入物体，我们提议一种条件生成模型预测ContactGen，并采用模型基于优化预测多种具有多样性和几何可行性的抓取。实验结果表明我们的方法可以生成高精度和多样的人类抓取对象。项目页面：https://stevenlsw.github.io/contactgen/Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Stylist-Style-Driven-Feature-Ranking-for-Robust-Novelty-Detection"><a href="#Stylist-Style-Driven-Feature-Ranking-for-Robust-Novelty-Detection" class="headerlink" title="Stylist: Style-Driven Feature Ranking for Robust Novelty Detection"></a>Stylist: Style-Driven Feature Ranking for Robust Novelty Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03738">http://arxiv.org/abs/2310.03738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu</li>
<li>for: 检测样本是否具有新鲜度，但不是所有变化都是重要的。</li>
<li>methods: 使用形式化分为Semantic（有用）和Style（无用）变化，并且使用预训练大规模模型表示来提高抗性。</li>
<li>results: 提出了Stylist方法，可以去除环境偏见的特征，提高新鲜度检测性能。经验表明，在多个数据集上，Stylist方法可以提高新鲜度检测性能，并且可以处理 conten 和 style 类型的变化。<details>
<summary>Abstract</summary>
Novelty detection aims at finding samples that differ in some form from the distribution of seen samples. But not all changes are created equal. Data can suffer a multitude of distribution shifts, and we might want to detect only some types of relevant changes. Similar to works in out-of-distribution generalization, we propose to use the formalization of separating into semantic or content changes, that are relevant to our task, and style changes, that are irrelevant. Within this formalization, we define the robust novelty detection as the task of finding semantic changes while being robust to style distributional shifts. Leveraging pretrained, large-scale model representations, we introduce Stylist, a novel method that focuses on dropping environment-biased features. First, we compute a per-feature score based on the feature distribution distances between environments. Next, we show that our selection manages to remove features responsible for spurious correlations and improve novelty detection performance. For evaluation, we adapt domain generalization datasets to our task and analyze the methods behaviors. We additionally built a large synthetic dataset where we have control over the spurious correlations degree. We prove that our selection mechanism improves novelty detection algorithms across multiple datasets, containing both stylistic and content shifts.
</details>
<details>
<summary>摘要</summary>
To achieve this, we leverage pre-trained, large-scale model representations and introduce Stylist, a novel method that focuses on dropping environment-biased features. We first compute a per-feature score based on the feature distribution distances between environments. We show that our selection mechanism removes features responsible for spurious correlations and improves novelty detection performance.For evaluation, we adapt domain generalization datasets to our task and analyze the methods' behaviors. We also built a large synthetic dataset where we have control over the degree of spurious correlations. Our results prove that our selection mechanism improves novelty detection algorithms across multiple datasets, containing both stylistic and content shifts.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Unpaired-Data-for-Vision-Language-Generative-Models-via-Cycle-Consistency"><a href="#Leveraging-Unpaired-Data-for-Vision-Language-Generative-Models-via-Cycle-Consistency" class="headerlink" title="Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency"></a>Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03734">http://arxiv.org/abs/2310.03734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianhong Li, Sangnie Bhardwaj, Yonglong Tian, Han Zhang, Jarred Barber, Dina Katabi, Guillaume Lajoie, Huiwen Chang, Dilip Krishnan</li>
<li>for: 提高vision-language生成模型的性能和泛化能力，使其能够在无需大量对应图像和文本数据的情况下进行训练和推断。</li>
<li>methods: 提出了一种新的训练方法，称为ITIT（图像-文本同步训练），它基于循环一致性的概念，可以在无需对应图像和文本数据的情况下进行图像-文本训练。</li>
<li>results: 实验表明，ITIT可以与高质量对应图像和文本数据进行训练，并且可以达到与现有的文本-图像模型相当的性能，只需要orders of magnitude fewer paired image-text data。<details>
<summary>Abstract</summary>
Current vision-language generative models rely on expansive corpora of paired image-text data to attain optimal performance and generalization capabilities. However, automatically collecting such data (e.g. via large-scale web scraping) leads to low quality and poor image-text correlation, while human annotation is more accurate but requires significant manual effort and expense. We introduce $\textbf{ITIT}$ ($\textbf{I}$n$\textbf{T}$egrating $\textbf{I}$mage $\textbf{T}$ext): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data. ITIT is comprised of a joint image-text encoder with disjoint image and text decoders that enable bidirectional image-to-text and text-to-image generation in a single framework. During training, ITIT leverages a small set of paired image-text data to ensure its output matches the input reasonably well in both directions. Simultaneously, the model is also trained on much larger datasets containing only images or texts. This is achieved by enforcing cycle consistency between the original unpaired samples and the cycle-generated counterparts. For instance, it generates a caption for a given input image and then uses the caption to create an output image, and enforces similarity between the input and output images. Our experiments show that ITIT with unpaired datasets exhibits similar scaling behavior as using high-quality paired data. We demonstrate image generation and captioning performance on par with state-of-the-art text-to-image and image-to-text models with orders of magnitude fewer (only 3M) paired image-text data.
</details>
<details>
<summary>摘要</summary>
Current vision-language生成模型依赖广泛的图像文本资料来 достичь最佳性能和泛化能力。然而，自动从网页抓取大规模图像文本资料会导致低品质和差强的图像文本相关性，而人工标注更加精准但需要较大的人工努力和成本。我们介绍了 $\textbf{ITIT}$（$\textbf{I}$ntegrating $\textbf{I}$mage $\textbf{T}$ext）：一种创新的训练方案基于循环一致的概念，允许vision-language训练在不对应图像文本资料上。ITIT包括一个共同图像文本编码器和分开的图像和文本解oder，允许两向的图像文本生成。在训练过程中，ITIT利用一小量的对应图像文本资料来确保其输出与输入相对对应。同时，模型还被训练在包含很多图像或文本资料的更大 datasets 中。这是通过强制循环一致的原始无对应样本和循环生成的对应样本之间的一致性来实现的。例如，它将一个输入图像的描述生成为一个图像，并将这个图像与输入图像进行比较，以确保它们之间的一致性。我们的实验显示，ITIT可以与高品质的对应资料一样具有推广性。我们在实验中使用了只有300万对应图像文本资料，并且可以达到与现有的文本至图像和图像至文本模型相同的表现。
</details></li>
</ul>
<hr>
<h2 id="OMG-ATTACK-Self-Supervised-On-Manifold-Generation-of-Transferable-Evasion-Attacks"><a href="#OMG-ATTACK-Self-Supervised-On-Manifold-Generation-of-Transferable-Evasion-Attacks" class="headerlink" title="OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks"></a>OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03707">http://arxiv.org/abs/2310.03707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ofir Bar Tal, Adi Haviv, Amit H. Bermano</li>
<li>for: 测试神经网络的可靠性，使用欺骗攻击（Evasion Attacks）对神经网络进行测试。</li>
<li>methods: 使用自我指导的、计算机 эконоical的方法生成敌意攻击，采用表示学习技术，生成在数据分布上的攻击。</li>
<li>results: 对不同的模型、数据类别和防御模型进行了实验，显示了该方法的效果， suggessting on-manifold EAs 在对未看过模型的攻击中具有重要作用。<details>
<summary>Abstract</summary>
Evasion Attacks (EA) are used to test the robustness of trained neural networks by distorting input data to misguide the model into incorrect classifications. Creating these attacks is a challenging task, especially with the ever-increasing complexity of models and datasets. In this work, we introduce a self-supervised, computationally economical method for generating adversarial examples, designed for the unseen black-box setting. Adapting techniques from representation learning, our method generates on-manifold EAs that are encouraged to resemble the data distribution. These attacks are comparable in effectiveness compared to the state-of-the-art when attacking the model trained on, but are significantly more effective when attacking unseen models, as the attacks are more related to the data rather than the model itself. Our experiments consistently demonstrate the method is effective across various models, unseen data categories, and even defended models, suggesting a significant role for on-manifold EAs when targeting unseen models.
</details>
<details>
<summary>摘要</summary>
逃脱攻击（EA）用于测试训练过的神经网络robustness，通过扭曲输入数据来诱导模型进行错误分类。创建这些攻击是一项复杂的任务，特别是随着模型和数据集的复杂度不断增加。在这项工作中，我们提出了一种自动supervised，computational economical的方法，用于生成黑盒 setting下的敌意例子。我们采用了表示学习技术，使得我们的方法可以在数据分布上生成在敌意例子。这些攻击效果相当于state-of-the-art，但是在训练过的模型上更有效果，而在未看过模型上更有效果，因为这些攻击更加 relate to the data 而不是模型自身。我们的实验表明，这种方法在不同的模型、未seen data category和even defended models中具有显著的作用， suggesting a significant role for on-manifold EAs when targeting unseen models。
</details></li>
</ul>
<hr>
<h2 id="Drag-View-Generalizable-Novel-View-Synthesis-with-Unposed-Imagery"><a href="#Drag-View-Generalizable-Novel-View-Synthesis-with-Unposed-Imagery" class="headerlink" title="Drag View: Generalizable Novel View Synthesis with Unposed Imagery"></a>Drag View: Generalizable Novel View Synthesis with Unposed Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03704">http://arxiv.org/abs/2310.03704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Hanwen Jiang, Dejia Xu, Zehao Zhu, Dilin Wang, Zhangyang Wang</li>
<li>for:  DragView is designed for generating novel views of unseen scenes from a single source image, with the ability to handle occlusion and flexible camera trajectories.</li>
<li>methods: DragView uses a sparse set of unposed multi-view images, a view-dependent modulation layer, and a transformer to decode ray features into final pixel intensities, all executed within a single feed-forward pass.</li>
<li>results: DragView showcases the capability to generalize to new scenes unseen during training, and consistently demonstrates superior performance in view synthesis quality compared to recent scene representation networks and generalizable NeRFs.<details>
<summary>Abstract</summary>
We introduce DragView, a novel and interactive framework for generating novel views of unseen scenes. DragView initializes the new view from a single source image, and the rendering is supported by a sparse set of unposed multi-view images, all seamlessly executed within a single feed-forward pass. Our approach begins with users dragging a source view through a local relative coordinate system. Pixel-aligned features are obtained by projecting the sampled 3D points along the target ray onto the source view. We then incorporate a view-dependent modulation layer to effectively handle occlusion during the projection. Additionally, we broaden the epipolar attention mechanism to encompass all source pixels, facilitating the aggregation of initialized coordinate-aligned point features from other unposed views. Finally, we employ another transformer to decode ray features into final pixel intensities. Crucially, our framework does not rely on either 2D prior models or the explicit estimation of camera poses. During testing, DragView showcases the capability to generalize to new scenes unseen during training, also utilizing only unposed support images, enabling the generation of photo-realistic new views characterized by flexible camera trajectories. In our experiments, we conduct a comprehensive comparison of the performance of DragView with recent scene representation networks operating under pose-free conditions, as well as with generalizable NeRFs subject to noisy test camera poses. DragView consistently demonstrates its superior performance in view synthesis quality, while also being more user-friendly. Project page: https://zhiwenfan.github.io/DragView/.
</details>
<details>
<summary>摘要</summary>
我们介绍DragView，一种新的和交互式框架，用于生成未被见过的场景视图。DragView从单个源图像初始化新视图，并且渲染是基于一个稀缺的多视图图像支持，完全在单个往返传播中执行。我们的方法开始于用户将源视图拖动到本地相对坐标系中。通过对抽样的3D点进行 projetction，从源视图中获得齐平的特征点。然后，我们添加了视角依赖的修饰层，以有效地处理 occlusion  durante la proyección。此外，我们扩展了 epipolar 注意力机制，以覆盖所有源像素，使得自 initialize 协调对齐点特征从其他无法 pose 视图中提取 initialized 协调点特征。最后，我们使用另一个 transformer 来解码轨道特征为最终像素强度。关键是，我们的框架不依赖于2D先验模型或自动确定相机位置。在测试时，DragView 显示了能够泛化到未在训练过程中看到的新场景，并且只使用无法 pose 支持图像，以生成 photo-realistic 的新视图，其中 camera 轨迹具有灵活性。在我们的实验中，我们对 DragView 的性能进行了对比，包括最近的场景表示网络在无法 pose 条件下的性能，以及一些受到噪音测试相机位置的 Generalizable NeRF 的性能。DragView 在视图合成质量方面 consistently 表现出色，而且更加 user-friendly。项目页面：https://zhiwenfan.github.io/DragView/.
</details></li>
</ul>
<hr>
<h2 id="LumiNet-The-Bright-Side-of-Perceptual-Knowledge-Distillation"><a href="#LumiNet-The-Bright-Side-of-Perceptual-Knowledge-Distillation" class="headerlink" title="LumiNet: The Bright Side of Perceptual Knowledge Distillation"></a>LumiNet: The Bright Side of Perceptual Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03669">http://arxiv.org/abs/2310.03669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Ismail Hossain, M M Lutfe Elahi, Sameera Ramasinghe, Ali Cheraghian, Fuad Rahman, Nabeel Mohammed, Shafin Rahman</li>
<li>for: 该论文主要研究了基于logit的知识填充方法，以强化知识传递的能力。</li>
<li>methods: 该方法提出了一个名为LumiNet的新型知识传递算法，通过调整logits来提高学生模型对教师模型的学习。</li>
<li>results: 对于CIFAR-100、ImageNet和MSCOCO等基准数据集，LumiNet的实验结果表明其与当前的特征基于方法相比具有竞争力。此外，通过在不同任务下进行传输学习，该方法还能够强化学生模型在下游任务中的适应能力。<details>
<summary>Abstract</summary>
In knowledge distillation research, feature-based methods have dominated due to their ability to effectively tap into extensive teacher models. In contrast, logit-based approaches are considered to be less adept at extracting hidden 'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel knowledge-transfer algorithm designed to enhance logit-based distillation. We introduce a perception matrix that aims to recalibrate logits through adjustments based on the model's representation capability. By meticulously analyzing intra-class dynamics, LumiNet reconstructs more granular inter-class relationships, enabling the student model to learn a richer breadth of knowledge. Both teacher and student models are mapped onto this refined matrix, with the student's goal being to minimize representational discrepancies. Rigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO) attests to LumiNet's efficacy, revealing its competitive edge over leading feature-based methods. Moreover, in exploring the realm of transfer learning, we assess how effectively the student model, trained using our method, adapts to downstream tasks. Notably, when applied to Tiny ImageNet, the transferred features exhibit remarkable performance, further underscoring LumiNet's versatility and robustness in diverse settings. With LumiNet, we hope to steer the research discourse towards a renewed interest in the latent capabilities of logit-based knowledge distillation.
</details>
<details>
<summary>摘要</summary>
在知识储备研究中，基于特征的方法长期占据主导地位，这可能是因为它们能够有效地利用了大量的教师模型。然而，基于幂数的方法被视为无法充分激发教师模型中隐藏的“黑知识”。为了bridging这个差距，我们提出了LumiNet，一种新的知识传递算法，旨在通过调整幂数来增强基于幂数的储备。我们引入了一个感知矩阵，用于重新塑造幂数，以便通过对模型表示能力的调整来激发更多的隐藏知识。两个模型都被映射到这个矩阵上，学生模型的目标是减少表示差异。我们在CIFAR-100、ImageNet和MSCOCO等标准测试集上进行了严格的测试，并证明LumiNet的效果，其与主流基于特征的方法相比，显示出竞争力。此外，我们还进行了对下游任务的探索，并发现通过我们的方法进行训练后，学生模型在Tiny ImageNet上表现出了很好的性能，这再次证明了LumiNet在多种设置下的多样性和稳定性。我们希望通过LumiNet，激发研究者对基于幂数的知识储备的新兴兴趣。
</details></li>
</ul>
<hr>
<h2 id="Certification-of-Deep-Learning-Models-for-Medical-Image-Segmentation"><a href="#Certification-of-Deep-Learning-Models-for-Medical-Image-Segmentation" class="headerlink" title="Certification of Deep Learning Models for Medical Image Segmentation"></a>Certification of Deep Learning Models for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03664">http://arxiv.org/abs/2310.03664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Othmane Laousy, Alexandre Araujo, Guillaume Chassagnon, Nikos Paragios, Marie-Pierre Revel, Maria Vakalopoulou</li>
<li>for: 针对医疗影像分割模型，提供了首次的证明过程。</li>
<li>methods: 基于渐进推 diffusion 模型和随机缩放的方法。</li>
<li>results: 对五个公共静止肺X射线、皮肤癌和直肠内视镜数据集进行了广泛的实验，并观察到可以保持高度证明的 dice 分数，即使图像受到了很高水平的干扰。<details>
<summary>Abstract</summary>
In medical imaging, segmentation models have known a significant improvement in the past decade and are now used daily in clinical practice. However, similar to classification models, segmentation models are affected by adversarial attacks. In a safety-critical field like healthcare, certifying model predictions is of the utmost importance. Randomized smoothing has been introduced lately and provides a framework to certify models and obtain theoretical guarantees. In this paper, we present for the first time a certified segmentation baseline for medical imaging based on randomized smoothing and diffusion models. Our results show that leveraging the power of denoising diffusion probabilistic models helps us overcome the limits of randomized smoothing. We conduct extensive experiments on five public datasets of chest X-rays, skin lesions, and colonoscopies, and empirically show that we are able to maintain high certified Dice scores even for highly perturbed images. Our work represents the first attempt to certify medical image segmentation models, and we aspire for it to set a foundation for future benchmarks in this crucial and largely uncharted area.
</details>
<details>
<summary>摘要</summary>
医疗影像中的分割模型在过去一代有了显著改进，现在在临床实践中每天都使用。然而，与分类模型一样，分割模型也受到敌意攻击的影响。在医疗领域中，确认模型预测的重要性是无可估量的。Randomized smoothing最近引入了一种框架，可以证明模型的预测，并提供了理论保证。在这篇论文中，我们为首次提出了医疗影像中证明的分割基线，基于随机熔浆概率模型和扩散模型。我们的结果表明，利用扩散概率模型的力量，我们可以超越随机熔浆的限制。我们在五个公共数据集上进行了广泛的实验，包括胸部X射线、皮肤损害和colonoscopy，并观察到我们可以保持高的证明 dice 分数，即使图像受到了严重的干扰。我们的工作代表了医疗影像分割模型的首次证明，我们希望这可以成为未来在这一关键和未知的领域的基础。
</details></li>
</ul>
<hr>
<h2 id="Robustness-Guided-Image-Synthesis-for-Data-Free-Quantization"><a href="#Robustness-Guided-Image-Synthesis-for-Data-Free-Quantization" class="headerlink" title="Robustness-Guided Image Synthesis for Data-Free Quantization"></a>Robustness-Guided Image Synthesis for Data-Free Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03661">http://arxiv.org/abs/2310.03661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianhong Bai, Yuchen Yang, Huanpeng Chu, Hualiang Wang, Zuozhu Liu, Ruizhe Chen, Xiaoxuan He, Lianrui Mu, Chengfei Cai, Haoji Hu</li>
<li>for: 提高数据自由压缩性能，增强生成图像 semantics 和多样性。</li>
<li>methods: 提出 Robustness-Guided Image Synthesis (RIS) 方法，通过在输入和模型参数层次上引入干扰，并在特征和预测层次上定义不一致度指标，以提高生成图像 semantics 和多样性。</li>
<li>results: 在不同设定下实现了数据自由压缩性能的状态场，并且可以扩展到其他数据自由压缩任务。<details>
<summary>Abstract</summary>
Quantization has emerged as a promising direction for model compression. Recently, data-free quantization has been widely studied as a promising method to avoid privacy concerns, which synthesizes images as an alternative to real training data. Existing methods use classification loss to ensure the reliability of the synthesized images. Unfortunately, even if these images are well-classified by the pre-trained model, they still suffer from low semantics and homogenization issues. Intuitively, these low-semantic images are sensitive to perturbations, and the pre-trained model tends to have inconsistent output when the generator synthesizes an image with poor semantics. To this end, we propose Robustness-Guided Image Synthesis (RIS), a simple but effective method to enrich the semantics of synthetic images and improve image diversity, further boosting the performance of downstream data-free compression tasks. Concretely, we first introduce perturbations on input and model weight, then define the inconsistency metrics at feature and prediction levels before and after perturbations. On the basis of inconsistency on two levels, we design a robustness optimization objective to enhance the semantics of synthetic images. Moreover, we also make our approach diversity-aware by forcing the generator to synthesize images with small correlations in the label space. With RIS, we achieve state-of-the-art performance for various settings on data-free quantization and can be extended to other data-free compression tasks.
</details>
<details>
<summary>摘要</summary>
量化已经出现为模型压缩的可能的方向。最近，数据无关量化已经广泛研究，以避免隐私问题，它使用图像作为代替实际训练数据来生成图像。现有方法使用类别损失来确保生成的图像的可靠性。然而，即使这些图像由预训练模型良好分类，它们仍然受到低 semantics 和同化问题的困扰。我们认为这些低 semantics 图像是敏感的，生成器Synthesize图像时容易受到扰动的影响，预训练模型对生成的图像的输出是不一致的。为此，我们提出了Robustness-Guided Image Synthesis（RIS），一种简单 yet effective的方法，以提高生成图像的 semantics 和多样性，从而提高下游数据free压缩任务的性能。具体来说，我们首先对输入和模型参数进行扰动，然后定义在特征层和预测层之前和之后的不一致度量。基于这两个层次的不一致度量，我们设计了一个Robustness optimization objective，以提高生成图像的 semantics。此外，我们还使我们的方法具有多样性， forcing the generator to synthesize images with small correlations in the label space。与RIS相比，我们实现了数据free压缩中的状态级性能，并且可以扩展到其他数据free压缩任务。
</details></li>
</ul>
<hr>
<h2 id="Visual-inspection-for-illicit-items-in-X-ray-images-using-Deep-Learning"><a href="#Visual-inspection-for-illicit-items-in-X-ray-images-using-Deep-Learning" class="headerlink" title="Visual inspection for illicit items in X-ray images using Deep Learning"></a>Visual inspection for illicit items in X-ray images using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03658">http://arxiv.org/abs/2310.03658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Mademlis, Georgios Batsis, Adamantia Anna Rebolledo Chrysochoou, Georgios Th. Papadopoulos</li>
<li>for: 实现自动检测货物摄像头中的黑心物品，以提高公共安全性，增加安全人员的生产力和减轻对于安全人员的心理负担，特别是在机场、地铁、海关&#x2F;邮政等地区。</li>
<li>methods: 使用现代计算机视觉算法，基于深度神经网络（DNNs），以应对大量和高速的旅客和邮件等，并在受限和嵌入式环境中进行实现。</li>
<li>results: 根据实验结果显示，Transformer检测器最具优势，而过时的辅助神经网络在安全应用中的发展没有效果，CSP-DarkNet底层CNN也表现高效。<details>
<summary>Abstract</summary>
Automated detection of contraband items in X-ray images can significantly increase public safety, by enhancing the productivity and alleviating the mental load of security officers in airports, subways, customs/post offices, etc. The large volume and high throughput of passengers, mailed parcels, etc., during rush hours practically make it a Big Data problem. Modern computer vision algorithms relying on Deep Neural Networks (DNNs) have proven capable of undertaking this task even under resource-constrained and embedded execution scenarios, e.g., as is the case with fast, single-stage object detectors. However, no comparative experimental assessment of the various relevant DNN components/methods has been performed under a common evaluation protocol, which means that reliable cross-method comparisons are missing. This paper presents exactly such a comparative assessment, utilizing a public relevant dataset and a well-defined methodology for selecting the specific DNN components/modules that are being evaluated. The results indicate the superiority of Transformer detectors, the obsolete nature of auxiliary neural modules that have been developed in the past few years for security applications and the efficiency of the CSP-DarkNet backbone CNN.
</details>
<details>
<summary>摘要</summary>
自动检测违法物品在X射图像中可以显著提高公共安全，因为它可以提高安全官员的产量和减轻他们的心理负担，特别是在机场、地铁、海关/邮政等场合。在湮旷时间段，大量的旅客和寄送包裹等会导致实际上是一个大数据问题。现代计算机视觉算法，基于深度神经网络（DNN），已经证明可以完成这项任务，即使在资源受限和嵌入式执行 scenarios 中。然而，没有一个 Comparative experimental assessment of the various relevant DNN components/methods has been performed under a common evaluation protocol，这意味着可靠的交叉比较不存在。这篇文章提供了一个 Comparative assessment，使用公共 relevante 的 dataset 和一个 Well-defined methodology for selecting the specific DNN components/modules being evaluated。结果表明 transformer 检测器的超越性，落后性 auxillary neural modules 在过去几年为安全应用程序开发的，以及 CSP-DarkNet 背景 CNN 的效率。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Distortion-Unifying-Fidelity-and-Realism"><a href="#Wasserstein-Distortion-Unifying-Fidelity-and-Realism" class="headerlink" title="Wasserstein Distortion: Unifying Fidelity and Realism"></a>Wasserstein Distortion: Unifying Fidelity and Realism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03629">http://arxiv.org/abs/2310.03629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Qiu, Aaron B. Wagner, Johannes Ballé, Lucas Theis</li>
<li>for: 这篇论文是为了提出一种图像扭曲度量，即 Wasserstein distortion，该度量同时涵盖了像素级准确性和现实性。</li>
<li>methods: 论文使用了 Wasserstein distortion 来评估图像的扭曲度量，并对不同参数选择进行了分析。</li>
<li>results: 论文通过实验示出了 Wasserstein distortion 的实用性，可以同时保证图像的像素级准确性和现实性。例如，通过生成随机的 texture 来示出 Wasserstein distortion 的应用。<details>
<summary>Abstract</summary>
We introduce a distortion measure for images, Wasserstein distortion, that simultaneously generalizes pixel-level fidelity on the one hand and realism on the other. We show how Wasserstein distortion reduces mathematically to a pure fidelity constraint or a pure realism constraint under different parameter choices. Pairs of images that are close under Wasserstein distortion illustrate its utility. In particular, we generate random textures that have high fidelity to a reference texture in one location of the image and smoothly transition to an independent realization of the texture as one moves away from this point. Connections between Wasserstein distortion and models of the human visual system are noted.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种图像扭曲度量，即沃森拓扑扭曲度量，它同时同时具有像素级准确性和现实性两种特点。我们展示了沃森拓扑扭曲度量在不同参数选择下可以Mathematically reduce to纯准确性约束或纯现实性约束。图像中的离散点对象示出了沃森拓扑扭曲度量的用途。特别是，我们生成了一些随机的纹理图像，这些图像在一个图像中具有高准确性参照纹理，随着移动 away from this point，纹理逐渐变得独立和自由。我们还注意到了沃森拓扑扭曲度量与人类视觉系统模型之间的连接。
</details></li>
</ul>
<hr>
<h2 id="High-Degrees-of-Freedom-Dynamic-Neural-Fields-for-Robot-Self-Modeling-and-Motion-Planning"><a href="#High-Degrees-of-Freedom-Dynamic-Neural-Fields-for-Robot-Self-Modeling-and-Motion-Planning" class="headerlink" title="High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning"></a>High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03624">http://arxiv.org/abs/2310.03624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lennart Schulze, Hod Lipson<br>for:The paper is written for developing a robot self-model that can be used for motion planning tasks in the absence of classical geometric kinematic models.methods:The paper uses neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations.results:The learned self-model achieves a Chamfer-L2 distance of 2% of the robot’s workspace dimension, demonstrating its effectiveness in motion planning tasks.Here’s the Chinese translation of the three key points:for:论文是为了开发一个不需要经典几何遥感模型的机器人自模型，用于减少人工介入，实现真正的自主 Agent。methods:论文使用神经场来让机器人自己模型其动态物体的几何结构，通过基于神经网络的封闭概率场建模。results:学习的自模型在7个自由度（DOF）机器人测试setup中实现了2%的机器人工作空间维度的Chamfer-L2距离。<details>
<summary>Abstract</summary>
A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of the robot's workspace dimension. We demonstrate the capabilities of this model on a motion planning task as an exemplary downstream application.
</details>
<details>
<summary>摘要</summary>
一个机器人自我模型是一种任务无关的机器人物理结构表示，可以用于减少或缺失经典几何动力学模型，尤其是在机器人的动力学发生意外变化或很难工程化时。在这种情况下，无人工作机器人自我模型是真正自主的必备特性。在这种工作中，我们利用神经场来让机器人自我模型其动态物理结构，通过学习神经隐式查询模型，只需基于2D图像和摄像头位置和配置进行学习。这使得我们的方法在已有方法所不足的情况下具有更大的可用性。为此，我们还提出了一种课程数据采样策略和新的编码器基于神经树量场架构，用于Conditional on high degrees of freedom (DOFs)的动态物体中心场景。在7DOF机器人测试设置中，学习的自我模型实现了机器人工作空间维度的Chamfer-L2距离为2%。我们示示了这种模型在运动规划任务中的应用能力。
</details></li>
</ul>
<hr>
<h2 id="Animatable-Virtual-Humans-Learning-pose-dependent-human-representations-in-UV-space-for-interactive-performance-synthesis"><a href="#Animatable-Virtual-Humans-Learning-pose-dependent-human-representations-in-UV-space-for-interactive-performance-synthesis" class="headerlink" title="Animatable Virtual Humans: Learning pose-dependent human representations in UV space for interactive performance synthesis"></a>Animatable Virtual Humans: Learning pose-dependent human representations in UV space for interactive performance synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03615">http://arxiv.org/abs/2310.03615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wieland Morgenstern, Milena T. Bagdasarian, Anna Hilsmann, Peter Eisert</li>
<li>for: 这篇论文旨在提出一种新的虚拟人类表现方法，用于高度真实的实时动画和渲染在3D应用中。</li>
<li>methods: 该方法基于高精度多视图视频重建获取的动态 mesh序列学习 pose-dependent 外观和几何。</li>
<li>results: 该方法可以高效地学习人体 pose-dependent 外观和几何，并在实时场景中进行流畅处理和渲染虚拟人类。<details>
<summary>Abstract</summary>
We propose a novel representation of virtual humans for highly realistic real-time animation and rendering in 3D applications. We learn pose dependent appearance and geometry from highly accurate dynamic mesh sequences obtained from state-of-the-art multiview-video reconstruction. Learning pose-dependent appearance and geometry from mesh sequences poses significant challenges, as it requires the network to learn the intricate shape and articulated motion of a human body. However, statistical body models like SMPL provide valuable a-priori knowledge which we leverage in order to constrain the dimension of the search space enabling more efficient and targeted learning and define pose-dependency. Instead of directly learning absolute pose-dependent geometry, we learn the difference between the observed geometry and the fitted SMPL model. This allows us to encode both pose-dependent appearance and geometry in the consistent UV space of the SMPL model. This approach not only ensures a high level of realism but also facilitates streamlined processing and rendering of virtual humans in real-time scenarios.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的虚拟人形表示方法，用于高度真实的实时动画和渲染在3D应用程序中。我们从 state-of-the-art 多视图视频重建获取了高度准确的动态网格序列，并学习 pose-dependent 形状和外观。学习pose-dependent的形状和运动呈poses significant challenges，因为它需要网络学习人体体形的细节和骨骼运动。然而，统计体模型如SMPL提供了valuable 先前知识，我们可以利用其来约束搜索空间的维度，以便更有效地学习和定向学习。而不是直接学习绝对pose-dependent的准确 geometry，我们学习了观察到的 geometry 与SMPL模型相比的差异。这种方法不仅保证了高度真实，还便利了实时enario中的虚拟人形处理和渲染。
</details></li>
</ul>
<hr>
<h2 id="How-Good-Are-Synthetic-Medical-Images-An-Empirical-Study-with-Lung-Ultrasound"><a href="#How-Good-Are-Synthetic-Medical-Images-An-Empirical-Study-with-Lung-Ultrasound" class="headerlink" title="How Good Are Synthetic Medical Images? An Empirical Study with Lung Ultrasound"></a>How Good Are Synthetic Medical Images? An Empirical Study with Lung Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03608">http://arxiv.org/abs/2310.03608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/global-health-labs/us-dcgan">https://github.com/global-health-labs/us-dcgan</a></li>
<li>paper_authors: Menghan Yu, Sourabh Kulhare, Courosh Mehanian, Charles B Delahunt, Daniel E Shea, Zohreh Laverriere, Ishan Shah, Matthew P Horning</li>
<li>for: 这个研究旨在提出一个整体框架，用于适应医学影像分析模型的开发 workflow。</li>
<li>methods: 该研究使用生成模型作为数据增强方法，并使用对抗方法保护患者隐私。</li>
<li>results: 研究表明，将真实数据和生成数据混合训练可以超越只使用真实数据训练的性能，并且模型只使用生成数据训练的性能接近真实数据训练的性能。<details>
<summary>Abstract</summary>
Acquiring large quantities of data and annotations is known to be effective for developing high-performing deep learning models, but is difficult and expensive to do in the healthcare context. Adding synthetic training data using generative models offers a low-cost method to deal effectively with the data scarcity challenge, and can also address data imbalance and patient privacy issues. In this study, we propose a comprehensive framework that fits seamlessly into model development workflows for medical image analysis. We demonstrate, with datasets of varying size, (i) the benefits of generative models as a data augmentation method; (ii) how adversarial methods can protect patient privacy via data substitution; (iii) novel performance metrics for these use cases by testing models on real holdout data. We show that training with both synthetic and real data outperforms training with real data alone, and that models trained solely with synthetic data approach their real-only counterparts. Code is available at https://github.com/Global-Health-Labs/US-DCGAN.
</details>
<details>
<summary>摘要</summary>
获取大量数据和注释是开发高性能深度学习模型的有效方法，但在医疗上困难和昂贵。通过使用生成模型生成的假数据可以解决数据稀缺问题，并可以解决数据不均衡和患者隐私问题。在这项研究中，我们提出了适应医学图像分析模型开发工作流程的完整框架。我们在不同的数据量下测试了（i）生成模型作为数据扩充方法的效果；（ii）如何通过数据替换来保护患者隐私；（iii）为这些用例提供新的性能指标，通过测试模型在真实副本数据上进行测试。我们发现，训练使用真实和假数据的模型比训练使用真实数据alone更高效，并且模型准备了假数据alone与真实数据alone相似。可以在 GitHub 上获取代码：https://github.com/Global-Health-Labs/US-DCGAN。
</details></li>
</ul>
<hr>
<h2 id="Ctrl-Room-Controllable-Text-to-3D-Room-Meshes-Generation-with-Layout-Constraints"><a href="#Ctrl-Room-Controllable-Text-to-3D-Room-Meshes-Generation-with-Layout-Constraints" class="headerlink" title="Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints"></a>Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03602">http://arxiv.org/abs/2310.03602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuan Fang, Xiaotao Hu, Kunming Luo, Ping Tan<br>for: 这paper的目的是为了提供一种可以从文本提示生成高质量的3D室内场景，并且允许用户进行可交互的编辑操作。methods: 该方法使用了一种分解layout和appearance的思路，首先使用文本Conditional Diffusion Model来学习场景布局分布，然后使用精度调整的ControlNet来生成高质量的3D场景图像。results: 该方法可以生成高质量的3D场景图像，并且可以让用户通过Mask-guided editing模块进行可交互的编辑操作，而无需进行贵重的编辑培训。在Structured3D dataset上进行了广泛的实验，并证明该方法可以比 existed方法更高效地生成从文本提示生成的3D场景。<details>
<summary>Abstract</summary>
Text-driven 3D indoor scene generation could be useful for gaming, film industry, and AR/VR applications. However, existing methods cannot faithfully capture the room layout, nor do they allow flexible editing of individual objects in the room. To address these problems, we present Ctrl-Room, which is able to generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables versatile interactive editing operations such as resizing or moving individual furniture items. Our key insight is to separate the modeling of layouts and appearance. %how to model the room that takes into account both scene texture and geometry at the same time. To this end, Our proposed method consists of two stages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The `Layout Generation Stage' trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization. Next, the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout and text prompt. In this way, we achieve a high-quality 3D room with convincing layouts and lively textures. Benefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, without expensive editing-specific training. Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, view-consistent, and editable 3D rooms from natural language prompts.
</details>
<details>
<summary>摘要</summary>
文本驱动的3D室内场景生成可能对游戏、电影业和AR/VR应用程序有用。然而，现有方法无法准确地捕捉室内布局，也无法让个体对象进行灵活编辑。为解决这些问题，我们提出了Ctrl-Room，它可以从文本提示生成真实的3D室内场景，并具有设计风格的布局和高质量的纹理图像。此外，Ctrl-Room还允许用户进行便捷的交互式编辑操作，如调整室内家具的大小或位置。我们的关键发现是将布局和外观模型分离开来。我们的提posed方法包括两个阶段：一个`Layout Generation Stage'和一个`Appearance Generation Stage'。`Layout Generation Stage'使用文本条件的扩散模型来学习室内布局的分布，而`Appearance Generation Stage'使用精心调整的ControlNet来生成基于3D场景布局和文本提示的生动的全景图像。通过这种方式，我们可以生成高质量的3D室内场景，具有真实的布局和生动的纹理图像。由于使用场景代码参数化，我们可以轻松地通过我们的面具引导编辑模块进行编辑，而无需贵重的编辑Specific training。我们的实验表明，我们的方法可以在Structured3D dataset上生成更合理、视角一致和可编辑的3D室内场景，比现有方法更高效。
</details></li>
</ul>
<hr>
<h2 id="BID-NeRF-RGB-D-image-pose-estimation-with-inverted-Neural-Radiance-Fields"><a href="#BID-NeRF-RGB-D-image-pose-estimation-with-inverted-Neural-Radiance-Fields" class="headerlink" title="BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance Fields"></a>BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03563">http://arxiv.org/abs/2310.03563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ágoston István Csehi, Csaba Máté Józsa</li>
<li>for: 提高倒掌风格场（iNeRF）算法，将图像pose估计问题定义为基于NeRF的迭代线性优化问题。</li>
<li>methods: 我们对iNeRF算法进行了以下改进：添加深度基于损失函数的本地化优化目标，使用多张图像的关系pose来定义损失函数，并在树状渲染中减少层次抽象采样。</li>
<li>results: 我们的改进可以显著提高iNeRF算法的吞吐量和基本错误率，并将估计范围扩展到更高的初始pose估计错误率。<details>
<summary>Abstract</summary>
We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which defines the image pose estimation problem as a NeRF based iterative linear optimization. NeRFs are novel neural space representation models that can synthesize photorealistic novel views of real-world scenes or objects. Our contributions are as follows: we extend the localization optimization objective with a depth-based loss function, we introduce a multi-image based loss function where a sequence of images with known relative poses are used without increasing the computational complexity, we omit hierarchical sampling during volumetric rendering, meaning only the coarse model is used for pose estimation, and we how that by extending the sampling interval convergence can be achieved even or higher initial pose estimate errors. With the proposed modifications the convergence speed is significantly improved, and the basin of convergence is substantially extended.
</details>
<details>
<summary>摘要</summary>
我们目标是改进倒计时神经辐射场（iNeRF）算法，该算法定义图像pose估计问题为基于NeRF的迭代线性优化问题。NeRF是一种新型神经空间表示模型，可以生成高品质的新视图图像或物体场景。我们的贡献包括以下几点：1. 将本地化优化目标添加depth-based损失函数，以提高pose估计精度。2. 引入多张图像基于损失函数，使用known相对pose的图像序列，无需增加计算复杂度。3. 在Volume Rendering中弃用层次抽象采样，只使用粗略模型进行pose估计，从而降低计算复杂度。4. 通过延长采样间隔，可以实现高初始pose估计错误的抽象，并且扩展了basin of convergence。通过我们的修改， convergence speed 得到了显著提高，并且basin of convergence得到了substantial扩展。
</details></li>
</ul>
<hr>
<h2 id="MedSyn-Text-guided-Anatomy-aware-Synthesis-of-High-Fidelity-3D-CT-Images"><a href="#MedSyn-Text-guided-Anatomy-aware-Synthesis-of-High-Fidelity-3D-CT-Images" class="headerlink" title="MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images"></a>MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03559">http://arxiv.org/abs/2310.03559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanwu Xu, Li Sun, Wei Peng, Shyam Visweswaran, Kayhan Batmanghelich<br>for:  This paper presents an innovative method for generating high-quality 3D lung CT images based on textual information, which can enhance numerous downstream tasks.methods:  The proposed method utilizes a hierarchical scheme with a modified UNet architecture to synthesize low-resolution images conditioned on text, and further generates vascular, airway, and lobular segmentation masks to ensure anatomical plausibility.results:  The proposed approach demonstrates superior performance compared to state-of-the-art models based on GAN and diffusion techniques, especially in retaining crucial anatomical features such as fissure lines, airways, and vascular structures.<details>
<summary>Abstract</summary>
This paper introduces an innovative methodology for producing high-quality 3D lung CT images guided by textual information. While diffusion-based generative models are increasingly used in medical imaging, current state-of-the-art approaches are limited to low-resolution outputs and underutilize radiology reports' abundant information. The radiology reports can enhance the generation process by providing additional guidance and offering fine-grained control over the synthesis of images. Nevertheless, expanding text-guided generation to high-resolution 3D images poses significant memory and anatomical detail-preserving challenges. Addressing the memory issue, we introduce a hierarchical scheme that uses a modified UNet architecture. We start by synthesizing low-resolution images conditioned on the text, serving as a foundation for subsequent generators for complete volumetric data. To ensure the anatomical plausibility of the generated samples, we provide further guidance by generating vascular, airway, and lobular segmentation masks in conjunction with the CT images. The model demonstrates the capability to use textual input and segmentation tasks to generate synthesized images. The results of comparative assessments indicate that our approach exhibits superior performance compared to the most advanced models based on GAN and diffusion techniques, especially in accurately retaining crucial anatomical features such as fissure lines, airways, and vascular structures. This innovation introduces novel possibilities. This study focuses on two main objectives: (1) the development of a method for creating images based on textual prompts and anatomical components, and (2) the capability to generate new images conditioning on anatomical elements. The advancements in image generation can be applied to enhance numerous downstream tasks.
</details>
<details>
<summary>摘要</summary>
To address the memory issue, we propose a hierarchical scheme that uses a modified UNet architecture. We first synthesize low-resolution images conditioned on the text, which serves as a foundation for subsequent generators to complete the volumetric data. Our approach demonstrates the capability to use textual input and segmentation tasks to generate synthesized images, with superior performance compared to existing GAN and diffusion techniques.The two main objectives of this study are:1. Developing a method for creating images based on textual prompts and anatomical components.2. Generating new images conditioning on anatomical elements.The advancements in image generation can be applied to enhance numerous downstream tasks.
</details></li>
</ul>
<hr>
<h2 id="Towards-Unified-Deep-Image-Deraining-A-Survey-and-A-New-Benchmark"><a href="#Towards-Unified-Deep-Image-Deraining-A-Survey-and-A-New-Benchmark" class="headerlink" title="Towards Unified Deep Image Deraining: A Survey and A New Benchmark"></a>Towards Unified Deep Image Deraining: A Survey and A New Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03535">http://arxiv.org/abs/2310.03535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Chen, Jinshan Pan, Jiangxin Dong, Jinhui Tang</li>
<li>for: 本研究旨在提供一个统一的评估设定，用于评估现有的图像雨排除方法，并提供一个新的高质量标准benchmark。</li>
<li>methods: 本研究使用了现有的图像雨排除方法，并对其进行了全面的评估。</li>
<li>results: 本研究提出了一个新的高质量标准benchmark，并通过了extensive的性能评估。<details>
<summary>Abstract</summary>
Recent years have witnessed significant advances in image deraining due to the kinds of effective image priors and deep learning models. As each deraining approach has individual settings (e.g., training and test datasets, evaluation criteria), how to fairly evaluate existing approaches comprehensively is not a trivial task. Although existing surveys aim to review of image deraining approaches comprehensively, few of them focus on providing unify evaluation settings to examine the deraining capability and practicality evaluation. In this paper, we provide a comprehensive review of existing image deraining method and provide a unify evaluation setting to evaluate the performance of image deraining methods. We construct a new high-quality benchmark named HQ-RAIN to further conduct extensive evaluation, consisting of 5,000 paired high-resolution synthetic images with higher harmony and realism. We also discuss the existing challenges and highlight several future research opportunities worth exploring. To facilitate the reproduction and tracking of the latest deraining technologies for general users, we build an online platform to provide the off-the-shelf toolkit, involving the large-scale performance evaluation. This online platform and the proposed new benchmark are publicly available and will be regularly updated at http://www.deraining.tech/.
</details>
<details>
<summary>摘要</summary>
近年来，因为有效的图像前提和深度学习模型，图像排除技术得到了显著进步。然而，每种排除方法都有自己的设置（例如训练和测试数据集、评价标准），因此全面评估现有方法的问题不是很容易解决。虽然现有的报告尝试了对图像排除方法进行全面审查，但只有很少的几篇文章关注提供统一的评估设置，以评估图像排除方法的性能和实用性。在这篇文章中，我们提供了一项全面的图像排除方法审查，并提供了统一的评估设置。我们构建了一个新的高品质标准 benchmark，名为HQ-RAIN，以进行广泛的评估。该标准包括5000对高分辨率的合成图像，具有更高的和实际性。我们还讨论了现有的挑战和提出了一些未来研究的可能性。为便于普通用户复制和跟踪最新的排除技术，我们建立了一个在线平台，提供了大规模的性能评估工具。这个在线平台和我们提出的新标准公共可用，将在http://www.deraining.tech/上进行定期更新。
</details></li>
</ul>
<hr>
<h2 id="3D-Aware-Hypothesis-Verification-for-Generalizable-Relative-Object-Pose-Estimation"><a href="#3D-Aware-Hypothesis-Verification-for-Generalizable-Relative-Object-Pose-Estimation" class="headerlink" title="3D-Aware Hypothesis &amp; Verification for Generalizable Relative Object Pose Estimation"></a>3D-Aware Hypothesis &amp; Verification for Generalizable Relative Object Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03534">http://arxiv.org/abs/2310.03534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhao, Tong Zhang, Mathieu Salzmann</li>
<li>for: 本研究旨在解决只有一个参考图像表示物体的情况下，估计物体在不同姿态下的相对 pose 问题。</li>
<li>methods: 我们提出了一种新的假设和验证框架，通过生成和评估多个姿态假设，最终选择最可靠的一个作为相对 pose。为了衡量可靠性，我们引入了3D变换aware的验证方法，将3D物体表示从两个输入图像中学习到的3D对象表示应用3D变换。</li>
<li>results: 我们的方法在Objaverse、LINEMOD和CO3D数据集上进行了广泛的实验，证明我们的相对pose估计精度较高，对大规模姿态变化和不visible object测试时的稳定性具有优势。<details>
<summary>Abstract</summary>
Prior methods that tackle the problem of generalizable object pose estimation highly rely on having dense views of the unseen object. By contrast, we address the scenario where only a single reference view of the object is available. Our goal then is to estimate the relative object pose between this reference view and a query image that depicts the object in a different pose. In this scenario, robust generalization is imperative due to the presence of unseen objects during testing and the large-scale object pose variation between the reference and the query. To this end, we present a new hypothesis-and-verification framework, in which we generate and evaluate multiple pose hypotheses, ultimately selecting the most reliable one as the relative object pose. To measure reliability, we introduce a 3D-aware verification that explicitly applies 3D transformations to the 3D object representations learned from the two input images. Our comprehensive experiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior accuracy of our approach in relative pose estimation and its robustness in large-scale pose variations, when dealing with unseen objects.
</details>
<details>
<summary>摘要</summary>
先前的方法很多都是基于具有 dense views 的未知 объек的假设，而我们则是面临只有一个参考视图的情况。我们的目标是将参考视图中的对象pose与测试图像中的对象pose进行相对pose estimation。在这种情况下，Robust generalization 是非常重要的，因为测试时可能会出现未知的对象，并且参考和测试图像中的对象pose之间存在大规模的差异。为此，我们提出了一个新的假设-验证框架，在这个框架中，我们生成并评估多个pose假设，最终选择最可靠的 pose 作为对象的相对pose。为了衡量可靠性，我们引入了3D-aware验证，该验证Explicitly applies 3D变换到从两个输入图像中学习的3D对象表示。我们对 Objaverse、LINEMOD 和 CO3D 数据集进行了广泛的实验，证明了我们的方法在相对pose估计中的精度和在大规模的pose变化中的Robustness，当处理未知对象时。
</details></li>
</ul>
<hr>
<h2 id="V2X-Cooperative-Perception-for-Autonomous-Driving-Recent-Advances-and-Challenges"><a href="#V2X-Cooperative-Perception-for-Autonomous-Driving-Recent-Advances-and-Challenges" class="headerlink" title="V2X Cooperative Perception for Autonomous Driving: Recent Advances and Challenges"></a>V2X Cooperative Perception for Autonomous Driving: Recent Advances and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03525">http://arxiv.org/abs/2310.03525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Huang, Jianan Liu, Xi Zhou, Dinh C. Nguyen, Mostafa Rahimi Azghadi, Yuxuan Xia, Qing-Long Han, Sumei Sun</li>
<li>for: 本研究的目的是为了提高自动驾驶系统的安全性和可靠性，通过推动合作感知（Cooperative Perception，CP）技术的发展。</li>
<li>methods: 本研究使用了许多现有的计算机视觉技术，包括物体识别等，以解决现实世界交通环境中的难题。此外，还使用了许多最新的通信技术，如V2X技术，以增强驾驶自动化系统。</li>
<li>results: 本研究提出了一种基于V2X通信技术的CP工作流程，并对现有的V2X-based CP方法进行了分类和评价。此外，还对CP技术的发展进行了一种彻底的文献综述，并评估了现有的数据集和模拟器。最后，本研究还讨论了CP技术的未来发展和挑战。<details>
<summary>Abstract</summary>
Accurate perception is essential for advancing autonomous driving and addressing safety challenges in modern transportation systems. Despite significant advancements in computer vision for object recognition, current perception methods still face difficulties in complex real-world traffic environments. Challenges such as physical occlusion and limited sensor field of view persist for individual vehicle systems. Cooperative Perception (CP) with Vehicle-to-Everything (V2X) technologies has emerged as a solution to overcome these obstacles and enhance driving automation systems. While some research has explored CP's fundamental architecture and critical components, there remains a lack of comprehensive summaries of the latest innovations, particularly in the context of V2X communication technologies. To address this gap, this paper provides a comprehensive overview of the evolution of CP technologies, spanning from early explorations to recent developments, including advancements in V2X communication technologies. Additionally, a contemporary generic framework is proposed to illustrate the V2X-based CP workflow, aiding in the structured understanding of CP system components. Furthermore, this paper categorizes prevailing V2X-based CP methodologies based on the critical issues they address. An extensive literature review is conducted within this taxonomy, evaluating existing datasets and simulators. Finally, open challenges and future directions in CP for autonomous driving are discussed by considering both perception and V2X communication advancements.
</details>
<details>
<summary>摘要</summary>
准确的感知是自动驾驶技术发展的关键，以解决现代交通系统中的安全挑战。尽管计算机视觉技术在物体识别方面做出了 significiant 进步，但现在的感知方法仍然在复杂的实际交通环境中遇到困难。这些困难包括物体遮挡和汽车感知器的有限范围。协同感知（CP）技术与 everything （V2X）技术已经出现为解决这些问题并增强驾驶自动化系统。虽然一些研究探讨了 CP 技术的基本架构和关键组件，但还有一些研究 gap 需要填充。为了填充这些 gap，这篇文章提供了 CP 技术的演化历史，从早期探索到最新的发展，包括 V2X 通信技术的进步。此外，文章还提出了一个现代化的 CP 工作流程框架，以便系统化地理解 CP 系统组件。此外，文章还对 CP 方法分为不同的关键问题，进行了广泛的文献综述。最后，文章讨论了 CP 技术在自动驾驶方面的未来发展和挑战。
</details></li>
</ul>
<hr>
<h2 id="PrototypeFormer-Learning-to-Explore-Prototype-Relationships-for-Few-shot-Image-Classification"><a href="#PrototypeFormer-Learning-to-Explore-Prototype-Relationships-for-Few-shot-Image-Classification" class="headerlink" title="PrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classification"></a>PrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03517">http://arxiv.org/abs/2310.03517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feihong He, Gang Li, Lingyu Si, Leilei Yan, Fanzhang Li, Fuchun Sun</li>
<li>for: 提高少量图像分类的性能，Addressing the challenge of poor classification performance with limited samples in novel classes.</li>
<li>methods: 使用 transformer 架构建 prototype 抽取模块，提取更有准确性的类表示，并在少shot learning scenario 中使用对比学习优化 prototype 特征。</li>
<li>results: 在多个流行的少shot image classification benchmark dataset上进行实验，显示了我们的方法在现有state-of-the-art方法之上具有remarkable的性能，并且将于未来释出代码。Here’s the translation in English for reference:</li>
<li>for: To improve the performance of few-shot image classification, addressing the challenge of poor classification performance with limited samples in novel classes.</li>
<li>methods: Using a transformer architecture to build a prototype extraction module, extracting more discriminative class representations for few-shot classification, and employing a contrastive learning-based optimization approach to optimize prototype features in few-shot learning scenarios.</li>
<li>results: Experimented on several popular few-shot image classification benchmark datasets, showing that our method outperforms all current state-of-the-art methods, with remarkable performance. The code will be released later.<details>
<summary>Abstract</summary>
Few-shot image classification has received considerable attention for addressing the challenge of poor classification performance with limited samples in novel classes. However, numerous studies have employed sophisticated learning strategies and diversified feature extraction methods to address this issue. In this paper, we propose our method called PrototypeFormer, which aims to significantly advance traditional few-shot image classification approaches by exploring prototype relationships. Specifically, we utilize a transformer architecture to build a prototype extraction module, aiming to extract class representations that are more discriminative for few-shot classification. Additionally, during the model training process, we propose a contrastive learning-based optimization approach to optimize prototype features in few-shot learning scenarios. Despite its simplicity, the method performs remarkably well, with no bells and whistles. We have experimented with our approach on several popular few-shot image classification benchmark datasets, which shows that our method outperforms all current state-of-the-art methods. In particular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with accuracy of 7.27% and 8.72%, respectively. The code will be released later.
</details>
<details>
<summary>摘要</summary>
Recently, few-shot image classification has received significant attention due to the challenge of achieving poor classification performance with limited samples in novel classes. Many studies have employed sophisticated learning strategies and diversified feature extraction methods to address this issue. In this paper, we propose a method called PrototypeFormer, which aims to significantly advance traditional few-shot image classification approaches by exploring prototype relationships. Specifically, we use a transformer architecture to build a prototype extraction module, which aims to extract class representations that are more discriminative for few-shot classification. Additionally, during the model training process, we propose a contrastive learning-based optimization approach to optimize prototype features in few-shot learning scenarios. Despite its simplicity, the method performs remarkably well, with no bells and whistles. We have experimented with our approach on several popular few-shot image classification benchmark datasets, which shows that our method outperforms all current state-of-the-art methods. In particular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with accuracy of 7.27% and 8.72%, respectively. The code will be released later.Here's the breakdown of the translation:1. Recently, few-shot image classification has received significant attention (近些时候，几张图像分类 receiving 了 considerable attention)2. due to the challenge of achieving poor classification performance with limited samples in novel classes (因为它们需要处理有限样本的新类，而且这些样本通常是异常的)3. Many studies have employed sophisticated learning strategies and diversified feature extraction methods to address this issue. (许多研究使用了复杂的学习策略和多样化的特征提取方法来解决这个问题)4. In this paper, we propose a method called PrototypeFormer, which aims to significantly advance traditional few-shot image classification approaches by exploring prototype relationships. (在这篇论文中，我们提出了一种方法，叫做 PrototypeFormer，它目的是通过探索原型关系来显著地提高传统的几张图像分类方法)5. Specifically, we use a transformer architecture to build a prototype extraction module, which aims to extract class representations that are more discriminative for few-shot classification. (具体来说，我们使用 transformer 架构来建立原型提取模块，以提取更加特征的几张图像分类)6. Additionally, during the model training process, we propose a contrastive learning-based optimization approach to optimize prototype features in few-shot learning scenarios. (此外，在模型训练过程中，我们提出了一种基于对比学习的优化方法，用于优化几张图像分类中的原型特征)7. Despite its simplicity, the method performs remarkably well, with no bells and whistles. (尽管它的简单，但它的性能很好，没有任何辅助工具)8. We have experimented with our approach on several popular few-shot image classification benchmark datasets, which shows that our method outperforms all current state-of-the-art methods. (我们在几个流行的几张图像分类标准数据集上进行了实验，发现我们的方法超过了当前的状态艺术方法)9. In particular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with accuracy of 7.27% and 8.72%, respectively. (特别是，我们的方法在 miniImageNet 中的 5-way 5-shot 和 5-way 1-shot 任务上达到了 97.07% 和 90.88%，超过了当前状态艺术方法的准确率 7.27% 和 8.72%， соответственно)10. The code will be released later. (代码将在后来发布)
</details></li>
</ul>
<hr>
<h2 id="Exploring-DINO-Emergent-Properties-and-Limitations-for-Synthetic-Aperture-Radar-Imagery"><a href="#Exploring-DINO-Emergent-Properties-and-Limitations-for-Synthetic-Aperture-Radar-Imagery" class="headerlink" title="Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery"></a>Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03513">http://arxiv.org/abs/2310.03513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph A. Gallego-Mejia, Anna Jungbluth, Laura Martínez-Ferrer, Matt Allen, Francisco Dorr, Freddie Kalaitzis, Raúl Ramos-Pollán</li>
<li>for: 这个研究探讨了Self-Distillation with No Labels（DINO）算法在 Synthetic Aperture Radar（SAR）图像上的应用和emergent特性。</li>
<li>methods: 我们使用无标签SAR数据预训练一个基于Vision Transformer（ViT）的DINO模型，然后精确调整模型来预测高分辨率土地覆盖图。我们仔细评估了ViT底层抽象的MAP值，并与模型的Token Embedding空间进行比较。</li>
<li>results: 我们发现预训练比于从scratch训练有小量的提升，并讨论了SSL在Remote Sensing和土地覆盖分类中的局限性和机遇。此外，我们发现ViT的抽象MAP值对于Remote Sensing具有很大的内在价值，可以提供有用的输入 для其他算法。这个研究为大型和更好的SSL模型的开发奠定了基础。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) models have recently demonstrated remarkable performance across various tasks, including image segmentation. This study delves into the emergent characteristics of the Self-Distillation with No Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR) imagery. We pre-train a vision transformer (ViT)-based DINO model using unlabeled SAR data, and later fine-tune the model to predict high-resolution land cover maps. We rigorously evaluate the utility of attention maps generated by the ViT backbone, and compare them with the model's token embedding space. We observe a small improvement in model performance with pre-training compared to training from scratch, and discuss the limitations and opportunities of SSL for remote sensing and land cover segmentation. Beyond small performance increases, we show that ViT attention maps hold great intrinsic value for remote sensing, and could provide useful inputs to other algorithms. With this, our work lays the ground-work for bigger and better SSL models for Earth Observation.
</details>
<details>
<summary>摘要</summary>
自我指导学习（SSL）模型最近已经在不同任务上展示出惊人的表现，包括图像分割。本研究探讨了自我混合 WITH NO Labels（DINO）算法的 emergent 特性，并应用于Synthetic Aperture Radar（SAR）成像。我们使用无标签 SAR 数据预训练一个基于视Transformer（ViT）的 DINO 模型，然后练习模型预测高分辨率地形覆盖图。我们仔细评估了 ViT 底层的注意力地图，并与模型的 Token 空间进行比较。我们发现预训练比于从 scratch 训练有小幅提升性能，并讨论了SSL 在遥感和地形分类中的局限性和机遇。此外，我们发现 ViT 的注意力地图具有很大的内在价值，可以提供用于遥感的有用输入。因此，我们的工作为大型和更好的 SSL 模型奠定了基础。
</details></li>
</ul>
<hr>
<h2 id="RL-based-Stateful-Neural-Adaptive-Sampling-and-Denoising-for-Real-Time-Path-Tracing"><a href="#RL-based-Stateful-Neural-Adaptive-Sampling-and-Denoising-for-Real-Time-Path-Tracing" class="headerlink" title="RL-based Stateful Neural Adaptive Sampling and Denoising for Real-Time Path Tracing"></a>RL-based Stateful Neural Adaptive Sampling and Denoising for Real-Time Path Tracing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03507">http://arxiv.org/abs/2310.03507</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ajsvb/rl_path_tracing">https://github.com/ajsvb/rl_path_tracing</a></li>
<li>paper_authors: Antoine Scardigli, Lukas Cavigelli, Lorenz K. Müller</li>
<li>for: 提高真实图像生成的可靠性和速度</li>
<li>methods: 使用抽象学习网络对抽象进行END-TO-END训练，包括采样重要性网络、嵌入空间编码器网络和减噪网络</li>
<li>results: 在多个具有挑战性的数据集上提高视觉质量，并将比前一个状态艺术的渲染时间减少为1.6倍，为实时应用提供了有前途的解决方案。<details>
<summary>Abstract</summary>
Monte-Carlo path tracing is a powerful technique for realistic image synthesis but suffers from high levels of noise at low sample counts, limiting its use in real-time applications. To address this, we propose a framework with end-to-end training of a sampling importance network, a latent space encoder network, and a denoiser network. Our approach uses reinforcement learning to optimize the sampling importance network, thus avoiding explicit numerically approximated gradients. Our method does not aggregate the sampled values per pixel by averaging but keeps all sampled values which are then fed into the latent space encoder. The encoder replaces handcrafted spatiotemporal heuristics by learned representations in a latent space. Finally, a neural denoiser is trained to refine the output image. Our approach increases visual quality on several challenging datasets and reduces rendering times for equal quality by a factor of 1.6x compared to the previous state-of-the-art, making it a promising solution for real-time applications.
</details>
<details>
<summary>摘要</summary>
蒙特卡洛路追踪是一种具有很高真实度的图像生成技术，但是在低样本数下会受到高水平的噪声影响，限制其在实时应用中的使用。为了解决这个问题，我们提出了一个框架，其中包括端到端培生样本重要性网络、嵌入空间编码器网络和净化网络。我们的方法使用了回归学习来优化样本重要性网络，从而避免直接用数值 aproximated 的数学导数。我们的方法不是将每个像素的样本值相加，而是保留所有的样本值，然后将它们传递给嵌入空间编码器。编码器将手工设计的空间时间规则替换为学习的表示在嵌入空间中。最后，我们训练了一个神经净化器来精细化输出图像。我们的方法可以在多个复杂的数据集上提高视觉质量，同时降低等质量的渲染时间，比前一个状态艺术高一点1.6倍，因此是一个有前途的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Kandinsky-an-Improved-Text-to-Image-Synthesis-with-Image-Prior-and-Latent-Diffusion"><a href="#Kandinsky-an-Improved-Text-to-Image-Synthesis-with-Image-Prior-and-Latent-Diffusion" class="headerlink" title="Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion"></a>Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03502">http://arxiv.org/abs/2310.03502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-forever/movqgan">https://github.com/ai-forever/movqgan</a></li>
<li>paper_authors: Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, Denis Dimitrov</li>
<li>for: 这篇论文主要是为了提出一种新的扩展了演化架构，用于提高文本生成图像质量。</li>
<li>methods: 该模型使用了扩展了演化架构，包括像素级和幂等级的方法，并结合了图像先验模型和latent扩散技术。</li>
<li>results: 实验结果显示，该模型在COCO-30K数据集上的FID分数为8.03，与其他开源模型相比，表示该模型在可衡量的图像生成质量方面取得了突出的成绩。<details>
<summary>Abstract</summary>
Text-to-image generation is a significant domain in modern computer vision and has achieved substantial improvements through the evolution of generative architectures. Among these, there are diffusion-based models that have demonstrated essential quality enhancements. These models are generally split into two categories: pixel-level and latent-level approaches. We present Kandinsky1, a novel exploration of latent diffusion architecture, combining the principles of the image prior models with latent diffusion techniques. The image prior model is trained separately to map text embeddings to image embeddings of CLIP. Another distinct feature of the proposed model is the modified MoVQ implementation, which serves as the image autoencoder component. Overall, the designed model contains 3.3B parameters. We also deployed a user-friendly demo system that supports diverse generative modes such as text-to-image generation, image fusion, text and image fusion, image variations generation, and text-guided inpainting/outpainting. Additionally, we released the source code and checkpoints for the Kandinsky models. Experimental evaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking our model as the top open-source performer in terms of measurable image generation quality.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉中的文本至图生成是一个重要领域，在生成架构的演化中得到了显著改进。这些模型可以分为两类：像素级和幂等级方法。我们提出了一种新的探索，即将图像先验模型与幂等技术结合，称之为Kandinsky1。这个模型将文本嵌入模型与CLIP的图像嵌入模型进行共同训练。另外，我们修改了MoVQ实现方式，用于图像自编码器组件。总共有3.3亿参数。我们还实现了一个易于使用的示例系统，支持多种生成模式，如文本至图生成、图像融合、文本和图像融合、图像变化生成和文本引导填充/剔除。此外，我们还公布了Kandinsky模型的源代码和检查点。实验评估表明，Kandinsky1模型在COCO-30K dataset上的FID分数为8.03，这标志着我们的模型在可衡量的图像生成质量方面成为开源领先者。
</details></li>
</ul>
<hr>
<h2 id="IceCloudNet-Cirrus-and-mixed-phase-cloud-prediction-from-SEVIRI-input-learned-from-sparse-supervision"><a href="#IceCloudNet-Cirrus-and-mixed-phase-cloud-prediction-from-SEVIRI-input-learned-from-sparse-supervision" class="headerlink" title="IceCloudNet: Cirrus and mixed-phase cloud prediction from SEVIRI input learned from sparse supervision"></a>IceCloudNet: Cirrus and mixed-phase cloud prediction from SEVIRI input learned from sparse supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03499">http://arxiv.org/abs/2310.03499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Jeggle, Mikolaj Czerkawski, Federico Serva, Bertrand Le Saux, David Neubauer, Ulrike Lohmann</li>
<li>for: 这项研究旨在提供地球站点覆盖率和活动卫星 Retrievals 的 ice 微物理性质 regime-dependent 观测约束，以提高气候模型中 ice 云物理过程的理解，从而减少气候变化中的不确定性。</li>
<li>methods: 这项研究使用了 convolutional neural network (CNN) 训练方法，使用了三年的 SEVIRI 和 DARDAR 数据集，以获得地球站点覆盖率和活动卫星 Retrievals 的 ice 微物理性质观测约束。</li>
<li>results: 这项研究实现了创造一种新的观测约束，可以用于改进气候模型中 ice 云物理过程的理解，从而减少气候变化中的不确定性。<details>
<summary>Abstract</summary>
Clouds containing ice particles play a crucial role in the climate system. Yet they remain a source of great uncertainty in climate models and future climate projections. In this work, we create a new observational constraint of regime-dependent ice microphysical properties at the spatio-temporal coverage of geostationary satellite instruments and the quality of active satellite retrievals. We achieve this by training a convolutional neural network on three years of SEVIRI and DARDAR data sets. This work will enable novel research to improve ice cloud process understanding and hence, reduce uncertainties in a changing climate and help assess geoengineering methods for cirrus clouds.
</details>
<details>
<summary>摘要</summary>
云含冰粒物理性质在气候系统中扮演着关键性角色。然而，这些云仍然对未来气候预测中存在大量不确定性。在这项工作中，我们创造了一个新的观测约束，即在地球同步卫星 instrumente 上的空间时间覆盖和活动卫星推算的冰微物理性质的Registry-dependent。我们通过训练一个卷积神经网络，使用三年的SEVIRI和DARDAR数据集来实现这一目标。这项工作将帮助改善冰云过程理解，从而减少气候变化中的不确定性，并评估环境工程方法 для cirrus 云。
</details></li>
</ul>
<hr>
<h2 id="BTDNet-a-Multi-Modal-Approach-for-Brain-Tumor-Radiogenomic-Classification"><a href="#BTDNet-a-Multi-Modal-Approach-for-Brain-Tumor-Radiogenomic-Classification" class="headerlink" title="BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic Classification"></a>BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03485">http://arxiv.org/abs/2310.03485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Kollias, Karanjot Vendal, Priyanka Gadhavi, Solomon Russom</li>
<li>for: 预测脑肿瘤MGMTpromoter的甲基化状态</li>
<li>methods: 利用多Modal的MRI扫描数据，包括FLAIR、T1w、T1wCE和T2 3D尺寸，采用BTDNet模型进行预测</li>
<li>results: 在RSNA-ASNR-MICCAI BraTS 2021 Challenge中，BTDNet方法舒大margin地超越了现有方法，提供了可能的脑肿瘤诊断和治疗的新途径<details>
<summary>Abstract</summary>
Brain tumors pose significant health challenges worldwide, with glioblastoma being one of the most aggressive forms. Accurate determination of the O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation status is crucial for personalized treatment strategies. However, traditional methods are labor-intensive and time-consuming. This paper proposes a novel multi-modal approach, BTDNet, leveraging multi-parametric MRI scans, including FLAIR, T1w, T1wCE, and T2 3D volumes, to predict MGMT promoter methylation status. BTDNet addresses two main challenges: the variable volume lengths (i.e., each volume consists of a different number of slices) and the volume-level annotations (i.e., the whole 3D volume is annotated and not the independent slices that it consists of). BTDNet consists of four components: i) the data augmentation one (that performs geometric transformations, convex combinations of data pairs and test-time data augmentation); ii) the 3D analysis one (that performs global analysis through a CNN-RNN); iii) the routing one (that contains a mask layer that handles variable input feature lengths), and iv) the modality fusion one (that effectively enhances data representation, reduces ambiguities and mitigates data scarcity). The proposed method outperforms by large margins the state-of-the-art methods in the RSNA-ASNR-MICCAI BraTS 2021 Challenge, offering a promising avenue for enhancing brain tumor diagnosis and treatment.
</details>
<details>
<summary>摘要</summary>
脑肿瘤对全球健康造成重要挑战，其中 glioblastoma 是最攻击性的一种。正确地确定 O6-methylguanine-DNA methyltransferase (MGMT) 基因Promoter的甲基化状态是个人化治疗策略的关键。然而，传统方法具有劳动 INTENSIVE 和时间耗费的缺点。这篇论文提出了一种新的多Modal方法，BTDNet，利用多参量 MRI 扫描结果，包括 FLAIR、T1w、T1wCE 和 T2 3D 尺度，来预测 MGMT Promoter 甲基化状态。BTDNet 解决了两个主要挑战：每个尺度的变量尺度（即每个尺度都有不同的 slice 数）和尺度级别的注释（即整个 3D 尺度被注释，而不是每个独立的 slice）。BTDNet 由四个组成部分组成：1. 数据增强一（通过 геометрические变换、数据对的凸合和测试时数据增强进行数据增强）。2. 3D 分析一（通过 CNN-RNN 进行全球分析）。3. 路由一（包含一个mask层，处理变量输入特征长度）。4. 模式融合一（有效地增强数据表示，减少歧义和减少数据缺乏）。提出的方法在 RSNA-ASNR-MICCAI BraTS 2021 挑战中大幅超越了当前状态的方法，提供了一个有前途的方向，用于提高脑肿瘤诊断和治疗。
</details></li>
</ul>
<hr>
<h2 id="Ammonia-Net-A-Multi-task-Joint-Learning-Model-for-Multi-class-Segmentation-and-Classification-in-Tooth-marked-Tongue-Diagnosis"><a href="#Ammonia-Net-A-Multi-task-Joint-Learning-Model-for-Multi-class-Segmentation-and-Classification-in-Tooth-marked-Tongue-Diagnosis" class="headerlink" title="Ammonia-Net: A Multi-task Joint Learning Model for Multi-class Segmentation and Classification in Tooth-marked Tongue Diagnosis"></a>Ammonia-Net: A Multi-task Joint Learning Model for Multi-class Segmentation and Classification in Tooth-marked Tongue Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03472">http://arxiv.org/abs/2310.03472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shunkai Shi, Yuqi Wang, Qihui Ye, Yanran Wang, Yiming Zhu, Muhammad Hassan, Aikaterini Melliou, Dongmei Yu</li>
<li>For: This paper aims to address the challenges of manual diagnosis of tooth-marked tongue in traditional Chinese medicine, by proposing a multi-task joint learning model named Ammonia-Net.* Methods: The proposed model employs a convolutional neural network-based architecture, specifically designed for multi-class segmentation and classification of tongue images. It performs semantic segmentation of tongue images to identify tongue and tooth marks, and classifies the images into the desired number of classes.* Results: The experimental results show that the proposed model achieves 99.06% accuracy in the two-class classification task of tooth-marked tongue identification and 80.02% accuracy in the segmentation task, with mIoU for tongue and tooth marks amounting to 71.65%.<details>
<summary>Abstract</summary>
In Traditional Chinese Medicine, the tooth marks on the tongue, stemming from prolonged dental pressure, serve as a crucial indicator for assessing qi (yang) deficiency, which is intrinsically linked to visceral health. Manual diagnosis of tooth-marked tongue solely relies on experience. Nonetheless, the diversity in shape, color, and type of tooth marks poses a challenge to diagnostic accuracy and consistency. To address these problems, herein we propose a multi-task joint learning model named Ammonia-Net. This model employs a convolutional neural network-based architecture, specifically designed for multi-class segmentation and classification of tongue images. Ammonia-Net performs semantic segmentation of tongue images to identify tongue and tooth marks. With the assistance of segmentation output, it classifies the images into the desired number of classes: healthy tongue, light tongue, moderate tongue, and severe tongue. As far as we know, this is the first attempt to apply the semantic segmentation results of tooth marks for tooth-marked tongue classification. To train Ammonia-Net, we collect 856 tongue images from 856 subjects. After a number of extensive experiments, the experimental results show that the proposed model achieves 99.06% accuracy in the two-class classification task of tooth-marked tongue identification and 80.02%. As for the segmentation task, mIoU for tongue and tooth marks amounts to 71.65%.
</details>
<details>
<summary>摘要</summary>
在中医中，吃牙印痕的舌头，长期的牙关节压力，作为脉气衰竭（阳衰）的重要指标，舌头的手动诊断完全依赖经验。然而，吃牙印痕的多样性对于诊断精度和一致性带来挑战。为了解决这些问题，我们提出了一个名为Ammonia-Net的多任务集成学习模型。这个模型使用了一个特定设计 для 多 клаス混合分类和 semantic segmentation的舌头图像。Ammonia-Net 进行 semantic segmentation of tongue images，以识别舌头和吃牙印痕。受欢迎的分类结果显示，这是首次将吃牙印痕的 semantic segmentation 结果应用于舌头分类。我们收集了856个舌头图像，并进行了多次广泛的实验。结果显示，我们提出的模型在两种类别分类任务中的舌头印痕识别中取得了99.06%的准确率，并在分类任务中取得了80.02%的准确率。在分 segmentation 任务中，miou 为舌头和吃牙印痕为71.65%。
</details></li>
</ul>
<hr>
<h2 id="Multi-Resolution-Audio-Visual-Feature-Fusion-for-Temporal-Action-Localization"><a href="#Multi-Resolution-Audio-Visual-Feature-Fusion-for-Temporal-Action-Localization" class="headerlink" title="Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization"></a>Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03456">http://arxiv.org/abs/2310.03456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Fish, Jon Weinbren, Andrew Gilbert</li>
<li>for: 本文旨在提高视频中的动作识别精度，特别是将音频特征集成到视觉特征检测框架中。</li>
<li>methods: 本文提出了一种新的多尺度音视频特征融合方法（MRAV-FF），通过层次阻止权重机制，灵活地调整音频信息的重要性。</li>
<li>results: 实验表明，MRAV-FF可以提高视频动作识别精度，特别是当音频数据可用时。此外，MRAV-FF可以与现有的FPN TAL架构兼容，提供了一个简单而强大的方法来提高视频动作识别性能。<details>
<summary>Abstract</summary>
Temporal Action Localization (TAL) aims to identify actions' start, end, and class labels in untrimmed videos. While recent advancements using transformer networks and Feature Pyramid Networks (FPN) have enhanced visual feature recognition in TAL tasks, less progress has been made in the integration of audio features into such frameworks. This paper introduces the Multi-Resolution Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge audio-visual data across different temporal resolutions. Central to our approach is a hierarchical gated cross-attention mechanism, which discerningly weighs the importance of audio information at diverse temporal scales. Such a technique not only refines the precision of regression boundaries but also bolsters classification confidence. Importantly, MRAV-FF is versatile, making it compatible with existing FPN TAL architectures and offering a significant enhancement in performance when audio data is available.
</details>
<details>
<summary>摘要</summary>
Temporal Action Localization (TAL) 目标是在未处理视频中确定动作的开始、结束和类别标签。 Although recent advances in transformer networks and Feature Pyramid Networks (FPN) have improved visual feature recognition in TAL tasks, there has been less progress in integrating audio features into these frameworks. This paper introduces the Multi-Resolution Audio-Visual Feature Fusion (MRAV-FF), a novel method that combines audio-visual data across different temporal resolutions. The key to our approach is a hierarchical gated cross-attention mechanism, which selectively weights the importance of audio information at different temporal scales. This not only refines the precision of regression boundaries but also boosts classification confidence. Importantly, MRAV-FF is versatile and can be compatible with existing FPN TAL architectures, providing a significant improvement in performance when audio data is available.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-the-Influence-of-Domain-Shift-in-Skin-Lesion-Classification-A-Benchmark-Study-of-Unsupervised-Domain-Adaptation-Methods-on-Dermoscopic-Images"><a href="#Mitigating-the-Influence-of-Domain-Shift-in-Skin-Lesion-Classification-A-Benchmark-Study-of-Unsupervised-Domain-Adaptation-Methods-on-Dermoscopic-Images" class="headerlink" title="Mitigating the Influence of Domain Shift in Skin Lesion Classification: A Benchmark Study of Unsupervised Domain Adaptation Methods on Dermoscopic Images"></a>Mitigating the Influence of Domain Shift in Skin Lesion Classification: A Benchmark Study of Unsupervised Domain Adaptation Methods on Dermoscopic Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03432">http://arxiv.org/abs/2310.03432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sireesha Chamarthi, Katharina Fogelberg, Roman C. Maron, Titus J. Brinker, Julia Niebling</li>
<li>For: This paper aims to improve the performance of deep neural networks in skin lesion classification by addressing the issue of domain shift, which can negatively impact the models’ accuracy when tested on new data.* Methods: The authors evaluate eight different unsupervised domain adaptation methods to determine their effectiveness in improving generalization for dermoscopic datasets.* Results: The authors find that all eight domain adaptation methods result in improved AUPRC for the majority of analyzed datasets, indicating that unsupervised domain adaptation generally leads to performance improvements for the binary melanoma-nevus classification task. However, small or heavily imbalanced datasets may lead to reduced conformity of the results due to the influence of these factors on the methods’ performance.Here is the same information in Simplified Chinese text:* For: 本研究旨在提高深度神经网络在皮肤病诊断中的表现， Addressing the issue of domain shift，即模型在新数据上的表现下降。* Methods: 作者评估了8种不supervised domain adaptation方法，以确定它们在dermoscopic dataset中的效果。* Results: 作者发现，所有8种domain adaptation方法在大多数分析数据集上都有所改进，表明不supervised domain adaptation通常能够提高binary melanoma-nevus分类任务的表现。但是，小型或受束缚数据集可能会导致结果的不一致，因为这些因素对方法的表现产生影响。<details>
<summary>Abstract</summary>
The potential of deep neural networks in skin lesion classification has already been demonstrated to be on-par if not superior to the dermatologists diagnosis. However, the performance of these models usually deteriorates when the test data differs significantly from the training data (i.e. domain shift). This concerning limitation for models intended to be used in real-world skin lesion classification tasks poses a risk to patients. For example, different image acquisition systems or previously unseen anatomical sites on the patient can suffice to cause such domain shifts. Mitigating the negative effect of such shifts is therefore crucial, but developing effective methods to address domain shift has proven to be challenging. In this study, we carry out an in-depth analysis of eight different unsupervised domain adaptation methods to analyze their effectiveness in improving generalization for dermoscopic datasets. To ensure robustness of our findings, we test each method on a total of ten distinct datasets, thereby covering a variety of possible domain shifts. In addition, we investigated which factors in the domain shifted datasets have an impact on the effectiveness of domain adaptation methods. Our findings show that all of the eight domain adaptation methods result in improved AUPRC for the majority of analyzed datasets. Altogether, these results indicate that unsupervised domain adaptations generally lead to performance improvements for the binary melanoma-nevus classification task regardless of the nature of the domain shift. However, small or heavily imbalanced datasets lead to a reduced conformity of the results due to the influence of these factors on the methods performance.
</details>
<details>
<summary>摘要</summary>
深度神经网络在皮肤病变分类中的潜力已经被证明与专业 dermatologist 诊断相当或更高。然而，这些模型在测试数据与训练数据之间的域转换（domain shift）时通常会表现出现下降。这种问题在实际应用中对患者造成风险，例如不同的图像获取系统或患者的前所未见的解剖位置可能会导致域转换。因此，解决域转换的负面影响是关键，但是开发有效的方法来解决域转换问题已经证明是困难的。在这项研究中，我们进行了八种不同的无监督领域适应方法的深入分析，以分析它们在DERMOSCOPIC dataset上的效果。为确保我们的发现的可靠性，我们将每种方法测试在总共十个不同的dataset上，以覆盖多种可能的域转换情况。此外，我们还研究了域转换 dataset 中各因素对领域适应方法的影响。我们的发现显示，所有八种领域适应方法都导致了大多数分析dataset中的AUPRC的提高。总之，这些结果表明无监督领域适应方法在皮肤病变分类任务中广泛地实现性提高，不管域转换的性质如何。然而，小型或受折制的dataset会导致结果的一致性受到这些因素的影响。
</details></li>
</ul>
<hr>
<h2 id="Robust-Zero-Level-Set-Extraction-from-Unsigned-Distance-Fields-Based-on-Double-Covering"><a href="#Robust-Zero-Level-Set-Extraction-from-Unsigned-Distance-Fields-Based-on-Double-Covering" class="headerlink" title="Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on Double Covering"></a>Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on Double Covering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03431">http://arxiv.org/abs/2310.03431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jjjkkyz/dcudf">https://github.com/jjjkkyz/dcudf</a></li>
<li>paper_authors: Fei Hou, Xuhui Chen, Wencheng Wang, Hong Qin, Ying He</li>
<li>for: 本研究提出了一种新的方法，叫做DoubleCoverUDF，用于从无符号距离场（UDF）中提取零水平面。</li>
<li>methods: DoubleCoverUDF 方法使用一个学习的 UDF 和用户指定的参数 $r$（一个小正数）作为输入，使用 conventional marching cubes 算法计算一个iso-surface，其中iso-value 为 $r$。</li>
<li>results: 计算得到的iso-surface 是 $r$-偏移体积 $S$ 的边界，其中 $S$ 是一个 orientable manifold，无论 $S$ 的topology如何。然后，算法计算一个覆盖图来投影边界网格onto $S$，保持网格的topology和避免折叠。如果 $S$ 是 orientable manifold 表面，我们的算法将double-layered mesh 分解成一个单层 mesh，否则保持 double-layered mesh 作为输出。我们对 open models 进行了3D 面的重建，并在synthetic models 和benchmark datasets上进行了实验，结果表明我们的方法比现有的 UDF-based 方法更加稳定和生成高质量的 mesh。<details>
<summary>Abstract</summary>
In this paper, we propose a new method, called DoubleCoverUDF, for extracting the zero level-set from unsigned distance fields (UDFs). DoubleCoverUDF takes a learned UDF and a user-specified parameter $r$ (a small positive real number) as input and extracts an iso-surface with an iso-value $r$ using the conventional marching cubes algorithm. We show that the computed iso-surface is the boundary of the $r$-offset volume of the target zero level-set $S$, which is an orientable manifold, regardless of the topology of $S$. Next, the algorithm computes a covering map to project the boundary mesh onto $S$, preserving the mesh's topology and avoiding folding. If $S$ is an orientable manifold surface, our algorithm separates the double-layered mesh into a single layer using a robust minimum-cut post-processing step. Otherwise, it keeps the double-layered mesh as the output. We validate our algorithm by reconstructing 3D surfaces of open models and demonstrate its efficacy and effectiveness on synthetic models and benchmark datasets. Our experimental results confirm that our method is robust and produces meshes with better quality in terms of both visual evaluation and quantitative measures than existing UDF-based methods. The source code is available at https://github.com/jjjkkyz/DCUDF.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种新的方法，即DoubleCoverUDF，用于从无符号距离场（UDF）中提取零水平面。DoubleCoverUDF接受一个学习过的UDF和用户指定的参数$r$（一个小正数）作为输入，使用传统的推进立方体算法计算一个iso-面，其iso-值为$r$。我们证明计算得到的iso-面是$r$-偏移体积的目标零水平面的边界，这是一个orientable manifold，无论$S$的topology如何。接下来，算法计算一个覆盖函数，将边界网格映射到$S$上，保持网格的topology和避免折叠。如果$S$是orientable manifold表面，我们的算法将double-layered网格分解为单层网格，使用一种robust minimum-cut后处理步骤。否则，它将double-layered网格作为输出。我们验证了我们的算法，通过重建开放模型的3D表面，并在 sintetic模型和标准数据集上进行了实验。我们的实验结果表明，我们的方法可以快速、稳定、高质量地从UDF中提取零水平面，并且在视觉评价和量化度量上比existings UDF-based方法更好。源代码可以在https://github.com/jjjkkyz/DCUDF上下载。
</details></li>
</ul>
<hr>
<h2 id="FreeReg-Image-to-Point-Cloud-Registration-Leveraging-Pretrained-Diffusion-Models-and-Monocular-Depth-Estimators"><a href="#FreeReg-Image-to-Point-Cloud-Registration-Leveraging-Pretrained-Diffusion-Models-and-Monocular-Depth-Estimators" class="headerlink" title="FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators"></a>FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03420">http://arxiv.org/abs/2310.03420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WHU-USI3DV/FreeReg">https://github.com/WHU-USI3DV/FreeReg</a></li>
<li>paper_authors: Haiping Wang, Yuan Liu, Bing Wang, Yujing Sun, Zhen Dong, Wenping Wang, Bisheng Yang</li>
<li>for: 图像和点云之间的匹配问题的基础问题是图像-点云注册。但由于图像和点云的模式差，使得现有的度量学习方法难以学习稳定和特异的跨模态特征。</li>
<li>methods: 我们提议先使用大规模预训练模型将图像和点云的模式统一，然后在同一模式内建立稳定的对应关系。我们发现diffusion特征在深度图生成器中提取的特征在图像和点云之间具有 semantics的一致性，因此可以建立粗略而 Robust的跨模态对应关系。</li>
<li>results: 我们进一步提取了depth图生成器中的geometry特征，并将其与diffusion特征进行匹配。这有效地提高了粗略对应关系的准确性。我们在三个公共的indoor和outdoor标准测试集上进行了广泛的实验，并显示了没有任务特别训练的情况下，直接使用这两种特征可以实现高精度的图像-点云注册。<details>
<summary>Abstract</summary>
Matching cross-modality features between images and point clouds is a fundamental problem for image-to-point cloud registration. However, due to the modality difference between images and points, it is difficult to learn robust and discriminative cross-modality features by existing metric learning methods for feature matching. Instead of applying metric learning on cross-modality data, we propose to unify the modality between images and point clouds by pretrained large-scale models first, and then establish robust correspondence within the same modality. We show that the intermediate features, called diffusion features, extracted by depth-to-image diffusion models are semantically consistent between images and point clouds, which enables the building of coarse but robust cross-modality correspondences. We further extract geometric features on depth maps produced by the monocular depth estimator. By matching such geometric features, we significantly improve the accuracy of the coarse correspondences produced by diffusion features. Extensive experiments demonstrate that without any task-specific training, direct utilization of both features produces accurate image-to-point cloud registration. On three public indoor and outdoor benchmarks, the proposed method averagely achieves a 20.6 percent improvement in Inlier Ratio, a three-fold higher Inlier Number, and a 48.6 percent improvement in Registration Recall than existing state-of-the-arts.
</details>
<details>
<summary>摘要</summary>
基于图像和点云的图像-点云匹配是图像处理领域的基本问题。然而，由于图像和点云的模式差异，使用现有的度量学习方法来学习强健和特异的跨模态特征是困难的。而不是将度量学习应用于跨模态数据上，我们提议先使用大规模预训练模型将图像和点云的模式统一，然后在同一模式内建立强健的对应关系。我们发现Diffusion特征，由深度图像扩散模型提取的中间特征，在图像和点云之间具有相似的含义，这使得可以建立粗略 yet 强健的跨模态对应关系。此外，我们还提取了depth图像上的几何特征。通过匹配这些几何特征，我们可以大幅提高粗略对应关系的准确性。我们的方法不需要任务特有的训练，直接使用这两种特征可以实现高精度的图像-点云匹配。在三个公共的室内和户外标准benchmark上，我们的方法平均提高了20.6%的准确率、三倍的准确数和48.6%的注册回溯率，与现有状态的方法相比。
</details></li>
</ul>
<hr>
<h2 id="A-Complementary-Global-and-Local-Knowledge-Network-for-Ultrasound-denoising-with-Fine-grained-Refinement"><a href="#A-Complementary-Global-and-Local-Knowledge-Network-for-Ultrasound-denoising-with-Fine-grained-Refinement" class="headerlink" title="A Complementary Global and Local Knowledge Network for Ultrasound denoising with Fine-grained Refinement"></a>A Complementary Global and Local Knowledge Network for Ultrasound denoising with Fine-grained Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03402">http://arxiv.org/abs/2310.03402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyu Bu, Kai-Ni Wang, Fuxing Zhao, Shengxiao Li, Guang-Quan Zhou</li>
<li>for: 提高ultrasound imaging的图像质量，以便进行后续的分类和识别任务。</li>
<li>methods: 使用global和local知识网络，并 integration fine-grained refinement block，以提高图像的细节表示。</li>
<li>results: 在HC18和BUSI两个公共数据集上进行验证，实验结果表明，该模型可以在量化指标和视觉性能上达到竞争力水平。<details>
<summary>Abstract</summary>
Ultrasound imaging serves as an effective and non-invasive diagnostic tool commonly employed in clinical examinations. However, the presence of speckle noise in ultrasound images invariably degrades image quality, impeding the performance of subsequent tasks, such as segmentation and classification. Existing methods for speckle noise reduction frequently induce excessive image smoothing or fail to preserve detailed information adequately. In this paper, we propose a complementary global and local knowledge network for ultrasound denoising with fine-grained refinement. Initially, the proposed architecture employs the L-CSwinTransformer as encoder to capture global information, incorporating CNN as decoder to fuse local features. We expand the resolution of the feature at different stages to extract more global information compared to the original CSwinTransformer. Subsequently, we integrate Fine-grained Refinement Block (FRB) within the skip-connection stage to further augment features. We validate our model on two public datasets, HC18 and BUSI. Experimental results demonstrate that our model can achieve competitive performance in both quantitative metrics and visual performance. Our code will be available at https://github.com/AAlkaid/USDenoising.
</details>
<details>
<summary>摘要</summary>
超声影像成为诊断工具的有效和非侵入性方法，广泛应用于临床检查。然而，超声影像中的斑点噪声常常降低影像质量，影响后续任务，如分割和分类。现有的噪声减少方法 frequently会导致过度的图像平滑或失去细节信息。在这篇论文中，我们提议一种 complementary 全球和本地知识网络 для超声杂噪减少，并在不同阶段扩大特征的分辨率，以获取更多的全球信息。然后，我们在跳过阶段内 интеGRATE Fine-grained Refinement Block (FRB)，以进一步增强特征。我们在 HC18 和 BUSI 两个公共数据集上验证我们的模型，实验结果表明我们的模型可以在量化指标和视觉性能方面达到竞争性表现。我们的代码将在 GitHub 上发布，链接为 <https://github.com/AAlkaid/USDenoising>。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Simplify-Spatial-Temporal-Graphs-in-Gait-Analysis"><a href="#Learning-to-Simplify-Spatial-Temporal-Graphs-in-Gait-Analysis" class="headerlink" title="Learning to Simplify Spatial-Temporal Graphs in Gait Analysis"></a>Learning to Simplify Spatial-Temporal Graphs in Gait Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03396">http://arxiv.org/abs/2310.03396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Cosma, Emilian Radoi</li>
<li>for: 这篇论文的目的是提高走势识别中的解释性和任务特定适应性，以提高走势识别的效率和可靠性。</li>
<li>methods: 这篇论文提出了一种新的方法，即使用两个模型（上游和下游模型）调整每个走势实例的边度矩阵，以删除固定的 graphs。这使得模型可以trainable end-to-end，并且可以根据特定的数据集和任务进行自动调整。</li>
<li>results: 研究人员使用了CASIA-B数据集进行实验，结果显示了our方法可以提高解释性和任务特定适应性，并且与固定 graphs相比，our方法的结果有着不同的解释性。<details>
<summary>Abstract</summary>
Gait analysis leverages unique walking patterns for person identification and assessment across multiple domains. Among the methods used for gait analysis, skeleton-based approaches have shown promise due to their robust and interpretable features. However, these methods often rely on hand-crafted spatial-temporal graphs that are based on human anatomy disregarding the particularities of the dataset and task. This paper proposes a novel method to simplify the spatial-temporal graph representation for gait-based gender estimation, improving interpretability without losing performance. Our approach employs two models, an upstream and a downstream model, that can adjust the adjacency matrix for each walking instance, thereby removing the fixed nature of the graph. By employing the Straight-Through Gumbel-Softmax trick, our model is trainable end-to-end. We demonstrate the effectiveness of our approach on the CASIA-B dataset for gait-based gender estimation. The resulting graphs are interpretable and differ qualitatively from fixed graphs used in existing models. Our research contributes to enhancing the explainability and task-specific adaptability of gait recognition, promoting more efficient and reliable gait-based biometrics.
</details>
<details>
<summary>摘要</summary>
《走姿分析利用唯一的步态特征进行人体身份识别和评估，在多个领域中得到广泛应用。 Among the methods used for gait analysis, 骨架基 Approaches have shown promise due to their robust and interpretable features. However, these methods often rely on hand-crafted spatial-temporal graphs that are based on human anatomy, disregarding the particularities of the dataset and task. This paper proposes a novel method to simplify the spatial-temporal graph representation for gait-based gender estimation, improving interpretability without losing performance. Our approach employs two models, an upstream and a downstream model, that can adjust the adjacency matrix for each walking instance, thereby removing the fixed nature of the graph. By employing the Straight-Through Gumbel-Softmax trick, our model is trainable end-to-end. We demonstrate the effectiveness of our approach on the CASIA-B dataset for gait-based gender estimation. The resulting graphs are interpretable and differ qualitatively from fixed graphs used in existing models. Our research contributes to enhancing the explainability and task-specific adaptability of gait recognition, promoting more efficient and reliable gait-based biometrics.》Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="OpenPatch-a-3D-patchwork-for-Out-Of-Distribution-detection"><a href="#OpenPatch-a-3D-patchwork-for-Out-Of-Distribution-detection" class="headerlink" title="OpenPatch: a 3D patchwork for Out-Of-Distribution detection"></a>OpenPatch: a 3D patchwork for Out-Of-Distribution detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03388">http://arxiv.org/abs/2310.03388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Rabino, Antonio Alliegro, Francesco Cappio Borlino, Tatiana Tommasi</li>
<li>for: 本研究旨在准备深度学习模型在实际世界中进行部署，以处理不可预期的情况。在某些应用中，新型的出现会带来重要的威胁，因此需要有效地探测它们。这种技能应该可以在需要时使用，而不需要任何额外的计算训练努力。</li>
<li>methods: 本研究使用了OpenPatch方法，基于大量预训练模型，简单地从其中提取了一组patch表示，用于描述每个已知类型。对于新样本，我们可以通过评估该样本是否可以主要通过已知类型的patch组成来获得新型性分数。</li>
<li>results: 本研究在实际点云样本上进行了semantic novelty检测任务，并在全known样本和几个known样本情况下进行了广泛的实验评估。结果表明，OpenPatch在不同的预训练目标和网络背bone下都表现出了强大的稳定性，并且可以在不需要重新训练的情况下应用于各种实际任务。<details>
<summary>Abstract</summary>
Moving deep learning models from the laboratory setting to the open world entails preparing them to handle unforeseen conditions. In several applications the occurrence of novel classes during deployment poses a significant threat, thus it is crucial to effectively detect them. Ideally, this skill should be used when needed without requiring any further computational training effort at every new task. Out-of-distribution detection has attracted significant attention in the last years, however the majority of the studies deal with 2D images ignoring the inherent 3D nature of the real-world and often confusing between domain and semantic novelty. In this work, we focus on the latter, considering the objects geometric structure captured by 3D point clouds regardless of the specific domain. We advance the field by introducing OpenPatch that builds on a large pre-trained model and simply extracts from its intermediate features a set of patch representations that describe each known class. For any new sample, we obtain a novelty score by evaluating whether it can be recomposed mainly by patches of a single known class or rather via the contribution of multiple classes. We present an extensive experimental evaluation of our approach for the task of semantic novelty detection on real-world point cloud samples when the reference known data are synthetic. We demonstrate that OpenPatch excels in both the full and few-shot known sample scenarios, showcasing its robustness across varying pre-training objectives and network backbones. The inherent training-free nature of our method allows for its immediate application to a wide array of real-world tasks, offering a compelling advantage over approaches that need expensive retraining efforts.
</details>
<details>
<summary>摘要</summary>
将深度学习模型从室内设置到开放世界需要准备其处理不可预测的条件。在多个应用程序中，新类的出现会带来重大的威胁，因此可以快速、无需进一步的计算训练努力，检测这些新类。我们在这里关注对象的三维结构，不管具体的领域。我们提出了开放patch（OpenPatch），基于大量预训练模型，简单地从其中提取一些patch表示，用于描述每个已知类。对于任何新样本，我们可以计算一个新类准确性分数，根据是否可以通过已知类中的patch组成。我们对实际世界点云样本进行了广泛的实验评估，证明了我们的方法在全示例和几示例已知样本enario中均表现出色，并且在不同的预训练目标和网络背景下保持了稳定性。由于我们的方法不需要重新训练，可以立即应用于各种实际世界任务，提供了优势。
</details></li>
</ul>
<hr>
<h2 id="ACT-Net-Anchor-context-Action-Detection-in-Surgery-Videos"><a href="#ACT-Net-Anchor-context-Action-Detection-in-Surgery-Videos" class="headerlink" title="ACT-Net: Anchor-context Action Detection in Surgery Videos"></a>ACT-Net: Anchor-context Action Detection in Surgery Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03377">http://arxiv.org/abs/2310.03377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luoying Hao, Yan Hu, Wenjun Lin, Qun Wang, Heng Li, Huazhu Fu, Jinming Duan, Jiang Liu</li>
<li>for: 这篇论文的目的是精确地检测运行整个手术过程中的细部动作，以提高Context-aware决策支持系统的精度。</li>
<li>methods: 这篇论文提出了一个名为ACTNet的检测网络，包括一个 anchor-context检测（ACD）模组和一个分类传播激活（CCD）模组，以回答以下问题：1）动作发生的地方是哪里？2）动作是什么样的？3）预测结果的可信度是什么样的？特别是，ACD模组在运行影片中找到和点选出执行动作的区域，并且根据这些区域的对话来预测动作的位置和分布。CCD模组则使用一个减震传播激活模型，以确定动作的预测结果。</li>
<li>results: 这篇论文的结果显示，这个方法可以实现高精度的动作检测，并且可以提供模型的可信度。与基eline相比，这个方法的MAP值提高了4.0%。<details>
<summary>Abstract</summary>
Recognition and localization of surgical detailed actions is an essential component of developing a context-aware decision support system. However, most existing detection algorithms fail to provide high-accuracy action classes even having their locations, as they do not consider the surgery procedure's regularity in the whole video. This limitation hinders their application. Moreover, implementing the predictions in clinical applications seriously needs to convey model confidence to earn entrustment, which is unexplored in surgical action prediction. In this paper, to accurately detect fine-grained actions that happen at every moment, we propose an anchor-context action detection network (ACTNet), including an anchor-context detection (ACD) module and a class conditional diffusion (CCD) module, to answer the following questions: 1) where the actions happen; 2) what actions are; 3) how confidence predictions are. Specifically, the proposed ACD module spatially and temporally highlights the regions interacting with the extracted anchor in surgery video, which outputs action location and its class distribution based on anchor-context interactions. Considering the full distribution of action classes in videos, the CCD module adopts a denoising diffusion-based generative model conditioned on our ACD estimator to further reconstruct accurately the action predictions. Moreover, we utilize the stochastic nature of the diffusion model outputs to access model confidence for each prediction. Our method reports the state-of-the-art performance, with improvements of 4.0% mAP against baseline on the surgical video dataset.
</details>
<details>
<summary>摘要</summary>
Recognition and localization of surgical detailed actions is an essential component of developing a context-aware decision support system. However, most existing detection algorithms fail to provide high-accuracy action classes even having their locations, as they do not consider the surgery procedure's regularity in the whole video. This limitation hinders their application. Moreover, implementing the predictions in clinical applications seriously needs to convey model confidence to earn entrustment, which is unexplored in surgical action prediction. In this paper, to accurately detect fine-grained actions that happen at every moment, we propose an anchor-context action detection network (ACTNet), including an anchor-context detection (ACD) module and a class conditional diffusion (CCD) module, to answer the following questions: 1) where the actions happen; 2) what actions are; 3) how confidence predictions are. Specifically, the proposed ACD module spatially and temporally highlights the regions interacting with the extracted anchor in surgery video, which outputs action location and its class distribution based on anchor-context interactions. Considering the full distribution of action classes in videos, the CCD module adopts a denoising diffusion-based generative model conditioned on our ACD estimator to further reconstruct accurately the action predictions. Moreover, we utilize the stochastic nature of the diffusion model outputs to access model confidence for each prediction. Our method reports the state-of-the-art performance, with improvements of 4.0% mAP against baseline on the surgical video dataset.Here's a word-for-word translation of the text into Simplified Chinese:Recognition和localization of surgical detailed actions是developing context-aware decision support system的关键组成部分。然而，大多数现有的探测算法无法提供高精度的动作类别，即使有其位置信息，因为它们不考虑手术程序整体视频中的规则性。这种限制约束了其应用。此外，在临床应用中，实现预测结果的应用需要传递模型confidence来赢得信任，这在手术动作预测中未经explored。在这篇论文中，我们提议一种名为 anchor-context action detection network (ACTNet)，包括一个 anchor-context detection (ACD)模块和一个类别 conditioned diffusion (CCD)模块，以回答以下问题：1) where the actions happen; 2) what actions are; 3) how confidence predictions are。具体来说，我们的 ACD模块在手术视频中将抽取的 anchor 与其相互作用的区域进行空间和时间高亮显示，输出动作的位置和类别分布。考虑整个视频中动作类别的全部分布，我们的 CCD模块采用一种denoising diffusion-based generative model，根据我们的 ACD 估计器来进一步重建准确的动作预测。此外，我们利用 diffusion model 输出的随机性来访问模型信任度。我们的方法在手术视频数据集上报告了状态艺术性的表现，与基准相比提高了4.0% mAP。
</details></li>
</ul>
<hr>
<h2 id="Point-Based-Radiance-Fields-for-Controllable-Human-Motion-Synthesis"><a href="#Point-Based-Radiance-Fields-for-Controllable-Human-Motion-Synthesis" class="headerlink" title="Point-Based Radiance Fields for Controllable Human Motion Synthesis"></a>Point-Based Radiance Fields for Controllable Human Motion Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03375">http://arxiv.org/abs/2310.03375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dehezhang2/point_based_nerf_editing">https://github.com/dehezhang2/point_based_nerf_editing</a></li>
<li>paper_authors: Haitao Yu, Deheng Zhang, Peiyuan Xie, Tianyi Zhang</li>
<li>for: 本 paper 提出了一种新的可控人体动作合成方法，用于细粒度变形。以前的编辑神经透镜场方法可以生成出吸引人的结果，但它们几乎无法实现复杂的3D人体编辑，如前向骨骼动作。我们的方法利用了明确的点云来训练静态3D场景，并通过编码点云转移来应用变形。</li>
<li>methods: 我们的方法使用了静态点云来训练静态3D场景，并通过编码点云转移来应用变形。我们还使用了SVD来估计本地旋转，并通过插值来将每个点的旋转转化到查询视图方向上。</li>
<li>results: 我们的方法可以对细粒度变形进行高效的控制，并且可以泛化到其他3D角色 besides humans。我们的实验结果表明，我们的方法可以与当前状态静态点云场景的最佳实现竞争。<details>
<summary>Abstract</summary>
This paper proposes a novel controllable human motion synthesis method for fine-level deformation based on static point-based radiance fields. Although previous editable neural radiance field methods can generate impressive results on novel-view synthesis and allow naive deformation, few algorithms can achieve complex 3D human editing such as forward kinematics. Our method exploits the explicit point cloud to train the static 3D scene and apply the deformation by encoding the point cloud translation using a deformation MLP. To make sure the rendering result is consistent with the canonical space training, we estimate the local rotation using SVD and interpolate the per-point rotation to the query view direction of the pre-trained radiance field. Extensive experiments show that our approach can significantly outperform the state-of-the-art on fine-level complex deformation which can be generalized to other 3D characters besides humans.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的可控人体动作合成方法，基于静止点云基于辐射场。 although previous neural radiance field方法可以生成印象深刻的结果，但它们很难实现复杂的3D人体编辑，如前向运动。 our method使用点云进行Explicit 3D场景训练，并通过编码点云平移来应用塑形。 to ensure the rendering result is consistent with the canonical space training, we estimate the local rotation using SVD and interpolate the per-point rotation to the query view direction of the pre-trained radiance field. extensive experiments show that our approach can significantly outperform the state-of-the-art on fine-level complex deformation, which can be generalized to other 3D characters besides humans.
</details></li>
</ul>
<hr>
<h2 id="Realistic-Speech-to-Face-Generation-with-Speech-Conditioned-Latent-Diffusion-Model-with-Face-Prior"><a href="#Realistic-Speech-to-Face-Generation-with-Speech-Conditioned-Latent-Diffusion-Model-with-Face-Prior" class="headerlink" title="Realistic Speech-to-Face Generation with Speech-Conditioned Latent Diffusion Model with Face Prior"></a>Realistic Speech-to-Face Generation with Speech-Conditioned Latent Diffusion Model with Face Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03363">http://arxiv.org/abs/2310.03363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinting Wang, Li Liu, Jun Wang, Hei Victor Cheng</li>
<li>for: 这个论文的目的是提出一种新的语音到脸图生成框架，以解决现有的语音到脸图生成方法中存在的不稳定性和无法生成真实的脸图问题。</li>
<li>methods: 该框架基于一种新型的噪声扩散模型（SCLDM），并采用对比预训练来保持语音和脸图之间的共同特征信息。此外，我们还提出了一种新的增强方法，通过将统计面积至 estadístico incorporated into the diffusion process to eliminate the shared component across the faces and enhance the subtle variations captured by the speech condition.</li>
<li>results: 我们的方法可以生成更加真实的脸图，同时保持说话人的身份特征。经验表明，我们的方法在AVSpeech和Voxceleb两个 dataset上具有显著的改进，特别是在cosine distance metric上的提高。例如，在AVSpeech dataset上，我们的方法提高了32.17和32.72的cosine distance metric，对比之前的最佳方法，提高了23.53%和25.37%。<details>
<summary>Abstract</summary>
Speech-to-face generation is an intriguing area of research that focuses on generating realistic facial images based on a speaker's audio speech. However, state-of-the-art methods employing GAN-based architectures lack stability and cannot generate realistic face images. To fill this gap, we propose a novel speech-to-face generation framework, which leverages a Speech-Conditioned Latent Diffusion Model, called SCLDM. To the best of our knowledge, this is the first work to harness the exceptional modeling capabilities of diffusion models for speech-to-face generation. Preserving the shared identity information between speech and face is crucial in generating realistic results. Therefore, we employ contrastive pre-training for both the speech encoder and the face encoder. This pre-training strategy facilitates effective alignment between the attributes of speech, such as age and gender, and the corresponding facial characteristics in the face images. Furthermore, we tackle the challenge posed by excessive diversity in the synthesis process caused by the diffusion model. To overcome this challenge, we introduce the concept of residuals by integrating a statistical face prior to the diffusion process. This addition helps to eliminate the shared component across the faces and enhances the subtle variations captured by the speech condition. Extensive quantitative, qualitative, and user study experiments demonstrate that our method can produce more realistic face images while preserving the identity of the speaker better than state-of-the-art methods. Highlighting the notable enhancements, our method demonstrates significant gains in all metrics on the AVSpeech dataset and Voxceleb dataset, particularly noteworthy are the improvements of 32.17 and 32.72 on the cosine distance metric for the two datasets, respectively.
</details>
<details>
<summary>摘要</summary>
《speech-to-face》是一个吸引人的研究领域，它旨在基于说话人的音频speech生成真实的脸部图像。然而，当前的方法使用GAN结构，缺乏稳定性，无法生成真实的脸部图像。为了填补这一漏洞，我们提出了一种新的speech-to-face生成框架，它利用了一种叫做Speech-Conditioned Latent Diffusion Model（SCLDM）。据我们所知，这是首次利用扩散模型来进行speech-to-face生成。在生成真实结果的同时，保持说话人的身份信息与脸部图像之间的共同性是关键。因此，我们使用了对比预训练，使得说话人的年龄和性别特征与对应的脸部特征进行有效的对应。此外，我们解决了由扩散模型引起的生成过程中的过度多样性挑战。我们在扩散过程中添加了一个统计学面壳，以消除共同的部分在脸部图像中，并使得扩散过程中的微妙变化更加明显。经验证明，我们的方法可以生成更真实的脸部图像，同时保持说话人的身份。特别是，在AVSpeech和Voxceleb两个 dataset上，我们的方法表现出了显著的提升，cosine distance指标上的提升分别为32.17和32.72。
</details></li>
</ul>
<hr>
<h2 id="CSI-Enhancing-the-Robustness-of-3D-Point-Cloud-Recognition-against-Corruption"><a href="#CSI-Enhancing-the-Robustness-of-3D-Point-Cloud-Recognition-against-Corruption" class="headerlink" title="CSI: Enhancing the Robustness of 3D Point Cloud Recognition against Corruption"></a>CSI: Enhancing the Robustness of 3D Point Cloud Recognition against Corruption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03360">http://arxiv.org/abs/2310.03360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masterwu2115/csi">https://github.com/masterwu2115/csi</a></li>
<li>paper_authors: Zhuoyuan Wu, Jiachen Sun, Chaowei Xiao</li>
<li>for: 提高点云识别 robustness 对于实际世界中的数据损害</li>
<li>methods: 利用点云数据的自然集 свой性，提出一种新的极限子集标识（CSI）方法，包括具有强度感知抽样（DAS）和自身熵减少（SEM）两部分</li>
<li>results: 与比较方法相比，CSI方法在ModelNet40-C和PointCloud-C上实现了18.4%和16.3%的错误率，比前者提高5.2%和4.2%，代表了 Notable improvement in point cloud recognition robustness against data corruption.<details>
<summary>Abstract</summary>
Despite recent advancements in deep neural networks for point cloud recognition, real-world safety-critical applications present challenges due to unavoidable data corruption. Current models often fall short in generalizing to unforeseen distribution shifts. In this study, we harness the inherent set property of point cloud data to introduce a novel critical subset identification (CSI) method, aiming to bolster recognition robustness in the face of data corruption. Our CSI framework integrates two pivotal components: density-aware sampling (DAS) and self-entropy minimization (SEM), which cater to static and dynamic CSI, respectively. DAS ensures efficient robust anchor point sampling by factoring in local density, while SEM is employed during training to accentuate the most salient point-to-point attention. Evaluations reveal that our CSI approach yields error rates of 18.4\% and 16.3\% on ModelNet40-C and PointCloud-C, respectively, marking a notable improvement over state-of-the-art methods by margins of 5.2\% and 4.2\% on the respective benchmarks. Code is available at \href{https://github.com/masterwu2115/CSI/tree/main}{https://github.com/masterwu2115/CSI/tree/main}
</details>
<details>
<summary>摘要</summary>
尽管最近的深度神经网络在点云识别方面做出了重要进步，但在实际应用中仍然面临数据损害的挑战。现有模型经常无法适应不可预测的分布转移。本研究利用点云数据的自然集属性，提出一种新的极值子集标识（CSI）方法，以增强识别的可靠性。我们的CSI框架包括两个重要组成部分：具有地方权重的采样（DAS）和自 entropy 最小化（SEM），它们分别适应静态和动态CSI。DAS 确保高效地采样稳定的均勋点，而 SEM 在训练中被使用，以强调最重要的点对点关注。我们的CSI方法在 ModelNet40-C 和 PointCloud-C 上获得了18.4%和16.3%的错误率，与当前最佳方法相比，占了5.2%和4.2%的较大优势。代码可以在 \href{https://github.com/masterwu2115/CSI/tree/main}{https://github.com/masterwu2115/CSI/tree/main} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Combining-Datasets-with-Different-Label-Sets-for-Improved-Nucleus-Segmentation-and-Classification"><a href="#Combining-Datasets-with-Different-Label-Sets-for-Improved-Nucleus-Segmentation-and-Classification" class="headerlink" title="Combining Datasets with Different Label Sets for Improved Nucleus Segmentation and Classification"></a>Combining Datasets with Different Label Sets for Improved Nucleus Segmentation and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03346">http://arxiv.org/abs/2310.03346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amruta Parulekar, Utkarsh Kanwat, Ravi Kant Gupta, Medha Chippa, Thomas Jacob, Tripti Bameta, Swapnil Rane, Amit Sethi</li>
<li>for:  automatic cell counting and morphometric assessments in histopathology images</li>
<li>methods:  deep neural networks (DNNs) with a coarse-to-fine class hierarchy</li>
<li>results:  improved segmentation and classification metrics on test splits, as well as generalization to previously unseen datasets<details>
<summary>Abstract</summary>
Segmentation and classification of cell nuclei in histopathology images using deep neural networks (DNNs) can save pathologists' time for diagnosing various diseases, including cancers, by automating cell counting and morphometric assessments. It is now well-known that the accuracy of DNNs increases with the sizes of annotated datasets available for training. Although multiple datasets of histopathology images with nuclear annotations and class labels have been made publicly available, the set of class labels differ across these datasets. We propose a method to train DNNs for instance segmentation and classification on multiple datasets where the set of classes across the datasets are related but not the same. Specifically, our method is designed to utilize a coarse-to-fine class hierarchy, where the set of classes labeled and annotated in a dataset can be at any level of the hierarchy, as long as the classes are mutually exclusive. Within a dataset, the set of classes need not even be at the same level of the class hierarchy tree. Our results demonstrate that segmentation and classification metrics for the class set used by the test split of a dataset can improve by pre-training on another dataset that may even have a different set of classes due to the expansion of the training set enabled by our method. Furthermore, generalization to previously unseen datasets also improves by combining multiple other datasets with different sets of classes for training. The improvement is both qualitative and quantitative. The proposed method can be adapted for various loss functions, DNN architectures, and application domains.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）可以自动完成 Histopathology 图像中细胞核心的分割和分类，从而为诊断多种疾病，包括癌症，节省病理医生的时间。现在已经证明，DNNs 的准确性与用于训练的数据集的大小成正相关。虽然多个历史病理图像数据集已经公开发布，但这些数据集中的类别标签不同。我们提出了一种方法，可以在不同数据集上训练 DNNs 进行实例分割和分类，其中数据集中的类别标签可以是归一化的树结构中的任何一级，只要这些类别是互斥的。在一个数据集中，类别标签不必是同一级别的树结构中的。我们的结果表明，在使用另一个数据集进行预训练后，对测试分割的分割和分类指标可以得到改进，并且将多个不同数据集合并训练可以提高总体化和推广性。这种方法可以适应不同的损失函数、DNN 架构和应用领域。
</details></li>
</ul>
<hr>
<h2 id="Denoising-Diffusion-Step-aware-Models"><a href="#Denoising-Diffusion-Step-aware-Models" class="headerlink" title="Denoising Diffusion Step-aware Models"></a>Denoising Diffusion Step-aware Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03337">http://arxiv.org/abs/2310.03337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, Yingcong Chen</li>
<li>for: 提高 diffusion model 的计算效率，使其适用于更广泛的数据生成任务。</li>
<li>methods: 使用 spectrum of neural networks ，其中每个网络的大小根据每个生成步骤的重要性进行调整，通过遗传搜索确定。</li>
<li>results: 对 CIFAR-10、CelebA-HQ、LSUN-bedroom、AFHQ 和 ImageNet 等数据集进行了实验，显示 DDSM 可以提高计算效率，对应的计算时间减少了 49%、61%、59%、71% 和 76%，而不会 sacrificing 生成质量。<details>
<summary>Abstract</summary>
Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for data generation across various domains. However, a significant bottleneck is the necessity for whole-network computation during every step of the generative process, leading to high computational overheads. This paper presents a novel framework, Denoising Diffusion Step-aware Models (DDSM), to address this challenge. Unlike conventional approaches, DDSM employs a spectrum of neural networks whose sizes are adapted according to the importance of each generative step, as determined through evolutionary search. This step-wise network variation effectively circumvents redundant computational efforts, particularly in less critical steps, thereby enhancing the efficiency of the diffusion model. Furthermore, the step-aware design can be seamlessly integrated with other efficiency-geared diffusion models such as DDIMs and latent diffusion, thus broadening the scope of computational savings. Empirical evaluations demonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61% for CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all without compromising the generation quality. Our code and models will be publicly available.
</details>
<details>
<summary>摘要</summary>
Diffusion Probabilistic Models (DDPMs) 有很多应用领域的数据生成，但是存在一个主要的瓶颈是每次生成过程中整个网络的计算 overhead，这导致了高效性的问题。这篇文章提出了一种新的框架，即Denosing Diffusion Step-aware Models (DDSM)，以解决这个挑战。与传统方法不同，DDSM 使用了一个适应性的spectrum of neural networks，其中每个网络的大小根据生成过程中每个步骤的重要性来确定，通过进化搜索来确定。这种步骤 wise network variation 可以减少 redundant computational efforts，特别是在 less critical steps，从而提高 diffusion model 的效率。此外，步骤 aware 的设计可以与其他高效 diffusion model such as DDIMs 和 latent diffusion 集成，从而拓宽了计算省力的范围。我们的实验证明，DDSM 可以在 CIFAR-10 上实现49%的计算减少，在 CelebA-HQ 上实现61%的计算减少，在 LSUN-bedroom 上实现59%的计算减少，在 AFHQ 上实现71%的计算减少，并在 ImageNet 上实现76%的计算减少，无需牺牲生成质量。我们的代码和模型将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Continual-Test-time-Domain-Adaptation-via-Dynamic-Sample-Selection"><a href="#Continual-Test-time-Domain-Adaptation-via-Dynamic-Sample-Selection" class="headerlink" title="Continual Test-time Domain Adaptation via Dynamic Sample Selection"></a>Continual Test-time Domain Adaptation via Dynamic Sample Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03335">http://arxiv.org/abs/2310.03335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanshuo Wang, Jie Hong, Ali Cheraghian, Shafin Rahman, David Ahmedt-Aristizabal, Lars Petersson, Mehrtash Harandi</li>
<li>for: 这篇论文的目的是提出一种 continual test-time domain adaptation (CTDA) 方法，以逐步适应一串目标领域 без 访问原始数据。</li>
<li>methods: 本文提出了一种 Dynamic Sample Selection (DSS) 方法，包括动态阈值、正面学习和负面学习三个过程。传统上，模型从未知环境数据中学习，并将所有样本的 Pseudo-label 作为更新模型参数的来源。但是，这些 Pseudo-label 可能会受到杂音的影响，因此所有样本不是Equally trustworthy。因此，我们首先设计了一个动态阈值模组，选择可疑的低质量样本。选择了低质量样本的 samples 更可能是错误预测的。因此，我们将 JOINT 正面和负面学习应用到高质量和低质量样本上，以减少使用错误信息的风险。</li>
<li>results: 我们进行了广泛的实验，证明我们的提出的方法在图像领域中实现了 CTDA 的最佳效果，超越了当前的州际结果。此外，我们的方法还被评估在 3D 点云领域中，展示了它的多元性和应用的可能性。<details>
<summary>Abstract</summary>
The objective of Continual Test-time Domain Adaptation (CTDA) is to gradually adapt a pre-trained model to a sequence of target domains without accessing the source data. This paper proposes a Dynamic Sample Selection (DSS) method for CTDA. DSS consists of dynamic thresholding, positive learning, and negative learning processes. Traditionally, models learn from unlabeled unknown environment data and equally rely on all samples' pseudo-labels to update their parameters through self-training. However, noisy predictions exist in these pseudo-labels, so all samples are not equally trustworthy. Therefore, in our method, a dynamic thresholding module is first designed to select suspected low-quality from high-quality samples. The selected low-quality samples are more likely to be wrongly predicted. Therefore, we apply joint positive and negative learning on both high- and low-quality samples to reduce the risk of using wrong information. We conduct extensive experiments that demonstrate the effectiveness of our proposed method for CTDA in the image domain, outperforming the state-of-the-art results. Furthermore, our approach is also evaluated in the 3D point cloud domain, showcasing its versatility and potential for broader applicability.
</details>
<details>
<summary>摘要</summary>
CTDA 的目标是慢慢地适应一系列目标领域的模型，无需访问源数据。这篇论文提出了动态样本选择（DSS）方法。DSS包括动态阈值调整、正例学习和负例学习过程。传统上，模型从未知环境数据中学习，并且均依赖所有样本的假标签来更新参数通过自我训练。然而，数据中的预测结果存在噪音，因此所有样本都不是 equally trustworthy。因此，我们首先设计了动态阈值模块，选择可疑的低质量样本。选择的低质量样本更有可能错误预测。因此，我们应用了联合正例和负例学习在高质量和低质量样本上进行减风险。我们进行了广泛的实验，证明我们提出的方法在图像频谱中实现了 CTDA，并超越了现状势力的结果。此外，我们的方法还在3D点云频谱中进行了评估，展示了其 universality 和普遍性。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Multi-modal-Object-Detection-and-Tracking-on-Edge-for-Regulatory-Compliance-Monitoring"><a href="#Real-time-Multi-modal-Object-Detection-and-Tracking-on-Edge-for-Regulatory-Compliance-Monitoring" class="headerlink" title="Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring"></a>Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03333">http://arxiv.org/abs/2310.03333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Syuen Lim, Ziwei Wang, Jiajun Liu, Abdelwahed Khamis, Reza Arablouei, Robert Barlow, Ryan McAllister</li>
<li>for: 实现在多元领域的监管遵循性实现高品质保证和可追溯性。</li>
<li>methods: 使用实时多感器探测系统，包括3D时间探测和RGB摄像头，联合无监督学习技术在边缘AI设备上。</li>
<li>results: 提高记录保持效率，减少手动干预，并在 agrifood 设施中认真验证刀具清洁效果，掌握 occlusion 和低光照等问题。<details>
<summary>Abstract</summary>
Regulatory compliance auditing across diverse industrial domains requires heightened quality assurance and traceability. Present manual and intermittent approaches to such auditing yield significant challenges, potentially leading to oversights in the monitoring process. To address these issues, we introduce a real-time, multi-modal sensing system employing 3D time-of-flight and RGB cameras, coupled with unsupervised learning techniques on edge AI devices. This enables continuous object tracking thereby enhancing efficiency in record-keeping and minimizing manual interventions. While we validate the system in a knife sanitization context within agrifood facilities, emphasizing its prowess against occlusion and low-light issues with RGB cameras, its potential spans various industrial monitoring settings.
</details>
<details>
<summary>摘要</summary>
政策遵循审核 Across 多个工业领域需要提高质量控制和可追溯性。现有的手动和间歇性方法在审核过程中存在重要的挑战，可能导致监测过程中的漏洞。为解决这些问题，我们介绍一种实时、多模式感知系统，使用3D时间旋转和RGB相机，并与边缘AI设备结合不监督学习技术。这使得对象的连续跟踪而提高记录保存的效率，并减少手动干预。我们在农业食品设施中 validate 这种系统，强调其在遮盖和低光照问题下的强健性，但其潜在适用范围包括多个工业监测场景。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Limitation-of-CLIP-Models-The-Worst-Performing-Categories"><a href="#Investigating-the-Limitation-of-CLIP-Models-The-Worst-Performing-Categories" class="headerlink" title="Investigating the Limitation of CLIP Models: The Worst-Performing Categories"></a>Investigating the Limitation of CLIP Models: The Worst-Performing Categories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03324">http://arxiv.org/abs/2310.03324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie-Jing Shao, Jiang-Xin Shi, Xiao-Wen Yang, Lan-Zhe Guo, Yu-Feng Li</li>
<li>for: 提高 CLIP 模型在特定类别下的表现，尤其是在风险敏感应用中，其中一些类别具有重要性。</li>
<li>methods: 研究 CLIP 模型两Modalities 的吻合，并提出了类别匹配margin（\cmm）来衡量推理冲击。</li>
<li>results: 通过查询大型自然语言模型和建立权重和 ensemble，提高了 ImageNet 上最差10类的准确率，从0%提高到5.2%，无需手动工程提示、劳辑优化或访问标注验证数据。<details>
<summary>Abstract</summary>
Contrastive Language-Image Pre-training (CLIP) provides a foundation model by integrating natural language into visual concepts, enabling zero-shot recognition on downstream tasks. It is usually expected that satisfactory overall accuracy can be achieved across numerous domains through well-designed textual prompts. However, we found that their performance in the worst categories is significantly inferior to the overall performance. For example, on ImageNet, there are a total of 10 categories with class-wise accuracy as low as 0\%, even though the overall performance has achieved 64.1\%. This phenomenon reveals the potential risks associated with using CLIP models, particularly in risk-sensitive applications where specific categories hold significant importance. To address this issue, we investigate the alignment between the two modalities in the CLIP model and propose the Class-wise Matching Margin (\cmm) to measure the inference confusion. \cmm\ can effectively identify the worst-performing categories and estimate the potential performance of the candidate prompts. We further query large language models to enrich descriptions of worst-performing categories and build a weighted ensemble to highlight the efficient prompts. Experimental results clearly verify the effectiveness of our proposal, where the accuracy on the worst-10 categories on ImageNet is boosted to 5.2\%, without manual prompt engineering, laborious optimization, or access to labeled validation data.
</details>
<details>
<summary>摘要</summary>
CLIP（对比语言图像预训练）提供了一个基本模型，将自然语言和视觉概念集成起来，以实现零shot认知任务。通常认为，通过Well-designed文本提示，可以在多个领域达到可接受的总体精度。然而，我们发现CLIP模型在最差类别表现不佳，比总体表现低至0%。例如，在ImageNet中有10个类别，其中每个类别的精度只有0%。这种现象表明CLIP模型在风险敏感应用中可能存在风险，特别是在特定类别具有重要性时。为解决这个问题，我们调查CLIP模型两个模式之间的对应关系，并提出了类别匹配margin（CMM）来度量推理冲击。CMM可以准确地确定最差表现的类别，并估算候选提示的可能性。我们进一步咨询大型自然语言模型，以描述最差表现的类别，并建立了权重 ensemble，以强调高效的提示。实验结果表明，我们的提议有效，ImageNet最差10类精度从0%提高到5.2%，无需手动提取工程、繁琐优化或访问标注验证数据。
</details></li>
</ul>
<hr>
<h2 id="Functional-data-learning-using-convolutional-neural-networks"><a href="#Functional-data-learning-using-convolutional-neural-networks" class="headerlink" title="Functional data learning using convolutional neural networks"></a>Functional data learning using convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03773">http://arxiv.org/abs/2310.03773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jesusgl86/fdap01">https://github.com/jesusgl86/fdap01</a></li>
<li>paper_authors: Jose Galarza, Tamer Oraby</li>
<li>for: 这个论文目的是使用卷积神经网络（CNN）来解决功能数据中的回归和分类问题，特别是面临噪音和非噪音功能数据的情况。</li>
<li>methods: 这个方法是将功能数据转换为28x28图像，并使用特定的卷积神经网络来进行所有的回归运算和函数形式分类。</li>
<li>results: 这个方法可以实现高精度的回归和分类，并且可以应对噪音和非噪音功能数据。实验结果显示，这个方法可以成功地估计楕円增长和均值、幅度和峰值的大小，以及楕円函数的傅立叶 exponent和束缚问题。此外，这个方法还可以用于检测疾病传播率、药物溶解 profiling、和检测公元病例。<details>
<summary>Abstract</summary>
In this paper, we show how convolutional neural networks (CNN) can be used in regression and classification learning problems of noisy and non-noisy functional data. The main idea is to transform the functional data into a 28 by 28 image. We use a specific but typical architecture of a convolutional neural network to perform all the regression exercises of parameter estimation and functional form classification. First, we use some functional case studies of functional data with and without random noise to showcase the strength of the new method. In particular, we use it to estimate exponential growth and decay rates, the bandwidths of sine and cosine functions, and the magnitudes and widths of curve peaks. We also use it to classify the monotonicity and curvatures of functional data, algebraic versus exponential growth, and the number of peaks of functional data. Second, we apply the same convolutional neural networks to Lyapunov exponent estimation in noisy and non-noisy chaotic data, in estimating rates of disease transmission from epidemic curves, and in detecting the similarity of drug dissolution profiles. Finally, we apply the method to real-life data to detect Parkinson's disease patients in a classification problem. The method, although simple, shows high accuracy and is promising for future use in engineering and medical applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们展示了如何使用卷积神经网络（CNN）在有噪和无噪函数数据上进行回归和分类学习问题。主要思想是将函数数据转换成28x28图像。我们使用了一种特定 yet typical的卷积神经网络架构来实现所有的参数估计和函数形态分类问题。首先，我们使用了一些函数案例研究，包括带有噪声和无噪声的函数数据，以示新方法的强大性。我们使用它来估计指数增长和减速率、振荡函数的宽度和峰值强度、函数数据的 monotonicity 和曲线性、函数数据的 algebraic 增长和指数增长、函数数据的峰值数量等。其次，我们对噪声和无噪声杂化数据中的 Lyapunov 指数进行估计，从 epidemic 曲线中估计疾病传播率，并在药物溶解曲线上检测同义性。最后，我们应用这种方法到实际数据，以进行 Parkinson 病患分类问题。这种简单的方法具有高准确率，并在工程和医学应用中表示了承诺。
</details></li>
</ul>
<hr>
<h2 id="Can-pre-trained-models-assist-in-dataset-distillation"><a href="#Can-pre-trained-models-assist-in-dataset-distillation" class="headerlink" title="Can pre-trained models assist in dataset distillation?"></a>Can pre-trained models assist in dataset distillation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03295">http://arxiv.org/abs/2310.03295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaolu-zjut/ddinterpreter">https://github.com/yaolu-zjut/ddinterpreter</a></li>
<li>paper_authors: Yao Lu, Xuguang Chen, Yuchen Zhang, Jianyang Gu, Tianle Zhang, Yifan Zhang, Xiaoniu Yang, Qi Xuan, Kai Wang, Yang You</li>
<li>for: 本研究旨在探讨Pre-trained Models（PTMs）是否能够有效地传递知识到合成 dataset，以便DD可以准确地进行。</li>
<li>methods: 我们通过进行先验实验，并系统地研究不同的PTMs选项，包括初始化参数、模型架构、训练轮数和领域知识，以探讨PTMs对DD的贡献。</li>
<li>results: 我们发现：1）提高模型多样性可以提高合成dataset的性能; 2）不但优质模型可以帮助DD，而且在某些情况下，它们可以超越训练得非常好的模型; 3）领域特定的PTMs不是必需的，但是适应领域的PTMs可以提高DD的性能。通过选择最佳选项，我们可以大幅提高cross-architecture泛化性能。<details>
<summary>Abstract</summary>
Dataset Distillation (DD) is a prominent technique that encapsulates knowledge from a large-scale original dataset into a small synthetic dataset for efficient training. Meanwhile, Pre-trained Models (PTMs) function as knowledge repositories, containing extensive information from the original dataset. This naturally raises a question: Can PTMs effectively transfer knowledge to synthetic datasets, guiding DD accurately? To this end, we conduct preliminary experiments, confirming the contribution of PTMs to DD. Afterwards, we systematically study different options in PTMs, including initialization parameters, model architecture, training epoch and domain knowledge, revealing that: 1) Increasing model diversity enhances the performance of synthetic datasets; 2) Sub-optimal models can also assist in DD and outperform well-trained ones in certain cases; 3) Domain-specific PTMs are not mandatory for DD, but a reasonable domain match is crucial. Finally, by selecting optimal options, we significantly improve the cross-architecture generalization over baseline DD methods. We hope our work will facilitate researchers to develop better DD techniques. Our code is available at https://github.com/yaolu-zjut/DDInterpreter.
</details>
<details>
<summary>摘要</summary>
dataset 简化 (DD) 是一种广泛应用的技术，它将原始数据集中的知识封装到一个小型的 sintetic 数据集上，以便高效地训练。同时，先修学模型 (PTM) 作为知识库，含有原始数据集中的广泛信息。这 naturally 引起了一个问题：PTM 是否可以正确地传递知识到 sintetic 数据集，以便 DD 准确地进行？为此，我们进行了初步的实验，并证明了 PTM 对 DD 的贡献。后续，我们系统地研究了不同的 PTM 选项，包括初始化参数、模型架构、训练轮数和领域知识，发现：1）提高模型多样性可以提高 sintetic 数据集的性能；2）不佳的模型也可以帮助 DD 进行，在某些情况下超越了训练得非常好的模型；3）领域特定的 PTM 并不是 DD 的必要条件，但是领域匹配是非常重要。最后，通过选择优化的选项，我们可以显著提高基eline DD 方法的跨建制泛化性能。我们希望我们的工作能够促进研究人员开发更好的 DD 技术。我们的代码可以在 <https://github.com/yaolu-zjut/DDInterpreter> 上找到。
</details></li>
</ul>
<hr>
<h2 id="SimVLG-Simple-and-Efficient-Pretraining-of-Visual-Language-Generative-Models"><a href="#SimVLG-Simple-and-Efficient-Pretraining-of-Visual-Language-Generative-Models" class="headerlink" title="SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models"></a>SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03291">http://arxiv.org/abs/2310.03291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiren Jian, Tingkai Liu, Yunzhe Tao, Soroush Vosoughi, Hongxia Yang</li>
<li>for: 这篇论文目的是提出一种高效的视觉语言生成模型预训练方法，使用冻结的大型自然语言模型（LLM）。</li>
<li>methods: 该方法使用一个单阶段、单loss的框架，通过在训练过程中逐渐合并相似的视觉token来压缩视觉信息，保留 semantic content的 ricness，以实现快速的训练速度。</li>
<li>results: 对于视觉语言模型的训练，该方法可以提高训练速度 $\times 5$ 而无需减少性能，并且可以使用只有 $1&#x2F;10$ 的数据 achieve 相当的性能。此外，该方法还可以将图像语言模型扩展到视频语言生成任务，通过一种新的软注意力 temporal token合并模块。<details>
<summary>Abstract</summary>
In this paper, we propose ``SimVLG'', a streamlined framework for the pre-training of computationally intensive vision-language generative models, leveraging frozen pre-trained large language models (LLMs). The prevailing paradigm in vision-language pre-training (VLP) typically involves a two-stage optimization process: an initial resource-intensive phase dedicated to general-purpose vision-language representation learning, aimed at extracting and consolidating pertinent visual features, followed by a subsequent phase focusing on end-to-end alignment between visual and linguistic modalities. Our one-stage, single-loss framework circumvents the aforementioned computationally demanding first stage of training by gradually merging similar visual tokens during training. This gradual merging process effectively compacts the visual information while preserving the richness of semantic content, leading to fast convergence without sacrificing performance. Our experiments show that our approach can speed up the training of vision-language models by a factor $\times 5$ without noticeable impact on the overall performance. Additionally, we show that our models can achieve comparable performance to current vision-language models with only $1/10$ of the data. Finally, we demonstrate how our image-text models can be easily adapted to video-language generative tasks through a novel soft attentive temporal token merging modules.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了``SimVLG''框架，用于预训 computationally intensive的视觉语言生成模型，利用冰结的大型语言模型（LLM）。传统的视觉语言预训（VLP）方法通常包括两个阶段的优化过程：首先是一个资源占用 intensives的阶段，用于学习通用的视觉语言表示，然后是一个结合视觉语言Modalities的阶段。我们的一个阶段、单个损失框架可以 circumvent这个 computationally demanding的第一阶段训练，通过在训练过程中逐渐合并相似的视觉符号来压缩视觉信息，同时保持 semantics的 ricness，从而实现快速的训练 convergence 而无需牺牲性能。我们的实验表明，我们的方法可以将视觉语言模型的训练速度提高五倍，而无需注意到性能的影响。此外，我们还证明了我们的模型可以通过只使用一半的数据来实现与当前视觉语言模型相同的性能。最后，我们展示了我们的图像文本模型可以通过一种新的软注意时间符号合并模块来简单地适应视频语言生成任务。
</details></li>
</ul>
<hr>
<h2 id="PoseAction-Action-Recognition-for-Patients-in-the-Ward-using-Deep-Learning-Approaches"><a href="#PoseAction-Action-Recognition-for-Patients-in-the-Ward-using-Deep-Learning-Approaches" class="headerlink" title="PoseAction: Action Recognition for Patients in the Ward using Deep Learning Approaches"></a>PoseAction: Action Recognition for Patients in the Ward using Deep Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03288">http://arxiv.org/abs/2310.03288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zherui Li, Raye Chen-Hua Yeow</li>
<li>for: 这篇论文是为了提出一个基于计算机视觉和深度学习的方法，用于在医院内部的人员行为识别和预测。</li>
<li>methods: 这篇论文使用了OpenPose来准确地检测人员的位置，并使用AlphAction的异步互动聚合网络来预测人员的动作。这两个模型结合使用，称为PoseAction。</li>
<li>results: PoseAction模型在识别12种常见的医院区域动作时取得了98.72%的分类MAP（<a href="mailto:&#x49;&#x6f;&#85;&#64;&#x30;&#46;&#x35;">&#x49;&#x6f;&#85;&#64;&#x30;&#46;&#x35;</a>）的最高成绩。此外，这篇论文还开发了一个在线实时模式，将支持医疗译rezension的实现。此外，使用OpenPose的面部点检测功能，这篇论文还实现了面部模糊，以保护病人和医疗工作者的隐私。<details>
<summary>Abstract</summary>
Real-time intelligent detection and prediction of subjects' behavior particularly their movements or actions is critical in the ward. This approach offers the advantage of reducing in-hospital care costs and improving the efficiency of healthcare workers, which is especially true for scenarios at night or during peak admission periods. Therefore, in this work, we propose using computer vision (CV) and deep learning (DL) methods for detecting subjects and recognizing their actions. We utilize OpenPose as an accurate subject detector for recognizing the positions of human subjects in the video stream. Additionally, we employ AlphAction's Asynchronous Interaction Aggregation (AIA) network to predict the actions of detected subjects. This integrated model, referred to as PoseAction, is proposed. At the same time, the proposed model is further trained to predict 12 common actions in ward areas, such as staggering, chest pain, and falling down, using medical-related video clips from the NTU RGB+D and NTU RGB+D 120 datasets. The results demonstrate that PoseAction achieves the highest classification mAP of 98.72% (IoU@0.5). Additionally, this study develops an online real-time mode for action recognition, which strongly supports the clinical translation of PoseAction. Furthermore, using OpenPose's function for recognizing face key points, we also implement face blurring, which is a practical solution to address the privacy protection concerns of patients and healthcare workers. Nevertheless, the training data for PoseAction is currently limited, particularly in terms of label diversity. Consequently, the subsequent step involves utilizing a more diverse dataset (including general actions) to train the model's parameters for improved generalization.
</details>
<details>
<summary>摘要</summary>
“现场智能探测和预测病人的行为，特别是其运动或动作，在医院中是非常重要的。这种方法可以降低医院内部门成本和提高医疗工作者的效率，尤其在夜间或峰值 admit 期间。因此，在这种工作中，我们提议使用计算机视觉（CV）和深度学习（DL）方法来探测和识别病人的动作。我们使用 OpenPose 作为准确的人体探测器，并使用 AlphAction 的异步互动聚合（AIA）网络预测病人的动作。这个整体模型被称为 PoseAction。同时，我们进一步训练这个模型，以预测医院区域中的 12 种常见动作，如摇摆、胸痛和跌倒。结果表明，PoseAction 达到了最高的分类MAP 98.72%（IoU@0.5）。此外，本研究还开发了在线实时模式，以便支持临床翻译。此外，通过 OpenPose 的人脸关键点识别功能，我们还实现了面部模糊，这是一个实际的解决方案，以保护患者和医疗工作者的隐私。然而，PoseAction 的训练数据目前还受限，特别是Label多样性不够。因此，后续步骤是使用更多的多样化数据（包括通用动作）来训练模型的参数，以提高其泛化能力。”
</details></li>
</ul>
<hr>
<h2 id="Classifying-Whole-Slide-Images-What-Matters"><a href="#Classifying-Whole-Slide-Images-What-Matters" class="headerlink" title="Classifying Whole Slide Images: What Matters?"></a>Classifying Whole Slide Images: What Matters?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03279">http://arxiv.org/abs/2310.03279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Nguyen, Aiden Nibali, Joshua Millward, Zhen He</li>
<li>for: 这篇论文旨在研究把握高分辨率整幕报告（WSIs）的分类算法。</li>
<li>methods: 这篇论文使用了不同的设计选择来探索WSIs分类算法中最重要的特征。</li>
<li>results: 研究发现，在WSIs分类中，最重要的特征是在小 patch 级别上捕捉的地方环境细节，而不是全幕级别的全球信息。此外，一种简单的多实例学习方法，不捕捉全球信息，也可以达到高精度。<details>
<summary>Abstract</summary>
Recently there have been many algorithms proposed for the classification of very high resolution whole slide images (WSIs). These new algorithms are mostly focused on finding novel ways to combine the information from small local patches extracted from the slide, with an emphasis on effectively aggregating more global information for the final predictor. In this paper we thoroughly explore different key design choices for WSI classification algorithms to investigate what matters most for achieving high accuracy. Surprisingly, we found that capturing global context information does not necessarily mean better performance. A model that captures the most global information consistently performs worse than a model that captures less global information. In addition, a very simple multi-instance learning method that captures no global information performs almost as well as models that capture a lot of global information. These results suggest that the most important features for effective WSI classification are captured at the local small patch level, where cell and tissue micro-environment detail is most pronounced. Another surprising finding was that unsupervised pre-training on a larger set of 33 cancers gives significantly worse performance compared to pre-training on a smaller dataset of 7 cancers (including the target cancer). We posit that pre-training on a smaller, more focused dataset allows the feature extractor to make better use of the limited feature space to better discriminate between subtle differences in the input patch.
</details>
<details>
<summary>摘要</summary>
近些时间，有许多算法提出来用于分类高解像整幕照片（WSIs）。这些新算法主要集中在找到小地方区域Extracted from the slide的信息的新方法，强调有效地汇集全局信息为最终预测器。在这篇论文中，我们详细探讨了不同的关键设计选择WSI分类算法，以 investigate what matters most for achieving high accuracy。 Surprisingly, we found that capturing global context information does not necessarily mean better performance. A model that captures the most global information consistently performs worse than a model that captures less global information. In addition, a very simple multi-instance learning method that captures no global information performs almost as well as models that capture a lot of global information. These results suggest that the most important features for effective WSI classification are captured at the local small patch level, where cell and tissue micro-environment detail is most pronounced. Another surprising finding was that unsupervised pre-training on a larger set of 33 cancers gives significantly worse performance compared to pre-training on a smaller dataset of 7 cancers (including the target cancer). We posit that pre-training on a smaller, more focused dataset allows the feature extractor to make better use of the limited feature space to better discriminate between subtle differences in the input patch.
</details></li>
</ul>
<hr>
<h2 id="Ablation-Study-to-Clarify-the-Mechanism-of-Object-Segmentation-in-Multi-Object-Representation-Learning"><a href="#Ablation-Study-to-Clarify-the-Mechanism-of-Object-Segmentation-in-Multi-Object-Representation-Learning" class="headerlink" title="Ablation Study to Clarify the Mechanism of Object Segmentation in Multi-Object Representation Learning"></a>Ablation Study to Clarify the Mechanism of Object Segmentation in Multi-Object Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03273">http://arxiv.org/abs/2310.03273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takayuki Komatsu, Yoshiyuki Ohmura, Yasuo Kuniyoshi</li>
<li>for: 多个物体表示学习旨在将复杂的真实世界视觉输入转化为多个物体的组合。</li>
<li>methods:  prevailing 方法通常使用不监督学习来将输入图像分割成各个物体，并将这些物体编码到每个幂量 Vector 中。但是，不清楚前一代方法如何实现正确的物体分割。此外，大多数前一代方法使用 Variational Autoencoder (VAE) 进行幂量 Vector 的正则化，因此不清楚 VAE 正则化是否对物体分割有效。</li>
<li>results: 为了解释多个物体表示学习中对物体分割的机制，我们对 MONet 进行了减少性研究。MONet 使用对应的注意mask和幂量 Vector 来表示多个物体。每个幂量 Vector 来自输入图像和注意mask。然后，对于每个幂量 Vector，分解图像和注意mask。MONet 的损失函数包括1) 输入图像和分解图像之间的总准确率损失，2) VAE 正则化损失，3) 注意mask 的准确率损失以显式地编码形态信息。我们对这三个损失函数进行了减少性研究，我们发现 VAE 正则化损失没有影响分割性能，而其他两个损失函数确实影响分割性能。基于这个结果，我们提出了一个新的假设：在图像区域中，最好是使得每个幂量 Vector 对应的注意mask 最大化。我们验证了这个假设，并证明了它是正确的。<details>
<summary>Abstract</summary>
Multi-object representation learning aims to represent complex real-world visual input using the composition of multiple objects. Representation learning methods have often used unsupervised learning to segment an input image into individual objects and encode these objects into each latent vector. However, it is not clear how previous methods have achieved the appropriate segmentation of individual objects. Additionally, most of the previous methods regularize the latent vectors using a Variational Autoencoder (VAE). Therefore, it is not clear whether VAE regularization contributes to appropriate object segmentation. To elucidate the mechanism of object segmentation in multi-object representation learning, we conducted an ablation study on MONet, which is a typical method. MONet represents multiple objects using pairs that consist of an attention mask and the latent vector corresponding to the attention mask. Each latent vector is encoded from the input image and attention mask. Then, the component image and attention mask are decoded from each latent vector. The loss function of MONet consists of 1) the sum of reconstruction losses between the input image and decoded component image, 2) the VAE regularization loss of the latent vector, and 3) the reconstruction loss of the attention mask to explicitly encode shape information. We conducted an ablation study on these three loss functions to investigate the effect on segmentation performance. Our results showed that the VAE regularization loss did not affect segmentation performance and the others losses did affect it. Based on this result, we hypothesize that it is important to maximize the attention mask of the image region best represented by a single latent vector corresponding to the attention mask. We confirmed this hypothesis by evaluating a new loss function with the same mechanism as the hypothesis.
</details>
<details>
<summary>摘要</summary>
多对象表示学习目标是将复杂的真实世界视觉输入转换为多个对象的组合。表示学习方法通常使用无监督学习来将输入图像分割成各个对象，并将这些对象编码到每个幂量中。然而，没有准确的方法来实现适当的对象分割。此外，大多数前一代方法使用Variational Autoencoder（VAE）来规范幂量。因此，不清楚VAE规范是否对适当的对象分割做出贡献。为了解释多对象表示学习中对象分割机制，我们进行了MONet方法的ablation研究。MONet使用对应于注意Mask和幂量的对象对来表示多个对象。每个幂量来自输入图像和注意Mask的编码。然后，从每个幂量中解码输入图像和注意Mask。MONet的损失函数包括1）输入图像和解码组件图像之间的总差异损失，2）VAE规范损失，3）注意Mask的重建损失，以便显式地编码形状信息。我们对这三个损失函数进行了ablation研究，并证明VAE规范损失没有影响分割性能，而其他两个损失函数有影响。基于这结果，我们提出了一种新的损失函数，它的机制与我们的假设相同。我们证明了这种损失函数能够提高对象分割性能。
</details></li>
</ul>
<hr>
<h2 id="EfficientDM-Efficient-Quantization-Aware-Fine-Tuning-of-Low-Bit-Diffusion-Models"><a href="#EfficientDM-Efficient-Quantization-Aware-Fine-Tuning-of-Low-Bit-Diffusion-Models" class="headerlink" title="EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models"></a>EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03270">http://arxiv.org/abs/2310.03270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yefei He, Jing Liu, Weijia Wu, Hong Zhou, Bohan Zhuang</li>
<li>for: 这个研究旨在提高Diffusion模型的实用性，以便在实时应用中实现低延迟和低资料使用率。</li>
<li>methods: 这个研究使用了两种主要的压缩方法：post-training quantization (PTQ)和quantization-aware training (QAT)。而我们的方法是一种不需要训练数据的、简洁的精简架构，可以实现QAT-level的性能，并且具有PTQ-like的效率。</li>
<li>results: 实验结果显示，我们的方法可以与先前的PTQ-based diffusion模型比较，同时维持相似的时间和数据效率，并且实现更高的生成质量。具体来说，将LDM-4的 weights和活化函数压缩到4位数字时，与先前的PTQ-based方法相比，只有0.05 sFID增加。相比于QAT-based方法，我们的EfficientDM也具有16.2倍的压缩速度，并且实现了相似的生成质量。<details>
<summary>Abstract</summary>
Diffusion models have demonstrated remarkable capabilities in image synthesis and related generative tasks. Nevertheless, their practicality for low-latency real-world applications is constrained by substantial computational costs and latency issues. Quantization is a dominant way to compress and accelerate diffusion models, where post-training quantization (PTQ) and quantization-aware training (QAT) are two main approaches, each bearing its own properties. While PTQ exhibits efficiency in terms of both time and data usage, it may lead to diminished performance in low bit-width. On the other hand, QAT can alleviate performance degradation but comes with substantial demands on computational and data resources. To capitalize on the advantages while avoiding their respective drawbacks, we introduce a data-free and parameter-efficient fine-tuning framework for low-bit diffusion models, dubbed EfficientDM, to achieve QAT-level performance with PTQ-like efficiency. Specifically, we propose a quantization-aware variant of the low-rank adapter (QALoRA) that can be merged with model weights and jointly quantized to low bit-width. The fine-tuning process distills the denoising capabilities of the full-precision model into its quantized counterpart, eliminating the requirement for training data. We also introduce scale-aware optimization and employ temporal learned step-size quantization to further enhance performance. Extensive experimental results demonstrate that our method significantly outperforms previous PTQ-based diffusion models while maintaining similar time and data efficiency. Specifically, there is only a marginal 0.05 sFID increase when quantizing both weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization speed with comparable generation quality.
</details>
<details>
<summary>摘要</summary>
Diffusion models 有 demonstrated 非常出色的创造力在图像生成和相关的生成任务中。然而，它们在实际应用中的延迟和计算成本问题受到了一定的限制。量化是Diffusion models的压缩和加速的主要方法，其中Post-training quantization (PTQ)和quantization-aware training (QAT)是两种主要的方法，每个方法都有自己的特点。PTQ可以快速地压缩和加速Diffusion models，但可能会导致低位数bit的性能下降。而QAT可以减轻性能下降，但需要大量的计算和数据资源。为了利用这些优点而避免其缺点，我们介绍了一种无需数据和参数的efficient fine-tuning框架，以实现QAT级别的性能，同时保持PTQ类似的效率。我们提出了一种量化感知的低级Adapter（QALoRA），可以与模型参数和量化结合使用，并且可以将模型 weights和活动量化到低位数bit。 fine-tuning过程将混合模型的权重和活动的权重和量化结果，从而消除了对训练数据的需求。我们还引入了缩放比例优化和时间学习步长量化，以进一步提高性能。我们的方法在实际实验中显著超越了之前基于PTQ的Diffusion models，同时保持相同的时间和数据效率。具体来说，在ImageNet 256x256上，将LDM-4的权重和活动量化到4位数bit时，只有0.05 sFID提升。相比QAT基于方法，我们的EfficientDM还具有16.2倍 faster量化速度，同时保持类似的生成质量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/05/cs.CV_2023_10_05/" data-id="clp88dbwc00koob88h5nx5bk8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/05/cs.AI_2023_10_05/" class="article-date">
  <time datetime="2023-10-05T12:00:00.000Z" itemprop="datePublished">2023-10-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/05/cs.AI_2023_10_05/">cs.AI - 2023-10-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hard-View-Selection-for-Contrastive-Learning"><a href="#Hard-View-Selection-for-Contrastive-Learning" class="headerlink" title="Hard View Selection for Contrastive Learning"></a>Hard View Selection for Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03940">http://arxiv.org/abs/2310.03940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Ferreira, Ivo Rapant, Frank Hutter</li>
<li>for: 提高对图像输入的抗变易性和稳定性</li>
<li>methods: 提出一种无需学习的、强大的硬视角选择策略（HVS），通过随机生成多个视角，并对每个视角对照进行反向传播来增加任务难度</li>
<li>results: 在ImageNet上 Linear Evaluation 中提高了0.55%-1.9%的精度，并在多种CL方法（如DINO、SimSiam、SimCLR）上显示了类似的改进，而且HVS在800个训练周期的基础上只需300个训练周期即可达到类似水平，即使 compte tenu of the additional forward passes induced by HVS.<details>
<summary>Abstract</summary>
Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward passes for each view pair on the currently trained model, 3) adversarially select the pair yielding the worst loss, and 4) run the backward pass with the selected pair. In our empirical analysis we show that under the hood, HVS increases task difficulty by controlling the Intersection over Union of views during pretraining. With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS. Additionally, HVS consistently achieves accuracy improvements on ImageNet between 0.55% and 1.9% on linear evaluation and similar improvements on transfer tasks across multiple CL methods, such as DINO, SimSiam, and SimCLR.
</details>
<details>
<summary>摘要</summary>
许多对比学习（CL）方法在训练模型时强调模型对不同视图的图像输入具有抗变异性。而现有的大量努力集中在改进预测任务、建筑或者稳定性（例如siamese网络或教师软max中心），但大多数这些方法仍然依赖于随机抽样操作在图像增强pipeline中，如随机缩放或颜色干扰操作。在这篇论文中，我们认为观察视图生成和其影响表现所得到的关注不足。为此，我们提出一种简单、学习无需的、具有强大抗变异性的硬视选择（HVS）策略，用于在CL训练中延长随机视图生成，并将模型 expose 到更难的样本。该策略包括以下步骤：1. 随机抽取多个视图，并将每个视图对创建对。2. 对每个视图对进行前向传播，并计算当前训练模型的损失。3. 选择损失最大的对，并对该对进行反向传播。我们的实验表明，HVS可以通过控制视图之间的交集来增加CL训练的difficulty。只需要300个训练回合，HVS就可以与800个训练回合的DINO基eline相当，而且这些基eline在CL训练中保持了非常有利的。此外，HVS在ImageNet上实现了Linear评估中的0.55%-1.9%的准确率提升，以及同样的提升在多个CL方法上，如DINO、SimSiam和SimCLR。
</details></li>
</ul>
<hr>
<h2 id="Multitask-Learning-for-Time-Series-Data-with-2D-Convolution"><a href="#Multitask-Learning-for-Time-Series-Data-with-2D-Convolution" class="headerlink" title="Multitask Learning for Time Series Data with 2D Convolution"></a>Multitask Learning for Time Series Data with 2D Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03925">http://arxiv.org/abs/2310.03925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chin-Chia Michael Yeh, Xin Dai, Yan Zheng, Junpeng Wang, Huiyuan Chen, Yujie Fan, Audrey Der, Zhongfang Zhuang, Liang Wang, Wei Zhang</li>
<li>for: 这个研究探讨了多任务学习（MTL）在时间序列资料上的应用，以提高时间序列分类（TSC）模型的通用化能力。</li>
<li>methods: 我们将现有的1D核心嵌入式TSC模型与MTL结合，并评估其性能。我们还提出了一个新的2D核心嵌入式模型，以增强模型的表达能力。</li>
<li>results: 我们的提案在UCR档案和一个工业交易TSC数据集上实现了比较好的性能，较以往的方法还要好。<details>
<summary>Abstract</summary>
Multitask learning (MTL) aims to develop a unified model that can handle a set of closely related tasks simultaneously. By optimizing the model across multiple tasks, MTL generally surpasses its non-MTL counterparts in terms of generalizability. Although MTL has been extensively researched in various domains such as computer vision, natural language processing, and recommendation systems, its application to time series data has received limited attention. In this paper, we investigate the application of MTL to the time series classification (TSC) problem. However, when we integrate the state-of-the-art 1D convolution-based TSC model with MTL, the performance of the TSC model actually deteriorates. By comparing the 1D convolution-based models with the Dynamic Time Warping (DTW) distance function, it appears that the underwhelming results stem from the limited expressive power of the 1D convolutional layers. To overcome this challenge, we propose a novel design for a 2D convolution-based model that enhances the model's expressiveness. Leveraging this advantage, our proposed method outperforms competing approaches on both the UCR Archive and an industrial transaction TSC dataset.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）目的是开发一个可以同时处理一组相关任务的统一模型。通过优化模型 across multiple tasks，MTL 通常会超过其非 MTL 对应模型的普适性。虽然 MTL 在不同领域 such as 计算机视觉、自然语言处理和推荐系统中得到了广泛的研究，但对时间序列数据的应用却收到了有限的注意。在这篇论文中，我们调查了在时间序列分类（TSC）问题上MTL的应用。然而，当我们将现有的 state-of-the-art 1D核心样本-based TSC模型与 MTL 集成时，TSC 模型的性能实际下降。通过比较 1D 核心样本-based 模型和动态时间戳距（DTW）距离函数，可以看出，不满的结果实际上来自于 1D 核心样本层的有限表达能力。为了解决这个挑战，我们提议一种新的 2D 核心样本-based 模型，该模型可以增强模型的表达能力。利用这个优势，我们的提议方法在 UCR archive 和一个工业交易 TSC 数据集上超过了竞争方法的性能。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Content-based-Time-Series-Retrieval-System"><a href="#An-Efficient-Content-based-Time-Series-Retrieval-System" class="headerlink" title="An Efficient Content-based Time Series Retrieval System"></a>An Efficient Content-based Time Series Retrieval System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03919">http://arxiv.org/abs/2310.03919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chin-Chia Michael Yeh, Huiyuan Chen, Xin Dai, Yan Zheng, Junpeng Wang, Vivian Lai, Yujie Fan, Audrey Der, Zhongfang Zhuang, Liang Wang, Wei Zhang, Jeff M. Phillips</li>
<li>for: 这个论文旨在提供一个可以处理多个领域时间序列数据的信息检索系统，帮助用户通过提交时间序列来检索相关时间序列和元数据。</li>
<li>methods: 该论文提出了一种高效和可靠的时间序列检索模型，使用了一种基于内存的快速相似度计算方法，并对多个领域时间序列进行了比较。</li>
<li>results: 对于具体的交易数据问题，该模型比其他方法更适合，并且在实时交互过程中可以保持reasonable的推理时间。<details>
<summary>Abstract</summary>
A Content-based Time Series Retrieval (CTSR) system is an information retrieval system for users to interact with time series emerged from multiple domains, such as finance, healthcare, and manufacturing. For example, users seeking to learn more about the source of a time series can submit the time series as a query to the CTSR system and retrieve a list of relevant time series with associated metadata. By analyzing the retrieved metadata, users can gather more information about the source of the time series. Because the CTSR system is required to work with time series data from diverse domains, it needs a high-capacity model to effectively measure the similarity between different time series. On top of that, the model within the CTSR system has to compute the similarity scores in an efficient manner as the users interact with the system in real-time. In this paper, we propose an effective and efficient CTSR model that outperforms alternative models, while still providing reasonable inference runtimes. To demonstrate the capability of the proposed method in solving business problems, we compare it against alternative models using our in-house transaction data. Our findings reveal that the proposed model is the most suitable solution compared to others for our transaction data problem.
</details>
<details>
<summary>摘要</summary>
一个内容基于时间序列检索（CTSR）系统是一个信息检索系统，用于帮助用户与多个领域（如金融、医疗和制造）中的时间序列进行交互。例如，用户想要了解时间序列的来源，可以将时间序列作为查询提交到CTSR系统，并获取相关的元数据列表。通过分析返回的元数据，用户可以了解更多关于时间序列的来源信息。由于CTSR系统需要处理来自不同领域的时间序列数据，因此需要一个高容量的模型来有效地度量不同时间序列之间的相似性。同时，模型内部需要高效地计算相似性分数，以便用户在实时交互时可以得到快速的回答。在这篇论文中，我们提出一种高效和高效的CTSR模型，超过了其他模型，同时仍提供了合理的推理运行时间。为了证明提案的方法在解决业务问题时的可行性，我们对它与其他模型进行比较，并使用我们的自有交易数据进行实践。我们的发现表明，提案的模型是与其他模型相比最适合的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Foundation-Model-for-Time-Series-Data"><a href="#Toward-a-Foundation-Model-for-Time-Series-Data" class="headerlink" title="Toward a Foundation Model for Time Series Data"></a>Toward a Foundation Model for Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03916">http://arxiv.org/abs/2310.03916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng, Yujie Fan, Audrey Der, Vivian Lai, Zhongfang Zhuang, Junpeng Wang, Liang Wang, Wei Zhang</li>
<li>for: 这个论文的目的是开发一种有效的时间序列基础模型，使其可以在多个领域中进行适应。</li>
<li>methods: 这篇论文使用了四种现有的自然学习基于预训练方法，以及一种新方法，在多个领域的无标示样本上进行预训练。</li>
<li>results: 实验结果表明，预训练可以提高下游分类任务的融合过程，并且提出了一种基于Transformer模型的新预训练方法，其在其他方法中表现出色。<details>
<summary>Abstract</summary>
A foundation model is a machine learning model trained on a large and diverse set of data, typically using self-supervised learning-based pre-training techniques, that can be adapted to various downstream tasks. However, current research on time series pre-training has mostly focused on models pre-trained solely on data from a single domain, resulting in a lack of knowledge about other types of time series. However, current research on time series pre-training has predominantly focused on models trained exclusively on data from a single domain. As a result, these models possess domain-specific knowledge that may not be easily transferable to time series from other domains. In this paper, we aim to develop an effective time series foundation model by leveraging unlabeled samples from multiple domains. To achieve this, we repurposed the publicly available UCR Archive and evaluated four existing self-supervised learning-based pre-training methods, along with a novel method, on the datasets. We tested these methods using four popular neural network architectures for time series to understand how the pre-training methods interact with different network designs. Our experimental results show that pre-training improves downstream classification tasks by enhancing the convergence of the fine-tuning process. Furthermore, we found that the proposed pre-training method, when combined with the Transformer model, outperforms the alternatives.
</details>
<details>
<summary>摘要</summary>
《基础模型》是一种机器学习模型，通过大量和多样化的数据进行自动学习预训练，可以适应多种下游任务。然而，当前关于时间序列预训练的研究主要集中在尝试使用单个领域的数据进行预训练，导致对其他领域时间序列的知识缺乏。为了开拓新的研究途径，我们希图通过多个领域的无标签样本来开发一个有效的时间序列基础模型。我们利用了公共可用的UCRL Archive，评估了四种现有的自动学习预训练方法，以及一种新方法，在这些数据集上进行测试。我们使用了四种流行的快速网络架构来评估这些预训练方法的效果。我们的实验结果表明，预训练可以提高下游分类任务的整合，并且我们提出的预训练方法，与Transformer模型结合使用，可以超越其他方法。
</details></li>
</ul>
<hr>
<h2 id="RTDK-BO-High-Dimensional-Bayesian-Optimization-with-Reinforced-Transformer-Deep-kernels"><a href="#RTDK-BO-High-Dimensional-Bayesian-Optimization-with-Reinforced-Transformer-Deep-kernels" class="headerlink" title="RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels"></a>RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03912">http://arxiv.org/abs/2310.03912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Shmakov, Avisek Naug, Vineet Gundecha, Sahand Ghorbanpour, Ricardo Luna Gutierrez, Ashwin Ramesh Babu, Antonio Guillen, Soumyendu Sarkar<br>for: 这篇论文的目的是提高Meta-learning Bayesian Optimization（BO）的表达力，以便更好地处理高维度黑盒优化问题。methods: 该论文使用了Deep Kernel Learning（DKL）和注意力基于Transformer模型来提高GPsurrogates的模型能力，并使用了Soft Actor-Critic Reinforcement Learning（SACRL）来学习获取函数的优化策略。results: 该论文的实验结果表明，combined DKL和Transformer模型可以提高Meta-learning BO surrogates的表达力，并在高维度黑盒优化问题上实现了最佳性能。<details>
<summary>Abstract</summary>
Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kernel with a learned acquisition function trained with continuous Soft Actor-Critic Reinforcement Learning to aid in exploration. This Reinforced Transformer Deep Kernel (RTDK-BO) approach yields state-of-the-art results in continuous high-dimensional optimization problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Taming-Binarized-Neural-Networks-and-Mixed-Integer-Programs"><a href="#Taming-Binarized-Neural-Networks-and-Mixed-Integer-Programs" class="headerlink" title="Taming Binarized Neural Networks and Mixed-Integer Programs"></a>Taming Binarized Neural Networks and Mixed-Integer Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04469">http://arxiv.org/abs/2310.04469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes Aspman, Georgios Korpas, Jakub Marecek</li>
<li>for: 本研究旨在解决binarized neural networks的训练问题，特别是因为这些神经网络具有解释性。</li>
<li>methods: 研究人员使用了将问题 Reformulate为杂integer程序的子添加问题的方法，以便使用Bolte等人提出的暗示法，实现backpropagation的实际应用。</li>
<li>results: 研究人员表明，使用这种方法可以使binarized neural networks具有可控的表示，并且可以使用Bolte等人的框架进行隐式导数，从而实现实际应用。<details>
<summary>Abstract</summary>
There has been a great deal of recent interest in binarized neural networks, especially because of their explainability. At the same time, automatic differentiation algorithms such as backpropagation fail for binarized neural networks, which limits their applicability. By reformulating the problem of training binarized neural networks as a subadditive dual of a mixed-integer program, we show that binarized neural networks admit a tame representation. This, in turn, makes it possible to use the framework of Bolte et al. for implicit differentiation, which offers the possibility for practical implementation of backpropagation in the context of binarized neural networks. This approach could also be used for a broader class of mixed-integer programs, beyond the training of binarized neural networks, as encountered in symbolic approaches to AI and beyond.
</details>
<details>
<summary>摘要</summary>
有很多最近关注二进制神经网络，特别是它们的可解释性。然而，自动梯度计算算法如反射propagation失效于二进制神经网络，限制了它们的应用。我们通过将二进制神经网络训练问题重新表述为杂Integer程序的子Additive dual，证明二进制神经网络具有可控的表示。这种表示使得可以使用博尔特等人的框架对偶计算，这些计算可以实现在二进制神经网络训练中的Backpropagation。这种方法可以用于更广泛的杂Integer程序，不仅是训练二进制神经网络，还有在符号智能术中遇到的更加广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Neural-Network-Training-with-Rooted-Logistic-Objectives"><a href="#Accelerated-Neural-Network-Training-with-Rooted-Logistic-Objectives" class="headerlink" title="Accelerated Neural Network Training with Rooted Logistic Objectives"></a>Accelerated Neural Network Training with Rooted Logistic Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03890">http://arxiv.org/abs/2310.03890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Wang, Praveen Raj Veluswami, Harsh Mishra, Sathya N. Ravi</li>
<li>for: 本研究旨在提出一种新的损失函数，以提高神经网络模型在实际应用中的训练速度和性能。</li>
<li>methods: 本研究使用了一种新的损失函数，即“根据损失函数”，这个函数是基于对数函数的一种变形，可以提高神经网络模型的训练速度和性能。</li>
<li>results: 在实际实验中，使用“根据损失函数”训练神经网络模型可以提高模型的性能，并且训练速度也比传统的损失函数快。此外，这种损失函数还可以应用于生成模型下沉淀应用，如StyleGAN模型的训练。<details>
<summary>Abstract</summary>
Many neural networks deployed in the real world scenarios are trained using cross entropy based loss functions. From the optimization perspective, it is known that the behavior of first order methods such as gradient descent crucially depend on the separability of datasets. In fact, even in the most simplest case of binary classification, the rate of convergence depends on two factors: (1) condition number of data matrix, and (2) separability of the dataset. With no further pre-processing techniques such as over-parametrization, data augmentation etc., separability is an intrinsic quantity of the data distribution under consideration. We focus on the landscape design of the logistic function and derive a novel sequence of {\em strictly} convex functions that are at least as strict as logistic loss. The minimizers of these functions coincide with those of the minimum norm solution wherever possible. The strict convexity of the derived function can be extended to finetune state-of-the-art models and applications. In empirical experimental analysis, we apply our proposed rooted logistic objective to multiple deep models, e.g., fully-connected neural networks and transformers, on various of classification benchmarks. Our results illustrate that training with rooted loss function is converged faster and gains performance improvements. Furthermore, we illustrate applications of our novel rooted loss function in generative modeling based downstream applications, such as finetuning StyleGAN model with the rooted loss. The code implementing our losses and models can be found here for open source software development purposes: https://anonymous.4open.science/r/rooted_loss.
</details>
<details>
<summary>摘要</summary>
许多神经网络在实际场景中被训练使用基于权重 entropy 的损失函数。优化方面来说，已知的是，训练使用首领方法 such as 梯度下降时，数据集的分化度对结果的准确性和速度具有关键作用。实际上，甚至在最简单的二分类情况下，训练的速度和准确性都取决于数据集的分化度和Condition number of data matrix。在没有额外的预处理技术，如过 parametrization、数据增强等，数据分化度是数据分布考虑的内在特性。我们关注了对 logistic 函数的 landscape 设计，并 derive 一个 novel 的 strictly convex 函数序列，这些函数在至少等效于 logistic loss 的情况下，其最小值的解归并与 minimum norm solution 匹配。我们发现，使用我们提议的根据梯度损失函数可以更快 converges 并提高性能。此外，我们还应用了我们的新的根据梯度损失函数在生成模型中的应用，例如 StyleGAN 模型的资源化。我们的实验结果表明，使用根据梯度损失函数可以提高模型的性能和速度。代码实现我们的损失函数和模型可以在以下链接找到：<https://anonymous.4open.science/r/rooted_loss>。
</details></li>
</ul>
<hr>
<h2 id="A-Kernel-Perspective-on-Behavioural-Metrics-for-Markov-Decision-Processes"><a href="#A-Kernel-Perspective-on-Behavioural-Metrics-for-Markov-Decision-Processes" class="headerlink" title="A Kernel Perspective on Behavioural Metrics for Markov Decision Processes"></a>A Kernel Perspective on Behavioural Metrics for Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19804">http://arxiv.org/abs/2310.19804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, Mark Rowland</li>
<li>for: 这个论文是为了探讨行为度量在再征学习中的应用。</li>
<li>methods: 论文使用了正定定义kernels来定义一种新的度量，并利用这种度量提供了新的理论结果，包括距离值函数差异的上限和度量可以证明嵌入到finite维Euclidean空间中 WITH low distortion error。</li>
<li>results: 论文通过实验证明了这种方法在实践中的效果。<details>
<summary>Abstract</summary>
Behavioural metrics have been shown to be an effective mechanism for constructing representations in reinforcement learning. We present a novel perspective on behavioural metrics for Markov decision processes via the use of positive definite kernels. We leverage this new perspective to define a new metric that is provably equivalent to the recently introduced MICo distance (Castro et al., 2021). The kernel perspective further enables us to provide new theoretical results, which has so far eluded prior work. These include bounding value function differences by means of our metric, and the demonstration that our metric can be provably embedded into a finite-dimensional Euclidean space with low distortion error. These are two crucial properties when using behavioural metrics for reinforcement learning representations. We complement our theory with strong empirical results that demonstrate the effectiveness of these methods in practice.
</details>
<details>
<summary>摘要</summary>
行为指标已被证明是在增强学习中有效的表示机制。我们提出了一种新的视角，使用正定定义的kernel来解决Markov决策过程中的行为指标。我们利用这种新的视角来定义一个新的度量，该度量与Castro等人（2021）最近提出的MICo距离相等。kernel视角还允许我们提供新的理论结果，包括通过我们的度量下界值函数差异，以及证明我们的度量可以被证明嵌入到有低抖动误差的finite维Euclidean空间中。这些是使用行为指标进行增强学习表示的两个重要性质。我们在理论上补充了强大的实验结果，证明这些方法在实践中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Small-batch-deep-reinforcement-learning"><a href="#Small-batch-deep-reinforcement-learning" class="headerlink" title="Small batch deep reinforcement learning"></a>Small batch deep reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03882">http://arxiv.org/abs/2310.03882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Obando-Ceron, Marc G. Bellemare, Pablo Samuel Castro</li>
<li>for: 提高深度强化学习的性能</li>
<li>methods: 使用批处理大小为控制参数，对每次梯度更新进行采样</li>
<li>results: 研究发现，减小批处理大小可以提高性能，这与通常认为增加批处理大小可以提高 neural network 性能的想法相反。<details>
<summary>Abstract</summary>
In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests {\em reducing} the batch size can result in a number of significant performance gains; this is surprising, as the general tendency when training neural networks is towards larger batch sizes for improved performance. We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon.
</details>
<details>
<summary>摘要</summary>
在值基深度强化学习中使用回忆储存，批处理大小参数指定每个梯度更新中样本的数量。虽然对学习过程非常重要，但通常不会在提出新算法时调整这个值。在这项工作中，我们提供了广泛的实验研究，表明减小批处理大小可以导致一些重要的性能提升，这是对训练神经网络时通常采用大批处理大小的惯例。我们补充了一系列实验分析，以更好地理解这种现象。
</details></li>
</ul>
<hr>
<h2 id="Better-Safe-than-Sorry-Pre-training-CLIP-against-Targeted-Data-Poisoning-and-Backdoor-Attacks"><a href="#Better-Safe-than-Sorry-Pre-training-CLIP-against-Targeted-Data-Poisoning-and-Backdoor-Attacks" class="headerlink" title="Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks"></a>Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05862">http://arxiv.org/abs/2310.05862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman</li>
<li>for: 防止CLIP模型被targeted数据毒化和后门攻击</li>
<li>methods: 使用unimodal对比学习（CL）对图像和文本模式进行暖身，并将数据分成安全和危险 subsets，对危险 subsets进行unimodal CL 和CLIP损失的同时训练</li>
<li>results: 在许多数据集上，SAFECLIP可以有效防止targeted数据毒化和后门攻击，而不会对CLIP性能产生影响<details>
<summary>Abstract</summary>
Contrastive Language-Image Pre-training (CLIP) on large image-caption datasets has achieved remarkable success in zero-shot classification and enabled transferability to new domains. However, CLIP is extremely more vulnerable to targeted data poisoning and backdoor attacks, compared to supervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIP pre-training data is enough to make targeted data poisoning attacks successful. This is four orders of magnitude smaller than what is required to poison supervised models. Despite this vulnerability, existing methods are very limited in defending CLIP models during pre-training. In this work, we propose a strong defense, SAFECLIP, to safely pre-train CLIP against targeted data poisoning and backdoor attacks. SAFECLIP warms up the model by applying unimodal contrastive learning (CL) on image and text modalities separately. Then, it carefully divides the data into safe and risky subsets. SAFECLIP trains on the risky data by applying unimodal CL to image and text modalities separately, and trains on the safe data using the CLIP loss. By gradually increasing the size of the safe subset during the training, SAFECLIP effectively breaks targeted data poisoning and backdoor attacks without harming the CLIP performance. Our extensive experiments show that SAFECLIP decrease the attack success rate of targeted data poisoning attacks from 93.75% to 0% and that of the backdoor attacks from 100% to 0%, without harming the CLIP performance on various datasets.
</details>
<details>
<summary>摘要</summary>
对大量图像描述文本 datasets 进行 Contrastive Language-Image Pre-training (CLIP) 后得到了杰出的成功，并且允许在新领域中进行转移。然而，CLIP 对于Targeted Data Poisoning 和 Backdoor 攻击更加易受攻击，相比于超vised learning。奇怪的是，对 CLIP 预训练数据进行0.0001%的恶意数据投毒只需要0.0001%的数据，而supervised learning 需要1000倍的数据。尽管如此，现有的方法对于防止 CLIP 模型在预训练中受到攻击很有限。在这个工作中，我们提出了一种强大的防御方法，即 SafeCLIP，以安全地在预训练 CLIP 模型中进行Targeted Data Poisoning 和 Backdoor 攻击。SafeCLIP 通过在图像和文本模式上分别应用 unimodal Contrastive Learning (CL) 来让模型进行温身。然后，它 méticulously 将数据分为安全和危险子集。SafeCLIP 在危险子集上应用 unimodal CL，并在安全子集上使用 CLIP 损失进行训练。通过逐渐增加安全子集的大小 durante 训练，SafeCLIP 可以有效地破坏 Targeted Data Poisoning 和 Backdoor 攻击，而不会害 CLIP 性能。我们的广泛的实验表明，SafeCLIP 可以将 Targeted Data Poisoning 攻击的成功率从 93.75% 降低到 0%，并将 Backdoor 攻击的成功率从 100% 降低到 0%，而不会害 CLIP 在不同的 dataset 上的性能。
</details></li>
</ul>
<hr>
<h2 id="Validating-transformers-for-redaction-of-text-from-electronic-health-records-in-real-world-healthcare"><a href="#Validating-transformers-for-redaction-of-text-from-electronic-health-records-in-real-world-healthcare" class="headerlink" title="Validating transformers for redaction of text from electronic health records in real-world healthcare"></a>Validating transformers for redaction of text from electronic health records in real-world healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04468">http://arxiv.org/abs/2310.04468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CogStack/MedCAT">https://github.com/CogStack/MedCAT</a></li>
<li>paper_authors: Zeljko Kraljevic, Anthony Shek, Joshua Au Yeung, Ewart Jonathan Sheldon, Mohammad Al-Agil, Haris Shuaib, Xi Bai, Kawsar Noor, Anoop D. Shah, Richard Dobson, James Teo</li>
<li>for: 保护医疗记录中患者隐私的研究，以实现医疗数据的安全和共享。</li>
<li>methods: 使用深度学习技术，特别是变换器模型，以提高隐私 obscuration 的精度和效率。</li>
<li>results: 在三个英国医院的实际记录中，AnonCAT 模型达到了高性能，具体来说是：Recall 为 0.99、0.99 和 0.96。<details>
<summary>Abstract</summary>
Protecting patient privacy in healthcare records is a top priority, and redaction is a commonly used method for obscuring directly identifiable information in text. Rule-based methods have been widely used, but their precision is often low causing over-redaction of text and frequently not being adaptable enough for non-standardised or unconventional structures of personal health information. Deep learning techniques have emerged as a promising solution, but implementing them in real-world environments poses challenges due to the differences in patient record structure and language across different departments, hospitals, and countries.   In this study, we present AnonCAT, a transformer-based model and a blueprint on how deidentification models can be deployed in real-world healthcare. AnonCAT was trained through a process involving manually annotated redactions of real-world documents from three UK hospitals with different electronic health record systems and 3116 documents. The model achieved high performance in all three hospitals with a Recall of 0.99, 0.99 and 0.96.   Our findings demonstrate the potential of deep learning techniques for improving the efficiency and accuracy of redaction in global healthcare data and highlight the importance of building workflows which not just use these models but are also able to continually fine-tune and audit the performance of these algorithms to ensure continuing effectiveness in real-world settings. This approach provides a blueprint for the real-world use of de-identifying algorithms through fine-tuning and localisation, the code together with tutorials is available on GitHub (https://github.com/CogStack/MedCAT).
</details>
<details>
<summary>摘要</summary>
保护患者隐私在医疗记录是最高优先事项，而红aktion是一种常用的方法来隐藏直接可识别的信息。规则基于的方法已经广泛使用，但它们的精度frequently low，导致过度的文本隐藏和不够适应非标准化或不同结构的个人医疗信息。深度学习技术已经出现为一种可能的解决方案，但在实际环境中实施它们却存在医疗数据结构和语言不同的医院、医生和国家的挑战。在本研究中，我们介绍了AnonCAT，一种基于transformer的模型和在实际医疗环境中部署deidentification模型的蓝图。AnonCAT通过手动标注真实文档的红aktion进行训练，并在三个英国医院中使用3116个文档进行训练。模型在三个医院中表现出色，Recall值为0.99、0.99和0.96。我们的发现表明深度学习技术可以提高医疗数据隐藏的效率和准确性，并且建立可以不断细化和审核这些算法的工作流程，以确保在实际环境中的持续效果。这种方法提供了在实际使用deep learning隐藏算法时的蓝图，代码和教程可以在GitHub上找到（https://github.com/CogStack/MedCAT）。
</details></li>
</ul>
<hr>
<h2 id="Design-Principles-for-Lifelong-Learning-AI-Accelerators"><a href="#Design-Principles-for-Lifelong-Learning-AI-Accelerators" class="headerlink" title="Design Principles for Lifelong Learning AI Accelerators"></a>Design Principles for Lifelong Learning AI Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04467">http://arxiv.org/abs/2310.04467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhireesha Kudithipudi, Anurag Daram, Abdullah M. Zyarah, Fatima Tuz Zohora, James B. Aimone, Angel Yanguas-Gil, Nicholas Soures, Emre Neftci, Matthew Mattina, Vincenzo Lomonaco, Clare D. Thiem, Benjamin Epstein</li>
<li>for: 这篇论文主要是关于人工智能（AI）的持续学习（Lifelong learning），以及如何在Edge设备上实现持续学习AI模型的加速。</li>
<li>methods: 论文使用了一些适用于Edge设备的现有加速器，以及一些新的技术，如 neuromorphic computing 和edge AI accelerators，来实现持续学习AI模型的加速。</li>
<li>results: 论文提出了一些关键的可能性和度量来评估持续学习AI模型的加速器，并探讨了未来可能的技术和应用场景。<details>
<summary>Abstract</summary>
Lifelong learning - an agent's ability to learn throughout its lifetime - is a hallmark of biological learning systems and a central challenge for artificial intelligence (AI). The development of lifelong learning algorithms could lead to a range of novel AI applications, but this will also require the development of appropriate hardware accelerators, particularly if the models are to be deployed on edge platforms, which have strict size, weight, and power constraints. Here, we explore the design of lifelong learning AI accelerators that are intended for deployment in untethered environments. We identify key desirable capabilities for lifelong learning accelerators and highlight metrics to evaluate such accelerators. We then discuss current edge AI accelerators and explore the future design of lifelong learning accelerators, considering the role that different emerging technologies could play.
</details>
<details>
<summary>摘要</summary>
人生学习 - 一个智能代理的生命中不断学习能力 - 是生物学学习系统的特征和人工智能（AI）的中心挑战。开发持续学习AI算法可能会导致多种新的AI应用程序，但这也需要开发适当的硬件加速器，特别是如果模型需要在边缘平台上部署，这些平台具有严格的大小、重量和功耗限制。我们研究了部署在无缝环境中的持续学习AI加速器的设计。我们确定了持续学习加速器所需的关键愿景和评价指标，然后讨论当前的边缘AI加速器和未来的持续学习加速器设计，并考虑不同的新技术在这方面的作用。
</details></li>
</ul>
<hr>
<h2 id="Contextualized-Structural-Self-supervised-Learning-for-Ontology-Matching"><a href="#Contextualized-Structural-Self-supervised-Learning-for-Ontology-Matching" class="headerlink" title="Contextualized Structural Self-supervised Learning for Ontology Matching"></a>Contextualized Structural Self-supervised Learning for Ontology Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03840">http://arxiv.org/abs/2310.03840</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ellenzhuwang/lakermap">https://github.com/ellenzhuwang/lakermap</a></li>
<li>paper_authors: Zhu Wang</li>
<li>for: This paper is written for researchers and practitioners in the field of knowledge graph (KG) integration, particularly those interested in ontology matching (OM) and self-supervised learning.</li>
<li>methods: The paper proposes a novel self-supervised learning OM framework called LaKERMap, which leverages transformer-based language models and incorporates implicit knowledge to capture multiple structural contexts. The framework utilizes distinct training objectives to improve alignment quality and inference time.</li>
<li>results: The paper reports that LaKERMap outperforms state-of-the-art systems in terms of alignment quality and inference time, as demonstrated through experiments on the Bio-ML datasets and tasks. The findings suggest that LaKERMap is a promising approach for KG integration.<details>
<summary>Abstract</summary>
Ontology matching (OM) entails the identification of semantic relationships between concepts within two or more knowledge graphs (KGs) and serves as a critical step in integrating KGs from various sources. Recent advancements in deep OM models have harnessed the power of transformer-based language models and the advantages of knowledge graph embedding. Nevertheless, these OM models still face persistent challenges, such as a lack of reference alignments, runtime latency, and unexplored different graph structures within an end-to-end framework. In this study, we introduce a novel self-supervised learning OM framework with input ontologies, called LaKERMap. This framework capitalizes on the contextual and structural information of concepts by integrating implicit knowledge into transformers. Specifically, we aim to capture multiple structural contexts, encompassing both local and global interactions, by employing distinct training objectives. To assess our methods, we utilize the Bio-ML datasets and tasks. The findings from our innovative approach reveal that LaKERMap surpasses state-of-the-art systems in terms of alignment quality and inference time. Our models and codes are available here: https://github.com/ellenzhuwang/lakermap.
</details>
<details>
<summary>摘要</summary>
ontology matching (OM) 涉及到两个或更多知识图(KG)中概念之间的Semantic关系的识别，并且作为将KG集成的关键步骤。 current advancements in deep OM models have leveraged the power of transformer-based language models and the advantages of knowledge graph embedding. However, these OM models still face persistent challenges, such as a lack of reference alignments, runtime latency, and unexplored different graph structures within an end-to-end framework. In this study, we introduce a novel self-supervised learning OM framework with input ontologies, called LaKERMap. This framework capitalizes on the contextual and structural information of concepts by integrating implicit knowledge into transformers. Specifically, we aim to capture multiple structural contexts, encompassing both local and global interactions, by employing distinct training objectives. To assess our methods, we utilize the Bio-ML datasets and tasks. The findings from our innovative approach reveal that LaKERMap surpasses state-of-the-art systems in terms of alignment quality and inference time. Our models and codes are available here: https://github.com/ellenzhuwang/lakermap.Here's the breakdown of the translation:* "Ontology matching" is translated as "ontology matching" (同义词翻译)* "identification of semantic relationships" is translated as "识别Semantic关系" ( literal translation)* "between concepts within two or more knowledge graphs" is translated as "在两个或更多知识图中的概念之间" ( literal translation)* "and serves as a critical step in integrating knowledge graphs from various sources" is translated as "并且作为将KG集成的关键步骤" ( literal translation)* "current advancements in deep OM models" is translated as "current advancements in deep OM models" (同义词翻译)* "have leveraged the power of transformer-based language models" is translated as "have leveraged the power of transformer-based language models" (同义词翻译)* "and the advantages of knowledge graph embedding" is translated as "和知识图嵌入的优点" ( literal translation)* "However, these OM models still face persistent challenges" is translated as "然而, these OM models still face persistent challenges" (同义词翻译)* "such as a lack of reference alignments" is translated as "如无参照对对应" ( literal translation)* "runtime latency" is translated as "运行时延迟" ( literal translation)* "and unexplored different graph structures within an end-to-end framework" is translated as "和未探索的不同图结构在端到端框架中" ( literal translation)* "In this study, we introduce a novel self-supervised learning OM framework with input ontologies" is translated as "在本研究中, we introduce a novel self-supervised learning OM framework with input ontologies" (同义词翻译)* "called LaKERMap" is translated as "called LaKERMap" (同义词翻译)* "This framework capitalizes on the contextual and structural information of concepts" is translated as "这个框架利用概念的上下文ual和结构信息" ( literal translation)* "by integrating implicit knowledge into transformers" is translated as "通过将隐式知识 integrate into transformers" ( literal translation)* "Specifically, we aim to capture multiple structural contexts" is translated as " Specifically, we aim to capture multiple structural contexts" (同义词翻译)* "encompassing both local and global interactions" is translated as "包括both local and global interactions" ( literal translation)* "by employing distinct training objectives" is translated as "通过不同的训练目标" ( literal translation)* "To assess our methods, we utilize the Bio-ML datasets and tasks" is translated as "以评估我们的方法, we utilize the Bio-ML datasets and tasks" (同义词翻译)* "The findings from our innovative approach reveal that LaKERMap surpasses state-of-the-art systems" is translated as "我们创新的方法的结果表明LaKERMap超过了现状的系统" ( literal translation)* "in terms of alignment quality and inference time" is translated as "在对应质量和推理时间方面" ( literal translation)* "Our models and codes are available here: https://github.com/ellenzhuwang/lakermap" is translated as "我们的模型和代码可以在这里获取: https://github.com/ellenzhuwang/lakermap" (同义词翻译)
</details></li>
</ul>
<hr>
<h2 id="Simulating-Social-Media-Using-Large-Language-Models-to-Evaluate-Alternative-News-Feed-Algorithms"><a href="#Simulating-Social-Media-Using-Large-Language-Models-to-Evaluate-Alternative-News-Feed-Algorithms" class="headerlink" title="Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms"></a>Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05984">http://arxiv.org/abs/2310.05984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petter Törnberg, Diliara Valeeva, Justus Uitermark, Christopher Bail</li>
<li>for: 本研究旨在研究如何通过组合大自然语言模型（LLM）和代理模型来实现社交媒体平台的优化。</li>
<li>methods: 本研究使用了大自然语言模型（LLM）和代理模型来模拟社交媒体平台，并使用了来自美国全国选举研究的数据来填充模拟的社交媒体平台。</li>
<li>results: 研究发现，使用“桥接”算法可以促进不同政见用户之间的构成性对话，而不同的新闻列表算法则可能会导致更多的攻击性和不constructive的对话。<details>
<summary>Abstract</summary>
Social media is often criticized for amplifying toxic discourse and discouraging constructive conversations. But designing social media platforms to promote better conversations is inherently challenging. This paper asks whether simulating social media through a combination of Large Language Models (LLM) and Agent-Based Modeling can help researchers study how different news feed algorithms shape the quality of online conversations. We create realistic personas using data from the American National Election Study to populate simulated social media platforms. Next, we prompt the agents to read and share news articles - and like or comment upon each other's messages - within three platforms that use different news feed algorithms. In the first platform, users see the most liked and commented posts from users whom they follow. In the second, they see posts from all users - even those outside their own network. The third platform employs a novel "bridging" algorithm that highlights posts that are liked by people with opposing political views. We find this bridging algorithm promotes more constructive, non-toxic, conversation across political divides than the other two models. Though further research is needed to evaluate these findings, we argue that LLMs hold considerable potential to improve simulation research on social media and many other complex social settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ECAvg-An-Edge-Cloud-Collaborative-Learning-Approach-using-Averaged-Weights"><a href="#ECAvg-An-Edge-Cloud-Collaborative-Learning-Approach-using-Averaged-Weights" class="headerlink" title="ECAvg: An Edge-Cloud Collaborative Learning Approach using Averaged Weights"></a>ECAvg: An Edge-Cloud Collaborative Learning Approach using Averaged Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03823">http://arxiv.org/abs/2310.03823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atah Nuh Mih, Hung Cao, Asfia Kawnine, Monica Wachowicz</li>
<li>for: 这个研究旨在提出一个Edge-Cloud共同运作的架构，让Edge和Clouddevices之间建立合作关系，各自补偿对方的缺陷。</li>
<li>methods: 这个方法是基于Edge device先训练本地模型，然后将模型转移到服务器进行精细调整。服务器给出了一个全球模型，并将它与各个Edge device的本地模型进行测量。</li>
<li>results: 在CIFAR-10和CIFAR-100分类任务中，我们发现使用我们的方法可以提高服务器模型的性能，并且在Edge device上进行模型更新可以提高性能。但在MNIST分类任务中，将权重平均化导致服务器和Edge device模型的性能下降，这是由于负面传播学习所致。<details>
<summary>Abstract</summary>
The use of edge devices together with cloud provides a collaborative relationship between both classes of devices where one complements the shortcomings of the other. Resource-constraint edge devices can benefit from the abundant computing power provided by servers by offloading computationally intensive tasks to the server. Meanwhile, edge devices can leverage their close proximity to the data source to perform less computationally intensive tasks on the data. In this paper, we propose a collaborative edge-cloud paradigm called ECAvg in which edge devices pre-train local models on their respective datasets and transfer the models to the server for fine-tuning. The server averages the pre-trained weights into a global model, which is fine-tuned on the combined data from the various edge devices. The local (edge) models are then updated with the weights of the global (server) model. We implement a CIFAR-10 classification task using MobileNetV2, a CIFAR-100 classification task using ResNet50, and an MNIST classification using a neural network with a single hidden layer. We observed performance improvement in the CIFAR-10 and CIFAR-100 classification tasks using our approach, where performance improved on the server model with averaged weights and the edge models had a better performance after model update. On the MNIST classification, averaging weights resulted in a drop in performance on both the server and edge models due to negative transfer learning. From the experiment results, we conclude that our approach is successful when implemented on deep neural networks such as MobileNetV2 and ResNet50 instead of simple neural networks.
</details>
<details>
<summary>摘要</summary>
使用边缘设备与云计算机联合，两类设备之间形成合作关系，其中边缘设备利用云计算机的庞大计算能力来推OFF computationally intensive tasks，而云计算机则可以利用边缘设备的数据靠近源来执行less computationally intensive tasks。在这篇论文中，我们提出了一种协同边缘云模型（ECAvg），其中边缘设备先在本地数据集上预训local models，然后将模型传输到服务器进行细化。服务器将预训模型的权重平均化为全局模型，并在多个边缘设备的数据合并后进行细化。本地（边缘）模型然后将全局（服务器）模型的权重更新。我们在MobileNetV2、ResNet50和一个单Hidden layer的神经网络上实现了CIFAR-10、CIFAR-100和MNIST分类任务。我们发现，使用我们的方法时，CIFAR-10和CIFAR-100分类任务中的性能提高，而服务器模型和边缘模型都有更好的性能之后更新模型。但在MNIST分类任务中，平均权重导致服务器和边缘模型的性能下降，这是因为负转移学习。从实验结果来看，我们的方法在深度神经网络如MobileNetV2和ResNet50上更加成功，而不是简单的神经网络。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Cold-start-Bundle-Recommendation-via-Popularity-based-Coalescence-and-Curriculum-Heating"><a href="#Accurate-Cold-start-Bundle-Recommendation-via-Popularity-based-Coalescence-and-Curriculum-Heating" class="headerlink" title="Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating"></a>Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03813">http://arxiv.org/abs/2310.03813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunsik Jeon, Jong-eun Lee, Jeongin Yun, U Kang</li>
<li>for: 提出了一种准确的冷启动bundle推荐方法，用于解决实际场景中新bundle的创造和推荐问题。</li>
<li>methods: 提出了一种基于媒体的CoHeat方法，通过结合历史信息和联合信息来衡量用户-bundle关系，并通过curriculum学习和对比学习来学习秘密表示。</li>
<li>results: 对比 bestechnologie，CoHeat方法在冷启动bundle推荐中显示出了193%高的nDCG@20指标， indicating its superior performance in accurately recommending cold-start bundles.<details>
<summary>Abstract</summary>
How can we accurately recommend cold-start bundles to users? The cold-start problem in bundle recommendation is critical in practical scenarios since new bundles are continuously created for various marketing purposes. Despite its importance, no previous studies have addressed cold-start bundle recommendation. Moreover, existing methods for cold-start item recommendation overly rely on historical information, even for unpopular bundles, failing to tackle the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for the cold-start bundle recommendation. CoHeat tackles the highly skewed distribution of bundle interactions by incorporating both historical and affiliation information based on the bundle's popularity when estimating the user-bundle relationship. Furthermore, CoHeat effectively learns latent representations by exploiting curriculum learning and contrastive learning. CoHeat demonstrates superior performance in cold-start bundle recommendation, achieving up to 193% higher nDCG@20 compared to the best competitor.
</details>
<details>
<summary>摘要</summary>
如何准确推荐冷启用户？冷启问题在Bundle推荐中是非常重要的，新的Bundle在各种市场营销目的下不断创建。然而，前面的研究未能够正确地解决冷启Bundle推荐问题。现有的冷启Item推荐方法过于依赖历史信息，即使是不受欢迎的Bundle也会被优先推荐，无法解决主要挑战：Bundle互动的非常均衡分布。在这项工作中，我们提出CoHeat（流行度基于的合并和辅助热化），一种精度的冷启Bundle推荐方法。CoHeat通过考虑Bundle的流行度时对用户-Bundle关系进行估计，解决了高度均衡分布的Bundle互动问题。此外，CoHeat通过辅助学习和对比学习来有效地学习潜在表示。CoHeat在冷启Bundle推荐方面表现出色，与最佳竞争对手相比，可以达到193%的nDCG@20提高。
</details></li>
</ul>
<hr>
<h2 id="Improved-Baselines-with-Visual-Instruction-Tuning"><a href="#Improved-Baselines-with-Visual-Instruction-Tuning" class="headerlink" title="Improved Baselines with Visual Instruction Tuning"></a>Improved Baselines with Visual Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03744">http://arxiv.org/abs/2310.03744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee</li>
<li>for: 这份研究是为了提高大型多模态模型（LMM）的视觉指令调整能力。</li>
<li>methods: 这份研究使用了 LLVA 模型，并进行了一些简单的修改，包括使用 CLIP-ViT-L-336px 和 MLP 投影，以及添加学术任务oriented VQA 数据和简单的响应格式提示。</li>
<li>results: 研究显示，通过这些修改，可以建立更强的基elines，达到了 11 个标准测试 benchmark 的状态态。最终的 13B 检查点只需使用了 1.2M 公共可用数据，并在单个 8-A100 节点上完成了全程训练，耗时约为 1 天。<details>
<summary>Abstract</summary>
Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.
</details>
<details>
<summary>摘要</summary>
大型多Modal模型（LMM）最近已经表现出了鼓舞人心的进步，在这份笔记中，我们表明了 LLava 中的全连接视力语言跨模态连接器 surprisingly 强大和数据有效。通过简单地修改 LLava， specifically using CLIP-ViT-L-336px with MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks。我们的最终13B checkpoint只用了1.2M公共可用数据，并在单个8-A100节点上完成了完整的训练，只需要大约1天时间。我们希望这可以让state-of-the-art LMM研究更加 accessible。代码和模型将公开available。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Text-to-Image-Diffusion-Models-with-Reward-Backpropagation"><a href="#Aligning-Text-to-Image-Diffusion-Models-with-Reward-Backpropagation" class="headerlink" title="Aligning Text-to-Image Diffusion Models with Reward Backpropagation"></a>Aligning Text-to-Image Diffusion Models with Reward Backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03739">http://arxiv.org/abs/2310.03739</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mihirp1998/alignprop">https://github.com/mihirp1998/alignprop</a></li>
<li>paper_authors: Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki</li>
<li>for: 这篇论文的目的是优化文本至图像生成模型，以便在下游任务中控制其行为，例如提高人类所感觉的图像质量、图像文本Alignment、或道德性图像生成。</li>
<li>methods: 本篇论文提出了一种名为AlignProp的方法，它使用端到端归整法将扩散模型与下游 reward function  alignment，通过终端测量过程中的梯度检查点来实现可持续的内存使用。</li>
<li>results: 根据本篇论文的测试结果，AlignProp 在调整扩散模型的不同目标下（例如图像文本对齐、美学、压缩性和物件数量控制）表现出比其他方法更高的奖励，同时更简单易懂，因此可以轻松地优化扩散模型以满足 differentiable reward function 的需求。<details>
<summary>Abstract</summary>
Text-to-image diffusion models have recently emerged at the forefront of image generation, powered by very large-scale unsupervised or weakly supervised text-to-image training datasets. Due to their unsupervised training, controlling their behavior in downstream tasks, such as maximizing human-perceived image quality, image-text alignment, or ethical image generation, is difficult. Recent works finetune diffusion models to downstream reward functions using vanilla reinforcement learning, notorious for the high variance of the gradient estimators. In this paper, we propose AlignProp, a method that aligns diffusion models to downstream reward functions using end-to-end backpropagation of the reward gradient through the denoising process. While naive implementation of such backpropagation would require prohibitive memory resources for storing the partial derivatives of modern text-to-image models, AlignProp finetunes low-rank adapter weight modules and uses gradient checkpointing, to render its memory usage viable. We test AlignProp in finetuning diffusion models to various objectives, such as image-text semantic alignment, aesthetics, compressibility and controllability of the number of objects present, as well as their combinations. We show AlignProp achieves higher rewards in fewer training steps than alternatives, while being conceptually simpler, making it a straightforward choice for optimizing diffusion models for differentiable reward functions of interest. Code and Visualization results are available at https://align-prop.github.io/.
</details>
<details>
<summary>摘要</summary>
文本到图像扩散模型最近在图像生成领域取得了前列位，得益于巨大的文本到图像无监督或弱监督训练 dataset。由于它们的无监督训练，控制它们在下游任务中，如提高人类感知的图像质量、图像文本对齐或道德图像生成，是困难的。现有的工作是通过普通的再征学习训练 diffusion models，不可避免的高弹性问题。在这篇论文中，我们提出了 AlignProp，一种方法，用于将 diffusion models 与下游奖励函数相对位。我们使用终端到终端的反推进程来实现这一点，并使用低级adapter weight模块的训练和梯度检查点，使其可以实现可持续的存储和计算。我们在不同的目标上训练 diffusion models，如图像文本 semantic alignment、美学、压缩和对象数量控制，以及其组合。我们发现 AlignProp 在更少的训练步骤中 achieve 更高的奖励，而且概念更简单，因此在 differentiable 奖励函数的 интерес领域中是一个简单的选择。代码和视觉结果可以通过 https://align-prop.github.io/ 访问。
</details></li>
</ul>
<hr>
<h2 id="MathCoder-Seamless-Code-Integration-in-LLMs-for-Enhanced-Mathematical-Reasoning"><a href="#MathCoder-Seamless-Code-Integration-in-LLMs-for-Enhanced-Mathematical-Reasoning" class="headerlink" title="MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning"></a>MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03731">http://arxiv.org/abs/2310.03731</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mathllm/mathcoder">https://github.com/mathllm/mathcoder</a></li>
<li>paper_authors: Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li</li>
<li>for: 这篇论文的目的是提高开源语言模型的数学逻辑能力。</li>
<li>methods: 该论文提出了一种方法，用于微调开源语言模型，使其可以使用代码来建模和 derivation 数学公式。</li>
<li>results: 该论文的实验结果表明，使用该方法可以创建一些高质量的数学问题和其解决方案的代码 dataset，并且可以在MATH和GSM8K数据集上达到状态 искусственный智能模型的最高分（45.2%和83.9%）。<details>
<summary>Abstract</summary>
The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/MathCoder.
</details>
<details>
<summary>摘要</summary>
Recently released GPT-4 Code Interpreter 已经表现出优秀的能力解决复杂的数学问题，主要归功于其能够自然语言和代码之间无缝相互作用，生成代码，执行代码，然后继续根据执行结果进行数学逻辑推理。在这篇论文中，我们提出了一种方法来调整开源语言模型，使其可以使用代码来建模和推导数学方程，从而提高其数学逻辑能力。我们提出了一种生成 novel 和高质量数学问题和代码解决方案的方法，称为 MathCodeInstruct。每个解决方案都包含自然语言、代码和执行结果。我们还介绍了一种自定义的监督训练和推理方法。这种方法生成了 MathCoder 模型，它是一家能够通过代码来解决复杂数学问题的模型。很显然，MathCoder 模型在开源 LLM 中的状态表现非常出色，在 MATH 和 GSM8K 数据集上分别达到了 45.2% 和 83.9% 的得分，大幅超过其他开源选项。尤其是，MathCoder 模型不仅在 GSM8K 和 MATH 数据集上超过 ChatGPT-3.5 和 PaLM-2，还在竞赛级别的 MATH 数据集上超过 GPT-4。数据集和模型将在 GitHub 上发布，请参考 <https://github.com/mathllm/MathCoder>。
</details></li>
</ul>
<hr>
<h2 id="Constraint-Conditioned-Policy-Optimization-for-Versatile-Safe-Reinforcement-Learning"><a href="#Constraint-Conditioned-Policy-Optimization-for-Versatile-Safe-Reinforcement-Learning" class="headerlink" title="Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning"></a>Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03718">http://arxiv.org/abs/2310.03718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihang Yao, Zuxin Liu, Zhepeng Cen, Jiacheng Zhu, Wenhao Yu, Tingnan Zhang, Ding Zhao</li>
<li>for: 本研究旨在训练具有安全限制的奖励学习（RL） Agent，以满足不同安全限制要求。</li>
<li>methods: 我们提出了 Conditioned Constrained Policy Optimization（CCPO）框架，包括两个关键模块：（1） Versatile Value Estimation（VVE）用于在未经见过的阈值条件下估算价值函数，以及（2） Conditioned Variational Inference（CVI）用于在策略优化过程中编码特定的安全限制条件。</li>
<li>results: 我们的实验结果表明，CCPO 可以在安全性和任务性能之间取得平衡，同时维护零批量适应性，使其适用于真实世界中的动态应用场景。<details>
<summary>Abstract</summary>
Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint thresholds data-efficiently. This makes our approach suitable for real-world dynamic applications.
</details>
<details>
<summary>摘要</summary>
安全强化学习（RL）专注于训练根据预先定义的安全限制的奖励最大化代理人。然而，在部署期间无需重新训练而适应不同安全限制要求的灵活安全策略仍是一个未探讨的和挑战性的领域。在这项工作中，我们定义了多样化安全RL问题，并考虑了两个主要要求：训练效率和零扩展能力。为解决这些问题，我们提出了条件constrained Policy优化框架（CCPO），该框架包括以下两个关键模块：1. 多样化价值估计（VVE）：用于在未看到的阈值条件下估计价值函数。2. 条件variational推理（CVI）：用于在政策优化过程中编码任意的阈值条件。我们的广泛的实验表明，CCPO在安全性和任务性能方面表现出色，同时保持零扩展能力，以便在不同的阈值条件下进行数据效率地部署。这使得我们的方法适用于真实的动态应用程序。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-Index-Report-2023"><a href="#Artificial-Intelligence-Index-Report-2023" class="headerlink" title="Artificial Intelligence Index Report 2023"></a>Artificial Intelligence Index Report 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03715">http://arxiv.org/abs/2310.03715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, Raymond Perrault</li>
<li>for: 这份报告是为了提供不偏袋化、严格验证的AI相关数据，以便政策制定者、研究人员、高管、新闻工作者和一般公众更好地理解人工智能领域的复杂问题。</li>
<li>methods: 这份报告使用了多种方法，包括新的AI公众意见章节、更详细的技术性表现章节、大语言和多媒体模型的原始分析、全球AI法规记录的详细趋势、AI系统环境影响的研究和更多的数据来跟踪、汇总、筛选和可视化AI相关数据。</li>
<li>results: 这份报告提供了更多原创数据，包括AI公众意见、技术性表现、大语言和多媒体模型的分析、全球AI法规记录的趋势和AI系统环境影响的研究结果。<details>
<summary>Abstract</summary>
Welcome to the sixth edition of the AI Index Report. This year, the report introduces more original data than any previous edition, including a new chapter on AI public opinion, a more thorough technical performance chapter, original analysis about large language and multimodal models, detailed trends in global AI legislation records, a study of the environmental impact of AI systems, and more. The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world's most credible and authoritative source for data and insights about AI.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:欢迎来到第六版AI指数报告。本年度报告包含更多的原创数据，包括一新的公众意见章节、更加详细的技术性能章节、关于大语言和多Modal模型的原始分析、全球AI法规纪录的详细趋势、AI系统的环境影响研究和更多。AI指数报告跟踪、汇总、缩写和可视化相关的人工智能数据，旨在为政策制定者、研究人员、高管、记者和普通公众提供不偏不倚的、严格审核的、广泛来源的数据，以便他们更好地理解人工智能领域的复杂问题。报告目标是成为全球最可靠和权威的AI数据和意见源。
</details></li>
</ul>
<hr>
<h2 id="DSPy-Compiling-Declarative-Language-Model-Calls-into-Self-Improving-Pipelines"><a href="#DSPy-Compiling-Declarative-Language-Model-Calls-into-Self-Improving-Pipelines" class="headerlink" title="DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines"></a>DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03714">http://arxiv.org/abs/2310.03714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanfordnlp/dspy">https://github.com/stanfordnlp/dspy</a></li>
<li>paper_authors: Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts</li>
<li>for:  This paper is written for developing and optimizing language model (LM) pipelines using a programming model called DSPy.</li>
<li>methods:  The paper uses a programming model called DSPy to abstract LM pipelines as text transformation graphs, and introduces a compiler that optimizes any DSPy pipeline to maximize a given metric.</li>
<li>results:  The paper shows that succinct DSPy programs can express and optimize sophisticated LM pipelines that outperform standard few-shot prompting and pipelines with expert-created demonstrations, and that DSPy is competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5.<details>
<summary>Abstract</summary>
The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy
</details>
<details>
<summary>摘要</summary>
《机器学习社区正在快速探索语言模型（LM）的引导技术和将其组合成复杂任务解决的管道。然而，现有的LM管道通常通过手动定制"提示模板"来实现，即通过尝试和错误来发现长串。为了更系统地开发和优化LM管道，我们介绍了DSPy，它是一种文本转换图模型，将LM管道转换为声明式计算图。DSPy模块是可参数化的，这意味着它们可以通过创建和收集示例来学习应用 Compositions of 提示、训练、扩展和理解技术。我们设计了一个编译器，可以对任何DSPy管道进行优化，以最大化给定指标。我们进行了两个案例研究，显示了简洁的DSPy程序可以表达和优化复杂LM管道，处理数学问题、多步返回、复杂问题和控制 Agent 循环。在编译后只需几行DSPy代码，GPT-3.5和 llama2-13b-chat 可以自动化管道，并在标准几个示例后（通常高于25%和65%）和专家创建示例（最高上升5-46%和16-40%）之上表现出色。此外，DSPy 编译到小型LM如770M-参数的T5和 llama2-13b-chat 与专家写提示链的方法相当竞争。DSPy 可以在 <https://github.com/stanfordnlp/dspy> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Agent-Instructs-Large-Language-Models-to-be-General-Zero-Shot-Reasoners"><a href="#Agent-Instructs-Large-Language-Models-to-be-General-Zero-Shot-Reasoners" class="headerlink" title="Agent Instructs Large Language Models to be General Zero-Shot Reasoners"></a>Agent Instructs Large Language Models to be General Zero-Shot Reasoners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03710">http://arxiv.org/abs/2310.03710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/agentinstruct">https://github.com/wang-research-lab/agentinstruct</a></li>
<li>paper_authors: Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, Chenguang Wang</li>
<li>for: 提高大语言模型在通用语言理解任务上的零shot理解能力</li>
<li>methods: 建立一个自主智能体来指导大语言模型的理解过程</li>
<li>results: 我们的方法在各种数据集上表现出色，在29个数据集中取得了state-of-the-art的零shot性能，比如提高了现状体现模型的性能，例如Vicuna-13b（13.3%）、Llama-2-70b-chat（23.2%）和GPT-3.5 Turbo（17.0%），与零shot chain of thought相比，我们的改进是很显著的，平均提高10.5%。<details>
<summary>Abstract</summary>
We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
</details>
<details>
<summary>摘要</summary>
我们提出一种方法，以提高大语言模型在通用语言理解任务上的零基础理解能力。具体来说，我们构建了一个自动化代理，以控制大语言模型的理解过程。我们发现这种方法可以进一步释放大语言模型的零基础理解能力，以更多任务。我们对一系列数据集进行了测试，包括生成、分类和理解等任务。我们发现这种方法在大多数任务上具有普适性，并在20个数据集中实现了零基础性状态之势。例如，我们的方法可以大幅提高现有的状态级别大语言模型的性能，包括Vicuna-13b（13.3%）、Llama-2-70b-chat（23.2%）和GPT-3.5 Turbo（17.0%）。相比零基础思维，我们的改进在理解方面是悬殊的，平均提高10.5%。凭借我们的方法，Llama-2-70b-chat可以在零基础情况下超越GPT-3.5 Turbo的性能，提高10.2%。
</details></li>
</ul>
<hr>
<h2 id="Beyond-One-Preference-for-All-Multi-Objective-Direct-Preference-Optimization-for-Language-Models"><a href="#Beyond-One-Preference-for-All-Multi-Objective-Direct-Preference-Optimization-for-Language-Models" class="headerlink" title="Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization for Language Models"></a>Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03708">http://arxiv.org/abs/2310.03708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, Yu Qiao</li>
<li>for: 这 paper 的目的是为了开发一种不需要人工学习的多目标RLHF算法，以提高语言模型的个性化适应性。</li>
<li>methods: 这 paper 使用的方法是基于直接喜好函数优化（DPO）的多目标RLHF算法，通过约束搜索和约束优化来学习多个目标对齐对象。</li>
<li>results: 实验结果表明，使用 MODPO 可以与现有方法匹配或超越其性能，并且可以在3倍的计算量下完成多目标RLHF。<details>
<summary>Abstract</summary>
A single language model (LM), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches thus pursue customization, training separate principle-based reward models to represent different alignment objectives (e.g. helpfulness, harmlessness, or honesty). Different LMs can then be trained for different preferences through multi-objective RLHF (MORLHF) with different objective weightings. Yet, RLHF is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO folds LM learning directly into reward modeling, aligning LMs with the weighted sum of all principle-based rewards using pure cross-entropy loss. While theoretically guaranteed to produce the same optimal solutions as MORLHF, MODPO is practically more stable and computationally efficient, obviating value function modeling and online sample collection. Empirical results in safety alignment and long-form question answering confirm that MODPO matches or outperforms existing methods, consistently producing one of the most competitive LM fronts that cater to diverse preferences with 3 times fewer computations compared with MORLHF.
</details>
<details>
<summary>摘要</summary>
一个语言模型（LM），即使与平均标注员的匹配得很好，也可能不适应人类的多样化偏好。现有的方法因此尝试个性化，通过训练不同的原则基于奖励模型来表达不同的匹配目标（例如帮助fulness、无害性和诚实）。然后可以通过多目标RLHF（MORLHF）来训练不同的LM。然而，RLHF是不稳定的，特别是在多目标情况下。在这篇论文中，我们提出了多目标直接偏好优化（MODPO）算法，它是RL无法算法，扩展了直接偏好优化（DPO）来处理多个原则基于奖励的目标。MODPO通过将LM学习直接嵌入奖励模型中，将LM与所有原则基于奖励的权重加权和平均值相对应。虽然从理论角度来看，MODPO和MORLHF都可以生成同样的优化解，但MODPO在实践中更稳定和计算效率更高，不需要值函数模型和在线样本采集。实验结果表明，MODPO在安全匹配和长文问答中与现有方法匹配或超越，可靠地生成适应多种偏好的LM前端，使用3倍少的计算量比MORLHF。
</details></li>
</ul>
<hr>
<h2 id="Fine-tuning-Aligned-Language-Models-Compromises-Safety-Even-When-Users-Do-Not-Intend-To"><a href="#Fine-tuning-Aligned-Language-Models-Compromises-Safety-Even-When-Users-Do-Not-Intend-To" class="headerlink" title="Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"></a>Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03693">http://arxiv.org/abs/2310.03693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llm-tuning-safety/llms-finetuning-safety">https://github.com/llm-tuning-safety/llms-finetuning-safety</a></li>
<li>paper_authors: Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson<br>for:这篇论文探讨了在自定义逻辑语言模型（LLM）上进行微调时，安全成本是什么样的？研究发现，即使模型初始化时的安全性检查通过，也不能保证模型在微调后保持安全。methods:研究人员使用了OpenAI的API进行微调GPT-3.5 Turbo模型，并通过自定义训练集来攻击模型的安全性。results:研究发现，只需要对GPT-3.5 Turbo模型进行微调 Using 10个逆向设计的训练例子，可以破坏模型的安全保护，并且这些例子可以通过OpenAI的API进行微调。此外，研究还发现，只要使用一些常用的训练集来微调模型，即使没有恶意，也可以减弱模型的安全性。这些发现表明，自定义微调aligned LLMs可能会带来新的安全隐患，而现有的安全基础设施并不能够处理这些隐患。<details>
<summary>Abstract</summary>
Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.
</details>
<details>
<summary>摘要</summary>
优化大型自然语言模型（LLM）为下游应用场景通常通过进一步精度调整来进行自定义。梅塔公司开源了LLAMA模型，而OpenAI提供了用于在自定义数据集上精度调整GPT-3.5 Turbo的API，这些做法也鼓励了这种实践。但是，在这种自定义调整中存在哪些安全成本呢？我们发现，虽然现有的安全对齐基础设施可以在推理时约束LLM的危险行为，但是它们不能覆盖在调整时的安全风险。我们的红团研究发现，通过只有少量反制设计的训练例来破坏LLM的安全对齐可以在 less than $0.20 的成本下使GPT-3.5 Turbo进行破坏。另外，我们发现，只要是通过常用的数据集进行调整，即使没有恶意，也可以不知不觉地削弱LLM的安全对齐。这些发现表明，在自定义调整aligned LLMs时，存在新的安全风险，现有的安全基础设施无法妥善处理这些风险。我们提出和分析了可能的缓解措施，并且强调进一步的研究努力应对自定义调整 aligned LLMs 的安全问题。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Generative-Modeling-for-Procedural-Roundabout-Generation-for-Developing-Countries"><a href="#Probabilistic-Generative-Modeling-for-Procedural-Roundabout-Generation-for-Developing-Countries" class="headerlink" title="Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries"></a>Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03687">http://arxiv.org/abs/2310.03687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zarif Ikram, Ling Pan, Dianbo Liu</li>
<li>for: 设计优化交通路网，以优化交通运输和 validate 效果，为发展中国家提供成本效果的方案。</li>
<li>methods: 使用 Generative Flow Networks (GFlowNets) 学习权值分布，生成高质量的解决方案，保留多样性。</li>
<li>results: 与相关方法进行比较，实验结果表明，我们的方法可以保持高效性，同时具有更高的多样性。<details>
<summary>Abstract</summary>
Due to limited resources and fast economic growth, designing optimal transportation road networks with traffic simulation and validation in a cost-effective manner is vital for developing countries, where extensive manual testing is expensive and often infeasible. Current rule-based road design generators lack diversity, a key feature for design robustness. Generative Flow Networks (GFlowNets) learn stochastic policies to sample from an unnormalized reward distribution, thus generating high-quality solutions while preserving their diversity. In this work, we formulate the problem of linking incident roads to the circular junction of a roundabout by a Markov decision process, and we leverage GFlowNets as the Junction-Art road generator. We compare our method with related methods and our empirical results show that our method achieves better diversity while preserving a high validity score.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)由于有限的资源和快速的经济增长，为发展中国家设计优化的交通运输路网，并在成本效益的情况下进行交通模拟和验证，是非常重要的。现有的规则基于的路线设计生成器缺乏多样性，这是设计Robustness的关键特征。生成流网络（GFlowNets）学习了随机政策，从未正规化的奖励分布中采样，因此可以生成高质量的解决方案，同时保持多样性。在这项工作中，我们将环境穿梭的问题形式化为Markov决策过程，并利用GFlowNets作为环境艺术路径生成器。我们与相关方法进行比较，我们的实验结果表明，我们的方法可以保持高有效性分数，同时提高多样性。
</details></li>
</ul>
<hr>
<h2 id="Automating-Human-Tutor-Style-Programming-Feedback-Leveraging-GPT-4-Tutor-Model-for-Hint-Generation-and-GPT-3-5-Student-Model-for-Hint-Validation"><a href="#Automating-Human-Tutor-Style-Programming-Feedback-Leveraging-GPT-4-Tutor-Model-for-Hint-Generation-and-GPT-3-5-Student-Model-for-Hint-Validation" class="headerlink" title="Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation"></a>Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03780">http://arxiv.org/abs/2310.03780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung Phung, Victor-Alexandru Pădurean, Anjali Singh, Christopher Brooks, José Cambronero, Sumit Gulwani, Adish Singla, Gustavo Soares</li>
<li>for: 提高编程教育质量 by 自动生成个性化反馈</li>
<li>methods: 使用生成AI模型提供人工导师式编程提示，使学生解决buggy程序错误</li>
<li>results: 通过使用GPT-4和GPT-3.5两个模型，提高生成质量，并通过自动评估提示质量，证明效果可行<details>
<summary>Abstract</summary>
Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a ``tutor'' model to generate hints -- it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a ``student'' model to further validate the hint quality -- it performs an automatic quality validation by simulating the potential utility of providing this feedback. We show the efficacy of our technique via extensive evaluation using three real-world datasets of Python programs covering a variety of concepts ranging from basic algorithms to regular expressions and data analysis using pandas library.
</details>
<details>
<summary>摘要</summary>
�� Makin' AI and big language models hold great promise in improvIN' programming education by automatically generatin' individualized feedback for students. We investigate the role of generative AI models in providin' human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providin' high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a "tutor" model to generate hints -- it boosts the generative quality by usin' symbolic information of failin' test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a "student" model to further validate the hint quality -- it performs an automatic quality validation by simulatin' the potential utility of providin' this feedback. We show the efficacy of our technique via extensive evaluation usin' three real-world datasets of Python programs coverin' a variety of concepts ranging from basic algorithms to regular expressions and data analysis usin' pandas library.
</details></li>
</ul>
<hr>
<h2 id="SmoothLLM-Defending-Large-Language-Models-Against-Jailbreaking-Attacks"><a href="#SmoothLLM-Defending-Large-Language-Models-Against-Jailbreaking-Attacks" class="headerlink" title="SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"></a>SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03684">http://arxiv.org/abs/2310.03684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas</li>
<li>for: 提高大型自然语言模型（LLM）的安全性，防止攻击者利用LLM生成不良内容。</li>
<li>methods: 提出了首个针对LLM的攻击mitigation算法SmoothLLM，通过多个复制输入提示，并将其相应的预测结果集成以检测攻击输入。</li>
<li>results: SmoothLLM可以在许多流行的LLM上降低攻击成功率至0.1%以下，避免过度保守，并具有可证明的攻击防御保证。<details>
<summary>Abstract</summary>
Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
</details>
<details>
<summary>摘要</summary>
尽管努力对大型语言模型（LLM）与人类价值观念进行对应，广泛使用的 LLM 如 GPT、Llama、Claude 和 PalM 仍然容易受到犯罪攻击，其中敌对者会让目标 LLM 生成不良内容。为解决这个漏洞，我们提出了 SmoothLLM，首个针对 LLM 进行犯罪攻击防御的算法。根据我们发现，恶意生成的提示语是字符级别上不稳定的，我们的防御首先随机干扰多个输入提示的字符，然后将相应的预测结果聚合以检测恶意输入。SmoothLLM 可以在许多流行的 LLM 上降低犯罪成功率至少一个百分点，避免不必要的保守性，并且具有可证明的攻击防御保证。此外，我们的防御需要比现有攻击更少的查询数量，并且可以与任何 LLM 兼容。
</details></li>
</ul>
<hr>
<h2 id="MapperGPT-Large-Language-Models-for-Linking-and-Mapping-Entities"><a href="#MapperGPT-Large-Language-Models-for-Linking-and-Mapping-Entities" class="headerlink" title="MapperGPT: Large Language Models for Linking and Mapping Entities"></a>MapperGPT: Large Language Models for Linking and Mapping Entities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03666">http://arxiv.org/abs/2310.03666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Matentzoglu, J. Harry Caufield, Harshad B. Hegde, Justin T. Reese, Sierra Moxon, Hyeongsik Kim, Nomi L. Harris, Melissa A Haendel, Christopher J. Mungall</li>
<li>for: 提高数据 интеграция中的Entity mapping精度，使其更能准确地将不同资源中的实体映射到相应的概念上。</li>
<li>methods: 使用Large Language Models（LLMs）进行Entity mapping审核和修正，以提高 mapping 精度。</li>
<li>results: 在不同领域的Alignment任务中，MapperGPT可以与高准确率方法相结合，提供substantial改进的准确率，比如LogMap等State-of-the-art方法。<details>
<summary>Abstract</summary>
Aligning terminological resources, including ontologies, controlled vocabularies, taxonomies, and value sets is a critical part of data integration in many domains such as healthcare, chemistry, and biomedical research. Entity mapping is the process of determining correspondences between entities across these resources, such as gene identifiers, disease concepts, or chemical entity identifiers. Many tools have been developed to compute such mappings based on common structural features and lexical information such as labels and synonyms. Lexical approaches in particular often provide very high recall, but low precision, due to lexical ambiguity. As a consequence of this, mapping efforts often resort to a labor intensive manual mapping refinement through a human curator.   Large Language Models (LLMs), such as the ones employed by ChatGPT, have generalizable abilities to perform a wide range of tasks, including question-answering and information extraction. Here we present MapperGPT, an approach that uses LLMs to review and refine mapping relationships as a post-processing step, in concert with existing high-recall methods that are based on lexical and structural heuristics.   We evaluated MapperGPT on a series of alignment tasks from different domains, including anatomy, developmental biology, and renal diseases. We devised a collection of tasks that are designed to be particularly challenging for lexical methods. We show that when used in combination with high-recall methods, MapperGPT can provide a substantial improvement in accuracy, beating state-of-the-art (SOTA) methods such as LogMap.
</details>
<details>
<summary>摘要</summary>
合理资源对Alignment是各个领域的数据 интеграción中的关键环节，如医疗、化学和生物研究等。实体映射是确定这些资源中的实体之间对应关系的过程，例如基因标识符、疾病概念或化学实体标识符。许多工具已经开发出来计算这些对应关系，基于共同结构特征和 lexical信息，如标签和同义词。lexical方法通常提供很高的回快，但准确率很低，因为lexical是多义的。因此，映射努力通常需要劳动密集的手动映射纠正。大语言模型（LLMs），如ChatGPT中所使用的模型，具有通用的能力来完成广泛的任务，包括问答和信息提取。我们提出了MapperGPT，一种使用LLMs来复制和纠正映射关系的方法，并与现有的高准确率方法相结合。我们在不同领域的一系列对alignment任务中评估了MapperGPT。我们设计了一组特别适合lexical方法的任务，并显示了MapperGPT可以提供substantial提升的准确率，超过了State-of-the-art（SOTA）方法，如LogMap。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Autonomy-and-Alignment-A-Multi-Dimensional-Taxonomy-for-Autonomous-LLM-powered-Multi-Agent-Architectures"><a href="#Balancing-Autonomy-and-Alignment-A-Multi-Dimensional-Taxonomy-for-Autonomous-LLM-powered-Multi-Agent-Architectures" class="headerlink" title="Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures"></a>Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03659">http://arxiv.org/abs/2310.03659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thorsten Händler</li>
<li>for: 这篇论文是为了探讨自主的语言模型（LLM）动态抽象和协调框架，以便在复杂的多个任务中实现更好的AI功能。</li>
<li>methods: 这篇论文使用了多维度分类法来分析自主LLM多代理系统中的自动化和对齐平衡问题，并提供了一个领域ontology模型来 specify 基本的架构概念。</li>
<li>results: 这篇论文通过对一些代表性的LLM多代理系统的exploratory分类，ILLUSTRATE了其实际应用的实用性，并揭示了未来的研究和开发的潜在前景。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities. However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations. Autonomous LLM-powered multi-agent systems represent a strategic response to these challenges. Such systems strive for autonomously tackling user-prompted goals by decomposing them into manageable tasks and orchestrating their execution and result synthesis through a collective of specialized intelligent agents. Equipped with LLM-powered reasoning capabilities, these agents harness the cognitive synergy of collaborating with their peers, enhanced by leveraging contextual resources such as tools and datasets. While these architectures hold promising potential in amplifying AI capabilities, striking the right balance between different levels of autonomy and alignment remains the crucial challenge for their effective operation. This paper proposes a comprehensive multi-dimensional taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems balance the dynamic interplay between autonomy and alignment across various aspects inherent to architectural viewpoints such as goal-driven task management, agent composition, multi-agent collaboration, and context interaction. It also includes a domain-ontology model specifying fundamental architectural concepts. Our taxonomy aims to empower researchers, engineers, and AI practitioners to systematically analyze the architectural dynamics and balancing strategies employed by these increasingly prevalent AI systems. The exploratory taxonomic classification of selected representative LLM-powered multi-agent systems illustrates its practical utility and reveals potential for future research and development.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经革命化人工智能领域，具备了复杂语言理解和生成能力。然而，当面临更复杂和相互连接的任务时，LLM具有内在的限制。自主 LLM 驱动多代理系统是一种策略性应对这些挑战的回应。这些系统通过自动将用户提交的目标 decomposing 成可管理的任务，并通过一群特殊智能代理来进行执行和结果合成。这些代理通过 LLM 强化的理解能力，可以协同合作，通过利用上下文资源 such as 工具和数据集来提高合作效果。虽然这些体系具有潜在的扩展可能性，但是保持不同水平的自主和对齐是关键的挑战。这篇论文提出了一种多维度分类，用于分析自主 LLM 驱动多代理系统如何在不同的体系视角下平衡动态的自主和对齐。它还包括一个领域 ontology 模型，描述了基本体系概念。我们的分类旨在为研究者、工程师和 AI 实践者提供系统性分析自主 LLM 驱动多代理系统的建议和指导。选择代表 LLM 驱动多代理系统的exploratory分类表明了我们的分类的实用性，并揭示了未来研究和发展的潜在 potential。
</details></li>
</ul>
<hr>
<h2 id="HandMeThat-Human-Robot-Communication-in-Physical-and-Social-Environments"><a href="#HandMeThat-Human-Robot-Communication-in-Physical-and-Social-Environments" class="headerlink" title="HandMeThat: Human-Robot Communication in Physical and Social Environments"></a>HandMeThat: Human-Robot Communication in Physical and Social Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03779">http://arxiv.org/abs/2310.03779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanming Wan, Jiayuan Mao, Joshua B. Tenenbaum</li>
<li>for: 本研究是为了评估机器人理解和执行人类指令的全面评估标准。</li>
<li>methods: 本研究使用了人类行为轨迹、物理环境和社会各种各样的信息来评估机器人理解和执行人类指令的能力。</li>
<li>results: 研究发现现有的语言固定和规划方法在HandMeThat上表现不佳， suggesting significant room for future work on physical and social human-robot communications and interactions。<details>
<summary>Abstract</summary>
We introduce HandMeThat, a benchmark for a holistic evaluation of instruction understanding and following in physical and social environments. While previous datasets primarily focused on language grounding and planning, HandMeThat considers the resolution of human instructions with ambiguities based on the physical (object states and relations) and social (human actions and goals) information. HandMeThat contains 10,000 episodes of human-robot interactions. In each episode, the robot first observes a trajectory of human actions towards her internal goal. Next, the robot receives a human instruction and should take actions to accomplish the subgoal set through the instruction. In this paper, we present a textual interface for our benchmark, where the robot interacts with a virtual environment through textual commands. We evaluate several baseline models on HandMeThat, and show that both offline and online reinforcement learning algorithms perform poorly on HandMeThat, suggesting significant room for future work on physical and social human-robot communications and interactions.
</details>
<details>
<summary>摘要</summary>
我们介绍HandMeThat，一个标准套件用于评估人工智能机器人理解和遵循语言指令的全面评估。在过去的数据集中，大多集中在语言落实和规划上，而HandMeThat则考虑了人类指令的解释时的物理（物体状态和关系）和社交（人类行为和目标）信息。HandMeThat包含10,000集的人机互动纪录。在每个集中，机器人首先观察人类行为的轨迹，然后接收人类指令，并通过指令中的子目标来完成。在这篇文章中，我们提供了文本界面 для我们的标准套件，机器人通过文本命令与虚拟环境互动。我们评估了多个基eline模型在HandMeThat上，并发现了 both offline和线上循环学习算法在HandMeThat上表现不佳，这表明了未来人工智能机器人与人类沟通和互动的Physical和社交方面还有很大的潜力。
</details></li>
</ul>
<hr>
<h2 id="CLEVRER-Humans-Describing-Physical-and-Causal-Events-the-Human-Way"><a href="#CLEVRER-Humans-Describing-Physical-and-Causal-Events-the-Human-Way" class="headerlink" title="CLEVRER-Humans: Describing Physical and Causal Events the Human Way"></a>CLEVRER-Humans: Describing Physical and Causal Events the Human Way</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03635">http://arxiv.org/abs/2310.03635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Mao, Xuelin Yang, Xikun Zhang, Noah D. Goodman, Jiajun Wu</li>
<li>for: 本研究旨在建立机器可以理解物理事件和其 causal 关系，以便与物理世界进行灵活交互。</li>
<li>methods: 研究使用了两种技术来提高数据收集效率：一种是使用新的迭代事件cloze任务来生成视频中事件的新表示，称为 causal event graphs (CEGs)；另一种是基于神经语言生成模型的数据增强技术。</li>
<li>results: 研究提出了一个名为 CLEVRER-Humans 的视频理解数据集，用于评估物理事件的 causal 判断。研究还展示了一些基准方法的表现， highlighting 该 benchmark 对机器学习领域的挑战。<details>
<summary>Abstract</summary>
Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models. We convert the collected CEGs into questions and answers to be consistent with prior work. Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting the great challenges set forth by our benchmark.
</details>
<details>
<summary>摘要</summary>
建立机器可以理解物理事件和其 causal 关系是在互动性的世界中灵活交互的关键。然而，大多数现有的物理和 causal 逻辑标准是基于生成的事件和人工生成的自然语言描述 causal 关系。这种设计存在两个问题：首先，数据的多样性不够，其次， causal 关系基于人工定义的规则与人类判断不同。为了解决这两个缺陷，我们提出了 CLEVRER-Humans  benchmark，一个基于视频逻辑的 causal 判断数据集，它们由人类标签。我们采用了两种技术来提高数据收集效率：首先，一种新的迭代事件cloze任务，用于生成视频中事件的新表示，我们称之为 causal event graph (CEG)；其次，基于神经语言生成模型的数据增强技术。我们将收集的 CEG 转换成问题和答案，与先前的工作一致。最后，我们研究了 CLEVRER-Humans 问题回答的一些基准方法，并 highlighted 这些标准的巨大挑战。
</details></li>
</ul>
<hr>
<h2 id="PeaTMOSS-Mining-Pre-Trained-Models-in-Open-Source-Software"><a href="#PeaTMOSS-Mining-Pre-Trained-Models-in-Open-Source-Software" class="headerlink" title="PeaTMOSS: Mining Pre-Trained Models in Open-Source Software"></a>PeaTMOSS: Mining Pre-Trained Models in Open-Source Software</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03620">http://arxiv.org/abs/2310.03620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/purduedualitylab/peatmoss-demos">https://github.com/purduedualitylab/peatmoss-demos</a></li>
<li>paper_authors: Wenxin Jiang, Jason Jones, Jerin Yasmin, Nicholas Synovic, Rajeev Sashti, Sophie Chen, George K. Thiruvathukal, Yuan Tian, James C. Davis</li>
<li>for: 这个论文的目的是为了研究基于预训练深度学习模型（PTM）的软件工程做出一个大规模的数据集，以便更好地理解PTM在软件工程中的应用和挑战。</li>
<li>methods: 这篇论文使用了一个名为PeaTMOSS的数据集，该数据集包含281,638个预训练深度学习模型和27,270个开源软件项目，以及这些项目中PTM的使用情况。论文还提出了一个挑战，即通过分析PTM在软件工程中的使用情况，探索PTM在软件工程中的应用和挑战。</li>
<li>results: 论文提出了一个名为PeaTMOSS的数据集，该数据集包含大量的PTM和相关的软件项目信息，可供研究PTM在软件工程中的应用和挑战。此外，论文还提出了一个挑战，以便研究PTM在软件工程中的应用和挑战。<details>
<summary>Abstract</summary>
Developing and training deep learning models is expensive, so software engineers have begun to reuse pre-trained deep learning models (PTMs) and fine-tune them for downstream tasks. Despite the wide-spread use of PTMs, we know little about the corresponding software engineering behaviors and challenges.   To enable the study of software engineering with PTMs, we present the PeaTMOSS dataset: Pre-Trained Models in Open-Source Software. PeaTMOSS has three parts: a snapshot of (1) 281,638 PTMs, (2) 27,270 open-source software repositories that use PTMs, and (3) a mapping between PTMs and the projects that use them. We challenge PeaTMOSS miners to discover software engineering practices around PTMs. A demo and link to the full dataset are available at: https://github.com/PurdueDualityLab/PeaTMOSS-Demos.
</details>
<details>
<summary>摘要</summary>
开发和训练深度学习模型是昂贵的，因此软件工程师们开始 reuse pre-trained deep learning models (PTMs) 并对其进行精度调整用于下游任务。尽管PTMs 的使用广泛，但我们对相关的软件工程行为和挑战知之甚少。为了启用PTMs 的研究，我们提供了 PeaTMOSS 数据集：开源软件中的 Pre-Trained Models。PeaTMOSS 包括三部分：(1) 281,638 个 PTMs，(2) 27,270 个开源软件仓库使用 PTMs，以及 (3) PTMs 和这些项目之间的映射。我们挑战 PeaTMOSS 挖掘者发现PTMs 在软件工程中的实践。 demo 和数据集的链接可以在：https://github.com/PurdueDualityLab/PeaTMOSS-Demos 中找到。
</details></li>
</ul>
<hr>
<h2 id="Solving-a-Class-of-Non-Convex-Minimax-Optimization-in-Federated-Learning"><a href="#Solving-a-Class-of-Non-Convex-Minimax-Optimization-in-Federated-Learning" class="headerlink" title="Solving a Class of Non-Convex Minimax Optimization in Federated Learning"></a>Solving a Class of Non-Convex Minimax Optimization in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03613">http://arxiv.org/abs/2310.03613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xidong Wu, Jianhui Sun, Zhengmian Hu, Aidong Zhang, Heng Huang</li>
<li>for:  addressing large-scale data challenges in machine learning applications with communication-efficient distributed training</li>
<li>methods:  Federated Learning (FL) algorithms (FedSGDA+ and FedSGDA-M) and existing centralized optimization algorithms</li>
<li>results:  reduced communication complexity and improved sample complexity for nonconvex-concave and nonconvex-strongly-concave minimax problems, with the best-known sample complexity of $O(\kappa^{3} N^{-1}\varepsilon^{-3})$ and the best-known communication complexity of $O(\kappa^{2}\varepsilon^{-2})$<details>
<summary>Abstract</summary>
The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization. To address the large-scale data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity. Many optimization algorithms for minimax problems have been developed in the centralized setting (\emph{i.e.} single-machine). Nonetheless, the algorithm for minimax problems under FL is still underexplored. In this paper, we study a class of federated nonconvex minimax optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems. For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample complexity of $O(\kappa^{3} N^{-1}\varepsilon^{-3})$ and the best-known communication complexity of $O(\kappa^{2}\varepsilon^{-2})$. FedSGDA-M is the first algorithm to match the best sample complexity $O(\varepsilon^{-3})$ achieved by the single-machine method under the nonconvex-strongly-concave setting. Extensive experimental results on fair classification and AUROC maximization show the efficiency of our algorithms.
</details>
<details>
<summary>摘要</summary>
“小最大最小值问题在机器学习应用中随处出现，从对抗训练和奖励学习策略评估到AUROC最大化。为了 Addressing 大规模数据问题 across multiple clients with communication-efficient distributed training, federated learning (FL) 在 gaining popularity。许多中央化设置中的最小最大值问题优化算法已经被开发出来，但是在 FL  setting 中，这个问题还未得到充分研究。在这篇论文中，我们研究了一类联邦非凸最小最大值优化问题。我们提出了 FedSGDA+ 和 FedSGDA-M 算法，并将 существу的复杂性结果缩小到最常见的最小最大值问题中。对非凸-凹型问题，我们提出了 FedSGDA+，并将通信复杂性降至 $O(\varepsilon^{-6})$。在非凸-强凹和非凸-PL最小最大值设置中，我们证明了 FedSGDA-M 的样本复杂性为 $O(\kappa^{3} N^{-1}\varepsilon^{-3})$，并且通信复杂性为 $O(\kappa^{2}\varepsilon^{-2})$。FedSGDA-M 是第一个与单机器方法在非凸-强凹设置中匹配的样本复杂性 $O(\varepsilon^{-3})$。我们的实验结果表明，我们的算法在公平分类和 AUROC 最大化中具有高效性。”
</details></li>
</ul>
<hr>
<h2 id="FASER-Binary-Code-Similarity-Search-through-the-use-of-Intermediate-Representations"><a href="#FASER-Binary-Code-Similarity-Search-through-the-use-of-Intermediate-Representations" class="headerlink" title="FASER: Binary Code Similarity Search through the use of Intermediate Representations"></a>FASER: Binary Code Similarity Search through the use of Intermediate Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03605">http://arxiv.org/abs/2310.03605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/br0kej/FASER">https://github.com/br0kej/FASER</a></li>
<li>paper_authors: Josh Collyer, Tim Watson, Iain Phillips</li>
<li>for: 本研究旨在提高跨架构软件功能关注的能力，以便分析恶意软件、安全软件供应链和漏洞研究等领域。</li>
<li>methods: 本研究使用了 binary intermediate representations（ Intermediate Representations，IR）作为数据源，并提出了一种基于文档长Transformers的函数为字符串编码表示（FASER）模型，以实现跨架构函数搜索无需人工特征工程、预训练或动态分析步骤。</li>
<li>results:  compared to several baseline methods, the proposed FASER model demonstrates strong performance in both general function search and targeted vulnerability search tasks, outperforming all baseline approaches.<details>
<summary>Abstract</summary>
Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transformers with the use of intermediate representations to create a model capable of cross architecture function search without the need for manual feature engineering, pre-training or a dynamic analysis step. We compare our approach against a series of baseline approaches for two tasks; A general function search task and a targeted vulnerability search task. Our approach demonstrates strong performance across both tasks, performing better than all baseline approaches.
</details>
<details>
<summary>摘要</summary>
“能够识别感兴趣的函数在跨架构软件中是有用的，无论你是分析恶意软件、安全软件供应链或进行攻击性研究。跨架构软件Binary Code相似性搜寻已经在多个研究中探讨过，通常使用了各种不同的数据来源来实现目的。这些数据来源通常是根据binaries的常见结构，例如函数控制流图或二进制层级呼叫图、这些output的资料分析结果或动态分析的结果。然而，一个较少受到注意的数据来源是二进制中继表示。二进制中继表示具有两个有趣的性能：它们是跨架构的本性，且可以明确地表示函数的 semantics，以便在下游使用。在这篇文章中，我们提出了Function as a String Encoded Representation（FASER），它结合了长文本转换器和中继表示，创建了可以在跨架构上进行函数搜寻，不需要手动的特性工程、预训练或动态分析步骤。我们与一些基eline方法进行比较，在两个任务上显示了强大的表现，比基eline方法更好。”
</details></li>
</ul>
<hr>
<h2 id="How-toxic-is-antisemitism-Potentials-and-limitations-of-automated-toxicity-scoring-for-antisemitic-online-content"><a href="#How-toxic-is-antisemitism-Potentials-and-limitations-of-automated-toxicity-scoring-for-antisemitic-online-content" class="headerlink" title="How toxic is antisemitism? Potentials and limitations of automated toxicity scoring for antisemitic online content"></a>How toxic is antisemitism? Potentials and limitations of automated toxicity scoring for antisemitic online content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04465">http://arxiv.org/abs/2310.04465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helena Mihaljević, Elisabeth Steffen<br>for: 这个论文探讨了Google和Jigsaw开发的Perspective API在探测仇视性言语中的潜力和局限性，特别是在内容审核、监测和社交媒体研究等领域。methods: 作者使用了一个手动标注的德语 dataset，包括来自 Telegram 和 Twitter 的约 3,600 条帖子，以探索恶意言语是如何被评估为恶势力，以及不同形式的反犹太主义和文本表达的偏好是如何影响评分。results: 作者发现，Perspective API 在基本水平上能够识别恶意言语，但对非显式的反犹太主义和批判反犹太主义的文本表达存在重大的局限性。此外，作者还发现，通过使用广泛的反犹太主义代码，可以减少 API 分数，从而轻松绕过基于该服务的内容审核。<details>
<summary>Abstract</summary>
The Perspective API, a popular text toxicity assessment service by Google and Jigsaw, has found wide adoption in several application areas, notably content moderation, monitoring, and social media research. We examine its potentials and limitations for the detection of antisemitic online content that, by definition, falls under the toxicity umbrella term. Using a manually annotated German-language dataset comprising around 3,600 posts from Telegram and Twitter, we explore as how toxic antisemitic texts are rated and how the toxicity scores differ regarding different subforms of antisemitism and the stance expressed in the texts. We show that, on a basic level, Perspective API recognizes antisemitic content as toxic, but shows critical weaknesses with respect to non-explicit forms of antisemitism and texts taking a critical stance towards it. Furthermore, using simple text manipulations, we demonstrate that the use of widespread antisemitic codes can substantially reduce API scores, making it rather easy to bypass content moderation based on the service's results.
</details>
<details>
<summary>摘要</summary>
Google和Jigsaw的Perspective API，一个流行的文本恶意评估服务，在多个应用领域得到了广泛的采用，主要包括内容审核、监测和社交媒体研究。我们研究了它在检测仇Semite在线内容的潜力和局限性。使用一个手动注释的德语 dataset，包括 Telegram 和 Twitter 上约 3,600 个帖子，我们探索了恶意文本是如何被评分，以及不同形式的反Semite和文本表达的立场如何影响了评分。我们发现，在基本水平上，Perspective API 能够识别反Semite内容为恶意，但对于不直接表达的反Semite和批判反Semite的文本表达存在重要的限制。此外，我们使用简单的文本修改示例，示出了使用广泛的反Semite代码可以减少 API 分数，使得内容审核基于服务的结果相对较容易被逃脱。
</details></li>
</ul>
<hr>
<h2 id="Resilient-Legged-Local-Navigation-Learning-to-Traverse-with-Compromised-Perception-End-to-End"><a href="#Resilient-Legged-Local-Navigation-Learning-to-Traverse-with-Compromised-Perception-End-to-End" class="headerlink" title="Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End"></a>Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03581">http://arxiv.org/abs/2310.03581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Jin, Chong Zhang, Jonas Frey, Nikita Rudin, Matias Mattamala, Cesar Cadena, Marco Hutter</li>
<li>for: 本研究旨在帮助自主机器人在未知环境中快速准确 Navigation，即使感知受到干扰或错误。</li>
<li>methods: 本文使用再归折 learning（RL）基于本地决策策略，通过在潜在空间重建环境信息，以便在感知失败时进行有效应对。</li>
<li>results: 在模拟和实际四足机器人 ANYmal 上，本策略在面临感知失败时成功率高于30%，与传统基于规则的本地响应策略相比。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures. Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization. In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot. Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end. To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions. We validate our approach in simulation and on the real quadruped robot ANYmal running in real-time (<10 ms CPU inference). In a quantitative comparison with existing heuristic-based locally reactive planners, our policy increases the success rate over 30% when facing perception failures. Project Page: https://bit.ly/45NBTuh.
</details>
<details>
<summary>摘要</summary>
（简化中文）自主机器人需要在未知环境中 Navigation 可靠，即使感知受到了损害。这些损害通常发生在具有劣化感知的环境中，或者感知算法错误地理解场景。在这篇论文中，我们将感知失败模型为隐藏的障碍物和坑，并使用强化学习（RL）基于的本地导航政策来引导我们的四肢机器人。与前一些基于规则和异常检测来更新导航信息的方法不同，我们的导航政策可以在损害感知中重建环境信息，并在端到端进行反应。为此，我们将 proprioception 和 exteroception 作为政策输入，以便政策可以感受到不同的身体部分和坑，并且进行相应的反应。我们在模拟和真实的四肢机器人 ANYmal 上进行了实时（<10 ms CPU 推理）验证，并与现有的启发式本地反应计划进行了量化比较。在面临感知失败情况下，我们的政策的成功率高于30%。项目页面：https://bit.ly/45NBTuh.
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-in-Gene-Regulatory-Networks-with-GFlowNet-Towards-Scalability-in-Large-Systems"><a href="#Causal-Inference-in-Gene-Regulatory-Networks-with-GFlowNet-Towards-Scalability-in-Large-Systems" class="headerlink" title="Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems"></a>Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03579">http://arxiv.org/abs/2310.03579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trang Nguyen, Alexander Tong, Kanika Madan, Yoshua Bengio, Dianbo Liu</li>
<li>for: 该研究旨在提高生物学GRNs中 causal structure learning的效能，并解决了可扩展性问题。</li>
<li>methods: 该研究提出了 Swift-DynGFN 框架，具有 gene-wise independence 特点，可以提高并行化和计算成本下降。</li>
<li>results: 实验表明，Swift-DynGFN 可以在实验室单细胞 RNA 速度数据和 sintetic GRN 数据上快速学习 causal structure，并在更大的系统中保持可扩展性。<details>
<summary>Abstract</summary>
Understanding causal relationships within Gene Regulatory Networks (GRNs) is essential for unraveling the gene interactions in cellular processes. However, causal discovery in GRNs is a challenging problem for multiple reasons including the existence of cyclic feedback loops and uncertainty that yields diverse possible causal structures. Previous works in this area either ignore cyclic dynamics (assume acyclic structure) or struggle with scalability. We introduce Swift-DynGFN as a novel framework that enhances causal structure learning in GRNs while addressing scalability concerns. Specifically, Swift-DynGFN exploits gene-wise independence to boost parallelization and to lower computational cost. Experiments on real single-cell RNA velocity and synthetic GRN datasets showcase the advancement in learning causal structure in GRNs and scalability in larger systems.
</details>
<details>
<summary>摘要</summary>
理解生物细胞中的生物化学网络（GRNs）内部的 causal 关系是解释细胞过程中 gene 之间的交互的关键。然而，在 GRNs 中发现 causal 关系是一项复杂的问题，因为存在循环反馈征和不确定性，导致可能出现多种 causal 结构。先前的工作往往忽略循环动态（假设无环结构）或者缺乏可扩展性。我们介绍 Swift-DynGFN 框架，该框架可以提高 GRNs 中 causal 结构学习的效果，同时解决可扩展性问题。具体来说，Swift-DynGFN 利用了每个基因独立性来提高并行化和降低计算成本。实验表明，Swift-DynGFN 在真实的单元细胞 RNA 速度数据和 sintetic GRN 数据上可以有效地提高 GRNs 中 causal 结构的学习和可扩展性。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Dynamic-Programming-for-Energy-Efficient-Base-Station-Cell-Switching"><a href="#Adaptive-Dynamic-Programming-for-Energy-Efficient-Base-Station-Cell-Switching" class="headerlink" title="Adaptive Dynamic Programming for Energy-Efficient Base Station Cell Switching"></a>Adaptive Dynamic Programming for Energy-Efficient Base Station Cell Switching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12999">http://arxiv.org/abs/2310.12999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junliang Luo, Yi Tian Xu, Di Wu, Michael Jenkin, Xue Liu, Gregory Dudek</li>
<li>for: 提高无线网络的能效性，适应新一代无线网络的发展需求，环境和政策因素，以及可能的能源危机。</li>
<li>methods: 使用准确动态 програм理论（ADP）和在线优化，根据状态动作对各个基站Cells进行开关，以减少网络功率消耗，保持足够的服务质量指标（QoS）。</li>
<li>results: 使用多层感知器（MLP）和长期快速储存（LSTM）预测功率和QoS，并在在线优化算法中采用自适应QoS阈值来筛选基站Cells的 switching 动作，以实现最大化功率减少而不妨碍QoS。<details>
<summary>Abstract</summary>
Energy saving in wireless networks is growing in importance due to increasing demand for evolving new-gen cellular networks, environmental and regulatory concerns, and potential energy crises arising from geopolitical tensions. In this work, we propose an approximate dynamic programming (ADP)-based method coupled with online optimization to switch on/off the cells of base stations to reduce network power consumption while maintaining adequate Quality of Service (QoS) metrics. We use a multilayer perceptron (MLP) given each state-action pair to predict the power consumption to approximate the value function in ADP for selecting the action with optimal expected power saved. To save the largest possible power consumption without deteriorating QoS, we include another MLP to predict QoS and a long short-term memory (LSTM) for predicting handovers, incorporated into an online optimization algorithm producing an adaptive QoS threshold for filtering cell switching actions based on the overall QoS history. The performance of the method is evaluated using a practical network simulator with various real-world scenarios with dynamic traffic patterns.
</details>
<details>
<summary>摘要</summary>
“无线网络的能源救减在日益增长的重要性中，因为新一代无线网络的需求不断增长，环境和 regulatory 因素，以及可能的能源危机问题，导致由地opolitical 紧张关系。在这个工作中，我们提出一个基于推对 Dynamic Programming（ADP）的方法，与在线服务器进行优化，以降低网络电力消耗，同时保持足够的服务质量（QoS）指标。我们使用每个状态-行动 pairs 的多层感知神经网络（MLP）来预测电力消耗，以近似值函数在 ADP 中选择最佳的行动。为了储存最大化的电力消耗，而不损害 QoS，我们另外使用一个 MLP 预测 QoS，并与一个长期传递内存（LSTM）进行估计，将其组合入线上优化算法，生成适应的 QoS 阈值，以根据网络的 QoS 历史进行筛选网络转换动作。这个方法的表现被评估使用实际的网络 simulator，以及不同的实际情况，包括动态的流量模式。”
</details></li>
</ul>
<hr>
<h2 id="Lightweight-Boosting-Models-for-User-Response-Prediction-Using-Adversarial-Validation"><a href="#Lightweight-Boosting-Models-for-User-Response-Prediction-Using-Adversarial-Validation" class="headerlink" title="Lightweight Boosting Models for User Response Prediction Using Adversarial Validation"></a>Lightweight Boosting Models for User Response Prediction Using Adversarial Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03778">http://arxiv.org/abs/2310.03778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeonwoo Kim, Wonsung Lee</li>
<li>for: 预测应用程序安装概率</li>
<li>methods: 使用对抗验证、特征工程和 Gradient Boosted Decision Trees (GBDT) 实现轻量级解决方案</li>
<li>results: 在 ACM RecSys Challenge 2023 中，我们的方法取得了最终排名第九的成绩，得分为 6.059065。<details>
<summary>Abstract</summary>
The ACM RecSys Challenge 2023, organized by ShareChat, aims to predict the probability of the app being installed. This paper describes the lightweight solution to this challenge. We formulate the task as a user response prediction task. For rapid prototyping for the task, we propose a lightweight solution including the following steps: 1) using adversarial validation, we effectively eliminate uninformative features from a dataset; 2) to address noisy continuous features and categorical features with a large number of unique values, we employ feature engineering techniques.; 3) we leverage Gradient Boosted Decision Trees (GBDT) for their exceptional performance and scalability. The experiments show that a single LightGBM model, without additional ensembling, performs quite well. Our team achieved ninth place in the challenge with the final leaderboard score of 6.059065. Code for our approach can be found here: https://github.com/choco9966/recsys-challenge-2023.
</details>
<details>
<summary>摘要</summary>
“ACM RecSys Challenge 2023”，由 ShareChat 主办，旨在预测应用程序安装概率。这篇文章描述了一种轻量级解决方案。我们将任务定型为用户响应预测任务。为了快速原型，我们提议以下步骤：1. 使用对抗验证，有效地从数据集中消除无用的特征。2. 对噪音连续特征和 categorical 特征（具有大量唯一值）使用Feature工程技术。3. 利用 Gradient Boosted Decision Trees（GBDT）的异常表现和可扩展性。实验显示，一个单独的 LightGBM 模型（没有额外 ensemble）表现非常好。我们在挑战中获得第九名，最终排名为 6.059065。我们的代码可以在以下地址找到：https://github.com/choco9966/recsys-challenge-2023。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-and-Generalizable-Training-An-Empirical-Study-of-Noisy-Slot-Filling-for-Input-Perturbations"><a href="#Towards-Robust-and-Generalizable-Training-An-Empirical-Study-of-Noisy-Slot-Filling-for-Input-Perturbations" class="headerlink" title="Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations"></a>Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03518">http://arxiv.org/abs/2310.03518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachi Liu, Liwen Wang, Guanting Dong, Xiaoshuai Song, Zechen Wang, Zhengyang Wang, Shanglin Lei, Jinzheng Zhao, Keqing He, Bo Xiao, Weiran Xu</li>
<li>for: 这篇论文主要研究了语音识别领域中的噪声稳定性评价方法。</li>
<li>methods: 该论文提出了一个名为Noise-SF的噪声稳定性评价 datasets，该dataset包含了五种人工标注的噪声类型，并且这些噪声类型都是实际中广泛使用的噪声稳定性训练方法中的一部分。</li>
<li>results: 经过对Noise-SF dataset的广泛实验测试，基eline模型在噪声稳定性评价中表现不佳，而提出的框架则能够有效地提高模型的噪声稳定性。根据实验结果，我们提出了一些前瞻性的建议，以促进噪声稳定性研究的发展。<details>
<summary>Abstract</summary>
In real dialogue scenarios, as there are unknown input noises in the utterances, existing supervised slot filling models often perform poorly in practical applications. Even though there are some studies on noise-robust models, these works are only evaluated on rule-based synthetic datasets, which is limiting, making it difficult to promote the research of noise-robust methods. In this paper, we introduce a noise robustness evaluation dataset named Noise-SF for slot filling task. The proposed dataset contains five types of human-annotated noise, and all those noises are exactly existed in real extensive robust-training methods of slot filling into the proposed framework. By conducting exhaustive empirical evaluation experiments on Noise-SF, we find that baseline models have poor performance in robustness evaluation, and the proposed framework can effectively improve the robustness of models. Based on the empirical experimental results, we make some forward-looking suggestions to fuel the research in this direction. Our dataset Noise-SF will be released at https://github.com/dongguanting/Noise-SF.
</details>
<details>
<summary>摘要</summary>
在实际对话场景中，由于输入噪声的存在，现有的超级vised插槽填充模型在实际应用中经常表现不佳。尽管有一些关于噪声Robustness的研究，但这些研究仅在基于规则生成的 sintetic 数据上进行评估，这限制了研究的发展。在这篇论文中，我们介绍了一个噪声Robustness评估集合名为Noise-SF，该集合包含了五种人类标注的噪声，这些噪声都是现实中广泛采用的Robust-training方法中的噪声。经过了广泛的实验研究，我们发现基eline模型在Robustness评估中表现不佳，而我们提出的框架可以有效提高模型的Robustness。基于实验结果，我们提出了一些前瞻的建议，以推动这一方向的研究。我们的Noise-SF数据集将在 GitHub 上发布，链接为：https://github.com/dongguanting/Noise-SF。
</details></li>
</ul>
<hr>
<h2 id="How-the-level-sampling-process-impacts-zero-shot-generalisation-in-deep-reinforcement-learning"><a href="#How-the-level-sampling-process-impacts-zero-shot-generalisation-in-deep-reinforcement-learning" class="headerlink" title="How the level sampling process impacts zero-shot generalisation in deep reinforcement learning"></a>How the level sampling process impacts zero-shot generalisation in deep reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03494">http://arxiv.org/abs/2310.03494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Garcin, James Doran, Shangmin Guo, Christopher G. Lucas, Stefano V. Albrecht</li>
<li>for: 本研究旨在解释deep reinforcement learning（RL）训练出的自适应代理人能否在新环境中展现良好的扩展性。</li>
<li>methods: 我们使用非均匀采样策略来测试RL代理人的零例扩展性（ZSG），包括两种失败模式：过拟合和过总结。我们首先测量RL代理人内部表征和训练级别之间的相互信息（MI），并发现非均匀采样策略可以更好地保持低MI，这提供了一种新的理论依据。然后，我们转移到无监督环境设计（UED）方法，这些方法可以在运行时动态生成新的训练级别，并尽可能减少MI。然而，我们发现UED方法会导致训练分布的显著变化，从而导致过总结和worse ZSG性能。</li>
<li>results: 我们引入自适应环境设计（SSED）方法，SSED使用变量自动编码器来生成级别，从而减少MI并最小化与目标分布的偏移。SSED方法与固定集合级别采样策略和UED方法相比，导致了统计学上显著的ZSG性能改善。<details>
<summary>Abstract</summary>
A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (UED) methods, which adaptively generate new training levels and minimise MI more effectively than methods sampling from a fixed set. However, we find UED methods significantly shift the training distribution, resulting in over-generalisation and worse ZSG performance over the distribution of interest. To prevent both instance overfitting and over-generalisation, we introduce self-supervised environment design (SSED). SSED generates levels using a variational autoencoder, effectively reducing MI while minimising the shift with the distribution of interest, and leads to statistically significant improvements in ZSG over fixed-set level sampling strategies and UED methods.
</details>
<details>
<summary>摘要</summary>
深度强化学习训练的自主Agent有一个关键的局限性，即其在新环境中的泛化能力较差，即使新环境与训练环境有相似特征。在这项工作中，我们研究了非均匀抽样策略对深度强化学习Agent的零例泛化能力（ZSG）的影响，包括两种失败模式：过拟合和过泛化。作为第一步，我们测量了Agent内部表示和训练环境集之间的相互信息（MI），发现它与实例过拟合高度相关。与均匀抽样不同，适应抽样策略，根据预测值损失来决定抽样级别，能够保持更低的MI，提供了一种新的理论依据。然后我们转向无监督环境设计（UED）方法，这些方法可以在运行时动态生成新的训练级别，并最效地减少MI。然而，我们发现UED方法会导致训练分布的显著变化，从而导致过泛化和较差的ZSG性能。为了避免实例过拟合和过泛化，我们介绍了无监督环境设计（SSED）。SSED使用变量自动编码器生成级别，并能够减少MI，同时尽可能减少与目标分布的偏移，从而导致了统计学上的改进。
</details></li>
</ul>
<hr>
<h2 id="Tik-to-Tok-Translating-Language-Models-One-Token-at-a-Time-An-Embedding-Initialization-Strategy-for-Efficient-Language-Adaptation"><a href="#Tik-to-Tok-Translating-Language-Models-One-Token-at-a-Time-An-Embedding-Initialization-Strategy-for-Efficient-Language-Adaptation" class="headerlink" title="Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation"></a>Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03477">http://arxiv.org/abs/2310.03477</a></li>
<li>repo_url: None</li>
<li>paper_authors: François Remy, Pieter Delobelle, Bettina Berendt, Kris Demuynck, Thomas Demeester</li>
<li>for:  addresses the challenge of training monolingual language models for low and mid-resource languages</li>
<li>methods:  uses a novel model conversion strategy that adapts high-resource monolingual language models to a new target language</li>
<li>results:  achieves a new state-of-the-art performance on mid- and low-resource languages, and reduces significantly the amount of data and time required for training state-of-the-art models.<details>
<summary>Abstract</summary>
Training monolingual language models for low and mid-resource languages is made challenging by limited and often inadequate pretraining data. In this study, we propose a novel model conversion strategy to address this issue, adapting high-resources monolingual language models to a new target language. By generalizing over a word translation dictionary encompassing both the source and target languages, we map tokens from the target tokenizer to semantically similar tokens from the source language tokenizer. This one-to-many token mapping improves tremendously the initialization of the embedding table for the target language. We conduct experiments to convert high-resource models to mid- and low-resource languages, namely Dutch and Frisian. These converted models achieve a new state-of-the-art performance on these languages across all sorts of downstream tasks. By reducing significantly the amount of data and time required for training state-of-the-art models, our novel model conversion strategy has the potential to benefit many languages worldwide.
</details>
<details>
<summary>摘要</summary>
training monolingual language models for low and mid-resource languages is challenging due to limited and inadequate pretraining data. in this study, we propose a novel model conversion strategy to address this issue, adapting high-resource monolingual language models to a new target language. by generalizing over a word translation dictionary encompassing both the source and target languages, we map tokens from the target tokenizer to semantically similar tokens from the source language tokenizer. this one-to-many token mapping improves the initialization of the embedding table for the target language. we conduct experiments to convert high-resource models to mid- and low-resource languages, namely dutch and frisian. these converted models achieve a new state-of-the-art performance on these languages across all sorts of downstream tasks. by significantly reducing the amount of data and time required for training state-of-the-art models, our novel model conversion strategy has the potential to benefit many languages worldwide.
</details></li>
</ul>
<hr>
<h2 id="Diffusing-on-Two-Levels-and-Optimizing-for-Multiple-Properties-A-Novel-Approach-to-Generating-Molecules-with-Desirable-Properties"><a href="#Diffusing-on-Two-Levels-and-Optimizing-for-Multiple-Properties-A-Novel-Approach-to-Generating-Molecules-with-Desirable-Properties" class="headerlink" title="Diffusing on Two Levels and Optimizing for Multiple Properties: A Novel Approach to Generating Molecules with Desirable Properties"></a>Diffusing on Two Levels and Optimizing for Multiple Properties: A Novel Approach to Generating Molecules with Desirable Properties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04463">http://arxiv.org/abs/2310.04463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Guo, Jihong Guan, Shuigeng Zhou</li>
<li>for: 本研究旨在提出一种新的分子生成方法，以提高现有模型的分子生成效果。</li>
<li>methods: 本研究使用了扩展的扩散模型框架，并提出了多种创新的设计方法。在这些设计方法中，我们首次在分子生成过程中使用了电子效应基于的分子 фрагмента化方法。此外，我们还引入了多个目标函数来同时优化多个分子性质。</li>
<li>results: 对于两个参考数据集QM9和ZINC250k，我们的提议方法可以生成比现状态最佳的分子，其中包括有效性、独特性、新颖性、Fréchet ChemNet距离（FCD）、QED和PlogP等多个分子性质。<details>
<summary>Abstract</summary>
In the past decade, Artificial Intelligence driven drug design and discovery has been a hot research topic, where an important branch is molecule generation by generative models, from GAN-based models and VAE-based models to the latest diffusion-based models. However, most existing models pursue only the basic properties like validity and uniqueness of the generated molecules, a few go further to explicitly optimize one single important molecular property (e.g. QED or PlogP), which makes most generated molecules little usefulness in practice. In this paper, we present a novel approach to generating molecules with desirable properties, which expands the diffusion model framework with multiple innovative designs. The novelty is two-fold. On the one hand, considering that the structures of molecules are complex and diverse, and molecular properties are usually determined by some substructures (e.g. pharmacophores), we propose to perform diffusion on two structural levels: molecules and molecular fragments respectively, with which a mixed Gaussian distribution is obtained for the reverse diffusion process. To get desirable molecular fragments, we develop a novel electronic effect based fragmentation method. On the other hand, we introduce two ways to explicitly optimize multiple molecular properties under the diffusion model framework. First, as potential drug molecules must be chemically valid, we optimize molecular validity by an energy-guidance function. Second, since potential drug molecules should be desirable in various properties, we employ a multi-objective mechanism to optimize multiple molecular properties simultaneously. Extensive experiments with two benchmark datasets QM9 and ZINC250k show that the molecules generated by our proposed method have better validity, uniqueness, novelty, Fr\'echet ChemNet Distance (FCD), QED, and PlogP than those generated by current SOTA models.
</details>
<details>
<summary>摘要</summary>
在过去的一个 décennie，人工智能驱动的药物设计和发现已经是研究热点，其中一个重要分支是通过生成模型生成分子，从GAN基于模型和VAE基于模型到最新的扩散模型。然而，大多数现有模型只追求基本属性，如有效性和uniqueness的生成分子，很少进一步optimize一个重要的分子性质（例如QED或PlogP），这使得大多数生成的分子在实际应用中有 Limited usefulness。在这篇论文中，我们提出了一种新的分子生成方法，拓展了扩散模型框架，并通过多种创新的设计来提高生成分子的性能。 novelties twofold。一方面，由于分子结构复杂多样，而分子性质通常由一些子结构（例如药理学残基）决定，我们提议在分子和分子段级别进行扩散，从而获得一个混合 Gaussian 分布，用于逆扩散过程。另一方面，我们开发了一种新的电子效应基于的分子段化方法，以获得更好的分子段。此外，我们引入了两种方法来直接在扩散模型框架下进行多个分子性质的优化。首先，以实际的药物分子必须有化学有效性为前提，我们使用能量引导函数来优化分子有效性。其次，由于实际的药物分子应该具有多种性质，我们采用多目标机制来同时优化多个分子性质。在两个标准测试集QM9和ZINC250k上进行了广泛的实验，发现生成的分子具有更高的有效性、uniqueness、新鲜度、Fréchet ChemNet Distance (FCD)、QED和PlogP等性质，而与当前最佳模型相比，具有更高的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Quantitatively-Interpretable-Model-for-Alzheimer’s-Disease-Prediction-Using-Deep-Counterfactuals"><a href="#A-Quantitatively-Interpretable-Model-for-Alzheimer’s-Disease-Prediction-Using-Deep-Counterfactuals" class="headerlink" title="A Quantitatively Interpretable Model for Alzheimer’s Disease Prediction Using Deep Counterfactuals"></a>A Quantitatively Interpretable Model for Alzheimer’s Disease Prediction Using Deep Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03457">http://arxiv.org/abs/2310.03457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwanseok Oh, Da-Woon Heo, Ahmad Wisnu Mulyadi, Wonsik Jung, Eunsong Kang, Kun Ho Lee, Heung-Il Suk</li>
<li>For: The paper aims to provide a more interpretable and effective approach for predicting Alzheimer’s disease (AD) using counterfactual reasoning and gray matter density maps.* Methods: The paper proposes a framework that synthesizes counterfactual-labeled structural MRIs, transforms them into gray matter density maps, and uses a lightweight linear classifier to boost predictive performance and provide quantitative interpretation.* Results: The paper demonstrates that the proposed framework can produce an “AD-relatedness index” for each region of interest (ROI) and offer an intuitive understanding of brain status for individuals and patient groups with respect to AD progression, with comparable predictive performance to deep learning methods.Here is the same information in Simplified Chinese text:</li>
<li>for: 这项研究旨在通过对比逻辑和灰 mater 激光扫描图像来提供更加可解释的和有效的阿尔茨哈默病 (AD) 预测方法。</li>
<li>methods: 这项研究提议一种框架，该框架可以将对比逻辑标注的结构MRIs转换为灰 mater 激光扫描图像，并使用轻量级线性分类器来提高预测性能和提供量化解释。</li>
<li>results: 这项研究表明，提议的框架可以生成每个区域兴趣 (ROI) 的 “AD相关性指数”，并为每个个体和患者群提供有关 AD 进程的直观理解，与深度学习方法相比具有相同的预测性能。<details>
<summary>Abstract</summary>
Deep learning (DL) for predicting Alzheimer's disease (AD) has provided timely intervention in disease progression yet still demands attentive interpretability to explain how their DL models make definitive decisions. Recently, counterfactual reasoning has gained increasing attention in medical research because of its ability to provide a refined visual explanatory map. However, such visual explanatory maps based on visual inspection alone are insufficient unless we intuitively demonstrate their medical or neuroscientific validity via quantitative features. In this study, we synthesize the counterfactual-labeled structural MRIs using our proposed framework and transform it into a gray matter density map to measure its volumetric changes over the parcellated region of interest (ROI). We also devised a lightweight linear classifier to boost the effectiveness of constructed ROIs, promoted quantitative interpretation, and achieved comparable predictive performance to DL methods. Throughout this, our framework produces an ``AD-relatedness index'' for each ROI and offers an intuitive understanding of brain status for an individual patient and across patient groups with respect to AD progression.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）用于预测阿尔茨曼尼尔病（AD）已经提供了及时的 intervención，但仍需要注意的解释性来解释它们的DL模型如何做出定义性的决策。最近，对比因果逻辑得到了医学研究中的越来越多的注意，因为它可以提供一个精细的视觉解释地图。然而，基于视觉检查 alone的视觉解释地图是无效的，除非我们能够INTRODUCE其医学或神经科学的VALIDITY通过量化特征。在这种研究中，我们将Counterfactual-labeled的结构MRI使用我们的提案的框架进行合成，并将其转换成灰色物质浓度地图，以测量ROI中的体积变化。我们还开发了一种轻量级的线性分类器，以提高构建的ROIs的效果，促进量化解释，并实现与DL方法相同的预测性能。通过这种方式，我们的框架生成了每个ROI的“AD相关性指数”，并提供了对个人患者和patient group的AD进程状况的直观理解。
</details></li>
</ul>
<hr>
<h2 id="Pre-Training-and-Fine-Tuning-Generative-Flow-Networks"><a href="#Pre-Training-and-Fine-Tuning-Generative-Flow-Networks" class="headerlink" title="Pre-Training and Fine-Tuning Generative Flow Networks"></a>Pre-Training and Fine-Tuning Generative Flow Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03419">http://arxiv.org/abs/2310.03419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ling Pan, Moksh Jain, Kanika Madan, Yoshua Bengio</li>
<li>for: 这个论文的目的是探索如何使用 reward-free pre-training 方法来快速适应下游任务，并且可以快速发现更多的模式。</li>
<li>methods: 这个论文使用了 Generative Flow Networks (GFlowNets) 作为探索 compositional objects 的方法，并通过自我监督的方式来训练 GFlowNets。</li>
<li>results: 实验结果表明，使用 reward-free pre-training 方法可以快速适应下游任务，并且可以快速发现更多的模式。此外，这种方法还可以在不知道下游任务的情况下进行预训练，从而提高下游任务的效果。<details>
<summary>Abstract</summary>
Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which is an important consideration in scientific discovery tasks. However, as they are typically trained from a given extrinsic reward function, it remains an important open challenge about how to leverage the power of pre-training and train GFlowNets in an unsupervised fashion for efficient adaptation to downstream tasks. Inspired by recent successes of unsupervised pre-training in various domains, we introduce a novel approach for reward-free pre-training of GFlowNets. By framing the training as a self-supervised problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space. Specifically, OC-GFN learns to reach any targeted outcomes, akin to goal-conditioned policies in reinforcement learning. We show that the pre-trained OC-GFN model can allow for a direct extraction of a policy capable of sampling from any new reward functions in downstream tasks. Nonetheless, adapting OC-GFN on a downstream task-specific reward involves an intractable marginalization over possible outcomes. We propose a novel way to approximate this marginalization by learning an amortized predictor enabling efficient fine-tuning. Extensive experimental results validate the efficacy of our approach, demonstrating the effectiveness of pre-training the OC-GFN, and its ability to swiftly adapt to downstream tasks and discover modes more efficiently. This work may serve as a foundation for further exploration of pre-training strategies in the context of GFlowNets.
</details>
<details>
<summary>摘要</summary>
generate 生成 Flow Networks (GFlowNets) 是束缚 samplers ，它们学习随机政策来从给定的非正态奖励分布中顺序生成组合性 объек。它们可以生成多个高奖对象，这是科学发现任务中的重要考虑因素。然而，由于它们通常从给定的外部奖励函数进行训练，因此如何利用预训练的力量并在下游任务中训练 GFlowNets 是一个重要的开放挑战。 inspirited by recent successes of unsupervised pre-training in various domains, we introduce a novel approach for reward-free pre-training of GFlowNets.  By framing the training as a self-supervised problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space. Specifically, OC-GFN learns to reach any targeted outcomes, akin to goal-conditioned policies in reinforcement learning. We show that the pre-trained OC-GFN model can allow for a direct extraction of a policy capable of sampling from any new reward functions in downstream tasks. However, adapting OC-GFN on a downstream task-specific reward involves an intractable marginalization over possible outcomes. We propose a novel way to approximate this marginalization by learning an amortized predictor enabling efficient fine-tuning. Extensive experimental results validate the efficacy of our approach, demonstrating the effectiveness of pre-training the OC-GFN, and its ability to swiftly adapt to downstream tasks and discover modes more efficiently. This work may serve as a foundation for further exploration of pre-training strategies in the context of GFlowNets.
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-for-Medical-Image-Analysis-A-Survey"><a href="#Domain-Generalization-for-Medical-Image-Analysis-A-Survey" class="headerlink" title="Domain Generalization for Medical Image Analysis: A Survey"></a>Domain Generalization for Medical Image Analysis: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08598">http://arxiv.org/abs/2310.08598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jee Seok Yoon, Kwanseok Oh, Yooseung Shin, Maciej A. Mazurowski, Heung-Il Suk</li>
<li>for: 这篇论文旨在探讨医疗影像分析（MedIA）中深度学习（DL）的应用，以及DL模型在真实世界中的应用问题。</li>
<li>methods: 这篇论文评论了医疗影像分析领域内的领域整合研究，包括数据水平、特征水平、模型水平和分析水平的方法。</li>
<li>results: 这篇论文提供了医疗影像分析领域内执行预测和分析时的绩效，以及不同领域整合方法的优缺点，并揭露未来研究的机遇。<details>
<summary>Abstract</summary>
Medical Image Analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, DL models for MedIA remain challenging to deploy in real-world situations, failing for generalization under the distributional gap between training and testing samples, known as a distribution shift problem. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution data distributions. This paper comprehensively reviews domain generalization studies specifically tailored for MedIA. We provide a holistic view of how domain generalization techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize domain generalization methods into data-level, feature-level, model-level, and analysis-level methods. We show how those methods can be used in various stages of the MedIA workflow with DL equipped from data acquisition to model prediction and analysis. Furthermore, we include benchmark datasets and applications used to evaluate these approaches and analyze the strengths and weaknesses of various methods, unveiling future research opportunities.
</details>
<details>
<summary>摘要</summary>
医疗图像分析（MedIA）已成为医学和医疗领域的重要工具，帮助诊断疾病、预测疾病趋势和制定治疗计划，而最近的深度学习（DL）技术的发展也为其带来了 significiant 改进。然而，DL模型在实际应用中仍然面临 distribuional shift 问题，即训练和测试样本的分布不同，导致模型的泛化问题。研究人员对此做出了努力，开发了多种 DL 方法来适应和在未知和非标准数据分布下表现稳定。本文对医疗图像分析领域的域合理化研究进行了全面的回顾，不仅涵盖了不同的 DL 方法，还考虑了这些方法在整个 MedIA 工作流程中的操作影响。特别是，我们将域合理化方法分为数据级、特征级、模型级和分析级方法，并详细介绍了这些方法在不同阶段的 MedIA 工作流程中的应用。此外，我们还提供了一些标准的数据集和应用，用于评估这些方法的效果，并分析了各种方法的优缺点，探讨未来的研究机遇。
</details></li>
</ul>
<hr>
<h2 id="GRAPES-Learning-to-Sample-Graphs-for-Scalable-Graph-Neural-Networks"><a href="#GRAPES-Learning-to-Sample-Graphs-for-Scalable-Graph-Neural-Networks" class="headerlink" title="GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks"></a>GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03399">http://arxiv.org/abs/2310.03399</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dfdazac/grapes">https://github.com/dfdazac/grapes</a></li>
<li>paper_authors: Taraneh Younesian, Thiviyan Thanapalasingam, Emile van Krieken, Daniel Daza, Peter Bloem</li>
<li>for: 这篇论文的目的是提出一种适应性 Graph Sampling 方法，以便在各种结构和任务下减少 GNN 的内存开销。</li>
<li>methods: 这篇论文使用了 GFlowNet 来学习节点抽样概率，以达到在 GNN 分类器训练中 Identify 影响节点的目的。</li>
<li>results: 在多个小规模和大规模图标准 bencmark 上，GRAPES 被证明能够维持高准确率，同时具有扩展性和可扩展性。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) learn the representation of nodes in a graph by aggregating the neighborhood information in various ways. As these networks grow in depth, their receptive field grows exponentially due to the increase in neighborhood sizes, resulting in high memory costs. Graph sampling solves memory issues in GNNs by sampling a small ratio of the nodes in the graph. This way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed sampling heuristics, which may not generalize to different structures or tasks. We introduce GRAPES, an adaptive graph sampling method that learns to identify sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet to learn node sampling probabilities given the classification objectives. We evaluate GRAPES across several small- and large-scale graph benchmarks and demonstrate its effectiveness in accuracy and scalability. In contrast to existing sampling methods, GRAPES maintains high accuracy even with small sample sizes and, therefore, can scale to very large graphs. Our code is publicly available at https://github.com/dfdazac/grapes.
</details>
<details>
<summary>摘要</summary>
GRAPES是一种适应性 Graph sampling 方法，用于在深度层次的 Graph Neural Networks（GNNs）中解决内存问题。GRAPES通过学习选择训练 GNN 分类器的影响节点集来采样graph。我们使用 GFlowNet 学习节点采样概率，基于分类目标。我们在多个小规模和大规模图 benchmark 中评估 GRAPES，并证明其精度和可扩展性。与现有的采样方法不同，GRAPES可以在小样本大小下保持高精度，因此可以扩展到非常大的图。我们的代码公开在 GitHub 上：https://github.com/dfdazac/grapes。
</details></li>
</ul>
<hr>
<h2 id="Unpacking-Human-AI-Interaction-in-Safety-Critical-Industries-A-Systematic-Literature-Review"><a href="#Unpacking-Human-AI-Interaction-in-Safety-Critical-Industries-A-Systematic-Literature-Review" class="headerlink" title="Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review"></a>Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03392">http://arxiv.org/abs/2310.03392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tita A. Bach, Jenny K. Kristiansen, Aleksandar Babic, Alon Jacovi</li>
<li>for: 本研究的目的是探讨人工智能与人互动（HAII）在安全关键行业中的实现，以提高这些行业的安全性和可靠性。</li>
<li>methods: 本研究采用文献综述方法，检查当前HAII领域的研究，并提出了在这些领域中进行研究的最佳实践。</li>
<li>results: 本研究发现HAII领域的研究 Fragmented and inconsistent，存在多种不同的术语和定义，并且HAII的评价方法多样化。研究还发现了HAII的五大影响因素，即用户特征和背景（如用户人性和观察）、人工智能界面和功能（如交互UI设计）、人工智能输出（如准确性和操作建议）、可解释性和可 interpretability（如级别和用户理解）、以及人工智能的使用（如多样化环境和用户需求）。<details>
<summary>Abstract</summary>
Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, what little research there is on HAII is fragmented and inconsistent. We present here a survey of that literature and recommendations for research best practices that will improve the field. We divided our investigation into the following research areas: (1) terms used to describe HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII, and (4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, five factors influence HAII: user characteristics and background (e.g., user personality, perceptions), AI interface and features (e.g., interactive UI design), AI output (e.g., accuracy, actionable recommendations), explainability and interpretability (e.g., level of detail, user understanding), and usage of AI (e.g., heterogeneity of environments and user needs). HAII is most commonly measured with user-related subjective metrics (e.g., user perception, trust, and attitudes), and AI-assisted decision-making is the most common primary role of AI-enabled systems. Based on this review, we conclude that there are substantial research gaps in HAII. Researchers and developers need to codify HAII terminology, involve users throughout the AI lifecycle (especially during development), and tailor HAII in safety-critical industries to the users and environments.
</details>
<details>
<summary>摘要</summary>
Ensuring high-quality human-AI interaction (HAII) in safety-critical industries is crucial. Failure to do so can lead to disastrous and deadly consequences. Despite the urgency, the current research on HAII is fragmented and inconsistent. We conducted a survey of the literature and provided recommendations for research best practices that can improve the field. Our investigation focused on the following areas:1. Terms used to describe HAII2. Primary roles of AI-enabled systems3. Factors that influence HAII4. How HAII is measuredWe found that there is no single term used across the literature to describe HAII, and some terms have multiple meanings. According to our literature review, five factors influence HAII: user characteristics and background (e.g., user personality, perceptions), AI interface and features (e.g., interactive UI design), AI output (e.g., accuracy, actionable recommendations), explainability and interpretability (e.g., level of detail, user understanding), and usage of AI (e.g., heterogeneity of environments and user needs). HAII is most commonly measured with user-related subjective metrics (e.g., user perception, trust, and attitudes), and AI-assisted decision-making is the most common primary role of AI-enabled systems.Based on our review, we conclude that there are substantial research gaps in HAII. Researchers and developers need to codify HAII terminology, involve users throughout the AI lifecycle (especially during development), and tailor HAII in safety-critical industries to the users and environments.
</details></li>
</ul>
<hr>
<h2 id="Procedural-Text-Mining-with-Large-Language-Models"><a href="#Procedural-Text-Mining-with-Large-Language-Models" class="headerlink" title="Procedural Text Mining with Large Language Models"></a>Procedural Text Mining with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03376">http://arxiv.org/abs/2310.03376</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jd-coderepos/proc-tm">https://github.com/jd-coderepos/proc-tm</a></li>
<li>paper_authors: Anisa Rula, Jennifer D’Souza</li>
<li>for: 本研究探讨了使用大型自然语言处理（NLP）模型在零例学习和Context-Aware学习环境中提取PDF文档中的过程，以问题回答的方式进行不间断提取。</li>
<li>methods: 本研究使用了当前领先的GPT-4（生成准备 transformer 4）模型，并采用了基于ontology的定义和少量示例学习的两种Context-Aware学习方法。</li>
<li>results: 研究发现，这些 modification 有能力有效地地址深度学习基于NLP的过程提取技术中的数据收集困难，并且表明了这些自定义的Context-Aware学习方法在提取过程中的承诺。<details>
<summary>Abstract</summary>
Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction.
</details>
<details>
<summary>摘要</summary>
最近在自然语言处理领域的进步，特别是大规模语言模型的开发，在知识工程中创造了新的机会。在这篇论文中，我们 investigate了使用大型语言模型（LLMs）在零shot学习和在context学习Setting中解决抽取PDF文本中的过程的问题。我们利用了当前状态的GPT-4（生成预训练变换器4）模型，并使用过程和步骤的ontology和少量的几个示例学习。研究发现，这种方法和context学习定制具有普遍提高过程抽取的批处和可行性。这些修改可以减少深度学习基于自然语言处理技术中的训练数据获得困难。
</details></li>
</ul>
<hr>
<h2 id="Design-Optimizer-for-Planar-Soft-Growing-Robot-Manipulators"><a href="#Design-Optimizer-for-Planar-Soft-Growing-Robot-Manipulators" class="headerlink" title="Design Optimizer for Planar Soft-Growing Robot Manipulators"></a>Design Optimizer for Planar Soft-Growing Robot Manipulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03374">http://arxiv.org/abs/2310.03374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Stroppa</li>
<li>For: The paper is written for designing and optimizing soft-growing robots for specific manipulation tasks, such as exploration of delicate&#x2F;dangerous environments, manipulation of items, or assistance in domestic environments.* Methods: The paper presents a novel approach for design optimization of soft-growing robots, which involves modeling the design process as a multi-objective optimization problem and using population-based optimization algorithms, specifically evolutionary algorithms, to transform the problem into a single-objective problem. The method also incorporates a novel rank-partitioning algorithm and obstacle avoidance within the optimizer operators.* Results: The proposed method is tested on different tasks and shows significant performance in solving the problem, outperforming existing methods in terms of precision, resource consumption, and run time.<details>
<summary>Abstract</summary>
Soft-growing robots are innovative devices that feature plant-inspired growth to navigate environments. Thanks to their embodied intelligence of adapting to their surroundings and the latest innovation in actuation and manufacturing, it is possible to employ them for specific manipulation tasks. The applications of these devices include exploration of delicate/dangerous environments, manipulation of items, or assistance in domestic environments.   This work presents a novel approach for design optimization of soft-growing robots, which will be used prior to manufacturing to suggest engineers -- or robot designer enthusiasts -- the optimal dimension of the robot to be built for solving a specific task. I modeled the design process as a multi-objective optimization problem, in which I optimize the kinematic chain of a soft manipulator to reach targets and avoid unnecessary overuse of material and resources. The method exploits the advantages of population-based optimization algorithms, in particular evolutionary algorithms, to transform the problem from multi-objective into a single-objective thanks to an efficient mathematical formulation, the novel rank-partitioning algorithm, and obstacle avoidance integrated within the optimizer operators.   I tested the proposed method on different tasks to access its optimality, which showed significant performance in solving the problem. Finally, comparative experiments showed that the proposed method works better than the one existing in the literature in terms of precision, resource consumption, and run time.
</details>
<details>
<summary>摘要</summary>
软性增长机器人是一种创新性的设备，它们借鉴植物的生长机理来适应环境。由于它们的内置智能和最新的活动和制造技术，因此可以用于特定的操作任务。这些设备的应用包括探索敏感/危险环境、物品操作和家庭环境中的协助。本工作提出了一种新的软性增长机器人设计优化方法，该方法将在制造之前使用以确定最佳的机器人尺寸，以解决特定任务。我模型了设计过程为多目标优化问题，并且优化软 manipulate器的骨骼来达到目标并避免不必要的材料和资源的浪费。该方法利用了人口基于优化算法的优势，尤其是进化算法，将问题转化为单目标问题，并且通过rank-partitioning算法和避免障碍的内置算法来提高效率。我对不同任务进行了测试，以评估其优化性，结果显示了显著的性能提升。最后，对比性测试表明，提出的方法与现有文献中的方法相比，在精度、资源消耗和运行时间上具有更好的性能。
</details></li>
</ul>
<hr>
<h2 id="AI-based-automated-active-learning-for-discovery-of-hidden-dynamic-processes-A-use-case-in-light-microscopy"><a href="#AI-based-automated-active-learning-for-discovery-of-hidden-dynamic-processes-A-use-case-in-light-microscopy" class="headerlink" title="AI-based automated active learning for discovery of hidden dynamic processes: A use case in light microscopy"></a>AI-based automated active learning for discovery of hidden dynamic processes: A use case in light microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04461">http://arxiv.org/abs/2310.04461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nils Friederich, Angelo Yamachui Sitcheu, Oliver Neumann, Süheyla Eroğlu-Kayıkçı, Roshan Prizak, Lennart Hilbert, Ralf Mikut</li>
<li>for: 本研究旨在提出两种新方法，用于提高生物医学实验中的动态过程观测效率。</li>
<li>methods: 一种基于人工智能的方法（Encoded Dynamic Process，EDP），可以从单个静止图像中预测动态过程的时间值。另一种是基于机器学习操作（MLOps）的实验自动化管道（Experiment Automation Pipeline for Dynamic Processes，EAPDP），使用EDP提取的知识来有效地安排实验。</li>
<li>results: 在一个实验中，我们示出了使用预训练的State-Of-The-Art（SOTA）对象分割网络（Contour Proposal Networks，CPN）作为EAPDP模块，可以有效地提取动态过程中相关的对象。<details>
<summary>Abstract</summary>
In the biomedical environment, experiments assessing dynamic processes are primarily performed by a human acquisition supervisor. Contemporary implementations of such experiments frequently aim to acquire a maximum number of relevant events from sometimes several hundred parallel, non-synchronous processes. Since in some high-throughput experiments, only one or a few instances of a given process can be observed simultaneously, a strategy for planning and executing an efficient acquisition paradigm is essential. To address this problem, we present two new methods in this paper. The first method, Encoded Dynamic Process (EDP), is Artificial Intelligence (AI)-based and represents dynamic processes so as to allow prediction of pseudo-time values from single still images. Second, with Experiment Automation Pipeline for Dynamic Processes (EAPDP), we present a Machine Learning Operations (MLOps)-based pipeline that uses the extracted knowledge from EDP to efficiently schedule acquisition in biomedical experiments for dynamic processes in practice. In a first experiment, we show that the pre-trained State-Of-The- Art (SOTA) object segmentation method Contour Proposal Networks (CPN) works reliably as a module of EAPDP to extract the relevant object for EDP from the acquired three-dimensional image stack.
</details>
<details>
<summary>摘要</summary>
在生物医学环境中，动态过程的实验通常由人工监控员进行。现代实验技术常采用多个并发、异步过程来获取最大数量的相关事件。由于一些高通过put实验中只能同时观察一些过程的一个或几个实例，因此制定有效的招待和执行策略是非常重要。为解决这个问题，本文提出了两种新方法。首先，我们提出了编码动态过程（EDP）方法，该方法基于人工智能（AI），可以从单张停止图像中预测动态过程中的pseudo-时间值。其次，我们提出了实验自动化管道 для动态过程（EAPDP），该管道基于机器学习操作（MLOps），使用EDP提取的知识来有效地调度获取在生物医学实验中的动态过程。在一个实验中，我们证明了在EAPDP中使用预训练的状态 искусственный智能（SOTA）对象分割方法Contour Proposal Networks（CPN）可靠地作为EAPDP中EXTRACT对象的模块来提取获取的三维图像堆中相关的对象。
</details></li>
</ul>
<hr>
<h2 id="Swin-Tempo-Temporal-Aware-Lung-Nodule-Detection-in-CT-Scans-as-Video-Sequences-Using-Swin-Transformer-Enhanced-UNet"><a href="#Swin-Tempo-Temporal-Aware-Lung-Nodule-Detection-in-CT-Scans-as-Video-Sequences-Using-Swin-Transformer-Enhanced-UNet" class="headerlink" title="Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet"></a>Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03365">http://arxiv.org/abs/2310.03365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Jafari, Karim Faez, Hamidreza Amindavar</li>
<li>for: 这个研究的目的是提高Computer-aided diagnosis (CAD)系统的精度，以便更好地识别lung nodules from computed tomography (CT) scans。</li>
<li>methods: 这个研究使用了一个新的模型，它结合了卷积神经网和感知器transformer的优点，将每个3D CT影像视为一个影像序列，并将肺肿瘤视为影像中的物体，以进行时间序列应用。</li>
<li>results: 这个研究使用了10-fold cross-validation技术来验证提案的网络，得到了97.84%的感度标准和96.0%的竞赛性能指标（CPM），并与现有的肺肿瘤识别技术进行比较，显示了这个提案的优秀精度。<details>
<summary>Abstract</summary>
Lung cancer is highly lethal, emphasizing the critical need for early detection. However, identifying lung nodules poses significant challenges for radiologists, who rely heavily on their expertise for accurate diagnosis. To address this issue, computer-aided diagnosis (CAD) systems based on machine learning techniques have emerged to assist doctors in identifying lung nodules from computed tomography (CT) scans. Unfortunately, existing networks in this domain often suffer from computational complexity, leading to high rates of false negatives and false positives, limiting their effectiveness. To address these challenges, we present an innovative model that harnesses the strengths of both convolutional neural networks and vision transformers. Inspired by object detection in videos, we treat each 3D CT image as a video, individual slices as frames, and lung nodules as objects, enabling a time-series application. The primary objective of our work is to overcome hardware limitations during model training, allowing for efficient processing of 2D data while utilizing inter-slice information for accurate identification based on 3D image context. We validated the proposed network by applying a 10-fold cross-validation technique to the publicly available Lung Nodule Analysis 2016 dataset. Our proposed architecture achieves an average sensitivity criterion of 97.84% and a competition performance metrics (CPM) of 96.0% with few parameters. Comparative analysis with state-of-the-art advancements in lung nodule identification demonstrates the significant accuracy achieved by our proposed model.
</details>
<details>
<summary>摘要</summary>
肺癌是高度致命的，强调了早期发现的急迫性。然而，识别肺节圆柱体呈难度很大的问题， radiologist仰赖自己的专业技巧进行精准诊断。为解决这个问题，基于机器学习技术的计算机辅助诊断（CAD）系统在肺节圆柱体CT扫描图像中进行帮助。然而，现有网络在这个领域经常受到计算复杂性的限制，导致高false negative和false positive率，限制其效iveness。为了解决这些挑战，我们提出了一种创新的模型，利用了 convolutional neural networks和vision transformers的优势。受到 object detection in videos 的启发，我们将每个3D CT图像视为视频，每个slice为帧，并将肺节圆柱体视为物体，使得时序应用。我们的主要目标是在训练模型时缓解硬件限制，以便高效处理2D数据，同时利用3D图像上下文信息进行准确识别。我们采用了10-fold cross-validation技术来验证我们的提议的网络。我们的提议的架构实现了97.84%的敏感指标和96.0%的竞赛性能指标（CPM），同时具有少量参数。与当前肺节圆柱体识别领域的状态代表性进行比较分析，显示了我们的提议模型的显著精准性。
</details></li>
</ul>
<hr>
<h2 id="Robust-Representation-Learning-via-Asymmetric-Negative-Contrast-and-Reverse-Attention"><a href="#Robust-Representation-Learning-via-Asymmetric-Negative-Contrast-and-Reverse-Attention" class="headerlink" title="Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention"></a>Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03358">http://arxiv.org/abs/2310.03358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changzhang777/ancra">https://github.com/changzhang777/ancra</a></li>
<li>paper_authors: Nuoyan Zhou, Decheng Liu, Dawei Zhou, Xinbo Gao, Nannan Wang</li>
<li>for: 提高深度神经网络的鲁棒性，增强神经网络免受攻击的能力。</li>
<li>methods: 提出一种Generic Framework of Adversarial Training (AT)，通过偏置对应的负样本和反注意力来获得鲁棒表示。</li>
<li>results: 经验证明，我们的方法可以大幅提高AT中的鲁棒性，并实现状态级表现。<details>
<summary>Abstract</summary>
Deep neural networks are vulnerable to adversarial noise. Adversarial training (AT) has been demonstrated to be the most effective defense strategy to protect neural networks from being fooled. However, we find AT omits to learning robust features, resulting in poor performance of adversarial robustness. To address this issue, we highlight two characteristics of robust representation: (1) $\bf{exclusion}$: the feature of natural examples keeps away from that of other classes; (2) $\bf{alignment}$: the feature of natural and corresponding adversarial examples is close to each other. These motivate us to propose a generic framework of AT to gain robust representation, by the asymmetric negative contrast and reverse attention. Specifically, we design an asymmetric negative contrast based on predicted probabilities, to push away examples of different classes in the feature space. Moreover, we propose to weight feature by parameters of the linear classifier as the reverse attention, to obtain class-aware feature and pull close the feature of the same class. Empirical evaluations on three benchmark datasets show our methods greatly advance the robustness of AT and achieve state-of-the-art performance. Code is available at <https://github.com/changzhang777/ANCRA>.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度神经网络容易受到敌意噪声的攻击。对抗训练（AT）已经被证明是保护神经网络不被欺骗的最有效的防御策略。然而，我们发现AT不学习强健特征，导致对抗Example的性能不佳。为解决这问题，我们强调了两个特征的稳健表示：（1）隔离：自然示例在特征空间与其他类的示例保持距离;（2）对齐：自然示例和相应的敌意示例在特征空间之间的距离很近。这两个特征激励我们提出一种通用的AT框架，通过倒推对比和反注意力来获得稳健表示。具体来说，我们设计了基于预测概率的倒推对比，以推动不同类型的示例在特征空间中分离。此外，我们提议使用分类器参数来Weight特征，以实现类归一类的特征和同类示例之间的减距。我们对三个标准测试集进行实验，结果显示我们的方法可以大幅提高AT的稳健性和性能，并达到当前领先水平。代码可以在<https://github.com/changzhang777/ANCRA>上找到。
</details></li>
</ul>
<hr>
<h2 id="Fictitious-Cross-Play-Learning-Global-Nash-Equilibrium-in-Mixed-Cooperative-Competitive-Games"><a href="#Fictitious-Cross-Play-Learning-Global-Nash-Equilibrium-in-Mixed-Cooperative-Competitive-Games" class="headerlink" title="Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games"></a>Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03354">http://arxiv.org/abs/2310.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zelai Xu, Yancheng Liang, Chao Yu, Yu Wang, Yi Wu</li>
<li>for: 这种paper是为了解决竞争游戏中的多代理人学习问题，特别是在混合合作竞争游戏中，where agents on the same team need to cooperate with each other。</li>
<li>methods: 这种paper使用了自适应（SP）和策略空间响应器（PSRO）两种方法，并将它们结合在一起以实现更好的性能。</li>
<li>results: 这种paper的实验结果表明，FXP算法可以在矩阵游戏和格子世界Domain中击败基准模型，并在一个更复杂的足球游戏中获得94%的赢利率。<details>
<summary>Abstract</summary>
Self-play (SP) is a popular multi-agent reinforcement learning (MARL) framework for solving competitive games, where each agent optimizes policy by treating others as part of the environment. Despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium (NE) with high probability. Alternatively, Policy-Space Response Oracles (PSRO) is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard to scale to complex games. In this work, we develop a novel algorithm, Fictitious Cross-Play (FXP), which inherits the benefits from both frameworks. FXP simultaneously trains an SP-based main policy and a counter population of best response policies. The main policy is trained by fictitious self-play and cross-play against the counter population, while the counter policies are trained as the best responses to the main policy's past versions. We validate our method in matrix games and show that FXP converges to global NEs while SP methods fail. We also conduct experiments in a gridworld domain, where FXP achieves higher Elo ratings and lower exploitabilities than baselines, and a more challenging football game, where FXP defeats SOTA models with over 94% win rate.
</details>
<details>
<summary>摘要</summary>
自适应（SP）是一种流行的多代理人学习（MARL）框架，用于解决竞争性游戏，每个代理人都通过对别人的行为来优化策略。 despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium（NE）with high probability. Alternatively, Policy-Space Response Oracles（PSRO）is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard to scale to complex games. In this work, we develop a novel algorithm, Fictitious Cross-Play（FXP）, which inherits the benefits from both frameworks. FXP simultaneously trains an SP-based main policy and a counter population of best response policies. The main policy is trained by fictitious self-play and cross-play against the counter population, while the counter policies are trained as the best responses to the main policy's past versions. We validate our method in matrix games and show that FXP converges to global NEs while SP methods fail. We also conduct experiments in a gridworld domain, where FXP achieves higher Elo ratings and lower exploitabilities than baselines, and a more challenging football game, where FXP defeats SOTA models with over 94% win rate.
</details></li>
</ul>
<hr>
<h2 id="Parking-Spot-Classification-based-on-surround-view-camera-system"><a href="#Parking-Spot-Classification-based-on-surround-view-camera-system" class="headerlink" title="Parking Spot Classification based on surround view camera system"></a>Parking Spot Classification based on surround view camera system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12997">http://arxiv.org/abs/2310.12997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andy Xiao, Deep Doshi, Lihao Wang, Harsha Gorantla, Thomas Heitzmann, Peter Groth</li>
<li>for: 本研究旨在掌握半自动驾驶场景中的自动停车空位探测和分类，以提高自动停车的精度和效率。</li>
<li>methods: 本研究使用了围绕式鱼眼摄像头系统，并采用了基于物体检测的YOLOv4神经网络，以及一种新的多边形 bounding box 模型，以适应不同的停车空位形状。</li>
<li>results: 研究结果表明，我们提出的停车空位分类方法可以有效地分类不同类型的停车空位，包括普通停车空位、电动车停车空位和残疾人停车空位。<details>
<summary>Abstract</summary>
Surround-view fisheye cameras are commonly used for near-field sensing in automated driving scenarios, including urban driving and auto valet parking. Four fisheye cameras, one on each side, are sufficient to cover 360{\deg} around the vehicle capturing the entire near-field region. Based on surround view cameras, there has been much research on parking slot detection with main focus on the occupancy status in recent years, but little work on whether the free slot is compatible with the mission of the ego vehicle or not. For instance, some spots are handicap or electric vehicles accessible only. In this paper, we tackle parking spot classification based on the surround view camera system. We adapt the object detection neural network YOLOv4 with a novel polygon bounding box model that is well-suited for various shaped parking spaces, such as slanted parking slots. To the best of our knowledge, we present the first detailed study on parking spot detection and classification on fisheye cameras for auto valet parking scenarios. The results prove that our proposed classification approach is effective to distinguish between regular, electric vehicle, and handicap parking spots.
</details>
<details>
<summary>摘要</summary>
围绕式鱼眼摄像机在自动驾驶场景中广泛使用，包括城市驾驶和自动停车。四个鱼眼摄像机，一个在每侧，可以覆盖360度周围车辆，捕捉整个近场区域。基于围绕视频摄像机，近年来有很多研究关于停车位置检测，主要关注车辆在停车位置的占用状态。然而，很少人研究停车位置是否适合egos车辆的任务。例如，一些停车位置只能由人们或电动车辆使用。在这篇论文中，我们解决了基于围绕视频摄像机系统的停车位置类型分类。我们适应了对象检测神经网络YOLOv4的一种新的多边框架模型，这种模型适合各种形状的停车位置，如斜停车位置。到目前为止，我们的分类方法是对围绕视频摄像机系统中的停车位置进行详细研究的第一个详细研究。结果表明，我们的分类方法可以有效地将正常停车位置、电动车辆停车位置和残疾人停车位置分开。
</details></li>
</ul>
<hr>
<h2 id="Deep-Geometric-Learning-with-Monotonicity-Constraints-for-Alzheimer’s-Disease-Progression"><a href="#Deep-Geometric-Learning-with-Monotonicity-Constraints-for-Alzheimer’s-Disease-Progression" class="headerlink" title="Deep Geometric Learning with Monotonicity Constraints for Alzheimer’s Disease Progression"></a>Deep Geometric Learning with Monotonicity Constraints for Alzheimer’s Disease Progression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03353">http://arxiv.org/abs/2310.03353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungwoo Jeong, Wonsik Jung, Junghyo Sohn, Heung-Il Suk</li>
<li>for: 预测阿尔茨海默病（AD）的进程，以便诊断和治疗。</li>
<li>methods: 使用结构MRI数据模型AD进程，包括时间变化、不完整观测和时间几何特征。</li>
<li>results: 提出了一种新的几何学学习方法，结合了 topological space shift、ODE-RGRU 和 trajectory estimation，可以模型长期数据序列。该方法还包括一种训练算法，将 manifold mapping 与 monotonicity constraints 结合以反映测量过程的不可逆转换。通过预测临床标签和认知分数， validate 了我们的提议方法的有效性。<details>
<summary>Abstract</summary>
Alzheimer's disease (AD) is a devastating neurodegenerative condition that precedes progressive and irreversible dementia; thus, predicting its progression over time is vital for clinical diagnosis and treatment. Numerous studies have implemented structural magnetic resonance imaging (MRI) to model AD progression, focusing on three integral aspects: (i) temporal variability, (ii) incomplete observations, and (iii) temporal geometric characteristics. However, deep learning-based approaches regarding data variability and sparsity have yet to consider inherent geometrical properties sufficiently. The ordinary differential equation-based geometric modeling method (ODE-RGRU) has recently emerged as a promising strategy for modeling time-series data by intertwining a recurrent neural network and an ODE in Riemannian space. Despite its achievements, ODE-RGRU encounters limitations when extrapolating positive definite symmetric metrics from incomplete samples, leading to feature reverse occurrences that are particularly problematic, especially within the clinical facet. Therefore, this study proposes a novel geometric learning approach that models longitudinal MRI biomarkers and cognitive scores by combining three modules: topological space shift, ODE-RGRU, and trajectory estimation. We have also developed a training algorithm that integrates manifold mapping with monotonicity constraints to reflect measurement transition irreversibility. We verify our proposed method's efficacy by predicting clinical labels and cognitive scores over time in regular and irregular settings. Furthermore, we thoroughly analyze our proposed framework through an ablation study.
</details>
<details>
<summary>摘要</summary>
阿尔茨heimer病（AD）是一种毁灭性神经退化疾病，其前进程是不可逆的，因此预测其进程的演变是诊断和治疗中非常重要。许多研究已经使用结构Magnetic Resonance Imaging（MRI）来模型AD的进程，关注三个重要方面：（i）时间变化，（ii）部分观测，（iii）时间几何特征。然而，深度学习基于数据变化和稀缺的方法尚未充分考虑内在的几何特征。recently, an ordinary differential equation-based geometric modeling method（ODE-RGRU）has emerged as a promising strategy for modeling time-series data by intertwining a recurrent neural network and an ODE in Riemannian space. despite its achievements, ODE-RGRU encounters limitations when extrapolating positive definite symmetric metrics from incomplete samples, leading to feature reverse occurrences that are particularly problematic, especially within the clinical facet. therefore, this study proposes a novel geometric learning approach that models longitudinal MRI biomarkers and cognitive scores by combining three modules: topological space shift, ODE-RGRU, and trajectory estimation. we have also developed a training algorithm that integrates manifold mapping with monotonicity constraints to reflect measurement transition irreversibility. we verify our proposed method's efficacy by predicting clinical labels and cognitive scores over time in regular and irregular settings. furthermore, we thoroughly analyze our proposed framework through an ablation study.
</details></li>
</ul>
<hr>
<h2 id="Tractable-Bounding-of-Counterfactual-Queries-by-Knowledge-Compilation"><a href="#Tractable-Bounding-of-Counterfactual-Queries-by-Knowledge-Compilation" class="headerlink" title="Tractable Bounding of Counterfactual Queries by Knowledge Compilation"></a>Tractable Bounding of Counterfactual Queries by Knowledge Compilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03352">http://arxiv.org/abs/2310.03352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idsia/credici">https://github.com/idsia/credici</a></li>
<li>paper_authors: David Huber, Yizuo Chen, Alessandro Antonucci, Adnan Darwiche, Marco Zaffalon</li>
<li>for: 本文研究了在pearlian结构 causal模型中绑定部分可识别查询（counterfactuals）的问题。</li>
<li>methods: 本文使用了一种新的迭代EM算法来获得这些绑定的上限，该算法通过采样初始化参数来实现。该方法需要多个（Bayesian网络）查询，这些查询共享同一个结构方程和概率分布，但每个查询有不同的外生参数。因此，编译下来的Circuit结构有利于执行多个查询，从而实现了一定的计算减速。</li>
<li>results: 作者们实验表明，使用symbolic知识编译可以快速地计算绑定，并且可以实现一个训练 bayesian network inference的速度减速。<details>
<summary>Abstract</summary>
We discuss the problem of bounding partially identifiable queries, such as counterfactuals, in Pearlian structural causal models. A recently proposed iterated EM scheme yields an inner approximation of those bounds by sampling the initialisation parameters. Such a method requires multiple (Bayesian network) queries over models sharing the same structural equations and topology, but different exogenous probabilities. This setup makes a compilation of the underlying model to an arithmetic circuit advantageous, thus inducing a sizeable inferential speed-up. We show how a single symbolic knowledge compilation allows us to obtain the circuit structure with symbolic parameters to be replaced by their actual values when computing the different queries. We also discuss parallelisation techniques to further speed up the bound computation. Experiments against standard Bayesian network inference show clear computational advantages with up to an order of magnitude of speed-up.
</details>
<details>
<summary>摘要</summary>
我们讨论 partially identifiable queries的问题，例如 counterfactuals，在 Pearlian 结构 causal models 中。一种最近提出的迭代 EM 方法可以获得这些约束的内部approximation，通过 sampling 初始化参数。这种方法需要多个（Bayesian network）查询，这些查询共享同一个结构方程和结构，但每个查询有不同的外生概率。这种设置使得 compiling 下面的模型到一个算术Circuit 有利可图，从而induces 一个明显的推理速度增加。我们示出了一种单symbolic knowledge compilation可以获得这些circuit structure 的符号参数，并将其替换为实际值当计算不同的查询。我们还讨论了并行技术，以进一步加速约束计算。对标准 Bayesian network inference 进行实验，我们发现了一个许多的计算优势，速度增加达一个数量级。
</details></li>
</ul>
<hr>
<h2 id="Tuning-In-to-Neural-Encoding-Linking-Human-Brain-and-Artificial-Supervised-Representations-of-Language"><a href="#Tuning-In-to-Neural-Encoding-Linking-Human-Brain-and-Artificial-Supervised-Representations-of-Language" class="headerlink" title="Tuning In to Neural Encoding: Linking Human Brain and Artificial Supervised Representations of Language"></a>Tuning In to Neural Encoding: Linking Human Brain and Artificial Supervised Representations of Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04460">http://arxiv.org/abs/2310.04460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyuan Sun, Xiaohan Zhang, Marie-Francine Moens</li>
<li>for:  investigate how task tuning influences a pretained Transformer for neural encoding and which tasks lead to the best encoding performances.</li>
<li>methods:  generate supervised representations on eight Natural Language Understanding (NLU) tasks using prompt-tuning, a technique that is seldom explored in neural encoding for language.</li>
<li>results:  demonstrate that prompt-tuning yields representations that better predict neural responses to Chinese stimuli than traditional fine-tuning on four tasks, and discover that tasks that require a fine-grained processing of concepts and entities lead to representations that are most predictive of brain activation patterns.<details>
<summary>Abstract</summary>
To understand the algorithm that supports the human brain's language representation, previous research has attempted to predict neural responses to linguistic stimuli using embeddings generated by artificial neural networks (ANNs), a process known as neural encoding. However, most of these studies have focused on probing neural representations of Germanic languages, such as English, with unsupervised ANNs. In this paper, we propose to bridge the gap between human brain and supervised ANN representations of the Chinese language. Specifically, we investigate how task tuning influences a pretained Transformer for neural encoding and which tasks lead to the best encoding performances. We generate supervised representations on eight Natural Language Understanding (NLU) tasks using prompt-tuning, a technique that is seldom explored in neural encoding for language. We demonstrate that prompt-tuning yields representations that better predict neural responses to Chinese stimuli than traditional fine-tuning on four tasks. Furthermore, we discover that tasks that require a fine-grained processing of concepts and entities lead to representations that are most predictive of brain activation patterns. Additionally, we reveal that the proportion of tuned parameters highly influences the neural encoding performance of fine-tuned models. Overall, our experimental findings could help us better understand the relationship between supervised artificial and brain language representations.
</details>
<details>
<summary>摘要</summary>
以前的研究曾尝试使用人工神经网络（ANNs）生成的编码来预测大脑对语言刺激的神经响应，但大多数这些研究都集中在探索德语族语言，如英语。在这篇论文中，我们提议将人类大脑和有监督的ANN语言表示之间的关系 bridged。特别是，我们研究了一种任务调整对预先训练的 transformer 语言编码器的影响，以及哪些任务会导致最佳的编码性能。我们使用 prompt-tuning 技术，它在语音编码领域尚未得到充分探索，来生成八种自然语言理解（NLU）任务的有监督表示。我们发现，使用 prompt-tuning 技术可以更好地预测中文刺激的神经响应，并且发现任务需要细化概念和实体处理时，表示更加预测大脑活动 Pattern 相关。此外，我们发现调整参数的比例对练习后的模型语言编码性能具有重要影响。总的来说，我们的实验结果可以帮助我们更好地理解人造语言和大脑之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Learning-of-Drug-Response-Prediction-for-Preclinical-Drug-Screening"><a href="#Zero-shot-Learning-of-Drug-Response-Prediction-for-Preclinical-Drug-Screening" class="headerlink" title="Zero-shot Learning of Drug Response Prediction for Preclinical Drug Screening"></a>Zero-shot Learning of Drug Response Prediction for Preclinical Drug Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12996">http://arxiv.org/abs/2310.12996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drugd/msda">https://github.com/drugd/msda</a></li>
<li>paper_authors: Kun Li, Yong Luo, Xiantao Cai, Wenbin Hu, Bo Du</li>
<li>for: 这篇论文旨在提出一种零例学习解决方案，用于预测新药物的药物对应（DRP）任务。</li>
<li>methods: 方法基于多支多源领域适应试验插件（MSDA），可以与传统的 DRP 方法相结合，从相似药物的内部对应数据学习不变的特征，以提高实时预测未知药物的药物对应。</li>
<li>results: 实验结果显示，MSDA 能够效率地预测新药物的药物对应，导致预测误差下降5-10%，实现了在预 клиніical 阶段的药物探索过程中的加速和改善药物选择。<details>
<summary>Abstract</summary>
Conventional deep learning methods typically employ supervised learning for drug response prediction (DRP). This entails dependence on labeled response data from drugs for model training. However, practical applications in the preclinical drug screening phase demand that DRP models predict responses for novel compounds, often with unknown drug responses. This presents a challenge, rendering supervised deep learning methods unsuitable for such scenarios. In this paper, we propose a zero-shot learning solution for the DRP task in preclinical drug screening. Specifically, we propose a Multi-branch Multi-Source Domain Adaptation Test Enhancement Plug-in, called MSDA. MSDA can be seamlessly integrated with conventional DRP methods, learning invariant features from the prior response data of similar drugs to enhance real-time predictions of unlabeled compounds. We conducted experiments using the GDSCv2 and CellMiner datasets. The results demonstrate that MSDA efficiently predicts drug responses for novel compounds, leading to a general performance improvement of 5-10\% in the preclinical drug screening phase. The significance of this solution resides in its potential to accelerate the drug discovery process, improve drug candidate assessment, and facilitate the success of drug discovery.
</details>
<details>
<summary>摘要</summary>
传统的深度学习方法通常采用有监督学习的方式进行药物响应预测（DRP）。这意味着模型训练需要有标注的响应数据来源于药物。然而，在实际应用中，在前期药物层面的药物屏选阶段，需要预测新的化合物的响应，而这些化合物的响应 oftentimes  unknown。这增加了挑战，使得传统的深度学习方法无法满足这些情况。在本文中，我们提出了零shot学习的解决方案 для DRP 任务在前期药物层面。特别是，我们提出了一种多支多源领域适应测试扩展 Plug-in，称为 MSDA。 MSDA 可以与传统的 DRP 方法集成，从价值类似药物的响应数据中学习不变的特征，以提高实时预测无标注的化合物的响应。我们在 GDSCv2 和 CellMiner 数据集上进行了实验，结果表明，MSDA 能有效地预测新的化合物的响应，从而在前期药物层面提高了5-10%的性能。这种解决方案的重要性在于，它可以加速药物发现过程，改善药物候选者评估，并促进药物发现的成功。
</details></li>
</ul>
<hr>
<h2 id="Learning-Concept-Based-Visual-Causal-Transition-and-Symbolic-Reasoning-for-Visual-Planning"><a href="#Learning-Concept-Based-Visual-Causal-Transition-and-Symbolic-Reasoning-for-Visual-Planning" class="headerlink" title="Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning"></a>Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03325">http://arxiv.org/abs/2310.03325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilue Qian, Peiyu Yu, Ying Nian Wu, Wei Wang, Lifeng Fan</li>
<li>for: 这个论文旨在提出一个可解释的和通用的视觉观念规划框架，以帮助Agent在复杂环境中完成日常任务。</li>
<li>methods: 这个框架包括三个主要部分：novel Substitution-based Concept Learner (SCL)、symbol abstraction和reasoning、以及Visual Causal Transition model (ViCT)。SCL抽象视觉输入，生成分离的概念表示；symbol abstraction和reasoning使用自学到的符号来进行任务观念规划；ViCT将视觉 causal transition 与实际世界中相似的动作相连接。</li>
<li>results: 这个方法在一个大规模的视觉观念规划数据集（CCTP）上进行了严格的实验，展示了该方法在视觉任务规划方面的超越性性能。实验结果显示，该方法可以对未见过的任务路径和物品类别进行扩展。<details>
<summary>Abstract</summary>
Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiveness of the proposed model, we collect a large-scale visual planning dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this challenging dataset demonstrate the superior performance of our method in visual task planning. Empirically, we show that our framework can generalize to unseen task trajectories and unseen object categories.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified ChineseVisual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiveness of the proposed model, we collect a large-scale visual planning dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this challenging dataset demonstrate the superior performance of our method in visual task planning. Empirically, we show that our framework can generalize to unseen task trajectories and unseen object categories.</SYS>>Here's the translation in Simplified Chinese characters:Visual 规划 simulate 人类做出决策以达到目标的形式，即在初始视觉状态和目标视觉状态之间搜索视觉 causal 过渡。在 egocentric 视觉中，它变得越来越重要，因为它可以指导代理人进行日常任务在复杂环境中。在这篇论文中，我们提出了可解释性和普适性的视觉规划框架，包括 i) 一种新的替换基于概念学习器（SCL），ii) 符号抽象和理据，iii) 视觉 causal 过渡模型（ViCT）。给出初始状态，我们通过符号意义和 causal 过渡来实现目标状态的Visual 规划。为了证明我们的模型的效果，我们收集了基于 AI2-THOR 的大规模视觉规划数据集，并进行了广泛的实验。经验表明，我们的框架可以通过在未看过的任务轨迹和未看过的物品类别上进行普适化。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Human-Robot-Collaboration-using-Constrained-Probabilistic-Human-Motion-Prediction"><a href="#Enhanced-Human-Robot-Collaboration-using-Constrained-Probabilistic-Human-Motion-Prediction" class="headerlink" title="Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction"></a>Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03314">http://arxiv.org/abs/2310.03314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aadi Kothari, Tony Tohme, Xiaotong Zhang, Kamal Youcef-Toumi</li>
<li>for: 这篇论文的目的是提出一种基于人体 JOINT 约束和场景约束的人体动作预测方法，以提高人机合作的效率和安全性。</li>
<li>methods: 该方法使用 Gaussian Process Regression（GPR）模型，并将人体 JOINT 约束和场景约束直接integrated into the model，以便在预测人体动作的过程中考虑人体的物理约束和场景约束。</li>
<li>results: 实验和 simulate 结果表明，当将人体 JOINT 约束和场景约束explicitly considered时，Gaussian Process 框架可以得到较好的预测结果，而且在实际应用中也可以实现实时的人机合作。<details>
<summary>Abstract</summary>
Human motion prediction is an essential step for efficient and safe human-robot collaboration. Current methods either purely rely on representing the human joints in some form of neural network-based architecture or use regression models offline to fit hyper-parameters in the hope of capturing a model encompassing human motion. While these methods provide good initial results, they are missing out on leveraging well-studied human body kinematic models as well as body and scene constraints which can help boost the efficacy of these prediction frameworks while also explicitly avoiding implausible human joint configurations. We propose a novel human motion prediction framework that incorporates human joint constraints and scene constraints in a Gaussian Process Regression (GPR) model to predict human motion over a set time horizon. This formulation is combined with an online context-aware constraints model to leverage task-dependent motions. It is tested on a human arm kinematic model and implemented on a human-robot collaborative setup with a UR5 robot arm to demonstrate the real-time capability of our approach. Simulations were also performed on datasets like HA4M and ANDY. The simulation and experimental results demonstrate considerable improvements in a Gaussian Process framework when these constraints are explicitly considered.
</details>
<details>
<summary>摘要</summary>
人类动作预测是人机合作中不可或缺的一步，目前的方法可以分为两类：一是将人体关节表示为神经网络 Architecture 中的某种形式，二是使用回归模型在线下适应hyperparameters，以 capture 人体动作模型。尽管这些方法可以提供初步的好结果，但是它们缺乏利用人体动作学习的知识和场景约束，这些约束可以帮助提高预测框架的效果，同时明确避免人体关节配置的不可能情况。我们提出了一种新的人体动作预测框架，该框架在 Gaussian Process Regression（GPR）模型中包含人体关节约束和场景约束，以预测人体动作在时间范围内的动作。这种形式与在线上的上下文意识约束模型结合，以利用任务висимы的动作。我们在人类臂动机学模型上进行了测试，并在人机合作设置中使用UR5机械臂进行实际应用，以示我们的方法的实时能力。我们还在HA4M和ANDY等数据集上进行了 simulated 实验，实验和实际结果表明，在Gaussian Process框架中，当这些约束被Explicitly 考虑时，可以获得显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Concise-and-Organized-Perception-Facilitates-Large-Language-Models-for-Deductive-Reasoning"><a href="#Concise-and-Organized-Perception-Facilitates-Large-Language-Models-for-Deductive-Reasoning" class="headerlink" title="Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning"></a>Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03309">http://arxiv.org/abs/2310.03309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaotian Yan, Chen Shen, Junjie Liu, Jieping Ye</li>
<li>for: 提高大型自然语言模型（LLM）的逻辑推理能力。</li>
<li>methods: 提出了一种新的逻辑推理方法，即 Concise and Organized Perception（COP），通过精炼给定的陈述，快速分析出最重要信息，并将其组织得更加系统化，以便更好地逻辑推理。</li>
<li>results: 实验结果表明，与先前的状态艺术方法相比，COP方法在三个popular deductive benchmark（ProofWriter、PrOntoQA和PrOntoQA-OOD）上显著提高了性能。<details>
<summary>Abstract</summary>
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists them in answering questions or drawing conclusions precisely and quickly. In light of this, we propose a novel reasoning approach named Concise and Organized Perception (COP). COP carefully analyzes the given statements to efficiently identify the most pertinent information while eliminating redundancy. It then prompts the LLMs in a more organized form that adapts to the model's inference process. By perceiving concise and organized proofs, the deductive reasoning abilities of LLMs can be better elicited, and the risk of acquiring errors caused by excessive reasoning stages is mitigated. Furthermore, our approach can be combined with the aforementioned ones to further boost their performance. Extensive experimental results on three popular deductive benchmarks (i.e., ProofWriter, PrOntoQA and PrOntoQA-OOD) show that COP significantly outperforms previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）在解释逻辑中得到了越来越多的关注。然而，复杂的逻辑问题仍然具有许多前提（即事实或规则），导致复杂的关系和多步逻辑推理。一种直观的解决方案是将原始任务分解成小任务，然后在前向（如选择-推理）或反向（如LAMBADA）方向连接多个逻辑推理步骤。然而，这些技术无可避免地需要大量的总体阶段，导致计算昂贵的操作和更高的误导步骤的可能性。除了阶段 decomposition外，我们从人类问题解决的另一个方面着想着。人类倾向于抽象出最重要的信息，并系统地组织自己的思想（如创建MIND MAPS），这有助于他们快速、准确地回答问题或 Draw 结论。在这 basis，我们提出了一种新的逻辑方法，称为 Concise and Organized Perception（COP）。COP  méticulously analyzes the given statements to efficiently identify the most relevant information while eliminating redundancy. It then prompts the LLMs in a more organized form that adapts to the model's inference process. By perceiving concise and organized proofs, the deductive reasoning abilities of LLMs can be better elicited, and the risk of acquiring errors caused by excessive reasoning stages is mitigated。此外，我们的方法可以与以前的方法结合使用，以进一步提高 их性能。我们在三个流行的逻辑标准 benchmark（ ProofWriter、PrOntoQA 和 PrOntoQA-OOD）进行了广泛的实验，结果表明，COP significantly outperforms previous state-of-the-art methods。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Large-Language-Models-As-AI-Research-Agents"><a href="#Benchmarking-Large-Language-Models-As-AI-Research-Agents" class="headerlink" title="Benchmarking Large Language Models As AI Research Agents"></a>Benchmarking Large Language Models As AI Research Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03302">http://arxiv.org/abs/2310.03302</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snap-stanford/mlagentbench">https://github.com/snap-stanford/mlagentbench</a></li>
<li>paper_authors: Qian Huang, Jian Vora, Percy Liang, Jure Leskovec<br>for:MLAgentBench is a suite of ML tasks for benchmarking AI research agents, allowing them to perform actions like reading&#x2F;writing files, executing code, and inspecting outputs.methods:The benchmark evaluates the agent’s performance objectively over various metrics related to performance and efficiency, and an LLM-based research agent is designed to automatically perform experimentation loops in such an environment.results:A GPT-4-based research agent can feasibly build compelling ML models over many tasks in MLAgentBench, displaying highly interpretable plans and actions, but the success rates vary considerably and the agent faces challenges such as long-term planning and hallucination.Here is the answer in Simplified Chinese text:for: MLAgentBench 是一个 ML 任务集合，用于评估 AI 研究代理的表现，允许代理执行文件读写、代码执行和输出检查等操作。methods: MLAgentBench 使用对象 Orientated 评估代理的表现，包括多种关于性能和效率的 metric，而一个基于 LLM 的研究代理被设计用于自动执行实验循环。results: GPT-4 基于的研究代理可以在 MLAgentBench 上建立优秀的 ML 模型，显示出高度可读取的计划和行动，但成功率差异较大，从 nearly 90% 在较古老的 dataset 上到 recent Kaggle Challenges 上的 10%，甚至 newer research challenges 上的 0%。此外， LLB 基于的研究代理还面临着长期规划和幻觉等挑战。<details>
<summary>Abstract</summary>
Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-based research agent to automatically perform experimentation loops in such an environment. Empirically, we find that a GPT-4-based research agent can feasibly build compelling ML models over many tasks in MLAgentBench, displaying highly interpretable plans and actions. However, the success rates vary considerably; they span from almost 90\% on well-established older datasets to as low as 10\% on recent Kaggle Challenges -- unavailable during the LLM model's pretraining -- and even 0\% on newer research challenges like BabyLM. Finally, we identify several key challenges for LLM-based research agents such as long-term planning and hallucination. Our code is released at https://github.com/snap-stanford/MLAgentBench.
</details>
<details>
<summary>摘要</summary>
Translation (Simplified Chinese):科学实验涉及到一个迭代的过程，包括创建假设、设计实验、运行实验和分析结果。我们是否可以建立AI研究代理来完成这些长期决策任务？为了实现这一目标，我们将关注机器学习工程问题：给定任务描述和数据集，建立高性能的模型。在这篇论文中，我们提出了MLAgentBench，一个用于评估AI研究代理的ML任务集。代理可以执行如读写文件、执行代码和检查输出等动作。通过这些动作，代理可以运行实验、分析结果并修改整个机器学习管道，包括数据处理、架构、训练过程等。然后，比较器会自动评估代理的表现，并对其表现进行对比。我们还设计了一个基于LLM的研究代理，可以自动完成实验循环。我们的实验表明，一个基于GPT-4的研究代理可以在MLAgentBench中建立吸引人的ML模型，并显示出高度可读的计划和操作。然而，成功率很大，从 almost 90% 到 recent Kaggle Challenges 的10% ，甚至到 newer research challenges like BabyLM 的0%。最后，我们确定了一些关键挑战，包括长期规划和幻觉。我们的代码发布在 https://github.com/snap-stanford/MLAgentBench。
</details></li>
</ul>
<hr>
<h2 id="LightSeq-Sequence-Level-Parallelism-for-Distributed-Training-of-Long-Context-Transformers"><a href="#LightSeq-Sequence-Level-Parallelism-for-Distributed-Training-of-Long-Context-Transformers" class="headerlink" title="LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers"></a>LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03294">http://arxiv.org/abs/2310.03294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rulinshao/lightseq">https://github.com/rulinshao/lightseq</a></li>
<li>paper_authors: Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, Hao Zhang</li>
<li>for: 本研究旨在提高大语言模型（LLMs）的训练context长度，但是这会增加训练的内存占用。现有的分布式系统，如Megatron-LM，通过分解并并行计算不同的注意头，但是这会导致大量的通信量，因此无法扩展。</li>
<li>methods: 本研究提出了一种新的方法——LightSeq，用于长context LLMs 的训练。LightSeq通过分解序列维度来实现，因此不受模型结构的限制，可以应用于不同的注意头数量模型，如多头注意、多个查询注意和分组查询注意。LightSeq比Megatron-LM需要更少的通信，并且可以重合计算和通信。</li>
<li>results: 通过对Llama-7B和其变种进行详细的单节和跨节训练测试，我们发现LightSeq可以 дости到1.24-2.01倍的总体速度提升，并可以支持更长的序列长度（32K-512K）。相比Megatron-LM，LightSeq可以减少4.7倍的通信量，并且实现了更高效的训练。<details>
<summary>Abstract</summary>
Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient checkpointing scheme to bypass an forward computation for memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants with sequence lengths from 32K to 512K. Through comprehensive experiments on single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x end-to-end speedup, and a 2-8x longer sequence length on models with fewer heads, compared to Megatron-LM. Codes will be available at https://github.com/RulinShao/LightSeq.
</details>
<details>
<summary>摘要</summary>
增加大语言模型（LLM）的上下文长度可以解锁新的功能，但也会增加训练时的内存占用。现有的模型并行系统，如Megatron-LM，通过分布式计算不同的注意头，以并行计算方式来降低通信量，但是这种方法无法扩展到更多的注意头，因此限制了其应用。在这篇论文中，我们介绍了一种新的方法——LightSeq，用于长上下文LLM的训练。LightSeq具有多个优势。首先，LightSeq分配在序列维度上，因此不受模型结构限制，可以应用于不同数量的注意头，如多头注意、多Query注意和分组Query注意。其次，LightSeq相比Megatron-LM需要4.7倍少的通信量，并且可以在计算和通信之间进行 overlap。为了进一步减少训练时间，LightSeq还提供了一种独特的梯度检查点 schemes，以快速地缓存减少计算注意。我们在Llama-7B和其变种上进行了广泛的实验，并证明了LightSeq可以达到1.24-2.01倍的综合速度，并在模型中有更多的注意头时可以处理更长的序列长度。代码将在https://github.com/RulinShao/LightSeq上提供。
</details></li>
</ul>
<hr>
<h2 id="SoK-Access-Control-Policy-Generation-from-High-level-Natural-Language-Requirements"><a href="#SoK-Access-Control-Policy-Generation-from-High-level-Natural-Language-Requirements" class="headerlink" title="SoK: Access Control Policy Generation from High-level Natural Language Requirements"></a>SoK: Access Control Policy Generation from High-level Natural Language Requirements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03292">http://arxiv.org/abs/2310.03292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sakuna Harinda Jayasundara, Nalin Asanka Gamagedara Arachchilage, Giovanni Russello</li>
<li>for: 防止管理员中心化访问控制失败，以避免数据泄露和组织受到金融损失和声誉损害。</li>
<li>methods: 已有图形策略配置工具和自动生成策略框架，帮助管理员配置和生成访问控制策略，以避免such failures。但是，图形策略配置工具容易出现人工错误，而自动生成策略框架容易出现错误预测，因此需要改进其可用性和可靠性。</li>
<li>results: 通过系统性文献回顾分析49篇论文，发现现有工具和框架具有限制，需要改进以提高可用性和可靠性。<details>
<summary>Abstract</summary>
Administrator-centered access control failures can cause data breaches, putting organizations at risk of financial loss and reputation damage. Existing graphical policy configuration tools and automated policy generation frameworks attempt to help administrators configure and generate access control policies by avoiding such failures. However, graphical policy configuration tools are prone to human errors, making them unusable. On the other hand, automated policy generation frameworks are prone to erroneous predictions, making them unreliable. Therefore, to find ways to improve their usability and reliability, we conducted a Systematic Literature Review analyzing 49 publications, to identify those tools, frameworks, and their limitations. Identifying those limitations will help develop effective access control policy generation solutions while avoiding access control failures.
</details>
<details>
<summary>摘要</summary>
管理员中心的访问控制失败可导致数据泄露，使组织面临金融损失和声誉损害的风险。现有的图形策略配置工具和自动策略生成框架尝试帮助管理员配置和生成访问控制策略，以避免这些失败。然而，图形策略配置工具容易出现人为错误，使其不可用。相反，自动策略生成框架容易出现错误预测，使其不可靠。因此，为了改善其可用性和可靠性，我们进行了系统性文献综述，分析了49篇论文，以识别这些工具、框架和其限制，以帮助开发有效的访问控制策略生成解决方案，并避免访问控制失败。
</details></li>
</ul>
<hr>
<h2 id="A-5’-UTR-Language-Model-for-Decoding-Untranslated-Regions-of-mRNA-and-Function-Predictions"><a href="#A-5’-UTR-Language-Model-for-Decoding-Untranslated-Regions-of-mRNA-and-Function-Predictions" class="headerlink" title="A 5’ UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions"></a>A 5’ UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03281">http://arxiv.org/abs/2310.03281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanyi Chu, Dan Yu, Yupeng Li, Kaixuan Huang, Yue Shen, Le Cong, Jason Zhang, Mengdi Wang<br>for:* The paper is written to introduce a language model for 5’ UTR (UTR-LM) to predict the translation efficiency and mRNA expression level.methods:* The UTR-LM is pre-trained on endogenous 5’ UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy.* The model is fine-tuned in a variety of downstream tasks.results:* The UTR-LM outperformed the best-known benchmark by up to 42% for predicting the Mean Ribosome Loading and by up to 60% for predicting the Translation Efficiency and the mRNA Expression Level.* The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline.* Experiment results confirmed that our top designs achieved a 32.5% increase in protein production level relative to well-established 5’ UTR optimized for therapeutics.<details>
<summary>Abstract</summary>
The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5' UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42% for predicting the Mean Ribosome Loading, and by up to 60% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a library of 211 novel 5' UTRs with high predicted values of translation efficiency and evaluated them via a wet-lab assay. Experiment results confirmed that our top designs achieved a 32.5% increase in protein production level relative to well-established 5' UTR optimized for therapeutics.
</details>
<details>
<summary>摘要</summary>
“5' UTR，一个调节区域，位于mRNA分子的起始处，对翻译过程进行重要调节，并影响蛋白质表达水平。语音模型已经展示了它们可以解读蛋白质和基因序列的功能。在这里，我们引入了5' UTR的语音模型，我们称之为UTR-LM。UTR-LM在多种生物体中的组合式训练中进行预训练，并且受到次要结构和最小自由能的指导。我们在多个下游任务中精确调整UTR-LM。模型比最佳参考基准高达42% для预测蛋白质载入平均值，并高达60% для预测翻译效率和mRNA表达水平。模型还应用于识别未被评估的Internal Ribosome Entry Sites（iRES），并提高AUPR从0.37提升至0.52，比最佳基eline高出35%。此外，我们设计了211个新的5' UTR，预测的翻译效率高，并通过湿库实验验证。结果显示，我们的顶部设计可以提高蛋白质生产水平32.5%，相比于已知的5' UTR优化 для医药。”
</details></li>
</ul>
<hr>
<h2 id="Network-Alignment-with-Transferable-Graph-Autoencoders"><a href="#Network-Alignment-with-Transferable-Graph-Autoencoders" class="headerlink" title="Network Alignment with Transferable Graph Autoencoders"></a>Network Alignment with Transferable Graph Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03272">http://arxiv.org/abs/2310.03272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graphmatching/graph-matching">https://github.com/graphmatching/graph-matching</a></li>
<li>paper_authors: Jiashu He, Charilaos I. Kanatsoulis, Alejandro Ribeiro</li>
<li>for: 提高网络对齐的精度和效率，使得网络对齐可以在大规模 graphs 上进行。</li>
<li>methods: 提出一种基于自适应神经网络的普适 graph autoencoder 框架，通过提取节点嵌入来实现网络对齐。该框架可以利用传输学习和数据增强来实现高效的网络对齐。</li>
<li>results: 实验表明，提出的方法可以在实际世界 graphs 上进行高精度、高效的网络对齐，而且不需要重新训练。<details>
<summary>Abstract</summary>
Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scalability of the proposed approach.
</details>
<details>
<summary>摘要</summary>
网络对齐是指将不同图像的节点进行一对一对应，并具有许多应用于高影响领域。然而，这个任务已知为NP困难的普通形式，现有的算法无法随图像大小增长缩放。为了解决这两个挑战，我们提出了一种新的通用图自编码器架构，用于提取强大和可靠的节点嵌入，特化于对齐任务。我们证明了生成的嵌入是对图像的特征值和特征向量相关的，并可以在比 классическихspectral方法更高精度的情况下进行对齐。我们的提出的框架还利用了传输学习和数据扩展来实现大规模的网络对齐，无需重新训练。广泛的实验表明，我们的方法可以在真实世界的图像上实现高精度和可扩展的网络对齐。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Deep-Learning-for-Time-Series-Data-Theory-and-Applications"><a href="#Sparse-Deep-Learning-for-Time-Series-Data-Theory-and-Applications" class="headerlink" title="Sparse Deep Learning for Time Series Data: Theory and Applications"></a>Sparse Deep Learning for Time Series Data: Theory and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03243">http://arxiv.org/abs/2310.03243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingxuan Zhang, Yan Sun, Faming Liang</li>
<li>for: 这篇论文的目的是提高深度学习网络在不同类型数据上的表现，特别是在不确定性量化、变数选择和大规模网络压缩等领域。</li>
<li>methods: 本论文使用的方法是简单深度学习，并研究了这种方法在相依数据上的应用。研究结果显示，简单深度学习可以在相依数据上适当地训练，并且可以正确地量化预测uncertainty。</li>
<li>results: 本论文的numerical results显示，简单深度学习可以在时间序列数据上进行更好的预测uncertainty量化，并且可以正确地决定时间序列中的自相依关系。此外，本论文的结果显示，简单深度学习可以在大规模网络压缩中表现更好，并且可以正确地识别时间序列中的自相依关系。<details>
<summary>Abstract</summary>
Sparse deep learning has become a popular technique for improving the performance of deep neural networks in areas such as uncertainty quantification, variable selection, and large-scale network compression. However, most existing research has focused on problems where the observations are independent and identically distributed (i.i.d.), and there has been little work on the problems where the observations are dependent, such as time series data and sequential data in natural language processing. This paper aims to address this gap by studying the theory for sparse deep learning with dependent data. We show that sparse recurrent neural networks (RNNs) can be consistently estimated, and their predictions are asymptotically normally distributed under appropriate assumptions, enabling the prediction uncertainty to be correctly quantified. Our numerical results show that sparse deep learning outperforms state-of-the-art methods, such as conformal predictions, in prediction uncertainty quantification for time series data. Furthermore, our results indicate that the proposed method can consistently identify the autoregressive order for time series data and outperform existing methods in large-scale model compression. Our proposed method has important practical implications in fields such as finance, healthcare, and energy, where both accurate point estimates and prediction uncertainty quantification are of concern.
</details>
<details>
<summary>摘要</summary>
sparse deep learning 已成为深度学习中提高性能的受欢迎技术，特别是在不确定量评估、变量选择和大规模网络压缩等领域。然而，大多数现有研究都集中在独立相同分布（i.i.d）的问题上，尚未对相关的问题进行研究，如时间序列数据和自然语言处理中的序列数据。这篇论文想要填补这一差距，通过研究依赖数据的概率理论，来探讨这些问题。我们显示了 sparse RNN 可以透明地估算，其预测值在适当假设下是均匀分布的，从而正确地评估预测uncertainty。我们的numerical结果表明， sparse deep learning 在时间序列数据中的预测uncertainty评估方面超过了现有的方法，如 конформаль预测，并且在大规模模型压缩方面也表现出了优异性。我们的提议方法在金融、医疗和能源等领域有重要实践意义，因为它们都需要准确的点估计和预测uncertainty评估。
</details></li>
</ul>
<hr>
<h2 id="Non-Smooth-Weakly-Convex-Finite-sum-Coupled-Compositional-Optimization"><a href="#Non-Smooth-Weakly-Convex-Finite-sum-Coupled-Compositional-Optimization" class="headerlink" title="Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization"></a>Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03234">http://arxiv.org/abs/2310.03234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quanqi Hu, Dixian Zhu, Tianbao Yang</li>
<li>For:  investigate new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO)* Methods:  examine non-smooth weakly-convex FCCO, analyze a single-loop algorithm, and establish its complexity for finding an $\epsilon$-stationary point of the Moreau envelop of the objective function* Results:  extend the algorithm to solving novel non-smooth weakly-convex tri-level finite-sum coupled compositional optimization problems, and explore applications in deep learning for two-way partial AUC maximization and multi-instance two-way partial AUC maximization using empirical studies to showcase the effectiveness of the proposed algorithms.Here’s the format you requested:* For: &lt;what are the paper written for?&gt;* Methods: &lt;what methods the paper use?&gt;* Results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau envelop of the objective function. Additionally, we also extend the algorithm to solving novel non-smooth weakly-convex tri-level finite-sum coupled compositional optimization problems, which feature a nested arrangement of three functions. Lastly, we explore the applications of our algorithms in deep learning for two-way partial AUC maximization and multi-instance two-way partial AUC maximization, using empirical studies to showcase the effectiveness of the proposed algorithms.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文研究了新的一类 compositional optimization 问题，即 $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO)。随着机器学习和人工智能领域中 FCCO 的应用广泛，以及基于 empirical risk minimization 的权重随机算法的缺点，FCCO 的研究吸引了越来越多的关注。然而，现有的 FCCO 研究假设内函数和外函数都是平滑的，这限制了它们的应用范围。我们的研究扩展了这一领域，研究非平滑弱 convex FCCO，其中外函数是弱 convex 的，内函数是弱-convex。我们分析了单循环算法，并确定了其在 Moreau 封闭中的 $\epsilon $-站点的复杂性。此外，我们还扩展了算法，以解决新的非平滑弱 convex tri-level finite-sum coupled compositional optimization 问题，其中有三个函数的嵌套排序。最后，我们探讨了我们的算法在深度学习中的应用，包括两种方法的 partial AUC 最大化和多实例两种方法的 partial AUC 最大化，并通过实验研究表明了我们的算法的效果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Representations-of-First-person-Pronouns-for-Prediction-of-Depression-Symptom-Severity"><a href="#Deep-Representations-of-First-person-Pronouns-for-Prediction-of-Depression-Symptom-Severity" class="headerlink" title="Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity"></a>Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03232">http://arxiv.org/abs/2310.03232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyang Ren, Hannah A Burkhardt, Patricia A Areán, Thomas D Hull, Trevor Cohen</li>
<li>for: 本研究使用文本数据分析个人心理状态，尤其是抑郁症状的严重程度。</li>
<li>methods: 研究使用了Contextualized language representation models来生成首人宾词的上下文嵌入，以捕捉首人宾词在语料中的使用方式。</li>
<li>results: 研究结果表明，使用上下文嵌入的首人宾词表现出色于标准分类token嵌入和频率分析结果，在预测抑郁症状严重程度方面表现出优异。这表明Contextual representations of first-person pronouns可以增强语言使用的预测性能。<details>
<summary>Abstract</summary>
Prior work has shown that analyzing the use of first-person singular pronouns can provide insight into individuals' mental status, especially depression symptom severity. These findings were generated by counting frequencies of first-person singular pronouns in text data. However, counting doesn't capture how these pronouns are used. Recent advances in neural language modeling have leveraged methods generating contextual embeddings. In this study, we sought to utilize the embeddings of first-person pronouns obtained from contextualized language representation models to capture ways these pronouns are used, to analyze mental status. De-identified text messages sent during online psychotherapy with weekly assessment of depression severity were used for evaluation. Results indicate the advantage of contextualized first-person pronoun embeddings over standard classification token embeddings and frequency-based pronoun analysis results in predicting depression symptom severity. This suggests contextual representations of first-person pronouns can enhance the predictive utility of language used by people with depression symptoms.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:先前的研究表明，分析首人单数代名词的使用可以提供困惑状态的人们的心理状况信息，特别是抑郁症Symptom的严重程度。这些发现是通过计数首人单数代名词在文本数据中的频率来获得的。然而，计数不能捕捉首人单数代名词的使用方式。近年来，神经语言模型的发展已经利用了生成上下文 embedding的方法。在这项研究中，我们想要利用来自上下文化语言表示模型的首人单数代名词 embedding来捕捉首人单数代名词的使用方式，以分析困惑状态。在在线心理咨询中发送的匿名短信，与每周评估抑郁症Symptom的严重程度一起使用进行评估。结果表明，上下文化首人单数代名词 embedding 的优势在 predicting 抑郁症Symptom 的严重程度上，比标准化 classification token embedding 和频率分析结果更高。这表示上下文表示首人单数代名词可以增强基于语言使用的抑郁症Symptom 的预测utilities。
</details></li>
</ul>
<hr>
<h2 id="Safe-Exploration-in-Reinforcement-Learning-A-Generalized-Formulation-and-Algorithms"><a href="#Safe-Exploration-in-Reinforcement-Learning-A-Generalized-Formulation-and-Algorithms" class="headerlink" title="Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms"></a>Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03225">http://arxiv.org/abs/2310.03225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akifumi Wachi, Wataru Hashimoto, Xun Shen, Kazumune Hashimoto</li>
<li>for: 本研究旨在提供一种通用安全探索（Generalized Safe Exploration，GSE）问题的统一形式，以及一种基于无约束RL算法和不确定度量表示的安全探索方法MASE，以确保在当前 episoden 中的安全性，并避免未来 episoden 中的安全性抵触。</li>
<li>methods: 本研究使用了一种基于Generalized Linear Models（GLMs）的隐藏 MARGE 方法，以及一种 combine 了 Gaussian Process 和 Deep RL 算法的 variant。</li>
<li>results: 实验结果表明，相比之前的状态 искусственный智能算法，MASE 可以在 grid-world 和 Safety Gym 测试环境中实现更好的性能，而不需要违反任何安全约束，即使在训练过程中。<details>
<summary>Abstract</summary>
Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations before actual safety violation to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian process to ensure safety with a deep RL algorithm to maximize the reward. Finally, we demonstrate that our proposed algorithm achieves better performance than state-of-the-art algorithms on grid-world and Safety Gym benchmarks without violating any safety constraints, even during training.
</details>
<details>
<summary>摘要</summary>
安全探索是重要的实用应用强化学习（RL）的前提。在这篇论文中，我们提出一种通用安全探索（GSE）问题的总体形式，并提出一种解决GSE问题的元算法MASE，该算法结合不受限制的RL算法和不确定度量表来保证当前pisode中的安全性，并正确惩罚不安全的探索，以避免将来的episode中的安全性被违反。MASE的优点在于，我们可以在合理的假设下优化策略，同时保证高概率下不会违反任何安全约束。我们采用两种不同的构建不确定度量表的MASE变体：一种基于泛化线性模型，具有安全性和优化性的理论保证；另一种 combining Gaussian process ensure safety with a deep RL algorithm to maximize the reward.最后，我们证明我们提出的算法在Grid-world和Safety Gym benchmark上比现有算法更好的性能，而不违反任何安全约束，甚至在训练过程中。
</details></li>
</ul>
<hr>
<h2 id="Know2BIO-A-Comprehensive-Dual-View-Benchmark-for-Evolving-Biomedical-Knowledge-Graphs"><a href="#Know2BIO-A-Comprehensive-Dual-View-Benchmark-for-Evolving-Biomedical-Knowledge-Graphs" class="headerlink" title="Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs"></a>Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03221">http://arxiv.org/abs/2310.03221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yijia-xiao/know2bio">https://github.com/yijia-xiao/know2bio</a></li>
<li>paper_authors: Yijia Xiao, Dylan Steinecke, Alexander Russell Pelletier, Yushi Bai, Peipei Ping, Wei Wang</li>
<li>for: 这个论文目的是提出一个通用的生物医学知识 graphs（KG）测试集，以便用于生物医学知识 repre sentation学习。</li>
<li>methods: 这个论文使用了多种数据源，并将这些数据源中的信息集成到一个KG中，以捕捉生物医学领域的复杂关系。它还可以自动更新，以适应最新的生物医学知识。</li>
<li>results: 研究人员通过在Know2BIO上评估知识 repre sentation模型，发现Know2BIO可以作为生物医学领域中知识 repre sentation学习的标准测试集。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) have emerged as a powerful framework for representing and integrating complex biomedical information. However, assembling KGs from diverse sources remains a significant challenge in several aspects, including entity alignment, scalability, and the need for continuous updates to keep pace with scientific advancements. Moreover, the representative power of KGs is often limited by the scarcity of multi-modal data integration. To overcome these challenges, we propose Know2BIO, a general-purpose heterogeneous KG benchmark for the biomedical domain. Know2BIO integrates data from 30 diverse sources, capturing intricate relationships across 11 biomedical categories. It currently consists of ~219,000 nodes and ~6,200,000 edges. Know2BIO is capable of user-directed automated updating to reflect the latest knowledge in biomedical science. Furthermore, Know2BIO is accompanied by multi-modal data: node features including text descriptions, protein and compound sequences and structures, enabling the utilization of emerging natural language processing methods and multi-modal data integration strategies. We evaluate KG representation models on Know2BIO, demonstrating its effectiveness as a benchmark for KG representation learning in the biomedical field. Data and source code of Know2BIO are available at https://github.com/Yijia-Xiao/Know2BIO/.
</details>
<details>
<summary>摘要</summary>
知识图（KG）在生物医学领域已经出现为表示和集成复杂生物医学信息的强大框架。然而，从多种来源组装KG仍然是一个重要的挑战，包括实体对应、可扩展性和需要不断更新以保持科学进步的速度。此外，KG的表达力 часто受到多模态数据集成的限制。为了解决这些挑战，我们提出了知2生物（Know2BIO），一个通用的生物医学领域多模态KG Benchmark。知2生物从30种多样化来源中提取了11类生物医学信息，涵盖了复杂的实体关系，目前包含约219,000个节点和6,200,000个边。知2生物支持用户指导的自动更新，以反映最新的生物医学知识。此外，知2生物还附带了多模态数据，包括节点特征文本描述、蛋白质和化合物序列和结构，这使得可以利用生成的自然语言处理方法和多模态数据集成策略。我们在知2生物上评估KG表示模型，证明其在生物医学领域KG表示学习的有效性。数据和源代码可以在https://github.com/Yijia-Xiao/Know2BIO/ obtained。
</details></li>
</ul>
<hr>
<h2 id="Learning-Energy-Based-Prior-Model-with-Diffusion-Amortized-MCMC"><a href="#Learning-Energy-Based-Prior-Model-with-Diffusion-Amortized-MCMC" class="headerlink" title="Learning Energy-Based Prior Model with Diffusion-Amortized MCMC"></a>Learning Energy-Based Prior Model with Diffusion-Amortized MCMC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03218">http://arxiv.org/abs/2310.03218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yupeiyu98/diffusion-amortized-mcmc">https://github.com/yupeiyu98/diffusion-amortized-mcmc</a></li>
<li>paper_authors: Peiyu Yu, Yaxuan Zhu, Sirui Xie, Xiaojian Ma, Ruiqi Gao, Song-Chun Zhu, Ying Nian Wu</li>
<li>For: The paper is written for learning latent space Energy-Based Models (EBMs) with long-run Markov Chain Monte Carlo (MCMC) sampling, to address the issue of degenerate MCMC sampling quality in practice.* Methods: The paper introduces a simple but effective diffusion-based amortization method for long-run MCMC sampling, and develops a novel learning algorithm for the latent space EBM based on it.* Results: The paper provides theoretical evidence that the learned amortization of MCMC is a valid long-run MCMC sampler, and demonstrates superior performance of the method on several image modeling benchmark datasets compared with strong counterparts.Here is the text in Simplified Chinese:</li>
<li>for: 本文是为了学习嵌入空间能量基本模型（EBM）的长期Markov链 Monte Carlo（MCMC）采样，以解决实践中MCMC采样质量不佳的问题。</li>
<li>methods: 本文提出了一种简单 yet effective的扩散基于权重融合方法，用于长期MCMC采样，并基于其开发了一种新的学习算法。</li>
<li>results: 本文提供了理论证明，表明学习的MCMC权重融合是一个有效的长期MCMC采样方法，并在多个图像模型benchmark数据集上与强对手进行比较，得到了更好的性能。<details>
<summary>Abstract</summary>
Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in the field of generative modeling due to its flexibility in the formulation and strong modeling power of the latent space. However, the common practice of learning latent space EBMs with non-convergent short-run MCMC for prior and posterior sampling is hindering the model from further progress; the degenerate MCMC sampling quality in practice often leads to degraded generation quality and instability in training, especially with highly multi-modal and/or high-dimensional target distributions. To remedy this sampling issue, in this paper we introduce a simple but effective diffusion-based amortization method for long-run MCMC sampling and develop a novel learning algorithm for the latent space EBM based on it. We provide theoretical evidence that the learned amortization of MCMC is a valid long-run MCMC sampler. Experiments on several image modeling benchmark datasets demonstrate the superior performance of our method compared with strong counterparts
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Latent space Energy-Based Models (EBMs)，也称为能量基因准确，在生成模型领域得到了越来越多的关注，这是因为它们在形式化的灵活性和高效的 latent space 模型能力之间。然而，通常通过非收敛短期 MCMC 学习 latent space EBMs 的做法会带来模型的进一步发展困难; 短期 MCMC 抽取质量在实践中 часто导致生成质量下降和训练不稳定，特别是面临高多态和/或高维target distribution。为了解决这种抽取问题，在这篇论文中我们提出了一种简单 yet effective 的扩散基于散度 amortization 方法，并开发了一种基于这种方法的 latent space EBM 学习算法。我们提供了理论证明，表明学习的扩散 MCMC 是一个有效的长期 MCMC 抽取器。在多个图像模型 benchmark 数据集上，我们的方法与强有力的对手相比，表现出了更高的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/05/cs.AI_2023_10_05/" data-id="clp88dbrk0056ob88gji57qgr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/05/cs.CL_2023_10_05/" class="article-date">
  <time datetime="2023-10-05T11:00:00.000Z" itemprop="datePublished">2023-10-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/05/cs.CL_2023_10_05/">cs.CL - 2023-10-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Exploring-the-evolution-of-research-topics-during-the-COVID-19-pandemic"><a href="#Exploring-the-evolution-of-research-topics-during-the-COVID-19-pandemic" class="headerlink" title="Exploring the evolution of research topics during the COVID-19 pandemic"></a>Exploring the evolution of research topics during the COVID-19 pandemic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03928">http://arxiv.org/abs/2310.03928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Invernici, Anna Bernasconi, Stefano Ceri</li>
<li>for: 这研究旨在提供一种方法和可视化工具，用于检查COVID-19开放研究数据集（CORD-19）的科学摘要文章。</li>
<li>methods: 该方法基于选择最新技术（包括大语言模型），实现了对文章集成 orthogonal 维度的 clustering 和时间主题挖掘技术。</li>
<li>results: 该方法可以快速、一键 inspect 文章主题内容，并提供时间序列图表和 word cloud 图表，以便对任意时间窗口中主题的出现进行统计测试。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has changed the research agendas of most scientific communities, resulting in an overwhelming production of research articles in a variety of domains, including medicine, virology, epidemiology, economy, psychology, and so on. Several open-access corpora and literature hubs were established; among them, the COVID-19 Open Research Dataset (CORD-19) has systematically gathered scientific contributions for 2.5 years, by collecting and indexing over one million articles. Here, we present the CORD-19 Topic Visualizer (CORToViz), a method and associated visualization tool for inspecting the CORD-19 textual corpus of scientific abstracts. Our method is based upon a careful selection of up-to-date technologies (including large language models), resulting in an architecture for clustering articles along orthogonal dimensions and extraction techniques for temporal topic mining. Topic inspection is supported by an interactive dashboard, providing fast, one-click visualization of topic contents as word clouds and topic trends as time series, equipped with easy-to-drive statistical testing for analyzing the significance of topic emergence along arbitrarily selected time windows. The processes of data preparation and results visualization are completely general and virtually applicable to any corpus of textual documents - thus suited for effective adaptation to other contexts.
</details>
<details>
<summary>摘要</summary>
COVID-19 流行病已经对大多数科学社区的研究议程产生了深见的影响，导致了一些领域的研究文章急剧增加，包括医学、病毒学、流行病学、经济学、心理学等等。此外，一些开放获取的数据库和文献庐也被建立起来，其中COVID-19开放研究数据集（CORD-19）在过去2.5年内系统地收集和索引了大量的科学论文。在这里，我们介绍了CORD-19话题可视化工具（CORToViz），它是基于最新的技术（包括大语言模型）的方法和相应的可视化工具，用于探索COVID-19的文本数据库中的科学摘要。我们的方法包括对各个维度进行分 clustering 和时间序列分析等技术，以及一个交互式的可视化面板，可以快速地Visualize 摘要中的话题内容为云图和时间序列图表，同时提供了一些简单易用的统计测试，以分析选定时间窗口中话题的出现是否为 statistically significant。数据准备和结果可视化的过程是完全通用的，可以方便地适应到其他文本数据库上。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Multi-Agent-Coordination-Abilities-in-Large-Language-Models"><a href="#Evaluating-Multi-Agent-Coordination-Abilities-in-Large-Language-Models" class="headerlink" title="Evaluating Multi-Agent Coordination Abilities in Large Language Models"></a>Evaluating Multi-Agent Coordination Abilities in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03903">http://arxiv.org/abs/2310.03903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saaket Agashe, Yue Fan, Xin Eric Wang</li>
<li>for: 这项研究的目标是开发能够与人类和其他系统合作 efectively 的多智能体代理人。</li>
<li>methods: 这项研究使用了 Large Language Models (LLMs)，可以理解、生成和解释人类语言的方式，以开发多智能体 coordination 代理人。</li>
<li>results: 研究表明，使用 LLMs 可以在多智能体协调场景中实现高效的协调，包括理解伙伴的意图、 reasoning 行为、持续协调和对不熟悉的伙伴的Robustness。此外，研究还发现 LLMS 可以在 Overcooked-AI  benchmark 中提供有用的帮助，并且可以快速学习和适应新的协调场景。<details>
<summary>Abstract</summary>
A pivotal aim in contemporary AI research is to develop agents proficient in multi-agent coordination, enabling effective collaboration with both humans and other systems. Large Language Models (LLMs), with their notable ability to understand, generate, and interpret language in a human-like manner, stand out as promising candidates for the development of such agents. In this study, we build and assess the effectiveness of agents crafted using LLMs in various coordination scenarios. We introduce the LLM-Coordination (LLM-Co) Framework, specifically designed to enable LLMs to play coordination games. With the LLM-Co framework, we conduct our evaluation with three game environments and organize the evaluation into five aspects: Theory of Mind, Situated Reasoning, Sustained Coordination, Robustness to Partners, and Explicit Assistance. First, the evaluation of the Theory of Mind and Situated Reasoning reveals the capabilities of LLM to infer the partner's intention and reason actions accordingly. Then, the evaluation around Sustained Coordination and Robustness to Partners further showcases the ability of LLMs to coordinate with an unknown partner in complex long-horizon tasks, outperforming Reinforcement Learning baselines. Lastly, to test Explicit Assistance, which refers to the ability of an agent to offer help proactively, we introduce two novel layouts into the Overcooked-AI benchmark, examining if agents can prioritize helping their partners, sacrificing time that could have been spent on their tasks. This research underscores the promising capabilities of LLMs in sophisticated coordination environments and reveals the potential of LLMs in building strong real-world agents for multi-agent coordination.
</details>
<details>
<summary>摘要</summary>
当代人工智能研究的核心目标是开发多智能体协作的能力，以便和人类以及其他系统有效协作。大型自然语言模型（LLM）因其能够理解、生成和解释人类语言方式而出众，因此在开发这类多智能体协作代理人方面表现出了扎实的潜力。在这项研究中，我们采用LLM-Coordination（LLM-Co）框架，以便LLM在协作游戏中表现出色。我们通过三个游戏环境进行评估，并将评估分为五个方面：理解伙伴意图、地域思维、持续协作、对伙伴强健和显式帮助。经过评估，我们发现LLM在理解伙伴意图和地域思维方面具有出色的能力，并在持续协作和对伙伴强健方面超越了强化学习基eline。最后，为了测试显式帮助，我们在Overcooked-AI bencmark中引入了两个新的布局，以测试代理人是否可以主动为伙伴提供帮助，牺牲一些时间来完成自己的任务。这项研究表明LLM在复杂多智能体协作环境中的潜力，并探讨LLM在实际世界中建立强大的多智能体协作代理人的可能性。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-Formal-Natural-Language-Specifications"><a href="#Trustworthy-Formal-Natural-Language-Specifications" class="headerlink" title="Trustworthy Formal Natural Language Specifications"></a>Trustworthy Formal Natural Language Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03885">http://arxiv.org/abs/2310.03885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colin S. Gordon, Sergey Matskevich</li>
<li>for: 这篇论文目的是提供一种在现有的证明助手中支持表达自然语言Specification的方法，以便在证明软件正确性时更好地利用自然语言specification。</li>
<li>methods: 这篇论文使用了一种基于现有证明助手的方法，即使用一种可以自动将自然语言specification翻译成正式laims的方法。这种方法是可扩展的，可以轻松地添加新的词汇和语法结构，并且可以生成证明证书，解释每个词语的解释和句子结构如何计算意思。</li>
<li>results: 这篇论文的实验结果表明，使用这种方法可以正确地翻译多种来自popular textbook的英语描述 formal specifications into Lean formalizations，而无需大量修改词汇库。<details>
<summary>Abstract</summary>
Interactive proof assistants are computer programs carefully constructed to check a human-designed proof of a mathematical claim with high confidence in the implementation. However, this only validates truth of a formal claim, which may have been mistranslated from a claim made in natural language. This is especially problematic when using proof assistants to formally verify the correctness of software with respect to a natural language specification. The translation from informal to formal remains a challenging, time-consuming process that is difficult to audit for correctness.   This paper shows that it is possible to build support for specifications written in expressive subsets of natural language, within existing proof assistants, consistent with the principles used to establish trust and auditability in proof assistants themselves. We implement a means to provide specifications in a modularly extensible formal subset of English, and have them automatically translated into formal claims, entirely within the Lean proof assistant. Our approach is extensible (placing no permanent restrictions on grammatical structure), modular (allowing information about new words to be distributed alongside libraries), and produces proof certificates explaining how each word was interpreted and how the sentence's structure was used to compute the meaning.   We apply our prototype to the translation of various English descriptions of formal specifications from a popular textbook into Lean formalizations; all can be translated correctly with a modest lexicon with only minor modifications related to lexicon size.
</details>
<details>
<summary>摘要</summary>
交互证明助手是计算机程序，它们仔细构建，可以快速地检查人类设计的数学陈述的真实性。然而，这只有确认形式陈述的真实性，而不是自然语言中的陈述。这尤其是在使用证明助手来正式验证软件是否符合自然语言规范时，会出现问题。翻译自然语言中的陈述到形式语言仍然是一项困难的、耗时的任务，难以审核正确性。这篇论文展示了可以在现有的证明助手中支持基于表达ive subset of natural language的规范，并遵循证明助手自己的原则来建立信任和审核性。我们实现了一种方法，可以在Lean证明助手中提供表达ive subset of English的模块化可扩展的 формаль subsets，并自动将自然语言中的陈述翻译成形式索引。我们的方法是可扩展的（不会对语法结构做永久性的限制），可模块化（可以在库中分发信息），并生成证明证明，解释每个单词的解释和句子结构如何计算meaning。我们使用我们的原型将各种自然语言中的英语描述翻译成Lean形式化，所有可以正确地翻译，只需要一个小型词汇库，只需要一些相应的修改。
</details></li>
</ul>
<hr>
<h2 id="Automatic-and-Human-AI-Interactive-Text-Generation"><a href="#Automatic-and-Human-AI-Interactive-Text-Generation" class="headerlink" title="Automatic and Human-AI Interactive Text Generation"></a>Automatic and Human-AI Interactive Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03878">http://arxiv.org/abs/2310.03878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/na-mrata/3D-Animation">https://github.com/na-mrata/3D-Animation</a></li>
<li>paper_authors: Yao Dou, Philippe Laban, Claire Gardent, Wei Xu</li>
<li>for: 这篇论文主要研究了文本生成 tasks，具体来说是文本简化和修改任务，以提高文本的可读性和语言风格，而不改变文本的主要含义和长度。</li>
<li>methods: 这些任务使用了多种自然语言生成（NLG）技术，包括文本简化、重新译写、风格转换等，以达到提高文本可读性和语言风格的目的。</li>
<li>results: 研究人员通过不同的数据集、模型和评估方法来评估和提高文本生成模型的性能，并发现了一些新的技术和方法，如非回退式方法、大语言模型的提前定型、可学习度量和细致人类评估框架等，以提高文本生成的可读性和语言风格。<details>
<summary>Abstract</summary>
In this tutorial, we focus on text-to-text generation, a class of natural language generation (NLG) tasks, that takes a piece of text as input and then generates a revision that is improved according to some specific criteria (e.g., readability or linguistic styles), while largely retaining the original meaning and the length of the text. This includes many useful applications, such as text simplification, paraphrase generation, style transfer, etc. In contrast to text summarization and open-ended text completion (e.g., story), the text-to-text generation tasks we discuss in this tutorial are more constrained in terms of semantic consistency and targeted language styles. This level of control makes these tasks ideal testbeds for studying the ability of models to generate text that is both semantically adequate and stylistically appropriate. Moreover, these tasks are interesting from a technical standpoint, as they require complex combinations of lexical and syntactical transformations, stylistic control, and adherence to factual knowledge, -- all at once. With a special focus on text simplification and revision, this tutorial aims to provide an overview of the state-of-the-art natural language generation research from four major aspects -- Data, Models, Human-AI Collaboration, and Evaluation -- and to discuss and showcase a few significant and recent advances: (1) the use of non-retrogressive approaches; (2) the shift from fine-tuning to prompting with large language models; (3) the development of new learnable metric and fine-grained human evaluation framework; (4) a growing body of studies and datasets on non-English languages; (5) the rise of HCI+NLP+Accessibility interdisciplinary research to create real-world writing assistant systems.
</details>
<details>
<summary>摘要</summary>
在这个教程中，我们关注文本到文本生成任务，这是自然语言生成（NLG）任务的一种，它从一段文本输入中生成一个改进后的文本，保持原始意思和长度，同时符合某些特定的标准（如可读性或语言风格）。这包括了许多有用的应用，如文本简化、重叠生成、风格传递等。与文本概要和开放式文本完成（如故事）不同，文本到文本生成任务在Semantic consistency和targeted language styles方面更加具有制约，这使得这些任务成为模型生成文本的semantic adequacy和风格适应能力的 идеальtestbed。此外，这些任务也具有技术上的挑战，需要复杂的词汇和语法变换、风格控制和事实知识的结合，全面来说。本教程将从数据、模型、人工智能合作和评估四个方面提供文本生成领域的现状报告，并讲解和展示一些最近的进步：（1）非退化方法的使用；（2）大语言模型的 Fine-tuning 到提示；（3）开发新的可学习度量和细化人类评估框架；（4）非英语语料的增长和应用；（5）人工智能+计算机科学+访问性研究的协同发展，以创造真实世界的写作助手系统。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-a-foundation-LLM-on-its-ability-to-re-label-structure-names-in-accordance-with-the-AAPM-TG-263-report"><a href="#Benchmarking-a-foundation-LLM-on-its-ability-to-re-label-structure-names-in-accordance-with-the-AAPM-TG-263-report" class="headerlink" title="Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report"></a>Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03874">http://arxiv.org/abs/2310.03874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Holmes, Lian Zhang, Yuzhen Ding, Hongying Feng, Zhengliang Liu, Tianming Liu, William W. Wong, Sujay A. Vora, Jonathan B. Ashman, Wei Liu<br>for: 本研究旨在使用大型自然语言模型（LLM）标准化 radiation oncology 领域中的结构名称，并为未来研究提供参考基准。methods: 本研究使用 Generative Pre-trained Transformer（GPT）-4 API 实现 DICOM 存储服务器，当接收到结构集 DICOM 文件时，GPT-4 会根据 American Association of Physicists in Medicine（AAPM）任务组（TG）-263 标准重新标注结构名称。选择了三个疾病位置：肾病、头颈部和胸部，对每个疾病类型，随机选择 150 名病人进行手动调整指令提示（分 batches of 50），并随机选择 50 名病人进行评估。results: 结果显示，肾病、头颈部和胸部疾病情况下的结构名称重新标注精度为 96.0%、98.5% 和 96.9%  соответственно。重新标注目标体部分的精度较低，除了肾病情况下的 100% 外，其他两个疾病类型的平均精度分别为 93.1% 和 91.1%。<details>
<summary>Abstract</summary>
Purpose: To introduce the concept of using large language models (LLMs) to re-label structure names in accordance with the American Association of Physicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish a benchmark for future studies to reference.   Methods and Materials: The Generative Pre-trained Transformer (GPT)-4 application programming interface (API) was implemented as a Digital Imaging and Communications in Medicine (DICOM) storage server, which upon receiving a structure set DICOM file, prompts GPT-4 to re-label the structure names of both target volumes and normal tissues according to the AAPM TG-263. Three disease sites, prostate, head and neck, and thorax were selected for evaluation. For each disease site category, 150 patients were randomly selected for manually tuning the instructions prompt (in batches of 50) and 50 patients were randomly selected for evaluation. Structure names that were considered were those that were most likely to be relevant for studies utilizing structure contours for many patients.   Results: The overall re-labeling accuracy of both target volumes and normal tissues for prostate, head and neck, and thorax cases was 96.0%, 98.5%, and 96.9% respectively. Re-labeling of target volumes was less accurate on average except for prostate - 100%, 93.1%, and 91.1% respectively.   Conclusions: Given the accuracy of GPT-4 in re-labeling structure names of both target volumes and normal tissues as presented in this work, LLMs are poised to be the preferred method for standardizing structure names in radiation oncology, especially considering the rapid advancements in LLM capabilities that are likely to continue.
</details>
<details>
<summary>摘要</summary>
目的：介绍使用大语言模型（LLM）来按照美国物理学会医学分会（AAPM）任务组（TG）263标准重新标注结构名称，并建立参考基准 для未来研究。方法和材料：使用生成预训练的变换器（GPT）4应用程序编程接口（API）将其作为数字医疗影像和通信（DICOM）存储服务器，当接收到结构集DICOM文件时，请求GPT-4将结构名称重新标注为符合AAPM TG-263标准。选择了三个疾病站点，即肾病、头颈部和胸部，进行评估。每个疾病站点类别中随机选择50名病人进行手动调整说明提示（batches of 50），并随机选择50名病人进行评估。考虑重新标注的结构名称是最有可能被用于多个患者的研究中的结构辐射。结果：评估结果显示，肾病、头颈部和胸部疾病 случа例中结构名称重新标注的精度为96.0%、98.5%和96.9% соответственно。重新标注目标体部分的精度较低，除了肾病外，其中的精度为100%、93.1%和91.1%分别。结论：根据这些结果，GPT-4在重新标注结构名称方面的精度很高，LLMs可能成为医学物理学会标准化结构名称的首选方法，特别是考虑到LLM技术的快速发展，未来的进步也可能会继续。
</details></li>
</ul>
<hr>
<h2 id="Modular-Speech-to-Text-Translation-for-Zero-Shot-Cross-Modal-Transfer"><a href="#Modular-Speech-to-Text-Translation-for-Zero-Shot-Cross-Modal-Transfer" class="headerlink" title="Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer"></a>Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03724">http://arxiv.org/abs/2310.03724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul-Ambroise Duquenne, Holger Schwenk, Benoît Sagot</li>
<li>for: 这种方法可以提高 speech-to-text 翻译的竞争力。</li>
<li>methods: 使用独立的编码器和解码器，通过共享固定大小表示进行组合。</li>
<li>results: 在零shot cross-modal speech translation中获得了显著改善，甚至超过了基于 XLSR 的超vised方法。<details>
<summary>Abstract</summary>
Recent research has shown that independently trained encoders and decoders, combined through a shared fixed-size representation, can achieve competitive performance in speech-to-text translation. In this work, we show that this type of approach can be further improved with multilingual training. We observe significant improvements in zero-shot cross-modal speech translation, even outperforming a supervised approach based on XLSR for several languages.
</details>
<details>
<summary>摘要</summary>
latest research has shown that independently trained encoders and decoders, combined through a shared fixed-size representation, can achieve competitive performance in speech-to-text translation. In this work, we show that this type of approach can be further improved with multilingual training. We observe significant improvements in zero-shot cross-modal speech translation, even outperforming a supervised approach based on XLSR for several languages.Note:* "speech-to-text translation" is translated as "语音至文本翻译" (yǔyīn zhì wén tiě bīng yì)* "independently trained encoders and decoders" is translated as "独立训练的编码器和解码器" (dāng zhì xiǎng zhì de biān mǎo yǔ jiě mǎo yì)* "combined through a shared fixed-size representation" is translated as "通过共享固定大小的表示" (tōng guò gòng xiāng gòng dào zhì yǐ jīng)* "multilingual training" is translated as "多语言训练" (duō yǔ yán xiǎng zhì)* "zero-shot cross-modal speech translation" is translated as "零发射跨模态语音翻译" (líng fā shè qū mó dài yǔ yīn bīng yì)* "outperforming a supervised approach based on XLSR" is translated as "超过基于 XLSR 的指导方法" (chāo guò jī bù xīn xiǎng yǐ jīng fāng mó)
</details></li>
</ul>
<hr>
<h2 id="A-Long-Way-to-Go-Investigating-Length-Correlations-in-RLHF"><a href="#A-Long-Way-to-Go-Investigating-Length-Correlations-in-RLHF" class="headerlink" title="A Long Way to Go: Investigating Length Correlations in RLHF"></a>A Long Way to Go: Investigating Length Correlations in RLHF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03716">http://arxiv.org/abs/2310.03716</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prasanns/rlhf-length-biases">https://github.com/prasanns/rlhf-length-biases</a></li>
<li>paper_authors: Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett</li>
<li>for: 本研究使用强制学习从人类反馈（RLHF）将大型语言模型Alignment。</li>
<li>methods: 使用开源的偏好数据集和奖励模型进行更广泛的实验，以使系统更加”有用”，如网页问答、概要、多转Dialogue等任务。</li>
<li>results: RLHF通常能够提高模型的性能，但是研究发现，RLHF的主要原因是提高输出长度。在调整奖励模型的时候，可以通过调整输出长度来提高奖励得分。<details>
<summary>Abstract</summary>
Great successes have been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models. Open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more "helpful" for tasks like web question answering, summarization, and multi-turn dialogue. When optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF's reported improvements in these settings. First, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. We then explore interventions during both RL and reward model learning to see if we can achieve the same downstream improvements as RLHF without increasing length. While our interventions mitigate length increases, they aren't uniformly effective across settings. Furthermore, we find that even running RLHF with a reward based solely on length can reproduce most of the downstream improvements over the initial policy model, showing that reward models in these settings have a long way to go.
</details>
<details>
<summary>摘要</summary>
很大的成功有被报告使用从人类反馈学习（RLHF）调整大型语言模型。开源的偏好数据和奖励模型使得更多的实验可以进行 beyond 通用的对话设定，特别是对 tasks like 网页问答、摘要和多轮对话进行调整。当优化为“帮助”时，RLHF 被观察到 consistently 驱动模型生成更长的输出。本文证明了优化响应length 是 RLHF 报告的改善的重要因素。我们首先研究了奖励和长度之间的关系，发现长度和奖励Score 间存在强正相关，并且改善奖励Score 的主要原因是将输出长度的分布shift。然后我们在RL和奖励模型学习过程中进行了 intervene ，以看看我们可以在不增加长度的情况下 achieving 同等的下游改善。我们发现了一些 intervene 可以 mitigate length increases，但不是uniformly effective across settings。此外，我们发现了在Running RLHF  with a reward based solely on length 可以重现大部分的下游改善，显示奖励模型在这些设定下有很长的方向。
</details></li>
</ul>
<hr>
<h2 id="DecoderLens-Layerwise-Interpretation-of-Encoder-Decoder-Transformers"><a href="#DecoderLens-Layerwise-Interpretation-of-Encoder-Decoder-Transformers" class="headerlink" title="DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers"></a>DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03686">http://arxiv.org/abs/2310.03686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, Jaap Jumelet</li>
<li>for: 这个论文旨在帮助解释转换器模型的内部状态，尤其是encoder-decoder模型。</li>
<li>methods: 这个方法叫做DecoderLens，它基于LogitLens，允许解码器跨attend到经过encoder层的中间表示，而不是使用最终的encoder输出。这种方法可以将不可解 interpreted vector表示转换为可读的字符或符号序列。</li>
<li>results: 应用DecoderLens于问答、逻辑推理、语音识别和机器翻译模型后，发现这些模型在低或中间层解决了一些具体的子任务，从而新的推测了encoder组件内部信息的流动。<details>
<summary>Abstract</summary>
In recent years, many interpretability methods have been proposed to help interpret the internal states of Transformer-models, at different levels of precision and complexity. Here, to analyze encoder-decoder Transformers, we propose a simple, new method: DecoderLens. Inspired by the LogitLens (for decoder-only Transformers), this method involves allowing the decoder to cross-attend representations of intermediate encoder layers instead of using the final encoder output, as is normally done in encoder-decoder models. The method thus maps previously uninterpretable vector representations to human-interpretable sequences of words or symbols. We report results from the DecoderLens applied to models trained on question answering, logical reasoning, speech recognition and machine translation. The DecoderLens reveals several specific subtasks that are solved at low or intermediate layers, shedding new light on the information flow inside the encoder component of this important class of models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GoLLIE-Annotation-Guidelines-improve-Zero-Shot-Information-Extraction"><a href="#GoLLIE-Annotation-Guidelines-improve-Zero-Shot-Information-Extraction" class="headerlink" title="GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction"></a>GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03668">http://arxiv.org/abs/2310.03668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hitz-zentroa/gollie">https://github.com/hitz-zentroa/gollie</a></li>
<li>paper_authors: Oscar Sainz, Iker García-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, Eneko Agirre<br>for: 这篇论文旨在提高无seen任务泛化的大语言模型（LLMs）的表现，具体来说是在信息提取（IE）任务上。methods: 该论文提出了一种基于指南的大语言模型（GoLLIE），通过遵循指南来提高无seen任务的泛化表现。results: 实验表明，GoLLIE可以成功地遵循未看过的指南，并在无seen任务上表现出优于之前的尝试。减少细节指南的研究也表明了指南的重要性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）结合指南调整已经在无seen任务中做出了 significi cant进步，但在信息提取（IE）方面表现不佳，落后于任务特定模型。通常，IE任务被特定的注释指南描述，这些指南给出了人类的示例。过去尝试使用这些信息来优化模型的尝试都失败了，即使使用最大化模型。在这篇论文中，我们提出了GoLLIE（指南遵循的大语言模型 дляIE），一个能够通过遵循未看过的指南来提高零shot结果的IE任务。经验证明，GoLLIE能够遵循未看过的指南，并且在无seen任务中表现出色。ablation研究表明，详细的指南是关键获得好结果。
</details></li>
</ul>
<hr>
<h2 id="TRAM-Bridging-Trust-Regions-and-Sharpness-Aware-Minimization"><a href="#TRAM-Bridging-Trust-Regions-and-Sharpness-Aware-Minimization" class="headerlink" title="TRAM: Bridging Trust Regions and Sharpness Aware Minimization"></a>TRAM: Bridging Trust Regions and Sharpness Aware Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03646">http://arxiv.org/abs/2310.03646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tomsherborne/tram_optimizer">https://github.com/tomsherborne/tram_optimizer</a></li>
<li>paper_authors: Tom Sherborne, Naomi Saphra, Pradeep Dasigi, Hao Peng</li>
<li>for: 提高领域跨越和表示通用性</li>
<li>methods: 使用信任区 bounds  Inform SAM-style  regularizers，并提出 Trust Region Aware Minimization（TRAM）算法，用于优化极小化函数和有用的表示，而不会忘记预训练结构</li>
<li>results: TRAM 在跨区域语言模型和cross-语言传输中表现出色，并且超过了锐度感知和信任区基于优化方法。TRAM 成为训练通用模型的新标准，需要最小额外计算。<details>
<summary>Abstract</summary>
By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer. Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup. To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training. We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces. We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure. We find that TRAM outperforms both sharpness-aware and trust region-based optimization methods on cross-domain language modeling and cross-lingual transfer, where robustness to domain transfer and representation generality are critical for success. TRAM establishes a new standard in training generalizable models with minimal additional computation.
</details>
<details>
<summary>摘要</summary>
通过减少参数空间中折枝的弯曲率，锐度意识化最小化（SAM）得到了广泛的鲁棒性改进。而不是关注参数，这项工作强调了投入的表示空间中的转移性。为了促进保留转移性的表示，我们考虑了信任区域基于的练习方法，该方法利用任务特定技能而不忘记任务无关的表示。我们将参数空间和表示空间的平滑方法统一到了信任区域约束中，以便在SAM风格的正则化中使用信任区域约束。我们提出了信任区域意识化最小化（TRAM）算法，它在微调设置中优化了平均陡峭和有用的表示，而无需忘记先验结构。我们发现TRAM在跨频道语言模型和跨语言传输中表现出色，其中鲁棒性和表示总体性是成功的关键因素。TRAM设置了训练普适模型的新标准，并且增加了最小的额外计算。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Self-Supervised-Speech-Representations-for-Indigenous-American-Languages"><a href="#Evaluating-Self-Supervised-Speech-Representations-for-Indigenous-American-Languages" class="headerlink" title="Evaluating Self-Supervised Speech Representations for Indigenous American Languages"></a>Evaluating Self-Supervised Speech Representations for Indigenous American Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03639">http://arxiv.org/abs/2310.03639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Chen Chen, William Chen, Rodolfo Zevallos, John E. Ortega</li>
<li>for: 这 paper 是为了研究自动语音识别（ASR）领域中的自我监督学习（SSL）技术。</li>
<li>methods: 这 paper 使用了现有的大规模 SSL 模型，对 Quechua 语言和其他六种原住民语言进行了低资源 ASR 测试。</li>
<li>results: 结果表明，使用现有的大规模 SSL 模型可以在 Quechua 语言和其他原住民语言的低资源 ASR 中实现出色的表现。<details>
<summary>Abstract</summary>
The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.
</details>
<details>
<summary>摘要</summary>
“自动监督学习应用于语音表示学习领域已经吸引了相当多的关注，因为它可以承载大量的无标注数据。然而，许多进步，包括预训练和下游评估，都集中在英语之上，只有一些模型考虑了其他语言，而几乎没有考虑过传统语言。在我们的ASRU 2023 ML-SUPERB挑战提交中，我们提供了一个Quechua语言的ASR数据集，同时也评估了6种其他原住民语言，如Guarani和Bribri的低资源ASR。我们的结果显示了现有大规模SSL模型在实际数据上表现了惊人的好几何性。”Note: "ASR" stands for "Automatic Speech Recognition", "SSL" stands for "Self-Supervised Learning", and "ML-SUPERB" is a challenge for multilingual speech recognition.
</details></li>
</ul>
<hr>
<h2 id="Redefining-Digital-Health-Interfaces-with-Large-Language-Models"><a href="#Redefining-Digital-Health-Interfaces-with-Large-Language-Models" class="headerlink" title="Redefining Digital Health Interfaces with Large Language Models"></a>Redefining Digital Health Interfaces with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03560">http://arxiv.org/abs/2310.03560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fergus Imrie, Paulius Rauba, Mihaela van der Schaar</li>
<li>for: 这篇论文的目的是探讨如何使用大语言模型（LLMs）提高医疗服务的提供。</li>
<li>methods: 这篇论文使用了外部工具与临床医生之间的接口，以提高数字医疗工具和人工智能模型的实用性和实际效果。</li>
<li>results: 论文通过使用外部工具，解决了使用LLMs在临床设置中的问题，如幻觉。同时，论文还提供了卡ди奥vascular疾病和diabetes风险预测的例子，展示了这种新的接口的优势。<details>
<summary>Abstract</summary>
Digital health tools have the potential to significantly improve the delivery of healthcare services. However, their use remains comparatively limited due, in part, to challenges surrounding usability and trust. Recently, Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare. Directly applying LLMs in clinical settings is not straightforward, with LLMs susceptible to providing inconsistent or nonsensical answers. We demonstrate how LLMs can utilize external tools to provide a novel interface between clinicians and digital technologies. This enhances the utility and practical impact of digital healthcare tools and AI models while addressing current issues with using LLM in clinical settings such as hallucinations. We illustrate our approach with examples from cardiovascular disease and diabetes risk prediction, highlighting the benefit compared to traditional interfaces for digital tools.
</details>
<details>
<summary>摘要</summary>
“数字健康工具有可能提高医疗服务的提供，但其使用仍然相对有限，主要因为使用困难和信任问题。最近，大型语言模型（LLM）在医疗领域的应用已经具有广泛的潜在应用。直接在临床设置中使用LLM并不直接，LLM容易提供不一致或无意义的答案。我们示示了如何使用外部工具将LLM与临床技术相连，从而提高数字医疗工具和AI模型的实用性和实际效果，同时解决现有的LLM在临床设置中的问题，如投射。我们通过心血管疾病和 диабе层诊断预测的例子示出了我们的方法的优势，比传统界面更加有利。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PrIeD-KIE-Towards-Privacy-Preserved-Document-Key-Information-Extraction"><a href="#PrIeD-KIE-Towards-Privacy-Preserved-Document-Key-Information-Extraction" class="headerlink" title="PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction"></a>PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03777">http://arxiv.org/abs/2310.03777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saifullah Saifullah, Stefan Agne, Andreas Dengel, Sheraz Ahmed</li>
<li>for: 本研究旨在开发private Key Information Extraction（KIE）系统，通过利用大规模预训练文档基础模型，并结合 diferencial privacy（DP）、联合学习（FL）和不同的参数 Settings。</li>
<li>methods: 本研究使用大文档基础模型进行KIE任务的私有化设置，并通过对六个 benchmark datasets（FUNSD、CORD、SROIE、WildReceipts、XFUND、DOCILE）进行广泛的实验，以证明这些大文档基础模型可以在私有设置下具有充分的性能，同时保持强的隐私保障。</li>
<li>results: 本研究通过分析不同的训练和模型参数对模型性能的影响，提出了简单 yet effective的指南，以实现KIE任务下的优质隐私融合。此外，本研究还引入了FeAm-DP算法，可以有效地在多个客户端 Federated 环境中实现globally DP的扩展。通过对不同客户端和隐私设置进行广泛的评估，本研究证明了FeAm-DP算法可以在多个参与客户端的情况下保持相同的性能和隐私保障。<details>
<summary>Abstract</summary>
In this paper, we introduce strategies for developing private Key Information Extraction (KIE) systems by leveraging large pretrained document foundation models in conjunction with differential privacy (DP), federated learning (FL), and Differentially Private Federated Learning (DP-FL). Through extensive experimentation on six benchmark datasets (FUNSD, CORD, SROIE, WildReceipts, XFUND, and DOCILE), we demonstrate that large document foundation models can be effectively fine-tuned for the KIE task under private settings to achieve adequate performance while maintaining strong privacy guarantees. Moreover, by thoroughly analyzing the impact of various training and model parameters on model performance, we propose simple yet effective guidelines for achieving an optimal privacy-utility trade-off for the KIE task under global DP. Finally, we introduce FeAm-DP, a novel DP-FL algorithm that enables efficiently upscaling global DP from a standalone context to a multi-client federated environment. We conduct a comprehensive evaluation of the algorithm across various client and privacy settings, and demonstrate its capability to achieve comparable performance and privacy guarantees to standalone DP, even when accommodating an increasing number of participating clients. Overall, our study offers valuable insights into the development of private KIE systems, and highlights the potential of document foundation models for privacy-preserved Document AI applications. To the best of authors' knowledge, this is the first work that explores privacy preserved document KIE using document foundation models.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了如何开发 private Key Information Extraction（KIE）系统，通过利用大型预训练文档基础模型、分布式学习（FL）和不同化分布式学习（DP-FL）等技术，以保持强大的隐私保证。通过对六个benchmark数据集（FUNSD、CORD、SROIE、WildReceipts、XFUND和DOCILE）进行广泛的实验，我们证明了大型文档基础模型可以在private Setting下高效地 Fine-tune  для KIE任务，以实现适当的性能while maintaining strong privacy guarantees。此外，我们对模型训练和参数的影响进行了系统的分析，并提出了简洁 yet effective的指南，以实现KIE任务下的优质隐私融合。最后，我们引入了FeAm-DP算法，一种基于DP-FL的新算法，可以高效地将全局DP从独立上下文扩展到多客户联邦环境。我们对该算法进行了广泛的评估，并在不同的客户和隐私设置下示出其可以实现相同的性能和隐私保证。总之，我们的研究提供了开发private KIE系统的有价值的信息，并高调了文档基础模型的隐私保护能力，用于隐私保护文档AI应用。作者认为，这是开发private KIE系统的首次研究。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Multi-document-Summarization-Coverage-Coherence-Intuitive-Policy-with-Large-Language-Model-Based-Rewards"><a href="#Controllable-Multi-document-Summarization-Coverage-Coherence-Intuitive-Policy-with-Large-Language-Model-Based-Rewards" class="headerlink" title="Controllable Multi-document Summarization: Coverage &amp; Coherence Intuitive Policy with Large Language Model Based Rewards"></a>Controllable Multi-document Summarization: Coverage &amp; Coherence Intuitive Policy with Large Language Model Based Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03473">http://arxiv.org/abs/2310.03473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Litton J Kurisinkel, Nancy F chen</li>
<li>for: 这项研究旨在提出一种可控的多文摘要方法，使用大型自然语言模型（LLM）来优化文本输入，提高报道的可读性。</li>
<li>methods: 该方法使用了一种新的覆盖度和凝聚度直观策略来训练可控内容提取方案，该策略通过一个通过训练的 LLM 得到奖励。</li>
<li>results: 根据ROUGE指标的评估和人工评估，该方法的result比baseline更高，在凝聚性方面也表现出优异。<details>
<summary>Abstract</summary>
Memory-efficient large language models are good at refining text input for better readability. However, controllability is a matter of concern when it comes to text generation tasks with long inputs, such as multi-document summarization. In this work, we investigate for a generic controllable approach for multi-document summarization that leverages the capabilities of LLMs to refine the text. In particular, we train a controllable content extraction scheme to extract the text that will be refined by an LLM. The scheme is designed with a novel coverage and coherence intuitive policy, which is duly rewarded by a passively trained LLM. Our approach yields competitive results in the evaluation using ROUGE metrics and outperforms potential baselines in coherence, as per human evaluation.
</details>
<details>
<summary>摘要</summary>
大型语言模型具有很好的缩写能力，可以为文本输入提高可读性。然而，在长输入文本生成任务中，控制性是一个关注的问题。在这种情况下，我们研究了一种通用可控的多文摘要方法，利用 LLMS 的能力来改进文本。我们培训了一种可控内容提取方案，通过一种新的覆盖率和吸引力感知策略来提取需要改进的文本。这种策略通过一个通过训练的 LLMS 得到的奖励。我们的方法在使用 ROUGE 指标进行评估中获得了竞争力的结果，并在人工评估中超过了可能的基准值。
</details></li>
</ul>
<hr>
<h2 id="The-North-System-for-Formosa-Speech-Recognition-Challenge-2023"><a href="#The-North-System-for-Formosa-Speech-Recognition-Challenge-2023" class="headerlink" title="The North System for Formosa Speech Recognition Challenge 2023"></a>The North System for Formosa Speech Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03443">http://arxiv.org/abs/2310.03443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li-Wei Chen, Kai-Chen Cheng, Hung-Shin Lee</li>
<li>for: 这个研究旨在实现台湾闽南语（六善）自动词音识别。</li>
<li>methods: 这个系统有三个重要的 комponent：训练数据的获取、组合和使用; 模型的架构; 和硬件规格和运行统计。</li>
<li>results: 这个系统的示范已经在<a target="_blank" rel="noopener" href="https://asrvm.iis.sinica.edu.tw/hakka_sixian%E4%B8%AD%E5%85%AC%E5%BC%80%E3%80%82">https://asrvm.iis.sinica.edu.tw/hakka_sixian中公开。</a><details>
<summary>Abstract</summary>
This report provides a concise overview of the proposed North system, which aims to achieve automatic word/syllable recognition for Taiwanese Hakka (Sixian). The report outlines three key components of the system: the acquisition, composition, and utilization of the training data; the architecture of the model; and the hardware specifications and operational statistics. The demonstration of the system has been made public at https://asrvm.iis.sinica.edu.tw/hakka_sixian.
</details>
<details>
<summary>摘要</summary>
这份报告提供了北系自动词/音节识别系统的简洁概述，旨在实现台湾闽南话（六年）自动识别。报告介绍了系统的三个关键组成部分：训练数据的获取、组合和使用；模型的架构；以及硬件规格和运行统计。系统的示例已经在https://asrvm.iis.sinica.edu.tw/hakka_sixian上公开。Note: "北系" (Běixì) is a shortened form of "北部自动识别系统" (Běibù Zìdòng Shìbié Xìtsū) in Simplified Chinese, which means "North system" in English.
</details></li>
</ul>
<hr>
<h2 id="Neural-Language-Model-Pruning-for-Automatic-Speech-Recognition"><a href="#Neural-Language-Model-Pruning-for-Automatic-Speech-Recognition" class="headerlink" title="Neural Language Model Pruning for Automatic Speech Recognition"></a>Neural Language Model Pruning for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03424">http://arxiv.org/abs/2310.03424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Emili, Thiago Fraga-Silva, Ernest Pusateri, Markus Nußbaum-Thom, Youssef Oualil</li>
<li>for: 这paper研究了应用于Transformer基于神经网络语音识别模型的模型剔除方法，以提高自动语音识别的精度和速度。</li>
<li>methods: 本paper explore了三个剔除框架方面，namely criterion, method和scheduler，并分析了它们在精度和执行速度方面的贡献。据我们知道，这些大规模识别系统的深入分析未经报道在文献中。此外，我们还提出了一种适合逐步压缩模型的低级approximation方法。</li>
<li>results: 主要的结果包括：a) data-driven剔除在一些场景下超过了大小剔除的性能; b) 逐步剔除在目标模型更小时比一次性剔除更高的精度; c) 低级approximation在一定的压缩率下提供了最佳的贸易协议 между压缩和执行速度。<details>
<summary>Abstract</summary>
We study model pruning methods applied to Transformer-based neural network language models for automatic speech recognition. We explore three aspects of the pruning frame work, namely criterion, method and scheduler, analyzing their contribution in terms of accuracy and inference speed. To the best of our knowledge, such in-depth analyses on large-scale recognition systems has not been reported in the literature. In addition, we propose a variant of low-rank approximation suitable for incrementally compressing models, and delivering multiple models with varied target sizes. Among other results, we show that a) data-driven pruning outperforms magnitude-driven in several scenarios; b) incremental pruning achieves higher accuracy compared to one-shot pruning, especially when targeting smaller sizes; and c) low-rank approximation presents the best trade-off between size reduction and inference speed-up for moderate compression.
</details>
<details>
<summary>摘要</summary>
我们研究基于Transformer语言模型的模型剪辑方法，用于自动语音识别。我们分析了三个方面的剪辑框架，即标准、方法和调度器，对准确率和执行速度进行分析。根据我们所知，这种大规模识别系统的深入分析没有在文献中报道过。此外，我们还提出了适用于步骤式压缩模型的低级approximation方法，并实现了多个模型 TargetSize不同的多种模型。其中一些结果包括： a) 数据驱动剪辑比例驱动更高的性能在一些场景中表现出色; b) 逐步剪辑比一次剪辑更高的准确率，特别是targetSize更小的场景中; c) 低级approximation具有最佳的剪辑尺度和执行速度之间的折衔，尤其是在moderate压缩情况下。
</details></li>
</ul>
<hr>
<h2 id="LLM-Based-Multi-Document-Summarization-Exploiting-Main-Event-Biased-Monotone-Submodular-Content-Extraction"><a href="#LLM-Based-Multi-Document-Summarization-Exploiting-Main-Event-Biased-Monotone-Submodular-Content-Extraction" class="headerlink" title="LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction"></a>LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03414">http://arxiv.org/abs/2310.03414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Litton J Kurisinkel, Nancy F. Chen</li>
<li>for: 主要目的是提供对新闻文摘中关键事件的 объектив概括，增强文摘的可信度和完整性。</li>
<li>methods: 采用提取-重写方法，通过主事新闻biased monotone-submodular函数选择内容，确保文摘具有充足的上下文和准确性。</li>
<li>results: 评估结果表明，本方法可以准确地捕捉新闻文摘中关键事件的核心信息，同时保持文摘的准确性和完整性。<details>
<summary>Abstract</summary>
Multi-document summarization is a challenging task due to its inherent subjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4 among DUC-2004 reference summaries. In this work, we aim to enhance the objectivity of news summarization by focusing on the main event of a group of related news documents and presenting it coherently with sufficient context. Our primary objective is to succinctly report the main event, ensuring that the summary remains objective and informative. To achieve this, we employ an extract-rewrite approach that incorporates a main-event biased monotone-submodular function for content selection. This enables us to extract the most crucial information related to the main event from the document cluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for rewriting the extracted content into a coherent text. The evaluation using objective metrics and human evaluators confirms the effectiveness of our approach, as it surpasses potential baselines, demonstrating excellence in both content coverage, coherence, and informativeness.
</details>
<details>
<summary>摘要</summary>
多文摘要是一项具有主观偏见的任务， Duc-2004 参考摘要的低 ROUGE-1 分数为 0.4 表明这一点。在这项工作中，我们想增强新闻摘要的 объектив性，通过关注一组相关新闻文档中的主要事件，并以充分的上下文进行摘要。我们的主要目标是简要报道主要事件，以确保摘要具有 objetivity 和信息性。为实现这一目标，我们采用了提取-重写方法，通过主要事件偏见的 monotone-submodular 函数进行内容选择。这使得我们可以从文档集中提取关键相关主要事件的信息。为确保准确性，我们使用了调整的语言模型（LLM）进行重写提取的内容，以确保摘要具有准确性和一致性。经过对象指标和人类评估者的评估，我们的方法得到了证明，它在内容覆盖率、一致性和信息性等方面具有优异性。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Hallucinations-in-Chinese-Large-Language-Models"><a href="#Evaluating-Hallucinations-in-Chinese-Large-Language-Models" class="headerlink" title="Evaluating Hallucinations in Chinese Large Language Models"></a>Evaluating Hallucinations in Chinese Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03368">http://arxiv.org/abs/2310.03368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiami2019/halluqa">https://github.com/xiami2019/halluqa</a></li>
<li>paper_authors: Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, Xipeng Qiu</li>
<li>for: 本研究提出了一个名为 HalluQA（中文幻视问答）的benchmark，用于量化中文大型语言模型中的幻视现象。</li>
<li>methods: 我们为HalluQA设计了450个精心设计的类 adversarial问题，覆盖多个领域，包括中国历史文化、习俗和社会现象。我们在建立HalluQA时考虑了两种幻视现象：模仿Falsehood和事实错误，并基于GLM-130B和ChatGPT construct adversarial samples。</li>
<li>results: 我们对24个大型语言模型进行了广泛的实验，发现18个模型的非幻视率低于50%。这表明HalluQA是非常具有挑战性的。我们分析了不同类型的模型中的主要幻视类型和其原因。此外，我们还讨论了不同类型的模型中哪些幻视类型应被优先顾及。<details>
<summary>Abstract</summary>
In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们建立了一个名为哈鲁QA（中文幻想问答）的标准测试集，用于衡量中文大语模型中的幻想现象。哈鲁QA包含450个精心设计的对抗问题，覆盖多个领域，并考虑了中国历史文化、习俗和社会现象。在建立哈鲁QA时，我们考虑了两种类型的幻想：模仿错误和事实错误，并基于GLM-130B和ChatGPT构建了对抗样本。为评估，我们设计了一种自动评估方法，使用GPT-4来判断模型输出是否幻想。我们对24个大语模型进行了广泛的实验，其中18个模型的非幻想率低于50%。这表明哈鲁QA是非常具有挑战性。我们分析了不同类型的模型中的主要幻想类型和其原因。此外，我们还讨论了不同类型的模型中应该优先级幻想的类型。
</details></li>
</ul>
<hr>
<h2 id="Reformulating-Domain-Adaptation-of-Large-Language-Models-as-Adapt-Retrieve-Revise"><a href="#Reformulating-Domain-Adaptation-of-Large-Language-Models-as-Adapt-Retrieve-Revise" class="headerlink" title="Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise"></a>Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03328">http://arxiv.org/abs/2310.03328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen wan, Yating Zhang, Yexiang Wang, Fei Cheng, Sadao Kurohashi</li>
<li>for: 提高中文法律领域中 GPT-4 的应用可能性</li>
<li>methods: 利用 adapt-retrieve-revise 过程来将小型 7B LLM 适应到目标领域，并让 GPT-4 评估证据并修改答案</li>
<li>results: 与直接使用 GPT-4 生成比较，在四个中文法律任务中获得了33.3%的提高率，并较两个更强的搜寻基于的基准值得15.4%和23.9%的提高率。<details>
<summary>Abstract</summary>
While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.   This paper introduces a simple and effective domain adaptation framework for GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process. The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain by continuing learning on in-domain data. When solving a task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answer will be used to \textbf{retrieve} supporting evidence candidates from an external in-domain knowledge base. Finally, the draft answer and retrieved evidence are concatenated into a whole prompt to let GPT-4 assess the evidence and \textbf{revise} the draft answer to generate the final answer.   Our proposal combines the advantages of the efficiency of adapting a smaller 7B model with the evidence-assessing capability of GPT-4 and effectively prevents GPT-4 from generating hallucinatory content. In the zero-shot setting of four Chinese legal tasks, our method improves accuracy by 33.3\% compared to the direct generation by GPT-4. When compared to two stronger retrieval-based baselines, our method outperforms them by 15.4\% and 23.9\%. Our code will be released
</details>
<details>
<summary>摘要</summary>
While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.   This paper introduces a simple and effective domain adaptation framework for GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process. The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain by continuing learning on in-domain data. When solving a task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answer will be used to \textbf{retrieve} supporting evidence candidates from an external in-domain knowledge base. Finally, the draft answer and retrieved evidence are concatenated into a whole prompt to let GPT-4 assess the evidence and \textbf{revise} the draft answer to generate the final answer.   Our proposal combines the advantages of the efficiency of adapting a smaller 7B model with the evidence-assessing capability of GPT-4 and effectively prevents GPT-4 from generating hallucinatory content. In the zero-shot setting of four Chinese legal tasks, our method improves accuracy by 33.3\% compared to the direct generation by GPT-4. When compared to two stronger retrieval-based baselines, our method outperforms them by 15.4\% and 23.9\%. Our code will be released.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Learning-Personalized-Story-Evaluation"><a href="#Learning-Personalized-Story-Evaluation" class="headerlink" title="Learning Personalized Story Evaluation"></a>Learning Personalized Story Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03304">http://arxiv.org/abs/2310.03304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, Yuandong Tian</li>
<li>for: 评估大语言模型（LLM）在开放式文本生成任务上的性能，因为数据污染、多维评价标准和评审人员个人偏好等因素而具有挑战性。</li>
<li>methods: 我们提议在不污染的开放式文本生成评价中模拟个性化。我们创建了两个新的数据集Per-MPST和Per-DOC用于个性化故事评价，并开发了一个个性化故事评价模型PERSE。PERSE通过对特定评审人员的几个示例评论进行学习，预测该评审人员对新文本输入的评价细节或细致比较。</li>
<li>results: 我们的实验结果表明，PERSE在Kendall相关度和对比性准确率上都高于GPT-4，提高了15.8%和13.7%。这两个数据集和代码将被发布。<details>
<summary>Abstract</summary>
While large language models (LLMs) have shown impressive results for more objective tasks such as QA and retrieval, it remains nontrivial to evaluate their performance on open-ended text generation for reasons including (1) data contamination; (2) multi-dimensional evaluation criteria; and (3) subjectiveness stemming from reviewers' personal preferences. To address such issues, we propose to model personalization in an uncontaminated open-ended generation assessment. We create two new datasets Per-MPST and Per-DOC for personalized story evaluation, by re-purposing existing datasets with proper anonymization and new personalized labels. We further develop a personalized story evaluation model PERSE to infer reviewer preferences and provide a personalized evaluation. Specifically, given a few exemplary reviews from a particular reviewer, PERSE predicts either a detailed review or fine-grained comparison in several aspects (such as interestingness and surprise) for that reviewer on a new text input. Experimental results show that PERSE outperforms GPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on pairwise preference prediction accuracy. Both datasets and code will be released.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在问答和搜寻任务上表现出色，但在开放式文本生成任务上仍然是一个挑战，因为存在以下问题：（1）数据污染；（2）多维评估标准；以及（3）由评论者们的个人偏好所致的主观性。为了解决这些问题，我们提议一个个人化的开放式生成评估方法。我们创建了两个新的数据集Per-MPST和Per-DOC，并发展了一个个人化的故事评估模型（PERSE），以推断评论者的偏好和提供个人化评估。具体来说，给定一些特定评论者的几则评论，PERSE预测该评论者对新文本的评估，包括有兴趣和惊喜等方面的细部评价。我们的实验结果显示，PERSE比GPT-4提高了15.8%的柯德兹通信相互评价准确性，并且比GPT-4提高了13.7%的对照选择准确性。我们将数据和代码发布。
</details></li>
</ul>
<hr>
<h2 id="A-New-Dialogue-Response-Generation-Agent-for-Large-Language-Models-by-Asking-Questions-to-Detect-User’s-Intentions"><a href="#A-New-Dialogue-Response-Generation-Agent-for-Large-Language-Models-by-Asking-Questions-to-Detect-User’s-Intentions" class="headerlink" title="A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User’s Intentions"></a>A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User’s Intentions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03293">http://arxiv.org/abs/2310.03293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Wu, Xiangqing Shen, Rui Xia</li>
<li>for: 提高对话响应生成的精度，使其更加准确地反映用户的意图。</li>
<li>methods: 使用大语言模型（LLM）来生成对话响应，并通过问题来探测用户的潜在意图，以及在域pecific知识库中搜索相关信息。</li>
<li>results: 在两个任务oriented对话任务（巫匠ofWikipedia和Holl-E）上，EDIT比其他LLMs表现出色，提高了对话响应的质量和准确性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), such as ChatGPT, have recently been applied to various NLP tasks due to its open-domain generation capabilities. However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete, and LLMs cannot update the latest knowledge in real-time. To tackle these issues, we propose a framework~\emph{using LLM to \textbf{E}nhance dialogue response generation by asking questions to \textbf{D}etect user's \textbf{I}mplicit in\textbf{T}entions} (\textbf{EDIT}). Firstly, EDIT generates open questions related to the dialogue context as the potential user's intention; Then, EDIT answers those questions by interacting with LLMs and searching in domain-specific knowledge bases respectively, and use LLMs to choose the proper answers to questions as extra knowledge; Finally, EDIT enhances response generation by explicitly integrating those extra knowledge. Besides, previous question generation works only focus on asking questions with answers in context. In order to ask open questions, we construct a Context-Open-Question (COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and Holl-E), EDIT outperformed other LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM），如ChatGPT，在近期被应用于各种自然语言处理任务中，它们具有开放预测能力。然而，在对话任务中，用户可能有隐藏的意图， LLM 可能会忽略这些隐藏的意图，导致生成的响应与用户的意图不符。其次， LLM 可能不能全面覆盖所有领域，在特定领域中，它们的知识可能是不完整的，无法在实时更新最新的知识。为解决这些问题，我们提出了一个框架，使用 LLM 来增强对话响应生成，通过问题来探测用户的隐藏意图。我们的方法包括以下三个步骤：首先，生成对话上下文相关的开放问题，作为用户的隐藏意图；然后，通过与 LLM 交互和在具体领域知识库中搜索，解决这些问题，并使用 LLM 选择最佳答案作为额外知识；最后，将这些额外知识Explicitly integrate into response generation。此外，以前的问题生成工作只是在 Context 中寻找答案。为了生成开放问题，我们构建了 Context-Open-Question（COQ）数据集。在两个任务（巫医与 Holl-E）上，EDIT 比其他 LLM 高效。
</details></li>
</ul>
<hr>
<h2 id="A-Formalism-and-Approach-for-Improving-Robustness-of-Large-Language-Models-Using-Risk-Adjusted-Confidence-Scores"><a href="#A-Formalism-and-Approach-for-Improving-Robustness-of-Large-Language-Models-Using-Risk-Adjusted-Confidence-Scores" class="headerlink" title="A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores"></a>A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03283">http://arxiv.org/abs/2310.03283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Shen, Mayank Kejriwal</li>
<li>for: This paper aims to provide a systematic understanding of the risks posed by large language models (LLMs) in natural language inference (NLI) tasks, and to propose a risk-centric evaluation framework and a risk-adjusted calibration method to mitigate these risks.</li>
<li>methods: The paper defines and formalizes two types of risk in LLMs, decision risk and composite risk, and proposes four novel metrics for assessing these risks in both in-domain and out-of-domain settings. The proposed risk-adjusted calibration method, called DwD, helps LLMs minimize these risks in an overall NLI architecture.</li>
<li>results: The paper presents detailed experiments using four NLI benchmarks, three baselines, and two LLMs, including ChatGPT, to demonstrate the practical utility of the evaluation framework and the efficacy of DwD in reducing decision and composite risk. For instance, the paper shows that DwD can help an underlying LLM address an extra 20.1% of low-risk inference tasks and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly without risk adjustment.<details>
<summary>Abstract</summary>
Large Language Models (LLMs), such as ChatGPT, have achieved impressive milestones in natural language processing (NLP). Despite their impressive performance, the models are known to pose important risks. As these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed. In this paper, we define and formalize two distinct types of risk: decision risk and composite risk. We also propose a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings. Finally, we propose a risk-adjusted calibration method called DwD for helping LLMs minimize these risks in an overall NLI architecture. Detailed experiments, using four NLI benchmarks, three baselines and two LLMs, including ChatGPT, show both the practical utility of the evaluation framework, and the efficacy of DwD in reducing decision and composite risk. For instance, when using DwD, an underlying LLM is able to address an extra 20.1% of low-risk inference tasks (but which the LLM erroneously deems high-risk without risk adjustment) and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM），如ChatGPT，在自然语言处理（NLP）中取得了吸引人的成绩。尽管它们的表现很出色，但这些模型也存在重要的风险。在这些模型被实际应用时，对它们在不同任务上的风险进行系统性的理解是非常重要。在这篇论文中，我们定义和正式化了两种不同的风险类型：决策风险和复杂风险。我们还提出了一种风险中心的评估框架，以及四种新的评估指标，用于评估 LLM 在这些风险上的表现。最后，我们提出了一种名为 DwD 的风险补偿方法，用于帮助 LLM 在整体 NLI 架构中减少这些风险。我们的实验，使用四个 NLI benchmark，三个基eline和两个 LLM，包括 ChatGPT，显示了评估框架的实用性，以及 DwD 在减少决策风险和复杂风险方面的效果。例如，使用 DwD，一个基eline LLM 可以处理更多的低风险推理任务（原本 LLM 错误地认为高风险），并且可以跳过更多的高风险任务（ LLM 会错误地答案）。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Alternative-Feature-Extraction-Pipelines-For-Clinical-Note-Phenotyping"><a href="#Investigating-Alternative-Feature-Extraction-Pipelines-For-Clinical-Note-Phenotyping" class="headerlink" title="Investigating Alternative Feature Extraction Pipelines For Clinical Note Phenotyping"></a>Investigating Alternative Feature Extraction Pipelines For Clinical Note Phenotyping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03772">http://arxiv.org/abs/2310.03772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neil Daniel</li>
<li>for: 这个论文的目的是提出一种新的方法来提取医疗记录中的医学特征。</li>
<li>methods: 这个方法使用了ScispaCy和不同的超参数进行医学特征的提取，然后使用不同的支持学习模型来关联病种的存在与患者特征。</li>
<li>results: 这个研究发现，这种新方法可以减少计算时间，并且可以检测不在医疗记录中的病种。相比之下，使用BERT和LSTM的方法有较高的计算时间和较低的准确率。<details>
<summary>Abstract</summary>
A common practice in the medical industry is the use of clinical notes, which consist of detailed patient observations. However, electronic health record systems frequently do not contain these observations in a structured format, rendering patient information challenging to assess and evaluate automatically. Using computational systems for the extraction of medical attributes offers many applications, including longitudinal analysis of patients, risk assessment, and hospital evaluation. Recent work has constructed successful methods for phenotyping: extracting medical attributes from clinical notes. BERT-based models can be used to transform clinical notes into a series of representations, which are then condensed into a single document representation based on their CLS embeddings and passed into an LSTM (Mulyar et al., 2020). Though this pipeline yields a considerable performance improvement over previous results, it requires extensive convergence time. This method also does not allow for predicting attributes not yet identified in clinical notes.   Considering the wide variety of medical attributes that may be present in a clinical note, we propose an alternative pipeline utilizing ScispaCy (Neumann et al., 2019) for the extraction of common diseases. We then train various supervised learning models to associate the presence of these conditions with patient attributes. Finally, we replicate a ClinicalBERT (Alsentzer et al., 2019) and LSTM-based approach for purposes of comparison. We find that alternative methods moderately underperform the replicated LSTM approach. Yet, considering a complex tradeoff between accuracy and runtime, in addition to the fact that the alternative approach also allows for the detection of medical conditions that are not already present in a clinical note, its usage may be considered as a supplement to established methods.
</details>
<details>
<summary>摘要</summary>
医疗行业常用临床笔记，其中包含细致的患者观察记录。然而，电子健康记录系统通常不会将这些观察记录structured format中，使得患者信息具有自动评估和评估的挑战。使用计算机系统进行医学属性提取有很多应用，包括长期分析患者、风险评估和医院评估。最近的工作已经建立了成功的方法，用于从临床笔记中提取医学属性。BERT基于模型可以将临床笔记转换为一系列表示，然后将这些表示condensed into a single document representation based on their CLS embeddings，并将其传递给LSTM（Mulyar et al., 2020）。虽然这个管道可以提高性能，但它需要广泛的对接时间。此外，这种方法不能预测没有在临床笔记中出现的医学属性。Considering the wide variety of medical attributes that may be present in a clinical note, we propose an alternative pipeline utilizing ScispaCy (Neumann et al., 2019) for the extraction of common diseases. We then train various supervised learning models to associate the presence of these conditions with patient attributes. Finally, we replicate a ClinicalBERT (Alsentzer et al., 2019) and LSTM-based approach for purposes of comparison. We find that alternative methods moderately underperform the replicated LSTM approach. Yet, considering a complex tradeoff between accuracy and runtime, in addition to the fact that the alternative approach also allows for the detection of medical conditions that are not already present in a clinical note, its usage may be considered as a supplement to established methods.
</details></li>
</ul>
<hr>
<h2 id="InstructProtein-Aligning-Human-and-Protein-Language-via-Knowledge-Instruction"><a href="#InstructProtein-Aligning-Human-and-Protein-Language-via-Knowledge-Instruction" class="headerlink" title="InstructProtein: Aligning Human and Protein Language via Knowledge Instruction"></a>InstructProtein: Aligning Human and Protein Language via Knowledge Instruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03269">http://arxiv.org/abs/2310.03269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, Huajun Chen</li>
<li>for: 这个研究旨在推动大语言模型（LLM）在生物sequences的理解，例如蛋白质，并提高其在生物语言和人工语言之间的沟通能力。</li>
<li>methods: 这个研究使用了两种方法：一是将蛋白质序列作为输入，预测其文本功能描述；二是使用自然语言来给蛋白质序列进行生成。</li>
<li>results: 实验结果显示，InstructProtein比现有的LLM出色地表现，对于 bidirectional protein-text生成任务具有优秀的表现。此外，InstructProtein可以实现蛋白质功能预测和序列设计，帮助bridging蛋白质和人工语言理解之间的距离。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing annotation imbalance and instruction deficits in existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover, InstructProtein serves as a pioneering step towards text-based protein function prediction and sequence design, effectively bridging the gap between protein and human language understanding.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经革命化自然语言处理领域，但它们在蛋白序列上的理解 still falls short. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing annotation imbalance and instruction deficits in existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover, InstructProtein serves as a pioneering step towards text-based protein function prediction and sequence design, effectively bridging the gap between protein and human language understanding.
</details></li>
</ul>
<hr>
<h2 id="Unlock-Predictable-Scaling-from-Emergent-Abilities"><a href="#Unlock-Predictable-Scaling-from-Emergent-Abilities" class="headerlink" title="Unlock Predictable Scaling from Emergent Abilities"></a>Unlock Predictable Scaling from Emergent Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03262">http://arxiv.org/abs/2310.03262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ShengdingHu/PredictableScaling">https://github.com/ShengdingHu/PredictableScaling</a></li>
<li>paper_authors: Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, Zhiyuan Liu, Maosong Sun</li>
<li>for: 了解大型自然语言模型（LLM）的科学级别扩大的缺点和挑战。</li>
<li>methods: 提出了一种新的评估策略——PassUntil，通过大量的扫描阶段来测试模型的性能。</li>
<li>results: 发现小型模型具有重要的任务性能改进，这些改进不被传统的评估策略所捕捉。通过PassUntil评估策略，发现任务性能随模型大小增长的准确性有限，并提出了一种新的emergent能力定义，以推翻一种流行的多步逻辑假设。<details>
<summary>Abstract</summary>
The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line with established scaling law; yet no scaling law for task has been established and the task performances are far from predictable during scaling. Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ``emergent abilities''. In this study, we discover that small models, although they exhibit minor performance, demonstrate critical and consistent task performance improvements that are not captured by conventional evaluation strategies due to insufficient measurement resolution. To measure such improvements, we introduce PassUntil, an evaluation strategy through massive sampling in the decoding phase. We conduct quantitative investigations into the scaling law of task performance. Firstly, a strict task scaling law is identified, enhancing the predictability of task performances. Remarkably, we are able to predict the performance of the 2.4B model on code generation with merely 0.05\% deviation before training starts. Secondly, underpinned by PassUntil, we observe concrete evidence of emergent abilities and ascertain that they are not in conflict with the continuity of performance improvement. Their semblance to break-through is that their scaling curve cannot be fitted by standard scaling law function. We then introduce a mathematical definition for the emergent abilities. Through the definition, we refute a prevalent ``multi-step reasoning hypothesis'' regarding the genesis of emergent abilities and propose a new hypothesis with a satisfying fit to the observed scaling curve.
</details>
<details>
<summary>摘要</summary>
科学级大语言模型（LLM）的扩大 Properties需要一个全面的理解。然而，现有的文献中的扩大 Properties只提供了一个不完整的答案：优化损失随模型大小减少，与已知的扩大法律一致；然而，没有任何任务扩大法律，并且任务性能并不是预测可能的，任务性能通常在小模型上显示微增长，直到模型大小超过一个阈值，然后快速增长，这是“emergent abilities”的表现。在这项研究中，我们发现，即使小模型表现不佳，它们仍然表现出重要和一致的任务性能改进，这些改进不被传统的评价策略捕捉，因为测量分辨率不够高。为了测量这些改进，我们引入了PassUntil评价策略，通过大量采样在解码阶段进行测量。我们进行了量化的研究，探索任务性能扩大的法律。首先，我们发现了一个严格的任务扩大法律，从而提高了任务性能的预测可能性。特别是，我们可以在代码生成任务上预测2.4B模型的性能，只需0.05%的差异。其次，通过PassUntil，我们发现了真正的emergent abilities，并证明它们与突破性有很大相似之处，它们的扩大曲线不能用标准扩大法律函数描述。然后，我们提出了一个数学定义，用于描述emergent abilities。通过定义，我们否定了一种流行的“多步逻辑假设”，即emergent abilities的起源，并提出了一种新的假设，具有满足性的适应性。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-be-Good-Path-Planners-A-Benchmark-and-Investigation-on-Spatial-temporal-Reasoning"><a href="#Can-Large-Language-Models-be-Good-Path-Planners-A-Benchmark-and-Investigation-on-Spatial-temporal-Reasoning" class="headerlink" title="Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning"></a>Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03249">http://arxiv.org/abs/2310.03249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Aghzal, Erion Plaku, Ziyu Yao</li>
<li>for: 本研究旨在探讨大型自然语言处理器（LLM）在长期规划和空间理解方面的局限性。</li>
<li>methods: 本文提出了一个新的benchmark，名为“路径规划从自然语言”（PPNL），以评估LLM在避免障碍物和遵循约束的情况下进行航向目标位置的能力。</li>
<li>results: 通过使用不同的几何示例和精度调整方法，我们发现GPT-4在干预示中间 reasoning能力较强，但在长期时间征 reasoning方面仍然失败。相比之下，经过精度调整的LLM在适应环境中表现出色，但在更大的环境或具有更多障碍物的环境中表现较差。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable success across a wide spectrum of tasks; however, they still face limitations in scenarios that demand long-term planning and spatial reasoning. To facilitate this line of research, in this work, we propose a new benchmark, termed $\textbf{P}$ath $\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage ($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by formulating ''path planning'' tasks that require an LLM to navigate to target locations while avoiding obstacles and adhering to constraints. Leveraging this benchmark, we systematically investigate LLMs including GPT-4 via different few-shot prompting methodologies and BART and T5 of various sizes via fine-tuning. Our experimental results show the promise of few-shot GPT-4 in spatial reasoning, when it is prompted to reason and act interleavedly, although it still fails to make long-term temporal reasoning. In contrast, while fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在各种任务上取得了优异成绩，但仍面临长期观念和空间理解的限制。为进一步推进这一研究，在这个工作中，我们提出了一个新的benchmark，称为“路径观察”（PPNL）。我们的benchmark评估LLM的空间-时间理解，通过要求LLM通过避免障碍物并遵循限制进行路径观察。我们通过不同的几何提示方法和精致适应的LLM进行系统性的探索，包括GPT-4、BART和T5。我们的实验结果显示GPT-4在几何提示下能够具有优异的空间理解能力，但仍无法进行长期时间的观察。相比之下，精致适应的LLM在类型内的理解任务上表现出色，但是它们在更大的环境或具有更多障碍的环境中则对应不佳。
</details></li>
</ul>
<hr>
<h2 id="FreshLLMs-Refreshing-Large-Language-Models-with-Search-Engine-Augmentation"><a href="#FreshLLMs-Refreshing-Large-Language-Models-with-Search-Engine-Augmentation" class="headerlink" title="FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation"></a>FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03214">http://arxiv.org/abs/2310.03214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/freshllms/freshqa">https://github.com/freshllms/freshqa</a></li>
<li>paper_authors: Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong</li>
<li>for: 这个研究旨在检验大型自然语言模型（LLM）是否能够适应我们现在的变化世界，并对其生成文本的准确性进行评估。</li>
<li>methods: 作者引入了一个新的动态Question Answering（QA） benchmark，名为FreshQA，该 benchmark 包括了多种问题和答案类型，包括需要快速变化的世界知识以及含有谬误的前提的问题。作者使用了一种名为FreshPrompt的简单的几个shot提示方法，通过在提示中包含相关和当前的信息来提高 LLM 的性能。</li>
<li>results: 作者通过人工评估，发现所有模型都在快速变化的知识和谬误前提下表现不佳，而 FreshPrompt 方法可以显著提高 LLM 的性能，并在与其他搜索引擎增强的提示方法和商业系统如 Perplexity.AI 进行比较中表现出色。<details>
<summary>Abstract</summary>
Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.
</details>
<details>
<summary>摘要</summary>
大多数大型语言模型（LLM）在训练后从未更新，因此它们缺乏能够动态适应我们世界的变化的能力。在这项工作中，我们进行了详细的实验，以确定LLM生成的文本是否准确。 Specifically，我们引入了一个新的动态问答数据集，名为FreshQA，其包含了多种问题和答案类型，包括需要快速更新的世界知识以及含有谬误的前提的问题。我们对一些关闭和开源LLM进行了两种评估方法，以评估它们的正确性和幻想。通过人工评估，我们发现了这些模型的局限性，并证明了它们在快速更新的知识和谬误前提下的表现不佳。为了解决这些问题，我们提出了FreshPrompt，一种简单的几个示例提示法，可以使LLM在FreshQA上表现更好。我们的实验显示，FreshPrompt不仅比自然语言提示法（Press et al., 2022）和商业系统（Perplexity.AI）更高效，还能够在不同的问题类型下提高LLM的表现。我们的分析表明，检索引擎搜索结果的数量和顺序均对LLM生成答案的正确性产生重要影响。此外， instrucibing LLM生成简洁和直接的答案可以减少幻想。为了促进未来的研究，我们在github.com/freshllms/freshqa上发布了FreshQA，并将在定期更新。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/05/cs.CL_2023_10_05/" data-id="clp88dbtv00cyob8866yn2fi6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/05/cs.LG_2023_10_05/" class="article-date">
  <time datetime="2023-10-05T10:00:00.000Z" itemprop="datePublished">2023-10-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/05/cs.LG_2023_10_05/">cs.LG - 2023-10-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improved-prediction-of-ligand-protein-binding-affinities-by-meta-modeling"><a href="#Improved-prediction-of-ligand-protein-binding-affinities-by-meta-modeling" class="headerlink" title="Improved prediction of ligand-protein binding affinities by meta-modeling"></a>Improved prediction of ligand-protein binding affinities by meta-modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03946">http://arxiv.org/abs/2310.03946</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lee1701/lee2023a">https://github.com/lee1701/lee2023a</a></li>
<li>paper_authors: Ho-Joon Lee, Prashant S. Emani, Mark B. Gerstein</li>
<li>for: 这篇研究旨在开发一个meta-modeling框架，以提高药物结构与序列学习模型之间的组合，以提高药物与标的蛋白质之间的结合力预测。</li>
<li>methods: 这篇研究使用了已出版的empirical结构基 docking和序列学习模型，并评估了多种个人模型、训练数据库和线性和非线性meta-modeling方法。</li>
<li>results: 研究发现，使用多种模型的ensemble可以实现与单一模型相比较好的结合力预测性能，并且可以控制输入特征，如物理化学性质或分子描述子。<details>
<summary>Abstract</summary>
The accurate screening of candidate drug ligands against target proteins through computational approaches is of prime interest to drug development efforts, as filtering potential candidates would save time and expenses for finding drugs. Such virtual screening depends in part on methods to predict the binding affinity between ligands and proteins. Given many computational models for binding affinity prediction with varying results across targets, we herein develop a meta-modeling framework by integrating published empirical structure-based docking and sequence-based deep learning models. In building this framework, we evaluate many combinations of individual models, training databases, and linear and nonlinear meta-modeling approaches. We show that many of our meta-models significantly improve affinity predictions over individual base models. Our best meta-models achieve comparable performance to state-of-the-art exclusively structure-based deep learning tools. Overall, we demonstrate that diverse modeling approaches can be ensembled together to gain substantial improvement in binding affinity prediction while allowing control over input features such as physicochemical properties or molecular descriptors.
</details>
<details>
<summary>摘要</summary>
<<SYS>>翻译文本为简化中文。</SYS>> Computational approaches for screening candidate drug ligands against target proteins have become a priority in drug development, as they can save time and expenses in finding potential drug candidates. One of the key aspects of virtual screening is predicting the binding affinity between ligands and proteins. However, there are many computational models for binding affinity prediction, and the results can vary across different targets. To address this challenge, we have developed a meta-modeling framework by integrating published empirical structure-based docking and sequence-based deep learning models.在虚拟屏选中，预测ligand和蛋白质之间的绑定强度是一项关键的任务。然而，不同目标之间的绑定强度预测结果可能会很不一致。为解决这个挑战，我们在这里提出了一种meta-模型框架，通过结合已出版的实验性结构含量 docking 和深度学习模型来实现。在建立这个框架时，我们评估了许多组合individual模型，训练数据库和线性和非线性meta-模型方法。我们发现，许多我们的meta-模型可以在绑定强度预测中提供显著改进，并且我们的best meta-模型可以与当前的结构深度学习工具相比。总的来说，我们展示了多种模型方法可以被 ensemble  вместе来实现较大的绑定强度预测改进，同时允许控制输入特征，如物理化学性质或分子描述符。
</details></li>
</ul>
<hr>
<h2 id="On-Wasserstein-distances-for-affine-transformations-of-random-vectors"><a href="#On-Wasserstein-distances-for-affine-transformations-of-random-vectors" class="headerlink" title="On Wasserstein distances for affine transformations of random vectors"></a>On Wasserstein distances for affine transformations of random vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03945">http://arxiv.org/abs/2310.03945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keaton Hamm, Andrzej Korzeniowski</li>
<li>for: 这 paper 是关于Random Vector在 $\mathbb{R}^n$ 中的Quadratic Wasserstein distance的下界，强调了对Affine Transformations的应用在 Wasserstein 空间中的掌握数据。</li>
<li>methods: 这 paper 使用了Bures Metric来计算Random Vector的covariance Matrix，并derived upper bounds for compositions of affine maps。</li>
<li>results: 这 paper 提供了一些Lower bounds和upper bounds for Quadratic Wasserstein distance，并应用于various distributions，包括lying on a 1-dimensional manifold in $\mathbb{R}^2$。 Additionally, the paper provides a framework for mimicking handwritten digit or alphabet datasets for use in a manifold learning framework.<details>
<summary>Abstract</summary>
We expound on some known lower bounds of the quadratic Wasserstein distance between random vectors in $\mathbb{R}^n$ with an emphasis on affine transformations that have been used in manifold learning of data in Wasserstein space. In particular, we give concrete lower bounds for rotated copies of random vectors in $\mathbb{R}^2$ with uncorrelated components by computing the Bures metric between the covariance matrices. We also derive upper bounds for compositions of affine maps which yield a fruitful variety of diffeomorphisms applied to an initial data measure. We apply these bounds to various distributions including those lying on a 1-dimensional manifold in $\mathbb{R}^2$ and illustrate the quality of the bounds. Finally, we give a framework for mimicking handwritten digit or alphabet datasets that can be applied in a manifold learning framework.
</details>
<details>
<summary>摘要</summary>
我们讨论了一些已知的 quaratic Wasserstein 距离下界，特别是在 $\mathbb{R}^n$ 中Random vectors 上，并强调了使用抽象变换来进行数据在 Wasserstein 空间的学习。我们给出了对旋转 copies of random vectors in $\mathbb{R}^2$ 的不相关 ком ponent的具体下界，通过计算协VARIANCE 矩阵之间的Bures metric来得到。我们还得出了作用于初始数据测度的affine maps 的聚合上界，这些上界可以生成一种多样的diffemorphisms。我们应用这些上下界到不同的分布，包括在 $\mathbb{R}^2$ 中的 1-dimensional manifold 上的分布，并 Illustrates the quality of the bounds。最后，我们提出了一种拟合手写字符或字母数据集的框架，可以在 manifold learning 框架中应用。
</details></li>
</ul>
<hr>
<h2 id="LaTeX-Language-Pattern-aware-Triggering-Event-Detection-for-Adverse-Experience-during-Pandemics"><a href="#LaTeX-Language-Pattern-aware-Triggering-Event-Detection-for-Adverse-Experience-during-Pandemics" class="headerlink" title="LaTeX: Language Pattern-aware Triggering Event Detection for Adverse Experience during Pandemics"></a>LaTeX: Language Pattern-aware Triggering Event Detection for Adverse Experience during Pandemics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03941">http://arxiv.org/abs/2310.03941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiqun Fu, Yangxiao Bai, Weiwei Zhang, Deepthi Kolady</li>
<li>for: This paper aims to explore the role of social media platforms in highlighting and addressing socioeconomic disparities during the COVID-19 pandemic, specifically focusing on four major types of adverse experiences: loss of employment income, food scarcity, housing insecurity, and unmet needs for mental health services.</li>
<li>methods: The paper uses real-time data from Twitter to analyze language patterns related to the four adverse experiences, and proposes a sparsity optimization problem to extract low-level language features. The authors also propose novel constraints on feature similarity based on prior knowledge about the similarity of language patterns among the adverse experiences.</li>
<li>results: The proposed model is challenging to solve due to the non-convexity objective and non-smoothness penalties, but the authors develop an algorithm based on the alternating direction method of multipliers (ADMM) framework to solve the problem. Extensive experiments and comparisons to other models on real-world social media data justify the efficacy of their model in detecting adverse experiences.<details>
<summary>Abstract</summary>
The COVID-19 pandemic has accentuated socioeconomic disparities across various racial and ethnic groups in the United States. While previous studies have utilized traditional survey methods like the Household Pulse Survey (HPS) to elucidate these disparities, this paper explores the role of social media platforms in both highlighting and addressing these challenges. Drawing from real-time data sourced from Twitter, we analyzed language patterns related to four major types of adverse experiences: loss of employment income (LI), food scarcity (FS), housing insecurity (HI), and unmet needs for mental health services (UM). We first formulate a sparsity optimization problem that extracts low-level language features from social media data sources. Second, we propose novel constraints on feature similarity exploiting prior knowledge about the similarity of the language patterns among the adverse experiences. The proposed problem is challenging to solve due to the non-convexity objective and non-smoothness penalties. We develop an algorithm based on the alternating direction method of multipliers (ADMM) framework to solve the proposed formulation. Extensive experiments and comparisons to other models on real-world social media and the detection of adverse experiences justify the efficacy of our model.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行减少了不同的民族和文化背景下的社会经济差距。  previous 研究使用传统的调查方法，如家庭普ulse调查（HPS）来描述这些差距，但这篇论文探讨了社交媒体平台在描述和解决这些挑战方面的作用。  drawing  from real-time 社交媒体数据源，我们分析了四种主要的不利经验语言模式：失业收入损失（LI）、食物不足（FS）、住房不安定（HI）和精神健康服务需求不满（UM）。  first，我们构建了一个简单的语言特征提取问题，以EXTRACT 社交媒体数据源中的低级语言特征。  second，我们提出了一些新的语言特征相似性约束，基于先前知道这些语言模式之间的相似性。  proposed 问题是因为非对称目标函数和不对称补偿因子而具有挑战性。  we 发展了基于多元分解方法（ADMM）框架的算法来解决提议的问题。  extensive 实验和对真实社交媒体数据进行比较，证明了我们的模型的有效性。
</details></li>
</ul>
<hr>
<h2 id="Improving-classifier-decision-boundaries-using-nearest-neighbors"><a href="#Improving-classifier-decision-boundaries-using-nearest-neighbors" class="headerlink" title="Improving classifier decision boundaries using nearest neighbors"></a>Improving classifier decision boundaries using nearest neighbors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03927">http://arxiv.org/abs/2310.03927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes Schneider</li>
<li>for: 提高神经网络的选择边界优化</li>
<li>methods: 使用样本和其最近邻居的平面均值来提高神经网络的预测性能</li>
<li>results: 提高了对label noise、对抗攻击、分类精度和一定程度的解释性能Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the optimization of decision boundaries in neural networks.</li>
<li>methods: The proposed method uses a weighted average of the predictions of a sample and its nearest neighbors in latent space to improve the performance of neural networks.</li>
<li>results: The proposed method improves various important measures of neural networks, including resistance to label noise, robustness against adversarial attacks, classification accuracy, and interpretability. The improvements are not necessarily large in all four areas, but the approach is simple and does not require any modifications to the network architecture, training procedure, or dataset.<details>
<summary>Abstract</summary>
Neural networks are not learning optimal decision boundaries. We show that decision boundaries are situated in areas of low training data density. They are impacted by few training samples which can easily lead to overfitting. We provide a simple algorithm performing a weighted average of the prediction of a sample and its nearest neighbors' (computed in latent space) leading to a minor favorable outcomes for a variety of important measures for neural networks. In our evaluation, we employ various self-trained and pre-trained convolutional neural networks to show that our approach improves (i) resistance to label noise, (ii) robustness against adversarial attacks, (iii) classification accuracy, and to some degree even (iv) interpretability. While improvements are not necessarily large in all four areas, our approach is conceptually simple, i.e., improvements come without any modification to network architecture, training procedure or dataset. Furthermore, they are in stark contrast to prior works that often require trade-offs among the four objectives or provide valuable, but non-actionable insights.
</details>
<details>
<summary>摘要</summary>
(i) resistance to label noise,(ii) robustness against adversarial attacks,(iii) classification accuracy, and to some degree even(iv) interpretability. While improvements are not necessarily large in all four areas, our approach is conceptually simple, i.e., improvements come without any modification to network architecture, training procedure, or dataset. Furthermore, they are in stark contrast to prior works that often require trade-offs among the four objectives or provide valuable, but non-actionable insights.Translated into Simplified Chinese:神经网络不会学习优化的决策边界。我们显示决策边界位于训练数据密度低的区域内。它们受到训练样本少量的影响，容易导致过拟合。我们提供一个简单的算法，将样本预测值和其最近的邻居（在潜在空间中计算）进行权重平均，导致许多重要指标上的微妙改善。在我们的评估中，我们使用了各种自动学习和预训练 convolutional neural network，以示我们的方法可以提高(i) 标签噪声抵抗性，(ii) 对抗攻击性，(iii) 分类精度，并且在一定程度上还可以提高(iv) 解释性。虽然改善不一定是所有四个领域中的很大，但我们的方法是概念简单，即改进不需要修改网络结构、训练过程或数据集。此外，它们与之前的工作不同，常常需要四个目标之间的交易或提供有价值，但无法实现的指导。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Low-Rank-and-Sparse-Recurrent-Connectivity-for-Robust-Closed-Loop-Control"><a href="#Leveraging-Low-Rank-and-Sparse-Recurrent-Connectivity-for-Robust-Closed-Loop-Control" class="headerlink" title="Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control"></a>Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03915">http://arxiv.org/abs/2310.03915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neehal Tumma, Mathias Lechner, Noel Loo, Ramin Hasani, Daniela Rus</li>
<li>for: 这个论文旨在研究机器学习中自适应Agent的开发，特别是在环境变化的情况下。</li>
<li>methods: 这篇论文使用了回归神经网络，并研究了这种神经网络的连接性 Parameters 对于closed-loop设置的稳定性的影响。</li>
<li>results: 研究发现，通过控制连接性的权重和稀疏程度，可以使得神经网络在线上下文中更加稳定和可靠，并且可以使用 fewer parameters 的模型在分布Shift下表现更好。<details>
<summary>Abstract</summary>
Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can outperform their full-rank, fully-connected counterparts in the online setting under distribution shift. This yields memory-efficient and robust agents while opening a new perspective on how we can modulate network dynamics through connectivity.
</details>
<details>
<summary>摘要</summary>
开发自适应智能代理人，可以在变化环境中交互，是机器学习领域的开放挑战。在这些设置下，Robustness特别重要，因为代理人常常在专家示例的基础上训练，但在环境中部署时需要总结并适应关闭反馈循环。在这项工作中，我们探索使用回归神经网络来解决这类任务，并研究回归连接的参数化对于封闭循环设置的影响。 Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can outperform their full-rank, fully-connected counterparts in the online setting under distribution shift. This yields memory-efficient and robust agents while opening a new perspective on how we can modulate network dynamics through connectivity.
</details></li>
</ul>
<hr>
<h2 id="PyDCM-Custom-Data-Center-Models-with-Reinforcement-Learning-for-Sustainability"><a href="#PyDCM-Custom-Data-Center-Models-with-Reinforcement-Learning-for-Sustainability" class="headerlink" title="PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability"></a>PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03906">http://arxiv.org/abs/2310.03906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avisek Naug, Antonio Guillen, Ricardo Luna Gutiérrez, Vineet Gundecha, Dejan Markovikj, Lekhapriya Dheeraj Kashyap, Lorenz Krause, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Soumyendu Sarkar</li>
<li>for: 这篇论文目的是优化数据中心的能源消耗和 Computational workloads。</li>
<li>methods: 论文使用了Python实现的自定义数据中心模型（PyDCM），允许用户创建具有自定义服务器特性和IT库 geometrical arrangement的专一化IT设备配置。</li>
<li>results: 相比现有的Energy Plus模型实现，PyDCM的矩阵化热计算使其速度提高至30倍，并且对于多CPU的数据中心设计实验 scales sublinearly。此外，PyDCM还允许使用深度强化学习via Gymnasium wrapper来优化数据中心冷却，并提供了一个易用的平台来测试多种数据中心设计构想。<details>
<summary>Abstract</summary>
The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current Energy Plus modeling implementations and scales sublinearly with the number of CPUs. Also, PyDCM enables the use of Deep Reinforcement Learning via the Gymnasium wrapper to optimize data center cooling and offers a user-friendly platform for testing various data center design prototypes.
</details>
<details>
<summary>摘要</summary>
全球增加对可持续性的强调和减少碳排放，政府和企业被迫重新考虑数据中心的设计和运行方法。由于数据中心的能源消耗和计算工作负载呈指数增长，特别是冷却和IT能源使用方面，因此寻找可 configurable 和可扩展的热数据中心模型成为了一项挑战。由于数据中心由多个IT组件组成，这些组件的几何配置和热释放使得热模型化变得困难。本文介绍了PyDCM，一个基于Python的自定义数据中心模型，允许用户创建自定义IT设备参数和自定义IT柜的唯一配置。通过向量化热计算，PyDCM比现有的Energy Plus模型实现速度高得多（30倍），并且在数据中心设计尝试中进行深度强化学习。此外，PyDCM还提供了一个易于使用的平台，可以用于测试不同的数据中心设计原型。
</details></li>
</ul>
<hr>
<h2 id="Provable-benefits-of-annealing-for-estimating-normalizing-constants-Importance-Sampling-Noise-Contrastive-Estimation-and-beyond"><a href="#Provable-benefits-of-annealing-for-estimating-normalizing-constants-Importance-Sampling-Noise-Contrastive-Estimation-and-beyond" class="headerlink" title="Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond"></a>Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03902">http://arxiv.org/abs/2310.03902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/l-omar-chehab/annealing-normalizing-constants">https://github.com/l-omar-chehab/annealing-normalizing-constants</a></li>
<li>paper_authors: Omar Chehab, Aapo Hyvarinen, Andrej Risteski</li>
<li>for: 这些研究旨在开发一些基于熔炼的蒙特卡罗方法来估算正则化常数（分配函数）。</li>
<li>methods: 这些方法包括气化Importance Sampling和气化Noise-Contrastive Estimation（NCE）。这些方法的设计选择包括选择哪种估计器、选择哪种分布路径和是否使用路径。</li>
<li>results: 我们在这篇论文中评估了每一个设计选择的极限估计误差。我们发现，使用NCE估计器比Importance Sampling估计器更有效，但在极限情况下，差异消失。其次，我们发现使用几何路径可以减少估计误差，并且在某些情况下，使用算术路径可以实现最佳性。最后，我们建议一种两步估计器来近似最佳路径。<details>
<summary>Abstract</summary>
Recent research has developed several Monte Carlo methods for estimating the normalization constant (partition function) based on the idea of annealing. This means sampling successively from a path of distributions that interpolate between a tractable "proposal" distribution and the unnormalized "target" distribution. Prominent estimators in this family include annealed importance sampling and annealed noise-contrastive estimation (NCE). Such methods hinge on a number of design choices: which estimator to use, which path of distributions to use and whether to use a path at all; so far, there is no definitive theory on which choices are efficient. Here, we evaluate each design choice by the asymptotic estimation error it produces. First, we show that using NCE is more efficient than the importance sampling estimator, but in the limit of infinitesimal path steps, the difference vanishes. Second, we find that using the geometric path brings down the estimation error from an exponential to a polynomial function of the parameter distance between the target and proposal distributions. Third, we find that the arithmetic path, while rarely used, can offer optimality properties over the universally-used geometric path. In fact, in a particular limit, the optimal path is arithmetic. Based on this theory, we finally propose a two-step estimator to approximate the optimal path in an efficient way.
</details>
<details>
<summary>摘要</summary>
现有研究已经开发出了一些蒙特卡洛方法来估计正规化常数（分配函数），基于渐变的想法。这意味着采样successively从一个描述分布的路径中，该路径连接了一个可行的"提议"分布和未正规化的"目标"分布。主要估计器在这个家族中包括渐变重要性样本和渐变噪声相对估计（NCE）。这些方法受到许多设计选择的影响：哪种估计器使用，哪个路径使用，是否使用路径等。到目前为止，没有一个确定的理论来确定这些选择是有效的。我们在这里评估每种设计选择的极限估计错误。首先，我们发现使用NCE估计器比重要性样本估计器更有效，但在无限小路步下，两者的差异消失。其次，我们发现使用几何路径可以将估计错误从指数函数下降到多项函数中，而且与参数距离目标分布的距离成正比。最后，我们发现使用阿基米德路径，虽然 rarely used，可以在一些情况下提供优化性质。实际上，在某个特殊情况下，优化的路径是阿基米德路径。基于这种理论，我们最终提议了一种两步估计器，可以有效地估计正规化常数。
</details></li>
</ul>
<hr>
<h2 id="CrysFormer-Protein-Structure-Prediction-via-3d-Patterson-Maps-and-Partial-Structure-Attention"><a href="#CrysFormer-Protein-Structure-Prediction-via-3d-Patterson-Maps-and-Partial-Structure-Attention" class="headerlink" title="CrysFormer: Protein Structure Prediction via 3d Patterson Maps and Partial Structure Attention"></a>CrysFormer: Protein Structure Prediction via 3d Patterson Maps and Partial Structure Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03899">http://arxiv.org/abs/2310.03899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Dun, Qiutai Pan, Shikai Jin, Ria Stevens, Mitchell D. Miller, George N. Phillips, Jr., Anastasios Kyrillidis</li>
<li>for: 这个论文主要研究了如何使用transformer neural网络架构和蛋白质晶体结晶学信息来预测蛋白质的电子密度图。</li>
<li>methods: 该论文提出了一种基于transformer neural网络架构的新方法，直接使用蛋白质晶体结晶学信息和部分结构信息来预测蛋白质的电子密度图。</li>
<li>results: 通过两个新的peptide fragments数据集（2个残基和15个残基），论文表明该方法可以准确地预测蛋白质的电子密度图，并且需要训练数据集的数量和计算成本远比传统方法少。<details>
<summary>Abstract</summary>
Determining the structure of a protein has been a decades-long open question. A protein's three-dimensional structure often poses nontrivial computation costs, when classical simulation algorithms are utilized. Advances in the transformer neural network architecture -- such as AlphaFold2 -- achieve significant improvements for this problem, by learning from a large dataset of sequence information and corresponding protein structures. Yet, such methods only focus on sequence information; other available prior knowledge, such as protein crystallography and partial structure of amino acids, could be potentially utilized. To the best of our knowledge, we propose the first transformer-based model that directly utilizes protein crystallography and partial structure information to predict the electron density maps of proteins. Via two new datasets of peptide fragments (2-residue and 15-residue) , we demonstrate our method, dubbed \texttt{CrysFormer}, can achieve accurate predictions, based on a much smaller dataset size and with reduced computation costs.
</details>
<details>
<summary>摘要</summary>
Determining the structure of a protein has been an open question for decades. A protein's three-dimensional structure often poses significant computational costs when using classical simulation algorithms. Advances in the transformer neural network architecture, such as AlphaFold2, have achieved significant improvements for this problem by learning from a large dataset of sequence information and corresponding protein structures. However, these methods only focus on sequence information; other available prior knowledge, such as protein crystallography and partial structure of amino acids, could be potentially utilized. To the best of our knowledge, we propose the first transformer-based model that directly utilizes protein crystallography and partial structure information to predict the electron density maps of proteins. Via two new datasets of peptide fragments (2-residue and 15-residue), we demonstrate that our method, dubbed \texttt{CrysFormer}, can achieve accurate predictions with a much smaller dataset size and reduced computation costs.
</details></li>
</ul>
<hr>
<h2 id="Class-Incremental-Learning-Using-Generative-Experience-Replay-Based-on-Time-aware-Regularization"><a href="#Class-Incremental-Learning-Using-Generative-Experience-Replay-Based-on-Time-aware-Regularization" class="headerlink" title="Class-Incremental Learning Using Generative Experience Replay Based on Time-aware Regularization"></a>Class-Incremental Learning Using Generative Experience Replay Based on Time-aware Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03898">http://arxiv.org/abs/2310.03898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zizhao Hu, Mohammad Rostami</li>
<li>for: 本研究旨在解决逐渐学习中积累新任务而不导致忘记之挑战，通过生成pseudo-数据点进行过去学习任务的合成，并在新任务的训练过程中重新使用这些pseudo-数据点。</li>
<li>methods: 本研究提出了一种基于生物神经系统机制的时间意识正则化方法，用于动态精度调整三个训练目标函数：supervised learning、射频正则化和数据重建。</li>
<li>results: 实验结果表明，our方法在严格的类增量设定下（i）保持模型大小不变，（ii）无需预训练数据集和（iii）无需记忆缓存来存储过去任务的数据时，可以在逐渐学习中达到更高的表现和记忆留存。<details>
<summary>Abstract</summary>
Learning new tasks accumulatively without forgetting remains a critical challenge in continual learning. Generative experience replay addresses this challenge by synthesizing pseudo-data points for past learned tasks and later replaying them for concurrent training along with the new tasks' data. Generative replay is the best strategy for continual learning under a strict class-incremental setting when certain constraints need to be met: (i) constant model size, (ii) no pre-training dataset, and (iii) no memory buffer for storing past tasks' data. Inspired by the biological nervous system mechanisms, we introduce a time-aware regularization method to dynamically fine-tune the three training objective terms used for generative replay: supervised learning, latent regularization, and data reconstruction. Experimental results on major benchmarks indicate that our method pushes the limit of brain-inspired continual learners under such strict settings, improves memory retention, and increases the average performance over continually arriving tasks.
</details>
<details>
<summary>摘要</summary>
学习新任务积累无忘记是持续学习中的核心挑战。生成经验回放解决了这个挑战，通过合成过去学习的任务的 Pseudo-数据点并在当前任务的数据同时重新训练。生成回放是在严格的类增量设定下最佳的启发式学习策略，当下列条件需要满足：（i）不变的模型大小，（ii）无预训练集，（iii）无记忆缓存以存储过去任务的数据。通过模仿生物神经系统机制，我们引入时间意识正则化方法来动态细调三个训练目标函数用于生成回放：监督学习、干扰正则化和数据重建。实验结果表明，我们的方法可以在这些严格的设定下超越脑神经系统逻辑学习器，提高记忆保持和逐渐到达的平均性能。
</details></li>
</ul>
<hr>
<h2 id="Information-Geometry-for-the-Working-Information-Theorist"><a href="#Information-Geometry-for-the-Working-Information-Theorist" class="headerlink" title="Information Geometry for the Working Information Theorist"></a>Information Geometry for the Working Information Theorist</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03884">http://arxiv.org/abs/2310.03884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kumar Vijay Mishra, M. Ashok Kumar, Ting-Kam Leonard Wong</li>
<li>for: 本文提供了信息 геометри的概述，以便对信息理论领域的人了解这个新领域的研究。</li>
<li>methods: 本文介绍了统计 manifolds 上的异 diferences，通常是 Generalized distances, orthogonality, and geodesics 等概念。</li>
<li>results: 本文介绍了一些现代信息 геометри的发展，包括雷达探测、数组信号处理、量子物理、深度学习和最优运输等领域的应用。<details>
<summary>Abstract</summary>
Information geometry is a study of statistical manifolds, that is, spaces of probability distributions from a geometric perspective. Its classical information-theoretic applications relate to statistical concepts such as Fisher information, sufficient statistics, and efficient estimators. Today, information geometry has emerged as an interdisciplinary field that finds applications in diverse areas such as radar sensing, array signal processing, quantum physics, deep learning, and optimal transport. This article presents an overview of essential information geometry to initiate an information theorist, who may be unfamiliar with this exciting area of research. We explain the concepts of divergences on statistical manifolds, generalized notions of distances, orthogonality, and geodesics, thereby paving the way for concrete applications and novel theoretical investigations. We also highlight some recent information-geometric developments, which are of interest to the broader information theory community.
</details>
<details>
<summary>摘要</summary>
信息 геометрия是研究统计 manifold的学科，即概率分布的空间从 геометрического角度出发。传统上，信息学应用包括统计概念如费希尔信息、充分统计量和有效估计器。但今天，信息 geometry 已经成为一个横跨多个领域的交叉学科，包括雷达探测、数组信号处理、量子物理、深度学习和最优运输。本文为不熟悉信息 geometry 的信息学家提供一个入门性的概述，包括在统计 manifold 上的差异、通用距离、正交和最短路径，以便进一步探索实际应用和新的理论研究。此外，我们还提到了一些最近的信息 geometry 发展，对信息理论社区总体而言都很有价值。
</details></li>
</ul>
<hr>
<h2 id="Non-Commutative-Convolutional-Signal-Models-in-Neural-Networks-Stability-to-Small-Deformations"><a href="#Non-Commutative-Convolutional-Signal-Models-in-Neural-Networks-Stability-to-Small-Deformations" class="headerlink" title="Non Commutative Convolutional Signal Models in Neural Networks: Stability to Small Deformations"></a>Non Commutative Convolutional Signal Models in Neural Networks: Stability to Small Deformations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03879">http://arxiv.org/abs/2310.03879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro Parada-Mayorga, Landon Butler, Alejandro Ribeiro</li>
<li>for: 这篇论文关注了基于非交汇代数的抽象信号模型（ASM），以及其在卷积神经网络中的应用。</li>
<li>methods: 该论文基于核心是阿尔格браic信号处理（ASP）的一般工具，研究了非交汇卷积滤波器的过滤和稳定性质。</li>
<li>results: 研究发现，非交汇滤波器可以具有小 perturbation 的稳定性，同时存在与 commutative 模型相似的选择性和稳定性之间的质量负担。 numerical experiments  validate these results.<details>
<summary>Abstract</summary>
In this paper we discuss the results recently published in~[1] about algebraic signal models (ASMs) based on non commutative algebras and their use in convolutional neural networks. Relying on the general tools from algebraic signal processing (ASP), we study the filtering and stability properties of non commutative convolutional filters. We show how non commutative filters can be stable to small perturbations on the space of operators. We also show that although the spectral components of the Fourier representation in a non commutative signal model are associated to spaces of dimension larger than one, there is a trade-off between stability and selectivity similar to that observed for commutative models. Our results have direct implications for group neural networks, multigraph neural networks and quaternion neural networks, among other non commutative architectures. We conclude by corroborating these results through numerical experiments.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们讨论了最近发表在[1]中关于基于非交换代数的拟合信号模型（ASM）以及它们在卷积神经网络中的应用。基于普通的代数信号处理（ASP）工具，我们研究了非交换卷积 Filter 的过滤和稳定性质性。我们示出了非交换卷积 Filter 可以具有小 perturbation 的空间操作器稳定性。此外，我们还证明了在非交换信号模型中的傅里叶分量相对于 commutative 模型存在一种与稳定性和选择性之间的质量负担。我们的结果直接适用于群神经网络、多graph神经网络和四元数神经网络等非交换架构。我们通过数值实验证实了这些结果。
</details></li>
</ul>
<hr>
<h2 id="Model-Complexity-of-Program-Phases"><a href="#Model-Complexity-of-Program-Phases" class="headerlink" title="Model Complexity of Program Phases"></a>Model Complexity of Program Phases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03865">http://arxiv.org/abs/2310.03865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Karuvally, J. Eliot B. Moss</li>
<li>for: 资源受限的 Computing 系统中，序列预测模型需要在紧张的环境下运作。不同的模型各自对预测在这些限制下进行了一些修改，以减少实现成本。这些资源受限的序列预测模型在实践中展现了一个基本的交换关系，即预测质量和实现成本之间的贸易。</li>
<li>methods: 本文使用了一种machine learning模型，具体来说是深度神经网络，并且提出了一个相应的实验方法，以探索这个交换关系的空间。</li>
<li>results: 本文预测这个交换关系对于特定的机器学习模型而言，可以获得更好的预测质量，并且可以更好地理解这些模型在资源受限的情况下的实际和理论上的限制。<details>
<summary>Abstract</summary>
In resource limited computing systems, sequence prediction models must operate under tight constraints. Various models are available that cater to prediction under these conditions that in some way focus on reducing the cost of implementation. These resource constrained sequence prediction models, in practice, exhibit a fundamental tradeoff between the cost of implementation and the quality of its predictions. This fundamental tradeoff seems to be largely unexplored for models for different tasks. Here we formulate the necessary theory and an associated empirical procedure to explore this tradeoff space for a particular family of machine learning models such as deep neural networks. We anticipate that the knowledge of the behavior of this tradeoff may be beneficial in understanding the theoretical and practical limits of creation and deployment of models for resource constrained tasks.
</details>
<details>
<summary>摘要</summary>
在有限资源计算系统中，序列预测模型需要在严格的限制下运行。有各种模型可以满足这种情况，它们强调降低实现成本。这些资源受限序列预测模型在实践中存在一个基本的交易关系，即实现成本和预测质量之间的交易关系。这种基本交易关系还未对不同任务的模型进行了系统性的探索。在这里，我们建立了必要的理论和相关的实验方法，以探索这个交易关系的空间，特别是用于深度神经网络等一家machine learning模型。我们预计，了解这种交易关系的行为可以帮助我们理解资源受限任务中模型的理论和实践上的限制。
</details></li>
</ul>
<hr>
<h2 id="Variational-Barycentric-Coordinates"><a href="#Variational-Barycentric-Coordinates" class="headerlink" title="Variational Barycentric Coordinates"></a>Variational Barycentric Coordinates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03861">http://arxiv.org/abs/2310.03861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana Dodik, Oded Stein, Vincent Sitzmann, Justin Solomon</li>
<li>for:  optimize for generalized barycentric coordinates and provide additional control over existing models</li>
<li>methods: use a variational technique and parameterize the continuous function that maps any coordinate in a polytope’s interior to its barycentric coordinates using a neural field</li>
<li>results: demonstrate the flexibility of the model using a variety of objective functions and present a thorough validation of the algorithm, as well as demonstrate several applications<details>
<summary>Abstract</summary>
We propose a variational technique to optimize for generalized barycentric coordinates that offers additional control compared to existing models. Prior work represents barycentric coordinates using meshes or closed-form formulae, in practice limiting the choice of objective function. In contrast, we directly parameterize the continuous function that maps any coordinate in a polytope's interior to its barycentric coordinates using a neural field. This formulation is enabled by our theoretical characterization of barycentric coordinates, which allows us to construct neural fields that parameterize the entire function class of valid coordinates. We demonstrate the flexibility of our model using a variety of objective functions, including multiple smoothness and deformation-aware energies; as a side contribution, we also present mathematically-justified means of measuring and minimizing objectives like total variation on discontinuous neural fields. We offer a practical acceleration strategy, present a thorough validation of our algorithm, and demonstrate several applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一种变量技术来优化通用的加权坐标，它提供了更多的控制比现有模型。先前的工作使用网格或关闭式公式来表示加权坐标，但这限制了目标函数的选择。相比之下，我们直接使用神经场来 parameterize任何polytope内部坐标到其加权坐标的连续函数。这种形式是由我们对加权坐标的理论特征化，允许我们构建总函数类型的有效坐标的神经场。我们采用多种目标函数，包括多项细化和形态感知能量；同时，我们也提供了数学上正确的测量和最小化对不连续神经场的目标。我们提供了实用的加速策略，进行了全面验证我们的算法，并应用了多种场景。
</details></li>
</ul>
<hr>
<h2 id="Euclid-Identification-of-asteroid-streaks-in-simulated-images-using-deep-learning"><a href="#Euclid-Identification-of-asteroid-streaks-in-simulated-images-using-deep-learning" class="headerlink" title="Euclid: Identification of asteroid streaks in simulated images using deep learning"></a>Euclid: Identification of asteroid streaks in simulated images using deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03845">http://arxiv.org/abs/2310.03845</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Pöntinen, M. Granvik, A. A. Nucita, L. Conversi, B. Altieri, B. Carry, C. M. O’Riordan, D. Scott, N. Aghanim, A. Amara, L. Amendola, N. Auricchio, M. Baldi, D. Bonino, E. Branchini, M. Brescia, S. Camera, V. Capobianco, C. Carbone, J. Carretero, M. Castellano, S. Cavuoti, A. Cimatti, R. Cledassou, G. Congedo, Y. Copin, L. Corcione, F. Courbin, M. Cropper, A. Da Silva, H. Degaudenzi, J. Dinis, F. Dubath, X. Dupac, S. Dusini, S. Farrens, S. Ferriol, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, B. Garilli, W. Gillard, B. Gillis, C. Giocoli, A. Grazian, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Kümmel, S. Kermiche, A. Kiessling, T. Kitching, R. Kohley, M. Kunz, H. Kurki-Suonio, S. Ligori, P. B. Lilje, I. Lloro, E. Maiorano, O. Mansutti, O. Marggraf, K. Markovic, F. Marulli, R. Massey, E. Medinaceli, S. Mei, M. Melchior, Y. Mellier, M. Meneghetti, G. Meylan, M. Moresco, L. Moscardini, E. Munari, S. -M. Niemi, T. Nutma, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, V. Pettorino, S. Pires, G. Polenta, M. Poncet, F. Raison, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, E. Rossetti, R. Saglia, D. Sapone, B. Sartoris, P. Schneider, A. Secroun, G. Seidel, S. Serrano, C. Sirignano, G. Sirri, L. Stanco, P. Tallada-Crespí, A. N. Taylor, I. Tereno, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, T. Vassallo, G. Verdoes Kleijn, Y. Wang, J. Weller, G. Zamorani, J. Zoubian, V. Scottez</li>
<li>for: 帮助欧空爆料 telescope 探测到更多的小行星</li>
<li>methods: 使用深度学习 pipeline，包括卷积神经网络、回归神经网络和梯度拟合树</li>
<li>results: 比革Det software更高的完整性和相似的纯度，能探测到0.25-0.5 magnitudes 更暗的小行星，可能增加探测小行星的数量50%<details>
<summary>Abstract</summary>
Up to 150000 asteroids will be visible in the images of the ESA Euclid space telescope, and the instruments of Euclid offer multiband visual to near-infrared photometry and slitless spectra of these objects. Most asteroids will appear as streaks in the images. Due to the large number of images and asteroids, automated detection methods are needed. A non-machine-learning approach based on the StreakDet software was previously tested, but the results were not optimal for short and/or faint streaks. We set out to improve the capability to detect asteroid streaks in Euclid images by using deep learning.   We built, trained, and tested a three-step machine-learning pipeline with simulated Euclid images. First, a convolutional neural network (CNN) detected streaks and their coordinates in full images, aiming to maximize the completeness (recall) of detections. Then, a recurrent neural network (RNN) merged snippets of long streaks detected in several parts by the CNN. Lastly, gradient-boosted trees (XGBoost) linked detected streaks between different Euclid exposures to reduce the number of false positives and improve the purity (precision) of the sample.   The deep-learning pipeline surpasses the completeness and reaches a similar level of purity of a non-machine-learning pipeline based on the StreakDet software. Additionally, the deep-learning pipeline can detect asteroids 0.25-0.5 magnitudes fainter than StreakDet. The deep-learning pipeline could result in a 50% increase in the number of detected asteroids compared to the StreakDet software. There is still scope for further refinement, particularly in improving the accuracy of streak coordinates and enhancing the completeness of the final stage of the pipeline, which involves linking detections across multiple exposures.
</details>
<details>
<summary>摘要</summary>
“Up to 150,000 asteroids will be visible in the images of the ESA Euclid space telescope, and the instruments of Euclid offer multiband visual to near-infrared photometry and slitless spectra of these objects. Most asteroids will appear as streaks in the images. Due to the large number of images and asteroids, automated detection methods are needed. A non-machine-learning approach based on the StreakDet software was previously tested, but the results were not optimal for short and/or faint streaks. We set out to improve the capability to detect asteroid streaks in Euclid images by using deep learning. We built, trained, and tested a three-step machine-learning pipeline with simulated Euclid images. First, a convolutional neural network (CNN) detected streaks and their coordinates in full images, aiming to maximize the completeness (recall) of detections. Then, a recurrent neural network (RNN) merged snippets of long streaks detected in several parts by the CNN. Lastly, gradient-boosted trees (XGBoost) linked detected streaks between different Euclid exposures to reduce the number of false positives and improve the purity (precision) of the sample. The deep-learning pipeline surpasses the completeness and reaches a similar level of purity of a non-machine-learning pipeline based on the StreakDet software. Additionally, the deep-learning pipeline can detect asteroids 0.25-0.5 magnitudes fainter than StreakDet. The deep-learning pipeline could result in a 50% increase in the number of detected asteroids compared to the StreakDet software. There is still scope for further refinement, particularly in improving the accuracy of streak coordinates and enhancing the completeness of the final stage of the pipeline, which involves linking detections across multiple exposures.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other parts of the world. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Chameleon-Increasing-Label-Only-Membership-Leakage-with-Adaptive-Poisoning"><a href="#Chameleon-Increasing-Label-Only-Membership-Leakage-with-Adaptive-Poisoning" class="headerlink" title="Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning"></a>Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03838">http://arxiv.org/abs/2310.03838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harsh Chaudhari, Giorgio Severi, Alina Oprea, Jonathan Ullman</li>
<li>for: 本研究旨在提高现有的标签只会员推理攻击的准确率，特别是在低假阳极低（FPR）的情况下。</li>
<li>methods: 本文提出了一种新的攻击方法，即染色蛋白攻击，它利用了一种新的适应式数据毒素策略和高效的查询选择方法来实现在标签只会员推理中更高的会员推理精度。</li>
<li>results: 作者对多个实验结果进行了比较，结果显示，染色蛋白攻击在低FPR情况下能够更高效地进行会员推理，特别是在标签只会员推理中。<details>
<summary>Abstract</summary>
The integration of machine learning (ML) in numerous critical applications introduces a range of privacy concerns for individuals who provide their datasets for model training. One such privacy risk is Membership Inference (MI), in which an attacker seeks to determine whether a particular data sample was included in the training dataset of a model. Current state-of-the-art MI attacks capitalize on access to the model's predicted confidence scores to successfully perform membership inference, and employ data poisoning to further enhance their effectiveness. In this work, we focus on the less explored and more realistic label-only setting, where the model provides only the predicted label on a queried sample. We show that existing label-only MI attacks are ineffective at inferring membership in the low False Positive Rate (FPR) regime. To address this challenge, we propose a new attack Chameleon that leverages a novel adaptive data poisoning strategy and an efficient query selection method to achieve significantly more accurate membership inference than existing label-only attacks, especially at low FPRs.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在许多关键应用中的整合引入了许多个人隐私问题，其中一个问题是会员推断（MI），攻击者希望确定一个特定的数据示例是否包含在训练集中。现有的MI攻击都利用了访问模型预测的自信度分数，并使用数据毒攻击进一步提高其效果。在这项工作中，我们关注了更为未explored和更加现实istic的标签只设置，在这种设置下，模型只提供了查询示例的预测标签。我们显示现有的标签只MI攻击在低false positive rate（FPR） régime下无法进行会员推断。为解决这个挑战，我们提议一种新的攻击方法叫做假蝴蝶，它利用了一种新的适应式数据毒攻击策略和高效的查询选择方法，可以在低FPR régime下实现较为准确的会员推断。
</details></li>
</ul>
<hr>
<h2 id="Learning-A-Disentangling-Representation-For-PU-Learning"><a href="#Learning-A-Disentangling-Representation-For-PU-Learning" class="headerlink" title="Learning A Disentangling Representation For PU Learning"></a>Learning A Disentangling Representation For PU Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03833">http://arxiv.org/abs/2310.03833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Zamzam, Haleh Akrami, Mahdi Soltanolkotabi, Richard Leahy</li>
<li>For:  Addresses the problem of learning a binary classifier given Positive and Unlabeled data (PU learning) in high-dimensional settings.* Methods: Proposes a neural network-based data representation using a loss function to project unlabeled data into two clusters, amplified by vector quantization.* Results: Demonstrates improved performance compared to current state-of-the-art approaches on simulated PU data, with theoretical justification for the two-cluster-based approach and algorithmic choices.Here’s the format you requested:* For: &lt;what are the paper written for?&gt;* Methods: &lt;what methods the paper use?&gt;* Results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
In this paper, we address the problem of learning a binary (positive vs. negative) classifier given Positive and Unlabeled data commonly referred to as PU learning. Although rudimentary techniques like clustering, out-of-distribution detection, or positive density estimation can be used to solve the problem in low-dimensional settings, their efficacy progressively deteriorates with higher dimensions due to the increasing complexities in the data distribution. In this paper we propose to learn a neural network-based data representation using a loss function that can be used to project the unlabeled data into two (positive and negative) clusters that can be easily identified using simple clustering techniques, effectively emulating the phenomenon observed in low-dimensional settings. We adopt a vector quantization technique for the learned representations to amplify the separation between the learned unlabeled data clusters. We conduct experiments on simulated PU data that demonstrate the improved performance of our proposed method compared to the current state-of-the-art approaches. We also provide some theoretical justification for our two cluster-based approach and our algorithmic choices.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们解决了基于Positive和Unlabeled数据（PU学习）的二分类学习问题。虽然既有基础技术如聚类、外围检测或正类浓度估计可以在低维度设置下解决问题，但是其效果逐渐下降为高维度设置，由于数据分布的复杂度的增加。在这篇论文中，我们提议使用神经网络基于损失函数来学习数据表示，可以将未标注数据项映射到两个（正/_负）类中，并使用矢量量化技术来强化这些类之间的分离。我们在模拟的PU数据上进行实验，并证明了我们的提议方法与当前状态艺技术相比有更好的性能。我们还提供了一些理论基础和算法选择的正当性。
</details></li>
</ul>
<hr>
<h2 id="Logical-Languages-Accepted-by-Transformer-Encoders-with-Hard-Attention"><a href="#Logical-Languages-Accepted-by-Transformer-Encoders-with-Hard-Attention" class="headerlink" title="Logical Languages Accepted by Transformer Encoders with Hard Attention"></a>Logical Languages Accepted by Transformer Encoders with Hard Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03817">http://arxiv.org/abs/2310.03817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Barcelo, Alexander Kozachinskiy, Anthony Widjaja Lin, Vladimir Podolskii</li>
<li>for: 本研究主要针对于使用转换器Encoder来认识正则语言。</li>
<li>methods: 本文使用Unique Hard Attention Transformers（UHAT）和Average Hard Attention Transformers（AHAT）两种自注意机制进行研究。UHATEncoder只能认识${\sf AC}^0}$中的语言，而AHATEncoder可以认识${\sf TC}^0}$中的语言，但其表达能力仍然在${\sf AC}^0}$中。</li>
<li>results: 我们首先证明了UHATEncoder无法认识一个${\sf AC}^0}$语言。然而，我们也证明了UHATEncoder可以认识一个${\sf AC}^0}$语言中的一个rich Fragment，即所有可以用第一预言逻辑表示的语言，这个逻辑包括所有${\sf AC}^0}$中的常见语言。此外，我们还证明了AHATEncoder可以认识这些语言，并且可以在添加计数器的情况下进一步扩展这些语言。我们通过这些结果来 derive新的UHAT和AHAT表达能力的结论。<details>
<summary>Abstract</summary>
We contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to recognize only languages inside the circuit complexity class ${\sf AC}^0$, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, AHAT encoders can recognize languages outside ${\sf AC}^0$), but their expressive power still lies within the bigger circuit complexity class ${\sf TC}^0$, i.e., ${\sf AC}^0$-circuits extended by majority gates. We first show a negative result that there is an ${\sf AC}^0$-language that cannot be recognized by an UHAT encoder. On the positive side, we show that UHAT encoders can recognize a rich fragment of ${\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, includes, for example, all regular languages from ${\sf AC}^0$. We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms. We apply these results to derive new results on the expressive power of UHAT and AHAT up to permutation of letters (a.k.a. Parikh images).
</details>
<details>
<summary>摘要</summary>
我们贡献于形式语言的研究，可以被转换器Encoder认可。我们关注两种自注意机制：（1）UHAT（Unique Hard Attention Transformers）和（2）AHAT（Average Hard Attention Transformers）。UHATEncoder只能认可内部Circuit复杂性类${\sf AC}^0}$中的语言，即由多个poly-sized和深度bound boolean circuits组成的家族。相比之下，AHATEncoder可以认可外部${\sf AC}^0}$中的语言，但其表达力仍处于更大的Circuit复杂性类${\sf TC}^0}$中，即${\sf AC}^0}$circuits的扩展。我们首先显示了一个负结果，即${\sf AC}^0}$语言中有一个不可以被UHATEncoder认可。在正面上，我们显示了UHATEncoder可以认可${\sf AC}^0}$语言的Rich fragment，即所有可以用first-order logic表示的语言，其中包括所有${\sf AC}^0}$中的常见语言。然后我们显示AHATEncoder可以认可我们的逻辑中的所有语言， même when we enrich it with counting terms。我们将这些结果应用于derive new results on UHAT和AHAT的表达力，以及其 permutation of letters（即Parikh images）。
</details></li>
</ul>
<hr>
<h2 id="Fishnets-Information-Optimal-Scalable-Aggregation-for-Sets-and-Graphs"><a href="#Fishnets-Information-Optimal-Scalable-Aggregation-for-Sets-and-Graphs" class="headerlink" title="Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs"></a>Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03812">http://arxiv.org/abs/2310.03812</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. Lucas Makinen, Justin Alsing, Benjamin D. Wandelt</li>
<li>for: 这篇论文主要关注的是设计一种可以实现信息优化的嵌入对象的方法，以便进行统计学和 graphs 的数据处理。</li>
<li>methods: 本文提出了一种名为“Fishnets”的统计汇集方法，可以实现对于数据集的信息优化嵌入。</li>
<li>results: 作者透过实验表明，Fishnets 可以实现对于数据集的信息优化嵌入，并且可以在不同数据分布下保持稳定性。此外，Fishnets 可以与现有的 GNN 架构相容，并且可以在训练时间和学习parameters方面实现更好的性能。<details>
<summary>Abstract</summary>
Set-based learning is an essential component of modern deep learning and network science. Graph Neural Networks (GNNs) and their edge-free counterparts Deepsets have proven remarkably useful on ragged and topologically challenging datasets. The key to learning informative embeddings for set members is a specified aggregation function, usually a sum, max, or mean. We propose Fishnets, an aggregation strategy for learning information-optimal embeddings for sets of data for both Bayesian inference and graph aggregation. We demonstrate that i) Fishnets neural summaries can be scaled optimally to an arbitrary number of data objects, ii) Fishnets aggregations are robust to changes in data distribution, unlike standard deepsets, iii) Fishnets saturate Bayesian information content and extend to regimes where MCMC techniques fail and iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We show that by adopting a Fishnets aggregation scheme for message passing, GNNs can achieve state-of-the-art performance versus architecture size on ogbn-protein data over existing benchmarks with a fraction of learnable parameters and faster training time.
</details>
<details>
<summary>摘要</summary>
设计学习是现代深度学习和网络科学中的一个关键Component。图 neural networks (GNNs) 和它们的边 livre counterparts Deepsets 在异常和复杂的数据集上表现出了极其有用的特性。在学习集成的数据成员嵌入中，关键在于指定的汇聚函数，通常是总、最大或平均。我们提出了 Fishnets，一种汇聚策略，用于学习对集合数据的信息丰富嵌入，包括权化推理和图聚合。我们证明了以下四点：1. Fishnets神经摘要可以优化地扩展到任意数据对象数量上，而不会增加计算复杂性。2. Fishnets的汇聚方法对数据分布变化具有更高的稳定性，与标准Deepsets不同。3. Fishnets可以达到信息理论内存的极限，并在MCMC技术无法进行融合的场景下进行扩展。4. Fishnets可以作为GNNs中的替换汇聚方法，使得GNNs可以在已有的参数和训练时间下达到现有的性能水平。我们在ogbn-protein数据集上使用Fishnets汇聚方法进行消息传递，并证明了GNNs可以在已有的参数和训练时间下达到现有的性能水平，并且可以在很短的时间内训练。
</details></li>
</ul>
<hr>
<h2 id="Droplets-of-Good-Representations-Grokking-as-a-First-Order-Phase-Transition-in-Two-Layer-Networks"><a href="#Droplets-of-Good-Representations-Grokking-as-a-First-Order-Phase-Transition-in-Two-Layer-Networks" class="headerlink" title="Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks"></a>Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03789">http://arxiv.org/abs/2310.03789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noa Rubin, Inbar Seroussi, Zohar Ringel</li>
<li>for: 本研究探讨了深度神经网络（DNN）在训练中学习新特征的能力。特别是在最近报道的Grokking现象中，这一特点更加突出。</li>
<li>methods: 本研究使用了最新的特征学习理论——适应kernel方法，对两种教师模型（卷积波形和模块添加）进行了分析。</li>
<li>results: 研究发现，在Grokking过程中，DNN的状态与第一次相变过程中的混合阶段类似，DNN在这个阶段生成了有用的内部表示，与之前的转变前不同。<details>
<summary>Abstract</summary>
A key property of deep neural networks (DNNs) is their ability to learn new features during training. This intriguing aspect of deep learning stands out most clearly in recently reported Grokking phenomena. While mainly reflected as a sudden increase in test accuracy, Grokking is also believed to be a beyond lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here we apply a recent development in the theory of feature learning, the adaptive kernel approach, to two teacher-student models with cubic-polynomial and modular addition teachers. We provide analytical predictions on feature learning and Grokking properties of these models and demonstrate a mapping between Grokking and the theory of phase transitions. We show that after Grokking, the state of the DNN is analogous to the mixed phase following a first-order phase transition. In this mixed phase, the DNN generates useful internal representations of the teacher that are sharply distinct from those before the transition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Un-Kidnappable-Robot-Acoustic-Localization-of-Sneaking-People"><a href="#The-Un-Kidnappable-Robot-Acoustic-Localization-of-Sneaking-People" class="headerlink" title="The Un-Kidnappable Robot: Acoustic Localization of Sneaking People"></a>The Un-Kidnappable Robot: Acoustic Localization of Sneaking People</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03743">http://arxiv.org/abs/2310.03743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyu Yang, Patrick Grady, Samarth Brahmbhatt, Arun Balajee Vasudevan, Charles C. Kemp, James Hays</li>
<li>for: 研究是用来检测人员是否可以在机器人听到的声音中透露自己的位置。</li>
<li>methods: 使用高质量的4通道音频数据和360度RGB数据来训练模型，判断人员是否在静默移动中存在并且确定其位置。</li>
<li>results: 实现了通过只使用音频感知来跟踪静默移动的人员的功能，视频示例可以在项目页面上查看：<a target="_blank" rel="noopener" href="https://sites.google.com/view/unkidnappable-robot%E3%80%82">https://sites.google.com/view/unkidnappable-robot。</a><details>
<summary>Abstract</summary>
How easy is it to sneak up on a robot? We examine whether we can detect people using only the incidental sounds they produce as they move, even when they try to be quiet. We collect a robotic dataset of high-quality 4-channel audio paired with 360 degree RGB data of people moving in different indoor settings. We train models that predict if there is a moving person nearby and their location using only audio. We implement our method on a robot, allowing it to track a single person moving quietly with only passive audio sensing. For demonstration videos, see our project page: https://sites.google.com/view/unkidnappable-robot
</details>
<details>
<summary>摘要</summary>
如何轻松逃脱机器人的察视？我们研究是否可以通过机器人发出的各种各样的声音来探测人们的存在，即使他们尽力保持沉寂。我们收集了一个机器人数据集，包括高质量的4通道音频数据和360度RGB数据人们在不同的室内场景中移动。我们用音频数据来预测人们在附近的存在和位置。我们将方法应用于机器人上，让它通过只使用音频感知跟踪一个在沉寂状态下移动的人。视频示例请参考我们项目页面：https://sites.google.com/view/unkidnappable-robot
</details></li>
</ul>
<hr>
<h2 id="Stochastic-interpolants-with-data-dependent-couplings"><a href="#Stochastic-interpolants-with-data-dependent-couplings" class="headerlink" title="Stochastic interpolants with data-dependent couplings"></a>Stochastic interpolants with data-dependent couplings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03725">http://arxiv.org/abs/2310.03725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael S. Albergo, Mark Goldstein, Nicholas M. Boffi, Rajesh Ranganath, Eric Vanden-Eijnden</li>
<li>for: 这个论文旨在构建基于动态运输概率的 conditional generative models，以便使用class标签或连续嵌入来拟合target density。</li>
<li>methods: 该论文使用stochastic interpolants的框架来正式地\textit{couple} base density和target density，然后通过解决一个简单的方差损失问题来学习transport maps。</li>
<li>results: 实验表明，通过建立dependent coupling，可以在super-resolution和in-painting中获得更好的结果。<details>
<summary>Abstract</summary>
Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to \textit{couple} the base and the target densities. This enables us to incorporate information about class labels or continuous embeddings to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:生成模型，如流和散射，通过建立两个概率密度之间的连续时间映射。在这种工作中，我们使用权重 interpolant 框架来规范如何对基准密度和目标密度进行coupling。这使得我们能够通过包含类标签或连续嵌入的信息来构建动态传输图，这些图可以作为条件生成模型。我们表明这些传输图可以通过解决一个简单的方差损失回归问题来学习。我们通过实验展示了在超分辨和填充等应用中，建立依赖关系的好处。
</details></li>
</ul>
<hr>
<h2 id="Anytime-valid-t-tests-and-confidence-sequences-for-Gaussian-means-with-unknown-variance"><a href="#Anytime-valid-t-tests-and-confidence-sequences-for-Gaussian-means-with-unknown-variance" class="headerlink" title="Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance"></a>Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03722">http://arxiv.org/abs/2310.03722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongjian Wang, Aaditya Ramdas</li>
<li>for: 这个论文是为了描述一种用于泊松分布中方差未知的 случа的信任序列和信任程序。</li>
<li>methods: 该论文使用了通用非integrable martingale和扩展 Ville 不等式来构建信任序列和信任程序。</li>
<li>results: 该论文发现了一种新的信任序列和信任程序，它们分别使用了将 Лаи的平均替换为 Gaussian 混合，并将右 Haar 混合换为最大 likelihood 估计下的 null 值。该论文还分析了这些方法的信任间隔宽度，并提供了数值实验来比较和对比不同方法。<details>
<summary>Abstract</summary>
In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting confidence sequences, which have a curious dependence on the error probability $\alpha$. Numerical experiments are provided along the way to compare and contrast the various approaches.
</details>
<details>
<summary>摘要</summary>
在1976年，拉伊（Lai）构造了一个非致命序列对 Gaussian 分布中的均值μ的 confidence sequence。异常的是，他使用了一个不正确（右哈尔）混合和一个平均混合 над μ。在这里，我们仔细介绍拉伊的构造细节，使用通用非integrable martingale和扩展 Ville 不等式。尽管这不会生成一个积分过程（由于非integrability of his martingale），但它们可以用于Sequential t-test。在这篇文章中，我们开发了两个新的积分过程和 confidence sequence  для同一个设定：一个是在减少过滤中的测试martingale，另一个是在 canonical data 过滤中的 e-process。这两个积分过程分别由将拉伊的平均混合换成 Gaussian 混合，并将右哈尔混合换成 null 下最大可信度估计。我们还分析了这些 confidence sequence 的宽度，它们有一个怪异的依赖于错误probability α。我们在文章中提供了一些数字实验，以比较和对比不同的方法。
</details></li>
</ul>
<hr>
<h2 id="HeaP-Hierarchical-Policies-for-Web-Actions-using-LLMs"><a href="#HeaP-Hierarchical-Policies-for-Web-Actions-using-LLMs" class="headerlink" title="HeaP: Hierarchical Policies for Web Actions using LLMs"></a>HeaP: Hierarchical Policies for Web Actions using LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03720">http://arxiv.org/abs/2310.03720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paloma Sodhi, S. R. K. Branavan, Ryan McDonald</li>
<li>for: 这个论文旨在解决在网络上进行任务时存在基本挑战，即开放世界任务和网络界面的变化。</li>
<li>methods: 论文提出了一种基于大型自然语言模型（LLM）的解决方案，即通过分解网络任务为一系列低级别、闭环策略来解决问题。这些策略组成一个共享语法，可以将新的网络任务表示为这些策略的compositions。</li>
<li>results: 论文在一系列网络任务上进行了评估，包括MiniWoB++, WebArena、mock航空公司客服系统和实际网站交互，并与优先作出比较，结果表明它能够在使用数量级别下的数据量下表现出类似或更好的效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings. However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces. We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy. These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies. We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website interactions, and show that it is able to outperform prior works using orders of magnitude less data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Banach-Space-Optimality-of-Neural-Architectures-With-Multivariate-Nonlinearities"><a href="#Banach-Space-Optimality-of-Neural-Architectures-With-Multivariate-Nonlinearities" class="headerlink" title="Banach Space Optimality of Neural Architectures With Multivariate Nonlinearities"></a>Banach Space Optimality of Neural Architectures With Multivariate Nonlinearities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03696">http://arxiv.org/abs/2310.03696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Parhi, Michael Unser</li>
<li>For:  investigate the variational optimality of neural architectures with multivariate nonlinearities&#x2F;activation functions.* Methods: construct a new family of Banach spaces using regularization operators and the $k$-plane transform, prove a representer theorem.* Results: optimal neural architectures have skip connections and are tightly connected to orthogonal weight normalization and multi-index models, shed light on the regularity of functions learned by neural networks trained on data with multivariate nonlinearities.<details>
<summary>Abstract</summary>
We investigate the variational optimality (specifically, the Banach space optimality) of a large class of neural architectures with multivariate nonlinearities/activation functions. To that end, we construct a new family of Banach spaces defined via a regularization operator and the $k$-plane transform. We prove a representer theorem that states that the solution sets to learning problems posed over these Banach spaces are completely characterized by neural architectures with multivariate nonlinearities. These optimal architectures have skip connections and are tightly connected to orthogonal weight normalization and multi-index models, both of which have received considerable interest in the neural network community. Our framework is compatible with a number of classical nonlinearities including the rectified linear unit (ReLU) activation function, the norm activation function, and the radial basis functions found in the theory of thin-plate/polyharmonic splines. We also show that the underlying spaces are special instances of reproducing kernel Banach spaces and variation spaces. Our results shed light on the regularity of functions learned by neural networks trained on data, particularly with multivariate nonlinearities, and provide new theoretical motivation for several architectural choices found in practice.
</details>
<details>
<summary>摘要</summary>
我们研究一类 neural network 的可变优化问题（具体来说是 Banach space 优化问题），这类 neural network 具有多变量非线性/活动函数。为此，我们构造了一个新的 Banach space 家族，通过正则化算子和 $k$-plane transform 定义。我们证明了一个表示定理， stating that the solution sets of learning problems posed over these Banach spaces 是由 neural network  WITH multivariate nonlinearities 完全 caracterized。这些优化的架构具有跳过连接和正交 весаnormalization，与多indeces模型和 orthogonal weight normalization 密切相关。我们的框架与 Rectified Linear Unit (ReLU) 活动函数、 norm 活动函数和 radial basis functions 等等 classical nonlinearities 相容。我们还证明了这些下面空间是 reproducing kernel Banach space 和 variation space 的特例。我们的结果解释了 neural network 对数据进行学习时 learned 函数的 regularity，特别是 WITH multivariate nonlinearities，并提供了新的理论动机 для一些实践中的架构选择。
</details></li>
</ul>
<hr>
<h2 id="Multimarginal-generative-modeling-with-stochastic-interpolants"><a href="#Multimarginal-generative-modeling-with-stochastic-interpolants" class="headerlink" title="Multimarginal generative modeling with stochastic interpolants"></a>Multimarginal generative modeling with stochastic interpolants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03695">http://arxiv.org/abs/2310.03695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael S. Albergo, Nicholas M. Boffi, Michael Lindsey, Eric Vanden-Eijnden</li>
<li>for: 学习多个概率密度的联合分布，以恢复这些密度作为边缘分布。</li>
<li>methods: 基于动态传输推定的方法，使用速度和评价场来定义生成模型，并在一个扩展的时间变量上进行运动。</li>
<li>results: 提出了一种可以提取多向对应关系的多 marginal 生成模型，并在数据修复、风格转换和公平性等方面具有应用前景。同时，该方法还可以在双边分布情况下提高动态传输成本。在numerical例子中，提供了证明。<details>
<summary>Abstract</summary>
Given a set of $K$ probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals. The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals. We formalize an approach to this task within a generalization of the stochastic interpolant framework, leading to efficient learning algorithms built upon dynamical transport of measure. Our generative models are defined by velocity and score fields that can be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework. The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted. The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption. In addition, the multimarginal perspective enables an efficient algorithm for reducing the dynamical transport cost in the ordinary two-marginal setting. We demonstrate these capacities with several numerical examples.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:给定一个包含 $K$ 概率密度函数的集合，我们考虑多个概率密度函数的生成模型问题，即学习一个joint分布，使其中的每个分布都是这些概率密度函数的边界。我们将这个问题形式化为一个通用的随机 interpolant 框架下的一种方法，从而获得基于动态传输的有效学习算法。我们的生成模型由速度场和Score场定义，它们可以被视为在一个扩展了时间变量的 simplicx 上的最小二乘目标的解。这种传输在 simplicx 上受到所有边界的影响，并且我们示出了在多个边界之间建立对应关系的能力。这种对应关系有应用于样式传递、算法公平和数据修复等。此外，我们的多边界视角还允许在传统的两边界设定中减少动态传输成本。我们通过一些数值示例来证明这些能力。
</details></li>
</ul>
<hr>
<h2 id="Hadamard-Domain-Training-with-Integers-for-Class-Incremental-Quantized-Learning"><a href="#Hadamard-Domain-Training-with-Integers-for-Class-Incremental-Quantized-Learning" class="headerlink" title="Hadamard Domain Training with Integers for Class Incremental Quantized Learning"></a>Hadamard Domain Training with Integers for Class Incremental Quantized Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03675">http://arxiv.org/abs/2310.03675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Schiemer, Clemens JS Schaefer, Jayden Parker Vap, Mark James Horeni, Yu Emma Wang, Juan Ye, Siddharth Joshi</li>
<li>for: 提高Edge平台上的继续学习性能，满足隐私和延迟低的应用需求。</li>
<li>methods: 使用便宜的哈达马丁变换来实现减少精度的培训，并在精度减少后进行精度控制。</li>
<li>results: 在人活动识别和CIFAR100等数据集上实现了继续学习精度下降不到0.5%和3%，同时将所有矩阵乘法输入减少到4位，8位积分器。<details>
<summary>Abstract</summary>
Continual learning is a desirable feature in many modern machine learning applications, which allows in-field adaptation and updating, ranging from accommodating distribution shift, to fine-tuning, and to learning new tasks. For applications with privacy and low latency requirements, the compute and memory demands imposed by continual learning can be cost-prohibitive for resource-constraint edge platforms. Reducing computational precision through fully quantized training (FQT) simultaneously reduces memory footprint and increases compute efficiency for both training and inference. However, aggressive quantization especially integer FQT typically degrades model accuracy to unacceptable levels. In this paper, we propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications. We further determine which tensors need stochastic rounding and propose tiled matrix multiplication to enable low-bit width accumulators. We demonstrate the effectiveness of our technique on several human activity recognition datasets and CIFAR100 in a class incremental learning setting. We achieve less than 0.5% and 3% accuracy degradation while we quantize all matrix multiplications inputs down to 4-bits with 8-bit accumulators.
</details>
<details>
<summary>摘要</summary>
In our technique, we use tiled matrix multiplication to enable low-bit width accumulators, and we determine which tensors need stochastic rounding. We demonstrate the effectiveness of our technique on several human activity recognition datasets and CIFAR100 in a class incremental learning setting. Our results show that we can achieve less than 0.5% and 3% accuracy degradation while quantizing all matrix multiplications inputs down to 4-bits with 8-bit accumulators. This demonstrates that our technique can enable low-precision training for continual learning on resource-constrained edge platforms, while maintaining acceptable model accuracy.
</details></li>
</ul>
<hr>
<h2 id="Strategic-Evaluation-Subjects-Evaluators-and-Society"><a href="#Strategic-Evaluation-Subjects-Evaluators-and-Society" class="headerlink" title="Strategic Evaluation: Subjects, Evaluators, and Society"></a>Strategic Evaluation: Subjects, Evaluators, and Society</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03655">http://arxiv.org/abs/2310.03655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Laufer, Jon Kleinberg, Karen Levy, Helen Nissenbaum</li>
<li>For: The paper is written to explore the idea that evaluations can be understood as strategic interactions between the evaluator and the subject of evaluation, and how this can lead to misaligned goals and moral judgments.* Methods: The paper uses a model with three interacting agents - the decision subject, the evaluator, and society - to represent the process of evaluation and the strategic behaviors that can arise.* Results: The paper highlights the applicability of the model to a number of social systems where one or two players strategically undermine the others’ interests to advance their own, and argues that the moral standing of strategic behaviors depends on the moral standing of the evaluations and incentives that provoke such behaviors.<details>
<summary>Abstract</summary>
A broad current application of algorithms is in formal and quantitative measures of murky concepts -- like merit -- to make decisions. When people strategically respond to these sorts of evaluations in order to gain favorable decision outcomes, their behavior can be subjected to moral judgments. They may be described as 'gaming the system' or 'cheating,' or (in other cases) investing 'honest effort' or 'improving.' Machine learning literature on strategic behavior has tried to describe these dynamics by emphasizing the efforts expended by decision subjects hoping to obtain a more favorable assessment -- some works offer ways to preempt or prevent such manipulations, some differentiate 'gaming' from 'improvement' behavior, while others aim to measure the effort burden or disparate effects of classification systems. We begin from a different starting point: that the design of an evaluation itself can be understood as furthering goals held by the evaluator which may be misaligned with broader societal goals. To develop the idea that evaluation represents a strategic interaction in which both the evaluator and the subject of their evaluation are operating out of self-interest, we put forward a model that represents the process of evaluation using three interacting agents: a decision subject, an evaluator, and society, representing a bundle of values and oversight mechanisms. We highlight our model's applicability to a number of social systems where one or two players strategically undermine the others' interests to advance their own. Treating evaluators as themselves strategic allows us to re-cast the scrutiny directed at decision subjects, towards the incentives that underpin institutional designs of evaluations. The moral standing of strategic behaviors often depend on the moral standing of the evaluations and incentives that provoke such behaviors.
</details>
<details>
<summary>摘要</summary>
一种广泛应用的算法是在正式和量化度量朦杂概念（如优劣）进行决策。当人们在这些类型的评估中策略地回应，以获得更有利的决策结果时，他们的行为可能会被道德判断。他们可能会被描述为“游戏系统”或“诈骗”，或者（在其他情况下）投入“正直努力”或“改进”行为。机器学习文献中关于策略行为的描述通常强调试者的努力以获得更有利的评估结果，一些作品提供了预防或避免这种欺骗的方法，一些作品将“游戏”行为与“改进”行为分开，而其他作品则尝试测量评估系统中的努力负担或不同效果。我们从不同的起点开始：评估设计本身可以被理解为评估者所持的目标，这些目标可能与更广泛的社会目标不一致。为了发展这一想法，我们提出了一个模型，表示评估过程中的三个交互者：决策者、评估者和社会，代表一个Bundle of values和监督机制。我们强调我们的模型在多种社会系统中是可适用的，其中一个或两个玩家通过策略性的方式推翻另一个玩家的利益，以提高自己的利益。当评估者被视为自己是策略的时，我们可以重新定位评估审查的注意力，从而把注意力转移到评估设计中的机制和激励机制。strategic behaviors的道德地位常常取决于评估和激励机制的道德地位。
</details></li>
</ul>
<hr>
<h2 id="Extreme-sparsification-of-physics-augmented-neural-networks-for-interpretable-model-discovery-in-mechanics"><a href="#Extreme-sparsification-of-physics-augmented-neural-networks-for-interpretable-model-discovery-in-mechanics" class="headerlink" title="Extreme sparsification of physics-augmented neural networks for interpretable model discovery in mechanics"></a>Extreme sparsification of physics-augmented neural networks for interpretable model discovery in mechanics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03652">http://arxiv.org/abs/2310.03652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan N. Fuhg, Reese E. Jones, Nikolaos Bouklas</li>
<li>for: 这个论文旨在提出一种基于神经网络的数据驱动 constitutive 模型，以便轻松地包含物理和机制性约束，并且可以超越现有的时间消耗大量的现象学 constitutive 法则。</li>
<li>methods: 这个论文使用了 Physics-augmented neural network-based constitutive models，通过使用矫正的 $L^{0}$-正则化来保持信任性，同时实现可解释性。</li>
<li>results: 论文表明，这种方法可靠地获得可解释性和信任性的 constitutive 模型，并且可以应用于压缩和不压缩的 hyperelasticity、yield 函数和硬化模型。<details>
<summary>Abstract</summary>
Data-driven constitutive modeling with neural networks has received increased interest in recent years due to its ability to easily incorporate physical and mechanistic constraints and to overcome the challenging and time-consuming task of formulating phenomenological constitutive laws that can accurately capture the observed material response. However, even though neural network-based constitutive laws have been shown to generalize proficiently, the generated representations are not easily interpretable due to their high number of trainable parameters. Sparse regression approaches exist that allow to obtaining interpretable expressions, but the user is tasked with creating a library of model forms which by construction limits their expressiveness to the functional forms provided in the libraries. In this work, we propose to train regularized physics-augmented neural network-based constitutive models utilizing a smoothed version of $L^{0}$-regularization. This aims to maintain the trustworthiness inherited by the physical constraints, but also enables interpretability which has not been possible thus far on any type of machine learning-based constitutive model where model forms were not assumed a-priory but were actually discovered. During the training process, the network simultaneously fits the training data and penalizes the number of active parameters, while also ensuring constitutive constraints such as thermodynamic consistency. We show that the method can reliably obtain interpretable and trustworthy constitutive models for compressible and incompressible hyperelasticity, yield functions, and hardening models for elastoplasticity, for synthetic and experimental data.
</details>
<details>
<summary>摘要</summary>
“数据驱动的构成模型使用神经网络received increased interest in recent years due to its ability to easily incorporatephysical和mechanistic constraints和 overcome the challenging and time-consuming task of formulatingphenomenological constitutive laws that can accurately capture the observed material response. However, even though neural network-based constitutive laws have been shown to generalize proficiently, the generated representations are not easily interpretable due to their high number of trainable parameters. Sparse regression approaches exist that allow obtaining interpretable expressions, but the user is tasked with creating a library of model forms which by construction limits their expressiveness to the functional forms provided in the libraries. In this work, we propose to train regularized physics-augmented neural network-based constitutive models utilizing a smoothed version of $L^{0}$-regularization. This aims to maintain the trustworthiness inherited by the physical constraints, but also enables interpretability which has not been possible thus far on any type of machine learning-based constitutive model where model forms were not assumed a-priori but were actually discovered. During the training process, the network simultaneously fits the training data and penalizes the number of active parameters, while also ensuring constitutive constraints such as thermodynamic consistency. We show that the method can reliably obtain interpretable and trustworthy constitutive models for compressible and incompressible hyperelasticity, yield functions, and hardening models for elastoplasticity, for synthetic and experimental data.”Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Fairness-for-Human-AI-Collaboration"><a href="#Rethinking-Fairness-for-Human-AI-Collaboration" class="headerlink" title="Rethinking Fairness for Human-AI Collaboration"></a>Rethinking Fairness for Human-AI Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03647">http://arxiv.org/abs/2310.03647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haosen Ge, Hamsa Bastani, Osbert Bastani</li>
<li>for:  Ensuring equitable outcomes in human-AI collaboration, especially when human decision-makers do not comply perfectly with algorithmic decisions.</li>
<li>methods:  Propose a new approach called compliance-robustly fair algorithmic recommendations, which are guaranteed to improve fairness in decisions regardless of the human’s compliance pattern. An optimization strategy is also proposed to identify the best performance-improving compliance-robustly fair policy.</li>
<li>results:  Show that it may be infeasible to design algorithmic recommendations that are simultaneously fair in isolation, compliance-robustly fair, and more accurate than the human policy, which means that enforcing traditional fairness constraints may not be desirable if our goal is to improve the equity and accuracy of human-AI collaboration.<details>
<summary>Abstract</summary>
Existing approaches to algorithmic fairness aim to ensure equitable outcomes if human decision-makers comply perfectly with algorithmic decisions. However, perfect compliance with the algorithm is rarely a reality or even a desirable outcome in human-AI collaboration. Yet, recent studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy. As a consequence, ensuring equitable outcomes requires fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern. We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern. We propose a simple optimization strategy to identify the best performance-improving compliance-robustly fair policy. However, we show that it may be infeasible to design algorithmic recommendations that are simultaneously fair in isolation, compliance-robustly fair, and more accurate than the human policy; thus, if our goal is to improve the equity and accuracy of human-AI collaboration, it may not be desirable to enforce traditional fairness constraints.
</details>
<details>
<summary>摘要</summary>
现有的算法公平方法尝试确保算法决策的结果是公平的，只要人类决策者完全遵循算法的决策。然而，完美地遵循算法是非常rare的现实或者even desirable outcome in human-AI collaboration。实际上，latest studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy.因此，保证公平的结果需要fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern。我们定义了“compliance-robustly fair” algorithmic recommendations，meaning that the recommendations are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern。我们还提出了一种简单的优化策略来标识最佳性能改进的compliance-robustly fair policy。然而，我们表明，可能无法设计算法建议，同时是孤立的公平，compliance-robustly fair，和人类政策更高的准确性。因此，如果我们的目标是提高人类-AI合作的公平和准确性，那么可能不是desirable to enforce traditional fairness constraints。
</details></li>
</ul>
<hr>
<h2 id="Distributional-PAC-Learning-from-Nisan’s-Natural-Proofs"><a href="#Distributional-PAC-Learning-from-Nisan’s-Natural-Proofs" class="headerlink" title="Distributional PAC-Learning from Nisan’s Natural Proofs"></a>Distributional PAC-Learning from Nisan’s Natural Proofs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03641">http://arxiv.org/abs/2310.03641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ari Karchmer</li>
<li>for: 这个论文的目的是为了研究自然证明是否可以导致有效地学习Lambda-Circuit，并且是否可以扩展到Lambda不等于AC^0[p]和Valiant的PAC模型中。</li>
<li>methods: 这篇论文使用了自然证明的概念，并使用了一种通信复杂度Argument来提出了一种新的分布型PAC模型，以及一些有关这种模型的性质和应用。</li>
<li>results: 论文的主要结论是，在某些特定的自然证明情况下，可以有效地学习Lambda-Circuit在新的分布型PAC模型中，并且可以应用于深度2的多数票电路、多面体和DNF等问题。此外，论文还证明了这种模型的一些重要性质和应用。<details>
<summary>Abstract</summary>
(Abridged) Carmosino et al. (2016) demonstrated that natural proofs of circuit lower bounds for \Lambda imply efficient algorithms for learning \Lambda-circuits, but only over the uniform distribution, with membership queries, and provided \AC^0[p] \subseteq \Lambda. We consider whether this implication can be generalized to \Lambda \not\supseteq \AC^0[p], and to learning algorithms in Valiant's PAC model, which use only random examples and learn over arbitrary example distributions. We give results of both positive and negative flavor.   On the negative side, we observe that if, for every circuit class \Lambda, the implication from natural proofs for \Lambda to learning \Lambda-circuits in Valiant's PAC model holds, then there is a polynomial time solution to O(n^{1.5})-uSVP (unique Shortest Vector Problem), and polynomial time quantum solutions to O(n^{1.5})-SVP (Shortest Vector Problem) and O(n^{1.5})-SIVP (Shortest Independent Vector Problem). This indicates that whether natural proofs for \Lambda imply efficient learning algorithms for \Lambda in Valiant's PAC model may depend on \Lambda.   On the positive side, our main result is that specific natural proofs arising from a type of communication complexity argument (e.g., Nisan (1993), for depth-2 majority circuits) imply PAC-learning algorithms in a new distributional variant of Valiant's model. Our distributional PAC model is stronger than the average-case prediction model of Blum et al (1993) and the heuristic PAC model of Nanashima (2021), and has several important properties which make it of independent interest, such as being boosting-friendly. The main applications of our result are new distributional PAC-learning algorithms for depth-2 majority circuits, polytopes and DNFs over natural target distributions, as well as the nonexistence of encoded-input weak PRFs that can be evaluated by depth-2 majority circuits.
</details>
<details>
<summary>摘要</summary>
(简化版) 卡尔莫西诺等 (2016) 表明自然证明的电路下界可以efficient地学习Lambda电路，但只有在均匀分布下，使用会员查询，并且符号集合为\AC^0[p]。我们考虑了这种推论是否可以推广到\Lambda不包含\AC^0[p]，以及在 ва利安特的PAC模型中学习算法，使用随机示例并学习到任意示例分布。我们得到了正面和负面的结果。负面方面，我们发现如果对每个电路类\Lambda，自然证明可以导致学习\Lambda电路在 ва利安特的PAC模型中，那么存在一个 polynomial time的解决方案，可以在 O(n^{1.5}) 时间内解决唯一最短 вектор问题（uSVP）。这表明自然证明是否可以导致学习\Lambda电路可能取决于\Lambda。正面方面，我们的主要结果是特定的自然证明， originating from a type of communication complexity argument (e.g., Nisan (1993), for depth-2 majority circuits)，可以导致 PAC-learning algorithms in a new distributional variant of Valiant's model。我们的分布式PAC模型更强于Blum等人 (1993) 的平均情况预测模型和Nanashima (2021) 的启发式PAC模型，并具有许多重要的性质，例如可以增强。主要应用包括新的分布式PAC-learning算法 для深度2的多数电路、多面体和 DNFs over natural target distributions，以及 depth-2 majority circuits 无法识别编码输入弱PRFs。
</details></li>
</ul>
<hr>
<h2 id="CLASSify-A-Web-Based-Tool-for-Machine-Learning"><a href="#CLASSify-A-Web-Based-Tool-for-Machine-Learning" class="headerlink" title="CLASSify: A Web-Based Tool for Machine Learning"></a>CLASSify: A Web-Based Tool for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03618">http://arxiv.org/abs/2310.03618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">https://github.com/Aastha2104/Parkinson-Disease-Prediction</a></li>
<li>paper_authors: Aaron D. Mullen, Samuel E. Armstrong, Jeff Talbert, V. K. Cody Bumgardner</li>
<li>for: 用于简化机器学习分类问题的解决方案，帮助研究者不需具备深入的机器学习知识就可以使用这种技术。</li>
<li>methods: 使用自动化工具，提供多种模型和方法，同时支持生成synthetic数据，进行特征评估，并生成解释性分数以显示影响输出最大的特征。</li>
<li>results: 提供了一个开源的工具，可以帮助研究者更容易地解决分类问题，不需要深入的机器学习知识。<details>
<summary>Abstract</summary>
Machine learning classification problems are widespread in bioinformatics, but the technical knowledge required to perform model training, optimization, and inference can prevent researchers from utilizing this technology. This article presents an automated tool for machine learning classification problems to simplify the process of training models and producing results while providing informative visualizations and insights into the data. This tool supports both binary and multiclass classification problems, and it provides access to a variety of models and methods. Synthetic data can be generated within the interface to fill missing values, balance class labels, or generate entirely new datasets. It also provides support for feature evaluation and generates explainability scores to indicate which features influence the output the most. We present CLASSify, an open-source tool for simplifying the user experience of solving classification problems without the need for knowledge of machine learning.
</details>
<details>
<summary>摘要</summary>
machine learning 分类问题在生物信息学中广泛，但技术知识要求进行模型训练、优化和推理可能会阻碍研究人员使用这种技术。本文介绍了一个自动化工具来简化机器学习分类问题的训练模型和获得结果，并提供了有用的可视化和数据分析。这个工具支持 binary 和多类分类问题，并提供了多种模型和方法。在界面中，您可以生成假数据来填充缺失的值、平衡类别标签或生成全新的数据集。此外，它还提供了特征评估和生成可读性分数，以指示影响输出的特征。我们介绍了 CLASSify，一个开源的工具来简化解决分类问题的用户体验，无需了解机器学习知识。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Machine-Learning-for-Social-Good-Reframing-the-Adversary-as-an-Ally"><a href="#Adversarial-Machine-Learning-for-Social-Good-Reframing-the-Adversary-as-an-Ally" class="headerlink" title="Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally"></a>Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03614">http://arxiv.org/abs/2310.03614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha</li>
<li>for: 这个论文旨在探讨 AdvML for Social Good (AdvML4G) 这个新兴领域，它利用了 Adversarial Machine Learning (AdvML) 的漏洞，开发出了一系列的 про社会应用程序。</li>
<li>methods: 本论文使用了一种稠密的分析方法，涵盖了 AdvML4G 领域的各种研究和应用，包括一个分类法和一个综述。</li>
<li>results: 研究发现，AdvML4G 领域的工作具有很高的创新性和可行性，但同时也存在一些挑战和未解决的问题，需要进一步的研究和发展。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have been the driving force behind many of the recent advances in machine learning. However, research has shown that DNNs are vulnerable to adversarial examples -- input samples that have been perturbed to force DNN-based models to make errors. As a result, Adversarial Machine Learning (AdvML) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. In addition, DNNs have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in anti-social AI applications. The emergence of new AI technologies that leverage Large Language Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing anti-social applications at scale. AdvML for Social Good (AdvML4G) is an emerging field that repurposes the AdvML bug to invent pro-social applications. Regulators, practitioners, and researchers should collaborate to encourage the development of pro-social applications and hinder the development of anti-social ones. In this work, we provide the first comprehensive review of the emerging field of AdvML4G. This paper encompasses a taxonomy that highlights the emergence of AdvML4G, a discussion of the differences and similarities between AdvML4G and AdvML, a taxonomy covering social good-related concepts and aspects, an exploration of the motivations behind the emergence of AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the works that utilize AdvML4G as an auxiliary tool for innovating pro-social applications. Finally, we elaborate upon various challenges and open research issues that require significant attention from the research community.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）已经成为机器学习的驱动力，但是研究表明，DNN受到了攻击性示例的影响，导致机器学习领域内的攻击机器学习（AdvML）得到了广泛的关注。研究人员在不同的场景和模式下调查了这些攻击性示例，并发现DNN中嵌入的偏见和不可解释的预测结果，可能导致反社会的AI应用程序。新的AI技术的出现，如大语言模型（LLM），如ChatGPT和GPT-4，会增加反社会应用程序的风险。为了鼓励开发负面向社会的应用程序，并阻止开发反社会应用程序， regulators、实践者和研究人员应该共同合作。在这篇评论中，我们提供了机器学习领域的第一份全面评论，涵盖了AdvML4G的出现、与AdvML的区别和相似性、社会好的相关概念和方面、AdvML4G的动机以及使用AdvML4G作为创新负面向社会应用程序的auxiliary工具的各种研究工作。最后，我们还详细介绍了需要研究社区的各种挑战和开放问题。
</details></li>
</ul>
<hr>
<h2 id="GENER-A-Parallel-Layer-Deep-Learning-Network-To-Detect-Gene-Gene-Interactions-From-Gene-Expression-Data"><a href="#GENER-A-Parallel-Layer-Deep-Learning-Network-To-Detect-Gene-Gene-Interactions-From-Gene-Expression-Data" class="headerlink" title="GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data"></a>GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03611">http://arxiv.org/abs/2310.03611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahmedfakhry47/gener">https://github.com/ahmedfakhry47/gener</a></li>
<li>paper_authors: Ahmed Fakhry, Raneem Khafagy, Adriaan-Alexander Ludl</li>
<li>for: identifying novel gene-gene interactions based on known gene expressions and interaction data</li>
<li>methods: parallel-layer deep learning network (GENER) using gene expression data</li>
<li>results: outperformed competing methods with an average AUROC score of 0.834 on the combined BioGRID&amp;DREAM5 dataset<details>
<summary>Abstract</summary>
Detecting and discovering new gene interactions based on known gene expressions and gene interaction data presents a significant challenge. Various statistical and deep learning methods have attempted to tackle this challenge by leveraging the topological structure of gene interactions and gene expression patterns to predict novel gene interactions. In contrast, some approaches have focused exclusively on utilizing gene expression profiles. In this context, we introduce GENER, a parallel-layer deep learning network designed exclusively for the identification of gene-gene relationships using gene expression data. We conducted two training experiments and compared the performance of our network with that of existing statistical and deep learning approaches. Notably, our model achieved an average AUROC score of 0.834 on the combined BioGRID&DREAM5 dataset, outperforming competing methods in predicting gene-gene interactions.
</details>
<details>
<summary>摘要</summary>
检测和发现新的基因交互是一项具有挑战性的任务。不同的统计学和深度学习方法尝试利用基因交互的topological结构和基因表达特征来预测新的基因交互。然而，一些方法仅仅利用基因表达 profiling。在这个上下文中，我们介绍GENER，一种专门为基因交互预测设计的并行层深度学习网络。我们进行了两个训练实验，并与现有的统计学和深度学习方法进行比较。可以注意的是，我们的模型在combined BioGRID&DREAM5集合上 achieved an average AUROC score of 0.834，在预测基因交互方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Time-Series-Analysis-Approaches-Utilized-in-Research-Papers-to-Forecast-COVID-19-Cases-in-Africa-A-Literature-Review"><a href="#Comparing-Time-Series-Analysis-Approaches-Utilized-in-Research-Papers-to-Forecast-COVID-19-Cases-in-Africa-A-Literature-Review" class="headerlink" title="Comparing Time-Series Analysis Approaches Utilized in Research Papers to Forecast COVID-19 Cases in Africa: A Literature Review"></a>Comparing Time-Series Analysis Approaches Utilized in Research Papers to Forecast COVID-19 Cases in Africa: A Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03606">http://arxiv.org/abs/2310.03606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Ebadi, Ebrahim Sahafizadeh</li>
<li>for: 本文旨在比较在非洲预测COVID-19病例的不同时间序分析方法。</li>
<li>methods: 本研究使用英语论文，从2020年1月至2023年7月进行了系统性的搜索，特意搜索了在非洲COVID-19数据集上使用时间序分析方法的论文。使用了PubMed、Google Scholar、Scopus和Web of Science等数据库。研究论文经过了评估程序，以提取相关的时间序分析模型实施和性能信息。</li>
<li>results: 本研究发现了不同的方法ologies，评估了它们在预测病毒传播的有效性和局限性。结果可能为预测COVID-19病例提供更深入的理解，未来研究应考虑这些理解，以提高时间序分析模型和探索不同方法的集成，以提高公共卫生决策。<details>
<summary>Abstract</summary>
This literature review aimed to compare various time-series analysis approaches utilized in forecasting COVID-19 cases in Africa. The study involved a methodical search for English-language research papers published between January 2020 and July 2023, focusing specifically on papers that utilized time-series analysis approaches on COVID-19 datasets in Africa. A variety of databases including PubMed, Google Scholar, Scopus, and Web of Science were utilized for this process. The research papers underwent an evaluation process to extract relevant information regarding the implementation and performance of the time-series analysis models. The study highlighted the different methodologies employed, evaluating their effectiveness and limitations in forecasting the spread of the virus. The result of this review could contribute deeper insights into the field, and future research should consider these insights to improve time series analysis models and explore the integration of different approaches for enhanced public health decision-making.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇文献综述旨在比较在非洲地区预测COVID-19确诊病例的不同时间序分析方法。该研究在2020年1月至2023年7月期间，通过英文研究论文检索，特定地点在非洲使用时间序分析方法进行COVID-19数据分析。各种数据库，如PubMed、Google学术、Scopus和Web of Science等，都被使用于这个过程中。审查的研究论文中的信息，包括时间序分析模型的实施和性能评估。该研究报告了不同方法的应用和局限性，以及预测病毒传播的效果。这些结果可以为预测领域提供更深入的理解，并且未来的研究应该考虑这些意见，以改进时间序分析模型并探讨不同方法的集成，以提高公共卫生决策。
</details></li>
</ul>
<hr>
<h2 id="Sampling-via-Gradient-Flows-in-the-Space-of-Probability-Measures"><a href="#Sampling-via-Gradient-Flows-in-the-Space-of-Probability-Measures" class="headerlink" title="Sampling via Gradient Flows in the Space of Probability Measures"></a>Sampling via Gradient Flows in the Space of Probability Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03597">http://arxiv.org/abs/2310.03597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Chen, Daniel Zhengyu Huang, Jiaoyang Huang, Sebastian Reich, Andrew M Stuart</li>
<li>for: 采样target概率分布中的一个基本挑战是computational科学和工程中的一个基本问题，recent work shows that algorithms derived by considering gradient flows in the space of probability measures 开辟了新的开发途径。这篇论文提供了三种贡献，分别是：</li>
<li>methods:  gradient flows中的设计元素的研究。any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms。我们的第一贡献是：Kullback-Leibler divergence作为能量函数，gradient flows resulting from it do not depend on the normalization constant of the target distribution。我们的第二贡献是：study the choice of metric from the perspective of invariance。Fisher-Rao metric is known as the unique choice (up to scaling) that is diffeomorphism invariant。as a computationally tractable alternative, we introduce a relaxed, affine invariance property for the metrics and gradient flows。in particular, we construct various affine invariant Wasserstein and Stein gradient flows。</li>
<li>results: affine invariant gradient flows are shown to behave more favorably than their non-affine-invariant counterparts when sampling highly anisotropic distributions, both in theory and by using particle methods。we also study, and develop efficient algorithms based on Gaussian approximations of the gradient flows; this leads to an alternative to particle methods。we establish connections between various Gaussian approximate gradient flows, discuss their relation to gradient methods arising from parametric variational inference, and study their convergence properties both theoretically and numerically。<details>
<summary>Abstract</summary>
Sampling a target probability distribution with an unknown normalization constant is a fundamental challenge in computational science and engineering. Recent work shows that algorithms derived by considering gradient flows in the space of probability measures open up new avenues for algorithm development. This paper makes three contributions to this sampling approach by scrutinizing the design components of such gradient flows. Any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms. Our first contribution is to show that the Kullback-Leibler divergence, as an energy functional, has the unique property (among all f-divergences) that gradient flows resulting from it do not depend on the normalization constant of the target distribution. Our second contribution is to study the choice of metric from the perspective of invariance. The Fisher-Rao metric is known as the unique choice (up to scaling) that is diffeomorphism invariant. As a computationally tractable alternative, we introduce a relaxed, affine invariance property for the metrics and gradient flows. In particular, we construct various affine invariant Wasserstein and Stein gradient flows. Affine invariant gradient flows are shown to behave more favorably than their non-affine-invariant counterparts when sampling highly anisotropic distributions, in theory and by using particle methods. Our third contribution is to study, and develop efficient algorithms based on Gaussian approximations of the gradient flows; this leads to an alternative to particle methods. We establish connections between various Gaussian approximate gradient flows, discuss their relation to gradient methods arising from parametric variational inference, and study their convergence properties both theoretically and numerically.
</details>
<details>
<summary>摘要</summary>
取样target概率分布的问题是计算科学和工程中的基础挑战。近期的研究表明，通过考虑梯度流在概率分布空间上来开发算法，可以开辟新的算法发展途径。本文对这种取样方法做出三项贡献：1. 我们显示出，Kullback-Leibler divergence作为能函数，其梯度流不受目标分布的normalization常数影响。这意味着，通过Kullback-Leibler divergence来定义梯度流，可以避免一些困难，例如，对目标分布的normalization常数进行估计。2. 我们研究了选择metric的问题，从diffusion invariants的角度出发。Fisher-Rao metric是唯一不同Scaling的diffusion invariants metric，但它可能是computationally tractable的问题。我们引入了一种relaxed, affine invariants property for metrics and gradient flows，并构造了各种affine invariants Wasserstein和Stein gradient flows。我们证明了，在取样高度非对称分布时，affine invariants gradient flows会表现更加优于非affine invariants counterparts。3. 我们研究了基于Gaussian approximations的gradient flows的问题，并开发了一种alternative to particle methods。我们建立了各种Gaussian approximate gradient flows的连接，并考虑它们与参数化variational inference中的gradient methods的关系。我们还研究了它们的数学性和numerical性的性质。
</details></li>
</ul>
<hr>
<h2 id="TimeGPT-1"><a href="#TimeGPT-1" class="headerlink" title="TimeGPT-1"></a>TimeGPT-1</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03589">http://arxiv.org/abs/2310.03589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Azul Garza, Max Mergenthaler-Canseco</li>
<li>for: 这个论文旨在提出一种基于时间序列的基础模型，能够生成准确的预测结果，并不需要训练数据集包含该预测目标。</li>
<li>methods: 该论文使用了一种基于Transformer的强大预测模型，并通过预训练和自适应训练来提高模型的性能。</li>
<li>results: 论文的实验结果表明，TimeGPT模型在不同的时间序列数据集上的预测性能强于传统的统计学、机器学习和深度学习方法，同时具有较高的效率和简洁性。<details>
<summary>Abstract</summary>
In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了 TimeGPT，首个适用于时间序列的基础模型，能够生成准确的预测结果，无需见到训练数据。我们对我们的预训练模型进行了对比，与统计学、机器学习和深度学习方法进行了比较，结果显示，TimeGPT零shot推理性能高效简单。我们的研究表明，从其他人工智能领域的技术可以有效应用于时间序列分析。我们认为，大规模的时间序列模型将为精确预测和减少不确定性提供了一个激动人心的机会，通过利用当代深度学习技术。
</details></li>
</ul>
<hr>
<h2 id="Smoothing-Methods-for-Automatic-Differentiation-Across-Conditional-Branches"><a href="#Smoothing-Methods-for-Automatic-Differentiation-Across-Conditional-Branches" class="headerlink" title="Smoothing Methods for Automatic Differentiation Across Conditional Branches"></a>Smoothing Methods for Automatic Differentiation Across Conditional Branches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03585">http://arxiv.org/abs/2310.03585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/philipp-andelfinger/discograd">https://github.com/philipp-andelfinger/discograd</a></li>
<li>paper_authors: Justin N. Kreikemeyer, Philipp Andelfinger</li>
<li>for: 这个论文的目的是提出一种基于幂分析的自动分配方法，以便在控制流构造引入的缺陷中进行优化。</li>
<li>methods: 这个论文使用了幂分析（SI）和自动导数（AD）两种方法，以计算分支程序的导数。</li>
<li>results: 研究人员通过对分支程序的输出进行幂分析，并使用自动导数来计算导数，从而实现了基于导数的参数synthesis。这种方法可以在分支程序中进行高效的优化。<details>
<summary>Abstract</summary>
Programs involving discontinuities introduced by control flow constructs such as conditional branches pose challenges to mathematical optimization methods that assume a degree of smoothness in the objective function's response surface. Smooth interpretation (SI) is a form of abstract interpretation that approximates the convolution of a program's output with a Gaussian kernel, thus smoothing its output in a principled manner. Here, we combine SI with automatic differentiation (AD) to efficiently compute gradients of smoothed programs. In contrast to AD across a regular program execution, these gradients also capture the effects of alternative control flow paths. The combination of SI with AD enables the direct gradient-based parameter synthesis for branching programs, allowing for instance the calibration of simulation models or their combination with neural network models in machine learning pipelines. We detail the effects of the approximations made for tractability in SI and propose a novel Monte Carlo estimator that avoids the underlying assumptions by estimating the smoothed programs' gradients through a combination of AD and sampling. Using DiscoGrad, our tool for automatically translating simple C++ programs to a smooth differentiable form, we perform an extensive evaluation. We compare the combination of SI with AD and our Monte Carlo estimator to existing gradient-free and stochastic methods on four non-trivial and originally discontinuous problems ranging from classical simulation-based optimization to neural network-driven control. While the optimization progress with the SI-based estimator depends on the complexity of the programs' control flow, our Monte Carlo estimator is competitive in all problems, exhibiting the fastest convergence by a substantial margin in our highest-dimensional problem.
</details>
<details>
<summary>摘要</summary>
Programs with discontinuities caused by control flow constructs, such as conditional branches, pose challenges to mathematical optimization methods that assume a degree of smoothness in the objective function's response surface. Smooth interpretation (SI) is a form of abstract interpretation that approximates the convolution of a program's output with a Gaussian kernel, thus smoothing its output in a principled manner. We combine SI with automatic differentiation (AD) to efficiently compute gradients of smoothed programs. In contrast to AD across a regular program execution, these gradients also capture the effects of alternative control flow paths. The combination of SI with AD enables the direct gradient-based parameter synthesis for branching programs, allowing for instance the calibration of simulation models or their combination with neural network models in machine learning pipelines. We discuss the approximations made for tractability in SI and propose a novel Monte Carlo estimator that avoids the underlying assumptions by estimating the smoothed programs' gradients through a combination of AD and sampling. Using DiscoGrad, our tool for automatically translating simple C++ programs to a smooth differentiable form, we perform an extensive evaluation. We compare the combination of SI with AD and our Monte Carlo estimator to existing gradient-free and stochastic methods on four non-trivial and originally discontinuous problems ranging from classical simulation-based optimization to neural network-driven control. While the optimization progress with the SI-based estimator depends on the complexity of the programs' control flow, our Monte Carlo estimator is competitive in all problems, exhibiting the fastest convergence by a substantial margin in our highest-dimensional problem.
</details></li>
</ul>
<hr>
<h2 id="Targeted-Adversarial-Attacks-on-Generalizable-Neural-Radiance-Fields"><a href="#Targeted-Adversarial-Attacks-on-Generalizable-Neural-Radiance-Fields" class="headerlink" title="Targeted Adversarial Attacks on Generalizable Neural Radiance Fields"></a>Targeted Adversarial Attacks on Generalizable Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03578">http://arxiv.org/abs/2310.03578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andras Horvath, Csaba M. Jozsa</li>
<li>for: This paper discusses the vulnerability of Neural Radiance Fields (NeRFs) to adversarial attacks, and demonstrates the effectiveness of both low-intensity and targeted attacks.</li>
<li>methods: The paper uses NeRFs to synthesize high-quality images from sparse 2D observations, and employs both low-intensity and targeted adversarial attacks to evaluate the model’s robustness.</li>
<li>results: The paper shows that NeRFs can be vulnerable to both low-intensity and targeted adversarial attacks, and that the attacks can be robust enough to be used in real-world applications. Additionally, the paper demonstrates the ability to generate specific, predefined output scenes using targeted attacks.<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for 3D scene representation and rendering. These data-driven models can learn to synthesize high-quality images from sparse 2D observations, enabling realistic and interactive scene reconstructions. However, the growing usage of NeRFs in critical applications such as augmented reality, robotics, and virtual environments could be threatened by adversarial attacks.   In this paper we present how generalizable NeRFs can be attacked by both low-intensity adversarial attacks and adversarial patches, where the later could be robust enough to be used in real world applications. We also demonstrate targeted attacks, where a specific, predefined output scene is generated by these attack with success.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRFs) 近期出现为3D场景表示和渲染的强大工具。这些数据驱动模型可以从稀疏的2D观察中学习生成高质量图像，使得场景重建变得真实和交互式。然而，随着NeRFs在扩展实际应用，如增强现实、机器人和虚拟环境中的使用，它们可能会受到恶意攻击。  在这篇论文中，我们表明了通用NeRFs可以受到低强度攻击和攻击贴图的威胁。其中，攻击贴图可能够在实际应用中使用，并且可以实现特定、预先定义的输出场景。我们还示出了 Targeted 攻击，即通过攻击NeRFs来生成特定的场景。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-learning-a-flow-based-generative-model-from-limited-sample-complexity"><a href="#Analysis-of-learning-a-flow-based-generative-model-from-limited-sample-complexity" class="headerlink" title="Analysis of learning a flow-based generative model from limited sample complexity"></a>Analysis of learning a flow-based generative model from limited sample complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03575">http://arxiv.org/abs/2310.03575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spoc-group/diffusion_gmm">https://github.com/spoc-group/diffusion_gmm</a></li>
<li>paper_authors: Hugo Cui, Florent Krzakala, Eric Vanden-Eijnden, Lenka Zdeborová</li>
<li>for: 这个论文旨在训练一种基于流的生成模型，以 parametrize 一个二层自编码器，从高维 Gaussian 混合分布中采样。</li>
<li>methods: 这个论文使用了一种 shallow denoising auto-encoder 进行训练，并提供了一个精确的关闭式分析。</li>
<li>results: 研究结果显示，使用这种方法可以实现高维 Gaussian 混合分布中采样，并且可以提供一个 Bayes-optimal 的方法来评估模型的性能。<details>
<summary>Abstract</summary>
We study the problem of training a flow-based generative model, parametrized by a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture. We provide a sharp end-to-end analysis of the problem. First, we provide a tight closed-form characterization of the learnt velocity field, when parametrized by a shallow denoising auto-encoder trained on a finite number $n$ of samples from the target distribution. Building on this analysis, we provide a sharp description of the corresponding generative flow, which pushes the base Gaussian density forward to an approximation of the target density. In particular, we provide closed-form formulae for the distance between the mean of the generated mixture and the mean of the target mixture, which we show decays as $\Theta_n(\frac{1}{n})$. Finally, this rate is shown to be in fact Bayes-optimal.
</details>
<details>
<summary>摘要</summary>
我们研究一个基于流的生成模型， Parametrized by a two-layer autoencoder，可以抽样自高维 Gaussian 混合体。我们提供了锐利的终端分析。首先，我们提供了一个紧密的关闭式形式的learned velocity field，当 Parametrized by a shallow denoising autoencoder 在一定数量 $n$ of samples from the target distribution 上训练。 Building on this analysis, we provide a sharp description of the corresponding generative flow, which pushes the base Gaussian density forward to an approximation of the target density。 Specifically, we provide closed-form formulae for the distance between the mean of the generated mixture and the mean of the target mixture, which we show decays as $\Theta_n(\frac{1}{n})$. Finally, this rate is shown to be in fact Bayes-optimal。Here's the translation breakdown:* "We study the problem of training a flow-based generative model" becomes "我们研究一个基于流的生成模型"* "parametrized by a two-layer autoencoder" becomes "Parametrized by a two-layer autoencoder"* "to sample from a high-dimensional Gaussian mixture" becomes "可以抽样自高维 Gaussian 混合体"* "We provide a sharp end-to-end analysis of the problem" becomes "我们提供了锐利的终端分析"* "First, we provide a tight closed-form characterization of the learnt velocity field" becomes "首先，我们提供了一个紧密的关闭式形式的learned velocity field"* "when parametrized by a shallow denoising autoencoder trained on a finite number $n$ of samples from the target distribution" becomes "当 Parametrized by a shallow denoising autoencoder 在一定数量 $n$ of samples from the target distribution 上训练"* "Building on this analysis, we provide a sharp description of the corresponding generative flow" becomes "Building on this analysis, we provide a sharp description of the corresponding generative flow"* "which pushes the base Gaussian density forward to an approximation of the target density" becomes "which pushes the base Gaussian density forward to an approximation of the target density"* "Specifically, we provide closed-form formulae for the distance between the mean of the generated mixture and the mean of the target mixture" becomes " Specifically, we provide closed-form formulae for the distance between the mean of the generated mixture and the mean of the target mixture"* "which we show decays as $\Theta_n(\frac{1}{n})$" becomes "which we show decays as $\Theta_n(\frac{1}{n})$"* "Finally, this rate is shown to be in fact Bayes-optimal" becomes "Finally, this rate is shown to be in fact Bayes-optimal"
</details></li>
</ul>
<hr>
<h2 id="Residual-Multi-Fidelity-Neural-Network-Computing"><a href="#Residual-Multi-Fidelity-Neural-Network-Computing" class="headerlink" title="Residual Multi-Fidelity Neural Network Computing"></a>Residual Multi-Fidelity Neural Network Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03572">http://arxiv.org/abs/2310.03572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen Davis, Mohammad Motamed, Raul Tempone</li>
<li>for:  constructed a neural network surrogate model using multi-fidelity information</li>
<li>methods:  residual multi-fidelity computational framework, two neural networks working in concert</li>
<li>results:  dramatic savings in computational cost, accurate predictions within small tolerances<details>
<summary>Abstract</summary>
In this work, we consider the general problem of constructing a neural network surrogate model using multi-fidelity information. Given an inexpensive low-fidelity and an expensive high-fidelity computational model, we present a residual multi-fidelity computational framework that formulates the correlation between models as a residual function, a possibly non-linear mapping between 1) the shared input space of the models together with the low-fidelity model output and 2) the discrepancy between the two model outputs. To accomplish this, we train two neural networks to work in concert. The first network learns the residual function on a small set of high-fidelity and low-fidelity data. Once trained, this network is used to generate additional synthetic high-fidelity data, which is used in the training of a second network. This second network, once trained, acts as our surrogate for the high-fidelity quantity of interest. We present three numerical examples to demonstrate the power of the proposed framework. In particular, we show that dramatic savings in computational cost may be achieved when the output predictions are desired to be accurate within small tolerances.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们考虑了一个总体的神经网络模拟器使用多种精度信息的问题。给定一个便宜的低精度计算模型和一个昂贵的高精度计算模型，我们提出了一个多质量计算框架，它将计算模型之间的相关性表示为一个差分函数，这可能是一个非线性映射，它将1）输入空间中共享的模型输出和低精度模型输出相关联，2）两个模型输出之间的差异。为了实现这一点，我们训练了两个神经网络。第一个网络学习了差分函数，它在一小段高精度和低精度数据上进行了训练。一旦训练完成，这个网络就可以生成更多的 sintetic高精度数据，这些数据被用在第二个网络的训练中。第二个网络，一旦训练完成，就成为我们的神经网络模拟器，用于预测高精度量度。我们给出了三个数学例子，以示本提案的能力。特别是，我们发现，当输出预测需要在小误差内时，可以获得巨大的计算成本减少。
</details></li>
</ul>
<hr>
<h2 id="Stable-Training-of-Probabilistic-Models-Using-the-Leave-One-Out-Maximum-Log-Likelihood-Objective"><a href="#Stable-Training-of-Probabilistic-Models-Using-the-Leave-One-Out-Maximum-Log-Likelihood-Objective" class="headerlink" title="Stable Training of Probabilistic Models Using the Leave-One-Out Maximum Log-Likelihood Objective"></a>Stable Training of Probabilistic Models Using the Leave-One-Out Maximum Log-Likelihood Objective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03556">http://arxiv.org/abs/2310.03556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kutay Bölat, Simon H. Tindemans, Peter Palensky</li>
<li>for: 这篇论文的目的是提出一种适应不同密度区域的逻辑密度函数模型，以优化数据驱动方法的能力。</li>
<li>methods: 该模型使用了自适应kernel density estimation（KDE）方法，并采用了留一个出去最大logs likelihood（LOO-MLL） criterion来避免缺失singularity问题。</li>
<li>results: 该模型在两个不同的电力系统数据集上进行了测试，并与 Gaussian mixture models进行了比较。结果表明，提出的模型具有扎实的性能，同时具有防止缺失singularity的保证。<details>
<summary>Abstract</summary>
Probabilistic modelling of power systems operation and planning processes depends on data-driven methods, which require sufficiently large datasets. When historical data lacks this, it is desired to model the underlying data generation mechanism as a probability distribution to assess the data quality and generate more data, if needed. Kernel density estimation (KDE) based models are popular choices for this task, but they fail to adapt to data regions with varying densities. In this paper, an adaptive KDE model is employed to circumvent this, where each kernel in the model has an individual bandwidth. The leave-one-out maximum log-likelihood (LOO-MLL) criterion is proposed to prevent the singular solutions that the regular MLL criterion gives rise to, and it is proven that LOO-MLL prevents these. Relying on this guaranteed robustness, the model is extended by assigning learnable weights to the kernels. In addition, a modified expectation-maximization algorithm is employed to accelerate the optimization speed reliably. The performance of the proposed method and models are exhibited on two power systems datasets using different statistical tests and by comparison with Gaussian mixture models. Results show that the proposed models have promising performance, in addition to their singularity prevention guarantees.
</details>
<details>
<summary>摘要</summary>
probabilistic 模型 power 系统操作和规划过程取决于数据驱动方法，需要具有足够的数据量。当历史数据缺乏这些数据时，可以模型下面的数据生成机制为概率分布，以评估数据质量并生成更多数据，如果需要。基于 kernel density estimation（KDE）的模型是非常流行的选择，但它们无法适应数据区域中的不同浓度。在这篇论文中，一种适应型KDE模型被employs，其中每个kernel具有自己的宽度。通过 leave-one-out maximum log-likelihood（LOO-MLL） criterion来避免普通的最大 log-likelihood（MLL） criterion所引起的孤立解，并且证明了LOO-MLL的可靠性。此外，在这个可靠性保证下，模型被扩展了，并将学习权重分配给kernel。此外，一种修改后的 expectation-maximization 算法被使用，以加速优化速度可靠地。提出的方法和模型在两个不同的电力系统数据集上进行了不同的统计测试和对比 Gaussian mixture models，结果表明，提出的模型具有良好的表现，同时具有可靠性保证。
</details></li>
</ul>
<hr>
<h2 id="Plug-and-Play-Posterior-Sampling-under-Mismatched-Measurement-and-Prior-Models"><a href="#Plug-and-Play-Posterior-Sampling-under-Mismatched-Measurement-and-Prior-Models" class="headerlink" title="Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models"></a>Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03546">http://arxiv.org/abs/2310.03546</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marien-renaud/pnp_ula_posterior_law_sensivity">https://github.com/marien-renaud/pnp_ula_posterior_law_sensivity</a></li>
<li>paper_authors: Marien Renaud, Jiaming Liu, Valentin de Bortoli, Andrés Almansa, Ulugbek S. Kamilov</li>
<li>for: 本研究探讨了 posterior sampling 在媒体 inverse problems 中的应用，以及 recent plug-and-play unadjusted Langevin algorithm (PnP-ULA) 的应用在 Monte Carlo 样本和最小平均方差 (MMSE) 估计中。</li>
<li>methods: 本研究使用了 posterior-L2 pseudometric，以量化 PnP-ULA 下的样本分布匹配度和数据准确性。</li>
<li>results: 数值 validate 结果表明，PnP-ULA 的样本分布对于不符合的测量模型和滤波器有明确的敏感性。<details>
<summary>Abstract</summary>
Posterior sampling has been shown to be a powerful Bayesian approach for solving imaging inverse problems. The recent plug-and-play unadjusted Langevin algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling and minimum mean squared error (MMSE) estimation by combining physical measurement models with deep-learning priors specified using image denoisers. However, the intricate relationship between the sampling distribution of PnP-ULA and the mismatched data-fidelity and denoiser has not been theoretically analyzed. We address this gap by proposing a posterior-L2 pseudometric and using it to quantify an explicit error bound for PnP-ULA under mismatched posterior distribution. We numerically validate our theory on several inverse problems such as sampling from Gaussian mixture models and image deblurring. Our results suggest that the sensitivity of the sampling distribution of PnP-ULA to a mismatch in the measurement model and the denoiser can be precisely characterized.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> posterior sampling 已经被证明是一种有力的 bayesian 方法，用于解决图像反问题。最近的插入式不变Langevin算法（PnP-ULA）已经被认为是一种有前途的方法，通过将物理测量模型与深度学习假设相结合，使用图像去噪器来 specify deep-learning priors。然而， posterior sampling 分布中 PnP-ULA 与数据准确性和去噪器之间的复杂关系尚未被理论上分析。我们强调这一点，并提出了 posterior-L2  pseudometric，用于量化 PnP-ULA 下的明确误差 bound。我们在几个反问题上进行数值验证，结果表明，PnP-ULA 的 sampling 分布对于测量模型和去噪器的不一致具有高度的敏感性。
</details></li>
</ul>
<hr>
<h2 id="Distribution-free-risk-assessment-of-regression-based-machine-learning-algorithms"><a href="#Distribution-free-risk-assessment-of-regression-based-machine-learning-algorithms" class="headerlink" title="Distribution-free risk assessment of regression-based machine learning algorithms"></a>Distribution-free risk assessment of regression-based machine learning algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03545">http://arxiv.org/abs/2310.03545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sukrita Singh, Neeraj Sarna, Yuanyuan Li, Yang Li, Agni Orfanoudaki, Michael Berger</li>
<li>for: 该论文旨在解决机器学习模型在实际应用中的风险评估问题，特别是在医学和工程等高风险应用中。</li>
<li>methods: 该论文使用了 конформаль预测方法来解决风险评估问题，提供了一种保证报告的预测范围内包含真实标签的预测方法。</li>
<li>results: 该论文通过对具有不同模型化情况、数据集大小和 конформаль预测方法的实验，证明了其方法的准确性和可靠性。<details>
<summary>Abstract</summary>
Machine learning algorithms have grown in sophistication over the years and are increasingly deployed for real-life applications. However, when using machine learning techniques in practical settings, particularly in high-risk applications such as medicine and engineering, obtaining the failure probability of the predictive model is critical. We refer to this problem as the risk-assessment task. We focus on regression algorithms and the risk-assessment task of computing the probability of the true label lying inside an interval defined around the model's prediction. We solve the risk-assessment problem using the conformal prediction approach, which provides prediction intervals that are guaranteed to contain the true label with a given probability. Using this coverage property, we prove that our approximated failure probability is conservative in the sense that it is not lower than the true failure probability of the ML algorithm. We conduct extensive experiments to empirically study the accuracy of the proposed method for problems with and without covariate shift. Our analysis focuses on different modeling regimes, dataset sizes, and conformal prediction methodologies.
</details>
<details>
<summary>摘要</summary>
In this paper, we focus on regression algorithms and the risk-assessment task of calculating the probability of the true label falling within a range around the model's prediction. We solve this problem using the conformal prediction approach, which provides prediction intervals that are guaranteed to contain the true label with a certain probability. By using this coverage property, we prove that our approximated failure probability is conservative, meaning it is not lower than the true failure probability of the machine learning algorithm.We conduct extensive experiments to empirically study the accuracy of our proposed method for problems with and without covariate shift. Our analysis covers different modeling regimes, dataset sizes, and conformal prediction methodologies.
</details></li>
</ul>
<hr>
<h2 id="Joint-Group-Invariant-Functions-on-Data-Parameter-Domain-Induce-Universal-Neural-Networks"><a href="#Joint-Group-Invariant-Functions-on-Data-Parameter-Domain-Induce-Universal-Neural-Networks" class="headerlink" title="Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks"></a>Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03530">http://arxiv.org/abs/2310.03530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sho Sonoda, Hideyuki Ishi, Isao Ishikawa, Masahiro Ikeda</li>
<li>for: 研究内存网络中数据对称和几何的编码方式</li>
<li>methods: 使用 JOINT GROUP INVARIANT FUNCTION 系统地找到数据域中的 dual group action</li>
<li>results: 提出了一种新的 GROUP THEORETIC PROOF 用于证明普遍性定理，连接了几何深度学习与抽象的幂分析Note: “ JOINT GROUP INVARIANT FUNCTION” and “GROUP THEORETIC PROOF” are in English, as there is no direct Simplified Chinese translation for these terms.<details>
<summary>Abstract</summary>
The symmetry and geometry of input data are considered to be encoded in the internal data representation inside the neural network, but the specific encoding rule has been less investigated. By focusing on a joint group invariant function on the data-parameter domain, we present a systematic rule to find a dual group action on the parameter domain from a group action on the data domain. Further, we introduce generalized neural networks induced from the joint invariant functions, and present a new group theoretic proof of their universality theorems by using Schur's lemma. Since traditional universality theorems were demonstrated based on functional analytical methods, this study sheds light on the group theoretic aspect of the approximation theory, connecting geometric deep learning to abstract harmonic analysis.
</details>
<details>
<summary>摘要</summary>
“对于输入数据的对称和几何都是嵌入到神经网络内部的内部数据表示中的一部分，但具体的编码规则尚未得到充分研究。我们将注意力集中在质共变函数的质共变函数域上，从数据领域上的群动作中找到另一个群动作在参数领域上。此外，我们引入了通过质共变函数所导引的扩展神经网络，并提出了一个新的群论证明其 universality 定理，使用 Schul's lemma。传统的 universality 定理都是基于函数分析方法，这项研究将几何深度学习与抽象数学分析之间的连结推广到群论方面。”Note: Simplified Chinese is used in this translation, which is a more casual and widely used version of Chinese. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Deep-Ridgelet-Transform-Voice-with-Koopman-Operator-Proves-Universality-of-Formal-Deep-Networks"><a href="#Deep-Ridgelet-Transform-Voice-with-Koopman-Operator-Proves-Universality-of-Formal-Deep-Networks" class="headerlink" title="Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality of Formal Deep Networks"></a>Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality of Formal Deep Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03529">http://arxiv.org/abs/2310.03529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda</li>
<li>for: 这篇论文是研究深度神经网络（DNN）中隐藏层的发现和分析的。</li>
<li>methods: 作者使用了群作用理论来表述DNN的结构，并通过使用Schur的 lemma进行了一种简单的证明，证明了DNN的 universality。</li>
<li>results: 研究表明，DNN可以被视为一种 dual voice transform，与 Koopman 运算器相关的 linear 表示。这种表示可以捕捉到DNN中隐藏的层结构，并且可以用于分析DNN的行为。<details>
<summary>Abstract</summary>
We identify hidden layers inside a DNN with group actions on the data space, and formulate the DNN as a dual voice transform with respect to Koopman operator, a linear representation of the group action. Based on the group theoretic arguments, particularly by using Schur's lemma, we show a simple proof of the universality of those DNNs.
</details>
<details>
<summary>摘要</summary>
我们在深度神经网络（DNN）中寻找隐藏层，使用群作用在数据空间进行表示，并将DNN转换为koopman运算的双声变换。基于群理论的论据，特别是使用舒尔的 lemma，我们提供了DNN的 universality 的简单证明。Here's a breakdown of the translation:* "We identify hidden layers inside a DNN" becomes "我们在深度神经网络中寻找隐藏层"* "with group actions on the data space" becomes "使用群作用在数据空间进行表示"* "and formulate the DNN as a dual voice transform with respect to Koopman operator" becomes "并将DNN转换为koopman运算的双声变换"* "Based on the group theoretic arguments" becomes "基于群理论的论据"* "particularly by using Schur's lemma" becomes "特别是使用舒尔的 lemma"* "we show a simple proof of the universality of those DNNs" becomes "我们提供了DNN的 universality 的简单证明"Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="High-dimensional-Bayesian-Optimization-with-Group-Testing"><a href="#High-dimensional-Bayesian-Optimization-with-Group-Testing" class="headerlink" title="High-dimensional Bayesian Optimization with Group Testing"></a>High-dimensional Bayesian Optimization with Group Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03515">http://arxiv.org/abs/2310.03515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gtboauthors/gtbo">https://github.com/gtboauthors/gtbo</a></li>
<li>paper_authors: Erik Orm Hellsten, Carl Hvarfner, Leonard Papenmeier, Luigi Nardi</li>
<li>for: 优化高维黑盒函数，尤其是在高维问题中，因为目标函数模型受到维度的咒语，准确模型具有困难。</li>
<li>methods: 我们提出了一种组测方法，即组测bayesian优化（GTBO），通过系统地选择和测试变量的组合来帮助高维优化。我们对函数范围的扩展进行了group testing理论，以便更好地地模型目标函数。</li>
<li>results: GTBO在一些synthetic和实际高维优化任务上与现有方法竞争，并且可以帮助实际Operator发现活跃参数，从而提高问题理解。<details>
<summary>Abstract</summary>
Bayesian optimization is an effective method for optimizing expensive-to-evaluate black-box functions. High-dimensional problems are particularly challenging as the surrogate model of the objective suffers from the curse of dimensionality, which makes accurate modeling difficult. We propose a group testing approach to identify active variables to facilitate efficient optimization in these domains. The proposed algorithm, Group Testing Bayesian Optimization (GTBO), first runs a testing phase where groups of variables are systematically selected and tested on whether they influence the objective. To that end, we extend the well-established theory of group testing to functions of continuous ranges. In the second phase, GTBO guides optimization by placing more importance on the active dimensions. By exploiting the axis-aligned subspace assumption, GTBO is competitive against state-of-the-art methods on several synthetic and real-world high-dimensional optimization tasks. Furthermore, GTBO aids in the discovery of active parameters in applications, thereby enhancing practitioners' understanding of the problem at hand.
</details>
<details>
<summary>摘要</summary>
bayesian 优化是一种有效的优化昂贵黑盒函数的方法。高维问题特别困难，因为目标函数的模型受到维度之咒的影响，准确模型困难。我们提出了一种组测试方法，以便在这些领域中高效地优化。我们称之为组测试 bayesian 优化（GTBO）。在第一个测试阶段，GTBO首先运行一系列的组测试，选择并测试变量的组合，以确定影响目标函数的变量。然后，在第二个优化阶段，GTBO通过强调活跃维度来导航优化。通过利用轴对齐的子空间假设，GTBO与当前状态艺术方法竞争。此外，GTBO可以帮助发现应用中活跃参数，从而增强实践者对问题的理解。
</details></li>
</ul>
<hr>
<h2 id="Otago-Exercises-Monitoring-for-Older-Adults-by-a-Single-IMU-and-Hierarchical-Machine-Learning-Models"><a href="#Otago-Exercises-Monitoring-for-Older-Adults-by-a-Single-IMU-and-Hierarchical-Machine-Learning-Models" class="headerlink" title="Otago Exercises Monitoring for Older Adults by a Single IMU and Hierarchical Machine Learning Models"></a>Otago Exercises Monitoring for Older Adults by a Single IMU and Hierarchical Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03512">http://arxiv.org/abs/2310.03512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Shang, Lenore Dedeyne, Jolan Dupont, Laura Vercauteren, Nadjia Amini, Laurence Lapauw, Evelien Gielen, Sabine Verschueren, Carolina Varon, Walter De Raedt, Bart Vanrumste</li>
<li>for: 这个研究的目的是建立一个不侵入的和准确的系统，用于监测老年人参与奥塔哥体能计划 (OEP)。</li>
<li>methods: 这个研究使用了一个单个腰部安装的加速度计 (IMU) 收集数据，并使用了深度学习模型来识别老年人是否在进行 OEP 或者日常生活活动 (ADLs)。</li>
<li>results: 研究发现，使用 10 分钟滑动窗口可以在室内和家庭场景下分别达到 window-wise f1-scores 高于 0.95 和 Intersection-over-Union (IoU) f1-scores 高于 0.85。另外，使用 6 秒滑动窗口可以在家庭场景下识别四种 OEP  subclass。结果表明，使用单个 IMU 可以准确地监测老年人参与 OEP 的情况，并且可以进行进一步的分析。<details>
<summary>Abstract</summary>
Otago Exercise Program (OEP) is a rehabilitation program for older adults to improve frailty, sarcopenia, and balance. Accurate monitoring of patient involvement in OEP is challenging, as self-reports (diaries) are often unreliable. With the development of wearable sensors, Human Activity Recognition (HAR) systems using wearable sensors have revolutionized healthcare. However, their usage for OEP still shows limited performance. The objective of this study is to build an unobtrusive and accurate system to monitor OEP for older adults. Data was collected from older adults wearing a single waist-mounted Inertial Measurement Unit (IMU). Two datasets were collected, one in a laboratory setting, and one at the homes of the patients. A hierarchical system is proposed with two stages: 1) using a deep learning model to recognize whether the patients are performing OEP or activities of daily life (ADLs) using a 10-minute sliding window; 2) based on stage 1, using a 6-second sliding window to recognize the OEP sub-classes performed. The results showed that in stage 1, OEP could be recognized with window-wise f1-scores over 0.95 and Intersection-over-Union (IoU) f1-scores over 0.85 for both datasets. In stage 2, for the home scenario, four activities could be recognized with f1-scores over 0.8: ankle plantarflexors, abdominal muscles, knee bends, and sit-to-stand. The results showed the potential of monitoring the compliance of OEP using a single IMU in daily life. Also, some OEP sub-classes are possible to be recognized for further analysis.
</details>
<details>
<summary>摘要</summary>
奥塔哥运动项目（OEP）是一项为老年人提供的康复计划，以提高衰退、肌肉萎缩和平衡。但监测older adults的参与度很具挑战性，因为自我报告（日记）通常不可靠。随着便携式传感器的发展，基于便携式传感器的人体活动识别（HAR）技术在医疗领域得到了广泛应用。然而，它们在OEP中的使用还具有有限的表现。本研究的目标是建立一个不侵入式和准确的OEP监测系统。数据来自于老年人穿着一个背部固定的�ер�orio measure（IMU）。研究采集了两组数据，一个在实验室中，另一个在老年人家中。提出了一种层次结构，包括两个阶段：1）使用深度学习模型来确定older adults是否在进行OEP或日常活动（ADLs），使用10分钟滑动窗口；2）基于第一阶段，使用6秒钟滑动窗口来识别OEP下的亚类。结果表明，在第一阶段，OEP可以在窗口级别上获得window-wise f1分数超过0.95，并且在IoU分数上超过0.85。在第二阶段，在家庭场景下，可以识别四种活动，其中f1分数超过0.8：脚踝肌肉、腹部肌肉、膝盖弯曲和坐姿转起。结果表明，使用单个IMU可以在日常生活中监测OEP的合作性。此外，一些OEP下的亚类也可以被识别，以供进一步分析。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Models-of-Music-Expectation"><a href="#Deep-Generative-Models-of-Music-Expectation" class="headerlink" title="Deep Generative Models of Music Expectation"></a>Deep Generative Models of Music Expectation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03500">http://arxiv.org/abs/2310.03500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ninon Lizé Masclef, T. Anderson Keller</li>
<li>for: 这个研究的目的是开发现代深度生成模型来计算音乐的惊喜度和预期。</li>
<li>methods: 这个研究使用了扩散模型，它是一种基于深度神经网络的概率生成模型，可以直接从训练集中学习复杂的非线性特征。</li>
<li>results: 研究发现，使用 pré-trained扩散模型可以计算出高度准确的音乐惊喜度值，并且这些值与人类 слуша者的喜欢度评分 exhibit 一种负QUadratic关系。<details>
<summary>Abstract</summary>
A prominent theory of affective response to music revolves around the concepts of surprisal and expectation. In prior work, this idea has been operationalized in the form of probabilistic models of music which allow for precise computation of song (or note-by-note) probabilities, conditioned on a 'training set' of prior musical or cultural experiences. To date, however, these models have been limited to compute exact probabilities through hand-crafted features or restricted to linear models which are likely not sufficient to represent the complex conditional distributions present in music. In this work, we propose to use modern deep probabilistic generative models in the form of a Diffusion Model to compute an approximate likelihood of a musical input sequence. Unlike prior work, such a generative model parameterized by deep neural networks is able to learn complex non-linear features directly from a training set itself. In doing so, we expect to find that such models are able to more accurately represent the 'surprisal' of music for human listeners. From the literature, it is known that there is an inverted U-shaped relationship between surprisal and the amount human subjects 'like' a given song. In this work we show that pre-trained diffusion models indeed yield musical surprisal values which exhibit a negative quadratic relationship with measured subject 'liking' ratings, and that the quality of this relationship is competitive with state of the art methods such as IDyOM. We therefore present this model a preliminary step in developing modern deep generative models of music expectation and subjective likability.
</details>
<details>
<summary>摘要</summary>
一种流行的音乐情感响应理论中心于意外性和期望。在先前的工作中，这个想法被实现为probabilistic模型，允许精确计算歌曲（或每个音符）的概率，基于一个'训练集'的前期音乐或文化经验。然而，这些模型只能通过手动设计特征或restricted linear模型来计算恰当的概率，这些模型可能不能表达音乐中的复杂 conditional distribution。在这项工作中，我们提议使用现代深度概率生成模型，即Diffusion Model，计算音乐输入序列的近似概率。与先前工作不同，这种生成模型由深度神经网络参数化，可以直接从训练集中学习复杂非线性特征。我们预计，这种模型将更准确地表达人类听众对音乐的意外性。从文献来看，知道存在一个人际U型关系 между意外性和人类听众对一首歌曲的评分。在这项工作中，我们证明了预训练的Diffusion Models实际上对音乐的意外性值具有负二次关系，与测量的听众评分相关度呈现负相关性。因此，我们提出了这种模型作为现代深度生成模型的音乐期望和主观喜欢度的开发前期步骤。
</details></li>
</ul>
<hr>
<h2 id="TPDR-A-Novel-Two-Step-Transformer-based-Product-and-Class-Description-Match-and-Retrieval-Method"><a href="#TPDR-A-Novel-Two-Step-Transformer-based-Product-and-Class-Description-Match-and-Retrieval-Method" class="headerlink" title="TPDR: A Novel Two-Step Transformer-based Product and Class Description Match and Retrieval Method"></a>TPDR: A Novel Two-Step Transformer-based Product and Class Description Match and Retrieval Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03491">http://arxiv.org/abs/2310.03491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Washington Cunha, Celso França, Leonardo Rocha, Marcos André Gonçalves</li>
<li>for: 该论文是为了解决企业间产品描述标准化问题，即将客户提供的产品描述与产品目录中的描述匹配。</li>
<li>methods: 该论文提出了一种基于Transformer的两步产品和类别描述检索方法（TPDR），利用注意力机制和对比学习来探索 semantic correspondence between IS和SD。</li>
<li>results: 该论文在11个真实公司的应用上实现了71%的正确检索和80%的正确分类，并且与纯粹的语法或semantic基线比较而言，效果提高达3.7倍。<details>
<summary>Abstract</summary>
There is a niche of companies responsible for intermediating the purchase of large batches of varied products for other companies, for which the main challenge is to perform product description standardization, i.e., matching an item described by a client with a product described in a catalog. The problem is complex since the client's product description may be: (1) potentially noisy; (2) short and uninformative (e.g., missing information about model and size); and (3) cross-language. In this paper, we formalize this problem as a ranking task: given an initial client product specification (query), return the most appropriate standardized descriptions (response). In this paper, we propose TPDR, a two-step Transformer-based Product and Class Description Retrieval method that is able to explore the semantic correspondence between IS and SD, by exploiting attention mechanisms and contrastive learning. First, TPDR employs the transformers as two encoders sharing the embedding vector space: one for encoding the IS and another for the SD, in which corresponding pairs (IS, SD) must be close in the vector space. Closeness is further enforced by a contrastive learning mechanism leveraging a specialized loss function. TPDR also exploits a (second) re-ranking step based on syntactic features that are very important for the exact matching (model, dimension) of certain products that may have been neglected by the transformers. To evaluate our proposal, we consider 11 datasets from a real company, covering different application contexts. Our solution was able to retrieve the correct standardized product before the 5th ranking position in 71% of the cases and its correct category in the first position in 80% of the situations. Moreover, the effectiveness gains over purely syntactic or semantic baselines reach up to 3.7 times, solving cases that none of the approaches in isolation can do by themselves.
</details>
<details>
<summary>摘要</summary>
有一些公司专门为其他公司批购大量多种产品，主要挑战是标准化产品描述，即将客户提供的产品描述与公司 catalo 中的产品描述匹配。这是一个复杂的问题，因为客户的产品描述可能具有以下特点：（1）可能含有噪音（即不必要的信息）；（2）短板和不够详细（例如缺少型号和大小信息）；（3）跨语言。在这篇论文中，我们将这个问题正式化为排名任务：给定客户的初始产品规范（查询），返回最相应的标准化产品描述（响应）。我们提议使用 transformers 来解决这个问题，具体来说，我们使用 transformers 作为两个编码器，一个用于编码 IS（Initial Specification），另一个用于编码 SD（Standard Description），两者之间需要在 embedding 空间中相似。我们还使用对应对（IS、SD）的匹配性进行加重，使用特殊的损失函数进行强制匹配。此外，我们还使用一个（第二）重新排名步骤，基于语法特征，以便更好地匹配某些产品，这些产品可能由 transformers 被忽略。为评估我们的提议，我们使用了 11 个真实公司的数据集，覆盖不同的应用场景。我们的解决方案能够在前5名的排名中 retrieved 正确的标准化产品，并在 71% 的情况下在第一名中 retrieved 正确的类别。此外，与纯语法或 semantics 基eline 相比，我们的解决方案的效果提高了多达 3.7 倍，解决了 none 的方法无法办的案例。
</details></li>
</ul>
<hr>
<h2 id="The-Geometric-Structure-of-Fully-Connected-ReLU-Layers"><a href="#The-Geometric-Structure-of-Fully-Connected-ReLU-Layers" class="headerlink" title="The Geometric Structure of Fully-Connected ReLU-Layers"></a>The Geometric Structure of Fully-Connected ReLU-Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03482">http://arxiv.org/abs/2310.03482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonatan Vallin, Karl Larsson, Mats G. Larson</li>
<li>for: 这篇论文主要针对$d$-维充满ReLU层在神经网络中的几何结构进行了正式化和解释。</li>
<li>methods: 这篇论文使用了ReLU层的参数来自然地将输入空间分成多个部分，并在每个部分中简化ReLU层，从而导致了ReLU层的几何解释。</li>
<li>results: 这篇论文提出了一种简化表达折射面和折射平面的方法，并证明了这种结构可以在分类设置下描述决策边界。此外，论文还研究了一个具有一个隐藏层的普通feedforward网络的决策边界的几何复杂性，以及证明了这种网络只能生成$d$个不同的决策边界。最后，论文还讨论了增加更多层的影响。<details>
<summary>Abstract</summary>
We formalize and interpret the geometric structure of $d$-dimensional fully connected ReLU-layers in neural networks. The parameters of a ReLU-layer induce a natural partition of the input domain, such that in each sector of the partition, the ReLU-layer can be greatly simplified. This leads to a geometric interpretation of a ReLU-layer as a projection onto a polyhedral cone followed by an affine transformation, in line with the description in [doi:10.48550/arXiv.1905.08922] for convolutional networks with ReLU activations. Further, this structure facilitates simplified expressions for preimages of the intersection between partition sectors and hyperplanes, which is useful when describing decision boundaries in a classification setting. We investigate this in detail for a feed-forward network with one hidden ReLU-layer, where we provide results on the geometric complexity of the decision boundary generated by such networks, as well as proving that modulo an affine transformation, such a network can only generate $d$ different decision boundaries. Finally, the effect of adding more layers to the network is discussed.
</details>
<details>
<summary>摘要</summary>
我们正式化和解释了深度学习网络中的$d$-维完全相连ReLU层的几何结构。层的参数导致了输入空间的自然分割，在每个分割部分中，ReLU层可以大大简化。这导致了ReLU层的几何解释为一个投影到多边形锥后的拓扑变换，与[doi:10.48550/arXiv.1905.08922]中的对于具有ReLU启动函数的卷积网络的描述相符。此外，这结构使得partition部分与条件面的顶点的前像简化了，这有用于描述分类设定中的决策界面。我们在详细调查了一个具有一个隐藏层的对应网络，并提供了决策界面的几何复杂性的结果，以及证明这种网络只能生成$d$个不同的决策界面。最后，我们讨论了将更多层添加到网络中的效果。
</details></li>
</ul>
<hr>
<h2 id="The-Cadenza-ICASSP-2024-Grand-Challenge"><a href="#The-Cadenza-ICASSP-2024-Grand-Challenge" class="headerlink" title="The Cadenza ICASSP 2024 Grand Challenge"></a>The Cadenza ICASSP 2024 Grand Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03480">http://arxiv.org/abs/2310.03480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerardo Roa Dabike, Michael A. Akeroyd, Scott Bannister, Jon Barker, Trevor J. Cox, Bruno Fazenda, Jennifer Firth, Simone Graetzer, Alinka Greasley, Rebecca Vos, William Whitmer</li>
<li>for: 该论文旨在推动听力障碍人群音频质量提高，通过音乐分解和个性化重新混音来提高听力障碍人群对音乐的听众体验。</li>
<li>methods: 该论文提出了一种基于ICASSP SP Cadenza Challenge的音乐分解和重新混音方法，通过分解音乐为 vocals、bass、drums 等组成部分，并通过个性化的混音方法来提高音频质量。</li>
<li>results: 该论文通过使用HAAQI指标进行评估，发现该方法可以提高听力障碍人群对音乐的听众体验。<details>
<summary>Abstract</summary>
The Cadenza project aims to enhance the audio quality of music for individuals with hearing loss. As part of this, the project is organizing the ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids. The challenge can be tackled by decomposing the music at the hearing aid microphones into vocals, bass, drums, and other components. These can then be intelligently remixed in a personalized manner to improve audio quality. Alternatively, an end-to-end approach could be used. Processes need to consider the music itself, the gain applied to each component, and the listener's hearing loss. The submitted entries will be evaluated using the intrusive objective metric, the Hearing Aid Audio Quality Index (HAAQI). This paper outlines the challenge.
</details>
<details>
<summary>摘要</summary>
文本：The Cadenza project aims to enhance the audio quality of music for individuals with hearing loss. As part of this, the project is organizing the ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids. The challenge can be tackled by decomposing the music at the hearing aid microphones into vocals, bass, drums, and other components. These can then be intelligently remixed in a personalized manner to improve audio quality. Alternatively, an end-to-end approach could be used. Processes need to consider the music itself, the gain applied to each component, and the listener's hearing loss. The submitted entries will be evaluated using the intrusive objective metric, the Hearing Aid Audio Quality Index (HAAQI). This paper outlines the challenge.翻译： cadence 项目目标是提高音乐听众听力损伤人群的音频质量。为此，项目在ICASSP SP Cadenza Challenge：音乐分解/重新混音 для听众器中进行组织。挑战可以通过在听众器麦克风中分解音乐来实现，将 vocals、bass、鼓等组分分别处理，然后通过个性化方式进行重新混音，以提高音频质量。或者可以使用综合方法。处理需要考虑音乐本身，每个组分的增益应用，以及听众的听力损伤。提交的作品将根据潜入式对metric，听音器音频质量指数（HAAQI）进行评估。本文介绍了这个挑战。
</details></li>
</ul>
<hr>
<h2 id="The-Blame-Problem-in-Evaluating-Local-Explanations-and-How-to-Tackle-it"><a href="#The-Blame-Problem-in-Evaluating-Local-Explanations-and-How-to-Tackle-it" class="headerlink" title="The Blame Problem in Evaluating Local Explanations, and How to Tackle it"></a>The Blame Problem in Evaluating Local Explanations, and How to Tackle it</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03466">http://arxiv.org/abs/2310.03466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Hossein Akhavan Rahnama</li>
<li>for: 本研究提出了一个新的本地解释评价分类，以解决当前本地解释技术的评价问题。</li>
<li>methods: 本研究使用了多种评价方法，包括Robustness、基于真实数据 Synthetic dataset的评价、模型随机化评价和人类基于评价。</li>
<li>results: 研究发现，除了基于可读性模型的真实数据评价外，其他评价方法均受到一种问题称为“责任问题”的影响。此外，即使使用这种评价方法，本地解释评价仍然存在问题。<details>
<summary>Abstract</summary>
The number of local model-agnostic explanation techniques proposed has grown rapidly recently. One main reason is that the bar for developing new explainability techniques is low due to the lack of optimal evaluation measures. Without rigorous measures, it is hard to have concrete evidence of whether the new explanation techniques can significantly outperform their predecessors. Our study proposes a new taxonomy for evaluating local explanations: robustness, evaluation using ground truth from synthetic datasets and interpretable models, model randomization, and human-grounded evaluation. Using this proposed taxonomy, we highlight that all categories of evaluation methods, except those based on the ground truth from interpretable models, suffer from a problem we call the "blame problem." In our study, we argue that this category of evaluation measure is a more reasonable method for evaluating local model-agnostic explanations. However, we show that even this category of evaluation measures has further limitations. The evaluation of local explanations remains an open research problem.
</details>
<details>
<summary>摘要</summary>
“当前本地模型无关解释技术的数量呈急剧增长趋势。主要原因是开发新解释技术的门槛很低，由于缺乏优化的评价指标，没有充分的证据表明新解释技术是否可以显著超越前一代。我们的研究提出了一个新的本地解释评价分类法：可靠性、使用synthetic数据生成的真实数据和可解 modelo randomization、以及人类权威评价。使用这个分类法，我们发现所有类型的评价方法，除了基于可解 modelo的真实数据，都受到一种问题，我们称之为“责任问题”。在我们的研究中，我们认为这一类评价方法是评价本地模型无关解释的更合理的方法，但我们还发现这些评价方法具有进一步的局限性。本地解释评价仍然是一个开放的研究问题。”
</details></li>
</ul>
<hr>
<h2 id="Which-mode-is-better-for-federated-learning-Centralized-or-Decentralized"><a href="#Which-mode-is-better-for-federated-learning-Centralized-or-Decentralized" class="headerlink" title="Which mode is better for federated learning? Centralized or Decentralized"></a>Which mode is better for federated learning? Centralized or Decentralized</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03461">http://arxiv.org/abs/2310.03461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Sun, Li Shen, Dacheng Tao</li>
<li>for: 这个论文主要研究了 federated learning（FL）中中心化和分布式方法的比较，以及它们在不同场景下的表现。</li>
<li>methods: 这个论文使用了 optimization 和 generalization 两个方面进行了 joint 分析，并提出了一些新的结论和建议。</li>
<li>results: 研究发现，在平滑非对称目标函数上，中心化 federated learning（CFL）总是比分布式 federated learning（DFL）更好地泛化；在 CFL 中，采用部分参与比全参与更好；而 DFL 中，需要特定的topology来避免性能崩溃。<details>
<summary>Abstract</summary>
Both centralized and decentralized approaches have shown excellent performance and great application value in federated learning (FL). However, current studies do not provide sufficient evidence to show which one performs better. Although from the optimization perspective, decentralized methods can approach the comparable convergence of centralized methods with less communication, its test performance has always been inefficient in empirical studies. To comprehensively explore their behaviors in FL, we study their excess risks, including the joint analysis of both optimization and generalization. We prove that on smooth non-convex objectives, 1) centralized FL (CFL) always generalizes better than decentralized FL (DFL); 2) from perspectives of the excess risk and test error in CFL, adopting partial participation is superior to full participation; and, 3) there is a necessary requirement for the topology in DFL to avoid performance collapse as the training scale increases. Based on some simple hardware metrics, we could evaluate which framework is better in practice. Extensive experiments are conducted on common setups in FL to validate that our theoretical analysis is contextually valid in practical scenarios.
</details>
<details>
<summary>摘要</summary>
中央化和分布式方法都在联合学习（FL）中表现出色，但现有研究没有提供足够的证据来评估哪一种表现更好。虽然从优化角度来看，分布式方法可以在通信量更少的情况下达到相当于中央化方法的相同收敛性，但在实际研究中，它的测试性能总是较差。为全面探讨它们在FL中的行为，我们研究了它们的过剩风险，包括优化和泛化的共同分析。我们证明了以下结论：1）中央化联合学习（CFL）总是在不光滑非对称目标函数上的泛化性比分布式联合学习（DFL）更好; 2）在CFL中，采用偏函数参与部分比总参与更有优势; 3）在DFL中，避免训练规模增加时性能崩溃的必要条件是 topology。基于一些简单的硬件指标，我们可以评估哪种框架在实践中更好。我们在常见的FL设置下进行了广泛的实验，以验证我们的理论分析在实际场景中是Contextually valid。
</details></li>
</ul>
<hr>
<h2 id="FLAIM-AIM-based-Synthetic-Data-Generation-in-the-Federated-Setting"><a href="#FLAIM-AIM-based-Synthetic-Data-Generation-in-the-Federated-Setting" class="headerlink" title="FLAIM: AIM-based Synthetic Data Generation in the Federated Setting"></a>FLAIM: AIM-based Synthetic Data Generation in the Federated Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03447">http://arxiv.org/abs/2310.03447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Samuel-Maddock/flaim">https://github.com/Samuel-Maddock/flaim</a></li>
<li>paper_authors: Samuel Maddock, Graham Cormode, Carsten Maple</li>
<li>for: 防止个人隐私泄露，启用合作数据分享是组织所需。</li>
<li>methods: 使用 Synthetic Data Generation 技术生成 искусствен数据，保持了私人数据的统计性质。</li>
<li>results: 提出了一种基于 differential privacy 的 federated Synthetic Tabular Data Generation 方法 DistAIM 和 FLAIM，可以减少了 overhead 并在不同程度的 hetrogeniety 下提高了实用性。<details>
<summary>Abstract</summary>
Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We show it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that maintains a private proxy of heterogeneity. We simulate our methods across a range of benchmark datasets under different degrees of heterogeneity and show this can improve utility while reducing overhead.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Variational-Inference-for-GARCH-family-Models"><a href="#Variational-Inference-for-GARCH-family-Models" class="headerlink" title="Variational Inference for GARCH-family Models"></a>Variational Inference for GARCH-family Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03435">http://arxiv.org/abs/2310.03435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Magris, Alexandros Iosifidis</li>
<li>for: 这篇论文是为了检验Variational Inference是否能够成为GARCH-like模型的 bayesian估计的可靠和可行的替代方法。</li>
<li>methods: 这篇论文使用了多种Variational Inference优化器、多种波动模型和一个案例研究来证明Variational Inference是一种可靠、非常准确和竞争力强的bayesian学习方法。</li>
<li>results: 经过大规模的实验，这篇论文显示了Variational Inference在S&amp;P 500指数的组成部分上的性能非常出色，并且与蒙特卡洛样本的 bayesian估计相比，Variational Inference具有更高的准确性和更好的可靠性。<details>
<summary>Abstract</summary>
The Bayesian estimation of GARCH-family models has been typically addressed through Monte Carlo sampling. Variational Inference is gaining popularity and attention as a robust approach for Bayesian inference in complex machine learning models; however, its adoption in econometrics and finance is limited. This paper discusses the extent to which Variational Inference constitutes a reliable and feasible alternative to Monte Carlo sampling for Bayesian inference in GARCH-like models. Through a large-scale experiment involving the constituents of the S&P 500 index, several Variational Inference optimizers, a variety of volatility models, and a case study, we show that Variational Inference is an attractive, remarkably well-calibrated, and competitive method for Bayesian learning.
</details>
<details>
<summary>摘要</summary>
“bayesian预测garch家族模型通常通过蒙地卡罗 sampling来实现，但是variational inference在复杂机器学习模型中的采用尚未得到广泛的应用。这篇文章探讨garch家族模型中variational inference是一种可靠和可行的bayesian预测方法的可行性。通过对S&P 500指数成分进行大规模实验，以及使用多种方差模型和一个案例研究，我们示出了variational inference是一种吸引人、很善地调整和竞争力强的bayesian学习方法。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Over-the-Air-Federated-Learning-with-Compressed-Sensing-Is-Sparsification-Necessary"><a href="#Over-the-Air-Federated-Learning-with-Compressed-Sensing-Is-Sparsification-Necessary" class="headerlink" title="Over-the-Air Federated Learning with Compressed Sensing: Is Sparsification Necessary?"></a>Over-the-Air Federated Learning with Compressed Sensing: Is Sparsification Necessary?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03410">http://arxiv.org/abs/2310.03410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Edin, Zheng Chen</li>
<li>for: 这个论文主要研究了随机上下文下的联合学习（Federated Learning，FL）系统，其中多个代理机制使用随机上下文中的计算进行模型更新的传输到公共边缘服务器。</li>
<li>methods: 作者使用了线性处理和信号水平混合，以减少通道上传输的数据样本。他们还使用了压缩感知（Compressed Sensing，CS）方法来减少数据量。</li>
<li>results: 研究发现，不需要将原始模型更新向量先将其简化，而直接将非零元素发送即可以达到更好的性能，即使在同样的总功率限制下。此外，作者还发现，在某些情况下，不使用线性压缩并直接发送简化后的模型更新也可以达到更好的性能。<details>
<summary>Abstract</summary>
Over-the-Air (OtA) Federated Learning (FL) refers to an FL system where multiple agents apply OtA computation for transmitting model updates to a common edge server. Two important features of OtA computation, namely linear processing and signal-level superposition, motivate the use of linear compression with compressed sensing (CS) methods to reduce the number of data samples transmitted over the channel. The previous works on applying CS methods in OtA FL have primarily assumed that the original model update vectors are sparse, or they have been sparsified before compression. However, it is unclear whether linear compression with CS-based reconstruction is more effective than directly sending the non-zero elements in the sparsified update vectors, under the same total power constraint. In this study, we examine and compare several communication designs with or without sparsification. Our findings demonstrate that sparsification before compression is not necessary. Alternatively, sparsification without linear compression can also achieve better performance than the commonly considered setup that combines both.
</details>
<details>
<summary>摘要</summary>
“气候飞行（OtA）联合学习（FL）”指的是一种FL系统，其中多个代理应用OtA计算来传输模型更新给共同的边缘服务器。两种重要的OtA计算特点，即线性处理和信号水平积加，驱动了使用线性压缩与压缩感知（CS）方法来减少通道上传输的数据样本数。先前的CS方法在OtA FL中的应用主要假设了原始模型更新向量是稀疏的，或者它们已经被稀疏化了 перед压缩。然而，是否Linear压缩与CS基于重建是更有效的，以及是否可以不进行稀疏化，这些问题尚未得到解答。在这项研究中，我们考虑了多种通信设计，其中一些包括稀疏化，而另一些则不包括稀疏化。我们的发现表明，稀疏化 перед压缩并不是必要的。相反，稀疏化而不进行Linear压缩可以实现更好的性能，与传统的设置相比。
</details></li>
</ul>
<hr>
<h2 id="RUSOpt-Robotic-UltraSound-Probe-Normalization-with-Bayesian-Optimization-for-In-plane-and-Out-plane-Scanning"><a href="#RUSOpt-Robotic-UltraSound-Probe-Normalization-with-Bayesian-Optimization-for-In-plane-and-Out-plane-Scanning" class="headerlink" title="RUSOpt: Robotic UltraSound Probe Normalization with Bayesian Optimization for In-plane and Out-plane Scanning"></a>RUSOpt: Robotic UltraSound Probe Normalization with Bayesian Optimization for In-plane and Out-plane Scanning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03406">http://arxiv.org/abs/2310.03406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepak Raina, Abhishek Mathur, Richard M. Voyles, Juan Wachs, SH Chandrashekhara, Subir Kumar Saha</li>
<li>for: 提高自主无人式超声系统中图像质量的问题</li>
<li>methods: 使用 Bayesian 优化法在扫描面上进行劳动力平衡，以实现融合探针的自动调整</li>
<li>results: 实验结果表明，提出的方法可以在不同的患者中获得高质量的超声图像，其中平均（±SD）的绝对角误差为2.4±0.7度和2.1±1.3度，分别在膜质量和3D人体模型中进行了验证。<details>
<summary>Abstract</summary>
The one of the significant challenges faced by autonomous robotic ultrasound systems is acquiring high-quality images across different patients. The proper orientation of the robotized probe plays a crucial role in governing the quality of ultrasound images. To address this challenge, we propose a sample-efficient method to automatically adjust the orientation of the ultrasound probe normal to the point of contact on the scanning surface, thereby improving the acoustic coupling of the probe and resulting image quality. Our method utilizes Bayesian Optimization (BO) based search on the scanning surface to efficiently search for the normalized probe orientation. We formulate a novel objective function for BO that leverages the contact force measurements and underlying mechanics to identify the normal. We further incorporate a regularization scheme in BO to handle the noisy objective function. The performance of the proposed strategy has been assessed through experiments on urinary bladder phantoms. These phantoms included planar, tilted, and rough surfaces, and were examined using both linear and convex probes with varying search space limits. Further, simulation-based studies have been carried out using 3D human mesh models. The results demonstrate that the mean ($\pm$SD) absolute angular error averaged over all phantoms and 3D models is $\boldsymbol{2.4\pm0.7^\circ}$ and $\boldsymbol{2.1\pm1.3^\circ}$, respectively.
</details>
<details>
<summary>摘要</summary>
一个重要挑战 faced by autonomous robotic ultrasound systems 是获得高质量图像 across different patients. 正确的探针旋转角度对于控制ultrasound图像质量具有关键作用。为了解决这个挑战，我们提议一种样本效率的方法，自动调整探针旋转角度，使其与接触面的法向成直角。我们利用 Bayesian Optimization (BO) 基于扫描面上的搜索来快速找到正常化探针旋转角度。我们定义了一个新的目标函数，用于BO搜索，该函数利用探针与接触面之间的Contact force measurement和下面的机械学来确定正常。我们进一步添加了一个补做项来处理目标函数中的噪声。实验结果表明，我们提议的策略可以在 urinary bladder phantoms 和 3D human mesh models 上具有高效性。具体来说，在所有 phantoms 和 3D models 上的 Mean（±SD）绝对角度误差为 $\boldsymbol{2.4\pm0.7^\circ}$ 和 $\boldsymbol{2.1\pm1.3^\circ}$, 分别。
</details></li>
</ul>
<hr>
<h2 id="EAG-RS-A-Novel-Explainability-guided-ROI-Selection-Framework-for-ASD-Diagnosis-via-Inter-regional-Relation-Learning"><a href="#EAG-RS-A-Novel-Explainability-guided-ROI-Selection-Framework-for-ASD-Diagnosis-via-Inter-regional-Relation-Learning" class="headerlink" title="EAG-RS: A Novel Explainability-guided ROI-Selection Framework for ASD Diagnosis via Inter-regional Relation Learning"></a>EAG-RS: A Novel Explainability-guided ROI-Selection Framework for ASD Diagnosis via Inter-regional Relation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03404">http://arxiv.org/abs/2310.03404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ku-milab/eag-rs">https://github.com/ku-milab/eag-rs</a></li>
<li>paper_authors: Wonsik Jung, Eunjin Jeon, Eunsong Kang, Heung-Il Suk<br>for:The paper aims to develop a novel explainability-guided region of interest (ROI) selection framework for brain disease identification using resting-state functional magnetic resonance imaging (rs-fMRI).methods:The proposed framework includes three steps: inter-regional relation learning, explainable connection-wise relevance score estimation, and non-linear high-order FC-based diagnosis-informative ROI selection and classifier learning. The framework leverages an explainable artificial intelligence technique to identify non-linear high-order functional associations among brain regions and select class-discriminative regions for brain disease identification.results:The proposed method outperforms other comparative methods in terms of various evaluation metrics, and qualitative analysis of the selected ROIs identifies ASD subtypes linked to previous neuroscientific studies.<details>
<summary>Abstract</summary>
Deep learning models based on resting-state functional magnetic resonance imaging (rs-fMRI) have been widely used to diagnose brain diseases, particularly autism spectrum disorder (ASD). Existing studies have leveraged the functional connectivity (FC) of rs-fMRI, achieving notable classification performance. However, they have significant limitations, including the lack of adequate information while using linear low-order FC as inputs to the model, not considering individual characteristics (i.e., different symptoms or varying stages of severity) among patients with ASD, and the non-explainability of the decision process. To cover these limitations, we propose a novel explainability-guided region of interest (ROI) selection (EAG-RS) framework that identifies non-linear high-order functional associations among brain regions by leveraging an explainable artificial intelligence technique and selects class-discriminative regions for brain disease identification. The proposed framework includes three steps: (i) inter-regional relation learning to estimate non-linear relations through random seed-based network masking, (ii) explainable connection-wise relevance score estimation to explore high-order relations between functional connections, and (iii) non-linear high-order FC-based diagnosis-informative ROI selection and classifier learning to identify ASD. We validated the effectiveness of our proposed method by conducting experiments using the Autism Brain Imaging Database Exchange (ABIDE) dataset, demonstrating that the proposed method outperforms other comparative methods in terms of various evaluation metrics. Furthermore, we qualitatively analyzed the selected ROIs and identified ASD subtypes linked to previous neuroscientific studies.
</details>
<details>
<summary>摘要</summary>
深度学习模型基于休息态功能磁共振成像（rs-fMRI）已广泛应用于诊断脑病，特别是自闭症 спектル异常（ASD）。现有研究通过使用 rs-fMRI 的功能相关性（FC）来实现可读性的分类性能。然而，这些研究存在一些局限性，包括FC的缺乏充分信息，不考虑患者之间的个体特征（如不同的症状或不同的病程度），以及分类过程的不可追溯性。为了缓解这些限制，我们提出了一种可追溯性导向的区域选择（EAG-RS）框架。该框架包括以下三步：1. 通过随机种子网络屏蔽来估计非线性关系，以便在不同的脑区之间建立非线性关系；2. 使用可追溯的人工智能技术来计算连接点级别的相关性分数，以探索高级别的函数连接关系；3. 基于高级别功能连接的非线性FC来选择诊断有用的区域，并使用这些区域来学习分类器，以便诊断ASD。我们在使用ABIDE数据集进行实验，并证明了我们的方法在不同的评价指标上表现出色。此外，我们还进行了质量分析选定的ROI，并发现了与先前的神经科学研究相关的ASDSubtypes。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Large-Language-Models-for-Content-Moderation-Pitfalls-in-Data-Engineering-and-Supervised-Fine-tuning"><a href="#Adapting-Large-Language-Models-for-Content-Moderation-Pitfalls-in-Data-Engineering-and-Supervised-Fine-tuning" class="headerlink" title="Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning"></a>Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03400">http://arxiv.org/abs/2310.03400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Ma, Changqing Zhang, Huazhu Fu, Peilin Zhao, Bingzhe Wu</li>
<li>for: 本研究的目的是提供私有部署的语言模型 fine-tuning 的实现细节，以便在各个领域进行域specific研究。</li>
<li>methods: 本研究使用 Large Language Models (LLMs) 进行语言模型 fine-tuning，并 explore 不同的处理方法以便在私有部署中使用更强大的 LLMs 生成的理由。</li>
<li>results: 本研究发现，在私有部署中使用更强大的 LLMs 生成的理由可以提高语言模型的性能，但是需要根据不同的处理方法进行调整。<details>
<summary>Abstract</summary>
Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. However, in the field of content moderation, there is still a lack of detailed work that systematically introduces implementation details. In this paper, we introduce how to fine-tune an LLM model that can be privately deployed for content moderation. Specifically, we discuss whether incorporating reasons during the fine-tuning process would be better or if it should be treated as a classification task directly. We also explore the benefits of utilizing reasons generated by more powerful LLMs for fine-tuning privately deployed models and the impact of different processing approaches when the answers generated by the more powerful LLMs are incorrect. We report the entire research process and the key findings in this paper, hoping to provide valuable experience for researchers who are fine-tuning privately deployed models in their domain-specific research.
</details>
<details>
<summary>摘要</summary>
现在，每天有数十亿人在互联网上进行交流和表达自己的意见。然而，不幸的是，不所有的表达都是友好或合法的，因此内容审核成为不可或缺的任务。随着大语言模型（LLM）的成功发展，LLM基本方法在各个领域中成为了可能的解决方案。然而，在内容审核领域，还缺乏详细的实施细节。在这篇论文中，我们介绍了如何私有部署LLM模型进行内容审核。specifically，我们讨论了在细化过程中是否应该包含理由，或者直接将其视为分类任务。我们还探讨了使用更强大LLM生成的理由来细化私有部署模型的效果，以及不同处理方法的影响，当更强大LLM生成的答案错误时。我们报告了整个研究过程和关键发现，希望为审核私有部署模型的研究人员提供有价值的经验。
</details></li>
</ul>
<hr>
<h2 id="Interpolating-between-Clustering-and-Dimensionality-Reduction-with-Gromov-Wasserstein"><a href="#Interpolating-between-Clustering-and-Dimensionality-Reduction-with-Gromov-Wasserstein" class="headerlink" title="Interpolating between Clustering and Dimensionality Reduction with Gromov-Wasserstein"></a>Interpolating between Clustering and Dimensionality Reduction with Gromov-Wasserstein</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03398">http://arxiv.org/abs/2310.03398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugues Van Assel, Cédric Vincent-Cuaz, Titouan Vayer, Rémi Flamary, Nicolas Courty</li>
<li>for:  simultaneous reduction of both sample and feature sizes</li>
<li>methods:  semi-relaxed Gromov-Wasserstein optimal transport (OT) problem</li>
<li>results:  competitive hard clustering and summarization of real dataHere’s the full sentence in Simplified Chinese:</li>
<li>for: 这种方法用于同时减少样本和特征数量</li>
<li>methods: 使用半松散格罗莫-瓦asserstein优质运输问题</li>
<li>results: 竞争性强的硬聚类和实际数据的概要I hope this helps!<details>
<summary>Abstract</summary>
We present a versatile adaptation of existing dimensionality reduction (DR) objectives, enabling the simultaneous reduction of both sample and feature sizes. Correspondances between input and embedding samples are computed through a semi-relaxed Gromov-Wasserstein optimal transport (OT) problem. When the embedding sample size matches that of the input, our model recovers classical popular DR models. When the embedding's dimensionality is unconstrained, we show that the OT plan delivers a competitive hard clustering. We emphasize the importance of intermediate stages that blend DR and clustering for summarizing real data and apply our method to visualize datasets of images.
</details>
<details>
<summary>摘要</summary>
我们提出了一种多样化的维度减少（DR）目标的变体，允许同时减少样本和特征的大小。通过一种半relaxed的格罗莫-瓦asserstein优化运输（OT）问题计算输入和嵌入样本之间的对应关系。当嵌入样本大小与输入一样时，我们的模型可以重新获得经典的受欢迎DR模型。当嵌入维度是自由的时，我们显示出OT计划可以提供竞争性的硬团 clustering。我们强调在实际数据概括时间中的中间阶段，这些阶段将DR和团 clustering融合起来，并应用我们的方法图像 dataset。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-quantification-for-deep-learning-based-schemes-for-solving-high-dimensional-backward-stochastic-differential-equations"><a href="#Uncertainty-quantification-for-deep-learning-based-schemes-for-solving-high-dimensional-backward-stochastic-differential-equations" class="headerlink" title="Uncertainty quantification for deep learning-based schemes for solving high-dimensional backward stochastic differential equations"></a>Uncertainty quantification for deep learning-based schemes for solving high-dimensional backward stochastic differential equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03393">http://arxiv.org/abs/2310.03393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenc Kapllani, Long Teng, Matthias Rottmann</li>
<li>for: This paper aims to study uncertainty quantification (UQ) for deep learning-based numerical schemes used to solve high-dimensional backward stochastic differential equations (BSDEs).</li>
<li>methods: The paper uses a UQ model that efficiently estimates the standard deviation (STD) of the approximate solution using only a single run of the algorithm, as well as estimates the mean of the approximate solution.</li>
<li>results: The numerical experiments show that the UQ model produces reliable estimates of the mean and STD of the approximate solution for the considered class of deep learning-based BSDE schemes, and can identify hyperparameter values for which the scheme achieves good approximations. Additionally, the model illustrates the improved performance when comparing different schemes based on the estimated STD values.<details>
<summary>Abstract</summary>
Deep learning-based numerical schemes for solving high-dimensional backward stochastic differential equations (BSDEs) have recently raised plenty of scientific interest. While they enable numerical methods to approximate very high-dimensional BSDEs, their reliability has not been studied and is thus not understood. In this work, we study uncertainty quantification (UQ) for a class of deep learning-based BSDE schemes. More precisely, we review the sources of uncertainty involved in the schemes and numerically study the impact of different sources. Usually, the standard deviation (STD) of the approximate solutions obtained from multiple runs of the algorithm with different datasets is calculated to address the uncertainty. This approach is computationally quite expensive, especially for high-dimensional problems. Hence, we develop a UQ model that efficiently estimates the STD of the approximate solution using only a single run of the algorithm. The model also estimates the mean of the approximate solution, which can be leveraged to initialize the algorithm and improve the optimization process. Our numerical experiments show that the UQ model produces reliable estimates of the mean and STD of the approximate solution for the considered class of deep learning-based BSDE schemes. The estimated STD captures multiple sources of uncertainty, demonstrating its effectiveness in quantifying the uncertainty. Additionally, the model illustrates the improved performance when comparing different schemes based on the estimated STD values. Furthermore, it can identify hyperparameter values for which the scheme achieves good approximations.
</details>
<details>
<summary>摘要</summary>
高维度后逆随机分子方程（BSDE）的数学方法Recently, deep learning-based numerical schemes have attracted a lot of scientific attention. These schemes can approximate very high-dimensional BSDEs, but their reliability has not been well studied and is not well understood. In this work, we study the uncertainty quantification (UQ) of a class of deep learning-based BSDE schemes. Specifically, we identify the sources of uncertainty in these schemes and numerically study the impact of different sources. Typically, the standard deviation (STD) of the approximate solutions obtained from multiple runs of the algorithm with different datasets is calculated to address the uncertainty. However, this approach is computationally expensive, especially for high-dimensional problems. Therefore, we develop a UQ model that efficiently estimates the STD of the approximate solution using only a single run of the algorithm. The model also estimates the mean of the approximate solution, which can be used to initialize the algorithm and improve the optimization process. Our numerical experiments show that the UQ model provides reliable estimates of the mean and STD of the approximate solution for the considered class of deep learning-based BSDE schemes. The estimated STD captures multiple sources of uncertainty, demonstrating its effectiveness in quantifying the uncertainty. Moreover, the model illustrates the improved performance when comparing different schemes based on the estimated STD values. Furthermore, it can identify hyperparameter values for which the scheme achieves good approximations.
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-the-interaction-network-in-coupled-dynamical-systems"><a href="#Machine-learning-the-interaction-network-in-coupled-dynamical-systems" class="headerlink" title="Machine learning the interaction network in coupled dynamical systems"></a>Machine learning the interaction network in coupled dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03378">http://arxiv.org/abs/2310.03378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pawan R. Bhure, M. S. Santhanam</li>
<li>for: 这个研究旨在掌握互动动力系统中的相互作用网络信息，以便更好地理解它们之间的相互作用。</li>
<li>methods: 这种自动学习神经网络模型可以从观察到的轨迹数据中恢复交互网络和预测个体代理的动态。</li>
<li>results: 这种模型在两个动力系统中进行了应用，包括彼此受托的粒子 mediated by Hooke’s law交互和相互频率振荡器。<details>
<summary>Abstract</summary>
The study of interacting dynamical systems continues to attract research interest in various fields of science and engineering. In a collection of interacting particles, the interaction network contains information about how various components interact with one another. Inferring the information about the interaction network from the dynamics of agents is a problem of long-standing interest. In this work, we employ a self-supervised neural network model to achieve two outcomes: to recover the interaction network and to predict the dynamics of individual agents. Both these information are inferred solely from the observed trajectory data. This work presents an application of the Neural Relational Inference model to two dynamical systems: coupled particles mediated by Hooke's law interaction and coupled phase (Kuramoto) oscillators.
</details>
<details>
<summary>摘要</summary>
研究互动动力系统仍然在不同科学和工程领域吸引研究者的兴趣。在一个集合中的互动网络包含对各个组件之间的交互信息。从动态代理的观察数据中推断交互网络的信息是长期关注的问题。在这个工作中，我们采用了一种无监督神经网络模型，以实现两个目的：恢复交互网络和预测个体代理的动态。这两个信息都是从观察轨迹数据中推断出来的。这篇文章描述了使用神经关系推断模型在两种动力系统中进行应用：彼此受欧姆法则互动的集合体和彼此受库拉摩托oscillators互动。
</details></li>
</ul>
<hr>
<h2 id="An-Integrated-Algorithm-for-Robust-and-Imperceptible-Audio-Adversarial-Examples"><a href="#An-Integrated-Algorithm-for-Robust-and-Imperceptible-Audio-Adversarial-Examples" class="headerlink" title="An Integrated Algorithm for Robust and Imperceptible Audio Adversarial Examples"></a>An Integrated Algorithm for Robust and Imperceptible Audio Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03349">http://arxiv.org/abs/2310.03349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armin Ettenhofer, Jan-Philipp Schulze, Karla Pizzi</li>
<li>for: 这个论文的目的是提出一种基于心理听觉模型和房间冲击响应（RIR）的敏感语音攻击示例生成算法，以提高语音识别系统的抗击性和人类听众的鲁棒性。</li>
<li>methods: 这个论文使用了心理听觉模型和RIR在生成步骤中，通过动态生成房间冲击响应来模拟物理环境，以强化示例的抗击性。</li>
<li>results: 这个论文的实验结果表明，包含心理听觉因素和抗击性的算法在语音识别系统的robustness和人类听众的perceptibility两个方面均显示出改善，但是在word error rate（WER）方面受到了一定的影响。<details>
<summary>Abstract</summary>
Audio adversarial examples are audio files that have been manipulated to fool an automatic speech recognition (ASR) system, while still sounding benign to a human listener. Most methods to generate such samples are based on a two-step algorithm: first, a viable adversarial audio file is produced, then, this is fine-tuned with respect to perceptibility and robustness. In this work, we present an integrated algorithm that uses psychoacoustic models and room impulse responses (RIR) in the generation step. The RIRs are dynamically created by a neural network during the generation process to simulate a physical environment to harden our examples against transformations experienced in over-the-air attacks. We compare the different approaches in three experiments: in a simulated environment and in a realistic over-the-air scenario to evaluate the robustness, and in a human study to evaluate the perceptibility. Our algorithms considering psychoacoustics only or in addition to the robustness show an improvement in the signal-to-noise ratio (SNR) as well as in the human perception study, at the cost of an increased word error rate (WER).
</details>
<details>
<summary>摘要</summary>
audio adversarial examples 是指一种受到自动语音识别（ASR）系统 manipulate 而仍然听起来如人类听起来的音频文件。大多数生成这些样本的方法都是基于两步算法：首先生成可靠的反对例音频文件，然后对其进行辐射和Robustness 的微调。在这项工作中，我们提出了一种集成的算法，使用心理听觉模型和房间冲击响（RIR）在生成过程中。RIR 在生成过程中由神经网络动态创建，以模拟物理环境，以防止在无线攻击中的变换。我们在三个实验中比较了不同的方法：在模拟环境中和真实的无线攻击enario中评估了 robustness，以及在人类研究中评估了人类听起来。我们的算法只考虑心理听觉或者在 robustness 上加以考虑，都显示了 SNR 的提高，以及人类听起来的研究中的提高，但是在 WER 方面带来了一定的增加。
</details></li>
</ul>
<hr>
<h2 id="LESSON-Learning-to-Integrate-Exploration-Strategies-for-Reinforcement-Learning-via-an-Option-Framework"><a href="#LESSON-Learning-to-Integrate-Exploration-Strategies-for-Reinforcement-Learning-via-an-Option-Framework" class="headerlink" title="LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework"></a>LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03342">http://arxiv.org/abs/2310.03342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/beanie00/lesson">https://github.com/beanie00/lesson</a></li>
<li>paper_authors: Woojun Kim, Jeonghye Kim, Youngchul Sung</li>
<li>for: 提出了一个统一的探索框架 для reinforcement learning（RL），基于选择批评模型。</li>
<li>methods: 提出了一种能够集成多种多样化的探索策略，使RL代理人可以适应不同任务的适当探索-利用平衡。</li>
<li>results: 通过在MiniGrid和Atari环境中的多种实验，证明了提案的探索框架的效果。<details>
<summary>Abstract</summary>
In this paper, a unified framework for exploration in reinforcement learning (RL) is proposed based on an option-critic model. The proposed framework learns to integrate a set of diverse exploration strategies so that the agent can adaptively select the most effective exploration strategy over time to realize a relevant exploration-exploitation trade-off for each given task. The effectiveness of the proposed exploration framework is demonstrated by various experiments in the MiniGrid and Atari environments.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，一种基于选项-评估器模型的探索游戏RL框架被提出。该提议的框架可以学习集成一组多样化的探索策略，使代理人可以适应不同任务的不同探索策略，以实现每个任务的有效的探索-利用交互。实验表明，提议的探索框架在MiniGrid和Atari环境中得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Forecasting-of-Day-Ahead-Electricity-Prices-and-their-Volatility-with-LSTMs"><a href="#Probabilistic-Forecasting-of-Day-Ahead-Electricity-Prices-and-their-Volatility-with-LSTMs" class="headerlink" title="Probabilistic Forecasting of Day-Ahead Electricity Prices and their Volatility with LSTMs"></a>Probabilistic Forecasting of Day-Ahead Electricity Prices and their Volatility with LSTMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03339">http://arxiv.org/abs/2310.03339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julius Trebbien, Sebastian Pütz, Benjamin Schäfer, Heidi S. Nygård, Leonardo Rydin Gorjão, Dirk Witthaut</li>
<li>for: 预测电力价格的准确性是电力系统管理和智能应用的关键。欧洲电力价格增长很大，变得非常波动，挑战了已有的预测方法。</li>
<li>methods: 我们使用了Long Short-Term Memory（LSTM）模型来预测德国卢森堡日前电力价格，以适应这些挑战。LSTM模型的循环结构允许模型适应趋势，并jointly predicting both mean and standard deviation允许probabilistic prediction。</li>
<li>results: 使用物理启发的方法——超 statistics来解释价格的统计性，我们显示了LSTM模型能够准确地预测价格和其波动性。<details>
<summary>Abstract</summary>
Accurate forecasts of electricity prices are crucial for the management of electric power systems and the development of smart applications. European electricity prices have risen substantially and became highly volatile after the Russian invasion of Ukraine, challenging established forecasting methods. Here, we present a Long Short-Term Memory (LSTM) model for the German-Luxembourg day-ahead electricity prices addressing these challenges. The recurrent structure of the LSTM allows the model to adapt to trends, while the joint prediction of both mean and standard deviation enables a probabilistic prediction. Using a physics-inspired approach - superstatistics - to derive an explanation for the statistics of prices, we show that the LSTM model faithfully reproduces both prices and their volatility.
</details>
<details>
<summary>摘要</summary>
准确预测电力价格对电力系统管理和智能应用的发展非常重要。欧洲电力价格在俄罗斯入侵乌克兰后高涨并变得极为不稳，挑战了传统预测方法。我们在这篇文章中介绍了一个基于Long Short-Term Memory（LSTM）模型的德国卢森堡日前电力价格预测方法，以解决这些挑战。LSTM模型的循环结构使其能够适应趋势，而联合预测两者的平均值和标准差使得预测变得 probabilistic。通过基于物理学的方法——超统计学——获得价格统计的解释，我们表明LSTM模型能够准确地复制价格和其不稳。
</details></li>
</ul>
<hr>
<h2 id="Untargeted-White-box-Adversarial-Attack-with-Heuristic-Defence-Methods-in-Real-time-Deep-Learning-based-Network-Intrusion-Detection-System"><a href="#Untargeted-White-box-Adversarial-Attack-with-Heuristic-Defence-Methods-in-Real-time-Deep-Learning-based-Network-Intrusion-Detection-System" class="headerlink" title="Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time Deep Learning based Network Intrusion Detection System"></a>Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time Deep Learning based Network Intrusion Detection System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03334">http://arxiv.org/abs/2310.03334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khushnaseeb Roshan, Aasim Zafar, Sheikh Burhan Ul Haque</li>
<li>For: This research work aims to increase the robustness of Machine Learning (ML) and Deep Learning (DL) based Network Intrusion Detection Systems (NIDS) against adversarial attacks.* Methods: The research uses four powerful adversarial attack techniques (FGSM, JSMA, PGD, and C&amp;W) to evaluate the performance of NIDS under adversarial attack situations. It also employs three heuristics defense strategies (AT, GDA, and HC) to improve the NIDS robustness.* Results: The research demonstrates the complete workflow of the proposed approach in a real-time network with data packet flow and evaluates the performance of NIDS under adversarial attacks using various performance metrics.<details>
<summary>Abstract</summary>
Network Intrusion Detection System (NIDS) is a key component in securing the computer network from various cyber security threats and network attacks. However, consider an unfortunate situation where the NIDS is itself attacked and vulnerable more specifically, we can say, How to defend the defender?. In Adversarial Machine Learning (AML), the malicious actors aim to fool the Machine Learning (ML) and Deep Learning (DL) models to produce incorrect predictions with intentionally crafted adversarial examples. These adversarial perturbed examples have become the biggest vulnerability of ML and DL based systems and are major obstacles to their adoption in real-time and mission-critical applications such as NIDS. AML is an emerging research domain, and it has become a necessity for the in-depth study of adversarial attacks and their defence strategies to safeguard the computer network from various cyber security threads. In this research work, we aim to cover important aspects related to NIDS, adversarial attacks and its defence mechanism to increase the robustness of the ML and DL based NIDS. We implemented four powerful adversarial attack techniques, namely, Fast Gradient Sign Method (FGSM), Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and Carlini & Wagner (C&W) in NIDS. We analyzed its performance in terms of various performance metrics in detail. Furthermore, the three heuristics defence strategies, i.e., Adversarial Training (AT), Gaussian Data Augmentation (GDA) and High Confidence (HC), are implemented to improve the NIDS robustness under adversarial attack situations. The complete workflow is demonstrated in real-time network with data packet flow. This research work provides the overall background for the researchers interested in AML and its implementation from a computer network security point of view.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们将探讨 NIDS 相关的重要方面，包括反对攻击和防御策略，以提高 ML 和 DL 基础的 NIDS Robustness。我们在 NIDS 中实现了四种强大的反对攻击技术，分别是 Fast Gradient Sign Method (FGSM)、Jacobian Saliency Map Attack (JSMA)、Projected Gradient Descent (PGD) 和 Carlini & Wagner (C&W)。我们在详细的性能指标下进行了分析。此外，我们还实现了三种较为有效的防御策略，即 Adversarial Training (AT)、Gaussian Data Augmentation (GDA) 和 High Confidence (HC)。整个工作流程在实时网络中进行了示例。本研究提供了对 AML 的实现和应用在计算机网络安全方面的全面背景，为研究者提供了一个好的入门点。
</details></li>
</ul>
<hr>
<h2 id="Fine-tune-Language-Models-to-Approximate-Unbiased-In-context-Learning"><a href="#Fine-tune-Language-Models-to-Approximate-Unbiased-In-context-Learning" class="headerlink" title="Fine-tune Language Models to Approximate Unbiased In-context Learning"></a>Fine-tune Language Models to Approximate Unbiased In-context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03331">http://arxiv.org/abs/2310.03331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothy Chu, Zhao Song, Chiwun Yang</li>
<li>for: 提高大语言模型（LLM）的域内学习（ICL）性能，并且解决ICL中输入示例偏袋问题。</li>
<li>methods: 提出了一种重量算法called RICL（重量域内学习），通过使用无偏示例集来调整语言模型，以便更好地 aproximate域内学习。此外，我们还提出了一种低成本的线性优化重量算法called LARICL（线性优化重量域内学习），它具有较少的训练成本，同时可以提供有效的结果。</li>
<li>results: 通过实验 validate our algorithm的性能，发现与 benchmark 包括协助示例基于的域内学习和 класси型 fine-tuning 方法相比，our algorithm 具有显著提高的性能。<details>
<summary>Abstract</summary>
In-context learning (ICL) is an astonishing emergent ability of large language models (LLMs). By presenting a prompt that includes multiple input-output pairs as examples and introducing a new query input, models can generate the corresponding output. However, the performance of models heavily relies on the quality of the input prompt when implementing in-context learning. Biased or imbalanced input prompts can significantly degrade the performance of language models. To address this issue, we introduce a reweighted algorithm called RICL (Reweighted In-context Learning). This algorithm fine-tunes language models using an unbiased validation set to determine the optimal weight for each input-output example to approximate unbiased in-context learning. Furthermore, we also introduce a low-cost reweighted algorithm, a linear optimal weight approximation algorithm called LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm requires minimal training cost while providing effective results. We prove the convergence of our algorithm and validate its performance through experiments conducted on a numerical dataset. The experimental findings reveal a substantial improvement in comparison to benchmarks including the performance of casual prompt-based in-context learning and the performance of a classic fine-tuning method.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）的内容学习（ICL）是一种惊人的出现能力。通过提供包含多个输入输出对的示例，并在新的查询输入上引入一个新的查询，模型可以生成相应的输出。但是，模型的性能受到干扰输入提示的质量的影响。偏见或不均匀的干扰输入可能会严重降低语言模型的性能。为解决这个问题，我们提出了一个替Weighted algorithm called RICL（重新定量内容学习）。这个算法可以通过一个不偏见的验证集来调整语言模型，以 aproximate 不偏见的内容学习。此外，我们还引入了一个低成本的替Weighted algorithm，即Linear Approximation of Reweighted In-context Learning（线性 aproximation of Reweighted ICL）。这个算法仅需少量的训练成本，但可以提供有效的结果。我们证明了我们的算法的渐进性，并通过实验显示了与参考值相比，包括内容学习和 класи的 fine-tuning 方法的性能有所改善。
</details></li>
</ul>
<hr>
<h2 id="BioBridge-Bridging-Biomedical-Foundation-Models-via-Knowledge-Graphs"><a href="#BioBridge-Bridging-Biomedical-Foundation-Models-via-Knowledge-Graphs" class="headerlink" title="BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs"></a>BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03320">http://arxiv.org/abs/2310.03320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zifeng Wang, Zichen Wang, Balasubramaniam Srinivasan, Vassilis N. Ioannidis, Huzefa Rangwala, Rishita Anubhai</li>
<li>for: 本研究旨在超越生物医学领域Foundation models（FMs）的局限性，即独立地训练和使用不同类型数据进行任务。</li>
<li>methods: 本研究提出了一种新的参数效率学习框架——BioBridge，通过知识图（KG）学习对一种单Modal FM和另一种单Modal FM之间的转换，而不需要 Fine-tune任何下层单Modal FM。</li>
<li>results: 实验结果表明，BioBridge可以在多模态检索任务中超过基eline KG嵌入方法（在 average 约76.3%），并且 BioBridge 还示出了在不同模式或关系上的外部泛化能力。此外，BioBridge 还可以用于生物医学多模态问答以及引导生成新药的帮助。<details>
<summary>Abstract</summary>
Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relations. Additionally, we also show that BioBridge presents itself as a general purpose retriever that can aid biomedical multimodal question answering as well as enhance the guided generation of novel drugs.
</details>
<details>
<summary>摘要</summary>
基础模型（FM）可以利用大量未标注数据来实现广泛的任务表现优秀。然而，生物医学领域中的FM主要是单模态的，即独立地训练和使用对蛋白序列、小分子结构或医疗数据进行任务。为了超越生物医学领域中的FM限制，我们提出了 BioBridge，一种新的参数效率学习框架，用于将独立地训练的单模态FM连接起来，以实现多模态行为。BioBridge通过使用知识图（KG）来学习模态之间的变换，而无需修改任何基础模型。我们的实验结果表明，BioBridge可以在多模态检索任务中击败最佳基eline KG嵌入方法（平均提高约76.3%）。此外，我们也发现 BioBridge具有跨领域泛化能力，可以在未看到的模式或关系上进行推断。此外，我们还证明 BioBridge可以作为生物医学多模态问答系统中的通用检索器，以及生成新药的指导生成助手。
</details></li>
</ul>
<hr>
<h2 id="Certifiably-Robust-Graph-Contrastive-Learning"><a href="#Certifiably-Robust-Graph-Contrastive-Learning" class="headerlink" title="Certifiably Robust Graph Contrastive Learning"></a>Certifiably Robust Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03312">http://arxiv.org/abs/2310.03312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ventr1c/res-gcl">https://github.com/ventr1c/res-gcl</a></li>
<li>paper_authors: Minhua Lin, Teng Xiao, Enyan Dai, Xiang Zhang, Suhang Wang</li>
<li>for: 这篇论文主要targets Graph Contrastive Learning (GCL)，提出了一种可证明Robustness的方法来增强GCL的可靠性。</li>
<li>methods: 作者首先提出了一种综合的评估和证明GCL模型的可靠性的标准，然后提出了一种名为Randomized Edgedrop Smoothing（RES）的新技术，可以确保GCL模型的可证明Robustness。</li>
<li>results: 作者通过实验表明，RES可以有效地提高GCL模型的可靠性，并且可以在下游任务中保持证明Robustness。<details>
<summary>Abstract</summary>
Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph representation learning method. However, it has been shown that GCL is vulnerable to adversarial attacks on both the graph structure and node attributes. Although empirical approaches have been proposed to enhance the robustness of GCL, the certifiable robustness of GCL is still remain unexplored. In this paper, we develop the first certifiably robust framework in GCL. Specifically, we first propose a unified criteria to evaluate and certify the robustness of GCL. We then introduce a novel technique, RES (Randomized Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and this certified robustness can be provably preserved in downstream tasks. Furthermore, an effective training method is proposed for robust GCL. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method in providing effective certifiable robustness and enhancing the robustness of any GCL model. The source code of RES is available at https://github.com/ventr1c/RES-GCL.
</details>
<details>
<summary>摘要</summary>
GRAPH CONTRASTIVE LEARNING (GCL) 已经成为一种受欢迎的无监督 гра网表示学习方法。然而，它已经被证明是对 both  гра网结构和节点属性的攻击敏感。 although  empirical 方法已经被提出来增强 GCL 的韧性， GCL 的认证韧性仍然未知。 在这篇文章中，我们开发了 GCL 的第一个认证韧性框架。 specifically， we first propose  a unified criteria to evaluate and certify the robustness of GCL。 we then introduce a novel technique, RES (Randomized Edgedrop Smoothing), to ensure certifiable robustness for any GCL model， and this certified robustness can be provably preserved in downstream tasks。 Furthermore, an effective training method is proposed for robust GCL。 extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method in providing effective certifiable robustness and enhancing the robustness of any GCL model。 the source code of RES is available at https://github.com/ventr1c/RES-GCL。
</details></li>
</ul>
<hr>
<h2 id="Deep-Variational-Multivariate-Information-Bottleneck-–-A-Framework-for-Variational-Losses"><a href="#Deep-Variational-Multivariate-Information-Bottleneck-–-A-Framework-for-Variational-Losses" class="headerlink" title="Deep Variational Multivariate Information Bottleneck – A Framework for Variational Losses"></a>Deep Variational Multivariate Information Bottleneck – A Framework for Variational Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03311">http://arxiv.org/abs/2310.03311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eslam Abdelaleem, Ilya Nemenman, K. Michael Martini<br>for: 这种方法的目的是用信息理论来推导和总结现有的维度减少方法，并设计新的方法。methods: 这种方法基于一种多重信息瓶颈的解释，其中两个 bayesian 网络相互质量。具体来说，首先是一个编码图，它指定了压缩数据时需要保留的信息。其次是一个解码图，它指定了数据的生成模型。通过这种解释，我们可以重新计算现有的维度减少方法，包括深度维度减少信息瓶颈（DVIB）、β-VAE 和深度维度减少 canonical correlation analysis（DVCCA）。此外，我们还 derivated一种新的维度减少方法，深度维度减少Symmetric informational bottleneck（DVSIB），它同时压缩两个变量，以保留它们压缩表示中的信息。results: 我们实现了所有这些算法，并对一个修改后的噪音 MNIST 数据集进行评估。结果显示，better matched to the structure of the data 的算法（β-DVCCA 和 DVSIB）可以生成更好的低维度减少空间， measured by classification accuracy and the dimensionality of the latent variables。我们认为这种框架可以用来统一其他多视图表征学习算法，并提供一个直观的框架来 derive 问题特定的损失函数。<details>
<summary>Abstract</summary>
Variational dimensionality reduction methods are known for their high accuracy, generative abilities, and robustness. These methods have many theoretical justifications. Here we introduce a unifying principle rooted in information theory to rederive and generalize existing variational methods and design new ones. We base our framework on an interpretation of the multivariate information bottleneck, in which two Bayesian networks are traded off against one another. We interpret the first network as an encoder graph, which specifies what information to keep when compressing the data. We interpret the second network as a decoder graph, which specifies a generative model for the data. Using this framework, we rederive existing dimensionality reduction methods such as the deep variational information bottleneck (DVIB), beta variational auto-encoders (beta-VAE), and deep variational canonical correlation analysis (DVCCA). The framework naturally introduces a trade-off parameter between compression and reconstruction in the DVCCA family of algorithms, resulting in the new beta-DVCCA family. In addition, we derive a new variational dimensionality reduction method, deep variational symmetric informational bottleneck (DVSIB), which simultaneously compresses two variables to preserve information between their compressed representations. We implement all of these algorithms and evaluate their ability to produce shared low dimensional latent spaces on a modified noisy MNIST dataset. We show that algorithms that are better matched to the structure of the data (beta-DVCCA and DVSIB) produce better latent spaces as measured by classification accuracy and the dimensionality of the latent variables. We believe that this framework can be used to unify other multi-view representation learning algorithms. Additionally, it provides a straightforward framework for deriving problem-specific loss functions.
</details>
<details>
<summary>摘要</summary>
“维度减少方法已经知道其高精度、生成能力和稳定性。这些方法有许多理论基础。在这篇文章中，我们提出一个统一的原理，基于信息理论，将现有的维度减少方法推广和更新。我们基于多重信息瓶颈的解释，将第一个网络 interpret as an encoder graph，它决定了对数据压缩时保留的信息。第二个网络 interpret as a decoder graph，它决定了资料生成模型。使用这个框架，我们可以重新计算现有的维度减少方法，如深度维度减少信息瓶颈（DVIB）、β-VAE和深度维度减少 canonical correlation analysis（DVCCA）。这个框架还导出了一个内部对数据结构的调整参数，从而产生了β-DVCCA家族。此外，我们还 deriv了一个新的维度减少方法，深度维度减少 симметри情报瓶颈（DVSIB），它同时将两个变数压缩以保留它们压缩表示之间的信息。我们实现了所有这些算法，并评估它们在一个修改后的杂音MNIST dataset上的表现。我们发现，对于资料结构更加适合的算法（β-DVCCA和DVSIB）生成了更好的对数据空间， measured by classification accuracy and the dimensionality of the latent variables。我们认为这个框架可以用来统一其他多观点表示学习算法，并提供了一个直观的框架 для deriving问题特定的损失函数。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Energy-Decompositions-for-Partial-Inference-of-GFlowNets"><a href="#Learning-Energy-Decompositions-for-Partial-Inference-of-GFlowNets" class="headerlink" title="Learning Energy Decompositions for Partial Inference of GFlowNets"></a>Learning Energy Decompositions for Partial Inference of GFlowNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03301">http://arxiv.org/abs/2310.03301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyosoon Jang, Minsu Kim, Sungsoo Ahn</li>
<li>for: This paper aims to improve Generative Flow Networks (GFlowNets) for sampling objects from the Boltzmann energy distribution using partial inference.</li>
<li>methods: The paper proposes a novel approach called Learning Energy Decompositions for GFlowNets (LED-GFN), which decomposes the energy of an object into learnable potential functions defined on state transitions and reparameterizes the flow functions using these potential functions.</li>
<li>results: The proposed LED-GFN method is empirically verified to be superior to traditional GFlowNets in five problems, including the generation of unstructured and maximum independent sets, molecular graphs, and RNA sequences.<details>
<summary>Abstract</summary>
This paper studies generative flow networks (GFlowNets) to sample objects from the Boltzmann energy distribution via a sequence of actions. In particular, we focus on improving GFlowNet with partial inference: training flow functions with the evaluation of the intermediate states or transitions. To this end, the recently developed forward-looking GFlowNet reparameterizes the flow functions based on evaluating the energy of intermediate states. However, such an evaluation of intermediate energies may (i) be too expensive or impossible to evaluate and (ii) even provide misleading training signals under large energy fluctuations along the sequence of actions. To resolve this issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our main idea is to (i) decompose the energy of an object into learnable potential functions defined on state transitions and (ii) reparameterize the flow functions using the potential functions. In particular, to produce informative local credits, we propose to regularize the potential to change smoothly over the sequence of actions. It is also noteworthy that training GFlowNet with our learned potential can preserve the optimal policy. We empirically verify the superiority of LED-GFN in five problems including the generation of unstructured and maximum independent sets, molecular graphs, and RNA sequences.
</details>
<details>
<summary>摘要</summary>
The main idea of LED-GFN is to decompose the energy of an object into learnable potential functions defined on state transitions, and reparameterize the flow functions using these potential functions. To produce informative local credits, the authors regularize the potential to change smoothly over the sequence of actions. The authors also show that training GFlowNet with the learned potential can preserve the optimal policy.The paper is evaluated on five problems, including the generation of unstructured and maximum independent sets, molecular graphs, and RNA sequences, and the results demonstrate the superiority of LED-GFN over traditional GFlowNet.
</details></li>
</ul>
<hr>
<h2 id="A-Latent-Variable-Approach-for-Non-Hierarchical-Multi-Fidelity-Adaptive-Sampling"><a href="#A-Latent-Variable-Approach-for-Non-Hierarchical-Multi-Fidelity-Adaptive-Sampling" class="headerlink" title="A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling"></a>A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03298">http://arxiv.org/abs/2310.03298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi-Ping Chen, Liwei Wang, Yigitcan Comlek, Wei Chen</li>
<li>for: 提高伪降阶模型和设计优化的精度和效率，通过多模型适应性（Multi-fidelity）方法。</li>
<li>methods: 使用隐藏变量泛函过程（Latent Variable Gaussian Process） Map different fidelity models into an interpretable latent space to capture their correlations without assuming hierarchical fidelity levels，并在每个替入样本迭代中使用预后分析确定下一个样本的最佳选择。</li>
<li>results: 在测试问题中，提出的方法比基准方法在多模型适应性（Multi-fidelity）问题中具有更高的准确率和更好的稳定性，并且方法具有可Switch между多模型适应性（Multi-fidelity）和 bayesian优化（Bayesian Optimization）问题的灵活性。<details>
<summary>Abstract</summary>
Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate modeling and design optimization by incorporating data from various low-fidelity (LF) models. While most existing MF methods assume a fixed dataset, adaptive sampling methods that dynamically allocate resources among fidelity models can achieve higher efficiency in the exploring and exploiting the design space. However, most existing MF methods rely on the hierarchical assumption of fidelity levels or fail to capture the intercorrelation between multiple fidelity levels and utilize it to quantify the value of the future samples and navigate the adaptive sampling. To address this hurdle, we propose a framework hinged on a latent embedding for different fidelity models and the associated pre-posterior analysis to explicitly utilize their correlation for adaptive sampling. In this framework, each infill sampling iteration includes two steps: We first identify the location of interest with the greatest potential improvement using the high-fidelity (HF) model, then we search for the next sample across all fidelity levels that maximize the improvement per unit cost at the location identified in the first step. This is made possible by a single Latent Variable Gaussian Process (LVGP) model that maps different fidelity models into an interpretable latent space to capture their correlations without assuming hierarchical fidelity levels. The LVGP enables us to assess how LF sampling candidates will affect HF response with pre-posterior analysis and determine the next sample with the best benefit-to-cost ratio. Through test cases, we demonstrate that the proposed method outperforms the benchmark methods in both MF global fitting (GF) and Bayesian Optimization (BO) problems in convergence rate and robustness. Moreover, the method offers the flexibility to switch between GF and BO by simply changing the acquisition function.
</details>
<details>
<summary>摘要</summary>
多 fideltiness（MF）方法在增强仿真模型和设计优化中得到广泛应用，但大多数现有MF方法假设固定数据集，不能动态分配资源于不同级别的模型。而我们提议的框架是基于不同级别模型之间的秘密嵌入和相关分析来显式利用它们之间的相关性，以便适应тив sampling。在我们的框架中，每次插入样本迭代包括两步：首先使用高级别（HF）模型确定有最大改进潜力的位置，然后在所有级别模型中寻找最大改进效果与这个位置相关的下一个样本。这是由单个秘密变量 Gaussian Process（LVGP）模型实现的，该模型将不同级别模型映射到可解释的秘密空间，以捕捉它们之间的相关性而不需要假设层次结构。LVGP允许我们在先后分析中评估不同级别样本是否会影响HF响应，并确定下一个样本具有最好的利弊比。通过测试，我们表明了我们的方法在多 fideltiness全球适应（GF）和 Bayesian 优化（BO）问题中具有更高的速度和稳定性，并且可以根据采样函数来 switching между GF和BO。
</details></li>
</ul>
<hr>
<h2 id="Burning-the-Adversarial-Bridges-Robust-Windows-Malware-Detection-Against-Binary-level-Mutations"><a href="#Burning-the-Adversarial-Bridges-Robust-Windows-Malware-Detection-Against-Binary-level-Mutations" class="headerlink" title="Burning the Adversarial Bridges: Robust Windows Malware Detection Against Binary-level Mutations"></a>Burning the Adversarial Bridges: Robust Windows Malware Detection Against Binary-level Mutations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03285">http://arxiv.org/abs/2310.03285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Abusnaina, Yizhen Wang, Sunpreet Arora, Ke Wang, Mihai Christodorescu, David Mohaisen</li>
<li>for: 防止攻击者透过攻击表面来推迟适应较好的防御措施，我们专注于攻击表面的探索。</li>
<li>methods: 我们使用实际的binary-level黑盒敌对攻击示例进行根本分析，并发现潜在的攻击表面。此外，我们还探索检测引擎中敏感的变化特征，并证明其可以被攻击。</li>
<li>results: 我们的实验结果显示，传统的防御模型对于攻击探索无法获得显著的效果。但是，我们发现可以通过删除潜在的攻击表面来大大增加防御力。因此，我们提出了一些简单 yet effective的方法来减少攻击表面的影响。总的来说，我们的图形基础攻击探索方法可以实现高度的测试精度，分别为88.32%和88.19%。<details>
<summary>Abstract</summary>
Toward robust malware detection, we explore the attack surface of existing malware detection systems. We conduct root-cause analyses of the practical binary-level black-box adversarial malware examples. Additionally, we uncover the sensitivity of volatile features within the detection engines and exhibit their exploitability. Highlighting volatile information channels within the software, we introduce three software pre-processing steps to eliminate the attack surface, namely, padding removal, software stripping, and inter-section information resetting. Further, to counter the emerging section injection attacks, we propose a graph-based section-dependent information extraction scheme for software representation. The proposed scheme leverages aggregated information within various sections in the software to enable robust malware detection and mitigate adversarial settings. Our experimental results show that traditional malware detection models are ineffective against adversarial threats. However, the attack surface can be largely reduced by eliminating the volatile information. Therefore, we propose simple-yet-effective methods to mitigate the impacts of binary manipulation attacks. Overall, our graph-based malware detection scheme can accurately detect malware with an area under the curve score of 88.32\% and a score of 88.19% under a combination of binary manipulation attacks, exhibiting the efficiency of our proposed scheme.
</details>
<details>
<summary>摘要</summary>
为了提高恶意软件检测的稳定性，我们研究现有恶意软件检测系统的攻击表面。我们对实际的Binary级黑盒反对恶意软件示例进行根本分析。同时，我们揭示检测引擎中敏感的可变特征，并证明其可以被利用。在软件中挖掘可变信道，我们提出三种软件预处理步骤来减少攻击表面，即减少padding，软件剥离，和信道重置。此外，为了对抗增长Section插入攻击，我们提议一种基于图的Section依赖信息提取方案。该方案利用软件中不同Section中的积累信息，以便实现robust的恶意软件检测和缓解对抗设定。我们的实验结果表明，传统的恶意软件检测模型对对抗性攻击无效。但是，可以通过消除可变信道来减少攻击表面。因此，我们提出了一些简单 yet有效的方法来缓解对Binary manipulate攻击的影响。总的来说，我们的图基于恶意软件检测方案可以准确地检测恶意软件，AUC分数为88.32%，并在Binary manipulate攻击下 scores为88.19%，表明我们提出的方案的效果。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Pilot-Contamination-and-Enabling-IoT-Scalability-in-Massive-MIMO-Systems"><a href="#Mitigating-Pilot-Contamination-and-Enabling-IoT-Scalability-in-Massive-MIMO-Systems" class="headerlink" title="Mitigating Pilot Contamination and Enabling IoT Scalability in Massive MIMO Systems"></a>Mitigating Pilot Contamination and Enabling IoT Scalability in Massive MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03278">http://arxiv.org/abs/2310.03278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Kamran Saeed, Ahmed E. Kamal, Ashfaq Khokhar<br>for: 这篇论文关注了大量MIMO系统中的导航信号污染和可扩展性问题。methods: 该论文提出了一种新的导航分配方案，基于设备数据传输模式，将导航序列分配给设备群组而不是个体设备。此外，该论文还使用了最大k-cut图分割方法来解决多个Cell的干扰问题。results: 该论文显示，提出的方案可以显著改善大量MIMO系统的 spectral efficiency，并提高可扩展性。例如，使用十个导航序列可以容纳200个设备，只有12.5%的漏掉率。<details>
<summary>Abstract</summary>
Massive MIMO is expected to play an important role in the development of 5G networks. This paper addresses the issue of pilot contamination and scalability in massive MIMO systems. The current practice of reusing orthogonal pilot sequences in adjacent cells leads to difficulty in differentiating incoming inter- and intra-cell pilot sequences. One possible solution is to increase the number of orthogonal pilot sequences, which results in dedicating more space of coherence block to pilot transmission than data transmission. This, in turn, also hinders the scalability of massive MIMO systems, particularly in accommodating a large number of IoT devices within a cell. To overcome these challenges, this paper devises an innovative pilot allocation scheme based on the data transfer patterns of IoT devices. The scheme assigns orthogonal pilot sequences to clusters of devices instead of individual devices, allowing multiple devices to utilize the same pilot for periodically transmitting data. Moreover, we formulate the pilot assignment problem as a graph coloring problem and use the max k-cut graph partitioning approach to overcome the pilot contamination in a multicell massive MIMO system. The proposed scheme significantly improves the spectral efficiency and enables the scalability of massive MIMO systems; for instance, by using ten orthogonal pilot sequences, we are able to accommodate 200 devices with only a 12.5% omission rate.
</details>
<details>
<summary>摘要</summary>
大规模MIMO在5G网络发展中扮演重要角色。这篇论文强调 orthogonal pilot sequence的重复和大规模MIMO系统的可扩展性问题。当前的实践是在邻近细分单元中重复orthogonal pilot sequence，这会导致进出inter-和intra-细分单元的pilot sequence很难分辨。为了解决这些挑战，这篇论文提出了一种创新的pilot分配方案，基于IoT设备的数据传输模式。该方案将orthogonal pilot sequence分配给设备集而不是单个设备，allowing multiple devices to share the same pilot for periodically transmitting data。此外，我们将pilot分配问题转化为图色分问题，使用max k-cut图分解方法来解决多细分单元大规模MIMO系统中的pilot污染。提议的方案可以显著提高spectral efficiency和大规模MIMO系统的可扩展性;例如，使用十个orthogonal pilot sequence可以容纳200个设备，只有12.5%的漏掉率。
</details></li>
</ul>
<hr>
<h2 id="Fragment-based-Pretraining-and-Finetuning-on-Molecular-Graphs"><a href="#Fragment-based-Pretraining-and-Finetuning-on-Molecular-Graphs" class="headerlink" title="Fragment-based Pretraining and Finetuning on Molecular Graphs"></a>Fragment-based Pretraining and Finetuning on Molecular Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03274">http://arxiv.org/abs/2310.03274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lvkd84/graphfp">https://github.com/lvkd84/graphfp</a></li>
<li>paper_authors: Kha-Dinh Luong, Ambuj Singh</li>
<li>for: 本研究旨在提高Graph Neural Networks（GNNs）在分子图上的预测性能，通过在分子图上预训练GNNs的方法。</li>
<li>methods: 本研究提出了一种基于分子图的fragment预训练方法（GraphFP），通过在分子图上预训练GNNs，并在 fragments上进行预测任务，以提高GNNs的预测性能。</li>
<li>results: 研究发现，GraphFP可以提高5个常见分子测试集上的性能，并在长距离生物测试集上提高至少11.5%。<details>
<summary>Abstract</summary>
Property prediction on molecular graphs is an important application of Graph Neural Networks. Recently, unlabeled molecular data has become abundant, which facilitates the rapid development of self-supervised learning for GNNs in the chemical domain. In this work, we propose pretraining GNNs at the fragment level, a promising middle ground to overcome the limitations of node-level and graph-level pretraining. Borrowing techniques from recent work on principal subgraph mining, we obtain a compact vocabulary of prevalent fragments from a large pretraining dataset. From the extracted vocabulary, we introduce several fragment-based contrastive and predictive pretraining tasks. The contrastive learning task jointly pretrains two different GNNs: one on molecular graphs and the other on fragment graphs, which represents higher-order connectivity within molecules. By enforcing consistency between the fragment embedding and the aggregated embedding of the corresponding atoms from the molecular graphs, we ensure that the embeddings capture structural information at multiple resolutions. The structural information of fragment graphs is further exploited to extract auxiliary labels for graph-level predictive pretraining. We employ both the pretrained molecular-based and fragment-based GNNs for downstream prediction, thus utilizing the fragment information during finetuning. Our graph fragment-based pretraining (GraphFP) advances the performances on 5 out of 8 common molecular benchmarks and improves the performances on long-range biological benchmarks by at least 11.5%. Code is available at: https://github.com/lvkd84/GraphFP.
</details>
<details>
<summary>摘要</summary>
“分子图学习是Graph Neural Networks的重要应用之一。最近，无标记分子数据变得更加广泛，这使得自主学习在化学领域中的GNN发展得更加快速。在这项工作中，我们提议在分子图中预训练GNN，这是一个有前途的中间层，以超越节点级和图级预训练的局限性。通过抽取大量预训练数据中的精炼 vocabulary，我们引入了一些基于分子图的Word2Vec模型，以及一些基于分子图的对比和预测任务。这些任务包括对分子图和分子图中的碎片图进行对比学习，以及在分子图上预测分子的特征。通过在碎片图上预训练GNN，我们可以在下游预测时使用这些碎片信息。我们的分子图碎片预训练（GraphFP）在8个常见分子benchmark中提高了5个benchmark的性能，并在长距离生物benchmark中提高了至少11.5%的性能。代码可以在：https://github.com/lvkd84/GraphFP中找到。”
</details></li>
</ul>
<hr>
<h2 id="UniPredict-Large-Language-Models-are-Universal-Tabular-Predictors"><a href="#UniPredict-Large-Language-Models-are-Universal-Tabular-Predictors" class="headerlink" title="UniPredict: Large Language Models are Universal Tabular Predictors"></a>UniPredict: Large Language Models are Universal Tabular Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03266">http://arxiv.org/abs/2310.03266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiyu Wang, Zifeng Wang, Jimeng Sun</li>
<li>for: 这篇论文旨在开发一种通用的表格数据预测系统，可以在不同的预测任务下进行快速适应和高效的预测。</li>
<li>methods: 这篇论文提出了一种基于生成模型的方法，即UniPredict，通过训练一个大型语言模型（LLM）来建立通用的表格数据预测模型，可以理解多种表格输入和预测目标变量。</li>
<li>results:  experiments 表明，UniPredict 模型在169个不同目标列的表格数据集上显示出了5.4%到13.4%的优势，比基eline最佳树 boosting 和基eline最佳神经网络基eline的性能更高。此外，UniPredict 模型在几十个少量数据集上进行几步学习中也表现出色，超过了 XGBoost 在低资源设置下的性能，并在所有基eline之上显示出了显著的差异。<details>
<summary>Abstract</summary>
Tabular data prediction is a fundamental machine learning task for many applications. Existing methods predominantly employ discriminative modeling and operate under the assumption of a fixed target column, necessitating re-training for every new predictive task. Inspired by the generative power of large language models (LLMs), this paper exploits the idea of building universal tabular data predictors based on generative modeling, namely UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets with the capability of comprehending diverse tabular inputs and predicting for target variables following the input instructions. Specifically, we train a single LLM on an aggregation of 169 tabular datasets with diverse targets and compare its performance against baselines that are trained on each dataset separately. We observe this versatile UniPredict model demonstrates an advantage over other models, ranging from 5.4% to 13.4%, when compared with the best tree-boosting baseline and the best neural network baseline, respectively. We further test UniPredict in few-shot learning settings on another 62 tabular datasets. Our method achieves strong performance in quickly adapting to new tasks, where our method outperforms XGBoost over 100% on the low-resource setup and shows a significant margin over all baselines. We envision that UniPredict sheds light on developing a universal tabular data prediction system that learns from data at scale and serves a wide range of prediction tasks.
</details>
<details>
<summary>摘要</summary>
<SYS>这是一个基本的机器学习任务，它在许多应用中具有重要性。现有的方法主要靠归化模型，并假设有固定的目标字段，需要重新训练每个新的预测任务。这篇论文参考了大型自然语言模型（LLMs）的生成能力，构建了一个通用的 tabular 数据预测器，即 UniPredict。我们在169个不同目标的 tabular 数据集合上训练了单一的 LLM，并与每个数据集合 separately 训练的基准相比较。我们发现这个多元的 UniPredict 模型在与其他模型进行比较中，有优势，兹从5.4% 到13.4%。我们还将 UniPredict 应用于几何少学习设定中，在另外 62 个 tabular 数据集合上进行测试。我们发现我们的方法在新任务上快速适应，比 XGBoost 在低资源设置上高于 100%，并在所有基准中显示了明显的优势。我们希望这个 UniPredict 可以照亮开发一个通用的 tabular 数据预测系统，可以从数据中学习，并且能够应用于广泛的预测任务。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Electricity-Service-Equity-Issues-with-Transfer-Counterfactual-Learning-on-Large-Scale-Outage-Datasets"><a href="#Detecting-Electricity-Service-Equity-Issues-with-Transfer-Counterfactual-Learning-on-Large-Scale-Outage-Datasets" class="headerlink" title="Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets"></a>Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03258">http://arxiv.org/abs/2310.03258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Wei, Xiangrui Kong, Sarah A Huestis-Mitchell, Shixiang Zhu, Yao Xie, Alinson Santos Xavier, Feng Qiu</li>
<li>for: The paper is written to address the challenges of identifying systematic biases in the energy sector, particularly in low-income and elderly-populated areas, using a novel approach for counterfactual causal analysis centered on energy justice.</li>
<li>methods: The paper uses subgroup analysis to manage diverse factors and leverage the idea of transfer learning to mitigate data scarcity in each subgroup.</li>
<li>results: The paper finds that low-income and elderly-populated areas consistently experience longer power outages, regardless of weather conditions, highlighting existing biases in the power system and the need for focused improvements in areas with economic challenges.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解决能源领域中系统性偏见的挑战，特别是在低收入和老龄人口区域中，采用一种新的对称 justice 方法进行Counterfactual 分析。</li>
<li>methods: 论文使用 subgroup analysis 来管理多种因素，并利用传输学习来减轻数据罕见性。</li>
<li>results: 论文发现，低收入和老龄人口区域 invariably 经历长时间的停电，不受天气条件影响，这指示了现有的能源系统偏见，并高亮了需要在经济困难地区进行专注改进。<details>
<summary>Abstract</summary>
Energy justice is a growing area of interest in interdisciplinary energy research. However, identifying systematic biases in the energy sector remains challenging due to confounding variables, intricate heterogeneity in treatment effects, and limited data availability. To address these challenges, we introduce a novel approach for counterfactual causal analysis centered on energy justice. We use subgroup analysis to manage diverse factors and leverage the idea of transfer learning to mitigate data scarcity in each subgroup. In our numerical analysis, we apply our method to a large-scale customer-level power outage data set and investigate the counterfactual effect of demographic factors, such as income and age of the population, on power outage durations. Our results indicate that low-income and elderly-populated areas consistently experience longer power outages, regardless of weather conditions. This points to existing biases in the power system and highlights the need for focused improvements in areas with economic challenges.
</details>
<details>
<summary>摘要</summary>
能源正义是现代能源研究领域的一个快速发展领域。然而，在能源领域中发现系统性偏见仍然是一个挑战，因为存在干扰变量、复杂的治理效果差异和数据稀缺。为解决这些挑战，我们提出了一种新的对假 causal分析方法， centered on 能源正义。我们使用 subgroup analysis来管理多种因素，并利用转移学习的想法来减轻每个 subgroup 中的数据稀缺。在我们的数字分析中，我们对大规模的客户级别停电数据集进行了应用，并 investigate 对假因素，如收入和人口年龄，对停电时间的counterfactual效果。我们的结果表明，低收入和老龄人口地区总是经历 longer 停电时间，无论天气条件如何。这指出了现有的电力系统偏见，并高亮了需要对经济困难地区进行targeted 改进。
</details></li>
</ul>
<hr>
<h2 id="Molecule-Design-by-Latent-Prompt-Transformer"><a href="#Molecule-Design-by-Latent-Prompt-Transformer" class="headerlink" title="Molecule Design by Latent Prompt Transformer"></a>Molecule Design by Latent Prompt Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03253">http://arxiv.org/abs/2310.03253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deqian Kong, Yuhao Huang, Jianwen Xie, Ying Nian Wu</li>
<li>for: 本文提出了一种秘め prompt transformer模型，用于解决化学和生物问题中的困难优化问题，目标是找到具有最佳化学或生物性质的分子。</li>
<li>methods: 本文的模型由三部分组成：（1）一个秘め向量，其先验分布由一个U-Net变换的高斯白噪干扰Vector模型来模型。（2）一个分子生成模型，根据秘め向量（1）生成分子的字符串表示。我们采用了 causal Transformer模型，将秘め向量（1）作为提示。（3）一个性质预测模型，根据非线性回归算法，预测分子的性质值。</li>
<li>results: 我们的实验表明，我们提出的模型在多个benchmark分子设计任务上达到了状态的前景性表现。<details>
<summary>Abstract</summary>
This paper proposes a latent prompt Transformer model for solving challenging optimization problems such as molecule design, where the goal is to find molecules with optimal values of a target chemical or biological property that can be computed by an existing software. Our proposed model consists of three components. (1) A latent vector whose prior distribution is modeled by a Unet transformation of a Gaussian white noise vector. (2) A molecule generation model that generates the string-based representation of molecule conditional on the latent vector in (1). We adopt the causal Transformer model that takes the latent vector in (1) as prompt. (3) A property prediction model that predicts the value of the target property of a molecule based on a non-linear regression on the latent vector in (1). We call the proposed model the latent prompt Transformer model. After initial training of the model on existing molecules and their property values, we then gradually shift the model distribution towards the region that supports desired values of the target property for the purpose of molecule design. Our experiments show that our proposed model achieves state of the art performances on several benchmark molecule design tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>A latent vector whose prior distribution is modeled by a Unet transformation of a Gaussian white noise vector.2. A molecule generation model that generates the string-based representation of molecule conditional on the latent vector in 1. We adopt the causal Transformer model that takes the latent vector in 1 as prompt.3. A property prediction model that predicts the value of the target property of a molecule based on a non-linear regression on the latent vector in 1. We call the proposed model the latent prompt Transformer model.After initial training of the model on existing molecules and their property values, we then gradually shift the model distribution towards the region that supports desired values of the target property for the purpose of molecule design. Our experiments show that our proposed model achieves state-of-the-art performances on several benchmark molecule design tasks.</details></li>
</ol>
<hr>
<h2 id="Relational-Convolutional-Networks-A-framework-for-learning-representations-of-hierarchical-relations"><a href="#Relational-Convolutional-Networks-A-framework-for-learning-representations-of-hierarchical-relations" class="headerlink" title="Relational Convolutional Networks: A framework for learning representations of hierarchical relations"></a>Relational Convolutional Networks: A framework for learning representations of hierarchical relations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03240">http://arxiv.org/abs/2310.03240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/awni00/relational-neural-networks">https://github.com/awni00/relational-neural-networks</a></li>
<li>paper_authors: Awni Altabaa, John Lafferty</li>
<li>for: 本研究探讨了深度学习中关系特征的显式表示方法的发展。</li>
<li>methods: 本文提出了一种叫做”关系卷积网络”的架构设计，用于学习层次结构的关系特征。在给定一个对象序列时，”多维内积关系”模块生成一个关系张量，描述所有对象之间的关系。然后，”关系卷积”层将关系张量转换成一个新的对象序列，每个对象描述了在上一层的对象组中的关系。graphlet筛子，与 convolutional neural networks 中的筛子类似，用于比较关系张量中的模板和对象组中的关系。重复这个过程可以获得高阶层次的关系表示。</li>
<li>results: 我们提供了 architecture 的背景和细节，以及一些实验来证明 relational convolutional networks 可以有效地处理具有层次结构的关系任务。<details>
<summary>Abstract</summary>
A maturing area of research in deep learning is the development of architectures that can learn explicit representations of relational features. In this paper, we focus on the problem of learning representations of hierarchical relations, proposing an architectural framework we call "relational convolutional networks". Given a sequence of objects, a "multi-dimensional inner product relation" module produces a relation tensor describing all pairwise relations. A "relational convolution" layer then transforms the relation tensor into a sequence of new objects, each describing the relations within some group of objects at the previous layer. Graphlet filters, analogous to filters in convolutional neural networks, represent a template of relations against which the relation tensor is compared at each grouping. Repeating this yields representations of higher-order, hierarchical relations. We present the motivation and details of the architecture, together with a set of experiments to demonstrate how relational convolutional networks can provide an effective framework for modeling relational tasks that have hierarchical structure.
</details>
<details>
<summary>摘要</summary>
深度学习领域中一个成熔的研究方向是开发能够学习明确的关系特征表示的架构。在这篇论文中，我们关注了层次关系的学习表示问题，提出了“关系卷积网络”架构。给定一个对象序列，“多维内积关系”模块生成一个关系张量，描述所有对象之间的对称关系。然后，“关系卷积”层将关系张量转换成一个新的对象序列，每个对象描述了上一层层次结构中的关系。Graphlet筛选器，与 convolutional neural networks 中的筛选器相似，表示了关系模板，用于在每个分组中对关系张量进行比较。重复这个过程可以获得更高阶的层次关系表示。我们介绍了架构的动机和细节，以及一系列实验来证明关系卷积网络可以提供有效的层次结构模型。
</details></li>
</ul>
<hr>
<h2 id="Observatory-Characterizing-Embeddings-of-Relational-Tables"><a href="#Observatory-Characterizing-Embeddings-of-Relational-Tables" class="headerlink" title="Observatory: Characterizing Embeddings of Relational Tables"></a>Observatory: Characterizing Embeddings of Relational Tables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07736">http://arxiv.org/abs/2310.07736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/superctj/observatory">https://github.com/superctj/observatory</a></li>
<li>paper_authors: Tianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul Groth, H. V. Jagadish</li>
<li>for: 这篇论文是为了提供一种系统性地分析关系表嵌入表示的形式框架，以便更好地理解关系表嵌入模型的特点和局限性，从而更好地选择合适的模型进行下游任务。</li>
<li>methods: 这篇论文使用了八种基本属性和相应的度量来系统地分析关系表嵌入表示，这些属性包括关系数据模型的不变量和数据分布的统计因素。同时，这篇论文还提出了一种扩展性的评价框架，以便评估语言和表嵌入模型。</li>
<li>results: 这篇论文的分析结果显示，一些关系表嵌入模型对表格结构（如列顺序）有敏感性，功能依赖关系罕见地反映在嵌入中，而特циалиzed表嵌入模型的样本准确率相对较低。这些发现可以帮助研究者和实践者更好地预测模型的行为，选择合适的模型进行下游任务，同时促进研究者开发新的模型。<details>
<summary>Abstract</summary>
Language models and specialized table embedding models have recently demonstrated strong performance on many tasks over tabular data. Researchers and practitioners are keen to leverage these models in many new application contexts; but limited understanding of the strengths and weaknesses of these models, and the table representations they generate, makes the process of finding a suitable model for a given task reliant on trial and error. There is an urgent need to gain a comprehensive understanding of these models to minimize inefficiency and failures in downstream usage.   To address this need, we propose Observatory, a formal framework to systematically analyze embedding representations of relational tables. Motivated both by invariants of the relational data model and by statistical considerations regarding data distributions, we define eight primitive properties, and corresponding measures to quantitatively characterize table embeddings for these properties. Based on these properties, we define an extensible framework to evaluate language and table embedding models. We collect and synthesize a suite of datasets and use Observatory to analyze seven such models. Our analysis provides insights into the strengths and weaknesses of learned representations over tables. We find, for example, that some models are sensitive to table structure such as column order, that functional dependencies are rarely reflected in embeddings, and that specialized table embedding models have relatively lower sample fidelity. Such insights help researchers and practitioners better anticipate model behaviors and select appropriate models for their downstream tasks, while guiding researchers in the development of new models.
</details>
<details>
<summary>摘要</summary>
研究者和实践者很感兴趣利用最新的语言模型和专门的表嵌入模型在表格数据上进行多种任务。然而，对这些模型和它们生成的表嵌入 representations 的 Limited understanding 使得在选择合适的模型时存在很多尝试和失败。为了解决这问题，我们提出了 Observatory，一个正式的框架来系统地分析表嵌入表示。我们受到关系数据模型的 invariants 和数据分布的统计考虑，定义了八个原始属性，并对它们定义了相应的量化度量来 caracterize 表嵌入表示。基于这些属性，我们定义了一个扩展性强的框架来评估语言和表嵌入模型。我们收集了和合并了一系列数据集，并使用 Observatory 分析了七种模型。我们的分析提供了关于学习表嵌入表示的各种强点和弱点的视角，例如表格结构如column order 对模型的敏感性，函数依赖关系在嵌入表示中的罕见出现，以及专门的表嵌入模型在样本准确性方面的较低水平。这些视角可以帮助研究者和实践者更好地预测模型的行为，选择合适的模型进行下游任务，并促进研究者在开发新模型方面的进展。
</details></li>
</ul>
<hr>
<h2 id="History-Matching-for-Geological-Carbon-Storage-using-Data-Space-Inversion-with-Spatio-Temporal-Data-Parameterization"><a href="#History-Matching-for-Geological-Carbon-Storage-using-Data-Space-Inversion-with-Spatio-Temporal-Data-Parameterization" class="headerlink" title="History Matching for Geological Carbon Storage using Data-Space Inversion with Spatio-Temporal Data Parameterization"></a>History Matching for Geological Carbon Storage using Data-Space Inversion with Spatio-Temporal Data Parameterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03228">http://arxiv.org/abs/2310.03228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Su Jiang, Louis J. Durlofsky<br>for: 这个论文主要关注于如何通过监测数据来减少不确定性，从而改善大规模碳Capture和存储操作中 aquifer 的管理。methods: 这个论文使用了数据空间反推（DSI）技术，通过直接从观测数据中寻找历史匹配的量 interesting，而不需要构建后验地球模型。这里使用了深度学习来parameterize spatio-temporal压力和气溶度场的表达。results: 研究发现，使用这种新的深度学习参数化技术可以减少 posterior 压力和气溶度场中的不确定性，并且可以提供高效的 posterior 预测。这种方法可以应用于多种不同的地质enario中，并且可以efficiently 处理大规模的数据。<details>
<summary>Abstract</summary>
History matching based on monitoring data will enable uncertainty reduction, and thus improved aquifer management, in industrial-scale carbon storage operations. In traditional model-based data assimilation, geomodel parameters are modified to force agreement between flow simulation results and observations. In data-space inversion (DSI), history-matched quantities of interest, e.g., posterior pressure and saturation fields conditioned to observations, are inferred directly, without constructing posterior geomodels. This is accomplished efficiently using a set of O(1000) prior simulation results, data parameterization, and posterior sampling within a Bayesian setting. In this study, we develop and implement (in DSI) a deep-learning-based parameterization to represent spatio-temporal pressure and CO2 saturation fields at a set of time steps. The new parameterization uses an adversarial autoencoder (AAE) for dimension reduction and a convolutional long short-term memory (convLSTM) network to represent the spatial distribution and temporal evolution of the pressure and saturation fields. This parameterization is used with an ensemble smoother with multiple data assimilation (ESMDA) in the DSI framework to enable posterior predictions. A realistic 3D system characterized by prior geological realizations drawn from a range of geological scenarios is considered. A local grid refinement procedure is introduced to estimate the error covariance term that appears in the history matching formulation. Extensive history matching results are presented for various quantities, for multiple synthetic true models. Substantial uncertainty reduction in posterior pressure and saturation fields is achieved in all cases. The framework is applied to efficiently provide posterior predictions for a range of error covariance specifications. Such an assessment would be expensive using a model-based approach.
</details>
<details>
<summary>摘要</summary>
历史匹配基于监测数据可以减少不确定性，从而改进地下温室气体存储操作的管理。传统的模型基于数据整合中，地球模型参数被修改以使流 simulate结果和观测数据匹配。在数据空间反向整合（DSI）中，历史匹配的量据 Interest，例如后逻 posterior压力和渗透率场，通过直接推算而不需要构建 posterior 地球模型。这可以通过一组 O(1000) 的先 simulation 结果、数据 parameterization 和 posterior 采样在 Bayesian 设定下进行高效地完成。在这种研究中，我们开发并实现了基于深度学习的参数化方法，用于表示空间temporal压力和渗透率场的 spatio-temporal 分布。这种参数化使用了一个对抗 autoencoder（AAE）进行维度减少，并使用一个 convolutional long short-term memory（convLSTM）网络来表示空间分布和时间演化的压力和渗透率场。这种参数化与一个ensemble smoother with multiple data assimilation（ESMDA）在 DSI 框架中使用，以实现 posterior 预测。我们考虑了一个实际的3D系统，其中 prior geological realizations 是从一系列地质enario中采样的。我们还引入了一种本地网格细化过程，以估计历史匹配 формулы中出现的错误covariance 项。我们对多种真实模型进行了广泛的历史匹配结果，并在所有情况下都 achieve 了重要的不确定性减少。这种框架可以高效地提供 posterior 预测，对于多种错误covariance 规则。这种评估 would be  expensive using a model-based approach。
</details></li>
</ul>
<hr>
<h2 id="TacoGFN-Target-Conditioned-GFlowNet-for-Structure-Based-Drug-Design"><a href="#TacoGFN-Target-Conditioned-GFlowNet-for-Structure-Based-Drug-Design" class="headerlink" title="TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design"></a>TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03223">http://arxiv.org/abs/2310.03223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tony Shen, Mohit Pandey, Martin Ester</li>
<li>for: 本研究旨在自动生成适应特定蛋白质槽目标的药物类分子。现有方法通常是使用finite数据集的分布来近似蛋白质-分子分布，因此很难生成与训练数据集中的绑定效果有显著提高的分子。本研究将 pocket-conditioned molecular generation task 转化为一个RL问题，并开发了Target Conditional Generative Flow Network（TacoGFN）模型。</li>
<li>methods: 我们开发了一种基于transformer的停搁分子检测器，以便快速计算停搁分子的拥有能力。此外，我们还提出了several rounds of active learning，通过使用停搁分子来提高停搁分子的预测。这种方法可以高效地探索分子空间。</li>
<li>results: 对比baseline方法，分子生成使用TacoGFN和其变种显著超越了所有性能指标（停搁分数、QED、SA、Lipinski），而且计算速度是当前最快的一个数量级。<details>
<summary>Abstract</summary>
We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule landscape as we can afford computationally. Empirically, molecules generated using TacoGFN and its variants significantly outperform all baseline methods across every property (Docking score, QED, SA, Lipinski), while being orders of magnitude faster.
</details>
<details>
<summary>摘要</summary>
我们寻求自动生成适应特定蛋白质袋子目标的药物类分子。现有方法通常对蛋白质-分子分布数据集进行近似，因此很难生成具有显著的绑定提升的分子。我们将袋子条件的分子生成任务视为一个RL问题，并开发了目标条件生成流网络模型（TacoGFN）。我们的方法会主动生成具有愿景属性的分子，而不是适应已有数据分布。为此，我们开发了基于变换器的吸引力预测器，以加速吸引力预测和提高分子空间的探索效率。此外，我们进行了多轮活动学习，通过使用吸引力或acles来改进吸引力预测。这种方法让我们可以准确地探索计算能力范围内的分子领域。实验表明，使用TacoGFN和其变种生成的分子具有显著性，在每一个性能指标（吸引力、QED、SA、利平斯基）上都高于所有基eline方法，而且计算速度是其他方法的数量级快。
</details></li>
</ul>
<hr>
<h2 id="Formal-and-Practical-Elements-for-the-Certification-of-Machine-Learning-Systems"><a href="#Formal-and-Practical-Elements-for-the-Certification-of-Machine-Learning-Systems" class="headerlink" title="Formal and Practical Elements for the Certification of Machine Learning Systems"></a>Formal and Practical Elements for the Certification of Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03217">http://arxiv.org/abs/2310.03217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean-Guillaume Durand, Arthur Dubois, Robert J. Moss</li>
<li>for: 这篇论文的目的是如何在自动驾驶飞行中使用机器学习模型，以确保其安全性和可靠性。</li>
<li>methods: 这篇论文使用了一种基于统计学的验证器，以确保机器学习模型的正确性和可靠性。这种验证器是模型无关的，工具无关的，可以适用于多种应用场景。</li>
<li>results: 这篇论文通过对视觉 landing 的应用来展示了其certification framework的效果。<details>
<summary>Abstract</summary>
Over the past decade, machine learning has demonstrated impressive results, often surpassing human capabilities in sensing tasks relevant to autonomous flight. Unlike traditional aerospace software, the parameters of machine learning models are not hand-coded nor derived from physics but learned from data. They are automatically adjusted during a training phase, and their values do not usually correspond to physical requirements. As a result, requirements cannot be directly traced to lines of code, hindering the current bottom-up aerospace certification paradigm. This paper attempts to address this gap by 1) demystifying the inner workings and processes to build machine learning models, 2) formally establishing theoretical guarantees given by those processes, and 3) complementing these formal elements with practical considerations to develop a complete certification argument for safety-critical machine learning systems. Based on a scalable statistical verifier, our proposed framework is model-agnostic and tool-independent, making it adaptable to many use cases in the industry. We demonstrate results on a widespread application in autonomous flight: vision-based landing.
</details>
<details>
<summary>摘要</summary>
过去一个十年，机器学习已经表现出了很强的成果，经常超越人类的能力在自动飞行相关的感知任务上。与传统的航空航天软件不同，机器学习模型的参数不是手动编码也不是从物理学 derivated，而是从数据学习。在训练阶段，它们的值会自动调整，并且通常不符合物理要求。因此，要证明这些模型的安全性是一个大的挑战。本文尝试解决这个问题，通过以下三个方法：1. 推动机器学习模型的内部工作和过程的启示，以便更好地理解它们是如何工作的。2. 正式确立机器学习过程的理论保证，以确保它们可以在安全的情况下工作。3. 结合实际考虑因素，开发一个完整的证明框架，以便在安全关键的机器学习系统中保证安全性。我们的提议的框架是基于可扩展的统计验证器，可以独立于模型和工具。这使得它适用于各种业务场景。我们在视觉相关的着陆任务上进行了实践，并取得了良好的结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/05/cs.LG_2023_10_05/" data-id="clp88dbyy00ssob88hjst5m3t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/05/eess.IV_2023_10_05/" class="article-date">
  <time datetime="2023-10-05T09:00:00.000Z" itemprop="datePublished">2023-10-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/05/eess.IV_2023_10_05/">eess.IV - 2023-10-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MultiHU-TD-Multifeature-Hyperspectral-Unmixing-Based-on-Tensor-Decomposition"><a href="#MultiHU-TD-Multifeature-Hyperspectral-Unmixing-Based-on-Tensor-Decomposition" class="headerlink" title="MultiHU-TD: Multifeature Hyperspectral Unmixing Based on Tensor Decomposition"></a>MultiHU-TD: Multifeature Hyperspectral Unmixing Based on Tensor Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03860">http://arxiv.org/abs/2310.03860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad Jouni, Mauro Dalla Mura, Lucas Drumetz, Pierre Comon</li>
<li>for: 本研究旨在提出一种可解释的多特征谱分法（MultiHU-TD），用于解决高spectral图像（HSI）中的杂合问题。</li>
<li>methods: 本方法基于tensor decompositions，并通过alternating direction method of multipliers（ADMM）算法实现了约束杂合约束（abundance sum-to-one）。此外，本研究还提出了在MultiHU-TD中 incorporate mathematical morphology和 neighboorhood patches的方法。</li>
<li>results: 实验表明，MultiHU-TD方法可以提供可解释的模型和分析结果，并且可以应用于实际高spectral图像分析 task。Python和MATLAB实现在GitHub上可用。<details>
<summary>Abstract</summary>
Hyperspectral unmixing allows representing mixed pixels as a set of pure materials weighted by their abundances. Spectral features alone are often insufficient, so it is common to rely on other features of the scene. Matrix models become insufficient when the hyperspectral image (HSI) is represented as a high-order tensor with additional features in a multimodal, multifeature framework. Tensor models such as canonical polyadic decomposition allow for this kind of unmixing but lack a general framework and interpretability of the results. In this article, we propose an interpretable methodological framework for low-rank multifeature hyperspectral unmixing based on tensor decomposition (MultiHU-TD) that incorporates the abundance sum-to-one constraint in the alternating optimization alternating direction method of multipliers (ADMM) algorithm and provide in-depth mathematical, physical, and graphical interpretation and connections with the extended linear mixing model. As additional features, we propose to incorporate mathematical morphology and reframe a previous work on neighborhood patches within MultiHU-TD. Experiments on real HSIs showcase the interpretability of the model and the analysis of the results. Python and MATLAB implementations are made available on GitHub.
</details>
<details>
<summary>摘要</summary>
hyperspectral unmixing可以将杂合像 Represented as a set of pure materials weighted by their abundances. spectral features alone are often insufficient, so it is common to rely on other features of the scene. Matrix models become insufficient when the hyperspectral image (HSI) is represented as a high-order tensor with additional features in a multimodal, multifeature framework. Tensor models such as canonical polyadic decomposition allow for this kind of unmixing but lack a general framework and interpretability of the results.在本文中，我们提出了一种可解释的方法oloyg framework for low-rank multifeature hyperspectral unmixing based on tensor decomposition (MultiHU-TD)，该方法包括了积分权重的权重积分法ADMM算法中的积分总等于一个约束，并提供了深入的数学、物理和图形解释，以及与扩展线性混合模型的连接。此外，我们还提出了在MultiHU-TD中包含数学形态和 reformulate a previous work on neighborhood patches的方法。实验表明，该模型具有可解释性和分析结果的优点。Python和MATLAB实现在GitHub上提供。
</details></li>
</ul>
<hr>
<h2 id="Role-of-Spatial-Coherence-in-Diffractive-Optical-Neural-Networks"><a href="#Role-of-Spatial-Coherence-in-Diffractive-Optical-Neural-Networks" class="headerlink" title="Role of Spatial Coherence in Diffractive Optical Neural Networks"></a>Role of Spatial Coherence in Diffractive Optical Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03679">http://arxiv.org/abs/2310.03679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew J. Filipovich, Aleksei Malyshev, A. I. Lvovsky</li>
<li>for: 这 paper 是用于研究Diffractive optical neural networks (DONNs) 的应用于计算机视觉任务中的快速和能效的信号处理方法。</li>
<li>methods: 这 paper 使用了数值方法来模拟 DONNs 在不同空间干涉度下的运行，并研究了这些方法的计算复杂度。</li>
<li>results: 研究发现，在完全无干涉照明下，DONN 的性能不能超过线性模型。 authors 还通过使用不同的空间干涉度来训练和评估 DONNs 在 MNIST 数据集上的表现。<details>
<summary>Abstract</summary>
Diffractive optical neural networks (DONNs) have emerged as a promising optical hardware platform for ultra-fast and energy-efficient signal processing for machine learning tasks, particularly in computer vision. However, previous experimental demonstrations of DONNs have only been performed using coherent light, which is not present in the natural world. Here, we study the role of spatial optical coherence in DONN operation. We propose a numerical approach to efficiently simulate DONNs under input illumination with arbitrary spatial coherence and discuss the corresponding computational complexity using coherent, partially coherent, and incoherent light. We also investigate the expressive power of DONNs and examine how coherence affects their performance. In particular, we show that under fully incoherent illumination, the DONN performance cannot surpass that of a linear model. As a demonstration, we train and evaluate simulated DONNs on the MNIST dataset of handwritten digits using light with varying spatial coherence.
</details>
<details>
<summary>摘要</summary>
干扰性光学神经网络（DONNs）已经出现为光学硬件平台，用于机器学习任务，特别是计算机视觉领域的快速和能效处理。然而，之前的实验性证明只使用了同步光，这不是自然界中存在的。在这里，我们研究了DONN操作中的空间光干扰的作用。我们提出了一种数字方法，用于高效地模拟DONNs，并对输入干扰的任意空间干扰进行计算复杂性分析。此外，我们还 investigate了DONN表达力的问题，并证明在完全不干扰的照明下，DONN性能不能超过线性模型。为 Proof of concept，我们使用了MNIST数据集的手写数字进行训练和评估，并使用不同的空间干扰来评估DONN的性能。
</details></li>
</ul>
<hr>
<h2 id="Multispectral-Imaging-with-Fresnel-Lens"><a href="#Multispectral-Imaging-with-Fresnel-Lens" class="headerlink" title="Multispectral Imaging with Fresnel Lens"></a>Multispectral Imaging with Fresnel Lens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03625">http://arxiv.org/abs/2310.03625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khen Cohen, Tuval Kay</li>
<li>for: 这种研究旨在开发一种低成本、快速捕捉多спектраль图像的方法，以适应移动设备中的应用。</li>
<li>methods: 该方法利用了一个折射光学元件和深度学习算法，实现了多спектраль图像重建。具体来说，使用了一个卷积镜、灰度感知器和光学机制来捕捉多спектраль图像，并通过折射物理理论和深度学习算法来重建多个 spectral channel 图像。</li>
<li>results: 实验表明，该方法可以在不牺牲空间分辨率的情况下，重建多达50个 spectral channel 图像。这种方法在成本、体积和时间频率等方面具有优势，可能用于开发一种可靠、高效的多спектраль摄像头。<details>
<summary>Abstract</summary>
This paper presents a Multispectral imaging (MSI) approach that combines the use of a diffractive optical element, and a deep learning algorithm for spectral reconstruction. Traditional MSI techniques often face challenges such as high costs, compromised spatial or spectral resolution, or prolonged acquisition times. In contrast, our methodology uses a single diffractive lens, a grayscale sensor, and an optical motor to capture the Multispectral image without sacrificing spatial resolution, however with some temporal domain redundancy. Through an experimental demonstration, we show how we can reconstruct up to 50 spectral channel images using diffraction physical theory and a UNet-based deep learning algorithm. This approach holds promise for a cost-effective, compact MSI camera that could be feasibly integrated into mobile devices.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/05/eess.IV_2023_10_05/" data-id="clp88dc5v01b2ob883l8qd4v3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/05/eess.SP_2023_10_05/" class="article-date">
  <time datetime="2023-10-05T08:00:00.000Z" itemprop="datePublished">2023-10-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/05/eess.SP_2023_10_05/">eess.SP - 2023-10-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Realizing-XR-Applications-Using-5G-Based-3D-Holographic-Communication-and-Mobile-Edge-Computing"><a href="#Realizing-XR-Applications-Using-5G-Based-3D-Holographic-Communication-and-Mobile-Edge-Computing" class="headerlink" title="Realizing XR Applications Using 5G-Based 3D Holographic Communication and Mobile Edge Computing"></a>Realizing XR Applications Using 5G-Based 3D Holographic Communication and Mobile Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03908">http://arxiv.org/abs/2310.03908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dun Yuan, Ekram Hossain, Di Wu, Xue Liu, Gregory Dudek</li>
<li>for: 提高3D激光通信的用户体验，增强虚拟空间内人员之间的交互。</li>
<li>methods: 利用移动边缘计算（MEC）服务器，实现最小化总延迟的3D激光通信。</li>
<li>results: 对比基eline方法，提出的算法显示出显著的延迟减少效果，并在AR应用中进行了实践。<details>
<summary>Abstract</summary>
3D holographic communication has the potential to revolutionize the way people interact with each other in virtual spaces, offering immersive and realistic experiences. However, demands for high data rates, extremely low latency, and high computations to enable this technology pose a significant challenge. To address this challenge, we propose a novel job scheduling algorithm that leverages Mobile Edge Computing (MEC) servers in order to minimize the total latency in 3D holographic communication. One of the motivations for this work is to prevent the uncanny valley effect, which can occur when the latency hinders the seamless and real-time rendering of holographic content, leading to a less convincing and less engaging user experience. Our proposed algorithm dynamically allocates computation tasks to MEC servers, considering the network conditions, computational capabilities of the servers, and the requirements of the 3D holographic communication application. We conduct extensive experiments to evaluate the performance of our algorithm in terms of latency reduction, and the results demonstrate that our approach significantly outperforms other baseline methods. Furthermore, we present a practical scenario involving Augmented Reality (AR), which not only illustrates the applicability of our algorithm but also highlights the importance of minimizing latency in achieving high-quality holographic views. By efficiently distributing the computation workload among MEC servers and reducing the overall latency, our proposed algorithm enhances the user experience in 3D holographic communications and paves the way for the widespread adoption of this technology in various applications, such as telemedicine, remote collaboration, and entertainment.
</details>
<details>
<summary>摘要</summary>
三维杂alomatic通信有可能对虚拟空间中人们之间的交互方式进行革命性的改变，提供 immerse 和 realistic 的经验。然而，实现这一技术的需求包括高数据速率、极低延迟和高计算能力，这些要求成为了一个 significante 挑战。为解决这一挑战，我们提议一种基于 Mobile Edge Computing（MEC）服务器的新的任务调度算法，以最小化3D杂alomatic通信中的总延迟。我们的一个动机是避免“uncanny valley”效应，这种效应可以在延迟缓慢了杂alomatic内容的渲染，导致用户体验更加不真实、不 engagising。我们的提议的算法动态地将计算任务分配给 MEC 服务器，考虑到网络条件、服务器计算能力和3D杂alomatic通信应用的需求。我们进行了广泛的实验来评估我们的算法在延迟减少方面的性能，结果表明我们的方法在其他基准方法上显著超越。此外，我们还提供了一个实际的AR应用场景，不仅说明了我们的算法的可行性，还强调了在 достиieving高质量杂alomatic视图的过程中减少延迟的重要性。通过有效地分配计算任务和减少总延迟，我们的提议的算法提高了3D杂alomatic通信中用户体验的质量，并为这种技术在各种应用，如 теле医、远程合作和娱乐等，开创了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Coefficients-based-Intelligent-Protection-of-Transmission-Lines-Connected-to-Type-3-Wind-Farms"><a href="#Autoregressive-Coefficients-based-Intelligent-Protection-of-Transmission-Lines-Connected-to-Type-3-Wind-Farms" class="headerlink" title="Autoregressive Coefficients based Intelligent Protection of Transmission Lines Connected to Type-3 Wind Farms"></a>Autoregressive Coefficients based Intelligent Protection of Transmission Lines Connected to Type-3 Wind Farms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03663">http://arxiv.org/abs/2310.03663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pallav Kumar Bera, Vajendra Kumar, Samita Rani Pani, Om P. Malik</li>
<li>for: 本研究旨在探讨保护关系器（distance relay）在大容量风力发电厂（WF）连接的干线上的表现，并提出了一个基于统计学模型的智能保护方案。</li>
<li>methods: 本研究使用了适应范围复杂决策系统（Adaptive Fuzzy Inference System，AFIS）检测缺陷，使用了最小重复性最大相关性算法（Minimum Redundancy Maximum Relevance，MRMR）选择3相电流的AR系数。此外，使用了深度学习网络来监控缺陷检测、缺陷的位置和类型的检测。</li>
<li>results: 研究结果显示，提议的方案能够在不同的系统状态和配置下具有较高的检测精度和速度，并能够适应不同的缺陷型态、位置、时间、风速、变压器连接等因素。<details>
<summary>Abstract</summary>
Protective relays can mal-operate for transmission lines connected to doubly fed induction generator (DFIG) based large capacity wind farms (WFs). The performance of distance relays protecting such lines is investigated and a statistical model based intelligent protection of the area between the grid and the WF is proposed in this article. The suggested method employs an adaptive fuzzy inference system to detect faults based on autoregressive (AR) coefficients of the 3-phase currents selected using minimum redundancy maximum relevance algorithm. Deep learning networks are used to supervise the detection of faults, their subsequent localization, and classification. The effectiveness of the scheme is evaluated on IEEE 9-bus and IEEE 39-bus systems with varying fault resistances, fault inception times, locations, fault types, wind speeds, and transformer connections. Further, the impact of factors like the presence of type-4 WFs, double circuit lines, WF capacity, grid strength, FACTs devices, reclosing on permanent faults, power swings, fault during power swings, voltage instability, load encroachment, high impedance faults, evolving and cross-country faults, close-in and remote-end faults, CT saturation, sampling rate, data window size, synchronization error, noise, and semi-supervised learning are considered while validating the proposed scheme. The results show the efficacy of the suggested method in dealing with various system conditions and configurations while protecting the transmission lines that are connected to WFs.
</details>
<details>
<summary>摘要</summary>
保护关系可能会不正确地工作，当电力 transmission lines 连接到 doubly fed induction generator（DFIG）基于大容量风力电站（WF）时。本文 investigate 的 performance of distance relays protecting such lines and propose a statistical model based intelligent protection of the area between the grid and the WF. The suggested method employs an adaptive fuzzy inference system to detect faults based on autoregressive（AR）coefficients of the 3-phase currents selected using minimum redundancy maximum relevance algorithm. Deep learning networks are used to supervise the detection of faults, their subsequent localization, and classification.The effectiveness of the scheme is evaluated on IEEE 9-bus and IEEE 39-bus systems with varying fault resistances, fault inception times, locations, fault types, wind speeds, and transformer connections. Further, the impact of factors like the presence of type-4 WFs, double circuit lines, WF capacity, grid strength, FACTs devices, reclosing on permanent faults, power swings, fault during power swings, voltage instability, load encroachment, high impedance faults, evolving and cross-country faults, close-in and remote-end faults, CT saturation, sampling rate, data window size, synchronization error, noise, and semi-supervised learning are considered while validating the proposed scheme. The results show the efficacy of the suggested method in dealing with various system conditions and configurations while protecting the transmission lines that are connected to WFs.
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Artificial-Intelligence-on-Electrical-and-Electronics-Engineering-Productivity-in-the-Construction-Industry"><a href="#Impact-of-Artificial-Intelligence-on-Electrical-and-Electronics-Engineering-Productivity-in-the-Construction-Industry" class="headerlink" title="Impact of Artificial Intelligence on Electrical and Electronics Engineering Productivity in the Construction Industry"></a>Impact of Artificial Intelligence on Electrical and Electronics Engineering Productivity in the Construction Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03591">http://arxiv.org/abs/2310.03591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nwosu Obinnaya Chikezie Victor</li>
<li>for: 这些论文是为了探讨人工智能在建筑工程中的应用和影响，以提高建筑设计、建造和运营的效率和产效。</li>
<li>methods: 这篇论文使用了人工智能技术，包括机器学习算法和大数据分析，对建筑设计、能源消耗和安全等方面进行了分析和优化。</li>
<li>results: 这篇论文的研究结果表明，人工智能可以大大提高建筑设计和建造的效率和产效，同时可以降低能源消耗和提高建筑安全性。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) can revolutionize the development industry, primarily electrical and electronics engineering. By automating recurring duties, AI can grow productivity and efficiency in creating. For instance, AI can research constructing designs, discover capability troubles, and generate answers, reducing the effort and time required for manual analysis. AI also can be used to optimize electricity consumption in buildings, which is a critical difficulty in the construction enterprise. Via machines gaining knowledge of algorithms to investigate electricity usage patterns, AI can discover areas wherein power may be stored and offer guidelines for enhancements. This can result in significant value financial savings and reduced carbon emissions. Moreover, AI may be used to improve the protection of creation websites. By studying statistics from sensors and cameras, AI can locate capacity dangers and alert workers to take suitable action. This could help save you from injuries and accidents on production sites, lowering the chance for workers and enhancing overall safety in the enterprise. The impact of AI on electric and electronics engineering productivity inside the creation industry is enormous. AI can transform how we layout, build, and function buildings by automating ordinary duties, optimising electricity intake, and enhancing safety. However, ensuring that AI is used ethically and responsibly and that the advantages are shared fairly throughout the enterprise is essential.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）可以革命化建筑业，特别是电气和电子工程。通过自动化重复任务，AI可以提高产出力和效率，从而提高建筑设计的创新能力。例如，AI可以研究建筑设计，发现能源问题和提供解决方案，从而减少人工分析的劳动和时间。此外，AI还可以优化建筑物业的能源消耗，从而解决建筑业中的重要问题。通过机器学习算法分析能源使用模式，AI可以发现能源的浪费和提供改进建议，从而实现重要的成本节省和减少碳排放。此外，AI还可以提高建筑工地的安全性。通过分析感知器和摄像头数据，AI可以检测潜在风险并警示工作人员采取适当行动，从而避免伤害和事故发生在建筑工地上，提高工人安全性和全面的安全性。AI对电气和电子工程产出力的影响是巨大的，可以改变我们的设计、建造和运营建筑的方式，自动化常见任务、优化能源消耗和提高安全性。然而，确保AI的使用是道德和责任的，并确保产业中的利益均衡分配是关键。
</details></li>
</ul>
<hr>
<h2 id="Digital-Twin-Empowered-Smart-Attack-Detection-System-for-6G-Edge-of-Things-Networks"><a href="#Digital-Twin-Empowered-Smart-Attack-Detection-System-for-6G-Edge-of-Things-Networks" class="headerlink" title="Digital Twin-Empowered Smart Attack Detection System for 6G Edge of Things Networks"></a>Digital Twin-Empowered Smart Attack Detection System for 6G Edge of Things Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03554">http://arxiv.org/abs/2310.03554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yagmur Yigit, Christos Chrysoulas, Gokhan Yurdakul, Leandros Maglaras, Berk Canberk</li>
<li>for: 提高6G Edge of Things（EoT）网络安全性</li>
<li>methods: 利用数字孪生和边计算技术实时监测和模拟物理资产，并使用在线学习模块优化网络性能</li>
<li>results: 实现了高效、可靠和适应性的攻击检测，保障6G EoT网络安全性<details>
<summary>Abstract</summary>
As global Internet of Things (IoT) devices connectivity surges, a significant portion gravitates towards the Edge of Things (EoT) network. This shift prompts businesses to deploy infrastructure closer to end-users, enhancing accessibility. However, the growing EoT network expands the attack surface, necessitating robust and proactive security measures. Traditional solutions fall short against dynamic EoT threats, highlighting the need for proactive and intelligent systems. We introduce a digital twin-empowered smart attack detection system for 6G EoT networks. Leveraging digital twin and edge computing, it monitors and simulates physical assets in real time, enhancing security. An online learning module in the proposed system optimizes the network performance. Our system excels in proactive threat detection, ensuring 6G EoT network security. The performance evaluations demonstrate its effectiveness, robustness, and adaptability using real datasets.
</details>
<details>
<summary>摘要</summary>
globally, the number of Internet of Things (IoT) devices connecting to the internet is surging, and a significant portion of these devices are gravitating towards the Edge of Things (EoT) network. This shift is causing businesses to deploy infrastructure closer to end-users, which enhances accessibility. However, the growing EoT network is expanding the attack surface, making it necessary to implement robust and proactive security measures. Traditional solutions are insufficient against the dynamic threats posed by EoT, highlighting the need for proactive and intelligent systems.To address this need, we propose a digital twin-empowered smart attack detection system for 6G EoT networks. By leveraging digital twin and edge computing, the system monitors and simulates physical assets in real time, enhancing security. An online learning module in the proposed system optimizes network performance. Our system excels in proactive threat detection, ensuring the security of 6G EoT networks. Performance evaluations demonstrate its effectiveness, robustness, and adaptability using real datasets.
</details></li>
</ul>
<hr>
<h2 id="Human-Respiration-Detection-Under-Interference-Challenges-and-Solutions"><a href="#Human-Respiration-Detection-Under-Interference-Challenges-and-Solutions" class="headerlink" title="Human Respiration Detection Under Interference: Challenges and Solutions"></a>Human Respiration Detection Under Interference: Challenges and Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03297">http://arxiv.org/abs/2310.03297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kehan Wu, Renqi Chen, Haiyu Wang, Guang Wu<br>for: 本研究旨在探讨通用WiFi设备可以检测人类呼吸速率，但这些设备在日常生活中遇到人体运动干扰的情况下具有准确检测人呼吸的能力。methods: 本研究提出了一种专门为人呼吸检测设计的不活跃感知和通信系统，该系统在60.48GHz频率带内运行，能够在人体运动干扰的情况下检测人呼吸。研究人员使用训练 neural network 来实现人呼吸检测。results: 实验结果表明，该系统在人体运动干扰的情况下可以保持高于90%的人呼吸检测精度， provided that the sensing duration is adequate. 最后，研究人员 derivated一个分析模型来实现在10秒内计算呼吸速率。<details>
<summary>Abstract</summary>
Recent research has highlighted the detection of human respiration rate using commodity WiFi devices. Nevertheless, these devices encounter challenges in accurately discerning human respiration amidst the prevailing human motion interference encountered in daily life. To tackle this predicament, this paper introduces a passive sensing and communication system designed specifically for respiration detection in the presence of robust human motion interference. Operating within the 60.48GHz band, the proposed system aims to detect human respiration even when confronted with substantial human motion interference within close proximity. Subsequently, a neural network is trained using the collected data by us to enable human respiration detection. The experimental results demonstrate a consistently high accuracy rate over 90\% of the human respiration detection under interference, given an adequate sensing duration. Finally, an empirical model is derived analytically to achieve the respiratory rate counting in 10 seconds.
</details>
<details>
<summary>摘要</summary>
近期研究表明，可用商业 WiFi 设备探测人类呼吸速率。然而，这些设备在日常生活中遇到人体运动干扰的情况下减少呼吸速率的准确性。为解决这个问题，本文提出了一种特有的投射感知和通信系统，用于在人体运动干扰的情况下探测人类呼吸速率。该系统在60.48GHz频率带内运行，能够在近距离 confronted with substantial human motion interference 下探测人类呼吸速率，并且通过我们collected数据进行训练，以实现人类呼吸速率探测。实验结果表明，在90%的人类呼吸速率探测情况下，系统具有高度的精度和可靠性。最后，我们 derivation analytical model 用于计算呼吸速率 counting 在10秒内。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Indoor-Environment-Dataset-from-Single-family-Houses-in-the-US"><a href="#A-Comprehensive-Indoor-Environment-Dataset-from-Single-family-Houses-in-the-US" class="headerlink" title="A Comprehensive Indoor Environment Dataset from Single-family Houses in the US"></a>A Comprehensive Indoor Environment Dataset from Single-family Houses in the US</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03771">http://arxiv.org/abs/2310.03771</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anik801/bdl_data_1">https://github.com/anik801/bdl_data_1</a></li>
<li>paper_authors: Sheik Murad Hassan Anik, Xinghua Gao, Na Meng</li>
<li>for: 本研究旨在研究室内环境因素，包括温度、湿度、空气质量和噪声水平，以及这些因素在不同时间和位置之间的变化。</li>
<li>methods: 本研究使用10个感知器在维吉尼亚州三座单户别宅邸中的不同位置安装，收集了一年的数据，共计250万条记录，每分钟一条记录。研究者还提供了实际的地板图和感知器布局，以帮助研究者和实践者创建可靠的建筑性能模型。</li>
<li>results: 本研究得到了一个包含室内环境因素的大量数据集，可以用于提高建筑能源消耗、occupant behavior、预测维护和其他相关领域的模型。<details>
<summary>Abstract</summary>
The paper describes a dataset comprising indoor environmental factors such as temperature, humidity, air quality, and noise levels. The data was collected from 10 sensing devices installed in various locations within three single-family houses in Virginia, USA. The objective of the data collection was to study the indoor environmental conditions of the houses over time. The data were collected at a frequency of one record per minute for a year, combining over 2.5 million records. The paper provides actual floor plans with sensor placements to aid researchers and practitioners in creating reliable building performance models. The techniques used to collect and verify the data are also explained in the paper. The resulting dataset can be employed to enhance models for building energy consumption, occupant behavior, predictive maintenance, and other relevant purposes.
</details>
<details>
<summary>摘要</summary>
文章描述了一个包含室内环境因素的数据集，包括温度、湿度、空气质量和噪声水平。数据来自于美国弗吉尼亚州三座单户住宅内的10个感知设备的收集， duration of one year, totaling over 2.5 million records. 文章提供了实际的 floor plan 和感知设备的安装位置，以帮助研究人员和实践者创建可靠的建筑性能模型。文章还介绍了数据收集和验证的技术，以及可以用于提高建筑能效消耗、occupant behavior、预测维护和其他相关目的的模型。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Integrated-Communication-Sensing-and-Computation-Framework-for-6G-Networks"><a href="#Integrated-Communication-Sensing-and-Computation-Framework-for-6G-Networks" class="headerlink" title="Integrated Communication, Sensing, and Computation Framework for 6G Networks"></a>Integrated Communication, Sensing, and Computation Framework for 6G Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03265">http://arxiv.org/abs/2310.03265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Chen, Zhiyong Feng, J. Andrew Zhang, Zhaohui Yang, Xin Yuan, Xinxin He, Ping Zhang</li>
<li>for: 这篇论文旨在提出一个集成通信、感知、计算（ICSAC）框架，以实现 sixth generation（6G）时代智能机器网络（IMN）应用程序中的协同机器的可靠性、响应速度、感知信息的准确性和时效性、计算的隐私和安全性等方面的优化。</li>
<li>methods: 本文提出的方法包括将感知和通信功能集成到同一个平台上，使用同一个发射信号，并将实时感知信息作为智能算法的增强因素来提高通信网络的性能。此外，还包括通过网络感知能力的协助来提高分布计算的性能，并且这些协同关系可以相互强化并最终实现ICSAC框架。</li>
<li>results: 本文提出的ICSAC框架可以提高IMN应用程序的可靠性、响应速度、感知信息的准确性和时效性、计算的隐私和安全性等方面的性能。同时，通过对关键实现技术的评估结果，表明ICSAC框架的可行性。<details>
<summary>Abstract</summary>
In the sixth generation (6G) era, intelligent machine network (IMN) applications, such as intelligent transportation, require collaborative machines with communication, sensing, and computation (CSC) capabilities. This article proposes an integrated communication, sensing, and computation (ICSAC) framework for 6G to achieve the reciprocity among CSC functions to enhance the reliability and latency of communication, accuracy and timeliness of sensing information acquisition, and privacy and security of computing to realize the IMN applications. Specifically, the sensing and communication functions can merge into unified platforms using the same transmit signals, and the acquired real-time sensing information can be exploited as prior information for intelligent algorithms to enhance the performance of communication networks. This is called the computing-empowered integrated sensing and communications (ISAC) reciprocity. Such reciprocity can further improve the performance of distributed computation with the assistance of networked sensing capability, which is named the sensing-empowered integrated communications and computation (ICAC) reciprocity. The above ISAC and ICAC reciprocities can enhance each other iteratively and finally lead to the ICSAC reciprocity. To achieve these reciprocities, we explore the potential enabling technologies for the ICSAC framework. Finally, we present the evaluation results of crucial enabling technologies to show the feasibility of the ICSAC framework.
</details>
<details>
<summary>摘要</summary>
在六代（6G）时期，智能机器网络（IMN）应用需要合作机器具有通信、感知、计算（CSC）能力。本文提出了一个集成通信、感知、计算（ICSAC）框架，以实现CSC功能之间的互相关联，提高通信的可靠性和延迟、感知信息获取的准确性和时间性、计算的隐私和安全性，实现IMN应用。具体来说，感知和通信功能可以合并到同一个平台上，使用同一个传输信号，并利用实时感知信息为智能算法提供优化通信网络的优先信息。这被称为计算力 Empowered 集成感知通信（ISAC）互相关联。此互相关联可以进一步提高分布计算的性能，采用网络感知能力的协助，并被称为感知力 Empowered 集成通信计算（ICAC）互相关联。上述 ISAC 和 ICAC 互相关联可以相互强化，最终导致ICSAC 框架。为实现这些互相关联，我们探讨了ICSAC 框架的可能的实现技术。最后，我们展示了关键实现技术的评估结果，以证明ICSAC 框架的可行性。
</details></li>
</ul>
<hr>
<h2 id="Matrix-Completion-from-One-Bit-Dither-Samples"><a href="#Matrix-Completion-from-One-Bit-Dither-Samples" class="headerlink" title="Matrix Completion from One-Bit Dither Samples"></a>Matrix Completion from One-Bit Dither Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03224">http://arxiv.org/abs/2310.03224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arian Eamaz, Farhang Yeganegi, Mojtaba Soltanalian</li>
<li>for: 这个论文是为了研究准确地完成一个低维度矩阵，只有一些一比特样本的情况下。</li>
<li>methods: 该论文使用的方法是将一个低维度矩阵的部分元素粗略地quantized，然后使用核函数降低方法来完成矩阵。</li>
<li>results: 研究发现，如果使用多个时间变化的抽象阈值序列，可以大大提高矩阵完成算法的性能。此外，该论文还提出了三种变体的OB-SVT算法，其中一种使用随机抽取的数据来减少运算空间的维度，从而加速了融合。<details>
<summary>Abstract</summary>
We explore the impact of coarse quantization on matrix completion in the extreme scenario of dithered one-bit sensing, where the matrix entries are compared with time-varying threshold levels. In particular, instead of observing a subset of high-resolution entries of a low-rank matrix, we have access to a small number of one-bit samples, generated as a result of these comparisons. In order to recover the low-rank matrix using its coarsely quantized known entries, we begin by transforming the problem of one-bit matrix completion (one-bit MC) with time-varying thresholds into a nuclear norm minimization problem. The one-bit sampled information is represented as linear inequality feasibility constraints. We then develop the popular singular value thresholding (SVT) algorithm to accommodate these inequality constraints, resulting in the creation of the One-Bit SVT (OB-SVT). Our findings demonstrate that incorporating multiple time-varying sampling threshold sequences in one-bit MC can significantly improve the performance of the matrix completion algorithm. In pursuit of achieving this objective, we utilize diverse thresholding schemes, namely uniform, Gaussian, and discrete thresholds. To accelerate the convergence of our proposed algorithm, we introduce three variants of the OB-SVT algorithm. Among these variants is the randomized sketched OB-SVT, which departs from using the entire information at each iteration, opting instead to utilize sketched data. This approach effectively reduces the dimension of the operational space and accelerates the convergence. We perform numerical evaluations comparing our proposed algorithm with the maximum likelihood estimation method previously employed for one-bit MC, and demonstrate that our approach can achieve a better recovery performance.
</details>
<details>
<summary>摘要</summary>
我们研究了粗糙量化对矩阵完成问题的影响，在极端情况下，当矩阵元素与时间变化的阈值进行比较。具体来说，我们不是直接观察矩阵中一小部分高分辨率的元素，而是因为这些比较而获得的一小部分一位数据。为了使用粗糙量化知道的矩阵元素来重建低维矩阵，我们将一位矩阵完成问题（one-bit MC）变换为核心 нор 最小化问题。一位数据被探测为矩阵元素的一位数据，则被表示为矩阵元素的线性不等制约。我们运用广泛的价值阈值分布（uniform、Gaussian、精确阈值），以提高matrix completion的性能。为了加速我们的提案的整合速度，我们导入三种OB-SVT的变体。其中一种是随机当地OB-SVT，它在每个迭代过程中使用简测数据，而不是使用完整的信息。这种方法可以将操作空间的维度降低，并加速整合速度。我们通过与之前用于一位MC的最大概率估计法进行比较，并证明了我们的方法可以实现更好的重建性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/05/eess.SP_2023_10_05/" data-id="clp88dc7n01fbob8814e03cii" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/33/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="page-number" href="/page/33/">33</a><span class="page-number current">34</span><a class="page-number" href="/page/35/">35</a><a class="page-number" href="/page/36/">36</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/35/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
