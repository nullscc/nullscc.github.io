
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/34/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.SP_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/eess.SP_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T08:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/eess.SP_2023_09_18/">eess.SP - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ROAR-Fed-RIS-Assisted-Over-the-Air-Adaptive-Resource-Allocation-for-Federated-Learning"><a href="#ROAR-Fed-RIS-Assisted-Over-the-Air-Adaptive-Resource-Allocation-for-Federated-Learning" class="headerlink" title="ROAR-Fed: RIS-Assisted Over-the-Air Adaptive Resource Allocation for Federated Learning"></a>ROAR-Fed: RIS-Assisted Over-the-Air Adaptive Resource Allocation for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09883">http://arxiv.org/abs/2309.09883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayu Mao, Aylin Yener</li>
<li>for: 这 paper 旨在探讨基于无线通信的 Federated Learning (FL) 方法，并利用 Reconfigurable Intelligent Surface (RIS) 提高学习性能。</li>
<li>methods: 该 paper 使用 Cross-layer 视角，对客户端数据集分布和个体资源进行协调优化，并采用 RIS 帮助实现更好的学习环境。</li>
<li>results: 数值结果表明，在非 i.i.d. 数据和不完全 CSI 下，ROAR-Fed 算法可以 convergent，并且在不同客户端资源和数据分布情况下显示出优异性。<details>
<summary>Abstract</summary>
Over-the-air federated learning (OTA-FL) integrates communication and model aggregation by exploiting the innate superposition property of wireless channels. The approach renders bandwidth efficient learning, but requires care in handling the wireless physical layer impairments. In this paper, federated edge learning is considered for a network that is heterogeneous with respect to client (edge node) data set distributions and individual client resources, under a general non-convex learning objective. We augment the wireless OTA-FL system with a Reconfigurable Intelligent Surface (RIS) to enable a propagation environment with improved learning performance in a realistic time varying physical layer. Our approach is a cross-layer perspective that jointly optimizes communication, computation and learning resources, in this general heterogeneous setting. We adapt the local computation steps and transmission power of the clients in conjunction with the RIS phase shifts. The resulting joint communication and learning algorithm, RIS-assisted Over-the-air Adaptive Resource Allocation for Federated learning (ROAR-Fed) is shown to be convergent in this general setting. Numerical results demonstrate the effectiveness of ROAR-Fed under heterogeneous (non i.i.d.) data and imperfect CSI, indicating the advantage of RIS assisted learning in this general set up.
</details>
<details>
<summary>摘要</summary>
随空 Federated Learning (OTA-FL) 利用无线通信频率层的自然superposition 特性来实现带宽效率的学习。然而，需要在无线物理层缺陷处理方面予以关注。在本文中，我们考虑了Edge federated learning 在具有不同客户（边缘节点）数据集分布和个体客户资源的网络上。我们在一般非对称学习目标下对无线OTA-FL系统进行扩展，并添加了智能表面（RIS）以实现改进的物理层媒体。我们采用了跨层视角，同时优化通信、计算和学习资源。在这种一般不同设置下，我们适应性地调整客户端的计算步骤和传输功率，并与RIS相关的相位Shift。得到的联合通信和学习算法，即RIS协助的Over-the-air Adaptive Resource Allocation for Federated learning（ROAR-Fed），在这种一般设置下被证明是收敛的。数据示ROAR-Fed在不同（非i.i.d.）数据和不完美CSI下的效果，表明RIS协助学习在这种一般设置下具有优势。
</details></li>
</ul>
<hr>
<h2 id="RIS-Assisted-Energy-Harvesting-Gains-for-Bistatic-Backscatter-Networks-Performance-Analysis-and-RIS-Phase-Optimization"><a href="#RIS-Assisted-Energy-Harvesting-Gains-for-Bistatic-Backscatter-Networks-Performance-Analysis-and-RIS-Phase-Optimization" class="headerlink" title="RIS-Assisted Energy Harvesting Gains for Bistatic Backscatter Networks: Performance Analysis and RIS Phase Optimization"></a>RIS-Assisted Energy Harvesting Gains for Bistatic Backscatter Networks: Performance Analysis and RIS Phase Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09859">http://arxiv.org/abs/2309.09859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diluka Galappaththige, Fatemeh Rezaei, Chintha Tellambura, Sanjeewa Herath<br>for:  This paper aims to improve the energy autonomy of inexpensive tags in IoT networks by deploying a reconfigurable intelligent surface (RIS) to enhance RF power and overcome energy insecurities.methods:  The paper explores the use of RIS to improve the energy harvesting (EH) capabilities of tags, and analyzes the impact of linear and non-linear EH models on single-tag and multi-tag scenarios. The paper also derives key metrics such as received power, harvested power, achievable rate, outage probability, bit error rate, and diversity order.results:  The paper shows significant improvements in activation distance, received power, and achievable rate compared to benchmarks without RIS or with random RIS-phase design. For instance, the optimal design with a \num{200}-element RIS increases the activation distance by \qty{270}{\percent} and \qty{55}{\percent} compared to the benchmarks. The paper demonstrates that RIS deployment improves the energy autonomy of tags while maintaining the basic tag design intact.<details>
<summary>Abstract</summary>
Inexpensive tags powered by energy harvesting (EH) can realize green (energy-efficient) Internet of Things (IoT) networks. However, tags are vulnerable to energy insecurities, resulting in poor communication ranges, activation distances, and data rates. To overcome these challenges, we explore the use of a reconfigurable intelligent surface (RIS) for EH-based IoT networks. The RIS is deployed to enhance RF power at the tag, improving EH capabilities. We consider linear and non-linear EH models and analyze single-tag and multi-tag scenarios. For single-tag networks, the tag's maximum received power and the reader's signal-to-noise ratio with the optimized RIS phase-shifts are derived. Key metrics, such as received power, harvested power, achievable rate, outage probability, bit error rate, and diversity order, are also evaluated. The impact of RIS phase shift quantization errors is also studied. For the multi-tag case, an algorithm to compute the optimal RIS phase-shifts is developed. Numerical results and simulations demonstrate significant improvements compared to the benchmarks of no-RIS case and random RIS-phase design. For instance, our optimal design with a \num{200}-element RIS increases the activation distance by \qty{270}{\percent} and \qty{55}{\percent} compared to those benchmarks. In summary, RIS deployment improves the energy autonomy of tags while maintaining the basic tag design intact.
</details>
<details>
<summary>摘要</summary>
低成本的标签（标签）由能量收集（EH）能够实现绿色（能量高效）互联网络（IoT）。然而，标签受到能源不安全性的挑战，导致通信范围、活动距离和数据速率受到影响。为了解决这些挑战，我们研究使用可重新配置的智能表面（RIS）来增强标签上的RF能量，提高EH能力。我们考虑了线性和非线性EH模型，对单标签和多标签场景进行分析。在单标签网络中，我们计算了标签上最大接收功率和读取器与优化RIS相位偏移后的信号强度比。我们还评估了关键指标，如接收功率、收集功率、可达速率、失业概率、比特错误率和多样度。我们还研究了RIS相位偏移量的量化误差的影响。在多标签场景中，我们开发了计算优化RIS相位偏移的算法。numericalResults和 simulations表明，我们的优化设计可以与无RIS场景和随机RIS相位设计相比，提高标签的活动距离和数据速率。例如，我们的优化设计使用200个RIS元素可以提高标签的活动距离270%和55%。总之，RIS部署可以提高标签的能源自主性，而不需要修改标签的基本设计。
</details></li>
</ul>
<hr>
<h2 id="A-Read-Margin-Enhancement-Circuit-with-Dynamic-Bias-Optimization-for-MRAM"><a href="#A-Read-Margin-Enhancement-Circuit-with-Dynamic-Bias-Optimization-for-MRAM" class="headerlink" title="A Read Margin Enhancement Circuit with Dynamic Bias Optimization for MRAM"></a>A Read Margin Enhancement Circuit with Dynamic Bias Optimization for MRAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09797">http://arxiv.org/abs/2309.09797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renhe Chen, Albert Lee, Zirui Wang, Di Wu, Xufeng Kou</li>
<li>for: 提高Magnetic Random Access Memory（MRAM）的读出效率</li>
<li>methods: 使用动态偏好优化（DBO）电路实现实时跟踪优化读偏好电压，以适应过程Voltage-Temperature（PVT）变化</li>
<li>results: 对28奈米1Mb MRAM宽 macro进行了模拟研究，结果显示DBO电路能够在PVT变化下保持优化读偏好电压的跟踪精度高于90%，并且能够降低读出错误率，提高MRAM性能和可靠性。<details>
<summary>Abstract</summary>
This brief introduces a read bias circuit to improve readout yield of magnetic random access memories (MRAMs). A dynamic bias optimization (DBO) circuit is proposed to enable the real-time tracking of the optimal read voltage across processvoltage-temperature (PVT) variations within an MRAM array. It optimizes read performance by adjusting the read bias voltage dynamically for maximum sensing margin. Simulation results on a 28-nm 1Mb MRAM macro show that the tracking accuracy of the proposed DBO circuit remains above 90% even when the optimal sensing voltage varies up to 50%. Such dynamic tracking strategy further results in up to two orders of magnitude reduction in the bit error rate with respect to different variations, highlighting its effectiveness in enhancing MRAM performance and reliability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Asymptotic-Performance-of-the-GSVD-Based-MIMO-NOMA-Communications-with-Rician-Fading"><a href="#Asymptotic-Performance-of-the-GSVD-Based-MIMO-NOMA-Communications-with-Rician-Fading" class="headerlink" title="Asymptotic Performance of the GSVD-Based MIMO-NOMA Communications with Rician Fading"></a>Asymptotic Performance of the GSVD-Based MIMO-NOMA Communications with Rician Fading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09711">http://arxiv.org/abs/2309.09711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenguang Rao, Zhiguo Ding, Kanapathippillai Cumanan, Xuchu Dai</li>
<li>for: 这篇论文研究了MIMO-NOMA系统中使用GSVD作为前处理算法的性能。</li>
<li>methods: 该论文使用了operator-valued free probability theory中的线性化技巧和决定性方法来分析通道矩阵的普通特征。</li>
<li>results: 该论文提出了一种迭代过程来计算通道矩阵的Cauchy转换，并通过这种方法得到了通信系统的均值数据速率。此外，当通道模型为Rayleigh折射时，也 deriv了closed-form表达式。实验结果 validate了分析结果。<details>
<summary>Abstract</summary>
In recent years, the multiple-input multiple-output (MIMO) non-orthogonal multiple-access (NOMA) systems have attracted a significant interest in the relevant research communities. As a potential precoding scheme, the generalized singular value decomposition (GSVD) can be adopted in MIMO-NOMA systems and has been proved to have high spectral efficiency. In this paper, the performance of the GSVD-based MIMO-NOMA communications with Rician fading is studied. In particular, the distribution characteristics of generalized singular values (GSVs) of channel matrices are analyzed. Two novel mathematical tools, the linearization trick and the deterministic equivalent method, which are based on operator-valued free probability theory, are exploited to derive the Cauchy transform of GSVs. An iterative process is proposed to obtain the numerical values of the Cauchy transform of GSVs, which can be exploited to derive the average data rates of the communication system. In addition, the special case when the channel is modeled as Rayleigh fading, i.e., the line-of-sight propagation is trivial, is analyzed. In this case, the closed-form expressions of average rates are derived from the proposed iterative process. Simulation results are provided to validate the derived analytical results.
</details>
<details>
<summary>摘要</summary>
在最近几年，多输入多输出（MIMO）不对称多访问（NOMA）系统已经吸引了研究领域的广泛关注。作为MIMO-NOMA系统的可能的预编码方案，泛化协方差矩阵（GSVD）可以应用于MIMO-NOMA系统，并且已经证明具有高spectral efficiency。本文研究了GSVD基于MIMO-NOMA通信系统中的 ricain fading 的性能。特别是分析了通道矩阵的泛化协方差值（GSV）的分布特征。基于 оператор值自由概率论的两种新的数学工具——线性化技巧和deterministic equivalent方法——被利用来 derivethe Cauchy transform of GSVs。一种迭代过程被提出来获取GSVs的Cauchy transform的数值，可以用来计算通信系统的平均数据率。此外，当通道模型为Rayleigh fading，即线性视程是rivial的特殊情况也被分析。在这种情况下，通过提posed迭代过程，得到了Closed-form表达式的平均率。Simulation results are provided to validate the derived analytical results.
</details></li>
</ul>
<hr>
<h2 id="Uplink-Power-Control-for-Distributed-Massive-MIMO-with-1-Bit-ADCs"><a href="#Uplink-Power-Control-for-Distributed-Massive-MIMO-with-1-Bit-ADCs" class="headerlink" title="Uplink Power Control for Distributed Massive MIMO with 1-Bit ADCs"></a>Uplink Power Control for Distributed Massive MIMO with 1-Bit ADCs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09665">http://arxiv.org/abs/2309.09665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bikshapathi Gouda, Italo Atzeni, Antti Tölli</li>
<li>For: 这篇论文研究了分布式巨量多输入多输出系统中基站（BS）装备1比特数字转换器（ADC）的上行功率控制问题。* Methods: 该论文使用了1比特ADC的信号干扰分析和优化策略，并提出了三种算法来优化UE传输功率和BS中的扰干。* Results: 研究发现，在多BS情况下，通过合适地调整BS中的扰干，可以使SNDR在输出 JOIN combiner中变为单modal函数，从而使UE可以有效地被多个BS服务。此外，在多UE情况下，该论文提出了最优化UE传输功率和扰干的算法，以达到最小功率和最大最小SINDR的目标。<details>
<summary>Abstract</summary>
We consider the problem of uplink power control for distributed massive multiple-input multiple-output systems where the base stations (BSs) are equipped with 1-bit analog-to-digital converters (ADCs). The scenario with a single-user equipment (UE) is first considered to provide insights into the signal-tonoise-and-distortion ratio (SNDR). With a single BS, the SNDR is a unimodal function of the UE transmit power. With multiple BSs, the SNDR at the output of the joint combiner can be made unimodal by adding properly tuned dithering at each BS. As a result, the UE can be effectively served by multiple BSs with 1-bit ADCs. Considering the signal-to-interference-plus-noise-anddistortion ratio (SINDR) in the multi-UE scenario, we aim at optimizing the UE transmit powers and the dithering at each BS based on the min-power and max-min-SINDR criteria. To this end, we propose three algorithms with different convergence and complexity properties. Numerical results show that, if the desired SINDR can only be achieved via joint combining across multiple BSs with properly tuned dithering, the optimal UE transmit power is imposed by the distance to the farthest serving BS (unlike in the unquantized case). In this context, dithering plays a crucial role in enhancing the SINDR, especially for UEs with significant path loss disparity among the serving BSs.
</details>
<details>
<summary>摘要</summary>
我们考虑了分布式巨量多输入多输出系统中的上行电力控制问题，其中基站（BS）具有1比特分析类比器（ADC）。我们首先考虑了单用户设备（UE）的情况，以获得对SNDR的深入理解。单一BS情况下，SNDR是单一函数UE传输电力。多个BS情况下，通过对每个BS添加适当的测量偏差，则SNDR在多BS联合器出口可以变成单一函数。因此，UE可以从多个BS中有效地获得服务，并且使用1比特ADC。对于多UE情况下的SINDR，我们寻找了对UE传输电力和BS中的测量偏差进行优化，以达到最小电力和最大最小SINDR的目标。为此，我们提出了三种不同的算法，每个算法具有不同的参数和复杂度。实验结果显示，如果只能通过多BS联合器实现所需的SINDR，则UE传输电力将被最远的服务BS的距离所限制（不同于不量化情况下）。在这种情况下，测量偏差在增强SINDR方面扮演至关重要的角色，特别是UE对服务BS的距离差异较大。
</details></li>
</ul>
<hr>
<h2 id="Turbo-Coded-OFDM-OQAM-Using-Hilbert-Transform"><a href="#Turbo-Coded-OFDM-OQAM-Using-Hilbert-Transform" class="headerlink" title="Turbo Coded OFDM-OQAM Using Hilbert Transform"></a>Turbo Coded OFDM-OQAM Using Hilbert Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09620">http://arxiv.org/abs/2309.09620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kasturi Vasudevan, Surendra Kota, Lov Kumar, Himanshu Bhusan Mishra</li>
<li>for: 本文探讨了使用哈尔杯变换生成 orthogonal frequency division multiplexing（OFDM）与偏角幂强调幂（OQAM）的方法，并证明了这种方法与单边幂调幂（SSB）相等。</li>
<li>methods: 本文使用了哈尔杯变换来生成OFDM-OQAM，并使用了复数valued transmit filter来实现。 receiver使用 matched filter 来回填消息。</li>
<li>results: 在 discrete time 中 simulated 系统，使用 turbo coding 可以实现 bit-error-rate（BER）为 $10^{-5}$， average signal-to-noise ratio（SNR）为每比特接近 0 db。<details>
<summary>Abstract</summary>
Orthogonal frequency division multiplexing (OFDM) with offset quadrature amplitude modulation (OQAM) has been widely discussed in the literature and is considered a popular waveform for 5th generation (5G) wireless telecommunications and beyond. In this work, we show that OFDM-OQAM can be generated using the Hilbert transform and is equivalent to single sideband modulation (SSB), that has roots in analog telecommunications. The transmit filter for OFDM-OQAM is complex valued whose real part is given by the pulse corresponding to the root raised cosine spectrum and the imaginary part is the Hilbert transform of the real part. The real-valued digital information (message) are passed through the transmit filter and frequency division multiplexed on orthogonal subcarriers. The message bandwidth corresponding to each subcarrier is assumed to be narrow enough so that the channel can be considered ideal. Therefore, at the receiver, a matched filter can used to recover the message. Turbo coding is used to achieve bit-error-rate (BER) as low as $10^{-5}$ at an average signal-to-noise ratio (SNR) per bit close to 0 db. The system has been simulated in discrete time.
</details>
<details>
<summary>摘要</summary>
Orthogonal frequency division multiplexing (OFDM) with offset quadrature amplitude modulation (OQAM) 已经在文献中广泛讨论，被视为5代移动通信和更进一步的各种应用的流行的波形。在这个工作中，我们展示了OFDM-OQAM可以使用希尔伯特变换生成，与单边干扰模式（SSB）相当，这种模式在分析通信中有深根。发送器的滤波器为复数值，其实部分是根提高cosinuspectrum的脉冲，幂部分是希尔伯特变换实部。接收器使用匹配滤波器来回填消息。在接收器中，使用隐藏编码来实现 bit-error-rate（BER）在10^-5以下，均值信号响应（SNR）每比特接近0 db。系统在离散时间下进行了模拟。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Unscented-Kalman-Filter-under-Minimum-Error-Entropy-with-Fiducial-Points-for-Non-Gaussian-Systems"><a href="#Adaptive-Unscented-Kalman-Filter-under-Minimum-Error-Entropy-with-Fiducial-Points-for-Non-Gaussian-Systems" class="headerlink" title="Adaptive Unscented Kalman Filter under Minimum Error Entropy with Fiducial Points for Non-Gaussian Systems"></a>Adaptive Unscented Kalman Filter under Minimum Error Entropy with Fiducial Points for Non-Gaussian Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09577">http://arxiv.org/abs/2309.09577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyu Tian, Haiquan Zhao</li>
<li>for: 提高非泛解决方案中的噪声和异常测量数据处理能力</li>
<li>methods: 基于最小错误 entropy with fiducial points (MEEF) 提出一种新的 UKF 算法，并通过添加 correntropy 进一步增强噪声和异常测量数据的抑制能力</li>
<li>results: 通过实验示例，表明提出的算法在面临复杂非泛解决方案中的噪声和异常测量数据处理中具有良好的稳定性和估计精度<details>
<summary>Abstract</summary>
The minimum error entropy (MEE) has been extensively used in unscented Kalman filter (UKF) to handle impulsive noises or abnormal measurement data in non-Gaussian systems. However, the MEE-UKF has poor numerical stability due to the inverse operation of singular matrix. In this paper, a novel UKF based on minimum error entropy with fiducial points (MEEF) is proposed \textcolor{black}{to improve the problem of non-positive definite key matrix. By adding the correntropy to the error entropy, the proposed algorithm further enhances the ability of suppressing impulse noise and outliers. At the same time, considering the uncertainty of noise distribution, the modified Sage-Husa estimator of noise statistics is introduced to adaptively update the noise covariance matrix. In addition, the convergence analysis of the proposed algorithm provides a guidance for the selection of kernel width. The robustness and estimation accuracy of the proposed algorithm are manifested by the state tracking examples under complex non-Gaussian noises.
</details>
<details>
<summary>摘要</summary>
“非标准几何系统中的访问频率（MEE）已经广泛使用在非标准 Kalman统计（UKF）中，以处理快速变化的数据或异常测量变量。但MEE-UKF具有对对称矩降的逆操作，导致严重的数值稳定问题。本文提出了一种基于MEE的新型UKF（MEEF），以解决这个问题。通过将correnticropy添加到错误熵中，提高了对于快速变化和偏差测量的抑制能力。同时，透过考虑随机变量的不确定性，引入修改的Sage-Husa估计器来适应更新随机矩降矩。此外，提出了修改了kernel宽度的选择指南，以确保算法的稳定性和准确性。实际应用中，提出的算法在复杂的非标准几何系统中进行了稳定追踪。”
</details></li>
</ul>
<hr>
<h2 id="AI-Native-Transceiver-Design-for-Near-Field-Ultra-Massive-MIMO-Principles-and-Techniques"><a href="#AI-Native-Transceiver-Design-for-Near-Field-Ultra-Massive-MIMO-Principles-and-Techniques" class="headerlink" title="AI-Native Transceiver Design for Near-Field Ultra-Massive MIMO: Principles and Techniques"></a>AI-Native Transceiver Design for Near-Field Ultra-Massive MIMO: Principles and Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09575">http://arxiv.org/abs/2309.09575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Yu, Yifan Ma, Hengtao He, Shenghui Song, Jun Zhang, Khaled B. Letaief</li>
<li>for: 这篇论文旨在探讨近场多输入多输出（UM-MIMO）技术的algorithmic设计，以提高无线网络的spectral和能量效率。</li>
<li>methods: 论文提出了两种基于人工智能（AI）的框架，用于设计近场UM-MIMO传输器。这两种框架分别是迭代式和非迭代式框架。</li>
<li>results: 文章通过使用这两种框架，实现了近场UM-MIMO传输器的near-field beam focusing和频率域预测等功能，并且在多个关键性能指标上达到了显著的优势。<details>
<summary>Abstract</summary>
Ultra-massive multiple-input multiple-output (UM-MIMO) is a cutting-edge technology that promises to revolutionize wireless networks by providing an unprecedentedly high spectral and energy efficiency. The enlarged array aperture of UM-MIMO facilitates the accessibility of the near-field region, thereby offering a novel degree of freedom for communications and sensing. Nevertheless, the transceiver design for such systems is challenging because of the enormous system scale, the complicated channel characteristics, and the uncertainties in propagation environments. Therefore, it is critical to study scalable, low-complexity, and robust algorithms that can efficiently characterize and leverage the properties of the near-field channel. In this article, we will advocate two general frameworks from an artificial intelligence (AI)-native perspective, which are tailored for the algorithmic design of near-field UM-MIMO transceivers. Specifically, the frameworks for both iterative and non-iterative algorithms are discussed. Near-field beam focusing and channel estimation are presented as two tutorial-style examples to demonstrate the significant advantages of the proposed AI-native frameworks in terms of various key performance indicators.
</details>
<details>
<summary>摘要</summary>
ultra-massive多输入多输出（UM-MIMO）是一种最前线的技术，可以带来无 precedent的 spectral和能量效率。 UM-MIMO 的扩大数组天线可以访问近场区域，从而提供一种新的通信和感知的自由度。然而，UM-MIMO 的接收机设计具有巨大的系统规模，复杂的通道特性和传播环境的不确定性，因此需要研究可扩展、低复杂度和Robust的算法，以有效地描述和利用近场通道的特性。在这篇文章中，我们将提出两个总体框架，从人工智能（AI）原生的角度出发，用于near-field UM-MIMO 接收机的算法设计。具体来说，我们将讨论iterative和非iterative算法的框架。 near-field  beam focusing和通道估计被用作两个教程风格的示例，以 Illustrate了我们提出的 AI-native 框架在不同的关键性能指标方面的显著优势。
</details></li>
</ul>
<hr>
<h2 id="A-Covariance-Adaptive-Student’s-t-Based-Kalman-Filter"><a href="#A-Covariance-Adaptive-Student’s-t-Based-Kalman-Filter" class="headerlink" title="A Covariance Adaptive Student’s t Based Kalman Filter"></a>A Covariance Adaptive Student’s t Based Kalman Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09565">http://arxiv.org/abs/2309.09565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benyang Gong, Jiacheng He, Gang Wang, Bei Peng</li>
<li>for: 本文为了提高Student’s t基于 kalman filter（TKF）的精度和稳定性，采用 Gaussian mixture model（GMM）来更正TKF中对测量噪声的covariance矩阵，从而超过了TKF的调整限制。</li>
<li>methods: 本文使用GMM来生成测量噪声的covariance矩阵，并将其用于TKF中更正测量噪声的confidence水平。</li>
<li>results: 本文的实验结果表明，使用GMM来更正TKF中测量噪声的covariance矩阵，可以提高TKF的精度和稳定性，并超过了传统的TKF。<details>
<summary>Abstract</summary>
In the classical Kalman filter(KF), the estimated state is a linear combination of the one-step predicted state and measurement state, their confidence level change when the prediction mean square error matrix and covariance matrix of measurement noise vary. The existing student's t based Kalman filter(TKF) works similarly to the way KF works, they both work well with impulse noise, but when it comes to Gaussian noise, TKF encounters an adjustment limit of the confidence level, this can lead to inaccuracies in such situations. This brief optimizes TKF by using the Gaussian mixture model(GMM), which generates a reasonable covariance matrix from the measurement noise to replace the one used in the existing algorithm and breaks the adjustment limit of the confidence level. At the end of the brief, the performance of the covariance adaptive student's t based Kalman filter(TGKF) is verified.
</details>
<details>
<summary>摘要</summary>
经 classical Kalman filter（KF），估计状态是一个线性组合于一步预测状态和测量状态，其信度水平随预测均值方差矩阵和测量噪声均值矩阵的变化而变化。现有的学生t基于Kalman filter（TKF）与KF相似，它们都能够处理冲击噪声，但在 Gaussian 噪声时，TKF会遇到信度水平调整限制，这可能导致准确性下降。本文优化TKF使用 Gaussian mixture model（GMM）生成测量噪声的合理协方差矩阵，并让信度水平不再受限制。文章结尾，covariance adaptive student's t based Kalman filter（TGKF）的性能得到了验证。
</details></li>
</ul>
<hr>
<h2 id="Multi-user-beamforming-in-RIS-aided-communications-and-experimental-validations"><a href="#Multi-user-beamforming-in-RIS-aided-communications-and-experimental-validations" class="headerlink" title="Multi-user beamforming in RIS-aided communications and experimental validations"></a>Multi-user beamforming in RIS-aided communications and experimental validations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09460">http://arxiv.org/abs/2309.09460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhibo Zhou, Haifan Yin, Li Tan, Ruikun Zhang, Kai Wang, Yingzhuang Liu</li>
<li>for: 这篇论文目的是提出一种基于智能表面的多用户通信系统，以提高 Spectral Efficiency。</li>
<li>methods: 论文使用了频率积分和优化矩阵来实现通信系统中的多用户耦合。</li>
<li>results: 实验结果表明，在使用智能表面的情况下，系统的 Spectral Efficiency 提高了 13.48bps&#x2F;Hz，对应的接收功率提高了 26.6dB和17.5dB。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) is a promising technology for future wireless communications due to its capability of optimizing the propagation environments. Nevertheless, in literature, there are few prototypes serving multiple users. In this paper, we propose a whole flow of channel estimation and beamforming design for RIS, and set up an RIS-aided multi-user system for experimental validations. Specifically, we combine a channel sparsification step with generalized approximate message passing (GAMP) algorithm, and propose to generate the measurement matrix as Rademacher distribution to obtain the channel state information (CSI). To generate the reflection coefficients with the aim of maximizing the spectral efficiency, we propose a quadratic transform-based low-rank multi-user beamforming (QTLM) algorithm. Our proposed algorithms exploit the sparsity and low-rank properties of the channel, which has the advantages of light calculation and fast convergence. Based on the universal software radio peripheral devices, we built a complete testbed working at 5.8GHz and implemented all the proposed algorithms to verify the possibility of RIS assisting multi-user systems. Experimental results show that the system has obtained an average spectral efficiency increase of 13.48bps/Hz, with respective received power gains of 26.6dB and 17.5dB for two users, compared with the case when RIS is powered-off.
</details>
<details>
<summary>摘要</summary>
《可重配置智能表面（RIS）是未来无线通信技术的前景之一，因其可以优化传播环境。然而，在文献中，有很少的多用户体系。在这篇论文中，我们提出了渠道估计和扬射设计的整体流程，并设置了基于RIS的多用户系统 для实验验证。具体来说，我们将渠道简化步骤与通用简化消息传递算法（GAMP）相结合，并提议使用拉德美纳分布生成测量矩阵来获取渠道状态信息（CSI）。为了通过扬射矩阵来最大化спектル效率，我们提议使用quadratic transform-based low-rank multi-user beamforming（QTLM）算法。我们的提议算法利用渠道的稀疏和低级特性，具有轻量级计算和快速收敛的优点。基于通用软件无线通信外围设备，我们建立了工作在5.8GHz频率上的完整测试床，并实现了所有的提议算法，以验证RIS助け多用户系统的可能性。实验结果显示，系统在RIS启用下获得了平均spectral efficiency提高13.48bps/Hz，与RIS灭活时的相比，分别提高了接收功率26.6dB和17.5dB。
</details></li>
</ul>
<hr>
<h2 id="Energy-efficient-Integrated-Sensing-and-Communication-System-and-DNLFM-Waveform"><a href="#Energy-efficient-Integrated-Sensing-and-Communication-System-and-DNLFM-Waveform" class="headerlink" title="Energy-efficient Integrated Sensing and Communication System and DNLFM Waveform"></a>Energy-efficient Integrated Sensing and Communication System and DNLFM Waveform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09415">http://arxiv.org/abs/2309.09415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihua Ma, Zhifeng Yuan, Shuqiang Xia, Chen Bai, Zhongbin Wang, Yuxin Wang</li>
<li>for: 本研究旨在提高Integrated sensing and communication (ISAC)系统的能效性。</li>
<li>methods: 本文提出了针对ISAC系统的专门探测信号，并在探测和通信信号之间进行时频匹配窗口。此外，本文还提出了使用Discrete non-linear frequency modulation (DNLFM)来实现时间频率匹配和自适应窗口重量。</li>
<li>results:  simulations 结果表明，提出的方法可以提高ISAC系统的能效性。<details>
<summary>Abstract</summary>
Integrated sensing and communication (ISAC) is a key enabler of 6G. Unlike communication radio links, the sensing signal requires to experience round trips from many scatters. Therefore, sensing is more power-sensitive and faces a severer multi-target interference. In this paper, the ISAC system employs dedicated sensing signals, which can be reused as the communication reference signal. This paper proposes to add time-frequency matched windows at both the transmitting and receiving sides, which avoids mismatch loss and increases energy efficiency. Discrete non-linear frequency modulation (DNLFM) is further proposed to achieve both time-domain constant modulus and frequency-domain arbitrary windowing weights. DNLFM uses very few Newton iterations and a simple geometrically-equivalent method to generate, which greatly reduces the complex numerical integral in the conventional method. Moreover, the spatial-domain matched window is proposed to achieve low sidelobes. The simulation results show that the proposed methods gain a higher energy efficiency than conventional methods.
</details>
<details>
<summary>摘要</summary>
integration of sensing and communication (ISAC) is a key enabler of 6G. Unlike communication radio links, the sensing signal requires to experience round trips from many scatters. Therefore, sensing is more power-sensitive and faces a severer multi-target interference. In this paper, the ISAC system employs dedicated sensing signals, which can be reused as the communication reference signal. This paper proposes to add time-frequency matched windows at both the transmitting and receiving sides, which avoids mismatch loss and increases energy efficiency. Discrete non-linear frequency modulation (DNLFM) is further proposed to achieve both time-domain constant modulus and frequency-domain arbitrary windowing weights. DNLFM uses very few Newton iterations and a simple geometrically-equivalent method to generate, which greatly reduces the complex numerical integral in the conventional method. Moreover, the spatial-domain matched window is proposed to achieve low sidelobes. The simulation results show that the proposed methods gain a higher energy efficiency than conventional methods.Here's the text in Traditional Chinese if you prefer:integration of sensing and communication (ISAC) is a key enabler of 6G. Unlike communication radio links, the sensing signal requires to experience round trips from many scatters. Therefore, sensing is more power-sensitive and faces a severer multi-target interference. In this paper, the ISAC system employs dedicated sensing signals, which can be reused as the communication reference signal. This paper proposes to add time-frequency matched windows at both the transmitting and receiving sides, which avoids mismatch loss and increases energy efficiency. Discrete non-linear frequency modulation (DNLFM) is further proposed to achieve both time-domain constant modulus and frequency-domain arbitrary windowing weights. DNLFM uses very few Newton iterations and a simple geometrically-equivalent method to generate, which greatly reduces the complex numerical integral in the conventional method. Moreover, the spatial-domain matched window is proposed to achieve low sidelobes. The simulation results show that the proposed methods gain a higher energy efficiency than conventional methods.
</details></li>
</ul>
<hr>
<h2 id="Improving-Axial-Resolution-of-Optical-Resolution-Photoacoustic-Microscopy-with-Advanced-Frequency-Domain-Eigenspace-Based-Minimum-Variance-Beamforming-Method"><a href="#Improving-Axial-Resolution-of-Optical-Resolution-Photoacoustic-Microscopy-with-Advanced-Frequency-Domain-Eigenspace-Based-Minimum-Variance-Beamforming-Method" class="headerlink" title="Improving Axial Resolution of Optical Resolution Photoacoustic Microscopy with Advanced Frequency Domain Eigenspace Based Minimum Variance Beamforming Method"></a>Improving Axial Resolution of Optical Resolution Photoacoustic Microscopy with Advanced Frequency Domain Eigenspace Based Minimum Variance Beamforming Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09409">http://arxiv.org/abs/2309.09409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Hsiang Yu, Meng-Lin Li</li>
<li>for: 高解析力照明探针（OR-PAM）可以利用光学 фокусинг和声学检测，实现微scopic optical absorption imaging。但是，它的光学 lateral resolution 高，而声学轴resolution 低，导致不好的3D可视化，因此通常只会present 2D maximum amplitude projection images。</li>
<li>methods: 以 axial resolution 为问题，我们提出了一种基于Frequency-domain eigenspace-based minimum variance（F-EIBMV）的扫描技术来减少axial sidelobe interference和噪声。这种方法可以同时提高OR-PAM的axial resolution和对比度。</li>
<li>results: 对于25MHz OR-PAM系统，我们发现了axial point spread function的半峰宽度从69.3μm降低到16.89μm，表明了axial resolution的显著提高。<details>
<summary>Abstract</summary>
Optical resolution photoacoustic microscopy (OR-PAM) leverages optical focusing and acoustic detection for microscopic optical absorption imaging. Intrinsically it owns high optical lateral resolution and poor acoustic axial resolution. Such anisometric resolution hinders good 3-D visualization; thus 2-D maximum amplitude projection images are commonly presented in the literature. Since its axial resolution is limited by the bandwidth of acoustic detectors, ultrahigh frequency, and wideband detectors with Wiener deconvolution have been proposed to address this issue. Nonetheless, they also introduce other issues such as severe high-frequency attenuation and limited imaging depth. In this work, we view axial resolution improvement as an axial signal reconstruction problem, and the axial resolution degradation is caused by axial sidelobe interference. We propose an advanced frequency-domain eigenspace-based minimum variance (F-EIBMV) beamforming technique to suppress axial sidelobe interference and noises. This method can simultaneously enhance the axial resolution and contrast of OR-PAM. For a 25-MHz OR-PAM system, the full-width at half-maximum of an axial point spread function decreased significantly from 69.3 $\mu$m to 16.89 $\mu$m, indicating a significant improvement in axial resolution.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/eess.SP_2023_09_18/" data-id="cloimipj00184s48886uk2r9k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.SD_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T15:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.SD_2023_09_17/">cs.SD - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sound-Source-Distance-Estimation-in-Diverse-and-Dynamic-Acoustic-Conditions"><a href="#Sound-Source-Distance-Estimation-in-Diverse-and-Dynamic-Acoustic-Conditions" class="headerlink" title="Sound Source Distance Estimation in Diverse and Dynamic Acoustic Conditions"></a>Sound Source Distance Estimation in Diverse and Dynamic Acoustic Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09288">http://arxiv.org/abs/2309.09288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saksham Singh Kushwaha, Iran R. Roman, Magdalena Fuentes, Juan Pablo Bello</li>
<li>for: 本研究旨在实现在真实世界中移动声源的方向到达（DOA）和距离投机器录音器的判别。</li>
<li>methods: 本研究使用数据驱动的方法，利用大量开源数据集，包括多个环境下的附加麦克风记录，进行优化。</li>
<li>results: 我们提出了一种CRNN模型，可以在多个数据集上表现出色地估计移动声源的距离，超过了一个最近发表的方法。我们还进行了模型性能的函数分析，发现在声源真实距离不同和不同训练损失下，模型的性能存在最佳化点。本研究是首次通过深度学习实现了声源距离估计在多种听音条件下。<details>
<summary>Abstract</summary>
Localizing a moving sound source in the real world involves determining its direction-of-arrival (DOA) and distance relative to a microphone. Advancements in DOA estimation have been facilitated by data-driven methods optimized with large open-source datasets with microphone array recordings in diverse environments. In contrast, estimating a sound source's distance remains understudied. Existing approaches assume recordings by non-coincident microphones to use methods that are susceptible to differences in room reverberation. We present a CRNN able to estimate the distance of moving sound sources across multiple datasets featuring diverse rooms, outperforming a recently-published approach. We also characterize our model's performance as a function of sound source distance and different training losses. This analysis reveals optimal training using a loss that weighs model errors as an inverse function of the sound source true distance. Our study is the first to demonstrate that sound source distance estimation can be performed across diverse acoustic conditions using deep learning.
</details>
<details>
<summary>摘要</summary>
本文描述了一种基于深度学习的Sound Source Distance Estimation（SSDE）模型，可以在多种不同的室内环境中Estimate the distance of moving sound sources。我们使用了大量的开源数据集，并且使用了一种叫做Convolutional Recurrent Neural Network（CRNN）的模型，可以在多个不同的 datasets 中Outperform 已有的方法。我们还进行了一些性能分析，包括模型的训练损失函数的选择，以及模型在不同的室内环境下的性能。我们的研究表明，可以使用深度学习来Estimate the distance of moving sound sources across diverse acoustic conditions。这是一个新的领域，尚未得到过足够的研究。我们的模型可以在多个不同的室内环境下提供高精度的距离估计，并且可以在不同的训练损失函数下进行优化。在本研究中，我们使用了一些不同的训练损失函数，包括损失函数的weighted sum，以及一种叫做“inverse distance”的损失函数。我们的研究表明，使用“inverse distance”损失函数可以提高模型的性能，并且可以在不同的室内环境下提供更高的精度。总之，我们的研究表明，可以使用深度学习来Estimate the distance of moving sound sources across diverse acoustic conditions。我们的模型可以在多个不同的室内环境下提供高精度的距离估计，并且可以在不同的训练损失函数下进行优化。这对于各种应用场景，如智能家居、智能城市等，都具有重要的意义。
</details></li>
</ul>
<hr>
<h2 id="PromptVC-Flexible-Stylistic-Voice-Conversion-in-Latent-Space-Driven-by-Natural-Language-Prompts"><a href="#PromptVC-Flexible-Stylistic-Voice-Conversion-in-Latent-Space-Driven-by-Natural-Language-Prompts" class="headerlink" title="PromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts"></a>PromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09262">http://arxiv.org/abs/2309.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jixun Yao, Yuguang Yang, Yi Lei, Ziqian Ning, Yanni Hu, Yu Pan, Jingjing Yin, Hongbin Zhou, Heng Lu, Lei Xie</li>
<li>for: This paper aims to improve the style voice conversion process by using natural language prompts to generate a style vector and adapting the duration of discrete tokens.</li>
<li>methods: The proposed approach, called PromptVC, employs a latent diffusion model to sample the style vector from noise, with the process being conditioned on natural language prompts. The system also uses HuBERT to extract discrete tokens and replace them with the K-Means center embedding to minimize residual style information.</li>
<li>results: The subjective and objective evaluation results demonstrate the effectiveness of the proposed system, with improved style expressiveness and adaptability to different styles.<details>
<summary>Abstract</summary>
Style voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.
</details>
<details>
<summary>摘要</summary>
《 Style Voice Conversion 》 aims to transform the style of source speech to a desired style based on real-world application demands. However, current methods rely on pre-defined labels or reference speech to control the conversion process, which limits style diversity and lacks intuitive and interpretable style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and the latent diffusion model is trained to sample the style vector from noise, conditioned on natural language prompts. To enhance style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to minimize residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, allowing for adaptive duration adjustment based on different styles. Subjective and objective evaluation results demonstrate the effectiveness of our proposed system.
</details></li>
</ul>
<hr>
<h2 id="Zero-and-Few-shot-Sound-Event-Localization-and-Detection"><a href="#Zero-and-Few-shot-Sound-Event-Localization-and-Detection" class="headerlink" title="Zero- and Few-shot Sound Event Localization and Detection"></a>Zero- and Few-shot Sound Event Localization and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09223">http://arxiv.org/abs/2309.09223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuki Shimada, Kengo Uchida, Yuichiro Koyama, Takashi Shibuya, Shusuke Takahashi, Yuki Mitsufuji, Tatsuya Kawahara</li>
<li>for: 这个论文是为了解决静音定位和检测（SELD）系统中的零或几个批处理任务。</li>
<li>methods: 这个论文使用的方法包括逻辑学习网络（NN）和对准抽取语音样本（CLAP）。</li>
<li>results: 该论文的结果表明，使用 embed-ACCDOA 模型可以在零或几个批处理任务中提高静音定位和检测的性能，并且和完整的训练数据进行比较。<details>
<summary>Abstract</summary>
Sound event localization and detection (SELD) systems estimate direction-of-arrival (DOA) and temporal activation for sets of target classes. Neural network (NN)-based SELD systems have performed well in various sets of target classes, but they only output the DOA and temporal activation of preset classes that are trained before inference. To customize target classes after training, we tackle zero- and few-shot SELD tasks, in which we set new classes with a text sample or a few audio samples. While zero-shot sound classification tasks are achievable by embedding from contrastive language-audio pretraining (CLAP), zero-shot SELD tasks require assigning an activity and a DOA to each embedding, especially in overlapping cases. To tackle the assignment problem in overlapping cases, we propose an embed-ACCDOA model, which is trained to output track-wise CLAP embedding and corresponding activity-coupled Cartesian direction-of-arrival (ACCDOA). In our experimental evaluations on zero- and few-shot SELD tasks, the embed-ACCDOA model showed a better location-dependent scores than a straightforward combination of the CLAP audio encoder and a DOA estimation model. Moreover, the proposed combination of the embed-ACCDOA model and CLAP audio encoder with zero- or few-shot samples performed comparably to an official baseline system trained with complete train data in an evaluation dataset.
</details>
<details>
<summary>摘要</summary>
声音事件 lokalisierung和检测（SELD）系统估算irection-of-arrival（DOA）和时间活动 для集合Target classes。基于神经网络（NN）的SELD系统在不同的Target classes中表现良好，但它们只会在预测前训练的类型上输出DOA和时间活动。为了自定义目标类型 después de training，我们面临着零和几个shot SELD任务，在其中我们可以通过提供文本样本或几个音频样本来设置新的类型。零shot声音分类任务可以通过语音-语言预training（CLAP）的嵌入来实现，但零shot SELD任务需要将每个嵌入分配到活动和DOA，特别是在重叠的情况下。为了解决重叠情况中的分配问题，我们提出了一种嵌入-ACCDOA模型，该模型通过输出track-wise CLAP嵌入和相应的活动-联合Cartesian DOA来解决问题。在我们的实验评估中，embeds-ACCDOA模型在零和几个shot SELD任务中表现出色，其location-dependent scores比直接将CLAP音频编码器和DOA估算模型组合的 scores更高。此外，我们将embeds-ACCDOA模型和CLAP音频编码器与零或几个shot样本组合起来，与完整的训练数据进行评估，结果与官方基eline系统在评估数据集中的表现相当。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.SD_2023_09_17/" data-id="cloimipe600v6s4883ez2gj3y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.CV_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T13:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.CV_2023_09_17/">cs.CV - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-conditional-generative-models-for-longitudinal-single-slice-abdominal-computed-tomography-harmonization"><a href="#Deep-conditional-generative-models-for-longitudinal-single-slice-abdominal-computed-tomography-harmonization" class="headerlink" title="Deep conditional generative models for longitudinal single-slice abdominal computed tomography harmonization"></a>Deep conditional generative models for longitudinal single-slice abdominal computed tomography harmonization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09392">http://arxiv.org/abs/2309.09392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masilab/c-slicegen">https://github.com/masilab/c-slicegen</a></li>
<li>paper_authors: Xin Yu, Qi Yang, Yucheng Tang, Riqiang Gao, Shunxing Bao, Leon Y. Cai, Ho Hin Lee, Yuankai Huo, Ann Zenobia Moore, Luigi Ferrucci, Bennett A. Landman</li>
<li>for: 用于解决对腹部CT成像数据进行长期分析时，因为不同年份获取的扫描 slice 位置不同，导致不同的器官&#x2F;组织被捕捉的问题。</li>
<li>methods: 我们提出了 C-SliceGen 方法，可以将任意的腹部 axial slice 作为输入，并在 latent space 中估算结构变化，以生成预定义的vertebral level slice。</li>
<li>results: 我们的实验表明，C-SliceGen 方法可以生成高质量的图像，具有真实性和相似性。此外，我们还证明了该方法可以减少腹部 CT 数据的 slice 位置差异，并且在1033名参与者的 Baltimore Longitudinal Study of Aging (BLSA) 数据集上进行了评估，并证明了该方法可以减少腹部 slice 的位置差异。<details>
<summary>Abstract</summary>
Two-dimensional single-slice abdominal computed tomography (CT) provides a detailed tissue map with high resolution allowing quantitative characterization of relationships between health conditions and aging. However, longitudinal analysis of body composition changes using these scans is difficult due to positional variation between slices acquired in different years, which leading to different organs/tissues captured. To address this issue, we propose C-SliceGen, which takes an arbitrary axial slice in the abdominal region as a condition and generates a pre-defined vertebral level slice by estimating structural changes in the latent space. Our experiments on 2608 volumetric CT data from two in-house datasets and 50 subjects from the 2015 Multi-Atlas Abdomen Labeling Challenge dataset (BTCV) Challenge demonstrate that our model can generate high-quality images that are realistic and similar. We further evaluate our method's capability to harmonize longitudinal positional variation on 1033 subjects from the Baltimore Longitudinal Study of Aging (BLSA) dataset, which contains longitudinal single abdominal slices, and confirmed that our method can harmonize the slice positional variance in terms of visceral fat area. This approach provides a promising direction for mapping slices from different vertebral levels to a target slice and reducing positional variance for single-slice longitudinal analysis. The source code is available at: https://github.com/MASILab/C-SliceGen.
</details>
<details>
<summary>摘要</summary>
“两维单片腹部计算机断层成像（CT）提供了高分辨率的组织地图，可以量化健康状况和年龄之间的关系。然而，使用这些扫描数据进行 longitudinal 分析的 Body 组成变化困难，因为不同年份扫描时的 slice 位置会有偏移。为解决这个问题，我们提出了 C-SliceGen，它可以将任意腹部 Axial slice 作为输入，并生成预定的vertebral level slice，通过估计 latent space 中的结构变化。我们的实验表明，我们的模型可以生成高质量的图像，具有实际和相似的特征。我们进一步评估了我们的方法在1033名参与者的 Baltimore Longitudinal Study of Aging（BLSA）数据集上的能力，并证明了我们的方法可以减少 slice 位置偏移的方差。这种方法提供了一个可行的方向，可以将不同 vertebral level 的 slice 映射到目标 slice 上，并减少 longitudinal 分析中的 slice 位置偏移。源代码可以在以下链接中获取：https://github.com/MASILab/C-SliceGen。”
</details></li>
</ul>
<hr>
<h2 id="a-critical-analysis-of-internal-reliability-for-uncertainty-quantification-of-dense-image-matching-in-multi-view-stereo"><a href="#a-critical-analysis-of-internal-reliability-for-uncertainty-quantification-of-dense-image-matching-in-multi-view-stereo" class="headerlink" title="a critical analysis of internal reliability for uncertainty quantification of dense image matching in multi-view stereo"></a>a critical analysis of internal reliability for uncertainty quantification of dense image matching in multi-view stereo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09379">http://arxiv.org/abs/2309.09379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debao Huang, Rongjun Qin</li>
<li>for: 该研究用于分析多视角摄影探测数据的内部可靠性，尤其是在无referencedata情况下。</li>
<li>methods: 该研究使用了多视角摄影幂等的内部匹配度量，包括射线聚合统计、交叉角度统计、DIM能量等。</li>
<li>results: 研究发现，使用不同的内部匹配度量可以对多视角摄影探测数据的内部可靠性进行评估，尤其是在LiDAR参照数据不 disponible情况下。<details>
<summary>Abstract</summary>
Nowadays, photogrammetrically derived point clouds are widely used in many civilian applications due to their low cost and flexibility in acquisition. Typically, photogrammetric point clouds are assessed through reference data such as LiDAR point clouds. However, when reference data are not available, the assessment of photogrammetric point clouds may be challenging. Since these point clouds are algorithmically derived, their accuracies and precisions are highly varying with the camera networks, scene complexity, and dense image matching (DIM) algorithms, and there is no standard error metric to determine per-point errors. The theory of internal reliability of camera networks has been well studied through first-order error estimation of Bundle Adjustment (BA), which is used to understand the errors of 3D points assuming known measurement errors. However, the measurement errors of the DIM algorithms are intricate to an extent that every single point may have its error function determined by factors such as pixel intensity, texture entropy, and surface smoothness. Despite the complexity, there exist a few common metrics that may aid the process of estimating the posterior reliability of the derived points, especially in a multi-view stereo (MVS) setup when redundancies are present. In this paper, by using an aerial oblique photogrammetric block with LiDAR reference data, we analyze several internal matching metrics within a common MVS framework, including statistics in ray convergence, intersection angles, DIM energy, etc.
</details>
<details>
<summary>摘要</summary>
现在，由光ogrammetry derive的点云在多种民用应用中广泛使用，主要因为它们的成本低廉和捕捉方式灵活。通常，光ogrammetric点云通过参考数据 such as LiDAR点云进行评估。然而，当参考数据不 disponible时，评估光ogrammetric点云可能具有挑战。由于这些点云是算法 derive的，其准确性和精度与摄像机网络、场景复杂度和密集图像匹配（DIM）算法有高度相关。而且没有标准的错误度量来确定每个点的错误。在摄像机网络的内部可靠性理论方面，已经进行了广泛的研究，包括第一个错误估计的Bundle Adjustment（BA）理论，以便理解3D点的错误。然而，DIM算法中的测量错误是复杂到每个点都有自己的错误函数，它们取决于因素如像素强度、текстура杂度和表面平滑性。尽管如此，还是有一些常见的度量可以帮助估计 derivated 点云的 posterior 可靠性，特别是在多视点雷达（MVS）设置中，当redundancy 存在时。在本文中，我们使用了一个飞行倾斜的光ogrammetric块，与LiDAR参考数据进行比较，分析了一些内部匹配度量，包括射线整合度、交叉角度、DIM能量等。
</details></li>
</ul>
<hr>
<h2 id="MOVIN-Real-time-Motion-Capture-using-a-Single-LiDAR"><a href="#MOVIN-Real-time-Motion-Capture-using-a-Single-LiDAR" class="headerlink" title="MOVIN: Real-time Motion Capture using a Single LiDAR"></a>MOVIN: Real-time Motion Capture using a Single LiDAR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09314">http://arxiv.org/abs/2309.09314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deok-Kyeong Jang, Dongseok Yang, Deok-Yun Jang, Byeoli Choi, Taeil Jin, Sung-Hee Lee<br>for:* 这个论文是为了解决现有的全身跟踪系统过于昂贵、需要专业技能运行或者穿着不适的问题，提供一种数据驱动的生成方法来实现实时全身跟踪。methods:* 这个方法使用了一个LiDAR传感器来获取3D点云数据，并使用一个自适应 conditional variational autoencoder（CVAE）模型来学习全身 pose 的分布。results:* 该方法可以准确地预测表演者的3D全身信息和局部关节细节，同时具有考虑时间相关性移动的能力。Here’s the simplified Chinese text in the format you requested:for:* 这个论文是为了解决现有的全身跟踪系统过于昂贵、需要专业技能运行或者穿着不适的问题，提供一种数据驱动的生成方法来实现实时全身跟踪。methods:* 这个方法使用了一个LiDAR传感器来获取3D点云数据，并使用一个自适应 conditional variational autoencoder（CVAE）模型来学习全身 pose 的分布。results:* 该方法可以准确地预测表演者的3D全身信息和局部关节细节，同时具有考虑时间相关性移动的能力。<details>
<summary>Abstract</summary>
Recent advancements in technology have brought forth new forms of interactive applications, such as the social metaverse, where end users interact with each other through their virtual avatars. In such applications, precise full-body tracking is essential for an immersive experience and a sense of embodiment with the virtual avatar. However, current motion capture systems are not easily accessible to end users due to their high cost, the requirement for special skills to operate them, or the discomfort associated with wearable devices. In this paper, we present MOVIN, the data-driven generative method for real-time motion capture with global tracking, using a single LiDAR sensor. Our autoregressive conditional variational autoencoder (CVAE) model learns the distribution of pose variations conditioned on the given 3D point cloud from LiDAR.As a central factor for high-accuracy motion capture, we propose a novel feature encoder to learn the correlation between the historical 3D point cloud data and global, local pose features, resulting in effective learning of the pose prior. Global pose features include root translation, rotation, and foot contacts, while local features comprise joint positions and rotations. Subsequently, a pose generator takes into account the sampled latent variable along with the features from the previous frame to generate a plausible current pose. Our framework accurately predicts the performer's 3D global information and local joint details while effectively considering temporally coherent movements across frames. We demonstrate the effectiveness of our architecture through quantitative and qualitative evaluations, comparing it against state-of-the-art methods. Additionally, we implement a real-time application to showcase our method in real-world scenarios. MOVIN dataset is available at \url{https://movin3d.github.io/movin_pg2023/}.
</details>
<details>
<summary>摘要</summary>
现代技术的发展带来了新的互动应用程序，如社交Metaverse，在这些应用程序中，用户通过他们的虚拟人物进行互动。在这些应用程序中，精准全身跟踪是实现卷积体验和虚拟人物embodying的关键。然而，目前的动作捕捉系统因为高价格、需要特殊技能运行以及穿着设备不舒适而不太可 accessible。在这篇论文中，我们提出了MOVIN，一种基于数据驱动的生成方法，通过单个LiDAR感知器实现实时动作捕捉，包括全身跟踪。我们的 autoencoder 模型学习了基于给定的 3D 点云数据的动作变化的分布，并通过一种新的特征编码器来学习 pose 的相关性。全身pose特征包括根据翻译、旋转和脚 contacts，而地方特征包括 JOINT 位置和旋转。然后，一个 pose 生成器将考虑 Sampled 随机变量以及前一帧的特征来生成一个可能的当前 pose。我们的框架可以准确预测演员的 3D 全身信息和局部关节细节，同时考虑了在帧之间的时间准确性。我们通过量化和质量评估来证明我们的架构的有效性，并与当前的方法进行比较。此外，我们还实现了一个实时应用，以示出我们的方法在实际场景中的应用。MOVIN 数据集可以在 \url{https://movin3d.github.io/movin_pg2023/} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-Debiasing-Frame-Length-Bias-in-Text-Video-Retrieval-via-Causal-Intervention"><a href="#Towards-Debiasing-Frame-Length-Bias-in-Text-Video-Retrieval-via-Causal-Intervention" class="headerlink" title="Towards Debiasing Frame Length Bias in Text-Video Retrieval via Causal Intervention"></a>Towards Debiasing Frame Length Bias in Text-Video Retrieval via Causal Intervention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09311">http://arxiv.org/abs/2309.09311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Burak Satar, Hongyuan Zhu, Hanwang Zhang, Joo Hwee Lim</li>
<li>for: 本研究旨在解决文本视频检索中的学习和推理偏见问题。</li>
<li>methods: 本研究使用了一种独特的方法，即考虑视频帧长度差异对于训练和测试集的影响，以避免学习和推理偏见。</li>
<li>results: 研究发现，该方法可以减少文本视频检索模型中的偏见问题，并在 Epic-Kitchens-100、YouCook2 和 MSR-VTT 等 datasets 上达到了领先的成绩。<details>
<summary>Abstract</summary>
Many studies focus on improving pretraining or developing new backbones in text-video retrieval. However, existing methods may suffer from the learning and inference bias issue, as recent research suggests in other text-video-related tasks. For instance, spatial appearance features on action recognition or temporal object co-occurrences on video scene graph generation could induce spurious correlations. In this work, we present a unique and systematic study of a temporal bias due to frame length discrepancy between training and test sets of trimmed video clips, which is the first such attempt for a text-video retrieval task, to the best of our knowledge. We first hypothesise and verify the bias on how it would affect the model illustrated with a baseline study. Then, we propose a causal debiasing approach and perform extensive experiments and ablation studies on the Epic-Kitchens-100, YouCook2, and MSR-VTT datasets. Our model overpasses the baseline and SOTA on nDCG, a semantic-relevancy-focused evaluation metric which proves the bias is mitigated, as well as on the other conventional metrics.
</details>
<details>
<summary>摘要</summary>
In this work, we present a unique and systematic study of a temporal bias due to frame length discrepancy between training and test sets of trimmed video clips, which is the first such attempt for a text-video retrieval task, to the best of our knowledge. We first hypothesize and verify the bias using a baseline study. Then, we propose a causal debiasing approach and perform extensive experiments and ablation studies on the Epic-Kitchens-100, YouCook2, and MSR-VTT datasets. Our model outperforms the baseline and SOTA on nDCG, a semantic-relevancy-focused evaluation metric, which proves that the bias is mitigated, as well as on other conventional metrics.
</details></li>
</ul>
<hr>
<h2 id="UGC-Unified-GAN-Compression-for-Efficient-Image-to-Image-Translation"><a href="#UGC-Unified-GAN-Compression-for-Efficient-Image-to-Image-Translation" class="headerlink" title="UGC: Unified GAN Compression for Efficient Image-to-Image Translation"></a>UGC: Unified GAN Compression for Efficient Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09310">http://arxiv.org/abs/2309.09310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxi Ren, Jie Wu, Peng Zhang, Manlin Zhang, Xuefeng Xiao, Qian He, Rui Wang, Min Zheng, Xin Pan</li>
<li>for: 本研究旨在提出一种新的学习模式，即统一gan压缩（UGC），以实现模型高效和标签高效的同时满足。</li>
<li>methods: 本研究使用 semi-supervised-driven 网络架构搜索和自适应在线 semi-supervised distillation 两个阶段，共同实现一种多样化、标签高效和性能优秀的模型。</li>
<li>results: 实验结果表明，UGC 可以在各种图像识别和生成任务上达到比较高的性能水平，而且比传统的 GAN 模型具有更好的计算效率和数据使用效率。<details>
<summary>Abstract</summary>
Recent years have witnessed the prevailing progress of Generative Adversarial Networks (GANs) in image-to-image translation. However, the success of these GAN models hinges on ponderous computational costs and labor-expensive training data. Current efficient GAN learning techniques often fall into two orthogonal aspects: i) model slimming via reduced calculation costs; ii)data/label-efficient learning with fewer training data/labels. To combine the best of both worlds, we propose a new learning paradigm, Unified GAN Compression (UGC), with a unified optimization objective to seamlessly prompt the synergy of model-efficient and label-efficient learning. UGC sets up semi-supervised-driven network architecture search and adaptive online semi-supervised distillation stages sequentially, which formulates a heterogeneous mutual learning scheme to obtain an architecture-flexible, label-efficient, and performance-excellent model.
</details>
<details>
<summary>摘要</summary>
近年来，人工智能领域内的生成对抗网络（GAN）在图像到图像翻译方面取得了很大进步。然而，GAN模型的成功受到了计算成本的约束和训练数据的劳动成本。当前的高效GAN学习技术通常分为两个垂直方面：i）模型缩减通过减少计算成本；ii）数据/标签高效学习 fewer 训练数据/标签。为了结合这两个方面的优点，我们提出了一种新的学习理念，即统一GAN压缩（UGC），它通过统一优化目标来融合模型高效和标签高效的学习。UGC设计了顺序执行 semi-supervised 驱动网络搜索和自适应在线 semi-supervised 熔化阶段，这种异质共同学习方案可以从数据中提取出高效、标签高效和表现优秀的模型。
</details></li>
</ul>
<hr>
<h2 id="Effective-Image-Tampering-Localization-via-Enhanced-Transformer-and-Co-attention-Fusion"><a href="#Effective-Image-Tampering-Localization-via-Enhanced-Transformer-and-Co-attention-Fusion" class="headerlink" title="Effective Image Tampering Localization via Enhanced Transformer and Co-attention Fusion"></a>Effective Image Tampering Localization via Enhanced Transformer and Co-attention Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09306">http://arxiv.org/abs/2309.09306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Guo, Haochen Zhu, Gang Cao</li>
<li>for: 本文提出了一种基于两极性增强变换encoder的图像修改地点检测网络（EITLNet），以提高图像修改检测的精度和Robustness。</li>
<li>methods: 本文使用了一种两极性增强变换encoder，并设计了一个特性增强模块来提高变换器encoder的特征表示能力。另外，通过均匀注意力模块在多个级别进行特征对比，以实现RGB和杂变流中提取的特征的有效拼接。</li>
<li>results: 实验结果表明，提出的方案在多个标准数据集上达到了当前最佳的总体化能力和Robustness。代码将于<a target="_blank" rel="noopener" href="https://github.com/multimediaFor/EITLNet%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/multimediaFor/EITLNet公开。</a><details>
<summary>Abstract</summary>
Powerful manipulation techniques have made digital image forgeries be easily created and widespread without leaving visual anomalies. The blind localization of tampered regions becomes quite significant for image forensics. In this paper, we propose an effective image tampering localization network (EITLNet) based on a two-branch enhanced transformer encoder with attention-based feature fusion. Specifically, a feature enhancement module is designed to enhance the feature representation ability of the transformer encoder. The features extracted from RGB and noise streams are fused effectively by the coordinate attention-based fusion module at multiple scales. Extensive experimental results verify that the proposed scheme achieves the state-of-the-art generalization ability and robustness in various benchmark datasets. Code will be public at https://github.com/multimediaFor/EITLNet.
</details>
<details>
<summary>摘要</summary>
powerful manipulation techniques have made digital image forgeries easily created and widespread without leaving visual anomalies. The blind localization of tampered regions becomes quite significant for image forensics. In this paper, we propose an effective image tampering localization network (EITLNet) based on a two-branch enhanced transformer encoder with attention-based feature fusion. Specifically, a feature enhancement module is designed to enhance the feature representation ability of the transformer encoder. The features extracted from RGB and noise streams are fused effectively by the coordinate attention-based fusion module at multiple scales. Extensive experimental results verify that the proposed scheme achieves the state-of-the-art generalization ability and robustness in various benchmark datasets. Code will be public at https://github.com/multimediaFor/EITLNet.Here's the translation in Traditional Chinese:powerful manipulation techniques have made digital image forgeries easily created and widespread without leaving visual anomalies. The blind localization of tampered regions becomes quite significant for image forensics. In this paper, we propose an effective image tampering localization network (EITLNet) based on a two-branch enhanced transformer encoder with attention-based feature fusion. Specifically, a feature enhancement module is designed to enhance the feature representation ability of the transformer encoder. The features extracted from RGB and noise streams are fused effectively by the coordinate attention-based fusion module at multiple scales. Extensive experimental results verify that the proposed scheme achieves the state-of-the-art generalization ability and robustness in various benchmark datasets. Code will be public at https://github.com/multimediaFor/EITLNet.
</details></li>
</ul>
<hr>
<h2 id="RenderIH-A-Large-scale-Synthetic-Dataset-for-3D-Interacting-Hand-Pose-Estimation"><a href="#RenderIH-A-Large-scale-Synthetic-Dataset-for-3D-Interacting-Hand-Pose-Estimation" class="headerlink" title="RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation"></a>RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09301">http://arxiv.org/abs/2309.09301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adwardlee/renderih">https://github.com/adwardlee/renderih</a></li>
<li>paper_authors: Lijun Li, Linrui Tian, Xindi Zhang, Qi Wang, Bang Zhang, Liefeng Bo, Mengyuan Liu, Chen Chen</li>
<li>for: 提高手势估计精度，增加数据的多样性和自然性。</li>
<li>methods: 使用 RenderIH 大规模生成 Synthetic 数据，并提出一种新的 pose 优化算法和一种基于 transformer 的 pose 估计网络 TransHand。</li>
<li>results: 实验表明，预训练在 RenderIH 数据上可以显著降低误差，从 6.76mm 降低至 5.79mm，并且 TransHand 超越了当前的方法。<details>
<summary>Abstract</summary>
The current interacting hand (IH) datasets are relatively simplistic in terms of background and texture, with hand joints being annotated by a machine annotator, which may result in inaccuracies, and the diversity of pose distribution is limited. However, the variability of background, pose distribution, and texture can greatly influence the generalization ability. Therefore, we present a large-scale synthetic dataset RenderIH for interacting hands with accurate and diverse pose annotations. The dataset contains 1M photo-realistic images with varied backgrounds, perspectives, and hand textures. To generate natural and diverse interacting poses, we propose a new pose optimization algorithm. Additionally, for better pose estimation accuracy, we introduce a transformer-based pose estimation network, TransHand, to leverage the correlation between interacting hands and verify the effectiveness of RenderIH in improving results. Our dataset is model-agnostic and can improve more accuracy of any hand pose estimation method in comparison to other real or synthetic datasets. Experiments have shown that pretraining on our synthetic data can significantly decrease the error from 6.76mm to 5.79mm, and our Transhand surpasses contemporary methods. Our dataset and code are available at https://github.com/adwardlee/RenderIH.
</details>
<details>
<summary>摘要</summary>
当前的互动手（IH）数据集相对简单，背景和文化环境相对有限，手关节被机器注意者注解，可能会导致错误，手姿 distribuition 的多样性也很有限。然而，背景、手姿分布和文化环境的变化可以对泛化能力产生很大的影响。因此，我们提供了一个大规模的 sintetic 数据集 RenderIH，包含100万个真实的、多样的互动手图像，具有多种背景、视角和手 texture。为生成自然和多样的互动姿势，我们提议了一个新的pose优化算法。此外，为更好地优化pose估计精度，我们引入了一种基于 transformer 的 pose估计网络 TransHand，以利用互动手之间的相关性。我们的数据集是model-agnostic，可以提高任何手姿估计方法的准确性，比较其他真实或 sintetic 数据集。我们的实验表明，预训练于我们的 sintetic 数据可以显著降低错误率，从6.76mm降低到5.79mm，而我们的 TransHand 突破了当今方法。我们的数据集和代码可以在 https://github.com/adwardlee/RenderIH 上下载。
</details></li>
</ul>
<hr>
<h2 id="Chasing-Day-and-Night-Towards-Robust-and-Efficient-All-Day-Object-Detection-Guided-by-an-Event-Camera"><a href="#Chasing-Day-and-Night-Towards-Robust-and-Efficient-All-Day-Object-Detection-Guided-by-an-Event-Camera" class="headerlink" title="Chasing Day and Night: Towards Robust and Efficient All-Day Object Detection Guided by an Event Camera"></a>Chasing Day and Night: Towards Robust and Efficient All-Day Object Detection Guided by an Event Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09297">http://arxiv.org/abs/2309.09297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahang Cao, Xu Zheng, Yuanhuiyi Lyu, Jiaxu Wang, Renjing Xu, Lin Wang</li>
<li>for: 这篇论文目的是提出一种robust和高效的all-day对象检测方法，以适应实际应用中的各种照明条件。</li>
<li>methods: 该方法基于一个轻量级的射频神经网络（SNN），并使用事件模式来高效地利用异步性。另外，我们还提出了一个事件时间注意力（ETA）模块，以学习事件中的高时间信息，同时保持重要的边缘信息。最后，我们提出了一种新的对RGB-事件特征的Symmetric RGB-Event Fusion（SREF）模块，以有效地融合RGB-事件特征，无需依赖于特定的感知模式。</li>
<li>results: 我们的EOLO方法在各种照明条件下表现出色，与现状的最佳方法（RENet）相比，增加了3.74%的mAP50。我们还建立了两个新的数据集，E-MSCOCO和E-VOC，以便进一步验证和改进我们的方法。<details>
<summary>Abstract</summary>
The ability to detect objects in all lighting (i.e., normal-, over-, and under-exposed) conditions is crucial for real-world applications, such as self-driving.Traditional RGB-based detectors often fail under such varying lighting conditions.Therefore, recent works utilize novel event cameras to supplement or guide the RGB modality; however, these methods typically adopt asymmetric network structures that rely predominantly on the RGB modality, resulting in limited robustness for all-day detection. In this paper, we propose EOLO, a novel object detection framework that achieves robust and efficient all-day detection by fusing both RGB and event modalities. Our EOLO framework is built based on a lightweight spiking neural network (SNN) to efficiently leverage the asynchronous property of events. Buttressed by it, we first introduce an Event Temporal Attention (ETA) module to learn the high temporal information from events while preserving crucial edge information. Secondly, as different modalities exhibit varying levels of importance under diverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion (SREF) module to effectively fuse RGB-Event features without relying on a specific modality, thus ensuring a balanced and adaptive fusion for all-day detection. In addition, to compensate for the lack of paired RGB-Event datasets for all-day training and evaluation, we propose an event synthesis approach based on the randomized optical flow that allows for directly generating the event frame from a single exposure image. We further build two new datasets, E-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC. Extensive experiments demonstrate that our EOLO outperforms the state-of-the-art detectors,e.g.,RENet,by a substantial margin (+3.74% mAP50) in all lighting conditions.Our code and datasets will be available at https://vlislab22.github.io/EOLO/
</details>
<details>
<summary>摘要</summary>
“能够检测各种照明（正常、过颤、和UNDER-EXPOSED）的能力是实际应用中的重要要素，例如自动驾驶。传统的RGB基于的探测器经常在这些不同的照明条件下失败。因此，现有的工作将使用新的事件摄像机来补充或导引RGB模式；然而，这些方法通常运用不对称的网络结构，导致仅对RGB模式进行有限的可靠性。在这篇文章中，我们提出了EOLO框架，一个可靠且高效的实现了全天探测的物体探测框架。我们的EOLO框架基于轻量级的神经网络（SNN），以有效地利用事件的异步性。此外，我们首先引入了一个事件时间注意力（ETA）模组，以学习事件中的高时间信息，同时保留重要的边缘信息。其次，由于不同的模式在不同的照明条件下展现出不同的重要性，我们提出了一个新的对称RGB-事件融合（SREF）模组，以有效地融合RGB-事件特征，并确保在所有照明条件下实现平衡和适应的融合。此外，为了补充缺乏RGB-事件的对称训练和评估数据，我们提出了一个基于随机抽象流的事件生成方法，允许从单一曝光图像中直接生成事件帧。 finally，我们建立了两个新的数据集，E-MSCOCO和E-VOC，基于知名的benchmark MSCOCO和PASCAL VOC。实验结果显示，我们的EOLO在所有照明条件下明显超过了现有的检测器，例如RENet，+3.74% mAP50。我们的代码和数据将在https://vlislab22.github.io/EOLO/ 网页上公开。”
</details></li>
</ul>
<hr>
<h2 id="LivelySpeaker-Towards-Semantic-Aware-Co-Speech-Gesture-Generation"><a href="#LivelySpeaker-Towards-Semantic-Aware-Co-Speech-Gesture-Generation" class="headerlink" title="LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation"></a>LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09294">http://arxiv.org/abs/2309.09294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyhbili/LivelySpeaker">https://github.com/zyhbili/LivelySpeaker</a></li>
<li>paper_authors: Yihao Zhi, Xiaodong Cun, Xuelin Chen, Xi Shen, Wen Guo, Shaoli Huang, Shenghua Gao</li>
<li>For: The paper is focused on developing a framework for generating co-speech gestures that are semantically aligned with the speech content, and it aims to provide several control handles for various applications.* Methods: The proposed framework consists of two stages: script-based gesture generation and audio-guided rhythm refinement. The script-based gesture generation uses pre-trained CLIP text embeddings as guidance, while the audio-guided rhythm refinement uses a simple but effective diffusion-based gesture generation backbone conditioned on audio signals.* Results: The proposed framework outperforms competing methods in terms of semantic awareness and rhythm alignment, and it also achieves state-of-the-art performance on two benchmarks. Additionally, the framework provides several applications such as changing the gesticulation style, editing co-speech gestures via textual prompting, and controlling the semantic awareness and rhythm alignment with guided diffusion.Here’s the simplified Chinese version of the three key points:* For: 这篇论文是关于开发一种基于语音内容的协调姿势生成框架，并提供了多种控制处理的目的。* Methods: 该框架包括两个阶段：脚本基于的姿势生成和音频导向的协调姿势细化。脚本基于的姿势生成使用预训练的 CLIP 文本嵌入为导航，而音频导向的姿势细化使用简单 yet effective 的扩散基本模型， conditioned on 音频信号。* Results: 该框架比前一代方法更加具有 semantics-aware 和 rhythm alignment 的优势，并在两个标准测试集上达到了领先的性能。此外，该框架还提供了多种应用，例如修改姿势风格、通过文本提示编辑协调姿势、控制semantic awareness和rhythm alignment with guided diffusion。<details>
<summary>Abstract</summary>
Gestures are non-verbal but important behaviors accompanying people's speech. While previous methods are able to generate speech rhythm-synchronized gestures, the semantic context of the speech is generally lacking in the gesticulations. Although semantic gestures do not occur very regularly in human speech, they are indeed the key for the audience to understand the speech context in a more immersive environment. Hence, we introduce LivelySpeaker, a framework that realizes semantics-aware co-speech gesture generation and offers several control handles. In particular, our method decouples the task into two stages: script-based gesture generation and audio-guided rhythm refinement. Specifically, the script-based gesture generation leverages the pre-trained CLIP text embeddings as the guidance for generating gestures that are highly semantically aligned with the script. Then, we devise a simple but effective diffusion-based gesture generation backbone simply using pure MLPs, that is conditioned on only audio signals and learns to gesticulate with realistic motions. We utilize such powerful prior to rhyme the script-guided gestures with the audio signals, notably in a zero-shot setting. Our novel two-stage generation framework also enables several applications, such as changing the gesticulation style, editing the co-speech gestures via textual prompting, and controlling the semantic awareness and rhythm alignment with guided diffusion. Extensive experiments demonstrate the advantages of the proposed framework over competing methods. In addition, our core diffusion-based generative model also achieves state-of-the-art performance on two benchmarks. The code and model will be released to facilitate future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统方法可以生成与语音节奏同步的手势，但是它们缺乏语音Semantic上的信息。虽然semantic手势在人类语言交流中并不太常见，但它们对听众理解语言场景的更深入的含义是非常重要的。因此，我们介绍了LivelySpeaker框架，它可以实现语音Semantic-aware co-speech手势生成，并提供了多个控制把柄。具体来说，我们的方法分为两个阶段：脚本基于的手势生成和音频导航的rhythm refinement。特别是，我们使用预训练的CLIP文本嵌入为生成高度semantic相align的手势的指导。然后，我们设计了一种简单 yet powerful的扩散基于多层perceptron（MLP）的手势生成模型，该模型通过只使用音频信号来生成真实的手势动作。我们利用这种强大的优先来谱匹配script-guided手势与音频信号，特别是在零扩展设定下。我们的新的两个阶段生成框架还具有多种应用，例如改变手势风格，通过文本提示编辑副音频手势，以及控制semantic awareness和rhythm alignment的扩散导航。我们的实验表明，我们的提议的框架比前一代方法具有更多的优势。此外，我们的核心扩散生成模型也在两个标准测试集上达到了状态的艺术性表现。我们计划将代码和模型发布，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="MVP-Meta-Visual-Prompt-Tuning-for-Few-Shot-Remote-Sensing-Image-Scene-Classification"><a href="#MVP-Meta-Visual-Prompt-Tuning-for-Few-Shot-Remote-Sensing-Image-Scene-Classification" class="headerlink" title="MVP: Meta Visual Prompt Tuning for Few-Shot Remote Sensing Image Scene Classification"></a>MVP: Meta Visual Prompt Tuning for Few-Shot Remote Sensing Image Scene Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09276">http://arxiv.org/abs/2309.09276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Zhu, Yiying Li, Chunping Qiu, Ke Yang, Naiyang Guan, Xiaodong Yi</li>
<li>for: 这个研究的目的是提出一个高效且灵活的几步演练类别模型，专门适用于遥感图像分类 задачі。</li>
<li>methods: 这个研究使用了已经预训的视觉对应器模型，并将其调整为适合遥感图像分类任务。另外，研究人员还提出了一个基于patch嵌入重复的资料增强技术，以增强Scene的表现和多样性。</li>
<li>results: 实验结果显示，提出的MVP方法在不同的设定下（包括不同的way和shot）均具有较好的性能，并且在跨领域适应中也具有良好的表现。<details>
<summary>Abstract</summary>
Vision Transformer (ViT) models have recently emerged as powerful and versatile models for various visual tasks. Recently, a work called PMF has achieved promising results in few-shot image classification by utilizing pre-trained vision transformer models. However, PMF employs full fine-tuning for learning the downstream tasks, leading to significant overfitting and storage issues, especially in the remote sensing domain. In order to tackle these issues, we turn to the recently proposed parameter-efficient tuning methods, such as VPT, which updates only the newly added prompt parameters while keeping the pre-trained backbone frozen. Inspired by VPT, we propose the Meta Visual Prompt Tuning (MVP) method. Specifically, we integrate the VPT method into the meta-learning framework and tailor it to the remote sensing domain, resulting in an efficient framework for Few-Shot Remote Sensing Scene Classification (FS-RSSC). Furthermore, we introduce a novel data augmentation strategy based on patch embedding recombination to enhance the representation and diversity of scenes for classification purposes. Experiment results on the FS-RSSC benchmark demonstrate the superior performance of the proposed MVP over existing methods in various settings, such as various-way-various-shot, various-way-one-shot, and cross-domain adaptation.
</details>
<details>
<summary>摘要</summary>
视野变换器（ViT）模型最近在视觉任务中表现出了强大和通用的能力。其中，一项工作名为PMF在几个shot图像分类中获得了可观的结果，但PMF使用全部精度调整，导致重要遗传和存储问题，尤其在远程感知领域。为解决这些问题，我们转向 reciently proposed 参数精度调整方法，如VPT，该方法只更新添加的提示参数，而保留预训练的背部锁定。 inspirited by VPT，我们提出了元视觉提示调整方法（MVP）。specifically，我们将VPT方法 integrate into 元学习框架，并适应远程感知领域，从而实现了高效的几个shot远程感知场景分类（FS-RSSC）。此外，我们引入了一种新的数据增强策略基于贴图嵌入重编，以提高分类目的场景表示和多样性。FS-RSSC标准测试集实验结果表明，我们提出的MVP方法在不同的设置下（如多种多shot、一种多shot和跨领域适应）都与现有方法进行了比较，并达到了更好的表现。
</details></li>
</ul>
<hr>
<h2 id="LiDAR-Data-Synthesis-with-Denoising-Diffusion-Probabilistic-Models"><a href="#LiDAR-Data-Synthesis-with-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models"></a>LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09256">http://arxiv.org/abs/2309.09256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuto Nakashima, Ryo Kurazume</li>
<li>for: 本研究旨在生成高精度的3D LiDAR数据，用于自适应移动机器人的规划和控制。</li>
<li>methods: 我们提出了一种基于DDPMs的生成模型，通过对图像表示的距离和反射强度进行描述，生成多种和高精度的3D场景点云。我们还提出了一种灵活的LiDAR完成管线，使用DDPMs的强大特性来实现。</li>
<li>results: 我们的方法在KITTI-360和KITTI-Raw数据集上的生成任务和KITTI-360数据集上的upsampling任务中表现出色，超过了基eline。我们的代码和预训练参数将在<a target="_blank" rel="noopener" href="https://github.com/kazuto1011/r2dm%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/kazuto1011/r2dm上提供。</a><details>
<summary>Abstract</summary>
Generative modeling of 3D LiDAR data is an emerging task with promising applications for autonomous mobile robots, such as scalable simulation, scene manipulation, and sparse-to-dense completion of LiDAR point clouds. Existing approaches have shown the feasibility of image-based LiDAR data generation using deep generative models while still struggling with the fidelity of generated data and training instability. In this work, we present R2DM, a novel generative model for LiDAR data that can generate diverse and high-fidelity 3D scene point clouds based on the image representation of range and reflectance intensity. Our method is based on the denoising diffusion probabilistic models (DDPMs), which have demonstrated impressive results among generative model frameworks and have been significantly progressing in recent years. To effectively train DDPMs on the LiDAR domain, we first conduct an in-depth analysis regarding data representation, training objective, and spatial inductive bias. Based on our designed model R2DM, we also introduce a flexible LiDAR completion pipeline using the powerful properties of DDPMs. We demonstrate that our method outperforms the baselines on the generation task of KITTI-360 and KITTI-Raw datasets and the upsampling task of KITTI-360 datasets. Our code and pre-trained weights will be available at https://github.com/kazuto1011/r2dm.
</details>
<details>
<summary>摘要</summary>
“三维LiDAR数据生成是一个emerging任务，具有吸引人的应用前景，如自动移动 robot的广泛simulation、scene操作和LiDAR点云的簇范 completion。现有的方法已经显示了对于深度生成模型的LiDAR数据生成的可能性，但是仍然面临生成数据的实际性和训练稳定性问题。在这个工作中，我们提出了R2DM，一个新的LiDAR数据生成模型，可以生成多样和高实际性的三维Scene点云，基于影像表现的距离和反射intensity。我们的方法基于减误散射概率模型（DDPMs），这些模型在生成模型框架中已经显示出了杰出的成果，并在最近几年内有所进步。为了有效地对LiDAR领域训练 DDPMs，我们首先进行了LiDAR领域的深入分析，包括数据表现、训练目标和空间传递偏好。基于我们的设计的R2DM模型，我们也提出了一个灵活的LiDAR完备管线，使用DDPMs的强大特性。我们的方法在KITTI-360和KITTI-Rawdataset上的生成和upsampling任务中表现出色，较基eline的方法更好。我们的代码和预训练 веса将在https://github.com/kazuto1011/r2dm上公开。”
</details></li>
</ul>
<hr>
<h2 id="Convex-Latent-Optimized-Adversarial-Regularizers-for-Imaging-Inverse-Problems"><a href="#Convex-Latent-Optimized-Adversarial-Regularizers-for-Imaging-Inverse-Problems" class="headerlink" title="Convex Latent-Optimized Adversarial Regularizers for Imaging Inverse Problems"></a>Convex Latent-Optimized Adversarial Regularizers for Imaging Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09250">http://arxiv.org/abs/2309.09250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huayu Wang, Chen Luo, Taofeng Xie, Qiyu Jin, Guoqing Chen, Zhuo-Xu Cui, Dong Liang</li>
<li>for: 这个研究旨在提出一个新的、可解释的数据驱动技术，以解决Magnetic Resonance Imaging（MRI）逆问题中的挑战。</li>
<li>methods: 这个方法使用了深度学习（DL）和条件调节的融合，并使用了潜在优化技术来对抗训练一个输入圆体神经网络。</li>
<li>results: 这个研究展示了CLEAR-informed的调节模型能够在真实数据上运行，并且能够稳定地重建图像，即使在测量干扰的情况下。此外，这个方法比 conventinal data-driven技术和传统调节方法更好，具有更高的重建质量和更好的稳定性。<details>
<summary>Abstract</summary>
Recently, data-driven techniques have demonstrated remarkable effectiveness in addressing challenges related to MR imaging inverse problems. However, these methods still exhibit certain limitations in terms of interpretability and robustness. In response, we introduce Convex Latent-Optimized Adversarial Regularizers (CLEAR), a novel and interpretable data-driven paradigm. CLEAR represents a fusion of deep learning (DL) and variational regularization. Specifically, we employ a latent optimization technique to adversarially train an input convex neural network, and its set of minima can fully represent the real data manifold. We utilize it as a convex regularizer to formulate a CLEAR-informed variational regularization model that guides the solution of the imaging inverse problem on the real data manifold. Leveraging its inherent convexity, we have established the convergence of the projected subgradient descent algorithm for the CLEAR-informed regularization model. This convergence guarantees the attainment of a unique solution to the imaging inverse problem, subject to certain assumptions. Furthermore, we have demonstrated the robustness of our CLEAR-informed model, explicitly showcasing its capacity to achieve stable reconstruction even in the presence of measurement interference. Finally, we illustrate the superiority of our approach using MRI reconstruction as an example. Our method consistently outperforms conventional data-driven techniques and traditional regularization approaches, excelling in both reconstruction quality and robustness.
</details>
<details>
<summary>摘要</summary>
最近，数据驱动技术已经在MR图像反问题中表现出了非常出色的效果。然而，这些方法仍然存在一定的可解释性和稳定性的限制。为回应，我们介绍了一种新的可解释的数据驱动方法，即Convex Latent-Optimized Adversarial Regularizers（CLEAR）。CLEAR是一种将深度学习（DL）和变量正则化相结合的新型数据驱动方法。我们使用了潜在优化技术来对输入的凸神经网络进行对抗训练，并将其集的最小值用作一种凸正则化模型，以指导图像反问题的解决。由于其内置的凸性，我们已经证明了对CLEAR-informed的正则化模型的投影下滤链落点梯度下降算法的 converges，这 garanties the attainment of a unique solution to the imaging inverse problem, subject to certain assumptions。此外，我们还证明了我们的CLEAR-informed模型的稳定性，并证明其能够在测量干扰存在时仍然实现稳定的重建。最后，我们使用MRI重建为例子，并证明了我们的方法可以与传统的数据驱动技术和正则化方法相比，在重建质量和稳定性两个方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="LiteTrack-Layer-Pruning-with-Asynchronous-Feature-Extraction-for-Lightweight-and-Efficient-Visual-Tracking"><a href="#LiteTrack-Layer-Pruning-with-Asynchronous-Feature-Extraction-for-Lightweight-and-Efficient-Visual-Tracking" class="headerlink" title="LiteTrack: Layer Pruning with Asynchronous Feature Extraction for Lightweight and Efficient Visual Tracking"></a>LiteTrack: Layer Pruning with Asynchronous Feature Extraction for Lightweight and Efficient Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09249">http://arxiv.org/abs/2309.09249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingmao Wei, Bi Zeng, Jianqi Liu, Li He, Guotian Zeng</li>
<li>for: 这个论文是为了提出一种高速的 transformer-based 视觉跟踪模型，以满足实时 робоτICS应用的需求。</li>
<li>methods: 该模型使用 asynchronous feature extraction 和模版和搜索区域之间的交互来提高特征融合和减少无用计算，并对加载的encoder层进行剪辑以达到更好的性能和速度平衡。</li>
<li>results: 模型在 GOT-10k 测试集上 achiev 65.2% AO，超过了所有之前的轻量级跟踪模型，并在 ONNX 上的 Jetson Orin NX 边缘设备上运行速度超过 100 fps。此外，模型在 NVIDIA 2080Ti GPU 上达到了 171 fps 的运行速度，并在 TrackingNet 测试集上达到了 72.2% AO 和 82.4% AUC。<details>
<summary>Abstract</summary>
The recent advancements in transformer-based visual trackers have led to significant progress, attributed to their strong modeling capabilities. However, as performance improves, running latency correspondingly increases, presenting a challenge for real-time robotics applications, especially on edge devices with computational constraints. In response to this, we introduce LiteTrack, an efficient transformer-based tracking model optimized for high-speed operations across various devices. It achieves a more favorable trade-off between accuracy and efficiency than the other lightweight trackers. The main innovations of LiteTrack encompass: 1) asynchronous feature extraction and interaction between the template and search region for better feature fushion and cutting redundant computation, and 2) pruning encoder layers from a heavy tracker to refine the balnace between performance and speed. As an example, our fastest variant, LiteTrack-B4, achieves 65.2% AO on the GOT-10k benchmark, surpassing all preceding efficient trackers, while running over 100 fps with ONNX on the Jetson Orin NX edge device. Moreover, our LiteTrack-B9 reaches competitive 72.2% AO on GOT-10k and 82.4% AUC on TrackingNet, and operates at 171 fps on an NVIDIA 2080Ti GPU. The code and demo materials will be available at https://github.com/TsingWei/LiteTrack.
</details>
<details>
<summary>摘要</summary>
Recent advancements in transformer-based visual trackers have led to significant progress, thanks to their strong modeling capabilities. However, as performance improves, running latency correspondingly increases, presenting a challenge for real-time robotics applications, especially on edge devices with computational constraints. In response to this, we introduce LiteTrack, an efficient transformer-based tracking model optimized for high-speed operations across various devices. It achieves a more favorable trade-off between accuracy and efficiency than other lightweight trackers. The main innovations of LiteTrack include:1. 异步特征提取和模板与搜索区域之间的交互，以实现更好的特征融合和减少 redundancy computation。2. 从一个重量级的跟踪器中剪辑encoder层，以进一步整合性和速度的平衡。例如，我们最快的变体LiteTrack-B4在GOT-10k标准测试集上达到了65.2%的AO，超过了所有之前的高效跟踪器，并在ONNX上的Jetson Orin NX边缘设备上运行速度达100帧/秒。此外，我们的LiteTrack-B9在GOT-10k和TrackingNet标准测试集上达到了72.2%的AO和82.4%的AUC，并在NVIDIA 2080Ti GPU上运行速度达171帧/秒。我们将在https://github.com/TsingWei/LiteTrack中提供代码和示例材料。
</details></li>
</ul>
<hr>
<h2 id="Image-level-supervision-and-self-training-for-transformer-based-cross-modality-tumor-segmentation"><a href="#Image-level-supervision-and-self-training-for-transformer-based-cross-modality-tumor-segmentation" class="headerlink" title="Image-level supervision and self-training for transformer-based cross-modality tumor segmentation"></a>Image-level supervision and self-training for transformer-based cross-modality tumor segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09246">http://arxiv.org/abs/2309.09246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malo de Boisredon, Eugene Vorontsov, William Trung Le, Samuel Kadoury<br>for:* 这个研究旨在提高医疗影像分割领域中的自动化医疗影像分类，特别是跨modalities的情况下。methods:* 提出了一个新的半supervised训练策略called MoDATTS，可以实现精准的跨modalities 3D肿瘤分类。* 使用了一个image-to-image translation策略来将不同modalities的影像转换为弹性target volume，以提高对不同modalities的普遍化。* 还引入了一个迭代自训程序来进一步关闭modalities之间的领域差。results:* MoDATTS在CrossMoDA 2022挑战中的reported top Dice score为0.87+&#x2F;-0.04，较其他参赛队伍的方法高。* MoDATTS在跨modalities的Brain Tumor Segmentation任务上显示了consistent的提高，其Dice score比baseline高出10%以上。* MoDATTS可以实现95%的目标supervised模型性能，并且可以透过更多的标注资料来进一步提高性能。<details>
<summary>Abstract</summary>
Deep neural networks are commonly used for automated medical image segmentation, but models will frequently struggle to generalize well across different imaging modalities. This issue is particularly problematic due to the limited availability of annotated data, making it difficult to deploy these models on a larger scale. To overcome these challenges, we propose a new semi-supervised training strategy called MoDATTS. Our approach is designed for accurate cross-modality 3D tumor segmentation on unpaired bi-modal datasets. An image-to-image translation strategy between imaging modalities is used to produce annotated pseudo-target volumes and improve generalization to the unannotated target modality. We also use powerful vision transformer architectures and introduce an iterative self-training procedure to further close the domain gap between modalities. MoDATTS additionally allows the possibility to extend the training to unannotated target data by exploiting image-level labels with an unsupervised objective that encourages the model to perform 3D diseased-to-healthy translation by disentangling tumors from the background. The proposed model achieves superior performance compared to other methods from participating teams in the CrossMoDA 2022 challenge, as evidenced by its reported top Dice score of 0.87+/-0.04 for the VS segmentation. MoDATTS also yields consistent improvements in Dice scores over baselines on a cross-modality brain tumor segmentation task composed of four different contrasts from the BraTS 2020 challenge dataset, where 95% of a target supervised model performance is reached. We report that 99% and 100% of this maximum performance can be attained if 20% and 50% of the target data is additionally annotated, which further demonstrates that MoDATTS can be leveraged to reduce the annotation burden.
</details>
<details>
<summary>摘要</summary>
深度神经网络通常用于自动医疗影像分割，但模型很难泛化到不同的成像方式。这个问题特别是由于有限的标注数据，使得这些模型在更大规模上部署变得困难。为了解决这些挑战，我们提出了一种新的半监督训练策略called MoDATTS。我们的方法适用于精准的三维肿瘤分割不同成像方式的不协调数据集。我们使用 между成像模式之间的图像转换策略生成标注 pseudo-目标Volume，以改善对目标成像模式的泛化。此外，我们使用强大的视觉转换架构，并引入迭代自我训练过程，以进一步减小成像模式之间的领域差。MoDATTS还允许在未标注目标数据上继续训练，通过利用图像水平标签来鼓励模型进行三维疾病到健康的图像翻译，从而分离肿瘤和背景。我们的模型在CrossMoDA 2022挑战中与其他参赛队列表示的方法相比，实现了最高的Dice分数0.87+/-0.04 для VS分割。MoDATTS还在四个不同的脑肿瘤分割任务中实现了相对于基eline的稳定改进，其中95%的目标监督模型性能可以达到。如果添加20%和50%的目标数据，则可以达到99%和100%的最大性能，这进一步证明了MoDATTS可以减少标注占用。
</details></li>
</ul>
<hr>
<h2 id="CaSAR-Contact-aware-Skeletal-Action-Recognition"><a href="#CaSAR-Contact-aware-Skeletal-Action-Recognition" class="headerlink" title="CaSAR: Contact-aware Skeletal Action Recognition"></a>CaSAR: Contact-aware Skeletal Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10001">http://arxiv.org/abs/2309.10001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junan Lin, Zhichao Sun, Enjie Cao, Taein Kwon, Mahdi Rad, Marc Pollefeys</li>
<li>for: 这篇论文的目的是提出一种新的skeletal action recognition方法，以便在AR&#x2F;VR镜头和人机器人交互中使用。</li>
<li>methods: 这篇论文使用了一种新的表示方法，即手指和物体之间的接触点和远离点，以捕捉手指和物体之间的空间关系。</li>
<li>results: 该方法在两个公共数据集上达到了91.3%和98.4%的高精度，比之前的最佳方法高出了10%以上。<details>
<summary>Abstract</summary>
Skeletal Action recognition from an egocentric view is important for applications such as interfaces in AR/VR glasses and human-robot interaction, where the device has limited resources. Most of the existing skeletal action recognition approaches use 3D coordinates of hand joints and 8-corner rectangular bounding boxes of objects as inputs, but they do not capture how the hands and objects interact with each other within the spatial context. In this paper, we present a new framework called Contact-aware Skeletal Action Recognition (CaSAR). It uses novel representations of hand-object interaction that encompass spatial information: 1) contact points where the hand joints meet the objects, 2) distant points where the hand joints are far away from the object and nearly not involved in the current action. Our framework is able to learn how the hands touch or stay away from the objects for each frame of the action sequence, and use this information to predict the action class. We demonstrate that our approach achieves the state-of-the-art accuracy of 91.3% and 98.4% on two public datasets, H2O and FPHA, respectively.
</details>
<details>
<summary>摘要</summary>
skeletal action recognition from an egocentric view 是很重要的，因为它们可以用于AR/VR镜头和人机交互，而这些设备具有有限的资源。现有的大多数skeletal action recognition方法使用手 JOINTS的3D坐标和物体的8个顶点 rectangle bounding box作为输入，但是它们不能捕捉手和物体之间的空间关系。在这篇论文中，我们提出了一个新的框架，即Contact-aware Skeletal Action Recognition（CaSAR）。它使用了一些新的手-物体交互表示，包括：1）手 JOINTS与物体之间的接触点，2）手 JOINTS在物体之外的远离点，这些点在当前动作序列中并不直接参与动作。我们的框架可以在每帧动作序列中学习手与物体之间的接触和远离情况，并使用这些信息预测动作类别。我们证明了我们的方法可以在两个公共数据集上达到91.3%和98.4%的状态态的准确率。
</details></li>
</ul>
<hr>
<h2 id="CryoAlign-feature-based-method-for-global-and-local-3D-alignment-of-EM-density-maps"><a href="#CryoAlign-feature-based-method-for-global-and-local-3D-alignment-of-EM-density-maps" class="headerlink" title="CryoAlign: feature-based method for global and local 3D alignment of EM density maps"></a>CryoAlign: feature-based method for global and local 3D alignment of EM density maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09217">http://arxiv.org/abs/2309.09217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bintao He, Fa Zhang, Chenjie Feng, Jianyi Yang, Xin Gao, Renmin Han</li>
<li>for: density maps的对Alignment和比较，以解释结构信息，如结构不一致性分析和原子模型组装。</li>
<li>methods: 使用本地密度特征描述符来捕捉空间结构相似性，快速建立点对点匹配和稳定定制参数。</li>
<li>results: 在实验评估中，CryoAlign表现出了较高的对Alignment精度和速度，胜过现有的方法。<details>
<summary>Abstract</summary>
Advances on cryo-electron imaging technologies have led to a rapidly increasing number of density maps. Alignment and comparison of density maps play a crucial role in interpreting structural information, such as conformational heterogeneity analysis using global alignment and atomic model assembly through local alignment. Here, we propose a fast and accurate global and local cryo-electron microscopy density map alignment method CryoAlign, which leverages local density feature descriptors to capture spatial structure similarities. CryoAlign is the first feature-based EM map alignment tool, in which the employment of feature-based architecture enables the rapid establishment of point pair correspondences and robust estimation of alignment parameters. Extensive experimental evaluations demonstrate the superiority of CryoAlign over the existing methods in both alignment accuracy and speed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="All-optical-image-denoising-using-a-diffractive-visual-processor"><a href="#All-optical-image-denoising-using-a-diffractive-visual-processor" class="headerlink" title="All-optical image denoising using a diffractive visual processor"></a>All-optical image denoising using a diffractive visual processor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09215">http://arxiv.org/abs/2309.09215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cagatay Isıl, Tianyi Gan, F. Onuralp Ardic, Koray Mentesoglu, Jagrit Digani, Huseyin Karaca, Hanlong Chen, Jingxi Li, Deniz Mengu, Mona Jarrahi, Kaan Akşit, Aydogan Ozcan</li>
<li>for:  removes noise&#x2F;artifacts from input images</li>
<li>methods:  all-optical and non-iterative, using deep learning-enabled analog diffractive image denoiser</li>
<li>results:  efficiently removes salt and pepper noise and image rendering-related spatial artifacts, with an output power efficiency of ~30-40%<details>
<summary>Abstract</summary>
Image denoising, one of the essential inverse problems, targets to remove noise/artifacts from input images. In general, digital image denoising algorithms, executed on computers, present latency due to several iterations implemented in, e.g., graphics processing units (GPUs). While deep learning-enabled methods can operate non-iteratively, they also introduce latency and impose a significant computational burden, leading to increased power consumption. Here, we introduce an analog diffractive image denoiser to all-optically and non-iteratively clean various forms of noise and artifacts from input images - implemented at the speed of light propagation within a thin diffractive visual processor. This all-optical image denoiser comprises passive transmissive layers optimized using deep learning to physically scatter the optical modes that represent various noise features, causing them to miss the output image Field-of-View (FoV) while retaining the object features of interest. Our results show that these diffractive denoisers can efficiently remove salt and pepper noise and image rendering-related spatial artifacts from input phase or intensity images while achieving an output power efficiency of ~30-40%. We experimentally demonstrated the effectiveness of this analog denoiser architecture using a 3D-printed diffractive visual processor operating at the terahertz spectrum. Owing to their speed, power-efficiency, and minimal computational overhead, all-optical diffractive denoisers can be transformative for various image display and projection systems, including, e.g., holographic displays.
</details>
<details>
<summary>摘要</summary>
图像去噪，是一个fundamental inverse problem，目标是从输入图像中除去噪声/特征。通常，计算机执行的数字图像去噪算法会出现延迟，因为它们在图像处理中需要许多迭代。深度学习启用的方法也会引入延迟和计算负担，导致增加的电力消耗。在这里，我们引入了一种光学diffractive图像去噪器，可以非 iteratively 和all-optically清理输入图像中的各种噪声和特征。这种光学图像去噪器包括优化的通过深度学习的透明层，以physically 扰动光模式，使其在输出图像Field-of-View（FoV）中产生折射。我们的结果表明，这种diffractive denoiser可以高效地除去盐和细颗粒噪声，以及图像渲染相关的空间artefacts，从输入相位或Intensity图像中获得 ~30-40%的输出功率效率。我们实验采用了一个3D打印的diffractive视觉处理器，在teraHz频谱下运行。由于它们的速度、功率效率和计算负担很低，光学diffractive denoiser可能会对各种图像显示和投影系统做出巨大的变革，例如投影式显示器。
</details></li>
</ul>
<hr>
<h2 id="Neural-Gradient-Learning-and-Optimization-for-Oriented-Point-Normal-Estimation"><a href="#Neural-Gradient-Learning-and-Optimization-for-Oriented-Point-Normal-Estimation" class="headerlink" title="Neural Gradient Learning and Optimization for Oriented Point Normal Estimation"></a>Neural Gradient Learning and Optimization for Oriented Point Normal Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09211">http://arxiv.org/abs/2309.09211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LeoQLi/NGLO">https://github.com/LeoQLi/NGLO</a></li>
<li>paper_authors: Qing Li, Huifang Feng, Kanle Shi, Yi Fang, Yu-Shen Liu, Zhizhong Han</li>
<li>for: 学习3D点云中的向量场，用于normal估计。</li>
<li>methods: 使用深度学习方法， parameterize对象函数生成点云中的导向量，并使用地方几何学习 angular distance field 进行精细化。</li>
<li>results: 提供了一种robust和精细的normal估计方法，可以抗抗噪、点云异常和点云分布变化。对比 précédents works，提高了normal估计的精度和泛化能力。<details>
<summary>Abstract</summary>
We propose Neural Gradient Learning (NGL), a deep learning approach to learn gradient vectors with consistent orientation from 3D point clouds for normal estimation. It has excellent gradient approximation properties for the underlying geometry of the data. We utilize a simple neural network to parameterize the objective function to produce gradients at points using a global implicit representation. However, the derived gradients usually drift away from the ground-truth oriented normals due to the lack of local detail descriptions. Therefore, we introduce Gradient Vector Optimization (GVO) to learn an angular distance field based on local plane geometry to refine the coarse gradient vectors. Finally, we formulate our method with a two-phase pipeline of coarse estimation followed by refinement. Moreover, we integrate two weighting functions, i.e., anisotropic kernel and inlier score, into the optimization to improve the robust and detail-preserving performance. Our method efficiently conducts global gradient approximation while achieving better accuracy and generalization ability of local feature description. This leads to a state-of-the-art normal estimator that is robust to noise, outliers and point density variations. Extensive evaluations show that our method outperforms previous works in both unoriented and oriented normal estimation on widely used benchmarks. The source code and pre-trained models are available at https://github.com/LeoQLi/NGLO.
</details>
<details>
<summary>摘要</summary>
我们提出了神经Gradient学习（NGL），一种深度学习方法，用于从3D点云中学习具有一致方向的梯度 вектор。它具有优秀的梯度近似性特性，用于下面的数据结构。我们使用了简单的神经网络来参数化目标函数，以生成点上的梯度。然而， derivated梯度通常会偏离实际的正见方向的 норма，因为缺乏本地细节描述。因此，我们引入了梯度向量优化（GVO），以学习基于本地平面几何的angular distance场，以重фине粗略梯度向量。最后，我们将方法拟合成为两阶段管道，首先进行粗略估计，然后进行细化。此外，我们将两个权重函数，即不同权重的kernel和准确度分数，integrated into the optimization，以提高方法的稳定性和细节描述能力。我们的方法可以高效地进行全局梯度近似，同时实现更高的准确性和地方特征描述能力。这使得我们的方法在噪声、异常点和点云变化等问题上具有更高的Robustness和普遍性。我们对广泛使用的标准准点Cloud进行了广泛的评估，并证明了我们的方法在不oriented和oriented normal estimation中具有state-of-the-art的性能。源代码和预训练模型可以在https://github.com/LeoQLi/NGLO中下载。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-SLAM-Helps-Deep-Learning-based-LiDAR-Perception-Tasks"><a href="#Differentiable-SLAM-Helps-Deep-Learning-based-LiDAR-Perception-Tasks" class="headerlink" title="Differentiable SLAM Helps Deep Learning-based LiDAR Perception Tasks"></a>Differentiable SLAM Helps Deep Learning-based LiDAR Perception Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09206">http://arxiv.org/abs/2309.09206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prashant Kumar, Dheeraj Vattikonda, Vedang Bhupesh Shenvi Nadkarni, Erqun Dong, Sabyasachi Sahoo</li>
<li>for: 这paper是为了研究一种新的自然语言处理方法，使用可微分SLAM架构来训练深度学习模型。</li>
<li>methods: 这paper使用了一种新的自然语言处理方法，即使用可微分SLAM架构来训练深度学习模型。</li>
<li>results: 实验结果表明，使用可微分SLAM架构可以提高两种深度学习应用程序（地面水平估计和动态到静止LiDAR翻译）的性能。总的来说，这些发现提供了重要的navidad的提高LiDAR基于导航系统性能的新方法。<details>
<summary>Abstract</summary>
We investigate a new paradigm that uses differentiable SLAM architectures in a self-supervised manner to train end-to-end deep learning models in various LiDAR based applications. To the best of our knowledge there does not exist any work that leverages SLAM as a training signal for deep learning based models. We explore new ways to improve the efficiency, robustness, and adaptability of LiDAR systems with deep learning techniques. We focus on the potential benefits of differentiable SLAM architectures for improving performance of deep learning tasks such as classification, regression as well as SLAM. Our experimental results demonstrate a non-trivial increase in the performance of two deep learning applications - Ground Level Estimation and Dynamic to Static LiDAR Translation, when used with differentiable SLAM architectures. Overall, our findings provide important insights that enhance the performance of LiDAR based navigation systems. We demonstrate that this new paradigm of using SLAM Loss signal while training LiDAR based models can be easily adopted by the community.
</details>
<details>
<summary>摘要</summary>
我们investigates一种新的思想，利用可微分SLAM架构来在自我超vised的方式下训练深度学习模型，用于各种LiDAR应用程序中。我们知道到目前为止，没有任何工作利用SLAM作为训练深度学习模型的信号。我们探索新的方法，以提高LiDAR系统的效率、可靠性和适应性。我们关注使用可微分SLAM架构来改善深度学习任务的性能，如分类、回归以及SLAM。我们的实验结果表明，将SLAM损失信号作为训练深度学习模型的一部分，可以提高Ground Level Estimation和Dynamic to Static LiDAR Translation两个深度学习应用程序的性能。总的来说，我们的发现提供了重要的意见，使LiDAR基于导航系统的性能得到了提高。我们示示了这新的思想可以轻松地被社区采纳。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Pyramid-Channel-Attention-Network-for-Pathological-Myopia-Detection"><a href="#Efficient-Pyramid-Channel-Attention-Network-for-Pathological-Myopia-Detection" class="headerlink" title="Efficient Pyramid Channel Attention Network for Pathological Myopia Detection"></a>Efficient Pyramid Channel Attention Network for Pathological Myopia Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09196">http://arxiv.org/abs/2309.09196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tommylitlle/epcanet">https://github.com/tommylitlle/epcanet</a></li>
<li>paper_authors: Xiaoqing Zhang, Jilu Zhao, Richu Jin, Yan Li, Hao Wu, Xiangtian Zhou, Jiang Liu</li>
<li>for: 检测普遍性近视（PM）的早期检测，以避免视力和失明。</li>
<li>methods: 利用注意模块设计，包括全面通道注意模块（EPCA），在特征图中高效地检测全球和本地病理信息。</li>
<li>results: 在三个数据集上进行了广泛的实验，证明了我们的EPCA-Net在检测PM方面的表现超过了现有方法。此外，我们还尝试了预训练和终端调整方法，并证明了与传统终端调整方法相比，我们的方法在参数更少的情况下实现了竞争力的表现。<details>
<summary>Abstract</summary>
Pathological myopia (PM) is the leading ocular disease for impaired vision and blindness worldwide. The key to detecting PM as early as possible is to detect informative features in global and local lesion regions, such as fundus tessellation, atrophy and maculopathy. However, applying classical convolutional neural networks (CNNs) to efficiently highlight global and local lesion context information in feature maps is quite challenging. To tackle this issue, we aim to fully leverage the potential of global and local lesion information with attention module design. Based on this, we propose an efficient pyramid channel attention (EPCA) module, which dynamically explores the relative importance of global and local lesion context information in feature maps. Then we combine the EPCA module with the backbone network to construct EPCA-Net for automatic PM detection based on fundus images. In addition, we construct a PM dataset termed PM-fundus by collecting fundus images of PM from publicly available datasets (e.g., the PALM dataset and ODIR dataset). The comprehensive experiments are conducted on three datasets, demonstrating that our EPCA-Net outperforms state-of-the-art methods in detecting PM. Furthermore, motivated by the recent pretraining-and-finetuning paradigm, we attempt to adapt pre-trained natural image models for PM detection by freezing them and treating the EPCA module and other attention modules as the adapters. The results show that our method with the pretraining-and-finetuning paradigm achieves competitive performance through comparisons to part of methods with traditional fine-tuning methods with fewer tunable parameters.
</details>
<details>
<summary>摘要</summary>
全球最主要的眼睛疾病之一是病理型近视（PM），它是全球视力和失明的主要原因。检测PM的关键在于检测背景和局部 lesion 区域中有用的特征。然而，使用传统的卷积神经网络（CNNs）来快速提取全球和局部 lesion 上下文信息在特征图中是很困难的。为了解决这个问题，我们想要全面利用全球和局部 lesion 信息的潜在能力，并设计了一种受注意模块（Attention Module）。基于这种设计，我们提出了一种高效的pyramid channel attention（EPCA）模块，它可以动态探索特征图中全球和局部 lesion 上下文信息的相对重要性。然后，我们将EPCA模块与背部网络结合，构建EPCA-Net以自动检测基于眼科图像的PM。此外，我们还构建了PM-fundus数据集，收集了PM眼科图像数据。我们在三个数据集上进行了广泛的实验，结果表明，我们的EPCA-Net可以超越现有方法的检测PM性能。此外，我们还尝试了采用先training-and-finetuning的方法，将预训练的自然图像模型适应PM检测。结果表明，我们的方法可以通过与一些传统 fine-tuning 方法进行比较，实现类似的性能。
</details></li>
</ul>
<hr>
<h2 id="CLIPUNetr-Assisting-Human-robot-Interface-for-Uncalibrated-Visual-Servoing-Control-with-CLIP-driven-Referring-Expression-Segmentation"><a href="#CLIPUNetr-Assisting-Human-robot-Interface-for-Uncalibrated-Visual-Servoing-Control-with-CLIP-driven-Referring-Expression-Segmentation" class="headerlink" title="CLIPUNetr: Assisting Human-robot Interface for Uncalibrated Visual Servoing Control with CLIP-driven Referring Expression Segmentation"></a>CLIPUNetr: Assisting Human-robot Interface for Uncalibrated Visual Servoing Control with CLIP-driven Referring Expression Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09183">http://arxiv.org/abs/2309.09183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Jiang, Yuchen Yang, Martin Jagersand</li>
<li>for: 这个论文目的是提高人机交互式视觉服务（UIBVS）中的图像基于视觉服务（image-based visual servoing，IBVS）精度和效率，使用 Referring Expression Segmentation（RES）技术提供更多的准确信息，以帮助机器人在 manipulate 任务中更好地理解人类的意图。</li>
<li>methods: 这个论文使用了一种新的 Referring Expression Segmentation 网络（CLIPUNetr），该网络利用 CLIP 的强大视觉语言表示能力来 segment 引用表达中的区域，同时利用其“U-shaped”编码器-解码器架构来生成更加精确的预测。此外，论文还提出了一种新的整体管道，用于将 CLIPUNetr 集成到 UIBVS 中，并在实际世界环境中应用。</li>
<li>results: 实验表明，使用 CLIPUNetr 可以提高边界和结构测量的准确性，平均提高120%，并成功地帮助实际世界中的 UIBVS 控制。<details>
<summary>Abstract</summary>
The classical human-robot interface in uncalibrated image-based visual servoing (UIBVS) relies on either human annotations or semantic segmentation with categorical labels. Both methods fail to match natural human communication and convey rich semantics in manipulation tasks as effectively as natural language expressions. In this paper, we tackle this problem by using referring expression segmentation, which is a prompt-based approach, to provide more in-depth information for robot perception. To generate high-quality segmentation predictions from referring expressions, we propose CLIPUNetr - a new CLIP-driven referring expression segmentation network. CLIPUNetr leverages CLIP's strong vision-language representations to segment regions from referring expressions, while utilizing its ``U-shaped'' encoder-decoder architecture to generate predictions with sharper boundaries and finer structures. Furthermore, we propose a new pipeline to integrate CLIPUNetr into UIBVS and apply it to control robots in real-world environments. In experiments, our method improves boundary and structure measurements by an average of 120% and can successfully assist real-world UIBVS control in an unstructured manipulation environment.
</details>
<details>
<summary>摘要</summary>
传统的人机交互界面在无调整图像基于视觉服务（UIBVS）中依赖于人类注释或semantic segmentation WITH categorical标签。这两种方法无法匹配人类自然的沟通方式，并且不能够具备rich semantics在操作任务中。在这篇论文中，我们解决这个问题，使用引用表达分 segmentation，以提供更多的信息来提高机器人的感知。为生成高质量的分 segmentation预测，我们提出了CLIPUNetr，一种基于CLIP的引用表达分 segmentation网络。CLIPUNetr利用CLIP的强视语表示能力，从引用表达中提取区域，同时利用其“U字形”编码器-解码器架构，生成预测更加精细的边界和结构。此外，我们提出了一种新的管道，将CLIPUNetr纳入UIBVS中，并在实际世界环境中控制机器人。在实验中，我们发现，我们的方法可以提高边界和结构测量的平均值120%，并成功地帮助实际世界中的UIBVS控制。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.CV_2023_09_17/" data-id="cloimip9600hms48886a2c94g" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.AI_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T12:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.AI_2023_09_17/">cs.AI - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ChatGPT-Hallucinates-when-Attributing-Answers"><a href="#ChatGPT-Hallucinates-when-Attributing-Answers" class="headerlink" title="ChatGPT Hallucinates when Attributing Answers"></a>ChatGPT Hallucinates when Attributing Answers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09401">http://arxiv.org/abs/2309.09401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guido Zuccon, Bevan Koopman, Razia Shaik</li>
<li>for: The paper aims to investigate the ability of ChatGPT to provide evidence to support its answers and to analyze the quality of the references it suggests.</li>
<li>methods: The paper uses a collection of domain-specific knowledge-based questions to prompt ChatGPT to provide answers and supporting evidence in the form of references to external sources.</li>
<li>results: The paper finds that ChatGPT provides correct or partially correct answers in about half of the cases (50.6% of the times), but its suggested references only exist 14% of the times. The generated references reveal common traits and often do not support the claims ChatGPT attributes to them.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是调查ChatGPT是否能够提供证据支持其答案，以及其建议的参考文献的质量。</li>
<li>methods: 论文使用具体领域知识基础问题来让ChatGPT提供答案和相关参考文献。</li>
<li>results: 论文发现ChatGPT在50.6%的情况下提供正确或部分正确的答案，但建议的参考文献只有14%存在。生成的参考文献显示出共同特征，并常常不支持ChatGPT所归功于它们的说法。<details>
<summary>Abstract</summary>
Can ChatGPT provide evidence to support its answers? Does the evidence it suggests actually exist and does it really support its answer? We investigate these questions using a collection of domain-specific knowledge-based questions, specifically prompting ChatGPT to provide both an answer and supporting evidence in the form of references to external sources. We also investigate how different prompts impact answers and evidence. We find that ChatGPT provides correct or partially correct answers in about half of the cases (50.6% of the times), but its suggested references only exist 14% of the times. We further provide insights on the generated references that reveal common traits among the references that ChatGPT generates, and show how even if a reference provided by the model does exist, this reference often does not support the claims ChatGPT attributes to it. Our findings are important because (1) they are the first systematic analysis of the references created by ChatGPT in its answers; (2) they suggest that the model may leverage good quality information in producing correct answers, but is unable to attribute real evidence to support its answers. Prompts, raw result files and manual analysis are made publicly available.
</details>
<details>
<summary>摘要</summary>
Can ChatGPT 提供证据支持其答案？Does the evidence it suggests 真的存在，并且确实支持其答案？我们通过一个领域专门知识基础的问题集来调查这些问题，具体来说是让 ChatGPT 提供答案和证据的形式为外部源引用。我们还发现了不同的提问对答案和证据的影响。我们发现 ChatGPT 在50.6% 的情况下提供正确或部分正确的答案，但是提供的参考文献只有14% 的情况下存在。我们进一步分析生成的参考文献，发现这些参考文献具有共同特征，并且即使参考文献确实存在，它们通常不支持 ChatGPT 所归功于它们的说法。我们的发现对（1）是首次系统性地分析 ChatGPT 答案中的参考文献，（2）表明模型可能在生成正确答案时利用了好几个信息，但是无法归功于它们的证据。我们提供的提问、 raw 结果文件和手动分析将公开发布。
</details></li>
</ul>
<hr>
<h2 id="CulturaX-A-Cleaned-Enormous-and-Multilingual-Dataset-for-Large-Language-Models-in-167-Languages"><a href="#CulturaX-A-Cleaned-Enormous-and-Multilingual-Dataset-for-Large-Language-Models-in-167-Languages" class="headerlink" title="CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages"></a>CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09400">http://arxiv.org/abs/2309.09400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen</li>
<li>for: 这个论文的目的是为了提高大型自然语言处理（LLM）模型的学习能力，并且将其让到公众使用，以促进更深入的研究和应用。</li>
<li>methods: 该论文使用了严格的整理和筛选程序来清洁和精确地准确地训练大型自然语言处理模型。</li>
<li>results: 该论文发现，这些精确地训练的大型自然语言处理模型在多种语言中的表现优化，并且可以用于多种应用。<details>
<summary>Abstract</summary>
The driving factors behind the development of large language models (LLMs) with impressive learning capabilities are their colossal model sizes and extensive training datasets. Along with the progress in natural language processing, LLMs have been frequently made accessible to the public to foster deeper investigation and applications. However, when it comes to training datasets for these LLMs, especially the recent state-of-the-art models, they are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is fully released to the public in HuggingFace to facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的发展因素包括其巨大的模型大小和广泛的训练数据。随着自然语言处理的进步，LLMs 经常被公开提供，以便更深入的研究和应用。然而，LLMs 的训练数据的进展却有一定的障碍。特别是最新的状态艺术模型，其训练数据经常不是完全公布的。为了创建高性能 LLMS 的训练数据，需要进行广泛的清洗和去重，以确保必要的质量水平。由于训练数据的不透明度，因此对 LLMS 的幻觉和偏见问题的研究和解决受到了阻碍，这也阻碍了社区的进一步发展。在多语言学习场景下，可以用的多语言文本数据库往往不够，而且清洗和去重的过程也往往不充分。为了解决这个问题，我们介绍了 CulturaX，一个具有6.3亿个字的167种语言的大型多语言数据集，适用于 LLMS 的开发。我们的数据集经过了多 Stage 的严格清洗和去重管道，包括语言标识、URL 基于的筛选、度量基于的清洗、文档级别和数据去重，以确保模型训练时的最佳质量。CulturaX 被完全公开发布到 HuggingFace，以便社区的研究和发展：https://huggingface.co/datasets/uonlp/CulturaX。
</details></li>
</ul>
<hr>
<h2 id="Do-Large-GPT-Models-Discover-Moral-Dimensions-in-Language-Representations-A-Topological-Study-Of-Sentence-Embeddings"><a href="#Do-Large-GPT-Models-Discover-Moral-Dimensions-in-Language-Representations-A-Topological-Study-Of-Sentence-Embeddings" class="headerlink" title="Do Large GPT Models Discover Moral Dimensions in Language Representations? A Topological Study Of Sentence Embeddings"></a>Do Large GPT Models Discover Moral Dimensions in Language Representations? A Topological Study Of Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09397">http://arxiv.org/abs/2309.09397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Fitz</li>
<li>for: 这篇论文的目的是研究基于GPT-3.5的语言模型 internal structure，以及这些模型在训练过程中对公平性的影响。</li>
<li>methods: 该论文使用了GPT-3.5语言模型的基础模型，并使用了社会心理学 литературе中的公平度度量来分析模型的内部结构。</li>
<li>results: 研究发现，基于GPT-3.5的语言模型可以将 sentences embedding decomposed into two submanifolds，表示公平和不公平的道德判断。这表明GPT-基于语言模型在其表示空间中发展了道德维度，并在训练过程中学习了公平性的概念。<details>
<summary>Abstract</summary>
As Large Language Models are deployed within Artificial Intelligence systems, that are increasingly integrated with human society, it becomes more important than ever to study their internal structures. Higher level abilities of LLMs such as GPT-3.5 emerge in large part due to informative language representations they induce from raw text data during pre-training on trillions of words. These embeddings exist in vector spaces of several thousand dimensions, and their processing involves mapping between multiple vector spaces, with total number of parameters on the order of trillions. Furthermore, these language representations are induced by gradient optimization, resulting in a black box system that is hard to interpret. In this paper, we take a look at the topological structure of neuronal activity in the "brain" of Chat-GPT's foundation language model, and analyze it with respect to a metric representing the notion of fairness. We develop a novel approach to visualize GPT's moral dimensions. We first compute a fairness metric, inspired by social psychology literature, to identify factors that typically influence fairness assessments in humans, such as legitimacy, need, and responsibility. Subsequently, we summarize the manifold's shape using a lower-dimensional simplicial complex, whose topology is derived from this metric. We color it with a heat map associated with this fairness metric, producing human-readable visualizations of the high-dimensional sentence manifold. Our results show that sentence embeddings based on GPT-3.5 can be decomposed into two submanifolds corresponding to fair and unfair moral judgments. This indicates that GPT-based language models develop a moral dimension within their representation spaces and induce an understanding of fairness during their training process.
</details>
<details>
<summary>摘要</summary>
As Large Language Models (LLMs) 在人工智能系统中部署，与人类社会越来越紧密相连，研究其内部结构变得更加重要。 GPT-3.5 等高级 LLM 的高级能力在大部分由 Raw text data 中抽象出的语言表示所带来，这些表示在 vector space 中存在数千维度的向量空间中，并且处理涉及多个 vector space 之间的映射，总参数数在万亿级别。此外，这些语言表示是通过梯度优化来实现的黑盒系统，具有很难理解的特点。在这篇论文中，我们将研究 Chat-GPT 的基础语言模型中神经元活动的 topological structure，并对其进行分析，以了解它们是如何实现公平性的。我们开发了一种新的方法，可以将 GPT 的道德维度视觉化。我们首先计算了一种公平度量， drawing inspiration from social psychology literature，以确定在人类中常见的公平评价因素，如合法性、需求和责任。然后，我们将 manifold 的形状摘要为一个几何体，其Topology是基于这种公平度量来确定的。我们将这个几何体用一个与公平度量相关的热图进行颜色标记，从而生成可读的 visualization 图像，以便更好地理解高级语言模型中的道德维度。我们的结果表明，基于 GPT-3.5 的 sentence embeddings 可以分解为两个子 manifold，每个子 manifold 都表示一种公平或不公平的道德评价。这表明 GPT 基于的语言模型在其表示空间中发展了道德维度，并在训练过程中学习了公平性。
</details></li>
</ul>
<hr>
<h2 id="Talk2Care-Facilitating-Asynchronous-Patient-Provider-Communication-with-Large-Language-Model"><a href="#Talk2Care-Facilitating-Asynchronous-Patient-Provider-Communication-with-Large-Language-Model" class="headerlink" title="Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model"></a>Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09357">http://arxiv.org/abs/2309.09357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Yang, Xuhai Xu, Bingsheng Yao, Shao Zhang, Ethan Rogers, Stephen Intille, Nawar Shara, Guodong, Gao, Dakuo Wang</li>
<li>for: 这个研究的目的是为了探索大语言模型（LLMs）在长期健康照顾和跨代通信中的潜在作用。</li>
<li>methods: 这个研究使用了两个访谈研究，一个是older adults（N&#x3D;10），另一个是健康提供者（N&#x3D;9），以了解他们对LLMs在患者-医疗提供者异步通信中的需求和机会。然后，他们根据这些洞察，建立了一个LLM-powered通信系统，名为Talk2Care，并设计了与两个群体互动的元件：一个是为older adults，他们使用语音助手（VAs）来实现有效的资讯收集；另一个是为健康提供者，他们建立了LLM-基于的互动画面，以 SUMMARIZE和显示老年人对VAs的访谈中重要的健康资讯。</li>
<li>results: 这两个使用者研究显示，Talk2Care可以帮助患者和医疗提供者之间的通信 процес，增加患者对健康信息的收集，并对医疗提供者的努力和时间节省几成。我们视这个研究为LLMs在医疗和人际通信之间的初步探索。<details>
<summary>Abstract</summary>
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults' conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers' efforts and time. We envision our work as an initial exploration of LLMs' capability in the intersection of healthcare and interpersonal communication.
</details>
<details>
<summary>摘要</summary>
尽管现有许多电健康应用程序可以帮助家庭older adults和医疗提供者，但是基本的消息和电话仍然是最常用的通信方法，这些方法受到有限的可用性、信息损失和流程不充分的限制。一种有前途的解决方案是利用大语言模型（LLMs），它们具有强大的自然语言对话和总结能力。然而，对LLMs在通信中的角色的理解还很有限。我们首先进行了10名older adults和9名医疗提供者的两次采访，以了解他们的需求和LLMs在患者-医生异步通信中的机会。基于这些发现，我们建立了一个LLM-powered通信系统，名为Talk2Care，并为两个群体设计了互动组件：1.  дляolder adults，我们利用了voice assistant（VA）的便捷和可达性，并为他们建立了LLM-powered VA界面，以便有效地收集健康信息。2.  для医疗提供者，我们建立了LLM基于的概要摘要界面，以显示older adults和VA之间的对话中重要的医疗信息。我们进行了两次用户研究，以评估Talk2Care的可用性。结果显示，Talk2Care可以改善患者-医生的通信过程，拓宽来自older adults的健康信息，并大幅减少医疗提供者的努力和时间。我们认为，我们的工作是LLMs在医疗和人际通信的交叉点的初步探索。
</details></li>
</ul>
<hr>
<h2 id="Speech-Gesture-GAN-Gesture-Generation-for-Robots-and-Embodied-Agents"><a href="#Speech-Gesture-GAN-Gesture-Generation-for-Robots-and-Embodied-Agents" class="headerlink" title="Speech-Gesture GAN: Gesture Generation for Robots and Embodied Agents"></a>Speech-Gesture GAN: Gesture Generation for Robots and Embodied Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09346">http://arxiv.org/abs/2309.09346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carson Yu Liu, Gelareh Mohammadi, Yang Song, Wafa Johal</li>
<li>for: 这 paper 的目的是为了帮助机器人和embodied agents在人类与人类交互中更好地表达他们的态度、情感和意图。</li>
<li>methods: 这 paper 使用了一种基于 Conditional Generative Adversarial Network (GAN) 的神经网络模型，学习了语音输入中的协作姿势和语音特征之间的关系。</li>
<li>results: 试验结果表明，这个姿势生成框架可以帮助机器人和embodied agents更好地与人类交互，并且可以在对话中表达他们的态度和情感。<details>
<summary>Abstract</summary>
Embodied agents, in the form of virtual agents or social robots, are rapidly becoming more widespread. In human-human interactions, humans use nonverbal behaviours to convey their attitudes, feelings, and intentions. Therefore, this capability is also required for embodied agents in order to enhance the quality and effectiveness of their interactions with humans. In this paper, we propose a novel framework that can generate sequences of joint angles from the speech text and speech audio utterances. Based on a conditional Generative Adversarial Network (GAN), our proposed neural network model learns the relationships between the co-speech gestures and both semantic and acoustic features from the speech input. In order to train our neural network model, we employ a public dataset containing co-speech gestures with corresponding speech audio utterances, which were captured from a single male native English speaker. The results from both objective and subjective evaluations demonstrate the efficacy of our gesture-generation framework for Robots and Embodied Agents.
</details>
<details>
<summary>摘要</summary>
现代智能机器人和虚拟代理人正在广泛应用。人类在人际交流中使用非语言行为表达自己的态度、情感和意图。因此，这种能力也是智能机器人需要的，以提高与人类交流的质量和效率。在这篇论文中，我们提出了一种新的姿势生成框架，可以根据语音文本和语音音频utterances生成肢体姿势。我们的提议的神经网络模型学习了语音输入中的听力和语义特征与手势之间的关系。为了训练我们的神经网络模型，我们使用了一个公共数据集，包括与语音音频utterances对应的手势记录，从单一的男性Native English speaker中获取。经过对象和主观评估，我们的姿势生成框架在机器人和虚拟代理人中得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Dynamic-Mode-Decomposition-and-Deep-Learning-for-Rainfall-Prediction-in-North-East-India"><a href="#Unleashing-the-Power-of-Dynamic-Mode-Decomposition-and-Deep-Learning-for-Rainfall-Prediction-in-North-East-India" class="headerlink" title="Unleashing the Power of Dynamic Mode Decomposition and Deep Learning for Rainfall Prediction in North-East India"></a>Unleashing the Power of Dynamic Mode Decomposition and Deep Learning for Rainfall Prediction in North-East India</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09336">http://arxiv.org/abs/2309.09336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paleti Nikhil Chowdary, Sathvika P, Pranav U, Rohan S, Sowmya V, Gopalakrishnan E A, Dhanya M</li>
<li>for: 预测北东部印度的降水量，提高灾害防御和减轻气候变化的影响</li>
<li>methods: 使用数据驱动方法Dynamic Mode Decomposition (DMD)和深度学习方法Long Short-Term Memory (LSTM)进行降水量预测，使用印度气象部门每天降水数据进行训练和验证</li>
<li>results: LSTM方法比DMD方法更加准确地预测降水量，表明LSTM方法可以更好地捕捉数据中的复杂非线性关系，这些发现可以帮助提高北东部印度的降水量预测精度，降低气候变化的影响<details>
<summary>Abstract</summary>
Accurate rainfall forecasting is crucial for effective disaster preparedness and mitigation in the North-East region of India, which is prone to extreme weather events such as floods and landslides. In this study, we investigated the use of two data-driven methods, Dynamic Mode Decomposition (DMD) and Long Short-Term Memory (LSTM), for rainfall forecasting using daily rainfall data collected from India Meteorological Department in northeast region over a period of 118 years. We conducted a comparative analysis of these methods to determine their relative effectiveness in predicting rainfall patterns. Using historical rainfall data from multiple weather stations, we trained and validated our models to forecast future rainfall patterns. Our results indicate that both DMD and LSTM are effective in forecasting rainfall, with LSTM outperforming DMD in terms of accuracy, revealing that LSTM has the ability to capture complex nonlinear relationships in the data, making it a powerful tool for rainfall forecasting. Our findings suggest that data-driven methods such as DMD and deep learning approaches like LSTM can significantly improve rainfall forecasting accuracy in the North-East region of India, helping to mitigate the impact of extreme weather events and enhance the region's resilience to climate change.
</details>
<details>
<summary>摘要</summary>
准确预测降水是北东地区灾害准备和mitigation的关键，这里容易受到洪水和山崩等极端天气事件的影响。在这项研究中，我们调查了使用两种数据驱动方法：动态模式分解（DMD）和长期记忆（LSTM），以预测降水。我们使用印度气象部门在北东地区收集的日常降水数据进行训练和验证。我们的结果表明，DMD和LSTM都有效地预测降水，但LSTM在准确性方面表现更好，表明LSTM可以捕捉数据中的复杂非线性关系，使其成为预测降水的强大工具。我们的发现表明，使用数据驱动方法如DMD和深度学习方法如LSTM可以大幅提高降水预测精度，帮助北东地区 Mitigate the impact of extreme weather events and enhance its resilience to climate change.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Knee-Osteoarthritis-severity-level-classification-using-diffusion-augmented-images"><a href="#Enhancing-Knee-Osteoarthritis-severity-level-classification-using-diffusion-augmented-images" class="headerlink" title="Enhancing Knee Osteoarthritis severity level classification using diffusion augmented images"></a>Enhancing Knee Osteoarthritis severity level classification using diffusion augmented images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09328">http://arxiv.org/abs/2309.09328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AKSHAY24-tech/Enhancing-Knee-Osteoarthritis-severity-level-classification-using-diffusion-augmented-images">https://github.com/AKSHAY24-tech/Enhancing-Knee-Osteoarthritis-severity-level-classification-using-diffusion-augmented-images</a></li>
<li>paper_authors: Paleti Nikhil Chowdary, Gorantla V N S L Vishnu Vardhan, Menta Sai Akshay, Menta Sai Aashish, Vadlapudi Sai Aravind, Garapati Venkata Krishna Rayalu, Aswathy P</li>
<li>for: 这个研究 paper 探讨了使用高级计算机视觉模型和扩充技术来分类膝部骨关节炎（OA）严重程度。</li>
<li>methods: 该研究使用了数据预处理，包括强化对比限定静止 histogram 平衡（CLAHE），以及数据扩充使用扩散模型。三个实验进行了：在原始数据集上训练模型，在预处理后的数据集上训练模型，以及在扩充后的数据集上训练模型。</li>
<li>results: 结果显示，数据预处理和扩充可以大幅提高模型的准确率。EfficientNetB3 模型在扩充后的数据集上达到了 84% 的最高准确率。此外，使用 Grad-CAM 等注意力视觉技术，可以提供详细的注意力地图，提高了模型的理解和信任性。这些发现指出，将高级模型与扩充数据和注意力视觉相结合，可以准确地分类膝部骨关节炎严重程度。<details>
<summary>Abstract</summary>
This research paper explores the classification of knee osteoarthritis (OA) severity levels using advanced computer vision models and augmentation techniques. The study investigates the effectiveness of data preprocessing, including Contrast-Limited Adaptive Histogram Equalization (CLAHE), and data augmentation using diffusion models. Three experiments were conducted: training models on the original dataset, training models on the preprocessed dataset, and training models on the augmented dataset. The results show that data preprocessing and augmentation significantly improve the accuracy of the models. The EfficientNetB3 model achieved the highest accuracy of 84\% on the augmented dataset. Additionally, attention visualization techniques, such as Grad-CAM, are utilized to provide detailed attention maps, enhancing the understanding and trustworthiness of the models. These findings highlight the potential of combining advanced models with augmented data and attention visualization for accurate knee OA severity classification.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇研究论文探讨了使用高级计算机视觉模型和扩充技术来分类膝关节风扁病（OA）严重程度的方法。研究检查了数据预处理，包括对比限适的自适应压缩（CLAHE），以及使用扩充模型来进行数据扩充。研究进行了三个实验：在原始数据集上训练模型，在预处理后的数据集上训练模型，以及在扩充后的数据集上训练模型。结果显示，数据预处理和扩充可以显著提高模型的准确率。EfficientNetB3模型在扩充后的数据集上达到了84%的最高准确率。此外，使用Grad-CAM等注意力可视化技术，为模型提供了详细的注意力地图，提高了对模型的理解和信任性。这些发现表明可以通过结合高级模型、扩充数据和注意力可视化来实现精准的膝关节风扁病严重程度分类。
</details></li>
</ul>
<hr>
<h2 id="Answering-Layer-3-queries-with-DiscoSCMs"><a href="#Answering-Layer-3-queries-with-DiscoSCMs" class="headerlink" title="Answering Layer 3 queries with DiscoSCMs"></a>Answering Layer 3 queries with DiscoSCMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09323">http://arxiv.org/abs/2309.09323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heyang Gong</li>
<li>For: This paper aims to address the issue of counterfactual degeneration in causal inference, specifically in the context of Layer 3 valuations and individual-level semantics.* Methods: The paper proposes a novel framework called DiscoSCM, which combines the strengths of both Potential Outcome (PO) and Structural Causal Model (SCM) frameworks, and could be seen as an extension of them. The DiscoSCM framework leverages the philosophy of individual causality to tackle the counterfactual degeneration problem.* Results: The paper demonstrates the superior performance of the DiscoSCM framework in answering counterfactual questions through several key results in the topic of unit select problems. The results show that the DiscoSCM framework can effectively address the issue of counterfactual degeneration and provide more accurate estimates of counterfactual parameters.<details>
<summary>Abstract</summary>
In the realm of causal inference, the primary frameworks are the Potential Outcome (PO) and the Structural Causal Model (SCM), both predicated on the consistency rule. However, when facing Layer 3 valuations, i.e., counterfactual queries that inherently belong to individual-level semantics, they both seem inadequate due to the issue of degeneration caused by the consistency rule. For instance, in personalized incentive scenarios within the internet industry, the probability of one particular user being a complier, denoted as $P(y_x, y'_{x'})$, degenerates to a parameter that can only take values of 0 or 1. This paper leverages the DiscoSCM framework to theoretically tackle the aforementioned counterfactual degeneration problem, which is a novel framework for causal modeling that combines the strengths of both PO and SCM, and could be seen as an extension of them. The paper starts with a brief introduction to the background of causal modeling frameworks. It then illustrates, through an example, the difficulty in recovering counterfactual parameters from data without imposing strong assumptions. Following this, we propose the DiscoSCM with independent potential noise framework to address this problem. Subsequently, the superior performance of the DiscoSCM framework in answering counterfactual questions is demonstrated by several key results in the topic of unit select problems. We then elucidate that this superiority stems from the philosophy of individual causality. In conclusion, we suggest that DiscoSCM may serve as a significant milestone in the causal modeling field for addressing counterfactual queries.
</details>
<details>
<summary>摘要</summary>
在 causal inference 领域，主要框架是 potential outcome (PO) 和 structural causal model (SCM)，都是基于一致性规则。但在面临层 3 评估（counterfactual queries）时，它们都显得不够，这是因为一致性规则导致的半极化问题。例如，在个性化奖励场景下，用户 $x$ 的行为 $y_x$ 的潜在结果 $P(y_x, y'_{x'})$ 会半极化为一个只能取 0 或 1 的参数。本文使用 DiscoSCM 框架来解决上述 counterfactual 半极化问题，这是一种结合 PO 和 SCM 的新框架，可以看作是它们的扩展。文章首先介绍了 causal modeling 框架的背景，然后通过一个例子示出在数据中无法回归 counterfactual 参数的困难。接着，我们提出了 DiscoSCM 独立潜在噪声框架来解决这个问题。文章后续展示了 DiscoSCM 框架在 unit select 问题上的优秀表现，并证明了这种优秀性源于个体 causality 哲学。最后，我们建议 DiscoSCM 可能是 causal modeling 领域内 Answering counterfactual questions 的一个重要突破口。
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-for-Semantic-Segmentation-with-Multi-class-Label-Query"><a href="#Active-Learning-for-Semantic-Segmentation-with-Multi-class-Label-Query" class="headerlink" title="Active Learning for Semantic Segmentation with Multi-class Label Query"></a>Active Learning for Semantic Segmentation with Multi-class Label Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09319">http://arxiv.org/abs/2309.09319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sehyun Hwang, Sohyun Lee, Hoyoung Kim, Minhyeon Oh, Jungseul Ok, Suha Kwak</li>
<li>for: 提出了一种新的活动学习方法 дляsemantic segmentation</li>
<li>methods: 使用了一种新的标注查询设计，采样了本地图像区域（例如超 pix），并向 oracle 请求每个区域的多类标签vector，以解决存在多类标签问题</li>
<li>results: 在Cityscapes和PASCAL VOC 2012上比前一个方法减少了标注成本，并且达到了更高的 segmentation 性能<details>
<summary>Abstract</summary>
This paper proposes a new active learning method for semantic segmentation. The core of our method lies in a new annotation query design. It samples informative local image regions (e.g., superpixels), and for each of such regions, asks an oracle for a multi-hot vector indicating all classes existing in the region. This multi-class labeling strategy is substantially more efficient than existing ones like segmentation, polygon, and even dominant class labeling in terms of annotation time per click. However, it introduces the class ambiguity issue in training since it assigns partial labels (i.e., a set of candidate classes) to individual pixels. We thus propose a new algorithm for learning semantic segmentation while disambiguating the partial labels in two stages. In the first stage, it trains a segmentation model directly with the partial labels through two new loss functions motivated by partial label learning and multiple instance learning. In the second stage, it disambiguates the partial labels by generating pixel-wise pseudo labels, which are used for supervised learning of the model. Equipped with a new acquisition function dedicated to the multi-class labeling, our method outperformed previous work on Cityscapes and PASCAL VOC 2012 while spending less annotation cost.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>In the first stage, the method trains a segmentation model directly with the partial labels using two new loss functions inspired by partial label learning and multiple instance learning.2. In the second stage, the method disambiguates the partial labels by generating pixel-wise pseudo labels, which are used for supervised learning of the model.The proposed method is equipped with a new acquisition function tailored to the multi-class labeling, which leads to better performance on Cityscapes and PASCAL VOC 2012 while reducing the annotation cost.In simplified Chinese, the text can be translated as:这篇论文提出了一种新的活动学习方法 дляsemantic segmentation。这个方法的核心在于一种新的注释查询设计，它将地图区域（例如superpixel）作为批处理，并问 oracle 提供多个类别的多hot вектор。这种多类标注策略比传统的分类、多边形和主导类标注更加高效，但是它会在训练中引入类别不确定性问题，因为每个像素都会被分配一组候选类。为了解决这个问题，该方法提出了两个阶段的方法：1. 在第一阶段，方法直接使用多个类别的partial label进行分类模型的训练，使用两种新的损失函数，启发自partial label学习和多个实例学习。2. 在第二阶段，方法使用像素级别的pseudo标签来解决类别不确定性问题，并将其用于模型的超级vised学习。该方法采用了一种专门为多类标注而设计的新的收购函数，使其在Cityscapes和PASCAL VOC 2012上表现优于之前的工作，同时减少了注释成本。</details></li>
</ol>
<hr>
<h2 id="Deep-Neighbor-Layer-Aggregation-for-Lightweight-Self-Supervised-Monocular-Depth-Estimation"><a href="#Deep-Neighbor-Layer-Aggregation-for-Lightweight-Self-Supervised-Monocular-Depth-Estimation" class="headerlink" title="Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation"></a>Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09272">http://arxiv.org/abs/2309.09272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boya Wang, Shuo Wang, Ziwen Dou, Dong Ye</li>
<li>for: 提高自主导航和机器人视觉系统中 depth estimation 模型的效率，避免使用大型和复杂的网络。</li>
<li>methods: 使用全连接层 fusion 技术，将高分辨率和低分辨率特征相互融合，以保留小目标和快移动物体的信息。采用轻量级频道注意力 Mechanism 来提高depth estimation结果。</li>
<li>results: 在 KITTI 数据集上进行实验，与许多大型模型相比，如 Monodepth2，我们的方法可以达到更高的准确率，仅使用 30 个参数。<details>
<summary>Abstract</summary>
With the frequent use of self-supervised monocular depth estimation in robotics and autonomous driving, the model's efficiency is becoming increasingly important. Most current approaches apply much larger and more complex networks to improve the precision of depth estimation. Some researchers incorporated Transformer into self-supervised monocular depth estimation to achieve better performance. However, this method leads to high parameters and high computation. We present a fully convolutional depth estimation network using contextual feature fusion. Compared to UNet++ and HRNet, we use high-resolution and low-resolution features to reserve information on small targets and fast-moving objects instead of long-range fusion. We further promote depth estimation results employing lightweight channel attention based on convolution in the decoder stage. Our method reduces the parameters without sacrificing accuracy. Experiments on the KITTI benchmark show that our method can get better results than many large models, such as Monodepth2, with only 30 parameters. The source code is available at https://github.com/boyagesmile/DNA-Depth.
</details>
<details>
<summary>摘要</summary>
随着自主导航和机器人领域中深度估计的频繁使用，模型的效率变得越来越重要。现有大多数方法使用更大和更复杂的网络来提高深度估计的精度。一些研究人员在自主导航中采用了Transformer来提高性能，但这会导致高参数和高计算量。我们提出了一种完全 convolutional 的深度估计网络，通过Contextual feature fusion来提高性能。我们使用高分辨率和低分辨率的特征来保留小目标和快速移动的信息，而不是长距离融合。我们还使用轻量级的通道注意力来提高decoder stage中的深度估计结果。我们的方法可以降低参数量不 sacrificing 精度。在 KITTI 标准测试集上，我们的方法可以超越许多大型模型，如 Monodepth2，并且只需30个参数。代码可以在 https://github.com/boyagesmile/DNA-Depth 上获取。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Modeling-of-the-Denoising-Process-for-Speech-Enhancement-Based-on-Deep-Learning"><a href="#Continuous-Modeling-of-the-Denoising-Process-for-Speech-Enhancement-Based-on-Deep-Learning" class="headerlink" title="Continuous Modeling of the Denoising Process for Speech Enhancement Based on Deep Learning"></a>Continuous Modeling of the Denoising Process for Speech Enhancement Based on Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09270">http://arxiv.org/abs/2309.09270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilu Guo, Jun Du, CHin-Hui Lee</li>
<li>for: 这个论文旨在提出一种连续模型基于深度学习的语音听写提升方法，关注听写过程中的干净化。</li>
<li>methods: 这个方法使用状态变量来表示听写过程，起始状态是噪声语音，结束状态是干净的语音。噪声分量在状态变量中随状态索引的变化而减少，直到噪声分量为0。在训练中，一个UNet-like神经网络学习估算每个状态变量，从连续听写过程中抽取样本。在测试中，我们引入一个控制因子作为嵌入，让神经网络控制噪声减少的水平。这种方法实现可控的语音提升和适应不同应用场景。</li>
<li>results: 实验结果表明，保留一小amount的噪声在干净目标中对语音提升具有利于效果，证明了对语音评价指标和自动语音识别性能的改进。<details>
<summary>Abstract</summary>
In this paper, we explore a continuous modeling approach for deep-learning-based speech enhancement, focusing on the denoising process. We use a state variable to indicate the denoising process. The starting state is noisy speech and the ending state is clean speech. The noise component in the state variable decreases with the change of the state index until the noise component is 0. During training, a UNet-like neural network learns to estimate every state variable sampled from the continuous denoising process. In testing, we introduce a controlling factor as an embedding, ranging from zero to one, to the neural network, allowing us to control the level of noise reduction. This approach enables controllable speech enhancement and is adaptable to various application scenarios. Experimental results indicate that preserving a small amount of noise in the clean target benefits speech enhancement, as evidenced by improvements in both objective speech measures and automatic speech recognition performance.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了深度学习基于的连续模型化 speech 减噪方法，强调减噪过程。我们使用状态变量来表示减噪过程。起始状态是噪声的 speech，结束状态是干净的 speech。噪声组分在状态变量中随状态索引的变化而减少，直到噪声组分为 0。在训练中，一种 UNet-like 神经网络学习 estimate 每个来自连续减噪过程的状态变量。在测试中，我们将一个控制因子作为嵌入，范围从 0 到 1，提供给神经网络，以控制噪声减少的水平。这种方法允许可控制的 speech 减噪和适应不同应用场景。实验结果表明，保留一些噪声在干净目标中有助于 speech 减噪，证明了对象测试 Speech 测量指标和自动语音识别性能的改进。
</details></li>
</ul>
<hr>
<h2 id="Sim-to-Real-Deep-Reinforcement-Learning-with-Manipulators-for-Pick-and-place"><a href="#Sim-to-Real-Deep-Reinforcement-Learning-with-Manipulators-for-Pick-and-place" class="headerlink" title="Sim-to-Real Deep Reinforcement Learning with Manipulators for Pick-and-place"></a>Sim-to-Real Deep Reinforcement Learning with Manipulators for Pick-and-place</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09247">http://arxiv.org/abs/2309.09247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxing Liu, Hanlin Niu, Robert Skilton, Joaquin Carrasco</li>
<li>For: This paper proposes a self-supervised vision-based deep reinforcement learning (DRL) method to improve the performance of transferring a DRL model from simulation to the real world.* Methods: The proposed method uses a height-sensitive action policy to deal with crowded and stacked objects in challenging environments. The training model is applied directly to a real suction task without any fine-tuning from the real world.* Results: The proposed method achieves a high suction success rate of 90% in a real experiment with novel objects, without any real-world fine-tuning. An experimental video is available at: <a target="_blank" rel="noopener" href="https://youtu.be/jSTC-EGsoFA.Here">https://youtu.be/jSTC-EGsoFA.Here</a> are the three key points in Simplified Chinese:* For: 这篇论文提出了一种基于自我监督视觉深度学习（DRL）方法，以提高将DRL模型从模拟世界转移到实际世界的性能。* Methods: 该方法使用了高度敏感的动作策略来处理充满杂物和堆叠物的复杂环境。模型直接应用于实际吸取任务，不需要任何实际世界细调。* Results: 该方法在实际实验中以90%的吸取成功率成功地应用于新的物体，无需任何实际世界细调。实验视频可以在：<a target="_blank" rel="noopener" href="https://youtu.be/jSTC-EGsoFA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://youtu.be/jSTC-EGsoFA中找到。</a><details>
<summary>Abstract</summary>
When transferring a Deep Reinforcement Learning model from simulation to the real world, the performance could be unsatisfactory since the simulation cannot imitate the real world well in many circumstances. This results in a long period of fine-tuning in the real world. This paper proposes a self-supervised vision-based DRL method that allows robots to pick and place objects effectively and efficiently when directly transferring a training model from simulation to the real world. A height-sensitive action policy is specially designed for the proposed method to deal with crowded and stacked objects in challenging environments. The training model with the proposed approach can be applied directly to a real suction task without any fine-tuning from the real world while maintaining a high suction success rate. It is also validated that our model can be deployed to suction novel objects in a real experiment with a suction success rate of 90\% without any real-world fine-tuning. The experimental video is available at: https://youtu.be/jSTC-EGsoFA.
</details>
<details>
<summary>摘要</summary>
Note:* " simulation" => 模拟 (mó dì)* "real world"  => 真实世界 (zhēn shí shì jì)* "fine-tuning"  => 微调 (wēi tiān)* "success rate"  => 成功率 (chéng gōng ràng)* "suction"  => 吸引 (xīu yì)* "novel objects"  => 新型物品 (xīn xíng wù jī)
</details></li>
</ul>
<hr>
<h2 id="Detection-and-Localization-of-Firearm-Carriers-in-Complex-Scenes-for-Improved-Safety-Measures"><a href="#Detection-and-Localization-of-Firearm-Carriers-in-Complex-Scenes-for-Improved-Safety-Measures" class="headerlink" title="Detection and Localization of Firearm Carriers in Complex Scenes for Improved Safety Measures"></a>Detection and Localization of Firearm Carriers in Complex Scenes for Improved Safety Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09236">http://arxiv.org/abs/2309.09236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intelligentMachines-ITU/LFC-Dataset">https://github.com/intelligentMachines-ITU/LFC-Dataset</a></li>
<li>paper_authors: Arif Mahmood, Abdul Basit, M. Akhtar Munir, Mohsen Ali</li>
<li>For: 本研究旨在提高人员携带武器的检测和精确定位，以提高安全和监控领域的效果。* Methods: 本研究提出了一种新的方法，利用人员与武器之间的互动信息，以提高携带武器人员的定位。该方法包括一个注意力机制，可以准确分别人员和背景，以及一种稳定性驱动的地方保持约束，以学习重要特征。* Results: 对比基eline方法，本研究的方法在新建的数据集上实现了显著更高的准精度（AP&#x3D;77.8%）。这表明，利用注意力机制和稳定性驱动的地方保持约束可以提高人员携带武器的检测精度。<details>
<summary>Abstract</summary>
Detecting firearms and accurately localizing individuals carrying them in images or videos is of paramount importance in security, surveillance, and content customization. However, this task presents significant challenges in complex environments due to clutter and the diverse shapes of firearms. To address this problem, we propose a novel approach that leverages human-firearm interaction information, which provides valuable clues for localizing firearm carriers. Our approach incorporates an attention mechanism that effectively distinguishes humans and firearms from the background by focusing on relevant areas. Additionally, we introduce a saliency-driven locality-preserving constraint to learn essential features while preserving foreground information in the input image. By combining these components, our approach achieves exceptional results on a newly proposed dataset. To handle inputs of varying sizes, we pass paired human-firearm instances with attention masks as channels through a deep network for feature computation, utilizing an adaptive average pooling layer. We extensively evaluate our approach against existing methods in human-object interaction detection and achieve significant results (AP=77.8\%) compared to the baseline approach (AP=63.1\%). This demonstrates the effectiveness of leveraging attention mechanisms and saliency-driven locality preservation for accurate human-firearm interaction detection. Our findings contribute to advancing the fields of security and surveillance, enabling more efficient firearm localization and identification in diverse scenarios.
</details>
<details>
<summary>摘要</summary>
探测火器和准确地local化携带火器的人在图像或视频中是安全监测和内容个性化的关键问题。然而，这个问题在复杂环境中存在 significativetranslation missing 挑战，主要是因为背景干扰和火器的多样化形状。为解决这个问题，我们提出了一种新的方法，利用人与火器互动信息，该信息提供了关键的携带人Localization的信息。我们的方法包括一个注意力机制，有效地从背景中分离人和火器，并且引入了一个带有Saliency的地方填充约束，以学习 essencial 特征。通过这些组件，我们的方法实现了出色的结果在一个新提出的数据集上。为处理不同大小的输入，我们通过一个深度网络传递paired人与火器实例，并使用适应平均抽取层来计算特征。我们对现有方法进行了广泛的评估，并实现了相比基eline方法（AP=63.1%）显著的结果（AP=77.8%）。这表明了注意力机制和Saliency-driven的地方填充约束对人与火器互动探测的精度具有重要作用。我们的发现对安全监测和识别领域的进步做出了贡献，可以在多种enario中更有效地检测和识别携带火器的人。
</details></li>
</ul>
<hr>
<h2 id="Improving-Speech-Inversion-Through-Self-Supervised-Embeddings-and-Enhanced-Tract-Variables"><a href="#Improving-Speech-Inversion-Through-Self-Supervised-Embeddings-and-Enhanced-Tract-Variables" class="headerlink" title="Improving Speech Inversion Through Self-Supervised Embeddings and Enhanced Tract Variables"></a>Improving Speech Inversion Through Self-Supervised Embeddings and Enhanced Tract Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09220">http://arxiv.org/abs/2309.09220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Adel Attia, Yashish M. Siriwardena, Carol Espy-Wilson</li>
<li>for: 这个论文主要是为了研究吗？</li>
<li>methods: 这个论文使用了哪些方法？</li>
<li>results: 这个论文得到了什么结果？Here are the answers in Simplified Chinese:</li>
<li>for: 这个论文主要是为了研究Acoustic-to-articulatory speech inversion（SI）系统的性能。</li>
<li>methods: 这个论文使用了自动学习模型（SSL） HuBERT 和改进的几何变换模型。</li>
<li>results: 通过结合这两种方法，SI 系统的 Pearson 产品积分相关性（PPMC）分数提高了从 0.7452 到 0.8141，即6.9% 的提高。<details>
<summary>Abstract</summary>
The performance of deep learning models depends significantly on their capacity to encode input features efficiently and decode them into meaningful outputs. Better input and output representation has the potential to boost models' performance and generalization. In the context of acoustic-to-articulatory speech inversion (SI) systems, we study the impact of utilizing speech representations acquired via self-supervised learning (SSL) models, such as HuBERT compared to conventional acoustic features. Additionally, we investigate the incorporation of novel tract variables (TVs) through an improved geometric transformation model. By combining these two approaches, we improve the Pearson product-moment correlation (PPMC) scores which evaluate the accuracy of TV estimation of the SI system from 0.7452 to 0.8141, a 6.9% increase. Our findings underscore the profound influence of rich feature representations from SSL models and improved geometric transformations with target TVs on the enhanced functionality of SI systems.
</details>
<details>
<summary>摘要</summary>
深度学习模型的性能受输入特征编码和输出解码的效率影响很大。更好的输入和输出表示有助于提高模型的性能和泛化。在声音逆转（SI）系统中，我们研究了使用自动监督学习（SSL）模型获得的声音表示，比如胡蜡BERT，与传统的声音特征相比。此外，我们还研究了通过改进的几何变换模型中的新的轨迹变量（TV）的添加。将这两种方法结合起来，可以提高声音逆转系统的归一化积分相互 correlatio（PPMC）分数，从0.7452提高到0.8141，即6.9%提高。我们的发现表明，rich的特征表示从SSL模型和改进的几何变换模型中的target TVs具有杰出的影响，提高了SI系统的功能。
</details></li>
</ul>
<hr>
<h2 id="SplitEE-Early-Exit-in-Deep-Neural-Networks-with-Split-Computing"><a href="#SplitEE-Early-Exit-in-Deep-Neural-Networks-with-Split-Computing" class="headerlink" title="SplitEE: Early Exit in Deep Neural Networks with Split Computing"></a>SplitEE: Early Exit in Deep Neural Networks with Split Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09195">http://arxiv.org/abs/2309.09195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Divya J. Bajpai, Vivek K. Trivedi, Sohan L. Yadav, Manjesh K. Hanawal</li>
<li>for: 这个研究的目的是提高资源受限的设备（边缘、移动、IoT）中深度神经网络（DNNs）的运行性能。</li>
<li>methods: 这个研究使用了分 computed 和早期终结的方法来解决这个问题。具体来说，它们使用了将 computation 分给云端进行最终推断（split computing），并在推断过程中选择性地终结推断。</li>
<li>results: 这个研究获得了较高的成本优化（&gt;50%），并且仅导致了小于2%的准确性下降。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have drawn attention because of their outstanding performance on various tasks. However, deploying full-fledged DNNs in resource-constrained devices (edge, mobile, IoT) is difficult due to their large size. To overcome the issue, various approaches are considered, like offloading part of the computation to the cloud for final inference (split computing) or performing the inference at an intermediary layer without passing through all layers (early exits). In this work, we propose combining both approaches by using early exits in split computing. In our approach, we decide up to what depth of DNNs computation to perform on the device (splitting layer) and whether a sample can exit from this layer or need to be offloaded. The decisions are based on a weighted combination of accuracy, computational, and communication costs. We develop an algorithm named SplitEE to learn an optimal policy. Since pre-trained DNNs are often deployed in new domains where the ground truths may be unavailable and samples arrive in a streaming fashion, SplitEE works in an online and unsupervised setup. We extensively perform experiments on five different datasets. SplitEE achieves a significant cost reduction ($>50\%$) with a slight drop in accuracy ($<2\%$) as compared to the case when all samples are inferred at the final layer. The anonymized source code is available at \url{https://anonymous.4open.science/r/SplitEE_M-B989/README.md}.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）吸引了关注，因其在多种任务上表现出色。然而，在资源有限的设备（边缘、移动、物联网）中部署完整的DNNs困难，因为它们的大小较大。为解决这个问题，一些方法被考虑，如将计算部分提取到云端进行最终推理（分 computation）或在设备上进行推理，而不是将所有层传递。在这种情况下，我们提出了结合这两种方法的方法，即使用早期退出在分 computation中。在我们的方法中，我们可以在设备上进行DNNs计算的深度层（分层），并决定一个样本是否可以在这层退出，或者需要被上传。这些决定是基于精度、计算和通信成本的权重平均值。我们开发了一个名为SplitEE的算法，用于学习优化策略。由于预训练的DNNs常常在新领域中部署，采用新的批处理方式和不可预测的样本流入，SplitEE在线上和无监督的设置下工作。我们对五个不同的数据集进行了广泛的实验。SplitEE可以在计算成本方面实现大于50%的减少，同时减少精度少于2%。详细的源代码可以在 \url{https://anonymous.4open.science/r/SplitEE_M-B989/README.md} 中找到。
</details></li>
</ul>
<hr>
<h2 id="From-Cooking-Recipes-to-Robot-Task-Trees-–-Improving-Planning-Correctness-and-Task-Efficiency-by-Leveraging-LLMs-with-a-Knowledge-Network"><a href="#From-Cooking-Recipes-to-Robot-Task-Trees-–-Improving-Planning-Correctness-and-Task-Efficiency-by-Leveraging-LLMs-with-a-Knowledge-Network" class="headerlink" title="From Cooking Recipes to Robot Task Trees – Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network"></a>From Cooking Recipes to Robot Task Trees – Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09181">http://arxiv.org/abs/2309.09181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sadman Sakib, Yu Sun</li>
<li>For: This paper is written for the task of robotic cooking, specifically in generating a sequence of actions for a robot to prepare a meal successfully.* Methods: The paper introduces a novel task tree generation pipeline that uses a large language model (LLM) to retrieve recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a task tree, capturing sequential and parallel dependencies among subtasks. The pipeline also mitigates the uncertainty and unreliable features of LLM outputs using task tree retrieval.* Results: The paper shows superior performance compared to previous works in task planning accuracy and efficiency, with improved planning correctness and improved execution efficiency.Here is the information in Simplified Chinese text:* For: 这篇论文是为了 robotic cooking 任务进行生成一个Successful sequence of actions。* Methods: 论文提出了一种新的任务树生成管道，使用大语言模型 (LLM)  retrieve recipe instructions，然后使用精度调整后的 GPT-3 将其转换为任务树，捕捉下一个和平行依赖关系。管道还使用任务树检索来降低 LLM 输出的不确定性和不可靠性。* Results: 论文的评估结果显示，与前一些工作相比，它在任务规划正确率和执行效率方面表现出色，得到了改进的规划正确率和执行效率。<details>
<summary>Abstract</summary>
Task planning for robotic cooking involves generating a sequence of actions for a robot to prepare a meal successfully. This paper introduces a novel task tree generation pipeline producing correct planning and efficient execution for cooking tasks. Our method first uses a large language model (LLM) to retrieve recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a task tree, capturing sequential and parallel dependencies among subtasks. The pipeline then mitigates the uncertainty and unreliable features of LLM outputs using task tree retrieval. We combine multiple LLM task tree outputs into a graph and perform a task tree retrieval to avoid questionable nodes and high-cost nodes to improve planning correctness and improve execution efficiency. Our evaluation results show its superior performance compared to previous works in task planning accuracy and efficiency.
</details>
<details>
<summary>摘要</summary>
任务观念生成 для robotic cooking 包括生成一系列动作来成功实现料理任务。本研究提出了一个新的任务树生成管线，可以生成正确的观念和高效的执行 для cooking 任务。我们的方法首先使用大型自然语言模型（LLM） retrieve 菜单 instrucions，然后使用精度调整的 GPT-3 将其转换为任务树，捕捉组成组件和平行依赖关系。然后，我们的管线将 LLM 输出中的不确定和高成本特征使用任务树搜寻来缓解。我们将多个 LLM 任务树输出融合为一个图形，并使用任务树搜寻来避免问题节点和高成本节点，以提高观念正确性和执行效率。我们的评估结果显示，该管线在任务观念正确性和执行效率方面表现更好于先前的工作。
</details></li>
</ul>
<hr>
<h2 id="Neural-Speaker-Diarization-Using-Memory-Aware-Multi-Speaker-Embedding-with-Sequence-to-Sequence-Architecture"><a href="#Neural-Speaker-Diarization-Using-Memory-Aware-Multi-Speaker-Embedding-with-Sequence-to-Sequence-Architecture" class="headerlink" title="Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding with Sequence-to-Sequence Architecture"></a>Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding with Sequence-to-Sequence Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09180">http://arxiv.org/abs/2309.09180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyunlongaaa/NSD-MS2S">https://github.com/liyunlongaaa/NSD-MS2S</a></li>
<li>paper_authors: Gaobin Yang, Maokui He, Shutong Niu, Ruoyu Wang, Yanyan Yue, Shuangqing Qian, Shilong Wu, Jun Du, Chin-Hui Lee</li>
<li>for: 这个论文旨在提出一种基于记忆感知多话者嵌入和序列到序列架构的新型神经网络演说者识别系统（NSD-MS2S），以提高效率和性能。</li>
<li>methods: 这个系统使用了记忆感知多话者嵌入（MA-MSE）和序列到序列架构（Seq2Seq）的优点，并将它们结合在一起，从而提高了效率和性能。在解码过程中，还采用了输入特征融合和多头注意力机制，以提高特征的捕捉和融合。</li>
<li>results: 根据CHiME-7 EVAL集的macro演说者识别错误率（DER）的评估结果，NSD-MS2S实现了15.9%的DER，相比官方基eline系统的49%的提升，表明该模型在主要轨道上实现了CHiME-7 DASR挑战赛的最佳性能。此外，我们还引入了深度互动模块（DIM），以更好地重新获取更清晰和更特征化的多话者嵌入，使当前模型超越了我们在CHiME-7 DASR挑战赛中使用的系统。<details>
<summary>Abstract</summary>
We propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates the strengths of memory-aware multi-speaker embedding (MA-MSE) and sequence-to-sequence (Seq2Seq) architecture, leading to improvement in both efficiency and performance. Next, we further decrease the memory occupation of decoding by incorporating input features fusion and then employ a multi-head attention mechanism to capture features at different levels. NSD-MS2S achieved a macro diarization error rate (DER) of 15.9% on the CHiME-7 EVAL set, which signifies a relative improvement of 49% over the official baseline system, and is the key technique for us to achieve the best performance for the main track of CHiME-7 DASR Challenge. Additionally, we introduce a deep interactive module (DIM) in MA-MSE module to better retrieve a cleaner and more discriminative multi-speaker embedding, enabling the current model to outperform the system we used in the CHiME-7 DASR Challenge. Our code will be available at https://github.com/liyunlongaaa/NSD-MS2S.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的神经网络发言人分类系统，使用记忆感知多 speaker嵌入Sequence-to-Sequence架构（NSD-MS2S），这种系统结合记忆感知多 speaker嵌入（MA-MSE）和Sequence-to-Sequence（Seq2Seq）架构的优点，从而提高效率和性能。然后，我们进一步减少解码过程中的内存占用量，通过输入特征融合和多头注意机制来捕捉不同级别的特征。NSD-MS2S在CHiME-7 EVAL集上达到了15.9%的macro分类错误率（DER），相比官方基eline系统，表示提高49%的相对提升，是我们在CHiME-7 DASR挑战的主轨上实现最佳性能的关键技术。此外，我们在MA-MSE模块中引入了深度互动模块（DIM），以更好地提取 cleaner和更特征化的多 speaker嵌入，使现在的模型超越了在CHiME-7 DASR挑战中使用的系统。我们的代码将于https://github.com/liyunlongaaa/NSD-MS2S上发布。
</details></li>
</ul>
<hr>
<h2 id="Syntax-Tree-Constrained-Graph-Network-for-Visual-Question-Answering"><a href="#Syntax-Tree-Constrained-Graph-Network-for-Visual-Question-Answering" class="headerlink" title="Syntax Tree Constrained Graph Network for Visual Question Answering"></a>Syntax Tree Constrained Graph Network for Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09179">http://arxiv.org/abs/2309.09179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangrui Su, Qi Zhang, Chongyang Shi, Jiachang Liu, Liang Hu</li>
<li>for: 本研究旨在提高Visual Question Answering（VQA）的精度，以便自动回答基于图像内容的自然语言问题。</li>
<li>methods: 本研究提议了一种新的Syntax Tree Constrained Graph Network（STCGN）模型，基于实体消息传递和语法树。该模型可以从问题中提取更加精确的语法树信息，并通过消息传递机制捕捉更加精确的实体特征。</li>
<li>results: 对VQA2.0数据集进行了广泛的实验，显示了我们提议的模型的超越性。<details>
<summary>Abstract</summary>
Visual Question Answering (VQA) aims to automatically answer natural language questions related to given image content. Existing VQA methods integrate vision modeling and language understanding to explore the deep semantics of the question. However, these methods ignore the significant syntax information of the question, which plays a vital role in understanding the essential semantics of the question and guiding the visual feature refinement. To fill the gap, we suggested a novel Syntax Tree Constrained Graph Network (STCGN) for VQA based on entity message passing and syntax tree. This model is able to extract a syntax tree from questions and obtain more precise syntax information. Specifically, we parse questions and obtain the question syntax tree using the Stanford syntax parsing tool. From the word level and phrase level, syntactic phrase features and question features are extracted using a hierarchical tree convolutional network. We then design a message-passing mechanism for phrase-aware visual entities and capture entity features according to a given visual context. Extensive experiments on VQA2.0 datasets demonstrate the superiority of our proposed model.
</details>
<details>
<summary>摘要</summary>
visual question answering (VQA) 目标是自动回答基于图像内容的自然语言问题。现有的 VQA 方法将视觉模型和语言理解结合以探索问题深层 semantics。然而，这些方法忽略了问题语法信息，这种信息在理解问题基本 semantics 和指导视觉特征细化方面发挥重要作用。为了填补这个空白，我们提议了一种基于实体消息传递和语法树的新方法，即Syntax Tree Constrained Graph Network (STCGN)。这个模型可以从问题中提取语法树，并且通过 hierarchy 的 convolutional network 提取Word 和 phrase 层次特征。然后，我们设计了一种message-passing机制，以便捕捉基于给定视觉上下文的视觉实体特征。我们在 VQA2.0 数据集上进行了广泛的实验，并证明了我们的提议模型的优越性。
</details></li>
</ul>
<hr>
<h2 id="Imbalanced-Data-Stream-Classification-using-Dynamic-Ensemble-Selection"><a href="#Imbalanced-Data-Stream-Classification-using-Dynamic-Ensemble-Selection" class="headerlink" title="Imbalanced Data Stream Classification using Dynamic Ensemble Selection"></a>Imbalanced Data Stream Classification using Dynamic Ensemble Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09175">http://arxiv.org/abs/2309.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priya. S, Haribharathi Sivakumar, Vijay Arvind. R</li>
<li>for: 这篇论文旨在解决现代流动数据分类中面临的概念迁移和类别不均等问题，以提高分类器的精度和正确性。</li>
<li>methods: 本文提出了一个新的框架，将数据预处理和动态集合选择组合使用，以适应非站点过渡不均等数据流。这个框架使用了七种数据预处理技术和两种动态集合选择方法。</li>
<li>results: 实验结果显示，将数据预处理与动态集合选择组合使用可以在不均等数据流中提高分类精度和正确性。<details>
<summary>Abstract</summary>
Modern streaming data categorization faces significant challenges from concept drift and class imbalanced data. This negatively impacts the output of the classifier, leading to improper classification. Furthermore, other factors such as the overlapping of multiple classes limit the extent of the correctness of the output. This work proposes a novel framework for integrating data pre-processing and dynamic ensemble selection, by formulating the classification framework for the nonstationary drifting imbalanced data stream, which employs the data pre-processing and dynamic ensemble selection techniques. The proposed framework was evaluated using six artificially generated data streams with differing imbalance ratios in combination with two different types of concept drifts. Each stream is composed of 200 chunks of 500 objects described by eight features and contains five concept drifts. Seven pre-processing techniques and two dynamic ensemble selection methods were considered. According to experimental results, data pre-processing combined with Dynamic Ensemble Selection techniques significantly delivers more accuracy when dealing with imbalanced data streams.
</details>
<details>
<summary>摘要</summary>
现代流处理数据分类面临着概念飘移和数据偏好问题，这会负面影响分类器的输出，导致不正确的分类。此外，多个类别的重叠也限制了正确性的范围。这项工作提出了一种新的框架，通过将数据预处理和动态ensemble选择相结合，对非站ARY飘移偏好数据流进行分类框架，该框架使用了数据预处理和动态ensemble选择技术。该提案在六个人工生成的数据流中进行了评估，每个流程包含200个块，每个块有500个对象，描述了八个特征。每个流程包含五次概念飘移。试验结果表明，数据预处理和动态ensemble选择技术的结合可以在偏好数据流中提供更高的准确性。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Understand-Real-World-Complex-Instructions"><a href="#Can-Large-Language-Models-Understand-Real-World-Complex-Instructions" class="headerlink" title="Can Large Language Models Understand Real-World Complex Instructions?"></a>Can Large Language Models Understand Real-World Complex Instructions?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09150">http://arxiv.org/abs/2309.09150</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abbey4799/cello">https://github.com/abbey4799/cello</a></li>
<li>paper_authors: Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen, Xintao Wang, Yuncheng Huang, Haoning Ye, Zihan Li, Shisong Chen, Yikai Zhang, Zhouhong Gu, Jiaqing Liang, Yanghua Xiao</li>
<li>for: 评估大型自然语言模型（LLM）能否系统地遵循复杂的指令，并可以应用于实际场景。</li>
<li>methods: 提出了八种复杂指令特征，并从实际场景中构建了评估数据集。还开发了四个评估标准和相应的指标，以替代现有的不充分、偏向或过于粗糙的评估方法。</li>
<li>results: 通过广泛的实验，发现代表中文和英文领域的模型在遵循复杂指令时表现不佳，具体的问题包括忽略语义约束、生成错误格式、违反长度或样本数约束、不准确反映输入文本等。<details>
<summary>Abstract</summary>
Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs' ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four criteria and develop corresponding metrics, as current ones are inadequate, biased or too strict and coarse-grained. We compare the performance of representative Chinese-oriented and English-oriented models in following complex instructions through extensive experiments. Resources of CELLO are publicly available at https://github.com/Abbey4799/CELLO.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Quantised-End-to-End-ASR-Models-via-Personalisation"><a href="#Enhancing-Quantised-End-to-End-ASR-Models-via-Personalisation" class="headerlink" title="Enhancing Quantised End-to-End ASR Models via Personalisation"></a>Enhancing Quantised End-to-End ASR Models via Personalisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09136">http://arxiv.org/abs/2309.09136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qmgzhao/Enhancing-Quantised-End-to-End-ASR-Models-via-Personalisation">https://github.com/qmgzhao/Enhancing-Quantised-End-to-End-ASR-Models-via-Personalisation</a></li>
<li>paper_authors: Qiuming Zhao, Guangzhi Sun, Chao Zhang, Mingxing Xu, Thomas Fang Zheng</li>
<li>for: 提高资源受限设备上自动语音识别（ASR）模型的性能。</li>
<li>methods: 提出了一种新的个性化策略（PQM），结合 speaker adaptive training（SAT）和模型归一化来改进压缩模型的性能。PQM使用了4位 NormalFloat Quantisation（NF4）方法进行模型归一化，并使用low-rank adaptation（LoRA）进行SAT。</li>
<li>results: 对于 LibriSpeech 和 TED-LIUM 3 数据集，PQM 可以在压缩模型中实现15.1%和23.3%的相对WRR（word error rate）下降，相比原始精度模型。此外，PQM 只需要加入1%的 speaker-specific 参数，可以实现7倍的模型大小减少。<details>
<summary>Abstract</summary>
Recent end-to-end automatic speech recognition (ASR) models have become increasingly larger, making them particularly challenging to be deployed on resource-constrained devices. Model quantisation is an effective solution that sometimes causes the word error rate (WER) to increase. In this paper, a novel strategy of personalisation for a quantised model (PQM) is proposed, which combines speaker adaptive training (SAT) with model quantisation to improve the performance of heavily compressed models. Specifically, PQM uses a 4-bit NormalFloat Quantisation (NF4) approach for model quantisation and low-rank adaptation (LoRA) for SAT. Experiments have been performed on the LibriSpeech and the TED-LIUM 3 corpora. Remarkably, with a 7x reduction in model size and 1% additional speaker-specific parameters, 15.1% and 23.3% relative WER reductions were achieved on quantised Whisper and Conformer-based attention-based encoder-decoder ASR models respectively, comparing to the original full precision models.
</details>
<details>
<summary>摘要</summary>
现代自动声音识别（ASR）模型已经变得越来越大，这使得它们在有限资源设备上部署变得更加困难。模型量化是一种有效的解决方案，但是有时会导致单词错误率（WER）增加。本文提出了一种个性化quantized模型（PQM）策略，该策略结合说话人适应训练（SAT）和模型量化来提高压缩模型的性能。具体来说，PQM使用4位NormalFloat量化（NF4）方法进行模型量化，并使用低级适应（LoRA）来实现SAT。在LibriSpeech和TED-LIUM 3 corpora上进行了实验，结果显示：与原始精度模型相比，使用PQM可以实现7倍压缩和1%额外的说话人特定参数，而WER的相对下降为15.1%和23.3%。
</details></li>
</ul>
<hr>
<h2 id="ChainForge-A-Visual-Toolkit-for-Prompt-Engineering-and-LLM-Hypothesis-Testing"><a href="#ChainForge-A-Visual-Toolkit-for-Prompt-Engineering-and-LLM-Hypothesis-Testing" class="headerlink" title="ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing"></a>ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09128">http://arxiv.org/abs/2309.09128</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ianarawjo/ChainForge">https://github.com/ianarawjo/ChainForge</a></li>
<li>paper_authors: Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman</li>
<li>for: 这篇论文是为了评估大型自然语言模型（LLM）的输出而写的。</li>
<li>methods: 这篇论文使用了一个开源的视觉工具箱，用于比较不同模型和提示的响应。</li>
<li>results: 研究发现，通过使用这个工具箱，不同的人可以Investigate各种假设，包括实际世界中的应用。<details>
<summary>Abstract</summary>
Evaluating outputs of large language models (LLMs) is challenging, requiring making -- and making sense of -- many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.
</details>
<details>
<summary>摘要</summary>
评估大语言模型（LLM）的输出具有挑战性，需要对多个响应进行评估和理解。然而，评估工具通常需要编程API知识，专注于窄领域，或者是关闭源代码。我们提出了链forge，一个开源的视觉工具箱，用于文本生成LLM的推荐工程和即时假设测试。链forge提供了一个图形化界面，用于比较不同模型和提示变化的响应。我们的系统旨在支持三个任务：模型选择、提示模板设计和假设测试（例如，审核）。我们在开发的早期就发布了链forge，并与学术界和在线用户合作进行了设计征提高。经过实验室和专访研究，我们发现了许多人可以使用链forge来调查他们关心的假设，包括在实际场景中。我们将评估工具分为三种模式：机会性探索、有限评估和迭代优化。
</details></li>
</ul>
<hr>
<h2 id="How-much-can-ChatGPT-really-help-Computational-Biologists-in-Programming"><a href="#How-much-can-ChatGPT-really-help-Computational-Biologists-in-Programming" class="headerlink" title="How much can ChatGPT really help Computational Biologists in Programming?"></a>How much can ChatGPT really help Computational Biologists in Programming?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09126">http://arxiv.org/abs/2309.09126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chowdhury Rafeed Rahman, Limsoon Wong</li>
<li>for: 本研究探讨了 chatGPT 在生物计算领域的潜在影响，包括代码生成、数据分析、机器学习模型和特征提取等方面。</li>
<li>methods: 本研究使用了 chatGPT 来执行不同的生物计算任务，如代码写作、代码审查、错误检测、代码转换、代码重构和管道化等。</li>
<li>results: 研究发现 chatGPT 在生物计算领域有多种可能的影响，包括代码生成、数据分析和机器学习模型的建立等方面，同时也存在一些潜在的负面影响，如代码质量和数据隐私等问题。<details>
<summary>Abstract</summary>
ChatGPT, a recently developed product by openAI, is successfully leaving its mark as a multi-purpose natural language based chatbot. In this paper, we are more interested in analyzing its potential in the field of computational biology. A major share of work done by computational biologists these days involve coding up Bioinformatics algorithms, analyzing data, creating pipelining scripts and even machine learning modeling & feature extraction. This paper focuses on the potential influence (both positive and negative) of ChatGPT in the mentioned aspects with illustrative examples from different perspectives. Compared to other fields of Computer Science, Computational Biology has: (1) less coding resources, (2) more sensitivity and bias issues (deals with medical data) and (3) more necessity of coding assistance (people from diverse background come to this field). Keeping such issues in mind, we cover use cases such as code writing, reviewing, debugging, converting, refactoring and pipelining using ChatGPT from the perspective of computational biologists in this paper.
</details>
<details>
<summary>摘要</summary>
chatGPT，由openAI开发的一款多功能自然语言基础的聊天机器人，在这篇论文中，我们更关心其在生物计算领域的潜在影响。计算生物学家们的工作中大量包括编程、数据分析、创建管道脚本以及机器学习模型和特征提取。本论文将对chatGPT在这些方面的影响进行分析，并通过不同角度提供示例。与其他计算机科学领域不同，计算生物学领域有以下特点：（1） coding资源更少，（2）更敏感和偏见问题（与医疗数据相关），（3）更需要编程帮助（人们来自不同背景）。基于这些问题，本文将从计算生物学家的视角来探讨使用chatGPT进行代码写作、审查、调试、转换、重构和管道化的可能性。
</details></li>
</ul>
<hr>
<h2 id="Using-Reinforcement-Learning-to-Simplify-Mealtime-Insulin-Dosing-for-People-with-Type-1-Diabetes-In-Silico-Experiments"><a href="#Using-Reinforcement-Learning-to-Simplify-Mealtime-Insulin-Dosing-for-People-with-Type-1-Diabetes-In-Silico-Experiments" class="headerlink" title="Using Reinforcement Learning to Simplify Mealtime Insulin Dosing for People with Type 1 Diabetes: In-Silico Experiments"></a>Using Reinforcement Learning to Simplify Mealtime Insulin Dosing for People with Type 1 Diabetes: In-Silico Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09125">http://arxiv.org/abs/2309.09125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anas El Fathi, Marc D. Breton</li>
<li>for: 这个论文的目的是为人类类型1糖尿病患者提供一种简单、可靠的药物剂量计算方法，以改善他们的糖尿病控制和生活质量。</li>
<li>methods: 这个研究使用了 actor-critic方法和长期记忆（LSTM）神经网络，通过训练80个虚拟Subject（VS）来实现。</li>
<li>results: 研究结果表明，提出的RL方法在26周的enario中表现出色，可以取代标准糖尿病控制方法，并且可以提高糖尿病患者的生活质量和血糖控制水平。特别是，在26周的enario中，时间在范围($70-180$mg&#x2F;dL)和时间在低糖（$&lt;70$mg&#x2F;dL）分别为73.1±11.6%和2.0±1.8%，与标准CC方法相比有所提高。<details>
<summary>Abstract</summary>
People with type 1 diabetes (T1D) struggle to calculate the optimal insulin dose at mealtime, especially when under multiple daily injections (MDI) therapy. Effectively, they will not always perform rigorous and precise calculations, but occasionally, they might rely on intuition and previous experience. Reinforcement learning (RL) has shown outstanding results in outperforming humans on tasks requiring intuition and learning from experience. In this work, we propose an RL agent that recommends the optimal meal-accompanying insulin dose corresponding to a qualitative meal (QM) strategy that does not require precise carbohydrate counting (CC) (e.g., a usual meal at noon.). The agent is trained using the soft actor-critic approach and comprises long short-term memory (LSTM) neurons. For training, eighty virtual subjects (VS) of the FDA-accepted UVA/Padova T1D adult population were simulated using MDI therapy and QM strategy. For validation, the remaining twenty VS were examined in 26-week scenarios, including intra- and inter-day variabilities in glucose. \textit{In-silico} results showed that the proposed RL approach outperforms a baseline run-to-run approach and can replace the standard CC approach. Specifically, after 26 weeks, the time-in-range ($70-180$mg/dL) and time-in-hypoglycemia ($<70$mg/dL) were $73.1\pm11.6$% and $ 2.0\pm 1.8$% using the RL-optimized QM strategy compared to $70.6\pm14.8$% and $ 1.5\pm 1.5$% using CC. Such an approach can simplify diabetes treatment, resulting in improved quality of life and glycemic outcomes.
</details>
<details>
<summary>摘要</summary>
人们 WITH 类型 1  диабеetes (T1D) 困难计算最佳注射剂量，特别是在多 daily 注射 (MDI) 治疗下。实际上，他们并不总是做出精确的计算，而是倾向于依靠直觉和之前的经验。人工学习 (RL) 已经表现出惊人的成绩，可以在需要直觉和经验学习的任务上超越人类。在这项工作中，我们提出一种RL代理人，可以根据qualitative meal (QM) 策略提供最佳陪食时注射剂量。该代理人使用 soft actor-critic 方法和长Short-Term Memory (LSTM) 神经元进行训练。为了训练，我们 simulate 了八十名虚拟Subject (VS)，分别采用 MDI 治疗和 QM 策略。为验证，剩下的二十 VS 在26周的enario中进行了检验，包括内部和外部的变化。 results showed that our proposed RL approach outperforms a baseline run-to-run approach and can replace the standard carbohydrate counting (CC) approach. Specifically, after 26 weeks, the time-in-range ($70-180$mg/dL) and time-in-hypoglycemia ($<70$mg/dL) were $73.1\pm11.6$% and $ 2.0\pm 1.8$% using the RL-optimized QM strategy compared to $70.6\pm14.8$% and $ 1.5\pm 1.5$% using CC. Such an approach can simplify diabetes treatment, resulting in improved quality of life and glycemic outcomes.
</details></li>
</ul>
<hr>
<h2 id="Conditional-Mutual-Information-Constrained-Deep-Learning-for-Classification"><a href="#Conditional-Mutual-Information-Constrained-Deep-Learning-for-Classification" class="headerlink" title="Conditional Mutual Information Constrained Deep Learning for Classification"></a>Conditional Mutual Information Constrained Deep Learning for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09123">http://arxiv.org/abs/2309.09123</a></li>
<li>repo_url: None</li>
<li>paper_authors: En-Hui Yang, Shayan Mohajer Hamidi, Linfeng Ye, Renhao Tan, Beverly Yang</li>
<li>for: 本文使用 conditional mutual information (CMI) 和 normalized conditional mutual information (NCMI) 来衡量深度神经网络 (DNN) 输出概率分布中的集中度和分化性能。</li>
<li>methods: 本文使用 NCMI 来评估 popular DNNs 在 ImageNet 文献中预训练的性能，并发现这些模型的验证精度与 NCMI 值成对关系。基于这一观察， authors 修改了标准深度学习 (DL) 框架，使其具有 CMI 约束，称为 CMI constrained deep learning (CMIC-DL)。</li>
<li>results: 对 CMIC-DL 的束定优化问题，提出了一种新的交叉学习算法。实验结果表明，使用 CMIC-DL 训练的 DNN 模型在精度和对抗攻击性能方面都高于文献中其他模型和损失函数。此外，通过 visualizing 学习过程中 CMI 和 NCMI 的变化，也能够更好地理解 DNN 的学习过程。<details>
<summary>Abstract</summary>
The concepts of conditional mutual information (CMI) and normalized conditional mutual information (NCMI) are introduced to measure the concentration and separation performance of a classification deep neural network (DNN) in the output probability distribution space of the DNN, where CMI and the ratio between CMI and NCMI represent the intra-class concentration and inter-class separation of the DNN, respectively. By using NCMI to evaluate popular DNNs pretrained over ImageNet in the literature, it is shown that their validation accuracies over ImageNet validation data set are more or less inversely proportional to their NCMI values. Based on this observation, the standard deep learning (DL) framework is further modified to minimize the standard cross entropy function subject to an NCMI constraint, yielding CMI constrained deep learning (CMIC-DL). A novel alternating learning algorithm is proposed to solve such a constrained optimization problem. Extensive experiment results show that DNNs trained within CMIC-DL outperform the state-of-the-art models trained within the standard DL and other loss functions in the literature in terms of both accuracy and robustness against adversarial attacks. In addition, visualizing the evolution of learning process through the lens of CMI and NCMI is also advocated.
</details>
<details>
<summary>摘要</summary>
《条件共mutual information（CMI）和正常化条件共mutual information（NCMI）的概念在深度学习（DL）中被引入，用于衡量深度神经网络（DNN）在输出概率分布空间中的集中度和分化性。CMI和NCMI之比分别表示DNN的内类集中度和对类分化程度。通过对popular DNNs在ImageNet文献中的验证性能进行NCMI评估，发现这些模型的验证性能与NCMI值相对负相关。基于此观察，我们提出了一种受CMI约束的DL框架（CMIC-DL），并提出了一种新的交互学习算法来解决这种受约束优化问题。实验结果表明，在CMIC-DL中训练的DNN模型比标准DL框架和其他文献中的损失函数训练模型在鲁棒性和鲁棒性对抗攻击方面表现更好。此外，通过CMI和NCMI的视觉化来描述学习过程的演化也被提出。》Note that Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="FDCNet-Feature-Drift-Compensation-Network-for-Class-Incremental-Weakly-Supervised-Object-Localization"><a href="#FDCNet-Feature-Drift-Compensation-Network-for-Class-Incremental-Weakly-Supervised-Object-Localization" class="headerlink" title="FDCNet: Feature Drift Compensation Network for Class-Incremental Weakly Supervised Object Localization"></a>FDCNet: Feature Drift Compensation Network for Class-Incremental Weakly Supervised Object Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09122">http://arxiv.org/abs/2309.09122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Vision-sejin/FDCNet">https://github.com/Vision-sejin/FDCNet</a></li>
<li>paper_authors: Sejin Park, Taehyung Lee, Yeejin Lee, Byeongkeun Kang</li>
<li>for: 这个研究旨在解决类增量弱监督物体Localization（CI-WSOL）任务，即逐渐学习新类的物体Localization，只使用图像级别的标签，而保持先前学习的类的Localization能力。</li>
<li>methods: 我们采用了类增量类ifiers的策略来抑制忘记现象，包括知识填充、保留先前任务中的小数据集、以及使用cosine normalization。此外，我们还提出了特征漂移补偿网络，以补偿因Feature Drift而导致的类分数和Localization图像的影响。</li>
<li>results: 我们通过在ImageNet-100和CUB-200两个公开available datasets上进行实验，发现我们的提议方法比基eline方法高效。<details>
<summary>Abstract</summary>
This work addresses the task of class-incremental weakly supervised object localization (CI-WSOL). The goal is to incrementally learn object localization for novel classes using only image-level annotations while retaining the ability to localize previously learned classes. This task is important because annotating bounding boxes for every new incoming data is expensive, although object localization is crucial in various applications. To the best of our knowledge, we are the first to address this task. Thus, we first present a strong baseline method for CI-WSOL by adapting the strategies of class-incremental classifiers to mitigate catastrophic forgetting. These strategies include applying knowledge distillation, maintaining a small data set from previous tasks, and using cosine normalization. We then propose the feature drift compensation network to compensate for the effects of feature drifts on class scores and localization maps. Since updating network parameters to learn new tasks causes feature drifts, compensating for the final outputs is necessary. Finally, we evaluate our proposed method by conducting experiments on two publicly available datasets (ImageNet-100 and CUB-200). The experimental results demonstrate that the proposed method outperforms other baseline methods.
</details>
<details>
<summary>摘要</summary>
这个研究强调了类增量弱监督对象定位任务（CI-WSOL）。目标是逐步学习新类对象定位，只使用图像级别的注释，保留之前学习的类别定位能力。这项任务非常重要，因为为每个新来的数据都要注释 bounding box 是非常昂贵的，但对象定位是许多应用场景中非常重要的。根据我们所知，我们是第一个对这项任务进行研究的。因此，我们首先提出了一个强大的基线方法 для CI-WSOL，通过适应类增量分类器的策略来减轻忘却性。这些策略包括应用知识传播、维护前一任务中的小数据集，以及使用仓颉 норamlization。然后，我们提议了特征漂移补做网络，以补做因更新网络参数学习新任务而导致的特征漂移的效果。最后，我们通过在 ImageNet-100 和 CUB-200 两个公共可用的数据集上进行实验，证明我们提出的方法在基eline方法上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Public-Perceptions-of-Gender-Bias-in-Large-Language-Models-Cases-of-ChatGPT-and-Ernie"><a href="#Public-Perceptions-of-Gender-Bias-in-Large-Language-Models-Cases-of-ChatGPT-and-Ernie" class="headerlink" title="Public Perceptions of Gender Bias in Large Language Models: Cases of ChatGPT and Ernie"></a>Public Perceptions of Gender Bias in Large Language Models: Cases of ChatGPT and Ernie</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09120">http://arxiv.org/abs/2309.09120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyrie Zhixuan Zhou, Madelyn Rose Sanfilippo</li>
<li>for: This paper aims to investigate and analyze public perceptions of gender bias in large language models (LLMs) trained in different cultural contexts.</li>
<li>methods: The authors conducted a content analysis of social media discussions to gather data on people’s observations of gender bias in their personal use of LLMs and scientific findings about gender bias in LLMs.</li>
<li>results: The study found that ChatGPT, a US-based LLM, exhibited more implicit gender bias, while Ernie, a China-based LLM, showed more explicit gender bias. The findings suggest that culture plays a significant role in shaping gender bias in LLMs and propose governance recommendations to regulate gender bias in these models.<details>
<summary>Abstract</summary>
Large language models are quickly gaining momentum, yet are found to demonstrate gender bias in their responses. In this paper, we conducted a content analysis of social media discussions to gauge public perceptions of gender bias in LLMs which are trained in different cultural contexts, i.e., ChatGPT, a US-based LLM, or Ernie, a China-based LLM. People shared both observations of gender bias in their personal use and scientific findings about gender bias in LLMs. A difference between the two LLMs was seen -- ChatGPT was more often found to carry implicit gender bias, e.g., associating men and women with different profession titles, while explicit gender bias was found in Ernie's responses, e.g., overly promoting women's pursuit of marriage over career. Based on the findings, we reflect on the impact of culture on gender bias and propose governance recommendations to regulate gender bias in LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型快速增长，但它们在回应中显示出性别偏见。在这篇研究中，我们通过社交媒体讨论分析公众对于语言模型中的性别偏见。人们分享了对于性别偏见的personal使用经验和科学发现，包括两个不同的语言模型：ChatGPT和Ernie。我们发现ChatGPT更常见偏见，例如将男女分配不同的职业标题，而Ernie的回应则显示了明显的性别偏见，例如过度推荐女性追求婚姻而不是职业。根据结果，我们思考了文化对性别偏见的影响和建议管理性别偏见在语言模型中。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-3D-Object-Level-Mapping-with-Deep-Shape-Priors"><a href="#Uncertainty-aware-3D-Object-Level-Mapping-with-Deep-Shape-Priors" class="headerlink" title="Uncertainty-aware 3D Object-Level Mapping with Deep Shape Priors"></a>Uncertainty-aware 3D Object-Level Mapping with Deep Shape Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09118">http://arxiv.org/abs/2309.09118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TRAILab/UncertainShapePose">https://github.com/TRAILab/UncertainShapePose</a></li>
<li>paper_authors: Ziwei Liao, Jun Yang, Jingxing Qian, Angela P. Schoellig, Steven L. Waslander<br>for:这篇论文的目的是提出一种高质量的物体级别地图生成方法，用于在探测过程中对未知物体进行高精度的重建。methods:该方法使用多个RGB-D图像作为输入，并输出高精度的3D形状和9DoF姿态（包括3个涂抹参数）。该方法利用学习的生成模型来作为对shape类别的先验知识，并使用概率uncertainty-aware优化框架来进行3D重建。results:我们在实验中取得了substantial的提高，与现有的方法相比。我们还示出了对下游机器人任务的活跃探测等方面的应用。<details>
<summary>Abstract</summary>
3D object-level mapping is a fundamental problem in robotics, which is especially challenging when object CAD models are unavailable during inference. In this work, we propose a framework that can reconstruct high-quality object-level maps for unknown objects. Our approach takes multiple RGB-D images as input and outputs dense 3D shapes and 9-DoF poses (including 3 scale parameters) for detected objects. The core idea of our approach is to leverage a learnt generative model for shape categories as a prior and to formulate a probabilistic, uncertainty-aware optimization framework for 3D reconstruction. We derive a probabilistic formulation that propagates shape and pose uncertainty through two novel loss functions. Unlike current state-of-the-art approaches, we explicitly model the uncertainty of the object shapes and poses during our optimization, resulting in a high-quality object-level mapping system. Moreover, the resulting shape and pose uncertainties, which we demonstrate can accurately reflect the true errors of our object maps, can also be useful for downstream robotics tasks such as active vision. We perform extensive evaluations on indoor and outdoor real-world datasets, achieving achieves substantial improvements over state-of-the-art methods. Our code will be available at https://github.com/TRAILab/UncertainShapePose.
</details>
<details>
<summary>摘要</summary>
三维对象级映射是Robotics中的基本问题，特别是当对象CAD模型不可用于推理时更加棘手。在这项工作中，我们提出了一个框架，可以重建高质量的对象级映射。我们的方法接受多个RGB-D图像作为输入，并输出密集的3D形状和9个DoF姿态（包括3个涉及度参数）。我们的核心思想是利用学习的生成模型来作为类别的先验，并使用概率、不确定性意识推导的优化框架来重建3D映射。我们 derive了一个概率形式，将形状和姿态不确定性传递给两个新的损失函数。不同于当前状态的方法，我们显式地模型对象的形状和姿态不确定性，从而实现了高质量的对象级映射系统。此外，我们所获得的形状和姿态不确定性，可以准确地反映我们对象映射的真实错误，同时也可以用于下游机器人任务，如活动视觉。我们在室内和室外实际数据集上进行了广泛的评估，实现了明显的提高。我们的代码将在https://github.com/TRAILab/UncertainShapePose上提供。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Decoding-Improves-Reasoning-in-Large-Language-Models"><a href="#Contrastive-Decoding-Improves-Reasoning-in-Large-Language-Models" class="headerlink" title="Contrastive Decoding Improves Reasoning in Large Language Models"></a>Contrastive Decoding Improves Reasoning in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09117">http://arxiv.org/abs/2309.09117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean O’Brien, Mike Lewis</li>
<li>for: 该研究旨在证明 Contrastive Decoding 可以大幅提高语言模型生成文本的质量，特别是在逻辑 reasoning 任务上。</li>
<li>methods: 该研究使用 Contrastive Decoding 方法，这是一种简单、计算上轻便，无需训练的文本生成方法。</li>
<li>results: 研究发现，Contrastive Decoding 可以在多种逻辑 reasoning 任务上大幅超越 greedy decoding，并且在 HellaSwag 常识逻辑测试 benchmark 和 GSM8K 数学单词逻辑测试 benchmark 上表现出色。<details>
<summary>Abstract</summary>
We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models.
</details>
<details>
<summary>摘要</summary>
我们证明了对比解码（Contrastive Decoding）——一种简单、计算机轻量级、训练无需的文本生成方法，提出于李等2022年的研究——在多种逻辑任务上实现了大量的出vat的改进。原本用于改善长文本生成的 perceived 质量，对比解码搜索最大化一个权重 diferencial of likelihood  между强度模型和弱度模型中的字符串。我们表明，对比解码使 LLama-65B 超过 LLama 2、GPT-3.5 和 PaLM 2-L 在 hellaSwag 常识逻辑标准 bencmark 上表现出色，以及在 GSM8K 数学单词逻辑标准 bencmark 上超过 LLaMA 2、GPT-3.5 和 PaLM-540B。此外，对比解码还能在其他任务上提高表现。分析表明，对比解码可以避免一些抽象逻辑错误，以及避免简单的模式，如输入段落复制。总之，对比解码超过 nucleus sampling  для长文本生成和 greedy decoding  para reasoning tasks，成为一种强大的通用方法 для生成语言模型中的文本。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.AI_2023_09_17/" data-id="cloimip54003zs488dixb69id" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.CL_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T11:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.CL_2023_09_17/">cs.CL - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Augmenting-text-for-spoken-language-understanding-with-Large-Language-Models"><a href="#Augmenting-text-for-spoken-language-understanding-with-Large-Language-Models" class="headerlink" title="Augmenting text for spoken language understanding with Large Language Models"></a>Augmenting text for spoken language understanding with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09390">http://arxiv.org/abs/2309.09390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roshan Sharma, Suyoun Kim, Daniel Lazar, Trang Le, Akshat Shrivastava, Kwanghoon Ahn, Piyush Kansal, Leda Sari, Ozlem Kalinli, Michael Seltzer</li>
<li>for: 该研究的目的是解决现有应用领域中的Robust模型训练问题，需要对应的 triplets 数据，但获取这些数据可以是非常复杂和昂贵的。</li>
<li>methods: 该研究使用了不同的方法来使用无对应的文本数据进行模型训练，包括Joint Audio Text (JAT)和Text-to-Speech (TTS)等方法。</li>
<li>results: 实验结果表明，使用无对应的文本数据可以提高模型的性能，对于现有领域和新领域来说，使用LLMs生成的文本可以提高EM的精度 by 1.4%和2.6%。<details>
<summary>Abstract</summary>
Spoken semantic parsing (SSP) involves generating machine-comprehensible parses from input speech. Training robust models for existing application domains represented in training data or extending to new domains requires corresponding triplets of speech-transcript-semantic parse data, which is expensive to obtain. In this paper, we address this challenge by examining methods that can use transcript-semantic parse data (unpaired text) without corresponding speech. First, when unpaired text is drawn from existing textual corpora, Joint Audio Text (JAT) and Text-to-Speech (TTS) are compared as ways to generate speech representations for unpaired text. Experiments on the STOP dataset show that unpaired text from existing and new domains improves performance by 2% and 30% in absolute Exact Match (EM) respectively. Second, we consider the setting when unpaired text is not available in existing textual corpora. We propose to prompt Large Language Models (LLMs) to generate unpaired text for existing and new domains. Experiments show that examples and words that co-occur with intents can be used to generate unpaired text with Llama 2.0. Using the generated text with JAT and TTS for spoken semantic parsing improves EM on STOP by 1.4% and 2.6% absolute for existing and new domains respectively.
</details>
<details>
<summary>摘要</summary>
spoken semantic parsing (SSP) 即从语音输入生成机器可理解的架构。为已有应用领域的训练数据或扩展到新领域训练模型而需要对应的三元组（语音、 trascript、semantic parse）数据，这是 expensive 的。在这篇论文中，我们解决这个挑战，通过不使用对应的语音，使用 trascript-semantic parse 数据（无对应的语音）来生成 speech 表示。首先，当 unpaired text 从现有的文本库中提取时，我们比较使用 Joint Audio Text (JAT) 和 Text-to-Speech (TTS) 生成 speech 表示。STOP 数据集上的实验结果显示，从现有和新领域的 unpaired text 提取可以提高表达的性能，相对于无提取情况下，减少了2%和30%的精确匹配（EM）。其次，当 unpaired text 不在现有的文本库中时，我们提议使用 Large Language Models (LLMs) 生成 unpaired text。我们发现，可以使用 intents 相关的例子和单词来生成 unpaired text。使用生成的文本与 JAT 和 TTS 生成的 speech 进行 spoken semantic parsing，可以提高 STOP 数据集上的 EM 表达，相对于无提取情况下，提高了1.4%和2.6%的精确匹配。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Shortcuts-in-Language-Models-with-Soft-Label-Encoding"><a href="#Mitigating-Shortcuts-in-Language-Models-with-Soft-Label-Encoding" class="headerlink" title="Mitigating Shortcuts in Language Models with Soft Label Encoding"></a>Mitigating Shortcuts in Language Models with Soft Label Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09380">http://arxiv.org/abs/2309.09380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirui He, Huiqi Deng, Haiyan Zhao, Ninghao Liu, Mengnan Du</li>
<li>for: 本研究的目的是解决大语言模型在自然语言理解（NLU）任务上依赖偶合关系的问题。</li>
<li>methods: 我们提出了一种简单 yet effective的减弱架构，名为软标签编码（SoftLE）。我们首先使用师模型通过硬标签训练每个样本的偶合度。然后，我们添加了一个假类来编码偶合度，并将其用于融合其他维度的真实标签生成软标签。这个新的真实标签用于训练更加可靠的学生模型。</li>
<li>results: 我们在两个 NLU  benchmark 任务上进行了广泛的实验，结果表明，SoftLE 可以显著提高对于样本集外的泛化性能，而保持满意的在样本集内的准确率。<details>
<summary>Abstract</summary>
Recent research has shown that large language models rely on spurious correlations in the data for natural language understanding (NLU) tasks. In this work, we aim to answer the following research question: Can we reduce spurious correlations by modifying the ground truth labels of the training data? Specifically, we propose a simple yet effective debiasing framework, named Soft Label Encoding (SoftLE). We first train a teacher model with hard labels to determine each sample's degree of relying on shortcuts. We then add one dummy class to encode the shortcut degree, which is used to smooth other dimensions in the ground truth label to generate soft labels. This new ground truth label is used to train a more robust student model. Extensive experiments on two NLU benchmark tasks demonstrate that SoftLE significantly improves out-of-distribution generalization while maintaining satisfactory in-distribution accuracy.
</details>
<details>
<summary>摘要</summary>
近期研究发现大语言模型在自然语言理解任务上依赖于偶合关系。在这项工作中，我们想回答以下研究问题：可以通过修改训练数据的真实标签来减少偶合关系吗？我们提出了一种简单 yet effective的减偶框架，名为软标签编码（SoftLE）。我们首先使用师模型通过硬标签确定每个样本的偶合度。然后，我们添加了一个幌子类来编码偶合度，并将其用于软标签的缓冲处理。这个新的真实标签用于训练更加可靠的学生模型。我们在两个NLU benchmark任务上进行了广泛的实验，结果表明，SoftLE可以明显提高异常情况泛化性，同时保持满意的内部准确率。
</details></li>
</ul>
<hr>
<h2 id="Embrace-Divergence-for-Richer-Insights-A-Multi-document-Summarization-Benchmark-and-a-Case-Study-on-Summarizing-Diverse-Information-from-News-Articles"><a href="#Embrace-Divergence-for-Richer-Insights-A-Multi-document-Summarization-Benchmark-and-a-Case-Study-on-Summarizing-Diverse-Information-from-News-Articles" class="headerlink" title="Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles"></a>Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09369">http://arxiv.org/abs/2309.09369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kung-Hsiang Huang, Philippe Laban, Alexander R. Fabbri, Prafulla Kumar Choubey, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu</li>
<li>for: 本研究旨在提出一种多文摘要新闻Summarization任务，即对多篇新闻文章中的不同信息进行摘要。</li>
<li>methods: 我们提出了一种数据收集方案，并开发了一个名为DiverseSumm的数据集，包括245篇新闻故事和10篇新闻文章。此外，我们还进行了全面的分析，探讨使用大语言模型（LLM）测试摘要的覆盖率和准确性的问题，以及与人类评估的相关性。</li>
<li>results: 我们的分析发现，使用LLM进行多文摘要时，主要存在覆盖率和准确性的问题，GPT-4只能覆盖不同信息的 Less than 40% 的情况。<details>
<summary>Abstract</summary>
Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, to our knowledge, the summarization of diverse information dispersed across multiple articles about an event has not been previously investigated. The latter imposes a different set of challenges for a summarization model. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Moreover, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of the summaries, as well as their correlation with human assessments. We applied our findings to study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover less than 40% of the diverse information on average.
</details>
<details>
<summary>摘要</summary>
To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Moreover, we conducted a comprehensive analysis to identify the position and verbosity biases when using Large Language Model (LLM)-based metrics to evaluate the coverage and faithfulness of the summaries, as well as their correlation with human assessments.We applied our findings to study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that while LLMs have extraordinary capabilities in single-document summarization, the proposed task remains a complex challenge for them, with GPT-4 only able to cover less than 40% of the diverse information on average.
</details></li>
</ul>
<hr>
<h2 id="Language-models-are-susceptible-to-incorrect-patient-self-diagnosis-in-medical-applications"><a href="#Language-models-are-susceptible-to-incorrect-patient-self-diagnosis-in-medical-applications" class="headerlink" title="Language models are susceptible to incorrect patient self-diagnosis in medical applications"></a>Language models are susceptible to incorrect patient self-diagnosis in medical applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09362">http://arxiv.org/abs/2309.09362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rojin Ziaei, Samuel Schmidgall</li>
<li>for: 这个研究旨在检验大型自然语言模型（LLMs）在医疗领域中的可用性，以及它们在实际患者-医生交流中的表现。</li>
<li>methods: 研究使用了多种LLMs，并对它们进行了多项选择和修改，以模拟实际患者自诊的情况。研究还使用了美国医学会考试的多选题来评估LLMs的诊断准确率。</li>
<li>results: 研究发现，当患者提供了错误的偏见验证信息时，LLMs的诊断准确率会受到极大的影响，表明LLMs在自诊情况下存在高度易错的问题。<details>
<summary>Abstract</summary>
Large language models (LLMs) are becoming increasingly relevant as a potential tool for healthcare, aiding communication between clinicians, researchers, and patients. However, traditional evaluations of LLMs on medical exam questions do not reflect the complexity of real patient-doctor interactions. An example of this complexity is the introduction of patient self-diagnosis, where a patient attempts to diagnose their own medical conditions from various sources. While the patient sometimes arrives at an accurate conclusion, they more often are led toward misdiagnosis due to the patient's over-emphasis on bias validating information. In this work we present a variety of LLMs with multiple-choice questions from United States medical board exams which are modified to include self-diagnostic reports from patients. Our findings highlight that when a patient proposes incorrect bias-validating information, the diagnostic accuracy of LLMs drop dramatically, revealing a high susceptibility to errors in self-diagnosis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Performance-of-the-Pre-Trained-Large-Language-Model-GPT-4-on-Automated-Short-Answer-Grading"><a href="#Performance-of-the-Pre-Trained-Large-Language-Model-GPT-4-on-Automated-Short-Answer-Grading" class="headerlink" title="Performance of the Pre-Trained Large Language Model GPT-4 on Automated Short Answer Grading"></a>Performance of the Pre-Trained Large Language Model GPT-4 on Automated Short Answer Grading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09338">http://arxiv.org/abs/2309.09338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerd Kortemeyer</li>
<li>for: 这 paper 是 investigate GPT-4 LLM 在 Automated Short Answer Grading (ASAG) 领域的性能。</li>
<li>methods: 这 paper 使用了 GPT-4 LLM 和手动设计的模型，对 SciEntsBank 和 Beetle 数据集进行测试。</li>
<li>results: 结果表明，GPT-4 LLM 的性能相对于手动设计的模型类似，但与特殊训练的 LLM 相比较差。<details>
<summary>Abstract</summary>
Automated Short Answer Grading (ASAG) has been an active area of machine-learning research for over a decade. It promises to let educators grade and give feedback on free-form responses in large-enrollment courses in spite of limited availability of human graders. Over the years, carefully trained models have achieved increasingly higher levels of performance. More recently, pre-trained Large Language Models (LLMs) emerged as a commodity, and an intriguing question is how a general-purpose tool without additional training compares to specialized models. We studied the performance of GPT-4 on the standard benchmark 2-way and 3-way datasets SciEntsBank and Beetle, where in addition to the standard task of grading the alignment of the student answer with a reference answer, we also investigated withholding the reference answer. We found that overall, the performance of the pre-trained general-purpose GPT-4 LLM is comparable to hand-engineered models, but worse than pre-trained LLMs that had specialized training.
</details>
<details>
<summary>摘要</summary>
自动化简答评分（ASAG）已经是机器学习研究的活跃领域之一，时间已经超过十年。它承诺能让教育工作者在大规模课程中评分和给出反馈，尽管人工评分员的可用性有限。总之，仔细训练的模型在过去的几年中取得了不断提高的性能。更加最近，大型自然语言模型（LLM）出现了，一个有趣的问题是一个通用工具没有进行特殊训练是如何与特殊训练的模型相比。我们研究了GPT-4在标准benchmark上的性能，包括SciEntsBank和Beetle dataset，我们还研究了不公开参考答案的情况。我们发现，总的来说，预训练的通用GPT-4 LLM的性能和手工设计的模型相比较，但是比特殊训练的LLM更差。
</details></li>
</ul>
<hr>
<h2 id="A-Few-Shot-Approach-to-Dysarthric-Speech-Intelligibility-Level-Classification-Using-Transformers"><a href="#A-Few-Shot-Approach-to-Dysarthric-Speech-Intelligibility-Level-Classification-Using-Transformers" class="headerlink" title="A Few-Shot Approach to Dysarthric Speech Intelligibility Level Classification Using Transformers"></a>A Few-Shot Approach to Dysarthric Speech Intelligibility Level Classification Using Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09329">http://arxiv.org/abs/2309.09329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paleti Nikhil Chowdary, Vadlapudi Sai Aravind, Gorantla V N S L Vishnu Vardhan, Menta Sai Akshay, Menta Sai Aashish, Jyothish Lal. G</li>
<li>for: 检测异常speech（dysarthria），以便开发治疗计划，提高人们的交流能力和生活质量。</li>
<li>methods: 使用transformer模型，采用几招shot学习法，对有限数据进行分类，并且解决之前研究中的数据泄露问题。</li>
<li>results: 使用whisper-large-v2 transformer模型，在UASpeech数据集中的中度智能水平患者上达到了85%的准确率，0.92的精度、0.8的准确率、0.85的F1分数和0.91的特异性。模型在’words’数据集上表现较好，比’letters’和’digits’数据集更好。多类模型达到67%的准确率。<details>
<summary>Abstract</summary>
Dysarthria is a speech disorder that hinders communication due to difficulties in articulating words. Detection of dysarthria is important for several reasons as it can be used to develop a treatment plan and help improve a person's quality of life and ability to communicate effectively. Much of the literature focused on improving ASR systems for dysarthric speech. The objective of the current work is to develop models that can accurately classify the presence of dysarthria and also give information about the intelligibility level using limited data by employing a few-shot approach using a transformer model. This work also aims to tackle the data leakage that is present in previous studies. Our whisper-large-v2 transformer model trained on a subset of the UASpeech dataset containing medium intelligibility level patients achieved an accuracy of 85%, precision of 0.92, recall of 0.8 F1-score of 0.85, and specificity of 0.91. Experimental results also demonstrate that the model trained using the 'words' dataset performed better compared to the model trained on the 'letters' and 'digits' dataset. Moreover, the multiclass model achieved an accuracy of 67%.
</details>
<details>
<summary>摘要</summary>
《异常speech》是一种语言障碍，影响人们的交流能力，由于说话困难。检测异常speech的重要性在于可以开发治疗计划，帮助人们提高交流能力和语言表达效果。许多文献关注提高ASR系统对异常speech的识别能力。目前研究的目标是开发一种可以准确地识别异常speech并提供语音elligibility水平信息的模型，使用少量数据和transformer模型进行几架 Approach。此外，本研究还希望解决过去研究中存在的数据泄露问题。我们使用UASpeech数据集中的中度语音智能水平患者训练了我们的whisper-large-v2 transformer模型，达到了85%的准确率、0.92的精度、0.8的 recall、0.85的F1得分和0.91的特异性。实验结果还表明，使用“words”数据集训练的模型比使用“letters”和“digits”数据集训练的模型性能更高。此外，多类模型达到了67%的准确率。
</details></li>
</ul>
<hr>
<h2 id="How-People-Perceive-The-Dynamic-Zero-COVID-Policy-A-Retrospective-Analysis-From-The-Perspective-of-Appraisal-Theory"><a href="#How-People-Perceive-The-Dynamic-Zero-COVID-Policy-A-Retrospective-Analysis-From-The-Perspective-of-Appraisal-Theory" class="headerlink" title="How People Perceive The Dynamic Zero-COVID Policy: A Retrospective Analysis From The Perspective of Appraisal Theory"></a>How People Perceive The Dynamic Zero-COVID Policy: A Retrospective Analysis From The Perspective of Appraisal Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09324">http://arxiv.org/abs/2309.09324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Na Yang, Kyrie Zhixuan Zhou, Yunzhe Li</li>
<li>for: 这 paper 是为了研究中国的动态零病政策三年来的情感反应和观点变化。</li>
<li>methods: 这 paper 使用了 sentiment analysis 方法来分析了 2,358 篇微博文章中的情感，并从 appraisal theory 的视角进行了深入的诠释。</li>
<li>results: 研究发现了四个代表点：政策初始化、情感态度快速变化、情感分数最低、政策终止。这些结果可能有助于未来卫生防疫控制措施的开发。<details>
<summary>Abstract</summary>
The Dynamic Zero-COVID Policy in China spanned three years and diverse emotional responses have been observed at different times. In this paper, we retrospectively analyzed public sentiments and perceptions of the policy, especially regarding how they evolved over time, and how they related to people's lived experiences. Through sentiment analysis of 2,358 collected Weibo posts, we identified four representative points, i.e., policy initialization, sharp sentiment change, lowest sentiment score, and policy termination, for an in-depth discourse analysis through the lens of appraisal theory. In the end, we reflected on the evolving public sentiments toward the Dynamic Zero-COVID Policy and proposed implications for effective epidemic prevention and control measures for future crises.
</details>
<details>
<summary>摘要</summary>
中国的动态零病政策覆盖了三年的时间，而人们对这政策的情感响应也在不同时间 exhibited 多样化的情感。在这篇论文中，我们通过对2358篇微博文章的情感分析，发现了四个代表点：政策 initialize，快速情感变化，最低情感分数，和政策终止，并通过评价理论进行深入的讨论。最后，我们回顾了公众对动态零病政策的情感发展，并提出了未来危机管理的有效措施。Note: "微博" (Weibo) is a popular social media platform in China, similar to Twitter.
</details></li>
</ul>
<hr>
<h2 id="A-novel-approach-to-measuring-patent-claim-scope-based-on-probabilities-obtained-from-large-language-models"><a href="#A-novel-approach-to-measuring-patent-claim-scope-based-on-probabilities-obtained-from-large-language-models" class="headerlink" title="A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models"></a>A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10003">http://arxiv.org/abs/2309.10003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sébastien Ragot</li>
<li>for: 这种方法用于测量专利索引的范围，它基于信息理论，假设罕见的概念更有启示性，因为它更有启示性。</li>
<li>methods: 该方法基于语言模型，从报告语言模型中计算自信息，从而计算专利索引的范围。五种语言模型被考虑，从 simplest 模型（每个词或字符从均匀分布中选择）到中间模型（使用平均词或字符频率），最后一个模型是 GPT2。</li>
<li>results: 结果表明，随着语言模型的复杂程度的增加，模型的性能也得到了改善。GPT2模型在多个静态测试中表现出色，超过了基于词和字符频率的模型，而这些模型本身也超过了基于词和字符计数的模型。<details>
<summary>Abstract</summary>
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope. The performance of the language models is then assessed with respect to several ad hoc tests. The more sophisticated the model, the better the results. The GPT2 model outperforms models based on word and character frequencies, which are themselves ahead of models based on word and character counts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AutoAM-An-End-To-End-Neural-Model-for-Automatic-and-Universal-Argument-Mining"><a href="#AutoAM-An-End-To-End-Neural-Model-for-Automatic-and-Universal-Argument-Mining" class="headerlink" title="AutoAM: An End-To-End Neural Model for Automatic and Universal Argument Mining"></a>AutoAM: An End-To-End Neural Model for Automatic and Universal Argument Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09300">http://arxiv.org/abs/2309.09300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lang Cao</li>
<li>for: 本文是为了提出一种基于神经网络的自动论证挖掘模型（AutoAM），以解决现有的论证挖掘技术尚未成熟和不具备准确描述论证关系的问题。</li>
<li>methods: 本文提出了一种新的论证组件注意力机制，可以更好地捕捉论证中相关的信息，从而提高论证挖掘的性能。此外，本文还提出了一种通用的终端框架，可以无需Constraints like tree structure，完成论证挖掘的三个子任务。</li>
<li>results: 实验结果表明， compared with现有的工作，我们的模型在两个公共数据集上的多个维度上表现出色，达到了更高的性能。<details>
<summary>Abstract</summary>
Argument mining is to analyze argument structure and extract important argument information from unstructured text. An argument mining system can help people automatically gain causal and logical information behind the text. As argumentative corpus gradually increases, like more people begin to argue and debate on social media, argument mining from them is becoming increasingly critical. However, argument mining is still a big challenge in natural language tasks due to its difficulty, and relative techniques are not mature. For example, research on non-tree argument mining needs to be done more. Most works just focus on extracting tree structure argument information. Moreover, current methods cannot accurately describe and capture argument relations and do not predict their types. In this paper, we propose a novel neural model called AutoAM to solve these problems. We first introduce the argument component attention mechanism in our model. It can capture the relevant information between argument components, so our model can better perform argument mining. Our model is a universal end-to-end framework, which can analyze argument structure without constraints like tree structure and complete three subtasks of argument mining in one model. The experiment results show that our model outperforms the existing works on several metrics in two public datasets.
</details>
<details>
<summary>摘要</summary>
Argument mining是分析对话结构，从不结构化文本中提取重要的论据信息的技术。一个有效的论据挖掘系统可以帮助人们自动获得文本中的 causal 和逻辑信息。随着社交媒体上的论据资源的不断增加，论据挖掘在自然语言任务中变得越来越重要。然而，论据挖掘仍然是自然语言任务中的大挑战，因为它的难度和相关技术还没有成熟。例如，研究非树结构论据挖掘还需要做得更多。大多数工作都是只关注EXTRACTING 树结构论据信息。此外，当前的方法无法准确地描述和捕捉论据关系，也无法预测它们的类型。在这篇论文中，我们提出了一种新的神经网络模型called AutoAM，以解决这些问题。我们首先介绍了我们模型中的论据组件注意力机制。它可以捕捉论据组件之间的相关信息，因此我们的模型可以更好地进行论据挖掘。我们的模型是一个通用的端到端框架，可以不受树结构的限制，完成论据挖掘中的三个子任务。实验结果表明，我们的模型在两个公共数据集上的多个纪录度量上都高于现有的作品。
</details></li>
</ul>
<hr>
<h2 id="OWL-A-Large-Language-Model-for-IT-Operations"><a href="#OWL-A-Large-Language-Model-for-IT-Operations" class="headerlink" title="OWL: A Large Language Model for IT Operations"></a>OWL: A Large Language Model for IT Operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09298">http://arxiv.org/abs/2309.09298</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Other-sources">https://github.com/Aryia-Behroziuan/Other-sources</a></li>
<li>paper_authors: Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, Xu Shi, Tieqiao Zheng, Liangfan Zheng, Bo Zhang, Ke Xu, Zhoujun Li</li>
<li>for: 这篇论文旨在探讨特有的大自然语言处理技术（NLP）在信息技术（IT）操作中的应用。</li>
<li>methods: 本论文使用了一个名为OWL的大语言模型，该模型在我们收集的OWL-Instruct数据集上进行了训练，该数据集包含了各种IT相关信息。在训练过程中，提出了混合适应器策略来提高参数效率的调整 across different domains or tasks。</li>
<li>results: 根据我们在OWL-Bench和开放IT相关的benchmark上进行的评估，OWL模型在IT任务上表现出色，与现有模型相比，具有显著的性能优势。此外，我们希望这些发现能够为IT操作技术的发展提供更多的思路和灵感。<details>
<summary>Abstract</summary>
With the rapid development of IT operations, it has become increasingly crucial to efficiently manage and analyze large volumes of data for practical applications. The techniques of Natural Language Processing (NLP) have shown remarkable capabilities for various tasks, including named entity recognition, machine translation and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various NLP downstream tasks. However, there is a lack of specialized LLMs for IT operations. In this paper, we introduce the OWL, a large language model trained on our collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks. Furthermore, we evaluate the performance of our OWL on the OWL-Bench established by us and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs.
</details>
<details>
<summary>摘要</summary>
随着信息技术运营的快速发展，管理和分析大量数据的效率已成为非常重要的。自然语言处理（NLP）技术在各种任务上表现出了惊人的能力，包括命名实体识别、机器翻译和对话系统。最近，大型自然语言模型（LLMs）在各种 NLP 下渠道任务上实现了显著的改进。然而，对 IT 运营的特殊化 LLMs 缺乏。本文介绍了 OWL，一个基于我们收集的 OWL-Instruct 数据集的大型自然语言模型，其中混合 adapter 策略可以在不同的领域或任务中进行参数高效调整。此外，我们评估了 OWL 在 OWL-Bench 和开放的 IT 相关benchmark上的性能，并发现 OWL 在 IT 任务上表现出了显著的优异性。此外，我们希望通过这些研究成果，为 IT 运营技术的发展提供更多的新思路和灵感。
</details></li>
</ul>
<hr>
<h2 id="Model-based-Subsampling-for-Knowledge-Graph-Completion"><a href="#Model-based-Subsampling-for-Knowledge-Graph-Completion" class="headerlink" title="Model-based Subsampling for Knowledge Graph Completion"></a>Model-based Subsampling for Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09296">http://arxiv.org/abs/2309.09296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xincanfeng/ms_kge">https://github.com/xincanfeng/ms_kge</a></li>
<li>paper_authors: Xincan Feng, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe</li>
<li>for: 提高 Knowledge Graph Embedding (KGE) 模型的适应性和性能</li>
<li>methods: 提出 Model-based Subsampling (MBS) 和 Mixed Subsampling (MIX) 方法，通过 KGE 模型的预测来估计不够频繁的查询的出现概率</li>
<li>results: 对 FB15k-237、WN18RR 和 YAGO3-10 等 dataset 进行评估，显示我们的提案的抽样方法可以提高受欢迎 KGE 模型的 KG 完成性能<details>
<summary>Abstract</summary>
Subsampling is effective in Knowledge Graph Embedding (KGE) for reducing overfitting caused by the sparsity in Knowledge Graph (KG) datasets. However, current subsampling approaches consider only frequencies of queries that consist of entities and their relations. Thus, the existing subsampling potentially underestimates the appearance probabilities of infrequent queries even if the frequencies of their entities or relations are high. To address this problem, we propose Model-based Subsampling (MBS) and Mixed Subsampling (MIX) to estimate their appearance probabilities through predictions of KGE models. Evaluation results on datasets FB15k-237, WN18RR, and YAGO3-10 showed that our proposed subsampling methods actually improved the KG completion performances for popular KGE models, RotatE, TransE, HAKE, ComplEx, and DistMult.
</details>
<details>
<summary>摘要</summary>
通过抽样可以有效地降低知识图（KG）数据集中的过拟合问题，但现有的抽样方法只考虑了实体和关系之间的频率。这可能会下降不常见的查询的出现概率，即使实体或关系的频率很高。为解决这个问题，我们提出了基于模型的抽样（MBS）和混合抽样（MIX）方法，通过KGE模型的预测来估计它们的出现概率。我们在FB15k-237、WN18RR和YAGO3-10等 dataset上进行了评估，结果表明，我们的提议的抽样方法实际上提高了各种KGE模型（RotatE、TransE、HAKE、ComplEx、DistMult）的KG完成性能。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Social-Discourse-to-Measure-Check-worthiness-of-Claims-for-Fact-checking"><a href="#Leveraging-Social-Discourse-to-Measure-Check-worthiness-of-Claims-for-Fact-checking" class="headerlink" title="Leveraging Social Discourse to Measure Check-worthiness of Claims for Fact-checking"></a>Leveraging Social Discourse to Measure Check-worthiness of Claims for Fact-checking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09274">http://arxiv.org/abs/2309.09274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megha Sundriyal, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: 本研究旨在提出一种细化的声明检查价值 Task，以优化现有的声明检查系统。</li>
<li>methods: 该研究使用了一个大量人工标注的Twitter数据集，并提出了一种基于人工评估的CheckMate方法，以 JOINTLY 确定声明是否值得检查，以及导致这个结论的因素。</li>
<li>results: 研究表明， integrating 多种因素可以提高声明检查的准确率和效率，并且人工评估 validate 了这些结论。<details>
<summary>Abstract</summary>
The expansion of online social media platforms has led to a surge in online content consumption. However, this has also paved the way for disseminating false claims and misinformation. As a result, there is an escalating demand for a substantial workforce to sift through and validate such unverified claims. Currently, these claims are manually verified by fact-checkers. Still, the volume of online content often outweighs their potency, making it difficult for them to validate every single claim in a timely manner. Thus, it is critical to determine which assertions are worth fact-checking and prioritize claims that require immediate attention. Multiple factors contribute to determining whether a claim necessitates fact-checking, encompassing factors such as its factual correctness, potential impact on the public, the probability of inciting hatred, and more. Despite several efforts to address claim check-worthiness, a systematic approach to identify these factors remains an open challenge. To this end, we introduce a new task of fine-grained claim check-worthiness, which underpins all of these factors and provides probable human grounds for identifying a claim as check-worthy. We present CheckIt, a manually annotated large Twitter dataset for fine-grained claim check-worthiness. We benchmark our dataset against a unified approach, CheckMate, that jointly determines whether a claim is check-worthy and the factors that led to that conclusion. We compare our suggested system with several baseline systems. Finally, we report a thorough analysis of results and human assessment, validating the efficacy of integrating check-worthiness factors in detecting claims worth fact-checking.
</details>
<details>
<summary>摘要</summary>
在线社交媒体平台的扩展导致在线内容的摄 consumption 增加，但也导致了假宣传和不准确信息的传播。因此，需要一大量的人力来筛选和验证这些未经证实的宣言。现在，这些宣言都是由 фак-checker 手动验证的。然而，在线内容的量太大，fact-checker 无法在有限时间内验证每一个宣言。因此，是非常重要的确定哪些宣言值得验证，并且需要立即验证的宣言。多种因素会影响哪些宣言需要验证，包括宣言的准确性、公众影响、激进的可能性和更多。虽然有很多努力来解决宣言验证价值的问题，但是一个系统atic approach 来确定这些因素仍然是一个开放的挑战。为此，我们介绍了一个新的细化的宣言验证任务，这个任务涵盖了所有这些因素，并提供了人类可靠的判据来确定宣言是否值得验证。我们提供了 CheckIt，一个手动标注的大型 Twitter 数据集，用于细化的宣言验证。我们对我们的数据集进行了对joint 方法 CheckMate 的比较，joint 方法可以同时判断一个宣言是否值得验证，以及这个宣言被验证的原因。我们与多个基线系统进行比较。最后，我们进行了详细的分析结果和人类评估，证明了将验证价值因素纳入检测宣言值得验证的效果。
</details></li>
</ul>
<hr>
<h2 id="Code-quality-assessment-using-transformers"><a href="#Code-quality-assessment-using-transformers" class="headerlink" title="Code quality assessment using transformers"></a>Code quality assessment using transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09264">http://arxiv.org/abs/2309.09264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mosleh Mahamud, Isak Samsten</li>
<li>for: 这个论文的目的是自动评估编程作业的正确性，但是编程任务可以有多种解决方案，其中许多方案尽管正确，但是代码具有冗长、糟糕的命名和重复等� субjective 质量。</li>
<li>methods: 这篇论文使用了 CodeBERT 来自动分配代码质量分数。作者们尝试了不同的模型和训练方法，并对一个新的代码质量评估数据集进行了实验。</li>
<li>results: 研究发现，代码质量有一定的可预测性，并且使用 transformer 基于的模型，使用任务适应预训练可以更有效地解决这个问题。<details>
<summary>Abstract</summary>
Automatically evaluate the correctness of programming assignments is rather straightforward using unit and integration tests. However, programming tasks can be solved in multiple ways, many of which, although correct, are inelegant. For instance, excessive branching, poor naming or repetitiveness make the code hard to understand and maintain. These subjective qualities of code are hard to automatically assess using current techniques. In this work we investigate the use of CodeBERT to automatically assign quality score to Java code. We experiment with different models and training paradigms. We explore the accuracy of the models on a novel dataset for code quality assessment. Finally, we assess the quality of the predictions using saliency maps. We find that code quality to some extent is predictable and that transformer based models using task adapted pre-training can solve the task more efficiently than other techniques.
</details>
<details>
<summary>摘要</summary>
自动评估程序作业正确性 relativamente直 forward 使用单元测试和集成测试。然而，程序任务可以通过多种方式解决，许多方法都是正确的，但是命名不佳、重复性强或分支过多，导致代码难以理解和维护。这些编程质量的subjective特征难以通过当前技术自动评估。在这种工作中，我们 investigate使用CodeBERT自动为Java代码分配质量分数。我们试用不同的模型和训练方法。我们探索一个新的代码质量评估数据集的准确性。最后，我们评估预测的质量使用Saliency Map。我们发现代码质量有一定的预测性，并且基于 transformer 模型的任务适应预处理可以更有效地解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-for-Text-Expansion-Datasets-Metrics-and-Baselines"><a href="#A-Benchmark-for-Text-Expansion-Datasets-Metrics-and-Baselines" class="headerlink" title="A Benchmark for Text Expansion: Datasets, Metrics, and Baselines"></a>A Benchmark for Text Expansion: Datasets, Metrics, and Baselines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09198">http://arxiv.org/abs/2309.09198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Chen, Haiyun Jiang, Wei Bi, Rui Wang, Longyue Wang, Shuming Shi, Ruifeng Xu</li>
<li>for: This paper presents a new task called Text Expansion (TE), which aims to insert fine-grained modifiers into plain text to make it more concrete and vivid.</li>
<li>methods: The authors use four complementary approaches to construct a dataset of 12 million automatically generated instances and 2K human-annotated references for both English and Chinese. They also design various metrics to evaluate the effectiveness of the expansions.</li>
<li>results: The proposed Locate&amp;Infill models demonstrate superiority over the Text2Text baselines, especially in expansion informativeness. Experiments verify the feasibility of the TE task and point out potential directions for future research.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文提出了一个新的文本扩展任务（TE），旨在插入细化修饰符到文本中，使其更加具体和生动。</li>
<li>methods: 作者使用了四种补充方法构建了一个包含12万自动生成实例和2000个人工标注的数据集，以及多种评价指标。</li>
<li>results: 提出的 Locate&amp;Infill 模型在扩展信息量方面表现出色，特别是在对 Text2Text 基elines 的比较中。实验证明了文本扩展任务的可行性，并指出了未来研究的可能性。<details>
<summary>Abstract</summary>
This work presents a new task of Text Expansion (TE), which aims to insert fine-grained modifiers into proper locations of the plain text to concretize or vivify human writings. Different from existing insertion-based writing assistance tasks, TE requires the model to be more flexible in both locating and generation, and also more cautious in keeping basic semantics. We leverage four complementary approaches to construct a dataset with 12 million automatically generated instances and 2K human-annotated references for both English and Chinese. To facilitate automatic evaluation, we design various metrics from multiple perspectives. In particular, we propose Info-Gain to effectively measure the informativeness of expansions, which is an important quality dimension in TE. On top of a pre-trained text-infilling model, we build both pipelined and joint Locate&Infill models, which demonstrate the superiority over the Text2Text baselines, especially in expansion informativeness. Experiments verify the feasibility of the TE task and point out potential directions for future research toward better automatic text expansion.
</details>
<details>
<summary>摘要</summary>
To create a dataset for this task, we use four complementary approaches to generate 12 million automatically generated instances and 2,000 human-annotated references for both English and Chinese. We also design various metrics to evaluate the effectiveness of the expansions, including a new metric called Info-Gain that measures the informativeness of the expansions.We build both pipelined and joint Locate&Infill models on top of a pre-trained text-infilling model, and compare them with Text2Text baselines. Our experiments show that the proposed models outperform the baselines, especially in terms of expansion informativeness. This demonstrates the feasibility of the TE task and provides a new direction for future research in automatic text expansion.
</details></li>
</ul>
<hr>
<h2 id="Detecting-covariate-drift-in-text-data-using-document-embeddings-and-dimensionality-reduction"><a href="#Detecting-covariate-drift-in-text-data-using-document-embeddings-and-dimensionality-reduction" class="headerlink" title="Detecting covariate drift in text data using document embeddings and dimensionality reduction"></a>Detecting covariate drift in text data using document embeddings and dimensionality reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10000">http://arxiv.org/abs/2309.10000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinayaksodar/nlp_drift_paper_code">https://github.com/vinayaksodar/nlp_drift_paper_code</a></li>
<li>paper_authors: Vinayak Sodar, Ankit Sekseria</li>
<li>for: 本研究旨在提高文本分析模型的可靠性和性能，通过检测文本数据中的covariate漂移。</li>
<li>methods: 本研究使用了三种文档嵌入：TF-IDF使用LSA进行维度减少， Doc2Vec和BERT嵌入，以及使用PCA进行维度减少。检测covariate漂移的方法包括KS统计和MMD测试。</li>
<li>results: 实验结果表明，某些嵌入、维度减少方法和漂移检测方法的组合表现较好，可以有效地检测文本数据中的covariate漂移。这些结果对于提高可靠的文本分析模型做出了贡献。<details>
<summary>Abstract</summary>
Detecting covariate drift in text data is essential for maintaining the reliability and performance of text analysis models. In this research, we investigate the effectiveness of different document embeddings, dimensionality reduction techniques, and drift detection methods for identifying covariate drift in text data. We explore three popular document embeddings: term frequency-inverse document frequency (TF-IDF) using Latent semantic analysis(LSA) for dimentionality reduction and Doc2Vec, and BERT embeddings, with and without using principal component analysis (PCA) for dimensionality reduction. To quantify the divergence between training and test data distributions, we employ the Kolmogorov-Smirnov (KS) statistic and the Maximum Mean Discrepancy (MMD) test as drift detection methods. Experimental results demonstrate that certain combinations of embeddings, dimensionality reduction techniques, and drift detection methods outperform others in detecting covariate drift. Our findings contribute to the advancement of reliable text analysis models by providing insights into effective approaches for addressing covariate drift in text data.
</details>
<details>
<summary>摘要</summary>
检测文本数据中的变量漂移是维护文本分析模型的可靠性和性能的关键。在这个研究中，我们研究了不同的文档嵌入、维度减少技术和变量漂移检测方法，以确定在文本数据中检测变量漂移的效果。我们探索了三种流行的文档嵌入：term frequency-inverse document frequency（TF-IDF）使用隐藏语义分析（LSA）进行维度减少，Doc2Vec和BERT嵌入，并使用主成分分析（PCA）进行维度减少。为了量化训练和测试数据分布之间的差异，我们采用了科维莫洛夫-斯密涅夫（KS）统计和最大均值差（MMD）测试作为变量漂移检测方法。实验结果表明，某些组合的嵌入、维度减少技术和变量漂移检测方法在检测变量漂移方面表现出色。我们的发现对于建立可靠的文本分析模型做出了贡献，提供了有关有效地在文本数据中检测变量漂移的信息。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.CL_2023_09_17/" data-id="cloimip7100ats488du84h0e3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/cs.LG_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T10:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/cs.LG_2023_09_17/">cs.LG - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-Over-Smoothing-and-Over-Squashing-using-Augmentations-of-Forman-Ricci-Curvature"><a href="#Mitigating-Over-Smoothing-and-Over-Squashing-using-Augmentations-of-Forman-Ricci-Curvature" class="headerlink" title="Mitigating Over-Smoothing and Over-Squashing using Augmentations of Forman-Ricci Curvature"></a>Mitigating Over-Smoothing and Over-Squashing using Augmentations of Forman-Ricci Curvature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09384">http://arxiv.org/abs/2309.09384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Fesser, Melanie Weber</li>
<li>For: This paper proposes a rewiring technique based on Augmented Forman-Ricci curvature (AFRC) to mitigate over-smoothing and over-squashing effects in message-passing Graph Neural Networks (GNNs).* Methods: The proposed technique uses AFRC, a scalable curvature notation that can be computed in linear time, to characterize over-smoothing and over-squashing effects in GNNs.* Results: The proposed approach achieves state-of-the-art performance while significantly reducing the computational cost in comparison with other methods. The paper also provides effective heuristics for hyperparameters in curvature-based rewiring, which avoids expensive hyperparameter searches.<details>
<summary>Abstract</summary>
While Graph Neural Networks (GNNs) have been successfully leveraged for learning on graph-structured data across domains, several potential pitfalls have been described recently. Those include the inability to accurately leverage information encoded in long-range connections (over-squashing), as well as difficulties distinguishing the learned representations of nearby nodes with growing network depth (over-smoothing). An effective way to characterize both effects is discrete curvature: Long-range connections that underlie over-squashing effects have low curvature, whereas edges that contribute to over-smoothing have high curvature. This observation has given rise to rewiring techniques, which add or remove edges to mitigate over-smoothing and over-squashing. Several rewiring approaches utilizing graph characteristics, such as curvature or the spectrum of the graph Laplacian, have been proposed. However, existing methods, especially those based on curvature, often require expensive subroutines and careful hyperparameter tuning, which limits their applicability to large-scale graphs. Here we propose a rewiring technique based on Augmented Forman-Ricci curvature (AFRC), a scalable curvature notation, which can be computed in linear time. We prove that AFRC effectively characterizes over-smoothing and over-squashing effects in message-passing GNNs. We complement our theoretical results with experiments, which demonstrate that the proposed approach achieves state-of-the-art performance while significantly reducing the computational cost in comparison with other methods. Utilizing fundamental properties of discrete curvature, we propose effective heuristics for hyperparameters in curvature-based rewiring, which avoids expensive hyperparameter searches, further improving the scalability of the proposed approach.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经成功地应用于不同领域的图数据上，但最近有一些潜在的坑害被描述了。这些坑害包括不能准确利用图中长距离连接中的信息 (过滤)，以及随着网络深度增加而导致近节点的学习表现相似化 (过滤)。一种有效的方式来描述这两种效果是离散曲率：图中长距离连接的离散曲率较低，而导致过滤的边的离散曲率较高。这一观察引起了重新连接技术的出现，这些技术通过添加或 removing 边来缓解过滤和过滤的问题。已有一些基于图特性的重新连接方法，如曲率或图laplacian的谱，被提出。然而，现有的方法，特别是基于曲率的方法，经常需要费时的优化和精心调整 hyperparameter，这限制了它们在大规模图上的应用。我们提出了基于 Augmented Forman-Ricci curvature (AFRC) 的重新连接技术，AFRC 是一种可以在线时间内计算的离散曲率表示法。我们证明 AFRC 能够有效地描述 GNN 中的过滤和过滤效果。我们通过实验证明，我们的方法可以 achieve state-of-the-art 性能，同时减少了与其他方法相比的计算成本。利用离散曲率的基本属性，我们提出了一些有效的启发式 hyperparameter 优化方法，以避免费时的寻找优化方法，进一步提高了我们的方法的可扩展性。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-in-Temporal-Heterogeneity"><a href="#Federated-Learning-in-Temporal-Heterogeneity" class="headerlink" title="Federated Learning in Temporal Heterogeneity"></a>Federated Learning in Temporal Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09381">http://arxiv.org/abs/2309.09381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junghwan Lee</li>
<li>for: 本研究探讨了 federated learning 中的时间不同客户端上的 temporally 不一致问题。</li>
<li>methods: 我们提出了一种基于 empirical 观察的方法来 mitigate temporally 不一致问题，以便更有效地进行 federated learning。</li>
<li>results: 我们发现 global model 使用 fix-length sequence 更快地 converges than varying-length sequence。<details>
<summary>Abstract</summary>
In this work, we explored federated learning in temporal heterogeneity across clients. We observed that global model obtained by \texttt{FedAvg} trained with fixed-length sequences shows faster convergence than varying-length sequences. We proposed methods to mitigate temporal heterogeneity for efficient federated learning based on the empirical observation.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们探索了联邦学习中的时间不同客户端之间的差异。我们发现，使用固定长度序列的\texttt{FedAvg}训练的全局模型在更快地尝试了。我们提出了一些缓解时间差异的方法，以便有效地进行联邦学习，基于实际观察。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Fully-Convolutional-Generative-Machine-Learning-Method-for-Accelerating-Non-Equilibrium-Greens-Function-Simulations"><a href="#Fully-Convolutional-Generative-Machine-Learning-Method-for-Accelerating-Non-Equilibrium-Greens-Function-Simulations" class="headerlink" title="Fully Convolutional Generative Machine Learning Method for Accelerating Non-Equilibrium Greens Function Simulations"></a>Fully Convolutional Generative Machine Learning Method for Accelerating Non-Equilibrium Greens Function Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09374">http://arxiv.org/abs/2309.09374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Preslav Aleksandrov, Ali Rezaei, Nikolas Xeni, Tapas Dutta, Asen Asenov, Vihar Georgiev</li>
<li>for: 这篇论文描述了一种新的模拟方法，它结合机器学习和设备模拟。这种模拟方法基于量子力学非平衡绿函数（NEGF）方法，并使用扩展到卷积生成网络。我们称之为ML-NEGF方法，并在我们的内部 simulate 软件（NESS）中实现。</li>
<li>methods: 这种模拟方法使用了机器学习方法，卷积生成网络来学习奈米薄膜晶体管的物理行为。</li>
<li>results: 据报告，ML-NEGF方法可以在相同精度下提高模拟速度，减少计算时间，平均提高了60%。<details>
<summary>Abstract</summary>
This work describes a novel simulation approach that combines machine learning and device modelling simulations. The device simulations are based on the quantum mechanical non-equilibrium Greens function (NEGF) approach and the machine learning method is an extension to a convolutional generative network. We have named our new simulation approach ML-NEGF and we have implemented it in our in-house simulator called NESS (nano-electronics simulations software). The reported results demonstrate the improved convergence speed of the ML-NEGF method in comparison to the standard NEGF approach. The trained ML model effectively learns the underlying physics of nano-sheet transistor behaviour, resulting in faster convergence of the coupled Poisson-NEGF simulations. Quantitatively, our ML- NEGF approach achieves an average convergence acceleration of 60%, substantially reducing the computational time while maintaining the same accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-on-Congestion-Control-and-Scheduling-for-Multipath-TCP-Machine-Learning-vs-Classical-Approaches"><a href="#A-Survey-on-Congestion-Control-and-Scheduling-for-Multipath-TCP-Machine-Learning-vs-Classical-Approaches" class="headerlink" title="A Survey on Congestion Control and Scheduling for Multipath TCP: Machine Learning vs Classical Approaches"></a>A Survey on Congestion Control and Scheduling for Multipath TCP: Machine Learning vs Classical Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09372">http://arxiv.org/abs/2309.09372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maisha Maliha, Golnaz Habibi, Mohammed Atiquzzaman</li>
<li>for: 本研究旨在解决多路 TCP (MPTCP) 中的几个问题，包括流量占用和延迟控制。</li>
<li>methods: 本研究使用两种主要方法：非数据驱动（传统）方法和数据驱动（机器学习）方法。</li>
<li>results: 本研究对这两种方法的优缺点进行比较，并提供实际环境中 MPCTP 的实现和模拟。<details>
<summary>Abstract</summary>
Multipath TCP (MPTCP) has been widely used as an efficient way for communication in many applications. Data centers, smartphones, and network operators use MPTCP to balance the traffic in a network efficiently. MPTCP is an extension of TCP (Transmission Control Protocol), which provides multiple paths, leading to higher throughput and low latency. Although MPTCP has shown better performance than TCP in many applications, it has its own challenges. The network can become congested due to heavy traffic in the multiple paths (subflows) if the subflow rates are not determined correctly. Moreover, communication latency can occur if the packets are not scheduled correctly between the subflows. This paper reviews techniques to solve the above-mentioned problems based on two main approaches; non data-driven (classical) and data-driven (Machine Learning) approaches. This paper compares these two approaches and highlights their strengths and weaknesses with a view to motivating future researchers in this exciting area of machine learning for communications. This paper also provides details on the simulation of MPTCP and its implementations in real environments.
</details>
<details>
<summary>摘要</summary>
multipath TCP (MPTCP) 已经广泛应用于许多应用程序中，以提高网络吞吐量和低延迟。数据中心、智能手机和网络运营商都使用 MPTCP 来均衡网络流量。MPTCP 是 TCP（传输控制协议）的扩展，它提供多个路径，从而实现更高的吞吐量和低延迟。虽然 MPTCP 在许多应用中表现了更好的性能，但它还存在一些挑战。如果多个流（subflow）的流量不是正确地确定的话，网络就可能变得拥堵。此外，如果包没有正确地安排的话，则会出现交通延迟。本文评论了解决上述问题的两种方法：非数据驱动（传统）方法和数据驱动（机器学习）方法。本文比较这两种方法的优劣，并强调它们在这一领域的挑战和未来研究的可能性。此外，本文还提供了 MPTCP 的模拟和实现在真实环境中的细节。
</details></li>
</ul>
<hr>
<h2 id="An-Automatic-Tuning-MPC-with-Application-to-Ecological-Cruise-Control"><a href="#An-Automatic-Tuning-MPC-with-Application-to-Ecological-Cruise-Control" class="headerlink" title="An Automatic Tuning MPC with Application to Ecological Cruise Control"></a>An Automatic Tuning MPC with Application to Ecological Cruise Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09358">http://arxiv.org/abs/2309.09358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Abtahi, Mahdis Rabbani, Shima Nazari</li>
<li>for: 这个论文是为了研究模型预测控制（MPC）的自动调整方法，以优化MPC控制器的性能。</li>
<li>methods: 该论文使用了动态Programming和神经网络来解决MPC控制器的自动调整问题，并在online操作中使用预览信息来适应道路坡度。</li>
<li>results:  simulations results show that the proposed approach can effectively optimize the fuel consumption of the ecological cruise control system under different road geometries.<details>
<summary>Abstract</summary>
Model predictive control (MPC) is a powerful tool for planning and controlling dynamical systems due to its capacity for handling constraints and taking advantage of preview information. Nevertheless, MPC performance is highly dependent on the choice of cost function tuning parameters. In this work, we demonstrate an approach for online automatic tuning of an MPC controller with an example application to an ecological cruise control system that saves fuel by using a preview of road grade. We solve the global fuel consumption minimization problem offline using dynamic programming and find the corresponding MPC cost function by solving the inverse optimization problem. A neural network fitted to these offline results is used to generate the desired MPC cost function weight during online operation. The effectiveness of the proposed approach is verified in simulation for different road geometries.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Structure-to-Property-Chemical-Element-Embeddings-and-a-Deep-Learning-Approach-for-Accurate-Prediction-of-Chemical-Properties"><a href="#Structure-to-Property-Chemical-Element-Embeddings-and-a-Deep-Learning-Approach-for-Accurate-Prediction-of-Chemical-Properties" class="headerlink" title="Structure to Property: Chemical Element Embeddings and a Deep Learning Approach for Accurate Prediction of Chemical Properties"></a>Structure to Property: Chemical Element Embeddings and a Deep Learning Approach for Accurate Prediction of Chemical Properties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09355">http://arxiv.org/abs/2309.09355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmamur/elembert">https://github.com/dmamur/elembert</a></li>
<li>paper_authors: Shokirbek Shermukhamedov, Dilorom Mamurjonova, Michael Probst</li>
<li>for: 本研究使用机器学习技术来预测分子性质，加速药物发现和材料设计。</li>
<li>methods: 本研究使用深度学习技术，包括多层编码器和解码器架构，进行分类任务。</li>
<li>results: 我们通过应用这种方法，在不同的输入数据上 achiev 高度预测力，例如在Matbench和Moleculenetbenchmarks上，并进行了分子数据表示 вектор的全面分析，揭示了分子数据中的下意识模式。<details>
<summary>Abstract</summary>
The application of machine learning (ML) techniques in computational chemistry has led to significant advances in predicting molecular properties, accelerating drug discovery, and material design. ML models can extract hidden patterns and relationships from complex and large datasets, allowing for the prediction of various chemical properties with high accuracy. The use of such methods has enabled the discovery of molecules and materials that were previously difficult to identify. This paper introduces a new ML model based on deep learning techniques, such as a multilayer encoder and decoder architecture, for classification tasks. We demonstrate the opportunities offered by our approach by applying it to various types of input data, including organic and inorganic compounds. In particular, we developed and tested the model using the Matbench and Moleculenet benchmarks, which include crystal properties and drug design-related benchmarks. We also conduct a comprehensive analysis of vector representations of chemical compounds, shedding light on the underlying patterns in molecular data. The models used in this work exhibit a high degree of predictive power, underscoring the progress that can be made with refined machine learning when applied to molecular and material datasets. For instance, on the Tox21 dataset, we achieved an average accuracy of 96%, surpassing the previous best result by 10%. Our code is publicly available at https://github.com/dmamur/elembert.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）技术在计算化学中应用得到了显著的进步，包括预测分子性质、加速药物发现和材料设计。ML模型可以从复杂大量数据中提取隐藏的模式和关系，以高度准确地预测各种化学性质。这种方法的应用使得可以发现 previously difficult to identify的分子和材料。本文介绍了一种基于深度学习技术的新ML模型，包括多层编码器和解码器建筑，用于分类任务。我们通过应用这种方法于不同类型的输入数据，包括有机和无机化合物，证明了我们的方法的可行性。具体来说，我们使用了Matbench和Moleculenetbenchmark，包括晶体性和药物设计相关的benchmark，进行了全面的分子表示vector分析，揭示了分子数据中的下面纲。使用的模型在这个工作中表现出了高度预测力，这将进一步推动了对分子和材料数据的机器学习应用。例如，在Tox21dataset上，我们实现了96%的平均准确率，比前一个最佳结果高出10%。我们的代码可以在https://github.com/dmamur/elembert上下载。
</details></li>
</ul>
<hr>
<h2 id="Simulation-based-Inference-for-Exoplanet-Atmospheric-Retrieval-Insights-from-winning-the-Ariel-Data-Challenge-2023-using-Normalizing-Flows"><a href="#Simulation-based-Inference-for-Exoplanet-Atmospheric-Retrieval-Insights-from-winning-the-Ariel-Data-Challenge-2023-using-Normalizing-Flows" class="headerlink" title="Simulation-based Inference for Exoplanet Atmospheric Retrieval: Insights from winning the Ariel Data Challenge 2023 using Normalizing Flows"></a>Simulation-based Inference for Exoplanet Atmospheric Retrieval: Insights from winning the Ariel Data Challenge 2023 using Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09337">http://arxiv.org/abs/2309.09337</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/astroai-cfa/ariel_data_challenge_2023_solution">https://github.com/astroai-cfa/ariel_data_challenge_2023_solution</a></li>
<li>paper_authors: Mayeul Aubin, Carolina Cuesta-Lazaro, Ethan Tregidga, Javier Viaña, Cecilia Garraffo, Iouli E. Gordon, Mercedes López-Morales, Robert J. Hargreaves, Vladimir Yu. Makhnev, Jeremy J. Drake, Douglas P. Finkbeiner, Phillip Cargile</li>
<li>for: 这项研究旨在提出新的机器学习模型，用于分析外层行星大气层谱。</li>
<li>methods: 该研究使用了 Normalizing Flows 技术，预测大气参数的 posterior 分布下不同大气假设。</li>
<li>results: 研究发现了一种新的机器学习模型，可以更高效地分析外层行星大气层谱。此外，研究还发现了一种更高性能的模型，即使在挑战中获得较低分而然。这些发现表明需要重新评估评价指标，并且探索更加高效和准确的方法来分析外层行星大气层谱。<details>
<summary>Abstract</summary>
Advancements in space telescopes have opened new avenues for gathering vast amounts of data on exoplanet atmosphere spectra. However, accurately extracting chemical and physical properties from these spectra poses significant challenges due to the non-linear nature of the underlying physics.   This paper presents novel machine learning models developed by the AstroAI team for the Ariel Data Challenge 2023, where one of the models secured the top position among 293 competitors. Leveraging Normalizing Flows, our models predict the posterior probability distribution of atmospheric parameters under different atmospheric assumptions.   Moreover, we introduce an alternative model that exhibits higher performance potential than the winning model, despite scoring lower in the challenge. These findings highlight the need to reevaluate the evaluation metric and prompt further exploration of more efficient and accurate approaches for exoplanet atmosphere spectra analysis.   Finally, we present recommendations to enhance the challenge and models, providing valuable insights for future applications on real observational data. These advancements pave the way for more effective and timely analysis of exoplanet atmospheric properties, advancing our understanding of these distant worlds.
</details>
<details>
<summary>摘要</summary>
This paper presents new machine learning models developed by the AstroAI team for the Ariel Data Challenge 2023. One of our models achieved the top position among 293 competitors by leveraging Normalizing Flows to predict the posterior probability distribution of atmospheric parameters under different atmospheric assumptions.Furthermore, we introduce an alternative model that exhibits higher performance potential than the winning model, despite scoring lower in the challenge. These findings highlight the need to reevaluate the evaluation metric and prompt further exploration of more efficient and accurate approaches for analyzing exoplanet atmosphere spectra.Finally, we provide recommendations to enhance the challenge and models, offering valuable insights for future applications on real observational data. These advancements pave the way for more effective and timely analysis of exoplanet atmospheric properties, deepening our understanding of these distant worlds.
</details></li>
</ul>
<hr>
<h2 id="Experiential-Informed-Data-Reconstruction-for-Fishery-Sustainability-and-Policies-in-the-Azores"><a href="#Experiential-Informed-Data-Reconstruction-for-Fishery-Sustainability-and-Policies-in-the-Azores" class="headerlink" title="Experiential-Informed Data Reconstruction for Fishery Sustainability and Policies in the Azores"></a>Experiential-Informed Data Reconstruction for Fishery Sustainability and Policies in the Azores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09326">http://arxiv.org/abs/2309.09326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brenda Nogueira, Gui M. Menezes, Nuno Moniz</li>
<li>for: 本研究的目的是重建附近葡萄牙阿鲁亚群岛渔业数据集（2010-2017年），以便更好地了解渔业捕捞方法对海洋生态系统的影响。</li>
<li>methods: 本研究使用了域知和机器学习方法来恢复数据集，并通过对每个鱼类捕捞数据进行分析来推断渔业工具的使用情况。</li>
<li>results: 研究结果表明，通过使用不同的模型方法可以有效地重建数据集，并提供了新的视角对不同渔业的行为和时间的影响，这些信息对未来鱼类人口评估和管理具有重要意义。<details>
<summary>Abstract</summary>
Fishery analysis is critical in maintaining the long-term sustainability of species and the livelihoods of millions of people who depend on fishing for food and income. The fishing gear, or metier, is a key factor significantly impacting marine habitats, selectively targeting species and fish sizes. Analysis of commercial catches or landings by metier in fishery stock assessment and management is crucial, providing robust estimates of fishing efforts and their impact on marine ecosystems. In this paper, we focus on a unique data set from the Azores' fishing data collection programs between 2010 and 2017, where little information on metiers is available and sparse throughout our timeline. Our main objective is to tackle the task of data set reconstruction, leveraging domain knowledge and machine learning methods to retrieve or associate metier-related information to each fish landing. We empirically validate the feasibility of this task using a diverse set of modeling approaches and demonstrate how it provides new insights into different fisheries' behavior and the impact of metiers over time, which are essential for future fish population assessments, management, and conservation efforts.
</details>
<details>
<summary>摘要</summary>
鱼业分析是维护生物种和渔业生产的长期可持续性的关键。鱼网（metier）是影响海洋生态系统的关键因素，可以选择性地目标种类和鱼的大小。在鱼业资源评估和管理中，商业捕捞数据的分析是非常重要的，可以提供坚实的捕捞努力和海洋生态系统的影响。本文关注Azores鱼业数据收集计划在2010年至2017年之间的独特数据集，因为这个数据集中有少量的鱼网信息，并且这些信息在时间线上是罕见的。我们的主要目标是使用领域知识和机器学习方法来重建这个数据集，并将鱼网相关信息与每个鱼投射相关联。我们通过多种模型方法进行实验验证，并证明这个任务的可行性，从而提供新的鱼业行为和鱼网的影响情况，这些信息对未来鱼种评估、管理和保护具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Kinematics-aware-Trajectory-Generation-and-Prediction-with-Latent-Stochastic-Differential-Modeling"><a href="#Kinematics-aware-Trajectory-Generation-and-Prediction-with-Latent-Stochastic-Differential-Modeling" class="headerlink" title="Kinematics-aware Trajectory Generation and Prediction with Latent Stochastic Differential Modeling"></a>Kinematics-aware Trajectory Generation and Prediction with Latent Stochastic Differential Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09317">http://arxiv.org/abs/2309.09317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruochen Jiao, Yixuan Wang, Xiangguo Liu, Chao Huang, Qi Zhu</li>
<li>for: 本研究旨在提高自动驾驶车辆的路径生成和预测能力，以便在开发和运行过程中更好地处理复杂的交通enario。</li>
<li>methods: 我们 integrate了机械知识和神经网络泊brace(SDE)，并基于novel latent kinematics-aware SDE（LK-SDE）开发了一种variational autoencoder，以生成车辆运动。我们的方法结合了模型基于和深度学习基于技术的优点。</li>
<li>results: 我们的方法在生成和预测车辆路径时比基eline方法表现出色，生成的路径更加真实、物理可行和精度可控。<details>
<summary>Abstract</summary>
Trajectory generation and trajectory prediction are two critical tasks for autonomous vehicles, which generate various trajectories during development and predict the trajectories of surrounding vehicles during operation, respectively. However, despite significant advances in improving their performance, it remains a challenging problem to ensure that the generated/predicted trajectories are realistic, explainable, and physically feasible. Existing model-based methods provide explainable results, but are constrained by predefined model structures, limiting their capabilities to address complex scenarios. Conversely, existing deep learning-based methods have shown great promise in learning various traffic scenarios and improving overall performance, but they often act as opaque black boxes and lack explainability. In this work, we integrate kinematic knowledge with neural stochastic differential equations (SDE) and develop a variational autoencoder based on a novel latent kinematics-aware SDE (LK-SDE) to generate vehicle motions. Our approach combines the advantages of both model-based and deep learning-based techniques. Experimental results demonstrate that our method significantly outperforms baseline approaches in producing realistic, physically-feasible, and precisely-controllable vehicle trajectories, benefiting both generation and prediction tasks.
</details>
<details>
<summary>摘要</summary>
几何轨迹生成和预测是自动车的两个关键任务，它们在开发过程中产生了许多轨迹，并在运行过程中预测周围车辆的轨迹。然而，即使有了重要的进步，仍然是一个挑战性的问题，确保生成/预测的轨迹是现实、可解释和物理可行的。现有的模型基方法可以提供可解释的结果，但它们受限于预先定义的模型结构，导致它们无法处理复杂的enario。相反，现有的深度学习基本方法在学习不同的交通enario中表现出色，但它们经常作为透明的黑盒子，无法提供可解释的结果。在这个工作中，我们结合了几何知识和神经统计学 differential equation (SDE)，开发了一个基于novel latent kinematics-aware SDE (LK-SDE)的抽象自动车动作统计模型。我们的方法结合了模型基的优点和深度学习基的优点。实验结果显示，我们的方法与基准方法相比，在生成和预测轨迹任务中表现出色，具有现实、物理可行和精确控制的轨迹。
</details></li>
</ul>
<hr>
<h2 id="Energy-stable-neural-network-for-gradient-flow-equations"><a href="#Energy-stable-neural-network-for-gradient-flow-equations" class="headerlink" title="Energy stable neural network for gradient flow equations"></a>Energy stable neural network for gradient flow equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10002">http://arxiv.org/abs/2309.10002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganghua Fan, Tianyu Jin, Yuan Lan, Yang Xiang, Luchan Zhang</li>
<li>for: 解决梯度流方程 equations 的方法</li>
<li>methods: 使用一种基于副变量的等价形式来更新解决方案，并使用一些能量衰退块来实现梯度流方程的演化过程中的稳定性</li>
<li>results: 通过实验证明，该网络能够生成高精度和稳定的预测结果<details>
<summary>Abstract</summary>
In this paper, we propose an energy stable network (EStable-Net) for solving gradient flow equations. The solution update scheme in our neural network EStable-Net is inspired by a proposed auxiliary variable based equivalent form of the gradient flow equation. EStable-Net enables decreasing of a discrete energy along the neural network, which is consistent with the property in the evolution process of the gradient flow equation. The architecture of the neural network EStable-Net consists of a few energy decay blocks, and the output of each block can be interpreted as an intermediate state of the evolution process of the gradient flow equation. This design provides a stable, efficient and interpretable network structure. Numerical experimental results demonstrate that our network is able to generate high accuracy and stable predictions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种能量稳定网络（EStable-Net），用于解决梯度流方程。我们的神经网络EStable-Net中的解决方案是基于提出的辅助变量基于等效形式的梯度流方程的想法。EStable-Net使得梯度流方程中的能量逐渐减少，与演化过程中的性质相一致。神经网络EStable-Net的架构包括一些能量衰减块，每个块的输出可以被解释为梯度流方程的演化过程中的中间状态。这种设计提供了稳定、高效和可解释的网络结构。数值实验结果表明，我们的网络能够生成高精度和稳定的预测。
</details></li>
</ul>
<hr>
<h2 id="Global-Convergence-of-SGD-For-Logistic-Loss-on-Two-Layer-Neural-Nets"><a href="#Global-Convergence-of-SGD-For-Logistic-Loss-on-Two-Layer-Neural-Nets" class="headerlink" title="Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets"></a>Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09258">http://arxiv.org/abs/2309.09258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pulkit Gopalani, Samyak Jha, Anirbit Mukherjee</li>
<li>for: 这个论文目的是证明SGD可以快速收敛到深度为2的抽象函数网络的全局最低点，无论数据是什么样的，Activation函数是否是sigmoid或tanh。</li>
<li>methods: 这个论文使用了SGD算法，并证明了其在 kontinuous time 下的快速收敛速率，包括使用SoftPlus activation function。</li>
<li>results: 论文证明了SGD在这些对象函数上的快速收敛，并且适用于任何数据和Activation函数。<details>
<summary>Abstract</summary>
In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates with adequately smooth and bounded activations like sigmoid and tanh. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence of Frobenius norm regularized logistic loss functions on constant-sized neural nets which are "Villani functions" and thus be able to build on recent progress with analyzing SGD on such objectives.
</details>
<details>
<summary>摘要</summary>
在这份备忘录中，我们证明了SGD在适当规则化的Logistic Empirical Risk函数的深度为2的神经网络上具有首次性的可证明收敛性，包括任意数据和任意数量的门控件，以及具有适当的平滑和缓冲的活化函数，如sigmoid和tanh。我们还证明了SGD在继续时间下的快速收敛速率，其适用于饱和不bounded的活化函数，如SoftPlus。我们的关键想法是证明常量大小神经网络上的Frobenius norm规则化Logistic loss函数是"Villani函数"，因此可以基于最近的SGD分析进程建立。
</details></li>
</ul>
<hr>
<h2 id="User-Assignment-and-Resource-Allocation-for-Hierarchical-Federated-Learning-over-Wireless-Networks"><a href="#User-Assignment-and-Resource-Allocation-for-Hierarchical-Federated-Learning-over-Wireless-Networks" class="headerlink" title="User Assignment and Resource Allocation for Hierarchical Federated Learning over Wireless Networks"></a>User Assignment and Resource Allocation for Hierarchical Federated Learning over Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09253">http://arxiv.org/abs/2309.09253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tinghao Zhang, Kwok-Yan Lam, Jun Zhao</li>
<li>for: 这篇论文主要关注于提高处理器的能效性和延迟时间，并且解决资料隐私问题。</li>
<li>methods: 这篇论文提出了一个弹性 Federated Learning（HFL）架构，并且提出了两个算法来优化资源分配和用户分配。</li>
<li>results: 实验结果显示，这个提案的HFL架构可以对现有的研究进行能源和延迟时间的明显优化。<details>
<summary>Abstract</summary>
The large population of wireless users is a key driver of data-crowdsourced Machine Learning (ML). However, data privacy remains a significant concern. Federated Learning (FL) encourages data sharing in ML without requiring data to leave users' devices but imposes heavy computation and communications overheads on mobile devices. Hierarchical FL (HFL) alleviates this problem by performing partial model aggregation at edge servers. HFL can effectively reduce energy consumption and latency through effective resource allocation and appropriate user assignment. Nevertheless, resource allocation in HFL involves optimizing multiple variables, and the objective function should consider both energy consumption and latency, making the development of resource allocation algorithms very complicated. Moreover, it is challenging to perform user assignment, which is a combinatorial optimization problem in a large search space. This article proposes a spectrum resource optimization algorithm (SROA) and a two-stage iterative algorithm (TSIA) for HFL. Given an arbitrary user assignment pattern, SROA optimizes CPU frequency, transmit power, and bandwidth to minimize system cost. TSIA aims to find a user assignment pattern that considerably reduces the total system cost. Experimental results demonstrate the superiority of the proposed HFL framework over existing studies in energy and latency reduction.
</details>
<details>
<summary>摘要</summary>
大量无线用户人群是数据拥有者学习（ML）的关键驱动力，但数据隐私保护仍然是一大问题。联邦学习（FL）鼓励数据在用户设备上进行学习，而不需要数据离开用户设备，但是在移动设备上进行计算和通信 overhead 占用了大量资源。层次联邦学习（HFL）解决了这个问题，通过在边缘服务器进行部分模型聚合来减少计算和通信 overhead。HFL 可以有效降低能源消耗和延迟，通过有效的资源分配和合适的用户分配。但是，资源分配在 HFL 中包括优化多个变量的问题，并且目标函数应该考虑到能源消耗和延迟两个方面，这使得资源分配算法的开发变得非常复杂。另外，用户分配是一个具有大量搜索空间的启发式优化问题。本文提出了一种spectrum resource optimization algorithm（SROA）和一种两个阶段迭代算法（TSIA）来解决 HFL 中的资源分配和用户分配问题。给定任意用户分配模式，SROA 将在用户设备上优化 CPU 频率、发射功率和带宽，以最小化系统成本。TSIA 则是一种希望找到一个考虑到总系统成本的用户分配模式。实验结果表明，提出的 HFL 框架在能源和延迟两个方面都有较好的性能。
</details></li>
</ul>
<hr>
<h2 id="High-dimensional-manifold-of-solutions-in-neural-networks-insights-from-statistical-physics"><a href="#High-dimensional-manifold-of-solutions-in-neural-networks-insights-from-statistical-physics" class="headerlink" title="High-dimensional manifold of solutions in neural networks: insights from statistical physics"></a>High-dimensional manifold of solutions in neural networks: insights from statistical physics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09240">http://arxiv.org/abs/2309.09240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enrico M. Malatesta</li>
<li>for: 这篇论文探讨了神经网络的统计力学方法，尤其是使用 binary 和连续权重的 perceptron 架构，在分类设定下。</li>
<li>methods: 论文使用了 Gardner 的 replica 方法，derived SAT&#x2F;UNSAT 转变在存储设定下。</li>
<li>results: 论文发现了 zero 训练错误配置的几何排序，并如何这种排序随训练集大小的增加而改变。 论文还证明了，在 binary 权重模型中，算法困难是因为解决区域的消失，这个区域可以到非常大的距离。最后，论文表明了研究线性模式连接 между解决方案可以提供解决批处理的平均形状的信息。<details>
<summary>Abstract</summary>
In these pedagogic notes I review the statistical mechanics approach to neural networks, focusing on the paradigmatic example of the perceptron architecture with binary an continuous weights, in the classification setting. I will review the Gardner's approach based on replica method and the derivation of the SAT/UNSAT transition in the storage setting. Then, I discuss some recent works that unveiled how the zero training error configurations are geometrically arranged, and how this arrangement changes as the size of the training set increases. I also illustrate how different regions of solution space can be explored analytically and how the landscape in the vicinity of a solution can be characterized. I give evidence how, in binary weight models, algorithmic hardness is a consequence of the disappearance of a clustered region of solutions that extends to very large distances. Finally, I demonstrate how the study of linear mode connectivity between solutions can give insights into the average shape of the solution manifold.
</details>
<details>
<summary>摘要</summary>
这些教学笔记中，我将对神经网络的统计力学方法进行介绍，以某种类型的感知器架构为例，并将着眼于分类设置下的情况。我将详细介绍加德纳的方法，包括使用复制方法的 derivation，以及存储设置下的 SAT/UNSAT 转变。然后，我会讨论一些最近的研究，描述了 zero training error 配置的几何排布，以及这个排布如何随训练集大小的变化。我还会说明如何在解决空间中分析不同区域的解，以及在解近 vicinity 中描述解决方案的场景。最后，我会展示如何在 binary weight 模型中，算法困难性是因为解决空间中的集中区域消失。此外，我还会讨论如何通过 linear mode 连接 между解来了解解决方案的平均形状。
</details></li>
</ul>
<hr>
<h2 id="Globally-Convergent-Accelerated-Algorithms-for-Multilinear-Sparse-Logistic-Regression-with-ell-0-constraints"><a href="#Globally-Convergent-Accelerated-Algorithms-for-Multilinear-Sparse-Logistic-Regression-with-ell-0-constraints" class="headerlink" title="Globally Convergent Accelerated Algorithms for Multilinear Sparse Logistic Regression with $\ell_0$-constraints"></a>Globally Convergent Accelerated Algorithms for Multilinear Sparse Logistic Regression with $\ell_0$-constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09239">http://arxiv.org/abs/2309.09239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weifeng-yang/mlsr">https://github.com/weifeng-yang/mlsr</a></li>
<li>paper_authors: Weifeng Yang, Wenwen Min</li>
<li>For: The paper is written for analyzing multidimensional data using a Multilinear Sparse Logistic Regression model with $\ell_0$-constraints (MLSR).* Methods: The paper proposes an Accelerated Proximal Alternating Linearized Minimization with Adaptive Momentum (APALM$^+$) method to solve the $\ell_0$-MLSR model, which is a novel approach that combines the advantages of both the $\ell_1$-norm and the $\ell_2$-norm.* Results: The paper demonstrates the superior performance of the proposed APALM$^+$ method in terms of both accuracy and speed, compared to other state-of-the-art methods, on synthetic and real-world datasets. Additionally, the paper provides a proof of convergence for the objective function of the $\ell_0$-MLSR model using the Kurdyka-Lojasiewicz property.<details>
<summary>Abstract</summary>
Tensor data represents a multidimensional array. Regression methods based on low-rank tensor decomposition leverage structural information to reduce the parameter count. Multilinear logistic regression serves as a powerful tool for the analysis of multidimensional data. To improve its efficacy and interpretability, we present a Multilinear Sparse Logistic Regression model with $\ell_0$-constraints ($\ell_0$-MLSR). In contrast to the $\ell_1$-norm and $\ell_2$-norm, the $\ell_0$-norm constraint is better suited for feature selection. However, due to its nonconvex and nonsmooth properties, solving it is challenging and convergence guarantees are lacking. Additionally, the multilinear operation in $\ell_0$-MLSR also brings non-convexity. To tackle these challenges, we propose an Accelerated Proximal Alternating Linearized Minimization with Adaptive Momentum (APALM$^+$) method to solve the $\ell_0$-MLSR model. We provide a proof that APALM$^+$ can ensure the convergence of the objective function of $\ell_0$-MLSR. We also demonstrate that APALM$^+$ is globally convergent to a first-order critical point as well as establish convergence rate by using the Kurdyka-Lojasiewicz property. Empirical results obtained from synthetic and real-world datasets validate the superior performance of our algorithm in terms of both accuracy and speed compared to other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
tensor数据表示多维数组。基于低维张量分解的回归方法利用结构信息来减少参数数。多线性логистиック回归作为多维数据分析的powerful工具。为了提高其效果和可解性，我们提出了多线性稀缺LOGISTIC回归模型（$\ell_0$-MLSR）。在$\ell_1$-norm和$\ell_2$-norm之外，$\ell_0$-norm约束更适合特征选择。然而，由于其非拟 convex和非均匀性质，解决它的困难重大，并且存在无法确保的收敛保证。此外，多线性操作在$\ell_0$-MLSR中也带来了非拟 convex性。为了解决这些挑战，我们提出了一种加速 proximal alternating linearized minimization with adaptive momentum（APALM$^+）方法来解决$\ell_0$-MLSR模型。我们提供了一个证明，表明APALM$^+$可以确保$\ell_0$-MLSR模型的目标函数收敛。此外，我们还证明APALM$^+$是全球收敛到一个第一阶关键点，并且使用库德ijk Lojasiewicz性质来确定收敛速率。实验结果表明，基于实验和实际数据，我们的算法在精度和速度方面与当前状态艺术方法相比具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Provable-learning-of-quantum-states-with-graphical-models"><a href="#Provable-learning-of-quantum-states-with-graphical-models" class="headerlink" title="Provable learning of quantum states with graphical models"></a>Provable learning of quantum states with graphical models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09235">http://arxiv.org/abs/2309.09235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liming Zhao, Naixu Guo, Ming-Xing Luo, Patrick Rebentrost</li>
<li>for: 本文研究了一种可以快速学习的量子状态的方法，具体来说是Restricted Boltzmann Machines（RBMs）。</li>
<li>methods: 本文使用了两个邻域学习算法，即 ferromagnetic 邻域学习算法和 locally consistent 邻域学习算法，以实现高效的量子状态学习。</li>
<li>results: 本文证明了使用这两种邻域学习算法可以在一定的情况下实现对量子状态的高效学习，并且可以比逻辑学习更快。<details>
<summary>Abstract</summary>
The complete learning of an $n$-qubit quantum state requires samples exponentially in $n$. Several works consider subclasses of quantum states that can be learned in polynomial sample complexity such as stabilizer states or high-temperature Gibbs states. Other works consider a weaker sense of learning, such as PAC learning and shadow tomography. In this work, we consider learning states that are close to neural network quantum states, which can efficiently be represented by a graphical model called restricted Boltzmann machines (RBMs). To this end, we exhibit robustness results for efficient provable two-hop neighborhood learning algorithms for ferromagnetic and locally consistent RBMs. We consider the $L_p$-norm as a measure of closeness, including both total variation distance and max-norm distance in the limit. Our results allow certain quantum states to be learned with a sample complexity \textit{exponentially} better than naive tomography. We hence provide new classes of efficiently learnable quantum states and apply new strategies to learn them.
</details>
<details>
<summary>摘要</summary>
完全学习一个 $n$-qubit量子状态需要样本数量呈指数函数关系于 $n$。一些作品考虑了一些量子状态的子集，可以在 polynomial 样本复杂性下学习，如稳定器状态或高温 Gibbs 状态。其他作品考虑了一种弱一种学习方式，如 PAC 学习和影子测试。在这项工作中，我们考虑了学习与神经网络状态相似的量子状态，可以有效地表示为受限 Boltzmann 机制（RBM）。为此，我们展示了二步邻居学习算法的Robustness 结果，包括 ferromagnetic 和本地一致 RBM。我们使用 $L_p$-norm 作为距离度量，包括总变分距离和最大 нор距离在限制中。我们的结果表示可以使用更好的样本复杂性学习一些量子状态，比Naive 测试更好。我们因此提供了新的有效地学习量子状态的类别，并应用新的策略来学习它们。
</details></li>
</ul>
<hr>
<h2 id="Double-Normalizing-Flows-Flexible-Bayesian-Gaussian-Process-ODEs-Learning"><a href="#Double-Normalizing-Flows-Flexible-Bayesian-Gaussian-Process-ODEs-Learning" class="headerlink" title="Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning"></a>Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09222">http://arxiv.org/abs/2309.09222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Xu, Shian Du, Junmei Yang, Xinghao Ding, John Paisley, Delu Zeng</li>
<li>for: 模型vector field continuous dynamical systems的bayesian inference</li>
<li>methods:  incorporate normalizing flows to reparameterize the vector field of ODEs, 使用normalizing flows进行 posterior inference</li>
<li>results: 提高了模型 uncertainty和精度 estimates, 在 simulate dynamical systems and real-world human motion data中得到了更好的结果<details>
<summary>Abstract</summary>
Recently, Gaussian processes have been utilized to model the vector field of continuous dynamical systems. Bayesian inference for such models \cite{hegde2022variational} has been extensively studied and has been applied in tasks such as time series prediction, providing uncertain estimates. However, previous Gaussian Process Ordinary Differential Equation (ODE) models may underperform on datasets with non-Gaussian process priors, as their constrained priors and mean-field posteriors may lack flexibility. To address this limitation, we incorporate normalizing flows to reparameterize the vector field of ODEs, resulting in a more flexible and expressive prior distribution. Additionally, due to the analytically tractable probability density functions of normalizing flows, we apply them to the posterior inference of GP ODEs, generating a non-Gaussian posterior. Through these dual applications of normalizing flows, our model improves accuracy and uncertainty estimates for Bayesian Gaussian Process ODEs. The effectiveness of our approach is demonstrated on simulated dynamical systems and real-world human motion data, including tasks such as time series prediction and missing data recovery. Experimental results indicate that our proposed method effectively captures model uncertainty while improving accuracy.
</details>
<details>
<summary>摘要</summary>
近期，Gaussian processes 被应用于连续动力系统的vector field模型中。Bayesian推理 для这些模型 \cite{hegde2022variational} 已经得到了广泛的研究，并在任务如时间序列预测中提供了不确定估计。然而，之前的Gaussian ProcessOrdinary Differential Equation（ODE）模型可能在非Gaussian process priors的数据集上表现不佳，因为它们的受限的先验和媒介质POSTerior可能缺乏灵活性。为了解决这个限制，我们将normalizing flows integration到了ODE的vector field中，从而获得了更灵活和表达力强的先验分布。此外，由于normalizing flows的概率密度函数是可微分的，我们可以将其应用到GP ODEs的后验推理中，生成一个非Gaussian posterior。通过这种双重应用normalizing flows，我们的模型可以提高Bayesian Gaussian Process ODEs的准确性和uncertainty估计。我们的方法在模拟动力系统和真实世界人类运动数据上进行了实验，包括时间序列预测和缺失数据恢复等任务，结果表明我们的提案方法可以有效地捕捉模型uncertainty，同时提高准确性。
</details></li>
</ul>
<hr>
<h2 id="MFRL-BI-Design-of-a-Model-free-Reinforcement-Learning-Process-Control-Scheme-by-Using-Bayesian-Inference"><a href="#MFRL-BI-Design-of-a-Model-free-Reinforcement-Learning-Process-Control-Scheme-by-Using-Bayesian-Inference" class="headerlink" title="MFRL-BI: Design of a Model-free Reinforcement Learning Process Control Scheme by Using Bayesian Inference"></a>MFRL-BI: Design of a Model-free Reinforcement Learning Process Control Scheme by Using Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09205">http://arxiv.org/abs/2309.09205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanrong Li, Juan Du, Wei Jiang</li>
<li>for: 本研究旨在提出一种基于模型自由强化学习（MFRL）的控制方案，以实时数据进行实验和控制优化，以适应实际生产系统中模型不准确的问题。</li>
<li>methods: 本研究使用了MFRL控制方案，通过bayesian推理更新干扰分布，以降低生产过程中干扰的大量变化。</li>
<li>results: 研究结果显示，提议的MFRL控制方案在无知过程模型情况下能够实现良好的控制性能，并且在数学性质上也得到了保证。计算研究也证明了我们的方法的有效性和效率。<details>
<summary>Abstract</summary>
Design of process control scheme is critical for quality assurance to reduce variations in manufacturing systems. Taking semiconductor manufacturing as an example, extensive literature focuses on control optimization based on certain process models (usually linear models), which are obtained by experiments before a manufacturing process starts. However, in real applications, pre-defined models may not be accurate, especially for a complex manufacturing system. To tackle model inaccuracy, we propose a model-free reinforcement learning (MFRL) approach to conduct experiments and optimize control simultaneously according to real-time data. Specifically, we design a novel MFRL control scheme by updating the distribution of disturbances using Bayesian inference to reduce their large variations during manufacturing processes. As a result, the proposed MFRL controller is demonstrated to perform well in a nonlinear chemical mechanical planarization (CMP) process when the process model is unknown. Theoretical properties are also guaranteed when disturbances are additive. The numerical studies also demonstrate the effectiveness and efficiency of our methodology.
</details>
<details>
<summary>摘要</summary>
制程控制方案的设计对制造系统质量保证具有关键性。以半导体制造为例，广泛的文献关注控制优化基于certain process models（通常是线性模型），这些模型通常通过实验 перед制造过程开始获得。然而，在实际应用中，预定义的模型可能不准确，特别是对于复杂的制造系统。为解决模型不准确的问题，我们提议使用无模型反馈学习（MFRL）方法来进行实验和控制优化同时，根据实时数据进行调整。 Specifically, we design a novel MFRL control scheme by updating the distribution of disturbances using Bayesian inference to reduce their large variations during manufacturing processes. As a result, the proposed MFRL controller is demonstrated to perform well in a nonlinear chemical mechanical planarization (CMP) process when the process model is unknown. 理论性质也得到保证，当干扰是加性的时候。 numerical studies also demonstrate the effectiveness and efficiency of our methodology.
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Optimized-Pipeline-for-Prediction-of-Protein-Folding-Kinetics"><a href="#End-to-End-Optimized-Pipeline-for-Prediction-of-Protein-Folding-Kinetics" class="headerlink" title="End-to-End Optimized Pipeline for Prediction of Protein Folding Kinetics"></a>End-to-End Optimized Pipeline for Prediction of Protein Folding Kinetics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09191">http://arxiv.org/abs/2309.09191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vijay Arvind. R, Haribharathi Sivakumar, Brindha. R</li>
<li>for: 预测蛋白质折叠动力学的高精度且低占用内存的算法 pipeline。</li>
<li>methods: 使用机器学习模型进行预测。</li>
<li>results: 比预先状态艺术模型高4.8%的准确率，并且占用内存327倍少和运行速度7.3%快。<details>
<summary>Abstract</summary>
Protein folding is the intricate process by which a linear sequence of amino acids self-assembles into a unique three-dimensional structure. Protein folding kinetics is the study of pathways and time-dependent mechanisms a protein undergoes when it folds. Understanding protein kinetics is essential as a protein needs to fold correctly for it to perform its biological functions optimally, and a misfolded protein can sometimes be contorted into shapes that are not ideal for a cellular environment giving rise to many degenerative, neuro-degenerative disorders and amyloid diseases. Monitoring at-risk individuals and detecting protein discrepancies in a protein's folding kinetics at the early stages could majorly result in public health benefits, as preventive measures can be taken. This research proposes an efficient pipeline for predicting protein folding kinetics with high accuracy and low memory footprint. The deployed machine learning (ML) model outperformed the state-of-the-art ML models by 4.8% in terms of accuracy while consuming 327x lesser memory and being 7.3% faster.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Driven-Reachability-Analysis-of-Stochastic-Dynamical-Systems-with-Conformal-Inference"><a href="#Data-Driven-Reachability-Analysis-of-Stochastic-Dynamical-Systems-with-Conformal-Inference" class="headerlink" title="Data-Driven Reachability Analysis of Stochastic Dynamical Systems with Conformal Inference"></a>Data-Driven Reachability Analysis of Stochastic Dynamical Systems with Conformal Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09187">http://arxiv.org/abs/2309.09187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Hashemi, Xin Qin, Lars Lindemann, Jyotirmoy V. Deshmukh</li>
<li>for: 本文针对数据驱动的抽象时间概率动力系统进行可达性分析，使用协Forms inference。</li>
<li>methods: 本文使用数据学习来建立一个代理模型，然后使用代理模型进行可达性分析，并使用协Forms inference来评估代理模型所受的误差。</li>
<li>results: 本文可以为learning-enabled控制系统提供可达性保证，并且可以处理复杂的closed-loop dynamics。<details>
<summary>Abstract</summary>
We consider data-driven reachability analysis of discrete-time stochastic dynamical systems using conformal inference. We assume that we are not provided with a symbolic representation of the stochastic system, but instead have access to a dataset of $K$-step trajectories. The reachability problem is to construct a probabilistic flowpipe such that the probability that a $K$-step trajectory can violate the bounds of the flowpipe does not exceed a user-specified failure probability threshold. The key ideas in this paper are: (1) to learn a surrogate predictor model from data, (2) to perform reachability analysis using the surrogate model, and (3) to quantify the surrogate model's incurred error using conformal inference in order to give probabilistic reachability guarantees. We focus on learning-enabled control systems with complex closed-loop dynamics that are difficult to model symbolically, but where state transition pairs can be queried, e.g., using a simulator. We demonstrate the applicability of our method on examples from the domain of learning-enabled cyber-physical systems.
</details>
<details>
<summary>摘要</summary>
我们考虑了数据驱动的可达性分析，用于离散时间渐进系统。我们假设我们没有符号表示法，而是有一个$K$-步轨迹数据集。我们的目标是构建一个流管，使得流管中的概率超过用户指定的失败概率阈值。我们的关键想法是：（1）从数据学习一个代理预测模型，（2）使用代理模型进行可达性分析，（3）使用凤凰推理来评估代理模型所吃进的误差，以提供可达性保证。我们关注learning-enabled控制系统，其中具有复杂的关闭环境，但可以通过 simulate 来查询状态转移对。我们在学习启发系统中的示例上进行了应用。
</details></li>
</ul>
<hr>
<h2 id="On-the-Connection-Between-Riemann-Hypothesis-and-a-Special-Class-of-Neural-Networks"><a href="#On-the-Connection-Between-Riemann-Hypothesis-and-a-Special-Class-of-Neural-Networks" class="headerlink" title="On the Connection Between Riemann Hypothesis and a Special Class of Neural Networks"></a>On the Connection Between Riemann Hypothesis and a Special Class of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09171">http://arxiv.org/abs/2309.09171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soufiane Hayou</li>
<li>for: 这份论文是关于里曼假设（RH）的一个检查和扩展。</li>
<li>methods: 论文使用了一种已知的分析条件，称为尼曼-贝尔灵 критерион，连接RH与一种特殊的神经网络最小化问题。</li>
<li>results: 论文提供了一种扩展的分析条件，以及一种新的方法来检查RH。<details>
<summary>Abstract</summary>
The Riemann hypothesis (RH) is a long-standing open problem in mathematics. It conjectures that non-trivial zeros of the zeta function all have real part equal to 1/2. The extent of the consequences of RH is far-reaching and touches a wide spectrum of topics including the distribution of prime numbers, the growth of arithmetic functions, the growth of Euler totient, etc. In this note, we revisit and extend an old analytic criterion of the RH known as the Nyman-Beurling criterion which connects the RH to a minimization problem that involves a special class of neural networks. This note is intended for an audience unfamiliar with RH. A gentle introduction to RH is provided.
</details>
<details>
<summary>摘要</summary>
里曼假设（RH）是数学中一个长期开放的问题。它假设非质数函数的非质数部分都是1/2的实数部分。这个假设的影响是广泛的，覆盖了许多数学领域，包括整数分布、算术函数的增长、欧拉 totient 函数的增长等。在这份notes中，我们重新访问和扩展了一个古老的分析 критерий，称为尼曼-欧拉 criterion，它将RH与一种特殊的神经网络相连接。这份notes是为那些不熟悉RH的读者而设计的。我们会提供一个温顺的引入，以便读者更好地了解RH。
</details></li>
</ul>
<hr>
<h2 id="Integration-of-geoelectric-and-geochemical-data-using-Self-Organizing-Maps-SOM-to-characterize-a-landfill"><a href="#Integration-of-geoelectric-and-geochemical-data-using-Self-Organizing-Maps-SOM-to-characterize-a-landfill" class="headerlink" title="Integration of geoelectric and geochemical data using Self-Organizing Maps (SOM) to characterize a landfill"></a>Integration of geoelectric and geochemical data using Self-Organizing Maps (SOM) to characterize a landfill</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09164">http://arxiv.org/abs/2309.09164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Camila Juliao, Johan Diaz, Yosmely BermÚdez, Milagrosa Aldana</li>
<li>For: 这个研究的目的是确定垃圾掩埋场周围区域是否存在潜在的污染风险，并通过不同方法来实现这一目的。* Methods: 本研究使用了地球电性资料（抗阻和IP）和表面甲烷测量数据，并使用了一个不supervised Neural Network（ Kohonen 型）来处理和分类这些数据。* Results: 研究结果显示，通过使用 Self-Organizing Classification Maps（SOM），可以实现精确地定义潜在污染风险区域，并将其分为不同的类别。两个图像出力被 obtiened 从训练过程中，每个图像都代表了不同的潜在污染风险区域。<details>
<summary>Abstract</summary>
Leachates from garbage dumps can significantly compromise their surrounding area. Even if the distance between these and the populated areas could be considerable, the risk of affecting the aquifers for public use is imminent in most cases. For this reason, the delimitation and monitoring of the leachate plume are of significant importance. Geoelectric data (resistivity and IP), and surface methane measurements, are integrated and classified using an unsupervised Neural Network to identify possible risk zones in areas surrounding a landfill. The Neural Network used is a Kohonen type, which generates; as a result, Self-Organizing Classification Maps or SOM (Self-Organizing Map). Two graphic outputs were obtained from the training performed in which groups of neurons that presented a similar behaviour were selected. Contour maps corresponding to the location of these groups and the individual variables were generated to compare the classification obtained and the different anomalies associated with each of these variables. Two of the groups resulting from the classification are related to typical values of liquids percolated in the landfill for the parameters evaluated individually. In this way, a precise delimitation of the affected areas in the studied landfill was obtained, integrating the input variables via SOMs. The location of the study area is not detailed for confidentiality reasons.
</details>
<details>
<summary>摘要</summary>
垃圾排泄物可以很大程度地对周围环境造成影响。即使垃圾排泄物和人口集中区之间的距离相对较远，但是影响公共饮水储存层的风险仍然很高。因此，垃圾排泄物泄洪和监测的重要性非常大。在这种情况下，利用不超级网络（Kohonen类）进行无监督学习，并将抵抗性和IP测量数据集成，以生成自组织分类地图（SOM）。在训练过程中，选择了表现相似的神经元组，并生成了对应的Contour地图，以比较不同变量之间的分类结果和异常相关性。两个组 resulting from the classification are related to typical liquid values percolated in the landfill for the parameters evaluated individually. In this way, a precise delimitation of the affected areas in the studied landfill was obtained, integrating the input variables via SOMs.  The location of the study area is not detailed for confidentiality reasons.
</details></li>
</ul>
<hr>
<h2 id="Total-Variation-Distance-Estimation-Is-as-Easy-as-Probabilistic-Inference"><a href="#Total-Variation-Distance-Estimation-Is-as-Easy-as-Probabilistic-Inference" class="headerlink" title="Total Variation Distance Estimation Is as Easy as Probabilistic Inference"></a>Total Variation Distance Estimation Is as Easy as Probabilistic Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09134">http://arxiv.org/abs/2309.09134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S. Meel, Dimitrios Myrisiotis, A. Pavan, N. V. Vinodchandran</li>
<li>for: 这个论文是关于total variation（TV）距离估计和概率推理之间的新连接。</li>
<li>methods: 这篇论文使用了一种有效、结构保持的减少方法，将相对的TV距离估计转化为概率推理 над指定的导航图模型中。</li>
<li>results: 这篇论文提出了一种基于Bayes网的FPRAS估计TV距离 между任意类型的分布，并且只需要有效的概率推理算法。此外，这种方法还可以用于估计高维分布的TV距离。<details>
<summary>Abstract</summary>
In this paper, we establish a novel connection between total variation (TV) distance estimation and probabilistic inference. In particular, we present an efficient, structure-preserving reduction from relative approximation of TV distance to probabilistic inference over directed graphical models. This reduction leads to a fully polynomial randomized approximation scheme (FPRAS) for estimating TV distances between distributions over any class of Bayes nets for which there is an efficient probabilistic inference algorithm. In particular, it leads to an FPRAS for estimating TV distances between distributions that are defined by Bayes nets of bounded treewidth. Prior to this work, such approximation schemes only existed for estimating TV distances between product distributions. Our approach employs a new notion of $partial$ couplings of high-dimensional distributions, which might be of independent interest.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们建立了一种新的连接，即全量变量（TV）距离估计和概率推理之间的连接。我们 Specifically, we present a structure-preserving reduction from relative approximation of TV distance to probabilistic inference over directed graphical models. This reduction leads to a fully polynomial randomized approximation scheme (FPRAS) for estimating TV distances between distributions over any class of Bayes nets for which there is an efficient probabilistic inference algorithm. In particular, it leads to an FPRAS for estimating TV distances between distributions that are defined by Bayes nets of bounded treewidth. Prior to this work, such approximation schemes only existed for estimating TV distances between product distributions. Our approach employs a new notion of $partial$ couplings of high-dimensional distributions, which might be of independent interest.Here's the translation in Traditional Chinese:在这篇论文中，我们建立了一种新的连接，即全量变量（TV）距离估计和概率推理之间的连接。我们 Specifically, we present a structure-preserving reduction from relative approximation of TV distance to probabilistic inference over directed graphical models. This reduction leads to a fully polynomial randomized approximation scheme (FPRAS) for estimating TV distances between distributions over any class of Bayes nets for which there is an efficient probabilistic inference algorithm. In particular, it leads to an FPRAS for estimating TV distances between distributions that are defined by Bayes nets of bounded treewidth. Prior to this work, such approximation schemes only existed for estimating TV distances between product distributions. Our approach employs a new notion of $partial$ couplings of high-dimensional distributions, which might be of independent interest.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/cs.LG_2023_09_17/" data-id="cloimipbi00ons488extf860y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/17/eess.SP_2023_09_17/" class="article-date">
  <time datetime="2023-09-17T08:00:00.000Z" itemprop="datePublished">2023-09-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/17/eess.SP_2023_09_17/">eess.SP - 2023-09-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Climate-Resilient-UAVs-Enhancing-Energy-Efficient-B5G-Communication-in-Harsh-Environments"><a href="#Climate-Resilient-UAVs-Enhancing-Energy-Efficient-B5G-Communication-in-Harsh-Environments" class="headerlink" title="Climate-Resilient UAVs: Enhancing Energy-Efficient B5G Communication in Harsh Environments"></a>Climate-Resilient UAVs: Enhancing Energy-Efficient B5G Communication in Harsh Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09387">http://arxiv.org/abs/2309.09387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdu Saif, Saeed Hamood Alsamhi, Edward Curry</li>
<li>for: 这篇论文探讨了无人机在超 fifth generation（B5G）通信网络中的重要作用，尤其是在雨、雾、雪等不利天气条件下。</li>
<li>methods: 这篇研究探讨了气候鲜度无人机和能效B5G通信之间的相互作用，并分析了各种天气元素对无人机覆盖和通信动态的影响。</li>
<li>results: 研究发现，气候鲜度无人机可以在不同的天气条件下提供更高的能效性、降低干扰、提高数据传输率，并且在不同的天气条件下可以获得最佳通道增强。<details>
<summary>Abstract</summary>
This paper explores the crucial role of Unmanned Aerial Vehicles (UAVs) in advancing Beyond Fifth Generation (B5G) communication networks, especially in adverse weather conditions like rain, fog, and snow.   The study investigates the synergy between climate-resilient UAVs and energy-efficient B5G communication.   Key findings include the impact of weather elements on UAV coverage and communication dynamics. The research demonstrates significant enhancements in energy efficiency, reduced interference, increased data transmission rates, and optimal channel gain under various weather conditions.   Overall, this paper emphasizes the potential of climate-resilient UAVs to improve energy-efficient B5G communication and highlights technology's role in mitigating climate change's impact on communication systems, promoting sustainability and resilience.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨无人飞行器（UAV）在 fifth Generation 以上通信网络中的重要作用，特别是在雨、雾和雪等不利天气 услови下。 研究发现了气候适应UAV和能效B5G通信之间的共同作用，以及不同天气条件下UAV覆盖和通信动态的影响。 研究显示在不同的天气条件下，气候适应UAV可以提高能效率，减少干扰，提高数据传输速率，并且在不同的天气条件下实现最佳通道增强。总的来说，这篇论文强调气候适应UAV在能效B5G通信中的潜在作用，并 highlights 技术在气候变化对通信系统的影响下的作用，推动可持续发展和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Domain-Detection-for-Molecular-Communication-with-Cross-Reactive-Receptors"><a href="#Frequency-Domain-Detection-for-Molecular-Communication-with-Cross-Reactive-Receptors" class="headerlink" title="Frequency-Domain Detection for Molecular Communication with Cross-Reactive Receptors"></a>Frequency-Domain Detection for Molecular Communication with Cross-Reactive Receptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09377">http://arxiv.org/abs/2309.09377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meltem Civas, Murat Kuscu, Ozgur B. Akan</li>
<li>for: This paper is written for the development of a frequency-domain detection (FDD) technique for bioFET-based molecular communication receivers (MC-Rxs) to overcome molecular cross-talk in the time domain.</li>
<li>methods: The paper proposes the use of a frequency-domain detection technique that exploits the difference in binding reaction rates of different ligand types reflected in the power spectrum of the ligand-receptor binding noise to decode transmitted concentration signals.</li>
<li>results: The paper demonstrates the effectiveness of the proposed FDD technique in decoding transmitted concentration signals under stochastic molecular interference compared to a widely used time-domain detection (TDD) technique, and verifies the analytical performance bounds of the FDD through a particle-based spatial stochastic simulator simulating reactions on the MC-Rx in microfluidic channels.<details>
<summary>Abstract</summary>
Molecular Communications (MC) is a bio-inspired communication paradigm that uses molecules as information carriers, requiring unconventional transceivers and modulation/detection techniques. Practical MC receivers (MC-Rxs) can be implemented using field-effect transistor biosensor (bioFET) architectures, where surface receptors reversibly react with ligands. The time-varying concentration of ligand-bound receptors is translated into electrical signals via field effect, which is used to decode the transmitted information. However, ligand-receptor interactions do not provide an ideal molecular selectivity, as similar ligand types, i.e., interferers, co-existing in the MC channel, can interact with the same type of receptors. Overcoming this molecular cross-talk in the time domain can be challenging, especially when Rx has no knowledge of the interferer statistics or operates near saturation. Therefore, we propose a frequency-domain detection (FDD) technique for bioFET-based MC-Rxs that exploits the difference in binding reaction rates of different ligand types reflected in the power spectrum of the ligand-receptor binding noise. We derive the bit error probability (BEP) of the FDD technique and demonstrate its effectiveness in decoding transmitted concentration signals under stochastic molecular interference compared to a widely used time-domain detection (TDD) technique. We then verified the analytical performance bounds of the FDD through a particle-based spatial stochastic simulator simulating reactions on the MC-Rx in microfluidic channels.
</details>
<details>
<summary>摘要</summary>
молекулярcommunications（MC）是一种生物体注意的通信模式，使用分子作为信息传递者，需要不同寻常的接收器和模ulation/探测技术。实际的MC接收器（MC-Rx）可以通过场效 транзистор生物感应（bioFET）建筑实现，其表面受体逆转受体与抗体结合。时间变化的抗体结合的分子浓度通过场效转换成电学信号，用于解码传输的信息。但是，抗体-受体交互不提供理想的分子选择性，因为同类抗体在MC通道中可能与同类受体结合。在时间频谱中解决这种分子交叉通信可以是挑战，特别是当接收器没有知道干扰者统计或在满载状态下操作时。因此，我们提出了频率域检测（FDD）技术，该技术利用不同抗体类型在绑定反应速率上的差异，反映在抗体-受体绑定噪声的能量спектrum中。我们计算了FDD技术的比特错误概率（BEP），并证明其在对抗杂噪声的情况下比时频域检测（TDD）技术更有效地解码传输的浓度信号。我们然后通过使用粒子基的空间随机仿真器模拟MC-Rx在微 fluidic通道中的反应，验证了我们的分析性能下限。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Estimation-Using-Complex-Valued-Shifted-Window-Transformer"><a href="#Frequency-Estimation-Using-Complex-Valued-Shifted-Window-Transformer" class="headerlink" title="Frequency Estimation Using Complex-Valued Shifted Window Transformer"></a>Frequency Estimation Using Complex-Valued Shifted Window Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09352">http://arxiv.org/abs/2309.09352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/josiahwsmith10/spectral-super-resolution-swin">https://github.com/josiahwsmith10/spectral-super-resolution-swin</a></li>
<li>paper_authors: Josiah W. Smith, Murat Torlak</li>
<li>for: 本研究targets at estimating closely spaced frequency components of a signal, which is a fundamental problem in statistical signal processing.</li>
<li>methods: 本文提出了一种基于Swin transformer的新方法，包括1-D real-valued和复数值Shifted window transformer（SwinFreq和CVSwinFreq），用于1-D复数值信号的线spectra频率估计。</li>
<li>results: 对比传统的Periodogram、MUSIC和OMP算法以及state-of-the-art的deep learning方法cResFreq，SwinFreq和CVSwinFreq具有更高的性能、更好的分辨率和更少的模型参数，因此更适合边缘和移动应用。此外， authors发现了real-valued Swin-Freq在某些任务上表现更好，而且具有较小的模型大小。最后， authors应用了提posed方法于实际雷达范profile超分辨率 task中，实验结果 validate了SwinFreq和CVSwinFreq的数值和实验上的优越性。<details>
<summary>Abstract</summary>
Estimating closely spaced frequency components of a signal is a fundamental problem in statistical signal processing. In this letter, we introduce 1-D real-valued and complex-valued shifted window (Swin) transformers, referred to as SwinFreq and CVSwinFreq, respectively, for line-spectra frequency estimation on 1-D complex-valued signals. Whereas 2-D Swin transformer-based models have gained traction for optical image super-resolution, we introduce for the first time a complex-valued Swin module designed to leverage the complex-valued nature of signals for a wide array of applications. The proposed approach overcomes the limitations of the classical algorithms such as the periodogram, MUSIC, and OMP in addition to state-of-the-art deep learning approach cResFreq. SwinFreq and CVSwinFreq boast superior performance at low signal-to-noise ratio SNR and improved resolution capability while requiring fewer model parameters than cResFreq, thus deeming it more suitable for edge and mobile applications. We find that the real-valued Swin-Freq outperforms its complex-valued counterpart CVSwinFreq for several tasks while touting a smaller model size. Finally, we apply the proposed techniques for radar range profile super-resolution using real data. The results from both synthetic and real experimentation validate the numerical and empirical superiority of SwinFreq and CVSwinFreq to the state-of-the-art deep learning-based techniques and traditional frequency estimation algorithms. The code and models are publicly available at https://github.com/josiahwsmith10/spectral-super-resolution-swin.
</details>
<details>
<summary>摘要</summary>
估计 closely spaced frequency component of a signal 是统计信号处理中的基本问题。在这封信件中，我们介绍了1维实数值和复数值偏移窗变换器（Swin），称为SwinFreq和CVSwinFreq，用于1维复数值信号的线pectra频率估计。而2维Swin transformer-based模型在光学超分解中获得了进步，我们是第一次引入一种用于信号的复数值Swin模块，用以利用信号的复数值特性，并且可以应用于各种应用程序。我们的方法可以超越经典算法，如期ogram、MUSIC和OMP，以及当前的深度学习方法cResFreq。SwinFreq和CVSwinFreq具有低SNR和提高分辨率的优势，同时需要 fewer model parameter than cResFreq，因此适用于边缘和移动应用。我们发现实数值Swin-Freq在一些任务上表现出色，而且具有较小的模型大小。最后，我们应用了提议的技术于雷达距离Profile超分解中使用实际数据。实验结果证明了SwinFreq和CVSwinFreq的数学和实验准确性，并超越了当前的深度学习基于技术和经典频率估计算法。代码和模型可以在https://github.com/josiahwsmith10/spectral-super-resolution-swin上获得。
</details></li>
</ul>
<hr>
<h2 id="Asymptotic-Analysis-of-the-Downlink-in-Cooperative-Massive-MIMO-Systems"><a href="#Asymptotic-Analysis-of-the-Downlink-in-Cooperative-Massive-MIMO-Systems" class="headerlink" title="Asymptotic Analysis of the Downlink in Cooperative Massive MIMO Systems"></a>Asymptotic Analysis of the Downlink in Cooperative Massive MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09273">http://arxiv.org/abs/2309.09273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itsik Bergel, Siddhartan Govindasamy</li>
<li>For: The paper is written for a cooperative cellular communications system, where multiple base stations around each mobile station cooperate to reduce interference and improve system performance.* Methods: The paper uses closed-form expressions to derive the asymptotic performance of the network as the number of antennas per base station increases, and includes Monte Carlo simulations to verify the results. The paper also proposes a power allocation algorithm that achieves near-optimal performance with reduced coordination overhead between base stations.* Results: The paper shows that the asymptotic results capture the trade-off between various system parameters, and characterize the joint effect of noise and interference. The results are useful even when the number of antennas per base station is only moderately large, and the proposed power allocation algorithm achieves near-optimal performance with reduced coordination overhead.Here is the format you requested for the simplified Chinese text:* For: 这篇论文是关于协同通信系统的下行频率调制，多个基站周围每个移动站协同进行零干扰，以降低接收到移动站的干扰。* Methods: 论文使用closed-form表达式来 deriv asymptotic性能表达式，并通过 Monte Carlo仿真来验证结果。论文还提出了一种功率分配算法，可以在基站之间减少协调开销。* Results: 论文显示， asymptotic结果 capture了系统参数之间的贸易off，并characterize了干扰和噪声的共同效应。结果是用于 moderately large antenna数per base station，并且提出的功率分配算法可以实现 near-optimal性能，同时减少协调开销。<details>
<summary>Abstract</summary>
We consider the downlink of a cooperative cellular communications system, where several base-stations around each mobile cooperate and perform zero-forcing to reduce the received interference at the mobile. We derive closed-form expressions for the asymptotic performance of the network as the number of antennas per base station grows large. These expressions capture the trade off between various system parameters, and characterize the joint effect of noise and interference (where either noise or interference is asymptotically dominant and where both are asymptotically relevant). The asymptotic results are verified using Monte Carlo simulations, which indicate that they are useful even when the number of antennas per base station is only moderately large. Additionally, we show that when the number of antennas per base station grows large, power allocation can be optimized locally at each base station. We hence present a power allocation algorithm that achieves near optimal performance while significantly reducing the coordination overhead between base stations. The presented analysis is significantly more challenging than the uplink analysis, due to the dependence between beamforming vectors of nearby base stations. This statistical dependence is handled by introducing novel bounds on marked shot-noise point processes with dependent marks, which are also useful in other contexts.
</details>
<details>
<summary>摘要</summary>
我团队考虑了一个合作的移动通信系统的下链，其中各个基站附近的移动站合作，并通过零干扰来减少移动站接收的干扰。我们 deriv了大面积表达式，用于描述系统的极限性能，这些表达式捕捉了系统参数之间的贸易OFF和干扰的共同作用，以及干扰和噪声之间的交互作用。我们使用Monte Carlo仿真来验证我们的结论，并发现这些结论在只有 moderately 大的antenna数时仍然有用。此外，我们还证明了当antenna数量增大时，每个基站可以地方Optimize its power allocation，以达到近似优化性能的目的，同时减少基站之间协调 overhead。在这个分析中，我们面临的挑战在于附近基站的扩散波形矩阵之间的统计依赖关系。我们使用了新的 bounds on marked shot-noise point processes with dependent marks来处理这种统计依赖关系。这些bounds也可以在其他 contexts 中使用。
</details></li>
</ul>
<hr>
<h2 id="Toward-Beamfocusing-Aided-Near-Field-Communications-Research-Advances-Potential-and-Challenges"><a href="#Toward-Beamfocusing-Aided-Near-Field-Communications-Research-Advances-Potential-and-Challenges" class="headerlink" title="Toward Beamfocusing-Aided Near-Field Communications: Research Advances, Potential, and Challenges"></a>Toward Beamfocusing-Aided Near-Field Communications: Research Advances, Potential, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09242">http://arxiv.org/abs/2309.09242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancheng An, Chau Yuen, Linglong Dai, Marco Di Renzo, Merouane Debbah, Lajos Hanzo</li>
<li>for: 这篇论文旨在探讨未来无线通信技术的发展，尤其是EXTREMELY大规模天线阵列（ELAA）和tera响communications（NFC）的潜在应用。</li>
<li>methods: 论文使用基于圆柱形波front的模型来准确描述近场无线传播通道特性。同时，论文还提出了NFC与传统远场通信的比较，并评估了NFC的频率响应和硬件设计等挑战。</li>
<li>results: 数值结果表明NFC可以提高空间多重化增量和位置准确性。此外，论文还提出了一些未来研究的开问，以便进一步探索NFC的潜在应用和发展。<details>
<summary>Abstract</summary>
Next-generation mobile networks promise to support high throughput, massive connectivity, and improved energy efficiency. To achieve these ambitious goals, extremely large-scale antenna arrays (ELAAs) and terahertz communications constitute a pair of promising technologies. This will result in future wireless communications occurring in the near-field regions. To accurately portray the channel characteristics of near-field wireless propagation, spherical wavefront-based models are required and present both opportunities as well as challenges. Following the basics of near-field communications (NFC), we contrast it to conventional far-field communications. Moreover, we cover the key challenges of NFC, including its channel modeling and estimation, near-field beamfocusing, as well as hardware design. Our numerical results demonstrate the potential of NFC in improving the spatial multiplexing gain and positioning accuracy. Finally, a suite of open issues are identified for motivating future research.
</details>
<details>
<summary>摘要</summary>
Next-generation mobile networks 将支持高速、大量连接和改善能源效率。为实现这些目标，极大规模天线阵列（ELAAs）和teraHz通信技术是两种承诺技术。这将导致未来无线通信发生在近场区域。为准确描述近场无线媒体特性，球形波front基于模型是必需的，并提供了机会和挑战。根据近场通信（NFC）的基本原理，我们对它与传统远场通信进行了比较。此外，我们还讨论了NFC的主要挑战，包括通道模型化和估计、近场焦点定向以及硬件设计。我们的数字结果表明NFC可以提高空间复用增量和位置准确性。最后，我们确定了一些未解决的问题，以便激励未来的研究。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Cramer-Rao-Bound-Optimization-for-Active-RIS-Empowered-ISAC-Systems"><a href="#Cramer-Rao-Bound-Optimization-for-Active-RIS-Empowered-ISAC-Systems" class="headerlink" title="Cramer-Rao Bound Optimization for Active RIS-Empowered ISAC Systems"></a>Cramer-Rao Bound Optimization for Active RIS-Empowered ISAC Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09207">http://arxiv.org/abs/2309.09207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Zhu, Ming Li, Rang Liu, Qian Liu</li>
<li>for: 这篇论文的目的是探讨使用活动智能表面（RIS）强化探测信号质量和通信性能的Integrated sensing and communication（ISAC）系统。</li>
<li>methods: 论文使用了基站传输预编码和活动RIS反射扩散 beamforming的共同设计来优化参数估算性能，并提出了一种高效的算法基于块坐标降解（BCD）、半definite relaxation（SDR）和主导化最小化（MM）来解决非核心问题。</li>
<li>results:  simulation results validate the effectiveness of the developed algorithm and the potential of employing active RIS in ISAC systems to enhance direct-of-arrival（DoA）估算性能。<details>
<summary>Abstract</summary>
Integrated sensing and communication (ISAC), which simultaneously performs sensing and communication functions using the same frequency band and hardware platform, has emerged as a promising technology for future wireless systems. However, the weak echo signal received by the low-sensitivity ISAC receiver severely limits the sensing performance. Active reconfigurable intelligent surface (RIS) has become a prospective solution by situationally manipulating the wireless propagations and amplifying the signals. In this paper, we investigate the deployment of active RIS-empowered ISAC systems to enhance radar echo signal quality as well as communication performance. In particular, we focus on the joint design of the base station (BS) transmit precoding and the active RIS reflection beamforming to optimize the parameter estimation performance in terms of Cramer-Rao bound (CRB) subject to the service users' signal-to-interference-plus-noise ratio (SINR) requirements. An efficient algorithm based on block coordinate descent (BCD), semidefinite relaxation (SDR), and majorization-minimization (MM) is proposed to solve the formulated challenging non-convex problem. Finally, simulation results validate the effectiveness of the developed algorithm and the potential of employing active RIS in ISAC systems to enhance direct-of-arrival (DoA) estimation performance.
</details>
<details>
<summary>摘要</summary>
Integrated sensing and communication (ISAC)技术，它同时执行感知和通信功能，使用同一个频率带和硬件平台，已经成为未来无线系统的一种优秀技术。然而，低敏感度ISAC接收器接收到的弱回声信号，严重限制了感知性能。活动可重配置表面（RIS）已成为一种可能的解决方案，通过 Situationally manipulating wireless propagation和增强信号。在这篇论文中，我们 investigate了活动RIS-empowered ISAC系统的部署，以提高雷达回声信号质量以及通信性能。具体来说，我们关注了基站（BS）传输 precoding 和活动RIS反射扩散的共同设计，以优化参数估计性能，并达到服务用户的信号干扰plus noise ratio（SINR）要求。我们提出了一种高效的算法，基于块坐标降解（BCD）、凸relaxation（SDR）和主要化-最小化（MM）来解决复杂非对称问题。最后，我们的实验结果证明了我们提出的算法的有效性，以及活动RIS在ISAC系统中的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="NOMA-Based-Coexistence-of-Near-Field-and-Far-Field-Massive-MIMO-Communications"><a href="#NOMA-Based-Coexistence-of-Near-Field-and-Far-Field-Massive-MIMO-Communications" class="headerlink" title="NOMA-Based Coexistence of Near-Field and Far-Field Massive MIMO Communications"></a>NOMA-Based Coexistence of Near-Field and Far-Field Massive MIMO Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09185">http://arxiv.org/abs/2309.09185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiguo Ding, Robert Schober, H. Vincent Poor</li>
<li>for: 提供了一种使用NOMA原理来支持传统近场用户的合作，以提高大量MIMO网络的性能。</li>
<li>methods: 使用预先配置的近场用户的空间束来服务更多的远场用户，并通过增加基站antenna数来提高NOMA-assisted大量MIMO网络的性能。</li>
<li>results: 研究结果表明，通过NOMA原理可以有效地支持近场和远场通信的合作，并且可以通过增加基站antenna数来提高NOMA-assisted大量MIMO网络的性能。<details>
<summary>Abstract</summary>
This letter considers a legacy massive multiple-input multiple-output (MIMO) network, in which spatial beams have been preconfigured for near-field users, and proposes to use the non-orthogonal multiple access (NOMA) principle to serve additional far-field users by exploiting the spatial beams preconfigured for the legacy near-field users. Our results reveal that the coexistence between near-field and far-field communications can be effectively supported via NOMA, and that the performance of NOMA-assisted massive MIMO can be efficiently improved by increasing the number of antennas at the base station.
</details>
<details>
<summary>摘要</summary>
这封信件考虑了一个传统的大规模多输入多输出（MIMO）网络，在near-field用户中预先配置了空间扩散，并提议使用非对称多access（NOMA）原则来为更远的far-field用户提供服务，利用预先配置的near-field用户的空间扩散。我们的结果表明，near-field和far-field通信的共存可以通过NOMA进行有效支持，并且通过提高基站antenna数量来提高NOMA-assisted大规模MIMO的性能。Note that Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Throughput-Analysis-of-IEEE-802-11bn-Coordinated-Spatial-Reuse"><a href="#Throughput-Analysis-of-IEEE-802-11bn-Coordinated-Spatial-Reuse" class="headerlink" title="Throughput Analysis of IEEE 802.11bn Coordinated Spatial Reuse"></a>Throughput Analysis of IEEE 802.11bn Coordinated Spatial Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09169">http://arxiv.org/abs/2309.09169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesc Wilhelmi, Lorenzo Galati-Giordano, Giovanni Geraci, Boris Bellalta, Gianluca Fontanesi, David Nuñez</li>
<li>for:  This paper focuses on the Coordinated Spatial Reuse (C-SR) feature of the Multi-Access Point Coordination (MAPC) in the IEEE 802.11bn amendment (Wi-Fi 8).</li>
<li>methods:  The authors use an analytical model based on Continuous Time Markov Chains (CTMCs) to characterize the throughput and spatial efficiency of C-SR.</li>
<li>results:  The authors show that C-SR can opportunistically enable parallel high-quality transmissions and achieve an average throughput gain of up to 59% compared to the legacy 802.11 Distributed Coordination Function (DCF) and up to 42% compared to the 802.11ax Overlapping Basic Service Set Packet Detect (OBSS&#x2F;PD) mechanism.Here is the result in Simplified Chinese text:</li>
<li>for: 这篇论文关注了802.11bn规定（Wi-Fi 8）中的多个访问点协调（MAPC）功能之一——协调空间重用（C-SR）。</li>
<li>methods: 作者们使用基于 kontinuous Time Markov Chains（CTMCs）的分析模型来描述C-SR的吞吐量和空间效率。</li>
<li>results: 作者们表明，C-SR可以机会地实现并发高质量传输，并实现与legacy 802.11分布协调函数（DCF）和802.11ax Overlapping Basic Service Set Packet Detect（OBSS&#x2F;PD）机制相比的吞吐量提高，最高达59%。<details>
<summary>Abstract</summary>
Multi-Access Point Coordination (MAPC) is becoming the cornerstone of the IEEE 802.11bn amendment, alias Wi-Fi 8. Among the MAPC features, Coordinated Spatial Reuse (C-SR) stands as one of the most appealing due to its capability to orchestrate simultaneous access point transmissions at a low implementation complexity. In this paper, we contribute to the understanding of C-SR by introducing an analytical model based on Continuous Time Markov Chains (CTMCs) to characterize its throughput and spatial efficiency. Applying the proposed model to several network topologies, we show that C-SR opportunistically enables parallel high-quality transmissions and yields an average throughput gain of up to 59% in comparison to the legacy 802.11 Distributed Coordination Function (DCF) and up to 42% when compared to the 802.11ax Overlapping Basic Service Set Packet Detect (OBSS/PD) mechanism.
</details>
<details>
<summary>摘要</summary>
多点存取协调（MAP）正成为IEEE 802.11bn 修订（即Wi-Fi 8）的核心。其中，协调空间重复（C-SR）是最吸引人的功能之一，因为它可以实现低实现 Complexity 下的同时多点变数通信。在本文中，我们对C-SR进行了分析，并基于状态空间过程（CTMC）引入了一个分析模型，以描述它的吞吐率和空间效率。我们将这个模型应用到了多个网络架构上，结果显示，C-SR可以允许高质量的平行传输，并产生了与传统802.11 DCF和802.11ax OBSS/PD Mechanism 相比的吞吐率增加，具体是59%。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Code-Multiple-Access-SCMA-Technique"><a href="#Sparse-Code-Multiple-Access-SCMA-Technique" class="headerlink" title="Sparse Code Multiple Access (SCMA) Technique"></a>Sparse Code Multiple Access (SCMA) Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09127">http://arxiv.org/abs/2309.09127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjeev Sharma, Kuntal Deka</li>
<li>For: This paper is written to introduce and analyze the code domain-based sparse code multiple access (SCMA) non-orthogonal multiple access (NOMA) scheme to enhance the spectral efficiency of wireless networks.* Methods: The paper uses code domain-based SCMA, which is designed and detected using a hybrid multiple access scheme that combines code-domain and power-domain NOMA. The paper also discusses the method for codebooks design and its impact on system performance.* Results: The paper includes simulation results to show the impact of various SCMA system parameters, such as the number of users, the spreading factor, and the code length, on the system performance. The results demonstrate the potential of SCMA to enhance the spectral efficiency of wireless networks.<details>
<summary>Abstract</summary>
Next-generation wireless networks require higher spectral efficiency and lower latency to meet the demands of various upcoming applications. Recently, non-orthogonal multiple access (NOMA) schemes are introduced in the literature for 5G and beyond. Various forms of NOMA are considered like power domain, code domain, pattern division multiple access, etc. to enhance the spectral efficiency of wireless networks. In this chapter, we introduce the code domain-based sparse code multiple access (SCMA) NOMA scheme to enhance the spectral efficiency of a wireless network. The design and detection of an SCMA system are analyzed in this chapter. Also, the method for codebooks design and its impact on system performance are highlighted. A hybrid multiple access scheme is also introduced using both code-domain and power-domain NOMA. Furthermore, simulation results are included to show the impact of various SCMA system parameters.ext-generation wireless networks require higher spectral efficiency and lower latency to meet the demands of various upcoming applications. Recently, non-orthogonal multiple access (NOMA) schemes are introduced in the literature for 5G and beyond. Various forms of NOMA are considered like power domain, code domain, pattern division multiple access, etc. to enhance the spectral efficiency of wireless networks. In this chapter, we introduce the code domainbased sparse code multiple access (SCMA) NOMA scheme to enhance the spectral efficiency of a wireless network. The design and detection of an SCMA system are analyzed in this chapter. Also, the method for codebooks design and its impact on system performance are highlighted. A hybrid multiple access scheme is also introduced using both code-domain and power-domain NOMA. Furthermore, simulation results are included to show the impact of various SCMA system parameters.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/17/eess.SP_2023_09_17/" data-id="cloimipiz0182s48856b8hc9j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/16/cs.SD_2023_09_16/" class="article-date">
  <time datetime="2023-09-16T15:00:00.000Z" itemprop="datePublished">2023-09-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/16/cs.SD_2023_09_16/">cs.SD - 2023-09-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-GAN-Based-Vocoders-with-Contrastive-Learning-Under-Data-limited-Condition"><a href="#Enhancing-GAN-Based-Vocoders-with-Contrastive-Learning-Under-Data-limited-Condition" class="headerlink" title="Enhancing GAN-Based Vocoders with Contrastive Learning Under Data-limited Condition"></a>Enhancing GAN-Based Vocoders with Contrastive Learning Under Data-limited Condition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09088">http://arxiv.org/abs/2309.09088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoming Guo, Seth Z. Zhao, Jiachen Lian, Gopala Anumanchipalli, Gerald Friedland</li>
<li>for: 本研究旨在提高 vocoder 模型在数据有限情况下的质量，不修改模型结构或添加更多数据。</li>
<li>methods: 本研究使用了对mel-spectrogram进行对比学习，以提高 vocoder 模型的语音质量。此外， authors 还尝试了在多Modal情况下使用waveform进行学习，以解决权值逐出问题。</li>
<li>results: 研究结果表明，通过对 vocoder 模型进行对比学习，可以在数据有限情况下提高模型性能，并且分析结果表明，提posed方法可以successfully解决权值逐出问题，并生成高质量的语音。<details>
<summary>Abstract</summary>
Vocoder models have recently achieved substantial progress in generating authentic audio comparable to human quality while significantly reducing memory requirement and inference time. However, these data-hungry generative models require large-scale audio data for learning good representations. In this paper, we apply contrastive learning methods in training the vocoder to improve the perceptual quality of the vocoder without modifying its architecture or adding more data. We design an auxiliary task with mel-spectrogram contrastive learning to enhance the utterance-level quality of the vocoder model under data-limited conditions. We also extend the task to include waveforms to improve the multi-modality comprehension of the model and address the discriminator overfitting problem. We optimize the additional task simultaneously with GAN training objectives. Our result shows that the tasks improve model performance substantially in data-limited settings. Our analysis based on the result indicates that the proposed design successfully alleviates discriminator overfitting and produces audio of higher fidelity.
</details>
<details>
<summary>摘要</summary>
很多最新的 vocoder 模型已经取得了很大的进步，可以生成比人类质量更高的真实音频，同时减少了内存需求和计算时间。然而，这些数据夹带的生成模型需要大量的音频数据来学习良好的表示。在这篇论文中，我们使用了对比学习方法来在训练 vocoder 模型中提高模型的感知质量，无需修改模型结构或添加更多的数据。我们设计了一项 auxiliary 任务，通过 mel-spectrogram 对比学习来提高 vocoder 模型在数据有限的情况下的话语质量。我们还将这项任务扩展到包括波形，以提高模型的多模式理解和解决探测器过拟合问题。我们同时优化了这些额外任务和 GAN 训练目标。我们的结果表明，这些任务可以在数据有限情况下提高模型性能的极大程度。我们的分析表明，我们的设计成功解决了探测器过拟合问题，并生成了更高的准确性和音频质量。
</details></li>
</ul>
<hr>
<h2 id="SynthTab-Leveraging-Synthesized-Data-for-Guitar-Tablature-Transcription"><a href="#SynthTab-Leveraging-Synthesized-Data-for-Guitar-Tablature-Transcription" class="headerlink" title="SynthTab: Leveraging Synthesized Data for Guitar Tablature Transcription"></a>SynthTab: Leveraging Synthesized Data for Guitar Tablature Transcription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09085">http://arxiv.org/abs/2309.09085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyi Zang, Yi Zhong, Frank Cwitkowitz, Zhiyao Duan</li>
<li>for: 这篇论文的目的是提高电子琴 Tablature Transcription (GTT) 模型的准确性和通用性，以应对现有的数据集规模和范围有限，导致现有的 GTT 模型容易过滤和没有通用性。</li>
<li>methods: 作者采用了多个商业电子琴和普通琴插件来生成 SynthTab，一个大规模的电子琴 Tablature Transcription 数据集。这个数据集是基于 DadaGP 提供的广泛的 Tablature 集，并且具有丰富的特征和技巧。</li>
<li>results: 实验显示，将先进 GTT 模型在 SynthTab 上进行预训后，可以提高同数据集的准确性，并且在跨数据集评估中具有较好的适应性和减少了过滤问题。<details>
<summary>Abstract</summary>
Guitar tablature is a form of music notation widely used among guitarists. It captures not only the musical content of a piece, but also its implementation and ornamentation on the instrument. Guitar Tablature Transcription (GTT) is an important task with broad applications in music education and entertainment. Existing datasets are limited in size and scope, causing state-of-the-art GTT models trained on such datasets to suffer from overfitting and to fail in generalization across datasets. To address this issue, we developed a methodology for synthesizing SynthTab, a large-scale guitar tablature transcription dataset using multiple commercial acoustic and electric guitar plugins. This dataset is built on tablatures from DadaGP, which offers a vast collection and the degree of specificity we wish to transcribe. The proposed synthesis pipeline produces audio which faithfully adheres to the original fingerings, styles, and techniques specified in the tablature with diverse timbre. Experiments show that pre-training state-of-the-art GTT model on SynthTab improves transcription accuracy in same-dataset tests. More importantly, it significantly mitigates overfitting problems of GTT models in cross-dataset evaluation.
</details>
<details>
<summary>摘要</summary>
吉他标谱是一种广泛用于吉他演奏的音乐notation，不仅记录了音乐内容，还包括 instru Meyer 和ornamentation。吉他标谱转写（GTT）是一个重要的任务，有广泛的应用在音乐教育和娱乐领域。现有的数据集 limitation 的size和scope，导致现有的GTT模型在这些数据集上进行训练后会出现过拟合和泛化问题。为解决这个问题，我们开发了一种方法ологи для生成 SynthTab，一个大规模的吉他标谱转写数据集，使用多种商业钢琴和电吉他插件。这个数据集基于DadaGP提供的大量标谱，我们可以根据我们的要求进行特定的转写。我们的合成管道可以生成具有多样 timbre 的音频，忠实地实现原始的手套、风格和技巧 specified in the tablature。实验表明，在 SynthTab 上先行训练 state-of-the-art GTT 模型可以提高同一个数据集的转写精度。更重要的是，它可以减轻 GTT 模型在不同数据集之间的过拟合问题。
</details></li>
</ul>
<hr>
<h2 id="Music-Generation-based-on-Generative-Adversarial-Networks-with-Transformer"><a href="#Music-Generation-based-on-Generative-Adversarial-Networks-with-Transformer" class="headerlink" title="Music Generation based on Generative Adversarial Networks with Transformer"></a>Music Generation based on Generative Adversarial Networks with Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09075">http://arxiv.org/abs/2309.09075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Jiang, Yi Zhong, Ruoxue Wu, Zhenghan Chen, Xiaoxuan Liang</li>
<li>for: 本研究旨在提高基于Transformers的自动生成音乐作品的质量，并且减少曝光偏见的影响。</li>
<li>methods: 我们使用了一种基于GAN框架的敌方损失函数，并使用了一个预训练的Span-BERT模型作为推论器。我们还使用了Gumbel-Softmax trick来实现整数序列的可微分化。</li>
<li>results: 我们通过人工评估和引入一种新的探测指标，证明了我们的方法比基于likelihood最大化的基eline模型具有更高的质量。<details>
<summary>Abstract</summary>
Autoregressive models based on Transformers have become the prevailing approach for generating music compositions that exhibit comprehensive musical structure. These models are typically trained by minimizing the negative log-likelihood (NLL) of the observed sequence in an autoregressive manner. However, when generating long sequences, the quality of samples from these models tends to significantly deteriorate due to exposure bias. To address this issue, we leverage classifiers trained to differentiate between real and sampled sequences to identify these failures. This observation motivates our exploration of adversarial losses as a complement to the NLL objective. We employ a pre-trained Span-BERT model as the discriminator in the Generative Adversarial Network (GAN) framework, which enhances training stability in our experiments. To optimize discrete sequences within the GAN framework, we utilize the Gumbel-Softmax trick to obtain a differentiable approximation of the sampling process. Additionally, we partition the sequences into smaller chunks to ensure that memory constraints are met. Through human evaluations and the introduction of a novel discriminative metric, we demonstrate that our approach outperforms a baseline model trained solely on likelihood maximization.
</details>
<details>
<summary>摘要</summary>
自适应模型基于Transformers已成为生成具有完整音乐结构的乐曲主要方法。这些模型通常通过逐步式拟合方式进行训练，以最小化负对数梯度（NLL）为目标。然而，在生成长序列时，这些模型的样本质量往往会受到曝光偏见的影响，导致样本质量下降。为 Addressing this issue, we leveraged classifiers trained to distinguish between real and sampled sequences to identify these failures. This observation motivates our exploration of adversarial losses as a complement to the NLL objective. We employed a pre-trained Span-BERT model as the discriminator in the Generative Adversarial Network (GAN) framework, which enhances training stability in our experiments. To optimize discrete sequences within the GAN framework, we utilized the Gumbel-Softmax trick to obtain a differentiable approximation of the sampling process. Additionally, we partitioned the sequences into smaller chunks to ensure that memory constraints were met. Through human evaluations and the introduction of a novel discriminative metric, we demonstrated that our approach outperformed a baseline model trained solely on likelihood maximization.Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Unifying-Robustness-and-Fidelity-A-Comprehensive-Study-of-Pretrained-Generative-Methods-for-Speech-Enhancement-in-Adverse-Conditions"><a href="#Unifying-Robustness-and-Fidelity-A-Comprehensive-Study-of-Pretrained-Generative-Methods-for-Speech-Enhancement-in-Adverse-Conditions" class="headerlink" title="Unifying Robustness and Fidelity: A Comprehensive Study of Pretrained Generative Methods for Speech Enhancement in Adverse Conditions"></a>Unifying Robustness and Fidelity: A Comprehensive Study of Pretrained Generative Methods for Speech Enhancement in Adverse Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09028">http://arxiv.org/abs/2309.09028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heming Wang, Meng Yu, Hao Zhang, Chunlei Zhang, Zhongweiyang Xu, Muqiao Yang, Yixuan Zhang, Dong Yu</li>
<li>for: 提高噪音环境下的语音信号质量</li>
<li>methods: 使用预训练的生成方法重新生成干净的语音信号</li>
<li>results: 实验表明，使用代码生成器可以获得更高的主观分数，并且生成的语音质量更高，噪音和反射减少。<details>
<summary>Abstract</summary>
Enhancing speech signal quality in adverse acoustic environments is a persistent challenge in speech processing. Existing deep learning based enhancement methods often struggle to effectively remove background noise and reverberation in real-world scenarios, hampering listening experiences. To address these challenges, we propose a novel approach that uses pre-trained generative methods to resynthesize clean, anechoic speech from degraded inputs. This study leverages pre-trained vocoder or codec models to synthesize high-quality speech while enhancing robustness in challenging scenarios. Generative methods effectively handle information loss in speech signals, resulting in regenerated speech that has improved fidelity and reduced artifacts. By harnessing the capabilities of pre-trained models, we achieve faithful reproduction of the original speech in adverse conditions. Experimental evaluations on both simulated datasets and realistic samples demonstrate the effectiveness and robustness of our proposed methods. Especially by leveraging codec, we achieve superior subjective scores for both simulated and realistic recordings. The generated speech exhibits enhanced audio quality, reduced background noise, and reverberation. Our findings highlight the potential of pre-trained generative techniques in speech processing, particularly in scenarios where traditional methods falter. Demos are available at https://whmrtm.github.io/SoundResynthesis.
</details>
<details>
<summary>摘要</summary>
增强语音信号质量在不利的听音环境中是一个长期挑战的问题。现有的深度学习基于的增强方法经常在实际场景中不能有效地除去背景噪声和反射，从而影响听众体验。为解决这些挑战，我们提出了一种新的方法，使用预训练的生成方法将清晰、无反射的语音重新生成出来。这项研究利用预训练的 vocoder 或 codec 模型来生成高质量的语音，同时提高了对挑战性场景的抗性。生成方法可以有效处理语音信号中的信息损失，从而生成具有提高的听音质量和减少的artefacts的语音。通过利用预训练模型的能力，我们实现了原始语音的忠实复制在不利条件下。实验评估表明，我们的提议方法在模拟数据集和实际采样中具有显著的效果和稳定性。特别是通过利用 codec，我们在模拟和实际录音中获得了更高的主观评分。生成的语音具有提高的听音质量、减少的背景噪声和反射。我们的发现表明，预训练的生成技术在语音处理中具有潜在的潜力，特别是在传统方法失效的场景下。 Demo 可以在 <https://whmrtm.github.io/SoundResynthesis> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Decoder-only-Architecture-for-Speech-Recognition-with-CTC-Prompts-and-Text-Data-Augmentation"><a href="#Decoder-only-Architecture-for-Speech-Recognition-with-CTC-Prompts-and-Text-Data-Augmentation" class="headerlink" title="Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation"></a>Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08876">http://arxiv.org/abs/2309.08876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe</li>
<li>for: 提高自动语音识别（ASR）模型的精度和效率，使其可以使用文本数据进行训练。</li>
<li>methods: 采用decoder-only架构，使用简单的文本扩充，并使用CTC预测来提供音频信息。</li>
<li>results: 在LibriSpeech和Switchboard datasets上，提出的模型比普通CTC预测减少了0.3%和1.4%的单词错误率，并在LibriSpeech 100h和Switchboard训练场景中超过了传统的encoder-decoder ASR模型。<details>
<summary>Abstract</summary>
Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.
</details>
<details>
<summary>摘要</summary>
收集音频文本对是costly的;但是可以轻松地获取文本数据。除非使用浅层融合，否则末端自动语音识别（ASR）模型需要建筑修改或额外训练方式来使用文本数据。受最近的语言模型（LM）的进步启发，我们提议使用decoder-only架构 для ASR，并使用简单的文本扩展。为了提供音频信息，encoder特征被CTC预测压缩后用作decoder的激活器，可以视为通过decoder-only模型来更正CTC预测。由于decoder架构与 autoregressive LM 相同，因此可以通过外部文本数据进行LM训练来增强模型。我们在LibriSpeech和Switchboard上进行了实验比较，发现我们提议的模型在文本扩展训练下降低了word error rate（PER）by 0.3%和1.4%在LibriSpeech test-clean和test-otherSet上，并且在Switchboard和CallHome上降低了2.9%和5.0%。此外，我们的模型在计算效率方面具有优势，并在LibriSpeech 100h和Switchboard训练enario上超过了传统的末端encoder-decoder ASR模型。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Latent-Space-Reconstruction-Learning-for-Audio-Text-Retrieval"><a href="#Contrastive-Latent-Space-Reconstruction-Learning-for-Audio-Text-Retrieval" class="headerlink" title="Contrastive Latent Space Reconstruction Learning for Audio-Text Retrieval"></a>Contrastive Latent Space Reconstruction Learning for Audio-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08839">http://arxiv.org/abs/2309.08839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiyi Luo, Xulong Zhang, Jianzong Wang, Huaxiong Li, Ning Cheng, Jing Xiao</li>
<li>for: 这个论文主要针对 audio-to-text 模式下的跨模态检索问题，即使用 audio clips 和文本进行对应。</li>
<li>methods: 该论文提出了一种新的 Contrastive Latent Space Reconstruction Learning (CLSR) 方法，它在对比表示学习中考虑了内模态分离性，并采用了 adaptive temperature control 策略。此外，该方法还包含了模态交互的latent representation reconstruction模块。</li>
<li>results: 对两个 audio-text 数据集进行比较，CLSR 方法表现出了较高的效果，胜过了一些当前最佳方法。<details>
<summary>Abstract</summary>
Cross-modal retrieval (CMR) has been extensively applied in various domains, such as multimedia search engines and recommendation systems. Most existing CMR methods focus on image-to-text retrieval, whereas audio-to-text retrieval, a less explored domain, has posed a great challenge due to the difficulty to uncover discriminative features from audio clips and texts. Existing studies are restricted in the following two ways: 1) Most researchers utilize contrastive learning to construct a common subspace where similarities among data can be measured. However, they considers only cross-modal transformation, neglecting the intra-modal separability. Besides, the temperature parameter is not adaptively adjusted along with semantic guidance, which degrades the performance. 2) These methods do not take latent representation reconstruction into account, which is essential for semantic alignment. This paper introduces a novel audio-text oriented CMR approach, termed Contrastive Latent Space Reconstruction Learning (CLSR). CLSR improves contrastive representation learning by taking intra-modal separability into account and adopting an adaptive temperature control strategy. Moreover, the latent representation reconstruction modules are embedded into the CMR framework, which improves modal interaction. Experiments in comparison with some state-of-the-art methods on two audio-text datasets have validated the superiority of CLSR.
</details>
<details>
<summary>摘要</summary>
跨模式检索（CMR）在不同领域得到了广泛应用，如多媒体搜索引擎和推荐系统。现有的大多数CMR方法强调图像到文本检索，而听音到文本检索则是一个未得到充分发展的领域，这主要是因为听音clip和文本之间找到特征点具有很大的挑战性。现有的研究受到以下两种限制：1. 大多数研究人员采用对偶学习来构建共同的特征空间，以便在数据之间可以测量相似性。然而，他们只考虑了跨模式变换，忽略了内模态分离性。此外，温度参数不适应性地调整，这会下降性能。2. 这些方法不会考虑隐藏表示的重建，这是必要的 дляsemantic alignment。本文提出了一种新的听音到文本 oriented CMR方法，称为对偶特征空间重建学习（CLSR）。CLSR方法改进了对偶表示学习，通过考虑内模态分离性和适应性温度控制策略。此外，模态交互模块被引入到CMR框架中，以提高模态交互。对两个音频到文本数据集进行比较 экспериментирова， Validated the superiority of CLSR。
</details></li>
</ul>
<hr>
<h2 id="FastGraphTTS-An-Ultrafast-Syntax-Aware-Speech-Synthesis-Framework"><a href="#FastGraphTTS-An-Ultrafast-Syntax-Aware-Speech-Synthesis-Framework" class="headerlink" title="FastGraphTTS: An Ultrafast Syntax-Aware Speech Synthesis Framework"></a>FastGraphTTS: An Ultrafast Syntax-Aware Speech Synthesis Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08837">http://arxiv.org/abs/2309.08837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzong Wang, Xulong Zhang, Aolan Sun, Ning Cheng, Jing Xiao</li>
<li>for: 这篇论文旨在 integrate graph-to-sequence 到一个终端文本至语音框架中，以实现 syntax-aware 模型化。</li>
<li>methods: 这篇论文使用了 dependency parsing 模块将输入文本解析成一个 sintactic graph，然后使用 graph encoder 对这个 sintactic graph 进行编码，提取 sintactic hidden information，并与 phoneme embedding 进行拼接，并输入到 alignment 和 flow-based decoding 模块中，生成 raw audio waveform。</li>
<li>results: 实验结果表明，这种模型可以提供更好的语音合成效果，并且在 subjective prosodic evaluation 中获得了更高的分数。此外，模型还可以进行voice conversion。此外，通过 AI chip operator 的设计，模型的效率得到了5x的加速。<details>
<summary>Abstract</summary>
This paper integrates graph-to-sequence into an end-to-end text-to-speech framework for syntax-aware modelling with syntactic information of input text. Specifically, the input text is parsed by a dependency parsing module to form a syntactic graph. The syntactic graph is then encoded by a graph encoder to extract the syntactic hidden information, which is concatenated with phoneme embedding and input to the alignment and flow-based decoding modules to generate the raw audio waveform. The model is experimented on two languages, English and Mandarin, using single-speaker, few samples of target speakers, and multi-speaker datasets, respectively. Experimental results show better prosodic consistency performance between input text and generated audio, and also get higher scores in the subjective prosodic evaluation, and show the ability of voice conversion. Besides, the efficiency of the model is largely boosted through the design of the AI chip operator with 5x acceleration.
</details>
<details>
<summary>摘要</summary>
这篇论文将graph-to-sequence integrate到了一个端到端的文本到语音框架中，以实现文本的 syntax-aware 模型化。具体来说，输入文本首先被依赖分析模块解析，形成一个语法图。然后，语法图被图编码器编码，以提取语法隐藏信息。这些隐藏信息与phoneme embedding相加，并输入到对齐和流程基于解码模块中，以生成原始的音频波形。模型在英语和普通话两种语言上进行了实验，使用单个说话者、少量目标说话者和多个说话者的数据集，分别进行了实验。实验结果表明，模型可以更好地保持输入文本和生成的音频波形之间的PROSODIC 一致性，并在主观的PROSODIC 评价中获得更高的分数。此外，模型的效率得到了通过AI芯片运算符的5倍加速的大幅提升。
</details></li>
</ul>
<hr>
<h2 id="Boosting-End-to-End-Multilingual-Phoneme-Recognition-through-Exploiting-Universal-Speech-Attributes-Constraints"><a href="#Boosting-End-to-End-Multilingual-Phoneme-Recognition-through-Exploiting-Universal-Speech-Attributes-Constraints" class="headerlink" title="Boosting End-to-End Multilingual Phoneme Recognition through Exploiting Universal Speech Attributes Constraints"></a>Boosting End-to-End Multilingual Phoneme Recognition through Exploiting Universal Speech Attributes Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08828">http://arxiv.org/abs/2309.08828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Yen, Sabato Marco Siniscalchi, Chin-Hui Lee</li>
<li>for: 该研究旨在提出一种多语言自动语音识别（ASR）系统，以利用语音生成器的知识来提高系统的性能。</li>
<li>methods: 该研究使用了一种基于语音特征的 attribute-to-phoneme 映射方法，将知识基于生成器的特征映射到输出phoneme上，以限制系统的预测。</li>
<li>results: 该研究在多种语言的测试数据上进行了比较，并发现了与传统多语言方法相比，提出的解决方案能够提高系统的性能，平均提高6.85%。此外，研究还发现了该解决方案能够消除与特征不一致的phoneme预测。<details>
<summary>Abstract</summary>
We propose a first step toward multilingual end-to-end automatic speech recognition (ASR) by integrating knowledge about speech articulators. The key idea is to leverage a rich set of fundamental units that can be defined "universally" across all spoken languages, referred to as speech attributes, namely manner and place of articulation. Specifically, several deterministic attribute-to-phoneme mapping matrices are constructed based on the predefined set of universal attribute inventory, which projects the knowledge-rich articulatory attribute logits, into output phoneme logits. The mapping puts knowledge-based constraints to limit inconsistency with acoustic-phonetic evidence in the integrated prediction. Combined with phoneme recognition, our phone recognizer is able to infer from both attribute and phoneme information. The proposed joint multilingual model is evaluated through phoneme recognition. In multilingual experiments over 6 languages on benchmark datasets LibriSpeech and CommonVoice, we find that our proposed solution outperforms conventional multilingual approaches with a relative improvement of 6.85% on average, and it also demonstrates a much better performance compared to monolingual model. Further analysis conclusively demonstrates that the proposed solution eliminates phoneme predictions that are inconsistent with attributes.
</details>
<details>
<summary>摘要</summary>
我们提出一个初步的多语言端到端自动语音识别（ASR）方法，通过 интеGRATE知识About speech articulators。关键思想是利用一个丰富的基本单元，可以在所有的口语语言中 Universally defined，称为speech attributes，namely manner and place of articulation。特别是，我们构建了一些决定性的 attribute-to-phoneme mapping矩阵，基于预定的universal attribute inventory，将知识医学特征logits项目到输出phoneme logits。这种映射带有知识基础的约束，以限制与语音-phonetic证据的不一致。与phoneme recognition结合，我们的电话识别器能够从both attribute和phoneme信息中进行推理。我们提出的联合多语言模型在LibriSpeech和CommonVoice多语言测试集上进行了phoneme recognition测试，并 obtAIN了相对改善6.85%的平均提升，以及和单语言模型的较好表现。进一步的分析表明，我们的解决方案可以消除与 attribute不一致的phoneme预测。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/16/cs.SD_2023_09_16/" data-id="cloimipe400v0s488eirx5yu8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/16/cs.CV_2023_09_16/" class="article-date">
  <time datetime="2023-09-16T13:00:00.000Z" itemprop="datePublished">2023-09-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/16/cs.CV_2023_09_16/">cs.CV - 2023-09-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FrameRS-A-Video-Frame-Compression-Model-Composed-by-Self-supervised-Video-Frame-Reconstructor-and-Key-Frame-Selector"><a href="#FrameRS-A-Video-Frame-Compression-Model-Composed-by-Self-supervised-Video-Frame-Reconstructor-and-Key-Frame-Selector" class="headerlink" title="FrameRS: A Video Frame Compression Model Composed by Self supervised Video Frame Reconstructor and Key Frame Selector"></a>FrameRS: A Video Frame Compression Model Composed by Self supervised Video Frame Reconstructor and Key Frame Selector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09083">http://arxiv.org/abs/2309.09083</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiqianfu/framemae">https://github.com/qiqianfu/framemae</a></li>
<li>paper_authors: Qiqian Fu, Guanhong Wang, Gaoang Wang</li>
<li>for: 本研究提出了一种框架重建模型：FrameRS，用于自动重建视频帧。</li>
<li>methods: 模型包括一个自我supervised的视频帧重建器和一个关键帧选择器。帧重建器FrameMAE是基于图像MAE的原理，适应视频上下文。关键帧选择器是基于CNN结构，通过从encoder中获取高级别semantic信息，可以低计算成本预测关键帧。</li>
<li>results: 模型可以有效地压缩视频clip，保留约30%的重要帧。性能方面，我们的模型在计算效率和竞争准确性方面表现出色，与传统的关键帧提取算法相比有所提升。代码可以在Github上下载。<details>
<summary>Abstract</summary>
In this paper, we present frame reconstruction model: FrameRS. It consists self-supervised video frame reconstructor and key frame selector. The frame reconstructor, FrameMAE, is developed by adapting the principles of the Masked Autoencoder for Images (MAE) for video context. The key frame selector, Frame Selector, is built on CNN architecture. By taking the high-level semantic information from the encoder of FrameMAE as its input, it can predicted the key frames with low computation costs. Integrated with our bespoke Frame Selector, FrameMAE can effectively compress a video clip by retaining approximately 30% of its pivotal frames. Performance-wise, our model showcases computational efficiency and competitive accuracy, marking a notable improvement over traditional Key Frame Extract algorithms. The implementation is available on Github
</details>
<details>
<summary>摘要</summary>
本文提出了一种框架重建模型：FrameRS。它包含自我超级视频框架重建器和关键帧选择器。框架重建器 FrameMAE 是基于图像隐藏autoencoder（MAE）的视频上的应用，而关键帧选择器 Frame Selector 是基于卷积神经网络架构。通过将高级 semantic 信息从 FrameMAE 的解码器作为输入，Frame Selector 可以预测关键帧，计算成本低。将 FrameMAE 与我们自己的 Frame Selector 结合使用，可以有效地压缩视频clip，保留约 30% 的关键帧。在性能方面，我们的模型表现出了计算效率和竞争性准确率，代表了传统关键帧提取算法的 Notable Improvement。实现可以在 Github 上找到。
</details></li>
</ul>
<hr>
<h2 id="Multi-camera-Bird’s-Eye-View-Perception-for-Autonomous-Driving"><a href="#Multi-camera-Bird’s-Eye-View-Perception-for-Autonomous-Driving" class="headerlink" title="Multi-camera Bird’s Eye View Perception for Autonomous Driving"></a>Multi-camera Bird’s Eye View Perception for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09080">http://arxiv.org/abs/2309.09080</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Unger, Nikhil Gosala, Varun Ravi Kumar, Shubhankar Borse, Abhinav Valada, Senthil Yogamani</li>
<li>for: 这 paper 主要是为了探讨多摄像头基于深度学习模型在逻辑投影空间（BEV）中的物体表示方法。</li>
<li>methods: 这 paper 使用的方法主要是基于深度学习模型，将摄像头图像转换到逻辑投影空间（BEV）中，并使用几何约束来保证转换的准确性。</li>
<li>results: 这 paper 的结果表明，使用深度学习模型对摄像头图像的转换在逻辑投影空间（BEV）中可以实现更高的准确性和灵活性，并且可以与其他感知器结合进行有效的感知融合。<details>
<summary>Abstract</summary>
Most automated driving systems comprise a diverse sensor set, including several cameras, Radars, and LiDARs, ensuring a complete 360\deg coverage in near and far regions. Unlike Radar and LiDAR, which measure directly in 3D, cameras capture a 2D perspective projection with inherent depth ambiguity. However, it is essential to produce perception outputs in 3D to enable the spatial reasoning of other agents and structures for optimal path planning. The 3D space is typically simplified to the BEV space by omitting the less relevant Z-coordinate, which corresponds to the height dimension.The most basic approach to achieving the desired BEV representation from a camera image is IPM, assuming a flat ground surface. Surround vision systems that are pretty common in new vehicles use the IPM principle to generate a BEV image and to show it on display to the driver. However, this approach is not suited for autonomous driving since there are severe distortions introduced by this too-simplistic transformation method. More recent approaches use deep neural networks to output directly in BEV space. These methods transform camera images into BEV space using geometric constraints implicitly or explicitly in the network. As CNN has more context information and a learnable transformation can be more flexible and adapt to image content, the deep learning-based methods set the new benchmark for BEV transformation and achieve state-of-the-art performance. First, this chapter discusses the contemporary trends of multi-camera-based DNN (deep neural network) models outputting object representations directly in the BEV space. Then, we discuss how this approach can extend to effective sensor fusion and coupling downstream tasks like situation analysis and prediction. Finally, we show challenges and open problems in BEV perception.
</details>
<details>
<summary>摘要</summary>
现代自动驾驶系统通常包括多种感知器，包括数个摄像头、雷达和LiDAR，以确保完整的360度覆盖 both near and far regions。不同于雷达和LiDAR，摄像头会 Capture a 2D perspective projection with inherent depth ambiguity。然而，以便实现最佳路径规划，需要生成3D感知输出。为了简化3D空间，通常会将Z坐标（高度维度）排除，得到BEV空间（bird's eye view）。将摄像头图像转换为BEV空间的最基本方法是IPM（平面地面假设）。许多新车型的围视系统都使用IPM原理生成BEV图像，并将其显示给司机。然而，这种方法不适用于自动驾驶，因为它会引入严重的扭曲。更近期的方法使用深度神经网络直接将摄像头图像转换为BEV空间。这些方法使用摄像头图像中的几何约束，以及深度神经网络学习的变换，以生成BEV图像。由于神经网络具有更多的内容信息和可学习的变换，深度学习基于方法已经设置了新的标准 дляBEV转换，并实现了状态前景性的表现。本章首先介绍了当代多摄像头基于神经网络（deep neural network）模型，输出对象表示直接在BEV空间。然后，我们讨论了如何扩展到有效的感知融合和下游任务，如情况分析和预测。最后，我们介绍了BEV感知的挑战和开放问题。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Green-Object-Tracker-GOT-without-Offline-Pre-training"><a href="#Unsupervised-Green-Object-Tracker-GOT-without-Offline-Pre-training" class="headerlink" title="Unsupervised Green Object Tracker (GOT) without Offline Pre-training"></a>Unsupervised Green Object Tracker (GOT) without Offline Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09078">http://arxiv.org/abs/2309.09078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiruo Zhou, Suya You, C. -C. Jay Kuo</li>
<li>for: 提高单个目标跟踪精度，降低标注成本和计算复杂性，实现灵活可 deployment on edge devices。</li>
<li>methods:  ensemble of three prediction branches：1) 全局对象基 correlator，2) 本地 patch-based correlator，3) superpixel-based segmentator，使用了简单的模型和低计算复杂性。</li>
<li>results: 与现有的半监督跟踪器相当，需要大量的Offline预训练，但GOT具有较低的计算复杂性和小型模型大小，可以轻松部署于移动和边缘设备。<details>
<summary>Abstract</summary>
Supervised trackers trained on labeled data dominate the single object tracking field for superior tracking accuracy. The labeling cost and the huge computational complexity hinder their applications on edge devices. Unsupervised learning methods have also been investigated to reduce the labeling cost but their complexity remains high. Aiming at lightweight high-performance tracking, feasibility without offline pre-training, and algorithmic transparency, we propose a new single object tracking method, called the green object tracker (GOT), in this work. GOT conducts an ensemble of three prediction branches for robust box tracking: 1) a global object-based correlator to predict the object location roughly, 2) a local patch-based correlator to build temporal correlations of small spatial units, and 3) a superpixel-based segmentator to exploit the spatial information of the target frame. GOT offers competitive tracking accuracy with state-of-the-art unsupervised trackers, which demand heavy offline pre-training, at a lower computation cost. GOT has a tiny model size (<3k parameters) and low inference complexity (around 58M FLOPs per frame). Since its inference complexity is between 0.1%-10% of DL trackers, it can be easily deployed on mobile and edge devices.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>supervised trackers在标注数据上训练的场景下占据单个对象跟踪领域的先导地位，因为标签成本和计算复杂性而降低其应用于边缘设备。不supervised learning方法也被研究以减少标签成本，但它们的复杂度仍然高。为了实现轻量级高性能的跟踪，不需要线上预训练、算法透明度和可行性，我们在这里提出了一种新的单对象跟踪方法，称为绿色对象跟踪器（GOT）。GOT使用三个预测分支来提供粗略对象位置预测和高精度跟踪：1）全局对象基于相关器来预测对象位置，2）本地小区域基于相关器来建立时间相关性，3）超像素基于分割器来利用目标帧中的空间信息。GOT可以与现有的无监督跟踪器相比，它们需要大量的线上预训练，并且具有较低的计算成本（<3k参数）和低的计算复杂度（约58M FLOPs每帧）。由于其计算复杂度在0.1%-10%之间，因此它可以轻松部署在移动和边缘设备上。
</details></li>
</ul>
<hr>
<h2 id="MMST-ViT-Climate-Change-aware-Crop-Yield-Prediction-via-Multi-Modal-Spatial-Temporal-Vision-Transformer"><a href="#MMST-ViT-Climate-Change-aware-Crop-Yield-Prediction-via-Multi-Modal-Spatial-Temporal-Vision-Transformer" class="headerlink" title="MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer"></a>MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09067">http://arxiv.org/abs/2309.09067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fudong03/mmst-vit">https://github.com/fudong03/mmst-vit</a></li>
<li>paper_authors: Fudong Lin, Summer Crawford, Kaleb Guillot, Yihe Zhang, Yan Chen, Xu Yuan, Li Chen, Shelby Williams, Robert Minvielle, Xiangming Xiao, Drew Gholson, Nicolas Ashwell, Tri Setiyono, Brenda Tubana, Lu Peng, Magdy Bayoumi, Nian-Feng Tzeng</li>
<li>for: 预测美国县级作物产量，考虑植物生长季节的天气变化和气候变化对作物的影响。</li>
<li>methods: 我们开发了一种深度学习基于解决方案，即多Modal Spatial-Temporal Vision Transformer（MMST-ViT），利用视觉遥感数据和短期天气数据来模型植物生长季节的天气变化对作物生长的影响。</li>
<li>results: 我们的MMST-ViT在200个美国县的实验中表现出色，与三个性能指标之间的比较结果都高于其他相似方法。<details>
<summary>Abstract</summary>
Precise crop yield prediction provides valuable information for agricultural planning and decision-making processes. However, timely predicting crop yields remains challenging as crop growth is sensitive to growing season weather variation and climate change. In this work, we develop a deep learning-based solution, namely Multi-Modal Spatial-Temporal Vision Transformer (MMST-ViT), for predicting crop yields at the county level across the United States, by considering the effects of short-term meteorological variations during the growing season and the long-term climate change on crops. Specifically, our MMST-ViT consists of a Multi-Modal Transformer, a Spatial Transformer, and a Temporal Transformer. The Multi-Modal Transformer leverages both visual remote sensing data and short-term meteorological data for modeling the effect of growing season weather variations on crop growth. The Spatial Transformer learns the high-resolution spatial dependency among counties for accurate agricultural tracking. The Temporal Transformer captures the long-range temporal dependency for learning the impact of long-term climate change on crops. Meanwhile, we also devise a novel multi-modal contrastive learning technique to pre-train our model without extensive human supervision. Hence, our MMST-ViT captures the impacts of both short-term weather variations and long-term climate change on crops by leveraging both satellite images and meteorological data. We have conducted extensive experiments on over 200 counties in the United States, with the experimental results exhibiting that our MMST-ViT outperforms its counterparts under three performance metrics of interest.
</details>
<details>
<summary>摘要</summary>
precise 农作物产量预测提供了重要的农业规划和决策过程中的信息。然而，在季节变化和气候变化的影响下，准确地预测农作物产量仍然是一项挑战。在这种情况下，我们开发了一种深度学习基于解决方案，即多Modal空间时间变换器（MMST-ViT），用于预测美国各县的农作物产量，并考虑了季节变化的短期天气影响和气候变化对农作物的影响。具体来说，我们的MMST-ViT包括多Modal变换器、空间变换器和时间变换器。多Modal变换器利用了远程感知数据和季节变化天气数据来模型季节变化对农作物生长的影响。空间变换器学习了高分辨率的空间相关性，以便准确地跟踪农作物的生长。时间变换器捕捉了长期时间相关性，以便学习气候变化对农作物的影响。此外，我们还开发了一种新的多Modal对比学习技术，以不需要大量的人工监督来预处理我们的模型。因此，我们的MMST-ViT可以 capture季节变化和气候变化对农作物的影响，并且在美国200多个县的实验结果表明，我们的MMST-ViT在三个关键性能指标上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Sub-action-Prototype-Learning-for-Point-level-Weakly-supervised-Temporal-Action-Localization"><a href="#Sub-action-Prototype-Learning-for-Point-level-Weakly-supervised-Temporal-Action-Localization" class="headerlink" title="Sub-action Prototype Learning for Point-level Weakly-supervised Temporal Action Localization"></a>Sub-action Prototype Learning for Point-level Weakly-supervised Temporal Action Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09060">http://arxiv.org/abs/2309.09060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueyang Li, Yonghong Hou, Wanqing Li</li>
<li>for: 提高点级弱监督时间动作地理化（PWTAL）的性能，使其能够基于唯一时间戳注释每个动作实例。</li>
<li>methods: 提出了一种新的子动作原型学习框架（SPL-Loc），包括子动作原型归一化（SPC）和顺序原型对齐（OPA）。 SPC 适应性地提取了表现出时间尺度和空间内容变化的动作实例的表示性质。 OPA 选择了相关的原型，以提供完整性提示符 дляpseudo标签生成。</li>
<li>results: 与现有SOTA PWTAL方法进行了广泛的实验，并显示了提档SPL-Loc可以准确地地理化动作边界。<details>
<summary>Abstract</summary>
Point-level weakly-supervised temporal action localization (PWTAL) aims to localize actions with only a single timestamp annotation for each action instance. Existing methods tend to mine dense pseudo labels to alleviate the label sparsity, but overlook the potential sub-action temporal structures, resulting in inferior performance. To tackle this problem, we propose a novel sub-action prototype learning framework (SPL-Loc) which comprises Sub-action Prototype Clustering (SPC) and Ordered Prototype Alignment (OPA). SPC adaptively extracts representative sub-action prototypes which are capable to perceive the temporal scale and spatial content variation of action instances. OPA selects relevant prototypes to provide completeness clue for pseudo label generation by applying a temporal alignment loss. As a result, pseudo labels are derived from alignment results to improve action boundary prediction. Extensive experiments on three popular benchmarks demonstrate that the proposed SPL-Loc significantly outperforms existing SOTA PWTAL methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>点级弱监视时间动作地点（PWTAL）目标是在每个动作实例只有一个时间标签的情况下地址动作。现有方法通常是利用密集假标签来减轻标签稀疏性，但是忽略了可能存在的次动作时间结构，导致性能较差。为解决这个问题，我们提出了一种新的子动作原型学习框架（SPL-Loc），它包括子动作原型聚类（SPC）和有序原型对齐（OPA）。SPC可适应性EXTRACT Representative sub-action prototypes, which are capable of perceiving the temporal scale and spatial content variation of action instances. OPA选择相关的原型来提供完整的假标签生成的准备，通过应用时间对齐损失。因此，假标签来自对齐结果进行改进动作边界预测。广泛的实验表明，提出的 SPL-Loc 明显超过现有SOTA PWTAL方法。
</details></li>
</ul>
<hr>
<h2 id="Microscale-3-D-Capacitance-Tomography-with-a-CMOS-Sensor-Array"><a href="#Microscale-3-D-Capacitance-Tomography-with-a-CMOS-Sensor-Array" class="headerlink" title="Microscale 3-D Capacitance Tomography with a CMOS Sensor Array"></a>Microscale 3-D Capacitance Tomography with a CMOS Sensor Array</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09039">http://arxiv.org/abs/2309.09039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manar Abdelatty, Joseph Incandela, Kangping Hu, Joseph W. Larkin, Sherief Reda, Jacob K. Rosenstein</li>
<li>for: 这个论文主要是用来描述电容 Tomatoesography（ECT）技术在微型系统中的应用。</li>
<li>methods: 该论文使用了CMOS微电极阵列来实现ECT成像，并提出了一种深度学习架构和改进的多目标训练方法来重建射电常数图像。</li>
<li>results: 实验结果表明，提议的方法能够高精度地重建微型系统中的3D结构，包括精确地测量微球体积和细菌生物胶囊的尺寸。 predictions accuracy为91.5%和82.7%。<details>
<summary>Abstract</summary>
Electrical capacitance tomography (ECT) is a nonoptical imaging technique in which a map of the interior permittivity of a volume is estimated by making capacitance measurements at its boundary and solving an inverse problem. While previous ECT demonstrations have often been at centimeter scales, ECT is not limited to macroscopic systems. In this paper, we demonstrate ECT imaging of polymer microspheres and bacterial biofilms using a CMOS microelectrode array, achieving spatial resolution of 10 microns. Additionally, we propose a deep learning architecture and an improved multi-objective training scheme for reconstructing out-of-plane permittivity maps from the sensor measurements. Experimental results show that the proposed approach is able to resolve microscopic 3-D structures, achieving 91.5% prediction accuracy on the microsphere dataset and 82.7% on the biofilm dataset, including an average of 4.6% improvement over baseline computational methods.
</details>
<details>
<summary>摘要</summary>
电容测量探测技术（ECT）是一种非光学图像技术，可以测量物体内部电容 coefficient的地图，并解决一个倒逼问题。而在过去的ECT示范中，通常是在厘米级别进行，但ECT并不限于巨观系统。在这篇论文中，我们使用CMOS微电极阵列进行ECT探测，实现了10μ米的空间分辨率。此外，我们提出了深度学习架构和改进的多目标训练方案，用于从传感器测量数据中重建垂直电容地图。实验结果表明，我们的方法能够分解微观三维结构，达到91.5%的预测精度（在微球数据集上）和82.7%的预测精度（在生物质层数据集上），其中平均与基线计算方法相差4.6%。
</details></li>
</ul>
<hr>
<h2 id="RingMo-lite-A-Remote-Sensing-Multi-task-Lightweight-Network-with-CNN-Transformer-Hybrid-Framework"><a href="#RingMo-lite-A-Remote-Sensing-Multi-task-Lightweight-Network-with-CNN-Transformer-Hybrid-Framework" class="headerlink" title="RingMo-lite: A Remote Sensing Multi-task Lightweight Network with CNN-Transformer Hybrid Framework"></a>RingMo-lite: A Remote Sensing Multi-task Lightweight Network with CNN-Transformer Hybrid Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09003">http://arxiv.org/abs/2309.09003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuelei Wang, Ting Zhang, Liangjin Zhao, Lin Hu, Zhechao Wang, Ziqing Niu, Peirui Cheng, Kaiqiang Chen, Xuan Zeng, Zhirui Wang, Hongqi Wang, Xian Sun</li>
<li>for: 这个论文旨在提出一个轻量级的RS类型视觉基础模型，以便在边缘设备上进行RS影像解释。</li>
<li>methods: 这个论文使用了一个CNN-Transformer混合架构，具有一个双支结构，其中使用了Transformer模组作为低通架构，以EXTRACTRS影像的全球特征；而CNN模组则被用作堆叠高通架构，以EXTRACTRS影像的细节特征。</li>
<li>results: 相比于RingMo，这个提案的RingMo-lite将参数减少了大约60%，并在不同的RS影像解释任务中保持了缩减的几成比，而且在大多数场景下，其精度下降了不到2%。此外，这个研究将在未来与MindSpore computng平台集成。<details>
<summary>Abstract</summary>
In recent years, remote sensing (RS) vision foundation models such as RingMo have emerged and achieved excellent performance in various downstream tasks. However, the high demand for computing resources limits the application of these models on edge devices. It is necessary to design a more lightweight foundation model to support on-orbit RS image interpretation. Existing methods face challenges in achieving lightweight solutions while retaining generalization in RS image interpretation. This is due to the complex high and low-frequency spectral components in RS images, which make traditional single CNN or Vision Transformer methods unsuitable for the task. Therefore, this paper proposes RingMo-lite, an RS multi-task lightweight network with a CNN-Transformer hybrid framework, which effectively exploits the frequency-domain properties of RS to optimize the interpretation process. It is combined by the Transformer module as a low-pass filter to extract global features of RS images through a dual-branch structure, and the CNN module as a stacked high-pass filter to extract fine-grained details effectively. Furthermore, in the pretraining stage, the designed frequency-domain masked image modeling (FD-MIM) combines each image patch's high-frequency and low-frequency characteristics, effectively capturing the latent feature representation in RS data. As shown in Fig. 1, compared with RingMo, the proposed RingMo-lite reduces the parameters over 60% in various RS image interpretation tasks, the average accuracy drops by less than 2% in most of the scenes and achieves SOTA performance compared to models of the similar size. In addition, our work will be integrated into the MindSpore computing platform in the near future.
</details>
<details>
<summary>摘要</summary>
在近年，远程感知（RS）视觉基础模型如RingMo出现并在各种下游任务中表现出色。然而，计算资源的高需求限制了这些模型在边缘设备上的应用。为了解决这个问题，这篇论文提出了RingMo-lite，一种RS多任务轻量级网络，它采用了CNN-Transformer混合框架，并且有效地利用RS图像的频率频谱特性来优化解释过程。RingMo-lite由Transformer模块作为低通滤波器，EXTRACTRS图像的全面特征，而CNN模块作为堆叠高通滤波器，EXTRACTRS图像的细腻细节。此外，在预训练阶段，我们设计了频率频谱遮盲图像模型（FD-MIM），该模型可以有效地捕捉RS数据中各个图像块的高频和低频特征，从而获得RS数据的秘密特征表示。根据图1，相比RingMo，我们提出的RingMo-lite减少了参数超过60%，在各种RS图像解释任务中，均低于2%的场景下，保持了SOTA的性能。此外，我们计划将这些工作与MindSpore计算平台集成。
</details></li>
</ul>
<hr>
<h2 id="OmniLRS-A-Photorealistic-Simulator-for-Lunar-Robotics"><a href="#OmniLRS-A-Photorealistic-Simulator-for-Lunar-Robotics" class="headerlink" title="OmniLRS: A Photorealistic Simulator for Lunar Robotics"></a>OmniLRS: A Photorealistic Simulator for Lunar Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08997">http://arxiv.org/abs/2309.08997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antoinerichard/lunarsim">https://github.com/antoinerichard/lunarsim</a></li>
<li>paper_authors: Antoine Richard, Junnosuke Kamohara, Kentaro Uno, Shreya Santra, Dave van der Meer, Miguel Olivares-Mendez, Kazuya Yoshida</li>
<li>for: The paper is written for developers and researchers who are interested in developing algorithms for lunar robotic exploration and need a high-fidelity simulator to evaluate their algorithms.</li>
<li>methods: The paper proposes a new lunar simulator called OmniLRS, which is based on Nvidia’s robotic simulator Isaac Sim. The simulator provides fast procedural environment generation, multi-robot capabilities, and a synthetic data pipeline for machine-learning applications.</li>
<li>results: The paper demonstrates the effectiveness of the simulator for image-based perception by performing sim-to-real rock instance segmentation. The results show that a YOLOv8 model trained on the simulator’s synthetic data achieves performance close to a model trained on real-world data, with a 5% performance gap. When finetuned with real data, the model achieves 14% higher average precision than the model trained on real-world data, demonstrating the simulator’s photorealism.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为开发 lunar 机器人探索算法而写的，需要高度实验环境来评估其算法。</li>
<li>methods: 这篇论文提出了一个基于 Nvidia 的 Isaac Sim 的 lunar 模拟器 OmniLRS，它提供了快速的生成环境、多机器人能力以及机器学习应用程序的数据管道。</li>
<li>results: 论文通过进行 sim-to-real 矿石实例分割来证明其模拟器的效果，结果显示一个 YOLOv8 模型在模拟器上训练的数据上达到与实际数据训练的模型几乎相同的性能，差距仅5%。当再进行 fine-tuning 后，模型与实际数据训练的模型之间的差距提高了14%。这示明了模拟器的真实性。<details>
<summary>Abstract</summary>
Developing algorithms for extra-terrestrial robotic exploration has always been challenging. Along with the complexity associated with these environments, one of the main issues remains the evaluation of said algorithms. With the regained interest in lunar exploration, there is also a demand for quality simulators that will enable the development of lunar robots. % In this paper, we explain how we built a Lunar simulator based on Isaac Sim, Nvidia's robotic simulator. In this paper, we propose Omniverse Lunar Robotic-Sim (OmniLRS) that is a photorealistic Lunar simulator based on Nvidia's robotic simulator. This simulation provides fast procedural environment generation, multi-robot capabilities, along with synthetic data pipeline for machine-learning applications. It comes with ROS1 and ROS2 bindings to control not only the robots, but also the environments. This work also performs sim-to-real rock instance segmentation to show the effectiveness of our simulator for image-based perception. Trained on our synthetic data, a yolov8 model achieves performance close to a model trained on real-world data, with 5% performance gap. When finetuned with real data, the model achieves 14% higher average precision than the model trained on real-world data, demonstrating our simulator's photorealism.% to realize sim-to-real. The code is fully open-source, accessible here: https://github.com/AntoineRichard/LunarSim, and comes with demonstrations.
</details>
<details>
<summary>摘要</summary>
开发外星 robotic 探索算法总是是一个挑战。随着这些环境的复杂性，一个主要的问题是评估这些算法。与月球探索的重新兴起相关，有一个需求是高质量的月球 simulator，可以帮助月球探索机器人的开发。在这篇论文中，我们介绍了我们如何基于 Isaac Sim 和 Nvidia 的机器人 simulator 建立了一个名为 Omniverse Lunar Robotic-Sim（OmniLRS）的月球 simulator。这个 simulate 提供了快速的过程生成环境、多机器人功能以及synthetic data pipeline  для机器学习应用。它还包括 ROS1 和 ROS2 绑定，可以控制不仅机器人，还可以控制环境。此外，我们还实现了 sim-to-real 的岩Instance segmentation，以示我们的 simulate 的实用性。我们在我们的synthetic数据上训练了一个 yolov8 模型，与实际数据训练的模型之间的性能差距只有5%。当 fins 化 With real data 时，模型的性能高于实际数据训练的模型， demonstrating 我们的 simulate 的 photorealism。我们的代码是完全开源的，可以在以下 GitHub 上获取：https://github.com/AntoineRichard/LunarSim，并包括示例。
</details></li>
</ul>
<hr>
<h2 id="RMP-A-Random-Mask-Pretrain-Framework-for-Motion-Prediction"><a href="#RMP-A-Random-Mask-Pretrain-Framework-for-Motion-Prediction" class="headerlink" title="RMP: A Random Mask Pretrain Framework for Motion Prediction"></a>RMP: A Random Mask Pretrain Framework for Motion Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08989">http://arxiv.org/abs/2309.08989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kth-rpl/rmp">https://github.com/kth-rpl/rmp</a></li>
<li>paper_authors: Yi Yang, Qingwen Zhang, Thomas Gilles, Nazre Batool, John Folkesson</li>
<li>for: 这篇论文是针对自驾车中的路径预测问题提出了一个框架。</li>
<li>methods: 本文使用了随机遮盾模型，将物体位置在随机时间步上遮盾，然后由学习的神经网络（NN）填充。可以根据遮盾profile的变化，轻松地切换到不同的动作相关任务。</li>
<li>results: 本文透过评估Argoverse和NuScenes dataset，表明我们的提案的预训框架可以处理噪音输入，提高路径预测精度和缺失率，特别是在时间遮盾下的物体遮盾。<details>
<summary>Abstract</summary>
As the pretraining technique is growing in popularity, little work has been done on pretrained learning-based motion prediction methods in autonomous driving. In this paper, we propose a framework to formalize the pretraining task for trajectory prediction of traffic participants. Within our framework, inspired by the random masked model in natural language processing (NLP) and computer vision (CV), objects' positions at random timesteps are masked and then filled in by the learned neural network (NN). By changing the mask profile, our framework can easily switch among a range of motion-related tasks. We show that our proposed pretraining framework is able to deal with noisy inputs and improves the motion prediction accuracy and miss rate, especially for objects occluded over time by evaluating it on Argoverse and NuScenes datasets.
</details>
<details>
<summary>摘要</summary>
As the pretraining technique becomes more popular, there has been little research on using learning-based motion prediction methods in autonomous driving. In this paper, we propose a framework to formalize the pretraining task for trajectory prediction of traffic participants. Our framework is inspired by the random masked model in natural language processing (NLP) and computer vision (CV), where the positions of objects at random timesteps are masked and then filled in by a learned neural network (NN). By changing the mask profile, our framework can easily switch among a range of motion-related tasks. We show that our proposed pretraining framework can handle noisy inputs and improve motion prediction accuracy and miss rate, especially for objects that are occluded over time, as evaluated on the Argoverse and NuScenes datasets.
</details></li>
</ul>
<hr>
<h2 id="Comparative-study-of-Deep-Learning-Models-for-Binary-Classification-on-Combined-Pulmonary-Chest-X-ray-Dataset"><a href="#Comparative-study-of-Deep-Learning-Models-for-Binary-Classification-on-Combined-Pulmonary-Chest-X-ray-Dataset" class="headerlink" title="Comparative study of Deep Learning Models for Binary Classification on Combined Pulmonary Chest X-ray Dataset"></a>Comparative study of Deep Learning Models for Binary Classification on Combined Pulmonary Chest X-ray Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10829">http://arxiv.org/abs/2309.10829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shabbir Ahmed Shuvo, Md Aminul Islam, Md. Mozammel Hoque, Rejwan Bin Sulaiman</li>
<li>for: 这个研究的目的是比较八种深度学习模型在同一个肺病影像数据集上的二分类表现。</li>
<li>methods: 这个研究使用了 eight 种深度学习模型，包括 DenseNet 121、DenseNet 169、DenseNet 201、EffecientNet b0、EffecientNet lite4、GoogleNet、MobileNet 和 ResNet18。</li>
<li>results: 研究发现，当应用于肺病影像数据集时，DenseNet 169 表现最佳，准确率为 89.38%，MobileNet 表现次之，准确率为 92.2%。<details>
<summary>Abstract</summary>
CNN-based deep learning models for disease detection have become popular recently. We compared the binary classification performance of eight prominent deep learning models: DenseNet 121, DenseNet 169, DenseNet 201, EffecientNet b0, EffecientNet lite4, GoogleNet, MobileNet, and ResNet18 for their binary classification performance on combined Pulmonary Chest Xrays dataset. Despite the widespread application in different fields in medical images, there remains a knowledge gap in determining their relative performance when applied to the same dataset, a gap this study aimed to address. The dataset combined Shenzhen, China (CH) and Montgomery, USA (MC) data. We trained our model for binary classification, calculated different parameters of the mentioned models, and compared them. The models were trained to keep in mind all following the same training parameters to maintain a controlled comparison environment. End of the study, we found a distinct difference in performance among the other models when applied to the pulmonary chest Xray image dataset, where DenseNet169 performed with 89.38 percent and MobileNet with 92.2 percent precision.   Keywords: Pulmonary, Deep Learning, Tuberculosis, Disease detection, Xray
</details>
<details>
<summary>摘要</summary>
Recently, CNN基于深度学习模型在疾病检测中获得了广泛应用。我们对8种知名深度学习模型进行比较：DenseNet 121、DenseNet 169、DenseNet 201、EffecientNet b0、EffecientNet lite4、GoogleNet、MobileNet和ResNet18，以确定它们在同一个 dataset 上的二分类性表现。尽管这些模型在医疗图像领域的不同应用场景中广泛使用，但是在应用于同一个 dataset 上的表现却存在知识空白，这项研究意图填补这个空白。我们使用了combined Shenzhen、China（CH）和Montgomery、USA（MC）的数据集。我们将模型进行二分类训练，计算了不同模型的参数，并进行了比较。所有模型均遵循同样的训练参数，以保证比较环境的控制。研究结束后，我们发现在肺部X射影像数据集上，DenseNet169表现出了89.38%的准确率，而MobileNet表现出了92.2%的准确率。关键词：肺部、深度学习、肺病、疾病检测、X射影像
</details></li>
</ul>
<hr>
<h2 id="FF-LOGO-Cross-Modality-Point-Cloud-Registration-with-Feature-Filtering-and-Local-to-Global-Optimization"><a href="#FF-LOGO-Cross-Modality-Point-Cloud-Registration-with-Feature-Filtering-and-Local-to-Global-Optimization" class="headerlink" title="FF-LOGO: Cross-Modality Point Cloud Registration with Feature Filtering and Local to Global Optimization"></a>FF-LOGO: Cross-Modality Point Cloud Registration with Feature Filtering and Local to Global Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08966">http://arxiv.org/abs/2309.08966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Ma, Mohan Wang, Yiheng Han, Yong-Jin Liu</li>
<li>for: 本文是一篇关于多modal点云注册的研究 paper, aiming to address the challenges of cross-modality point cloud registration.</li>
<li>methods: 本文提出了一种名为 FF-LOGO 的多modal点云注册方法，包括Feature Filtering和本地 global optimization两个模块。Feature Filtering 模块可以抽象出不同感知器的点云特征，并通过特征匹配来进行点云选择。本地 Adaptive Key Region Aggregation 模块和全Modal Consistency Fusion Optimization 模块是为了优化点云注册精度。</li>
<li>results: 实验结果表明，我们的两阶段优化可以显著提高点云注册精度，特征关联和选择模块的准确率从40.59%提高到75.74%。这表明我们的方法可以有效地解决跨模态点云注册中的挑战。<details>
<summary>Abstract</summary>
Cross-modality point cloud registration is confronted with significant challenges due to inherent differences in modalities between different sensors. We propose a cross-modality point cloud registration framework FF-LOGO: a cross-modality point cloud registration method with feature filtering and local-global optimization. The cross-modality feature correlation filtering module extracts geometric transformation-invariant features from cross-modality point clouds and achieves point selection by feature matching. We also introduce a cross-modality optimization process, including a local adaptive key region aggregation module and a global modality consistency fusion optimization module. Experimental results demonstrate that our two-stage optimization significantly improves the registration accuracy of the feature association and selection module. Our method achieves a substantial increase in recall rate compared to the current state-of-the-art methods on the 3DCSR dataset, improving from 40.59% to 75.74%. Our code will be available at https://github.com/wangmohan17/FFLOGO.
</details>
<details>
<summary>摘要</summary>
cross-modality point cloud registration 面临着不同感知器的内生差异问题，我们提出了一种 cross-modality point cloud registration 框架 FF-LOGO：一种通过特征筛选和本地adaptive key region aggregation模块、全Modal consistency fusion优化模块实现的 cross-modality point cloud registration 方法。在Feature correlation filtering模块中，我们提取了不同感知器的Point cloud中的几何变换不变的特征，并通过特征匹配来实现点选择。我们还引入了一种 across-modality optimization process，包括一个本地adaptive key region aggregation模块和一个全Modal consistency fusion优化模块。实验结果表明，我们的两stage优化significantly improves the registration accuracy of the feature association and selection module。我们的方法在3DCSR dataset上实现了75.74%的回卷率提升，比前一个state-of-the-art方法提高了40.59%。我们的代码将在https://github.com/wangmohan17/FFLOGO上公开。
</details></li>
</ul>
<hr>
<h2 id="Tightening-Classification-Boundaries-in-Open-Set-Domain-Adaptation-through-Unknown-Exploitation"><a href="#Tightening-Classification-Boundaries-in-Open-Set-Domain-Adaptation-through-Unknown-Exploitation" class="headerlink" title="Tightening Classification Boundaries in Open Set Domain Adaptation through Unknown Exploitation"></a>Tightening Classification Boundaries in Open Set Domain Adaptation through Unknown Exploitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08964">http://arxiv.org/abs/2309.08964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LucasFernando-aes/UnkE-OVANet">https://github.com/LucasFernando-aes/UnkE-OVANet</a></li>
<li>paper_authors: Lucas Fernando Alvarenga e Silva, Nicu Sebe, Jurandy Almeida</li>
<li>for: 本研究旨在提高Open Set Domain Adaptation (OSDA)方法的性能，以应对具有不同类别和预设变数的非控制环境。</li>
<li>methods: 本研究提出了一种基于高信度Unknown Instance的强制约束，以提高OSDA方法的分类精度。具体来说，我们采用了三种不同的损失函数来训练OSDA模型，包括直接使用静止负项目集，随机扭转负项目集使用数据增强技术，以及将Synthetically生成的负项目集中的反击特征加以训练。</li>
<li>results: 我们在OVANet上进行了广泛的实验，发现在Office-31和Office-Home dataset上，这些方法可以实现绝对改进，获得最大改进为1.3%的精度和5.8%的H-Score在Office-31 dataset上，以及4.7%的精度和5.8%的H-Score在Office-Home dataset上。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have brought revolutionary advances to many research areas due to their capacity of learning from raw data. However, when those methods are applied to non-controllable environments, many different factors can degrade the model's expected performance, such as unlabeled datasets with different levels of domain shift and category shift. Particularly, when both issues occur at the same time, we tackle this challenging setup as Open Set Domain Adaptation (OSDA) problem. In general, existing OSDA approaches focus their efforts only on aligning known classes or, if they already extract possible negative instances, use them as a new category learned with supervision during the course of training. We propose a novel way to improve OSDA approaches by extracting a high-confidence set of unknown instances and using it as a hard constraint to tighten the classification boundaries of OSDA methods. Especially, we adopt a new loss constraint evaluated in three different means, (1) directly with the pristine negative instances; (2) with randomly transformed negatives using data augmentation techniques; and (3) with synthetically generated negatives containing adversarial features. We assessed all approaches in an extensive set of experiments based on OVANet, where we could observe consistent improvements for two public benchmarks, the Office-31 and Office-Home datasets, yielding absolute gains of up to 1.3% for both Accuracy and H-Score on Office-31 and 5.8% for Accuracy and 4.7% for H-Score on Office-Home.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）已经为许多研究领域带来了革命性的进步，因为它们可以从原始数据中学习。然而，当这些方法应用于不可控的环境时，许多不同的因素可以降低模型的预期性能，如不标注数据集中的不同水平域转移和类别转移。特别是当这两个问题同时出现时，我们称之为开放集领域适应（OSDA）问题。总的来说，现有的OSDA方法通常只关注知道的类别的Alignment，或者如果已经提取了可能的负样本，则在训练过程中使用它们作为新学习的类别。我们提出了一种新的方法来改进OSDA方法，通过提取高 confidence 的未知实例集并将其作为硬件约束使用，以紧张化OSDA方法的分类边界。特别是，我们采用了三种不同的损失约束，分别是：(1) 直接使用不损失的负样本；(2) 使用数据扩展技术生成的随机变换负样本；以及(3) 使用生成的负样本，含有抗击性特征。我们在一系列实验中评估了所有方法，基于 OVANet，可以看到在 Office-31 和 Office-Home 两个公共 benchmark 上，我们可以获得相对准确率和 H-Score 的绝对提升，最高达 1.3% 和 5.8%。
</details></li>
</ul>
<hr>
<h2 id="ExBluRF-Efficient-Radiance-Fields-for-Extreme-Motion-Blurred-Images"><a href="#ExBluRF-Efficient-Radiance-Fields-for-Extreme-Motion-Blurred-Images" class="headerlink" title="ExBluRF: Efficient Radiance Fields for Extreme Motion Blurred Images"></a>ExBluRF: Efficient Radiance Fields for Extreme Motion Blurred Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08957">http://arxiv.org/abs/2309.08957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongwoo Lee, Jeongtaek Oh, Jaesung Lim, Sunghyun Cho, Kyoung Mu Lee</li>
<li>for: 本研究旨在提出一种基于高效光场优化的新视觉合成方法，用于处理极大运动模糊图像。</li>
<li>methods: 本方法包括两个主要组件：6DOF摄像机轨迹基于运动模糊形式和 voxel-based 光场。从极其模糊图像中，我们优化锐化的光场，并通过相互关联摄像机轨迹来生成极大运动模糊图像。在训练中，我们将多个射线与摄像机轨迹相集成，以重建单个模糊颜色图像，这与物理运动模糊操作相同。我们在模糊图像空间内减少照度一致损失，并通过摄像机轨迹来获得锐化的光场。</li>
<li>results: 与现有方法相比，我们的方法可以很好地还原极大运动模糊视图中的3D场景，并且具有训练时间和GPU内存占用的减少。<details>
<summary>Abstract</summary>
We present ExBluRF, a novel view synthesis method for extreme motion blurred images based on efficient radiance fields optimization. Our approach consists of two main components: 6-DOF camera trajectory-based motion blur formulation and voxel-based radiance fields. From extremely blurred images, we optimize the sharp radiance fields by jointly estimating the camera trajectories that generate the blurry images. In training, multiple rays along the camera trajectory are accumulated to reconstruct single blurry color, which is equivalent to the physical motion blur operation. We minimize the photo-consistency loss on blurred image space and obtain the sharp radiance fields with camera trajectories that explain the blur of all images. The joint optimization on the blurred image space demands painfully increasing computation and resources proportional to the blur size. Our method solves this problem by replacing the MLP-based framework to low-dimensional 6-DOF camera poses and voxel-based radiance fields. Compared with the existing works, our approach restores much sharper 3D scenes from challenging motion blurred views with the order of 10 times less training time and GPU memory consumption.
</details>
<details>
<summary>摘要</summary>
我们提出了ExBluRF，一种新的视图合成方法，用于处理极大运动模糊图像。我们的方法包括两个主要组成部分：基于6DOF相机轨迹的运动模糊形式和 voxel-based 辐射场。从极端模糊图像中，我们优化了锐利的辐射场，并同时估计相机轨迹，以生成模糊图像。在训练中，多个光束与相机轨迹相互垂直，以重建单个模糊颜色。我们在模糊图像空间中减少了照相一致损失，并从相机轨迹中获得了锐利的辐射场。我们的方法解决了训练时间和 GPU 内存占用的问题，相比之下，现有的方法可以在极大的运动模糊视图中还原许多更加锐利的3D场景，并且减少了训练时间和 GPU 内存占用的规模。
</details></li>
</ul>
<hr>
<h2 id="IntelliBeeHive-An-Automated-Honey-Bee-Pollen-and-Varroa-Destructor-Monitoring-System"><a href="#IntelliBeeHive-An-Automated-Honey-Bee-Pollen-and-Varroa-Destructor-Monitoring-System" class="headerlink" title="IntelliBeeHive: An Automated Honey Bee, Pollen, and Varroa Destructor Monitoring System"></a>IntelliBeeHive: An Automated Honey Bee, Pollen, and Varroa Destructor Monitoring System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08955">http://arxiv.org/abs/2309.08955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian I. Narcia-Macias, Joselito Guardado, Jocell Rodriguez, Joanne Rampersad-Ammons, Erik Enriquez, Dong-Chul Kim<br>for: 这个研究旨在提高我们对蜜蜂群体疾病、蜜蜂行为、人口减少和巢健康的理解，通过发展一个基于计算机视觉的蜜蜂监测系统。methods: 这个监测系统使用计算机视觉技术，包括机器学习算法，实时监测蜜蜂群体活动和健康状况，无需对蜜蜂进行干扰。results: 我们的监测系统可以准确地识别蜜蜂、识别蜜和检测 Varroa 螨，并且可以实时提供蜜蜂群体活动和健康状况的数据。系统的总性能达到 96.28%，蜜蜂模型的 F1 分数为 0.95，蜜模型的 F1 分数为 0.831。<details>
<summary>Abstract</summary>
Utilizing computer vision and the latest technological advancements, in this study, we developed a honey bee monitoring system that aims to enhance our understanding of Colony Collapse Disorder, honey bee behavior, population decline, and overall hive health. The system is positioned at the hive entrance providing real-time data, enabling beekeepers to closely monitor the hive's activity and health through an account-based website. Using machine learning, our monitoring system can accurately track honey bees, monitor pollen-gathering activity, and detect Varroa mites, all without causing any disruption to the honey bees. Moreover, we have ensured that the development of this monitoring system utilizes cost-effective technology, making it accessible to apiaries of various scales, including hobbyists, commercial beekeeping businesses, and researchers. The inference models used to detect honey bees, pollen, and mites are based on the YOLOv7-tiny architecture trained with our own data. The F1-score for honey bee model recognition is 0.95 and the precision and recall value is 0.981. For our pollen and mite object detection model F1-score is 0.95 and the precision and recall value is 0.821 for pollen and 0.996 for "mite". The overall performance of our IntelliBeeHive system demonstrates its effectiveness in monitoring the honey bee's activity, achieving an accuracy of 96.28 % in tracking and our pollen model achieved a F1-score of 0.831.
</details>
<details>
<summary>摘要</summary>
使用计算机视觉和最新的技术进步，在这项研究中，我们开发了一套蜜蜂监测系统，以提高我们对蜂巢病毒蜂巢病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒病毒��
</details></li>
</ul>
<hr>
<h2 id="Robust-Backdoor-Attacks-on-Object-Detection-in-Real-World"><a href="#Robust-Backdoor-Attacks-on-Object-Detection-in-Real-World" class="headerlink" title="Robust Backdoor Attacks on Object Detection in Real World"></a>Robust Backdoor Attacks on Object Detection in Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08953">http://arxiv.org/abs/2309.08953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaguan Qian, Boyuan Ji, Shuke He, Shenhui Huang, Xiang Ling, Bin Wang, Wei Wang</li>
<li>for: 这个论文的目的是提出一种适应不同大小攻击对象的变量大小触发器，以及一种基于恶意对抗训练的后门训练方法，以提高在实际世界中的攻击成功率。</li>
<li>methods: 该论文使用的方法包括变量大小触发器和基于恶意对抗训练的后门训练方法，以适应不同大小攻击对象和物理干扰。</li>
<li>results: 实验结果显示，这种 Robust Backdoor Attack (RBA) 可以在实际世界中提高攻击成功率。<details>
<summary>Abstract</summary>
Deep learning models are widely deployed in many applications, such as object detection in various security fields. However, these models are vulnerable to backdoor attacks. Most backdoor attacks were intensively studied on classified models, but little on object detection. Previous works mainly focused on the backdoor attack in the digital world, but neglect the real world. Especially, the backdoor attack's effect in the real world will be easily influenced by physical factors like distance and illumination. In this paper, we proposed a variable-size backdoor trigger to adapt to the different sizes of attacked objects, overcoming the disturbance caused by the distance between the viewing point and attacked object. In addition, we proposed a backdoor training named malicious adversarial training, enabling the backdoor object detector to learn the feature of the trigger with physical noise. The experiment results show this robust backdoor attack (RBA) could enhance the attack success rate in the real world.
</details>
<details>
<summary>摘要</summary>
深度学习模型在多个应用领域广泛应用，如物体检测等安全领域。然而，这些模型容易受到后门攻击。大多数后门攻击研究targeted于分类模型，而对物体检测模型的研究很少。先前的工作主要集中在数字世界中进行后门攻击研究，忽略了实际世界。特别是，实际世界中后门攻击的效果会受到物体距离观察点以及照明等物理因素的影响。在这篇论文中，我们提出了可变大小的后门触发器，以适应不同大小的攻击对象，并且在不同距离和照明条件下对后门触发器进行了适应。此外，我们还提出了一种名为“邪恶学习训练”的后门训练方法，使得后门检测器能够学习触发器的特征与物理噪声。实验结果显示，我们的robust后门攻击（RBA）可以在实际世界中提高攻击成功率。
</details></li>
</ul>
<hr>
<h2 id="Staged-Contact-Aware-Global-Human-Motion-Forecasting"><a href="#Staged-Contact-Aware-Global-Human-Motion-Forecasting" class="headerlink" title="Staged Contact-Aware Global Human Motion Forecasting"></a>Staged Contact-Aware Global Human Motion Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08947">http://arxiv.org/abs/2309.08947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/l-scofano/stag">https://github.com/l-scofano/stag</a></li>
<li>paper_authors: Luca Scofano, Alessio Sampieri, Elisabeth Schiele, Edoardo De Matteis, Laura Leal-Taixé, Fabio Galasso<br>for:Scene-aware global human motion forecasting is critical for manifold applications, including virtual reality, robotics, and sports. The task combines human trajectory and pose forecasting within the provided scene context, which represents a significant challenge.methods:We propose a STAGed contact-aware global human motion forecasting STAG, a novel three-stage pipeline for predicting global human motion in a 3D environment. We first consider the scene and the respective human interaction as contact points. Secondly, we model the human trajectory forecasting within the scene, predicting the coarse motion of the human body as a whole. The third and last stage matches a plausible fine human joint motion to complement the trajectory considering the estimated contacts.results:Compared to the state-of-the-art (SoA), STAG achieves a 1.8% and 16.2% overall improvement in pose and trajectory prediction, respectively, on the scene-aware GTA-IM dataset. A comprehensive ablation study confirms the advantages of staged modeling over end-to-end approaches.<details>
<summary>Abstract</summary>
Scene-aware global human motion forecasting is critical for manifold applications, including virtual reality, robotics, and sports. The task combines human trajectory and pose forecasting within the provided scene context, which represents a significant challenge.   So far, only Mao et al. NeurIPS'22 have addressed scene-aware global motion, cascading the prediction of future scene contact points and the global motion estimation. They perform the latter as the end-to-end forecasting of future trajectories and poses. However, end-to-end contrasts with the coarse-to-fine nature of the task and it results in lower performance, as we demonstrate here empirically.   We propose a STAGed contact-aware global human motion forecasting STAG, a novel three-stage pipeline for predicting global human motion in a 3D environment. We first consider the scene and the respective human interaction as contact points. Secondly, we model the human trajectory forecasting within the scene, predicting the coarse motion of the human body as a whole. The third and last stage matches a plausible fine human joint motion to complement the trajectory considering the estimated contacts.   Compared to the state-of-the-art (SoA), STAG achieves a 1.8% and 16.2% overall improvement in pose and trajectory prediction, respectively, on the scene-aware GTA-IM dataset. A comprehensive ablation study confirms the advantages of staged modeling over end-to-end approaches. Furthermore, we establish the significance of a newly proposed temporal counter called the "time-to-go", which tells how long it is before reaching scene contact and endpoints. Notably, STAG showcases its ability to generalize to datasets lacking a scene and achieves a new state-of-the-art performance on CMU-Mocap, without leveraging any social cues. Our code is released at: https://github.com/L-Scofano/STAG
</details>
<details>
<summary>摘要</summary>
Scene-aware global human motion forecasting是应用广泛的领域之一，包括虚拟现实、 робо测控和体育等。这个任务需要在提供的场景上下文中预测人体轨迹和姿势，这是一项非常具有挑战性的任务。  到目前为止，只有Mao等人在NeuIPS'22中Addressed scene-aware global motion，通过综合预测未来场景接触点和全身运动的方式来完成。他们在预测未来轨迹和姿势方面进行了终端预测，但终端预测与场景中的运动预测不匹配，这会导致性能下降，我们在实验中证明了这一点。  我们提出了一种名为STAG的三个阶段管道，用于预测在3D环境中的全身人类运动。我们首先考虑场景和人类之间的接触点，然后预测场景内人体轨迹，并且预测全身人体的粗略运动。最后一个阶段是匹配合理的人体 JOINT 运动，以补做轨迹中的细微运动。  与现状的最佳实践（SoA）相比，STAG在Scene-aware GTA-IM dataset上表现出1.8%和16.2%的总体改进，分别是 pose 和轨迹预测。我们还进行了一项完整的减少研究，证明了分阶段模型的优势。此外，我们还证明了我们提出的一种新的时间对象“时间剩下”的重要性，该对象表示人体在场景接触点和终点之前剩下的时间。值得注意的是，STAG在缺少场景的 dataset 上表现出新的状态机制，并在CMU-Mocap dataset上 дости得了新的最佳实践，无需使用任何社会cue。我们的代码可以在https://github.com/L-Scofano/STAG 上获取。
</details></li>
</ul>
<hr>
<h2 id="AffordPose-A-Large-scale-Dataset-of-Hand-Object-Interactions-with-Affordance-driven-Hand-Pose"><a href="#AffordPose-A-Large-scale-Dataset-of-Hand-Object-Interactions-with-Affordance-driven-Hand-Pose" class="headerlink" title="AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven Hand Pose"></a>AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven Hand Pose</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08942">http://arxiv.org/abs/2309.08942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gentlesjan/affordpose">https://github.com/gentlesjan/affordpose</a></li>
<li>paper_authors: Juntao Jian, Xiuping Liu, Manyi Li, Ruizhen Hu, Jian Liu</li>
<li>for: 这篇论文主要针对的是人机交互中对象的函数角色如何影响人的手势，以及如何通过大量人类示例来学习和理解合适的手势交互。</li>
<li>methods: 该论文提出了AffordPose数据集，包括26.7K个手势交互示例，每个示例包括3D对象形状、部分可行性标签和手势位置。</li>
<li>results: 数据分析表明，手势交互中的具体手势pose受到对象的可行性影响，同时也展现了一定的多样性。实验表明，AffordPose数据集可以有效地学习和理解细腻的手势交互。<details>
<summary>Abstract</summary>
How human interact with objects depends on the functional roles of the target objects, which introduces the problem of affordance-aware hand-object interaction. It requires a large number of human demonstrations for the learning and understanding of plausible and appropriate hand-object interactions. In this work, we present AffordPose, a large-scale dataset of hand-object interactions with affordance-driven hand pose. We first annotate the specific part-level affordance labels for each object, e.g. twist, pull, handle-grasp, etc, instead of the general intents such as use or handover, to indicate the purpose and guide the localization of the hand-object interactions. The fine-grained hand-object interactions reveal the influence of hand-centered affordances on the detailed arrangement of the hand poses, yet also exhibit a certain degree of diversity. We collect a total of 26.7K hand-object interactions, each including the 3D object shape, the part-level affordance label, and the manually adjusted hand poses. The comprehensive data analysis shows the common characteristics and diversity of hand-object interactions per affordance via the parameter statistics and contacting computation. We also conduct experiments on the tasks of hand-object affordance understanding and affordance-oriented hand-object interaction generation, to validate the effectiveness of our dataset in learning the fine-grained hand-object interactions. Project page: https://github.com/GentlesJan/AffordPose.
</details>
<details>
<summary>摘要</summary>
人类与物体之间的互动取决于目标物体的功能角色，这引入了有关手持物体互动的具体性和适应性的问题。为了解决这个问题，需要大量的人类示例来学习和理解合适的手持物体互动。在这个工作中，我们提出了AffordPose数据集，包括手持物体互动中的具体部分可行性标签，如拧、拧、握持等，而不是通用的用途或交给意图。这些细腻的手持物体互动显示了手中心可行性对手姿的细部安排产生了影响，同时也表现出一定的多样性。我们收集了26700个手持物体互动样本，每个样本包括3D物体形状、部分可行性标签和手姿调整。经过全面的数据分析，我们发现每种可行性下的手持物体互动具有共同特征和多样性，并通过参数统计和接触计算 validate our dataset的有效性。我们还进行了手持物体可行性理解和手持物体互动生成任务的实验，以验证我们的数据集在学习细腻手持物体互动方面的效果。项目页面：https://github.com/GentlesJan/AffordPose。
</details></li>
</ul>
<hr>
<h2 id="Semantics-aware-LiDAR-Only-Pseudo-Point-Cloud-Generation-for-3D-Object-Detection"><a href="#Semantics-aware-LiDAR-Only-Pseudo-Point-Cloud-Generation-for-3D-Object-Detection" class="headerlink" title="Semantics-aware LiDAR-Only Pseudo Point Cloud Generation for 3D Object Detection"></a>Semantics-aware LiDAR-Only Pseudo Point Cloud Generation for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08932">http://arxiv.org/abs/2309.08932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiago Cortinhal, Idriss Gouigah, Eren Erdal Aksoy</li>
<li>for: 提高自动驾驶系统中LiDAR感知器的精度和精细度，使其能够更好地检测远距离的细节物体。</li>
<li>methods: 利用LiDAR感知器alone，通过提取场景 semantics并使用多Modal domain translator生成增强的Synthetic dense point clouds，提高3D对象检测性能。</li>
<li>results: 在不同的高级3D对象检测方法上实现了最多2.9%的性能提升，并在KITTI 3D对象检测数据集上达到了与其他状态体LiDAR-only探测器相当的性能。<details>
<summary>Abstract</summary>
Although LiDAR sensors are crucial for autonomous systems due to providing precise depth information, they struggle with capturing fine object details, especially at a distance, due to sparse and non-uniform data. Recent advances introduced pseudo-LiDAR, i.e., synthetic dense point clouds, using additional modalities such as cameras to enhance 3D object detection. We present a novel LiDAR-only framework that augments raw scans with denser pseudo point clouds by solely relying on LiDAR sensors and scene semantics, omitting the need for cameras. Our framework first utilizes a segmentation model to extract scene semantics from raw point clouds, and then employs a multi-modal domain translator to generate synthetic image segments and depth cues without real cameras. This yields a dense pseudo point cloud enriched with semantic information. We also introduce a new semantically guided projection method, which enhances detection performance by retaining only relevant pseudo points. We applied our framework to different advanced 3D object detection methods and reported up to 2.9% performance upgrade. We also obtained comparable results on the KITTI 3D object detection dataset, in contrast to other state-of-the-art LiDAR-only detectors.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管LiDAR传感器对自动驾驶系统非常重要，因为它们提供精确的深度信息，但它们在距离较远的场景中很难捕捉细节，主要是因为LiDAR数据 sparse和不均匀。最近，人们提出了 Pseudo-LiDAR，即使用其他感知模式，如摄像头，来增强3D物体检测。我们提出了一种完全依靠LiDAR感知器和场景 semantics的 LiDAR-only 框架，可以增强 raw 扫描数据的精度。我们的框架首先使用一种分割模型提取场景 semantics，然后使用一种多模态领域翻译器生成无需真正摄像头的Synthetic 图像段和深度cue。这将生成一个充满 semantic 信息的 dense pseudo point cloud。我们还引入了一种受 semantics 引导的投影方法，可以提高检测性能。我们将我们的框架应用到不同的高级3D物体检测方法上，并reported 提高性能达2.9%。我们还在 KITTI 3D 物体检测数据集上获得了与其他状态 искусternal LiDAR-only 检测器相比的相似结果。
</details></li>
</ul>
<hr>
<h2 id="In-Style-Bridging-Text-and-Uncurated-Videos-with-Style-Transfer-for-Text-Video-Retrieval"><a href="#In-Style-Bridging-Text-and-Uncurated-Videos-with-Style-Transfer-for-Text-Video-Retrieval" class="headerlink" title="In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval"></a>In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08928">http://arxiv.org/abs/2309.08928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ninatu/in_style">https://github.com/ninatu/in_style</a></li>
<li>paper_authors: Nina Shvetsova, Anna Kukleva, Bernt Schiele, Hilde Kuehne</li>
<li>for: 文章目的是提出一种新的文本视频检索任务 setting，即使没有手动标注的对应文本视频数据。</li>
<li>methods: 提出了一种名为 In-Style 的方法，可以学习文本查询的风格并将其传递给网络视频。此外，我们还提出了一种多种风格冲突训练方法，以提高模型的通用性。</li>
<li>results: 我们通过对多个数据集进行测试，证明了我们的风格传递框架可以在无需手动标注对应文本视频数据的情况下提高文本视频检索任务的性能，并超越了零shot文本视频检索任务的州OF-THE-ART表现。<details>
<summary>Abstract</summary>
Large-scale noisy web image-text datasets have been proven to be efficient for learning robust vision-language models. However, when transferring them to the task of video retrieval, models still need to be fine-tuned on hand-curated paired text-video data to adapt to the diverse styles of video descriptions. To address this problem without the need for hand-annotated pairs, we propose a new setting, text-video retrieval with uncurated & unpaired data, that during training utilizes only text queries together with uncurated web videos without any paired text-video data. To this end, we propose an approach, In-Style, that learns the style of the text queries and transfers it to uncurated web videos. Moreover, to improve generalization, we show that one model can be trained with multiple text styles. To this end, we introduce a multi-style contrastive training procedure that improves the generalizability over several datasets simultaneously. We evaluate our model on retrieval performance over multiple datasets to demonstrate the advantages of our style transfer framework on the new task of uncurated & unpaired text-video retrieval and improve state-of-the-art performance on zero-shot text-video retrieval.
</details>
<details>
<summary>摘要</summary>
大规模嘈吵网络图片文本数据集已经证明可以有效地学习Robust vision-language模型。然而，在将其转移到视频检索任务时，模型仍需要通过手动批注的文本视频数据进行微调以适应不同的视频描述风格。为解决这个问题而不需要手动批注对，我们提出了一个新的设定：文本视频检索无curated & unpaired数据。在训练时，我们只使用文本查询，并使用无curated的网络视频，没有任何文本视频批注数据。为此，我们提出了一种方法，In-Style，它可以学习文本查询的风格，并将其传递给无curated的网络视频。此外，为提高泛化性，我们表明一个模型可以通过多种文本风格进行训练。为此，我们引入了多种风格强制对照训练程序，以提高模型的泛化性。我们对多个数据集进行了检索性能评估，以证明我们的风格传递框架在新任务中的优势，并超越了零shot文本视频检索的州场性能。
</details></li>
</ul>
<hr>
<h2 id="DynaMoN-Motion-Aware-Fast-And-Robust-Camera-Localization-for-Dynamic-NeRF"><a href="#DynaMoN-Motion-Aware-Fast-And-Robust-Camera-Localization-for-Dynamic-NeRF" class="headerlink" title="DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF"></a>DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08927">http://arxiv.org/abs/2309.08927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Asim Karaoglu, Hannah Schieber, Nicolas Schischka, Melih Görgülü, Florian Grötzner, Alexander Ladikos, Daniel Roth, Nassir Navab, Benjamin Busam</li>
<li>for: 该论文旨在提出一种基于同时定位和地图建模（SLAM）的动态重建方法，以便更好地处理动态场景中的相机位置和场景内容的变化。</li>
<li>methods: 该方法使用了同时定位和地图建模（SLAM）和动态掩码（motion masking）结合，以提高动态重建的精度和效率。</li>
<li>results: 广泛的实验 validate了该方法在真实世界 dataset 上的优势，包括 TUM RGB-D、BONN RGB-D 动态和 DyCheck 的 iPhone 数据集。该方法不仅提高了相机pose估计的精度，还提高了动态重建的质量。<details>
<summary>Abstract</summary>
Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera poses. These are often hard to retrieve with existing structure-from-motion (SfM) pipelines as both camera and scene content can change. We propose DynaMoN that leverages simultaneous localization and mapping (SLAM) jointly with motion masking to handle dynamic scene content. Our robust SLAM-based tracking module significantly accelerates the training process of the dynamic NeRF while improving the quality of synthesized views at the same time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's iPhone dataset, three real-world datasets, shows the advantages of DynaMoN both for camera pose estimation and novel view synthesis.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用神经辐射场 (NeRF) 进行动态重建需要准确的摄像头位置。现有的结构从动作 (SfM) 管道通常Difficult to retrieve this information, as both the camera and scene content can change. We propose DynaMoN, which leverages simultaneous localization and mapping (SLAM) and motion masking to handle dynamic scene content. Our robust SLAM-based tracking module significantly accelerates the training process of the dynamic NeRF while improving the quality of synthesized views at the same time. Experimental validation on TUM RGB-D, BONN RGB-D Dynamic, and the DyCheck's iPhone dataset, three real-world datasets, shows the advantages of DynaMoN for both camera pose estimation and novel view synthesis.</SYS>>Here's the text with some notes on the translation:* "动态重建" (dòngtài chóngběn) is used to refer to the process of reconstructing a 3D scene from a set of 2D images.* "神经辐射场" (jīngxīn chángshì chǎng) is a neural network-based method for reconstructing 3D scenes from 2D images.* "摄像头位置" (shèxiàngtou weiità) refers to the position of the camera that took the images.* "结构从动作" (jiégòng cóng dòngwù) is a computer vision technique used to reconstruct 3D scenes from 2D images.* "同时地图" (tóngshí dìmào) is a technique used to create a map of an environment while simultaneously localizing a device within that environment.* "动态Scene" (dòngtài scēn) refers to a scene that is changing or dynamic.* "视图合成" (wèi vision hébìng) is the process of generating new views of a scene from a set of existing views.I hope this helps! Let me know if you have any further questions.
</details></li>
</ul>
<hr>
<h2 id="Pixel-Adapter-A-Graph-Based-Post-Processing-Approach-for-Scene-Text-Image-Super-Resolution"><a href="#Pixel-Adapter-A-Graph-Based-Post-Processing-Approach-for-Scene-Text-Image-Super-Resolution" class="headerlink" title="Pixel Adapter: A Graph-Based Post-Processing Approach for Scene Text Image Super-Resolution"></a>Pixel Adapter: A Graph-Based Post-Processing Approach for Scene Text Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08919">http://arxiv.org/abs/2309.08919</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenyu1009/rtsrn">https://github.com/wenyu1009/rtsrn</a></li>
<li>paper_authors: Wenyu Zhang, Xin Deng, Baojun Jia, Xingtong Yu, Yifan Chen, jin Ma, Qing Ding, Xinming Zhang</li>
<li>For: 提高文本图像超分辨率图像生成的精度和效率。* Methods: 提出Pixel Adapter Module (PAM)基于图注意力来解决升采样导致的像素扭曲问题，并引入MLP-based Sequential Residual Block (MSRB)来提取文本图像的稳定特征。* Results: 在TextZoom上进行了广泛的实验，并取得了高质量的超分辨率图像生成结果，比现有方法提高了认知率。对单stage和多stage策略进行了改进，分别提高了0.7%和2.6%，从52.6%和53.7%提高到53.3%和56.3%。<details>
<summary>Abstract</summary>
Current Scene text image super-resolution approaches primarily focus on extracting robust features, acquiring text information, and complex training strategies to generate super-resolution images. However, the upsampling module, which is crucial in the process of converting low-resolution images to high-resolution ones, has received little attention in existing works. To address this issue, we propose the Pixel Adapter Module (PAM) based on graph attention to address pixel distortion caused by upsampling. The PAM effectively captures local structural information by allowing each pixel to interact with its neighbors and update features. Unlike previous graph attention mechanisms, our approach achieves 2-3 orders of magnitude improvement in efficiency and memory utilization by eliminating the dependency on sparse adjacency matrices and introducing a sliding window approach for efficient parallel computation. Additionally, we introduce the MLP-based Sequential Residual Block (MSRB) for robust feature extraction from text images, and a Local Contour Awareness loss ($\mathcal{L}_{lca}$) to enhance the model's perception of details. Comprehensive experiments on TextZoom demonstrate that our proposed method generates high-quality super-resolution images, surpassing existing methods in recognition accuracy. For single-stage and multi-stage strategies, we achieved improvements of 0.7\% and 2.6\%, respectively, increasing the performance from 52.6\% and 53.7\% to 53.3\% and 56.3\%. The code is available at https://github.com/wenyu1009/RTSRN.
</details>
<details>
<summary>摘要</summary>
当前场景文本图像超解像方法主要集中于提取稳定特征、获取文本信息和复杂的训练策略，以生成超解像图像。然而，upsampling模块，该模块在将低分辨率图像转换为高分辨率图像的过程中扮演关键角色，在现有工作中受到了少量关注。为了解决这个问题，我们提议使用图像注意力机制（PAM）来处理像素扭曲。PAM可以有效地捕捉本地结构信息，让每个像素与邻居像素进行互动，更新特征。与过去的图像注意力机制不同，我们的方法可以在效率和内存利用上实现2-3个数量级的提升，而不需要针对稀疏相邻矩阵的依赖。此外，我们还引入了多层感知核（MLP）基于的Sequential Residual Block（MSRB），用于robust特征提取从文本图像中，以及Local Contour Awareness损失（$\mathcal{L}_{lca}$），以提高模型对细节的感知。在TextZoom上进行了广泛的实验，我们的提议方法可以生成高质量的超解像图像，超过现有方法的认知率。对单阶段和多阶段策略，我们实现了提升0.7%和2.6%，从52.6%和53.7%提升到53.3%和56.3%。代码可以在https://github.com/wenyu1009/RTSRN中获取。
</details></li>
</ul>
<hr>
<h2 id="Delving-into-Multimodal-Prompting-for-Fine-grained-Visual-Classification"><a href="#Delving-into-Multimodal-Prompting-for-Fine-grained-Visual-Classification" class="headerlink" title="Delving into Multimodal Prompting for Fine-grained Visual Classification"></a>Delving into Multimodal Prompting for Fine-grained Visual Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08912">http://arxiv.org/abs/2309.08912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Jiang, Hao Tang, Junyao Gao, Xiaoyu Du, Shengfeng He, Zechao Li</li>
<li>for: 这个研究旨在应用多modal描述来解决细分类的挑战，特别是在细分类中存在轻微的间隔和大量内分量的情况下。</li>
<li>methods: 这个研究使用了对称语言图像模型（CLIP），并提出了一个新的多modal诱导解决方案（MP-FGVC），包括多modal诱导方案和多modal适应方案。多modal诱导方案包括特定分类的视觉推问（SsVP）和差异感知文本推问（DaTP），它们强调了特定分类的视觉和语言差异。多modal适应方案将视觉和文本推问元素调节到共同semantic空间，以便跨modal协力推理，并通过视觉语言融合模组（VLFM）进一步提高FGVC。</li>
<li>results: 实验结果显示MP-FGVC在四个FGVC数据集上具有优秀的效果。<details>
<summary>Abstract</summary>
Fine-grained visual classification (FGVC) involves categorizing fine subdivisions within a broader category, which poses challenges due to subtle inter-class discrepancies and large intra-class variations. However, prevailing approaches primarily focus on uni-modal visual concepts. Recent advancements in pre-trained vision-language models have demonstrated remarkable performance in various high-level vision tasks, yet the applicability of such models to FGVC tasks remains uncertain. In this paper, we aim to fully exploit the capabilities of cross-modal description to tackle FGVC tasks and propose a novel multimodal prompting solution, denoted as MP-FGVC, based on the contrastive language-image pertaining (CLIP) model. Our MP-FGVC comprises a multimodal prompts scheme and a multimodal adaptation scheme. The former includes Subcategory-specific Vision Prompt (SsVP) and Discrepancy-aware Text Prompt (DaTP), which explicitly highlights the subcategory-specific discrepancies from the perspectives of both vision and language. The latter aligns the vision and text prompting elements in a common semantic space, facilitating cross-modal collaborative reasoning through a Vision-Language Fusion Module (VLFM) for further improvement on FGVC. Moreover, we tailor a two-stage optimization strategy for MP-FGVC to fully leverage the pre-trained CLIP model and expedite efficient adaptation for FGVC. Extensive experiments conducted on four FGVC datasets demonstrate the effectiveness of our MP-FGVC.
</details>
<details>
<summary>摘要</summary>
低级分类视觉（FGVC）涉及到细分类划分，这会带来细腻的间隔差异和大量内类变化。然而，现有的方法主要集中于单模visual概念。 recent advances in pre-trained vision-language models have shown remarkable performance in various high-level vision tasks, but the applicability of such models to FGVC tasks remains uncertain. In this paper, we aim to fully exploit the capabilities of cross-modal description to tackle FGVC tasks and propose a novel multimodal prompting solution, denoted as MP-FGVC, based on the contrastive language-image pertaining (CLIP) model. Our MP-FGVC consists of a multimodal prompts scheme and a multimodal adaptation scheme. The former includes Subcategory-specific Vision Prompt (SsVP) and Discrepancy-aware Text Prompt (DaTP), which explicitly highlights the subcategory-specific discrepancies from the perspectives of both vision and language. The latter aligns the vision and text prompting elements in a common semantic space, facilitating cross-modal collaborative reasoning through a Vision-Language Fusion Module (VLFM) for further improvement on FGVC. Moreover, we tailor a two-stage optimization strategy for MP-FGVC to fully leverage the pre-trained CLIP model and expedite efficient adaptation for FGVC. Extensive experiments conducted on four FGVC datasets demonstrate the effectiveness of our MP-FGVC.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Visual-Perception-in-Novel-Environments-via-Incremental-Data-Augmentation-Based-on-Style-Transfer"><a href="#Enhancing-Visual-Perception-in-Novel-Environments-via-Incremental-Data-Augmentation-Based-on-Style-Transfer" class="headerlink" title="Enhancing Visual Perception in Novel Environments via Incremental Data Augmentation Based on Style Transfer"></a>Enhancing Visual Perception in Novel Environments via Incremental Data Augmentation Based on Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08851">http://arxiv.org/abs/2309.08851</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhibha1807/robustifying_visual_perception">https://github.com/abhibha1807/robustifying_visual_perception</a></li>
<li>paper_authors: Abhibha Gupta, Rully Agus Hendrawan, Mansur Arief</li>
<li>for: 强化自适应代理在真实世界场景中的部署，应对“未知未知”（ novell 和不期望的环境）。</li>
<li>methods: 使用 Variational Prototyping Encoder (VPE) 实现可靠地识别和处理新Input，然后逐步增强数据使用神经抽象转移来增强受测数据。</li>
<li>results: 比较仅从原始数据训练的模型和从原始和增强数据训练的模型，发现增强数据训练对模型性能有着明显的改善，这证明了数据增强的重要性。<details>
<summary>Abstract</summary>
The deployment of autonomous agents in real-world scenarios is challenged by "unknown unknowns", i.e. novel unexpected environments not encountered during training, such as degraded signs. While existing research focuses on anomaly detection and class imbalance, it often fails to address truly novel scenarios. Our approach enhances visual perception by leveraging the Variational Prototyping Encoder (VPE) to adeptly identify and handle novel inputs, then incrementally augmenting data using neural style transfer to enrich underrepresented data. By comparing models trained solely on original datasets with those trained on a combination of original and augmented datasets, we observed a notable improvement in the performance of the latter. This underscores the critical role of data augmentation in enhancing model robustness. Our findings suggest the potential benefits of incorporating generative models for domain-specific augmentation strategies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>实际场景中自动化代理的部署受到“未知未知”的挑战，即在训练中没有遇到的新不期望环境，如受损标志。现有研究主要关注异常检测和类别偏度，但经常无法处理真正新的场景。我们的方法通过利用变量抽象编码器（VPE）来有效地识别和处理新输入，然后逐步增强数据使用神经风格传输来润色不足表示。我们比较了固定在原始数据集上训练的模型和将原始数据集和增强数据集组合训练的模型，发现后者表现更佳。这表明数据增强对模型Robustness具有重要作用。我们的发现表明可能将生成模型integrated into domain-specific augmentation strategies中。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="MA-SAM-Modality-agnostic-SAM-Adaptation-for-3D-Medical-Image-Segmentation"><a href="#MA-SAM-Modality-agnostic-SAM-Adaptation-for-3D-Medical-Image-Segmentation" class="headerlink" title="MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation"></a>MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08842">http://arxiv.org/abs/2309.08842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cchen-cc/ma-sam">https://github.com/cchen-cc/ma-sam</a></li>
<li>paper_authors: Cheng Chen, Juzheng Miao, Dufan Wu, Zhiling Yan, Sekeun Kim, Jiang Hu, Aoxiao Zhong, Zhengliang Liu, Lichao Sun, Xiang Li, Tianming Liu, Pheng-Ann Heng, Quanzheng Li<br>for:* 这个研究是为了提高基于自然图像的图像分类模型（Segment Anything Model，SAM）在医疗图像分类任务中的表现。methods:* 这个研究使用了SAM的预训 weights，并在 fine-tuning 过程中添加了3D adapter，将3D信息融入到SAM的图像Encoder中。results:* 这个研究在4个医疗图像分类任务上进行了广泛的评估，并证明了MA-SAM在这些任务中的表现比其他竞争方法更好，包括nnU-Net。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM), a foundation model for general image segmentation, has demonstrated impressive zero-shot performance across numerous natural image segmentation tasks. However, SAM's performance significantly declines when applied to medical images, primarily due to the substantial disparity between natural and medical image domains. To effectively adapt SAM to medical images, it is important to incorporate critical third-dimensional information, i.e., volumetric or temporal knowledge, during fine-tuning. Simultaneously, we aim to harness SAM's pre-trained weights within its original 2D backbone to the fullest extent. In this paper, we introduce a modality-agnostic SAM adaptation framework, named as MA-SAM, that is applicable to various volumetric and video medical data. Our method roots in the parameter-efficient fine-tuning strategy to update only a small portion of weight increments while preserving the majority of SAM's pre-trained weights. By injecting a series of 3D adapters into the transformer blocks of the image encoder, our method enables the pre-trained 2D backbone to extract third-dimensional information from input data. The effectiveness of our method has been comprehensively evaluated on four medical image segmentation tasks, by using 10 public datasets across CT, MRI, and surgical video data. Remarkably, without using any prompt, our method consistently outperforms various state-of-the-art 3D approaches, surpassing nnU-Net by 0.9%, 2.6%, and 9.9% in Dice for CT multi-organ segmentation, MRI prostate segmentation, and surgical scene segmentation respectively. Our model also demonstrates strong generalization, and excels in challenging tumor segmentation when prompts are used. Our code is available at: https://github.com/cchen-cc/MA-SAM.
</details>
<details>
<summary>摘要</summary>
segments Anything Model (SAM)，一种基础模型 для通用图像分割，在各种自然图像分割任务上表现出色，但是在医疗图像上表现较差，主要是因为医疗图像和自然图像领域之间存在巨大的差异。为了有效地适应医疗图像，需要在精度调整中包含关键的三维信息，即体积或时间信息。同时，我们希望能充分利用SAM已经预训练的参数。在这篇文章中，我们提出了一种不同类型的SAM适应框架，名为MA-SAM，可以应用于各种体积和视频医疗数据。我们的方法基于精度调整策略，只更新一小部分的参数增量，保留SAM预训练的大部分参数。通过在图像编码器中插入3D适应器，我们的方法使得预训练的2D背景能够从输入数据中提取三维信息。我们的方法在四种医疗图像分割任务上进行了广泛的评估，使用了10个公共数据集，包括CT、MRI和手术视频数据。可见地，无需使用提示，我们的方法在各种状态前的3D方法之上升级，nnU-Net的Dice值上升9.9%、2.6%和0.9%。我们的模型还表现出了强大的泛化能力，在提示使用时也能够出色地完成肿瘤分割任务。我们的代码可以在https://github.com/cchen-cc/MA-SAM上获取。
</details></li>
</ul>
<hr>
<h2 id="AOSR-Net-All-in-One-Sandstorm-Removal-Network"><a href="#AOSR-Net-All-in-One-Sandstorm-Removal-Network" class="headerlink" title="AOSR-Net: All-in-One Sandstorm Removal Network"></a>AOSR-Net: All-in-One Sandstorm Removal Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08838">http://arxiv.org/abs/2309.08838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yazhong Si, Xulong Zhang, Fan Yang, Jianzong Wang, Ning Cheng, Jing Xiao</li>
<li>for: 本研究旨在解决现有的尘埃图像提高方法存在的应用限制和复杂算法结构问题。</li>
<li>methods: 该研究提出了一种全新的图像恢复模型，名为“一体化尘埃除网络”（AOSR-Net），该模型基于重新定义的尘埃散射模型，直接建立了图像映射关系，并且将中间参数集成到模型中，从而有效地解决了过强增强和Week Generalization问题。</li>
<li>results: 对于 synthetic 和实际尘埃图像进行了实验，结果显示，提出的 AOSR-Net 模型在对尘埃图像进行提高时表现出色，超过了当前最佳算法（SOTA）的性能。<details>
<summary>Abstract</summary>
Most existing sandstorm image enhancement methods are based on traditional theory and prior knowledge, which often restrict their applicability in real-world scenarios. In addition, these approaches often adopt a strategy of color correction followed by dust removal, which makes the algorithm structure too complex. To solve the issue, we introduce a novel image restoration model, named all-in-one sandstorm removal network (AOSR-Net). This model is developed based on a re-formulated sandstorm scattering model, which directly establishes the image mapping relationship by integrating intermediate parameters. Such integration scheme effectively addresses the problems of over-enhancement and weak generalization in the field of sand dust image enhancement. Experimental results on synthetic and real-world sandstorm images demonstrate the superiority of the proposed AOSR-Net over state-of-the-art (SOTA) algorithms.
</details>
<details>
<summary>摘要</summary>
现有的沙尘抑制方法多数基于传统理论和先知知识，这常限制其在实际场景中的应用。另外，这些方法通常采用色调修正后followed by dust removal的策略，使算法结构变得太复杂。为解决这问题，我们介绍了一种新的图像恢复模型，即全 inclusive sandstorm removal network (AOSR-Net)。该模型基于重新表述的沙尘散射模型，直接建立图像映射关系，并通过 интеGRATION scheme来有效地解决抑制过激和弱泛化问题。实验结果表明，提出的AOSR-Net在 synthetic和实际沙尘图像中的表现优于state-of-the-art（SOTA）算法。
</details></li>
</ul>
<hr>
<h2 id="Dual-Camera-Joint-Deblurring-Denoising"><a href="#Dual-Camera-Joint-Deblurring-Denoising" class="headerlink" title="Dual-Camera Joint Deblurring-Denoising"></a>Dual-Camera Joint Deblurring-Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08826">http://arxiv.org/abs/2309.08826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shayan Shekarforoush, Amanpreet Walia, Marcus A. Brubaker, Konstantinos G. Derpanis, Alex Levinshtein</li>
<li>for: 提高低光照照片质量</li>
<li>methods: 使用同步短暂曝光图像和长暂曝光图像，并将其进行拼接和权重调整</li>
<li>results: 实现了对同步双摄像头图像进行优化，并在实验中显示出比其他方法更高的质量和稳定性<details>
<summary>Abstract</summary>
Recent image enhancement methods have shown the advantages of using a pair of long and short-exposure images for low-light photography. These image modalities offer complementary strengths and weaknesses. The former yields an image that is clean but blurry due to camera or object motion, whereas the latter is sharp but noisy due to low photon count. Motivated by the fact that modern smartphones come equipped with multiple rear-facing camera sensors, we propose a novel dual-camera method for obtaining a high-quality image. Our method uses a synchronized burst of short exposure images captured by one camera and a long exposure image simultaneously captured by another. Having a synchronized short exposure burst alongside the long exposure image enables us to (i) obtain better denoising by using a burst instead of a single image, (ii) recover motion from the burst and use it for motion-aware deblurring of the long exposure image, and (iii) fuse the two results to further enhance quality. Our method is able to achieve state-of-the-art results on synthetic dual-camera images from the GoPro dataset with five times fewer training parameters compared to the next best method. We also show that our method qualitatively outperforms competing approaches on real synchronized dual-camera captures.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:现代低光照摄像头技术已经显示了使用两枚长和短曝光图像的优点。这两种图像模式具有补偿的优势和缺点。前者生成了清晰但涂抹的图像，由于摄像头或物体运动而导致涂抹；而后者具有高分辨率，但由于低光子数而导致噪声。我们受到现代智能手机搭载多个后置摄像头的启示，我们提出了一种新的双摄像头方法，以实现高质量图像。我们的方法使用同步短曝光快照集的一个摄像头，同时使用另一个摄像头拍摄长曝光图像。具有同步短曝光快照的存在，我们可以（i）通过快照集取得更好的噪声纠正，（ii）从快照集中回归运动，并用于运动感知滤除长曝光图像中的涂抹，以及（iii）将两个结果融合，进一步提高质量。我们的方法在GoPro数据集上实现了状态最佳的结果，与接下来最佳方法相比，训练参数只需五分之一。我们还显示了我们的方法在真实同步双摄像头捕捉中的质量超越竞争方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/16/cs.CV_2023_09_16/" data-id="cloimip9500hks4881il036v6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/16/cs.AI_2023_09_16/" class="article-date">
  <time datetime="2023-09-16T12:00:00.000Z" itemprop="datePublished">2023-09-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/16/cs.AI_2023_09_16/">cs.AI - 2023-09-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Interactively-Teaching-an-Inverse-Reinforcement-Learner-with-Limited-Feedback"><a href="#Interactively-Teaching-an-Inverse-Reinforcement-Learner-with-Limited-Feedback" class="headerlink" title="Interactively Teaching an Inverse Reinforcement Learner with Limited Feedback"></a>Interactively Teaching an Inverse Reinforcement Learner with Limited Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09095">http://arxiv.org/abs/2309.09095</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rzayanov/irl-teaching-limited-feedback">https://github.com/rzayanov/irl-teaching-limited-feedback</a></li>
<li>paper_authors: Rustam Zayanov, Francisco S. Melo, Manuel Lopes</li>
<li>for: 本研究强调教学via示例在顺序决策任务中，特别是教师无法访问学生模型和策略的情况下。</li>
<li>methods: 本文使用 inverse reinforcement learning 和 active learning 方法，教师可以通过选择开始状态和推断学生策略来解决这个教学问题。</li>
<li>results: 在一个人工汽车驾驶环境中测试了提议的算法，结果显示该算法在学生反馈有限时是一个有效的解决方案。<details>
<summary>Abstract</summary>
We study the problem of teaching via demonstrations in sequential decision-making tasks. In particular, we focus on the situation when the teacher has no access to the learner's model and policy, and the feedback from the learner is limited to trajectories that start from states selected by the teacher. The necessity to select the starting states and infer the learner's policy creates an opportunity for using the methods of inverse reinforcement learning and active learning by the teacher. In this work, we formalize the teaching process with limited feedback and propose an algorithm that solves this teaching problem. The algorithm uses a modified version of the active value-at-risk method to select the starting states, a modified maximum causal entropy algorithm to infer the policy, and the difficulty score ratio method to choose the teaching demonstrations. We test the algorithm in a synthetic car driving environment and conclude that the proposed algorithm is an effective solution when the learner's feedback is limited.
</details>
<details>
<summary>摘要</summary>
我们研究教学via示例在时序决策任务中。特别是在教师无法访问学生的模型和策略的情况下，并且学生对教师的反馈仅仅是从教师选择的状态开始的路径。因为选择开始状态和推理学生策略创造了教师可以使用反对抗学习和活动学习的机会。在这个研究中，我们将教学过程 formalize 为有限反馈的情况，并提出一个解决这个教学问题的算法。这个算法使用修改版的活跃值-at-risk方法选择开始状态，修改最大 causal entropy 算法推理学生策略，以及对教学示例选择的困难分数比率方法。我们在人工智能汽车驾驶环境中试验了这个算法，结果显示，我们的提案算法在学生的反馈有限时是一个有效的解决方案。
</details></li>
</ul>
<hr>
<h2 id="RMDM-A-Multilabel-Fakenews-Dataset-for-Vietnamese-Evidence-Verification"><a href="#RMDM-A-Multilabel-Fakenews-Dataset-for-Vietnamese-Evidence-Verification" class="headerlink" title="RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification"></a>RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09071">http://arxiv.org/abs/2309.09071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai-Long Nguyen, Thi-Kieu-Trang Pham, Thai-Son Le, Tan-Minh Nguyen, Thi-Hai-Yen Vuong, Ha-Thanh Nguyen</li>
<li>for: 这个研究是为了评估大语言模型（LLM）在电子信息相关法律上的性能，特别是用于识别假新闻作为电子证据的输入。</li>
<li>methods: 该研究使用了一个新的和挑战性的多标签越南语 dataset（RMDM），包括四个标签：实用、误差、恶意和恶假，表示实际信息、误差信息、恶意信息和假信息。</li>
<li>results: 研究发现，使用 GPT 和 BERT 模型测试 RMDM 数据集时，每个标签的性能异常，表明该数据集能够挑战不同语言模型对于识别各种类型的电子信息的能力。研究结果表明，用于识别电子信息相关法律上的 fake news 仍然是一个困难的问题，需要更多的研究人员投入，以提高 AI 模型的可靠性。<details>
<summary>Abstract</summary>
In this study, we present a novel and challenging multilabel Vietnamese dataset (RMDM) designed to assess the performance of large language models (LLMs), in verifying electronic information related to legal contexts, focusing on fake news as potential input for electronic evidence. The RMDM dataset comprises four labels: real, mis, dis, and mal, representing real information, misinformation, disinformation, and mal-information, respectively. By including these diverse labels, RMDM captures the complexities of differing fake news categories and offers insights into the abilities of different language models to handle various types of information that could be part of electronic evidence. The dataset consists of a total of 1,556 samples, with 389 samples for each label. Preliminary tests on the dataset using GPT-based and BERT-based models reveal variations in the models' performance across different labels, indicating that the dataset effectively challenges the ability of various language models to verify the authenticity of such information. Our findings suggest that verifying electronic information related to legal contexts, including fake news, remains a difficult problem for language models, warranting further attention from the research community to advance toward more reliable AI models for potential legal applications.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一个新的和挑战性的多标签越南语数据集（RMDM），用于评估大语言模型（LLM）在电子信息相关法律上下文中的性能，特点是通过假新闻作为电子证据的输入。RMDM数据集包括四个标签：真实、误information、 désinformation和mal-information，分别表示真实信息、误信息、恶意误导和危险信息。由于这些多样化的标签，RMDM数据集能够捕捉各种假新闻类型的复杂性，并为不同语言模型的性能进行评估。该数据集包括总共1556个样本，每个标签各有389个样本。初步测试表明，使用基于GPT和BERT模型的语言模型在不同标签上表现有很大差异，这表明RMDM数据集对不同语言模型的挑战性很高。我们的发现表明，在法律上下文中电子信息的验证仍然是一个具有挑战性的问题，需要研究人员继续努力，以提出更可靠的AI模型，用于 potential legal applications。
</details></li>
</ul>
<hr>
<h2 id="NOWJ1-ALQAC-2023-Enhancing-Legal-Task-Performance-with-Classic-Statistical-Models-and-Pre-trained-Language-Models"><a href="#NOWJ1-ALQAC-2023-Enhancing-Legal-Task-Performance-with-Classic-Statistical-Models-and-Pre-trained-Language-Models" class="headerlink" title="NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic Statistical Models and Pre-trained Language Models"></a>NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic Statistical Models and Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09070">http://arxiv.org/abs/2309.09070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tan-Minh Nguyen, Xuan-Hoa Nguyen, Ngoc-Duy Mai, Minh-Quan Hoang, Van-Huan Nguyen, Hoang-Viet Nguyen, Ha-Thanh Nguyen, Thi-Hai-Yen Vuong</li>
<li>for: 本研究旨在提高法律任务性能，通过精心搭配经典统计模型和预训练语言模型（PLMs）。</li>
<li>methods: 我们实施了一个预处理步骤，以解决输入限制，并应用学习到rank方法，以整合各种模型中的特征。在问答任务中，我们分成两个子任务：句子分类和答案提取。我们采用了当今最佳实践，以开发每个子任务的独特系统，并利用经典统计模型和预训练语言模型。</li>
<li>results: 实验结果表明，我们提出的方法在比赛中具有扎实的潜力。<details>
<summary>Abstract</summary>
This paper describes the NOWJ1 Team's approach for the Automated Legal Question Answering Competition (ALQAC) 2023, which focuses on enhancing legal task performance by integrating classical statistical models and Pre-trained Language Models (PLMs). For the document retrieval task, we implement a pre-processing step to overcome input limitations and apply learning-to-rank methods to consolidate features from various models. The question-answering task is split into two sub-tasks: sentence classification and answer extraction. We incorporate state-of-the-art models to develop distinct systems for each sub-task, utilizing both classic statistical models and pre-trained Language Models. Experimental results demonstrate the promising potential of our proposed methodology in the competition.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字符" or "简体字".Here's the translation in Traditional Chinese:这份研究报告描述了NOWJ1队的2023年自动法律问题回答比赛（ALQAC）方法，强调通过结合古典统计模型和预训语言模型（PLMs）来提高法律任务性能。 для文档搜寻任务，我们实现了预处理步骤以超过输入限制，并使用学习排名方法将不同模型中的特征集成。问题回答任务分为两个子任务：句子分类和答案抽取。我们使用了现代模型，开发了两个不同的系统，一个是基于古典统计模型，另一个是基于预训语言模型。实验结果显示了我们的提案方法在比赛中的应用潜力。
</details></li>
</ul>
<hr>
<h2 id="GenDOM-Generalizable-One-shot-Deformable-Object-Manipulation-with-Parameter-Aware-Policy"><a href="#GenDOM-Generalizable-One-shot-Deformable-Object-Manipulation-with-Parameter-Aware-Policy" class="headerlink" title="GenDOM: Generalizable One-shot Deformable Object Manipulation with Parameter-Aware Policy"></a>GenDOM: Generalizable One-shot Deformable Object Manipulation with Parameter-Aware Policy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09051">http://arxiv.org/abs/2309.09051</a></li>
<li>repo_url: None</li>
<li>paper_authors: So Kuroki, Jiaxian Guo, Tatsuya Matsushima, Takuya Okubo, Masato Kobayashi, Yuya Ikeda, Ryosuke Takanami, Paul Yoo, Yutaka Matsuo, Yusuke Iwasawa</li>
<li>for: 一个可以实现单一示范的弹性物品操作框架 (a framework that can achieve one-shot deformable object manipulation)</li>
<li>methods: 使用弹性物品参数来条件政策并在训练过程中使用多种弹性物品模拟，以将政策适应不同弹性物品 (using deformable object parameters to condition the policy and training it with a diverse range of simulated deformable objects, so that the policy can adapt to different objects)</li>
<li>results: 实际验证范例显示，our方法可以实现不同弹性物品的一个示范操作 (empirical validations show that our method can manipulate different objects with a single demonstration)，并在实际环境中比基eline表现更好 (and significantly outperform the baseline in both simulation and real-world environments)<details>
<summary>Abstract</summary>
Due to the inherent uncertainty in their deformability during motion, previous methods in deformable object manipulation, such as rope and cloth, often required hundreds of real-world demonstrations to train a manipulation policy for each object, which hinders their applications in our ever-changing world. To address this issue, we introduce GenDOM, a framework that allows the manipulation policy to handle different deformable objects with only a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable object parameters and training it with a diverse range of simulated deformable objects so that the policy can adjust actions based on different object parameters. At the time of inference, given a new object, GenDOM can estimate the deformable object parameters with only a single real-world demonstration by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations in a differentiable physics simulator. Empirical validations on both simulated and real-world object manipulation setups clearly show that our method can manipulate different objects with a single demonstration and significantly outperforms the baseline in both environments (a 62% improvement for in-domain ropes and a 15% improvement for out-of-distribution ropes in simulation, as well as a 26% improvement for ropes and a 50% improvement for cloths in the real world), demonstrating the effectiveness of our approach in one-shot deformable object manipulation.
</details>
<details>
<summary>摘要</summary>
由于物体的自然变形性在运动中的不确定性，过去的方法在弹性物体把握中经常需要数百个实际世界示例来训练把握策略，这限制了它们在我们的变化世界中的应用。为解决这个问题，我们介绍GenDOM框架，它允许把握策略处理不同的弹性物体，只需要单个实际世界示例。为达到这个目标，我们在策略中添加了基于弹性物体参数的条件，并在训练策略时使用了多种 simulate 的弹性物体，以便策略可以根据不同的物体参数调整动作。在推理时，对于新的物体，GenDOM可以通过在一个分解器上进行最小化，使得策略可以在具有不同物体参数的情况下进行适应。我们的实验证明，我们的方法可以在各种 simulate 和实际环境中一shot 把握不同的弹性物体，并且明显超过基eline，这证明了我们的方法在一shot 弹性物体把握中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-Driven-Storytelling-A-New-Era-for-Marketing"><a href="#Generative-AI-Driven-Storytelling-A-New-Era-for-Marketing" class="headerlink" title="Generative AI-Driven Storytelling: A New Era for Marketing"></a>Generative AI-Driven Storytelling: A New Era for Marketing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09048">http://arxiv.org/abs/2309.09048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marko Vidrih, Shiva Mayahi</li>
<li>for: 这篇论文探讨了用生成AI驱动的故事创作在市场策略中的转变力。</li>
<li>methods: 该论文使用实际业界例子，如Google、Netflix和Stitch Fix，解释了如何使用此技术自定义消费者体验， navigate 相关挑战。</li>
<li>results: 论文描述了将来的发展方向和建议，包括实时个性化 Storytelling、 immerse  Storytelling 体验和社交媒体 Storytelling。In English, the three key points are:</li>
<li>for: This paper explores the transformative power of Generative AI-driven storytelling in marketing.</li>
<li>methods: The paper uses real-world examples from industry leaders like Google, Netflix, and Stitch Fix to illustrate how this technology personalizes consumer experiences and navigates challenges.</li>
<li>results: The paper describes future directions and recommendations for generative AI-driven storytelling, including prospective applications such as real-time personalized storytelling, immersive storytelling experiences, and social media storytelling.<details>
<summary>Abstract</summary>
This paper delves into the transformative power of Generative AI-driven storytelling in the realm of marketing. Generative AI, distinct from traditional machine learning, offers the capability to craft narratives that resonate with consumers on a deeply personal level. Through real-world examples from industry leaders like Google, Netflix and Stitch Fix, we elucidate how this technology shapes marketing strategies, personalizes consumer experiences, and navigates the challenges it presents. The paper also explores future directions and recommendations for generative AI-driven storytelling, including prospective applications such as real-time personalized storytelling, immersive storytelling experiences, and social media storytelling. By shedding light on the potential and impact of generative AI-driven storytelling in marketing, this paper contributes to the understanding of this cutting-edge approach and its transformative power in the field of marketing.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了生成AI驱动的故事创作在市场营销中的变革力。生成AI与传统机器学习不同，具有制作吸引消费者深层次共鸣的故事的能力。通过实业领导者如Google、Netflix和Stitch Fix的实践例子，我们详细介绍了该技术如何影响市场策略、个性化消费者经验和解决相关挑战。这篇论文还探讨了未来生成AI驱动的故事创作的发展趋势和建议，包括实时个性化storytelling、 immerse storytelling经验和社交媒体storytelling。这篇论文通过探讨生成AI驱动的故事创作在市场营销中的潜力和影响，贡献于这一领域的理解和发展。
</details></li>
</ul>
<hr>
<h2 id="A-store-and-forward-cloud-based-telemonitoring-system-for-automatic-assessing-dysarthria-evolution-in-neurological-diseases-from-video-recording-analysis"><a href="#A-store-and-forward-cloud-based-telemonitoring-system-for-automatic-assessing-dysarthria-evolution-in-neurological-diseases-from-video-recording-analysis" class="headerlink" title="A store-and-forward cloud-based telemonitoring system for automatic assessing dysarthria evolution in neurological diseases from video-recording analysis"></a>A store-and-forward cloud-based telemonitoring system for automatic assessing dysarthria evolution in neurological diseases from video-recording analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09038">http://arxiv.org/abs/2309.09038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucia Migliorelli, Daniele Berardini, Kevin Cela, Michela Coccia, Laura Villani, Emanuele Frontoni, Sara Moccia</li>
<li>For: This study aims to provide a remote telemonitoring system to support clinicians in monitoring the evolution of dysarthria in patients with neurological diseases.* Methods: The system uses a convolutional neural network (CNN) to analyze video recordings of individuals with dysarthria, and locates facial landmarks as a prior for assessing orofacial functions related to speech.* Results: The proposed CNN achieved a normalized mean error of 1.79 on localizing facial landmarks when tested on a public dataset, and showed promising outcomes in a real-life scenario with 11 bulbar-onset ALS subjects.<details>
<summary>Abstract</summary>
Background and objectives: Patients suffering from neurological diseases may develop dysarthria, a motor speech disorder affecting the execution of speech. Close and quantitative monitoring of dysarthria evolution is crucial for enabling clinicians to promptly implement patient management strategies and maximizing effectiveness and efficiency of communication functions in term of restoring, compensating or adjusting. In the clinical assessment of orofacial structures and functions, at rest condition or during speech and non-speech movements, a qualitative evaluation is usually performed, throughout visual observation. Methods: To overcome limitations posed by qualitative assessments, this work presents a store-and-forward self-service telemonitoring system that integrates, within its cloud architecture, a convolutional neural network (CNN) for analyzing video recordings acquired by individuals with dysarthria. This architecture, called facial landmark Mask RCNN, aims at locating facial landmarks as a prior for assessing the orofacial functions related to speech and examining dysarthria evolution in neurological diseases. Results: When tested on the Toronto NeuroFace dataset, a publicly available annotated dataset of video recordings from patients with amyotrophic lateral sclerosis (ALS) and stroke, the proposed CNN achieved a normalized mean error equal to 1.79 on localizing the facial landmarks. We also tested our system in a real-life scenario on 11 bulbar-onset ALS subjects, obtaining promising outcomes in terms of facial landmark position estimation. Discussion and conclusions: This preliminary study represents a relevant step towards the use of remote tools to support clinicians in monitoring the evolution of dysarthria.
</details>
<details>
<summary>摘要</summary>
背景和目标：神经疾病患者可能发展出语言障碍，称为肌肉语言障碍，影响语言执行。为了帮助临床医生尽快实施患者管理策略并最大化对语言功能的效果和效率，close和量化监测肌肉语言障碍的发展是非常重要的。在评估嘴部结构和功能方面，通常采用观察方式进行评估。方法：为了超越质量评估的限制，本工作提出了一个自助式抽象云端系统，其中包含一个基于卷积神经网络（CNN）的脸部特征检测模型，用于分析患者所录制的视频记录。这个架构被称为脸部特征Mask RCNN，旨在在视频记录中检测肌肉语言障碍的发展，并评估嘴部结构和功能相关的语言功能。结果：当测试在多伦多大学的脸部数据集上时，我们的CNN模型实现了1.79的正常化平均错误，用于定位脸部特征。我们还在11名有患有贝叶静脉性肌肉疾病的实际情况下测试了我们的系统，并获得了可观的结果，表明我们的系统可以准确地定位脸部特征。讨论和结论：这一初步研究表明了远程工具的使用可以帮助临床医生更好地监测肌肉语言障碍的发展。通过评估肌肉语言障碍的发展，可以帮助临床医生更好地评估患者的状况，并采取更加有效的治疗策略。此外，这种远程监测技术可以帮助患者更好地与医生进行沟通，从而提高患者的生活质量。
</details></li>
</ul>
<hr>
<h2 id="Improve-Deep-Forest-with-Learnable-Layerwise-Augmentation-Policy-Schedule"><a href="#Improve-Deep-Forest-with-Learnable-Layerwise-Augmentation-Policy-Schedule" class="headerlink" title="Improve Deep Forest with Learnable Layerwise Augmentation Policy Schedule"></a>Improve Deep Forest with Learnable Layerwise Augmentation Policy Schedule</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09030">http://arxiv.org/abs/2309.09030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dbsxfz/augdf">https://github.com/dbsxfz/augdf</a></li>
<li>paper_authors: Hongyu Zhu, Sichu Liang, Wentao Hu, Fang-Qi Li, Yali yuan, Shi-Lin Wang, Guang Cheng</li>
<li>for: 提高 Deep Forest 模型的表现和泛化能力，对 tabular 数据进行更好的处理和分类。</li>
<li>methods: 使用可学习的层 wise 数据增强策略，包括 Cut Mix for Tabular data 技术，并使用人口基数搜索算法来调整增强程度。</li>
<li>results: 在多个 tabular 分类任务中达到新的 state-of-the-art 水平，超过了树ensemble、深度森林、深度神经网络和 AutoML 竞争对手。<details>
<summary>Abstract</summary>
As a modern ensemble technique, Deep Forest (DF) employs a cascading structure to construct deep models, providing stronger representational power compared to traditional decision forests. However, its greedy multi-layer learning procedure is prone to overfitting, limiting model effectiveness and generalizability. This paper presents an optimized Deep Forest, featuring learnable, layerwise data augmentation policy schedules. Specifically, We introduce the Cut Mix for Tabular data (CMT) augmentation technique to mitigate overfitting and develop a population-based search algorithm to tailor augmentation intensity for each layer. Additionally, we propose to incorporate outputs from intermediate layers into a checkpoint ensemble for more stable performance. Experimental results show that our method sets new state-of-the-art (SOTA) benchmarks in various tabular classification tasks, outperforming shallow tree ensembles, deep forests, deep neural network, and AutoML competitors. The learned policies also transfer effectively to Deep Forest variants, underscoring its potential for enhancing non-differentiable deep learning modules in tabular signal processing.
</details>
<details>
<summary>摘要</summary>
为了提高表格分类性能，我们提出了一种优化的深度森林（DF）技术，具有更强的表达力。这种技术利用级别结构来构建深度模型，从而提高模型的表达力。然而，这种积极多层学习的方法容易过拟合，导致模型的效果和泛化性受限。为了解决这个问题，我们在这篇论文中提出了一种可学习的层weise数据增强策略，包括了Cut Mix for Tabular data（CMT）增强技术来mitigate过拟合。此外，我们还提出了一种基于人口的搜索算法来调整增强intensity的每层策略。此外，我们还提出了将 intermediate层的输出集成到检查点ensemble中，以确保模型的稳定性。实验结果显示，我们的方法在不同的表格分类任务中设置了新的最佳性能记录（SOTA），超过了树ensemble、深度森林、深度神经网络和AutoML竞争对手。学习的策略也可以有效地传递到 Deep Forest 的变体中，这 подтвержда了其在表格信号处理中的潜在应用。
</details></li>
</ul>
<hr>
<h2 id="Earth-Virtualization-Engines-–-A-Technical-Perspective"><a href="#Earth-Virtualization-Engines-–-A-Technical-Perspective" class="headerlink" title="Earth Virtualization Engines – A Technical Perspective"></a>Earth Virtualization Engines – A Technical Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09002">http://arxiv.org/abs/2309.09002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Torsten Hoefler, Bjorn Stevens, Andreas F. Prein, Johanna Baehr, Thomas Schulthess, Thomas F. Stocker, John Taylor, Daniel Klocke, Pekka Manninen, Piers M. Forster, Tobias Kölling, Nicolas Gruber, Hartwig Anzt, Claudia Frauen, Florian Ziemen, Milan Klöwer, Karthik Kashinath, Christoph Schär, Oliver Fuhrer, Bryan N. Lawrence</li>
<li>for: 提高气候变化应对能力</li>
<li>methods: 结合物理模型和机器学习技术，提高气候预测的准确性、效率和可读性</li>
<li>results: 实现了高分辨率的气候数据访问和分析，为气候变化的研究和应对做出了重要贡献<details>
<summary>Abstract</summary>
Participants of the Berlin Summit on Earth Virtualization Engines (EVEs) discussed ideas and concepts to improve our ability to cope with climate change. EVEs aim to provide interactive and accessible climate simulations and data for a wide range of users. They combine high-resolution physics-based models with machine learning techniques to improve the fidelity, efficiency, and interpretability of climate projections. At their core, EVEs offer a federated data layer that enables simple and fast access to exabyte-sized climate data through simple interfaces. In this article, we summarize the technical challenges and opportunities for developing EVEs, and argue that they are essential for addressing the consequences of climate change.
</details>
<details>
<summary>摘要</summary>
BERLIN峰会上的地球虚拟化引擎（EVEs）参与者们讨论了如何改善对气候变化的应对能力。EVEs旨在提供互动性强、易于访问的气候模拟和数据，为广泛的用户群提供。它们结合高分辨率物理模型和机器学习技术，以提高模拟结果的准确性、效率和可解释性。EVEs的核心是一个联邦数据层，可以通过简单的接口访问数据，并且可以在毫秒级别进行数据交互。本文将讨论EVEs的技术挑战和机遇，并认为它们是Addressing the consequences of climate change的关键工具。
</details></li>
</ul>
<hr>
<h2 id="Deliberative-Context-Aware-Ambient-Intelligence-System-for-Assisted-Living-Homes"><a href="#Deliberative-Context-Aware-Ambient-Intelligence-System-for-Assisted-Living-Homes" class="headerlink" title="Deliberative Context-Aware Ambient Intelligence System for Assisted Living Homes"></a>Deliberative Context-Aware Ambient Intelligence System for Assisted Living Homes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08984">http://arxiv.org/abs/2309.08984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohannad Babli, Jaime A Rincon, Eva Onaindia, Carlos Carrascosa, Vicente Julian</li>
<li>for: 这个论文的目的是提出一种 ambient intelligence 健康应用的决策架构，用于舒缓受抚恤的老年人受到负面情绪的情况下，并在助生活机构中进行实施。</li>
<li>methods: 该架构使用了决策函数，以实现 Context-aware 的人机交互、感知、规划功能、反应性和环境意识等特性。文章还进行了一些实验研究，用以证明方法的效果和有效性。</li>
<li>results: 实验结果表明，提出的决策函数已经成功地实现了其决策目标，并且在 simulate 的助生活机构enario 中得到了有效的结果。<details>
<summary>Abstract</summary>
Monitoring wellbeing and stress is one of the problems covered by ambient intelligence, as stress is a significant cause of human illnesses directly affecting our emotional state. The primary aim was to propose a deliberation architecture for an ambient intelligence healthcare application. The architecture provides a plan for comforting stressed seniors suffering from negative emotions in an assisted living home and executes the plan considering the environment's dynamic nature. Literature was reviewed to identify the convergence between deliberation and ambient intelligence and the latter's latest healthcare trends. A deliberation function was designed to achieve context-aware dynamic human-robot interaction, perception, planning capabilities, reactivity, and context-awareness with regard to the environment. A number of experimental case studies in a simulated assisted living home scenario were conducted to demonstrate the approach's behavior and validity. The proposed methods were validated to show classification accuracy. The validation showed that the deliberation function has effectively achieved its deliberative objectives.
</details>
<details>
<summary>摘要</summary>
监测健康和压力是智能环境技术中一个主要问题，因为压力直接影响我们的情感状态，是人类疾病的直接原因之一。我们的目标是提议一种 ambient intelligence 健康应用的决策建构。这种建构提供了一个计划，用于使用智能环境技术来慰宠受到压力的长者，并在考虑环境的动态特点下执行该计划。我们查看了相关 литераature，以确定决策和智能环境之间的叉合，以及智能环境最新的医疗趋势。我们设计了一种决策函数，以实现 context-aware 的人机交互、感知、规划能力、感应性和对环境的Context-awareness。我们进行了一些在 simulated 助生活场景中的实验案例，以证明方法的行为和有效性。我们验证了提案的方法，并证明了决策函数已经成功完成了它的决策目标。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-In-Browser-Deep-Learning-Inference-on-Diverse-Edge-Clients-through-Just-in-Time-Kernel-Optimizations"><a href="#Accelerating-In-Browser-Deep-Learning-Inference-on-Diverse-Edge-Clients-through-Just-in-Time-Kernel-Optimizations" class="headerlink" title="Accelerating In-Browser Deep Learning Inference on Diverse Edge Clients through Just-in-Time Kernel Optimizations"></a>Accelerating In-Browser Deep Learning Inference on Diverse Edge Clients through Just-in-Time Kernel Optimizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08978">http://arxiv.org/abs/2309.08978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fucheng Jia, Shiqi Jiang, Ting Cao, Wei Cui, Tianrui Xia, Xu Cao, Yuanchun Li, Deyu Zhang, Ju Ren, Yunxin Liu, Lili Qiu, Mao Yang</li>
<li>for: 这篇论文的目的是提出一个首个在浏览器中进行深度学习（DL）推理的系统，以提高浏览器中DL推理的性能。</li>
<li>methods: 这篇论文使用了两种新的网络编程技术来实现自动生成优化的加速器，包括：Tensor-Web Compiling Co-Design和Web-Specific Lite Kernel Optimization Space Design。这两种技术可以减少加速器生成成本，同时维持或甚至提高性能。</li>
<li>results: 对现代转换器模型进行评估，nn-JIT.web可以在各种客户端设备上实现到8.2倍的加速，包括ARM、Intel、AMD和Nvidia的主流CPUs和GPUs。<details>
<summary>Abstract</summary>
Web applications are increasingly becoming the primary platform for AI service delivery, making in-browser deep learning (DL) inference more prominent. However, current in-browser inference systems fail to effectively utilize advanced web programming techniques and customize kernels for various client devices, leading to suboptimal performance.   To address the issues, this paper presents the first in-browser inference system, nn-JIT.web, which enables just-in-time (JIT) auto-generation of optimized kernels for both CPUs and GPUs during inference. The system achieves this by using two novel web programming techniques that can significantly reduce kernel generation time, compared to other tensor compilers such as TVM, while maintaining or even improving performance. The first technique, Tensor-Web Compiling Co-Design, lowers compiling costs by unifying tensor and web compiling and eliminating redundant and ineffective compiling passes. The second technique, Web-Specific Lite Kernel Optimization Space Design, reduces kernel tuning costs by focusing on web programming requirements and efficient hardware resource utilization, limiting the optimization space to only dozens.   nn-JIT.web is evaluated for modern transformer models on a range of client devices, including the mainstream CPUs and GPUs from ARM, Intel, AMD and Nvidia. Results show that nn-JIT.web can achieve up to 8.2x faster within 30 seconds compared to the baselines across various models.
</details>
<details>
<summary>摘要</summary>
现代浏览器中的 Web 应用程序正在成为人工智能服务的主要平台，使得在浏览器内进行深度学习（DL）推理变得更加重要。然而，当前的浏览器推理系统无法有效利用高级网络编程技术和自定义核心 для各种客户端设备，导致性能下降。为解决这些问题，这篇论文提出了第一个在浏览器内进行推理的系统，即 nn-JIT.web。该系统可以在推理过程中通过实时生成优化的核心，以提高 CPU 和 GPU 的性能。该系统使用了两种新的网络编程技术来减少核心生成时间，相比于其他tensor编译器such as TVM，而无需增加编译成本。第一种技术是tensor-web编译合理化，它将tensor编译和网络编译融合起来，从而减少了无用的编译步骤。第二种技术是网络特定的轻量级核心优化空间设计，它将精力集中在了网络编程需求和高效硬件资源利用上，从而减少了优化空间的范围，只有几十。nn-JIT.web在多种现代转换器模型上进行了测试，包括ARM、Intel、AMD和Nvidia等主流CPU和GPU。结果显示，nn-JIT.web可以在30秒内达到8.2倍的速度提升，与基准值相比。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Reachability-using-Christoffel-Functions-and-Conformal-Prediction"><a href="#Data-driven-Reachability-using-Christoffel-Functions-and-Conformal-Prediction" class="headerlink" title="Data-driven Reachability using Christoffel Functions and Conformal Prediction"></a>Data-driven Reachability using Christoffel Functions and Conformal Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08976">http://arxiv.org/abs/2309.08976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelmouaiz Tebjou, Goran Frehse, Faïcel Chamroukhi</li>
<li>for: 本研究旨在提出一种数据驱动的方法，用于估计动力系统中可达的状态集（reach set），而无需知道系统动力学模型的准确参数。</li>
<li>methods: 本方法基于Christoffel函数的approximation来估计 reach set，并且通过使用数据驱动的方法来提高样本效率和鲁棒性。</li>
<li>results: 本研究显示，使用这种方法可以提高样本效率和鲁棒性，并且可以避免出现在训练集和校准集中的异常样本的影响。<details>
<summary>Abstract</summary>
An important mathematical tool in the analysis of dynamical systems is the approximation of the reach set, i.e., the set of states reachable after a given time from a given initial state. This set is difficult to compute for complex systems even if the system dynamics are known and given by a system of ordinary differential equations with known coefficients. In practice, parameters are often unknown and mathematical models difficult to obtain. Data-based approaches are promised to avoid these difficulties by estimating the reach set based on a sample of states. If a model is available, this training set can be obtained through numerical simulation. In the absence of a model, real-life observations can be used instead. A recently proposed approach for data-based reach set approximation uses Christoffel functions to approximate the reach set. Under certain assumptions, the approximation is guaranteed to converge to the true solution. In this paper, we improve upon these results by notably improving the sample efficiency and relaxing some of the assumptions by exploiting statistical guarantees from conformal prediction with training and calibration sets. In addition, we exploit an incremental way to compute the Christoffel function to avoid the calibration set while maintaining the statistical convergence guarantees. Furthermore, our approach is robust to outliers in the training and calibration set.
</details>
<details>
<summary>摘要</summary>
“一个重要的数学工具在动态系统分析中是精确地计算可达集，即从一个初始状态到一个给定时间后可达的状态集。这个集是复杂系统的情况下难以计算，即使系统动态知道并且以常微分方程表示。实际上，参数通常未知，数学模型难以取得。数据驱动的方法可以避免这些问题，通过基于数据的估计来Estimate the reach set。如果有模型可用，则可以通过数值 simulations obtain training set。在缺乏模型的情况下，则可以使用实际观察。一种最近提出的方法是使用Christoffel函数估计可达集。在某些假设下，这个估计将会趋向真实解。在这篇论文中，我们会提高这些结果，包括提高样本效率和松动一些假设，通过滤节点和训练集的统计保证。此外，我们还会利用增量式计算Christoffel函数，以避免训练集的需求，同时保持统计的测度保证。此外，我们的方法也能够抗抗噪。”
</details></li>
</ul>
<hr>
<h2 id="Multiagent-Reinforcement-Learning-with-an-Attention-Mechanism-for-Improving-Energy-Efficiency-in-LoRa-Networks"><a href="#Multiagent-Reinforcement-Learning-with-an-Attention-Mechanism-for-Improving-Energy-Efficiency-in-LoRa-Networks" class="headerlink" title="Multiagent Reinforcement Learning with an Attention Mechanism for Improving Energy Efficiency in LoRa Networks"></a>Multiagent Reinforcement Learning with an Attention Mechanism for Improving Energy Efficiency in LoRa Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08965">http://arxiv.org/abs/2309.08965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Zhang, Ziqi Lin, Shimin Gong, Bo Gu, Dusit Niyato</li>
<li>for: 该研究旨在提高LoRa网络的能源效率（EE），适用于工业互联网物联网（IIoT）。</li>
<li>methods: 该研究首先提出了一个分析模型来计算LoRa网络的系统EE性能。然后，基于多代理强化学习（MALoRa）算法，对LoRa网络中每个终端设备（ED）的传输参数分配进行优化，以最大化系统EE。</li>
<li>results:  simulation结果表明，相比基eline算法，MALoRa算法可以显著提高LoRa网络的系统EE，但是同时也导致了一定的数据包交换率（PDR）的下降。<details>
<summary>Abstract</summary>
Long Range (LoRa) wireless technology, characterized by low power consumption and a long communication range, is regarded as one of the enabling technologies for the Industrial Internet of Things (IIoT). However, as the network scale increases, the energy efficiency (EE) of LoRa networks decreases sharply due to severe packet collisions. To address this issue, it is essential to appropriately assign transmission parameters such as the spreading factor and transmission power for each end device (ED). However, due to the sporadic traffic and low duty cycle of LoRa networks, evaluating the system EE performance under different parameter settings is time-consuming. Therefore, we first formulate an analytical model to calculate the system EE. On this basis, we propose a transmission parameter allocation algorithm based on multiagent reinforcement learning (MALoRa) with the aim of maximizing the system EE of LoRa networks. Notably, MALoRa employs an attention mechanism to guide each ED to better learn how much ''attention'' should be given to the parameter assignments for relevant EDs when seeking to improve the system EE. Simulation results demonstrate that MALoRa significantly improves the system EE compared with baseline algorithms with an acceptable degradation in packet delivery rate (PDR).
</details>
<details>
<summary>摘要</summary>
长距离无线技术（LoRa）， caracterizada por baja consumición de energía y una comunicación larga distancia, es considerada una de las tecnologías clave para la Internet de las Cosas Industriales (IIoT). Sin embargo, a medida que se incrementa la escala de la red, la eficiencia energética (EE) de las redes LoRa disminuye drásticamente debido a colisiones de paquetes graves. Para abordar este problema, es esencial asignar los parámetros de transmisión, como el factor de spreading y la potencia de transmisión, de manera adecuada para cada dispositivo de end (ED). Sin embargo, debido al tráfico esporádico y baja tasa de actividad de las redes LoRa, evaluar el desempeño de sistema EE bajo diferentes configuraciones de parámetros es un proceso tiempoconsumidor. Por lo tanto, primero formulamos un modelo analítico para calcular el sistema EE. A partir de este modelo, propusimos un algoritmo de asignación de parámetros de transmisión basado en aprendizaje por refuerzo multientidad (MALoRa) con el objetivo de maximizar el sistema EE de las redes LoRa. Destacablemente, MALoRa utiliza un mecanismo de atención para guiar a cada ED en cómo asignar la atención adecuada a las asignaciones de parámetros relevantes para mejorar el sistema EE. Los resultados de la simulación demuestran que MALoRa mejora significativamente el sistema EE en comparación con los algoritmos de referencia con una degradación aceptable en la tasa de entrega de paquetes (PDR).
</details></li>
</ul>
<hr>
<h2 id="Monolingual-or-Multilingual-Instruction-Tuning-Which-Makes-a-Better-Alpaca"><a href="#Monolingual-or-Multilingual-Instruction-Tuning-Which-Makes-a-Better-Alpaca" class="headerlink" title="Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca"></a>Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08958">http://arxiv.org/abs/2309.08958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Barry Haddow, Kenneth Heafield</li>
<li>for: 本研究旨在探讨基础大语言模型（LLM）可以如何通过具体的指令微调来开发开放式问答能力，以便应用程序如AI助手等。</li>
<li>methods: 本研究采用了Alpaca数据集和机器翻译对其进行多语言训练数据的组合，然后通过低级别适应和全参数训练来微调LLMs。</li>
<li>results: 研究发现，虽然多语言微调不对英语表现有直接影响，但它对多语言环境下LLM的稳定性至关重要。具有固定预算的情况下，一个多语言指令微调模型，只需在减少数据上进行微调，可以和每种语言单独训练的模型相比。这些发现可以为减少计算资源的情况下扩展语言支持而提供指南。<details>
<summary>Abstract</summary>
Foundational large language models (LLMs) can be instruction-tuned to develop open-ended question-answering capability, facilitating applications such as the creation of AI assistants. While such efforts are often carried out in a single language, building on prior research, we empirically analyze cost-efficient approaches of monolingual and multilingual tuning, shedding light on the efficacy of LLMs in responding to queries across monolingual and multilingual contexts. Our study employs the Alpaca dataset and machine translations of it to form multilingual training data, which is then used to tune LLMs through low-rank adaptation and full-parameter training. Comparisons reveal that multilingual tuning is not crucial for an LLM's English performance, but is key to its robustness in a multilingual environment. With a fixed budget, a multilingual instruction-tuned model, merely trained on downsampled data, can be as powerful as training monolingual models for each language. Our findings serve as a guide for expanding language support through instruction tuning with constrained computational resources.
</details>
<details>
<summary>摘要</summary>
基础大语言模型（LLM）可以通过指令调整来发展开放式问答能力，用于应用程序，如创建人工智能助手。虽然这些努力通常在单语言上进行，基于先前的研究，但我们employs the Alpaca dataset and machine translations of it to form multilingual training data，然后使用低级别适应和全参数训练来调整LLM。对比发现，在多语言环境中，多语言调整对LLM的英语性能并无关系，但对多语言环境的稳定性至关重要。假设有固定预算，一个多语言指令调整模型，只需在减样数据上进行训练，可以与每种语言 separately 训练的模型相当有力。我们的发现可以 serve as a guide for expanding language support through instruction tuning with constrained computational resources。
</details></li>
</ul>
<hr>
<h2 id="Cross-Lingual-Knowledge-Editing-in-Large-Language-Models"><a href="#Cross-Lingual-Knowledge-Editing-in-Large-Language-Models" class="headerlink" title="Cross-Lingual Knowledge Editing in Large Language Models"></a>Cross-Lingual Knowledge Editing in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08952">http://arxiv.org/abs/2309.08952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, Jiarong Xu</li>
<li>for: 本研究旨在 investigate the cross-lingual effect of knowledge editing in natural language processing.</li>
<li>methods: 我们首先收集了一个大规模的 across-lingual synthetic dataset，并对不同知识编辑方法进行了英语编辑。然后，我们对这些编辑后的模型进行了中文评估，并 vice versa。</li>
<li>results: 我们发现了编辑后模型的可靠性、通用性、地域性和可移植性在不同语言之间存在差异。此外，我们还分析了编辑后模型的不一致行为和特定挑战。<details>
<summary>Abstract</summary>
Knowledge editing aims to change language models' performance on several special cases (i.e., editing scope) by infusing the corresponding expected knowledge into them. With the recent advancements in large language models (LLMs), knowledge editing has been shown as a promising technique to adapt LLMs to new knowledge without retraining from scratch. However, most of the previous studies neglect the multi-lingual nature of some main-stream LLMs (e.g., LLaMA, ChatGPT and GPT-4), and typically focus on monolingual scenarios, where LLMs are edited and evaluated in the same language. As a result, it is still unknown the effect of source language editing on a different target language. In this paper, we aim to figure out this cross-lingual effect in knowledge editing. Specifically, we first collect a large-scale cross-lingual synthetic dataset by translating ZsRE from English to Chinese. Then, we conduct English editing on various knowledge editing methods covering different paradigms, and evaluate their performance in Chinese, and vice versa. To give deeper analyses of the cross-lingual effect, the evaluation includes four aspects, i.e., reliability, generality, locality and portability. Furthermore, we analyze the inconsistent behaviors of the edited models and discuss their specific challenges.
</details>
<details>
<summary>摘要</summary>
知识编辑目标是改善语言模型在特定场景（即编辑范围）的表现，通过涂抹相应的预期知识到其中。随着大语言模型（LLM）的发展，知识编辑被证明为一种有希望的技术，可以在不重新训练的情况下，使LML在新的知识上进行适应。然而，大多数先前的研究忽视了主流LLM的多语言特性（例如LLaMA、ChatGPT和GPT-4），通常集中于单语言enario，而不是考虑多语言enario。因此， edit Language Model在不同目标语言下的效果仍然未知。在这篇论文中，我们想要解决这种跨语言效果。specifically，我们首先收集了一个大规模的跨语言合成数据集，将英语ZsRE翻译成中文。然后，我们对不同知识编辑方法进行英语编辑，并在中文和vice versa中进行评估。为了更深入地分析跨语言效果，评估包括四个方面：可靠性、通用性、本地性和可移植性。此外，我们还分析了编辑后模型的不一致行为，并讨论了其特定挑战。
</details></li>
</ul>
<hr>
<h2 id="Universal-Metric-Learning-with-Parameter-Efficient-Transfer-Learning"><a href="#Universal-Metric-Learning-with-Parameter-Efficient-Transfer-Learning" class="headerlink" title="Universal Metric Learning with Parameter-Efficient Transfer Learning"></a>Universal Metric Learning with Parameter-Efficient Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08944">http://arxiv.org/abs/2309.08944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungyeon Kim, Donghyun Kim, Suha Kwak</li>
<li>for: 本文提出了一种新的度量学习方法，即通用度量学习（UML），该方法可以捕捉多个不同分布数据之间的关系。</li>
<li>methods: 本文提出了一种新的度量学习方法，即通用度量学习（UML），该方法包括一个预先固定的模型和两个附加模块：随机适应器和提示池。这些模块可以捕捉 dataset-specific 知识，同时避免倾斜到主导分布的偏见。</li>
<li>results: 本文的实验结果表明，使用 Parametric Universal Metric leArning（PUMA）方法可以在多个不同分布数据上实现更好的性能，并使用约 69 倍 fewer 可变参数。<details>
<summary>Abstract</summary>
A common practice in metric learning is to train and test an embedding model for each dataset. This dataset-specific approach fails to simulate real-world scenarios that involve multiple heterogeneous distributions of data. In this regard, we introduce a novel metric learning paradigm, called Universal Metric Learning (UML), which learns a unified distance metric capable of capturing relations across multiple data distributions. UML presents new challenges, such as imbalanced data distribution and bias towards dominant distributions. To address these challenges, we propose Parameter-efficient Universal Metric leArning (PUMA), which consists of a pre-trained frozen model and two additional modules, stochastic adapter and prompt pool. These modules enable to capture dataset-specific knowledge while avoiding bias towards dominant distributions. Additionally, we compile a new universal metric learning benchmark with a total of 8 different datasets. PUMA outperformed the state-of-the-art dataset-specific models while using about 69 times fewer trainable parameters.
</details>
<details>
<summary>摘要</summary>
通常在 метри学习中，对每个数据集进行特定的模型训练和测试。这种数据集特定的方法无法模拟实际世界中存在多个不同类型数据的场景。为此，我们介绍了一种新的度量学习方法，即通用度量学习（UML），它学习了一个可以捕捉多个数据分布关系的统一距离度量。UML带来了新的挑战，如数据分布偏斜和主导分布的偏袋。为解决这些挑战，我们提出了Parameter-efficient Universal Metric leArning（PUMA），它包括一个预训练的冻结模型和两个附加模块，随机适应器和提示池。这些模块允许捕捉数据集特定的知识，而不是偏袋主导分布。此外，我们编译了一个新的通用度量学习Benchmark，包括8个不同的数据集。PUMA在比较州时表现了与当前最佳数据集特定模型相比，使用了约69倍少的可训练参数。
</details></li>
</ul>
<hr>
<h2 id="An-Unified-Search-and-Recommendation-Foundation-Model-for-Cold-Start-Scenario"><a href="#An-Unified-Search-and-Recommendation-Foundation-Model-for-Cold-Start-Scenario" class="headerlink" title="An Unified Search and Recommendation Foundation Model for Cold-Start Scenario"></a>An Unified Search and Recommendation Foundation Model for Cold-Start Scenario</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08939">http://arxiv.org/abs/2309.08939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, Guannan Zhang</li>
<li>for: 这种 paper 的目的是提出一种基于多个领域的搜索和推荐系统模型，以提高系统的性能和灵活性。</li>
<li>methods: 该 paper 使用了大语言模型（LLM）来提取域无关的文本特征，并使用方面闭合合并来将 ID 特征、域无关文本特征和任务特定的多元稀热特征 merge 到获得查询和 Item 的表示。同时，该 paper 还提出了多个搜索和推荐enario 的适应Multi-task 模块来训练多个领域的基础模型。</li>
<li>results: 该 paper 通过在冷启动场景中使用 pre-train finetune 方式应用 S&amp;R Multi-Domain Foundation 模型，实现了与其他 SOTA 传输学习方法相比较好的性能。此外，S&amp;R Multi-Domain Foundation 模型已经成功应用在阿里巴巴手机应用程序中的内容查询推荐和服务卡推荐等方面。<details>
<summary>Abstract</summary>
In modern commercial search engines and recommendation systems, data from multiple domains is available to jointly train the multi-domain model. Traditional methods train multi-domain models in the multi-task setting, with shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of features, labels, and sample distributions of individual tasks. With the development of large language models, LLM can extract global domain-invariant text features that serve both search and recommendation tasks. We propose a novel framework called S\&R Multi-Domain Foundation, which uses LLM to extract domain invariant features, and Aspect Gating Fusion to merge the ID feature, domain invariant text features and task-specific heterogeneous sparse features to obtain the representations of query and item. Additionally, samples from multiple search and recommendation scenarios are trained jointly with Domain Adaptive Multi-Task module to obtain the multi-domain foundation model. We apply the S\&R Multi-Domain foundation model to cold start scenarios in the pretrain-finetune manner, which achieves better performance than other SOTA transfer learning methods. The S\&R Multi-Domain Foundation model has been successfully deployed in Alipay Mobile Application's online services, such as content query recommendation and service card recommendation, etc.
</details>
<details>
<summary>摘要</summary>
现代商业搜索引擎和推荐系统中，数据来自多个领域可以共同训练多领域模型。传统方法在多任务设定下训练多领域模型，使分享参数学习多任务之间的相似性，而任务特定参数学习多任务之间的差异。随着大语言模型的发展，LLM可以提取全局领域不变的文本特征，用于搜索和推荐任务。我们提出了一种新的框架，即S\&R多领域基础框架，使用LLM提取领域不变特征，并使用方面闭合合并模块将ID特征、领域不变文本特征和任务特有的多样化稀缺特征拼接而成查询和物品的表示。此外，我们在多个搜索和推荐场景中共同训练域 adapted multi-task模块，以获得多领域基础模型。我们在寒冷开始场景中使用预训练- fine-tune方式应用S\&R多领域基础模型，实现了与其他SOTA传输学习方法相比较好的性能。S\&R多领域基础模型已经成功部署在阿里巴巴手机应用程序内的内容查询推荐和服务卡推荐等功能中。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Neural-symbolic-System-under-Statistical-Relational-Learning"><a href="#A-Novel-Neural-symbolic-System-under-Statistical-Relational-Learning" class="headerlink" title="A Novel Neural-symbolic System under Statistical Relational Learning"></a>A Novel Neural-symbolic System under Statistical Relational Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08931">http://arxiv.org/abs/2309.08931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongran Yu, Xueyan Liu, Shirui Pan, Anchen Li, Bo Yang</li>
<li>for: The paper aims to develop a cognitive model that can exhibit human-like intellectual capabilities through neural-symbolic systems, which combine the strengths of deep learning and symbolic reasoning.</li>
<li>methods: The proposed method is a general bi-level probabilistic graphical reasoning framework called GBPGR, which leverages statistical relational learning to effectively integrate deep learning models and symbolic reasoning in a mutually beneficial manner.</li>
<li>results: The approach achieves high performance and exhibits effective generalization in both transductive and inductive tasks, as demonstrated through extensive experiments.Here’s the same information in Simplified Chinese:</li>
<li>for: 本研究旨在通过神经符号系统实现人类智能水平的认知模型。</li>
<li>methods: 提议的方法是一种通用二级概率图解架构（GBPGR），利用统计关系学来有效地结合深度学习模型和符号逻辑。</li>
<li>results: 方法在推uctive和概率任务中具有高性能和有效的泛化能力，经过广泛的实验证明。<details>
<summary>Abstract</summary>
A key objective in field of artificial intelligence is to develop cognitive models that can exhibit human-like intellectual capabilities. One promising approach to achieving this is through neural-symbolic systems, which combine the strengths of deep learning and symbolic reasoning. However, current approaches in this area have been limited in their combining way, generalization and interpretability. To address these limitations, we propose a general bi-level probabilistic graphical reasoning framework called GBPGR. This framework leverages statistical relational learning to effectively integrate deep learning models and symbolic reasoning in a mutually beneficial manner. In GBPGR, the results of symbolic reasoning are utilized to refine and correct the predictions made by the deep learning models. At the same time, the deep learning models assist in enhancing the efficiency of the symbolic reasoning process. Through extensive experiments, we demonstrate that our approach achieves high performance and exhibits effective generalization in both transductive and inductive tasks.
</details>
<details>
<summary>摘要</summary>
“一个关键目标在人工智能领域是开发人工智能模型，能够展现人类智能水平的 cognitive 能力。一种具有推进性的方法是通过神经做数学系统，结合深度学习和符号推理。但现有方法在这个领域有限，导致混合、扩展和解释性不足。为了解决这些限制，我们提出一个通用二级概率 graf 推理框架 called GBPGR。这个框架利用 Statistical Relational Learning 技术，实现深度学习模型和符号推理的共同优化。在 GBPGR 中，符号推理的结果用于修正和改善深度学习模型的预测结果。另一方面，深度学习模型帮助提高符号推理的效率。经过广泛的实验，我们证明了我们的方法在转掌和推理任务中具有高性能和有效扩展。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="DOMAIN-MilDly-COnservative-Model-BAsed-OfflINe-Reinforcement-Learning"><a href="#DOMAIN-MilDly-COnservative-Model-BAsed-OfflINe-Reinforcement-Learning" class="headerlink" title="DOMAIN: MilDly COnservative Model-BAsed OfflINe Reinforcement Learning"></a>DOMAIN: MilDly COnservative Model-BAsed OfflINe Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08925">http://arxiv.org/abs/2309.08925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao-Yin Liu, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Zhen-Qiu Feng, Hao Li, Mei-Jiang Gui, Tian-Yu Xiang, De-Xing Huang, Zeng-Guang Hou</li>
<li>for: This paper proposes a new model-based reinforcement learning algorithm called DOMAIN to address the problem of distribution shift in offline RL.</li>
<li>methods: The DOMAIN algorithm uses adaptive sampling of model samples to adjust the model data penalty and does not rely on model uncertainty estimation.</li>
<li>results: The paper shows that the DOMAIN algorithm is less conservative than previous model-based offline RL algorithms and achieves better performance than other RL algorithms on tasks that require generalization.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文提出了一种新的基于模型的强化学习算法called DOMAIN，以解决停机shift问题。</li>
<li>methods: DOMAIN算法使用适应样本的模型样本抽象来调整模型数据罚款，不需要模型不确定性估计。</li>
<li>results: 论文表明DOMAIN算法比前一代基于模型的停机RL算法更加保守，并在需要总体化的任务上达到更好的性能。<details>
<summary>Abstract</summary>
Model-based reinforcement learning (RL), which learns environment model from offline dataset and generates more out-of-distribution model data, has become an effective approach to the problem of distribution shift in offline RL. Due to the gap between the learned and actual environment, conservatism should be incorporated into the algorithm to balance accurate offline data and imprecise model data. The conservatism of current algorithms mostly relies on model uncertainty estimation. However, uncertainty estimation is unreliable and leads to poor performance in certain scenarios, and the previous methods ignore differences between the model data, which brings great conservatism. Therefore, this paper proposes a milDly cOnservative Model-bAsed offlINe RL algorithm (DOMAIN) without estimating model uncertainty to address the above issues. DOMAIN introduces adaptive sampling distribution of model samples, which can adaptively adjust the model data penalty. In this paper, we theoretically demonstrate that the Q value learned by the DOMAIN outside the region is a lower bound of the true Q value, the DOMAIN is less conservative than previous model-based offline RL algorithms and has the guarantee of security policy improvement. The results of extensive experiments show that DOMAIN outperforms prior RL algorithms on the D4RL dataset benchmark, and achieves better performance than other RL algorithms on tasks that require generalization.
</details>
<details>
<summary>摘要</summary>
模型基于的再增强学习（RL），从偏置数据集中学习环境模型，并生成更多的不同于实际环境的模型数据，已成为解决偏移 Distribution shift 在线上 RL 中的有效方法。由于模型与实际环境之间的差距，需要在算法中加入保守性来平衡准确的偏置数据和不准确的模型数据。现有算法中的保守性主要基于模型uncertainty 估计。然而，不确定性估计不可靠，导致在某些场景下表现不佳，而之前的方法忽略了模型数据之间的差异，这会带来很大的保守性。因此，这篇论文提出了一种milDly cOnservative Model-bAsed offlINe RL算法（DOMAIN），不需要估计模型uncertainty，以解决上述问题。DOMAIN 引入适应样本分布的模型样本折扣，可以适应性地调整模型数据罚款。在本论文中，我们理论上展示了 DOMAIN 外部区域上学习的 Q 值是真实 Q 值的下界，DOMAIN 比前一代模型基于的offline RL算法更加保守，并且有安全政策改进的 garant。实验结果表明，DOMAIN 在 D4RL 数据集上比前一代 RL 算法表现出色，并在需要总结的任务上达到更好的表现。
</details></li>
</ul>
<hr>
<h2 id="Exploration-of-TPUs-for-AI-Applications"><a href="#Exploration-of-TPUs-for-AI-Applications" class="headerlink" title="Exploration of TPUs for AI Applications"></a>Exploration of TPUs for AI Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08918">http://arxiv.org/abs/2309.08918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Sanmartín Carrión, Vera Prohaska</li>
<li>for: 这篇论文主要是关于 tensor processing units (TPU) 的特有硬件加速器，用于深度学习，以及其在边缘计算中的实现。</li>
<li>methods: 论文首先提供了 TPU 的概述，包括神经网络设计、通用架构、编译技术和支持框架。然后进行了云和边缘 TPU 性能对比其他相似架构芯片。</li>
<li>results: 结果显示，TPU 可以在云和边缘计算中提供显著性能提升。同时，论文还提出了在边缘 TPU 上部署更多架构的需求，以及在边缘计算中需要更多robust的比较。<details>
<summary>Abstract</summary>
Tensor Processing Units (TPUs) are specialized hardware accelerators for deep learning developed by Google. This paper explores the performance of TPU with a focus on AI and its implementation in edge computing. It first provides an overview of TPUs, specifically their design in relation to neural networks, their general architecture, compilation techniques and supporting frameworks. Furthermore, we provide a comparative analysis of Cloud and Edge TPU performance against other counterpart chip architectures. It is then discussed how TPUs can be used to speed up AI workloads. The results show that TPUs can provide significant performance improvements both in cloud and edge computing. Additionally, we address the need for further research for the deployment of more architectures in the Edge TPU, as well as the need for the development of more robust comparisons in edge computing.
</details>
<details>
<summary>摘要</summary>
tensor 处理单元 (TPU) 是 Google 开发的特циализирован硬件加速器，用于深度学习。本文通过对 TPU 的性能进行分析，特别是在 AI 和边缘计算中的应用。首先，文章提供了 TPU 的概述，包括神经网络设计、通用架构、编译技术和支持框架。然后，文章进行了云和边缘 TPU 性能的比较分析，与其他相关芯片架构进行比较。最后，文章讨论了如何使用 TPU 加速 AI 工作负荷。结果表明，TPU 可以在云和边缘计算中提供显著性能提升。此外，文章还提出了进一步研究边缘 TPU 部署的需求，以及边缘计算中更加robust的比较开发需求。
</details></li>
</ul>
<hr>
<h2 id="Bidirectional-Graph-GAN-Representing-Brain-Structure-Function-Connections-for-Alzheimer’s-Disease"><a href="#Bidirectional-Graph-GAN-Representing-Brain-Structure-Function-Connections-for-Alzheimer’s-Disease" class="headerlink" title="Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer’s Disease"></a>Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08916">http://arxiv.org/abs/2309.08916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuqiang Wang, Chen Ding</li>
<li>for: 本研究旨在探讨脑结构和功能之间的关系，以揭示脑病、如阿尔茨海默病（AD）的发生机制。</li>
<li>methods: 本研究提出了一种bidirectional graph生成对抗网络（BGGAN），用于表征脑结构和功能之间的连接。 Specifically,  InnerGCN模块被设计来使生成器使用直接和间接脑区域的特征来学习映射函数。 另外，一个名为Balancer的模块被设计来对生成器和判别器进行平衡优化。</li>
<li>results: 对ADNI数据集进行实验表明，生成的结构连接和功能连接都可以提高识别AD的准确率。此外，基于提出的模型，发现脑结构和功能之间不是一对一的对应关系。脑结构是脑功能的基础，强的结构连接通常 accompanies 强的功能连接。<details>
<summary>Abstract</summary>
The relationship between brain structure and function is critical for revealing the pathogenesis of brain disease, including Alzheimer's disease (AD). However, it is a great challenge to map brain structure-function connections due to various reasons. In this work, a bidirectional graph generative adversarial networks (BGGAN) is proposed to represent brain structure-function connections. Specifically, by designing a module incorporating inner graph convolution network (InnerGCN), the generators of BGGAN can employ features of direct and indirect brain regions to learn the mapping function between structural domain and functional domain. Besides, a new module named Balancer is designed to counterpoise the optimization between generators and discriminators. By introducing the Balancer into BGGAN, both the structural generator and functional generator can not only alleviate the issue of mode collapse but also learn complementarity of structural and functional features. Experimental results using ADNI datasets show that the both the generated structure connections and generated function connections can improve the identification accuracy of AD. More importantly, based the proposed model, it is found that the relationship between brain structure and function is not a complete one-to-one correspondence. Brain structure is the basis of brain function. The strong structural connections are almost accompanied by strong functional connections.
</details>
<details>
<summary>摘要</summary>
《Brain Structure-Function Relationship and Alzheimer's Disease》Introduction: brain structure-function relationship is crucial for understanding the pathogenesis of brain diseases, including Alzheimer's disease (AD). However, mapping brain structure-function connections is a significant challenge due to various reasons. In this work, we propose a bidirectional graph generative adversarial networks (BGGAN) to represent brain structure-function connections.Methodology:1. Inner Graph Convolution Network (InnerGCN): We design a module incorporating InnerGCN to enable the generators of BGGAN to learn the mapping function between structural domain and functional domain using features of direct and indirect brain regions.2. Balancer: To counterpoise the optimization between generators and discriminators, we introduce a new module named Balancer. This module allows both the structural generator and functional generator to alleviate the issue of mode collapse and learn complementarity of structural and functional features.Results:1. Improved identification accuracy of AD: Experimental results using ADNI datasets show that the generated structure connections and functional connections can improve the identification accuracy of AD.2. Relationship between brain structure and function: Based on the proposed model, we found that the relationship between brain structure and function is not a complete one-to-one correspondence. Brain structure is the basis of brain function, and strong structural connections are almost accompanied by strong functional connections.Conclusion:BGGAN provides a novel approach to mapping brain structure-function connections, which can be used to better understand the pathogenesis of brain diseases such as AD. The proposed model highlights the importance of considering both structural and functional features when studying brain function and disease.
</details></li>
</ul>
<hr>
<h2 id="A-Statistical-Turing-Test-for-Generative-Models"><a href="#A-Statistical-Turing-Test-for-Generative-Models" class="headerlink" title="A Statistical Turing Test for Generative Models"></a>A Statistical Turing Test for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08913">http://arxiv.org/abs/2309.08913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hayden Helm, Carey E. Priebe, Weiwei Yang</li>
<li>for: 本研究旨在量化人类和机器生成内容的分布差异，以便评估生成模型是否具备人类化能力。</li>
<li>methods: 本研究使用统计模式识别语言框架，描述当前生成模型的评估方法，并用该框架进行生成模型的评估。</li>
<li>results: 研究发现，当前的生成模型在评估上的表现有所提高，但仍有一定的差异与人类生成内容的分布。<details>
<summary>Abstract</summary>
The emergence of human-like abilities of AI systems for content generation in domains such as text, audio, and vision has prompted the development of classifiers to determine whether content originated from a human or a machine. Implicit in these efforts is an assumption that the generation properties of a human are different from that of the machine. In this work, we provide a framework in the language of statistical pattern recognition that quantifies the difference between the distributions of human and machine-generated content conditioned on an evaluation context. We describe current methods in the context of the framework and demonstrate how to use the framework to evaluate the progression of generative models towards human-like capabilities, among many axes of analysis.
</details>
<details>
<summary>摘要</summary>
人类化能力的AI系统在文本、音频和视觉等领域的内容生成中得到了广泛应用，这导致了判断内容是人类还是机器生成的分类器的发展。这种假设是人类生成的特征和机器生成的特征不同。在这项工作中，我们提供了一个基于统计模式识别语言的框架，以量化人类和机器生成的内容在评估上下文中的分布差异。我们将当前方法与此框架中的方法进行描述，并通过这个框架来评估生成模型在多个轴上的进步，包括人类化能力。
</details></li>
</ul>
<hr>
<h2 id="V2CE-Video-to-Continuous-Events-Simulator"><a href="#V2CE-Video-to-Continuous-Events-Simulator" class="headerlink" title="V2CE: Video to Continuous Events Simulator"></a>V2CE: Video to Continuous Events Simulator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08891">http://arxiv.org/abs/2309.08891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongyang Zhang, Shuyang Cui, Kaidong Chai, Haowen Yu, Subhasis Dasgupta, Upal Mahbub, Tauhidur Rahman</li>
<li>for: 本文旨在提出一种基于动态视场传感器（DVS）的视频转事件流转换方法，以提高DVS在计算机视觉任务中的表现。</li>
<li>methods: 本文提出了一种基于多视角的事件流转换方法，并采用了一系列特别设计的损失函数来提高生成的事件VOXEL的质量。此外，本文还提出了一种基于本地动态特征的时间推测策略，以准确地恢复事件时间排序和消除时间层次问题。</li>
<li>results: 根据对量化指标进行严格验证，本文的方法在所有阶段的管道中具有最高精度，可以视为当前最佳实践（SOTA）。<details>
<summary>Abstract</summary>
Dynamic Vision Sensor (DVS)-based solutions have recently garnered significant interest across various computer vision tasks, offering notable benefits in terms of dynamic range, temporal resolution, and inference speed. However, as a relatively nascent vision sensor compared to Active Pixel Sensor (APS) devices such as RGB cameras, DVS suffers from a dearth of ample labeled datasets. Prior efforts to convert APS data into events often grapple with issues such as a considerable domain shift from real events, the absence of quantified validation, and layering problems within the time axis. In this paper, we present a novel method for video-to-events stream conversion from multiple perspectives, considering the specific characteristics of DVS. A series of carefully designed losses helps enhance the quality of generated event voxels significantly. We also propose a novel local dynamic-aware timestamp inference strategy to accurately recover event timestamps from event voxels in a continuous fashion and eliminate the temporal layering problem. Results from rigorous validation through quantified metrics at all stages of the pipeline establish our method unquestionably as the current state-of-the-art (SOTA).
</details>
<details>
<summary>摘要</summary>
dynamically 视场传感器（DVS）基本解决方案在各种计算机视觉任务中受到了广泛的关注，提供了明显的优势，包括动态范围、时间分辨率和推理速度。然而，作为与活动像素感知器（APS）设备，如RGB摄像头相比较新的视觉传感器，DVS受到了充分的标注数据的缺乏问题。先前的尝试将APS数据转换为事件经常遇到问题，如实际事件与模拟事件之间的很大领域转移、缺乏量化验证和时间轴层叠问题。在本文中，我们提出了一种新的视频到事件流转换方法，考虑了DVS的特点。一系列仔细设计的损失函数可以提高生成的事件粒子质量。我们还提出了一种新的本地动态感知时间推断策略，可以准确地从事件粒子中恢复事件时间戳，消除时间轴层叠问题。经过严格的验证，我们的方法在所有阶段的管道中都有明显的优势，无疑成为当前最佳实践（SOTA）。
</details></li>
</ul>
<hr>
<h2 id="GCL-Gradient-Guided-Contrastive-Learning-for-Medical-Image-Segmentation-with-Multi-Perspective-Meta-Labels"><a href="#GCL-Gradient-Guided-Contrastive-Learning-for-Medical-Image-Segmentation-with-Multi-Perspective-Meta-Labels" class="headerlink" title="GCL: Gradient-Guided Contrastive Learning for Medical Image Segmentation with Multi-Perspective Meta Labels"></a>GCL: Gradient-Guided Contrastive Learning for Medical Image Segmentation with Multi-Perspective Meta Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08888">http://arxiv.org/abs/2309.08888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan Wu, Jintai Chen, Jiahuan Yan, Yiheng Zhu, Danny Z. Chen, Jian Wu</li>
<li>for: 降低错误标签成本，为医疗影像分类 зада增加效率的方法</li>
<li>methods: 利用Gradient Mitigator方法与Gradient Filter方法，将多种多面 semantics整合为一个单一的高级semantic recognition能力</li>
<li>results: 透过实验证明，新方法GCL可以从有限标签的情况下，学习出有用的医疗影像表示，并且在不同数据集上展现出良好的一致性和普遍性<details>
<summary>Abstract</summary>
Since annotating medical images for segmentation tasks commonly incurs expensive costs, it is highly desirable to design an annotation-efficient method to alleviate the annotation burden. Recently, contrastive learning has exhibited a great potential in learning robust representations to boost downstream tasks with limited labels. In medical imaging scenarios, ready-made meta labels (i.e., specific attribute information of medical images) inherently reveal semantic relationships among images, which have been used to define positive pairs in previous work. However, the multi-perspective semantics revealed by various meta labels are usually incompatible and can incur intractable "semantic contradiction" when combining different meta labels. In this paper, we tackle the issue of "semantic contradiction" in a gradient-guided manner using our proposed Gradient Mitigator method, which systematically unifies multi-perspective meta labels to enable a pre-trained model to attain a better high-level semantic recognition ability. Moreover, we emphasize that the fine-grained discrimination ability is vital for segmentation-oriented pre-training, and develop a novel method called Gradient Filter to dynamically screen pixel pairs with the most discriminating power based on the magnitude of gradients. Comprehensive experiments on four medical image segmentation datasets verify that our new method GCL: (1) learns informative image representations and considerably boosts segmentation performance with limited labels, and (2) shows promising generalizability on out-of-distribution datasets.
</details>
<details>
<summary>摘要</summary>
自带笔迹标注医疗图像分割任务的成本高昂，因此极其感到需要设计一种笔迹效率的方法，以减轻笔迹负担。近年来，对比学习表现出了巨大的潜力，可以通过有限的标签来提高后续任务的性能。在医疗图像场景中，已有的meta标签（即医疗图像特征信息）自然地暴露了图像之间的semantic关系，这些meta标签在过去的工作中已经被用来定义正例对。然而，医疗图像场景中的多个meta标签之间的semantic关系通常是不兼容的，这会导致"semantic contradiction"现象，从而使得组合不同meta标签的"semantic contradiction"变得不可持续。在这篇论文中，我们解决了"semantic contradiction"问题，我们提出了一种 Gradient Mitigator 方法，该方法可以系统地统一多个视角的 meta标签，使得预训练模型能够更好地捕捉高级别semantic认知能力。此外，我们强调了分割预训练中的细腻分辨率的重要性，我们开发了一种 Gradient Filter 方法，该方法可以根据梯度的大小来动态屏蔽图像对的最有分辨力的像素对。我们在四个医疗图像分割数据集进行了广泛的实验，结果表明，我们的新方法 GCL 可以：（1）学习有用的图像表示，对有限标签进行分割任务进行明显的提升，以及（2）在不同数据集上表现出良好的普适性。
</details></li>
</ul>
<hr>
<h2 id="Solving-Satisfiability-Modulo-Counting-for-Symbolic-and-Statistical-AI-Integration-With-Provable-Guarantees"><a href="#Solving-Satisfiability-Modulo-Counting-for-Symbolic-and-Statistical-AI-Integration-With-Provable-Guarantees" class="headerlink" title="Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration With Provable Guarantees"></a>Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration With Provable Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08883">http://arxiv.org/abs/2309.08883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinzhao Li, Nan Jiang, Yexiang Xue</li>
<li>for: 这篇论文主要是解决 Symbolic Artificial Intelligence 和 Statistical Artificial Intelligence 的交叉问题，即 Satisfiability Modulo Counting (SMC) 问题。</li>
<li>methods: 该论文提出了一种基于 NP-oracle 的多项式算法 XOR-SMC，可以解决高度NP-完全的 SMC 问题，并提供了常量近似保证。 XOR-SMC 将 SMC 问题转化为满足随机 XOR 约束的 SAT 方程问题。</li>
<li>results:  experiments 表明，XOR-SMC 能够在解决重要的 SMC 问题时，与基线相比，提供更好的近似解决方案，并且其近似精度较高。<details>
<summary>Abstract</summary>
Satisfiability Modulo Counting (SMC) encompasses problems that require both symbolic decision-making and statistical reasoning. Its general formulation captures many real-world problems at the intersection of symbolic and statistical Artificial Intelligence. SMC searches for policy interventions to control probabilistic outcomes. Solving SMC is challenging because of its highly intractable nature($\text{NP}^{\text{PP}$-complete), incorporating statistical inference and symbolic reasoning. Previous research on SMC solving lacks provable guarantees and/or suffers from sub-optimal empirical performance, especially when combinatorial constraints are present. We propose XOR-SMC, a polynomial algorithm with access to NP-oracles, to solve highly intractable SMC problems with constant approximation guarantees. XOR-SMC transforms the highly intractable SMC into satisfiability problems, by replacing the model counting in SMC with SAT formulae subject to randomized XOR constraints. Experiments on solving important SMC problems in AI for social good demonstrate that XOR-SMC finds solutions close to the true optimum, outperforming several baselines which struggle to find good approximations for the intractable model counting in SMC.
</details>
<details>
<summary>摘要</summary>
满足性模ulo counting（SMC）包括问题需要 Both symbolic decision-making 和统计学推理。 Its general formulation capture 了许多实际世界问题的交叉点，位于符号AI和统计AI之间。 SMC寻找策略性的输入，以控制 probabilistic outcomes。解决 SMC 是困难的，因为它具有非常困难的性质（NP 完全），包括统计推理和符号推理。 Previous research on SMC solving 缺乏可证明的保证和/或具有不佳的实际性能，特别是在存在 combinatorial constraints 时。 We propose XOR-SMC，一种可 polynomials 算法，使用 NP-oracles，解决高度困难的 SMC 问题，并提供常量approximation guarantees。 XOR-SMC 将高度困难的 SMC 转换成满足性问题，通过将 SMC 中的模型计数换成 SAT 方程Subject to randomized XOR 约束。实验表明，XOR-SMC 能够在解决重要的 SMC 问题中，提供近似 true optimum 的解决方案，超越了许多基准值，它们在 intractable model counting 中的 SMC 中寻找具有好的approximation 的解决方案。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-4-with-Code-Interpreter-can-be-used-to-solve-introductory-college-level-vector-calculus-and-electromagnetism-problems"><a href="#ChatGPT-4-with-Code-Interpreter-can-be-used-to-solve-introductory-college-level-vector-calculus-and-electromagnetism-problems" class="headerlink" title="ChatGPT-4 with Code Interpreter can be used to solve introductory college-level vector calculus and electromagnetism problems"></a>ChatGPT-4 with Code Interpreter can be used to solve introductory college-level vector calculus and electromagnetism problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08881">http://arxiv.org/abs/2309.08881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanuj Kumar, Mikhail A. Kats</li>
<li>for: 这个论文是为了测试 chatGPT 3.5、4、4 with Code Interpreter 在大学二年级电工和电磁学问题上的性能。</li>
<li>methods: 作者使用了一组13个问题，并使用了不同的 chatGPT 实例来解决这些问题多次。</li>
<li>results: 结果显示，使用 Code Interpreter 的 chatGPT 4 可以成功解决大多数问题，而不使用 Code Interpreter 的 chatGPT 3.5 和 chatGPT 4 的性能较差。<details>
<summary>Abstract</summary>
We evaluated ChatGPT 3.5, 4, and 4 with Code Interpreter on a set of college-level engineering-math and electromagnetism problems, such as those often given to sophomore electrical engineering majors. We selected a set of 13 problems, and had ChatGPT solve them multiple times, using a fresh instance (chat) each time. We found that ChatGPT-4 with Code Interpreter was able to satisfactorily solve most problems we tested most of the time -- a major improvement over the performance of ChatGPT-4 (or 3.5) without Code Interpreter. The performance of ChatGPT was observed to be somewhat stochastic, and we found that solving the same problem N times in new ChatGPT instances and taking the most-common answer was an effective strategy. Based on our findings and observations, we provide some recommendations for instructors and students of classes at this level.
</details>
<details>
<summary>摘要</summary>
我们对 chatGPT 3.5、4 和 4 进行了测试，使用 college-level 工程学和电磁学问题，类似于第二年电机工程学生常 receives 的问题。我们选择了 13 个问题，让 chatGPT 多次解决这些问题，每次使用新的 chat 实例。我们发现，在使用 Code Interpreter 的情况下，chatGPT 4 能够大幅提高解决这些问题的能力，比不使用 Code Interpreter 的情况下要好。chatGPT 的性能被观察到有一定的随机性，我们发现，对同一个问题多次解决，并取得最常见的答案是一个有效的策略。根据我们的发现和观察，我们对教师和学生提供了一些建议。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-H-infinity-Control-with-a-Real-Time-and-Efficient-Reinforcement-Learning-Algorithm-An-Application-to-Autonomous-Mobility-on-Demand-Systems"><a href="#Data-Driven-H-infinity-Control-with-a-Real-Time-and-Efficient-Reinforcement-Learning-Algorithm-An-Application-to-Autonomous-Mobility-on-Demand-Systems" class="headerlink" title="Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems"></a>Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08880">http://arxiv.org/abs/2309.08880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Aalipour, Alireza Khani</li>
<li>for: 这篇论文旨在开发一个基于Q学习的无模型实时控制算法，用于解决线性碎时系统的H$_{\infty}$控制问题。</li>
<li>methods: 提议的算法使用了Q学习方法，并且在线性碎时系统中实现了实时和数据效率的控制。computational complexity降低至$\mathcal{O}(\underline{q}^2)$，比Literature中的$\mathcal{O}(\underline{q}^3)$更低。</li>
<li>results: 实验研究显示，提议的算法可以实现实时和数据效率的控制，并且不需要初始稳定政策。在一个实际应用中，将提议的算法应用到了一个自主移动需求系统中，并且得到了良好的效果。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a class of artificial intelligence algorithms being used to design adaptive optimal controllers through online learning. This paper presents a model-free, real-time, data-efficient Q-learning-based algorithm to solve the H$_{\infty}$ control of linear discrete-time systems. The computational complexity is shown to reduce from $\mathcal{O}(\underline{q}^3)$ in the literature to $\mathcal{O}(\underline{q}^2)$ in the proposed algorithm, where $\underline{q}$ is quadratic in the sum of the size of state variables, control inputs, and disturbance. An adaptive optimal controller is designed and the parameters of the action and critic networks are learned online without the knowledge of the system dynamics, making the proposed algorithm completely model-free. Also, a sufficient probing noise is only needed in the first iteration and does not affect the proposed algorithm. With no need for an initial stabilizing policy, the algorithm converges to the closed-form solution obtained by solving the Riccati equation. A simulation study is performed by applying the proposed algorithm to real-time control of an autonomous mobility-on-demand (AMoD) system for a real-world case study to evaluate the effectiveness of the proposed algorithm.
</details>
<details>
<summary>摘要</summary>
“强化学习（RL）是一类人工智能算法，用于设计适应最佳控制器通过在线学习。本文提出了一种无模型、实时、数据有效的Q学习基于算法，用于解决线性时间隔系统的H$_{\infty}$控制问题。在文章中，我们显示了计算复杂性从$\mathcal{O}(\underline{q}^3)$降低到$\mathcal{O}(\underline{q}^2)$，其中$\underline{q}$是状态变量、控制输入和干扰的总和的二次函数。我们实现了无模型的优化控制器，并在线学习行为和评价网络的参数，不需要系统动力学模型的知识，因此完全无模型。此外，我们只需在第一轮执行充分的探测噪声，并不影响提议算法。无需初始稳定策略，算法可以到达由解决里氏方程得到的闭合形解。我们对实时控制一个真实的自动化移动需求系统进行了一个实验研究，以评估提议算法的有效性。”Note: "Simplified Chinese" is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="PDFTriage-Question-Answering-over-Long-Structured-Documents"><a href="#PDFTriage-Question-Answering-over-Long-Structured-Documents" class="headerlink" title="PDFTriage: Question Answering over Long, Structured Documents"></a>PDFTriage: Question Answering over Long, Structured Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08872">http://arxiv.org/abs/2309.08872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, Ryan A. Rossi, Franck Dernoncourt</li>
<li>for: 本研究旨在解决大语言模型（LLM）在文档问答（QA）中遇到的问题，即当文档不能适应LLM的小上下文长度时。</li>
<li>methods: 本研究提议一种名为PDFTriage的方法，可以基于结构或内容来检索文档上下文。</li>
<li>results: 我们的实验显示，使用PDFTriage可以在多种问题类型中提高文档QA的效果，而现有的检索-加以LLM则失败。此外，我们还发布了一个包含80个结构化文档和900多个人工生成的问题的数据集，以便进一步研究这一基本问题。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmented models across several classes of questions where existing retrieval-augmented LLMs fail. To facilitate further research on this fundamental problem, we release our benchmark dataset consisting of 900+ human-generated questions over 80 structured documents from 10 different categories of question types for document QA.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MHLAT-Multi-hop-Label-wise-Attention-Model-for-Automatic-ICD-Coding"><a href="#MHLAT-Multi-hop-Label-wise-Attention-Model-for-Automatic-ICD-Coding" class="headerlink" title="MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding"></a>MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08868">http://arxiv.org/abs/2309.08868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwen Duan, Han Jiang, Ying Yu</li>
<li>for: 医疗记录编码（ICD编码）任务是将医疗记录中的病理诊断代码分配给临床病理诊断。</li>
<li>methods: 我们提出了一种简单 yet effective 的模型，即 Multi-Hop Label-wise ATtention（MHLAT），其中使用多步标签层权重来获得更精准和有用的表示。</li>
<li>results: 我们在三个 MIMIC 数据集上进行了广泛的实验，并证明了我们的方法在七个指标中具有显著更好或竞争性表现，并且具有更少的参数优化。<details>
<summary>Abstract</summary>
International Classification of Diseases (ICD) coding is the task of assigning ICD diagnosis codes to clinical notes. This can be challenging given the large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000 tokens). However, unlike the single-pass reading process in previous works, humans tend to read the text and label definitions again to get more confident answers. Moreover, although pretrained language models have been used to address these problems, they suffer from huge memory usage. To address the above problems, we propose a simple but effective model called the Multi-Hop Label-wise ATtention (MHLAT), in which multi-hop label-wise attention is deployed to get more precise and informative representations. Extensive experiments on three benchmark MIMIC datasets indicate that our method achieves significantly better or competitive performance on all seven metrics, with much fewer parameters to optimize.
</details>
<details>
<summary>摘要</summary>
国际疾病分类 (ICD) 编码是将临床笔记中的 ICD 诊断代码赋予的任务。这可能是由于大量的标签（约9,000）和长文本（达8,000个token）所带来的挑战。然而，与之前的单一扫描过程不同，人类通常会重读文本和标签定义以获取更加自信的答案。此外，尽管已使用预训练语言模型来解决这些问题，但它们受到巨大的内存使用带来问题。为解决上述问题，我们提议一种简单 yet 有效的模型，称为多跳标签wise ATtention (MHLAT)，其中多跳标签wise ATtention 被部署以获取更加精确和有用的表示。我们在三个 MIMIC 数据集上进行了广泛的实验，结果显示，我们的方法在七个指标中均达到了显著更好或竞争性的性能，而且具有许多参数优化的多少。
</details></li>
</ul>
<hr>
<h2 id="Trajectory-Tracking-Control-of-Skid-Steering-Mobile-Robots-with-Slip-and-Skid-Compensation-using-Sliding-Mode-Control-and-Deep-Learning"><a href="#Trajectory-Tracking-Control-of-Skid-Steering-Mobile-Robots-with-Slip-and-Skid-Compensation-using-Sliding-Mode-Control-and-Deep-Learning" class="headerlink" title="Trajectory Tracking Control of Skid-Steering Mobile Robots with Slip and Skid Compensation using Sliding-Mode Control and Deep Learning"></a>Trajectory Tracking Control of Skid-Steering Mobile Robots with Slip and Skid Compensation using Sliding-Mode Control and Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08863">http://arxiv.org/abs/2309.08863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Payam Nourizadeh, Fiona J Stevens McFadden, Will N Browne</li>
<li>For: 这篇研究旨在提供一种可行的线上运行于开放环境中的游戏机器人运行控制系统，以减少游戏机器人在不可预测的环境中的追踪错误。* Methods: 本研究使用陡缓度控制技术设计了一个可靠的轨迹追踪系统，并将两个先前开发的深度学习模型 [1], [2] 组合到控制反馈循环中，以实时估算游戏机器人的滑行和不适合的滑行，并将补偿值传递到补偿器中。* Results: 实验结果显示，提案的控制器与补偿器可以将轨迹追踪系统的表现提高超过27%。<details>
<summary>Abstract</summary>
Slip and skid compensation is crucial for mobile robots' navigation in outdoor environments and uneven terrains. In addition to the general slipping and skidding hazards for mobile robots in outdoor environments, slip and skid cause uncertainty for the trajectory tracking system and put the validity of stability analysis at risk. Despite research in this field, having a real-world feasible online slip and skid compensation is still challenging due to the complexity of wheel-terrain interaction in outdoor environments. This paper presents a novel trajectory tracking technique with real-world feasible online slip and skid compensation at the vehicle-level for skid-steering mobile robots in outdoor environments. The sliding mode control technique is utilized to design a robust trajectory tracking system to be able to consider the parameter uncertainty of this type of robot. Two previously developed deep learning models [1], [2] are integrated into the control feedback loop to estimate the robot's slipping and undesired skidding and feed the compensator in a real-time manner. The main advantages of the proposed technique are (1) considering two slip-related parameters rather than the conventional three slip parameters at the wheel-level, and (2) having an online real-world feasible slip and skid compensator to be able to reduce the tracking errors in unforeseen environments. The experimental results show that the proposed controller with the slip and skid compensator improves the performance of the trajectory tracking system by more than 27%.
</details>
<details>
<summary>摘要</summary>
滑动和滑倒补偿是移动机器人在户外环境中的导航关键，它们会导致轨迹追踪系统的不确定性和稳定分析的风险。尽管在这一领域进行了大量研究，但实现在线可行的滑动和滑倒补偿仍然是一个挑战，因为轮胎与地面的互动在户外环境中非常复杂。这篇论文提出了一种新的轨迹追踪技术，使用滑模控制技术设计一个可靠的轨迹追踪系统，并将两个先前开发的深度学习模型[1]、[2]integrated into the control feedback loop来估计机器人的滑动和不良滑倒，并在实时 manner中将其传递给补偿器。该技术的主要优点包括：一、考虑了机器人的两个滑动参数而不是传统的三个滑动参数，二、在实时可行的情况下实现了滑动和滑倒的补偿，从而降低了轨迹追踪系统的跟踪错误。实验结果显示，提案的控制器与滑动和滑倒补偿器可以提高轨迹追踪系统的性能，比例超过27%。
</details></li>
</ul>
<hr>
<h2 id="Emerging-Approaches-for-THz-Array-Imaging-A-Tutorial-Review-and-Software-Tool"><a href="#Emerging-Approaches-for-THz-Array-Imaging-A-Tutorial-Review-and-Software-Tool" class="headerlink" title="Emerging Approaches for THz Array Imaging: A Tutorial Review and Software Tool"></a>Emerging Approaches for THz Array Imaging: A Tutorial Review and Software Tool</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08844">http://arxiv.org/abs/2309.08844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josiah W. Smith, Murat Torlak</li>
<li>for: 该文章主要是为了介绍在近场TERAHertz频率域中的THzSynthetic Aperture Radar（SAR）系统和算法。</li>
<li>methods: 该文章提出了一种组合信号处理和机器学习技术的新算法，以及一些传统的CLASSICAL和数据驱动的THz SAR算法，包括物体检测和SAR图像超分辨。</li>
<li>results: 该文章提出了一些Future研究方向，包括系统和算法标准化测试、采用当前最佳的深度学习技术、信号处理优化机器学习算法和гибридного数据驱动信号处理算法。<details>
<summary>Abstract</summary>
Accelerated by the increasing attention drawn by 5G, 6G, and Internet of Things applications, communication and sensing technologies have rapidly evolved from millimeter-wave (mmWave) to terahertz (THz) in recent years. Enabled by significant advancements in electromagnetic (EM) hardware, mmWave and THz frequency regimes spanning 30 GHz to 300 GHz and 300 GHz to 3000 GHz, respectively, can be employed for a host of applications. The main feature of THz systems is high-bandwidth transmission, enabling ultra-high-resolution imaging and high-throughput communications; however, challenges in both the hardware and algorithmic arenas remain for the ubiquitous adoption of THz technology. Spectra comprising mmWave and THz frequencies are well-suited for synthetic aperture radar (SAR) imaging at sub-millimeter resolutions for a wide spectrum of tasks like material characterization and nondestructive testing (NDT). This article provides a tutorial review of systems and algorithms for THz SAR in the near-field with an emphasis on emerging algorithms that combine signal processing and machine learning techniques. As part of this study, an overview of classical and data-driven THz SAR algorithms is provided, focusing on object detection for security applications and SAR image super-resolution. We also discuss relevant issues, challenges, and future research directions for emerging algorithms and THz SAR, including standardization of system and algorithm benchmarking, adoption of state-of-the-art deep learning techniques, signal processing-optimized machine learning, and hybrid data-driven signal processing algorithms...
</details>
<details>
<summary>摘要</summary>
带动了5G、6G和物联网应用的增加关注，通信和感测技术在最近几年内快速发展从毫米波（mmWave）频率范围到tera响（THz）。通过电romagnetic（EM）硬件的重要进步，mmWave和THz频率范围分别是30GHz至300GHz和300GHz至3000GHz可以用于多种应用。THz系统的主要特点是高频带宽传输，使得超高分辨率成像和高速通信 possible;但是，硬件和算法领域中的挑战还需要解决才能广泛采用THz技术。包括mmWave和THz频率的spectrum适用于sub-millimeter分辨率的Synthetic Aperture Radar（SAR）成像，用于各种任务，如材料Characterization和非 destruктив测试（NDT）。本文提供了关于THz SAR的 tutorials review，强调emerging算法的发展，包括Signal Processing和机器学习技术的结合。本文还提供了经典和数据驱动THz SAR算法的概述，专注于安全应用中的对象探测。此外，我们还讨论了相关的问题、挑战和未来研究方向，包括系统和算法标准化、采用当前最佳的深度学习技术、Signal Processing优化的机器学习算法和混合数据驱动Signal Processing算法。
</details></li>
</ul>
<hr>
<h2 id="Bias-and-Fairness-in-Chatbots-An-Overview"><a href="#Bias-and-Fairness-in-Chatbots-An-Overview" class="headerlink" title="Bias and Fairness in Chatbots: An Overview"></a>Bias and Fairness in Chatbots: An Overview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08836">http://arxiv.org/abs/2309.08836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jintang Xue, Yun-Cheng Wang, Chengwei Wei, Xiaofeng Liu, Jonghye Woo, C. -C. Jay Kuo</li>
<li>for: 本研究旨在提供一份对聊天机器人系统偏见和公平性的全面综述，以帮助开发者更好地设计和实现公平和无偏见的聊天机器人系统。</li>
<li>methods: 本研究使用了大量的文献综述和分析方法，检视了聊天机器人系统的历史和类别，分析了偏见的来源和应用中的可能的危害，并考虑了设计公平和无偏见的聊天机器人系统的因素。</li>
<li>results: 本研究结果表明，现代聊天机器人系统具有更高的功能和应用前景，但也存在偏见和公平性的担忧。通过分析偏见的来源和应用中的影响，以及考虑设计公平和无偏见的因素，可以更好地设计和实现公平和无偏见的聊天机器人系统。<details>
<summary>Abstract</summary>
Chatbots have been studied for more than half a century. With the rapid development of natural language processing (NLP) technologies in recent years, chatbots using large language models (LLMs) have received much attention nowadays. Compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. There are however, bias and fairness concerns in modern chatbot design. Due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. Thus, a comprehensive overview on bias and fairness in chatbot systems is given in this paper. The history of chatbots and their categories are first reviewed. Then, bias sources and potential harms in applications are analyzed. Considerations in designing fair and unbiased chatbot systems are examined. Finally, future research directions are discussed.
</details>
<details>
<summary>摘要</summary>
chatbots 已经被研究了 более than half a century。随着自然语言处理（NLP）技术的快速发展，使用大型语言模型（LLMs）的 chatbots 在最近几年收到了很多关注。相比传统的 chatbots，现代 chatbots 更加强大，已经在实际应用中使用。然而，现代 chatbots 中存在偏见和公平问题。由于庞大的训练数据、极大的模型大小和解释性的缺失，现代 chatbots 的偏见缓和和公平保持是挑战。这个论文提供了对偏见和公平问题在 chatbot 系统中的全面回顾。首先，摘要了 chatbots 的历史和类别。然后，分析了偏见的来源和应用中的潜在危害。考虑了设计公平和不偏见 chatbot 系统的因素。最后，讨论了未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="SLIDE-Reference-free-Evaluation-for-Machine-Translation-using-a-Sliding-Document-Window"><a href="#SLIDE-Reference-free-Evaluation-for-Machine-Translation-using-a-Sliding-Document-Window" class="headerlink" title="SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window"></a>SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08832">http://arxiv.org/abs/2309.08832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikas Raunak, Tom Kocmi, Matt Post</li>
<li>for: 这篇论文是关于语义评估的研究，旨在检验文档中的句子上是否可以提供同样的信息，如同人工参考。</li>
<li>methods: 这篇论文使用了一个新的评估指标，即SLIDE（SLiding Document Evaluator），它在文档中的块 Sentence 上使用了一个滑动窗口，将每个块传递给一个未修改的、市场上可得的质量评估模型进行评估。</li>
<li>results: 研究发现，SLIDE 可以达到高级系统精度，在某些情况下，甚至可以与人工参考 metric 减少差距。这表明，文档中的源Context 可以提供同样的信息，如同人工参考。<details>
<summary>Abstract</summary>
Reference-based metrics that operate at the sentence level typically outperform quality estimation metrics, which have access only to the source and system output. This is unsurprising, since references resolve ambiguities that may be present in the source. We investigate whether additional source context can effectively substitute for a reference. We present a metric, SLIDE (SLiding Document Evaluator), which operates on blocks of sentences using a window that slides over each document in the test set, feeding each chunk into an unmodified, off-the-shelf quality estimation model. We find that SLIDE obtains significantly higher pairwise system accuracy than its sentence-level baseline, in some cases even eliminating the gap with reference-base metrics. This suggests that source context may provide the same information as a human reference.
</details>
<details>
<summary>摘要</summary>
通常情况下，参考基于的度量器在句子水平上表现比质量估计度量器更好，这不奇怪，因为参考可以解决源文本中的歧义。我们研究了 Whether additional source context can effectively substitute for a reference. We present a metric, SLIDE (SLiding Document Evaluator), which operates on blocks of sentences using a window that slides over each document in the test set, feeding each chunk into an unmodified, off-the-shelf quality estimation model. We find that SLIDE obtains significantly higher pairwise system accuracy than its sentence-level baseline, in some cases even eliminating the gap with reference-based metrics. This suggests that source context may provide the same information as a human reference.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="S3-DST-Structured-Open-Domain-Dialogue-Segmentation-and-State-Tracking-in-the-Era-of-LLMs"><a href="#S3-DST-Structured-Open-Domain-Dialogue-Segmentation-and-State-Tracking-in-the-Era-of-LLMs" class="headerlink" title="S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs"></a>S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08827">http://arxiv.org/abs/2309.08827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarkar Snigdha Sarathi Das, Chirag Shah, Mengting Wan, Jennifer Neville, Longqi Yang, Reid Andersen, Georg Buscher, Tara Safavi</li>
<li>for: 提高open-domain对话系统中 Dialogue State Tracking（DST）的精度和 robustness，以适应大语言模型（LLM）驱动的对话系统中的复杂性和多样性。</li>
<li>methods: 提出了一种joint dialogue segmentation和state tracking的方法，使用Pre-Analytical Recollection机制来改进长期上下文跟踪。</li>
<li>results: 在一个Proprietary anonymous open-domain对话数据集以及公共可用的DST和分割数据集上进行了评估，与现状态的最佳方法进行比较，结果表明S3-DST在joint segmentation和state tracking中具有强大和稳定的性能。<details>
<summary>Abstract</summary>
The traditional Dialogue State Tracking (DST) problem aims to track user preferences and intents in user-agent conversations. While sufficient for task-oriented dialogue systems supporting narrow domain applications, the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues. These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts. To handle these intricacies arising from evolving LLM-based chat systems, we propose joint dialogue segmentation and state tracking per segment in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a true open-domain dialogue system, we propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking. To demonstrate the efficacy of our proposed approach in joint segmentation and state tracking, we evaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as well as publicly available DST and segmentation datasets. Across all datasets and settings, S3-DST consistently outperforms the state-of-the-art, demonstrating its potency and robustness the next generation of LLM-based chat systems.
</details>
<details>
<summary>摘要</summary>
traditional Dialogue State Tracking (DST) problem aims to track user preferences and intents in user-agent conversations. While sufficient for task-oriented dialogue systems supporting narrow domain applications, the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues. These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts. To handle these intricacies arising from evolving LLM-based chat systems, we propose joint dialogue segmentation and state tracking per segment in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a true open-domain dialogue system, we propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking. To demonstrate the efficacy of our proposed approach in joint segmentation and state tracking, we evaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as well as publicly available DST and segmentation datasets. Across all datasets and settings, S3-DST consistently outperforms the state-of-the-art, demonstrating its potency and robustness for the next generation of LLM-based chat systems.Here's the breakdown of the translation:* "traditional Dialogue State Tracking (DST) problem" becomes "传统的对话状态追踪问题" (traditional DST problem)* "aims to track user preferences and intents in user-agent conversations" becomes "目标是跟踪用户首选和意图在用户代理对话中" (targets tracking user preferences and intents in user-agent conversations)* "While sufficient for task-oriented dialogue systems supporting narrow domain applications" becomes "然而对于支持窄领域应用的任务导向对话系统来说， sufficient" (However, for task-oriented dialogue systems supporting narrow domain applications, sufficient)* "the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues" becomes "LLM基于对话系统的出现引入了许多实际世界中的复杂性，在开放领域对话中" (The advent of LLM-based chat systems has introduced many complexities in open-domain dialogues)* "These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts" becomes "这些复杂性表现为对话中的增加复杂性，更长的对话会话，涵盖更多的话题，以及更频繁的上下文转换" (These complexities manifest as increased complexity in contextual interactions, longer dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts)* "To handle these intricacies arising from evolving LLM-based chat systems" becomes "面对这些来自演进的 LLM 基于对话系统的复杂性" (To handle these complexities arising from evolving LLM-based chat systems)* "we propose joint dialogue segmentation and state tracking per segment in open-domain dialogue systems" becomes "我们提议在开放领域对话系统中实现对话段化和状态追踪" (We propose joint dialogue segmentation and state tracking in open-domain dialogue systems)* "Assuming a zero-shot setting appropriate to a true open-domain dialogue system" becomes "假设真正的开放领域对话系统的零枪射设定" (Assuming a zero-shot setting appropriate to a true open-domain dialogue system)* "we propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking" becomes "我们提议 S3-DST，一种基于 Pre-Analytical Recollection 的结构化提示技术，用于改进长上下文跟踪" (We propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking)* "To demonstrate the efficacy of our proposed approach in joint segmentation and state tracking" becomes "用于证明我们提议的方法在对话段化和状态追踪中的有效性" (To demonstrate the efficacy of our proposed approach in joint segmentation and state tracking)* "we evaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as well as publicly available DST and segmentation datasets" becomes "我们在一个 propriety 隐私化的开放领域对话数据集上评估 S3-DST，以及公共可用的 DST 和分段数据集" (We evaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as well as publicly available DST and segmentation datasets)* "Across all datasets and settings, S3-DST consistently outperforms the state-of-the-art" becomes "在所有数据集和设定下，S3-DST 一直表现出优于当前领先的状态" (Across all datasets and settings, S3-DST consistently outperforms the state-of-the-art)* "demonstrating its potency and robustness for the next generation of LLM-based chat systems" becomes "这种表现力和可靠性为下一代 LLM 基于对话系统的发展提供了启示" (demonstrating its potency and robustness for the next generation of LLM-based chat systems)
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Post-hoc-Classifiers-under-Prior-Shifts"><a href="#Distributionally-Robust-Post-hoc-Classifiers-under-Prior-Shifts" class="headerlink" title="Distributionally Robust Post-hoc Classifiers under Prior Shifts"></a>Distributionally Robust Post-hoc Classifiers under Prior Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08825">http://arxiv.org/abs/2309.08825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weijiaheng/drops">https://github.com/weijiaheng/drops</a></li>
<li>paper_authors: Jiaheng Wei, Harikrishna Narasimhan, Ehsan Amid, Wen-Sheng Chu, Yang Liu, Abhishek Kumar</li>
<li>for: 本研究旨在强化机器学习模型对分布变化的抗预测能力。</li>
<li>methods: 我们提出了一种极其轻量级的后处理方法，通过在预训练模型上计算并应用批处理调整来减少一个目标分布下的抗预测损失。</li>
<li>results: 我们的方法可以提供减少分布变化导致的抗预测损失的保证，并且在实际实现中具有强大的表现。<details>
<summary>Abstract</summary>
The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization objective is inspired by a natural notion of robustness to controlled distribution shifts. Our method comes with provable guarantees and empirically makes a strong case for distributional robust post-hoc classifiers. An empirical implementation is available at https://github.com/weijiaheng/Drops.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GPT-as-a-Baseline-for-Recommendation-Explanation-Texts"><a href="#GPT-as-a-Baseline-for-Recommendation-Explanation-Texts" class="headerlink" title="GPT as a Baseline for Recommendation Explanation Texts"></a>GPT as a Baseline for Recommendation Explanation Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08817">http://arxiv.org/abs/2309.08817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joyce Zhou, Thorsten Joachims</li>
<li>for: 这个研究探讨了现代模型生成的电影推荐文本解释如何帮助用户，以及用户对不同组成部分的评价。</li>
<li>methods: 研究使用现代自然语言处理技术生成电影推荐文本解释，并对用户的评价进行分析。</li>
<li>results: 研究发现参与者对电影推荐文本解释的评价没有显著差异，但参与者对已经见过的电影的评价更高。此外，参与者 также标记了电影评论文本中重要的特征。<details>
<summary>Abstract</summary>
In this work, we establish a baseline potential for how modern model-generated text explanations of movie recommendations may help users, and explore what different components of these text explanations that users like or dislike, especially in contrast to existing human movie reviews. We found that participants gave no significantly different rankings between movies, nor did they give significantly different individual quality scores to reviews of movies that they had never seen before. However, participants did mark reviews as significantly better when they were movies they had seen before. We also explore specific aspects of movie review texts that participants marked as important for each quality. Overall, we establish that modern LLMs are a promising source of recommendation explanations, and we intend on further exploring personalizable text explanations in the future.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们建立了现代模型生成的电影推荐文本解释的基线潜力，并探索用户对不同组成部分的响应，特别是与现有人类电影评论相比。我们发现参与者没有提供不同电影的排名，也没有对每部电影的质量分数作出不同的评价。但参与者确实将已经见过的电影的评论标记为更好。我们还探究每部电影评论文本中各个重要方面，参与者认为哪些方面是重要的。总的来说，我们发现现代LLM是可靠的推荐解释来源，我们将在未来进一步探索个性化文本解释。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/16/cs.AI_2023_09_16/" data-id="cloimip53003vs4885ca3b1ai" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/33/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="page-number" href="/page/33/">33</a><span class="page-number current">34</span><a class="page-number" href="/page/35/">35</a><a class="page-number" href="/page/36/">36</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/35/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
